{
  "author": {
    "id": "anton-abyzov",
    "display_name": "Anton Abyzov",
    "avatar_url": "https://avatars.githubusercontent.com/u/979819?u=b50f0586ea056682b5024c94a5be20d6f19673bd&v=4"
  },
  "marketplaces": [
    {
      "name": "specweave",
      "version": "1.0.33",
      "description": "SpecWeave - Spec-Driven Development Framework. PM-led planning, intelligent model selection, living documentation, multi-tool support.",
      "repo_full_name": "anton-abyzov/specweave",
      "repo_url": "https://github.com/anton-abyzov/specweave",
      "repo_description": "Autonomous AI Development Framework. Build production software with specs, tests, and docs that write themselves. Works with Claude, Cursor, Copilot.",
      "signals": {
        "stars": 30,
        "forks": 3,
        "pushed_at": "2026-02-02T09:55:07Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"specweave\",\n  \"version\": \"1.0.33\",\n  \"description\": \"SpecWeave - Spec-Driven Development Framework. PM-led planning, intelligent model selection, living documentation, multi-tool support.\",\n  \"owner\": {\n    \"name\": \"Anton Abyzov\",\n    \"email\": \"anton.abyzov@gmail.com\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"sw\",\n      \"description\": \"SpecWeave framework - increment lifecycle, living docs, PM-led planning\",\n      \"source\": \"./plugins/specweave\",\n      \"category\": \"development\",\n      \"version\": \"0.25.0\",\n      \"author\": {\n        \"name\": \"Anton Abyzov\",\n        \"email\": \"anton.abyzov@gmail.com\"\n      }\n    },\n    {\n      \"name\": \"sw-github\",\n      \"description\": \"GitHub integration - bidirectional sync, issue tracking, task management\",\n      \"source\": \"./plugins/specweave-github\",\n      \"category\": \"productivity\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Anton Abyzov\",\n        \"email\": \"anton.abyzov@gmail.com\"\n      }\n    },\n    {\n      \"name\": \"sw-jira\",\n      \"description\": \"JIRA integration - bidirectional sync with epics/stories\",\n      \"source\": \"./plugins/specweave-jira\",\n      \"category\": \"productivity\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Anton Abyzov\",\n        \"email\": \"anton.abyzov@gmail.com\"\n      }\n    },\n    {\n      \"name\": \"sw-ado\",\n      \"description\": \"Azure DevOps integration - bidirectional sync with work items\",\n      \"source\": \"./plugins/specweave-ado\",\n      \"category\": \"productivity\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Anton Abyzov\",\n        \"email\": \"anton.abyzov@gmail.com\"\n      }\n    },\n    {\n      \"name\": \"sw-infra\",\n      \"description\": \"Cloud infrastructure - Hetzner, Prometheus, Grafana, monitoring\",\n      \"source\": \"./plugins/specweave-infrastructure\",\n      \"category\": \"development\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Anton Abyzov\",\n        \"email\": \"anton.abyzov@gmail.com\"\n      }\n    },\n    {\n      \"name\": \"sw-ml\",\n      \"description\": \"Machine learning - ML pipelines, training, deployment\",\n      \"source\": \"./plugins/specweave-ml\",\n      \"category\": \"development\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Anton Abyzov\",\n        \"email\": \"anton.abyzov@gmail.com\"\n      }\n    },\n    {\n      \"name\": \"sw-release\",\n      \"description\": \"Release management - version alignment, RC workflows, multi-repo coordination, semantic versioning\",\n      \"source\": \"./plugins/specweave-release\",\n      \"category\": \"development\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Anton Abyzov\",\n        \"email\": \"anton.abyzov@gmail.com\"\n      }\n    },\n    {\n      \"name\": \"sw-kafka\",\n      \"description\": \"Apache Kafka event streaming integration with MCP servers, CLI tools (kcat), Terraform modules, and comprehensive observability stack\",\n      \"source\": \"./plugins/specweave-kafka\",\n      \"category\": \"development\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Anton Abyzov\",\n        \"email\": \"anton.abyzov@gmail.com\"\n      }\n    },\n    {\n      \"name\": \"sw-kafka-streams\",\n      \"description\": \"Kafka Streams library integration - Stream processing with Java/Kotlin, topology design, state stores, windowing, joins, exactly-once semantics\",\n      \"source\": \"./plugins/specweave-kafka-streams\",\n      \"category\": \"development\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Anton Abyzov\",\n        \"email\": \"anton.abyzov@gmail.com\"\n      }\n    },\n    {\n      \"name\": \"sw-n8n\",\n      \"description\": \"n8n workflow automation with Kafka integration - Event-driven workflows, Kafka triggers, producers, no-code/low-code event processing patterns\",\n      \"source\": \"./plugins/specweave-n8n\",\n      \"category\": \"development\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Anton Abyzov\",\n        \"email\": \"anton.abyzov@gmail.com\"\n      }\n    },\n    {\n      \"name\": \"sw-backend\",\n      \"description\": \"Backend development - Node.js, Python, .NET, REST APIs, database design, microservices architecture\",\n      \"source\": \"./plugins/specweave-backend\",\n      \"category\": \"development\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Anton Abyzov\",\n        \"email\": \"anton.abyzov@gmail.com\"\n      }\n    },\n    {\n      \"name\": \"sw-confluent\",\n      \"description\": \"Confluent Cloud integration - Schema Registry, ksqlDB, Kafka Connect, Flink, stream processing, enterprise Kafka features\",\n      \"source\": \"./plugins/specweave-confluent\",\n      \"category\": \"development\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Anton Abyzov\",\n        \"email\": \"anton.abyzov@gmail.com\"\n      }\n    },\n    {\n      \"name\": \"sw-k8s\",\n      \"description\": \"Kubernetes deployment - K8s manifests, Helm charts, GitOps, cluster management, container orchestration\",\n      \"source\": \"./plugins/specweave-kubernetes\",\n      \"category\": \"development\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Anton Abyzov\",\n        \"email\": \"anton.abyzov@gmail.com\"\n      }\n    },\n    {\n      \"name\": \"sw-mobile\",\n      \"description\": \"React Native & Expo mobile development - environment setup, debugging, performance optimization, native modules, testing\",\n      \"source\": \"./plugins/specweave-mobile\",\n      \"category\": \"development\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Anton Abyzov\",\n        \"email\": \"anton.abyzov@gmail.com\"\n      }\n    },\n    {\n      \"name\": \"sw-payments\",\n      \"description\": \"Payment processing - Stripe, PayPal, billing, PCI compliance, subscription management, payment gateways\",\n      \"source\": \"./plugins/specweave-payments\",\n      \"category\": \"development\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Anton Abyzov\",\n        \"email\": \"anton.abyzov@gmail.com\"\n      }\n    },\n    {\n      \"name\": \"sw-frontend\",\n      \"description\": \"Frontend development - React, Next.js, component generation, design systems, UI scaffolding\",\n      \"source\": \"./plugins/specweave-frontend\",\n      \"category\": \"development\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Anton Abyzov\",\n        \"email\": \"anton.abyzov@gmail.com\"\n      }\n    },\n    {\n      \"name\": \"sw-testing\",\n      \"description\": \"Testing framework - E2E with Playwright, unit testing, test coverage, test generation\",\n      \"source\": \"./plugins/specweave-testing\",\n      \"category\": \"development\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Anton Abyzov\",\n        \"email\": \"anton.abyzov@gmail.com\"\n      }\n    },\n    {\n      \"name\": \"sw-figma\",\n      \"description\": \"Figma integration - Design import, component sync, design-to-code workflows\",\n      \"source\": \"./plugins/specweave-figma\",\n      \"category\": \"productivity\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Anton Abyzov\",\n        \"email\": \"anton.abyzov@gmail.com\"\n      }\n    },\n    {\n      \"name\": \"sw-diagrams\",\n      \"description\": \"Architecture diagram generation - Mermaid, C4 Model, sequence diagrams, ER diagrams, deployment diagrams\",\n      \"source\": \"./plugins/specweave-diagrams\",\n      \"category\": \"development\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Anton Abyzov\",\n        \"email\": \"anton.abyzov@gmail.com\"\n      }\n    },\n    {\n      \"name\": \"sw-ui\",\n      \"description\": \"Browser automation and UI tools - Element inspection, automated testing, Playwright integration\",\n      \"source\": \"./plugins/specweave-ui\",\n      \"category\": \"development\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Anton Abyzov\",\n        \"email\": \"anton.abyzov@gmail.com\"\n      }\n    },\n    {\n      \"name\": \"sw-docs\",\n      \"description\": \"Documentation generation and management - Docusaurus, spec-driven docs, API documentation, living documentation\",\n      \"source\": \"./plugins/specweave-docs\",\n      \"category\": \"productivity\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Anton Abyzov\",\n        \"email\": \"anton.abyzov@gmail.com\"\n      }\n    },\n    {\n      \"name\": \"sw-cost\",\n      \"description\": \"Cloud cost optimization - AWS, GCP, Azure cost analysis, right-sizing, reserved instances, savings plans\",\n      \"source\": \"./plugins/specweave-cost-optimizer\",\n      \"category\": \"development\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Anton Abyzov\",\n        \"email\": \"anton.abyzov@gmail.com\"\n      }\n    },\n    {\n      \"name\": \"sw-plugin-dev\",\n      \"description\": \"Plugin development tools - Create, validate, and scaffold SpecWeave plugins, skills, commands, and agents\",\n      \"source\": \"./plugins/specweave-plugin-dev\",\n      \"category\": \"development\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Anton Abyzov\",\n        \"email\": \"anton.abyzov@gmail.com\"\n      }\n    }\n  ]\n}\n",
        "README.md": "# SpecWeave\n\n**The Enterprise Layer for AI Coding.**\n\n*Permanent memory, GitHub/JIRA sync, quality gates, autonomous execution. Ship features while you sleep.*\n\n[![NPM Version](https://img.shields.io/npm/v/specweave?color=brightgreen)](https://www.npmjs.com/package/specweave)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Test & Validate](https://github.com/anton-abyzov/specweave/actions/workflows/test.yml/badge.svg?branch=develop)](https://github.com/anton-abyzov/specweave/actions/workflows/test.yml)\n[![Discord](https://img.shields.io/badge/Discord-Join-5865F2?logo=discord&logoColor=white)](https://discord.gg/UYg4BGJ65V)\n[![YouTube](https://img.shields.io/badge/YouTube-Tutorials-red?logo=youtube&logoColor=white)](https://www.youtube.com/@antonabyzov)\n\n```bash\nnpm install -g specweave   # Requires Node.js 20.12.0+\n```\n\n---\n\n## Quick Demo\n\n```bash\n/sw:increment \"User authentication\"\n/sw:auto                              # Ship while you sleep\n```\n\n**What happens:**\n- AI creates spec + plan + tasks\n- Executes autonomously for **hours**\n- Tests, fixes failures, syncs to GitHub/JIRA\n- You review finished work\n\n```\n[08:23:41] [Planning]      Analyzing T-003: Implement refresh token rotation\n[08:24:12] [Implementing]  Writing src/auth/token-manager.ts\n[08:25:33] [Testing]       Running tests... FAILED\n[08:25:47] [Fixing]        Adjusting implementation...\n[08:26:15] [Testing]       Re-running... PASSED\n[08:26:22] [Done]          T-003 complete. Moving to T-004...\n```\n\n---\n\n## Why SpecWeave?\n\nEvery AI coding tool loses context when the chat ends. SpecWeave creates **permanent documentation**:\n\n```\n.specweave/increments/0001-oauth/\n‚îú‚îÄ‚îÄ spec.md    <- WHAT: User stories, acceptance criteria\n‚îú‚îÄ‚îÄ plan.md    <- HOW: Architecture decisions, tech choices\n‚îî‚îÄ‚îÄ tasks.md   <- DO: Implementation tasks with tests\n```\n\n**After 6 months**: Search \"OAuth\" ‚Üí find exact decisions, who approved, why it was built that way.\n\n---\n\n## Key Differentiators\n\n### Lazy Plugin Loading (99% Token Savings)\n\nSpecWeave loads plugins **on-demand** based on your prompt keywords:\n\n| Scenario | Without Lazy Loading | With Lazy Loading |\n|----------|---------------------|-------------------|\n| Non-SpecWeave work | ~60k tokens | ~500 tokens |\n| SpecWeave work | ~60k tokens | ~60k (when needed) |\n\nSay \"React frontend\" ‚Üí frontend plugin loads. Say \"Kubernetes deploy\" ‚Üí k8s plugin loads. No manual configuration.\n\n### Self-Improving Skills\n\nSpecWeave learns from corrections. When you fix something, it captures the learning:\n\n```markdown\n## Skill Memories\n<!-- Auto-captured by SpecWeave reflect -->\n- Always use `vi.hoisted()` for ESM mocking in Vitest 4.x+\n- Prefer native `fs` over fs-extra in new code\n```\n\nNext time, it won't make the same mistake.\n\n### Structured Documentation (No Root Bloat)\n\nEverything stays organized in `.specweave/`:\n\n```\n.specweave/\n‚îú‚îÄ‚îÄ increments/####-name/     # Feature specs + tasks\n‚îú‚îÄ‚îÄ docs/internal/            # Living documentation\n‚îÇ   ‚îú‚îÄ‚îÄ architecture/adr/     # Architecture Decision Records\n‚îÇ   ‚îî‚îÄ‚îÄ specs/                # Feature specifications\n‚îî‚îÄ‚îÄ config.json               # Project configuration\n```\n\nYour project root stays clean. No scattered markdown files.\n\n### Deep Interview Mode (NEW)\n\nFor complex features, enable **Deep Interview Mode** during init. Claude asks 40+ questions about architecture, integrations, UI/UX, and tradeoffs before creating specifications:\n\n```\nDeep Interview Mode\n\nFor big features, Claude can ask 40+ questions about architecture,\nintegrations, UI/UX, and tradeoffs before creating specifications.\n\nEnable Deep Interview Mode? [y/N]\n```\n\nInspired by [Thariq's workflow](https://x.com/trq212/status/2005315275026260309) (Claude Code creator): *\"For big features Claude might ask me 40+ questions and I end up with a much more detailed spec.\"*\n\n### 15+ AI Agents Working Together\n\n| Agent | Role |\n|-------|------|\n| **PM** | Requirements, user stories, acceptance criteria |\n| **Architect** | System design, ADRs, tech decisions |\n| **QA Lead** | Test strategy, quality gates |\n| **Security** | OWASP review, vulnerability scanning |\n| **DevOps** | CI/CD, infrastructure, deployment |\n\nAgents auto-activate based on context. Mention \"security\" ‚Üí security expertise loads.\n\n### LSP Integration (100x Faster Code Understanding)\n\nSpecWeave leverages **Language Server Protocol** for semantic code intelligence:\n\n| Operation | Without LSP | With LSP |\n|-----------|-------------|----------|\n| Find all references | Grep + read 15 files (~10K tokens) | Semantic query (~500 tokens) |\n| Check type errors | Build + parse output (~5K tokens) | getDiagnostics (~1K tokens) |\n| Navigate to definition | Grep + verify (~8K tokens) | goToDefinition (~200 tokens) |\n\n**LSP plugins work automatically** when editing code. Edit a `.cs` file ‚Üí `csharp-lsp` activates. Edit `.ts` ‚Üí `typescript-lsp` activates. No configuration needed.\n\n```bash\n# Install language servers for your stack\nnpm install -g typescript-language-server typescript  # TypeScript\npip install pyright                                    # Python\ndotnet tool install -g csharp-ls                      # C#\n```\n\n**[Full LSP Guide ‚Üí](https://spec-weave.com/docs/guides/lsp-integration)**\n\n---\n\n## Install\n\n### Prerequisites\n\n**Node.js 20.12.0+** required (recommend Node.js 22 LTS).\n\n```bash\nnode --version   # Check version\n```\n\n> **Getting `SyntaxError: Unexpected token 'with'`?** Your Node.js is too old. [Upgrade instructions ‚Üí](https://spec-weave.com/docs/guides/troubleshooting/common-errors#node-version-error)\n\n### New Project\n\n```bash\nnpm install -g specweave\nmkdir my-app && cd my-app\nspecweave init .\n```\n\nThen describe what you want in Claude Code:\n```\n\"Build a calculator app with React\"\n```\n\n### Existing Project\n\n```bash\nnpm install -g specweave\ncd your-project\nspecweave init .\n```\n\nThen:\n```bash\n/sw:increment \"Add dark mode\"   # Create spec + plan + tasks\n/sw:auto                        # Ship while you sleep\n```\n\n---\n\n## Core Commands\n\n| Command | Purpose |\n|---------|---------|\n| `/sw:increment \"feature\"` | Create spec + plan + tasks |\n| `/sw:auto` | Autonomous execution (hours) |\n| `/sw:do` | Execute one task at a time |\n| `/sw:done 0001` | Close with quality validation |\n| `/sw:sync-progress` | Push to GitHub/JIRA/ADO |\n| `/sw:next` | Auto-close + suggest next |\n\n**[53 total commands ‚Üí](https://spec-weave.com/docs/commands/overview)**\n\n---\n\n## External Integrations\n\n| Platform | Capabilities |\n|----------|--------------|\n| **GitHub** | Issues, PRs, milestones, bidirectional sync |\n| **JIRA** | Epics, stories, status sync |\n| **Azure DevOps** | Work items, area paths |\n\n**Automatic sync**: When you close an increment (`/sw:done`), external tools update immediately. Task progress syncs at session end.\n\n```bash\n/sw:sync-progress         # Manual: Push updates to ALL tools\n/sw-github:sync 0001      # Manual: Sync specific increment to GitHub\n/sw-jira:sync 0001        # Manual: Sync specific increment to JIRA\n```\n\n---\n\n## Works On Everything\n\n| Scenario | What Happens |\n|----------|-------------|\n| **10-year legacy codebase** | Brownfield analysis detects doc gaps |\n| **Weekend MVP** | Full spec-driven development |\n| **50-team enterprise** | Multi-project sync to JIRA/ADO |\n\n---\n\n## CLI Commands\n\n| Command | Purpose |\n|---------|---------|\n| `specweave init .` | Initialize project |\n| `specweave update` | **Full update**: CLI + plugins + instructions |\n\n### Troubleshooting\n\nRun `specweave update` - this fixes 98% of issues (updates CLI, plugins, and instructions).\n\n---\n\n## Requirements\n\n- **Node.js 20.12.0+** (recommend 22 LTS)\n- Any AI coding tool (Claude Code recommended)\n- Git repository\n\n---\n\n## Built With SpecWeave\n\n> This framework builds itself. Every feature, bug fix, and release is spec-driven.\n\n[![Deploy Frequency](https://img.shields.io/badge/dynamic/json?url=https://raw.githubusercontent.com/anton-abyzov/specweave/develop/.specweave/metrics/dora-latest.json&query=$.metrics.deploymentFrequency.value&label=Deploy%20Frequency&suffix=/month&color=brightgreen)](https://github.com/anton-abyzov/specweave/blob/develop/.specweave/docs/internal/delivery/dora-metrics.md)\n[![Features](https://img.shields.io/badge/Features-140+-blue)](https://github.com/anton-abyzov/specweave/tree/develop/.specweave/increments)\n\n**[Browse our increments ‚Üí](https://github.com/anton-abyzov/specweave/tree/develop/.specweave/increments)**\n\n---\n\n## Documentation\n\n**[spec-weave.com](https://spec-weave.com)** - Full documentation, guides, and examples.\n\n---\n\n## Community\n\n[Discord](https://discord.gg/UYg4BGJ65V) | [YouTube](https://www.youtube.com/@antonabyzov) | [GitHub Issues](https://github.com/anton-abyzov/specweave/issues)\n\n---\n\n## License\n\nMIT - [github.com/anton-abyzov/specweave](https://github.com/anton-abyzov/specweave)\n",
        "plugins/specweave-ml/README.md": "# SpecWeave ML Plugin\n\n**Complete ML/AI workflow integration for SpecWeave - From experiment tracking to production deployment**\n\nTransform chaotic ML experimentation into disciplined, reproducible ML engineering using SpecWeave's increment-based workflow.\n\n---\n\n## üéØ What This Plugin Does\n\nBrings the same engineering discipline to ML that SpecWeave brings to software:\n\n- ‚úÖ **ML as Increments**: Every ML feature is a spec ‚Üí plan ‚Üí tasks ‚Üí implement workflow\n- ‚úÖ **Experiment Tracking**: All experiments logged, versioned, and tied to increments\n- ‚úÖ **Reproducibility**: Reproduce any experiment from any increment\n- ‚úÖ **Living Documentation**: ML decisions captured in architecture docs\n- ‚úÖ **Production Ready**: Deployment artifacts, monitoring, A/B testing built-in\n\n**The Problem**: ML development is often chaotic‚ÄîJupyter notebooks with no version control, experiments without documentation, models deployed with no reproducibility.\n\n**The Solution**: SpecWeave ML plugin brings software engineering discipline to data science.\n\n---\n\n## üöÄ Quick Start\n\n### Installation\n\n```bash\n# Install SpecWeave ML plugin\n/plugin install sw-ml@specweave\n\n# Verify installation\n/plugin list\n# Should show: specweave-ml (13 skills, 1 agent, 3 commands)\n```\n\n### Your First ML Increment\n\n```bash\n# Create ML increment\n/sw:inc \"build recommendation model\"\n\n# The ml-pipeline-orchestrator skill activates and creates:\n# .specweave/increments/0042-recommendation-model/\n# ‚îú‚îÄ‚îÄ spec.md            # ML requirements, success metrics\n# ‚îú‚îÄ‚îÄ plan.md            # Pipeline architecture\n# ‚îú‚îÄ‚îÄ tasks.md           # Implementation tasks\n# ‚îú‚îÄ‚îÄ tests.md           # Evaluation criteria\n# ‚îú‚îÄ‚îÄ experiments/       # Experiment tracking\n# ‚îú‚îÄ‚îÄ data/              # Data samples, schemas\n# ‚îú‚îÄ‚îÄ models/            # Trained models\n# ‚îî‚îÄ‚îÄ notebooks/         # Exploratory notebooks\n\n# Execute ML tasks\n/sw:do\n\n# The skill guides you through:\n# 1. Data exploration\n# 2. Feature engineering\n# 3. Baseline models (mandatory!)\n# 4. Candidate models (3-5 algorithms)\n# 5. Hyperparameter tuning\n# 6. Comprehensive evaluation\n# 7. Model explainability\n# 8. Deployment preparation\n```\n\n---\n\n## üß† Thirteen Comprehensive Skills\n\n### Core ML Lifecycle (5 skills)\n\n#### 1. **ml-pipeline-orchestrator**\n\nOrchestrates complete ML pipelines within SpecWeave increments.\n\n**Activates for**: \"ML pipeline\", \"train model\", \"build ML system\"\n\n**What it does**:\n- Creates ML-specific increment structure\n- Generates ML spec (problem definition, success metrics, data requirements)\n- Guides through data ‚Üí train ‚Üí eval ‚Üí deploy workflow\n- Auto-tracks all experiments to increment folder\n- Ensures baseline comparison, cross-validation, explainability\n\n**Example**:\n```bash\nUser: \"Build a fraud detection model\"\n\nSkill creates increment 0051-fraud-detection:\n- spec.md: Binary classification, 99% precision target\n- plan.md: Imbalanced data handling, threshold tuning\n- tasks.md: EDA ‚Üí baseline ‚Üí XGBoost/LightGBM ‚Üí SHAP ‚Üí deploy\n- experiments/: Tracks all model attempts\n```\n\n### 2. **experiment-tracker**\n\nManages ML experiment tracking with MLflow, W&B, or built-in tracking.\n\n**Activates for**: \"track experiments\", \"MLflow\", \"wandb\", \"compare experiments\"\n\n**What it does**:\n- Auto-configures tracking tools to log to increment folders\n- Tracks params, metrics, artifacts for every experiment\n- Generates experiment comparison reports\n- Syncs experiment findings to living docs\n\n**Example**:\n```python\nfrom specweave import track_experiment\n\n# Automatically logs to: .specweave/increments/0042.../experiments/\nwith track_experiment(\"xgboost-v1\") as exp:\n    model.fit(X_train, y_train)\n    exp.log_metric(\"accuracy\", 0.87)\n    exp.save_model(model, \"model.pkl\")\n\n# Creates:\n# - params.json\n# - metrics.json\n# - model.pkl\n# - metadata.yaml\n```\n\n### 3. **model-evaluator**\n\nComprehensive model evaluation with multiple metrics and statistical testing.\n\n**Activates for**: \"evaluate model\", \"model metrics\", \"compare models\"\n\n**What it does**:\n- Computes classification/regression/ranking metrics\n- Generates confusion matrices, ROC curves, residual plots\n- Performs cross-validation with confidence intervals\n- Compares models statistically (vs baseline, vs previous version)\n\n**Example**:\n```python\nfrom specweave import ModelEvaluator\n\nevaluator = ModelEvaluator(model, X_test, y_test, increment=\"0042\")\nreport = evaluator.evaluate_all()\n\n# Generates:\n# - evaluation-report.md\n# - confusion_matrix.png\n# - roc_curve.png\n# - comparison.md (vs baseline)\n```\n\n### 4. **model-explainer**\n\nModel interpretability using SHAP, LIME, and feature importance.\n\n**Activates for**: \"explain model\", \"SHAP\", \"feature importance\"\n\n**What it does**:\n- Generates global explanations (feature importance, partial dependence)\n- Generates local explanations (SHAP/LIME for individual predictions)\n- Creates human-readable explanation reports\n- Critical for trust, debugging, regulatory compliance\n\n**Example**:\n```python\nfrom specweave import ModelExplainer\n\nexplainer = ModelExplainer(model, X_train, increment=\"0042\")\nexplainer.generate_all_reports()\n\n# Creates:\n# - feature-importance.png\n# - shap-summary.png\n# - pdp-plots/\n# - local-explanations/\n# - explainability-report.md\n```\n\n#### 5. **ml-deployment-helper**\n\nPrepares models for production with APIs, containers, monitoring, A/B testing.\n\n**Activates for**: \"deploy model\", \"production deployment\", \"model API\"\n\n**What it does**:\n- Generates FastAPI endpoints for model serving\n- Creates Dockerfiles for containerization\n- Sets up Prometheus/Grafana monitoring\n- Configures A/B testing infrastructure\n- Load tests models before deployment\n\n**Example**:\n```python\nfrom specweave import create_model_api\n\napi = create_model_api(\n    model_path=\"models/model-v3.pkl\",\n    increment=\"0042\",\n    framework=\"fastapi\"\n)\n\n# Creates:\n# - api/main.py\n# - api/models.py\n# - Dockerfile\n# - requirements.txt\n# - monitoring/\n# - ab-test/\n```\n\n### ML Engineering (2 skills)\n\n#### 6. **feature-engineer**\n\nComprehensive feature engineering: data quality assessment, feature creation, selection, transformation, and validation.\n\n**Activates for**: \"feature engineering\", \"create features\", \"data preprocessing\", \"encode categorical\"\n\n**What it does**:\n- **Phase 1**: Data quality assessment (missing values, outliers, data types)\n- **Phase 2**: Feature creation (temporal, aggregation, interaction, ratio, binning, text features)\n- **Phase 3**: Feature selection (correlation, variance, statistical, model-based, RFE)\n- **Phase 4**: Feature transformation (scaling, encoding, log transform, power transform)\n- **Phase 5**: Feature validation (data leakage detection, distribution drift, missing/invalid values)\n\n#### 7. **automl-optimizer**\n\nAutomated machine learning with intelligent hyperparameter optimization and model selection.\n\n**Activates for**: \"automl\", \"hyperparameter tuning\", \"optimize hyperparameters\", \"neural architecture search\"\n\n**What it does**:\n- Bayesian hyperparameter optimization (Optuna, Hyperopt)\n- Automated algorithm selection (tries multiple models)\n- Neural architecture search for deep learning\n- Intelligent search space exploration\n- Multi-objective optimization (accuracy + speed)\n\n### Domain-Specific Pipelines (2 skills)\n\n#### 8. **cv-pipeline-builder**\n\nComputer vision ML pipelines for images: classification, object detection, segmentation.\n\n**Activates for**: \"computer vision\", \"image classification\", \"object detection\", \"CNN\", \"YOLO\"\n\n**What it does**:\n- Image preprocessing and data augmentation\n- CNN architectures (ResNet, EfficientNet, Vision Transformer)\n- Transfer learning from ImageNet\n- Object detection (YOLO, Faster R-CNN)\n- Semantic segmentation (U-Net, DeepLab)\n\n#### 9. **nlp-pipeline-builder**\n\nNatural language processing pipelines: text classification, NER, sentiment analysis, generation.\n\n**Activates for**: \"nlp\", \"text classification\", \"sentiment analysis\", \"BERT\", \"transformers\"\n\n**What it does**:\n- Text preprocessing and tokenization\n- Transformer models (BERT, RoBERTa, GPT)\n- Fine-tuning on custom datasets\n- Named entity recognition\n- Text generation\n- Sentiment analysis\n\n### Additional Domain-Specific Skills (4 skills) ‚ú® NEW\n\n#### 10. **time-series-forecaster**\n\nTime series forecasting with ARIMA, Prophet, LSTM, and statistical methods.\n\n**Activates for**: \"time series\", \"forecasting\", \"predict future\", \"ARIMA\", \"Prophet\", \"sales forecast\"\n\n**What it does**:\n- Statistical methods (ARIMA, seasonal decomposition, stationarity testing)\n- Prophet (Facebook) - Handles multiple seasonality + holidays\n- Deep learning (LSTM, GRU) - Complex patterns, multivariate forecasting\n- Multivariate forecasting (VAR) - Multiple related time series\n- Time series-specific validation (no data leakage, temporal split)\n\n#### 11. **anomaly-detector**\n\nAnomaly and outlier detection using Isolation Forest, One-Class SVM, autoencoders.\n\n**Activates for**: \"anomaly detection\", \"fraud detection\", \"outlier detection\", \"intrusion detection\"\n\n**What it does**:\n- Statistical methods (Z-score, IQR)\n- Isolation Forest (general purpose, high-dimensional)\n- One-Class SVM (trained on normal data only)\n- Autoencoders (deep learning, complex patterns)\n- LOF (Local Outlier Factor) - Density-based anomalies\n\n#### 12. **data-visualizer**\n\nAutomated data visualization for EDA, model performance, and business reporting.\n\n**Activates for**: \"visualize data\", \"EDA\", \"create plots\", \"confusion matrix\", \"ROC curve\"\n\n**What it does**:\n- EDA visualizations (distributions, correlations, missing values, outliers)\n- Model performance (confusion matrix, ROC curve, learning curves, calibration)\n- Feature analysis (importance, SHAP, partial dependence)\n- Time series plots (decomposition, ACF/PACF, forecast with intervals)\n- Interactive dashboards (Plotly, Grafana)\n\n#### 13. **model-registry**\n\nCentralized model versioning, staging, and lifecycle management.\n\n**Activates for**: \"model registry\", \"model versioning\", \"deploy to production\", \"rollback model\"\n\n**What it does**:\n- Model versioning (semantic versioning: major.minor.patch)\n- Staging pipeline (dev ‚Üí staging ‚Üí production ‚Üí archived)\n- Model metadata tracking (performance, hyperparameters, lineage)\n- Model lineage (data ‚Üí features ‚Üí training ‚Üí deployment)\n- Safe rollback (one-command revert to stable version)\n\n---\n\n## ü§ñ ML Engineer Agent\n\nSpecialized agent for ML system design and implementation.\n\n**When to use**: Complex ML decisions, architecture reviews, best practices guidance\n\n**Expertise**:\n- Algorithm selection (when to use XGBoost vs neural nets)\n- Hyperparameter tuning strategies\n- Preventing ML anti-patterns (data leakage, overfitting, class imbalance)\n- Production ML architecture\n- MLOps best practices\n\n**Example**:\n```bash\nUser: \"My model has 95% train accuracy but 65% test accuracy\"\n\nML Engineer Agent:\n\"Classic overfitting. Here's the fix:\n1. Reduce model complexity (max_depth, layers)\n2. Add regularization (L1/L2, dropout)\n3. More training data if possible\n4. Feature selection (remove noisy features)\n\nFor XGBoost:\nparams = {\n    'max_depth': 3,  # Reduce from default 6\n    'min_child_weight': 5,\n    'gamma': 0.1,\n    'subsample': 0.8\n}\n\nTrack experiments in increment 0042 so we see what works.\"\n```\n\n---\n\n## üéì Commands\n\n### `/ml:evaluate <increment_id>`\n\nEvaluate ML model with comprehensive metrics.\n\n```bash\n/ml:evaluate 0042\n\n# Generates:\n# - evaluation-report.md\n# - Confusion matrix, ROC curves\n# - Cross-validation results\n# - Baseline comparison\n# - Statistical significance tests\n```\n\n### `/ml:explain <increment_id>`\n\nGenerate model explainability reports.\n\n```bash\n/ml:explain 0042\n\n# Generates:\n# - feature-importance.png\n# - shap-summary.png\n# - pdp-plots/\n# - local-explanations/\n# - explainability-report.md\n```\n\n### `/ml:deploy <increment_id>`\n\nGenerate deployment artifacts.\n\n```bash\n/ml:deploy 0042\n\n# Generates:\n# - FastAPI app (api/)\n# - Dockerfile\n# - Monitoring (monitoring/)\n# - A/B test infrastructure (ab-test/)\n# - Load test results\n# - DEPLOYMENT.md runbook\n```\n\n---\n\n## üí° Complete ML Workflow Example\n\n### Step 1: Create ML Increment\n\n```bash\n/sw:inc \"build product recommendation model\"\n```\n\n**What happens**:\n- ml-pipeline-orchestrator skill activates\n- Creates increment: `0042-product-recommendation-model`\n- Generates ML-specific spec.md, plan.md, tasks.md\n\n**Generated spec.md**:\n```markdown\n## ML Problem Definition\n- Problem type: Ranking (collaborative filtering)\n- Input: User behavior history (clicks, purchases)\n- Output: Top-10 product recommendations\n- Success metrics: Precision@10 > 0.25, NDCG@10 > 0.30\n\n## Data Requirements\n- Training data: 6 months user interactions\n- Validation: Last month (time-based split)\n- Features: User profile, product attributes, interaction history\n\n## Model Requirements\n- Latency: <100ms inference\n- Throughput: 1000 req/sec\n- Explainability: Show why products recommended\n```\n\n### Step 2: Execute ML Tasks\n\n```bash\n/sw:do\n```\n\n**Guided workflow**:\n\n**Task 1: Data Exploration**\n```python\n# Auto-generated EDA template\nfrom specweave import track_experiment\n\nwith track_experiment(\"exp-001-eda\", increment=\"0042\") as exp:\n    df = pd.read_csv(\"data/interactions.csv\")\n    \n    exp.log_param(\"dataset_size\", len(df))\n    exp.log_metric(\"unique_users\", df[\"user_id\"].nunique())\n    exp.log_metric(\"unique_products\", df[\"product_id\"].nunique())\n    \n    # Auto-generates: eda-summary.md\n```\n\n**Task 3: Train Baseline**\n```python\n# Baseline models (mandatory!)\nbaselines = [\"random\", \"popularity\"]\n\nfor strategy in baselines:\n    with track_experiment(f\"baseline-{strategy}\", increment=\"0042\") as exp:\n        model = BaselineRecommender(strategy=strategy)\n        model.fit(interactions)\n        \n        precision_10 = evaluate_recommendations(model, test_users)\n        exp.log_metric(\"precision@10\", precision_10)\n```\n\n**Task 4: Train Candidate Models**\n```python\n# Try multiple algorithms\ncandidates = {\n    \"collaborative-filtering\": CollaborativeFiltering(),\n    \"matrix-factorization\": MatrixFactorization(factors=50),\n    \"neural-cf\": NeuralCollaborativeFiltering(layers=[64, 32])\n}\n\nfor name, model in candidates.items():\n    with track_experiment(name, increment=\"0042\") as exp:\n        model.fit(train_interactions)\n        \n        metrics = evaluate_model(model, test_users)\n        exp.log_metrics(metrics)\n        exp.save_model(model, f\"{name}.pkl\")\n```\n\n**Task 6: Model Evaluation**\n```python\n# Comprehensive evaluation\nevaluator = ModelEvaluator(\n    model=best_model,\n    test_data=test_users,\n    increment=\"0042\"\n)\n\nreport = evaluator.evaluate_all()\n# Generates: evaluation-report.md with all metrics\n```\n\n**Task 7: Model Explainability**\n```python\n# Generate SHAP explanations\nexplainer = ModelExplainer(best_model, train_data, increment=\"0042\")\nexplainer.generate_all_reports()\n\n# Creates:\n# - feature-importance.png\n# - shap-summary.png\n# - example-recommendations-explained.md\n```\n\n### Step 3: Validate Increment\n\n```bash\n/sw:validate 0042\n```\n\n**Checks**:\n- ‚úÖ All experiments logged\n- ‚úÖ Best model saved (matrix-factorization)\n- ‚úÖ Evaluation metrics documented\n- ‚úÖ Model meets success criteria (precision@10=0.28 > 0.25 target)\n- ‚úÖ Explainability artifacts present\n\n### Step 4: Complete Increment\n\n```bash\n/sw:done 0042\n```\n\n**Generates COMPLETION-SUMMARY.md**:\n```markdown\n## Product Recommendation Model - COMPLETE\n\n### Experiments Run: 5\n1. baseline-random: precision@10=0.05\n2. baseline-popularity: precision@10=0.12\n3. collaborative-filtering: precision@10=0.22\n4. matrix-factorization: precision@10=0.28 ‚úÖ BEST\n5. neural-cf: precision@10=0.26\n\n### Best Model\n- Algorithm: Matrix Factorization (50 factors)\n- Metrics: precision@10=0.28, ndcg@10=0.32\n- Training time: 12 min\n- Model size: 8 MB\n- Inference latency: 35ms (target: <100ms)\n\n### Deployment Ready\n- ‚úÖ Meets accuracy target (precision@10 > 0.25)\n- ‚úÖ Latency acceptable (<100ms)\n- ‚úÖ Explainability: Top factors computed\n- ‚úÖ A/B test plan documented\n```\n\n### Step 5: Deploy to Production\n\n```bash\n# Create deployment increment\n/sw:inc \"0043-deploy-recommendation-model\"\n\n# Generate deployment artifacts\n/ml:deploy 0042\n\n# Creates:\n# - api/ (FastAPI app)\n# - Dockerfile\n# - monitoring/ (Grafana dashboards)\n# - ab-test/ (10% traffic to new model)\n# - load-tests/ (benchmarked at 1000 RPS)\n```\n\n### Step 6: Sync Living Docs\n\n```bash\n/sw:sync-docs update\n```\n\n**Updates**:\n```markdown\n<!-- .specweave/docs/internal/architecture/ml-models.md -->\n\n## Recommendation Model (Increment 0042)\n\n### Algorithm\nMatrix Factorization with 50 latent factors\n\n### Performance\n- Precision@10: 0.28 (44% better than popularity baseline)\n- NDCG@10: 0.32\n- Inference: 35ms\n\n### Why This Model?\n- Tried 5 approaches (random, popularity, CF, MF, neural)\n- MF best balance of accuracy and speed\n- Neural CF only 2% better but 10x slower\n\n### Deployment\n- Deployed: 2024-01-15\n- A/B test: 10% traffic\n- Monitoring: Grafana dashboard (link)\n```\n\n---\n\n## üèÜ ML Best Practices (Enforced)\n\n### 1. Always Compare to Baseline\n\n```python\n# The skill REQUIRES baseline models\nbaselines = [\"random\", \"majority\", \"popularity\", \"rule-based\"]\n\n# New model must beat best baseline by significant margin (20%+)\nif new_model_accuracy < baseline_accuracy * 1.2:\n    warn(\"New model not significantly better than baseline\")\n```\n\n### 2. Always Use Cross-Validation\n\n```python\n# Never trust single train/test split\ncv_scores = cross_val_score(model, X, y, cv=5)\n\nif cv_scores.std() > 0.1:\n    warn(\"High variance - model unstable across folds\")\n```\n\n### 3. Always Track Experiments\n\n```python\n# Every experiment logged, no exceptions\nwith track_experiment(\"xgboost-v1\", increment=\"0042\") as exp:\n    exp.log_params(params)\n    exp.log_metrics(metrics)\n    exp.save_model(model)\n    exp.log_note(\"Why this configuration\")\n```\n\n### 4. Always Explain Models\n\n```python\n# Production models MUST be explainable\nexplainer = ModelExplainer(model, X_train)\nexplainer.generate_all_reports(increment=\"0042\")\n\n# No \"black boxes\" in production\n```\n\n### 5. Always Load Test Before Production\n\n```python\n# Benchmark performance before deploying\nload_test_results = load_test_model(\n    api_url=api_url,\n    target_rps=100,\n    duration=60\n)\n\nif load_test_results[\"p95_latency\"] > 100:  # ms\n    raise DeploymentError(\"Latency too high\")\n```\n\n---\n\n## üîß Configuration\n\n### MLflow Integration\n\n```python\n# Auto-configured to log to increment\nimport mlflow\nfrom specweave import configure_mlflow\n\nconfigure_mlflow(increment=\"0042\")\n\n# All MLflow logs ‚Üí .specweave/increments/0042.../experiments/\n```\n\n### Weights & Biases Integration\n\n```python\nimport wandb\nfrom specweave import configure_wandb\n\nconfigure_wandb(increment=\"0042\")\n\n# W&B project = increment ID\n# Logs both to W&B dashboard + local increment folder\n```\n\n### Custom Tracking Backend\n\n```python\nfrom specweave import register_tracking_backend\n\n# Use your own tracking system\nregister_tracking_backend(MyCustomTracker)\n```\n\n---\n\n## üìä Integration with SpecWeave\n\n### With Increments\n\nAll ML work is an increment:\n```\n0042-recommendation-model       # ML feature development\n0043-deploy-recommendation      # Deployment\n0044-retrain-with-q1-data       # Model retraining\n```\n\n### With Living Docs\n\nML decisions captured in docs:\n```\n.specweave/docs/internal/\n‚îú‚îÄ‚îÄ architecture/\n‚îÇ   ‚îú‚îÄ‚îÄ ml-models.md              # All models documented\n‚îÇ   ‚îú‚îÄ‚îÄ adr/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 0015-use-matrix-factorization.md\n‚îÇ   ‚îî‚îÄ‚îÄ diagrams/\n‚îÇ       ‚îî‚îÄ‚îÄ ml-pipeline.mmd\n‚îî‚îÄ‚îÄ delivery/\n    ‚îî‚îÄ‚îÄ runbooks/\n        ‚îî‚îÄ‚îÄ model-retraining.md\n```\n\n### With GitHub Integration\n\n```bash\n# ML increments sync to GitHub issues\n/sw:github:sync\n\n# Creates issue: \"0042: Build recommendation model\"\n# Tracks: experiments, model versions, deployment\n```\n\n---\n\n## üéØ When to Use This Plugin\n\n**Use specweave-ml when you need to**:\n\n- ‚úÖ Build ML features with same discipline as software\n- ‚úÖ Track experiments systematically (not scattered notebooks)\n- ‚úÖ Ensure reproducibility (anyone can recreate results)\n- ‚úÖ Maintain team knowledge (living docs capture decisions)\n- ‚úÖ Deploy ML models to production (APIs, monitoring, A/B tests)\n- ‚úÖ Comply with regulations (explainability, audit trails)\n\n**Don't use if**:\n\n- Quick exploratory analysis (use notebooks directly)\n- One-off experiments (no need for increment overhead)\n- Non-production ML (prototypes, research)\n\n---\n\n## üöÄ Advanced Features\n\n### Multi-Stage ML Pipelines\n\n```python\nfrom specweave import ExperimentPipeline\n\npipeline = ExperimentPipeline(\"recommendation-full-pipeline\")\n\n# Stage 1: Preprocessing\nwith pipeline.stage(\"preprocessing\") as stage:\n    df_clean = preprocess(df)\n    stage.log_metric(\"rows_after_cleaning\", len(df_clean))\n\n# Stage 2: Feature engineering\nwith pipeline.stage(\"features\") as stage:\n    features = engineer_features(df_clean)\n    stage.log_metric(\"num_features\", features.shape[1])\n\n# Stage 3: Model training\nwith pipeline.stage(\"training\") as stage:\n    model = train_model(features)\n    stage.log_metric(\"accuracy\", accuracy)\n\n# Logs entire pipeline with stage dependencies\n```\n\n### Model Registry\n\n```python\nfrom specweave import ModelRegistry\n\n# Register model versions\nregistry = ModelRegistry()\n\nregistry.register(\n    model_path=\"models/model-v3.pkl\",\n    name=\"recommendation-model\",\n    version=\"0042-v3\",\n    metadata={\"accuracy\": 0.87, \"training_date\": \"2024-01-15\"}\n)\n\n# Production deployment\nregistry.promote_to_production(\"0042-v3\")\n\n# Rollback if needed\nregistry.rollback_to(\"0042-v2\")\n```\n\n### Feature Store Integration\n\n```python\nfrom specweave import FeatureStore\n\n# Define features in increment\nfeatures = FeatureStore(increment=\"0042\")\n\nfeatures.register(\n    name=\"user_7day_purchase_count\",\n    definition=\"COUNT(purchases) WHERE date >= NOW() - 7 days\",\n    type=\"int\",\n    category=\"user\"\n)\n\n# Use features across increments\nfeatures_df = features.get_features([\"user_7day_purchase_count\"])\n```\n\n---\n\n## üìñ Resources\n\n- **SpecWeave Docs**: https://spec-weave.com\n- **ML Plugin Guide**: https://spec-weave.com/plugins/ml\n- **Example Increments**: [examples/ml/](/examples/ml/)\n- **Community**: https://github.com/anton-abyzov/specweave/discussions\n\n---\n\n## ü§ù Contributing\n\nWant to improve the ML plugin?\n\n```bash\n# Clone SpecWeave\ngit clone https://github.com/anton-abyzov/specweave\n\n# Navigate to ML plugin\ncd plugins/specweave-ml\n\n# Make improvements\nvim skills/ml-pipeline-orchestrator/SKILL.md\n\n# Test\nnpm test\n\n# Submit PR\ngit push && create PR\n```\n\n---\n\n## üìù License\n\nMIT License - See [LICENSE](../../LICENSE)\n\n---\n\n**Transform ML chaos into ML discipline with SpecWeave ML Plugin** ü§ñüìä‚ú®\n\n**Version**: 1.0.0\n**Last Updated**: 2024-01-15\n",
        "plugins/specweave-release/README.md": "# SpecWeave Release Management Plugin\n\n**Comprehensive release management for single-repo, multi-repo, and monorepo architectures.**\n\n## Overview\n\nThe **specweave-release** plugin provides end-to-end release management capabilities including:\n\n- **Release Strategy Design** - Analyze architecture and recommend optimal versioning approach\n- **Version Alignment** - Coordinate versions across multiple repositories\n- **RC Workflows** - Manage release candidates from creation to production promotion\n- **Multi-Repo Coordination** - Orchestrate complex releases across dozens of services\n- **Brownfield Detection** - Automatically detect and document existing release patterns\n- **Living Documentation** - Maintain release strategy and version matrices\n\n## Components\n\n### Skills (4 total)\n\n| Skill | Purpose | When to Use |\n|-------|---------|-------------|\n| **release-strategy-advisor** | Analyze architecture and recommend release strategy | \"What release strategy should we use?\" |\n| **release-coordinator** | Coordinate multi-repo releases with dependency management | \"Coordinate release across 5 microservices\" |\n| **version-aligner** | Align versions across repos using semantic versioning | \"Align versions before release\" |\n| **rc-manager** | Manage release candidate lifecycle and testing | \"Create RC for v2.0.0\" |\n\n### Agents (1 total)\n\n| Agent | Purpose | Tools |\n|-------|---------|-------|\n| **release-manager** | Master orchestrator for all release activities | Read, Write, Edit, Bash, Glob, Grep |\n\n### Commands (4+ total)\n\n| Command | Purpose |\n|---------|---------|\n| `/sw-release:init` | Initialize or analyze release strategy |\n| `/sw-release:align` | Align versions across repositories |\n| `/sw-release:rc` | Manage release candidates |\n| `/sw-release:coordinate` | Plan coordinated multi-repo releases |\n| `/sw-release:publish` | Execute releases (single or multi-repo) |\n| `/sw-release:rollback` | Rollback failed releases |\n| `/sw-release:matrix` | Show/update version matrix |\n\n## Installation\n\n**Automatic** (via SpecWeave init):\n```bash\n# Already installed if you ran specweave init\nnpx specweave init .\n```\n\n**Manual** (if needed):\n```bash\n/plugin install sw-release@specweave\n```\n\n## Quick Start\n\n### Greenfield Project (New Release Strategy)\n\n```bash\n# 1. Initialize release strategy\n/sw-release:init\n\n# Answer questions:\n# - How many repositories? [3]\n# - Team structure? [One team]\n# - Deployment frequency? [Weekly]\n# - Coupling? [Moderate]\n\n# ‚Üí Creates: .specweave/docs/internal/delivery/release-strategy.md\n# ‚Üí Recommends: Umbrella versioning strategy\n\n# 2. Create first release\n/sw:increment \"0001-v1-0-0-release\"\n\n# 3. Align versions\n/sw-release:align\n\n# 4. Create RC\n/sw-release:rc create 1.0.0\n\n# 5. Test and promote\n/sw-release:rc promote 1.0.0-rc.3\n```\n\n### Brownfield Project (Existing Release Process)\n\n```bash\n# 1. Analyze existing release patterns\n/sw-release:init\n\n# ‚Üí Detects:\n#   - Git tags (v1.0.0, v1.1.0, ...)\n#   - CI/CD (.github/workflows/release.yml)\n#   - Monorepo tools (Lerna, Nx)\n#   - Current strategy (independent versioning)\n\n# ‚Üí Creates: .specweave/docs/internal/delivery/release-strategy.md\n# ‚Üí Documents: Existing process + recommendations\n\n# 2. Start using SpecWeave for next release\n/sw:increment \"0010-v2-0-0-release\"\n/sw-release:align\n/sw-release:rc create 2.0.0\n```\n\n## Use Cases\n\n### Single-Repo Release\n\n**Scenario**: NPM package with semantic versioning\n\n```bash\n# Current: v1.5.0\n# Goal: Release v2.0.0 (breaking changes)\n\n# 1. Initialize strategy (if not done)\n/sw-release:init\n\n# 2. Create release increment\n/sw:increment \"0050-v2-release\"\n\n# 3. Align version (analyze commits ‚Üí suggest v2.0.0)\n/sw-release:align\n\n# 4. Create RC\n/sw-release:rc create 2.0.0\n# ‚Üí v2.0.0-rc.1 ‚Üí staging ‚Üí testing\n\n# 5. Iterate if needed\n# Fix bugs ‚Üí /sw-release:rc iterate 2.0.0-rc.1\n# ‚Üí v2.0.0-rc.2\n\n# 6. Promote to production\n/sw-release:rc promote 2.0.0-rc.3\n# ‚Üí v2.0.0 ‚Üí gradual rollout\n```\n\n### Multi-Repo Coordinated Release\n\n**Scenario**: 5 microservices with umbrella versioning\n\n```bash\n# Repos:\n# - frontend v4.2.0\n# - backend v2.8.0\n# - api-gateway v3.1.0\n# - auth-service v2.0.0\n# - shared-lib v1.5.0\n\n# Goal: Product v5.0.0 (breaking changes in 3 services)\n\n# 1. Create product release increment\n/sw:increment \"0060-product-v5-release\"\n\n# 2. Analyze and align versions\n/sw-release:align\n# ‚Üí Suggests:\n#   frontend: v5.0.0 (breaking)\n#   backend: v2.9.0 (feature)\n#   api-gateway: v4.0.0 (breaking)\n#   auth-service: v2.0.0 (no change)\n#   shared-lib: v2.0.0 (breaking)\n\n# 3. Create coordinated RC\n/sw-release:rc create-multi product-v5.0.0\n# ‚Üí Tags all repos with rc.1\n\n# 4. Test cross-service\n# ‚Üí Staging deployment (all services)\n# ‚Üí E2E tests (cross-service flows)\n\n# 5. Promote when ready\n/sw-release:rc promote-multi product-v5.0.0-rc.2\n# ‚Üí Wave-by-wave deployment\n# ‚Üí Updates version matrix\n```\n\n### Monorepo Release (Lerna/Nx)\n\n**Scenario**: Lerna monorepo with 12 packages (independent versioning)\n\n```bash\n# Current: 12 packages with different versions\n\n# 1. Initialize strategy (detects Lerna)\n/sw-release:init\n# ‚Üí Detects: Lerna independent mode\n# ‚Üí Documents: Current approach\n\n# 2. Align versions (only changed packages)\n/sw-release:align\n# ‚Üí Analyzes commits per package\n# ‚Üí Suggests bumps for changed packages only\n# ‚Üí Validates inter-package dependencies\n\n# 3. Execute release\n# ‚Üí Integration with Lerna: lerna publish\n# ‚Üí Or manual: npm version + npm publish per package\n```\n\n## Release Strategies\n\n### Lockstep Versioning\n\n**Use When**:\n- Tight coupling between repos\n- Small team (all work together)\n- Breaking changes affect all repos\n\n**How It Works**:\n```yaml\nAll repos share same version:\n  frontend: v3.0.0\n  backend: v3.0.0\n  api: v3.0.0\n\nBump together:\n  Breaking change in any repo ‚Üí All bump to v4.0.0\n```\n\n### Independent Versioning\n\n**Use When**:\n- Loose coupling between repos\n- Large team (autonomous squads)\n- Frequent releases (daily/weekly)\n\n**How It Works**:\n```yaml\nEach repo has own version:\n  frontend: v5.0.0\n  backend: v2.9.0\n  api: v4.0.0\n\nBump independently:\n  frontend breaking ‚Üí v6.0.0\n  backend feature ‚Üí v2.10.0\n  api unchanged ‚Üí v4.0.0\n```\n\n### Umbrella Versioning\n\n**Use When**:\n- Medium/large team\n- Product-level milestones important\n- Services evolve at different rates\n\n**How It Works**:\n```yaml\nProduct version + service versions:\n  Product v5.0.0:\n    frontend: v5.0.0\n    backend: v2.9.0\n    api: v4.0.0\n\nVersion matrix tracks combinations\n```\n\n## Release Candidate Workflow\n\n**Three-Stage Pre-Release**:\n\n```mermaid\ngraph LR\n    A[Alpha] -->|Stable API| B[Beta]\n    B -->|Production Ready| C[RC]\n    C -->|Validated| D[Production]\n```\n\n**Alpha** (Early Development):\n- Tag: v1.0.0-alpha.1, v1.0.0-alpha.2\n- Audience: Internal developers\n- Stability: Unstable (breaking changes OK)\n- Duration: Weeks to months\n\n**Beta** (Feature Complete):\n- Tag: v1.0.0-beta.1, v1.0.0-beta.2\n- Audience: Beta testers, QA team\n- Stability: Mostly stable\n- Duration: 2-6 weeks\n\n**RC** (Production Ready):\n- Tag: v1.0.0-rc.1, v1.0.0-rc.2\n- Audience: Pre-production, canary users\n- Stability: Production-ready\n- Duration: 1-2 weeks\n\n**Production**:\n- Tag: v1.0.0 (final)\n- Audience: All users\n- Stability: Stable\n\n## Living Documentation\n\n**Release Strategy Document**:\n```\n.specweave/docs/internal/delivery/release-strategy.md\n```\n\n**Contains**:\n- Repository overview\n- Versioning strategy (lockstep/independent/umbrella)\n- RC workflow\n- CI/CD integration\n- Changelog management\n- Hotfix strategy\n- Release checklist\n- DORA metrics\n- Decision history\n\n**Version Matrix** (Umbrella Releases):\n```\n.specweave/docs/internal/delivery/version-matrix.md\n```\n\n**Contains**:\n- Product version history\n- Service version mappings\n- Compatibility matrix\n- Breaking changes per release\n\n## Integration with Brownfield Analyzer\n\n**Automatic Release Pattern Detection**:\n\nWhen brownfield-analyzer skill runs, it:\n1. Detects repository structure\n2. Scans git tags for version patterns\n3. Analyzes CI/CD configurations\n4. Checks package manager files\n5. Identifies monorepo tools\n\nThen invokes release-strategy-advisor to:\n- Classify release strategy\n- Document in living docs\n- Suggest improvements\n\n**Example Output**:\n```markdown\n## Detected Release Strategy\n\nType: Monorepo with independent versioning\n\nEvidence:\n  - lerna.json found (12 packages)\n  - Independent mode enabled\n  - Git tags: 145 total (per-package versioning)\n  - CI/CD: GitHub Actions + semantic-release\n  - Frequency: Weekly\n\nRecommendations:\n  ‚úì Current strategy is solid\n  ‚Üí Consider: Umbrella versioning for product milestones\n  ‚Üí Add: Version matrix documentation\n```\n\n## Best Practices\n\n**Version Discipline**:\n- Always follow semantic versioning\n- Never skip version numbers\n- Use pre-release tags for testing\n- Document breaking changes clearly\n\n**Release Safety**:\n- Always use RC for major versions\n- Gradual rollout (canary ‚Üí 10% ‚Üí 25% ‚Üí 50% ‚Üí 100%)\n- Monitor for 1+ hour at each stage\n- Have rollback plan ready\n\n**Documentation**:\n- Update living docs after every release\n- Maintain version matrix for umbrella releases\n- Document lessons learned\n- Link to ADRs for major decisions\n\n**Testing**:\n- Run ALL tests before releasing\n- Validate cross-repo compatibility\n- Load test at 150% expected capacity\n- Security scan for every release\n\n## Configuration\n\n**In `.specweave/config.json`**:\n\n```json\n{\n  \"release\": {\n    \"strategy\": \"independent\",\n    \"rcRequired\": [\"major\"],\n    \"gradualRollout\": {\n      \"canary\": 0.05,\n      \"wave1\": 0.10,\n      \"wave2\": 0.25,\n      \"wave3\": 0.50,\n      \"wave4\": 1.00\n    },\n    \"monitoring\": {\n      \"errorRateThreshold\": 0.05,\n      \"latencyThreshold\": 2.0\n    }\n  }\n}\n```\n\n## Advanced Topics\n\n### Coordinated RC Across Repos\n\n```bash\n# Tag all repos with same RC iteration\n/sw-release:rc create-multi product-v3.0.0\n\n# Creates:\n# - frontend: v5.0.0-rc.1\n# - backend: v3.0.0-rc.1\n# - api-gateway: v4.0.0-rc.1\n\n# Test cross-service compatibility\n# Deploy all to staging\n# Run E2E tests\n\n# Found bugs ‚Üí iterate\n/sw-release:rc iterate-multi product-v3.0.0-rc.1\n\n# Creates:\n# - frontend: v5.0.0-rc.2 (bug fixed)\n# - api-gateway: v4.0.0-rc.2 (bug fixed)\n# - backend: v3.0.0-rc.1 (unchanged)\n\n# All pass ‚Üí promote\n/sw-release:rc promote-multi product-v3.0.0-rc.2\n```\n\n### Hotfix Workflow\n\n```bash\n# Production: v2.0.0\n# Critical bug discovered\n\n# 1. Create hotfix branch from tag\ngit checkout -b hotfix/v2.0.1 v2.0.0\n\n# 2. Fix bug\n# ... make changes ...\n\n# 3. Fast-track testing\nnpm test  # Critical tests only\n\n# 4. Release hotfix\ngit tag v2.0.1\ngit push origin v2.0.1\n\n# 5. Merge back to main\ngit checkout main\ngit merge hotfix/v2.0.1\n```\n\n## Troubleshooting\n\n**Issue**: Version conflicts detected\n```bash\n# Solution: Update dependencies to compatible versions\n# Then retry alignment\n```\n\n**Issue**: RC testing failed\n```bash\n# Solution: Fix issues, iterate to next RC\n/sw-release:rc iterate 1.0.0-rc.1\n```\n\n**Issue**: Canary deployment showing errors\n```bash\n# Solution: Rollback immediately\n/sw-release:rc rollback 1.0.0-rc.3\n```\n\n## Examples\n\nSee `/examples` directory for:\n- Single-repo release workflow\n- Multi-repo coordinated release\n- Monorepo (Lerna) release\n- RC workflow with testing\n- Version matrix for umbrella releases\n\n## Related Documentation\n\n- **SpecWeave Core**: Main framework documentation\n- **Living Docs**: `.specweave/docs/internal/delivery/`\n- **ADRs**: Architecture decision records\n- **Release Strategy**: Project-specific strategy docs\n\n## Support\n\n- **Issues**: https://github.com/anton-abyzov/specweave/issues\n- **Discussions**: https://github.com/anton-abyzov/specweave/discussions\n- **Website**: https://spec-weave.com\n\n---\n\n**Version**: 1.0.0\n**Author**: Anton Abyzov\n**License**: MIT\n",
        "plugins/specweave-kafka/README.md": "# specweave-kafka\n\n**Apache Kafka Event Streaming Integration Plugin for SpecWeave**\n\nComprehensive plugin providing MCP server integration, CLI tools (kcat), Terraform infrastructure modules, and full observability stack (Prometheus/Grafana/OpenTelemetry) for Apache Kafka.\n\n## Features\n\n### üöÄ Core Capabilities\n\n- **MCP Server Integration**: Auto-detect and configure 4 MCP servers (kanapuli, tuannvm, Joel-hanson, Confluent)\n- **CLI Tool Wrappers**: Type-safe TypeScript wrappers for kcat, kcli, kaf, kafkactl\n- **Multi-Platform Support**: Apache Kafka, Confluent Cloud, Redpanda, AWS MSK, Azure Event Hubs\n- **Infrastructure as Code**: Terraform modules for all major platforms\n- **Local Development**: Docker Compose templates (Kafka KRaft, Redpanda)\n- **Observability**: Prometheus, Grafana dashboards, OpenTelemetry instrumentation\n\n### üìö Skills (6)\n\n**Organized for Maximum Efficiency**: Skills are separated by concern for better activation and productivity.\n\n- `kafka-architecture` - Event-driven patterns, CQRS, saga patterns, data modeling, capacity planning\n- `kafka-mcp-integration` - MCP server configuration and operations\n- `kafka-cli-tools` - kcat, kcli, kaf, kafkactl usage patterns\n- `kafka-iac-deployment` - Terraform modules, multi-cloud infrastructure deployment (AWS MSK, Azure Event Hubs, Apache Kafka)\n- `kafka-kubernetes` - Strimzi Operator, Confluent Operator, Bitnami Helm chart deployment patterns\n- `kafka-observability` - Prometheus + Grafana setup, JMX exporter, 5 dashboards, 14 alerting rules, SLO definitions\n\n### ü§ñ Agents (3)\n\n- `kafka-architect` - Architecture decisions, capacity planning, design patterns\n- `kafka-devops` - Deployment, configuration management, troubleshooting\n- `kafka-observability` - Monitoring setup, dashboarding, alerting\n\n### ‚ö° Commands (4)\n\n- `/sw-kafka:deploy` - Deploy Kafka cluster via Terraform (AWS MSK, Azure Event Hubs, Apache Kafka)\n- `/sw-kafka:monitor-setup` - Setup Prometheus/Grafana monitoring stack with 5 dashboards and 14 alerts\n- `/sw-kafka:mcp-configure` - Configure MCP server integration (auto-detects kanapuli, tuannvm, Joel-hanson, Confluent)\n- `/sw-kafka:dev-env` - Setup local development environment (Docker Compose: Kafka KRaft or Redpanda)\n\n## Installation\n\n### Prerequisites\n\n- Node.js 18+\n- Docker 20+ (for local development)\n- Terraform 1.5+ (for infrastructure deployment)\n- Claude Code (SpecWeave runtime)\n\n### Install Plugin\n\n```bash\n# Via SpecWeave marketplace\nspecweave plugin install sw-kafka\n\n# Or via Claude Code plugin system\n/plugin install sw-kafka@specweave\n```\n\n## Quick Start\n\n### 1. Start Local Kafka Cluster\n\n```bash\n/sw-kafka:dev-env start\n```\n\nThis starts a Kafka cluster (KRaft mode) with Schema Registry and Kafka UI on your local machine.\n\n### 2. Configure MCP Server\n\n```bash\n/sw-kafka:mcp-configure\n```\n\nAuto-detects available MCP servers and generates configuration.\n\n### 3. Produce/Consume Messages\n\nUse the provided code templates:\n\n```javascript\n// Producer example (plugins/specweave-kafka/templates/examples/nodejs-producer.js)\nconst { Kafka } = require('kafkajs');\n\nconst kafka = new Kafka({\n  clientId: 'my-producer',\n  brokers: ['localhost:9092']\n});\n\nconst producer = kafka.producer();\nawait producer.connect();\nawait producer.send({\n  topic: 'test-topic',\n  messages: [{ value: 'Hello Kafka!' }]\n});\n```\n\n### 4. Setup Monitoring\n\n```bash\n/sw-kafka:monitor-setup\n```\n\nDeploys Prometheus, Grafana, and pre-built dashboards for Kafka monitoring.\n\n## Architecture\n\n### Skill Coordination\n\nSkills work together in coordinated workflows:\n\n**Deployment Workflow** (`/sw-kafka:deploy`):\n1. `kafka-architect` ‚Üí Cluster sizing and partitioning strategy\n2. `kafka-iac-deployment` ‚Üí Generate Terraform modules\n3. `kafka-kubernetes` ‚Üí Kubernetes manifests (if K8s deployment)\n4. `kafka-observability` ‚Üí Monitoring stack configuration\n\n**Monitoring Workflow** (`/sw-kafka:monitor-setup`):\n1. `kafka-observability` ‚Üí JMX exporter configuration\n2. `kafka-observability` ‚Üí Prometheus scraping setup\n3. `kafka-observability` ‚Üí Grafana dashboard provisioning (5 dashboards)\n4. `kafka-observability` ‚Üí Alerting rules deployment (14 critical/high/warning alerts)\n\n**Local Development** (`/sw-kafka:dev-env`):\n1. `kafka-cli-tools` ‚Üí Docker Compose stack selection (Kafka or Redpanda)\n2. `kafka-observability` ‚Üí Monitoring integration\n3. `kafka-mcp-integration` ‚Üí Configure MCP server for local cluster\n\n### Supported Platforms\n\n| Platform | Auth Methods | Features |\n|----------|-------------|----------|\n| **Apache Kafka** | SASL/SCRAM, mTLS, PLAINTEXT | Full control, KRaft mode support |\n| **Confluent Cloud** | API Keys | Managed, eCKUs, Schema Registry, ksqlDB |\n| **Redpanda** | SASL/SCRAM | Kafka-compatible, faster startup, no JVM |\n| **AWS MSK** | IAM, SASL | AWS integration, CloudWatch metrics |\n| **Azure Event Hubs** | Azure AD | Azure integration, Kafka protocol |\n\n### Terraform Modules\n\n- `apache-kafka/` - Self-hosted Kafka on Kubernetes (Strimzi)\n- `aws-msk/` - AWS MSK cluster provisioning\n- `azure-event-hubs/` - Azure Event Hubs namespace\n- `monitoring/` - Prometheus + Grafana stack\n\n### MCP Servers\n\n| Server | Language | Auth Support | Special Features |\n|--------|----------|--------------|------------------|\n| **kanapuli/mcp-kafka** | Node.js | SASL_PLAINTEXT | Basic operations |\n| **tuannvm/kafka-mcp-server** | Go | SCRAM-SHA-256/512 | Advanced SASL |\n| **Joel-hanson/kafka-mcp-server** | Python | Standard | Claude Desktop integration |\n| **Confluent MCP** | Official | OAuth | Natural language, Flink SQL |\n\n## Usage Examples\n\n**Complete runnable examples** are available in [`examples/`](./examples/):\n- `simple-producer-consumer/` - Basic Kafka operations (beginner)\n- `avro-schema-registry/` - Schema-based serialization (intermediate)\n- `exactly-once-semantics/` - Zero message loss (advanced)\n- `kafka-streams-app/` - Real-time stream processing (advanced)\n- `n8n-workflow/` - No-code event-driven automation (beginner)\n\n### Deploy to AWS MSK\n\n```bash\n/sw-kafka:deploy aws-msk\n```\n\nInteractive prompts guide you through:\n- Instance type selection\n- Storage configuration\n- Authentication setup\n- VPC configuration\n\n### Setup Security (SASL/SCRAM)\n\nUse the provided configuration templates:\n\n```bash\ncp plugins/specweave-kafka/templates/security/sasl-scram-config.properties ./kafka-config.properties\n```\n\nEdit and apply:\n- Broker configuration\n- Client configuration\n- User credentials (use secrets manager)\n\n### Monitor Multiple Clusters\n\n```bash\n/sw-kafka:monitor-setup --multi-cluster\n```\n\nCreates unified Grafana dashboard with cluster selector.\n\n## Testing\n\n```bash\n# Unit tests\nnpm test\n\n# Integration tests (requires Docker)\nnpm run test:integration\n\n# E2E tests (requires Kafka cluster)\nnpm run test:e2e\n\n# Coverage report\nnpm run test:coverage\n```\n\n**Coverage Target**: 85-90%\n\n## Benchmarks\n\nPerformance benchmarks are available in [`benchmarks/`](./benchmarks/):\n\n```bash\n# Run Kafka throughput benchmarks\nnpx ts-node benchmarks/kafka-throughput.benchmark.ts\n```\n\nMeasures: producer/consumer throughput, end-to-end latency (p50/p95/p99), batch size impact, compression comparison, concurrent producers.\n\n**Target**: 100K+ msgs/sec throughput.\n\n## Documentation\n\n- **Getting Started**: `.specweave/docs/public/guides/kafka-getting-started.md`\n- **Advanced Usage**: `.specweave/docs/public/guides/kafka-advanced-usage.md`\n- **Terraform Guide**: `.specweave/docs/public/guides/kafka-terraform.md`\n- **Troubleshooting**: `.specweave/docs/public/guides/kafka-troubleshooting.md`\n- **API Reference**: Generated via TypeDoc\n\n## Contributing\n\nSee [CONTRIBUTING.md](../../CONTRIBUTING.md) for development setup and guidelines.\n\n## Related Plugins\n\n- **specweave-confluent** - Confluent Cloud specific features\n- **specweave-kafka-streams** - Kafka Streams + Red Hat AMQ Streams\n- **specweave-n8n** - n8n workflow automation with Kafka\n\n## License\n\nMIT License - see [LICENSE](../../LICENSE)\n\n## Support\n\n- **Documentation**: https://spec-weave.com/docs/plugins/kafka\n- **Issues**: https://github.com/anton-abyzov/specweave/issues\n- **Discussions**: https://github.com/anton-abyzov/specweave/discussions\n\n---\n\n**Version**: 1.0.0\n**Last Updated**: 2025-11-15\n**Status**: ‚úÖ Production Ready\n",
        "plugins/specweave-kafka-streams/README.md": "# specweave-kafka-streams\n\n**Kafka Streams Library Integration Plugin for SpecWeave**\n\nStream processing with Java/Kotlin using Kafka Streams library - topology patterns, state management, windowing, joins, and testing frameworks.\n\n## Features\n\n### üöÄ Core Capabilities\n\n- **Topology Design**: KStream, KTable, GlobalKTable abstractions\n- **Stream Operations**: Filter, map, flatMap, branch, merge\n- **Joins**: Stream-Stream, Stream-Table, Table-Table, Stream-GlobalKTable\n- **Windowing**: Tumbling, hopping, session, sliding windows\n- **State Stores**: Key-value, window, session stores\n- **Exactly-Once Semantics**: EOS v2 for reliable processing\n- **Testing**: Topology Test Driver for unit testing\n\n### üìö Skills (1)\n\n- `kafka-streams-topology` - Topology design, KStream/KTable, joins, windowing, exactly-once semantics\n\n### ü§ñ Agents (0)\n\n*Agents coming in future releases*\n\n### ‚ö° Commands (0)\n\n*Commands coming in future releases*\n\n## Installation\n\n### Prerequisites\n\n- Java 11+ or Kotlin 1.5+\n- Apache Kafka 2.8+ cluster\n- Gradle or Maven build tool\n\n### Install Plugin\n\n```bash\n# Via SpecWeave marketplace\nspecweave plugin install sw-kafka-streams\n\n# Or via Claude Code plugin system\n/plugin install sw-kafka-streams@specweave\n```\n\n## Quick Start\n\n### 1. Basic Kafka Streams Application\n\n```java\nimport org.apache.kafka.streams.*;\nimport org.apache.kafka.streams.kstream.*;\n\npublic class WordCountApp {\n    public static void main(String[] args) {\n        StreamsBuilder builder = new StreamsBuilder();\n\n        // Input stream\n        KStream<String, String> text = builder.stream(\"text-input\");\n\n        // Word count topology\n        KTable<String, Long> wordCounts = text\n            .flatMapValues(value -> Arrays.asList(value.toLowerCase().split(\"\\\\s+\")))\n            .groupBy((key, word) -> word)\n            .count(Materialized.as(\"word-counts\"));\n\n        // Output stream\n        wordCounts.toStream().to(\"word-counts-output\");\n\n        // Start streams\n        Properties props = new Properties();\n        props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"word-count-app\");\n        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n\n        KafkaStreams streams = new KafkaStreams(builder.build(), props);\n        streams.start();\n    }\n}\n```\n\n### 2. Stream-Table Join (Event Enrichment)\n\n```java\n// Users table (current state)\nKTable<Long, User> users = builder.table(\"users\");\n\n// Click events\nKStream<Long, ClickEvent> clicks = builder.stream(\"clicks\");\n\n// Enrich clicks with user data\nKStream<Long, EnrichedClick> enriched = clicks.leftJoin(\n    users,\n    (click, user) -> new EnrichedClick(\n        click.getPage(),\n        user != null ? user.getName() : \"unknown\",\n        user != null ? user.getEmail() : \"unknown\"\n    )\n);\n\nenriched.to(\"enriched-clicks\");\n```\n\n### 3. Windowed Aggregation\n\n```java\n// Count clicks per user, per 5-minute window\nKTable<Windowed<Long>, Long> clickCounts = clicks\n    .groupByKey()\n    .windowedBy(TimeWindows.ofSizeWithNoGrace(Duration.ofMinutes(5)))\n    .count(Materialized.as(\"click-counts\"));\n\n// Convert to stream for output\nclickCounts.toStream()\n    .map((windowedKey, count) -> {\n        Long userId = windowedKey.key();\n        Instant start = windowedKey.window().startTime();\n        return KeyValue.pair(userId, new WindowedCount(userId, start, count));\n    })\n    .to(\"click-counts-output\");\n```\n\n## Architecture\n\n### Topology Patterns\n\n**Filter and Transform**:\n```java\nKStream<Long, Event> filtered = events\n    .filter((key, value) -> value.isValid())\n    .mapValues(value -> value.toUpperCase());\n```\n\n**Branch by Condition**:\n```java\nMap<String, KStream<Long, Order>> branches = orders\n    .split(Named.as(\"order-\"))\n    .branch((k, v) -> v.getTotal() > 1000, Branched.as(\"high\"))\n    .branch((k, v) -> v.getTotal() > 100, Branched.as(\"medium\"))\n    .defaultBranch(Branched.as(\"low\"));\n```\n\n**Stateful Processing**:\n```java\nKStream<Long, Event> deduplicated = events\n    .transformValues(\n        () -> new DeduplicationTransformer(),\n        \"dedup-store\"\n    );\n```\n\n### State Store Types\n\n**Key-Value Store**:\n```java\nStoreBuilder<KeyValueStore<Long, User>> storeBuilder =\n    Stores.keyValueStoreBuilder(\n        Stores.persistentKeyValueStore(\"users-store\"),\n        Serdes.Long(),\n        userSerde\n    );\n```\n\n**Window Store**:\n```java\nStoreBuilder<WindowStore<Long, Long>> windowStoreBuilder =\n    Stores.windowStoreBuilder(\n        Stores.persistentWindowStore(\"window-store\", Duration.ofMinutes(10), Duration.ofMinutes(1), false),\n        Serdes.Long(),\n        Serdes.Long()\n    );\n```\n\n**Session Store**:\n```java\nStoreBuilder<SessionStore<Long, Long>> sessionStoreBuilder =\n    Stores.sessionStoreBuilder(\n        Stores.persistentSessionStore(\"session-store\", Duration.ofMinutes(30)),\n        Serdes.Long(),\n        Serdes.Long()\n    );\n```\n\n## Usage Examples\n\n### Exactly-Once Semantics\n\n```java\nProperties props = new Properties();\nprops.put(StreamsConfig.APPLICATION_ID_CONFIG, \"my-app\");\nprops.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n\n// Enable EOS v2 (recommended)\nprops.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG,\n    StreamsConfig.EXACTLY_ONCE_V2);\n\n// Commit frequently for low latency\nprops.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 100);\n\nKafkaStreams streams = new KafkaStreams(topology, props);\n```\n\n### Interactive Queries\n\n```java\n// Materialized store for queries\nKTable<Long, Long> counts = events\n    .groupByKey()\n    .count(Materialized.as(\"user-counts\"));\n\n// Query from REST API\nReadOnlyKeyValueStore<Long, Long> store =\n    streams.store(StoreQueryParameters.fromNameAndType(\n        \"user-counts\",\n        QueryableStoreTypes.keyValueStore()\n    ));\n\nLong count = store.get(userId);\n```\n\n### Testing with Topology Test Driver\n\n```java\n@Test\npublic void testWordCount() {\n    // Build topology\n    StreamsBuilder builder = new StreamsBuilder();\n    // ... topology code ...\n\n    // Create test driver\n    TopologyTestDriver testDriver = new TopologyTestDriver(\n        builder.build(),\n        props\n    );\n\n    // Input topic\n    TestInputTopic<String, String> inputTopic =\n        testDriver.createInputTopic(\n            \"text-input\",\n            Serdes.String().serializer(),\n            Serdes.String().serializer()\n        );\n\n    // Output topic\n    TestOutputTopic<String, Long> outputTopic =\n        testDriver.createOutputTopic(\n            \"word-counts-output\",\n            Serdes.String().deserializer(),\n            Serdes.Long().deserializer()\n        );\n\n    // Send test data\n    inputTopic.pipeInput(\"hello world hello\");\n\n    // Assert output\n    Map<String, Long> output = outputTopic.readKeyValuesToMap();\n    assertEquals(2L, output.get(\"hello\"));\n    assertEquals(1L, output.get(\"world\"));\n\n    testDriver.close();\n}\n```\n\n## Testing\n\n```bash\n# Unit tests with Topology Test Driver\n./gradlew test\n\n# Integration tests (requires Kafka cluster)\n./gradlew integrationTest\n\n# Coverage report\n./gradlew jacocoTestReport\n```\n\n## Documentation\n\n- **Getting Started**: `.specweave/docs/public/guides/kafka-streams-getting-started.md`\n- **Topology Patterns**: `.specweave/docs/public/guides/kafka-streams-patterns.md`\n- **State Stores**: `.specweave/docs/public/guides/kafka-streams-state.md`\n- **Testing Guide**: `.specweave/docs/public/guides/kafka-streams-testing.md`\n\n## Contributing\n\nSee [CONTRIBUTING.md](../../CONTRIBUTING.md) for development setup and guidelines.\n\n## Related Plugins\n\n- **specweave-kafka** - Core Kafka plugin (Apache Kafka, producers, consumers)\n- **specweave-confluent** - Confluent Cloud features (Schema Registry, ksqlDB)\n- **specweave-n8n** - n8n workflow automation with Kafka integration\n\n## License\n\nMIT License - see [LICENSE](../../LICENSE)\n\n## Support\n\n- **Documentation**: https://spec-weave.com/docs/plugins/kafka-streams\n- **Issues**: https://github.com/anton-abyzov/specweave/issues\n- **Discussions**: https://github.com/anton-abyzov/specweave/discussions\n\n---\n\n**Version**: 1.0.0\n**Last Updated**: 2025-11-15\n**Status**: ‚úÖ Production Ready\n",
        "plugins/specweave-n8n/README.md": "# specweave-n8n\n\n**n8n Workflow Automation Integration with Kafka for SpecWeave**\n\nEvent-driven workflows, Kafka triggers, producers, and no-code/low-code patterns for workflow automation with Apache Kafka.\n\n## Features\n\n### üöÄ Core Capabilities\n\n- **Kafka Trigger Node**: Event-driven workflow activation on Kafka messages\n- **Kafka Producer Node**: Send messages to Kafka topics from workflows\n- **Event-Driven Patterns**: Fan-out, retry with DLQ, batch processing, CDC\n- **Error Handling**: Try/Catch, exponential backoff, circuit breaker, idempotency\n- **Integration**: HTTP API enrichment, database sync, email/Slack notifications\n- **No-Code/Low-Code**: Visual workflow builder for Kafka event processing\n\n### üìö Skills (1)\n\n- `n8n-kafka-workflows` - Workflow patterns, Kafka triggers, producers, error handling, no-code automation\n\n### ü§ñ Agents (0)\n\n*Agents coming in future releases*\n\n### ‚ö° Commands (0)\n\n*Commands coming in future releases*\n\n## Installation\n\n### Prerequisites\n\n- n8n installed (self-hosted or cloud)\n- Apache Kafka 2.8+ cluster\n- Node.js 18+ (for n8n)\n\n### Install Plugin\n\n```bash\n# Via SpecWeave marketplace\nspecweave plugin install sw-n8n\n\n# Or via Claude Code plugin system\n/plugin install sw-n8n@specweave\n```\n\n## Quick Start\n\n### 1. Configure Kafka Credentials in n8n\n\n**n8n UI ‚Üí Credentials ‚Üí Add Credential**:\n```json\n{\n  \"name\": \"My Kafka Cluster\",\n  \"type\": \"kafka\",\n  \"brokers\": \"localhost:9092\",\n  \"clientId\": \"n8n-workflow\",\n  \"ssl\": false,\n  \"sasl\": {\n    \"mechanism\": \"plain\",\n    \"username\": \"{{$env.KAFKA_USER}}\",\n    \"password\": \"{{$env.KAFKA_PASSWORD}}\"\n  }\n}\n```\n\n### 2. Create Event-Driven Workflow\n\n**Workflow**: Process orders from Kafka, enrich with customer data, save to database\n\n```\n[Kafka Trigger] ‚Üí [HTTP Request] ‚Üí [Set] ‚Üí [PostgreSQL]\n     ‚Üì                 ‚Üì            ‚Üì\n  orders topic    Get customer   Merge data\n                                     ‚Üì\n                                Save to DB\n```\n\n**1. Kafka Trigger Node**:\n- Credential: My Kafka Cluster\n- Topics: `orders`\n- Consumer Group: `order-processor`\n- Offset: `latest`\n- Auto Commit: `true`\n\n**2. HTTP Request Node** (Enrich):\n- URL: `https://api.example.com/customers/{{$json.customerId}}`\n- Method: GET\n- Authentication: Bearer Token\n\n**3. Set Node** (Transform):\n```javascript\nreturn {\n  orderId: $json.order.id,\n  customerId: $json.order.customerId,\n  customerName: $json.customer.name,\n  total: $json.order.total,\n  timestamp: new Date().toISOString()\n};\n```\n\n**4. PostgreSQL Node** (Save):\n- Operation: INSERT\n- Table: `enriched_orders`\n\n### 3. Fan-Out Pattern (Publish to Multiple Topics)\n\n**Workflow**: Single event triggers multiple downstream topics\n\n```\n[Kafka Trigger] ‚Üí [Switch] ‚Üí [Kafka Producer] (high-value-orders)\n     ‚Üì                ‚Üì\n  orders          ‚îî‚îÄ‚Üí [Kafka Producer] (all-orders)\n                       ‚îî‚îÄ‚Üí [Kafka Producer] (analytics)\n```\n\n**Switch Node**:\n- Route 1: `{{$json.total > 1000}}` ‚Üí `high-value-orders`\n- Route 2: Always ‚Üí `all-orders`\n- Route 3: Always ‚Üí `analytics`\n\n**Kafka Producer Nodes**: Send to respective topics\n\n## Architecture\n\n### Event-Driven Workflow Patterns\n\n**1. Filter and Transform**:\n```\n[Kafka Trigger] ‚Üí [Filter] ‚Üí [Transform] ‚Üí [Kafka Producer]\n     ‚Üì              ‚Üì            ‚Üì\n  Raw events    Drop invalid  Enrich data\n                                  ‚Üì\n                            Publish processed\n```\n\n**2. Retry with DLQ**:\n```\n[Kafka Trigger] ‚Üí [Try] ‚Üí [Process] ‚Üí [Success]\n     ‚Üì              ‚Üì\n  Input          [Catch]\n                     ‚Üì\n             [Increment Retry]\n                     ‚Üì retry < 3\n            [Kafka Producer] (retry topic)\n                     ‚Üì retry >= 3\n            [Kafka Producer] (dlq topic)\n```\n\n**3. Batch Processing**:\n```\n[Kafka Trigger] ‚Üí [Aggregate] ‚Üí [HTTP Batch API] ‚Üí [Kafka Producer]\n     ‚Üì               ‚Üì\n  Events         Buffer 100 msgs\n                     ‚Üì\n               Send batch to API\n```\n\n**4. Change Data Capture (CDC)**:\n```\n[Cron] ‚Üí [PostgreSQL] ‚Üí [Compare] ‚Üí [Kafka Producer]\n   ‚Üì         ‚Üì             ‚Üì\nEvery 1m  Get new rows  Detect changes\n                            ‚Üì\n                      Publish CDC events\n```\n\n### Error Handling Strategies\n\n**Exponential Backoff**:\n```javascript\nconst retryCount = $json.headers?.['retry-count'] || 0;\nconst backoffMs = Math.min(1000 * Math.pow(2, retryCount), 60000);\nreturn { retryCount: retryCount + 1, backoffMs };\n```\n\n**Circuit Breaker**:\n```javascript\nconst failureRate = $json.metrics.failures / $json.metrics.total;\nif (failureRate > 0.5) {\n  return { circuitState: 'OPEN', skipProcessing: true };\n}\n```\n\n**Idempotency**:\n```javascript\nconst messageId = $json.headers?.['message-id'];\nif (await $('Redis').exists(messageId)) {\n  return { skip: true, reason: 'duplicate' };\n}\n```\n\n## Usage Examples\n\n### Kafka + HTTP API Enrichment\n\n```\n[Kafka Trigger: user-events]\n  ‚Üì\n[HTTP Request: GET /users/{{$json.userId}}]\n  ‚Üì\n[Set: Merge user data]\n  ‚Üì\n[Kafka Producer: enriched-events]\n```\n\n### Kafka + Database Sync\n\n```\n[Kafka Trigger: orders]\n  ‚Üì\n[PostgreSQL: UPSERT into orders table]\n  ‚Üì\n[Kafka Producer: order-processed]\n```\n\n### Kafka + Slack Alerts\n\n```\n[Kafka Trigger: errors]\n  ‚Üì\n[If: severity === 'critical']\n  ‚Üì true\n[Slack: Send to #alerts]\n  ‚Üì\n[Kafka Producer: alert-sent]\n```\n\n### Kafka + Email Notifications\n\n```\n[Kafka Trigger: high-value-orders]\n  ‚Üì\n[Send Email: Notify sales team]\n  ‚Üì\n[Kafka Producer: notification-sent]\n```\n\n## Testing\n\n### Manual Testing\n\n**1. Test Kafka Trigger**:\n```bash\n# Produce test message\necho '{\"userId\": 123, \"event\": \"click\"}' | \\\n  kcat -P -b localhost:9092 -t user-events\n```\n\n**2. Test Kafka Producer**:\n```bash\n# Consume test topic\nkcat -C -b localhost:9092 -t enriched-events -o beginning\n```\n\n**3. n8n UI Testing**:\n- Click \"Execute Workflow\"\n- Use \"Test Step\" to check node outputs\n- View execution history\n\n### Automated Testing\n\n```bash\n# Execute workflow via CLI\nn8n execute workflow --file workflow.json --input test-data.json\n\n# Export workflow\nn8n export:workflow --id=123 --output=my-workflow.json\n```\n\n## Performance Optimization\n\n**1. Enable Batching**:\n```\nKafka Trigger:\n  Batch Size: 100\n  Batch Timeout: 5000ms\n```\n\n**2. Parallel Processing**:\n```\n[Kafka Trigger] ‚Üí [Split in Batches] ‚Üí [HTTP Request]\n     ‚Üì                  ‚Üì\n  1000 events      Process 100 at a time\n```\n\n**3. Use Compression**:\n```\nKafka Producer:\n  Compression: lz4\n  Batch Size: 1000\n```\n\n## Troubleshooting\n\n### Issue 1: Consumer Lag Building Up\n\n**Solutions**:\n- Increase consumer group size (deploy more n8n instances)\n- Enable batching (process 100+ messages at once)\n- Use Split in Batches for parallel HTTP requests\n- Optimize database queries (use batch UPSERT)\n\n### Issue 2: Duplicate Messages\n\n**Solution**: Add idempotency check:\n```javascript\nconst messageId = $json.headers?.['message-id'];\nconst exists = await $('Redis').exists(messageId);\nif (exists) return { skip: true };\n```\n\n### Issue 3: Workflow Execution Timeout\n\n**Solution**: Use async patterns:\n```\n[Kafka Trigger] ‚Üí [Webhook] ‚Üí [Wait for Webhook] ‚Üí [Process]\n     ‚Üì               ‚Üì\n  Trigger job    Async callback\n```\n\n## Documentation\n\n- **Getting Started**: `.specweave/docs/public/guides/n8n-kafka-getting-started.md`\n- **Workflow Patterns**: `.specweave/docs/public/guides/n8n-kafka-patterns.md`\n- **Error Handling**: `.specweave/docs/public/guides/n8n-error-handling.md`\n- **Best Practices**: `.specweave/docs/public/guides/n8n-best-practices.md`\n\n## Contributing\n\nSee [CONTRIBUTING.md](../../CONTRIBUTING.md) for development setup and guidelines.\n\n## Related Plugins\n\n- **specweave-kafka** - Core Kafka plugin (Apache Kafka integration)\n- **specweave-confluent** - Confluent Cloud features (Schema Registry, ksqlDB)\n- **specweave-kafka-streams** - Kafka Streams library (Java/Kotlin stream processing)\n\n## License\n\nMIT License - see [LICENSE](../../LICENSE)\n\n## Support\n\n- **Documentation**: https://spec-weave.com/docs/plugins/n8n\n- **Issues**: https://github.com/anton-abyzov/specweave/issues\n- **Discussions**: https://github.com/anton-abyzov/specweave/discussions\n\n---\n\n**Version**: 1.0.0\n**Last Updated**: 2025-11-15\n**Status**: ‚úÖ Production Ready\n",
        "plugins/specweave-confluent/README.md": "# specweave-confluent\n\n**Confluent Cloud Integration Plugin for SpecWeave**\n\nEnterprise Kafka features including Schema Registry, ksqlDB stream processing, Cluster Linking, and Confluent Cloud architecture patterns.\n\n## Features\n\n### üöÄ Core Capabilities\n\n- **Schema Registry**: Avro, Protobuf, JSON Schema management with compatibility modes\n- **ksqlDB**: SQL-like stream processing with real-time queries and materialized views\n- **Confluent Cloud**: eCKU sizing, multi-region architecture, cluster linking\n- **Stream Governance**: Schema validation, data quality, lineage tracking\n- **Enterprise Features**: Cluster Linking, Tiered Storage, Private Networking\n\n### üìö Skills (2)\n\n**Organized for Enterprise Kafka**:\n\n- `confluent-schema-registry` - Schema management, evolution strategies, Avro/Protobuf/JSON Schema, compatibility modes\n- `confluent-ksqldb` - Stream processing with SQL, joins, aggregations, windowing, materialized views\n\n### ü§ñ Agents (1)\n\n- `confluent-architect` - eCKU sizing, cluster linking, multi-region strategies, cost optimization\n\n### ‚ö° Commands (0)\n\n*Commands coming in future releases*\n\n## Installation\n\n### Prerequisites\n\n- Node.js 18+\n- Confluent Cloud account (or self-hosted Confluent Platform)\n- Schema Registry credentials\n- ksqlDB cluster (optional)\n\n### Install Plugin\n\n```bash\n# Via SpecWeave marketplace\nspecweave plugin install sw-confluent\n\n# Or via Claude Code plugin system\n/plugin install sw-confluent@specweave\n```\n\n## Quick Start\n\n### 1. Schema Registry - Register Avro Schema\n\n```javascript\nconst { SchemaRegistry } = require('@kafkajs/confluent-schema-registry');\n\nconst registry = new SchemaRegistry({\n  host: 'https://schema-registry.us-east-1.aws.confluent.cloud',\n  auth: {\n    username: process.env.SR_API_KEY,\n    password: process.env.SR_API_SECRET\n  }\n});\n\n// Define Avro schema\nconst schema = `\n{\n  \"type\": \"record\",\n  \"name\": \"User\",\n  \"fields\": [\n    {\"name\": \"id\", \"type\": \"long\"},\n    {\"name\": \"name\", \"type\": \"string\"},\n    {\"name\": \"email\", \"type\": [\"null\", \"string\"], \"default\": null}\n  ]\n}\n`;\n\n// Register schema\nconst { id } = await registry.register({\n  type: SchemaType.AVRO,\n  schema\n});\n\nconsole.log(`Schema registered with ID: ${id}`);\n\n// Encode message\nconst payload = await registry.encode(id, {\n  id: 1,\n  name: 'John Doe',\n  email: 'john@example.com'\n});\n\n// Send to Kafka\nawait producer.send({\n  topic: 'users',\n  messages: [{ value: payload }]\n});\n```\n\n### 2. ksqlDB - Real-Time Stream Processing\n\n```sql\n-- Create stream from Kafka topic\nCREATE STREAM clicks_stream (\n  user_id BIGINT,\n  page VARCHAR,\n  timestamp TIMESTAMP\n) WITH (\n  kafka_topic='clicks',\n  value_format='AVRO'\n);\n\n-- Filter important events\nCREATE STREAM checkout_clicks AS\nSELECT user_id, page, timestamp\nFROM clicks_stream\nWHERE page = 'checkout'\nEMIT CHANGES;\n\n-- Real-time aggregation (5-minute tumbling window)\nCREATE TABLE user_clicks_per_5min AS\nSELECT\n  user_id,\n  WINDOWSTART AS window_start,\n  COUNT(*) AS click_count\nFROM clicks_stream\nWINDOW TUMBLING (SIZE 5 MINUTES)\nGROUP BY user_id\nEMIT CHANGES;\n\n-- Query current window\nSELECT * FROM user_clicks_per_5min\nWHERE user_id = 123;\n```\n\n### 3. Confluent Cloud - Cluster Sizing\n\nAsk the `confluent-architect` agent:\n\n```\nMe: \"I need 50K msg/sec, 7-day retention. How many eCKUs?\"\n\nConfluent Architect:\n- Throughput: 50K msg/sec √ó 1KB = 50 MB/sec\n- eCKU calculation: 50 MB/sec / 30 MB/sec per CKU = 2 CKUs\n- Recommended: 4 CKUs (100% headroom for bursts)\n- Cost: 4 CKUs √ó $0.11/hour √ó 730 hours = $321/month\n- Cluster type: Standard (99.95% SLA)\n```\n\n## Architecture\n\n### Schema Registry Integration\n\n**Producer with Avro**:\n```javascript\nconst { Kafka } = require('kafkajs');\nconst { SchemaRegistry, SchemaType } = require('@kafkajs/confluent-schema-registry');\n\nconst kafka = new Kafka({\n  clientId: 'my-producer',\n  brokers: ['pkc-xxx.us-east-1.aws.confluent.cloud:9092'],\n  ssl: true,\n  sasl: {\n    mechanism: 'plain',\n    username: process.env.KAFKA_API_KEY,\n    password: process.env.KAFKA_API_SECRET\n  }\n});\n\nconst registry = new SchemaRegistry({\n  host: 'https://psrc-xxx.us-east-1.aws.confluent.cloud',\n  auth: {\n    username: process.env.SR_API_KEY,\n    password: process.env.SR_API_SECRET\n  }\n});\n\nconst producer = kafka.producer();\nawait producer.connect();\n\n// Encode with schema\nconst payload = await registry.encode(schemaId, { id: 1, name: 'John' });\n\nawait producer.send({\n  topic: 'users',\n  messages: [{ value: payload }]\n});\n```\n\n**Consumer with Avro**:\n```javascript\nconst consumer = kafka.consumer({ groupId: 'user-processor' });\nawait consumer.subscribe({ topic: 'users' });\n\nawait consumer.run({\n  eachMessage: async ({ message }) => {\n    // Decode automatically (schema ID in header)\n    const user = await registry.decode(message.value);\n    console.log(user); // { id: 1, name: 'John' }\n  }\n});\n```\n\n### ksqlDB Deployment Patterns\n\n**Confluent Cloud**:\n- Managed ksqlDB clusters (CSUs - Confluent Streaming Units)\n- 1 CSU = 1 vCPU + 4GB RAM\n- Auto-scaling based on query load\n- 99.95% SLA\n\n**Self-Hosted**:\n- Deploy on Kubernetes (Helm chart)\n- 3+ nodes for HA (multi-AZ)\n- Persistent query state in RocksDB\n- Standby replicas for failover\n\n### Cluster Linking Topology\n\n**Active-Passive (DR)**:\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  Primary        ‚îÇ           ‚îÇ  Secondary      ‚îÇ\n‚îÇ  us-east-1      ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>  ‚îÇ  us-west-2      ‚îÇ\n‚îÇ  (4 CKUs)       ‚îÇ  Linking  ‚îÇ  (2 CKUs)       ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n**Active-Active (Multi-Region)**:\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  <‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  US Cluster     ‚îÇ  Linking   ‚îÇ  EU Cluster     ‚îÇ\n‚îÇ  us-east-1      ‚îÇ  (Bi-Dir)  ‚îÇ  eu-west-1      ‚îÇ\n‚îÇ  (6 CKUs)       ‚îÇ            ‚îÇ  (6 CKUs)       ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n## Usage Examples\n\n### Schema Evolution - Add Optional Field\n\n```javascript\n// V1 schema\nconst schemaV1 = `\n{\n  \"type\": \"record\",\n  \"name\": \"User\",\n  \"fields\": [\n    {\"name\": \"id\", \"type\": \"long\"},\n    {\"name\": \"name\", \"type\": \"string\"}\n  ]\n}\n`;\n\n// V2 schema - BACKWARD compatible (added optional field)\nconst schemaV2 = `\n{\n  \"type\": \"record\",\n  \"name\": \"User\",\n  \"fields\": [\n    {\"name\": \"id\", \"type\": \"long\"},\n    {\"name\": \"name\", \"type\": \"string\"},\n    {\"name\": \"email\", \"type\": [\"null\", \"string\"], \"default\": null}\n  ]\n}\n`;\n\n// Test compatibility BEFORE registering\nconst compatible = await registry.checkCompatibility({\n  schema: schemaV2,\n  subject: 'users-value'\n});\n\nif (compatible) {\n  await registry.register({ schema: schemaV2 });\n}\n```\n\n### ksqlDB - Enrich Events with Stream-Table Join\n\n```sql\n-- Create users table (current state)\nCREATE TABLE users (\n  user_id BIGINT PRIMARY KEY,\n  name VARCHAR,\n  email VARCHAR\n) WITH (\n  kafka_topic='users',\n  value_format='AVRO'\n);\n\n-- Enrich click events with user data\nCREATE STREAM enriched_clicks AS\nSELECT\n  c.user_id,\n  c.page,\n  c.timestamp,\n  u.name,\n  u.email\nFROM clicks_stream c\nLEFT JOIN users u ON c.user_id = u.user_id\nEMIT CHANGES;\n```\n\n### Confluent Cloud - eCKU Sizing Calculator\n\n```\nInputs:\n- Throughput: 100K msg/sec √ó 1KB avg = 100 MB/sec\n- Peak factor: 2.0x (200 MB/sec peak)\n- Retention: 7 days\n- Partitions: 20 topics √ó 24 partitions = 480 total\n\neCKU Calculation:\n- Write throughput: 200 MB/sec / 30 MB/sec per CKU = 6.67 CKUs\n- Recommended: 8 CKUs (rounded up)\n- Partition validation: 480 partitions / 8 CKUs = 60 partitions/CKU (OK, <1500 limit)\n\nCost:\n- 8 CKUs √ó $0.11/hour √ó 730 hours = $642/month\n- Storage: Included (800 GB total)\n\nAlternative (Dedicated):\n- 2 dedicated CKUs (higher performance)\n- Cost: $2,190/month\n- Use when: >10 CKUs OR >1000 partitions OR <5ms latency required\n```\n\n## Testing\n\n```bash\n# Unit tests\nnpm test\n\n# Integration tests (requires Confluent Cloud credentials)\nnpm run test:integration\n\n# E2E tests (requires ksqlDB cluster)\nnpm run test:e2e\n```\n\n## Documentation\n\n- **Schema Registry Guide**: `.specweave/docs/public/guides/confluent-schema-registry.md`\n- **ksqlDB Tutorial**: `.specweave/docs/public/guides/confluent-ksqldb.md`\n- **Cluster Linking**: `.specweave/docs/public/guides/confluent-cluster-linking.md`\n- **Cost Optimization**: `.specweave/docs/public/guides/confluent-cost-optimization.md`\n\n## Contributing\n\nSee [CONTRIBUTING.md](../../CONTRIBUTING.md) for development setup and guidelines.\n\n## Related Plugins\n\n- **specweave-kafka** - Core Kafka plugin (Apache Kafka, AWS MSK, Azure Event Hubs)\n- **specweave-kafka-streams** - Kafka Streams library and patterns\n- **specweave-n8n** - n8n workflow automation with Kafka/Confluent integration\n\n## License\n\nMIT License - see [LICENSE](../../LICENSE)\n\n## Support\n\n- **Documentation**: https://spec-weave.com/docs/plugins/confluent\n- **Issues**: https://github.com/anton-abyzov/specweave/issues\n- **Discussions**: https://github.com/anton-abyzov/specweave/discussions\n\n---\n\n**Version**: 1.0.0\n**Last Updated**: 2025-11-15\n**Status**: ‚úÖ Production Ready\n",
        "plugins/specweave-mobile/README.md": "# SpecWeave Mobile Plugin\n\nComprehensive **React Native** and **Expo** development support for SpecWeave. Streamlines mobile app development with expert guidance on setup, debugging, performance optimization, and testing.\n\n**IMPORTANT**: This plugin provides patterns and architectural guidance. For version-specific APIs, always use Context7 to fetch current React Native and Expo documentation.\n\n## Overview\n\nThe SpecWeave Mobile plugin provides specialized skills and agents for modern React Native and Expo development, covering the entire mobile development lifecycle from environment setup to production deployment.\n\n### Key Capabilities\n\n- **New Architecture**: Turbo Modules, Fabric, JSI\n- **Modern React**: Activity component, useEffectEvent, concurrent features\n- **Expo Workflows**: EAS Build, EAS Update, native tabs, file-based routing\n- **Performance**: Hermes engine, FlashList, Intersection Observer, Web Performance APIs\n- **Platform-Specific**: iOS blur/glass effects, Android edge-to-edge\n\n## Features\n\n### 7 Specialized Skills\n\n1. **react-native-setup** - Environment setup and configuration\n   - Node.js, Xcode, Android Studio setup\n   - iOS simulators and Android emulators\n   - CocoaPods, watchman, EAS Build setup\n   - New Architecture setup and troubleshooting\n\n2. **expo-workflow** - Expo development workflows\n   - EAS Build and EAS Update\n   - Native tab navigation and Expo Router\n   - Platform-specific UI (iOS blur, Android edge-to-edge)\n   - expo-video/expo-audio\n\n3. **mobile-debugging** - Debugging strategies\n   - React DevTools Desktop App\n   - Flipper, Chrome DevTools\n   - Network request debugging\n   - Error boundaries and crash analysis\n\n4. **performance-optimization** - Modern performance tuning\n   - Hermes engine optimization\n   - React Activity component for state preservation\n   - Intersection Observer API for lazy loading\n   - Web Performance APIs (performance.now, User Timing)\n   - FlashList, expo-image\n\n5. **native-modules** - New Architecture native integration\n   - Turbo Modules with Codegen\n   - Fabric components\n   - JSI for synchronous calls\n   - Expo config plugins\n   - Interop layer for legacy Bridge modules\n\n6. **device-testing** - Testing strategies\n   - Jest unit and integration testing\n   - React Native Testing Library\n   - Detox E2E testing\n   - Maestro testing\n   - Mocking strategies\n\n7. **metro-bundler** - Metro configuration and optimization\n   - Custom transformers (SVG, images)\n   - Bundle size analysis\n   - Cache management\n   - Monorepo configuration\n   - Hermes bytecode optimization\n\n### Mobile Architect Agent\n\nThe `mobile-architect` agent specializes in:\n- Application architecture design for React Native / Expo\n- State management selection (Zustand, TanStack Query, Jotai, Legend State)\n- Navigation with Expo Router and React Navigation\n- Performance architecture with modern React features\n- Platform-specific strategies (iOS design patterns, Android edge-to-edge)\n- Testing architecture\n- Build and deployment pipelines with EAS\n\n## Installation\n\nThe plugin is automatically installed with SpecWeave. To verify installation:\n\n```bash\n# List installed plugins\nclaude plugin list --installed | grep sw-mobile\n```\n\nTo reinstall or update:\n\n```bash\n# Reinstall from marketplace\nclaude plugin install sw-mobile@specweave\n```\n\n## Usage\n\n### Skills Auto-Activate\n\nSkills automatically activate based on conversation keywords:\n\n```\nYou: \"How do I set up Xcode for React Native development?\"\n‚Üí react-native-setup skill activates\n\nYou: \"How do I optimize FlatList performance?\"\n‚Üí performance-optimization skill activates\n\nYou: \"I need to debug network requests in my app\"\n‚Üí mobile-debugging skill activates\n```\n\n### Invoke the Mobile Architect Agent\n\nFor architectural decisions and system design:\n\n```\nUse the Task tool to invoke the mobile-architect agent:\n\nTask(\n  subagent_type: \"specweave-mobile:mobile-architect:mobile-architect\",\n  description: \"Design architecture for social media app\",\n  prompt: \"Design a scalable React Native architecture for a social media app\n  with feed, messaging, and profile features. Include state management,\n  navigation, and performance considerations.\"\n)\n```\n\n## Common Workflows\n\n### 1. Initial Environment Setup\n\n**User asks**: \"Help me set up React Native development on my Mac\"\n\n**What happens**:\n- `react-native-setup` skill activates\n- Provides step-by-step installation guide\n- Verifies prerequisites\n- Troubleshoots common issues\n\n### 2. Performance Optimization\n\n**User asks**: \"My app is laggy when scrolling the feed\"\n\n**What happens**:\n- `performance-optimization` skill activates\n- Analyzes FlatList usage\n- Recommends optimizations (getItemLayout, removeClippedSubviews, FlashList)\n- Provides code examples\n\n### 3. Debugging Network Issues\n\n**User asks**: \"API calls are failing on Android but working on iOS\"\n\n**What happens**:\n- `mobile-debugging` skill activates\n- Guides through network debugging\n- Checks localhost vs 10.0.2.2 configuration\n- Sets up Flipper network inspector\n\n### 4. Architecture Design\n\n**User invokes**: mobile-architect agent\n\n**What happens**:\n- Agent analyzes requirements\n- Recommends folder structure\n- Selects state management solution\n- Designs navigation architecture\n- Provides implementation templates\n\n## Integration with SpecWeave Workflows\n\n### During Increment Planning\n\nWhen using `/sw:increment` for mobile features:\n\n1. **Spec Creation** - Mobile architect reviews requirements\n2. **Architecture Design** - Agent recommends patterns and structure\n3. **Task Generation** - Includes setup, development, testing, and optimization tasks\n4. **Test Planning** - Embeds test cases in tasks.md (BDD format)\n\n### During Implementation\n\nWhen using `/sw:do`:\n\n1. **Environment Setup** - react-native-setup skill guides configuration\n2. **Development** - Skills activate based on task context\n3. **Debugging** - mobile-debugging skill assists with issues\n4. **Testing** - device-testing skill provides testing strategies\n5. **Optimization** - performance-optimization skill reviews code\n\n### Documentation\n\n- **Setup Guide**: See `.specweave/docs/public/guides/mobile/react-native-setup-guide.md`\n- **Architecture Decisions**: Documented in ADRs (`.specweave/docs/internal/architecture/adr/`)\n- **Performance Reports**: Track bundle size and metrics in increment reports\n\n## Skill Activation Keywords\n\n| Skill | Activation Keywords |\n|-------|-------------------|\n| **react-native-setup** | environment setup, installation, xcode, android studio, simulator, emulator, sdk, cocoapods |\n| **expo-workflow** | expo, eas build, eas update, expo go, dev client, app.json, eas.json |\n| **mobile-debugging** | debugging, flipper, devtools, breakpoint, crash, error, network debugging |\n| **performance-optimization** | performance, slow, lag, bundle size, flatlist, optimization, memory leak |\n| **native-modules** | native module, bridge, turbo module, swift, kotlin, ios native, android native |\n| **device-testing** | testing, jest, detox, e2e, unit test, mock, snapshot test |\n| **metro-bundler** | metro, bundler, cache, transformer, port 8081, unable to resolve module |\n\n## Examples\n\n### Example 1: Setup iOS Simulator\n\n```\nYou: \"I installed Xcode but can't run the iOS simulator\"\n\nClaude (with react-native-setup skill):\nLet me help you troubleshoot iOS simulator setup.\n\nFirst, let's verify Xcode command line tools:\n```bash\nxcode-select --install\nsudo xcodebuild -license accept\n```\n\nCheck available simulators:\n```bash\nxcrun simctl list devices\n```\n\n[Provides detailed troubleshooting steps...]\n```\n\n### Example 2: Optimize Performance\n\n```\nYou: \"My feed is rendering slowly with 1000 items\"\n\nClaude (with performance-optimization skill):\nI see you're rendering a long list. Here are optimizations:\n\n1. Use FlashList instead of FlatList (60% faster):\n```javascript\nimport { FlashList } from \"@shopify/flash-list\";\n\n<FlashList\n  data={items}\n  renderItem={({ item }) => <ItemCard item={item} />}\n  estimatedItemSize={80}\n/>\n```\n\n2. Implement getItemLayout for fixed heights:\n[Provides implementation...]\n\n3. Use React.memo for ItemCard:\n[Provides code example...]\n```\n\n### Example 3: Architecture Design\n\n```\nUser invokes: mobile-architect agent\n\n\"Design architecture for a social media app with posts, comments, and real-time chat\"\n\nAgent response:\n# Architecture Recommendation: Social Media App\n\n## Folder Structure (Feature-Based)\n```\nsrc/\n‚îú‚îÄ‚îÄ features/\n‚îÇ   ‚îú‚îÄ‚îÄ feed/\n‚îÇ   ‚îú‚îÄ‚îÄ chat/\n‚îÇ   ‚îî‚îÄ‚îÄ profile/\n‚îú‚îÄ‚îÄ shared/\n‚îî‚îÄ‚îÄ navigation/\n```\n\n## State Management\nRecommendation: Redux Toolkit + React Query\n- Redux: Global state (auth, user profile)\n- React Query: Server state (feed, posts, comments)\n\n## Navigation\nReact Navigation with:\n- Stack Navigator (root)\n- Tab Navigator (main)\n- Chat Navigator (nested)\n\n[Provides complete architecture with code examples...]\n```\n\n## Pro Tips\n\n1. **Fast Device Testing**: Use development builds with EAS for custom native code\n2. **Performance**: Enable Hermes V1 (experimental) for faster startup and smaller bundles\n3. **Debugging**: Use the new React DevTools Desktop App (RN 0.83)\n4. **Testing**: Start with React Native Testing Library, add Detox for critical paths\n5. **Optimization**: Use Web Performance APIs (performance.now, User Timing) for profiling\n6. **iOS**: Target iOS 26+ for Liquid Glass effects\n7. **Android**: Use API 35 with edge-to-edge for modern UI\n\n## Troubleshooting\n\n### Skill Not Activating?\n\n1. Use relevant keywords from the activation list above\n2. Be specific about the problem (e.g., \"iOS simulator not working\" vs \"help with mobile\")\n3. Restart Claude Code if recently installed plugin\n\n### Need More Help?\n\n- Check the [React Native Setup Guide](.specweave/docs/public/guides/mobile/react-native-setup-guide.md)\n- Review skill documentation in `skills/*/SKILL.md`\n- Invoke the mobile-architect agent for architectural questions\n\n## Contributing\n\nTo add new skills or improve existing ones:\n\n1. Fork the SpecWeave repository\n2. Add/modify skills in `plugins/specweave-mobile/skills/`\n3. Follow the skill template in existing skills\n4. Test with Claude Code\n5. Submit a pull request\n\n## Version History\n\n- **2.1.0** (January 2026)\n  - Removed hardcoded version numbers from descriptions\n  - Added Context7 integration for fetching current documentation\n  - Skills now focus on patterns/concepts, not specific versions\n  - Added instructions to verify current APIs before advising\n\n- **2.0.0** (December 2025)\n  - Added modern React patterns (Activity component, useEffectEvent)\n  - Added Expo workflows (native tabs, platform effects)\n  - Hermes engine optimization support\n  - Turbo Modules with Codegen documentation\n  - Intersection Observer and Web Performance APIs\n  - expo-video/expo-audio patterns\n\n- **1.0.0** (November 2024)\n  - Initial release\n  - 7 specialized skills\n  - Mobile architect agent\n  - Comprehensive setup guide\n\n## Support\n\n- **Issues**: [GitHub Issues](https://github.com/anton-abyzov/specweave/issues)\n- **Discussions**: [GitHub Discussions](https://github.com/anton-abyzov/specweave/discussions)\n- **Documentation**: [SpecWeave Docs](https://spec-weave.com)\n\n## License\n\nMIT License - See [LICENSE](../../LICENSE) for details\n\n---\n\n**Built with ‚ù§Ô∏è for the React Native community**\n"
      },
      "plugins": [
        {
          "name": "sw",
          "description": "SpecWeave framework - increment lifecycle, living docs, PM-led planning",
          "source": "./plugins/specweave",
          "category": "development",
          "version": "0.25.0",
          "author": {
            "name": "Anton Abyzov",
            "email": "anton.abyzov@gmail.com"
          },
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add anton-abyzov/specweave",
            "/plugin install sw@specweave"
          ]
        },
        {
          "name": "sw-github",
          "description": "GitHub integration - bidirectional sync, issue tracking, task management",
          "source": "./plugins/specweave-github",
          "category": "productivity",
          "version": "1.0.0",
          "author": {
            "name": "Anton Abyzov",
            "email": "anton.abyzov@gmail.com"
          },
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add anton-abyzov/specweave",
            "/plugin install sw-github@specweave"
          ]
        },
        {
          "name": "sw-jira",
          "description": "JIRA integration - bidirectional sync with epics/stories",
          "source": "./plugins/specweave-jira",
          "category": "productivity",
          "version": "1.0.0",
          "author": {
            "name": "Anton Abyzov",
            "email": "anton.abyzov@gmail.com"
          },
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add anton-abyzov/specweave",
            "/plugin install sw-jira@specweave"
          ]
        },
        {
          "name": "sw-ado",
          "description": "Azure DevOps integration - bidirectional sync with work items",
          "source": "./plugins/specweave-ado",
          "category": "productivity",
          "version": "1.0.0",
          "author": {
            "name": "Anton Abyzov",
            "email": "anton.abyzov@gmail.com"
          },
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add anton-abyzov/specweave",
            "/plugin install sw-ado@specweave"
          ]
        },
        {
          "name": "sw-infra",
          "description": "Cloud infrastructure - Hetzner, Prometheus, Grafana, monitoring",
          "source": "./plugins/specweave-infrastructure",
          "category": "development",
          "version": "1.0.0",
          "author": {
            "name": "Anton Abyzov",
            "email": "anton.abyzov@gmail.com"
          },
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add anton-abyzov/specweave",
            "/plugin install sw-infra@specweave"
          ]
        },
        {
          "name": "sw-ml",
          "description": "Machine learning - ML pipelines, training, deployment",
          "source": "./plugins/specweave-ml",
          "category": "development",
          "version": "1.0.0",
          "author": {
            "name": "Anton Abyzov",
            "email": "anton.abyzov@gmail.com"
          },
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add anton-abyzov/specweave",
            "/plugin install sw-ml@specweave"
          ]
        },
        {
          "name": "sw-release",
          "description": "Release management - version alignment, RC workflows, multi-repo coordination, semantic versioning",
          "source": "./plugins/specweave-release",
          "category": "development",
          "version": "1.0.0",
          "author": {
            "name": "Anton Abyzov",
            "email": "anton.abyzov@gmail.com"
          },
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add anton-abyzov/specweave",
            "/plugin install sw-release@specweave"
          ]
        },
        {
          "name": "sw-kafka",
          "description": "Apache Kafka event streaming integration with MCP servers, CLI tools (kcat), Terraform modules, and comprehensive observability stack",
          "source": "./plugins/specweave-kafka",
          "category": "development",
          "version": "1.0.0",
          "author": {
            "name": "Anton Abyzov",
            "email": "anton.abyzov@gmail.com"
          },
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add anton-abyzov/specweave",
            "/plugin install sw-kafka@specweave"
          ]
        },
        {
          "name": "sw-kafka-streams",
          "description": "Kafka Streams library integration - Stream processing with Java/Kotlin, topology design, state stores, windowing, joins, exactly-once semantics",
          "source": "./plugins/specweave-kafka-streams",
          "category": "development",
          "version": "1.0.0",
          "author": {
            "name": "Anton Abyzov",
            "email": "anton.abyzov@gmail.com"
          },
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add anton-abyzov/specweave",
            "/plugin install sw-kafka-streams@specweave"
          ]
        },
        {
          "name": "sw-n8n",
          "description": "n8n workflow automation with Kafka integration - Event-driven workflows, Kafka triggers, producers, no-code/low-code event processing patterns",
          "source": "./plugins/specweave-n8n",
          "category": "development",
          "version": "1.0.0",
          "author": {
            "name": "Anton Abyzov",
            "email": "anton.abyzov@gmail.com"
          },
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add anton-abyzov/specweave",
            "/plugin install sw-n8n@specweave"
          ]
        },
        {
          "name": "sw-backend",
          "description": "Backend development - Node.js, Python, .NET, REST APIs, database design, microservices architecture",
          "source": "./plugins/specweave-backend",
          "category": "development",
          "version": "1.0.0",
          "author": {
            "name": "Anton Abyzov",
            "email": "anton.abyzov@gmail.com"
          },
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add anton-abyzov/specweave",
            "/plugin install sw-backend@specweave"
          ]
        },
        {
          "name": "sw-confluent",
          "description": "Confluent Cloud integration - Schema Registry, ksqlDB, Kafka Connect, Flink, stream processing, enterprise Kafka features",
          "source": "./plugins/specweave-confluent",
          "category": "development",
          "version": "1.0.0",
          "author": {
            "name": "Anton Abyzov",
            "email": "anton.abyzov@gmail.com"
          },
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add anton-abyzov/specweave",
            "/plugin install sw-confluent@specweave"
          ]
        },
        {
          "name": "sw-k8s",
          "description": "Kubernetes deployment - K8s manifests, Helm charts, GitOps, cluster management, container orchestration",
          "source": "./plugins/specweave-kubernetes",
          "category": "development",
          "version": "1.0.0",
          "author": {
            "name": "Anton Abyzov",
            "email": "anton.abyzov@gmail.com"
          },
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add anton-abyzov/specweave",
            "/plugin install sw-k8s@specweave"
          ]
        },
        {
          "name": "sw-mobile",
          "description": "React Native & Expo mobile development - environment setup, debugging, performance optimization, native modules, testing",
          "source": "./plugins/specweave-mobile",
          "category": "development",
          "version": "1.0.0",
          "author": {
            "name": "Anton Abyzov",
            "email": "anton.abyzov@gmail.com"
          },
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add anton-abyzov/specweave",
            "/plugin install sw-mobile@specweave"
          ]
        },
        {
          "name": "sw-payments",
          "description": "Payment processing - Stripe, PayPal, billing, PCI compliance, subscription management, payment gateways",
          "source": "./plugins/specweave-payments",
          "category": "development",
          "version": "1.0.0",
          "author": {
            "name": "Anton Abyzov",
            "email": "anton.abyzov@gmail.com"
          },
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add anton-abyzov/specweave",
            "/plugin install sw-payments@specweave"
          ]
        },
        {
          "name": "sw-frontend",
          "description": "Frontend development - React, Next.js, component generation, design systems, UI scaffolding",
          "source": "./plugins/specweave-frontend",
          "category": "development",
          "version": "1.0.0",
          "author": {
            "name": "Anton Abyzov",
            "email": "anton.abyzov@gmail.com"
          },
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add anton-abyzov/specweave",
            "/plugin install sw-frontend@specweave"
          ]
        },
        {
          "name": "sw-testing",
          "description": "Testing framework - E2E with Playwright, unit testing, test coverage, test generation",
          "source": "./plugins/specweave-testing",
          "category": "development",
          "version": "1.0.0",
          "author": {
            "name": "Anton Abyzov",
            "email": "anton.abyzov@gmail.com"
          },
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add anton-abyzov/specweave",
            "/plugin install sw-testing@specweave"
          ]
        },
        {
          "name": "sw-figma",
          "description": "Figma integration - Design import, component sync, design-to-code workflows",
          "source": "./plugins/specweave-figma",
          "category": "productivity",
          "version": "1.0.0",
          "author": {
            "name": "Anton Abyzov",
            "email": "anton.abyzov@gmail.com"
          },
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add anton-abyzov/specweave",
            "/plugin install sw-figma@specweave"
          ]
        },
        {
          "name": "sw-diagrams",
          "description": "Architecture diagram generation - Mermaid, C4 Model, sequence diagrams, ER diagrams, deployment diagrams",
          "source": "./plugins/specweave-diagrams",
          "category": "development",
          "version": "1.0.0",
          "author": {
            "name": "Anton Abyzov",
            "email": "anton.abyzov@gmail.com"
          },
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add anton-abyzov/specweave",
            "/plugin install sw-diagrams@specweave"
          ]
        },
        {
          "name": "sw-ui",
          "description": "Browser automation and UI tools - Element inspection, automated testing, Playwright integration",
          "source": "./plugins/specweave-ui",
          "category": "development",
          "version": "1.0.0",
          "author": {
            "name": "Anton Abyzov",
            "email": "anton.abyzov@gmail.com"
          },
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add anton-abyzov/specweave",
            "/plugin install sw-ui@specweave"
          ]
        },
        {
          "name": "sw-docs",
          "description": "Documentation generation and management - Docusaurus, spec-driven docs, API documentation, living documentation",
          "source": "./plugins/specweave-docs",
          "category": "productivity",
          "version": "1.0.0",
          "author": {
            "name": "Anton Abyzov",
            "email": "anton.abyzov@gmail.com"
          },
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add anton-abyzov/specweave",
            "/plugin install sw-docs@specweave"
          ]
        },
        {
          "name": "sw-cost",
          "description": "Cloud cost optimization - AWS, GCP, Azure cost analysis, right-sizing, reserved instances, savings plans",
          "source": "./plugins/specweave-cost-optimizer",
          "category": "development",
          "version": "1.0.0",
          "author": {
            "name": "Anton Abyzov",
            "email": "anton.abyzov@gmail.com"
          },
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add anton-abyzov/specweave",
            "/plugin install sw-cost@specweave"
          ]
        },
        {
          "name": "sw-plugin-dev",
          "description": "Plugin development tools - Create, validate, and scaffold SpecWeave plugins, skills, commands, and agents",
          "source": "./plugins/specweave-plugin-dev",
          "category": "development",
          "version": "1.0.0",
          "author": {
            "name": "Anton Abyzov",
            "email": "anton.abyzov@gmail.com"
          },
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add anton-abyzov/specweave",
            "/plugin install sw-plugin-dev@specweave"
          ]
        }
      ]
    }
  ]
}