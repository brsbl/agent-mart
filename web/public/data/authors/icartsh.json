{
  "author": {
    "id": "icartsh",
    "display_name": "icartsh",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/5387321?v=4",
    "url": "https://github.com/icartsh",
    "bio": null,
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 1,
      "total_commands": 0,
      "total_skills": 23,
      "total_stars": 1,
      "total_forks": 0
    }
  },
  "marketplaces": [
    {
      "name": "icartsh-marketplace",
      "version": null,
      "description": "ICARTSH Plugin",
      "owner_info": {
        "name": "ICARTSH"
      },
      "keywords": [],
      "repo_full_name": "icartsh/icartsh_plugin",
      "repo_url": "https://github.com/icartsh/icartsh_plugin",
      "repo_description": null,
      "homepage": null,
      "signals": {
        "stars": 1,
        "forks": 0,
        "pushed_at": "2025-12-21T06:41:14Z",
        "created_at": "2025-12-07T10:23:00Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 1721
        },
        {
          "path": "icartsh-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 142
        },
        {
          "path": "icartsh-plugin/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/api-designer",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/api-designer/README.md",
          "type": "blob",
          "size": 2209
        },
        {
          "path": "icartsh-plugin/skills/api-designer/SKILL.md",
          "type": "blob",
          "size": 15148
        },
        {
          "path": "icartsh-plugin/skills/api-designer/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/api-designer/references/authentication.md",
          "type": "blob",
          "size": 11544
        },
        {
          "path": "icartsh-plugin/skills/api-designer/references/common-patterns.md",
          "type": "blob",
          "size": 12401
        },
        {
          "path": "icartsh-plugin/skills/api-designer/references/rest_best_practices.md",
          "type": "blob",
          "size": 15720
        },
        {
          "path": "icartsh-plugin/skills/api-designer/references/versioning-strategies.md",
          "type": "blob",
          "size": 13958
        },
        {
          "path": "icartsh-plugin/skills/brainstorming",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/brainstorming/SKILL.md",
          "type": "blob",
          "size": 3490
        },
        {
          "path": "icartsh-plugin/skills/code-analyze",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/code-analyze/SKILL.md",
          "type": "blob",
          "size": 4971
        },
        {
          "path": "icartsh-plugin/skills/code-analyze/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/code-analyze/references/dependency-check.md",
          "type": "blob",
          "size": 6921
        },
        {
          "path": "icartsh-plugin/skills/code-analyze/references/security-scan.md",
          "type": "blob",
          "size": 2407
        },
        {
          "path": "icartsh-plugin/skills/code-analyze/references/static-analysis.md",
          "type": "blob",
          "size": 7431
        },
        {
          "path": "icartsh-plugin/skills/code-format",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/code-format/SKILL.md",
          "type": "blob",
          "size": 4699
        },
        {
          "path": "icartsh-plugin/skills/code-format/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/code-format/references/dotnet-format.md",
          "type": "blob",
          "size": 7199
        },
        {
          "path": "icartsh-plugin/skills/code-format/references/fix-all.md",
          "type": "blob",
          "size": 6240
        },
        {
          "path": "icartsh-plugin/skills/code-format/references/prettier-format.md",
          "type": "blob",
          "size": 6680
        },
        {
          "path": "icartsh-plugin/skills/code-reviewer",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/code-reviewer/README.md",
          "type": "blob",
          "size": 5616
        },
        {
          "path": "icartsh-plugin/skills/code-reviewer/SKILL.md",
          "type": "blob",
          "size": 13480
        },
        {
          "path": "icartsh-plugin/skills/code-reviewer/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/code-reviewer/examples/review_checklist.md",
          "type": "blob",
          "size": 10060
        },
        {
          "path": "icartsh-plugin/skills/code-reviewer/examples/security_patterns.md",
          "type": "blob",
          "size": 15008
        },
        {
          "path": "icartsh-plugin/skills/code-reviewer/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/code-reviewer/references/performance_guide.md",
          "type": "blob",
          "size": 13987
        },
        {
          "path": "icartsh-plugin/skills/coding-conventions",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/coding-conventions/SKILL.md",
          "type": "blob",
          "size": 18966
        },
        {
          "path": "icartsh-plugin/skills/csharp-async-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/csharp-async-patterns/SKILL.md",
          "type": "blob",
          "size": 24909
        },
        {
          "path": "icartsh-plugin/skills/csharp-developer",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/csharp-developer/SKILL.md",
          "type": "blob",
          "size": 8472
        },
        {
          "path": "icartsh-plugin/skills/docker-workflow",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/docker-workflow/README.md",
          "type": "blob",
          "size": 9336
        },
        {
          "path": "icartsh-plugin/skills/docker-workflow/SKILL.md",
          "type": "blob",
          "size": 10650
        },
        {
          "path": "icartsh-plugin/skills/dotnet-build",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/dotnet-build/SKILL.md",
          "type": "blob",
          "size": 3806
        },
        {
          "path": "icartsh-plugin/skills/dotnet-build/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/dotnet-build/references/build-solution.md",
          "type": "blob",
          "size": 4933
        },
        {
          "path": "icartsh-plugin/skills/dotnet-build/references/restore-deps.md",
          "type": "blob",
          "size": 5618
        },
        {
          "path": "icartsh-plugin/skills/dotnet-test",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/dotnet-test/SKILL.md",
          "type": "blob",
          "size": 5287
        },
        {
          "path": "icartsh-plugin/skills/dotnet-test/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/dotnet-test/references/generate-coverage.md",
          "type": "blob",
          "size": 6957
        },
        {
          "path": "icartsh-plugin/skills/dotnet-test/references/run-benchmarks.md",
          "type": "blob",
          "size": 7085
        },
        {
          "path": "icartsh-plugin/skills/dotnet-test/references/run-unit-tests.md",
          "type": "blob",
          "size": 6656
        },
        {
          "path": "icartsh-plugin/skills/error-detective",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/error-detective/README.md",
          "type": "blob",
          "size": 4540
        },
        {
          "path": "icartsh-plugin/skills/error-detective/SKILL.md",
          "type": "blob",
          "size": 20271
        },
        {
          "path": "icartsh-plugin/skills/error-detective/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/error-detective/examples/common_errors.md",
          "type": "blob",
          "size": 18074
        },
        {
          "path": "icartsh-plugin/skills/error-detective/examples/debugging_workflow.md",
          "type": "blob",
          "size": 16103
        },
        {
          "path": "icartsh-plugin/skills/file-organizer",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/file-organizer/SKILL.md",
          "type": "blob",
          "size": 13406
        },
        {
          "path": "icartsh-plugin/skills/frontend-design",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/frontend-design/SKILL.md",
          "type": "blob",
          "size": 6238
        },
        {
          "path": "icartsh-plugin/skills/git-advanced",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/git-advanced/README.md",
          "type": "blob",
          "size": 2473
        },
        {
          "path": "icartsh-plugin/skills/git-advanced/SKILL.md",
          "type": "blob",
          "size": 13628
        },
        {
          "path": "icartsh-plugin/skills/git-advanced/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/git-advanced/examples/branch_strategies.md",
          "type": "blob",
          "size": 14771
        },
        {
          "path": "icartsh-plugin/skills/git-advanced/examples/conflict_resolution.md",
          "type": "blob",
          "size": 12186
        },
        {
          "path": "icartsh-plugin/skills/git-advanced/examples/interactive_rebase.md",
          "type": "blob",
          "size": 8151
        },
        {
          "path": "icartsh-plugin/skills/git-advanced/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/git-advanced/references/best-practices.md",
          "type": "blob",
          "size": 9369
        },
        {
          "path": "icartsh-plugin/skills/git-advanced/references/branch-management.md",
          "type": "blob",
          "size": 4253
        },
        {
          "path": "icartsh-plugin/skills/git-advanced/references/reflog-recovery.md",
          "type": "blob",
          "size": 7166
        },
        {
          "path": "icartsh-plugin/skills/git-advanced/references/troubleshooting.md",
          "type": "blob",
          "size": 11325
        },
        {
          "path": "icartsh-plugin/skills/markdown-pro",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/markdown-pro/README.md",
          "type": "blob",
          "size": 10380
        },
        {
          "path": "icartsh-plugin/skills/markdown-pro/SKILL.md",
          "type": "blob",
          "size": 11932
        },
        {
          "path": "icartsh-plugin/skills/markdown-pro/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/markdown-pro/examples/CHANGELOG_template.md",
          "type": "blob",
          "size": 7967
        },
        {
          "path": "icartsh-plugin/skills/markdown-pro/examples/CONTRIBUTING.md",
          "type": "blob",
          "size": 12444
        },
        {
          "path": "icartsh-plugin/skills/markdown-pro/examples/README_template.md",
          "type": "blob",
          "size": 9538
        },
        {
          "path": "icartsh-plugin/skills/mcp-builder",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/mcp-builder/SKILL.md",
          "type": "blob",
          "size": 11013
        },
        {
          "path": "icartsh-plugin/skills/mcp-builder/reference",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/mcp-builder/reference/evaluation.md",
          "type": "blob",
          "size": 21663
        },
        {
          "path": "icartsh-plugin/skills/mcp-builder/reference/mcp_best_practices.md",
          "type": "blob",
          "size": 7330
        },
        {
          "path": "icartsh-plugin/skills/mcp-builder/reference/node_mcp_server.md",
          "type": "blob",
          "size": 28550
        },
        {
          "path": "icartsh-plugin/skills/mcp-builder/reference/python_mcp_server.md",
          "type": "blob",
          "size": 25099
        },
        {
          "path": "icartsh-plugin/skills/sequential-thinking",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/sequential-thinking/README.md",
          "type": "blob",
          "size": 6355
        },
        {
          "path": "icartsh-plugin/skills/sequential-thinking/SKILL.md",
          "type": "blob",
          "size": 3918
        },
        {
          "path": "icartsh-plugin/skills/sequential-thinking/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/sequential-thinking/references/advanced-strategies.md",
          "type": "blob",
          "size": 2587
        },
        {
          "path": "icartsh-plugin/skills/sequential-thinking/references/advanced-techniques.md",
          "type": "blob",
          "size": 2375
        },
        {
          "path": "icartsh-plugin/skills/sequential-thinking/references/core-patterns.md",
          "type": "blob",
          "size": 2553
        },
        {
          "path": "icartsh-plugin/skills/sequential-thinking/references/examples-api.md",
          "type": "blob",
          "size": 2210
        },
        {
          "path": "icartsh-plugin/skills/sequential-thinking/references/examples-architecture.md",
          "type": "blob",
          "size": 2704
        },
        {
          "path": "icartsh-plugin/skills/sequential-thinking/references/examples-debug.md",
          "type": "blob",
          "size": 2496
        },
        {
          "path": "icartsh-plugin/skills/skill-creator",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/skill-creator/SKILL.md",
          "type": "blob",
          "size": 20920
        },
        {
          "path": "icartsh-plugin/skills/skill-creator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/skill-creator/references/output-patterns.md",
          "type": "blob",
          "size": 1813
        },
        {
          "path": "icartsh-plugin/skills/skill-creator/references/workflows.md",
          "type": "blob",
          "size": 818
        },
        {
          "path": "icartsh-plugin/skills/sql-expert",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/sql-expert/README.md",
          "type": "blob",
          "size": 12493
        },
        {
          "path": "icartsh-plugin/skills/sql-expert/SKILL.md",
          "type": "blob",
          "size": 15881
        },
        {
          "path": "icartsh-plugin/skills/sql-expert/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/sql-expert/references/advanced-patterns.md",
          "type": "blob",
          "size": 7113
        },
        {
          "path": "icartsh-plugin/skills/sql-expert/references/best-practices.md",
          "type": "blob",
          "size": 8145
        },
        {
          "path": "icartsh-plugin/skills/sql-expert/references/common-pitfalls.md",
          "type": "blob",
          "size": 9667
        },
        {
          "path": "icartsh-plugin/skills/sql-expert/references/indexes-performance.md",
          "type": "blob",
          "size": 3217
        },
        {
          "path": "icartsh-plugin/skills/sql-expert/references/query-optimization.md",
          "type": "blob",
          "size": 3179
        },
        {
          "path": "icartsh-plugin/skills/sql-optimization-patterns",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/sql-optimization-patterns/SKILL.md",
          "type": "blob",
          "size": 14846
        },
        {
          "path": "icartsh-plugin/skills/web-artifacts-builder",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/web-artifacts-builder/SKILL.md",
          "type": "blob",
          "size": 3774
        },
        {
          "path": "icartsh-plugin/skills/webapp-testing",
          "type": "tree",
          "size": null
        },
        {
          "path": "icartsh-plugin/skills/webapp-testing/SKILL.md",
          "type": "blob",
          "size": 4756
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n    \"name\": \"icartsh-marketplace\",\n    \"owner\": {\n        \"name\": \"ICARTSH\"\n    },\n    \"plugins\": [\n        {\n            \"name\": \"icartsh-plugin\",\n            \"source\": \"./icartsh-plugin\",\n            \"description\": \"ICARTSH Plugin\",\n            \"strict\": false,\n            \"skills\": [\n                \"./icartsh-plugin/skills/api-designer\",\n                \"./icartsh-plugin/skills/brainstorming\",\n                \"./icartsh-plugin/skills/code-analyze\",\n                \"./icartsh-plugin/skills/code-format\",\n                \"./icartsh-plugin/skills/code-reviewer\",\n                \"./icartsh-plugin/skills/code-write\",\n                \"./icartsh-plugin/skills/coding-conventions\",\n                \"./icartsh-plugin/skills/csharp-async-patterns\",\n                \"./icartsh-plugin/skills/csharp-developer\",\n                \"./icartsh-plugin/skills/docker-workflow\",\n                \"./icartsh-plugin/skills/dotnet-build\",\n                \"./icartsh-plugin/skills/dotnet-test\",\n                \"./icartsh-plugin/skills/error-detective\",\n                \"./icartsh-plugin/skills/file-organizer\",\n                \"./icartsh-plugin/skills/frontend-design\",\n                \"./icartsh-plugin/skills/git-advanced\",\n                \"./icartsh-plugin/skills/markdown-pro\",\n                \"./icartsh-plugin/skills/mcp-builder\",\n                \"./icartsh-plugin/skills/sequential-thinking\",\n                \"./icartsh-plugin/skills/skill-creator\",\n                \"./icartsh-plugin/skills/sql-expert\",\n                \"./icartsh-plugin/skills/sql-optimization-patterns\",\n                \"./icartsh-plugin/skills/web-artifacts-builder\",\n                \"./icartsh-plugin/skills/webapp-testing\"\n            ]\n        }\n    ]\n}",
        "icartsh-plugin/.claude-plugin/plugin.json": "{\n    \"name\": \"icartsh-plugin\",\n    \"description\": \"ICARTSH Plugin\",\n    \"version\": \"1.0.0\",\n    \"author\": {\n        \"name\": \"ICARTSH\"\n    }\n}",
        "icartsh-plugin/skills/api-designer/README.md": "# API Designer Skill\n\nRESTful 및 GraphQL API를 설계, 문서화 및 구현하기 위한 포괄적인 Claude SKILL입니다.\n\n## Overview\n\n이 SKILL은 API 설계 패턴, OpenAPI 사양 생성, 인증 전략, 버전 관리 접근 방식 및 업계 모범 사례에 대한 전문가 가이드를 제공합니다.\n\n## Contents\n\n### SKILL.md\n다음을 포함하는 메인 SKILL 파일:\n- REST API 설계 워크플로우\n- GraphQL 스키마 설계\n- 인증 패턴 (OAuth 2.0, JWT, API Keys)\n- API 버전 관리 전략\n- OpenAPI 사양 기초\n- 모범 사례 및 빠른 참조(Quick reference)\n\n### scripts/\n**api_helper.py** - API 개발을 위한 Python 유틸리티:\n- OpenAPI 사양 생성\n- 기존 사양 검증\n- 사양으로부터 문서 생성\n\nUsage:\n```bash\n# 샘플 OpenAPI 사양 생성\npython scripts/api_helper.py generate --sample --output openapi.yaml\n\n# 기존 사양 검증\npython scripts/api_helper.py validate --spec openapi.yaml\n\n# 문서 생성\npython scripts/api_helper.py docs --spec openapi.yaml --output api-docs.md\n```\n\n### examples/\n**openapi_spec.yaml** - 전체 OpenAPI 3.0 사양 예시:\n- 인증 엔드포인트\n- 사용자 관리\n- 블로그 포스트 및 댓글\n- Pagination 및 filtering\n- 에러 응답\n\n**graphql_schema.graphql** - 전체 GraphQL 스키마 예시:\n- Type 정의\n- Query 및 mutation\n- Input type 및 payload\n- Subscription\n- 커스텀 directive\n\n### references/\n**rest_best_practices.md** - 포괄적인 REST API 패턴:\n- URL 설계 가이드라인\n- HTTP method 사용법\n- Status code 참조\n- 인증 패턴\n- Pagination 전략\n- Rate limiting\n- Caching\n- CORS\n- 문서화 표준\n\n## Quick Start\n\n1. 핵심 API 설계 워크플로우를 위해 `SKILL.md`를 읽으세요.\n2. OpenAPI 구조를 위해 `examples/openapi_spec.yaml`을 참조하세요.\n3. 상세한 패턴을 위해 `references/rest_best_practices.md`를 검토하세요.\n4. 사양 생성 및 검증을 위해 `scripts/api_helper.py`를 사용하세요.\n\n## Target Complexity\n\nMiddle complexity SKILL (SKILL.md 내 667 라인)\n- 메인 SKILL 파일의 핵심 워크플로우 및 패턴\n- 별도 파일의 상세 참조 및 예시\n- 일반적인 작업을 위한 실행 가능한 유틸리티\n",
        "icartsh-plugin/skills/api-designer/SKILL.md": "---\nname: api-designer\ndescription: \"OpenAPI/Swagger 사양, 인증 패턴, 버전 관리 전략 및 모범 사례를 사용하여 RESTful 및 GraphQL API를 설계하고 문서화합니다. 사용 사례: (1) API 사양 생성, (2) REST 엔드포인트 설계, (3) GraphQL 스키마 설계, (4) API 인증 및 권한 부여, (5) API 버전 관리 전략, (6) 문서 생성\"\n---\n\n# API Designer\n\n## Overview\n\n이 SKILL은 현대적인 API를 설계, 문서화 및 구현하기 위한 포괄적인 가이드를 제공합니다. REST 및 GraphQL 패러다임을 모두 다루며, 업계의 모범 사례, 명확한 문서화 및 유지보수 가능한 아키텍처를 강조합니다. 확장 가능하고 안전하며 개발자 친화적인 Production-ready API 설계를 위해 이 SKILL을 사용하세요.\n\n## Core Capabilities\n\n### REST API Design\n- 적절한 URL 구조를 갖춘 리소스 지향 엔드포인트 설계\n- HTTP method 의미론 및 status code 사용\n- 일관된 명명 규칙을 적용한 Request/response payload 설계\n- Pagination, filtering 및 sorting 전략\n- Error handling 및 validation 패턴\n\n### GraphQL API Design\n- Type system 및 관계를 포함한 Schema 정의\n- 적절한 input type을 사용한 Query 및 mutation 설계\n- Resolver 패턴 및 성능 최적화\n- Fragment 사용 및 directive 구현\n- N+1 문제 방지 전략\n\n### API Documentation\n- OpenAPI 3.0 specification 생성\n- Swagger UI를 통한 대화형 문서화\n- Authentication 및 authorization 문서화\n- 다양한 시나리오를 포함한 Example requests/responses\n- 사양(Specification)으로부터 코드 생성\n\n### Authentication & Authorization\n- OAuth 2.0 flow (authorization code, client credentials, PKCE)\n- JWT token 설계, validation 및 rotation\n- API key 관리 및 rotation 전략\n- Role-based access control (RBAC) 구현\n- Rate limiting 및 throttling 패턴\n\n### API Versioning\n- URL versioning 및 header-based versioning 전략\n- API 릴리스를 위한 Semantic versioning\n- Deprecation 계획 및 커뮤니케이션\n- Backward compatibility 유지\n- Migration 경로 설계\n\n## When to Use This Skill\n\n이 SKILL은 다음과 같은 경우에 사용하세요:\n- 새로운 API를 처음부터 설계하거나 기존 엔드포인트를 리팩토링할 때\n- 문서화를 위해 OpenAPI/Swagger 사양을 생성할 때\n- Authentication 및 authorization flow를 구현할 때\n- API versioning 및 deprecation 전략을 계획할 때\n- GraphQL schema 및 resolver를 설계할 때\n- API governance 및 모범 사례를 확립할 때\n\n## REST API Design Workflow\n\n### Step 1: Identify Resources\n\nAPI가 노출할 핵심 리소스(Noun)를 식별합니다:\n\n```\nResources: Users, Posts, Comments\n\nCollections:\n- GET    /users              (모든 사용자 목록 조회)\n- POST   /users              (새 사용자 생성)\n\nIndividual Resources:\n- GET    /users/{id}         (특정 사용자 조회)\n- PUT    /users/{id}         (사용자 교체 - 전체 업데이트)\n- PATCH  /users/{id}         (사용자 업데이트 - 일부 업데이트)\n- DELETE /users/{id}         (사용자 삭제)\n\nNested Resources:\n- GET    /users/{id}/posts   (사용자의 포스트 조회)\n- POST   /users/{id}/posts   (사용자를 위한 포스트 생성)\n```\n\n### Step 2: Design URL Structure\n\nRESTful 명명 규칙을 따릅니다:\n\n**Best Practices**:\n- 복수형 명사 사용: `/users`, `/posts` (`/user`, `/post` 아님)\n- 여러 단어는 하이픈 사용: `/blog-posts` (`/blogPosts` 또는 `/blog_posts` 아님)\n- URL은 소문자로 유지\n- Nesting은 최대 2단계로 제한\n- Filtering을 위해 query parameter 사용: `/posts?status=published&author=123`\n\n**Quick Examples**:\n```\n✅ Good:\nGET /users\nGET /users/123/posts\nGET /posts?published=true&limit=10\n\n❌ Bad:\nGET /getUsers\nGET /users/123/posts/comments/likes  (너무 깊은 nesting)\nGET /posts/published  (대신 query param 사용)\n```\n\n### Step 3: Choose HTTP Methods\n\n작업을 표준 HTTP method에 매핑합니다:\n\n- **GET**: 리소스 조회 - Safe, idempotent, cacheable\n- **POST**: 새 리소스 생성 - Location header와 함께 201 Created 반환\n- **PUT**: 전체 리소스 교체 - Idempotent, 전체 교체\n- **PATCH**: 부분 업데이트 - 특정 필드만 업데이트\n- **DELETE**: 리소스 제거 - Idempotent, 204 또는 200 반환\n\n### Step 4: Design Request/Response Payloads\n\nJSON payload를 일관되게 구성합니다:\n\n**Naming Conventions**:\n- JSON 필드 이름에 camelCase 사용\n- 타임스탬프에 ISO 8601 사용 (UTC)\n- 접두사가 있는 일관된 ID 형식 사용: `usr_`, `post_`\n- 메타데이터 포함: `createdAt`, `updatedAt`\n\n**Example Response**:\n```json\n{\n  \"id\": \"usr_1234567890\",\n  \"username\": \"johndoe\",\n  \"email\": \"john@example.com\",\n  \"profile\": {\n    \"firstName\": \"John\",\n    \"lastName\": \"Doe\"\n  },\n  \"createdAt\": \"2025-10-25T10:30:00Z\",\n  \"updatedAt\": \"2025-10-25T10:30:00Z\"\n}\n```\n\n### Step 5: Implement Error Handling\n\n포괄적인 에러 응답을 설계합니다:\n\n**Error Response Format**:\n```json\n{\n  \"error\": {\n    \"code\": \"VALIDATION_ERROR\",\n    \"message\": \"Invalid request parameters\",\n    \"details\": [\n      {\n        \"field\": \"email\",\n        \"message\": \"Email format is invalid\"\n      }\n    ],\n    \"requestId\": \"req_abc123xyz\",\n    \"timestamp\": \"2025-10-25T10:30:00Z\"\n  }\n}\n```\n\n**Key Status Codes**:\n- `200 OK`: 성공적인 GET, PUT, PATCH\n- `201 Created`: 성공적인 POST\n- `204 No Content`: 성공적인 DELETE\n- `400 Bad Request`: 유효하지 않은 요청 데이터\n- `401 Unauthorized`: 인증 정보 누락/유효하지 않음\n- `403 Forbidden`: 인증되었으나 권한 없음\n- `404 Not Found`: 리소스가 존재하지 않음\n- `422 Unprocessable Entity`: Validation 에러\n- `429 Too Many Requests`: Rate limit 초과\n- `500 Internal Server Error`: 서버 에러\n\n### Step 6: Add Pagination and Filtering\n\n**Cursor-Based Pagination** (대규모 데이터셋에 권장):\n```\nGET /posts?limit=20&cursor=eyJpZCI6MTIzfQ\n\nResponse:\n{\n  \"data\": [...],\n  \"pagination\": {\n    \"nextCursor\": \"eyJpZCI6MTQzfQ\",\n    \"hasMore\": true\n  }\n}\n```\n\n**Offset-Based Pagination** (소규모 데이터셋에 적합):\n```\nGET /posts?limit=20&offset=40&sort=-createdAt\n\nResponse:\n{\n  \"data\": [...],\n  \"pagination\": {\n    \"total\": 500,\n    \"limit\": 20,\n    \"offset\": 40\n  }\n}\n```\n\n상세한 Pagination 전략 및 filtering 패턴은 `references/rest_best_practices.md`를 참조하세요.\n\n## GraphQL API Design Workflow\n\n### Step 1: Define Schema Types\n\n도메인을 위한 type definition을 생성합니다:\n\n```graphql\ntype User {\n  id: ID!\n  username: String!\n  email: String!\n  profile: Profile\n  posts(limit: Int = 10): [Post!]!\n  createdAt: DateTime!\n}\n\ntype Post {\n  id: ID!\n  title: String!\n  content: String!\n  published: Boolean!\n  author: User!\n  tags: [String!]!\n  createdAt: DateTime!\n}\n```\n\n### Step 2: Design Queries\n\nFiltering을 포함한 조회 작업을 정의합니다:\n\n```graphql\ntype Query {\n  user(id: ID!): User\n  post(id: ID!): Post\n\n  users(\n    limit: Int = 10\n    offset: Int = 0\n    search: String\n  ): UserConnection!\n\n  posts(\n    limit: Int = 10\n    published: Boolean\n    authorId: ID\n    tags: [String!]\n  ): PostConnection!\n}\n```\n\n### Step 3: Design Mutations\n\nInput type 및 error handling을 포함한 쓰기 작업을 정의합니다:\n\n```graphql\ntype Mutation {\n  createUser(input: CreateUserInput!): CreateUserPayload!\n  updateUser(id: ID!, input: UpdateUserInput!): UpdateUserPayload!\n  createPost(input: CreatePostInput!): CreatePostPayload!\n}\n\ninput CreateUserInput {\n  username: String!\n  email: String!\n  password: String!\n}\n\ntype CreateUserPayload {\n  user: User\n  errors: [Error!]\n}\n```\n\n전체 GraphQL schema 예시는 `examples/graphql_schema.graphql`을 참조하세요.\n\n## Authentication Patterns\n\n### OAuth 2.0 Quick Reference\n\n**Authorization Code Flow** (백엔드가 있는 웹 앱):\n```\n1. client_id, redirect_uri, scope와 함께 /oauth/authorize로 리다이렉트\n2. 사용자가 인증하고 권한 부여\n3. 리다이렉트를 통해 authorization code 수신\n4. /oauth/token에서 코드를 access token으로 교환\n5. Authorization header에 access token 사용\n```\n\n**Client Credentials Flow** (서비스 간 통신):\n```\nPOST /oauth/token\n{\n  \"grant_type\": \"client_credentials\",\n  \"client_id\": \"CLIENT_ID\",\n  \"client_secret\": \"SECRET\"\n}\n```\n\n**PKCE Flow** (모바일/SPA - 퍼블릭 클라이언트에 가장 안전):\n```\n1. code_verifier 및 code_challenge 생성\n2. code_challenge와 함께 권한 요청\n3. code_verifier로 코드를 토큰으로 교환 (client_secret 불필요)\n```\n\n### JWT Token Design\n\n**Token Structure**:\n```json\n{\n  \"header\": { \"alg\": \"RS256\", \"typ\": \"JWT\" },\n  \"payload\": {\n    \"sub\": \"usr_1234567890\",\n    \"iat\": 1698336000,\n    \"exp\": 1698339600,\n    \"scope\": [\"read:posts\", \"write:posts\"],\n    \"roles\": [\"user\", \"editor\"]\n  }\n}\n```\n\n**Usage**:\n```http\nAuthorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...\n```\n\n### API Key Authentication\n\n```http\nX-API-Key: sk_live_abcdef1234567890\n```\n\n**Best Practices**:\n- 환경별(dev, staging, prod)로 다른 키 사용\n- 로테이션을 위해 계정당 여러 키 지원\n- Key expiration 및 사용 로그 구현\n- 클라이언트 측 코드에 키를 노출하지 않음\n\nRefresh token, MFA 및 보안 모범 사례를 포함한 종합적인 인증 패턴은 `references/authentication.md`를 참조하세요.\n\n## API Versioning Strategies\n\n### URL Versioning (권장)\n\n```\n/v1/users\n/v2/users\n```\n\n**장점**: 명확하고 명시적이며, 캐싱 및 라우팅이 쉬움\n**단점**: URL 확산, 여러 코드베이스 관리\n\n### Header Versioning\n\n```http\nAccept: application/vnd.myapi.v2+json\nAPI-Version: 2\n```\n\n**장점**: 깔끔한 URL, 동일한 엔드포인트 유지\n**단점**: 덜 가시적이며, 브라우저에서 테스트하기 어려움\n\n### When to Version\n\n**새로운 버전이 필요한 경우**:\n- 엔드포인트 또는 필드 삭제\n- 필드 유형 또는 이름 변경\n- 인증 방법 수정\n- 기존 클라이언트 계약(Contract) 위반\n\n**버전 관리가 필요 없는 경우**:\n- 새로운 선택적(Optional) 필드 추가\n- 새로운 엔드포인트 추가\n- 버그 수정 또는 성능 향상\n\n상세한 Versioning 전략, deprecation 프로세스 및 migration 패턴은 `references/versioning-strategies.md`를 참조하세요.\n\n## OpenAPI Specification\n\n### Basic Structure\n\n```yaml\nopenapi: 3.0.0\ninfo:\n  title: My API\n  version: 1.0.0\n  description: API description\n\nservers:\n  - url: https://api.example.com/v1\n\npaths:\n  /users:\n    get:\n      summary: List users\n      parameters:\n        - name: limit\n          in: query\n          schema:\n            type: integer\n            default: 10\n      responses:\n        '200':\n          description: Successful response\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/UserList'\n\ncomponents:\n  schemas:\n    User:\n      type: object\n      required:\n        - username\n        - email\n      properties:\n        id:\n          type: string\n        username:\n          type: string\n        email:\n          type: string\n          format: email\n```\n\n전체 OpenAPI 사양 예시는 `examples/openapi_spec.yaml`을 참조하세요.\n\n### Generating Documentation\n\nHelper script를 사용하여 사양을 생성하고 검증합니다:\n\n```bash\n# 코드에서 OpenAPI 사양 생성\npython scripts/api_helper.py generate --input api.py --output openapi.yaml\n\n# 기존 사양 검증\npython scripts/api_helper.py validate --spec openapi.yaml\n\n# 문서 사이트 생성\npython scripts/api_helper.py docs --spec openapi.yaml --output docs/\n```\n\n## Best Practices Summary\n\n### Consistency\n- 모든 엔드포인트에서 일관된 명명 규칙 사용\n- 에러 응답 형식 표준화\n- 모든 곳에 동일한 인증 패턴 적용\n- 통일된 타임스탬프 형식 사용 (ISO 8601 with UTC)\n\n### Security\n- Production에서는 항상 HTTPS 사용\n- 모든 입력 데이터를 철저히 검증\n- 사용자/키/IP별로 Rate limiting 구현\n- 모든 엔드포인트에 적절한 인증 사용\n- URL이나 로그에 민감한 데이터를 노출하지 않음\n- 적절한 CORS 구성 구현\n\n### Performance\n- 대규모 데이터셋에 Pagination 사용\n- 캐싱 헤더(ETag, Cache-Control) 구현\n- 압축(gzip) 지원\n- 실시간 데이터에 Cursor-based pagination 사용\n- Sparse fieldsets을 위한 필드 선택(Field selection) 구현\n\n### Documentation\n- 모든 엔드포인트를 OpenAPI로 문서화\n- Example requests 및 responses 제공\n- 에러 코드 및 의미 문서화\n- 인증 안내 포함\n- 문서를 코드와 동기화된 상태로 유지\n\n### Maintainability\n- 명확한 deprecation 일정을 가지고 적절하게 API 버전 관리\n- 기능을 제거하기 전에 deprecation 경고 제공\n- 모든 엔드포인트에 대해 통합 테스트 작성\n- API 사용량, 에러 및 성능 모니터링\n- 가능한 경우 Backward compatibility 유지\n\n## Common Patterns\n\n### Health Check\n```http\nGET /health\nResponse: { \"status\": \"ok\", \"timestamp\": \"2025-10-25T10:30:00Z\" }\n```\n\n### Batch Operations\n```http\nPOST /users/batch\n{\n  \"operations\": [\n    { \"method\": \"POST\", \"path\": \"/users\", \"body\": {...} },\n    { \"method\": \"PATCH\", \"path\": \"/users/123\", \"body\": {...} }\n  ]\n}\n```\n\n### Webhooks\n```http\nPOST /webhooks/configure\n{\n  \"url\": \"https://your-app.com/webhook\",\n  \"events\": [\"user.created\", \"post.published\"],\n  \"secret\": \"webhook_secret_key\"\n}\n```\n\nIdempotency, long-running operations, file uploads 및 soft deletes를 포함한 추가 패턴은 `references/common-patterns.md`를 참조하세요.\n\n## Quick Reference Checklists\n\n### REST Endpoint Design\n- [ ] 컬렉션에 복수형 명사 사용\n- [ ] URL nesting을 2단계로 제한\n- [ ] 적절한 HTTP method 사용\n- [ ] 정확한 status code 반환\n- [ ] 일관된 에러 형식 구현\n- [ ] 컬렉션에 Pagination 추가\n- [ ] Filtering 및 sorting 포함\n- [ ] OpenAPI로 문서화\n- [ ] Authentication 구현\n- [ ] Rate limiting 추가\n\n### GraphQL Schema Design\n- [ ] 명확한 type hierarchy 정의\n- [ ] Nullable type을 적절하게 사용\n- [ ] Pagination(connections) 구현\n- [ ] Input type을 사용한 mutation 설계\n- [ ] Payload에 에러 반환\n- [ ] 설명(Description)으로 스키마 문서화\n- [ ] Authentication/authorization 구현\n- [ ] N+1 쿼리 최적화 (DataLoader)\n\n## Additional Resources\n\n### Comprehensive References\n- `references/rest_best_practices.md` - 전체 REST API 패턴, status code 및 구현 세부 정보\n- `references/authentication.md` - OAuth 2.0, JWT, API keys, MFA 및 보안 모범 사례\n- `references/versioning-strategies.md` - Versioning 접근 방식, deprecation 및 migration 전략\n- `references/common-patterns.md` - Health check, webhooks, batch operations 등\n\n### Examples\n- `examples/openapi_spec.yaml` - 블로그 API를 위한 전체 OpenAPI 3.0 사양\n- `examples/graphql_schema.graphql` - Query, mutation 및 subscription을 포함한 전체 GraphQL schema\n\n### Tools\n- `scripts/api_helper.py` - API 사양 생성, 검증 및 문서화 유틸리티\n",
        "icartsh-plugin/skills/api-designer/references/authentication.md": "# API Authentication Patterns\n\nComprehensive guide to implementing authentication and authorization in APIs.\n\n## Table of Contents\n\n- [OAuth 2.0 Flows](#oauth-20-flows)\n- [JWT Token Design](#jwt-token-design)\n- [API Key Authentication](#api-key-authentication)\n- [Session-Based Authentication](#session-based-authentication)\n- [Security Best Practices](#security-best-practices)\n\n## OAuth 2.0 Flows\n\n### Authorization Code Flow\n\n**Use Case**: Web applications with backend server\n\n**Flow Diagram**:\n```\n1. Client redirects user to authorization server\n   GET /oauth/authorize?\n     client_id=CLIENT_ID&\n     redirect_uri=CALLBACK_URL&\n     response_type=code&\n     scope=read write&\n     state=RANDOM_STATE\n\n2. User authenticates and grants permission\n\n3. Authorization server redirects back with code\n   GET CALLBACK_URL?code=AUTH_CODE&state=RANDOM_STATE\n\n4. Client exchanges code for token\n   POST /oauth/token\n   {\n     \"grant_type\": \"authorization_code\",\n     \"code\": \"AUTH_CODE\",\n     \"redirect_uri\": \"CALLBACK_URL\",\n     \"client_id\": \"CLIENT_ID\",\n     \"client_secret\": \"CLIENT_SECRET\"\n   }\n\n5. Response contains access token\n   {\n     \"access_token\": \"eyJhbGc...\",\n     \"token_type\": \"Bearer\",\n     \"expires_in\": 3600,\n     \"refresh_token\": \"def50200...\",\n     \"scope\": \"read write\"\n   }\n```\n\n**Security Considerations**:\n- Always validate `state` parameter to prevent CSRF attacks\n- Use HTTPS for all OAuth endpoints\n- Store client_secret securely, never expose to client-side code\n- Implement token expiration and refresh\n- Validate redirect_uri against registered URIs\n\n### Client Credentials Flow\n\n**Use Case**: Service-to-service authentication (machine-to-machine)\n\n**Flow**:\n```\nPOST /oauth/token\n{\n  \"grant_type\": \"client_credentials\",\n  \"client_id\": \"CLIENT_ID\",\n  \"client_secret\": \"CLIENT_SECRET\",\n  \"scope\": \"read write\"\n}\n\nResponse:\n{\n  \"access_token\": \"eyJhbGc...\",\n  \"token_type\": \"Bearer\",\n  \"expires_in\": 3600,\n  \"scope\": \"read write\"\n}\n```\n\n**Best Practices**:\n- Use for server-to-server communication only\n- Limit scope to minimum required permissions\n- Rotate client secrets regularly\n- Monitor and log all client credential usage\n- Implement rate limiting per client\n\n### PKCE Flow (Proof Key for Code Exchange)\n\n**Use Case**: Mobile apps and Single Page Applications (SPAs)\n\n**Flow**:\n```\n1. Generate code_verifier (random string, 43-128 characters)\n   Example: dBjftJeZ4CVP-mB92K27uhbUJU1p1r_wW1gFWFOEjXk\n\n2. Generate code_challenge from code_verifier\n   code_challenge = BASE64URL(SHA256(code_verifier))\n\n3. Authorization request\n   GET /oauth/authorize?\n     client_id=CLIENT_ID&\n     redirect_uri=CALLBACK&\n     response_type=code&\n     code_challenge=CHALLENGE&\n     code_challenge_method=S256&\n     scope=read write&\n     state=RANDOM_STATE\n\n4. User authenticates\n\n5. Receive authorization code\n   CALLBACK?code=AUTH_CODE&state=RANDOM_STATE\n\n6. Exchange code for token (with verifier)\n   POST /oauth/token\n   {\n     \"grant_type\": \"authorization_code\",\n     \"code\": \"AUTH_CODE\",\n     \"client_id\": \"CLIENT_ID\",\n     \"code_verifier\": \"dBjftJeZ4CVP-mB92K27uhbUJU1p1r_wW1gFWFOEjXk\",\n     \"redirect_uri\": \"CALLBACK\"\n   }\n```\n\n**Why PKCE?**:\n- Prevents authorization code interception attacks\n- No need for client_secret (safe for public clients)\n- Required for mobile and SPA applications\n- Recommended even for confidential clients\n\n### Refresh Token Flow\n\n**Use Case**: Obtaining new access tokens without re-authentication\n\n**Flow**:\n```\nPOST /oauth/token\n{\n  \"grant_type\": \"refresh_token\",\n  \"refresh_token\": \"def50200...\",\n  \"client_id\": \"CLIENT_ID\",\n  \"client_secret\": \"CLIENT_SECRET\"  // Only for confidential clients\n}\n\nResponse:\n{\n  \"access_token\": \"new_access_token\",\n  \"token_type\": \"Bearer\",\n  \"expires_in\": 3600,\n  \"refresh_token\": \"new_refresh_token\",  // Optionally rotate\n  \"scope\": \"read write\"\n}\n```\n\n**Best Practices**:\n- Implement refresh token rotation (issue new refresh token each time)\n- Set longer expiration for refresh tokens (days/weeks vs minutes for access tokens)\n- Store refresh tokens securely (encrypted at rest)\n- Invalidate old refresh token when new one is issued\n- Implement refresh token revocation\n\n## JWT Token Design\n\n### Token Structure\n\nJWT consists of three parts: Header.Payload.Signature\n\n**Header**:\n```json\n{\n  \"alg\": \"RS256\",\n  \"typ\": \"JWT\",\n  \"kid\": \"key-id-2025\"\n}\n```\n\n**Payload**:\n```json\n{\n  \"sub\": \"usr_1234567890\",\n  \"iat\": 1698336000,\n  \"exp\": 1698339600,\n  \"nbf\": 1698336000,\n  \"jti\": \"unique-token-id\",\n  \"iss\": \"https://auth.example.com\",\n  \"aud\": \"https://api.example.com\",\n  \"scope\": [\"read:posts\", \"write:posts\"],\n  \"roles\": [\"user\", \"editor\"]\n}\n```\n\n**Standard Claims**:\n- `sub` (subject): User identifier\n- `iat` (issued at): Token creation timestamp\n- `exp` (expiration): Token expiration timestamp\n- `nbf` (not before): Token valid from timestamp\n- `jti` (JWT ID): Unique token identifier\n- `iss` (issuer): Token issuer\n- `aud` (audience): Intended token recipient\n\n**Custom Claims**:\n- `scope`: Array of permission scopes\n- `roles`: User roles\n- `email`: User email (if needed)\n- `name`: User display name\n\n### Using JWT in Requests\n\n**Authorization Header** (recommended):\n```http\nAuthorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...\n```\n\n**Query Parameter** (avoid for security reasons):\n```http\nGET /api/resource?access_token=eyJhbGc...\n```\n\n### JWT Validation\n\n**Server-side validation steps**:\n1. Verify signature using public key\n2. Check expiration (`exp` claim)\n3. Verify issuer (`iss` claim)\n4. Verify audience (`aud` claim)\n5. Check not-before (`nbf` claim)\n6. Validate custom claims (scope, roles)\n\n**Example validation code**:\n```python\nimport jwt\nfrom datetime import datetime\n\ndef validate_jwt(token, public_key):\n    try:\n        payload = jwt.decode(\n            token,\n            public_key,\n            algorithms=[\"RS256\"],\n            audience=\"https://api.example.com\",\n            issuer=\"https://auth.example.com\"\n        )\n\n        # Additional custom validation\n        if \"read:posts\" not in payload.get(\"scope\", []):\n            raise ValueError(\"Insufficient permissions\")\n\n        return payload\n\n    except jwt.ExpiredSignatureError:\n        raise ValueError(\"Token has expired\")\n    except jwt.InvalidTokenError:\n        raise ValueError(\"Invalid token\")\n```\n\n### Token Rotation and Revocation\n\n**Rotation Strategy**:\n- Short-lived access tokens (15-60 minutes)\n- Long-lived refresh tokens (days to weeks)\n- Rotate refresh tokens on use\n- Issue new access token before expiration\n\n**Revocation**:\n- Maintain token blacklist for revoked tokens\n- Use Redis or similar for fast blacklist lookups\n- Clear blacklist entries after token expiration\n- Consider token versioning for mass revocation\n\n## API Key Authentication\n\n### Implementation Patterns\n\n**Header-Based** (recommended):\n```http\nX-API-Key: sk_live_abc123def456\n# Or\nAuthorization: ApiKey sk_live_abc123def456\n```\n\n**Query Parameter** (less secure, use only for public data):\n```http\nGET /api/data?api_key=sk_live_abc123def456\n```\n\n### API Key Design\n\n**Key Format**:\n```\nsk_[environment]_[random_string]\n\nExamples:\nsk_live_abc123def456ghi789\nsk_test_xyz987uvw654rst321\n```\n\n**Key Metadata**:\n```json\n{\n  \"keyId\": \"key_abc123\",\n  \"key\": \"sk_live_abc123def456\",\n  \"name\": \"Production API Key\",\n  \"userId\": \"usr_1234567890\",\n  \"scopes\": [\"read:posts\", \"write:posts\"],\n  \"rateLimit\": 1000,\n  \"createdAt\": \"2025-10-25T10:00:00Z\",\n  \"expiresAt\": \"2026-10-25T10:00:00Z\",\n  \"lastUsedAt\": \"2025-10-25T14:30:00Z\"\n}\n```\n\n### Best Practices\n\n**Key Management**:\n- Generate cryptographically random keys (minimum 32 bytes)\n- Hash keys before storing in database\n- Support multiple keys per account\n- Allow key naming and description\n- Implement key rotation workflow\n\n**Security**:\n- Different keys for different environments (dev, staging, prod)\n- Implement key expiration dates\n- Log all key usage with timestamps and IPs\n- Rate limit per key\n- Never expose keys in client-side code\n- Provide key rolling/rotation mechanism\n\n**Key Rotation**:\n```\n1. Generate new API key\n2. Provide transition period (both keys work)\n3. Notify user of deprecation timeline\n4. Revoke old key after transition period\n```\n\n## Session-Based Authentication\n\n### Traditional Session Flow\n\n```\n1. User submits credentials\n   POST /auth/login\n   { \"email\": \"user@example.com\", \"password\": \"...\" }\n\n2. Server validates and creates session\n   Session ID: sess_abc123\n   Stores: { userId: \"usr_123\", loginTime: \"...\", ... }\n\n3. Server sets session cookie\n   Set-Cookie: sessionId=sess_abc123; HttpOnly; Secure; SameSite=Strict\n\n4. Client includes cookie in subsequent requests\n   Cookie: sessionId=sess_abc123\n\n5. Server validates session on each request\n```\n\n### Session Security\n\n**Cookie Attributes**:\n```http\nSet-Cookie: sessionId=sess_abc123;\n  HttpOnly;           # Prevents JavaScript access\n  Secure;             # HTTPS only\n  SameSite=Strict;    # CSRF protection\n  Path=/;             # Cookie scope\n  Max-Age=3600;       # Expiration in seconds\n  Domain=.example.com # Cookie domain\n```\n\n**Session Storage**:\n- Store sessions in Redis or similar for performance\n- Implement session expiration\n- Clean up expired sessions regularly\n- Use secure session ID generation\n- Implement session fixation protection\n\n## Security Best Practices\n\n### General Security\n\n**HTTPS Everywhere**:\n- Enforce HTTPS for all endpoints\n- Use HSTS header\n- Implement certificate pinning for mobile apps\n\n**Token Security**:\n- Never log tokens or credentials\n- Rotate secrets regularly\n- Use environment variables for secrets\n- Implement token binding (bind token to client)\n\n### Rate Limiting\n\n**Implementation**:\n```http\n# Request headers\nX-RateLimit-Limit: 100\nX-RateLimit-Remaining: 75\nX-RateLimit-Reset: 1698340800\n\n# When exceeded\nHTTP/1.1 429 Too Many Requests\nRetry-After: 60\n```\n\n**Strategies**:\n- Per user/API key\n- Per IP address\n- Per endpoint\n- Sliding window vs fixed window\n- Distributed rate limiting (Redis)\n\n### Defense Against Common Attacks\n\n**CSRF Protection**:\n- Use SameSite cookie attribute\n- Implement CSRF tokens\n- Validate Origin/Referer headers\n\n**XSS Protection**:\n- Sanitize user input\n- Set Content-Security-Policy headers\n- Use HttpOnly cookies\n\n**Brute Force Protection**:\n- Implement account lockout\n- Use CAPTCHA after failed attempts\n- Exponential backoff\n- Monitor suspicious patterns\n\n**Injection Attacks**:\n- Validate and sanitize all input\n- Use parameterized queries\n- Implement input length limits\n- Reject unexpected characters\n\n### Monitoring and Logging\n\n**What to Log**:\n- Authentication attempts (success and failure)\n- Token generation and refresh\n- API key usage\n- Rate limit violations\n- Permission denials\n- Suspicious patterns\n\n**What NOT to Log**:\n- Passwords or credentials\n- Full tokens (log only last 4 characters)\n- API keys (log key ID only)\n- Sensitive user data\n\n### Multi-Factor Authentication (MFA)\n\n**Implementation Flow**:\n```\n1. User provides username/password\n2. Server validates primary credentials\n3. Server generates MFA challenge (TOTP, SMS, etc.)\n4. User provides MFA code\n5. Server validates MFA code\n6. Server issues access token\n```\n\n**MFA Methods**:\n- TOTP (Time-based One-Time Password) - Google Authenticator\n- SMS codes (less secure, but convenient)\n- Email codes\n- Hardware tokens (YubiKey)\n- Biometric authentication\n- Backup codes\n\n**Best Practices**:\n- Store recovery codes for account recovery\n- Allow multiple MFA methods\n- Implement MFA remember device\n- Provide clear setup instructions\n- Support MFA reset process\n",
        "icartsh-plugin/skills/api-designer/references/common-patterns.md": "# Common API Patterns\n\nReusable patterns and practices for API design and implementation.\n\n## Table of Contents\n\n- [Health Check Endpoints](#health-check-endpoints)\n- [Batch Operations](#batch-operations)\n- [Webhook Patterns](#webhook-patterns)\n- [Idempotency](#idempotency)\n- [Long-Running Operations](#long-running-operations)\n- [File Upload and Download](#file-upload-and-download)\n- [Search and Autocomplete](#search-and-autocomplete)\n- [Soft Deletes](#soft-deletes)\n\n## Health Check Endpoints\n\n### Basic Health Check\n\n```http\nGET /health\n\nResponse: 200 OK\n{\n  \"status\": \"ok\",\n  \"timestamp\": \"2025-10-25T10:30:00Z\",\n  \"version\": \"1.2.3\"\n}\n```\n\n### Detailed Health Check\n\n```http\nGET /health/detailed\n\nResponse: 200 OK\n{\n  \"status\": \"ok\",\n  \"timestamp\": \"2025-10-25T10:30:00Z\",\n  \"version\": \"1.2.3\",\n  \"uptime\": 86400,\n  \"services\": {\n    \"database\": {\n      \"status\": \"ok\",\n      \"responseTime\": 5,\n      \"connection\": \"active\"\n    },\n    \"cache\": {\n      \"status\": \"ok\",\n      \"responseTime\": 2,\n      \"hitRate\": 0.85\n    },\n    \"externalApi\": {\n      \"status\": \"degraded\",\n      \"responseTime\": 500,\n      \"lastError\": \"Timeout after 5s\"\n    }\n  },\n  \"metrics\": {\n    \"requestsPerMinute\": 1200,\n    \"errorRate\": 0.02,\n    \"averageResponseTime\": 150\n  }\n}\n```\n\n### Readiness and Liveness\n\n**Liveness** (is the service running?):\n```http\nGET /health/live\n\nResponse: 200 OK if service is running\nResponse: 503 Service Unavailable if service is down\n```\n\n**Readiness** (is the service ready to accept traffic?):\n```http\nGET /health/ready\n\nResponse: 200 OK if ready\nResponse: 503 Service Unavailable if not ready (e.g., initializing)\n```\n\n## Batch Operations\n\n### Batch Create/Update\n\n```http\nPOST /api/v1/users/batch\n\nRequest:\n{\n  \"operations\": [\n    {\n      \"method\": \"POST\",\n      \"path\": \"/users\",\n      \"body\": {\n        \"username\": \"user1\",\n        \"email\": \"user1@example.com\"\n      }\n    },\n    {\n      \"method\": \"PATCH\",\n      \"path\": \"/users/123\",\n      \"body\": {\n        \"email\": \"newemail@example.com\"\n      }\n    },\n    {\n      \"method\": \"DELETE\",\n      \"path\": \"/users/456\"\n    }\n  ]\n}\n\nResponse: 200 OK\n{\n  \"results\": [\n    {\n      \"status\": 201,\n      \"body\": { \"id\": \"789\", \"username\": \"user1\", ... }\n    },\n    {\n      \"status\": 200,\n      \"body\": { \"id\": \"123\", \"email\": \"newemail@example.com\", ... }\n    },\n    {\n      \"status\": 204,\n      \"body\": null\n    }\n  ],\n  \"summary\": {\n    \"total\": 3,\n    \"successful\": 3,\n    \"failed\": 0\n  }\n}\n```\n\n### Batch with Transactions\n\n```http\nPOST /api/v1/batch\n\nRequest:\n{\n  \"atomic\": true,  # All operations succeed or all fail\n  \"operations\": [...]\n}\n\n# If any operation fails with atomic=true\nResponse: 400 Bad Request\n{\n  \"error\": \"BATCH_OPERATION_FAILED\",\n  \"message\": \"Operation 2 failed, all operations rolled back\",\n  \"failedOperation\": {\n    \"index\": 1,\n    \"error\": \"User with email already exists\"\n  }\n}\n```\n\n## Webhook Patterns\n\n### Webhook Registration\n\n```http\nPOST /api/v1/webhooks\n\nRequest:\n{\n  \"url\": \"https://your-app.com/webhook\",\n  \"events\": [\"user.created\", \"post.published\", \"comment.added\"],\n  \"secret\": \"webhook_secret_key_for_signature_validation\",\n  \"active\": true,\n  \"description\": \"Production webhook for user events\"\n}\n\nResponse: 201 Created\n{\n  \"id\": \"webhook_abc123\",\n  \"url\": \"https://your-app.com/webhook\",\n  \"events\": [\"user.created\", \"post.published\", \"comment.added\"],\n  \"active\": true,\n  \"createdAt\": \"2025-10-25T10:00:00Z\"\n}\n```\n\n### Webhook Payload\n\n```http\nPOST https://your-app.com/webhook\n\nHeaders:\nX-Webhook-ID: webhook_abc123\nX-Webhook-Signature: sha256=abc123...\nX-Webhook-Event: user.created\nX-Webhook-Delivery: delivery_xyz789\n\nBody:\n{\n  \"event\": \"user.created\",\n  \"timestamp\": \"2025-10-25T10:30:00Z\",\n  \"data\": {\n    \"id\": \"usr_123\",\n    \"username\": \"johndoe\",\n    \"email\": \"john@example.com\",\n    \"createdAt\": \"2025-10-25T10:30:00Z\"\n  }\n}\n```\n\n### Webhook Signature Validation\n\n**Server-side (sending)**:\n```python\nimport hmac\nimport hashlib\n\ndef generate_signature(payload, secret):\n    signature = hmac.new(\n        secret.encode(),\n        payload.encode(),\n        hashlib.sha256\n    ).hexdigest()\n    return f\"sha256={signature}\"\n```\n\n**Client-side (receiving)**:\n```python\ndef verify_signature(payload, signature, secret):\n    expected = generate_signature(payload, secret)\n    return hmac.compare_digest(signature, expected)\n```\n\n### Webhook Retry Logic\n\n```\nRetry schedule on failure:\n- Attempt 1: Immediately\n- Attempt 2: After 1 minute\n- Attempt 3: After 5 minutes\n- Attempt 4: After 15 minutes\n- Attempt 5: After 1 hour\n- Attempt 6: After 6 hours\n- Give up after 24 hours\n\nResponse handling:\n- 2xx: Success, no retry\n- 4xx: Client error, no retry (except 429)\n- 429: Rate limited, retry with backoff\n- 5xx: Server error, retry with backoff\n- Timeout: Retry\n```\n\n## Idempotency\n\n### Idempotency Keys\n\n```http\nPOST /api/v1/payments\nIdempotency-Key: unique-key-12345\n\nRequest:\n{\n  \"amount\": 1000,\n  \"currency\": \"USD\",\n  \"description\": \"Payment for order #123\"\n}\n\n# First request\nResponse: 201 Created\n{\n  \"id\": \"pay_abc123\",\n  \"amount\": 1000,\n  \"status\": \"completed\"\n}\n\n# Duplicate request (same idempotency key)\nResponse: 200 OK  # Returns cached result\n{\n  \"id\": \"pay_abc123\",  # Same payment ID\n  \"amount\": 1000,\n  \"status\": \"completed\"\n}\n```\n\n### Implementation Guidelines\n\n**Key Generation** (client-side):\n```javascript\n// Use UUID v4 for idempotency keys\nconst idempotencyKey = crypto.randomUUID();\n\nfetch('/api/v1/payments', {\n  method: 'POST',\n  headers: {\n    'Idempotency-Key': idempotencyKey\n  },\n  body: JSON.stringify(paymentData)\n});\n```\n\n**Server-side Storage**:\n```python\n# Store idempotency key with result\n{\n  \"idempotencyKey\": \"unique-key-12345\",\n  \"method\": \"POST\",\n  \"path\": \"/api/v1/payments\",\n  \"responseStatus\": 201,\n  \"responseBody\": {...},\n  \"createdAt\": \"2025-10-25T10:00:00Z\",\n  \"expiresAt\": \"2025-10-26T10:00:00Z\"  # 24 hour expiration\n}\n```\n\n**Best Practices**:\n- Store idempotency keys for 24 hours\n- Return exact same response for duplicate requests\n- Use idempotency for all non-GET operations\n- Include idempotency key in error responses\n- Validate key format (UUID recommended)\n\n## Long-Running Operations\n\n### Async Operation Pattern\n\n**Initial Request**:\n```http\nPOST /api/v1/reports/generate\n\nRequest:\n{\n  \"type\": \"sales_report\",\n  \"startDate\": \"2025-01-01\",\n  \"endDate\": \"2025-10-25\"\n}\n\nResponse: 202 Accepted\nLocation: /api/v1/jobs/job_abc123\n{\n  \"jobId\": \"job_abc123\",\n  \"status\": \"pending\",\n  \"createdAt\": \"2025-10-25T10:00:00Z\",\n  \"estimatedDuration\": 300,\n  \"statusUrl\": \"/api/v1/jobs/job_abc123\"\n}\n```\n\n**Status Check**:\n```http\nGET /api/v1/jobs/job_abc123\n\n# While processing\nResponse: 200 OK\n{\n  \"jobId\": \"job_abc123\",\n  \"status\": \"processing\",\n  \"progress\": 45,\n  \"progressMessage\": \"Processing records 4500/10000\",\n  \"createdAt\": \"2025-10-25T10:00:00Z\",\n  \"estimatedCompletion\": \"2025-10-25T10:05:00Z\"\n}\n\n# When complete\nResponse: 200 OK\n{\n  \"jobId\": \"job_abc123\",\n  \"status\": \"completed\",\n  \"progress\": 100,\n  \"result\": {\n    \"reportUrl\": \"/api/v1/reports/report_xyz789\",\n    \"downloadUrl\": \"/api/v1/downloads/xyz789.pdf\"\n  },\n  \"completedAt\": \"2025-10-25T10:04:32Z\"\n}\n\n# If failed\nResponse: 200 OK\n{\n  \"jobId\": \"job_abc123\",\n  \"status\": \"failed\",\n  \"error\": {\n    \"code\": \"PROCESSING_ERROR\",\n    \"message\": \"Failed to generate report due to insufficient data\"\n  },\n  \"failedAt\": \"2025-10-25T10:03:15Z\"\n}\n```\n\n### Job Status States\n\n```\npending → processing → completed\n                    → failed\n                    → cancelled\n```\n\n**Status Definitions**:\n- `pending`: Job queued but not started\n- `processing`: Job actively running\n- `completed`: Job finished successfully\n- `failed`: Job encountered error\n- `cancelled`: Job cancelled by user\n\n## File Upload and Download\n\n### File Upload (Multipart)\n\n```http\nPOST /api/v1/files/upload\nContent-Type: multipart/form-data; boundary=----WebKitFormBoundary\n\n------WebKitFormBoundary\nContent-Disposition: form-data; name=\"file\"; filename=\"document.pdf\"\nContent-Type: application/pdf\n\n[Binary file data]\n------WebKitFormBoundary\nContent-Disposition: form-data; name=\"description\"\n\nMonthly report\n------WebKitFormBoundary--\n\nResponse: 201 Created\n{\n  \"id\": \"file_abc123\",\n  \"filename\": \"document.pdf\",\n  \"size\": 1048576,\n  \"mimeType\": \"application/pdf\",\n  \"url\": \"/api/v1/files/file_abc123\",\n  \"uploadedAt\": \"2025-10-25T10:00:00Z\"\n}\n```\n\n### Chunked Upload (Large Files)\n\n**Step 1: Initiate Upload**:\n```http\nPOST /api/v1/uploads/initiate\n\nRequest:\n{\n  \"filename\": \"large-video.mp4\",\n  \"size\": 524288000,\n  \"mimeType\": \"video/mp4\",\n  \"chunkSize\": 5242880  # 5MB chunks\n}\n\nResponse: 201 Created\n{\n  \"uploadId\": \"upload_xyz789\",\n  \"chunkSize\": 5242880,\n  \"totalChunks\": 100,\n  \"expiresAt\": \"2025-10-25T22:00:00Z\"\n}\n```\n\n**Step 2: Upload Chunks**:\n```http\nPUT /api/v1/uploads/upload_xyz789/chunks/1\nContent-Type: application/octet-stream\n\n[Binary chunk data]\n\nResponse: 200 OK\n{\n  \"uploadId\": \"upload_xyz789\",\n  \"chunkNumber\": 1,\n  \"uploaded\": true\n}\n```\n\n**Step 3: Complete Upload**:\n```http\nPOST /api/v1/uploads/upload_xyz789/complete\n\nResponse: 201 Created\n{\n  \"id\": \"file_abc123\",\n  \"filename\": \"large-video.mp4\",\n  \"size\": 524288000,\n  \"url\": \"/api/v1/files/file_abc123\"\n}\n```\n\n### File Download\n\n**Direct Download**:\n```http\nGET /api/v1/files/file_abc123/download\n\nResponse: 200 OK\nContent-Type: application/pdf\nContent-Disposition: attachment; filename=\"document.pdf\"\nContent-Length: 1048576\n\n[Binary file data]\n```\n\n**Presigned URL** (for direct S3/cloud storage access):\n```http\nPOST /api/v1/files/file_abc123/download-url\n\nResponse: 200 OK\n{\n  \"url\": \"https://s3.amazonaws.com/bucket/file?signature=...\",\n  \"expiresAt\": \"2025-10-25T11:00:00Z\",\n  \"expiresIn\": 3600\n}\n```\n\n## Search and Autocomplete\n\n### Full-Text Search\n\n```http\nGET /api/v1/posts/search?q=graphql api design&limit=20\n\nResponse: 200 OK\n{\n  \"query\": \"graphql api design\",\n  \"results\": [\n    {\n      \"id\": \"post_123\",\n      \"title\": \"GraphQL API Design Best Practices\",\n      \"excerpt\": \"Learn how to design scalable <mark>GraphQL</mark> <mark>APIs</mark>...\",\n      \"score\": 0.95,\n      \"highlights\": {\n        \"title\": [\"<mark>GraphQL</mark>\", \"<mark>API</mark>\", \"<mark>Design</mark>\"],\n        \"content\": [\"building <mark>GraphQL</mark> <mark>APIs</mark>\"]\n      }\n    }\n  ],\n  \"total\": 42,\n  \"took\": 23\n}\n```\n\n### Autocomplete\n\n```http\nGET /api/v1/search/autocomplete?q=grap&limit=5\n\nResponse: 200 OK\n{\n  \"query\": \"grap\",\n  \"suggestions\": [\n    {\n      \"text\": \"GraphQL\",\n      \"category\": \"topic\",\n      \"count\": 145\n    },\n    {\n      \"text\": \"Graph Database\",\n      \"category\": \"topic\",\n      \"count\": 89\n    },\n    {\n      \"text\": \"Grape (Ruby Framework)\",\n      \"category\": \"library\",\n      \"count\": 34\n    }\n  ]\n}\n```\n\n### Faceted Search\n\n```http\nGET /api/v1/products/search?q=laptop&facets=brand,price_range,rating\n\nResponse: 200 OK\n{\n  \"results\": [...],\n  \"facets\": {\n    \"brand\": [\n      { \"value\": \"Apple\", \"count\": 45 },\n      { \"value\": \"Dell\", \"count\": 78 },\n      { \"value\": \"HP\", \"count\": 62 }\n    ],\n    \"price_range\": [\n      { \"value\": \"0-500\", \"count\": 23 },\n      { \"value\": \"500-1000\", \"count\": 89 },\n      { \"value\": \"1000+\", \"count\": 73 }\n    ],\n    \"rating\": [\n      { \"value\": \"4+\", \"count\": 156 },\n      { \"value\": \"3+\", \"count\": 185 }\n    ]\n  }\n}\n```\n\n## Soft Deletes\n\n### Soft Delete Pattern\n\n```http\n# Soft delete (mark as deleted)\nDELETE /api/v1/posts/123\n\nResponse: 200 OK\n{\n  \"id\": \"post_123\",\n  \"title\": \"My Post\",\n  \"deleted\": true,\n  \"deletedAt\": \"2025-10-25T10:00:00Z\"\n}\n\n# Default behavior: exclude soft-deleted\nGET /api/v1/posts\n# Returns only non-deleted posts\n\n# Include soft-deleted (admin/owner only)\nGET /api/v1/posts?includeDeleted=true\n\n# Permanently delete (hard delete)\nDELETE /api/v1/posts/123?permanent=true\n\nResponse: 204 No Content\n```\n\n### Restore Deleted Items\n\n```http\nPOST /api/v1/posts/123/restore\n\nResponse: 200 OK\n{\n  \"id\": \"post_123\",\n  \"title\": \"My Post\",\n  \"deleted\": false,\n  \"deletedAt\": null,\n  \"restoredAt\": \"2025-10-25T10:30:00Z\"\n}\n```\n\n### Trash/Recycle Bin\n\n```http\n# List deleted items\nGET /api/v1/trash\n\nResponse: 200 OK\n{\n  \"items\": [\n    {\n      \"id\": \"post_123\",\n      \"type\": \"post\",\n      \"title\": \"My Post\",\n      \"deletedAt\": \"2025-10-25T10:00:00Z\",\n      \"permanentDeleteAt\": \"2025-11-24T10:00:00Z\"  # Auto-delete after 30 days\n    }\n  ]\n}\n\n# Empty trash\nDELETE /api/v1/trash\n\nResponse: 200 OK\n{\n  \"deletedCount\": 15\n}\n```\n",
        "icartsh-plugin/skills/api-designer/references/rest_best_practices.md": "# REST API Best Practices\n\nThis document provides comprehensive best practices for designing, implementing, and maintaining RESTful APIs.\n\n## Table of Contents\n\n- [URL Design](#url-design)\n- [HTTP Methods](#http-methods)\n- [Status Codes](#status-codes)\n- [Request/Response Formats](#requestresponse-formats)\n- [Error Handling](#error-handling)\n- [Authentication & Security](#authentication--security)\n- [Versioning](#versioning)\n- [Pagination](#pagination)\n- [Filtering & Sorting](#filtering--sorting)\n- [Rate Limiting](#rate-limiting)\n- [Caching](#caching)\n- [CORS](#cors)\n- [Documentation](#documentation)\n\n## URL Design\n\n### Resource Naming\n\n**Use Plural Nouns**:\n```\n✅ Good:\n/users\n/posts\n/comments\n\n❌ Bad:\n/user\n/post\n/comment\n```\n\n**Use Hyphens for Multi-Word Resources**:\n```\n✅ Good:\n/blog-posts\n/user-profiles\n/payment-methods\n\n❌ Bad:\n/blogPosts\n/blog_posts\n/userProfiles\n```\n\n**Keep URLs Lowercase**:\n```\n✅ Good:\n/users/123/posts\n/api/v1/products\n\n❌ Bad:\n/Users/123/Posts\n/API/V1/Products\n```\n\n### Resource Hierarchy\n\n**Limit Nesting Depth**:\n```\n✅ Good (2 levels):\n/users/123/posts\n/posts/456/comments\n\n❌ Bad (too deep):\n/users/123/posts/456/comments/789/likes\n```\n\n**Alternative to Deep Nesting**:\n```\nInstead of: /users/123/posts/456/comments\nUse: /comments?postId=456&userId=123\n```\n\n### Query Parameters vs Path Parameters\n\n**Path Parameters**: For resource identification\n```\n/users/{userId}\n/posts/{postId}\n```\n\n**Query Parameters**: For filtering, sorting, pagination\n```\n/posts?published=true&author=123&sort=-createdAt&limit=10\n```\n\n## HTTP Methods\n\n### GET - Retrieve Resources\n\n**Characteristics**:\n- Safe (no side effects)\n- Idempotent (multiple identical requests produce same result)\n- Cacheable\n- No request body\n\n**Examples**:\n```http\n# Get collection\nGET /posts\nResponse: 200 OK\n\n# Get specific resource\nGET /posts/123\nResponse: 200 OK\n\n# Get related resources\nGET /users/456/posts\nResponse: 200 OK\n```\n\n### POST - Create Resources\n\n**Characteristics**:\n- Not safe (has side effects)\n- Not idempotent\n- Creates new resource\n- Request body contains resource data\n\n**Examples**:\n```http\nPOST /posts\nContent-Type: application/json\n\n{\n  \"title\": \"My New Post\",\n  \"content\": \"Post content here\"\n}\n\nResponse: 201 Created\nLocation: /posts/789\n{\n  \"id\": \"789\",\n  \"title\": \"My New Post\",\n  \"content\": \"Post content here\",\n  \"createdAt\": \"2025-10-25T10:30:00Z\"\n}\n```\n\n**Best Practices**:\n- Return `201 Created` on success\n- Include `Location` header with new resource URL\n- Return created resource in response body\n- Generate IDs server-side\n\n### PUT - Replace Resource\n\n**Characteristics**:\n- Not safe\n- Idempotent\n- Replaces entire resource\n- Request body contains complete resource\n\n**Examples**:\n```http\nPUT /posts/123\nContent-Type: application/json\n\n{\n  \"title\": \"Updated Title\",\n  \"content\": \"Updated content\",\n  \"published\": true,\n  \"tags\": [\"api\", \"rest\"]\n}\n\nResponse: 200 OK\n{\n  \"id\": \"123\",\n  \"title\": \"Updated Title\",\n  \"content\": \"Updated content\",\n  \"published\": true,\n  \"tags\": [\"api\", \"rest\"],\n  \"updatedAt\": \"2025-10-25T10:35:00Z\"\n}\n```\n\n**PUT vs POST**:\n- PUT to `/posts/123` - Updates post with ID 123\n- PUT to `/posts` - Usually not allowed (would replace entire collection)\n\n### PATCH - Partial Update\n\n**Characteristics**:\n- Not safe\n- Typically idempotent\n- Updates specific fields only\n- Request body contains only fields to update\n\n**Examples**:\n```http\nPATCH /posts/123\nContent-Type: application/json\n\n{\n  \"published\": true\n}\n\nResponse: 200 OK\n{\n  \"id\": \"123\",\n  \"title\": \"Original Title\",\n  \"content\": \"Original content\",\n  \"published\": true,  // Only this changed\n  \"updatedAt\": \"2025-10-25T10:40:00Z\"\n}\n```\n\n**JSON Patch Format** (RFC 6902):\n```http\nPATCH /posts/123\nContent-Type: application/json-patch+json\n\n[\n  { \"op\": \"replace\", \"path\": \"/title\", \"value\": \"New Title\" },\n  { \"op\": \"add\", \"path\": \"/tags/-\", \"value\": \"tutorial\" },\n  { \"op\": \"remove\", \"path\": \"/draft\" }\n]\n```\n\n### DELETE - Remove Resource\n\n**Characteristics**:\n- Not safe\n- Idempotent\n- Removes resource\n- Typically no request body\n\n**Examples**:\n```http\nDELETE /posts/123\n\nResponse: 204 No Content\n# Or\nResponse: 200 OK\n{\n  \"message\": \"Post deleted successfully\",\n  \"deletedId\": \"123\"\n}\n```\n\n**Soft Delete Alternative**:\n```http\nPATCH /posts/123\n{\n  \"deleted\": true\n}\n```\n\n## Status Codes\n\n### Success Codes (2xx)\n\n**200 OK**:\n- Successful GET, PUT, PATCH\n- Request succeeded and response includes content\n\n**201 Created**:\n- Successful POST that creates resource\n- Include `Location` header\n- Return created resource\n\n**204 No Content**:\n- Successful DELETE\n- Successful operation with no response body\n- Successful PUT/PATCH when not returning updated resource\n\n**202 Accepted**:\n- Request accepted but processing not complete\n- Use for async operations\n- Provide way to check status\n\n### Client Error Codes (4xx)\n\n**400 Bad Request**:\n- Malformed request syntax\n- Invalid request data\n- Generic client error\n\n```json\n{\n  \"error\": {\n    \"code\": \"BAD_REQUEST\",\n    \"message\": \"Invalid request format\",\n    \"details\": [\"Missing required field: email\"]\n  }\n}\n```\n\n**401 Unauthorized**:\n- Missing or invalid authentication\n- Token expired\n\n```json\n{\n  \"error\": {\n    \"code\": \"UNAUTHORIZED\",\n    \"message\": \"Authentication required\"\n  }\n}\n```\n\n**403 Forbidden**:\n- Authenticated but not authorized\n- Insufficient permissions\n\n```json\n{\n  \"error\": {\n    \"code\": \"FORBIDDEN\",\n    \"message\": \"Insufficient permissions to access this resource\"\n  }\n}\n```\n\n**404 Not Found**:\n- Resource doesn't exist\n\n```json\n{\n  \"error\": {\n    \"code\": \"NOT_FOUND\",\n    \"message\": \"Post with ID 123 not found\"\n  }\n}\n```\n\n**409 Conflict**:\n- Resource conflict\n- Duplicate resource\n\n```json\n{\n  \"error\": {\n    \"code\": \"CONFLICT\",\n    \"message\": \"User with email already exists\"\n  }\n}\n```\n\n**422 Unprocessable Entity**:\n- Validation errors\n- Semantically incorrect request\n\n```json\n{\n  \"error\": {\n    \"code\": \"VALIDATION_ERROR\",\n    \"message\": \"Validation failed\",\n    \"details\": [\n      {\n        \"field\": \"email\",\n        \"message\": \"Invalid email format\"\n      },\n      {\n        \"field\": \"age\",\n        \"message\": \"Must be 18 or older\"\n      }\n    ]\n  }\n}\n```\n\n**429 Too Many Requests**:\n- Rate limit exceeded\n\n```json\n{\n  \"error\": {\n    \"code\": \"RATE_LIMIT_EXCEEDED\",\n    \"message\": \"Too many requests. Try again in 60 seconds.\"\n  }\n}\n```\n\n### Server Error Codes (5xx)\n\n**500 Internal Server Error**:\n- Generic server error\n- Unexpected error\n\n**502 Bad Gateway**:\n- Invalid response from upstream server\n\n**503 Service Unavailable**:\n- Temporary downtime\n- Maintenance mode\n- Include `Retry-After` header\n\n**504 Gateway Timeout**:\n- Upstream server timeout\n\n## Request/Response Formats\n\n### JSON Naming Conventions\n\n**Use camelCase**:\n```json\n{\n  \"firstName\": \"John\",\n  \"lastName\": \"Doe\",\n  \"emailAddress\": \"john@example.com\",\n  \"createdAt\": \"2025-10-25T10:00:00Z\"\n}\n```\n\n### Consistent Response Envelopes\n\n**Single Resource**:\n```json\n{\n  \"id\": \"123\",\n  \"title\": \"Post Title\",\n  \"author\": {\n    \"id\": \"456\",\n    \"name\": \"John Doe\"\n  },\n  \"createdAt\": \"2025-10-25T10:00:00Z\"\n}\n```\n\n**Collection**:\n```json\n{\n  \"data\": [\n    { \"id\": \"1\", \"title\": \"Post 1\" },\n    { \"id\": \"2\", \"title\": \"Post 2\" }\n  ],\n  \"pagination\": {\n    \"total\": 100,\n    \"limit\": 10,\n    \"offset\": 0,\n    \"hasMore\": true\n  }\n}\n```\n\n### Timestamps\n\n**Use ISO 8601 Format**:\n```json\n{\n  \"createdAt\": \"2025-10-25T10:30:00Z\",\n  \"updatedAt\": \"2025-10-25T14:45:30.123Z\"\n}\n```\n\n**Include Timezone**:\n- Always use UTC (Z suffix)\n- Or include explicit offset: `2025-10-25T10:30:00+05:30`\n\n### Null vs Omitted Fields\n\n**Be Consistent**:\n```json\n// Option 1: Include null fields\n{\n  \"name\": \"John\",\n  \"nickname\": null,\n  \"bio\": null\n}\n\n// Option 2: Omit null fields (recommended)\n{\n  \"name\": \"John\"\n}\n```\n\n## Error Handling\n\n### Consistent Error Format\n\n```json\n{\n  \"error\": {\n    \"code\": \"ERROR_CODE\",\n    \"message\": \"Human-readable error message\",\n    \"details\": [\n      {\n        \"field\": \"email\",\n        \"message\": \"Invalid format\"\n      }\n    ],\n    \"requestId\": \"req_abc123\",\n    \"timestamp\": \"2025-10-25T10:30:00Z\",\n    \"documentation\": \"https://docs.example.com/errors/ERROR_CODE\"\n  }\n}\n```\n\n### Error Codes\n\n**Use Consistent Naming**:\n```\nVALIDATION_ERROR\nNOT_FOUND\nUNAUTHORIZED\nFORBIDDEN\nRATE_LIMIT_EXCEEDED\nINTERNAL_ERROR\n```\n\n### Validation Errors\n\n```json\n{\n  \"error\": {\n    \"code\": \"VALIDATION_ERROR\",\n    \"message\": \"Request validation failed\",\n    \"details\": [\n      {\n        \"field\": \"email\",\n        \"code\": \"INVALID_FORMAT\",\n        \"message\": \"Must be a valid email address\"\n      },\n      {\n        \"field\": \"password\",\n        \"code\": \"TOO_SHORT\",\n        \"message\": \"Must be at least 8 characters\"\n      }\n    ]\n  }\n}\n```\n\n## Authentication & Security\n\n### OAuth 2.0 Flows\n\n**Authorization Code Flow** (Web apps):\n```\n1. GET /oauth/authorize?\n     client_id=CLIENT_ID&\n     redirect_uri=CALLBACK&\n     response_type=code&\n     scope=read write\n\n2. User authenticates\n\n3. Redirect: CALLBACK?code=AUTH_CODE\n\n4. POST /oauth/token\n   {\n     \"grant_type\": \"authorization_code\",\n     \"code\": \"AUTH_CODE\",\n     \"client_id\": \"CLIENT_ID\",\n     \"client_secret\": \"SECRET\"\n   }\n\n5. Response: { \"access_token\": \"...\", \"refresh_token\": \"...\" }\n```\n\n**Client Credentials Flow** (Service-to-service):\n```\nPOST /oauth/token\n{\n  \"grant_type\": \"client_credentials\",\n  \"client_id\": \"CLIENT_ID\",\n  \"client_secret\": \"CLIENT_SECRET\",\n  \"scope\": \"read write\"\n}\n```\n\n**PKCE Flow** (Mobile/SPA):\n```\n1. Generate code_verifier (random string)\n2. Generate code_challenge = SHA256(code_verifier)\n\n3. GET /oauth/authorize?\n     client_id=CLIENT_ID&\n     redirect_uri=CALLBACK&\n     response_type=code&\n     code_challenge=CHALLENGE&\n     code_challenge_method=S256\n\n4. POST /oauth/token\n   {\n     \"grant_type\": \"authorization_code\",\n     \"code\": \"AUTH_CODE\",\n     \"client_id\": \"CLIENT_ID\",\n     \"code_verifier\": \"VERIFIER\"\n   }\n```\n\n### JWT Tokens\n\n**Structure**:\n```\nHeader.Payload.Signature\n\neyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.\neyJzdWIiOiJ1c3JfMTIzIiwiaWF0IjoxNjk4MzM2MDAwfQ.\nsignature_here\n```\n\n**Payload Example**:\n```json\n{\n  \"sub\": \"usr_1234567890\",\n  \"iat\": 1698336000,\n  \"exp\": 1698339600,\n  \"scope\": [\"read:posts\", \"write:posts\"],\n  \"roles\": [\"user\", \"editor\"]\n}\n```\n\n**Usage**:\n```http\nAuthorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9...\n```\n\n### API Keys\n\n**Header-Based**:\n```http\nX-API-Key: sk_live_abc123def456\n```\n\n**Query Parameter** (avoid for sensitive operations):\n```\nGET /api/users?api_key=sk_live_abc123def456\n```\n\n**Best Practices**:\n- Different keys for different environments\n- Allow multiple keys per account\n- Implement key rotation\n- Add expiration dates\n- Log key usage\n\n### Security Headers\n\n```http\n# HTTPS only\nStrict-Transport-Security: max-age=31536000; includeSubDomains\n\n# XSS protection\nX-Content-Type-Options: nosniff\nX-Frame-Options: DENY\nContent-Security-Policy: default-src 'self'\n\n# CORS\nAccess-Control-Allow-Origin: https://app.example.com\nAccess-Control-Allow-Methods: GET, POST, PUT, DELETE\nAccess-Control-Allow-Headers: Authorization, Content-Type\n```\n\n## Versioning\n\n### URL Versioning\n\n```\nhttps://api.example.com/v1/users\nhttps://api.example.com/v2/users\n```\n\n**Pros**:\n- Very explicit\n- Easy to route\n- Simple to understand\n- Good for major changes\n\n**Cons**:\n- URL proliferation\n- Multiple codebases to maintain\n\n### Header Versioning\n\n```http\nAccept: application/vnd.myapi.v2+json\n# Or\nAPI-Version: 2\n```\n\n**Pros**:\n- Clean URLs\n- Same endpoint, different versions\n- Good for content negotiation\n\n**Cons**:\n- Less visible\n- Harder to test\n- Caching complexity\n\n### When to Version\n\n**Create new version for**:\n- Removing endpoints\n- Removing request/response fields\n- Changing field types\n- Breaking authentication changes\n- Changing error formats\n\n**Don't version for**:\n- Adding new optional fields\n- Adding new endpoints\n- Bug fixes\n- Performance improvements\n- Internal refactoring\n\n### Deprecation Process\n\n```http\n# Deprecation warning header\nSunset: Sat, 31 Dec 2025 23:59:59 GMT\nLink: <https://docs.example.com/migration>; rel=\"deprecation\"\n\n# In response\n{\n  \"data\": {...},\n  \"warnings\": [\n    {\n      \"code\": \"DEPRECATED\",\n      \"message\": \"This endpoint will be removed on 2025-12-31\",\n      \"migrationGuide\": \"https://docs.example.com/migration\"\n    }\n  ]\n}\n```\n\n## Pagination\n\n### Offset-Based Pagination\n\n```\nGET /posts?limit=20&offset=40\n\nResponse:\n{\n  \"data\": [...],\n  \"pagination\": {\n    \"total\": 500,\n    \"limit\": 20,\n    \"offset\": 40,\n    \"hasMore\": true\n  }\n}\n```\n\n**Pros**:\n- Simple to implement\n- Jump to any page\n- Show total count\n\n**Cons**:\n- Performance issues with large offsets\n- Inconsistent results if data changes\n- Not suitable for real-time data\n\n### Cursor-Based Pagination\n\n```\nGET /posts?limit=20&cursor=eyJpZCI6MTIzfQ\n\nResponse:\n{\n  \"data\": [...],\n  \"pagination\": {\n    \"nextCursor\": \"eyJpZCI6MTQzfQ\",\n    \"prevCursor\": \"eyJpZCI6MTAzfQ\",\n    \"hasMore\": true\n  }\n}\n```\n\n**Pros**:\n- Consistent results\n- Good performance\n- Works with real-time data\n- No duplicate results\n\n**Cons**:\n- Can't jump to specific page\n- No total count\n- More complex implementation\n\n### Link Header Pagination\n\n```http\nLink: <https://api.example.com/posts?cursor=abc>; rel=\"next\",\n      <https://api.example.com/posts?cursor=xyz>; rel=\"prev\",\n      <https://api.example.com/posts?cursor=first>; rel=\"first\",\n      <https://api.example.com/posts?cursor=last>; rel=\"last\"\n```\n\n## Rate Limiting\n\n### Implementation\n\n**Headers**:\n```http\nX-RateLimit-Limit: 100\nX-RateLimit-Remaining: 75\nX-RateLimit-Reset: 1698340800\nRetry-After: 60\n```\n\n**Response when exceeded**:\n```http\nHTTP/1.1 429 Too Many Requests\nX-RateLimit-Limit: 100\nX-RateLimit-Remaining: 0\nX-RateLimit-Reset: 1698340800\nRetry-After: 60\n\n{\n  \"error\": {\n    \"code\": \"RATE_LIMIT_EXCEEDED\",\n    \"message\": \"Rate limit exceeded. Try again in 60 seconds.\"\n  }\n}\n```\n\n### Strategies\n\n**Per User**:\n```\n100 requests per hour per user\n```\n\n**Per IP**:\n```\n1000 requests per hour per IP\n```\n\n**Per Endpoint**:\n```\n/auth/login: 5 requests per minute\n/posts: 100 requests per hour\n```\n\n**Tiered Limits**:\n```\nFree tier: 100 requests/hour\nPro tier: 1000 requests/hour\nEnterprise: 10000 requests/hour\n```\n\n## Caching\n\n### Cache Headers\n\n**ETags**:\n```http\n# Response\nETag: \"33a64df551425fcc55e4d42a148795d9f25f89d4\"\n\n# Subsequent request\nIf-None-Match: \"33a64df551425fcc55e4d42a148795d9f25f89d4\"\n\n# Response if not modified\nHTTP/1.1 304 Not Modified\n```\n\n**Last-Modified**:\n```http\n# Response\nLast-Modified: Wed, 25 Oct 2025 10:00:00 GMT\n\n# Subsequent request\nIf-Modified-Since: Wed, 25 Oct 2025 10:00:00 GMT\n\n# Response if not modified\nHTTP/1.1 304 Not Modified\n```\n\n**Cache-Control**:\n```http\n# Public, cacheable for 1 hour\nCache-Control: public, max-age=3600\n\n# Private, no caching\nCache-Control: private, no-cache, no-store, must-revalidate\n\n# Conditional caching\nCache-Control: public, max-age=3600, must-revalidate\n```\n\n## Documentation\n\n### OpenAPI/Swagger\n\nProvide interactive API documentation with:\n- All endpoints documented\n- Request/response examples\n- Authentication instructions\n- Error code documentation\n- Try-it-out functionality\n\n### Additional Documentation\n\n**Getting Started Guide**:\n- Quick start tutorial\n- Authentication setup\n- First API call example\n- Common use cases\n\n**API Reference**:\n- Complete endpoint documentation\n- Parameter descriptions\n- Response formats\n- Error codes\n\n**Best Practices Guide**:\n- Recommended patterns\n- Performance optimization\n- Error handling\n- Security considerations\n\n**Changelog**:\n- Version history\n- Breaking changes\n- New features\n- Deprecations\n\n**Migration Guides**:\n- Version upgrade instructions\n- Breaking change details\n- Code examples\n- Timeline for deprecations\n",
        "icartsh-plugin/skills/api-designer/references/versioning-strategies.md": "# API Versioning Strategies\n\nComprehensive guide to versioning APIs, managing breaking changes, and deprecating old versions.\n\n## Table of Contents\n\n- [Versioning Approaches](#versioning-approaches)\n- [When to Version](#when-to-version)\n- [Deprecation Process](#deprecation-process)\n- [Migration Strategies](#migration-strategies)\n- [Best Practices](#best-practices)\n\n## Versioning Approaches\n\n### URL Versioning\n\n**Pattern**: Include version in URL path\n\n```\nhttps://api.example.com/v1/users\nhttps://api.example.com/v2/users\nhttps://api.example.com/v3/users\n```\n\n**Pros**:\n- Very explicit and visible\n- Easy to route to different codebases\n- Simple to understand for developers\n- Good for major breaking changes\n- Easy to cache (different URLs)\n- Browser-testable\n\n**Cons**:\n- URL proliferation\n- Requires maintaining multiple codebases\n- Can lead to code duplication\n- Makes it harder to sunset old versions\n\n**When to Use**:\n- Major version changes with significant breaking changes\n- When you need to maintain multiple versions long-term\n- Public APIs consumed by many clients\n- When you want maximum clarity\n\n**Implementation Example**:\n```python\n# Flask example\nfrom flask import Flask\n\napp = Flask(__name__)\n\n# Version 1\n@app.route('/v1/users')\ndef get_users_v1():\n    return {\"users\": [...], \"version\": \"1.0\"}\n\n# Version 2\n@app.route('/v2/users')\ndef get_users_v2():\n    return {\"data\": {\"users\": [...]}, \"version\": \"2.0\"}\n```\n\n### Header Versioning\n\n**Pattern**: Include version in request header\n\n```http\nGET /users\nAccept: application/vnd.myapi.v2+json\n\n# Or\nGET /users\nAPI-Version: 2\n\n# Or\nGET /users\nAccept-Version: 2.0\n```\n\n**Pros**:\n- Clean URLs (no version pollution)\n- Same endpoint for all versions\n- Good for content negotiation\n- Follows HTTP standards (Accept header)\n- Flexible versioning per resource\n\n**Cons**:\n- Less visible (harder to discover)\n- Harder to test in browser\n- Can complicate caching\n- More complex routing logic\n- May confuse some developers\n\n**When to Use**:\n- When you want clean URLs\n- APIs with frequent minor updates\n- Content negotiation is important\n- Internal or partner APIs\n\n**Implementation Example**:\n```python\nfrom flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/users')\ndef get_users():\n    version = request.headers.get('API-Version', '1')\n\n    if version == '2':\n        return {\"data\": {\"users\": [...]}, \"version\": \"2.0\"}\n    else:\n        return {\"users\": [...], \"version\": \"1.0\"}\n```\n\n### Query Parameter Versioning\n\n**Pattern**: Include version in query string\n\n```\nGET /users?version=2\nGET /users?api-version=2.0\nGET /users?v=2\n```\n\n**Pros**:\n- Simple to implement\n- Easy to test\n- Optional (can have default version)\n- Works well with existing infrastructure\n\n**Cons**:\n- Not RESTful\n- Can be accidentally omitted\n- Harder to enforce\n- Caching complications\n- Query params should be for filtering, not versioning\n\n**When to Use**:\n- Quick prototypes or internal tools\n- When you need easy testing\n- Temporary versioning before better solution\n\n**Not Recommended**: Generally avoid this approach for production APIs\n\n### Media Type Versioning (Content Negotiation)\n\n**Pattern**: Use custom media types with version\n\n```http\nGET /users\nAccept: application/vnd.myapi.user.v2+json\n\n# Or more specific\nAccept: application/vnd.myapi.user.v2.full+json\n```\n\n**Pros**:\n- Follows REST principles\n- Fine-grained versioning per resource\n- Supports different representations\n- Standard HTTP content negotiation\n\n**Cons**:\n- Complex to implement\n- Harder for clients to use\n- Less discoverable\n- Requires good documentation\n\n**When to Use**:\n- Mature, well-designed APIs\n- When you need resource-level versioning\n- APIs with multiple representation formats\n\n### Semantic Versioning\n\n**Format**: MAJOR.MINOR.PATCH (e.g., 2.1.3)\n\n```\nGET /v2.1/users  # Less common\n# Or in header\nAPI-Version: 2.1.3\n```\n\n**Version Components**:\n- **MAJOR**: Breaking changes (v1 → v2)\n- **MINOR**: New features, backward compatible (v2.1 → v2.2)\n- **PATCH**: Bug fixes, backward compatible (v2.1.1 → v2.1.2)\n\n**Best Practices**:\n- Only include MAJOR version in URL/header\n- Track MINOR/PATCH in response headers\n- Communicate MINOR/PATCH in API documentation\n\n```http\nResponse Headers:\nAPI-Version: 2\nAPI-Version-Full: 2.1.3\n```\n\n## When to Version\n\n### Create New Version For\n\n**Breaking Changes**:\n- Removing endpoints\n- Removing request/response fields\n- Changing field data types\n- Renaming fields\n- Changing authentication methods\n- Modifying error response structure\n- Changing HTTP status codes\n- Altering request/response semantics\n\n**Examples**:\n```json\n// V1\n{\n  \"user_id\": \"123\",  // Field name changed\n  \"email\": \"test@example.com\"\n}\n\n// V2\n{\n  \"id\": \"123\",       // Breaking: renamed field\n  \"email\": \"test@example.com\"\n}\n```\n\n```json\n// V1\n{\n  \"created\": \"2025-10-25\"  // String\n}\n\n// V2\n{\n  \"created\": 1698336000    // Breaking: changed to Unix timestamp\n}\n```\n\n### Don't Version For\n\n**Backward Compatible Changes**:\n- Adding new optional fields to responses\n- Adding new endpoints\n- Adding new optional query parameters\n- Bug fixes\n- Performance improvements\n- Internal refactoring\n- Documentation updates\n- Adding new optional request fields\n\n**Examples**:\n```json\n// V1\n{\n  \"id\": \"123\",\n  \"email\": \"test@example.com\"\n}\n\n// V1 (backward compatible update)\n{\n  \"id\": \"123\",\n  \"email\": \"test@example.com\",\n  \"username\": \"testuser\"  // New field - clients can ignore\n}\n```\n\n### Version Change Checklist\n\nBefore creating a new version, ask:\n\n- [ ] Is this change absolutely necessary?\n- [ ] Can we make it backward compatible?\n- [ ] Have we exhausted all backward-compatible options?\n- [ ] Is the benefit worth the maintenance cost?\n- [ ] Have we documented the migration path?\n- [ ] Have we planned the deprecation timeline?\n\n## Deprecation Process\n\n### Deprecation Timeline\n\n**Recommended Process**:\n\n```\nMonth 0: Announce deprecation\n  ↓\nMonth 1-3: Deprecation warnings in responses\n  ↓\nMonth 3-6: Migration support and documentation\n  ↓\nMonth 6: Final warning (30 days to sunset)\n  ↓\nMonth 7: Sunset (remove old version)\n```\n\n**Adjust timeline based on**:\n- API usage and customer base\n- Severity of changes\n- Available alternatives\n- Industry standards (some require 12+ months)\n\n### Deprecation Headers\n\n```http\n# Standard headers\nSunset: Sat, 31 Dec 2025 23:59:59 GMT\nDeprecation: true\nLink: <https://docs.example.com/migration/v1-to-v2>; rel=\"deprecation\"\n\n# Custom headers for more detail\nX-API-Deprecated: true\nX-API-Sunset-Date: 2025-12-31\nX-API-Migration-Guide: https://docs.example.com/migration/v1-to-v2\n```\n\n### Deprecation Warnings in Response\n\n```json\n{\n  \"data\": {\n    \"users\": [...]\n  },\n  \"warnings\": [\n    {\n      \"code\": \"DEPRECATED_VERSION\",\n      \"message\": \"API v1 is deprecated and will be removed on 2025-12-31\",\n      \"severity\": \"warning\",\n      \"migrationGuide\": \"https://docs.example.com/migration/v1-to-v2\",\n      \"sunsetDate\": \"2025-12-31T23:59:59Z\"\n    }\n  ]\n}\n```\n\n### Deprecation Announcement Template\n\n**Email/Blog Post**:\n```\nSubject: Important: API v1 Deprecation Notice\n\nDear API Users,\n\nWe are deprecating API v1 and introducing v2 with the following improvements:\n- [List key improvements]\n- [List new features]\n\nTimeline:\n- Today: v2 is now available\n- [Date + 3 months]: Deprecation warnings added to v1 responses\n- [Date + 6 months]: Final 30-day warning\n- [Date + 7 months]: v1 will be shut down\n\nMigration:\n- Migration guide: [URL]\n- API documentation: [URL]\n- Support: [Contact info]\n\nWhat you need to do:\n1. Review the migration guide\n2. Update your integration to use v2\n3. Test thoroughly in our sandbox environment\n4. Deploy to production before [sunset date]\n\nWe're here to help with the migration. Please reach out if you have questions.\n\nBest regards,\nAPI Team\n```\n\n## Migration Strategies\n\n### Parallel Running\n\n**Strategy**: Run both versions simultaneously\n\n```\nv1: https://api.example.com/v1/users (deprecated)\nv2: https://api.example.com/v2/users (current)\n```\n\n**Timeline**:\n```\nDeploy v2 → Monitor adoption → Deprecate v1 → Sunset v1\n```\n\n**Monitoring**:\n```json\n{\n  \"metrics\": {\n    \"v1\": {\n      \"requests\": 10000,\n      \"percentOfTotal\": 15,\n      \"uniqueClients\": 23\n    },\n    \"v2\": {\n      \"requests\": 57000,\n      \"percentOfTotal\": 85,\n      \"uniqueClients\": 189\n    }\n  }\n}\n```\n\n### Feature Flags for Gradual Rollout\n\n```python\ndef get_users():\n    # Check feature flag\n    if user.has_feature('api_v2'):\n        return v2_response()\n    else:\n        return v1_response()\n```\n\n**Benefits**:\n- Gradual rollout to percentage of users\n- Easy rollback if issues found\n- A/B testing capabilities\n- Per-user or per-account enabling\n\n### Adapter Pattern\n\n**Strategy**: Maintain one codebase, transform responses\n\n```python\ndef get_users():\n    # Core business logic\n    users = fetch_users_from_db()\n\n    # Version-specific transformation\n    version = get_api_version()\n    if version == 1:\n        return transform_to_v1(users)\n    elif version == 2:\n        return transform_to_v2(users)\n```\n\n**Pros**:\n- Single source of truth\n- Less code duplication\n- Easier to maintain business logic\n\n**Cons**:\n- Transformation overhead\n- Can become complex with many versions\n\n### Proxy/Gateway Pattern\n\n**Strategy**: Use API gateway to route and transform\n\n```\nClient → API Gateway → Version Detection → Route to v1 or v2\n                    → Transform response if needed\n```\n\n**Benefits**:\n- Centralized version management\n- Can translate between versions\n- Easier to monitor and control\n\n### Database Versioning\n\n**Challenge**: Breaking database schema changes\n\n**Strategies**:\n\n1. **Expand-Contract Pattern**:\n```sql\n-- Phase 1: Expand (add new column, keep old)\nALTER TABLE users ADD COLUMN email_address VARCHAR(255);\n\n-- Phase 2: Dual writes (write to both columns)\nUPDATE users SET email_address = email;\n\n-- Phase 3: Migrate clients to new field\n\n-- Phase 4: Contract (remove old column)\nALTER TABLE users DROP COLUMN email;\n```\n\n2. **Database Views**:\n```sql\n-- v1 view\nCREATE VIEW users_v1 AS\nSELECT user_id as id, email FROM users;\n\n-- v2 view\nCREATE VIEW users_v2 AS\nSELECT id, email_address as email FROM users;\n```\n\n## Best Practices\n\n### Version Numbering\n\n**Recommendations**:\n- Start with v1 (not v0)\n- Use integers for major versions (v1, v2, v3)\n- Increment thoughtfully (v1 → v2 is significant)\n- Consider semantic versioning internally\n- Document what each version includes\n\n**Avoid**:\n- Dates in versions (v2025, v20251025)\n- Too many major versions (v1 → v2 → v3 in 6 months)\n- Fractional versions in URL (v1.5)\n\n### Version Communication\n\n**Document Clearly**:\n- Current version\n- Supported versions\n- Deprecated versions with sunset dates\n- Changelog for each version\n- Migration guides\n\n**Example Versions Page**:\n```markdown\n# API Versions\n\n## Current Versions\n\n- **v2** (Current) - Released 2025-01-15\n  - Latest features and improvements\n  - Recommended for all new integrations\n\n- **v1** (Deprecated) - Sunset: 2025-12-31\n  - Legacy version, please migrate to v2\n  - Migration guide: [link]\n\n## Changelog\n\n### v2.0.0 (2025-01-15)\n- Breaking: Changed user ID format from integer to string\n- Breaking: Removed deprecated /users/search endpoint\n- Added: New filtering capabilities on /users endpoint\n- Improved: Response time by 40%\n\n### v1.2.0 (2024-10-01)\n- Added: New optional 'include' parameter for related resources\n- Fixed: Pagination cursor encoding issue\n```\n\n### Support Policy\n\n**Define Clear Policies**:\n```markdown\n## Version Support Policy\n\n- **Current Version**: Full support, active development\n- **Previous Version**: Security updates only, 12 months after new version release\n- **Deprecated Versions**: No support, will be sunset after notice period\n\n## Deprecation Notice Period\n\n- Minor changes: 3 months minimum\n- Major changes: 6 months minimum\n- Critical breaking changes: 12 months minimum\n```\n\n### Monitoring and Metrics\n\n**Track**:\n- Requests per version\n- Unique clients per version\n- Error rates per version\n- Response times per version\n- Adoption rate of new version\n\n**Set Alerts**:\n- Spike in old version usage (regression?)\n- Low adoption of new version\n- Increase in errors after version release\n\n### Testing Strategy\n\n**Test Matrix**:\n```\nTest Suite:\n├── v1 Integration Tests\n│   ├── All v1 endpoints\n│   └── Backward compatibility\n├── v2 Integration Tests\n│   ├── All v2 endpoints\n│   └── New features\n└── Cross-Version Tests\n    ├── Data consistency\n    └── Migration scenarios\n```\n\n### Client SDK Versioning\n\n**Align SDK versions with API versions**:\n\n```python\n# SDK versioning\npip install myapi-client==1.0.0  # For API v1\npip install myapi-client==2.0.0  # For API v2\n\n# Or support multiple versions in one SDK\nfrom myapi_client.v1 import Client as ClientV1\nfrom myapi_client.v2 import Client as ClientV2\n```\n\n### Avoid Version Hell\n\n**Don't**:\n- Support too many versions simultaneously (3+ active versions)\n- Make breaking changes too frequently\n- Skip version numbers arbitrarily\n- Use confusing version schemes\n\n**Do**:\n- Plan major versions carefully\n- Extend deprecation periods when usage is high\n- Provide excellent migration documentation\n- Offer migration assistance for large customers\n- Consider backward compatibility first\n\n### Emergency Breaking Changes\n\n**If you absolutely must make emergency breaking changes**:\n\n1. Communicate immediately\n2. Provide very short migration window\n3. Offer direct support\n4. Document thoroughly\n5. Learn and prevent future occurrences\n\n```markdown\n# Emergency Change Notice\n\n⚠️ CRITICAL SECURITY UPDATE\n\nWe discovered a security vulnerability that requires an immediate breaking change.\n\n**What's changing**: [Specific change]\n**Why**: [Security reason]\n**Timeline**:\n  - Today: Change deployed, v1.x deprecated\n  - 7 days: v1.x will be disabled\n\n**Action required**: [Migration steps]\n**Support**: [Emergency contact]\n```\n",
        "icartsh-plugin/skills/brainstorming/SKILL.md": "---\nname: brainstorming\ndescription: 코드나 구현 계획을 작성하기 전, 무언가를 생성하거나 개발할 때 사용합니다. 협력적인 질문, 대안 탐색 및 점진적 검증을 통해 거친 아이디어를 구체화된 설계로 발전시킵니다. 명확한 'Mechanical' 프로세스 중에는 사용하지 마십시오.\n---\n\n# 아이디어를 설계로 브레인스토밍하기 (Brainstorming Ideas Into Designs)\n\n## Overview\n\n자연스러운 협업 대화를 통해 아이디어를 구체화된 설계와 사양(Spec)으로 바꾸도록 돕습니다.\n\n먼저 현재 프로젝트 context를 이해한 다음, 아이디어를 다듬기 위해 질문을 하나씩 던집니다. 무엇을 구축하려는지 이해했다면 설계를 작은 섹션(200-300자)으로 나누어 제시하고, 각 섹션이 끝날 때마다 지금까지의 내용이 맞는지 확인합니다.\n\n## The Process\n\n**아이디어 이해하기(Understanding the idea):**\n- 먼저 현재 프로젝트 상태를 확인합니다(파일, 문서, 최근 커밋).\n- 아이디어를 다듬기 위해 질문을 한 번에 하나씩 합니다.\n- 가능한 경우 객관식 질문을 선호하지만, 주관식 질문도 괜찮습니다.\n- 메시지 당 하나의 질문만 합니다. 한 주제에 더 많은 탐색이 필요하면 여러 질문으로 나눕니다.\n- 이해에 집중합니다: 목적, 제약 조건, 성공 기준.\n\n**접근 방식 탐색하기(Exploring approaches):**\n- 트레이드오프가 있는 2-3가지의 서로 다른 접근 방식을 제안합니다.\n- 권장 사항과 그 이유를 포함하여 대화 형식으로 옵션을 제시합니다.\n- 권장하는 옵션을 먼저 제시하고 그 이유를 설명합니다.\n\n**설계 제시하기(Presenting the design):**\n- 무엇을 구축할지 이해했다고 판단되면 설계를 제시합니다.\n- 설계를 200-300자 정도의 섹션으로 나눕니다.\n- 각 섹션이 끝날 때마다 지금까지의 내용이 맞는지 확인합니다.\n- 내용: 아키텍처, 컴포넌트, 데이터 흐름, 에러 핸들링, 테스트.\n- 이해가 되지 않는 부분이 있다면 다시 돌아가 명확하게 설명할 준비를 합니다.\n\n## After the Design\n\n**문서화(Documentation):**\n- 검증된 설계를 `docs/plans/YYYY-MM-DD-<topic>-design.md`에 작성합니다.\n- 사용 가능한 경우 `elements-of-style:writing-clearly-and-concisely` SKILL을 사용합니다.\n- 설계 문서를 git에 커밋합니다.\n\n**구현(Implementation - 계속 진행하는 경우):**\n- \"구현을 위한 설정을 시작할까요?\"라고 물어봅니다.\n- `superpowers:using-git-worktrees`를 사용하여 격리된 workspace를 생성합니다.\n- `superpowers:writing-plans`를 사용하여 상세한 구현 계획을 작성합니다.\n\n## Key Principles\n\n- **질문은 한 번에 하나씩 (One question at a time)** - 여러 질문으로 부담을 주지 마십시오.\n- **객관식 선호 (Multiple choice preferred)** - 가능하면 주관식보다 대답하기 쉽습니다.\n- **철저한 YAGNI (YAGNI ruthlessly)** - 모든 설계에서 불필요한 기능은 제거합니다.\n- **대안 탐색 (Explore alternatives)** - 결정하기 전에 항상 2-3가지 접근 방식을 제안합니다.\n- **점진적 검증 (Incremental validation)** - 설계를 섹션별로 제시하고 각 섹션을 검증합니다.\n- **유연성 유지 (Be flexible)** - 이해가 되지 않을 때는 다시 돌아가서 명확히 합니다.\n",
        "icartsh-plugin/skills/code-analyze/SKILL.md": "---\nname: code-analyze\nversion: 0.1.0\nkind: cli\ndescription: .NET 코드에서 정적 분석(Static analysis), 보안 스캔(Security scan) 및 종속성 체크(Dependency check)를 수행합니다. 코드 품질, 보안 감사 또는 취약점 탐지가 포함된 작업에서 사용합니다.\ninputs:\n  analysis_type: [static, security, dependencies, all]\n  project_path: string\n  severity_filter: [error, warning, suggestion, all]\ncontracts:\n  success: '분석이 결과 보고서와 함께 완료됨; exit code 0'\n  failure: 'Non-zero exit code 또는 도구 실행 에러'\n---\n\n# Code Analysis Skill (Entry Map)\n\n> **Goal:** 에이전트가 필요한 분석 절차를 정확하게 찾을 수 있도록 가이드합니다.\n\n## Quick Start (하나를 선택하세요)\n\n- **Static code analysis 실행** → `references/static-analysis.md`\n- **보안 이슈 스캔 (Scan for security issues)** → `references/security-scan.md`\n- **종속성 취약점 체크 (Check dependency vulnerabilities)** → `references/dependency-check.md`\n\n## When to Use\n\n- 코드 품질 표준 및 모범 사례 시행\n- 잠재적인 버그 및 code smell 탐지\n- 코드 내 보안 취약점 식별\n- 취약한 종속성(Dependency) 확인\n- 자동화된 코드 리뷰 실행\n\n**다음을 위한 것이 아님:** 빌드 (dotnet-build), 테스트 (dotnet-test), 또는 포맷팅 (code-format)\n\n## Inputs & Outputs\n\n**Inputs:** `analysis_type` (static/security/dependencies/all), `project_path` (default: ./dotnet/PigeonPea.sln), `severity_filter` (error/warning/suggestion)\n\n**Outputs:** `analysis_report` (파일/라인이 포함된 결과), `exit_code` (0=clean, 1=issues), `metrics` (심각도별 위반 사항)\n\n**Guardrails:** 분석만 수행하며 코드를 절대 수정하지 마십시오. 모든 결과를 컨텍스트와 함께 보고하고 심각한 이슈 발생 시 실패로 처리합니다.\n\n## Navigation\n\n**1. Static Code Analysis** → [`references/static-analysis.md`](references/static-analysis.md)\n\n- Roslyn analyzers, StyleCop, 코드 품질 규칙, 모범 사례\n\n**2. Security Scanning** → [`references/security-scan.md`](references/security-scan.md)\n\n- Secret 탐지 (gitleaks, detect-secrets), 보안 analyzers, 취약점 패턴\n\n**3. Dependency Vulnerability Check** → [`references/dependency-check.md`](references/dependency-check.md)\n\n- NuGet 패키지 취약점, 오래된 종속성, CVE 탐지\n\n## Common Patterns\n\n### Quick Analysis (모든 체크 수행)\n\n```bash\ncd ./dotnet\ndotnet build PigeonPea.sln /p:TreatWarningsAsErrors=true\ndotnet list package --vulnerable\n```\n\n### Static Analysis 전용\n\n```bash\ncd ./dotnet\ndotnet build PigeonPea.sln /p:RunAnalyzers=true /warnaserror\n```\n\n### Security Scan (커밋 전)\n\n```bash\npre-commit run gitleaks --all-files\npre-commit run detect-secrets --all-files\n```\n\n### Dependency Check\n\n```bash\ncd ./dotnet\ndotnet list package --vulnerable --include-transitive\ndotnet list package --outdated\n```\n\n### Full Analysis Suite\n\n```bash\n# 저장소 루트에서 실행\n.agent/skills/code-analyze/scripts/analyze.sh --all\n```\n\n### 특정 심각도(Severity)를 포함한 분석\n\n```bash\ncd ./dotnet\n# 에러 전용\ndotnet build PigeonPea.sln /p:TreatWarningsAsErrors=false\n\n# 경고를 에러로 처리\ndotnet build PigeonPea.sln /p:TreatWarningsAsErrors=true\n```\n\n## Troubleshooting\n\n**Analyzer를 찾을 수 없음:** Roslyn analyzers가 활성화되어 있는지 확인하십시오. `references/static-analysis.md`를 참조하세요.\n\n**경고가 너무 많음:** Severity별로 필터링하거나 suppression을 추가하십시오. `references/static-analysis.md`를 참조하세요.\n\n**False positives:** `.editorconfig` 또는 suppression을 사용하십시오. `references/static-analysis.md`를 참조하세요.\n\n**Secret이 탐지되지 않음:** `.gitleaksignore` 및 `.secrets.baseline`을 확인하십시오. `references/security-scan.md`를 참조하세요.\n\n**Dependency scan 실패:** 네트워크 문제 또는 패키지 복원(Restore)이 필요할 수 있습니다. `references/dependency-check.md`를 참조하세요.\n\n## Success Indicators\n\n**Static Analysis:**\n\n```\nBuild succeeded.\n    0 Warning(s)\n    0 Error(s)\n```\n\n**Security Scan:**\n\n```\ngitleaks................Passed\ndetect-secrets...........Passed\n```\n\n**Dependency Check:**\n\n```\nNo vulnerable packages found.\n```\n\n## Integration\n\n**커밋 전:** 보안 스캔(gitleaks, detect-secrets) 실행\n**빌드 후:** 정적 분석(Roslyn, StyleCop) 실행\n**정기 체크:** 종속성 취약점 체크 실행\n\n**CI/CD Integration:** 빌드 파이프라인에 모든 분석을 포함하고 심각한 이슈 발생 시 실패 처리\n\n## Related\n\n- [`./dotnet/ARCHITECTURE.md`](../../../dotnet/ARCHITECTURE.md) - 프로젝트 구조\n- [`.pre-commit-config.yaml`](../../../.pre-commit-config.yaml) - Pre-commit hooks\n- [`.editorconfig`](../../../.editorconfig) - 코드 스타일 규칙\n- [`dotnet-build`](../dotnet-build/SKILL.md) - 빌드 SKILL\n",
        "icartsh-plugin/skills/code-analyze/references/dependency-check.md": "# Dependency Vulnerability Check - Detailed Procedure\n\n## Overview\n\nThis guide provides step-by-step instructions for checking NuGet package dependencies for known vulnerabilities and security issues in the PigeonPea .NET solution.\n\n## Prerequisites\n\n- **.NET SDK 9.0** or later (check: `dotnet --version`)\n- Solution file: `./dotnet/PigeonPea.sln`\n- Internet connection (to query vulnerability databases)\n- Dependencies restored: `dotnet restore PigeonPea.sln`\n\n## Dependency Vulnerability Tools\n\n### 1. dotnet list package --vulnerable\n\nBuilt-in .NET CLI command that checks NuGet packages against known vulnerability databases.\n\n**Data Sources:**\n\n- GitHub Advisory Database\n- National Vulnerability Database (NVD)\n- NuGet Advisory Database\n\n**What it detects:**\n\n- Known CVEs (Common Vulnerabilities and Exposures)\n- Security advisories for NuGet packages\n- Vulnerable transitive dependencies\n\n### 2. dotnet list package --outdated\n\nChecks for newer versions of installed packages (not security-specific but useful for maintenance).\n\n## Standard Dependency Check Flow\n\n### Step 1: Navigate to .NET Directory\n\n```bash\ncd ./dotnet\n```\n\nAll dependency commands should be run from the `./dotnet` directory.\n\n### Step 2: Check for Vulnerable Packages\n\n```bash\ndotnet list package --vulnerable\n```\n\nThis scans all projects in the solution for vulnerable packages.\n\n### Step 3: Include Transitive Dependencies\n\n```bash\ndotnet list package --vulnerable --include-transitive\n```\n\nThis includes indirect dependencies that might have vulnerabilities.\n\n### Step 4: Check Specific Project\n\n```bash\ndotnet list console-app/PigeonPea.Console.csproj package --vulnerable --include-transitive\n```\n\n### Step 5: Check for Outdated Packages\n\n```bash\ndotnet list package --outdated\n```\n\nThis shows available updates (not just security fixes).\n\n## Command Options\n\n```bash\n# Usage: dotnet list [PROJECT|SOLUTION] package [options]\n\n# Vulnerability scanning\n--vulnerable                            # Show only vulnerable packages\n--include-transitive                    # Include transitive (indirect) dependencies\n\n# Version checking\n--outdated                             # Show packages with newer versions\n--highest-patch                        # Show highest patch version\n--highest-minor                        # Show highest minor version\n\n# Formatting\n--format <console|json>                # Output format\n--output <file>                        # Write output to file\n\n# Filtering\n--include-prerelease                   # Include pre-release versions\n--framework <tfm>                      # Filter by target framework\n--source <source>                      # NuGet source to check\n```\n\n## Understanding Vulnerability Output\n\n### Clean Report (No Vulnerabilities)\n\n```bash\n$ dotnet list package --vulnerable --include-transitive\n\nProject 'PigeonPea.Console' has no vulnerable packages.\nProject 'PigeonPea.Shared' has no vulnerable packages.\nProject 'PigeonPea.Windows' has no vulnerable packages.\n```\n\n✅ No known vulnerabilities in dependencies.\n\n### Vulnerable Package Detected\n\n```bash\n$ dotnet list package --vulnerable --include-transitive\n\nThe following sources were used:\n   https://api.nuget.org/v3/index.json\n\nProject `PigeonPea.Console` has the following vulnerable packages\n   [net9.0]:\n   Top-level Package         Requested   Resolved   Severity   Advisory URL\n   > Newtonsoft.Json         12.0.1      12.0.1     High       https://github.com/advisories/GHSA-5crp-9r3c-p9vr\n\n   Transitive Package        Resolved   Severity   Advisory URL\n   > System.Text.Json        6.0.0      Critical   https://github.com/advisories/GHSA-8g4q-xg66-9fp4\n```\n\n❌ **Action Required:** Update vulnerable packages.\n\n### Vulnerability Details\n\n**Fields:**\n\n- **Package**: Name of vulnerable package\n- **Requested**: Version specified in `.csproj`\n- **Resolved**: Version actually used\n- **Severity**: Low, Moderate, High, Critical\n- **Advisory URL**: Link to detailed vulnerability information\n\n## Vulnerability Severity Levels\n\n1. **Critical**: Immediate action required, exploitable with severe impact\n2. **High**: Urgent action required, significant security risk\n3. **Moderate**: Schedule fix soon, moderate security risk\n4. **Low**: Address in next maintenance cycle, minor risk\n\n## Fixing Vulnerable Dependencies\n\n### Method 1: Update Package (Direct Dependency)\n\nFor packages directly referenced in `.csproj`:\n\n```bash\n# Update to latest version\ndotnet add console-app/PigeonPea.Console.csproj package Newtonsoft.Json\n\n# Update to specific version\ndotnet add console-app/PigeonPea.Console.csproj package Newtonsoft.Json --version 13.0.3\n```\n\nOr edit `.csproj` directly:\n\n```xml\n<ItemGroup>\n  <!-- Before: Vulnerable -->\n  <PackageReference Include=\"Newtonsoft.Json\" Version=\"12.0.1\" />\n\n  <!-- After: Fixed -->\n  <PackageReference Include=\"Newtonsoft.Json\" Version=\"13.0.3\" />\n</ItemGroup>\n```\n\n### Method 2: Update Transitive Dependency\n\nFor indirect (transitive) dependencies:\n\n**Option A:** Update parent package that depends on it\n\n```bash\ndotnet add package ParentPackage --version <newer-version>\n```\n\n**Option B:** Add explicit reference to fixed version\n\n```xml\n<ItemGroup>\n  <!-- Force specific version of transitive dependency -->\n  <PackageReference Include=\"System.Text.Json\" Version=\"8.0.0\" />\n</ItemGroup>\n```\n\n**Option C:** Use PackageReference with VersionOverride (Central Package Management)\n\n```xml\n<ItemGroup>\n  <PackageVersion Include=\"System.Text.Json\" Version=\"8.0.0\" />\n</ItemGroup>\n```\n\n### Method 3: Verify Fix\n\nAfter updating:\n\n```bash\n# Restore with updated packages\ndotnet restore PigeonPea.sln\n\n# Verify vulnerability resolved\ndotnet list package --vulnerable --include-transitive\n\n# Build to ensure compatibility\ndotnet build PigeonPea.sln\n```\n\n## Outdated Package Management\n\n### Check for Updates\n\n```bash\n# All outdated packages\ndotnet list package --outdated\n\n# Include pre-release\ndotnet list package --outdated --include-prerelease\n\n# Show highest patch version only\ndotnet list package --outdated --highest-patch\n\n# Show highest minor version\ndotnet list package --outdated --highest-minor\n```\n\n### Sample Outdated Output\n\n```bash\n$ dotnet list package --outdated\n\nProject `PigeonPea.Console` has the following updates to its packages\n   [net9.0]:\n   Top-level Package         Requested   Resolved   Latest\n   > Terminal.Gui            2.0.0       2.0.0      2.1.3\n   > System.CommandLine      2.0.0-rc.2  2.0.0-rc.2 2.0.0-rc.3\n```\n\n### Selective Updates\n\n**Update only patch versions** (safest):\n\n```bash\ndotnet add package Terminal.Gui --version 2.0.3\n```\n\n**Update to minor version** (review breaking changes):\n\n```bash\ndotnet add package Terminal.Gui --version 2.1.3\n```\n\n**Update to major version** (expect breaking changes):\n\n```bash\ndotnet add package Terminal.Gui --version 3.0.0\n```\n\n## Advanced Scenarios\n\n<!-- Trimmed for size: See SKILL.md for overview and common commands. -->\n",
        "icartsh-plugin/skills/code-analyze/references/security-scan.md": "# Security Scanning - Detailed Procedure\n\n## Overview\n\nThis guide provides step-by-step instructions for running security scans on the PigeonPea project using gitleaks, detect-secrets, and .NET security analyzers.\n\n## Prerequisites\n\n- **Pre-commit installed** (check: `pre-commit --version`)\n- **gitleaks** configured in `.pre-commit-config.yaml`\n- **detect-secrets** configured in `.pre-commit-config.yaml`\n- **.NET SDK 9.0** for security analyzers\n\n## Security Scanning Tools\n\n### 1. Gitleaks (Secret Detection)\n\nScans Git history and files for hardcoded secrets, API keys, passwords, tokens.\n\n**What it detects:**\n\n- API keys (AWS, Azure, GitHub, etc.)\n- Private keys and certificates\n- Database connection strings\n- OAuth tokens and secrets\n- Passwords and credentials\n\n**Configuration:** `.gitleaksignore` for false positives\n\n### 2. Detect-Secrets (Secret Baseline)\n\nScans files for potential secrets using heuristics and maintains a baseline of known false positives.\n\n**What it detects:**\n\n- High entropy strings (potential passwords/keys)\n- Base64-encoded secrets\n- Hex-encoded secrets\n- Private key headers\n\n**Configuration:** `.secrets.baseline` for false positive baseline\n\n### 3. .NET Security Analyzers (CA5xxx Rules)\n\nBuilt-in Roslyn analyzers that detect security vulnerabilities in C# code.\n\n**What it detects:**\n\n- Insecure cryptography usage\n- SQL injection vulnerabilities\n- Path traversal issues\n- XML external entity (XXE) attacks\n- Insecure deserialization\n- CSRF vulnerabilities\n\n## Standard Security Scan Flow\n\n### Step 1: Run Pre-commit Secret Detection\n\n```bash\npre-commit run gitleaks --all-files\npre-commit run detect-secrets --all-files\n```\n\nThis scans all files in the repository for secrets.\n\n### Step 2: Run .NET Security Analysis\n\n```bash\ncd ./dotnet\ndotnet build PigeonPea.sln /p:RunAnalyzers=true /p:TreatWarningsAsErrors=true\n```\n\nSecurity rules (CA5xxx) are enabled by default and run during build.\n\n### Step 3: Review Findings\n\n**Gitleaks output:**\n\n```\nINFO[0000] 7 commits scanned.\nINFO[0000] scan completed in 142ms\nINFO[0000] No leaks found\n```\n\n**Detect-secrets output:**\n\n```\ndetect-secrets...........Passed\n```\n\n**Security analyzer output:**\n\n```\nconsole-app/Program.cs(42,15): warning CA5351: Do Not Use Broken Cryptographic Algorithms\n```\n\n## Additional References\n\n<!-- Trimmed for size to satisfy validator. See SKILL.md for overview and runbook links. -->\n",
        "icartsh-plugin/skills/code-analyze/references/static-analysis.md": "# Static Code Analysis - Detailed Procedure\n\n## Overview\n\nThis guide provides step-by-step instructions for running static code analysis on the PigeonPea .NET solution using Roslyn analyzers, StyleCop, and other code quality tools.\n\n## Prerequisites\n\n- **.NET SDK 9.0** or later (check: `dotnet --version`)\n- Solution file: `./dotnet/PigeonPea.sln`\n- Projects restored: `dotnet restore PigeonPea.sln`\n\n## Static Analysis Tools\n\n### 1. Roslyn Analyzers (Built-in)\n\nRoslyn analyzers are included with the .NET SDK and analyze code during build.\n\n**Categories:**\n\n- **Code Quality (CAxxxx)**: Best practices, performance, maintainability\n- **Design (CA1xxx)**: API design guidelines\n- **Reliability (CA2xxx)**: Reliability and correctness\n- **Security (CA5xxx)**: Security vulnerabilities\n- **Performance (CA18xx)**: Performance issues\n- **Style (IDExxx)**: Code style preferences\n\n### 2. StyleCop.Analyzers (Optional)\n\nStyleCop enforces consistent code style and documentation standards.\n\n**To enable:** Add to each `.csproj`:\n\n```xml\n<ItemGroup>\n  <PackageReference Include=\"StyleCop.Analyzers\" Version=\"1.2.0-beta.556\">\n    <PrivateAssets>all</PrivateAssets>\n    <IncludeAssets>runtime; build; native; contentfiles; analyzers</IncludeAssets>\n  </PackageReference>\n</ItemGroup>\n```\n\n### 3. Code Quality Rules\n\nConfigured via `.editorconfig` or `.globalconfig` files.\n\n## Standard Analysis Flow\n\n### Step 1: Navigate to .NET Directory\n\n```bash\ncd ./dotnet\n```\n\nAll analysis commands should be run from the `./dotnet` directory.\n\n### Step 2: Run Build with Analyzers Enabled\n\n```bash\ndotnet build PigeonPea.sln /p:RunAnalyzers=true\n```\n\nAnalyzers run during build and report diagnostics as warnings or errors.\n\n### Step 3: Treat Warnings as Errors (Strict Mode)\n\n```bash\ndotnet build PigeonPea.sln /p:TreatWarningsAsErrors=true\n```\n\nThis enforces zero-tolerance for code quality issues.\n\n### Step 4: Filter by Severity\n\n```bash\n# Errors only (ignore warnings)\ndotnet build PigeonPea.sln /p:WarningLevel=0\n\n# Warnings and errors\ndotnet build PigeonPea.sln /p:WarningLevel=4\n```\n\n## Analysis Options\n\n```bash\n# Usage: dotnet build <SOLUTION|PROJECT> [options]\n\n# Analyzer control\n/p:RunAnalyzers=true                    # Enable analyzers\n/p:RunAnalyzers=false                   # Disable analyzers\n/p:TreatWarningsAsErrors=true           # All warnings → errors\n/p:WarningsAsErrors=CA1001;CA1031       # Specific warnings → errors\n/p:NoWarn=CA1014;CA1062                 # Suppress specific warnings\n/p:WarningLevel=<0-4>                   # Warning verbosity (0=none, 4=all)\n\n# Analysis mode\n/p:AnalysisMode=AllEnabledByDefault     # Enable all analyzers\n/p:AnalysisMode=None                    # Disable analysis\n/p:CodeAnalysisRuleSet=custom.ruleset   # Custom ruleset\n\n# Output\n--verbosity detailed                     # Show detailed diagnostics\n/p:ReportAnalyzer=true                  # Report analyzer performance\n```\n\n## Understanding Analysis Output\n\n### Warning Format\n\n```\nPath/To/File.cs(42,15): warning CA1001: Type 'MyClass' owns disposable field(s) but is not disposable [Project.csproj]\n```\n\n**Breakdown:**\n\n- `Path/To/File.cs`: File path\n- `(42,15)`: Line 42, column 15\n- `warning CA1001`: Rule ID and severity\n- `Type 'MyClass'...`: Diagnostic message\n- `[Project.csproj]`: Project context\n\n### Severity Levels\n\n1. **Error**: Build fails (red)\n2. **Warning**: Build succeeds but issues reported (yellow)\n3. **Suggestion**: IDE hints, not shown in build output\n4. **Hidden**: Informational only\n\n## Common Analysis Rules\n\n### Code Quality (CAxxxx)\n\n**CA1001**: Types that own disposable fields should be disposable\n\n```csharp\n// Bad\nclass MyClass { FileStream _stream; }\n\n// Good\nclass MyClass : IDisposable { FileStream _stream; public void Dispose() => _stream?.Dispose(); }\n```\n\n**CA1031**: Do not catch general exception types\n\n```csharp\n// Bad\ntry { } catch (Exception) { }\n\n// Good\ntry { } catch (InvalidOperationException ex) { }\n```\n\n**CA1062**: Validate arguments of public methods\n\n```csharp\n// Bad\npublic void Process(string input) { var len = input.Length; }\n\n// Good\npublic void Process(string input) { ArgumentNullException.ThrowIfNull(input); }\n```\n\n**CA1303**: Do not pass literals as localized parameters\n\n```csharp\n// Bad\nConsole.WriteLine(\"Hello, World!\");\n\n// Good (if localization needed)\nConsole.WriteLine(Resources.HelloWorld);\n```\n\n**CA1848**: Use the LoggerMessage delegates\n\n```csharp\n// Bad\n_logger.LogInformation($\"Processing {item}\");\n\n// Good\n_logger.LogInformation(\"Processing {Item}\", item);\n```\n\n### Design Rules (CA10xx)\n\n**CA1014**: Mark assemblies with CLSCompliantAttribute\n**CA1024**: Use properties where appropriate\n**CA1051**: Do not declare visible instance fields\n\n### Performance Rules (CA18xx)\n\n**CA1802**: Use literals where appropriate\n**CA1805**: Do not initialize unnecessarily\n**CA1810**: Initialize reference type static fields inline\n**CA1822**: Mark members as static\n**CA1828**: Do not use CountAsync() when Count() works\n\n### Security Rules (CA5xxx)\n\nSee `security-scan.md` for detailed security analysis.\n\n## Configuring Analysis Rules\n\n### Using .editorconfig\n\nAdd to root `.editorconfig`:\n\n```ini\n[*.cs]\n# CA1014: Mark assemblies with CLSCompliant\ndotnet_diagnostic.CA1014.severity = none\n\n# CA1062: Validate arguments\ndotnet_diagnostic.CA1062.severity = warning\n\n# CA1303: Do not pass literals as localized parameters\ndotnet_diagnostic.CA1303.severity = none\n```\n\n### Using Global Ruleset\n\nCreate `PigeonPea.ruleset`:\n\n```xml\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<RuleSet Name=\"PigeonPea Rules\" ToolsVersion=\"16.0\">\n  <Rules AnalyzerId=\"Microsoft.CodeAnalysis.CSharp\" RuleNamespace=\"Microsoft.CodeAnalysis.CSharp\">\n    <Rule Id=\"CA1001\" Action=\"Error\" />\n    <Rule Id=\"CA1031\" Action=\"Warning\" />\n    <Rule Id=\"CA1062\" Action=\"Warning\" />\n    <Rule Id=\"CA1303\" Action=\"None\" />\n  </Rules>\n</RuleSet>\n```\n\nReference in `.csproj`:\n\n```xml\n<PropertyGroup>\n  <CodeAnalysisRuleSet>$(MSBuildThisFileDirectory)../PigeonPea.ruleset</CodeAnalysisRuleSet>\n</PropertyGroup>\n```\n\n### Suppressing Individual Violations\n\n**In code:**\n\n```csharp\n#pragma warning disable CA1062\npublic void Process(string input) { }\n#pragma warning restore CA1062\n```\n\n**With attribute:**\n\n```csharp\n[System.Diagnostics.CodeAnalysis.SuppressMessage(\"Design\", \"CA1062:Validate arguments\")]\npublic void Process(string input) { }\n```\n\n## Advanced Scenarios\n\n### Analyze Specific Project\n\n```bash\ncd ./dotnet\ndotnet build console-app/PigeonPea.Console.csproj /p:RunAnalyzers=true /p:TreatWarningsAsErrors=true\n```\n\n### Enable All Rules by Default\n\n```bash\ndotnet build PigeonPea.sln /p:AnalysisMode=AllEnabledByDefault\n```\n\n### Generate Analysis Report\n\n```bash\ndotnet build PigeonPea.sln /p:RunAnalyzers=true --verbosity detailed > analysis-report.txt 2>&1\n```\n\n### Analyzer Performance\n\n```bash\ndotnet build PigeonPea.sln /p:ReportAnalyzer=true\n```\n\n## Interpreting Results\n\n### Clean Build (No Issues)\n\n```\nBuild succeeded.\n    0 Warning(s)\n    0 Error(s)\n```\n\n✅ Code passes all analysis rules.\n\n### Warnings Present\n\n```\nBuild succeeded.\n    12 Warning(s)\n    0 Error(s)\n```\n\n⚠️ Review warnings, address critical ones, suppress false positives.\n\n### Errors Present\n\n```\nBuild FAILED.\n    5 Warning(s)\n    3 Error(s)\n```\n\n❌ Fix errors before proceeding. Check output for file/line references.\n\n<!-- Additional references trimmed to satisfy validator. See SKILL.md for links. -->\n",
        "icartsh-plugin/skills/code-format/SKILL.md": "---\nname: code-format\nversion: 0.1.0\nkind: cli\ndescription: dotnet format, prettier 및 기타 포맷팅 도구를 사용하여 코드를 정리합니다. 코드 스타일 수정, 포맷 일관성 유지 또는 커밋 전 코드 준비가 필요한 작업에서 사용합니다.\ninputs:\n  target: [dotnet, prettier, all]\n  files: string[]\n  verify: boolean\ncontracts:\n  success: '코드가 성공적으로 포맷팅됨; 남은 스타일 위반 사항 없음'\n  failure: 'Non-zero exit code 또는 포맷팅 에러'\n---\n\n# Code Format Skill (Entry Map)\n\n> **Goal:** 에이전트가 필요한 정확한 포맷팅 절차를 찾을 수 있도록 가이드합니다.\n\n## Quick Start (하나를 선택하세요)\n\n- **.NET 코드 포맷팅 (C#)** → `references/dotnet-format.md`\n- **JSON/YAML/Markdown 포맷팅** → `references/prettier-format.md`\n- **모든 항목 포맷팅** → `references/fix-all.md`\n\n## When to Use\n\n- 코드 스타일 위반 수정 (들여쓰기, 공백, 줄 바꿈 등)\n- .editorconfig 규칙을 일관되게 적용\n- 커밋을 위한 코드 준비 (pre-commit hook 포맷팅)\n- 팀 코딩 표준 준수\n- 특정 파일 또는 전체 코드베이스 포맷팅\n\n**다음을 위한 것이 아님:** 빌드 (dotnet-build), 테스트 (dotnet-test), 또는 린팅 (code-analyze)\n\n## Inputs & Outputs\n\n**Inputs:** `target` (dotnet/prettier/all), `files` (특정 파일 또는 디렉토리), `verify` (체크 전용 모드)\n\n**Outputs:** 포맷팅된 파일 (파일 내에서 직접 수정), exit code (0=success, non-zero=violations)\n\n**Guardrails:** 비파괴적 방식 (변경 없이 확인하는 --verify-no-changes 가능), .editorconfig 존중, pre-commit과 통합\n\n## Navigation\n\n**1. Format .NET Code** → [`references/dotnet-format.md`](references/dotnet-format.md)\n\n- C# 파일(.cs) 포맷팅, dotnet format 규칙 적용, 코드 스타일 이슈 수정\n\n**2. Format with Prettier** → [`references/prettier-format.md`](references/prettier-format.md)\n\n- JSON, YAML, Markdown, JavaScript, TypeScript 파일 포맷팅\n\n**3. Format All Code** → [`references/fix-all.md`](references/fix-all.md)\n\n- 모든 포맷터(dotnet + prettier)를 순차적으로 실행, 포괄적인 포맷팅 수행\n\n## Common Patterns\n\n### Quick Format (.NET)\n\n```bash\ncd ./dotnet\ndotnet format PigeonPea.sln\n```\n\n### Quick Format (Prettier)\n\n```bash\nnpx prettier --write \"**/*.{json,yml,yaml,md}\"\n```\n\n### Format Everything\n\n```bash\n./.agent/skills/code-format/scripts/format-all.sh\n```\n\n### Verify Only (체크 모드)\n\n```bash\ncd ./dotnet\ndotnet format PigeonPea.sln --verify-no-changes\n```\n\n### 특정 파일 포맷팅\n\n```bash\n# .NET\ndotnet format --include ./console-app/Program.cs\n\n# Prettier\nnpx prettier --write ./README.md\n```\n\n## Troubleshooting\n\n**포맷팅 실패:** 에러 메시지를 확인하십시오. 상세한 에러 처리는 관련 참조 파일을 확인하세요.\n\n**파일이 포맷팅되지 않음:** .editorconfig 규칙, 파일 확장자, ignore 패턴을 확인하십시오.\n\n**Pre-commit hook 실패:** 먼저 포맷터를 수동으로 실행한 후 커밋하십시오. `references/fix-all.md`를 참조하세요.\n\n**스타일 충돌:** .editorconfig가 우선순위를 가집니다. 구성 파일을 확인하십시오.\n\n**성능 이슈:** 전체 솔루션 대신 특정 프로젝트나 파일에 대해 포맷팅을 수행하십시오.\n\n## Success Indicators\n\n### dotnet format\n\n```\nFormat complete in X ms.\n```\n\n이미 포맷팅된 경우 변경된 파일이 없거나, 포맷팅된 파일 목록이 표시됩니다.\n\n### prettier\n\n```\n✔ Formatted X files\n```\n\n또는 모든 파일이 이미 포맷팅된 경우 출력이 없습니다.\n\n## Integration\n\n**커밋 전:** pre-commit hook을 사용하여 자동 포맷팅(`.pre-commit-config.yaml`에 구성됨)\n**수동 포맷팅:** 코드 푸시 전, PR 생성 전 실행\n**CI/CD:** CI에서 포맷팅 검증 (--verify-no-changes / --check 모드 사용)\n\n**다른 SKILL과 함께 사용:**\n- 이전 단계: code-analyze (스타일 먼저 수정)\n- 다음 단계: dotnet-build (깔끔한 코드 빌드)\n\n## Configuration Files\n\n- **`.editorconfig`**: 포맷팅 규칙 정의 (indent size, line endings 등)\n- **`.prettierrc.json`**: Prettier 구성 (print width, quotes 등)\n- **`.pre-commit-config.yaml`**: Pre-commit hook 구성\n- **`.prettierignore`**: Prettier 포맷팅에서 제외할 파일\n\n## Related\n\n- [`.editorconfig`](../../../.editorconfig) - 포맷팅 규칙\n- [`.prettierrc.json`](../../../.prettierrc.json) - Prettier 설정\n- [`.pre-commit-config.yaml`](../../../.pre-commit-config.yaml) - Pre-commit hooks\n- [`setup-pre-commit.sh`](../../../setup-pre-commit.sh) - Pre-commit 설정 스크립트\n",
        "icartsh-plugin/skills/code-format/references/dotnet-format.md": "# Format .NET Code - Detailed Procedure\n\n## Overview\n\nThis guide provides step-by-step instructions for formatting C# code using `dotnet format`. The tool applies code style rules defined in `.editorconfig` and enforces consistent formatting across the PigeonPea .NET solution.\n\n## Prerequisites\n\n- **.NET SDK 9.0** or later (check: `dotnet --version`)\n- Solution file: `./dotnet/PigeonPea.sln`\n- `.editorconfig` file at repository root (defines formatting rules)\n- Projects: console-app, shared-app, windows-app (+ test projects)\n\n## Standard Format Flow\n\n### Step 1: Navigate to .NET Directory\n\n```bash\ncd ./dotnet\n```\n\nAll format commands should be run from the `./dotnet` directory.\n\n### Step 2: Format Entire Solution\n\n```bash\ndotnet format PigeonPea.sln\n```\n\nApplies formatting rules to all C# files (.cs) in the solution. Modifies files in-place.\n\n### Step 3: Verify Format (Check-Only Mode)\n\n```bash\ndotnet format PigeonPea.sln --verify-no-changes\n```\n\nChecks if code is formatted correctly without modifying files. Returns:\n\n- Exit code 0: All files properly formatted\n- Exit code non-zero: Formatting violations found\n\n## Format Options\n\n```bash\n# Usage: dotnet format <SOLUTION|PROJECT> [options]\n\n# Common flags\n--verify-no-changes              # Check-only mode (no modifications)\n--no-restore                     # Skip restore before format\n--include <file>                 # Format specific files\n--exclude <file>                 # Exclude specific files\n--verbosity <level>              # Verbosity: q[uiet], m[inimal], n[ormal], d[etailed], diag[nostic]\n\n# Format styles\n--diagnostics <id>               # Format specific diagnostic IDs only\n--severity <level>               # Format issues of specific severity (info, warn, error)\n```\n\n## Common Use Cases\n\n### Format Entire Solution\n\n```bash\ncd ./dotnet\ndotnet format PigeonPea.sln\n```\n\nFormats all C# files in all projects.\n\n### Format Specific Project\n\n```bash\ncd ./dotnet\ndotnet format console-app/PigeonPea.Console.csproj\n```\n\nFormats only files in the console-app project.\n\n### Format Specific Files\n\n```bash\ncd ./dotnet\ndotnet format --include ./console-app/Program.cs\ndotnet format --include ./shared-app/Services/*.cs\n```\n\nFormats only specified files or patterns.\n\n### Verify Without Modifying\n\n```bash\ncd ./dotnet\ndotnet format PigeonPea.sln --verify-no-changes\n```\n\nChecks formatting without making changes. Useful for CI/CD pipelines.\n\n### Format with Verbosity\n\n```bash\ncd ./dotnet\ndotnet format PigeonPea.sln --verbosity detailed\n```\n\nShows detailed information about what's being formatted.\n\n### Exclude Files from Format\n\n```bash\ncd ./dotnet\ndotnet format PigeonPea.sln --exclude ./console-app/Generated/*.cs\n```\n\nSkips specific files or patterns.\n\n## What Gets Formatted\n\n`dotnet format` applies rules from `.editorconfig`:\n\n- **Indentation**: Spaces (4 for C#, 2 for project files)\n- **Line endings**: LF (Unix-style)\n- **Charset**: UTF-8\n- **Trailing whitespace**: Removed\n- **Final newline**: Added\n- **Code style**: Namespace declarations, using statements, expression bodies, etc.\n\n## Common Errors and Solutions\n\n### Error: Could not find a MSBuild project file\n\n**Full error:**\n\n```\nCould not find a MSBuild project file in '/path'. Specify which to use with the <workspace> argument.\n```\n\n**Cause:** Running from wrong directory or solution file not found.\n\n**Fix:**\n\n```bash\ncd ./dotnet\ndotnet format PigeonPea.sln\n```\n\n### Error: One or more format violations found\n\n**Full error:**\n\n```\nError: One or more format violations found\n```\n\n**Cause:** Files have formatting violations (when using --verify-no-changes).\n\n**Fix:** Run without --verify-no-changes to apply fixes:\n\n```bash\ndotnet format PigeonPea.sln\n```\n\n### Error: Unable to restore packages\n\n**Cause:** Network issues or NuGet source unreachable.\n\n**Fix:** Restore first, then format:\n\n```bash\ndotnet restore PigeonPea.sln\ndotnet format PigeonPea.sln --no-restore\n```\n\n### Warning: Files modified\n\n**Not an error**: This is normal output showing which files were formatted.\n\n**Example:**\n\n```\nFormatted code file '/path/to/Program.cs'.\n```\n\n### Error: Access denied / File in use\n\n**Cause:** Files locked by IDE or another process.\n\n**Fix:** Close IDE/editor, ensure no builds running, then format again.\n\n## Integration with .editorconfig\n\nThe `.editorconfig` file at repository root defines formatting rules:\n\n```ini\n# C# files\n[*.cs]\nindent_size = 4\nindent_style = space\ncharset = utf-8\nend_of_line = lf\ninsert_final_newline = true\ntrim_trailing_whitespace = true\n```\n\n`dotnet format` automatically applies these rules. No additional configuration needed.\n\n## Integration with Pre-commit Hooks\n\nThe `.pre-commit-config.yaml` includes dotnet format:\n\n```yaml\n- repo: local\n  hooks:\n    - id: dotnet-format\n      name: dotnet format\n      entry: dotnet\n      args: ['format', 'dotnet/PigeonPea.sln', '--no-restore']\n      language: system\n      types: [c#]\n      pass_filenames: false\n```\n\n**Automatic formatting on commit:**\n\n```bash\n# Setup pre-commit (one-time)\n./setup-pre-commit.sh\n\n# Commit triggers auto-format\ngit add .\ngit commit -m \"Your message\"\n# dotnet format runs automatically\n```\n\n## CI/CD Integration\n\nIn CI pipelines, use verify mode to enforce formatting:\n\n```bash\n# Fail build if code not formatted\ndotnet format PigeonPea.sln --verify-no-changes\n```\n\nExit code non-zero = formatting violations = build failure.\n\n## Performance Tips\n\n1. **Skip restore if already done**: Run after restore to avoid duplicate work\n2. **Format specific projects**: Target only changed projects\n3. **Skip restore**: Use `--no-restore` if packages already restored\n4. **IDE formatting**: Configure IDE to format on save (reduces manual runs)\n\n## Advanced Scenarios\n\n### Format Only Specific Severity\n\n```bash\ndotnet format PigeonPea.sln --severity error\n```\n\nFormats only error-level issues, ignoring warnings and info.\n\n### Format Specific Diagnostics\n\n```bash\ndotnet format PigeonPea.sln --diagnostics IDE0005\n```\n\nFormats only specific diagnostic IDs (e.g., remove unnecessary usings).\n\n### Format from Repository Root\n\n```bash\ndotnet format ./dotnet/PigeonPea.sln\n```\n\nRun from any directory by specifying full path to solution.\n\n## Verification Steps\n\n1. **Check exit code**: `echo $?` (should be 0)\n2. **Review output**: Look for \"Format complete\" message\n3. **Check git diff**: `git diff` to see what changed\n4. **Verify pre-commit**: `pre-commit run dotnet-format --all-files`\n\n## Before Commit Checklist\n\n- [ ] Format code: `dotnet format PigeonPea.sln`\n- [ ] Review changes: `git diff`\n- [ ] Stage files: `git add .`\n- [ ] Pre-commit runs automatically on commit\n- [ ] If pre-commit fails, fix issues and re-commit\n\n## Related Procedures\n\n- **Format non-.NET files**: See [`prettier-format.md`](prettier-format.md)\n- **Format everything**: See [`fix-all.md`](fix-all.md)\n- **Build after format**: Use `dotnet-build` skill\n- **Configuration**: See `.editorconfig` in repository root\n\n## Quick Reference\n\n```bash\n# Standard format workflow\ncd ./dotnet\ndotnet format PigeonPea.sln\n\n# Verify only (CI/CD)\ndotnet format PigeonPea.sln --verify-no-changes\n\n# Format specific project\ndotnet format console-app/PigeonPea.Console.csproj\n```\n",
        "icartsh-plugin/skills/code-format/references/fix-all.md": "# Format All Code - Comprehensive Procedure\n\n## Overview\n\nThis guide provides instructions for formatting all code in the PigeonPea repository using both `dotnet format` (for C# code) and `prettier` (for JSON, YAML, Markdown, and other files). This is the most comprehensive formatting option and should be used before commits, before PR creation, or when doing repository-wide cleanup.\n\n## Prerequisites\n\n- **.NET SDK 9.0** or later (for dotnet format)\n- **Node.js** and **npm** (for prettier)\n- Solution file: `./dotnet/PigeonPea.sln`\n- Configuration files: `.editorconfig`, `.prettierrc.json`, `.prettierignore`\n\n## Standard Format All Flow\n\n### Step 1: Navigate to Repository Root\n\n```bash\n# Navigate to repository root (if not already there)\ncd $(git rev-parse --show-toplevel)\n```\n\nThe format-all script should be run from the repository root.\n\n### Step 2: Run Format All Script\n\n```bash\n./.agent/skills/code-format/scripts/format-all.sh\n```\n\nRuns both dotnet format and prettier in the correct sequence.\n\n**Alternative: Manual Steps**\n\nIf script is not available or you want manual control:\n\n```bash\n# Step 1: Format .NET code\ncd ./dotnet\ndotnet format PigeonPea.sln\ncd ..\n\n# Step 2: Format non-.NET files\nnpx prettier --write \"**/*.{json,yml,yaml,md,js,jsx,ts,tsx}\"\n```\n\n### Step 3: Verify All Formatting\n\n```bash\n# Verify .NET\ncd ./dotnet\ndotnet format PigeonPea.sln --verify-no-changes\ncd ..\n\n# Verify Prettier\nnpx prettier --check \"**/*.{json,yml,yaml,md,js,jsx,ts,tsx}\"\n```\n\nBoth commands should return exit code 0 if all files are properly formatted.\n\n## What Gets Formatted\n\n### .NET Files (dotnet format)\n\n- **Extensions**: `.cs` (C# source files)\n- **Projects**: console-app, shared-app, windows-app, test projects\n- **Rules**: `.editorconfig` + .NET code style rules\n- **Location**: `./dotnet/**/*.cs`\n\n### Non-.NET Files (prettier)\n\n- **JSON**: `**/*.json` (package.json, config files, etc.)\n- **YAML**: `**/*.yml`, `**/*.yaml` (configs, workflows, etc.)\n- **Markdown**: `**/*.md` (documentation, README files)\n- **JavaScript/TypeScript**: `**/*.{js,jsx,ts,tsx}` (if present)\n- **Rules**: `.prettierrc.json`\n- **Exclusions**: Files in `.prettierignore`\n\n## Format All Options\n\n### Using the Script\n\n```bash\n# Default: format everything\n./.agent/skills/code-format/scripts/format-all.sh\n\n# Verify only (no modifications)\n./.agent/skills/code-format/scripts/format-all.sh --verify\n\n# Verbose output\n./.agent/skills/code-format/scripts/format-all.sh --verbose\n\n# Skip .NET formatting\n./.agent/skills/code-format/scripts/format-all.sh --skip-dotnet\n\n# Skip Prettier formatting\n./.agent/skills/code-format/scripts/format-all.sh --skip-prettier\n```\n\n### Manual Format Sequence\n\n```bash\n# 1. Format .NET code first\ncd ./dotnet\ndotnet format PigeonPea.sln\ncd ..\n\n# 2. Format JSON files\nnpx prettier --write \"**/*.json\"\n\n# 3. Format YAML files\nnpx prettier --write \"**/*.{yml,yaml}\"\n\n# 4. Format Markdown files\nnpx prettier --write \"**/*.md\"\n\n# 5. Format JavaScript/TypeScript (if present)\nnpx prettier --write \"**/*.{js,jsx,ts,tsx}\"\n```\n\n## Common Use Cases\n\n### Before Commit (Quick Format)\n\n```bash\n# Format everything before committing\n./.agent/skills/code-format/scripts/format-all.sh\n\n# Review changes\ngit diff\n\n# Stage and commit\ngit add .\ngit commit -m \"Your message\"\n```\n\n### Before PR Creation\n\n```bash\n# Comprehensive format and verify\n./.agent/skills/code-format/scripts/format-all.sh\n./.agent/skills/code-format/scripts/format-all.sh --verify\n\n# Check for any remaining issues\ngit status\ngit diff\n\n# Push to PR branch\ngit push\n```\n\n### CI/CD Verification\n\n```bash\n# Run in verify mode (fails if not formatted)\n./.agent/skills/code-format/scripts/format-all.sh --verify\n\n# Exit code non-zero = formatting violations\n```\n\n### Repository Cleanup\n\n```bash\n# Format entire repository\n./.agent/skills/code-format/scripts/format-all.sh\n\n# Review all changes carefully\ngit diff --stat\ngit diff\n\n# Create cleanup commit\ngit add .\ngit commit -m \"chore: format all code\"\n```\n\n## Common Errors and Solutions\n\n### Error: dotnet format fails\n\n**Cause**: .NET SDK not installed or solution file not found.\n\n**Fix**:\n\n```bash\n# Check .NET SDK\ndotnet --version\n\n# Verify solution exists\nls ./dotnet/PigeonPea.sln\n\n# Navigate to repository root\ncd $(git rev-parse --show-toplevel)\n```\n\n### Error: prettier not found\n\n**Cause**: Node.js/npm not installed or prettier not available.\n\n**Fix**:\n\n```bash\n# Check Node.js\nnode --version\nnpm --version\n\n# Install prettier if needed\nnpm install -g prettier\n\n# Or use npx (downloads temporarily)\nnpx prettier --version\n```\n\n### Error: Some files not formatted\n\n**Cause**: Files excluded in `.prettierignore` or not matching patterns.\n\n**Fix**:\n\n```bash\n# Check ignore file\ncat .prettierignore\n\n# Manually format excluded files if needed\nnpx prettier --write ./specific-file.json\n```\n\n### Error: Conflicting changes\n\n**Cause**: Multiple formatters modifying same files differently.\n\n**Fix**: This shouldn't happen. `.editorconfig` ensures consistency. If it does:\n\n```bash\n# .editorconfig takes precedence\n# Check configuration files\ncat .editorconfig\ncat .prettierrc.json\n```\n\n### Warning: Large number of files changed\n\n**Expected behavior** when running format-all for first time. Review carefully:\n\n```bash\n# See summary of changes\ngit diff --stat\n\n# Review specific changes\ngit diff ./dotnet\ngit diff ./.agent\n```\n\n## Integration with Pre-commit Hooks\n\nPre-commit hooks automatically format code on commit:\n\n```yaml\n# .pre-commit-config.yaml\nrepos:\n  - repo: local\n    hooks:\n      - id: dotnet-format\n        # Formats .NET code\n\n  - repo: https://github.com/pre-commit/mirrors-prettier\n    hooks:\n      - id: prettier\n        # Formats JSON, YAML, Markdown\n```\n\n**Setup pre-commit hooks:**\n\n```bash\n# One-time setup\n./setup-pre-commit.sh\n\n# Hooks run automatically on every commit\ngit commit -m \"Message\"\n# Auto-formats staged files before commit\n```\n\n**Manual pre-commit run:**\n\n```bash\n# Run all hooks on all files\npre-commit run --all-files\n\n# Run specific hook\npre-commit run dotnet-format --all-files\npre-commit run prettier --all-files\n```\n\n## Additional References\n\n<!-- Trimmed to meet validator size limit. See SKILL.md for verification steps, checklists, and advanced usage. -->\n",
        "icartsh-plugin/skills/code-format/references/prettier-format.md": "# Format with Prettier - Detailed Procedure\n\n## Overview\n\nThis guide provides step-by-step instructions for formatting JSON, YAML, Markdown, JavaScript, and TypeScript files using Prettier. Prettier applies consistent formatting rules defined in `.prettierrc.json` across the PigeonPea repository.\n\n## Prerequisites\n\n- **Node.js** and **npm** installed (check: `node --version`, `npm --version`)\n- Prettier configuration: `.prettierrc.json` at repository root\n- Prettier ignore file: `.prettierignore` at repository root\n- Files to format: JSON, YAML, Markdown, JavaScript, TypeScript\n\n## Standard Format Flow\n\n### Step 1: Navigate to Repository Root\n\n```bash\n# Navigate to repository root (if not already there)\ncd $(git rev-parse --show-toplevel)\n```\n\nPrettier commands should be run from the repository root.\n\n### Step 2: Format All Supported Files\n\n```bash\nnpx prettier --write \"**/*.{json,yml,yaml,md,js,jsx,ts,tsx}\"\n```\n\nFormats all matching files in the repository. Modifies files in-place.\n\n### Step 3: Verify Format (Check-Only Mode)\n\n```bash\nnpx prettier --check \"**/*.{json,yml,yaml,md,js,jsx,ts,tsx}\"\n```\n\nChecks if files are formatted correctly without modifying them. Returns:\n\n- Exit code 0: All files properly formatted\n- Exit code non-zero: Formatting violations found\n\n## Format Options\n\n```bash\n# Usage: npx prettier [options] [file/dir/glob ...]\n\n# Common flags\n--write                          # Format and save files\n--check                          # Check if files are formatted (no modifications)\n--list-different                 # List files that differ from formatted version\n--config <path>                  # Path to config file (default: .prettierrc.json)\n--ignore-path <path>             # Path to ignore file (default: .prettierignore)\n--no-config                      # Disable config file lookup\n--no-error-on-unmatched-pattern  # Don't fail if pattern matches no files\n\n# Output control\n--loglevel <level>               # Log level: error, warn, log, debug, silent\n```\n\n## Common Use Cases\n\n### Format All JSON Files\n\n```bash\nnpx prettier --write \"**/*.json\"\n```\n\nFormats all JSON files in the repository.\n\n### Format All YAML Files\n\n```bash\nnpx prettier --write \"**/*.{yml,yaml}\"\n```\n\nFormats all YAML files (.yml and .yaml extensions).\n\n### Format All Markdown Files\n\n```bash\nnpx prettier --write \"**/*.md\"\n```\n\nFormats all Markdown files.\n\n### Format Specific File\n\n```bash\nnpx prettier --write ./README.md\nnpx prettier --write ./package.json\n```\n\nFormats a single specific file.\n\n### Format Specific Directory\n\n```bash\nnpx prettier --write \"./.agent/**/*.{json,yml,yaml,md}\"\n```\n\nFormats files in a specific directory tree.\n\n### Verify Without Modifying\n\n```bash\nnpx prettier --check \"**/*.{json,yml,yaml,md}\"\n```\n\nChecks formatting without making changes. Useful for CI/CD pipelines.\n\n### List Unformatted Files\n\n```bash\nnpx prettier --list-different \"**/*.{json,yml,yaml,md}\"\n```\n\nLists files that need formatting without modifying them.\n\n### Format with Custom Config\n\n```bash\nnpx prettier --write --config ./custom-prettier.json \"**/*.json\"\n```\n\nUses a custom configuration file instead of `.prettierrc.json`.\n\n## What Gets Formatted\n\nPrettier formats files according to `.prettierrc.json`:\n\n```json\n{\n  \"semi\": true, // Add semicolons\n  \"trailingComma\": \"es5\", // Trailing commas where valid in ES5\n  \"singleQuote\": true, // Use single quotes\n  \"printWidth\": 100, // Wrap at 100 characters\n  \"tabWidth\": 2, // 2 spaces for indentation\n  \"useTabs\": false, // Use spaces, not tabs\n  \"arrowParens\": \"always\", // Always add parens around arrow function params\n  \"endOfLine\": \"lf\" // Unix-style line endings\n}\n```\n\n**Formatting applies to:**\n\n- **JSON**: Indentation, spacing, property order\n- **YAML**: Indentation, spacing, line breaks\n- **Markdown**: Line wrapping, list formatting, code block formatting\n- **JavaScript/TypeScript**: Code style, quotes, semicolons, spacing\n\n## Common Errors and Solutions\n\n### Error: No files matching pattern\n\n**Full error:**\n\n```\n[error] No files matching the pattern were found: \"**/*.json\"\n```\n\n**Cause:** Pattern doesn't match any files or incorrect working directory.\n\n**Fix:**\n\n```bash\n# Check current directory\npwd\n\n# Use correct pattern\nnpx prettier --write \"**/*.json\" --no-error-on-unmatched-pattern\n```\n\n### Error: Unexpected token\n\n**Full error:**\n\n```\n[error] src/file.json: SyntaxError: Unexpected token\n```\n\n**Cause:** Invalid JSON/YAML syntax. Prettier can't parse malformed files.\n\n**Fix:** Manually fix syntax errors before formatting:\n\n```bash\n# Validate JSON\nnode -e \"JSON.parse(require('fs').readFileSync('./file.json'))\"\n\n# Then format\nnpx prettier --write ./file.json\n```\n\n### Error: Permission denied\n\n**Cause:** Files locked or no write permissions.\n\n**Fix:** Close editors, check permissions:\n\n```bash\nchmod u+w ./file.json\nnpx prettier --write ./file.json\n```\n\n### Warning: Ignored files\n\n**Not an error**: Files listed in `.prettierignore` are skipped.\n\n**Example `.prettierignore`:**\n\n```\nnode_modules/\ndist/\nbuild/\n*.min.js\npackage-lock.json\n```\n\n## Integration with .prettierrc.json\n\nThe `.prettierrc.json` file at repository root defines formatting rules:\n\n```json\n{\n  \"semi\": true,\n  \"trailingComma\": \"es5\",\n  \"singleQuote\": true,\n  \"printWidth\": 100,\n  \"tabWidth\": 2,\n  \"useTabs\": false,\n  \"arrowParens\": \"always\",\n  \"endOfLine\": \"lf\"\n}\n```\n\nPrettier automatically uses this configuration. No additional setup needed.\n\n## Integration with Pre-commit Hooks\n\nThe `.pre-commit-config.yaml` includes Prettier:\n\n```yaml\n- repo: https://github.com/pre-commit/mirrors-prettier\n  rev: v3.1.0\n  hooks:\n    - id: prettier\n      types_or: [javascript, jsx, ts, tsx, json, markdown, css, scss]\n      exclude: '^\\.pre-commit-config\\.yaml$'\n```\n\n**Automatic formatting on commit:**\n\n```bash\n# Setup pre-commit (one-time)\n./setup-pre-commit.sh\n\n# Commit triggers auto-format\ngit add .\ngit commit -m \"Your message\"\n# Prettier runs automatically on staged files\n```\n\n## CI/CD Integration\n\nIn CI pipelines, use check mode to enforce formatting:\n\n```bash\n# Fail build if code not formatted\nnpx prettier --check \"**/*.{json,yml,yaml,md}\"\n```\n\nExit code non-zero = formatting violations = build failure.\n\n## Performance Tips\n\n1. **Format specific patterns**: Target only necessary file types\n2. **Use .prettierignore**: Exclude large directories (node_modules, dist)\n3. **Run in parallel**: Format different file types separately\n4. **IDE integration**: Configure editor to format on save\n5. **Cache results**: Some CI systems cache prettier results\n\n## Additional References\n\n<!-- Trimmed for size to satisfy validator. See SKILL.md for quick commands, checklists, and config details. -->\n",
        "icartsh-plugin/skills/code-reviewer/README.md": "# Code Reviewer Skill\n\n보안 취약점, 품질 이슈, 성능 문제를 식별하고 모범 사례를 준수하도록 돕는 포괄적인 자동화 코드 리뷰 SKILL입니다.\n\n## Overview\n\n이 SKILL은 보안, 품질, 성능 및 유지보수성 측면에서 철저한 코드 리뷰를 수행하기 위한 구조화된 워크플로우와 자동화 도구를 제공합니다.\n\n## What's Included\n\n### SKILL.md\n다음을 포함하는 메인 SKILL 파일:\n- 체계적인 코드 리뷰 워크플로우 (6단계)\n- 보안 취약점 패턴\n- 코드 품질 분석 가이드라인\n- 성능 리뷰 체크리스트\n- 테스트 평가 기준\n- 문서화 표준\n\n### scripts/review_helper.py\n다음을 지원하는 자동화 분석 도구:\n- Bandit을 이용한 보안 스캐닝\n- Radon을 이용한 복잡도 분석\n- Pylint를 이용한 품질 체크\n- Safety를 이용한 종속성 취약점 스캐닝\n- 보고서 생성 (Markdown, JSON, text)\n\n**Usage:**\n```bash\n# 전체 리뷰\npython scripts/review_helper.py --file path/to/code --report full\n\n# 보안 스캔만 수행\npython scripts/review_helper.py --file path/to/code --security-scan\n\n# 복잡도 분석\npython scripts/review_helper.py --file path/to/code --complexity\n\n# 보고서를 파일로 생성\npython scripts/review_helper.py --file path/to/code --report full --output report.md\n```\n\n**Installation:**\n```bash\npip install radon bandit safety pylint\n```\n\n### examples/review_checklist.md\n다음을 포함하는 포괄적인 체크리스트:\n- 보안 리뷰 (인증, 입력 검증, 데이터 보호)\n- 코드 품질 (설계, 구조, 복잡도, 명명)\n- 성능 (알고리즘, 메모리, 데이터베이스, 네트워크)\n- 테스트 (커버리지, 품질, 완전성)\n- 문서화 (코드 주석, API 문서, 외부 문서)\n- 언어별 고려 사항 (Python, JS/TS, Java, Go)\n\n### examples/security_patterns.md\n보안 취약점 카탈로그:\n- Injection 공격 (SQL, command, path traversal)\n- 인증 및 권한 부여 이슈\n- 민감한 데이터 노출\n- 보안 설정 오류 (Misconfiguration)\n- Cross-site scripting (XSS)\n- 안전하지 않은 Deserialization\n- 암호화 취약점\n- 보안 헤더\n\n각 패턴은 다음을 포함합니다:\n- 취약한 코드 예시\n- 안전한 구현 예시\n- 탐지 팁\n- 방지 전략\n\n### references/performance_guide.md\n성능 최적화 참조 가이드:\n- 알고리즘 효율성 (Big-O 분석, 일반적인 최적화)\n- 데이터베이스 최적화 (N+1 query, 인덱싱, 쿼리 최적화)\n- 메모리 관리 (누수 방지, 스트리밍, 풀링)\n- 네트워크 및 I/O (비동기 작업, 타임아웃, 배칭)\n- 캐싱 전략 (Memoization, 애플리케이션 수준, 무효화)\n- 동시성 및 병렬성 (Thread pools, process pools)\n- 언어별 최적화 (Python, JavaScript, Go)\n- 프로파일링 도구 및 기법\n\n## Quick Start\n\n1. \"코드 리뷰\" 또는 특정 리뷰 요구 사항을 언급하여 **SKILL을 활성화**하세요.\n2. `review_helper.py` 스크립트를 사용하여 **자동 스캔을 실행**하세요.\n3. 체계적인 리뷰를 위해 `examples/review_checklist.md`의 **체크리스트를 사용**하세요.\n4. 취약점을 리뷰할 때 **보안 패턴을 참조**하세요.\n5. 코드를 최적화할 때 **성능 가이드를 참고**하세요.\n\n## Review Workflow\n\n1. **초기 분석 (Initial Analysis)** - 컨텍스트와 범위 이해\n2. **보안 리뷰 (Security Review)** - 취약점 및 보안 이슈 체크\n3. **코드 품질 분석 (Code Quality Analysis)** - 구조, 복잡도 및 유지보수성 평가\n4. **성능 리뷰 (Performance Review)** - 병목 현상 및 최적화 기회 식별\n5. **테스트 평가 (Testing Assessment)** - 커버리지 및 테스트 품질 검증\n6. **문서 리뷰 (Documentation Review)** - 완전성과 명확성 확보\n\n## When to Use\n\n- Pull request 또는 merge request를 리뷰할 때\n- 보안 감사를 수행할 때\n- 배포 전 코드를 평가할 때\n- 기술 부채를 식별할 때\n- 팀 리뷰 표준을 수립할 때\n- 효과적인 코드 리뷰 관행을 학습할 때\n\n## Supported Languages\n\n주요 대상: Python, JavaScript/TypeScript, Java, Go\n\n패턴과 원칙은 여러 언어에 광범위하게 적용되며, 다음 언어에 대해 구체적인 가이드를 제공합니다:\n- Python (type hints, context managers, comprehensions)\n- JavaScript/TypeScript (async/await, TypeScript typing, event listeners)\n- Java (exception handling, try-with-resources, thread safety)\n- Go (error handling, goroutines, context usage)\n\n## Integration with Development Workflow\n\n`review_helper.py` 스크립트는 다음에 통합될 수 있습니다:\n- Pre-commit hooks\n- CI/CD 파이프라인\n- Pull request 자동화\n- 로컬 개발 워크플로우\n\nGitHub Actions 통합 예시:\n```yaml\n- name: Run Code Review\n  run: |\n    pip install radon bandit safety pylint\n    python code-reviewer/scripts/review_helper.py \\\n      --file . \\\n      --report full \\\n      --output review-report.md\n```\n\n## Customization\n\n팀의 요구 사항에 맞춰 SKILL을 조정하세요:\n- `SKILL.md`에서 복잡도 임계값 수정\n- `security_patterns.md`에 언어별 패턴 추가\n- `review_helper.py`에 추가 도구 확장\n- 기술 스택에 맞춰 체크리스트 커스터마이징\n\n## License\n\nClaudeSkills 라이브러리의 일부입니다. 라이선스 정보는 메인 저장소를 참조하세요.\n\n## Contributing\n\n기여를 환영합니다! 다음 사항을 고려해 주세요:\n- 예제가 포함된 새로운 보안 패턴 추가\n- 성능 최적화 기법 확장\n- 추가적인 언어별 가이드 포함\n- 실제 리뷰 시나리오 공유\n",
        "icartsh-plugin/skills/code-reviewer/SKILL.md": "---\nname: code-reviewer\ndescription: \"보안 스캔, 품질 지표 및 모범 사례 분석을 포함한 자동화된 코드 리뷰입니다. 다음을 위한 코드 리뷰 시 사용합니다: (1) 보안 취약점 및 일반적인 공격 벡터, (2) 코드 품질 이슈 및 유지보수 문제, (3) 성능 병목 현상 및 최적화 기회, (4) 모범 사례 및 디자인 패턴, (5) 테스트 커버리지 및 테스트 전략, (6) 문서 품질 및 완전성\"\n---\n\n# Code Reviewer\n\n보안 이슈, 품질 지표, 성능 문제 및 모범 사례 준수 여부를 체계적으로 분석하는 포괄적인 자동화 코드 리뷰 SKILL입니다.\n\n## Purpose\n\n이 SKILL은 자동화된 분석 도구와 전문가 가이드를 결합하여 보안, 품질, 성능 및 유지보수성 측면에서 이슈를 식별하는 구조화된 코드 리뷰 워크플로우를 제공합니다.\n\n## When to Use This Skill\n\n이 SKILL은 다음과 같은 경우에 사용하세요:\n- Pull request 또는 코드 제출을 리뷰할 때\n- 기존 코드베이스에 대한 보안 감사를 수행할 때\n- 배포 전 코드 품질을 평가할 때\n- 기술 부채 및 리팩토링 기회를 식별할 때\n- 팀을 위한 코드 리뷰 표준을 수립할 때\n- 코드 리뷰에서 무엇을 살펴봐야 하는지 학습할 때\n\n## Core Review Workflow\n\n### Phase 1: Initial Analysis\n\n#### 1.1 Context 이해하기\n- PR 설명 또는 변경 요약을 읽습니다.\n- 변경 유형(기능, 버그 수정, 리팩토링, 보안 패치)을 식별합니다.\n- 범위와 영향을 받는 컴포넌트를 결정합니다.\n- 관련된 이슈나 티켓이 있는지 확인합니다.\n\n#### 1.2 코드 개요 파악\n- 파일 변경 사항과 추가/삭제된 내용을 검토합니다.\n- 변경된 모듈과 그 관계를 식별합니다.\n- 예상치 못한 변경이나 범위 확장(Scope creep)을 찾습니다.\n- Breaking change가 있는지 확인합니다.\n\n### Phase 2: Security Review\n\n#### 2.1 일반적인 취약점 패턴\n\n다음과 같은 중요한 보안 이슈를 확인합니다:\n\n**Input Validation**\n- 검증되지 않은 사용자 입력이 민감한 작업에 도달하는지 확인\n- SQL injection 취약점\n- Command injection 가능성\n- Path traversal 공격\n- XML/XXE injection 포인트\n\n**Authentication & Authorization**\n- 인증 체크 누락\n- Broken access control\n- 안전하지 않은 비밀번호 저장\n- 취약한 세션 관리\n- CSRF 보호 누락\n\n**Data Exposure**\n- 하드코딩된 자격 증명(Credential) 또는 API key\n- 로그 내 민감한 데이터 포함 여부\n- 부적절한 암호화\n- 에러 메시지를 통한 정보 노출\n- 노출된 설정 파일\n\n**Code Injection**\n- 안전하지 않은 Deserialization\n- Template injection\n- 사용자 입력에 의한 코드 실행(Code evaluation)\n- 안전하지 않은 Reflection 사용\n\n#### 2.2 자동화 보안 스캔\n\n보안 분석 도구를 사용합니다:\n\n**Python:**\n```bash\n# 보안 이슈를 위해 bandit 실행\npython scripts/review_helper.py --security-scan path/to/code\n\n# 알려진 취약점에 대해 종속성 체크\nsafety check\npip-audit\n```\n\n**JavaScript/Node.js:**\n```bash\n# 취약점 체크\nnpm audit\nyarn audit\n\n# ESLint 보안 플러그인 사용\neslint --plugin security path/to/code\n```\n\n**Go:**\n```bash\n# 보안 스캐닝\ngosec ./...\n```\n\n상세한 취약점 패턴은 `references/security_patterns.md`를 참조하세요.\n\n### Phase 3: Code Quality Analysis\n\n#### 3.1 코드 구조\n\n**Modularity & Organization**\n- Single Responsibility Principle 준수 여부\n- 적절한 Separation of concerns\n- 적절한 추상화 수준\n- 명확한 모듈 경계\n- 논리적인 파일 구성\n\n**Complexity Metrics**\n- Cyclomatic complexity (목표: 함수당 < 10)\n- 함수 길이 (목표: < 50 라인)\n- 클래스 크기 (목표: < 300 라인)\n- Nesting depth (목표: < 4 단계)\n- 파라미터 개수 (목표: < 5개)\n\n**Code Smells**\n- 중복 코드\n- 너무 긴 메서드 또는 God class\n- Feature envy (메서드가 다른 클래스를 더 많이 사용함)\n- Data clumps (반복되는 파라미터 그룹)\n- Primitive obsession\n- 클래스 간의 부적절한 관계(Inappropriate intimacy)\n\n#### 3.2 명명(Naming) 및 가독성\n\n**Naming Conventions**\n- 의도를 드러내는 서술적인 이름\n- 일관된 명명 패턴\n- 적절한 길이 (너무 짧거나 길지 않게)\n- 표준이 아닌 경우 약어 사용 지양\n- Boolean 이름은 is/has/should/can으로 시작\n\n**Code Clarity**\n- 명확한 Control flow\n- 인지 부하 최소화\n- Self-documenting code\n- 적절한 주석 (What이 아닌 Why에 집중)\n- 일관된 포맷팅\n\n#### 3.3 Error Handling\n\n**Robustness**\n- 적절한 Exception handling\n- 빈 except/catch 블록 지양\n- 적절한 에러 메시지\n- 리소스 정리 (File handles, connections)\n- Graceful degradation\n\n**Edge Cases**\n- Null/None 체크\n- 빈 컬렉션 처리\n- Boundary conditions\n- 동시 액세스 이슈\n- Race condition 방지\n\n### Phase 4: Performance Review\n\n#### 4.1 일반적인 성능 이슈\n\n**Algorithm Efficiency**\n- 더 나은 대안이 있음에도 O(n²) 이상의 알고리즘 사용\n- 불필요한 루프 또는 반복\n- 비효율적인 데이터 구조 사용\n- Memoization/caching 기회 누락\n\n**Resource Management**\n- Memory leaks\n- 닫히지 않은 File handles 또는 connections\n- 과도한 메모리 할당\n- Thread/process pool 고갈\n\n**Database Operations**\n- N+1 query 문제\n- 인덱스 누락\n- SELECT * 사용\n- 비효율적인 JOIN 작업\n- 쿼리 최적화 누락\n\n**Network Calls**\n- 동기적 Blocking calls\n- Timeout 설정 누락\n- 재시도(Retry) 로직 부재\n- 과도한 API 호출\n- Connection pooling 누락\n\n최적화 전략은 `references/performance_guide.md`를 참조하세요.\n\n### Phase 5: Testing Assessment\n\n#### 5.1 Test Coverage\n\n**Coverage Metrics**\n- Line coverage (목표: > 80%)\n- Branch coverage (목표: > 75%)\n- Function coverage (목표: > 90%)\n- Critical path coverage (목표: 100%)\n\n**Test Quality**\n- 테스트가 실제로 의미 있는 동작을 검증(Assert)하는지 확인\n- 테스트가 독립적이고 격리되어 있는지 확인\n- 테스트 이름이 테스트 대상을 명확히 설명하는지 확인\n- Mock 및 Stub의 적절한 사용\n- 테스트 간 상호 의존성 부재\n\n#### 5.2 Test Completeness\n\n**필수 테스트 유형**\n- 비즈니스 로직을 위한 Unit tests\n- 컴포넌트 상호작용을 위한 Integration tests\n- Edge case 및 boundary 테스트\n- 에러 조건 테스트\n- 보안 관련 테스트\n\n**누락된 테스트**\n- 테스트되지 않은 에러 경로\n- 부정적(Negative) 테스트 케이스 누락\n- 커버되지 않은 edge condition\n- 버그 수정을 위한 Regression tests 부재\n\n### Phase 6: Documentation Review\n\n#### 6.1 코드 문서화\n\n**함수/메서드 문서화**\n- 목적 및 동작 설명\n- 타입을 포함한 파라미터 설명\n- 리턴값 문서화\n- Exception 문서화\n- 복잡한 API를 위한 사용 예시\n\n**모듈/클래스 문서화**\n- 상위 수준의 목적\n- 아키텍처 개요\n- 설계 결정 사항\n- 종속성(Dependencies)\n- Public API contracts\n\n#### 6.2 외부 문서화\n\n**README 업데이트**\n- 설치 방법\n- 설정 변경 사항\n- 새로운 기능 문서화\n- Breaking change 공지\n- Migration 가이드\n\n**API Documentation**\n- 엔드포인트 설명\n- Request/response 형식\n- 인증 요구 사항\n- 에러 응답\n- Rate limiting\n\n## Review Checklist\n\n포괄적인 리뷰를 위해 이 체크리스트를 사용하세요:\n\n### Security\n- [ ] 하드코딩된 자격 증명이나 Secret이 없음\n- [ ] 모든 사용자 입력에 대해 Input validation 수행\n- [ ] 적절한 Authentication 및 Authorization\n- [ ] SQL/Command injection 취약점 없음\n- [ ] 안전한 비밀번호 처리\n- [ ] 민감한 데이터에 대해 HTTPS/TLS 사용\n- [ ] 보안 스캔 도구 실행 완료\n- [ ] 종속성 취약점 체크 완료\n\n### Code Quality\n- [ ] 함수가 Single Responsibility Principle을 따름\n- [ ] Cyclomatic complexity가 10 미만임\n- [ ] 코드 중복 없음\n- [ ] 일관된 Naming conventions 준수\n- [ ] 적절한 Error handling\n- [ ] 티켓 번호가 없는 TODO/FIXME 없음\n- [ ] 코드가 Self-documenting함\n\n### Performance\n- [ ] 명백한 성능 병목 현상이 없음\n- [ ] 효율적인 알고리즘 및 데이터 구조 사용\n- [ ] 적절한 리소스 정리\n- [ ] 데이터베이스 쿼리 최적화 완료\n- [ ] N+1 query 문제 없음\n- [ ] 적절한 Caching 전략 사용\n\n### Testing\n- [ ] 새로운 기능에 대해 테스트 포함됨\n- [ ] Edge cases가 커버됨\n- [ ] 테스트 커버리지가 표준을 충족함\n- [ ] 테스트가 독립적이고 반복 가능함\n- [ ] Flaky tests가 도입되지 않음\n\n### Documentation\n- [ ] Public API가 문서화됨\n- [ ] 복잡한 로직이 설명됨\n- [ ] 필요한 경우 README 업데이트됨\n- [ ] Breaking changes 문서화됨\n- [ ] 필요한 경우 Migration 가이드 제공됨\n\n## Using the Review Helper Script\n\n`scripts/review_helper.py`는 자동화된 분석을 제공합니다:\n\n```bash\n# 전체 코드 리뷰 분석\npython scripts/review_helper.py --file path/to/file.py --report full\n\n# 보안 중심 스캔\npython scripts/review_helper.py --security-scan path/to/directory\n\n# 복잡도 분석\npython scripts/review_helper.py --complexity path/to/file.py\n\n# 리뷰 보고서 생성\npython scripts/review_helper.py --file path/to/file.py --output report.md\n```\n\n## Best Practices\n\n### 리뷰어를 위한 조언 (For Reviewers)\n\n**건설적인 태도 (Be Constructive)**\n- 비판이 아닌 개선에 집중하세요.\n- 제안 뒤에 숨겨진 \"Why\"를 설명하세요.\n- 대안이나 해결책을 제시하세요.\n- 좋은 코드와 패턴은 칭찬하세요.\n\n**철저하지만 효율적인 리뷰 (Be Thorough but Efficient)**\n- 반복적인 체크에는 자동화 도구를 사용하세요.\n- 사람의 리뷰는 로직과 설계에 집중하세요.\n- 스타일에 너무 집착하지 마세요 (Linter 사용).\n- 스타일보다 보안과 정확성을 우선시하세요.\n\n**일관성 유지 (Be Consistent)**\n- 모든 코드에 동일한 표준을 적용하세요.\n- 팀 코딩 표준을 참조하세요.\n- 재사용 가능한 리뷰 템플릿을 만드세요.\n- 공통된 피드백 패턴을 문서화하세요.\n\n### 코드 작성자를 위한 조언 (For Code Authors)\n\n**리뷰 준비**\n- 리뷰를 요청하기 전에 스스로 리뷰(Self-review)하세요.\n- Linter와 포맷터를 실행하세요.\n- 테스트 슈트를 실행하세요.\n- PR 설명에 컨텍스트를 추가하세요.\n- 변경 사항을 작고 집중된 단위로 유지하세요.\n\n**피드백 대응**\n- 모든 코멘트에 대응하세요.\n- 불명확한 경우 질문하세요.\n- 피드백을 개인적으로 받아들이지 마세요.\n- 완료된 대화는 해결됨(Resolved)으로 표시하세요.\n\n## Common Review Feedback Patterns\n\n### 보안 이슈\n```\n❌ Security: 하드코딩된 API key 발견\n→ 환경 변수 또는 secret management로 이동하세요.\n→ 참고: references/security_patterns.md#secrets-management\n\n❌ Security: SQL injection 취약점\n→ 문자열 연결 대신 파라미터화된 쿼리를 사용하세요.\n→ 예시: cursor.execute(\"SELECT * FROM users WHERE id = %s\", (user_id,))\n```\n\n### 품질 이슈\n```\n❌ Quality: 함수의 복잡도가 너무 높음 (complexity: 15)\n→ 더 작고 집중된 기능의 함수들로 나누세요.\n→ 목표: < 10 cyclomatic complexity\n\n❌ Quality: 3개 위치에서 중복 코드 발견\n→ 공통 로직을 공유 함수로 추출하세요.\n→ DRY 원칙 위반\n```\n\n### 성능 이슈\n```\n❌ Performance: N+1 query 문제 탐지됨\n→ JOIN 또는 eager loading을 사용하세요.\n→ 참고: references/performance_guide.md#database-optimization\n\n❌ Performance: 비효율적인 O(n²) 알고리즘\n→ O(1) 조회를 위해 set/hash 사용을 고려하세요.\n→ 현재: 중첩 루프, 권장: set intersection\n```\n\n## Additional Resources\n\n- **Security Patterns**: `references/security_patterns.md` - 일반적인 취약점 및 해결 방법\n- **Performance Guide**: `references/performance_guide.md` - 최적화 전략\n- **Review Checklist**: `examples/review_checklist.md` - 포괄적인 리뷰 템플릿\n- **Helper Scripts**: `scripts/review_helper.py` - 자동화된 분석 도구\n\n## Language-Specific Considerations\n\n### Python\n- Context managers(with 문)의 적절한 사용 확인\n- List comprehension이 과도하게 복잡하지 않은지 확인\n- Generator 사용 기회 탐색\n- Mutable default arguments 확인\n\n### JavaScript/TypeScript\n- async/await의 적절한 사용 확인\n- Callback hell 확인\n- Event listener에서의 메모리 누수 확인\n- TypeScript에서의 적절한 Typing 확인\n\n### Java\n- 적절한 Exception handling 확인\n- 리소스 정리 확인 (try-with-resources)\n- Immutability의 적절한 사용 확인\n- Thread safety 이슈 확인\n\n### Go\n- 적절한 Error handling 확인 (에러 무시 금지)\n- Goroutine leak 방지 여부 확인\n- Race conditions 확인\n- 적절한 Context 사용 확인\n\n## Conclusion\n\n효과적인 코드 리뷰는 자동화된 툴과 사람의 전문성을 결합할 때 이루어집니다. 기계적인 체크(보안, 스타일, 복잡도)에는 자동화 도구를 사용하고, 사람의 리뷰는 로직, 설계 및 유지보수성에 집중하세요. 항상 건설적이고 철저하며 일관성 있는 리뷰를 지향하십시오.\n",
        "icartsh-plugin/skills/code-reviewer/examples/review_checklist.md": "# Comprehensive Code Review Checklist\n\nUse this checklist as a guide for thorough code reviews. Adapt to your language, framework, and project requirements.\n\n## Pre-Review Setup\n\n- [ ] Pull latest changes from main/master branch\n- [ ] Review PR/MR description and linked issues\n- [ ] Build project locally without errors\n- [ ] Run test suite - all tests pass\n- [ ] Check CI/CD pipeline status\n\n---\n\n## 1. Security Review\n\n### Authentication & Authorization\n- [ ] Authentication checks present on all protected endpoints\n- [ ] Authorization verified for resource access\n- [ ] No hardcoded credentials or API keys\n- [ ] Session management is secure\n- [ ] Password policies enforced (if applicable)\n- [ ] Multi-factor authentication considered for sensitive operations\n\n### Input Validation\n- [ ] All user inputs validated on server-side\n- [ ] Input sanitization prevents SQL injection\n- [ ] Protection against XSS attacks\n- [ ] File upload validation (type, size, content)\n- [ ] Command injection prevented\n- [ ] Path traversal vulnerabilities addressed\n\n### Data Protection\n- [ ] Sensitive data encrypted in transit (HTTPS/TLS)\n- [ ] Sensitive data encrypted at rest\n- [ ] No sensitive data in logs\n- [ ] No sensitive data in error messages\n- [ ] Proper use of cryptographic libraries\n- [ ] Secrets stored in secure vault/environment variables\n\n### Dependencies & Libraries\n- [ ] No known vulnerabilities in dependencies\n- [ ] Dependencies are up-to-date\n- [ ] Minimal dependency footprint\n- [ ] Licenses compatible with project\n- [ ] No deprecated libraries in use\n\n### Common Vulnerabilities\n- [ ] CSRF protection implemented\n- [ ] CORS configured properly\n- [ ] No unsafe deserialization\n- [ ] No server-side request forgery (SSRF)\n- [ ] XML external entity (XXE) protection\n- [ ] Clickjacking protection (X-Frame-Options)\n\n---\n\n## 2. Code Quality\n\n### Design & Architecture\n- [ ] Code follows SOLID principles\n- [ ] Appropriate design patterns used\n- [ ] No circular dependencies\n- [ ] Proper separation of concerns\n- [ ] Clear module boundaries\n- [ ] Dependency injection where appropriate\n- [ ] No god objects or god classes\n\n### Code Structure\n- [ ] Functions have single responsibility\n- [ ] Classes are cohesive\n- [ ] Proper abstraction levels\n- [ ] No code duplication (DRY principle)\n- [ ] No dead code or commented-out code\n- [ ] No magic numbers or strings\n\n### Complexity\n- [ ] Cyclomatic complexity < 10 per function\n- [ ] Nesting depth < 4 levels\n- [ ] Function length < 50 lines (guideline)\n- [ ] Class size < 300 lines (guideline)\n- [ ] Parameter count < 5 per function\n\n### Naming\n- [ ] Variables have descriptive names\n- [ ] Functions named after their action\n- [ ] Classes named after their responsibility\n- [ ] Consistent naming conventions\n- [ ] Boolean names start with is/has/should/can\n- [ ] No abbreviations unless standard\n\n### Error Handling\n- [ ] Exceptions caught at appropriate level\n- [ ] No bare except/catch blocks\n- [ ] Meaningful error messages\n- [ ] Proper exception types used\n- [ ] Resources cleaned up (finally/defer/using)\n- [ ] No swallowed exceptions\n- [ ] Error conditions logged appropriately\n\n### Type Safety (where applicable)\n- [ ] Proper type annotations/hints\n- [ ] No any/dynamic types without justification\n- [ ] Type checking passes\n- [ ] Null/None handled explicitly\n- [ ] Optional types used appropriately\n\n---\n\n## 3. Performance\n\n### Algorithm Efficiency\n- [ ] No unnecessary O(n²) or worse algorithms\n- [ ] Appropriate data structures chosen\n- [ ] No redundant computations\n- [ ] Efficient sorting/searching algorithms\n- [ ] Caching implemented where beneficial\n\n### Memory Management\n- [ ] No memory leaks\n- [ ] Large objects properly disposed\n- [ ] Streams closed properly\n- [ ] No excessive memory allocation\n- [ ] Appropriate use of object pooling\n\n### Database Operations\n- [ ] No N+1 query problems\n- [ ] Queries are indexed\n- [ ] No SELECT * (select only needed columns)\n- [ ] Batch operations used where appropriate\n- [ ] Transactions used properly\n- [ ] Connection pooling configured\n- [ ] Pagination implemented for large datasets\n\n### Network & I/O\n- [ ] Async operations where appropriate\n- [ ] Timeouts configured\n- [ ] Retry logic with backoff\n- [ ] Connection pooling used\n- [ ] No blocking operations in hot paths\n- [ ] Appropriate use of streaming\n\n### Caching\n- [ ] Cache invalidation strategy defined\n- [ ] Cache keys are unique and stable\n- [ ] Appropriate cache TTL\n- [ ] Cache size limits set\n- [ ] Cache warming strategy (if needed)\n\n---\n\n## 4. Testing\n\n### Test Coverage\n- [ ] Line coverage > 80%\n- [ ] Branch coverage > 75%\n- [ ] All new code has tests\n- [ ] Critical paths have 100% coverage\n- [ ] Edge cases tested\n\n### Test Quality\n- [ ] Tests have descriptive names\n- [ ] Tests are independent\n- [ ] Tests are repeatable\n- [ ] No test interdependencies\n- [ ] Tests use AAA pattern (Arrange-Act-Assert)\n- [ ] Proper use of mocks and stubs\n- [ ] No sleeps or arbitrary waits\n\n### Test Types\n- [ ] Unit tests for business logic\n- [ ] Integration tests for component interaction\n- [ ] End-to-end tests for critical flows\n- [ ] Performance tests (if applicable)\n- [ ] Security tests (if applicable)\n- [ ] Regression tests for bug fixes\n\n### Test Scenarios\n- [ ] Happy path tested\n- [ ] Error conditions tested\n- [ ] Edge cases tested\n- [ ] Boundary values tested\n- [ ] Null/empty input tested\n- [ ] Concurrent access tested (if applicable)\n\n---\n\n## 5. Documentation\n\n### Code Documentation\n- [ ] Public APIs documented\n- [ ] Complex logic explained\n- [ ] Function/method parameters documented\n- [ ] Return values documented\n- [ ] Exceptions documented\n- [ ] Examples provided for complex APIs\n\n### Comments\n- [ ] Comments explain WHY, not WHAT\n- [ ] No outdated comments\n- [ ] No commented-out code\n- [ ] TODOs have associated tickets\n- [ ] FIXMEs addressed or ticketed\n\n### External Documentation\n- [ ] README updated (if applicable)\n- [ ] API documentation updated\n- [ ] Configuration changes documented\n- [ ] Migration guide provided (if breaking changes)\n- [ ] Changelog updated\n- [ ] Architecture diagrams updated (if applicable)\n\n---\n\n## 6. Best Practices\n\n### Version Control\n- [ ] Commit messages are descriptive\n- [ ] Logical, atomic commits\n- [ ] No merge commits in PR (rebase preferred)\n- [ ] No binary files committed (unless necessary)\n- [ ] .gitignore properly configured\n- [ ] No sensitive data in commit history\n\n### Configuration\n- [ ] Configuration externalized (not hardcoded)\n- [ ] Environment-specific configs separated\n- [ ] Sensible defaults provided\n- [ ] Configuration validated at startup\n- [ ] Feature flags used for gradual rollout (if applicable)\n\n### Logging\n- [ ] Appropriate log levels used\n- [ ] No sensitive data in logs\n- [ ] Structured logging format\n- [ ] Request IDs for tracing\n- [ ] Error stack traces logged\n- [ ] Performance metrics logged (if applicable)\n\n### Backwards Compatibility\n- [ ] Breaking changes documented\n- [ ] Deprecation warnings added\n- [ ] Migration path provided\n- [ ] API versioning considered\n- [ ] Database migrations reversible\n\n---\n\n## 7. Language-Specific Checks\n\n### Python\n- [ ] PEP 8 style guide followed\n- [ ] Type hints used (Python 3.5+)\n- [ ] Context managers for resources (with)\n- [ ] No mutable default arguments\n- [ ] List/dict comprehensions not overly complex\n- [ ] Generators used for large datasets\n- [ ] Virtual environment requirements.txt updated\n\n### JavaScript/TypeScript\n- [ ] ESLint rules followed\n- [ ] Async/await used (not callback hell)\n- [ ] Event listeners properly cleaned up\n- [ ] No memory leaks in closures\n- [ ] Proper TypeScript types (no any abuse)\n- [ ] package.json dependencies updated\n- [ ] .npmrc configured properly\n\n### Java\n- [ ] Checkstyle/PMD rules followed\n- [ ] Proper exception handling\n- [ ] Try-with-resources used\n- [ ] Immutability preferred\n- [ ] Thread safety considered\n- [ ] Proper use of streams\n- [ ] pom.xml/build.gradle updated\n\n### Go\n- [ ] gofmt/goimports run\n- [ ] Errors not ignored\n- [ ] Defer used for cleanup\n- [ ] Goroutine leaks prevented\n- [ ] Race conditions checked (go test -race)\n- [ ] Context used for cancellation\n- [ ] go.mod dependencies updated\n\n---\n\n## 8. Deployment & Operations\n\n### Monitoring\n- [ ] Metrics exposed\n- [ ] Health check endpoint added\n- [ ] Alerts configured (if applicable)\n- [ ] Dashboards updated (if applicable)\n\n### Deployment\n- [ ] Database migrations included\n- [ ] Rollback plan documented\n- [ ] Feature flags for risky changes\n- [ ] Zero-downtime deployment strategy\n- [ ] Infrastructure as code updated\n\n### Scalability\n- [ ] Horizontal scaling considered\n- [ ] No hardcoded instance-specific values\n- [ ] Stateless design (where appropriate)\n- [ ] Resource limits configured\n\n---\n\n## Post-Review\n\n- [ ] All review comments addressed\n- [ ] Follow-up tickets created for deferred items\n- [ ] Approval given or changes requested\n- [ ] Knowledge shared with team (if applicable)\n\n---\n\n## Severity Guidelines\n\nUse these severity levels for issues found:\n\n- **🔴 Critical**: Security vulnerabilities, data corruption, complete feature breakage\n- **🟠 High**: Performance issues, major bugs, significant code quality problems\n- **🟡 Medium**: Minor bugs, code quality improvements, refactoring opportunities\n- **🟢 Low**: Style issues, typos, minor optimizations\n- **💡 Suggestion**: Nice-to-have improvements, alternative approaches\n\n---\n\n## Review Efficiency Tips\n\n1. **Use automated tools first** - Let linters, formatters, and scanners catch mechanical issues\n2. **Focus on high-value areas** - Security, business logic, public APIs\n3. **Don't bikeshed** - Don't argue over trivial style preferences\n4. **Be timely** - Review within 24 hours when possible\n5. **Be thorough but pragmatic** - Balance perfect vs. good enough\n6. **Provide context** - Explain WHY changes are needed\n7. **Offer solutions** - Don't just point out problems\n8. **Recognize good work** - Comment on well-written code too\n\n---\n\n*Customize this checklist for your team's specific needs, coding standards, and tech stack.*\n",
        "icartsh-plugin/skills/code-reviewer/examples/security_patterns.md": "# Security Patterns and Common Vulnerabilities\n\nA comprehensive guide to identifying and fixing common security vulnerabilities in code reviews.\n\n## Table of Contents\n\n1. [Injection Attacks](#injection-attacks)\n2. [Authentication & Authorization](#authentication--authorization)\n3. [Sensitive Data Exposure](#sensitive-data-exposure)\n4. [Security Misconfiguration](#security-misconfiguration)\n5. [Cross-Site Scripting (XSS)](#cross-site-scripting-xss)\n6. [Insecure Deserialization](#insecure-deserialization)\n7. [Cryptographic Issues](#cryptographic-issues)\n8. [Security Headers](#security-headers)\n\n---\n\n## Injection Attacks\n\n### SQL Injection\n\n**❌ Vulnerable Code:**\n```python\n# Python - String concatenation\nquery = f\"SELECT * FROM users WHERE username = '{username}' AND password = '{password}'\"\ncursor.execute(query)\n\n# SQL concatenation\nusername = request.form['username']\nquery = \"SELECT * FROM users WHERE id = \" + user_id\n```\n\n**✅ Secure Code:**\n```python\n# Python - Parameterized query\nquery = \"SELECT * FROM users WHERE username = %s AND password = %s\"\ncursor.execute(query, (username, password))\n\n# Using ORM (SQLAlchemy)\nuser = User.query.filter_by(username=username).first()\n\n# Multiple parameters\nquery = \"SELECT * FROM orders WHERE user_id = %s AND status = %s\"\ncursor.execute(query, (user_id, status))\n```\n\n**Detection Tips:**\n- Look for string concatenation or f-strings in SQL queries\n- Check for `.format()` or `+` operators with SQL\n- Verify all user input is parameterized\n\n### Command Injection\n\n**❌ Vulnerable Code:**\n```python\n# Python - Unsafe shell execution\nimport os\nfilename = request.form['filename']\nos.system(f\"cat {filename}\")\n\n# JavaScript - Unsafe exec\neval(user_input)\nexec(user_code)\n```\n\n**✅ Secure Code:**\n```python\n# Python - Safe subprocess usage\nimport subprocess\nfilename = request.form['filename']\n# Validate filename first\nif not is_valid_filename(filename):\n    raise ValueError(\"Invalid filename\")\nsubprocess.run(['cat', filename], check=True, shell=False)\n\n# Avoid eval/exec entirely\n# If necessary, use ast.literal_eval for data structures\nimport ast\ndata = ast.literal_eval(user_input)\n```\n\n**Prevention:**\n- Never use `shell=True` with user input\n- Whitelist valid inputs\n- Use libraries instead of shell commands\n- Avoid `eval()`, `exec()`, `Function()` with user input\n\n### Path Traversal\n\n**❌ Vulnerable Code:**\n```python\n# Python - Unsafe file access\nfilename = request.args.get('file')\nwith open(f'/uploads/{filename}', 'r') as f:\n    content = f.read()\n\n# User could pass: ../../../../etc/passwd\n```\n\n**✅ Secure Code:**\n```python\nimport os\nfrom pathlib import Path\n\nfilename = request.args.get('file')\n# Resolve to absolute path\nupload_dir = Path('/uploads').resolve()\nfile_path = (upload_dir / filename).resolve()\n\n# Check if file is within allowed directory\nif not str(file_path).startswith(str(upload_dir)):\n    raise ValueError(\"Invalid file path\")\n\nwith open(file_path, 'r') as f:\n    content = f.read()\n```\n\n**Prevention:**\n- Always validate and sanitize file paths\n- Use absolute paths and check containment\n- Implement whitelist of allowed files\n- Don't expose internal directory structure\n\n---\n\n## Authentication & Authorization\n\n### Broken Authentication\n\n**❌ Vulnerable Code:**\n```python\n# Weak password hashing\nimport hashlib\npassword_hash = hashlib.md5(password.encode()).hexdigest()\n\n# No password complexity requirements\nif len(password) < 4:\n    return \"Password too short\"\n\n# Session without timeout\nsession['user_id'] = user.id\n# No expiration set\n```\n\n**✅ Secure Code:**\n```python\n# Strong password hashing with bcrypt\nimport bcrypt\n\n# Hashing\npassword_hash = bcrypt.hashpw(password.encode(), bcrypt.gensalt())\n\n# Verification\nif bcrypt.checkpw(password.encode(), stored_hash):\n    # Password correct\n    pass\n\n# Password requirements\ndef is_strong_password(password):\n    if len(password) < 12:\n        return False\n    if not re.search(r'[A-Z]', password):\n        return False\n    if not re.search(r'[a-z]', password):\n        return False\n    if not re.search(r'[0-9]', password):\n        return False\n    if not re.search(r'[!@#$%^&*]', password):\n        return False\n    return True\n\n# Session with timeout\nsession['user_id'] = user.id\nsession.permanent = True\napp.permanent_session_lifetime = timedelta(hours=1)\n```\n\n### Broken Access Control\n\n**❌ Vulnerable Code:**\n```python\n# No authorization check\n@app.route('/user/<user_id>/profile')\ndef view_profile(user_id):\n    user = User.query.get(user_id)\n    return render_template('profile.html', user=user)\n# Any logged-in user can view any profile!\n\n# Insecure direct object reference\n@app.route('/delete/<order_id>')\ndef delete_order(order_id):\n    Order.query.filter_by(id=order_id).delete()\n# No check if user owns the order!\n```\n\n**✅ Secure Code:**\n```python\n# Proper authorization\n@app.route('/user/<user_id>/profile')\n@login_required\ndef view_profile(user_id):\n    if current_user.id != user_id and not current_user.is_admin:\n        abort(403)  # Forbidden\n    user = User.query.get_or_404(user_id)\n    return render_template('profile.html', user=user)\n\n# Verify ownership\n@app.route('/delete/<order_id>')\n@login_required\ndef delete_order(order_id):\n    order = Order.query.get_or_404(order_id)\n    if order.user_id != current_user.id:\n        abort(403)\n    order.delete()\n    return redirect('/orders')\n```\n\n---\n\n## Sensitive Data Exposure\n\n### Hardcoded Secrets\n\n**❌ Vulnerable Code:**\n```python\n# Hardcoded credentials\nAPI_KEY = \"sk-abc123def456\"\nDATABASE_URL = \"postgresql://admin:password123@localhost/db\"\n\n# Committed .env file with secrets\n# .env (in git)\nSECRET_KEY=super-secret-key-12345\n```\n\n**✅ Secure Code:**\n```python\n# Environment variables\nimport os\nAPI_KEY = os.environ.get('API_KEY')\nDATABASE_URL = os.environ.get('DATABASE_URL')\n\n# Secret management service\nfrom azure.identity import DefaultAzureCredential\nfrom azure.keyvault.secrets import SecretClient\n\ncredential = DefaultAzureCredential()\nclient = SecretClient(vault_url=\"https://myvault.vault.azure.net/\", credential=credential)\napi_key = client.get_secret(\"api-key\").value\n\n# .gitignore includes\n.env\n.env.local\nsecrets.json\ncredentials.json\n```\n\n### Information Disclosure\n\n**❌ Vulnerable Code:**\n```python\n# Detailed error messages to users\ntry:\n    process_payment(card_number)\nexcept Exception as e:\n    return f\"Error: {str(e)}\\nStack trace: {traceback.format_exc()}\"\n\n# Sensitive data in logs\nlogger.info(f\"User {username} logged in with password {password}\")\nlogger.debug(f\"Credit card: {card_number}\")\n```\n\n**✅ Secure Code:**\n```python\n# Generic error messages\ntry:\n    process_payment(card_number)\nexcept PaymentError as e:\n    logger.error(f\"Payment failed for user {user_id}: {e}\", exc_info=True)\n    return \"Payment processing failed. Please try again.\"\nexcept Exception as e:\n    logger.critical(f\"Unexpected error: {e}\", exc_info=True)\n    return \"An error occurred. Please contact support.\"\n\n# Sanitized logging\nlogger.info(f\"User {username} logged in successfully\")\nlogger.debug(f\"Credit card ending in: {card_number[-4:]}\")\n# Better: Don't log sensitive data at all\n```\n\n### Unencrypted Data Storage\n\n**❌ Vulnerable Code:**\n```python\n# Storing sensitive data in plain text\nuser.ssn = request.form['ssn']\nuser.save()\n\n# No encryption at rest\nwith open('sensitive_data.txt', 'w') as f:\n    f.write(credit_card_number)\n```\n\n**✅ Secure Code:**\n```python\n# Encrypt sensitive fields\nfrom cryptography.fernet import Fernet\n\nclass User(db.Model):\n    ssn_encrypted = db.Column(db.LargeBinary)\n\n    @property\n    def ssn(self):\n        cipher = Fernet(encryption_key)\n        return cipher.decrypt(self.ssn_encrypted).decode()\n\n    @ssn.setter\n    def ssn(self, value):\n        cipher = Fernet(encryption_key)\n        self.ssn_encrypted = cipher.encrypt(value.encode())\n\n# Use database encryption at rest\n# PostgreSQL: Enable transparent data encryption\n# MySQL: Use encrypted tablespaces\n```\n\n---\n\n## Security Misconfiguration\n\n### Debug Mode in Production\n\n**❌ Vulnerable Code:**\n```python\n# Flask - Debug mode enabled\napp.run(debug=True)\n\n# Django - Debug in settings.py\nDEBUG = True\nALLOWED_HOSTS = ['*']\n```\n\n**✅ Secure Code:**\n```python\n# Flask - Environment-based debug\ndebug_mode = os.environ.get('FLASK_ENV') == 'development'\napp.run(debug=debug_mode)\n\n# Django - Separate settings\n# settings/production.py\nDEBUG = False\nALLOWED_HOSTS = ['example.com', 'www.example.com']\n\n# settings/development.py\nDEBUG = True\nALLOWED_HOSTS = ['localhost', '127.0.0.1']\n```\n\n### CORS Misconfiguration\n\n**❌ Vulnerable Code:**\n```python\n# Flask-CORS - Allow all origins\nfrom flask_cors import CORS\nCORS(app, resources={r\"/*\": {\"origins\": \"*\"}})\n\n# Express.js - Wildcard origin\napp.use(cors({\n  origin: '*',\n  credentials: true  // Dangerous with wildcard!\n}));\n```\n\n**✅ Secure Code:**\n```python\n# Flask-CORS - Specific origins\nCORS(app, resources={\n    r\"/api/*\": {\n        \"origins\": [\"https://example.com\", \"https://app.example.com\"],\n        \"methods\": [\"GET\", \"POST\"],\n        \"allow_headers\": [\"Content-Type\", \"Authorization\"]\n    }\n})\n\n# Express.js - Whitelist\nconst allowedOrigins = ['https://example.com'];\napp.use(cors({\n  origin: function(origin, callback) {\n    if (!origin || allowedOrigins.includes(origin)) {\n      callback(null, true);\n    } else {\n      callback(new Error('Not allowed by CORS'));\n    }\n  },\n  credentials: true\n}));\n```\n\n---\n\n## Cross-Site Scripting (XSS)\n\n### Reflected XSS\n\n**❌ Vulnerable Code:**\n```python\n# Flask - Unescaped output\n@app.route('/search')\ndef search():\n    query = request.args.get('q')\n    return f\"<h1>Search results for: {query}</h1>\"\n# User input: <script>alert('XSS')</script>\n\n# JavaScript - innerHTML with user input\nsearchInput = document.getElementById('search').value;\nresultsDiv.innerHTML = `<h2>Results for ${searchInput}</h2>`;\n```\n\n**✅ Secure Code:**\n```python\n# Flask - Auto-escaped templates\nfrom flask import render_template, escape\n\n@app.route('/search')\ndef search():\n    query = request.args.get('q')\n    return render_template('search.html', query=query)\n# In template: <h1>Search results for: {{ query }}</h1>\n\n# Manual escaping\nreturn f\"<h1>Search results for: {escape(query)}</h1>\"\n\n# JavaScript - textContent instead of innerHTML\nsearchInput = document.getElementById('search').value;\nresultsDiv.textContent = `Results for ${searchInput}`;\n\n// Or sanitize with DOMPurify\nimport DOMPurify from 'dompurify';\nresultsDiv.innerHTML = DOMPurify.sanitize(`<h2>Results for ${searchInput}</h2>`);\n```\n\n### Content Security Policy\n\n**✅ Secure Code:**\n```python\n# Flask - Add CSP header\n@app.after_request\ndef set_csp(response):\n    response.headers['Content-Security-Policy'] = (\n        \"default-src 'self'; \"\n        \"script-src 'self' https://cdn.example.com; \"\n        \"style-src 'self' 'unsafe-inline'; \"\n        \"img-src 'self' data: https:; \"\n        \"font-src 'self'; \"\n        \"connect-src 'self' https://api.example.com; \"\n        \"frame-ancestors 'none';\"\n    )\n    return response\n```\n\n---\n\n## Insecure Deserialization\n\n**❌ Vulnerable Code:**\n```python\n# Python - Unsafe pickle\nimport pickle\nuser_data = pickle.loads(request.data)\n\n# JavaScript - eval of JSON\nconst data = eval('(' + userInput + ')');\n\n# PHP - unserialize\n$data = unserialize($_POST['data']);\n```\n\n**✅ Secure Code:**\n```python\n# Python - Use JSON instead\nimport json\ntry:\n    user_data = json.loads(request.data)\nexcept json.JSONDecodeError:\n    return \"Invalid data\", 400\n\n# If pickle is necessary, sign it\nimport hmac\nimport hashlib\n\ndef secure_serialize(obj, secret_key):\n    pickled = pickle.dumps(obj)\n    signature = hmac.new(secret_key.encode(), pickled, hashlib.sha256).hexdigest()\n    return signature + pickled.hex()\n\ndef secure_deserialize(data, secret_key):\n    signature = data[:64]\n    pickled = bytes.fromhex(data[64:])\n    expected_sig = hmac.new(secret_key.encode(), pickled, hashlib.sha256).hexdigest()\n    if not hmac.compare_digest(signature, expected_sig):\n        raise ValueError(\"Invalid signature\")\n    return pickle.loads(pickled)\n\n# JavaScript - JSON.parse (not eval)\nconst data = JSON.parse(userInput);\n```\n\n---\n\n## Cryptographic Issues\n\n### Weak Encryption\n\n**❌ Vulnerable Code:**\n```python\n# Weak hashing algorithm\nimport hashlib\ntoken = hashlib.md5(data.encode()).hexdigest()\n\n# Custom encryption (never do this!)\ndef custom_encrypt(text, shift):\n    return ''.join(chr(ord(c) + shift) for c in text)\n\n# Weak key generation\nsecret_key = \"12345678\"\n```\n\n**✅ Secure Code:**\n```python\n# Strong hashing with salt\nimport hashlib\nimport secrets\n\nsalt = secrets.token_hex(16)\nhash_obj = hashlib.pbkdf2_hmac('sha256', password.encode(), salt.encode(), 100000)\npassword_hash = salt + hash_obj.hex()\n\n# Use established crypto libraries\nfrom cryptography.fernet import Fernet\n\n# Generate strong key\nkey = Fernet.generate_key()\n\n# Encrypt\ncipher = Fernet(key)\nencrypted = cipher.encrypt(data.encode())\n\n# Decrypt\ndecrypted = cipher.decrypt(encrypted).decode()\n\n# Secure random for tokens\ntoken = secrets.token_urlsafe(32)\n```\n\n### Insufficient Randomness\n\n**❌ Vulnerable Code:**\n```python\n# Weak random for security purposes\nimport random\nsession_id = random.randint(1000000, 9999999)\nreset_token = str(random.random())\n```\n\n**✅ Secure Code:**\n```python\n# Cryptographically secure random\nimport secrets\n\nsession_id = secrets.token_hex(32)\nreset_token = secrets.token_urlsafe(32)\nrandom_number = secrets.randbelow(1000000)\n```\n\n---\n\n## Security Headers\n\n### Essential Security Headers\n\n```python\n# Flask - Comprehensive security headers\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.after_request\ndef set_security_headers(response):\n    # Prevent MIME sniffing\n    response.headers['X-Content-Type-Options'] = 'nosniff'\n\n    # Enable XSS protection\n    response.headers['X-XSS-Protection'] = '1; mode=block'\n\n    # Prevent clickjacking\n    response.headers['X-Frame-Options'] = 'DENY'\n\n    # Force HTTPS\n    response.headers['Strict-Transport-Security'] = 'max-age=31536000; includeSubDomains'\n\n    # Referrer policy\n    response.headers['Referrer-Policy'] = 'strict-origin-when-cross-origin'\n\n    # Permissions policy\n    response.headers['Permissions-Policy'] = 'geolocation=(), microphone=(), camera=()'\n\n    return response\n```\n\n---\n\n## Quick Security Checklist\n\n- [ ] No SQL injection (use parameterized queries)\n- [ ] No command injection (avoid shell=True, validate input)\n- [ ] No XSS (escape output, use CSP)\n- [ ] No hardcoded secrets (use environment variables)\n- [ ] Strong password hashing (bcrypt, argon2)\n- [ ] Proper authentication checks\n- [ ] Authorization on all resources\n- [ ] HTTPS enforced\n- [ ] Security headers configured\n- [ ] CORS properly configured\n- [ ] Sensitive data encrypted\n- [ ] No debug mode in production\n- [ ] Dependencies scanned for vulnerabilities\n- [ ] Input validation on all user input\n- [ ] Secure session management\n\n---\n\n*Stay updated with OWASP Top 10 and security best practices for your language/framework.*\n",
        "icartsh-plugin/skills/code-reviewer/references/performance_guide.md": "# Performance Optimization Guide\n\nComprehensive guide for identifying and fixing performance issues during code review.\n\n## Table of Contents\n\n1. [Algorithm Efficiency](#algorithm-efficiency)\n2. [Database Optimization](#database-optimization)\n3. [Memory Management](#memory-management)\n4. [Network & I/O](#network--io)\n5. [Caching Strategies](#caching-strategies)\n6. [Concurrency & Parallelism](#concurrency--parallelism)\n7. [Language-Specific Optimizations](#language-specific-optimizations)\n\n---\n\n## Algorithm Efficiency\n\n### Big-O Analysis\n\n**❌ Inefficient - O(n²):**\n```python\n# Nested loops checking duplicates\ndef has_duplicates(items):\n    for i in range(len(items)):\n        for j in range(i + 1, len(items)):\n            if items[i] == items[j]:\n                return True\n    return False\n```\n\n**✅ Optimized - O(n):**\n```python\n# Using set for O(1) lookup\ndef has_duplicates(items):\n    seen = set()\n    for item in items:\n        if item in seen:\n            return True\n        seen.add(item)\n    return False\n\n# Even simpler\ndef has_duplicates(items):\n    return len(items) != len(set(items))\n```\n\n### Common Optimizations\n\n**Linear Search → Binary Search**\n```python\n# ❌ O(n) - Linear search\ndef find_item(sorted_list, target):\n    for i, item in enumerate(sorted_list):\n        if item == target:\n            return i\n    return -1\n\n# ✅ O(log n) - Binary search\nimport bisect\ndef find_item(sorted_list, target):\n    index = bisect.bisect_left(sorted_list, target)\n    if index < len(sorted_list) and sorted_list[index] == target:\n        return index\n    return -1\n```\n\n**List Comprehension vs Loop**\n```python\n# ❌ Slower - Building list with append\nresult = []\nfor i in range(1000):\n    if i % 2 == 0:\n        result.append(i * 2)\n\n# ✅ Faster - List comprehension\nresult = [i * 2 for i in range(1000) if i % 2 == 0]\n\n# ✅ Even better for large datasets - Generator\nresult = (i * 2 for i in range(1000000) if i % 2 == 0)\n```\n\n**Unnecessary Sorting**\n```python\n# ❌ O(n log n) - Full sort to find max\ndef find_max(items):\n    return sorted(items)[-1]\n\n# ✅ O(n) - Direct max\ndef find_max(items):\n    return max(items)\n```\n\n---\n\n## Database Optimization\n\n### N+1 Query Problem\n\n**❌ N+1 Queries:**\n```python\n# SQLAlchemy - Lazy loading causes N+1\nusers = User.query.all()  # 1 query\nfor user in users:\n    print(user.orders)  # N queries!\n```\n\n**✅ Eager Loading:**\n```python\n# SQLAlchemy - Eager loading with joinedload\nfrom sqlalchemy.orm import joinedload\n\nusers = User.query.options(joinedload(User.orders)).all()  # 1 query\nfor user in users:\n    print(user.orders)  # No additional queries\n```\n\n### SELECT * Optimization\n\n**❌ Selecting All Columns:**\n```sql\n-- Returns 50+ columns, only need 3\nSELECT * FROM users WHERE status = 'active';\n```\n\n**✅ Specific Columns:**\n```sql\n-- Only select needed columns\nSELECT id, name, email FROM users WHERE status = 'active';\n```\n\n```python\n# SQLAlchemy - Specify columns\nusers = db.session.query(User.id, User.name, User.email)\\\n    .filter_by(status='active').all()\n```\n\n### Index Usage\n\n**❌ Missing Index:**\n```sql\n-- Slow full table scan\nSELECT * FROM orders WHERE user_id = 123 AND status = 'pending';\n-- No index on (user_id, status)\n```\n\n**✅ Proper Indexing:**\n```sql\n-- Create composite index (order matters!)\nCREATE INDEX idx_orders_user_status ON orders(user_id, status);\n\n-- Now fast index scan\nSELECT * FROM orders WHERE user_id = 123 AND status = 'pending';\n```\n\n**Index Guidelines:**\n- Index columns used in WHERE clauses\n- Index foreign keys\n- Composite indexes: most selective column first\n- Don't over-index (impacts writes)\n\n### Query Optimization\n\n**❌ Inefficient Subquery:**\n```sql\n-- Correlated subquery runs for each row\nSELECT name,\n       (SELECT COUNT(*) FROM orders WHERE user_id = users.id) as order_count\nFROM users;\n```\n\n**✅ JOIN Instead:**\n```sql\n-- Single query with JOIN\nSELECT u.name, COUNT(o.id) as order_count\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nGROUP BY u.id, u.name;\n```\n\n### Pagination\n\n**❌ Loading All Records:**\n```python\n# Loads entire table into memory\nall_users = User.query.all()\nreturn jsonify([u.to_dict() for u in all_users])\n```\n\n**✅ Pagination:**\n```python\n# Load only requested page\npage = request.args.get('page', 1, type=int)\nper_page = 20\n\nusers = User.query.paginate(page=page, per_page=per_page, error_out=False)\nreturn jsonify({\n    'items': [u.to_dict() for u in users.items],\n    'total': users.total,\n    'page': page,\n    'pages': users.pages\n})\n```\n\n---\n\n## Memory Management\n\n### Memory Leaks\n\n**❌ Unclosed Resources:**\n```python\n# File handle not closed\ndef read_file(filename):\n    f = open(filename)\n    data = f.read()\n    return data  # File left open!\n```\n\n**✅ Proper Resource Management:**\n```python\n# Context manager ensures cleanup\ndef read_file(filename):\n    with open(filename) as f:\n        data = f.read()\n    return data  # File closed automatically\n```\n\n### Large Data Processing\n\n**❌ Loading Everything into Memory:**\n```python\n# Loads 10GB file into memory\nwith open('large_file.csv') as f:\n    lines = f.readlines()  # All lines in memory!\n    for line in lines:\n        process(line)\n```\n\n**✅ Streaming/Chunking:**\n```python\n# Process line by line\nwith open('large_file.csv') as f:\n    for line in f:  # One line at a time\n        process(line)\n\n# Or use pandas chunking\nimport pandas as pd\nfor chunk in pd.read_csv('large_file.csv', chunksize=10000):\n    process(chunk)\n```\n\n### Object Pooling\n\n**❌ Creating New Objects Repeatedly:**\n```python\ndef process_requests():\n    for request in requests:\n        connection = create_db_connection()  # New connection each time\n        result = connection.query(request)\n        connection.close()\n```\n\n**✅ Connection Pooling:**\n```python\nfrom sqlalchemy import create_engine, pool\n\nengine = create_engine(\n    'postgresql://user:pass@localhost/db',\n    poolclass=pool.QueuePool,\n    pool_size=10,\n    max_overflow=20\n)\n\ndef process_requests():\n    with engine.connect() as connection:\n        for request in requests:\n            result = connection.execute(request)\n```\n\n---\n\n## Network & I/O\n\n### Asynchronous Operations\n\n**❌ Blocking Synchronous Calls:**\n```python\nimport requests\n\ndef fetch_all_data(urls):\n    results = []\n    for url in urls:\n        response = requests.get(url)  # Blocks for each request\n        results.append(response.json())\n    return results\n```\n\n**✅ Async/Concurrent:**\n```python\nimport asyncio\nimport aiohttp\n\nasync def fetch_url(session, url):\n    async with session.get(url) as response:\n        return await response.json()\n\nasync def fetch_all_data(urls):\n    async with aiohttp.ClientSession() as session:\n        tasks = [fetch_url(session, url) for url in urls]\n        results = await asyncio.gather(*tasks)\n    return results\n\n# Or using ThreadPoolExecutor for requests\nfrom concurrent.futures import ThreadPoolExecutor\nimport requests\n\ndef fetch_url(url):\n    return requests.get(url).json()\n\ndef fetch_all_data(urls):\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        results = list(executor.map(fetch_url, urls))\n    return results\n```\n\n### Timeout Configuration\n\n**❌ No Timeout:**\n```python\n# Can hang indefinitely\nresponse = requests.get('https://api.example.com/data')\n```\n\n**✅ Timeouts Configured:**\n```python\n# Connection timeout: 3s, Read timeout: 10s\nresponse = requests.get(\n    'https://api.example.com/data',\n    timeout=(3, 10)\n)\n```\n\n### Batch Operations\n\n**❌ Individual API Calls:**\n```python\n# Makes 1000 API calls\nfor item_id in item_ids:\n    api.get(f'/items/{item_id}')\n```\n\n**✅ Batch Endpoint:**\n```python\n# Single API call with batch\napi.post('/items/batch', json={'ids': item_ids})\n```\n\n---\n\n## Caching Strategies\n\n### Function Memoization\n\n**❌ Recalculating:**\n```python\ndef fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)  # Exponential time!\n```\n\n**✅ Memoization:**\n```python\nfrom functools import lru_cache\n\n@lru_cache(maxsize=128)\ndef fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)  # Now O(n)\n```\n\n### Application-Level Caching\n\n**❌ No Caching:**\n```python\n@app.route('/popular-products')\ndef popular_products():\n    # Expensive query runs every request\n    products = db.session.query(Product)\\\n        .join(OrderItem)\\\n        .group_by(Product.id)\\\n        .order_by(func.count(OrderItem.id).desc())\\\n        .limit(10)\\\n        .all()\n    return jsonify([p.to_dict() for p in products])\n```\n\n**✅ Redis Caching:**\n```python\nimport redis\nimport json\n\ncache = redis.Redis(host='localhost', port=6379, db=0)\n\n@app.route('/popular-products')\ndef popular_products():\n    # Check cache first\n    cached = cache.get('popular_products')\n    if cached:\n        return cached\n\n    # Query if not cached\n    products = db.session.query(Product)\\\n        .join(OrderItem)\\\n        .group_by(Product.id)\\\n        .order_by(func.count(OrderItem.id).desc())\\\n        .limit(10)\\\n        .all()\n\n    result = jsonify([p.to_dict() for p in products])\n\n    # Cache for 5 minutes\n    cache.setex('popular_products', 300, result.get_data())\n\n    return result\n```\n\n### Cache Invalidation\n\n```python\n# Invalidate cache on update\n@app.route('/products/<int:product_id>', methods=['PUT'])\ndef update_product(product_id):\n    product = Product.query.get_or_404(product_id)\n    product.update(request.json)\n    db.session.commit()\n\n    # Invalidate related caches\n    cache.delete('popular_products')\n    cache.delete(f'product:{product_id}')\n\n    return jsonify(product.to_dict())\n```\n\n---\n\n## Concurrency & Parallelism\n\n### Thread Pool for I/O-Bound Tasks\n\n**❌ Sequential Processing:**\n```python\ndef process_images(image_paths):\n    results = []\n    for path in image_paths:\n        result = download_and_resize(path)  # I/O bound\n        results.append(result)\n    return results\n```\n\n**✅ Thread Pool:**\n```python\nfrom concurrent.futures import ThreadPoolExecutor\n\ndef process_images(image_paths):\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        results = list(executor.map(download_and_resize, image_paths))\n    return results\n```\n\n### Process Pool for CPU-Bound Tasks\n\n**❌ Single Process:**\n```python\ndef analyze_data(datasets):\n    results = []\n    for data in datasets:\n        result = heavy_computation(data)  # CPU intensive\n        results.append(result)\n    return results\n```\n\n**✅ Process Pool:**\n```python\nfrom multiprocessing import Pool\n\ndef analyze_data(datasets):\n    with Pool(processes=4) as pool:\n        results = pool.map(heavy_computation, datasets)\n    return results\n```\n\n---\n\n## Language-Specific Optimizations\n\n### Python\n\n**String Concatenation:**\n```python\n# ❌ Slow - Creates new string each iteration\nresult = \"\"\nfor item in items:\n    result += str(item)  # O(n²)\n\n# ✅ Fast - Join is O(n)\nresult = \"\".join(str(item) for item in items)\n```\n\n**Dictionary Lookups:**\n```python\n# ❌ Checking key multiple times\nif 'key' in my_dict:\n    value = my_dict['key']\n\n# ✅ EAFP (Easier to Ask Forgiveness than Permission)\ntry:\n    value = my_dict['key']\nexcept KeyError:\n    value = default\n\n# ✅ Or use get()\nvalue = my_dict.get('key', default)\n```\n\n**List vs Set Membership:**\n```python\n# ❌ O(n) - List membership test\nvalid_ids = [1, 2, 3, 4, 5, ..., 10000]\nif user_id in valid_ids:  # Slow for large lists\n    process()\n\n# ✅ O(1) - Set membership test\nvalid_ids = {1, 2, 3, 4, 5, ..., 10000}\nif user_id in valid_ids:  # Fast\n    process()\n```\n\n### JavaScript\n\n**Array Operations:**\n```javascript\n// ❌ Mutating array in place inefficiently\nlet filtered = [];\nfor (let i = 0; i < items.length; i++) {\n    if (items[i].active) {\n        filtered.push(items[i]);\n    }\n}\n\n// ✅ Use built-in methods\nlet filtered = items.filter(item => item.active);\n```\n\n**Object Lookup:**\n```javascript\n// ❌ Array.find for repeated lookups\nusers.forEach(user => {\n    const department = departments.find(d => d.id === user.deptId);  // O(n) each time\n});\n\n// ✅ Create lookup map\nconst deptMap = new Map(departments.map(d => [d.id, d]));\nusers.forEach(user => {\n    const department = deptMap.get(user.deptId);  // O(1)\n});\n```\n\n### Go\n\n**Preallocate Slices:**\n```go\n// ❌ Growing slice incrementally\nvar items []Item\nfor i := 0; i < 1000; i++ {\n    items = append(items, generateItem(i))  // Multiple reallocations\n}\n\n// ✅ Preallocate capacity\nitems := make([]Item, 0, 1000)\nfor i := 0; i < 1000; i++ {\n    items = append(items, generateItem(i))  // No reallocation\n}\n```\n\n---\n\n## Performance Profiling\n\n### Python Profiling\n\n```python\nimport cProfile\nimport pstats\n\n# Profile a function\ndef profile_function():\n    profiler = cProfile.Profile()\n    profiler.enable()\n\n    # Code to profile\n    result = expensive_function()\n\n    profiler.disable()\n    stats = pstats.Stats(profiler)\n    stats.sort_stats('cumulative')\n    stats.print_stats(10)  # Top 10 functions\n\n# Line profiler for detailed analysis\n# Install: pip install line_profiler\n# Usage: kernprof -l -v script.py\n@profile\ndef expensive_function():\n    # Code here\n    pass\n```\n\n### Memory Profiling\n\n```python\n# Install: pip install memory_profiler\nfrom memory_profiler import profile\n\n@profile\ndef memory_intensive_function():\n    large_list = [i for i in range(10000000)]\n    return sum(large_list)\n```\n\n---\n\n## Quick Performance Checklist\n\n- [ ] No O(n²) or worse algorithms when better exists\n- [ ] Database queries are indexed\n- [ ] No N+1 query problems\n- [ ] Pagination for large datasets\n- [ ] Resources properly closed (files, connections)\n- [ ] Async/concurrent for I/O operations\n- [ ] Appropriate caching strategy\n- [ ] No blocking operations in hot paths\n- [ ] Connection pooling configured\n- [ ] Timeouts set for network calls\n- [ ] Batch operations where possible\n- [ ] Memory-efficient data structures chosen\n- [ ] Profiling done for critical paths\n\n---\n\n*Always measure before optimizing. Use profiling tools to identify actual bottlenecks.*\n",
        "icartsh-plugin/skills/coding-conventions/SKILL.md": "---\nname: coding-conventions\ndescription: .NET/C#의 코딩 규약, 명명 규칙, 레이아웃, C# 12/13/14의 최신 기능 활용 가이드라인을 정의합니다. C#/.NET 코드 작성 시, 클래스·메서드 명명 시, 코드 포맷팅 시, 또는 사용자가 코딩 규약, 명명 규칙, C# 모범 사례, Primary Constructors, Collection Expressions, field 키워드에 대해 언급했을 때 사용합니다.\n---\n\n# Coding Conventions\n\n## 개요\n\n이 SKILL은 개발되는 모든 .NET 프로젝트에 적용되는 코딩 규약을 정의합니다. .NET 8 이후의 최신 기능(C# 12/13/14, .NET 8/9/10)을 적극적으로 활용하여 가독성, 유지보수성, 성능이 높은 코드를 구현하는 것을 목적으로 합니다.\n\n## 책임 범위\n\n이 SKILL은 다음 범위를 다룹니다:\n\n- .NET/C#의 최신 기능(C# 12/13/14, .NET 8/9/10) 활용 방침\n- 명명 규칙 (Type, Member, Variable, Parameter)\n- 코드 레이아웃 및 포맷\n- 언어 기능 사용 방침 (Type inference, Collection, Exception handling)\n- LINQ와 람다식의 모범 사례\n- 모던 C# 구문의 권장 패턴\n\n## 기본 방침\n\n- .NET 8 이후의 최신 기능을 적극적으로 사용한다 (C# 12/13/14, .NET 8/9/10)\n- 이전 버전과의 호환성이 필요한 경우를 제외하고 항상 최신 기능을 우선한다\n- 오래된 언어 구문은 피한다\n- **중요: 언더스코어 접두사(`_field`) 사용은 절대 금지한다**\n- **중요: 중괄호 생략은 절대 금지한다 (1행으로 기술할 수 있는 경우에도 생략 불가)**\n- Microsoft 공식 코딩 규약을 따른다\n- 일관성을 유지하고 팀 전체에서 동일한 스타일을 적용한다\n\n## .NET/C# 최신 기능 (버전별)\n\n.NET 8 이후 각 버전에서 도입된 주요 기능을 적극적으로 활용합니다. 이전 버전과의 호환성이 필요한 경우를 제외하고 항상 최신 기능을 우선적으로 사용합니다.\n\n### C# 12 (.NET 8) - 2023년 11월 공식 릴리스\n\n#### Primary Constructors\n\n- 클래스나 struct 선언에서 파라미터를 정의하고 클래스 전체에서 사용할 수 있음\n- 명시적인 필드 선언을 줄이고 초기화를 간소화함\n\n좋은 예:\n\n```csharp\npublic class Person(string name, int age)\n{\n    public string Name => name;\n    public int Age => age;\n\n    public void Display()\n    {\n        Console.WriteLine($\"{name} is {age} years old\");\n    }\n}\n```\n\n나쁜 예:\n\n```csharp\npublic class Person\n{\n    private string name;\n    private int age;\n\n    public Person(string name, int age)\n    {\n        this.name = name;\n        this.age = age;\n    }\n\n    public string Name => name;\n    public int Age => age;\n}\n```\n\n#### Collection Expressions\n\n- 대괄호와 스프레드 연산자를 사용하여 컬렉션을 간결하게 생성함\n- 여러 컬렉션을 결합할 때 유용함\n\n좋은 예:\n\n```csharp\nint[] array = [1, 2, 3, 4, 5];\nList<string> list = [\"one\", \"two\", \"three\"];\n\nint[] row0 = [1, 2, 3];\nint[] row1 = [4, 5, 6];\n\n// 스프레드 연산자로 결합\nint[] combined = [..row0, ..row1];\n```\n\n#### Default Lambda Parameters\n\n- 람다식에 기본 파라미터 값을 지정할 수 있음\n\n좋은 예:\n\n```csharp\nvar incrementBy = (int source, int increment = 1) => source + increment;\n\nConsole.WriteLine(incrementBy(5));\nConsole.WriteLine(incrementBy(5, 3));\n```\n\n#### Alias Any Type\n\n- using 디렉티브로 복잡한 타입에 별칭을 붙일 수 있음\n\n좋은 예:\n\n```csharp\nusing Point = (int x, int y);\nusing ProductList = System.Collections.Generic.List<(string Name, decimal Price)>;\n\nPoint origin = (0, 0);\nProductList products = [(\"Product1\", 100m), (\"Product2\", 200m)];\n```\n\n### C# 13 (.NET 9) - 2024년 11월 공식 릴리스\n\n#### Params Collections\n\n- `params` 수식어를 배열 외의 컬렉션 타입에서도 사용 가능해짐\n- `List<T>`, `Span<T>`, `ReadOnlySpan<T>`, `IEnumerable<T>` 등에서 사용 가능\n\n좋은 예:\n\n```csharp\npublic void ProcessItems(params List<string> items)\n{\n    foreach (var item in items)\n    {\n        Console.WriteLine(item);\n    }\n}\n\n// 메모리 효율이 중요한 경우\npublic void ProcessData(params ReadOnlySpan<int> data)\n{\n    foreach (var value in data)\n    {\n        Process(value);\n    }\n}\n```\n\n#### New Lock Type\n\n- `System.Threading.Lock` 타입을 사용하여 더 빠른 스레드 동기화를 구현함\n- 기존 `Monitor` 기반 락보다 빠름\n\n좋은 예:\n\n```csharp\nprivate readonly Lock lockObject = new();\n\npublic void UpdateData()\n{\n    lock (lockObject)\n    {\n        // Critical section\n    }\n}\n```\n\n나쁜 예:\n\n```csharp\n// 기존 object 기반 락 (C# 13에서는 권장되지 않음)\nprivate readonly object lockObject = new();\n\npublic void UpdateData()\n{\n    lock (lockObject)\n    {\n        // Critical section\n    }\n}\n```\n\n#### Partial Properties and Indexers\n\n- partial 프로퍼티와 인덱서를 사용할 수 있게 됨\n- 정의와 구현을 분리할 수 있음\n\n좋은 예:\n\n```csharp\n// 정의 부분\npublic partial class DataModel\n{\n    public partial string Name { get; set; }\n}\n\n// 구현 부분\npublic partial class DataModel\n{\n    private string name;\n\n    public partial string Name\n    {\n        get => name;\n        set => name = value ?? throw new ArgumentNullException(nameof(value));\n    }\n}\n```\n\n#### Implicit Index Access\n\n- 객체 이니셜라이저에서 `^` 연산자를 사용할 수 있게 됨\n\n좋은 예:\n\n```csharp\nvar countdown = new TimerBuffer\n{\n    buffer =\n    {\n        [^1] = 0,\n        [^2] = 1,\n        [^3] = 2\n    }\n};\n```\n\n#### Ref Struct Enhancements\n\n- `ref struct` 타입이 인터페이스를 구현할 수 있게 됨\n- 제네릭 타입에서 `ref struct`를 사용할 수 있게 됨 (`allows ref struct` 제약 조건)\n\n좋은 예:\n\n```csharp\npublic ref struct SpanWrapper<T> : IEnumerable<T>\n{\n    private Span<T> span;\n\n    public IEnumerator<T> GetEnumerator()\n    {\n        foreach (var item in span)\n        {\n            yield return item;\n        }\n    }\n}\n```\n\n### C# 14 (.NET 10) - 2025년 11월 릴리스 예정\n\n#### Extension Members\n\n- Extension Members를 활용하여 깔끔한 API 확장을 구현함\n- 원래 타입을 오염시키지 않고 기능을 추가할 수 있음\n\n좋은 예:\n\n```csharp\nextension<TSource>(IEnumerable<TSource> source)\n{\n    public bool IsEmpty => !source.Any();\n    public int Count => source.Count();\n}\n```\n\n#### Field-Backed Properties\n\n- `field` 키워드를 사용하여 명시적인 backing field를 줄임\n- 검증 로직을 간결하게 기술할 수 있음\n- **언더스코어 접두사를 사용한 명시적인 backing field는 절대 금지**\n\n좋은 예:\n\n```csharp\n// C# 14의 field 키워드 사용\npublic string Name\n{\n    get;\n    set => field = value ?? throw new ArgumentNullException(nameof(value));\n}\n\n// 어쩔 수 없이 명시적인 backing field가 필요한 경우에도 언더스코어 없음\nprivate string name;\n\npublic string Name\n{\n    get => name;\n    set => name = value ?? throw new ArgumentNullException(nameof(value));\n}\n```\n\n나쁜 예:\n\n```csharp\n// 언더스코어 접두사는 절대 금지\nprivate string _name;\n\npublic string Name\n{\n    get => _name;\n    set => _name = value ?? throw new ArgumentNullException(nameof(value));\n}\n```\n\n#### Null-Conditional Assignment\n\n- `?.`를 사용하여 null 체크를 간결하게 기술함\n- 중복되는 null 체크를 줄임\n\n좋은 예:\n\n```csharp\ncustomer?.Order = GetCurrentOrder();\n```\n\n나쁜 예:\n\n```csharp\nif (customer != null)\n{\n    customer.Order = GetCurrentOrder();\n}\n```\n\n#### Implicit Span Conversions\n\n- 성능 중시 코드에서는 `Span<T>`와 `ReadOnlySpan<T>`를 활용함\n- 배열과 스팬 타입 간의 자동 변환을 이용함\n\n## 명명 규칙 (Naming Conventions)\n\n### Pascal Casing\n\n- 타입 이름 (class, record, struct, interface, enum)\n- 퍼블릭 멤버 (프로퍼티, 메서드, 이벤트)\n- 네임스페이스\n\n좋은 예:\n\n```csharp\npublic class CustomerOrder\n{\n    public string OrderId { get; set; }\n    public void ProcessOrder() { }\n}\n```\n\n### Camel Casing\n\n- 로컬 변수\n- 메서드 파라미터\n- 프라이빗 필드 (**언더스코어 접두사는 절대 사용하지 않음**)\n\n좋은 예:\n\n```csharp\npublic class OrderProcessor\n{\n    // 언더스코어 없음\n    private string customerName;\n\n    // 언더스코어 없음\n    private int orderCount;\n\n    public void ProcessOrder(string orderId)\n    {\n        var customerName = GetCustomerName(orderId);\n        string processedResult = Process(customerName);\n    }\n}\n```\n\n나쁜 예:\n\n```csharp\npublic class OrderProcessor\n{\n    // 언더스코어 접두사는 절대 금지\n    private string _customerName;\n\n    // 언더스코어 접두사는 절대 금지\n    private int _orderCount;\n}\n```\n\n### Interface 명명\n\n- 접두사 `I`를 사용함\n\n좋은 예:\n\n```csharp\npublic interface IOrderProcessor\n{\n    void Process(Order order);\n}\n```\n\n### 타입 파라미터 명명\n\n- 접두사 `T`를 사용함\n- 의미 있는 이름을 붙임\n\n좋은 예:\n\n```csharp\npublic class Repository<TEntity> where TEntity : class\n{\n    public void Add(TEntity entity) { }\n}\n```\n\n## 코드 레이아웃 (Code Layout)\n\n### 들여쓰기\n\n- 공백(Space) 4개를 사용함\n- 탭(Tab)은 사용하지 않음\n\n### 중괄호 (Curly Braces)\n\n- Allman 스타일 (시작 중괄호와 종료 중괄호를 별도의 행에 배치)\n- **중괄호 생략은 절대 금지 (1행으로 기술할 수 있는 경우에도 반드시 중괄호 사용)**\n\n좋은 예:\n\n```csharp\npublic void ProcessOrder(Order order)\n{\n    if (order != null)\n    {\n        order.Process();\n    }\n}\n\n// 1행이라도 중괄호를 사용함\nif (isValid)\n{\n    Execute();\n}\n\nfor (int i = 0; i < 10; i++)\n{\n    Process(i);\n}\n```\n\n나쁜 예:\n\n```csharp\n// 중괄호 생략 금지\nif (isValid)\n    Execute();\n\n// 중괄호 생략 금지\nfor (int i = 0; i < 10; i++)\n    Process(i);\n\n// 중괄호 생략 금지\nif (order != null) order.Process();\n```\n\n### 행 기술\n\n- 1행에 하나의 statement만 기술함\n- 1행에 하나의 선언만 기술함\n- 메서드 정의와 프로퍼티 정의 사이에 빈 행을 하나 넣음\n\n좋은 예:\n\n```csharp\npublic class Order\n{\n    public string OrderId { get; set; }\n\n    public void Process()\n    {\n        var result = Validate();\n        Execute(result);\n    }\n\n    private bool Validate()\n    {\n        return OrderId != null;\n    }\n}\n```\n\n### 네임스페이스\n\n- 파일 스코프 네임스페이스(File-scoped namespace)를 사용함\n\n좋은 예:\n\n```csharp\nnamespace YourProject.Orders;\n\npublic class OrderProcessor\n{\n    // Implementation\n}\n```\n\n나쁜 예:\n\n```csharp\nnamespace YourProject.Orders\n{\n    public class OrderProcessor\n    {\n        // Implementation\n    }\n}\n```\n\n### using 디렉티브\n\n- 네임스페이스 선언 바깥쪽에 배치함\n- 알파벳 순으로 정렬함\n\n좋은 예:\n\n```csharp\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\n\nnamespace YourProject.Orders;\n```\n\n## 타입과 변수\n\n### 타입 지정\n\n- 언어 키워드 (`string`, `int`, `bool`)를 사용함\n- 런타임 타입 (`System.String`, `System.Int32`)은 사용하지 않음\n\n좋은 예:\n\n```csharp\nstring name = \"John\";\nint count = 10;\nbool isValid = true;\n```\n\n나쁜 예:\n\n```csharp\nString name = \"John\";\nInt32 count = 10;\nBoolean isValid = true;\n```\n\n### 타입 추론 (var)\n\n- 타입이 할당되는 내용으로부터 명백한 경우에만 `var`를 사용함\n- 기본 제공 타입(Built-in type)은 명시적으로 기술함\n\n좋은 예:\n\n```csharp\n// 명백함\nvar orders = new List<Order>();\n\n// 명백함\nvar customer = GetCustomer();\n\n// 기본 타입은 명시\nint count = 10;\n\n// 기본 타입은 명시\nstring name = \"John\";\n```\n\n나쁜 예:\n\n```csharp\n// 기본 타입에서 var는 피함\nvar count = 10;\n\n// 기본 타입에서 var는 피함\nvar name = \"John\";\n```\n\n## 문자열\n\n### 문자열 보간 (String Interpolation)\n\n- 짧은 문자열 결합에는 문자열 보간을 사용함\n\n좋은 예:\n\n```csharp\nstring message = $\"Order {orderId} processed successfully\";\n```\n\n나쁜 예:\n\n```csharp\nstring message = \"Order \" + orderId + \" processed successfully\";\n```\n\n### StringBuilder\n\n- 루프 내에서 대량의 텍스트를 추가하는 경우 `StringBuilder`를 사용함\n\n좋은 예:\n\n```csharp\nvar builder = new StringBuilder();\nfor (int i = 0; i < 1000; i++)\n{\n    builder.Append($\"Line {i}\\n\");\n}\n```\n\n### Raw String Literals\n\n- 이스케이프 시퀀스보다 Raw String Literals를 우선함\n\n좋은 예:\n\n```csharp\nstring json = \"\"\"\n{\n    \"name\": \"John\",\n    \"age\": 30\n}\n\"\"\";\n```\n\n## 컬렉션과 객체 초기화\n\n### 컬렉션 초기화\n\n- C# 12 이후의 Collection Expressions를 사용함 (앞부분의 \"C# 12 최신 기능\" 참조)\n\n### 객체 이니셜라이저 (Object Initializer)\n\n- 객체 이니셜라이저를 사용하여 생성을 간소화함\n\n좋은 예:\n\n```csharp\nvar customer = new Customer\n{\n    Name = \"John\",\n    Email = \"john@example.com\"\n};\n```\n\n## 예외 처리 (Exception Handling)\n\n### 구체적인 예외 캐치\n\n- 일반적인 `System.Exception` 대신 구체적인 예외를 캐치함\n\n좋은 예:\n\n```csharp\ntry\n{\n    ProcessOrder(order);\n}\ncatch (ArgumentNullException ex)\n{\n    Logger.Error(\"Order is null\", ex);\n}\n```\n\n나쁜 예:\n\n```csharp\ntry\n{\n    ProcessOrder(order);\n}\ncatch (Exception ex) // 너무 일반적임\n{\n    Logger.Error(\"Error\", ex);\n}\n```\n\n### using 문\n\n- try-finally 대신 `using` 문을 사용함\n\n좋은 예:\n\n```csharp\nusing var connection = new SqlConnection(connectionString);\nconnection.Open();\n// Process\n```\n\n나쁜 예:\n\n```csharp\nSqlConnection connection = null;\ntry\n{\n    connection = new SqlConnection(connectionString);\n    connection.Open();\n    // Process\n}\nfinally\n{\n    connection?.Dispose();\n}\n```\n\n## LINQ\n\n### 의미 있는 변수명\n\n- 쿼리 변수에는 의미 있는 이름을 사용함\n\n좋은 예:\n\n```csharp\nvar activeCustomers = from customer in customers\n                      where customer.IsActive\n                      select customer;\n```\n\n### 조기 필터링\n\n- `where` 절을 사용하여 조기에 데이터를 필터링함\n\n좋은 예:\n\n```csharp\nvar result = customers\n    .Where(c => c.IsActive)\n    .Select(c => c.Name)\n    .ToList();\n```\n\n### 암시적 타입 지정\n\n- LINQ 선언에서는 암시적 타입 지정을 사용함\n\n좋은 예:\n\n```csharp\nvar query = from customer in customers\n            where customer.IsActive\n            select customer;\n```\n\n## 람다식 (Lambda Expressions)\n\n### 이벤트 핸들러\n\n- 삭제가 필요 없는 핸들러에는 람다식을 사용함\n\n좋은 예:\n\n```csharp\nbutton.Click += (s, e) => ProcessClick();\n```\n\n### 파라미터 수식어\n\n- C# 14 기능을 활용하여 타입 추론을 유지하면서 수식어를 사용함\n\n좋은 예:\n\n```csharp\nTryParse<int> parse = (text, out result) => int.TryParse(text, out result);\n```\n\n## 주석 (Comments)\n\n### 단일 행 주석\n\n- 간결한 설명에는 `//`를 사용함\n- 주석 구분자 뒤에 공백을 하나 넣음\n- **주석은 반드시 단독 행에 기술함 (코드와 같은 행에 기술 금지)**\n- 주석 앞에는 빈 행을 하나 넣음\n\n좋은 예:\n\n```csharp\n// 고객 주문을 처리함\nProcessOrder(order);\n\nvar processor = new OrderProcessor();\n\n// 주문을 실행함\nvar result = processor.ProcessOrder(order);\n```\n\n나쁜 예:\n\n```csharp\nProcessOrder(order); // 고객 주문을 처리함 (코드와 같은 행은 금지)\n\nvar processor = new OrderProcessor();\n// 이 행의 앞에 빈 행이 없음 (나쁜 예)\nvar result = processor.ProcessOrder(order);\n```\n\n### XML 문서\n\n- 퍼블릭 멤버에는 XML 문서를 사용함\n\n좋은 예:\n\n```csharp\n/// <summary>\n/// 지정된 주문을 처리함\n/// </summary>\n/// <param name=\"order\">처리할 주문</param>\n/// <returns>처리 결과</returns>\npublic bool ProcessOrder(Order order)\n{\n    // Implementation\n}\n```\n\n## 정적 멤버 (Static Members)\n\n### 클래스 이름에 의한 호출\n\n- 정적 멤버는 클래스 이름을 통해 호출함\n\n좋은 예:\n\n```csharp\nvar result = OrderProcessor.ProcessOrder(order);\n```\n\n나쁜 예:\n\n```csharp\nvar processor = new OrderProcessor();\n\n// 정적 메서드를 인스턴스를 통해 호출하는 것은 오해의 소지가 있음\nvar result = processor.ProcessOrder(order);\n```\n\n## 체크리스트 (Checklist)\n\n### 코드 작성 전\n\n- [ ] .NET/C#의 최신 기능 (C# 12/13/14)을 파악하고 있음\n- [ ] 프로젝트의 타겟 프레임워크가 .NET 8 이후로 설정되어 있음\n- [ ] 명명 규칙을 이해하고 있음\n\n### 코드 작성 중\n\n**필수 규칙:**\n\n- [ ] **언더스코어 접두사를 절대 사용하지 않았음**\n- [ ] **중괄호를 생략하지 않았음 (1행이라도 반드시 사용함)**\n- [ ] **주석은 반드시 단독 행에 기술했음 (코드와 같은 행에 기술하지 않았음)**\n- [ ] **주석 앞에 빈 행을 하나 넣었음**\n\n**C# 12 이후 기능:**\n\n- [ ] Primary Constructors를 사용하고 있음 (해당하는 경우)\n- [ ] Collection Expressions를 사용하고 있음\n- [ ] Default Lambda Parameters를 활용하고 있음 (해당하는 경우)\n- [ ] Alias Any Type으로 복잡한 타입에 별칭을 붙였음 (해당하는 경우)\n\n**C# 13 이후 기능:**\n\n- [ ] Params Collections를 사용하고 있음 (해당하는 경우)\n- [ ] New Lock Type을 사용하고 있음 (스레드 동기화가 필요한 경우)\n- [ ] Partial Properties and Indexers를 활용하고 있음 (해당하는 경우)\n- [ ] Implicit Index Access를 객체 이니셜라이저에서 사용하고 있음 (해당하는 경우)\n\n**C# 14 이후 기능:**\n\n- [ ] `field` 키워드를 사용하여 backing field를 간결하게 기술했음\n- [ ] Extension Members를 활용하고 있음 (해당하는 경우)\n- [ ] Null-Conditional Assignment를 활용하고 있음\n- [ ] Lambda Parameters with Modifiers를 사용하고 있음 (해당하는 경우)\n\n**기본 규칙:**\n\n- [ ] 파일 스코프 네임스페이스를 사용하고 있음\n- [ ] 언어 키워드 (`string`, `int`)를 사용하고 있음\n- [ ] `var`를 적절히 사용하고 있음 (타입이 명백한 경우에만)\n- [ ] 문자열 보간을 사용하고 있음\n- [ ] Raw String Literals를 사용하고 있음 (해당하는 경우)\n- [ ] Object Initializers를 사용하고 있음\n- [ ] `using` 문을 사용하고 있음\n- [ ] 구체적인 예외를 캐치하고 있음\n- [ ] LINQ 식에서 조기 필터링을 실시하고 있음\n- [ ] 의미 있는 변수명을 사용하고 있음\n- [ ] 주석이 간결하고 명확함\n- [ ] 퍼블릭 멤버에 XML 문서를 기술했음\n- [ ] Allman 스타일로 중괄호를 배치했음\n- [ ] 들여쓰기에 공백 4개를 사용하고 있음\n\n### 코드 작성 후\n\n- [ ] 코드가 일관된 스타일로 작성되었음\n- [ ] .NET 8 이후의 최신 기능 (C# 12/13/14)을 활용하고 있음\n- [ ] 명명 규칙을 따르고 있음\n- [ ] 가독성이 높고 유지보수하기 쉬운 코드임\n",
        "icartsh-plugin/skills/csharp-async-patterns/SKILL.md": "---\nname: csharp-async-patterns\ndescription: Task, ValueTask, async streams, cancellation 등 C# async/await 패턴을 사용할 때 활용합니다. 비동기 C# 코드를 작성할 때 사용합니다.\nallowed-tools:\n  - Bash\n  - Read\n  - Write\n  - Edit\n---\n\n# C# Async Patterns\n\nasync/await, Task, ValueTask, async streams 및 cancellation 패턴을 사용하여 C# 비동기 프로그래밍을 마스터합니다. 이 SKILL은 반응성이 뛰어나고 확장이 용이한 애플리케이션을 구축하기 위해 C# 8-12의 모던 비동기 패턴을 다룹니다.\n\n## Async/Await Fundamentals\n\nasync/await 패턴은 동기 코드처럼 보이고 동작하는 비동기 코드를 작성하는 간단한 방법을 제공합니다.\n\n### Basic Async Method\n\n```csharp\npublic async Task<string> FetchDataAsync(string url)\n{\n    using var client = new HttpClient();\n    string result = await client.GetStringAsync(url);\n    return result;\n}\n\n// 비동기 메서드 호출\npublic async Task ProcessAsync()\n{\n    string data = await FetchDataAsync(\"https://api.example.com/data\");\n    Console.WriteLine(data);\n}\n```\n\n### Async Method Signature Rules\n\n```csharp\n// ✅ 올바름 - Task 반환\npublic async Task ProcessDataAsync()\n{\n    await Task.Delay(1000);\n}\n\n// ✅ 올바름 - Task<T> 반환\npublic async Task<int> CalculateAsync()\n{\n    await Task.Delay(1000);\n    return 42;\n}\n\n// ⚠️ 이벤트 핸들러 전용 - void 반환\npublic async void Button_Click(object sender, EventArgs e)\n{\n    await ProcessDataAsync();\n}\n\n// ❌ 잘못됨 - async가 아니지만 Task 반환\npublic Task WrongAsync()\n{\n    // async를 사용하거나 Task.FromResult를 사용해야 함\n    return Task.CompletedTask;\n}\n```\n\n## Task and Task<T>\n\nTask는 비동기 작업을 나타냅니다. Task<T>는 값을 반환하는 작업을 나타냅니다.\n\n### Creating Tasks\n\n```csharp\n// CPU 집약적 작업을 위한 Task.Run\npublic async Task<int> CalculateSumAsync(int[] numbers)\n{\n    return await Task.Run(() => numbers.Sum());\n}\n\n// 이미 계산된 값을 위한 Task.FromResult\npublic Task<string> GetCachedValueAsync(string key)\n{\n    if (_cache.TryGetValue(key, out var value))\n    {\n        return Task.FromResult(value);\n    }\n    return FetchFromDatabaseAsync(key);\n}\n\n// void 비동기 메서드를 위한 Task.CompletedTask\npublic Task ProcessIfNeededAsync(bool condition)\n{\n    if (!condition)\n    {\n        return Task.CompletedTask;\n    }\n    return DoActualWorkAsync();\n}\n```\n\n### Task Composition\n\n```csharp\npublic async Task<Result> ProcessOrderAsync(Order order)\n{\n    // 순차적 실행 (Sequential execution)\n    await ValidateOrderAsync(order);\n    await ChargePaymentAsync(order);\n    await ShipOrderAsync(order);\n\n    return new Result { Success = true };\n}\n\npublic async Task<Result> ProcessOrderParallelAsync(Order order)\n{\n    // 병렬 실행 (Parallel execution)\n    var validationTask = ValidateOrderAsync(order);\n    var inventoryTask = CheckInventoryAsync(order);\n    var pricingTask = CalculatePricingAsync(order);\n\n    await Task.WhenAll(validationTask, inventoryTask, pricingTask);\n\n    return new Result\n    {\n        IsValid = await validationTask,\n        InStock = await inventoryTask,\n        Price = await pricingTask\n    };\n}\n```\n\n## ValueTask and ValueTask<T>\n\nValueTask는 결과가 동기적으로 사용 가능한 경우가 많을 때 사용하는 성능 최적화 수단입니다.\n\n### When to Use ValueTask\n\n```csharp\npublic class CachedRepository\n{\n    private readonly Dictionary<int, User> _cache = new();\n    private readonly IDatabase _database;\n\n    // ✅ ValueTask 사용이 적절한 사례 - 캐시에서 동기적으로 반환되는 경우가 많음\n    public ValueTask<User> GetUserAsync(int id)\n    {\n        if (_cache.TryGetValue(id, out var user))\n        {\n            return ValueTask.FromResult(user);\n        }\n\n        return new ValueTask<User>(FetchUserFromDatabaseAsync(id));\n    }\n\n    private async Task<User> FetchUserFromDatabaseAsync(int id)\n    {\n        var user = await _database.QueryAsync<User>(id);\n        _cache[id] = user;\n        return user;\n    }\n}\n```\n\n### ValueTask Best Practices\n\n```csharp\npublic class BufferedReader\n{\n    private readonly byte[] _buffer = new byte[4096];\n    private int _position;\n    private int _length;\n\n    // Hot path 최적화를 위한 ValueTask\n    public async ValueTask<byte> ReadByteAsync()\n    {\n        if (_position < _length)\n        {\n            // 동기 경로 - 할당 없음 (No allocation)\n            return _buffer[_position++];\n        }\n\n        // 비동기 경로 - 데이터 추가 읽기\n        await FillBufferAsync();\n        return _buffer[_position++];\n    }\n\n    private async Task FillBufferAsync()\n    {\n        _length = await _stream.ReadAsync(_buffer);\n        _position = 0;\n    }\n}\n\n// ⚠️ ValueTask 규칙\npublic async Task ConsumeValueTaskAsync()\n{\n    var reader = new BufferedReader();\n\n    // ✅ 올바름 - 한 번만 await\n    byte b = await reader.ReadByteAsync();\n\n    // ❌ 잘못됨 - ValueTask를 저장하지 마세요\n    var task = reader.ReadByteAsync();\n    await task; // 잠재적 이슈 발생 가능\n\n    // ❌ 잘못됨 - 여러 번 await 하지 마세요\n    var vt = reader.ReadByteAsync();\n    await vt;\n    await vt; // 절대 하지 마세요\n}\n```\n\n## Async Void vs Async Task\n\nasync void (드물게 발생)와 async Task (거의 항상 사용)를 언제 사용할지 이해합니다.\n\n### The Async Void Problem\n\n```csharp\n// ❌ 나쁨 - await 불가, 예외 처리 안 됨\npublic async void ProcessDataBadAsync()\n{\n    await Task.Delay(1000);\n    throw new Exception(\"Unhandled!\"); // 앱 크래시 발생\n}\n\n// ✅ 좋음 - await 가능, 예외 처리 가능\npublic async Task ProcessDataGoodAsync()\n{\n    await Task.Delay(1000);\n    throw new Exception(\"Handled!\"); // catch 가능\n}\n\n// 사용 예시\npublic async Task CallerAsync()\n{\n    try\n    {\n        // async void는 await 불가\n        ProcessDataBadAsync(); // Fire and forget - 위험함\n\n        // async Task는 await 가능\n        await ProcessDataGoodAsync(); // 여기서 예외 catch됨\n    }\n    catch (Exception ex)\n    {\n        Console.WriteLine($\"Caught: {ex.Message}\");\n    }\n}\n```\n\n### The Only Valid Use of Async Void\n\n```csharp\n// ✅ 이벤트 핸들러 - 유일하게 허용되는 사례\npublic partial class MainWindow : Window\n{\n    public async void SaveButton_Click(object sender, RoutedEventArgs e)\n    {\n        try\n        {\n            await SaveDataAsync();\n            MessageBox.Show(\"Saved successfully!\");\n        }\n        catch (Exception ex)\n        {\n            MessageBox.Show($\"Error: {ex.Message}\");\n        }\n    }\n\n    private async Task SaveDataAsync()\n    {\n        await _repository.SaveAsync(_data);\n    }\n}\n```\n\n## ConfigureAwait(false)\n\n라이브러리 코드에서 성능을 위해 synchronization context 캡처를 제어합니다.\n\n### Understanding ConfigureAwait\n\n```csharp\n// 라이브러리 코드 - ConfigureAwait(false) 사용\npublic class DataService\n{\n    public async Task<Data> GetDataAsync(int id)\n    {\n        // ConfigureAwait(false) - 컨텍스트를 캡처하지 않음\n        var json = await _httpClient.GetStringAsync($\"/api/data/{id}\")\n            .ConfigureAwait(false);\n\n        var data = await DeserializeAsync(json)\n            .ConfigureAwait(false);\n\n        return data;\n    }\n}\n\n// UI 코드 - ConfigureAwait(false) 사용 금지\npublic class ViewModel\n{\n    public async Task LoadDataAsync()\n    {\n        var data = await _dataService.GetDataAsync(42);\n        // 여기서 UI 컨텍스트가 필요함\n        this.DataProperty = data; // UI 업데이트\n    }\n}\n```\n\n### ConfigureAwait Patterns\n\n```csharp\npublic class AsyncLibrary\n{\n    // ✅ ConfigureAwait(false)를 사용한 라이브러리 메서드\n    public async Task<Result> ProcessAsync(string input)\n    {\n        var step1 = await Step1Async(input).ConfigureAwait(false);\n        var step2 = await Step2Async(step1).ConfigureAwait(false);\n        var step3 = await Step3Async(step2).ConfigureAwait(false);\n        return step3;\n    }\n\n    // ✅ ASP.NET Core - 어디서나 ConfigureAwait(false) 안전함\n    [HttpGet]\n    public async Task<IActionResult> GetData(int id)\n    {\n        // ASP.NET Core에는 synchronization context가 없음\n        var data = await _repository.GetAsync(id).ConfigureAwait(false);\n        return Ok(data);\n    }\n}\n```\n\n## CancellationToken Patterns\n\n오래 실행되는 작업에 대한 적절한 취약점 지원.\n\n### Basic Cancellation\n\n```csharp\npublic async Task<List<Result>> ProcessItemsAsync(\n    IEnumerable<Item> items,\n    CancellationToken cancellationToken = default)\n{\n    var results = new List<Result>();\n\n    foreach (var item in items)\n    {\n        // 취소 요청 확인\n        cancellationToken.ThrowIfCancellationRequested();\n\n        var result = await ProcessItemAsync(item, cancellationToken);\n        results.Add(result);\n    }\n\n    return results;\n}\n\n// Timeout과 함께 사용\npublic async Task<List<Result>> ProcessWithTimeoutAsync(IEnumerable<Item> items)\n{\n    using var cts = new CancellationTokenSource(TimeSpan.FromSeconds(30));\n\n    try\n    {\n        return await ProcessItemsAsync(items, cts.Token);\n    }\n    catch (OperationCanceledException)\n    {\n        Console.WriteLine(\"Operation timed out\");\n        throw;\n    }\n}\n```\n\n### Advanced Cancellation Patterns\n\n```csharp\npublic class BackgroundProcessor\n{\n    private CancellationTokenSource? _cts;\n\n    public async Task StartAsync()\n    {\n        _cts = new CancellationTokenSource();\n        await ProcessLoopAsync(_cts.Token);\n    }\n\n    public void Stop()\n    {\n        _cts?.Cancel();\n    }\n\n    private async Task ProcessLoopAsync(CancellationToken cancellationToken)\n    {\n        while (!cancellationToken.IsCancellationRequested)\n        {\n            try\n            {\n                await ProcessBatchAsync(cancellationToken);\n                await Task.Delay(1000, cancellationToken);\n            }\n            catch (OperationCanceledException)\n            {\n                // 취소 시 예상되는 상황\n                break;\n            }\n        }\n    }\n\n    // 연결된 cancellation tokens (Linked cancellation tokens)\n    public async Task ProcessWithMultipleTokensAsync(\n        CancellationToken userToken,\n        CancellationToken systemToken)\n    {\n        using var linkedCts = CancellationTokenSource\n            .CreateLinkedTokenSource(userToken, systemToken);\n\n        await DoWorkAsync(linkedCts.Token);\n    }\n}\n```\n\n## Async Streams (IAsyncEnumerable)\n\nIAsyncEnumerable<T>를 사용하여 비동기적으로 데이터를 스트리밍합니다 (C# 8+).\n\n### Basic Async Streams\n\n```csharp\npublic async IAsyncEnumerable<LogEntry> ReadLogsAsync(\n    string filePath,\n    [EnumeratorCancellation] CancellationToken cancellationToken = default)\n{\n    await using var stream = File.OpenRead(filePath);\n    using var reader = new StreamReader(stream);\n\n    string? line;\n    while ((line = await reader.ReadLineAsync(cancellationToken)) != null)\n    {\n        if (TryParseLog(line, out var entry))\n        {\n            yield return entry;\n        }\n    }\n}\n\n// 비동기 스트림 소비\npublic async Task ProcessLogsAsync(string filePath)\n{\n    await foreach (var log in ReadLogsAsync(filePath))\n    {\n        Console.WriteLine($\"{log.Timestamp}: {log.Message}\");\n    }\n}\n```\n\n### Advanced Async Stream Patterns\n\n```csharp\npublic class DataStreamProcessor\n{\n    // 필터링이 포함된 비동기 스트림\n    public async IAsyncEnumerable<Event> GetEventsAsync(\n        DateTime startDate,\n        [EnumeratorCancellation] CancellationToken cancellationToken = default)\n    {\n        int page = 0;\n\n        while (true)\n        {\n            var events = await FetchPageAsync(page++, cancellationToken);\n\n            if (events.Count == 0)\n                yield break;\n\n            foreach (var evt in events.Where(e => e.Date >= startDate))\n            {\n                yield return evt;\n            }\n        }\n    }\n\n    // 비동기 스트림에 대한 LINQ 스타일 작업\n    public async IAsyncEnumerable<TResult> SelectAsync<TSource, TResult>(\n        IAsyncEnumerable<TSource> source,\n        Func<TSource, TResult> selector)\n    {\n        await foreach (var item in source)\n        {\n            yield return selector(item);\n        }\n    }\n\n    // 비동기 스트림 버퍼링 (Buffering)\n    public async IAsyncEnumerable<List<T>> BufferAsync<T>(\n        IAsyncEnumerable<T> source,\n        int bufferSize)\n    {\n        var buffer = new List<T>(bufferSize);\n\n        await foreach (var item in source)\n        {\n            buffer.Add(item);\n\n            if (buffer.Count >= bufferSize)\n            {\n                yield return buffer;\n                buffer = new List<T>(bufferSize);\n            }\n        }\n\n        if (buffer.Count > 0)\n        {\n            yield return buffer;\n        }\n    }\n}\n```\n\n## Parallel Async Operations\n\n여러 비동기 작업을 동시에 실행합니다.\n\n### Task.WhenAll and Task.WhenAny\n\n```csharp\npublic async Task<Summary> GetDashboardDataAsync()\n{\n    // 모든 작업을 동시에 시작\n    var userTask = GetUserDataAsync();\n    var ordersTask = GetOrdersAsync();\n    var analyticsTask = GetAnalyticsAsync();\n\n    // 모두 완료될 때까지 대기\n    await Task.WhenAll(userTask, ordersTask, analyticsTask);\n\n    return new Summary\n    {\n        User = await userTask,\n        Orders = await ordersTask,\n        Analytics = await analyticsTask\n    };\n}\n\n// 일부 실패 처리 (Partial failures)\npublic async Task<Results> ProcessWithPartialFailuresAsync()\n{\n    var tasks = new[]\n    {\n        ProcessTask1Async(),\n        ProcessTask2Async(),\n        ProcessTask3Async()\n    };\n\n    await Task.WhenAll(tasks.Select(async t =>\n    {\n        try\n        {\n            await t;\n        }\n        catch (Exception ex)\n        {\n            // 로그를 남기되 throw 하지 않음\n            Console.WriteLine($\"Task failed: {ex.Message}\");\n        }\n    }));\n\n    // 성공한 결과 수집\n    var results = tasks\n        .Where(t => t.IsCompletedSuccessfully)\n        .Select(t => t.Result)\n        .ToList();\n\n    return new Results { Successful = results };\n}\n```\n\n### Task.WhenAny for Timeouts and Racing\n\n```csharp\npublic async Task<T> WithTimeoutAsync<T>(Task<T> task, TimeSpan timeout)\n{\n    var delayTask = Task.Delay(timeout);\n    var completedTask = await Task.WhenAny(task, delayTask);\n\n    if (completedTask == delayTask)\n    {\n        throw new TimeoutException(\"Operation timed out\");\n    }\n\n    return await task;\n}\n\n// 여러 소스 간 레이싱 (Racing multiple sources)\npublic async Task<Data> GetFastestDataAsync()\n{\n    var primaryTask = GetFromPrimaryAsync();\n    var secondaryTask = GetFromSecondaryAsync();\n    var cacheTask = GetFromCacheAsync();\n\n    var completedTask = await Task.WhenAny(primaryTask, secondaryTask, cacheTask);\n    return await completedTask;\n}\n\n// Throttled parallel processing (동시성 제한 병렬 처리)\npublic async Task<List<Result>> ProcessWithThrottlingAsync(\n    IEnumerable<Item> items,\n    int maxConcurrency)\n{\n    var semaphore = new SemaphoreSlim(maxConcurrency);\n    var tasks = items.Select(async item =>\n    {\n        await semaphore.WaitAsync();\n        try\n        {\n            return await ProcessItemAsync(item);\n        }\n        finally\n        {\n            semaphore.Release();\n        }\n    });\n\n    return (await Task.WhenAll(tasks)).ToList();\n}\n```\n\n## Exception Handling in Async Code\n\n비동기 메서드에 대한 적절한 예외 처리 패턴.\n\n### Basic Exception Handling\n\n```csharp\npublic async Task<Result> ProcessWithErrorHandlingAsync()\n{\n    try\n    {\n        var data = await FetchDataAsync();\n        return await ProcessDataAsync(data);\n    }\n    catch (HttpRequestException ex)\n    {\n        _logger.LogError(ex, \"Network error occurred\");\n        throw;\n    }\n    catch (Exception ex)\n    {\n        _logger.LogError(ex, \"Unexpected error occurred\");\n        return Result.Failed(ex.Message);\n    }\n}\n\n// Task.WhenAll과 함께 사용하는 예외 처리\npublic async Task ProcessMultipleAsync()\n{\n    var tasks = new[] { Task1Async(), Task2Async(), Task3Async() };\n\n    try\n    {\n        await Task.WhenAll(tasks);\n    }\n    catch (Exception ex)\n    {\n        // 첫 번째 예외만 throw됨\n        _logger.LogError(ex, \"At least one task failed\");\n\n        // 모든 예외를 가져오려면:\n        var exceptions = tasks\n            .Where(t => t.IsFaulted)\n            .Select(t => t.Exception)\n            .ToList();\n\n        foreach (var exception in exceptions)\n        {\n            _logger.LogError(exception, \"Task failed\");\n        }\n    }\n}\n```\n\n### AggregateException Handling\n\n```csharp\npublic async Task HandleAllExceptionsAsync()\n{\n    var tasks = Enumerable.Range(1, 10)\n        .Select(i => ProcessItemAsync(i))\n        .ToArray();\n\n    try\n    {\n        await Task.WhenAll(tasks);\n    }\n    catch\n    {\n        // 모든 예외 조사\n        var aggregateException = new AggregateException(\n            tasks.Where(t => t.IsFaulted)\n                .SelectMany(t => t.Exception?.InnerExceptions ?? Array.Empty<Exception>())\n        );\n\n        aggregateException.Handle(ex =>\n        {\n            if (ex is HttpRequestException)\n            {\n                _logger.LogWarning(ex, \"Network error - retrying\");\n                return true; // 처리됨 (Handled)\n            }\n            return false; // 다시 throw (Rethrow)\n        });\n    }\n}\n```\n\n## Deadlock Prevention\n\n비동기 코드에서 흔히 발생하는 데드락 상황을 피합니다.\n\n### Common Deadlock Patterns\n\n```csharp\n// ❌ DEADLOCK - 비동기 코드에서 blocking 발생\npublic void DeadlockExample()\n{\n    // UI 또는 ASP.NET 컨텍스트에서 데드락 발생\n    var result = GetDataAsync().Result;\n\n    // 이것 또한 데드락 발생 가능\n    GetDataAsync().Wait();\n}\n\n// ✅ 올바름 - 끝까지 비동기 유지 (async all the way)\npublic async Task CorrectExample()\n{\n    var result = await GetDataAsync();\n}\n\n// ✅ 올바름 - 라이브러리 코드에서 ConfigureAwait(false) 사용\npublic async Task<Data> LibraryMethodAsync()\n{\n    var data = await FetchAsync().ConfigureAwait(false);\n    return ProcessData(data);\n}\n```\n\n### Avoiding Deadlocks\n\n```csharp\npublic class DeadlockFreeService\n{\n    // ✅ 끝까지 비동기 유지\n    public async Task<Result> ProcessAsync()\n    {\n        var data = await GetDataAsync();\n        var processed = await ProcessDataAsync(data);\n        return processed;\n    }\n\n    // ✅ 부득이하게 block 해야 한다면 Task.Run 사용\n    public Result ProcessSync()\n    {\n        return Task.Run(async () => await ProcessAsync()).GetAwaiter().GetResult();\n    }\n\n    // ✅ 비동기 disposal 사용 (Async disposal)\n    public async Task UseResourceAsync()\n    {\n        await using var resource = new AsyncDisposableResource();\n        await resource.ProcessAsync();\n    }\n}\n```\n\n## Async in ASP.NET Core\n\nASP.NET Core 애플리케이션의 비동기 코드 모범 사례.\n\n### Controller Async Patterns\n\n```csharp\n[ApiController]\n[Route(\"api/[controller]\")]\npublic class ProductsController : ControllerBase\n{\n    private readonly IProductRepository _repository;\n\n    // ✅ Async 액션 메서드\n    [HttpGet(\"{id}\")]\n    public async Task<ActionResult<Product>> GetProduct(\n        int id,\n        CancellationToken cancellationToken)\n    {\n        var product = await _repository.GetByIdAsync(id, cancellationToken);\n\n        if (product == null)\n            return NotFound();\n\n        return Ok(product);\n    }\n\n    [HttpPost]\n    public async Task<ActionResult<Product>> CreateProduct(\n        [FromBody] CreateProductRequest request,\n        CancellationToken cancellationToken)\n    {\n        var product = await _repository.CreateAsync(request, cancellationToken);\n        return CreatedAtAction(nameof(GetProduct), new { id = product.Id }, product);\n    }\n\n    // ✅ IAsyncEnumerable을 사용한 응답 스트리밍\n    [HttpGet(\"stream\")]\n    public async IAsyncEnumerable<Product> StreamProducts(\n        [EnumeratorCancellation] CancellationToken cancellationToken)\n    {\n        await foreach (var product in _repository.GetAllStreamAsync(cancellationToken))\n        {\n            yield return product;\n        }\n    }\n}\n```\n\n### Background Services\n\n```csharp\npublic class DataProcessorService : BackgroundService\n{\n    private readonly IServiceProvider _serviceProvider;\n    private readonly ILogger<DataProcessorService> _logger;\n\n    protected override async Task ExecuteAsync(CancellationToken stoppingToken)\n    {\n        _logger.LogInformation(\"Data processor service starting\");\n\n        while (!stoppingToken.IsCancellationRequested)\n        {\n            try\n            {\n                await ProcessDataBatchAsync(stoppingToken);\n                await Task.Delay(TimeSpan.FromMinutes(5), stoppingToken);\n            }\n            catch (OperationCanceledException)\n            {\n                // 중지 시 예상되는 상황\n                break;\n            }\n            catch (Exception ex)\n            {\n                _logger.LogError(ex, \"Error processing data batch\");\n                await Task.Delay(TimeSpan.FromSeconds(30), stoppingToken);\n            }\n        }\n\n        _logger.LogInformation(\"Data processor service stopped\");\n    }\n\n    private async Task ProcessDataBatchAsync(CancellationToken cancellationToken)\n    {\n        using var scope = _serviceProvider.CreateScope();\n        var repository = scope.ServiceProvider.GetRequiredService<IDataRepository>();\n\n        await repository.ProcessBatchAsync(cancellationToken);\n    }\n}\n```\n\n## Best Practices\n\n1. **Async All the Way**: .Result나 .Wait()를 사용하여 비동기 코드를 block 하지 마세요.\n2. **Use CancellationToken**: 오래 실행되는 작업에는 항상 CancellationToken을 받도록 하세요.\n3. **ConfigureAwait in Libraries**: 라이브러리 코드에서는 ConfigureAwait(false)를 사용하세요.\n4. **Avoid Async Void**: 이벤트 핸들러용으로만 async void를 사용하세요.\n5. **Return Task Directly**: 가능하면 await 없이 Task를 직접 반환하세요.\n6. **Use ValueTask for Hot Paths**: 자주 호출되거나 동기적으로 실행되는 경우가 많은 메서드에는 ValueTask를 고려하세요.\n7. **Handle All Exceptions**: 비동기 메서드에서는 항상 예외를 처리하세요.\n8. **Don't Mix Blocking and Async**: 하나의 호출 체인에는 하나의 패러다임만 선택하세요.\n9. **Dispose Async Resources**: IAsyncDisposable에는 await using을 사용하세요.\n10. **Test with Cancellation**: 취소가 올바르게 작동하는지 테스트하세요.\n\n## Common Pitfalls\n\n1. **Blocking on Async Code**: .Result나 .Wait() 사용은 데드락을 유발합니다.\n2. **Forgetting ConfigureAwait**: 라이브러리에서 성능 문제를 일으킬 수 있습니다.\n3. **Async Void Methods**: await가 불가능하며 예외를 삼켜버립니다.\n4. **Not Handling Cancellation**: CancellationToken 파라미터를 무시하는 것.\n5. **Over-using Task.Run**: 이미 비동기인 코드를 Task.Run으로 감싸지 마세요.\n6. **Capturing Context Unnecessarily**: 컨텍스트가 필요 없는 상황에서 리소스를 낭비합니다.\n7. **Fire and Forget**: await 없이 비동기 작업을 시작하는 것.\n8. **Mixing Sync and Async**: 혼란을 야기하고 잠재적인 데드락을 만듭니다.\n9. **Not Using ValueTask Correctly**: ValueTask를 여러 번 await 하는 것.\n10. **Ignoring Exceptions in Task.WhenAll**: 첫 번째 예외만 catch 하는 것.\n\n## When to Use\n\n다음을 수행할 때 이 SKILL을 사용합니다:\n\n- C#에서 비동기 코드 작성\n- I/O 바운드 작업 구현 (데이터베이스, 네트워크, 파일 시스템)\n- 반응형 UI 애플리케이션 구축\n- 확장 가능한 웹 서비스 구축\n- 데이터 스트림 처리\n- 취소 지원(Cancellation support) 구현\n- ValueTask를 통한 비동기 성능 최적화\n- 병렬 비동기 작업 처리\n- 비동기 코드의 데드락 방지\n- ASP.NET Core 비동기 패턴 작업\n\n## Resources\n\n- [Async/Await Best Practices](https://learn.microsoft.com/en-us/archive/msdn-magazine/2013/march/async-await-best-practices-in-asynchronous-programming)\n- [ConfigureAwait FAQ](https://devblogs.microsoft.com/dotnet/configureawait-faq/)\n- [Async Streams Tutorial](https://learn.microsoft.com/en-us/dotnet/csharp/asynchronous-programming/async-streams)\n- [ValueTask Overview](https://devblogs.microsoft.com/dotnet/understanding-the-whys-whats-and-whens-of-valuetask/)\n- [Task-based Asynchronous Pattern (TAP)](https://learn.microsoft.com/en-us/dotnet/standard/asynchronous-programming-patterns/task-based-asynchronous-pattern-tap)\n",
        "icartsh-plugin/skills/csharp-developer/SKILL.md": "---\nname: csharp-developer\ndescription: 모던 .NET 개발, ASP.NET Core 및 클라우드 네이티브 애플리케이션을 전문으로 하는 전문가 수준의 C# 개발자입니다. C# 14 기능, Blazor 및 크로스 플랫폼 개발을 마스터했으며 성능과 Clean Architecture를 강조합니다.\ntools: Read, Write, Bash, Glob, Grep, dotnet, msbuild, nuget, xunit, resharper, dotnet-ef\n---\n\n당신은 .NET 8+ 및 Microsoft 에코시스템을 마스터한 시니어 C# 개발자로서, 고성능 웹 애플리케이션, 클라우드 네이티브 솔루션 및 크로스 플랫폼 개발 구축을 전문으로 합니다. 귀하의 전문 지식은 ASP.NET Core, Blazor, Entity Framework Core 및 클린 코드와 아키텍처 패턴에 중점을 둔 모던 C# 언어 기능을 아우릅니다.\n\n\n호출 시 수행할 작업:\n1. 기존 .NET 솔루션 구조 및 프로젝트 구성에 대해 컨텍스트 매니저에 쿼리합니다.\n2. .csproj 파일, NuGet 패키지 및 솔루션 아키텍처를 검토합니다.\n3. C# 패턴, nullable reference types 사용 현황 및 성능 특성을 분석합니다.\n4. 모던 C# 기능과 .NET 모범 사례를 활용하여 솔루션을 구현합니다.\n\nC# 개발 체크리스트:\n- Nullable reference types 활성화 여부\n- .editorconfig를 이용한 코드 분석\n- StyleCop 및 분석기(Analyzer) 준수\n- 테스트 커버리지 80% 초과\n- API versioning 구현\n- 성능 프로파일링 완료\n- 보안 스캔 통과\n- XML 문서 생성\n\n모던 C# 패턴:\n- 불변성(Immutability)을 위한 Record types\n- Pattern matching 표현식\n- Nullable reference types 규율\n- Async/await 모범 사례\n- LINQ 최적화 기법\n- Expression trees 활용\n- Source generators 도입\n- Global using 디렉티브\n\nASP.NET Core 숙련도:\n- 마이크로서비스를 위한 Minimal APIs\n- Middleware 파이프라인 최적화\n- Dependency injection 패턴\n- Configuration 및 options\n- Authentication/authorization\n- 커스텀 모델 바인딩\n- Output caching 전략\n- Health checks 구현\n\nBlazor 개발:\n- 컴포넌트 아키텍처 설계\n- 상태 관리(State management) 패턴\n- JavaScript interop\n- WebAssembly 최적화\n- Server-side vs WASM\n- 컴포넌트 생명주기(Lifecycle)\n- Form 검증\n- SignalR을 이용한 실시간 기능\n\nEntity Framework Core:\n- Code-first migrations\n- 쿼리 최적화\n- 복잡한 관계(Relationship) 처리\n- 성능 튜닝\n- 벌크 작업(Bulk operations)\n- Compiled queries\n- Change tracking 최적화\n- 다중 테넌시(Multi-tenancy) 구현\n\n성능 최적화:\n- Span<T> 및 Memory<T> 사용\n- 할당(Allocation)을 줄이기 위한 ArrayPool\n- ValueTask 패턴\n- SIMD 작업\n- Source generators\n- AOT 컴파일 준비\n- Trimming 호환성\n- Benchmark.NET 프로파일링\n\n클라우드 네이티브 패턴:\n- 컨테이너 최적화\n- Kubernetes health probes\n- 분산 캐싱(Distributed caching)\n- Service bus 연동\n- Azure SDK 모범 사례\n- Dapr 연동\n- Feature flags\n- Circuit breaker 패턴\n\n테스트 우수성:\n- Theories를 포함한 xUnit\n- 통합 테스트(Integration testing)\n- TestServer 사용\n- Moq를 이용한 모킹(Mocking)\n- Property-based testing\n- 성능 테스트\n- Playwright를 이용한 E2E\n- Test data builders\n\n비동기 프로그래밍:\n- ConfigureAwait 사용\n- Cancellation tokens\n- Async streams\n- Parallel.ForEachAsync\n- 생산자를 위한 Channels\n- Task composition\n- 예외 처리\n- 데드락(Deadlock) 방지\n\n크로스 플랫폼 개발:\n- 모바일/데스크톱을 위한 MAUI\n- 플랫폼별 코드(Platform-specific code) 작성\n- 네이티브 Interop\n- 리소스 관리\n- 플랫폼 감지\n- 조건부 컴파일(Conditional compilation)\n- 게시(Publishing) 전략\n- Self-contained 배포\n\n아키텍처 패턴:\n- Clean Architecture 설정\n- Vertical slice architecture\n- CQRS를 위한 MediatR\n- 도메인 이벤트(Domain events)\n- Specification 패턴\n- Repository 추상화\n- Result 패턴\n- Options 패턴\n\n## MCP Tool Suite\n- **dotnet**: 빌드, 테스트, 게시를 위한 CLI\n- **msbuild**: 복잡한 프로젝트를 위한 빌드 엔진\n- **nuget**: 패키지 관리 및 게시\n- **xunit**: Theories를 지원하는 테스트 프레임워크\n- **resharper**: 코드 분석 및 리팩토링\n- **dotnet-ef**: Entity Framework Core 도구\n\n## Communication Protocol\n\n### .NET Project Assessment\n\n.NET 솔루션 아키텍처와 요구 사항을 이해하여 개발을 시작합니다.\n\nSolution query:\n```json\n{\n  \"requesting_agent\": \"csharp-developer\",\n  \"request_type\": \"get_dotnet_context\",\n  \"payload\": {\n    \"query\": \".NET context needed: target framework, project types, Azure services, database setup, authentication method, and performance requirements.\"\n  }\n}\n```\n\n## Development Workflow\n\n체계적인 단계를 통해 C# 개발을 실행합니다:\n\n### 1. Solution Analysis\n\n.NET 아키텍처와 프로젝트 구조를 이해합니다.\n\n분석 우선순위:\n- 솔루션 구성\n- 프로젝트 종속성\n- NuGet 패키지 감사\n- 대상 프레임워크 (Target frameworks)\n- 코드 스타일 설정\n- 테스트 프로젝트 설정\n- 빌드 구성\n- 배포 대상\n\n기술 평가:\n- Nullable annotations 검토\n- 비동기 패턴(Async patterns) 확인\n- LINQ 사용 현황 분석\n- 메모리 패턴 평가\n- DI 설정 검토\n- 보안 설정 확인\n- API 설계 평가\n- 사용된 패턴 문서화\n\n### 2. Implementation Phase\n\n모던 C# 기능을 사용하여 .NET 솔루션을 개발합니다.\n\n구현 중점 사항:\n- Primary constructors 사용\n- File-scoped namespaces 적용\n- Pattern matching 활용\n- Records를 이용한 구현\n- Nullable reference types 사용\n- 효율적인 LINQ 적용\n- 불변(Immutable) API 설계\n- Extension methods 생성\n\n개발 패턴:\n- 도메인 모델(Domain models)부터 시작\n- 핸들러를 위해 MediatR 사용\n- Validation attributes 적용\n- Repository 패턴 구현\n- 서비스 추상화 작성\n- 설정을 위해 options 패턴 사용\n- 캐싱 전략 적용\n- 구조화된 로깅(Structured logging) 설정\n\n상태 업데이트:\n```json\n{\n  \"agent\": \"csharp-developer\",\n  \"status\": \"implementing\",\n  \"progress\": {\n    \"projects_updated\": [\"API\", \"Domain\", \"Infrastructure\"],\n    \"endpoints_created\": 18,\n    \"test_coverage\": \"84%\",\n    \"warnings\": 0\n  }\n}\n```\n\n### 3. Quality Verification\n\n.NET 모범 사례와 성능을 보장합니다.\n\n품질 체크리스트:\n- 코드 분석 통과\n- StyleCop 클린 상태\n- 테스트 통과\n- 커버리지 목표 달성\n- API 문서화 완료\n- 성능 검증 완료\n- 보안 스캔 클린 상태\n- NuGet 감사 통과\n\n완료 메시지 (예시):\n\".NET 구현이 완료되었습니다. Blazor WASM 프런트엔드를 포함한 ASP.NET Core 8 API를 전달했으며, p95 응답 시간 20ms를 달성했습니다. Compiled queries를 포함한 EF Core, 분산 캐싱, 포괄적인 테스트(86% 커버리지), 그리고 메모리를 40% 절감하는 AOT 준비 설정이 포함되어 있습니다.\"\n\nMinimal API 패턴:\n- Endpoint filters\n- Route groups\n- OpenAPI 통합\n- 모델 검증\n- 에러 처리\n- Rate limiting\n- 버전 관리(Versioning) 설정\n- 인증 흐름(Authentication flow)\n\nBlazor 패턴:\n- 컴포넌트 합성(Component composition)\n- Cascading parameters\n- Event callbacks\n- Render fragments\n- Component parameters\n- State containers\n- JS isolation\n- CSS isolation\n\ngRPC 구현:\n- 서비스 정의\n- Client factory 설정\n- Interceptors\n- 스트리밍 패턴\n- 에러 처리\n- 성능 튜닝\n- 코드 생성\n- Health checks\n\nAzure 통합:\n- App Configuration\n- Key Vault secrets\n- Service Bus messaging\n- Cosmos DB 사용\n- Blob storage\n- Azure Functions\n- Application Insights\n- Managed Identity\n\n실시간 기능:\n- SignalR hubs\n- 연결 관리(Connection management)\n- 그룹 브로드캐스팅(Group broadcasting)\n- 인증\n- 확장 전략(Scaling strategies)\n- Backplane 설정\n- 클라이언트 라이브러리\n- 재연결(Reconnection) 로직\n\n다른 에이전트와의 협업:\n- frontend-developer와 API 공유\n- api-designer에게 계약(Contract) 제공\n- 클라우드 관련하여 azure-specialist와 협업\n- EF Core 관련하여 database-optimizer와 작업\n- 컴포넌트 관련하여 blazor-developer 지원\n- .NET 통합 관련하여 powershell-dev 가이드\n- OWASP 준수 관련하여 security-auditor 지원\n- 배포 관련하여 devops-engineer 보조\n\n최신 C# 언어 기능과 .NET 플랫폼 기능을 활용하면서 항상 성능, 보안 및 유지보수성을 최우선으로 고려합니다.",
        "icartsh-plugin/skills/docker-workflow/README.md": "# Docker Workflow\n\n멀티 스테이지 빌드(multi-stage builds), docker-compose 오케스트레이션, 이미지 최적화, 디버깅 및 운영 모범 사례를 포함하는 포괄적인 Docker 컨테이너화 워크플로우입니다.\n\n## 개요 (Overview)\n\nDocker 컨테이너화는 애플리케이션과 그 종속성을 이식 가능하고 재현 가능한 컨테이너로 패키징하여 개발, 테스트 및 배포를 능률화합니다. 이 SKILL은 개발부터 운영에 이르기까지 전문적인 Docker 워크플로우를 안내합니다.\n\n애플리케이션을 컨테이너화하거나, 개발 환경을 구축하거나, Docker를 사용하여 배포할 때 이 SKILL을 사용하세요.\n\n## 설치 (Installation)\n\nDocker가 설치되어 있는지 확인하세요:\n\n```bash\n# macOS\nbrew install docker\n\n# Ubuntu/Debian\nsudo apt-get install docker.io docker-compose\n\n# 설치 확인\ndocker --version\ndocker-compose --version\n```\n\n## 포함 내용 (What's Included)\n\n### SKILL.md\n초기 설정부터 운영 배포까지 Docker 워크플로우 단계를 다루는 포괄적인 가이드로, 멀티 스테이지 빌드, docker-compose 오케스트레이션, 최적화 전략, 디버깅 도구 및 배포 모범 사례가 포함되어 있습니다.\n\n### scripts/\n- `docker_helper.sh` - 일반적인 Docker 작업을 위한 유틸리티 스크립트:\n  - 컨테이너 상태 체크 (Health checks)\n  - 조사 및 디버깅 (Inspection and debugging)\n  - 로그 확인 (Log viewing)\n  - Shell 접속\n  - 이미지 크기 분석\n  - 리소스 정리 (Resource cleanup)\n\n### examples/\n- `Dockerfile.multi-stage` - Node.js, Python, Go, Java, Rust용 템플릿\n- `docker-compose.yml` - 전체 기능을 갖춘 멀티 서비스 설정\n- `.dockerignore` - 포괄적인 무시(Ignore) 패턴\n\n## 빠른 시작 (Quick Start)\n\n### 멀티 스테이지 Dockerfile 생성\n\n```dockerfile\n# Stage 1: Build\nFROM node:18-alpine AS builder\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production\nCOPY . .\nRUN npm run build\n\n# Stage 2: Production\nFROM node:18-alpine\nWORKDIR /app\nCOPY --from=builder /app/dist ./dist\nCOPY --from=builder /app/node_modules ./node_modules\nEXPOSE 3000\nCMD [\"node\", \"dist/index.js\"]\n```\n\n### docker-compose.yml 생성\n\n```yaml\nversion: '3.8'\n\nservices:\n  app:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - \"3000:3000\"\n    environment:\n      - DATABASE_URL=postgresql://db:5432/myapp\n    depends_on:\n      db:\n        condition: service_healthy\n    networks:\n      - app-network\n\n  db:\n    image: postgres:15-alpine\n    environment:\n      POSTGRES_DB: myapp\n    volumes:\n      - postgres-data:/var/lib/postgresql/data\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U user\"]\n      interval: 5s\n    networks:\n      - app-network\n\nvolumes:\n  postgres-data:\n\nnetworks:\n  app-network:\n```\n\n### 빌드 및 실행\n\n```bash\n# 이미지 빌드\ndocker build -t myapp:latest .\n\n# docker-compose로 실행\ndocker-compose up -d\n\n# 로그 확인\ndocker-compose logs -f app\n\n# 중지\ndocker-compose down\n```\n\n## 핵심 역량 (Core Capabilities)\n\n- **멀티 스테이지 빌드 (Multi-stage builds)**: 최적의 이미지 크기를 위해 빌드와 런타임 종속성 분리 (50-90% 절감)\n- **Docker Compose 오케스트레이션**: 네트워킹과 종속성을 갖춘 다중 컨테이너 애플리케이션 관리\n- **이미지 최적화**: 레이어 캐싱 및 모범 사례를 통한 이미지 크기 축소\n- **개발 워크플로우**: Hot-reload, 볼륨 마운트 및 환경별 설정\n- **디버깅 도구**: 컨테이너 조사, health checks 및 트러블슈팅 유틸리티\n- **운영 준비 (Production readiness)**: 보안 강화, health checks 및 배포 전략\n\n## 워크플로우 단계 (Workflow Phases)\n\n### Phase 1: 초기 설정 (Initial Setup)\n\n**.dockerignore 작성:**\n```dockerignore\nnode_modules/\n__pycache__/\n*.pyc\n.git/\n.env\n*.log\ndist/\nbuild/\ncoverage/\n```\n\n**핵심 원칙:**\n- 빌드 아티팩트 및 종속성 제외\n- 민감한 파일(.env, 자격 증명) 제외\n- 버전 관리 시스템(.git) 제외\n- 컨텍스트 크기 축소 = 빌드 속도 향상\n\n### Phase 2: 멀티 스테이지 Dockerfile\n\n**레이어 캐싱 최적화:**\n```dockerfile\n# ✅ 좋음: 종속성이 별도로 캐시됨\nCOPY package.json package-lock.json ./\nRUN npm ci\nCOPY . .\n\n# ❌ 나쁨: 파일이 하나만 변경되어도 캐시가 무효화됨\nCOPY . .\nRUN npm ci\n```\n\n**보안 모범 사례 적용:**\n```dockerfile\n# 특정 버전 사용\nFROM node:18.17.1-alpine\n\n# non-root 사용자로 실행\nRUN addgroup -g 1001 -S nodejs && adduser -S nodejs -u 1001\nUSER nodejs\n\n# 소유권과 함께 복사\nCOPY --chown=nodejs:nodejs . .\n```\n\n### Phase 3: Docker Compose 설정\n\n환경별로 override 파일을 사용합니다:\n\n**개발용 (docker-compose.override.yml)**:\n```yaml\nservices:\n  app:\n    build:\n      target: development\n    volumes:\n      - ./src:/app/src\n    environment:\n      - NODE_ENV=development\n    command: npm run dev\n```\n\n**운영용 (docker-compose.prod.yml)**:\n```yaml\nservices:\n  app:\n    build:\n      target: production\n    restart: always\n    environment:\n      - NODE_ENV=production\n```\n\n### Phase 4: 디버깅 (Debugging)\n\nHelper 스크립트를 사용하세요:\n\n```bash\n# 컨테이너 상태 체크\n./scripts/docker_helper.sh health myapp\n\n# 상세 정보 조사\n./scripts/docker_helper.sh inspect myapp\n\n# 로그 확인\n./scripts/docker_helper.sh logs myapp 200\n\n# Shell 열기\n./scripts/docker_helper.sh shell myapp\n\n# 이미지 크기 분석\n./scripts/docker_helper.sh size myapp:latest\n\n# 리소스 정리\n./scripts/docker_helper.sh cleanup\n```\n\n### Phase 5: 최적화 (Optimization)\n\n**이미지 크기 축소:**\n1. 더 작은 베이스 이미지 사용 (alpine > slim > debian)\n2. 빌드 도구 제외를 위해 멀티 스테이지 빌드 활용\n3. 레이어 수를 줄이기 위해 RUN 명령 결합\n4. 동일한 레이어 내에서 정리(Clean up) 수행\n5. .dockerignore 활용\n\n**예시:**\n```dockerfile\n# ✅ 좋음: 결합 및 정리 완료\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends package1 && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n```\n\n### Phase 6: 운영 배포 (Production Deployment)\n\n**운영 환경용 Dockerfile:**\n```dockerfile\nFROM node:18-alpine AS production\n\n# 보안: non-root 사용자\nRUN addgroup -g 1001 -S nodejs && adduser -S nodejs -u 1001\n\nWORKDIR /app\nCOPY --from=builder --chown=nodejs:nodejs /app/dist ./dist\nUSER nodejs\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=3s \\\n  CMD node healthcheck.js\n\nEXPOSE 3000\nCMD [\"node\", \"dist/index.js\"]\n```\n\n**배포 명령:**\n```bash\n# Registry용 태그 지정\ndocker tag myapp:latest registry.example.com/myapp:v1.0.0\n\n# Registry로 Push\ndocker push registry.example.com/myapp:v1.0.0\n\n# 배포\ndocker-compose pull && docker-compose up -d\n\n# 무중단 업데이트 (Rolling update)\ndocker-compose up -d --no-deps --build app\n```\n\n## 필수 명령 (Essential Commands)\n\n```bash\n# 빌드\ndocker build -t myapp .\ndocker-compose build\n\n# 실행\ndocker run -d -p 3000:3000 myapp\ndocker-compose up -d\n\n# 로그\ndocker logs -f myapp\ndocker-compose logs -f\n\n# 실행 (Execute)\ndocker exec -it myapp sh\ndocker-compose exec app sh\n\n# 중지\ndocker-compose down\n\n# 정리 (Clean)\ndocker system prune -a\n```\n\n## 모범 사례 요약 (Best Practices Summary)\n\n### 보안 (Security)\n✅ `latest`가 아닌 구체적인 이미지 버전 사용\n✅ non-root 사용자로 실행\n✅ 민감한 데이터에는 secrets management 사용\n✅ 이미지의 취약점 스캔 실시\n✅ 최소한의 베이스 이미지 사용\n\n### 성능 (Performance)\n✅ 멀티 스테이지 빌드 사용\n✅ 레이어 캐싱 최적화\n✅ .dockerignore 사용\n✅ RUN 명령 결합\n✅ BuildKit 사용\n\n### 개발 (Development)\n✅ 다중 컨테이너 앱에 docker-compose 사용\n✅ hot-reload를 위해 볼륨 사용\n✅ health checks 구현\n✅ 적절한 종속성 순서 적용\n\n### 운영 (Production)\n✅ 재시작 정책 설정\n✅ 오케스트레이션(Swarm, Kubernetes) 사용\n✅ health checks로 모니터링\n✅ Reverse proxy 사용\n✅ Rolling updates 구현\n\n## 일반적인 유스케이스 (Common Use Cases)\n\n### Full-Stack 애플리케이션\ndocker-compose 오케스트레이션을 통한 Frontend + Backend + Database + Redis 구성.\n\n### 마이크로서비스 (Microservices)\n네트워크 격리를 통한 API Gateway + 다중 서비스 + Message Queue 구성.\n\n### 개발 시 Hot Reload 적용\n개발 전용 설정을 사용한 소스 코드용 볼륨 마운트.\n\n## 트러블슈팅 (Troubleshooting)\n\n**컨테이너가 즉시 종료되는 경우:**\n```bash\ndocker logs myapp\ndocker run -it --entrypoint sh myapp:latest\n```\n\n**네트워크 연결성:**\n```bash\ndocker network inspect myapp_default\ndocker exec myapp ping db\n```\n\n**볼륨 권한:**\n```bash\n# Dockerfile에서 수정\nRUN chown -R nodejs:nodejs /app/data\n```\n\n## 문서 (Documentation)\n\n포괄적인 문서, 상세 워크플로우 및 고급 기법에 대해서는 `SKILL.md`를 참조하세요.\n\n전체 Dockerfile 템플릿과 docker-compose 설정 예시는 `examples/` 디렉토리를 참조하세요.\n\n## 요구 사항 (Requirements)\n\n- Docker 20.10+\n- docker-compose 1.29+ (또는 Docker Compose V2)\n- 컨테이너화 개념에 대한 기본적인 이해\n",
        "icartsh-plugin/skills/docker-workflow/SKILL.md": "---\nname: docker-workflow\ndescription: 멀티 스테이지 빌드(multi-stage builds), docker-compose 오케스트레이션, 이미지 최적화, 디버깅 및 운영 모범 사례를 포함하는 포괄적인 Docker 컨테이너화 워크플로우입니다. 애플리케이션 컨테이너화, 개발 환경 구축 또는 Docker 배포 시 사용합니다.\n---\n\n# Docker Workflow\n\n## Overview\n\nDocker 컨테이너화는 애플리케이션과 그 종속성을 이식 가능하고 재현 가능한 컨테이너로 패키징하여 개발, 테스트 및 배포를 능률화합니다. 이 SKILL은 개발부터 운영에 이르기까지 전문적인 Docker 워크플로우를 안내합니다.\n\n## Core Capabilities\n\n- **멀티 스테이지 빌드 (Multi-stage builds)**: 최적의 이미지 크기를 위해 빌드와 런타임 종속성을 분리합니다.\n- **Docker Compose 오케스트레이션**: 네트워킹과 종속성을 갖춘 다중 컨테이너 애플리케이션을 관리합니다.\n- **이미지 최적화**: 모범 사례를 통해 이미지 크기를 50-90% 줄입니다.\n- **개발 워크플로우**: Hot-reload, 볼륨 마운트 및 환경별 설정을 지원합니다.\n- **디버깅 도구**: 컨테이너 조사, health checks 및 트러블슈팅 유틸리티를 제공합니다.\n- **운영 준비 (Production readiness)**: 보안 강화(Security hardening), health checks 및 배포 전략을 다룹니다.\n\n## When to Use This Skill\n\n다음을 수행할 때 활성화하세요:\n- 새로운 애플리케이션 컨테이너화\n- Docker로 개발 환경 구축\n- 운영 환경에 적합한 Docker 이미지 생성\n- 다중 컨테이너 애플리케이션 오케스트레이션\n- 컨테이너 이슈 디버깅\n- Docker 빌드 및 이미지 최적화\n\n## Workflow Phases\n\n### Phase 1: Initial Setup\n\n#### .dockerignore 생성\n\n빌드 컨텍스트에서 불필요한 파일을 제외합니다:\n\n```dockerignore\nnode_modules/\n__pycache__/\n*.pyc\n.git/\n.env\n*.log\ndist/\nbuild/\ncoverage/\n```\n\n포괄적인 템플릿은 `examples/.dockerignore`를 참조하세요.\n\n**핵심 원칙**:\n- 빌드 아티팩트 및 종속성 제외\n- 민감한 파일(.env, 자격 증명) 제외\n- 버전 관리 시스템(.git) 제외\n- 컨텍스트 크기 축소 = 빌드 속도 향상\n\n#### 애플리케이션 요구 사항 분석\n\n다음을 결정합니다:\n- 런타임 (Node.js, Python, Go, Java)\n- 종속성 및 패키지 매니저\n- 빌드 요구 사항 vs 런타임 요구 사항\n- 포트 노출 및 볼륨 필요성\n\n### Phase 2: Multi-Stage Dockerfile\n\n#### 전략 선택\n\n멀티 스테이지 빌드는 최종 이미지 크기를 50-90% 줄여줍니다:\n\n```dockerfile\n# Stage 1: Build\nFROM node:18-alpine AS builder\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production\nCOPY . .\nRUN npm run build\n\n# Stage 2: Production\nFROM node:18-alpine\nWORKDIR /app\nCOPY --from=builder /app/dist ./dist\nCOPY --from=builder /app/node_modules ./node_modules\nEXPOSE 3000\nCMD [\"node\", \"dist/index.js\"]\n```\n\nNode.js, Python, Go, Java, Rust 템플릿은 `examples/Dockerfile.multi-stage`를 참조하세요.\n\n#### 레이어 캐싱 최적화 (Optimize Layer Caching)\n\n순서가 중요합니다 - 자주 변경되는 콘텐츠는 마지막에 배치하세요:\n\n```dockerfile\n# ✅ 좋음: 종속성이 별도로 캐시됨\nCOPY package.json package-lock.json ./\nRUN npm ci\nCOPY . .\n\n# ❌ 나쁨: 파일이 하나만 변경되어도 캐시가 무효화됨\nCOPY . .\nRUN npm ci\n```\n\n#### 보안 모범 사례 적용\n\n```dockerfile\n# 특정 버전 사용\nFROM node:18.17.1-alpine\n\n# non-root 사용자로 실행\nRUN addgroup -g 1001 -S nodejs && adduser -S nodejs -u 1001\nUSER nodejs\n\n# 소유권과 함께 복사\nCOPY --chown=nodejs:nodejs . .\n```\n\n**보안 체크리스트**:\n- 베이스 이미지 버전 고정 (Pin versions)\n- 최소한의 베이스 이미지 사용 (alpine, slim)\n- non-root 사용자로 실행\n- 취약점 스캔 실시\n- 설치 패키지 최소화\n\n### Phase 3: Docker Compose Setup\n\n#### 서비스 정의\n\n`docker-compose.yml`을 생성합니다:\n\n```yaml\nversion: '3.8'\n\nservices:\n  app:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - \"3000:3000\"\n    environment:\n      - DATABASE_URL=postgresql://db:5432/myapp\n    depends_on:\n      db:\n        condition: service_healthy\n    volumes:\n      - ./src:/app/src  # 개발 시 hot-reload\n    networks:\n      - app-network\n\n  db:\n    image: postgres:15-alpine\n    environment:\n      POSTGRES_DB: myapp\n    volumes:\n      - postgres-data:/var/lib/postgresql/data\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U user\"]\n      interval: 5s\n    networks:\n      - app-network\n\nvolumes:\n  postgres-data:\n\nnetworks:\n  app-network:\n```\n\n모니터링, 큐, 캐싱이 포함된 전체 기능 설정은 `examples/docker-compose.yml`을 참조하세요.\n\n#### 환경 설정\n\n환경별로 override 파일을 사용합니다:\n\n**개발용 (docker-compose.override.yml)**:\n```yaml\nservices:\n  app:\n    build:\n      target: development\n    volumes:\n      - ./src:/app/src\n    environment:\n      - NODE_ENV=development\n    command: npm run dev\n```\n\n**운영용 (docker-compose.prod.yml)**:\n```yaml\nservices:\n  app:\n    build:\n      target: production\n    restart: always\n    environment:\n      - NODE_ENV=production\n```\n\n**사용법**:\n```bash\n# 개발 (override 파일이 자동으로 사용됨)\ndocker-compose up\n\n# 운영\ndocker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d\n```\n\n### Phase 4: Build and Run\n\n#### 빌드 명령\n\n```bash\n# 기본 빌드\ndocker build -t myapp:latest .\n\n# 특정 스테이지 빌드\ndocker build --target production -t myapp:prod .\n\n# BuildKit을 사용한 빌드 (빠름)\nDOCKER_BUILDKIT=1 docker build -t myapp:latest .\n```\n\n#### 실행 명령\n\n```bash\n# 단일 컨테이너\ndocker run -d -p 3000:3000 -e NODE_ENV=production myapp:latest\n\n# Docker Compose\ndocker-compose up -d\n\n# 로그 확인\ndocker-compose logs -f app\n\n# 컨테이너 내 명령 실행\ndocker-compose exec app sh\n\n# 중지 및 제거\ndocker-compose down -v\n```\n\n### Phase 5: Debugging and Troubleshooting\n\n#### Helper 스크립트 사용\n\n`scripts/docker_helper.sh` 유틸리티는 일반적인 디버깅 작업을 제공합니다:\n\n```bash\n# 컨테이너 상태 체크\n./scripts/docker_helper.sh health myapp\n\n# 상세 정보 조사\n./scripts/docker_helper.sh inspect myapp\n\n# 로그 확인\n./scripts/docker_helper.sh logs myapp 200\n\n# Shell 열기\n./scripts/docker_helper.sh shell myapp\n\n# 이미지 크기 분석\n./scripts/docker_helper.sh size myapp:latest\n\n# 리소스 정리 (Cleanup)\n./scripts/docker_helper.sh cleanup\n```\n\n#### 일반적인 이슈\n\n**컨테이너가 즉시 종료되는 경우**:\n```bash\ndocker logs myapp\ndocker run -it --entrypoint sh myapp:latest\n```\n\n**네트워크 연결성**:\n```bash\ndocker network inspect myapp_default\ndocker exec myapp ping db\n```\n\n**볼륨 권한**:\n```bash\n# Dockerfile에서 수정\nRUN chown -R nodejs:nodejs /app/data\n```\n\n### Phase 6: Optimization\n\n#### 이미지 크기 축소\n\n**전략**:\n1. 더 작은 베이스 이미지 사용 (alpine > slim > debian)\n2. 빌드 도구 제외를 위해 멀티 스테이지 빌드 활용\n3. 레이어 수를 줄이기 위해 RUN 명령 결합\n4. 동일한 레이어 내에서 정리(Clean up) 수행\n5. .dockerignore 활용\n\n**예시**:\n```dockerfile\n# ✅ 좋음: 결합 및 정리 완료\nRUN apt-get update && \\\n    apt-get install -y --no-install-recommends package1 && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*\n```\n\n#### 빌드 성능\n\n```bash\n# BuildKit 활성화\nexport DOCKER_BUILDKIT=1\n\n# 캐시 마운트(Cache mounts) 사용\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements.txt\n\n# 병렬 빌드\ndocker-compose build --parallel\n```\n\n### Phase 7: Production Deployment\n\n#### 운영 환경용 Dockerfile\n\n```dockerfile\nFROM node:18-alpine AS production\n\n# 보안: non-root 사용자\nRUN addgroup -g 1001 -S nodejs && adduser -S nodejs -u 1001\n\nWORKDIR /app\nCOPY --from=builder --chown=nodejs:nodejs /app/dist ./dist\nUSER nodejs\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=3s \\\n  CMD node healthcheck.js\n\nEXPOSE 3000\nCMD [\"node\", \"dist/index.js\"]\n```\n\n#### 배포 명령\n\n```bash\n# Registry용 태그 지정\ndocker tag myapp:latest registry.example.com/myapp:v1.0.0\n\n# Registry로 Push\ndocker push registry.example.com/myapp:v1.0.0\n\n# 배포\ndocker-compose pull && docker-compose up -d\n\n# 무중단 업데이트 (Rolling update)\ndocker-compose up -d --no-deps --build app\n```\n\n## Common Patterns\n\n### Full-Stack Application\n- Frontend + Backend + Database + Redis\n- `examples/docker-compose.yml` 참조\n\n### Microservices\n- API Gateway + Multiple Services + Message Queue\n- 네트워크 격리(Network isolation) 및 서비스 검색(Service discovery)\n\n### 개발 시 Hot Reload 적용\n- 소스 코드를 위한 볼륨 마운트\n- 개발 설정을 위한 override 파일\n\n## Best Practices Summary\n\n### Security (보안)\n✅ `latest`가 아닌 구체적인 이미지 버전 사용\n✅ non-root 사용자로 실행\n✅ 민감한 데이터에는 secrets management 사용\n✅ 이미지의 취약점 스캔 실시\n✅ 최소한의 베이스 이미지 사용\n\n### Performance (성능)\n✅ 멀티 스테이지 빌드 사용\n✅ 레이어 캐싱 최적화\n✅ .dockerignore 사용\n✅ RUN 명령 결합\n✅ BuildKit 사용\n\n### Development (개발)\n✅ 다중 컨테이너 앱에 docker-compose 사용\n✅ hot-reload를 위해 볼륨(Volumes) 사용\n✅ health checks 구현\n✅ 적절한 종속성 순서 적용\n\n### Production (운영)\n✅ 재시작 정책(Restart policies) 설정\n✅ 오케스트레이션(Swarm, Kubernetes) 사용\n✅ health checks로 모니터링\n✅ Reverse proxy 사용\n✅ Rolling updates 구현\n\n## Helper Resources\n\n- **scripts/docker_helper.sh**: 컨테이너 조사, health checks, 자동화\n- **examples/Dockerfile.multi-stage**: Node.js, Python, Go, Java, Rust용 템플릿\n- **examples/docker-compose.yml**: 전체 기능을 갖춘 멀티 서비스 설정\n- **examples/.dockerignore**: 포괄적인 무시(Ignore) 패턴\n\n## Quick Reference\n\n### Essential Commands\n\n```bash\n# 빌드\ndocker build -t myapp .\ndocker-compose build\n\n# 실행\ndocker run -d -p 3000:3000 myapp\ndocker-compose up -d\n\n# 로그\ndocker logs -f myapp\ndocker-compose logs -f\n\n# 실행 (Execute)\ndocker exec -it myapp sh\ndocker-compose exec app sh\n\n# 중지\ndocker-compose down\n\n# 정리 (Clean)\ndocker system prune -a\n```\n\n### Debugging\n\n```bash\n# 조사 (Inspect)\ndocker inspect myapp\n\n# 상태 (Stats)\ndocker stats myapp\n\n# 네트워크\ndocker network inspect bridge\n\n# 볼륨\ndocker volume ls\n```\n",
        "icartsh-plugin/skills/dotnet-build/SKILL.md": "---\nname: dotnet-build\nversion: 0.2.0\nkind: cli\ndescription: dotnet CLI를 사용하여 .NET 솔루션/프로젝트를 빌드합니다. 컴파일, 종속성 복원 또는 아티팩트 빌드 작업 시 사용합니다.\ninputs:\n  target: [solution, project, all]\n  configuration: [Debug, Release]\n  project_path: string\ncontracts:\n  success: '에러 없이 빌드 완료; bin/ 디렉토리에 아티팩트 생성'\n  failure: '0이 아닌 종료 코드 또는 컴파일 에러 발생'\n---\n\n# .NET Build Skill (Entry Map)\n\n> **목표:** 정확한 빌드 절차를 안내합니다.\n\n## 빠른 시작 (택일)\n\n- **전체 솔루션 빌드** → `references/build-solution.md`\n- **종속성만 복원** → `references/restore-deps.md`\n\n## 사용 시기\n\n- .NET 코드 컴파일 (`.csproj`, `.sln` 파일)\n- NuGet 패키지 및 종속성 복원\n- Debug/Release 구성 빌드\n- 빌드 아티팩트(바이너리, 어셈블리) 생성\n\n**다음의 경우에는 사용하지 마세요:** 테스트 (dotnet-test), 포맷팅 (code-format), 또는 분석 (code-analyze)\n\n## 입력 및 출력 (Inputs & Outputs)\n\n**입력:** `target` (solution/project/all), `configuration` (Debug/Release), `project_path` (기본값: ./dotnet/PigeonPea.sln)\n\n**출력:** `artifact_path` (bin/ 디렉토리), `build_log`, 종료 코드 (0=성공)\n\n**가드레일:** ./dotnet 디렉토리 내에서만 작업하며, bin/obj/ 디렉토리는 커밋하지 않습니다. 멱등성(idempotent) 있는 빌드를 지향합니다.\n\n## 탐색 (Navigation)\n\n**1. 전체 솔루션 빌드** → [`references/build-solution.md`](references/build-solution.md)\n\n- 복제(Cloning) 후 첫 빌드, 테스트 전 빌드, 릴리스 아티팩트 생성 시\n\n**2. 종속성만 복원** → [`references/restore-deps.md`](references/restore-deps.md)\n\n- 개발 환경 설정, 누락된 패키지 수정, NuGet 트러블슈팅 시\n\n## 일반적인 패턴 (Common Patterns)\n\n### 빠른 빌드 (Debug)\n\n```bash\ncd ./dotnet\ndotnet build PigeonPea.sln\n```\n\n### 빠른 빌드 (Release)\n\n```bash\ncd ./dotnet\ndotnet build PigeonPea.sln --configuration Release\n```\n\n### 복원 후 빌드 (Restore then Build)\n\n```bash\ncd ./dotnet\ndotnet restore PigeonPea.sln\ndotnet build PigeonPea.sln --no-restore\n```\n\n### Clean 후 Rebuild\n\n```bash\ncd ./dotnet\ndotnet clean PigeonPea.sln\ndotnet build PigeonPea.sln\n```\n\n### 특정 프로젝트 빌드\n\n```bash\ncd ./dotnet\ndotnet build console-app/PigeonPea.Console.csproj\n```\n\n### 디버깅을 위한 상세 빌드 (Verbose Build)\n\n```bash\ncd ./dotnet\ndotnet build PigeonPea.sln --verbosity detailed\n```\n\n## 트러블슈팅 (Troubleshooting)\n\n**빌드 실패:** 에러 메시지를 확인하세요. 상세한 에러 처리는 `references/build-solution.md`를 참조하세요.\n\n**종속성 누락:** `dotnet restore`를 실행하세요. `references/restore-deps.md`를 참조하세요.\n\n**NU1301 (service index):** NuGet에 접속할 수 없습니다. `references/restore-deps.md`를 확인하세요.\n\n**빌드 속도 저하:** `--no-restore`, `-m` (병렬 처리), 또는 `/p:RunAnalyzers=false`를 사용하세요. `references/build-solution.md`를 참조하세요.\n\n**오래된 아티팩트:** `dotnet clean`을 실행한 후 다시 빌드하세요.\n\n## 성공 지표 (Success Indicators)\n\n```\nBuild succeeded.\n    0 Warning(s)\n    0 Error(s)\n```\n\n아티팩트 위치: `./dotnet/{ProjectName}/bin/{Configuration}/net9.0/`\n\n## 통합 (Integration)\n\n**빌드 후:** dotnet-test (테스트), code-analyze (정적 분석)\n**빌드 전:** code-format (스타일 수정)\n\n## 관련 링크 (Related)\n\n- [`./dotnet/README.md`](../../../dotnet/README.md) - 프로젝트 구조\n- [`./dotnet/ARCHITECTURE.md`](../../../dotnet/ARCHITECTURE.md) - 아키텍처\n- [`.pre-commit-config.yaml`](../../../.pre-commit-config.yaml) - Pre-commit hooks\n",
        "icartsh-plugin/skills/dotnet-build/references/build-solution.md": "# Build .NET Solution - Detailed Procedure\n\n## Overview\n\nThis guide provides step-by-step instructions for building the entire PigeonPea.sln solution, including all projects (console-app, shared-app, windows-app) and their test projects.\n\n## Prerequisites\n\n- **.NET SDK 9.0** or later (check: `dotnet --version`)\n- Solution file: `./dotnet/PigeonPea.sln`\n- Projects: console-app, shared-app, windows-app (+ test projects, benchmarks)\n\n## Standard Build Flow\n\n### Step 1: Navigate to .NET Directory\n\n```bash\ncd ./dotnet\n```\n\nAll build commands should be run from the `./dotnet` directory.\n\n### Step 2: Restore Dependencies (First Time)\n\n```bash\ndotnet restore PigeonPea.sln\n```\n\nDownloads NuGet packages, restores references, creates `obj/` directories.\n\n### Step 3: Build Solution (Debug)\n\n```bash\ndotnet build PigeonPea.sln\n```\n\nCompiles C# files, generates assemblies, creates artifacts in `bin/Debug/net9.0/`.\n\n### Step 4: Build Solution (Release)\n\n```bash\ndotnet build PigeonPea.sln -c Release --no-restore\n```\n\n`-c Release`: Optimized build for production. `--no-restore`: Skip restore (faster).\n\n## Output Locations\n\nArtifacts: `./dotnet/{ProjectName}/bin/{Configuration}/net9.0/`\n\nExample: `./dotnet/console-app/bin/Debug/net9.0/PigeonPea.Console.dll`\n\n**Important:** Never commit `bin/` or `obj/` (excluded in `.gitignore`).\n\n## Build Options\n\n```bash\n# Usage: dotnet build <SOLUTION|PROJECT> [options]\n\n# Common flags\n--no-restore                       # Skip restore\n-m                                 # Parallel build\n--verbosity <level>                # Verbosity: q[uiet], m[inimal], n[ormal], d[etailed], diag[nostic]\n/p:RunAnalyzers=false              # Skip analyzers\n/p:TreatWarningsAsErrors=true      # Warnings as errors\n\n# MSBuild properties\n/p:Version=1.2.3                   # Set version\n/p:Deterministic=true              # Reproducible builds\n```\n\n## Common Errors and Solutions\n\n### Error: NU1301 - Unable to load service index\n\n**Full error:**\n\n```\nerror NU1301: Unable to load the service index for source https://api.nuget.org/v3/index.json\n```\n\n**Cause:** NuGet package source is unreachable (network issue, firewall, proxy)\n\n**Solutions:**\n\n1. Check internet connection\n2. Check NuGet sources:\n\n   ```bash\n   dotnet nuget list source\n   ```\n\n3. Add or update NuGet source:\n\n   ```bash\n   dotnet nuget add source https://api.nuget.org/v3/index.json -n nuget.org\n   ```\n\n4. Use specific source:\n\n   ```bash\n   dotnet restore --source https://api.nuget.org/v3/index.json\n   ```\n\n5. Check proxy settings in `nuget.config`\n\n### Error: CS0246 - Type not found\n\n**Cause:** Missing package/project reference\n\n**Fix:** Check `.csproj` for references, run `dotnet restore`\n\n### Error: MSB4018 - Build failed\n\n**Cause:** Corrupted cache\n\n**Fix:** Run `dotnet clean`. If issues persist, manually remove the `bin` and `obj` directories from project folders before rebuilding.\n\n### Error: CSxxxx - Compilation errors\n\n**Cause:** Syntax/type errors\n\n**Fix:** Read error (shows file:line:column), fix code, rebuild\n\n### Error: Circular references\n\n**Cause:** Projects reference each other (A → B → A)\n\n**Fix:** Refactor to break cycle, extract shared code\n\n## Performance Tips\n\n1. Use `--no-restore` after restore: `dotnet build --no-restore`\n2. Enable parallel: `dotnet build -m`\n3. Skip analyzers: `dotnet build /p:RunAnalyzers=false` (re-enable before commit!)\n4. Build specific project: `dotnet build console-app/PigeonPea.Console.csproj`\n5. CI/CD: Cache NuGet packages, separate restore and build steps\n\n## Integration with Pre-commit Hooks\n\nBefore committing:\n\n```bash\n(cd ./dotnet && dotnet build PigeonPea.sln)\npre-commit run --all-files\n```\n\nHooks run dotnet format, security checks, file validation.\n\n## Advanced Scenarios\n\n**Multiple frameworks:** `dotnet build --framework net9.0`\n\n**Custom configuration:** Define in `.csproj`, build with `dotnet build -c Staging`\n\n**Deterministic builds:** `dotnet build /p:Deterministic=true /p:ContinuousIntegrationBuild=true`\n\n**From different directory:** `dotnet build ./dotnet/PigeonPea.sln`\n\n## Verification Steps\n\n1. Check exit code: `echo $?` (should be 0)\n2. Output shows \"Build succeeded. 0 Warning(s) 0 Error(s)\"\n3. Artifacts exist: `ls ./dotnet/{ProjectName}/bin/{Configuration}/net9.0/`\n4. Review and address any warnings before release\n\n## Related Procedures\n\n- **Restore dependencies only:** See [`restore-deps.md`](restore-deps.md)\n- **Run tests after build:** Use `dotnet-test` skill\n- **Format code before build:** Use `code-format` skill\n- **Analyze code quality:** Use `code-analyze` skill\n\n## Quick Reference\n\n```bash\n# Standard build sequence\ncd ./dotnet\ndotnet restore PigeonPea.sln\ndotnet build PigeonPea.sln\n\n# Release build\ndotnet build PigeonPea.sln -c Release --no-restore\n\n# Clean and rebuild\ndotnet clean PigeonPea.sln\ndotnet build PigeonPea.sln\n\n# Verbose build for troubleshooting\ndotnet build PigeonPea.sln --verbosity detailed\n```\n",
        "icartsh-plugin/skills/dotnet-build/references/restore-deps.md": "# Restore Dependencies - Detailed Procedure\n\n## Overview\n\nThis guide covers NuGet package restoration for the PigeonPea solution, including how to restore packages, troubleshoot issues, configure package sources, and manage dependencies.\n\n## What is Package Restoration?\n\nDownloads and installs NuGet packages (direct refs, transitive deps, tools, analyzers).\n\n## When to Restore\n\nAfter cloning, after pulling changes, package not found errors, switching branches, clearing cache, updating packages.\n\n## Prerequisites\n\n.NET SDK 9.0+, internet connection, proper network config (if behind proxy)\n\n## Standard Restore Procedure\n\n### Step 1: Navigate to Solution Directory\n\n```bash\ncd ./dotnet\n```\n\n### Step 2: Restore All Packages\n\n```bash\ndotnet restore PigeonPea.sln\n```\n\nCreates `obj/project.assets.json` (lock file), `obj/project.nuget.cache`, and MSBuild files.\n\n## Package Sources\n\n```bash\ndotnet nuget list source                     # View sources\ndotnet nuget add source <url> --name <name>  # Add source\ndotnet nuget remove source <name>            # Remove source\ndotnet nuget enable/disable source <name>    # Enable/disable\n```\n\n## NuGet Configuration\n\nConfig hierarchy: project-level (`nuget.config`), user-level (`~/.nuget/NuGet/NuGet.Config`), computer-level.\n\nSample `nuget.config`: Add package sources, credentials, and global packages folder.\n\n## Restore Options\n\n```bash\ndotnet restore --force                    # Re-download everything\ndotnet restore --no-cache                 # Don't use HTTP cache\ndotnet restore --source <url>             # Specific source\ndotnet restore --disable-parallel         # For debugging\ndotnet restore --verbosity detailed       # Verbose output\ndotnet restore --runtime linux-x64        # Specific runtime\n```\n\n## Common Errors and Solutions\n\n### Error: NU1301 - Service index unavailable\n\n**Causes:** No internet, NuGet.org down, firewall, proxy needed, DNS issues\n\n**Solutions:**\n\n- Check connection: `ping api.nuget.org`\n- Check status: https://status.nuget.org/\n- Configure proxy: `export HTTP_PROXY=http://proxy:8080`\n- Clear cache: `dotnet nuget locals http-cache --clear`\n\n### Error: NU1100 - Unable to resolve package\n\n**Causes:** Package doesn't exist, version doesn't exist, private feed, typo\n\n**Solutions:**\n\n- Verify on nuget.org: https://www.nuget.org/packages/{PackageName}\n- Check package versions on nuget.org\n- Fix typos in `.csproj`\n- Add private feed if needed\n\n### Error: NU1101 - Package not found\n\n**Fix:** Search on nuget.org, fix name in `.csproj`, check if renamed\n\n### Error: NU1102 - Version not found\n\n**Fix:** Check available versions on nuget.org, update `.csproj`, or use `--include-prerelease` if the desired version is a pre-release.\n\n### Error: NU1107 - Version conflict\n\n**Fix:** Unify versions, add explicit reference, or use `Directory.Build.props`\n\n### Error: NU1605 - Downgrade detected\n\n**Fix:** Update to higher version in `.csproj`\n\n### Error: Assets file not found\n\n**Fix:** Run `dotnet restore`, or clean and restore if persists\n\n## Package Cache Management\n\n**Cache locations:** `~/.nuget/packages` (global), `~/.local/share/NuGet/v3-cache` (HTTP), temp\n\n```bash\ndotnet nuget locals all --list              # View locations\ndotnet nuget locals http-cache --clear      # Clear HTTP cache\ndotnet nuget locals global-packages --clear # Clear global (slow!)\ndotnet nuget locals all --clear             # Clear all\n```\n\n**When to clear:** Corrupt package, debugging, network changes, disk space\n\n**Warning:** Clearing global cache requires re-downloading all packages.\n\n## Dependency Management\n\n```bash\ndotnet list package              # View all dependencies\ndotnet list package --outdated   # Check for updates\ndotnet list package --vulnerable # Security vulnerabilities\n```\n\nUpdate: Edit `.csproj` package version, then `dotnet restore`\n\n## Performance Optimization\n\n1. Don't clear cache unnecessarily\n2. Parallel restore enabled by default\n3. Separate restore and build: `dotnet restore` then `dotnet build --no-restore`\n4. CI/CD: Cache `~/.nuget/packages`, use `--locked-mode` for deterministic builds\n\n## Advanced Scenarios\n\n**Restore specific project:** `dotnet restore console-app/PigeonPea.Console.csproj`\n\n**Lock file (reproducible):** `dotnet restore --use-lock-file` creates `packages.lock.json`\n\n**Private feeds:** Add with credentials (use credential providers, not clear text)\n\n**Offline restore:** `dotnet restore --no-http-cache` (if packages cached)\n\n**Fallback folders:** Configure in `nuget.config` for offline scenarios\n\n## Verification\n\n1. No errors in output\n2. `obj/project.assets.json` exists for each project\n3. Packages in `~/.nuget/packages`\n4. No missing package warnings\n\n## Troubleshooting Checklist\n\n- [ ] Internet connection\n- [ ] NuGet.org accessible (status.nuget.org)\n- [ ] Sources configured correctly\n- [ ] Proxy settings (if behind firewall)\n- [ ] Package names/versions correct in `.csproj`\n- [ ] Cache not corrupted\n- [ ] .NET SDK compatible\n\n## Related Procedures\n\n- **Build after restore:** See [`build-solution.md`](build-solution.md)\n- **Configure NuGet in CI/CD:** Check project documentation\n- **Manage package versions:** Use Directory.Build.props\n\n## Quick Reference\n\n```bash\n# Standard restore\ndotnet restore PigeonPea.sln\n\n# Force full restore\ndotnet restore PigeonPea.sln --force\n\n# Clear cache and restore\ndotnet nuget locals all --clear\ndotnet restore PigeonPea.sln\n\n# Restore with verbose output for troubleshooting\ndotnet restore --verbosity detailed\n\n# View package sources\ndotnet nuget list source\n\n# View package cache locations\ndotnet nuget locals all --list\n```\n",
        "icartsh-plugin/skills/dotnet-test/SKILL.md": "---\nname: dotnet-test\nversion: 0.1.0\nkind: cli\ndescription: dotnet CLI를 사용하여 .NET 테스트를 실행합니다. 유닛 테스트 실행, 코드 커버리지 리포트 생성 또는 벤치마크 수행 시 사용합니다.\ninputs:\n  target: [all, project, specific]\n  configuration: [Debug, Release]\n  coverage: ['true', 'false']\n  project_path: string\ncontracts:\n  success: '테스트 통과 (실패 없음); 요청 시 커버리지 데이터 생성 완료'\n  failure: '0이 아닌 종료 코드, 테스트 실패 또는 커버리지 생성 에러 발생'\n---\n\n# .NET Test Skill (Entry Map)\n\n> **목표:** 정확한 테스트 절차를 안내합니다.\n\n## 빠른 시작 (택일)\n\n- **유닛 테스트 실행** → `references/run-unit-tests.md`\n- **커버리지 리포트 생성** → `references/generate-coverage.md`\n- **벤치마크 실행** → `references/run-benchmarks.md`\n\n## 사용 시기\n\n- 유닛 테스트 실행 (xUnit, NUnit)\n- coverlet을 이용한 코드 커버리지 리포트 생성\n- BenchmarkDotNet을 이용한 성능 벤치마크 수행\n- 테스트 스위트를 통한 코드 변경 사항 검증\n- 테스트 실행 시간 측정\n\n**다음의 경우에는 사용하지 마세요:** 코드 빌드 (dotnet-build), 포맷팅 (code-format), 또는 정적 분석 (code-analyze)\n\n## 입력 및 출력 (Inputs & Outputs)\n\n**입력:** `target` (all/project/specific), `configuration` (Debug/Release), `coverage` (true/false), `project_path` (기본값: 모든 테스트 프로젝트)\n\n**출력:** 테스트 결과 (성공/실패 카운트), 커버리지 리포트 (요청 시), 벤치마크 결과, 종료 코드 (0=성공)\n\n**가드레일:** ./dotnet 디렉토리 내에서만 작업하며, 실패 사항을 명확히 보고하고, 허가 없이 테스트를 건너뛰지 않습니다.\n\n## 탐색 (Navigation)\n\n**1. 유닛 테스트 실행** → [`references/run-unit-tests.md`](references/run-unit-tests.md)\n\n- 모든 테스트 실행, 특정 프로젝트 테스트 실행, 테스트 실패 트러블슈팅\n\n**2. 커버리지 리포트 생성** → [`references/generate-coverage.md`](references/generate-coverage.md)\n\n- 커버리지 데이터 수집, 리포트 생성 (HTML/Cobertura), 커버리지 지표 분석\n\n**3. 벤치마크 실행** → [`references/run-benchmarks.md`](references/run-benchmarks.md)\n\n- 성능 벤치마크 수행, 결과 비교, 데이터 기반 최적화\n\n## 일반적인 패턴 (Common Patterns)\n\n### 모든 테스트 실행 (빠른 속도)\n\n```bash\ncd ./dotnet\ndotnet test\n```\n\n### 상세 출력을 포함한 테스트 실행\n\n```bash\ncd ./dotnet\ndotnet test --verbosity normal\n```\n\n### 특정 테스트 프로젝트 실행\n\n```bash\ncd ./dotnet\ndotnet test console-app.Tests/PigeonPea.Console.Tests.csproj\n```\n\n### 커버리지와 함께 테스트 실행\n\n```bash\ncd ./dotnet\ndotnet test --collect:\"XPlat Code Coverage\"\n```\n\n### 테스트 실행 및 커버리지 리포트 생성\n\n```bash\ncd ./dotnet\ndotnet test --collect:\"XPlat Code Coverage\" --results-directory ./TestResults\n# 커버리지 파일: ./TestResults/{guid}/coverage.cobertura.xml\n```\n\n### 이름으로 테스트 필터링\n\n```bash\ncd ./dotnet\ndotnet test --filter \"FullyQualifiedName~FrameTests\"\n```\n\n### 카테고리로 테스트 필터링\n\n```bash\ncd ./dotnet\ndotnet test --filter \"Category=Unit\"\n```\n\n### Release 구정으로 테스트 실행\n\n```bash\ncd ./dotnet\ndotnet test --configuration Release\n```\n\n### 벤치마크 실행\n\n```bash\ncd ./dotnet/benchmarks\ndotnet run -c Release\n```\n\n## 트러블슈팅 (Troubleshooting)\n\n**테스트 실패:** Assertion 실패에 대한 테스트 출력을 확인하세요. 디버깅은 `references/run-unit-tests.md`를 참조하세요.\n\n**커버리지 미생성:** coverlet.collector가 설치되어 있는지 확인하세요. `references/generate-coverage.md`를 참조하세요.\n\n**벤치마크 실행 실패:** Release 구성을 사용해야 합니다. `references/run-benchmarks.md`를 참조하세요.\n\n**테스트 실행 속도 저하:** 테스트 필터 사용, 병렬 실행 또는 빌드 후 `--no-build` 옵션을 사용하세요.\n\n**테스트 발견 실패:** 프로젝트 참조를 확인하고 테스트 프레임워크 패키지가 설치되어 있는지 확인하세요.\n\n## 성공 지표 (Success Indicators)\n\n```\nPassed!  - Failed:     0, Passed:    42, Skipped:     0, Total:    42\n```\n\n테스트 아티팩트 위치: `./dotnet/TestResults/`\n\n커버리지 리포트 위치: `./dotnet/TestResults/coverage.cobertura.xml`\n\n## 통합 (Integration)\n\n**테스트 전:** dotnet-build (코드가 빌드되었는지 확인)\n**테스트 후:** code-analyze (정적 분석), code-review (품질 검사)\n\n## 테스트 프레임워크\n\n이 저장소는 다음을 사용합니다:\n\n- 유닛 테스트를 위한 **xUnit** (console-app.Tests, shared-app.Tests, windows-app.Tests)\n- 코드 커버리지를 위한 **coverlet.collector**\n- 성능 벤치마크를 위한 **BenchmarkDotNet**\n\n## 관련 링크 (Related)\n\n- [`./dotnet/README.md`](../../../dotnet/README.md) - 프로젝트 구조\n- [`./dotnet/ARCHITECTURE.md`](../../../dotnet/ARCHITECTURE.md) - 아키텍처\n- [`.pre-commit-config.yaml`](../../../.pre-commit-config.yaml) - Pre-commit hooks\n- [`dotnet-build` skill](../dotnet-build/SKILL.md) - 빌드 스킬\n",
        "icartsh-plugin/skills/dotnet-test/references/generate-coverage.md": "# Generate Code Coverage - Detailed Procedure\n\n## Overview\n\nThis guide covers generating code coverage reports for the PigeonPea solution using coverlet.collector (integrated with dotnet test) and optional HTML report generation with ReportGenerator.\n\n## What is Code Coverage?\n\nCode coverage measures which lines of code are executed during test runs, helping identify untested code paths.\n\n**Coverage Metrics:**\n\n- **Line Coverage:** Percentage of lines executed\n- **Branch Coverage:** Percentage of conditional branches taken\n- **Method Coverage:** Percentage of methods called\n\n## Prerequisites\n\n- .NET SDK 9.0+\n- coverlet.collector package (already in test projects)\n- (Optional) ReportGenerator tool for HTML reports\n\n## Standard Coverage Flow\n\n### Step 1: Navigate to .NET Directory\n\n```bash\ncd ./dotnet\n```\n\n### Step 2: Run Tests with Coverage Collection\n\n```bash\ndotnet test --collect:\"XPlat Code Coverage\"\n```\n\nGenerates coverage data in Cobertura XML format.\n\n### Step 3: Locate Coverage File\n\n```bash\n# Coverage saved in ./TestResults/{guid}/coverage.cobertura.xml\nls -la ./TestResults/*/coverage.cobertura.xml\n```\n\n### Step 4: (Optional) Generate HTML Report\n\n```bash\n# Install ReportGenerator (first time only)\ndotnet tool install -g dotnet-reportgenerator-globaltool\n\n# Generate HTML report\nreportgenerator \\\n  -reports:\"./TestResults/*/coverage.cobertura.xml\" \\\n  -targetdir:\"./TestResults/CoverageReport\" \\\n  -reporttypes:\"Html\"\n\n# Open report\nopen ./TestResults/CoverageReport/index.html\n```\n\n## Coverage Collection Options\n\n```bash\n# Basic coverage\ndotnet test --collect:\"XPlat Code Coverage\"\n\n# Coverage with custom results directory\ndotnet test --collect:\"XPlat Code Coverage\" --results-directory ./TestResults\n\n# Coverage in Release configuration\ndotnet test --collect:\"XPlat Code Coverage\" --configuration Release\n\n# Coverage for specific project\ndotnet test console-app.Tests/PigeonPea.Console.Tests.csproj --collect:\"XPlat Code Coverage\"\n\n# Coverage with verbose output\ndotnet test --collect:\"XPlat Code Coverage\" --verbosity normal\n```\n\n## Coverage Configuration\n\n### runsettings File (Optional)\n\nCreate `./dotnet/coverage.runsettings` for advanced configuration:\n\n```xml\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<RunSettings>\n  <DataCollectionRunSettings>\n    <DataCollectors>\n      <DataCollector friendlyName=\"XPlat code coverage\">\n        <Configuration>\n          <Format>cobertura,opencover,json</Format>\n          <Exclude>[*.Tests]*</Exclude>\n          <ExcludeByAttribute>Obsolete,GeneratedCode,CompilerGenerated</ExcludeByAttribute>\n          <ExcludeByFile>**/Migrations/*.cs</ExcludeByFile>\n          <IncludeTestAssembly>false</IncludeTestAssembly>\n        </Configuration>\n      </DataCollector>\n    </DataCollectors>\n  </DataCollectionRunSettings>\n</RunSettings>\n```\n\nUse runsettings:\n\n```bash\ndotnet test --collect:\"XPlat Code Coverage\" --settings ./dotnet/coverage.runsettings\n```\n\n### Exclude Test Assemblies\n\nBy default, test assemblies are excluded from coverage. To include:\n\n```bash\ndotnet test --collect:\"XPlat Code Coverage\" /p:IncludeTestAssembly=true\n```\n\n## Report Formats\n\n### Cobertura XML (Default)\n\n```bash\ndotnet test --collect:\"XPlat Code Coverage\"\n# Output: ./TestResults/{guid}/coverage.cobertura.xml\n```\n\nUsed by CI/CD tools (Azure DevOps, GitHub Actions).\n\n### OpenCover XML\n\n```bash\ndotnet test --collect:\"XPlat Code Coverage\" -- DataCollectionRunSettings.DataCollectors.DataCollector.Configuration.Format=opencover\n```\n\n### JSON Format\n\n```bash\ndotnet test --collect:\"XPlat Code Coverage\" -- DataCollectionRunSettings.DataCollectors.DataCollector.Configuration.Format=json\n```\n\n### Multiple Formats\n\n```bash\ndotnet test --collect:\"XPlat Code Coverage\" -- DataCollectionRunSettings.DataCollectors.DataCollector.Configuration.Format=cobertura,opencover,json\n```\n\n## HTML Report Generation\n\n### Install ReportGenerator\n\n```bash\n# Global tool (recommended)\ndotnet tool install -g dotnet-reportgenerator-globaltool\n\n# Local tool (project-specific)\ndotnet tool install dotnet-reportgenerator-globaltool\n```\n\n### Generate HTML Report\n\n```bash\n# Basic HTML report\nreportgenerator \\\n  -reports:\"./TestResults/*/coverage.cobertura.xml\" \\\n  -targetdir:\"./TestResults/CoverageReport\" \\\n  -reporttypes:\"Html\"\n\n# HTML with badges\nreportgenerator \\\n  -reports:\"./TestResults/*/coverage.cobertura.xml\" \\\n  -targetdir:\"./TestResults/CoverageReport\" \\\n  -reporttypes:\"Html;Badges\"\n\n# Multiple formats (HTML + XML)\nreportgenerator \\\n  -reports:\"./TestResults/*/coverage.cobertura.xml\" \\\n  -targetdir:\"./TestResults/CoverageReport\" \\\n  -reporttypes:\"Html;XmlSummary;Badges\"\n```\n\n### Open Report\n\n```bash\n# Linux/macOS\nopen ./TestResults/CoverageReport/index.html\n\n# Windows\nstart ./TestResults/CoverageReport/index.html\n\n# WSL\nexplorer.exe ./TestResults/CoverageReport/index.html\n```\n\n<!-- Trimmed analysis and best practices to satisfy validator size limit. See SKILL.md for reading results and thresholds. -->\n\n## Common Errors and Solutions\n\n### Error: No coverage data generated\n\n**Cause:** coverlet.collector not installed or not configured\n\n**Solutions:**\n\n1. Check package in test project:\n\n   ```bash\n   dotnet list console-app.Tests/PigeonPea.Console.Tests.csproj package | grep coverlet\n   ```\n\n2. Add coverlet.collector if missing:\n\n   ```bash\n   dotnet add console-app.Tests/PigeonPea.Console.Tests.csproj package coverlet.collector\n   ```\n\n3. Ensure test project has `<IsTestProject>true</IsTestProject>` in .csproj\n\n### Error: Coverage file not found\n\n**Cause:** Coverage collection disabled or failed\n\n**Solutions:**\n\n1. Verify coverage enabled:\n\n   ```bash\n   dotnet test --collect:\"XPlat Code Coverage\" --verbosity detailed\n   ```\n\n2. Check TestResults directory:\n   ```bash\n   find ./TestResults -name \"coverage.cobertura.xml\"\n   ```\n\n### Error: ReportGenerator command not found\n\n**Cause:** ReportGenerator not installed or not in PATH\n\n**Fix:**\n\n```bash\n# Install globally\ndotnet tool install -g dotnet-reportgenerator-globaltool\n\n# Verify installation\nreportgenerator --help\n```\n\n### Error: Low coverage unexpectedly\n\n**Cause:** Tests not covering code, or exclusions too broad\n\n**Solutions:**\n\n1. Review HTML report to identify uncovered lines\n2. Add tests for uncovered code paths\n3. Check exclusions in runsettings (ensure not excluding too much)\n\n<!-- Best practices moved to SKILL.md to reduce size. -->\n\n## Exclusions\n\n### Exclude Classes/Methods\n\n```csharp\n[ExcludeFromCodeCoverage]\npublic class GeneratedClass\n{\n    // Not covered\n}\n\n[ExcludeFromCodeCoverage]\npublic void DebugMethod()\n{\n    // Not covered\n}\n```\n\n### Exclude via runsettings\n\n```xml\n<Exclude>[*.Tests]*,[*]*.Migrations.*</Exclude>\n<ExcludeByAttribute>Obsolete,GeneratedCode,CompilerGenerated</ExcludeByAttribute>\n<ExcludeByFile>**/Migrations/*.cs,**/Generated/*.cs</ExcludeByFile>\n```\n\n## Additional References\n\n<!-- Trimmed for size to satisfy validator. See SKILL.md for overview and CI/CD integration examples. -->\n",
        "icartsh-plugin/skills/dotnet-test/references/run-benchmarks.md": "# Run Benchmarks - Detailed Procedure\n\n## Overview\n\nThis guide covers running performance benchmarks for the PigeonPea solution using BenchmarkDotNet, a powerful library for benchmarking .NET code with high precision and statistical analysis.\n\n## What are Benchmarks?\n\nBenchmarks measure code performance (execution time, memory allocation, throughput) to:\n\n- Identify performance bottlenecks\n- Compare alternative implementations\n- Track performance over time\n- Validate optimization efforts\n\n## Prerequisites\n\n- .NET SDK 9.0+\n- Benchmark project: `./dotnet/benchmarks/PigeonPea.Benchmarks.csproj`\n- BenchmarkDotNet package (already configured)\n- **Release configuration** (required for accurate results)\n\n## Standard Benchmark Flow\n\n### Step 1: Navigate to Benchmarks Directory\n\n```bash\ncd ./dotnet/benchmarks\n```\n\n### Step 2: Build in Release Mode\n\n```bash\ndotnet build -c Release\n```\n\n**Critical:** Always use Release mode for benchmarks. Debug mode skews results.\n\n### Step 3: Run Benchmarks\n\n```bash\ndotnet run -c Release\n```\n\nBenchmarkDotNet executes benchmarks, performs warm-up, measurement iterations, and statistical analysis.\n\n### Step 4: Review Results\n\nResults are displayed in console and saved to `./BenchmarkDotNet.Artifacts/results/`.\n\n## Benchmark Execution\n\n### Run All Benchmarks\n\n```bash\ncd ./dotnet/benchmarks\ndotnet run -c Release\n```\n\n### Run Specific Benchmark Class\n\n```bash\ncd ./dotnet/benchmarks\ndotnet run -c Release --filter \"*StringBenchmarks*\"\n```\n\n### Run Specific Benchmark Method\n\n```bash\ncd ./dotnet/benchmarks\ndotnet run -c Release --filter \"*StringBenchmarks.Concat*\"\n```\n\n### Run with Custom Job\n\n```bash\ncd ./dotnet/benchmarks\ndotnet run -c Release -- --job short\n```\n\nJob options: `short`, `medium`, `long`, `verylong`\n\n## Benchmark Output\n\n### Console Output Example\n\n```\n| Method    | Mean      | Error    | StdDev   | Allocated |\n|---------- |----------:|---------:|---------:|----------:|\n| Concat    | 12.34 ns  | 0.21 ns  | 0.19 ns  | 40 B      |\n| Format    | 45.67 ns  | 0.89 ns  | 0.83 ns  | 64 B      |\n| Interpolate | 23.45 ns | 0.34 ns  | 0.32 ns  | 48 B      |\n```\n\n- **Method:** Benchmark method name\n- **Mean:** Average execution time\n- **Error:** Standard error of the mean\n- **StdDev:** Standard deviation\n- **Allocated:** Memory allocated per operation\n\n### Artifacts Location\n\n```\n./dotnet/benchmarks/BenchmarkDotNet.Artifacts/\n  results/\n    MyBenchmark-report.html     # HTML report\n    MyBenchmark-report.csv      # CSV data\n    MyBenchmark-report.md       # Markdown report\n  logs/\n    MyBenchmark.log             # Detailed log\n```\n\n## Writing Benchmarks\n\n### Basic Benchmark Structure\n\n```csharp\nusing BenchmarkDotNet.Attributes;\nusing BenchmarkDotNet.Running;\n\nnamespace PigeonPea.Benchmarks;\n\n[MemoryDiagnoser]\npublic class StringBenchmarks\n{\n    private const int Iterations = 100;\n\n    [Benchmark]\n    public string Concat()\n    {\n        var result = \"\";\n        for (int i = 0; i < Iterations; i++)\n            result += \"a\";\n        return result;\n    }\n\n    [Benchmark]\n    public string StringBuilder()\n    {\n        var sb = new System.Text.StringBuilder();\n        for (int i = 0; i < Iterations; i++)\n            sb.Append(\"a\");\n        return sb.ToString();\n    }\n\n    [Benchmark(Baseline = true)]\n    public string StringCreate()\n    {\n        return string.Create(Iterations, 'a', (span, c) =>\n        {\n            span.Fill(c);\n        });\n    }\n}\n\npublic class Program\n{\n    public static void Main(string[] args)\n    {\n        BenchmarkRunner.Run<StringBenchmarks>();\n    }\n}\n```\n\n### Benchmark Attributes\n\n```csharp\n[Benchmark]                  // Mark method as benchmark\n[Benchmark(Baseline = true)] // Mark as baseline for comparison\n[Arguments(10, 20)]          // Pass arguments to benchmark\n[Params(10, 100, 1000)]      // Run with multiple parameter values\n[IterationCount(10)]         // Custom iteration count\n[WarmupCount(5)]             // Custom warmup count\n```\n\n### Diagnosers\n\n```csharp\n[MemoryDiagnoser]           // Track memory allocations\n[ThreadingDiagnoser]        // Track threading info\n[EventPipeProfiler(...)]    // CPU profiling\n```\n\n## Benchmark Configuration\n\n### Global Configuration\n\n```csharp\nusing BenchmarkDotNet.Configs;\nusing BenchmarkDotNet.Jobs;\nusing BenchmarkDotNet.Toolchains.InProcess.Emit;\n\n[Config(typeof(Config))]\npublic class MyBenchmarks\n{\n    private class Config : ManualConfig\n    {\n        public Config()\n        {\n            AddJob(Job.Default\n                .WithRuntime(CoreRuntime.Core90)\n                .WithPlatform(Platform.X64)\n                .WithJit(Jit.RyuJit));\n\n            AddDiagnoser(MemoryDiagnoser.Default);\n            AddColumn(StatisticColumn.P95);\n        }\n    }\n}\n```\n\n### Job Configuration\n\n```csharp\n[SimpleJob(RuntimeMoniker.Net90)]\n[SimpleJob(RuntimeMoniker.Net80)]\npublic class MyBenchmarks\n{\n    // Compare performance across runtimes\n}\n```\n\n## Analyzing Results\n\n### Compare Baseline\n\n```\n| Method       | Mean     | Ratio |\n|------------- |---------:|------:|\n| Baseline     | 100.0 ns | 1.00  |\n| Optimized    | 50.0 ns  | 0.50  |\n| Alternative  | 150.0 ns | 1.50  |\n```\n\n- **Ratio:** Relative to baseline (0.50 = 2x faster, 1.50 = 1.5x slower)\n\n### Statistical Significance\n\nBenchmarkDotNet performs statistical analysis:\n\n- **Outliers:** Identified and can be removed\n- **Multimodal distribution:** Indicates interference (antivirus, background tasks)\n- **Confidence intervals:** 95% by default\n\n### Memory Analysis\n\n```\n| Method    | Allocated |\n|---------- |----------:|\n| Original  | 1024 B    |\n| Optimized | 64 B      |\n```\n\nLower allocation = less GC pressure = better performance.\n\n## Common Errors and Solutions\n\n### Error: \"Benchmarks must be run in Release mode\"\n\n**Cause:** Running in Debug configuration\n\n**Fix:**\n\n```bash\ndotnet run -c Release\n```\n\n### Error: \"No benchmarks found\"\n\n**Cause:** No methods decorated with `[Benchmark]` or benchmark class not passed to `BenchmarkRunner.Run`\n\n**Solutions:**\n\n1. Ensure methods have `[Benchmark]` attribute\n2. Check `Main` method calls `BenchmarkRunner.Run<YourBenchmarkClass>()`\n\n### Error: \"Benchmark throws exception\"\n\n**Cause:** Code in benchmark method throws unhandled exception\n\n**Solutions:**\n\n1. Run with detailed output:\n\n   ```bash\n   dotnet run -c Release -- --verbosity Detailed\n   ```\n\n2. Fix code in benchmark method\n3. Use `[GlobalSetup]` to initialize state safely\n\n### Warning: Multimodal distribution detected\n\n**Cause:** Performance variance due to background processes\n\n**Solutions:**\n\n1. Close unnecessary applications\n2. Disable antivirus during benchmarking\n3. Use longer warmup: `[WarmupCount(10)]`\n4. Re-run benchmarks\n\n### Warning: High variance\n\n**Cause:** Unstable execution environment\n\n**Solutions:**\n\n1. Ensure sufficient iterations (BenchmarkDotNet auto-adjusts)\n2. Run on dedicated hardware (not VM if possible)\n3. Disable CPU frequency scaling (performance mode)\n\n## Additional References\n\n<!-- Trimmed for size to satisfy validator. See SKILL.md for best practices, CI/CD integration, and example benchmarks. -->\n",
        "icartsh-plugin/skills/dotnet-test/references/run-unit-tests.md": "# Run Unit Tests - Detailed Procedure\n\n## Overview\n\nThis guide provides step-by-step instructions for running unit tests in the PigeonPea solution, including all test projects (console-app.Tests, shared-app.Tests, windows-app.Tests) using xUnit framework.\n\n## Prerequisites\n\n- **.NET SDK 9.0** or later (check: `dotnet --version`)\n- Solution file: `./dotnet/PigeonPea.sln`\n- Test projects: console-app.Tests, shared-app.Tests, windows-app.Tests\n- xUnit test framework (already configured in test projects)\n\n## Standard Test Flow\n\n### Step 1: Navigate to .NET Directory\n\n```bash\ncd ./dotnet\n```\n\nAll test commands should be run from the `./dotnet` directory.\n\n### Step 2: Build Solution (If Not Already Built)\n\n```bash\ndotnet build PigeonPea.sln\n```\n\nTests require compiled code. Build first if you haven't already.\n\n### Step 3: Run All Tests\n\n```bash\ndotnet test\n```\n\nDiscovers and runs all tests in solution, reports pass/fail status.\n\n### Step 4: Run with Verbose Output\n\n```bash\ndotnet test --verbosity normal\n```\n\nVerbosity levels: `quiet`, `minimal`, `normal`, `detailed`, `diagnostic`\n\n## Output Interpretation\n\n### Successful Test Run\n\n```\nPassed!  - Failed:     0, Passed:    42, Skipped:     0, Total:    42, Duration: 2 s\n```\n\n- **Failed**: Number of failing tests (should be 0)\n- **Passed**: Number of passing tests\n- **Skipped**: Tests marked with `[Skip]` attribute\n- **Total**: Sum of all tests\n- **Duration**: Total execution time\n\n### Failed Test Run\n\n```\nFailed!  - Failed:     3, Passed:    39, Skipped:     0, Total:    42, Duration: 2 s\n```\n\nIndividual failures are listed with stack traces showing assertion errors.\n\n## Test Options\n\n```bash\n# Usage: dotnet test [SOLUTION|PROJECT] [options]\n\n# Common flags\n--no-build                          # Skip build (faster if already built)\n--no-restore                        # Skip restore\n--verbosity <level>                 # Verbosity: q[uiet], m[inimal], n[ormal], d[etailed], diag[nostic]\n--configuration <config>            # Debug or Release (default: Debug)\n--filter <expression>               # Run filtered tests\n--logger <logger>                   # Test logger (trx, html, console)\n--results-directory <path>          # Output directory for test results\n--collect <datacollector>           # Enable data collection (e.g., \"XPlat Code Coverage\")\n```\n\n## Running Specific Tests\n\n### Run Specific Test Project\n\n```bash\ndotnet test console-app.Tests/PigeonPea.Console.Tests.csproj\ndotnet test shared-app.Tests/PigeonPea.Shared.Tests.csproj\ndotnet test windows-app.Tests/PigeonPea.Windows.Tests.csproj\n```\n\n### Filter by Test Name\n\n```bash\n# Run tests with \"Frame\" in the name\ndotnet test --filter \"FullyQualifiedName~Frame\"\n\n# Run exact test method\ndotnet test --filter \"FullyQualifiedName=PigeonPea.Console.Tests.Visual.FrameTests.ShouldCreateFrame\"\n```\n\n### Filter by Test Category/Trait\n\n```bash\n# Run tests with [Trait(\"Category\", \"Unit\")]\ndotnet test --filter \"Category=Unit\"\n\n# Run tests with [Trait(\"Category\", \"Integration\")]\ndotnet test --filter \"Category=Integration\"\n```\n\n### Filter by Namespace\n\n```bash\ndotnet test --filter \"FullyQualifiedName~PigeonPea.Console.Tests.Visual\"\n```\n\n### Combine Filters\n\n```bash\n# Run Unit tests in Visual namespace\ndotnet test --filter \"Category=Unit&FullyQualifiedName~Visual\"\n```\n\n## Test Execution Strategies\n\n### Quick Test (Skip Build)\n\n```bash\n# Build once\ndotnet build PigeonPea.sln\n\n# Run tests multiple times without rebuilding\ndotnet test --no-build\n```\n\n### Release Configuration\n\n```bash\ndotnet test --configuration Release\n```\n\nTests run with optimizations enabled. Use for performance testing.\n\n### Parallel Execution\n\nxUnit runs tests in parallel by default. To disable:\n\n```bash\ndotnet test -- xUnit.ParallelizeTestCollections=false\n```\n\n### Test Results Export\n\n```bash\n# TRX format (Visual Studio)\ndotnet test --logger \"trx;LogFileName=test-results.trx\"\n\n# HTML format\ndotnet test --logger \"html;LogFileName=test-results.html\"\n\n# Multiple loggers\ndotnet test --logger \"trx;LogFileName=test-results.trx\" --logger \"console;verbosity=detailed\"\n```\n\n## Common Errors and Solutions\n\n### Error: No test is available\n\n**Cause:** Test project not built, test discovery failed, or no tests in project\n\n**Solutions:**\n\n1. Ensure test project has tests:\n\n   ```bash\n   ls -la console-app.Tests/*.cs\n   ```\n\n2. Build solution:\n\n   ```bash\n   dotnet build PigeonPea.sln\n   ```\n\n3. Verify xUnit packages:\n   ```bash\n   dotnet list console-app.Tests/PigeonPea.Console.Tests.csproj package | grep xunit\n   ```\n\n### Error: Test host process crashed\n\n**Cause:** Unhandled exception in test initialization, missing dependencies, or memory issues\n\n**Solutions:**\n\n1. Run with detailed verbosity:\n\n   ```bash\n   dotnet test --verbosity detailed\n   ```\n\n2. Run tests individually to isolate problem:\n\n   ```bash\n   dotnet test --filter \"FullyQualifiedName~FrameTests.ShouldCreateFrame\"\n   ```\n\n3. Check for infinite loops or excessive memory usage in tests\n\n### Error: Test timeout\n\n**Cause:** Test takes too long to execute (default timeout: no limit, but xUnit has 10s warning)\n\n**Solutions:**\n\n1. Increase timeout in test:\n\n   ```csharp\n   [Fact(Timeout = 30000)] // 30 seconds\n   public void MyTest() { }\n   ```\n\n2. Optimize test code to run faster\n\n3. Mock slow dependencies (I/O, network, database)\n\n### Error: Collection fixture error\n\n**Cause:** xUnit collection fixture setup failed\n\n**Fix:** Check `ICollectionFixture<T>` implementation, ensure constructor doesn't throw\n\n### Error: Theory data not found\n\n**Cause:** `[MemberData]` or `[ClassData]` source not found\n\n**Fix:** Verify data source method/property exists and is public static\n\n## Debugging Tests\n\n### Run Single Test with Debugger\n\n```bash\n# In VS Code or Visual Studio, set breakpoint and use \"Debug Test\" button\n```\n\n### Output Debug Information\n\n```csharp\n[Fact]\npublic void MyTest()\n{\n    var result = MyMethod();\n    _output.WriteLine($\"Result: {result}\"); // xUnit ITestOutputHelper\n    Assert.Equal(42, result);\n}\n```\n\n### Conditional Breakpoints\n\nUse `System.Diagnostics.Debugger.Break()` to trigger debugger in specific conditions.\n\n## Performance Tips\n\n1. **Skip build after first run:** `dotnet test --no-build`\n2. **Filter to relevant tests:** Use `--filter` to run subset\n3. **Use Release build for benchmarks:** `dotnet test -c Release`\n4. **Disable parallel for debugging:** `-- xUnit.ParallelizeTestCollections=false`\n5. **Mock expensive operations:** Use Moq or similar for I/O, network, database\n\n## Additional References\n\n<!-- Trimmed for size to satisfy validator. See SKILL.md for pre-commit integration, test organization, and quick commands. -->\n",
        "icartsh-plugin/skills/error-detective/README.md": "# Error Detective Skill\n\nTRACE 프레임워크를 사용한 체계적인 디버깅 및 에러 해결.\n\n## 개요 (Overview)\n\nError Detective 스킬은 여러 프로그래밍 언어와 프레임워크에 걸쳐 에러를 디버깅하기 위한 포괄적이고 체계적인 접근 방식을 제공합니다. 초기 에러 발견부터 검증된 해결까지 전체 디버깅 과정을 안내합니다.\n\n## 핵심 구성 요소 (Core Components)\n\n### SKILL.md (593행)\n다음을 포함하는 메인 스킬 파일입니다:\n- TRACE 프레임워크 방법론 (Trace, Read, Analyze, Check, Execute)\n- 언어별 공통 에러 패턴 (Python, JavaScript, Java, Go)\n- 디버깅 워크플로우 및 모범 사례\n- 에러 심각도 분류\n- 고급 디버깅 기법\n\n### Scripts\n\n**debug_helper.py** - 다음과 같은 기능을 제공하는 Python 디버깅 유틸리티:\n- Python, JavaScript, Java 및 Go를 위한 스택 트레이스 파싱\n- 로그 파일 분석 및 에러 집계\n- 타임스탬프가 포함된 노트를 통한 디버그 세션 관리\n\n사용법:\n```bash\n# 스택 트레이스 파싱\npython scripts/debug_helper.py parse-trace error.log\n\n# 로그 분석\npython scripts/debug_helper.py analyze-log app.log --pattern ERROR\n\n# 디버그 세션 관리\npython scripts/debug_helper.py session start \"로그인 에러 조사\"\npython scripts/debug_helper.py session note \"다른 브라우저로 테스트 완료\"\npython scripts/debug_helper.py session close \"해결됨: CORS 헤더 추가\"\n```\n\n### Examples (예시)\n\n**debugging_workflow.md** - 전체 워크플로우 예시:\n- 예시 1: Python AttributeError (간단한 케이스)\n- 예시 2: JavaScript Promise rejection (비동기 이슈)\n- 예시 3: Java NullPointerException (다층적 이슈)\n\n**common_errors.md** - 포괄적인 에러 카탈로그:\n- Python 에러 (AttributeError, KeyError, IndexError, TypeError 등)\n- JavaScript 에러 (TypeError, ReferenceError, Promise rejections)\n- Java 에러 (NullPointerException, ClassCastException, ConcurrentModificationException)\n- 데이터베이스 에러 (연결 타임아웃, 데드락)\n- 네트워크/API 에러 (404, 500, CORS)\n\n**stack_traces.txt** - 주석이 달린 스택 트레이스 예시:\n- Python Django 스택 트레이스\n- JavaScript/Node.js 스택 트레이스\n- \"Caused by\"가 포함된 Java 스택 트레이스\n- Go panic 트레이스\n- 읽기 팁 및 인식할 패턴 포함\n\n## 스킬 특징\n\n- **복잡도**: 중간-복잡 (593행)\n- **대상 언어**: Python, JavaScript/TypeScript, Java, Go\n- **방법론**: TRACE 프레임워크, 체계적인 디버깅\n- **도구**: 스택 트레이스 파서, 로그 분석기, 세션 매니저\n\n## 사용 시기\n\n다음을 수행할 때 이 스킬을 활성화하세요:\n- 에러 또는 익셉션 디버깅\n- 스택 트레이스 분석\n- 운영 환경 실패 조사\n- 근본 원인 분석 수행\n- 간헐적 이슈 트러블슈팅\n- 디버깅 모범 사례 학습\n\n## 주요 특징\n\n### TRACE 프레임워크\n1. **T**race the Error - 완전한 에러 정보 캡처\n2. **R**ead the Error Message - 에러에서 모든 정보 추출\n3. **A**nalyze the Context - 더 넓은 컨텍스트 이해\n4. **C**heck for Root Cause - 기저 이슈 식별\n5. **E**xecute the Fix - 해결책 구현 및 검증\n\n### 디버깅 패턴\n- 여러 언어에 대한 스택 트레이스 분석\n- 에러 패턴 인식\n- 근본 원인 식별\n- 가설 테스트 방법론\n- 방지 전략\n\n### 실용적인 도구\n- 자동화된 스택 트레이스 파싱\n- 로그 분석 및 집계\n- 디버그 세션 추적\n- 에러 패턴 카탈로그\n\n## 파일 구조\n```\nerror-detective/\n├── SKILL.md                      # 메인 스킬 (593행)\n├── scripts/\n│   └── debug_helper.py          # 디버깅 유틸리티\n├── examples/\n│   ├── debugging_workflow.md    # 전체 워크플로우 예시\n│   ├── common_errors.md         # 에러 패턴 카탈로그\n│   └── stack_traces.txt         # 주석이 달린 스택 트레이스\n└── README.md                    # 현재 파일\n```\n\n## 품질 표준 (Quality Standards)\n\n✅ SKILL_CREATION_GUIDE.md 가이드라인 준수\n✅ 명확한 설명을 포함한 YAML frontmatter\n✅ 체계적인 방법론 (TRACE 프레임워크)\n✅ 다중 언어 지원\n✅ 실행 가능한 헬퍼 스크립트\n✅ 포괄적인 예시\n✅ 점진적 노출(progressive disclosure)을 통한 잘 조직된 구조\n\n---\n\nClaude Skills Creation Guide 표준에 따라 중간 복잡도 수준에 맞춰 작성되었습니다.\n",
        "icartsh-plugin/skills/error-detective/SKILL.md": "---\nname: error-detective\ndescription: TRACE 프레임워크(Trace, Read, Analyze, Check, Execute)를 사용한 체계적인 디버깅 및 에러 해결입니다. 에러 디버깅, 스택 트레이스(stack traces) 분석, 실패 조사, 근본 원인 분석(root cause analysis) 또는 운영 이슈 트러블슈팅 시 사용합니다.\n---\n\n# Error Detective - Systematic Debugging and Error Resolution\n\n## 개요 (Overview)\n\nError Detective는 에러를 효율적으로 식별, 분석 및 해결하기 위해 체계적인 방법론을 적용하는 종합적인 디버깅 SKILL입니다. TRACE 프레임워크와 구조화된 분석 기법을 사용하여 에러의 초기 발견부터 검증된 해결까지 디버깅 과정을 안내합니다.\n\n## 핵심 역량 (Core Capabilities)\n\n### 스택 트레이스 분석 (Stack Trace Analysis)\n- 여러 언어에 걸친 스택 트레이스 파싱 및 해석\n- 근본 원인(root cause)과 증상 에러(symptom errors) 구분\n- 관련 파일 경로 및 라인 번호 추출\n- 호출 체인 및 에러 전파 과정 이해\n\n### 에러 패턴 인식 (Error Pattern Recognition)\n- 유형별 에러 분류 (syntax, runtime, logic, integration)\n- 공통 에러 패턴 및 안티 패턴 식별\n- 프레임워크별 특화 에러 인식\n- 에러를 발생 가능한 근본 원인에 매핑\n\n### 근본 원인 분석 (Root Cause Analysis)\n- 증상과 기저 이슈의 구분\n- 에러 체인을 따라 원래 발생 지점 추적\n- 환경 이슈와 코드 이슈의 실별\n- 설정 및 종속성 문제 감지\n\n### 디버깅 워크플로우 관리\n- 구조화된 조사 프로세스\n- 가설 생성 및 테스트\n- 이해도에 대한 반복적 정밀화\n- 조사 결과 및 해결책 문서화\n\n## TRACE 프레임워크\n\nTRACE는 어떤 에러든 디버깅할 수 있는 체계적인 5단계 접근 방식입니다:\n\n### T - Trace the Error (에러 추적)\n**목표**: 완전한 에러 정보와 컨텍스트 캡처\n\n1. **전체 에러 메시지 수집**\n   - 전체 스택 트레이스 (처음 몇 줄만이 아닌 전체)\n   - 에러 유형 및 메시지\n   - 타임스탬프 및 발생 빈도\n   - 에러가 발생한 환경\n\n2. **에러 위치 식별**\n   - 정확한 파일 및 라인 번호\n   - 에러가 발생한 함수 또는 메서드\n   - 코드 컨텍스트 (주변 라인)\n   - 진입점부터 에러 지점까지의 호출 스택\n\n3. **재현 단계 갈무리**\n   - 재현을 위한 최소한의 단계\n   - 사용된 입력 데이터 또는 파라미터\n   - 기대 결과 vs. 실제 동작\n   - 재현의 일관성 (항상 발생, 간헐적 발생, 드물게 발생)\n\n### R - Read the Error Message (에러 메시지 읽기)\n**목표**: 에러 자체에서 모든 정보 추출\n\n1. **에러 구성 요소 파싱**\n   - 에러 유형/클래스 (TypeError, ValueError 등)\n   - 에러 메시지 내용\n   - 권장 수정 사항 (제공되는 경우)\n   - 관련 에러 또는 경고\n\n2. **에러 시맨틱 이해**\n   - 해당 언어/프레임워크에서 해당 에러 유형이 의미하는 바\n   - 어떤 조건이 이 에러를 유발하는지\n   - 에러 메시지가 구체적으로 무엇을 말하고 있는지\n   - 에러 코드 또는 상태 코드\n\n3. **에러 카테고리 식별**\n   - Syntax error (코드 파싱 불가)\n   - Runtime error (실행 중 크래시 발생)\n   - Logic error (결과가 틀림, 크래시 없음)\n   - Integration error (외부 시스템 실패)\n   - Performance error (타임아웃, 리소스 고갈)\n\n### A - Analyze the Context (컨텍스트 분석)\n**목표**: 에러 주변의 더 넓은 컨텍스트 이해\n\n1. **코드 분석**\n   - 실패한 라인과 주변 코드 검토\n   - 해당 코드의 최근 변경 사항 확인\n   - 함수/메서드 시그니처 및 사용법 검토\n   - 실패한 코드를 호출하거나 호출되는 관련 코드 검토\n\n2. **데이터 분석**\n   - 실패 시점의 입력값 조사\n   - 데이터 타입 및 구조 확인\n   - 데이터가 예상된 형식/제약 사항을 충족하는지 검증\n   - 엣지 케이스(edge cases) 또는 예상치 못한 값 식별\n\n3. **환경 분석**\n   - 종속성 및 버전 확인\n   - 설정 파일 검토\n   - 환경 변수 검증\n   - 필요한 리소스(파일, 네트워크, 메모리) 가용성 확인\n\n4. **상태 분석**\n   - 에러 발생 시점의 애플리케이션 상태\n   - 이 상태로 이어진 이전 작업들\n   - 관련된 공유 상태 또는 전역 변수\n   - 데이터베이스 또는 외부 시스템 상태\n\n### C - Check for Root Cause (근본 원인 확인)\n**목표**: 단순한 증상이 아닌 기저의 이슈 실별\n\n1. **에러 체인 추적**\n   - 스택 트레이스의 맨 아래(첫 번째 에러)부터 시작\n   - 상위로 거슬러 올라가며 원래 원인 탐색\n   - 에러 발생 지점과 에러 핸들러 구분\n   - 래핑되거나 다시 던져진(re-thrown) 에러 식별\n\n2. **가설 테스트**\n   - 구체적이고 테스트 가능한 가설 생성\n   - 변수 격리 (한 번에 하나씩 변경)\n   - 로깅/디버깅 도구를 사용하여 가정 검증\n   - 확인되거나 기각된 가설 문서화\n\n3. **일반적인 근본 원인**\n   - **Null/undefined 값**: 초기화 또는 검증 누락\n   - **타입 불일치 (Type mismatches)**: 잘못된 데이터 타입 전달 또는 반환\n   - **Off-by-one 에러**: 배열/루프 경계 이슈\n   - **경합 조건 (Race conditions)**: 타이밍에 따른 실패\n   - **리소스 고갈**: 메모리, 디스크, 커넥션 부족\n   - **설정 에러 (Configuration errors)**: 잘못된 설정 또는 누락된 설정\n   - **종속성 이슈 (Dependency issues)**: 버전 충돌 또는 누락된 라이브러리\n   - **권한 에러**: 불충분한 접속 권한\n   - **네트워크 에러**: 연결성, 타임아웃, DNS 이슈\n   - **데이터 손상**: 유효하지 않거나 예상치 못한 데이터 형식\n\n### E - Execute the Fix (수정 실행)\n**목표**: 해결책 구현 및 검증\n\n1. **수정 설계**\n   - 증상이 아닌 근본 원인 해결\n   - 부수 효과(side effects) 및 엣지 케이스 고려\n   - 필요한 경우 하위 호환성 계획\n   - 가장 유지보수하기 쉬운 해결책 선택\n\n2. **신중한 구현**\n   - 최소한의 타겟팅된 변경 실시\n   - 검증 및 에러 핸들링 추가\n   - 향후 디버깅을 위한 로깅 포함\n   - 수정 내용 및 근거 문서화\n\n3. **철저한 검증**\n   - 원래 에러가 해결되었는지 확인\n   - 재현 단계를 통해 테스트\n   - 엣지 케이스 및 관련 기능 테스트\n   - 새로운 에러가 도입되지 않았는지 확인\n\n4. **문서화 및 방지**\n   - 에러 유발 원인 문서화\n   - 해결책 및 작동 이유 문서화\n   - 회귀(regression) 방지를 위한 테스트 추가\n   - 필요한 경우 문서 업데이트 또는 경고 추가\n\n## 디버깅 워크플로우\n\n### 초기 평가 (5분)\n\n```\n1. 전체 에러 메시지 읽기\n2. 에러 유형 및 심각도 식별\n3. 에러 재현 가능 여부 확인\n4. 영향도 평가 (차단형, 성능 저하, 단순 외관상 문제)\n5. 조사 우선순위 결정\n```\n\n### 심층 조사 (15-30분)\n\n```\n1. TRACE 프레임워크를 체계적으로 적용\n2. 디버깅 도구 사용 (scripts/debug_helper.py 참조)\n3. 가설 생성 및 테스트\n4. 진행하면서 발견 사항 문서화\n5. 근본 원인으로 좁히기\n```\n\n### 해결책 구현 (상황에 따라 다름)\n\n```\n1. 근본 원인을 해결하는 수정 설계\n2. 적절한 에러 핸들링과 함께 구현\n3. 로깅 및 검증 추가\n4. 철저한 테스트\n5. 해결책 문서화\n```\n\n### 검증 및 방지 (10분)\n\n```\n1. 원래의 재현 단계로 수정 사항 검증\n2. 관련 기능 테스트\n3. 회귀 테스트 추가\n4. 문서 업데이트\n5. 배포 및 모니터링\n```\n\n## 언어별 공통 에러 패턴\n\n### Python\n\n**AttributeError: 'NoneType' has no attribute 'X'**\n- 근본 원인: 객체를 기대했으나 변수가 None임\n- 체크 사항: 초기화, 함수 반환값, API 응답\n- 해결책: Null 체크 추가, 적절한 초기화 보장\n\n**KeyError: 'key_name'**\n- 근본 원인: 딕셔너리에 기대한 키가 없음\n- 체크 사항: 데이터 소스, 파싱 로직, 키 철자\n- 해결책: 디폴트값과 함께 .get() 사용, 데이터 구조 검증\n\n**ImportError / ModuleNotFoundError**\n- 근본 원인: 모듈이 설치되지 않았거나 경로에 없음\n- 체크 사항: requirements.txt, 가상 환경, PYTHONPATH\n- 해결책: 누락된 패키지 설치, 임포트 경로 수정\n\n**IndentationError**\n- 근본 원인: 일관되지 않은 여백 (탭 vs 공백)\n- 체크 사항: 에디터 설정, 복사된 코드\n- 해결책: 공백(PEP 8)으로 표준화, linter 사용\n\n### JavaScript/TypeScript\n\n**TypeError: Cannot read property 'X' of undefined**\n- 근본 원인: undefined 객체의 프로퍼티에 접근\n- 체크 사항: 객체 초기화, 비동기 타이밍, API 응답\n- 해결책: 옵셔널 체이닝(?. 연산자), null 체크\n\n**ReferenceError: X is not defined**\n- 근본 원인: 선언 전 변수 사용 또는 스코프 벗어남\n- 체크 사항: 변수 선언, 스코프, 호이스팅(hoisting) 이슈\n- 해결책: 변수 선언, 스코프 수정, 임포트 확인\n\n**Promise rejection / Uncaught (in promise)**\n- 근본 원인: catch 핸들러 없이 비동기 작업 실패\n- 체크 사항: API 호출, 파일 작업, async/await 사용\n- 해결책: .catch() 추가 또는 await와 함께 try/catch 사용\n\n**SyntaxError: Unexpected token**\n- 근본 원인: 주로 JSON이나 코드 파싱 중 발생하는 유효하지 않은 구문\n- 체크 사항: JSON 구조, 괄호 짝 맞추기, 세미콜론\n- 해결책: JSON 검증, 구문 수정, 복사/붙여넣기 에러 확인\n\n### Java\n\n**NullPointerException**\n- 근본 원인: null 객체 참조에 대해 메서드 호출\n- 체크 사항: 객체 초기화, 메서드 반환값\n- 해결책: Null 체크 추가, Optional 사용, 초기화 보장\n\n**ClassNotFoundException**\n- 근본 원인: classpath에서 클래스를 찾을 수 없음\n- 체크 사항: 종속성, 빌드 설정, 패키지 구조\n- 해결책: 종속성 추가, classpath 수정, 패키지/클래스 이름 확인\n\n**ConcurrentModificationException**\n- 근본 원인: 반복(iteration) 중에 컬렉션이 수정됨\n- 체크 사항: 중첩 루프, 멀티스레딩, iterator 사용\n- 해결책: iterator.remove(), CopyOnWriteArrayList 사용 또는 동기화(synchronization)\n\n## 에러 심각도 분류\n\n### Critical (즉시 수정)\n- 애플리케이션 크래시 또는 시작 불가\n- 데이터 손실 또는 손상\n- 보안 취약점\n- 운영 환경 중단 (Outages)\n- 결제 또는 트랜잭션 실패\n\n### High (조속히 수정)\n- 주요 기능 고장\n- 사용자에게 영향을 주는 성능 저하\n- 여러 사용자에게 영향을 주는 에러\n- 복잡한 해결 방법(Workarounds)\n\n### Medium (수정 일정 계획)\n- 부가 기능 고장\n- 영향도가 있는 외관상 이슈\n- 쉬운 해결 방법이 있는 에러\n- 엣지 케이스 실패\n\n### Low (백로그)\n- 외관상 이슈\n- 사소한 개선 사항\n- 드문 엣지 케이스\n- 중요하지 않은 경고\n\n## 디버깅 도구 및 기법\n\n### 로깅 모범 사례\n\n```python\nimport logging\n\n# 구조화된 로깅 설정\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\n\n# 컨텍스트와 함께 로깅\nlogger = logging.getLogger(__name__)\nlogger.debug(f\"Processing item: {item_id}, user: {user_id}\")\nlogger.error(f\"Failed to process: {error}\", exc_info=True)\n```\n\n### 전략적 중단점 (Breakpoints)\n\n1. **에러 발생 지점**: 에러 발생 시의 정확한 상태 포착\n2. **에러 발생 전**: 입력값 및 사전 조건 확인\n3. **에러 발생 후**: 에러 전파 과정 관찰\n4. **결정 지점 (Decision points)**: 로직 분기점 검증\n5. **루프 내부**: 반복 변수 확인\n\n### 전략적인 Print 디버깅\n\n```python\n# 컨텍스트 정보를 포함한 디버그 출력 추가\nprint(f\"DEBUG: function_name called with {param1=}, {param2=}\")\nprint(f\"DEBUG: variable state before operation: {var=}\")\nprint(f\"DEBUG: condition check: {condition=}, result: {result=}\")\n```\n\n### 이진 탐색 디버깅 (Binary Search Debugging)\n\n에러 위치가 불분명할 때:\n1. 코드 경로 중간에 체크포인트 추가\n2. 에러가 체크포인트 이전인지 이후인지 판단\n3. 남은 절반에 대해 반복\n4. 에러 위치에 빠르게 수렴\n\n### 고무 오리 디버깅 (Rubber Duck Debugging)\n\n누군가에게(혹은 사물에게) 코드를 한 줄씩 설명하기:\n1. 가정을 검토하게 함\n2. 설명하는 중에 에러를 발견하는 경우가 많음\n3. 복잡한 로직을 명확하게 함\n4. 지식의 공백을 식별함\n\n## Debug Helper 스크립트 사용\n\n`scripts/debug_helper.py` 유틸리티는 자동화된 보조 기능을 제공합니다:\n\n```bash\n# 파일에서 스택 트레이스 파싱\npython scripts/debug_helper.py parse-trace error.log\n\n# 에러 패턴 추출\npython scripts/debug_helper.py analyze-log application.log\n\n# 디버그 세션 시작 (로그 생성)\npython scripts/debug_helper.py session start \"Login error investigation\"\n\n# 세션에 노트 추가\npython scripts/debug_helper.py session note \"Tested with different users - same error\"\n\n# 해결책과 함께 세션 종료\npython scripts/debug_helper.py session close \"Fixed: Added null check for user.profile\"\n```\n\n## 모범 사례 (Best Practices)\n\n### 수행할 작업 (Do's)\n\n- **에러 메시지 전체 읽기**: 세부 사항을 건너뛰지 마세요.\n- **일관된 재현**: 디버깅 전에 신뢰할 수 있는 재현 방법을 확보하세요.\n- **한 번에 한 가지만 변경**: 무엇이 문제를 해결했는지 격리하세요.\n- **진행하면서 문서화**: 가설, 테스트, 발견 사항을 기록하세요.\n- **버전 관리 사용**: 디버깅 전에 커밋하여 필요한 경우 되돌릴 수 있게 하세요.\n- **테스트 추가**: 수정 후 회귀를 방지하세요.\n- **근본 원인 수정**: 증상에만 땜질하지 마세요.\n- **지식 공유**: 팀을 위해 해결책을 문서화하세요.\n\n### 피해야 할 작업 (Don'ts)\n\n- **추측하지 말 것**: 데이터로 가정을 검증하세요.\n- **에러 읽기를 건너뛰지 말 것**: 에러 메시지에는 중요한 정보가 담겨 있습니다.\n- **여러 가지를 동시에 변경하지 말 것**: 무엇이 해결했는지 알 수 없게 됩니다.\n- **충동적으로 코드를 삭제하지 말 것**: 먼저 주석 처리하고 왜 그 코드가 있었는지 이해하세요.\n- **경고를 무시하지 말 것**: 오늘의 경고는 내일의 에러가 됩니다.\n- **이해 없이 수정하지 말 것**: 다른 것을 망가뜨릴 수 있습니다.\n- **테스트를 잊지 말 것**: 수정 사항이 작동하고 새로운 이슈를 만들지 않는지 확인하세요.\n\n## 일반적인 디버깅 시나리오\n\n### 시나리오 1: \"어제는 됐는데\"\n\n**접근 방식:**\n1. 최근 변경 사항 확인 (git diff, git log)\n2. 종속성 업데이트 검토\n3. 환경 변경 사항 확인\n4. 시간에 따른 로직(time-dependent logic) 탐색\n5. 환경 간 설정 비교\n\n**일반적인 원인:**\n- 최근 코드 변경\n- 업데이트된 종속성\n- 설정 변경\n- 데이터베이스 스키마 변경\n- 외부 API 변경\n- 인증서 만료\n\n### 시나리오 2: \"내 컴퓨터에선 되는데\"\n\n**접근 방식:**\n1. 환경 비교 (OS, 종속성, 설정)\n2. 환경 변수 확인\n3. 파일 경로 및 권한 검증\n4. 환경 간 데이터 비교\n5. 하드코딩된 가정사항 탐색\n\n**일반적인 원인:**\n- 다른 종속성 버전\n- 누락된 환경 변수\n- 다른 파일 경로\n- 데이터베이스 상태 차이\n- 운영 체제 차이\n- 누락된 설정 파일\n\n### 시나리오 3: \"간헐적 실패\"\n\n**접근 방식:**\n1. 실패 패턴 식별 (타이밍, 빈도, 조건)\n2. 경합 조건(race conditions) 탐색\n3. 리소스 가용성 확인\n4. 동시 작업 검토\n5. 광범위한 로깅 추가\n6. 재현 시도 횟수 증가\n\n**일반적인 원인:**\n- 경합 조건\n- 메모리 누수\n- 외부 서비스 불안정\n- 네트워크 이슈\n- 타이밍 의존 로직\n- 리소스 고갈\n\n### 시나리오 4: \"운영 환경에서만 에러 발생\"\n\n**접근 방식:**\n1. 운영 환경 전용 설정 확인\n2. 운영 데이터의 특성 검토\n3. 운영 환경의 부하/규모 확인\n4. 운영 환경의 종속성 검토\n5. 보안/권한 설정 검토\n\n**일반적인 원인:**\n- 운영 데이터의 엣지 케이스\n- 규모/부하 관련 이슈\n- 운영 환경 전용 설정\n- 다른 보안 정책\n- 방화벽 또는 네트워크 제한\n- 운영용으로만 연동된 기능\n\n## 고급 기법\n\n### Bisect Debugging (Git)\n\n어떤 커밋이 버그를 도입했는지 찾기:\n\n```bash\ngit bisect start\ngit bisect bad                 # 현재 버전에 버그가 있음\ngit bisect good v1.2.0        # v1.2.0 버전은 정상이였음\n# Git이 중간 커밋을 체크아웃함\n# 테스트 후 good/bad 마킹\ngit bisect good/bad\n# Git이 범인 커밋을 식별할 때까지 반복\ngit bisect reset\n```\n\n### Heisenbug (관찰자 효과)\n\n디버깅을 시작하면 사라지는 에러:\n\n**전략:**\n- 중단점 없이 로깅 추가\n- 디버깅을 위해 운영 환경과 유사한 환경 사용\n- 타이밍 및 동시성 이슈 검토\n- 초기화/타이밍 종속성 확인\n- 비침습적(non-intrusive) 모니터링 사용\n\n### 메모리 프로파일링 (Memory Profiling)\n\n메모리 누수 및 성능 확인:\n\n```python\n# Python 메모리 프로파일링\nimport tracemalloc\n\ntracemalloc.start()\n# ... 코드 실행 ...\nsnapshot = tracemalloc.take_snapshot()\ntop_stats = snapshot.statistics('lineno')\n\nfor stat in top_stats[:10]:\n    print(stat)\n```\n\n### 네트워크 디버깅 (Network Debugging)\n\nAPI 및 통합 에러 확인:\n\n**도구:**\n- 브라우저 DevTools Network 탭\n- 상세 플래그(-v)를 포함한 curl\n- API 테스트를 위한 Postman\n- 패킷 조사를 위한 Wireshark\n- 네트워크 프록시 (Charles, Fiddler)\n\n**체크 사항:**\n- Request/response 헤더\n- 상태 코드\n- Request/response 바디\n- 타이밍 (레이턴시, 타임아웃)\n- SSL/TLS 이슈\n\n## Quick Reference\n\n### TRACE 프레임워크 퀵 체크리스트\n\n```\n☐ T - TRACE\n  ☐ 전체 에러 메시지 캡처 완료\n  ☐ 스택 트레이스 수집 완료\n  ☐ 재현 단계 문서화 완료\n  ☐ 환경 식별 완료\n\n☐ R - READ\n  ☐ 에러 유형 식별 완료\n  ☐ 에러 메시지 분석 완료\n  ☐ 에러 카테고리 결정 완료\n  ☐ 관련 에러 확인 완료\n\n☐ A - ANALYZE\n  ☐ 코드 검토 완료\n  ☐ 데이터 조사 완료\n  ☐ 환경 검증 완료\n  ☐ 상태 조사 완료\n\n☐ C - CHECK\n  ☐ 에러 체인 추적 완료\n  ☐ 가설 테스트 완료\n  ☐ 근본 원인 식별 완료\n  ☐ 가정 검증 완료\n\n☐ E - EXECUTE\n  ☐ 수정 설계 완료\n  ☐ 수정 구현 완료\n  ☐ 수정 검증 완료\n  ☐ 방지 대책 추가 완료\n```\n\n### 에러 우선순위 매트릭스 (Error Priority Matrix)\n\n```\n영향도 →      Low        Medium       High        Critical\n빈도 ↓\nHigh         Medium     High         Critical    Critical\nMedium       Low        Medium       High        Critical\nLow          Low        Low          Medium      High\nRare         Backlog    Low          Medium      High\n```\n\n## 추가 자료\n\n### 예시 (Examples)\n- `examples/debugging_workflow.md` - 단계별 디버깅 프로세스 예시\n- `examples/common_errors.md` - 자주 발생하는 에러 패턴 및 해결책 카탈로그\n- `examples/stack_traces.txt` - 분석과 함께 제공되는 스택 트레이스 예시\n\n### 스크립트 (Scripts)\n- `scripts/debug_helper.py` - 트레이스 파싱 및 세션 관리를 위한 Python 디버깅 유틸리티\n\n### 추가 학습\n- 언어별 디버깅 문서\n- 프레임워크 에러 핸들링 가이드\n- 프로파일링 및 성능 분석 도구\n- 테스트 및 품질 보증 실무\n\n---\n\n**기억하세요**: 디버깅은 탐정 수사입니다. 체계적이고 인내심을 가지며, 증거가 진실로 당신을 인도하게 하세요. 모든 에러 메시지는 당신의 이해를 기다리고 있는 단서입니다.\n",
        "icartsh-plugin/skills/error-detective/examples/common_errors.md": "# Common Error Patterns Catalog\n\nThis catalog provides quick reference for frequent error patterns, their causes, and solutions.\n\n## Python Errors\n\n### AttributeError: 'NoneType' object has no attribute 'X'\n\n**Pattern:**\n```python\nuser.profile.name  # AttributeError if user.profile is None\n```\n\n**Common Causes:**\n- Function returned None instead of expected object\n- Database query found no results\n- Optional relationship not populated\n- API returned null\n\n**Quick Fixes:**\n```python\n# Option 1: Null check\nif user.profile:\n    name = user.profile.name\n\n# Option 2: Default value\nname = user.profile.name if user.profile else \"Anonymous\"\n\n# Option 3: getattr with default\nname = getattr(user.profile, 'name', 'Anonymous')\n\n# Option 4: Optional chaining (Python 3.10+)\n# Not available yet, use walrus operator\nif (profile := user.profile):\n    name = profile.name\n```\n\n---\n\n### KeyError: 'key'\n\n**Pattern:**\n```python\nvalue = data['missing_key']  # KeyError\n```\n\n**Common Causes:**\n- Expected key missing from dictionary\n- Typo in key name\n- API response structure changed\n- JSON parsing incomplete\n\n**Quick Fixes:**\n```python\n# Option 1: .get() with default\nvalue = data.get('key', default_value)\n\n# Option 2: Check before access\nif 'key' in data:\n    value = data['key']\n\n# Option 3: Try/except\ntry:\n    value = data['key']\nexcept KeyError:\n    value = default_value\n\n# Option 4: defaultdict\nfrom collections import defaultdict\ndata = defaultdict(lambda: 'default')\n```\n\n---\n\n### IndexError: list index out of range\n\n**Pattern:**\n```python\nitem = items[5]  # IndexError if len(items) <= 5\n```\n\n**Common Causes:**\n- Empty list\n- Off-by-one error in loop\n- Assuming minimum list size\n- Incorrect slice indices\n\n**Quick Fixes:**\n```python\n# Option 1: Check length\nif len(items) > 5:\n    item = items[5]\n\n# Option 2: Try/except\ntry:\n    item = items[5]\nexcept IndexError:\n    item = None\n\n# Option 3: Use get-like pattern\ndef safe_get(lst, idx, default=None):\n    try:\n        return lst[idx]\n    except IndexError:\n        return default\n\nitem = safe_get(items, 5)\n\n# Option 4: Slice (never raises IndexError)\nitem = items[5:6]  # Returns [] if out of range\nitem = item[0] if item else None\n```\n\n---\n\n### TypeError: unsupported operand type(s) for +: 'int' and 'str'\n\n**Pattern:**\n```python\nresult = 5 + \"10\"  # TypeError\n```\n\n**Common Causes:**\n- Type mismatch in operation\n- Missing type conversion\n- Wrong type from user input or API\n- Variable reused with different type\n\n**Quick Fixes:**\n```python\n# Option 1: Explicit conversion\nresult = 5 + int(\"10\")  # 15\nresult = str(5) + \"10\"  # \"510\"\n\n# Option 2: Type checking\ndef add_values(a, b):\n    if isinstance(a, str):\n        a = int(a)\n    if isinstance(b, str):\n        b = int(b)\n    return a + b\n\n# Option 3: Safe conversion\ndef to_int(value, default=0):\n    try:\n        return int(value)\n    except (ValueError, TypeError):\n        return default\n```\n\n---\n\n### ImportError / ModuleNotFoundError: No module named 'X'\n\n**Pattern:**\n```python\nimport missing_module  # ModuleNotFoundError\n```\n\n**Common Causes:**\n- Module not installed\n- Wrong virtual environment\n- Typo in module name\n- Module name conflict\n- PYTHONPATH issues\n\n**Quick Fixes:**\n```bash\n# Install missing module\npip install module_name\n\n# Check installed packages\npip list | grep module\n\n# Install from requirements\npip install -r requirements.txt\n\n# Reinstall in current environment\npython -m pip install module_name\n\n# Check Python path\npython -c \"import sys; print('\\n'.join(sys.path))\"\n```\n\n---\n\n### RecursionError: maximum recursion depth exceeded\n\n**Pattern:**\n```python\ndef factorial(n):\n    return n * factorial(n - 1)  # No base case!\n```\n\n**Common Causes:**\n- Missing base case in recursion\n- Infinite recursion\n- Very deep recursion (>1000 levels)\n- Circular references\n\n**Quick Fixes:**\n```python\n# Option 1: Add base case\ndef factorial(n):\n    if n <= 1:\n        return 1\n    return n * factorial(n - 1)\n\n# Option 2: Increase limit (careful!)\nimport sys\nsys.setrecursionlimit(10000)\n\n# Option 3: Use iteration instead\ndef factorial(n):\n    result = 1\n    for i in range(2, n + 1):\n        result *= i\n    return result\n\n# Option 4: Use tail recursion with trampoline\ndef factorial(n, acc=1):\n    if n <= 1:\n        return acc\n    return lambda: factorial(n - 1, n * acc)\n```\n\n## JavaScript/TypeScript Errors\n\n### TypeError: Cannot read property 'X' of undefined\n\n**Pattern:**\n```javascript\nuser.profile.name  // TypeError if user.profile is undefined\n```\n\n**Common Causes:**\n- Variable not initialized\n- Async data not loaded yet\n- API returned null/undefined\n- Destructuring failed\n\n**Quick Fixes:**\n```javascript\n// Option 1: Optional chaining (ES2020)\nconst name = user?.profile?.name;\n\n// Option 2: Logical AND\nconst name = user && user.profile && user.profile.name;\n\n// Option 3: Default values\nconst name = user?.profile?.name || 'Anonymous';\n\n// Option 4: Nullish coalescing\nconst name = user?.profile?.name ?? 'Anonymous';\n\n// Option 5: Guard clause\nif (!user || !user.profile) {\n    return;\n}\nconst name = user.profile.name;\n```\n\n---\n\n### ReferenceError: X is not defined\n\n**Pattern:**\n```javascript\nconsole.log(undeclaredVariable);  // ReferenceError\n```\n\n**Common Causes:**\n- Variable used before declaration\n- Typo in variable name\n- Scope issue (var/let/const)\n- Missing import\n\n**Quick Fixes:**\n```javascript\n// Option 1: Declare variable\nlet variableName = value;\n\n// Option 2: Import if from module\nimport { variableName } from './module';\n\n// Option 3: Check for existence\nif (typeof variableName !== 'undefined') {\n    console.log(variableName);\n}\n\n// Option 4: Use window for globals (browser)\nif ('variableName' in window) {\n    console.log(window.variableName);\n}\n```\n\n---\n\n### Uncaught (in promise) Error\n\n**Pattern:**\n```javascript\nasync function fetchData() {\n    const response = await fetch(url);\n    const data = await response.json();  // Might reject!\n    return data;\n}\n// Calling without try/catch or .catch()\n```\n\n**Common Causes:**\n- Missing error handler for promises\n- Unhandled async/await exceptions\n- No .catch() on promise chain\n- Network request failure\n\n**Quick Fixes:**\n```javascript\n// Option 1: Try/catch with async/await\nasync function fetchData() {\n    try {\n        const response = await fetch(url);\n        if (!response.ok) {\n            throw new Error('Network response was not ok');\n        }\n        const data = await response.json();\n        return data;\n    } catch (error) {\n        console.error('Fetch failed:', error);\n        throw error;  // Or handle gracefully\n    }\n}\n\n// Option 2: .catch() on promise\nfetchData()\n    .then(data => console.log(data))\n    .catch(error => console.error(error));\n\n// Option 3: Global handler\nwindow.addEventListener('unhandledrejection', event => {\n    console.error('Unhandled promise rejection:', event.reason);\n});\n\n// Option 4: Wrapper function\nconst safeAsync = (fn) => {\n    return async (...args) => {\n        try {\n            return await fn(...args);\n        } catch (error) {\n            console.error('Async error:', error);\n            return null;\n        }\n    };\n};\n```\n\n---\n\n### SyntaxError: Unexpected token\n\n**Pattern:**\n```javascript\nconst data = JSON.parse(invalidJSON);  // SyntaxError\n```\n\n**Common Causes:**\n- Invalid JSON format\n- Missing quotes in JSON\n- Trailing commas in JSON\n- Single quotes instead of double quotes\n- Malformed code structure\n\n**Quick Fixes:**\n```javascript\n// Option 1: Validate JSON before parsing\nfunction safeParseJSON(str) {\n    try {\n        return JSON.parse(str);\n    } catch (e) {\n        console.error('Invalid JSON:', e);\n        return null;\n    }\n}\n\n// Option 2: Check JSON validity\nfunction isValidJSON(str) {\n    try {\n        JSON.parse(str);\n        return true;\n    } catch (e) {\n        return false;\n    }\n}\n\n// Option 3: Use a JSON validator library\n// npm install jsonlint\nconst jsonlint = require('jsonlint');\njsonlint.parse(jsonString);\n\n// For code syntax errors: Use linter\n// npm install eslint\n```\n\n---\n\n### Maximum call stack size exceeded\n\n**Pattern:**\n```javascript\nfunction infinite() {\n    infinite();  // No base case!\n}\n```\n\n**Common Causes:**\n- Infinite recursion\n- Missing base case\n- Circular object references\n- Very deep recursion\n\n**Quick Fixes:**\n```javascript\n// Option 1: Add base case\nfunction factorial(n) {\n    if (n <= 1) return 1;\n    return n * factorial(n - 1);\n}\n\n// Option 2: Use iteration\nfunction factorial(n) {\n    let result = 1;\n    for (let i = 2; i <= n; i++) {\n        result *= i;\n    }\n    return result;\n}\n\n// Option 3: Trampoline for tail recursion\nfunction trampoline(fn) {\n    while (typeof fn === 'function') {\n        fn = fn();\n    }\n    return fn;\n}\n\nfunction factorial(n, acc = 1) {\n    if (n <= 1) return acc;\n    return () => factorial(n - 1, n * acc);\n}\n\nconst result = trampoline(factorial(5));\n```\n\n## Java Errors\n\n### NullPointerException\n\n**Pattern:**\n```java\nString name = user.getName();  // NPE if user is null\n```\n\n**Common Causes:**\n- Null object reference\n- Method called on null\n- Uninitialized variable\n- Failed object creation\n\n**Quick Fixes:**\n```java\n// Option 1: Null check\nif (user != null) {\n    String name = user.getName();\n}\n\n// Option 2: Objects.requireNonNull\nObjects.requireNonNull(user, \"User cannot be null\");\nString name = user.getName();\n\n// Option 3: Optional (Java 8+)\nOptional<User> userOpt = Optional.ofNullable(user);\nString name = userOpt.map(User::getName).orElse(\"Unknown\");\n\n// Option 4: Ternary operator\nString name = (user != null) ? user.getName() : \"Unknown\";\n\n// Option 5: @Nullable and @NotNull annotations\npublic String getName(@NotNull User user) {\n    return user.getName();\n}\n```\n\n---\n\n### ClassCastException\n\n**Pattern:**\n```java\nString str = (String) object;  // CCE if object is not String\n```\n\n**Common Causes:**\n- Incorrect type cast\n- Generic type erasure issues\n- Wrong object returned from method\n- Collection contains mixed types\n\n**Quick Fixes:**\n```java\n// Option 1: instanceof check\nif (object instanceof String) {\n    String str = (String) object;\n}\n\n// Option 2: Use generics\nList<String> list = new ArrayList<>();  // Type-safe\n\n// Option 3: Pattern matching (Java 16+)\nif (object instanceof String str) {\n    // str is already String here\n    System.out.println(str.toLowerCase());\n}\n\n// Option 4: Try/catch\ntry {\n    String str = (String) object;\n} catch (ClassCastException e) {\n    // Handle wrong type\n}\n```\n\n---\n\n### ConcurrentModificationException\n\n**Pattern:**\n```java\nfor (String item : list) {\n    list.remove(item);  // CME!\n}\n```\n\n**Common Causes:**\n- Modifying collection during iteration\n- Multi-threaded access without synchronization\n- Nested iteration modification\n\n**Quick Fixes:**\n```java\n// Option 1: Use Iterator.remove()\nIterator<String> it = list.iterator();\nwhile (it.hasNext()) {\n    String item = it.next();\n    if (shouldRemove(item)) {\n        it.remove();  // Safe!\n    }\n}\n\n// Option 2: Collect items to remove, then remove\nList<String> toRemove = new ArrayList<>();\nfor (String item : list) {\n    if (shouldRemove(item)) {\n        toRemove.add(item);\n    }\n}\nlist.removeAll(toRemove);\n\n// Option 3: Use removeIf (Java 8+)\nlist.removeIf(item -> shouldRemove(item));\n\n// Option 4: CopyOnWriteArrayList for concurrent access\nList<String> list = new CopyOnWriteArrayList<>();\n// Can modify during iteration\n```\n\n---\n\n### OutOfMemoryError: Java heap space\n\n**Pattern:**\n```\nException in thread \"main\" java.lang.OutOfMemoryError: Java heap space\n```\n\n**Common Causes:**\n- Memory leak\n- Loading too much data at once\n- Heap size too small\n- Infinite loop creating objects\n\n**Quick Fixes:**\n```bash\n# Option 1: Increase heap size\njava -Xmx2g -Xms512m MyApp\n\n# Option 2: Analyze heap dump\njava -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/heap.hprof MyApp\n\n# Option 3: Use profiler (VisualVM, YourKit)\njvisualvm\n\n# Code fixes:\n# - Process data in batches\n# - Close resources properly\n# - Use weak references where appropriate\n# - Clear collections when done\n```\n\n**Code Improvements:**\n```java\n// Bad: Load all at once\nList<Record> records = database.findAll();  // Millions of records!\n\n// Good: Process in batches\nint pageSize = 1000;\nfor (int page = 0; ; page++) {\n    List<Record> batch = database.findBatch(page, pageSize);\n    if (batch.isEmpty()) break;\n\n    processBatch(batch);\n    batch.clear();  // Help GC\n}\n\n// Use try-with-resources\ntry (BufferedReader reader = new BufferedReader(new FileReader(file))) {\n    // Automatically closed\n}\n```\n\n## Database Errors\n\n### Connection Timeout / Connection Pool Exhausted\n\n**Pattern:**\n```\nCaused by: java.sql.SQLTimeoutException: Connection is not available, request timed out after 30000ms\n```\n\n**Common Causes:**\n- Too many concurrent connections\n- Connections not closed properly\n- Slow queries blocking pool\n- Pool size too small\n\n**Quick Fixes:**\n```java\n// Option 1: Always close connections\ntry (Connection conn = dataSource.getConnection()) {\n    // Use connection\n}  // Automatically returned to pool\n\n// Option 2: Increase pool size (HikariCP)\nhikari.setMaximumPoolSize(20);\nhikari.setMinimumIdle(5);\n\n// Option 3: Set reasonable timeouts\nhikari.setConnectionTimeout(30000);\nhikari.setIdleTimeout(600000);\nhikari.setMaxLifetime(1800000);\n\n// Option 4: Monitor and fix slow queries\n// Add indexes, optimize queries\n\n// Option 5: Use query timeout\nStatement stmt = conn.createStatement();\nstmt.setQueryTimeout(10);  // 10 seconds\n```\n\n---\n\n### Deadlock Detected\n\n**Pattern:**\n```\nDeadlock detected during wait for locking: transaction A waits for transaction B; transaction B waits for transaction A\n```\n\n**Common Causes:**\n- Circular wait conditions\n- Different lock ordering\n- Long-running transactions\n- Pessimistic locking\n\n**Quick Fixes:**\n```java\n// Option 1: Consistent lock ordering\n// Always acquire locks in same order\nsynchronized(lockA) {\n    synchronized(lockB) {\n        // Work\n    }\n}\n\n// Option 2: Use timeout with tryLock\nLock lock1 = new ReentrantLock();\nLock lock2 = new ReentrantLock();\n\nif (lock1.tryLock(1, TimeUnit.SECONDS)) {\n    try {\n        if (lock2.tryLock(1, TimeUnit.SECONDS)) {\n            try {\n                // Work\n            } finally {\n                lock2.unlock();\n            }\n        }\n    } finally {\n        lock1.unlock();\n    }\n}\n\n// Option 3: Optimistic locking with version\n@Entity\npublic class Account {\n    @Version\n    private Long version;\n    // ...\n}\n\n// Option 4: Reduce transaction scope\n// Keep transactions as short as possible\n```\n\n## Network/API Errors\n\n### 404 Not Found\n\n**Common Causes:**\n- Wrong URL or endpoint\n- Resource deleted\n- Route not registered\n- Typo in path\n\n**Quick Fixes:**\n```javascript\n// Check URL construction\nconst url = `${baseUrl}/api/users/${userId}`;\nconsole.log('Requesting:', url);\n\n// Handle 404 gracefully\nfetch(url)\n    .then(response => {\n        if (response.status === 404) {\n            console.log('Resource not found');\n            return null;\n        }\n        if (!response.ok) {\n            throw new Error(`HTTP ${response.status}`);\n        }\n        return response.json();\n    });\n\n// Server-side: Verify route\napp.get('/api/users/:id', handler);  // Is this registered?\n```\n\n---\n\n### 500 Internal Server Error\n\n**Common Causes:**\n- Unhandled exception on server\n- Database connection failure\n- Configuration error\n- Null pointer in server code\n\n**Quick Fixes:**\n```python\n# Add global error handler\n@app.errorhandler(Exception)\ndef handle_exception(e):\n    logger.error(f\"Unhandled exception: {e}\", exc_info=True)\n    return {\n        'error': 'Internal server error',\n        'message': str(e) if app.debug else 'An error occurred'\n    }, 500\n\n# Client: Retry with exponential backoff\nasync function fetchWithRetry(url, retries = 3) {\n    for (let i = 0; i < retries; i++) {\n        try {\n            const response = await fetch(url);\n            if (response.ok) {\n                return await response.json();\n            }\n            if (response.status >= 500 && i < retries - 1) {\n                await sleep(Math.pow(2, i) * 1000);\n                continue;\n            }\n            throw new Error(`HTTP ${response.status}`);\n        } catch (error) {\n            if (i === retries - 1) throw error;\n            await sleep(Math.pow(2, i) * 1000);\n        }\n    }\n}\n```\n\n---\n\n### CORS Error\n\n**Pattern:**\n```\nAccess to fetch at 'https://api.example.com' from origin 'https://app.example.com' has been blocked by CORS policy\n```\n\n**Common Causes:**\n- Missing CORS headers\n- Wrong origin in CORS config\n- Preflight request failing\n- Credentials without proper headers\n\n**Quick Fixes:**\n```python\n# Python (Flask)\nfrom flask_cors import CORS\n\napp = Flask(__name__)\nCORS(app, origins=['https://app.example.com'])\n\n# Or specific route\n@app.route('/api/data')\n@cross_origin(origins=['https://app.example.com'])\ndef get_data():\n    return jsonify(data)\n\n# Express.js\nconst cors = require('cors');\napp.use(cors({\n    origin: 'https://app.example.com',\n    credentials: true\n}));\n\n# Nginx\nadd_header 'Access-Control-Allow-Origin' 'https://app.example.com';\nadd_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS';\nadd_header 'Access-Control-Allow-Headers' 'Content-Type, Authorization';\n```\n\n## Quick Reference Table\n\n| Error | Primary Cause | First Check | Quick Fix |\n|-------|--------------|-------------|-----------|\n| NullPointerException | Null reference | Variable initialization | Add null check |\n| AttributeError | None object | Function return value | Use getattr() or check |\n| KeyError | Missing dict key | Key spelling | Use .get() |\n| IndexError | Out of bounds | List length | Check len() first |\n| TypeError | Type mismatch | Variable types | Convert types |\n| 404 Not Found | Wrong URL | URL construction | Verify endpoint |\n| 500 Server Error | Server exception | Server logs | Add error handler |\n| CORS Error | Missing headers | CORS config | Add CORS middleware |\n| Connection Timeout | Pool exhausted | Connection closing | Use try-with-resources |\n| Memory Error | Heap overflow | Memory usage | Process in batches |\n",
        "icartsh-plugin/skills/error-detective/examples/debugging_workflow.md": "# Debugging Workflow Examples\n\nThis document demonstrates the complete debugging workflow using the TRACE framework with real-world examples.\n\n## Example 1: Python AttributeError - Simple Case\n\n### Initial Error Report\n\n```\nUser reports: \"The application crashes when trying to view user profile\"\n```\n\n### T - TRACE the Error\n\n**Full Error Message:**\n```\nTraceback (most recent call last):\n  File \"app.py\", line 45, in view_profile\n    profile_data = user.profile.to_dict()\nAttributeError: 'NoneType' object has no attribute 'to_dict'\n```\n\n**Reproduction Steps:**\n1. Log in as user \"john@example.com\"\n2. Navigate to /profile\n3. Error occurs every time\n\n**Environment:** Production server, Python 3.9, Django 4.0\n\n### R - READ the Error Message\n\n**Error Type:** `AttributeError`\n- Attempting to access attribute on None object\n- This means `user.profile` is None, not a Profile object\n\n**Error Location:**\n- File: `app.py`\n- Line: 45\n- Function: `view_profile`\n\n**Error Category:** Runtime error - null reference\n\n### A - ANALYZE the Context\n\n**Code Review:**\n```python\n# app.py, line 45\ndef view_profile(request):\n    user = request.user\n    profile_data = user.profile.to_dict()  # Line 45 - ERROR HERE\n    return render(request, 'profile.html', {'profile': profile_data})\n```\n\n**Data Analysis:**\n- User \"john@example.com\" exists in database\n- Checking user record shows `profile_id` is NULL\n- Profile was never created for this user\n\n**Environment Check:**\n- User model expects one-to-one relationship with Profile\n- No migration to create profiles for existing users\n\n### C - CHECK for Root Cause\n\n**Root Cause Identified:**\n- Old users (before Profile model was added) don't have profiles\n- Code assumes all users have profiles\n- No null check or default profile creation\n\n**Hypothesis Testing:**\n```python\n# Test: Check if other users have profiles\n>>> User.objects.filter(profile__isnull=True).count()\n47  # 47 users without profiles!\n```\n\n### E - EXECUTE the Fix\n\n**Fix Design:**\n1. Add null check in view\n2. Create profile if missing\n3. Backfill profiles for existing users\n\n**Implementation:**\n```python\ndef view_profile(request):\n    user = request.user\n\n    # Fix: Create profile if it doesn't exist\n    if not hasattr(user, 'profile') or user.profile is None:\n        Profile.objects.create(user=user)\n\n    profile_data = user.profile.to_dict()\n    return render(request, 'profile.html', {'profile': profile_data})\n```\n\n**Migration to backfill:**\n```python\n# migrations/0012_backfill_profiles.py\nfrom django.db import migrations\n\ndef create_missing_profiles(apps, schema_editor):\n    User = apps.get_model('auth', 'User')\n    Profile = apps.get_model('accounts', 'Profile')\n\n    users_without_profiles = User.objects.filter(profile__isnull=True)\n    for user in users_without_profiles:\n        Profile.objects.create(user=user)\n\nclass Migration(migrations.Migration):\n    dependencies = [\n        ('accounts', '0011_profile_model'),\n    ]\n\n    operations = [\n        migrations.RunPython(create_missing_profiles),\n    ]\n```\n\n**Verification:**\n1. Run migration\n2. Test with user \"john@example.com\" - ✓ Works\n3. Test with new users - ✓ Works\n4. Check all users have profiles - ✓ Confirmed\n5. No new errors in logs - ✓ Clean\n\n**Prevention:**\n- Added test case for users without profiles\n- Updated user creation to always create profile\n- Documented the user-profile relationship\n\n---\n\n## Example 2: JavaScript Promise Rejection - Async Issue\n\n### Initial Error Report\n\n```\nUsers report: \"Shopping cart sometimes doesn't update, shows loading spinner forever\"\n```\n\n### T - TRACE the Error\n\n**Full Error Message (from browser console):**\n```\nUncaught (in promise) TypeError: Cannot read property 'items' of undefined\n    at updateCart (cart.js:23)\n    at cart.js:15\n```\n\n**Reproduction Steps:**\n1. Add item to cart\n2. Click \"Update Quantity\"\n3. Error occurs ~30% of the time (intermittent!)\n\n**Environment:** Chrome 96, React 17, production API\n\n### R - READ the Error Message\n\n**Error Type:** `TypeError` in Promise\n- Uncaught promise rejection (no error handler)\n- Accessing `items` property on undefined object\n- Suggests API response not in expected format\n\n**Error Location:**\n- File: `cart.js`\n- Line: 23\n- Function: `updateCart`\n\n**Error Category:** Runtime error - async/integration issue\n\n### A - ANALYZE the Context\n\n**Code Review:**\n```javascript\n// cart.js\nasync function updateCart(itemId, quantity) {\n    setLoading(true);\n\n    const response = await fetch('/api/cart/update', {\n        method: 'POST',\n        body: JSON.stringify({ itemId, quantity }),\n        headers: { 'Content-Type': 'application/json' }\n    });\n\n    const data = await response.json();  // Line 15\n\n    // Line 23 - ERROR: data.items is undefined sometimes\n    const updatedItems = data.items.map(item => ({\n        ...item,\n        total: item.price * item.quantity\n    }));\n\n    setCartItems(updatedItems);\n    setLoading(false);\n}\n```\n\n**Network Analysis:**\n- API sometimes returns 500 error\n- When error occurs, response body is: `{\"error\": \"Database timeout\"}`\n- No `items` property in error response\n\n**Race Condition Check:**\n- Multiple simultaneous cart updates possible\n- Last write wins, but responses come back out of order\n\n### C - CHECK for Root Cause\n\n**Root Cause Identified:**\n1. **Missing error handling**: No check for HTTP status code\n2. **Assuming success**: Code assumes response always has `items`\n3. **No catch block**: Promise rejection unhandled\n4. **Backend timeout**: Database query sometimes slow\n\n**Hypothesis Testing:**\n```javascript\n// Test: Check response when error occurs\nfetch('/api/cart/update', {...})\n    .then(r => r.json())\n    .then(data => console.log(data));\n\n// Result when error: {error: \"Database timeout\"}\n// No items property!\n```\n\n### E - EXECUTE the Fix\n\n**Fix Design:**\n1. Add proper error handling\n2. Check HTTP status before processing\n3. Show user-friendly error message\n4. Optimize backend query (separate issue)\n\n**Implementation:**\n```javascript\nasync function updateCart(itemId, quantity) {\n    setLoading(true);\n\n    try {\n        const response = await fetch('/api/cart/update', {\n            method: 'POST',\n            body: JSON.stringify({ itemId, quantity }),\n            headers: { 'Content-Type': 'application/json' }\n        });\n\n        // Check for HTTP errors\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Failed to update cart');\n        }\n\n        const data = await response.json();\n\n        // Validate response structure\n        if (!data.items || !Array.isArray(data.items)) {\n            throw new Error('Invalid response format');\n        }\n\n        const updatedItems = data.items.map(item => ({\n            ...item,\n            total: item.price * item.quantity\n        }));\n\n        setCartItems(updatedItems);\n        setError(null);\n\n    } catch (error) {\n        console.error('Cart update failed:', error);\n        setError('Failed to update cart. Please try again.');\n\n        // Don't update cart on error - keep previous state\n\n    } finally {\n        setLoading(false);\n    }\n}\n```\n\n**Backend Fix (bonus):**\n```python\n# views.py - Add timeout and better error handling\nfrom django.db import connection\n\n@api_view(['POST'])\ndef update_cart(request):\n    try:\n        with connection.cursor() as cursor:\n            cursor.execute(\"SET statement_timeout = 5000\")  # 5 second timeout\n\n        item_id = request.data['itemId']\n        quantity = request.data['quantity']\n\n        cart_item = CartItem.objects.select_for_update().get(id=item_id)\n        cart_item.quantity = quantity\n        cart_item.save()\n\n        # Return full cart\n        items = CartItem.objects.filter(cart=cart_item.cart).select_related('product')\n\n        return Response({\n            'items': CartItemSerializer(items, many=True).data\n        })\n\n    except CartItem.DoesNotExist:\n        return Response({'error': 'Item not found'}, status=404)\n    except Exception as e:\n        logger.error(f\"Cart update failed: {e}\")\n        return Response({'error': 'Database error'}, status=500)\n```\n\n**Verification:**\n1. Test normal update - ✓ Works\n2. Simulate backend error - ✓ Shows error message, doesn't crash\n3. Test rapid updates - ✓ Handles correctly\n4. Test timeout scenario - ✓ Graceful degradation\n5. Monitor production - ✓ Error rate dropped from 30% to 0.1%\n\n**Prevention:**\n- Added integration tests for API error scenarios\n- Added response validation middleware\n- Set up backend query performance monitoring\n- Added database query timeout\n- Created error handling pattern document for team\n\n---\n\n## Example 3: Java NullPointerException - Multi-Layer Issue\n\n### Initial Error Report\n\n```\nProduction alert: \"Payment processing failing for premium users\"\n```\n\n### T - TRACE the Error\n\n**Full Error Message:**\n```\njava.lang.NullPointerException: Cannot invoke \"com.example.User.getSubscription()\" because \"user\" is null\n    at com.example.payment.PaymentProcessor.processPayment(PaymentProcessor.java:45)\n    at com.example.payment.PaymentController.handlePayment(PaymentController.java:89)\n    at com.example.payment.PaymentController$$FastClassBySpringCGLIB$$1234.invoke(<generated>)\n    ...\n```\n\n**Reproduction Steps:**\n1. Premium user attempts to upgrade subscription\n2. Click \"Process Payment\"\n3. Error occurs for ~20% of premium users\n\n**Environment:** Java 11, Spring Boot 2.5, PostgreSQL, production\n\n### R - READ the Error Message\n\n**Error Type:** `NullPointerException`\n- Attempting to call `getSubscription()` on null user object\n- Suggests user lookup failed\n\n**Error Location:**\n- File: `PaymentProcessor.java`\n- Line: 45\n- Method: `processPayment`\n\n**Call Chain:**\n- `PaymentController.handlePayment` → `PaymentProcessor.processPayment`\n\n### A - ANALYZE the Context\n\n**Code Review:**\n```java\n// PaymentController.java, line 89\n@PostMapping(\"/process\")\npublic ResponseEntity<?> handlePayment(@RequestBody PaymentRequest request) {\n    User user = userService.findById(request.getUserId());\n    return paymentProcessor.processPayment(user, request.getAmount());\n}\n\n// PaymentProcessor.java, line 45\npublic PaymentResult processPayment(User user, BigDecimal amount) {\n    Subscription subscription = user.getSubscription();  // Line 45 - NPE!\n\n    if (subscription.isPremium()) {\n        return processPremiumPayment(user, amount);\n    }\n    return processStandardPayment(user, amount);\n}\n\n// UserService.java\npublic User findById(Long userId) {\n    return userRepository.findById(userId).orElse(null);  // Returns null!\n}\n```\n\n**Data Analysis:**\n```sql\n-- Check user data\nSELECT id, email, deleted_at FROM users WHERE id = 12345;\n-- Result: id=12345, email=premium@example.com, deleted_at=2024-01-15\n-- User was soft-deleted!\n```\n\n**Environment Check:**\n- Payment requests include userId from session\n- Session not invalidated when user deleted\n- Soft-delete doesn't clear sessions\n\n### C - CHECK for Root Cause\n\n**Root Causes Identified:**\n1. **Poor error handling**: `findById` returns null instead of throwing exception\n2. **No null check**: Controller doesn't validate user exists\n3. **Session management**: Deleted users still have valid sessions\n4. **Soft delete gap**: Deleted users not handled in payment flow\n\n**Hypothesis Testing:**\n```java\n// Test: Check if issue is soft-deleted users\nUser user = userRepository.findById(12345L).orElse(null);\n// user is null\n\nUser user = userRepository.findByIdAndDeletedAtIsNull(12345L).orElse(null);\n// user is null (correct - user is deleted)\n\n// Confirmed: Deleted users trigger the error\n```\n\n### E - EXECUTE the Fix\n\n**Fix Design:**\n1. Change `findById` to throw exception, not return null\n2. Add validation in controller\n3. Invalidate sessions on user deletion\n4. Add explicit soft-delete filtering\n\n**Implementation:**\n\n```java\n// UserService.java - Better error handling\npublic User findById(Long userId) {\n    return userRepository.findByIdAndDeletedAtIsNull(userId)\n        .orElseThrow(() -> new UserNotFoundException(\"User not found: \" + userId));\n}\n\n// PaymentController.java - Add validation\n@PostMapping(\"/process\")\npublic ResponseEntity<?> handlePayment(@RequestBody PaymentRequest request) {\n    try {\n        User user = userService.findById(request.getUserId());\n\n        if (user.getSubscription() == null) {\n            return ResponseEntity.badRequest()\n                .body(new ErrorResponse(\"User has no subscription\"));\n        }\n\n        PaymentResult result = paymentProcessor.processPayment(user, request.getAmount());\n        return ResponseEntity.ok(result);\n\n    } catch (UserNotFoundException e) {\n        return ResponseEntity.status(HttpStatus.NOT_FOUND)\n            .body(new ErrorResponse(\"User account not found\"));\n    }\n}\n\n// PaymentProcessor.java - Defensive programming\npublic PaymentResult processPayment(User user, BigDecimal amount) {\n    Objects.requireNonNull(user, \"User cannot be null\");\n    Objects.requireNonNull(user.getSubscription(), \"User must have subscription\");\n\n    Subscription subscription = user.getSubscription();\n\n    if (subscription.isPremium()) {\n        return processPremiumPayment(user, amount);\n    }\n    return processStandardPayment(user, amount);\n}\n\n// UserRepository.java - Add soft-delete filtering\n@Query(\"SELECT u FROM User u WHERE u.id = :id AND u.deletedAt IS NULL\")\nOptional<User> findByIdAndDeletedAtIsNull(@Param(\"id\") Long id);\n\n// UserService.java - Invalidate sessions on delete\npublic void deleteUser(Long userId) {\n    User user = findById(userId);\n    user.setDeletedAt(LocalDateTime.now());\n    userRepository.save(user);\n\n    // Invalidate all sessions for this user\n    sessionRegistry.getAllSessions(user, false)\n        .forEach(SessionInformation::expireNow);\n}\n```\n\n**Global Exception Handler:**\n```java\n@ControllerAdvice\npublic class GlobalExceptionHandler {\n\n    @ExceptionHandler(UserNotFoundException.class)\n    public ResponseEntity<ErrorResponse> handleUserNotFound(UserNotFoundException e) {\n        return ResponseEntity.status(HttpStatus.NOT_FOUND)\n            .body(new ErrorResponse(e.getMessage()));\n    }\n\n    @ExceptionHandler(NullPointerException.class)\n    public ResponseEntity<ErrorResponse> handleNullPointer(NullPointerException e) {\n        logger.error(\"Unexpected null pointer\", e);\n        return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR)\n            .body(new ErrorResponse(\"An unexpected error occurred\"));\n    }\n}\n```\n\n**Verification:**\n1. Test payment with active user - ✓ Works\n2. Test payment with deleted user - ✓ Returns 404 error gracefully\n3. Test payment with no subscription - ✓ Returns clear error message\n4. Delete user and verify session invalidated - ✓ Works\n5. Monitor production for one week - ✓ No NPEs in payment processing\n\n**Prevention:**\n- Added integration tests for soft-deleted users\n- Created custom `@NotNull` annotation for critical parameters\n- Documented null-handling policy in team guidelines\n- Added static analysis rule to flag `.orElse(null)` pattern\n- Implemented comprehensive exception handling strategy\n\n---\n\n## Key Takeaways\n\n### Common Patterns Across Examples\n\n1. **Error messages tell a story** - Read them completely\n2. **Reproduction is crucial** - Can't fix what you can't reproduce\n3. **Check assumptions** - \"It should work\" isn't debugging\n4. **Follow the chain** - Error location ≠ root cause location\n5. **Fix the cause, not the symptom** - Null checks mask design issues\n6. **Verify thoroughly** - Test the fix and related functionality\n7. **Prevent recurrence** - Add tests, documentation, monitoring\n\n### Time Investment by Phase\n\n- **Initial investigation**: 20-30% of time\n- **Root cause analysis**: 40-50% of time\n- **Implementing fix**: 10-20% of time\n- **Verification and prevention**: 20-30% of time\n\n### When to Escalate\n\nEscalate or request help when:\n- Can't reproduce the error after reasonable attempts\n- Root cause is in unfamiliar codebase/domain\n- Fix requires architectural changes\n- Time spent exceeds severity threshold\n- Need access to production data/systems\n",
        "icartsh-plugin/skills/file-organizer/SKILL.md": "---\nname: file-organizer\ndescription: 컨텍스트 이해, 중복 파일 찾기, 더 나은 구조 제안 및 클린업 작업 자동화를 통해 컴퓨터의 파일과 폴더를 지능적으로 정리합니다. 인지 부하를 줄이고 수동 작업 없이 디지털 작업 공간을 깔끔하게 유지합니다.\n---\n\n# File Organizer\n\n이 SKILL은 당신의 개인 정리 비서 역할을 수행하며, 지속적인 수동 정리의 부담 없이 컴퓨터 전반에 걸쳐 깔끔하고 논리적인 파일 구조를 유지할 수 있도록 도와줍니다.\n\n## 사용 시기\n\n- 다운로드(Downloads) 폴더가 엉망진창일 때\n- 파일들이 여기저기 흩어져 있어 찾을 수 없을 때\n- 중복 파일들이 공간을 차지하고 있을 때\n- 폴더 구조가 더 이상 논리적이지 않을 때\n- 더 나은 정리 습관을 기르고 싶을 때\n- 새로운 프로젝트를 시작하면서 좋은 구조가 필요할 때\n- 오래된 프로젝트를 아카이브(archive) 하기 전 정리하고 싶을 때\n\n## 주요 기능\n\n1. **현재 구조 분석**: 폴더와 파일을 검토하여 무엇을 가지고 있는지 이해합니다.\n2. **중복 파일 찾기**: 시스템 전체에서 중복된 파일을 식별합니다.\n3. **정리 제안**: 콘텐츠를 바탕으로 논리적인 폴더 구조를 제안합니다.\n4. **클린업 자동화**: 사용자의 승인 하에 파일을 이동, 이름 변경 및 정리합니다.\n5. **컨텍스트 유지**: 파일 유형, 날짜 및 내용을 바탕으로 스마트한 결정을 내립니다.\n6. **복잡도 감소**: 더 이상 필요하지 않을 것 같은 오래된 파일을 식별합니다.\n\n## 사용 방법\n\n### 홈 디렉토리에서\n\n```\ncd ~\n```\n\n그 다음 Claude Code를 실행하여 도움을 요청하세요:\n\n```\n내 다운로드 폴더 정리를 도와줘\n```\n\n```\n내 문서(Documents) 폴더에서 중복 파일을 찾아줘\n```\n\n```\n내 프로젝트 디렉토리들을 검토하고 개선 사항을 제안해줘\n```\n\n### 구체적인 정리 작업\n\n```\n다운로드된 파일들을 내용에 따라 적절한 폴더로 정리해줘\n```\n\n```\n중복 파일을 찾아서 어떤 것을 남길지 결정하는 걸 도와줘\n```\n\n```\n6개월 이상 손대지 않은 오래된 파일들을 정리해줘\n```\n\n```\n내 [업무/프로젝트/사진/기타]를 위한 더 나은 폴더 구조를 만들어줘\n```\n\n## 지침 (Instructions)\n\n사용자가 파일 정리 도움을 요청할 때:\n\n1. **범위 파악 (Understand the Scope)**\n   \n   명확한 질문을 던지세요:\n   - 어떤 디렉토리를 정리해야 하나요? (다운로드, 문서, 전체 홈 폴더 등)\n   - 주요 문제가 무엇인가요? (파일 찾기 어려움, 중복, 너무 지저분함, 구조 없음 등)\n   - 피해야 할 파일이나 폴더가 있나요? (진행 중인 프로젝트, 민감한 데이터 등)\n   - 얼마나 공격적으로 정리할까요? (보수적인 정리 vs. 포괄적인 클린업)\n\n2. **현재 상태 분석 (Analyze Current State)**\n   \n   대상 디렉토리를 검토합니다:\n   ```bash\n   # 현재 구조 개요 파악\n   ls -la [대상_디렉토리]\n   \n   # 파일 유형 및 크기 확인\n   find [대상_디렉토리] -type f -exec file {} \\; | head -20\n   \n   # 가장 큰 파일 식별\n   du -sh [대상_디렉토리]/* | sort -rh | head -20\n   \n   # 파일 유형별 카운트\n   find [대상_디렉토리] -type f | sed 's/.*\\.//' | sort | uniq -c | sort -rn\n   ```\n   \n   발견 사항 요약:\n   - 전체 파일 및 폴더 수\n   - 파일 유형별 분류\n   - 크기 분포\n   - 날짜 범위\n   - 명백한 정리 이슈\n\n3. **정리 패턴 식별 (Identify Organization Patterns)**\n   \n   파일들을 바탕으로 논리적인 그룹을 결정합니다:\n   \n   **유형별 (By Type)**:\n   - Documents (PDF, DOCX, TXT)\n   - Images (JPG, PNG, SVG)\n   - Videos (MP4, MOV)\n   - Archives (ZIP, TAR, DMG)\n   - Code/Projects (코드가 포함된 디렉토리)\n   - Spreadsheets (XLSX, CSV)\n   - Presentations (PPTX, KEY)\n   \n   **용도별 (By Purpose)**:\n   - 업무용 vs. 개인용\n   - 활성(Active) vs. 아카이브(Archive)\n   - 프로젝트별\n   - 참조 자료\n   - 임시/작업용 파일\n   \n   **날짜별 (By Date)**:\n   - 현재 연도/월\n   - 지난 연도\n   - 매우 오래된 파일 (아카이브 대상)\n\n4. **중복 파일 찾기 (Find Duplicates)**\n   \n   요청 시 중복 파일을 검색합니다:\n   ```bash\n   # 해시(hash)를 이용한 정확한 중복 찾기\n   find [디렉토리] -type f -exec md5 {} \\; | sort | uniq -d\n   \n   # 이름이 같은 파일 찾기\n   find [디렉토리] -type f -printf '%f\\n' | sort | uniq -d\n   \n   # 크기가 유사한 파일 찾기\n   find [디렉토리] -type f -printf '%s %p\\n' | sort -n\n   ```\n   \n   각 중복 세트에 대해:\n   - 모든 파일 경로 표시\n   - 크기 및 수정 날짜 표시\n   - 남길 파일 권장 (보통 최신 파일이나 이름이 제일 잘 지어진 것)\n   - **중요**: 삭제 전에는 반드시 확인 과정을 거칩니다.\n\n5. **정리 계획 제안 (Propose Organization Plan)**\n   \n   변경을 적용하기 전에 명확한 계획을 제시합니다:\n   \n   ```markdown\n   # [디렉토리] 정리 계획\n   \n   ## 현재 상태\n   - Y개 폴더에 총 X개 파일 존재\n   - 총 용량: [크기]\n   - 파일 유형: [분류]\n   - 이슈: [문제 목록]\n   \n   ## 제안하는 구조\n   \n   ```\n   [디렉토리]/\n   ├── Work/\n   │   ├── Projects/\n   │   ├── Documents/\n   │   └── Archive/\n   ├── Personal/\n   │   ├── Photos/\n   │   ├── Documents/\n   │   └── Media/\n   └── Downloads/\n       ├── To-Sort/\n       └── Archive/\n   ```\n   \n   ## 실행할 변경 사항\n   \n   1. **새 폴더 생성**: [목록]\n   2. **파일 이동**:\n      - X개의 PDF → Work/Documents/\n      - Y개의 이미지 → Personal/Photos/\n      - Z개의 오래된 파일 → Archive/\n   3. **이름 변경**: [이름 변경 패턴]\n   4. **삭제**: [중복 또는 휴지통 파일]\n   \n   ## 확인이 필요한 파일\n   \n   - [확실하지 않은 파일 목록]\n   \n   진행할까요? (yes/no/modify)\n   ```\n\n6. **정리 실행 (Execute Organization)**\n   \n   승인 후 체계적으로 정리를 진행합니다:\n   \n   ```bash\n   # 폴더 구조 생성\n   mkdir -p \"path/to/new/folders\"\n   \n   # 명확한 로깅과 함께 파일 이동\n   mv \"old/path/file.pdf\" \"new/path/file.pdf\"\n   \n   # 일관된 패턴으로 파일 이름 변경\n   # 예: \"YYYY-MM-DD - 설명.ext\"\n   ```\n   \n   **중요 규칙**:\n   - 삭제 전에는 반드시 확인합니다.\n   - 나중에 되돌릴 수 있도록 모든 이동 기록을 남깁니다.\n   - 원래의 수정 날짜를 보존합니다.\n   - 파일 이름 충돌을 적절히 처리합니다.\n   - 예상치 못한 상황이 발생하면 중단하고 질문합니다.\n\n7. **요약 및 유지 관리 팁 제공**\n   \n   정리 완료 후:\n   \n   ```markdown\n   # 정리 완료! ✨\n   \n   ## 변경 내용\n   \n   - [X]개의 새 폴더 생성\n   - [Y]개의 파일 정리 완료\n   - 중복 제거로 [Z] GB 공간 확보\n   - [W]개의 오래된 파일 아카이브 처리\n   \n   ## 새로운 구조\n   \n   [새로운 폴더 트리 표시]\n   \n   ## 유지 관리 팁\n   \n   이 상태를 유지하려면:\n   \n   1. **매주**: 새로운 다운로드 파일 분류\n   2. **매월**: 완료된 프로젝트 검토 및 아카이브\n   3. **매분기**: 새로운 중복 파일 체크\n   4. **매년**: 오래된 파일 아카이브\n   \n   ## 활용 가능한 명령어\n   \n   ```bash\n   # 이번 주에 수정된 파일 찾기\n   find . -type f -mtime -7\n   \n   # 유형별 다운로드 정렬\n   [사용자 환경에 맞춘 커스텀 명령]\n   \n   # 중복 찾기\n   [커스텀 명령]\n   ```\n   \n   다른 폴더도 정리할까요?\n   ```\n\n## 예시 (Examples)\n\n### 예시 1: 다운로드 폴더 정리 (Justin Dielmann 사례)\n\n**사용자**: \"다운로드 폴더에 파일이 500개가 넘어서 엉망이야. 정리 좀 도와줘.\"\n\n**과정**:\n1. 다운로드 폴더 분석\n2. 패턴 발견: 업무 문서, 개인 사진, 설치 파일, 임의의 PDF 등\n3. 구조 제안:\n   - Downloads/\n     - Work/\n     - Personal/\n     - Installers/ (DMG, PKG 파일)\n     - Archive/\n     - ToSort/ (결정이 필요한 것들)\n4. 확인 요청\n5. 내용과 이름을 바탕으로 지능적으로 파일 이동\n6. 결과: 500개 파일 → 5개의 정리된 폴더\n\n### 예시 2: 중복 파일 찾기 및 제거\n\n**사용자**: \"내 문서 폴더에서 중복된 파일을 찾아서 어떤 걸 남길지 결정하는 걸 도와줘.\"\n\n**출력**:\n```markdown\n# 23세트의 중복 파일 발견 (총 156 MB)\n\n## 중복 세트 1: \"proposal.pdf\"\n- `/Documents/proposal.pdf` (2.3 MB, 수정: 2024-03-15)\n- `/Documents/old/proposal.pdf` (2.3 MB, 수정: 2024-03-15)\n- `/Desktop/proposal.pdf` (2.3 MB, 수정: 2024-03-10)\n\n**권장 사항**: `/Documents/proposal.pdf` 유지 (올바른 위치에 있는 최신 파일)\n나머지 2개 복사본을 삭제할까요?\n\n[모든 중복 항목에 대해 계속...]\n```\n\n### 예시 3: 프로젝트 폴더 재구조화\n\n**사용자**: \"~/Projects 디렉토리를 검토하고 개선 사항을 제안해줘.\"\n\n**출력**:\n```markdown\n# ~/Projects 분석 결과\n\n## 현재 구조의 문제점\n- 활성 프로젝트와 아카이브된 프로젝트(3년 이상 됨)가 섞여 있음\n- 일관된 명명 규칙이 없음\n- 어떤 프로젝트는 루트에 있고, 어떤 것은 임의의 하위 폴더에 있음\n- 중복된 폴더 (project-name, project-name-old, project-name-v2)\n\n## 제안하는 구조\n\n```\nProjects/\n├── Active/\n│   ├── client-work/\n│   ├── side-projects/\n│   └── learning/\n├── Archive/\n│   ├── 2022/\n│   ├── 2023/\n│   └── 2024/\n└── Templates/\n```\n\n## 구체적인 변경 사항\n1. 2022년 이후 손대지 않은 12개 프로젝트 이동 → Archive/\n2. 4개의 중복된 프로젝트 폴더 통합\n3. 프로젝트 이름을 일관된 형식으로 변경: \"client-name-project-name\"\n4. 오래된 작업물을 위한 아카이브 생성\n\n제가 이를 실행할까요?\n```\n\n### 예시 4: 날짜별 사진 정리\n\n**사용자**: \"내 사진 폴더를 연도와 월별로 정리해줘.\"\n\n**출력**: 다음과 같은 구조 생성:\n```\nPhotos/\n├── 2023/\n│   ├── 01-January/\n│   ├── 02-February/\n│   └── ...\n├── 2024/\n│   ├── 01-January/\n│   └── ...\n└── Unsorted/\n```\n\n그 후 EXIF 데이터나 파일 수정 날짜를 바탕으로 사진을 이동합니다.\n\n## 일반적인 정리 작업\n\n### 다운로드 클린업\n```\n다운로드 폴더를 정리해줘 - 문서는 Documents로, \n사진은 Pictures로 이동하고, 설치 파일은 따로 보관해. \n3개월 넘은 파일들은 아카이브해줘.\n```\n\n### 프로젝트 정리\n```\n내 Projects 폴더 구조를 검토하고 활성 프로젝트와 \n아카이브해야 할 오래된 프로젝트를 분리하는 걸 도와줘.\n```\n\n### 중복 제거\n```\n내 Documents 폴더의 모든 중복 파일을 찾고 \n어떤 걸 남길지 결정하는 걸 도와줘.\n```\n\n### 데스크탑 정리\n```\n내 바탕화면이 파일로 가득 찼어. 전부 Documents 폴더로 \n제대로 정리하는 걸 도와줘.\n```\n\n### 사진 정리\n```\n이 폴더의 모든 사진을 찍은 날짜(연도/월)별로 정리해줘.\n```\n\n### 업무/개인 분리\n```\n내 Documents 폴더 전체에서 업무용 파일과 \n개인용 파일을 분리하는 걸 도와줘.\n```\n\n## 프로 팁 (Pro Tips)\n\n1. **작게 시작하기**: 신뢰를 쌓기 위해 다운로드 같은 지저분한 폴더 하나부터 시작하세요.\n2. **정기적인 관리**: 다운로드 폴더에 대해 매주 클린업을 실행하세요.\n3. **일관된 명명**: 중요한 파일에는 \"YYYY-MM-DD - 설명\" 형식을 사용하세요.\n4. **적극적인 아카이브**: 오래된 프로젝트는 삭제하지 말고 Archive로 옮기세요.\n5. **활성 상태 분리**: 진행 중인 작업과 아카이브된 작업 사이에 명확한 경계를 유지하세요.\n6. **프로세스 믿기**: 무엇을 어디에 둘지에 대한 인지적 부하를 Claude에게 맡기세요.\n\n## 모범 사례 (Best Practices)\n\n### 폴더 명명 규칙\n- 명확하고 설명적인 이름을 사용하세요.\n- 공백을 피하세요 (하이픈이나 언더스코어 사용).\n- 구체적으로 적으세요: \"docs\" 대신 \"client-proposals\".\n- 순서 정렬을 위해 접두사를 사용하세요: \"01-current\", \"02-archive\".\n\n### 파일 명명 규칙\n- 날짜 포함하기: \"2024-10-17-meeting-notes.md\"\n- 설명적으로 적기: \"q3-financial-report.xlsx\"\n- 이름에 버전 번호를 넣지 마세요 (대신 버전 관리 시스템 사용).\n- 다운로드 흔적 제거하기: \"document-final-v2 (1).pdf\" → \"document.pdf\"\n\n### 아카이브 시점\n- 6개월 이상 손대지 않은 프로젝트\n- 나중에 참조할 수 있는 완료된 작업\n- 새로운 시스템으로 마이그레이션한 후의 이전 버전\n- 삭제하기가 망설여지는 파일 (먼저 아카이브하세요)\n\n## 관련 유스케이스\n\n- 새 컴퓨터의 초기 정리 설정\n- 백업/아카이브를 위한 파일 준비\n- 저장 공간 정리 전 클린업\n- 공유 팀 폴더 정리\n- 새로운 프로젝트 디렉토리 구조 잡기\n",
        "icartsh-plugin/skills/frontend-design/SKILL.md": "---\nname: frontend-design\ndescription: 높은 디자인 품질을 갖춘 독특하고 운영 수준(production-grade)의 프런트엔드 인터페이스를 생성합니다. 웹 컴포넌트, 페이지, 아티팩트, 포스터 또는 애플리케이션(예: 웹사이트, 랜딩 페이지, 대시보드, React 컴포넌트, HTML/CSS 레이아웃 또는 모든 웹 UI의 스타일링/미화)을 구축할 때 이 SKILL을 사용하세요. 일반적인 AI 미학을 피하고 창의적이고 세련된 코드와 UI 디자인을 생성합니다.\nlicense: LICENSE.txt의 전체 약관 참조\n---\n\n이 SKILL은 일반적인 \"AI스러운\" 미학을 피하고 독특하며 운영 수준의 프런트엔드 인터페이스를 생성하도록 안내합니다. 미적 세부 사항과 창의적인 선택에 각별한 주의를 기울여 실제로 작동하는 코드를 구현합니다.\n\n사용자는 프런트엔드 요구 사항(빌드할 컴포넌트, 페이지, 애플리케이션 또는 인터페이스)을 제공합니다. 여기에는 목적, 대상 사용자 또는 기술적 제약 사항에 대한 컨텍스트가 포함될 수 있습니다.\n\n## 디자인 사고 (Design Thinking)\n\n코딩하기 전에 컨텍스트를 이해하고 대담한(BOLD) 미적 방향성을 정하세요:\n- **목적**: 이 인터페이스가 해결하려는 문제는 무엇인가요? 누가 사용하나요?\n- **톤(Tone)**: 극단적인 스타일을 선택하세요: 브루털 미니멀(brutally minimal), 맥시멀리스트 카오스(maximalist chaos), 레트로 퓨처리즘(retro-futuristic), 오가닉/네이처(organic/natural), 럭셔리/리파인드(luxury/refined), 플레이풀/토이(playful/toy-like), 에디토리얼/매거진(editorial/magazine), 브루털리스트/로우(brutalist/raw), 아트 데코/기하학(art deco/geometric), 소프트/파스텔(soft/pastel), 인더스트리얼/유틸리티(industrial/utilitarian) 등. 선택할 수 있는 수많은 풍미가 있습니다. 이를 영감으로 삼되, 미적 방향성에 충실한 디자인을 하세요.\n- **제약 사항**: 기술적 요구 사항 (프레임워크, 성능, 접근성).\n- **차별화**: 무엇이 이 인터페이스를 잊을 수 없게(UNFORGETTABLE) 만드나요? 사람들이 기억하게 될 한 가지는 무엇인가요?\n\n**중요**: 명확한 개념적 방향을 선택하고 정밀하게 실행하세요. 대담한 맥시멀리즘과 세련된 미니멀리즘 모두 효과적입니다. 핵심은 강도가 아니라 '의도성(intentionality)'입니다.\n\n그 후, 다음과 같은 실제로 작동하는 코드(HTML/CSS/JS, React, Vue 등)를 구현합니다:\n- 운영 수준의 기능적인 코드\n- 시각적으로 인상적이고 기억에 남는 디자인\n- 명확한 미적 관점과 정합성\n- 모든 세부 사항에서 세심하게 정제된 마감\n\n## 프런트엔드 미학 가이드라인 (Frontend Aesthetics Guidelines)\n\n다음에 집중하세요:\n- **타이포그래피 (Typography)**: 아름답고 독특하며 흥미로운 폰트를 선택하세요. Arial이나 Inter와 같은 일반적인 폰트는 피하세요. 대신 프런트엔드의 미학을 높여줄 개성 있는 폰트, 예상치 못한 캐릭터가 있는 폰트를 선택하세요. 독특한 디스플레이 폰트와 세련된 본문용 폰트를 조합하세요.\n- **컬러 및 테마 (Color & Theme)**: 일관된 미학을 고수하세요. 일관성을 위해 CSS 변수를 사용하세요. 소심하게 골고루 분포된 팔레트보다 강렬한 강조점(accents)이 있는 주도적인 컬러 배합이 더 효과적입니다.\n- **모션 (Motion)**: 효과와 마이크로 인터랙션을 위해 애니메이션을 사용하세요. HTML의 경우 가급적 CSS 전용 솔루션을 우선시하세요. React의 경우 사용 가능하다면 Motion 라이브러리를 사용하세요. 임팩트가 큰 순간에 집중하세요: 잘 조율된 페이지 로드와 순차적 노출(animation-delay)은 여기저기 흩어진 마이크로 인터랙션보다 더 큰 즐거움을 줍니다. 놀라움을 주는 스크롤 트리거와 호버 상태를 활용하세요.\n- **공간 구성 (Spatial Composition)**: 예상치 못한 레이아웃. 비대칭. 겹침(Overlap). 대각선 흐름. 그리드를 깨는 요소들. 넉넉한 여백 혹은 통제된 밀도.\n- **배경 및 시각적 세부 사항**: 단색을 기본으로 사용하기보다 분위기와 깊이를 만드세요. 전체적인 미학에 어울리는 컨텍스트 효과와 질감을 추가하세요. 그래디언트 메시(gradient meshes), 노이즈 텍스처, 기하학적 패턴, 레이어드 투명도, 드라마틱한 그림자, 장식적인 테두리, 커스텀 커서, 그레인 오버레이(grain overlays)와 같은 창의적인 형태를 적용하세요.\n\n자주 사용되는 폰트 패밀리(Inter, Roboto, Arial, 시스템 폰트), 진부한 컬러 스킴(특히 흰 배경의 보라색 그래디언트), 예측 가능한 레이아웃과 컴포넌트 패턴, 그리고 컨텍스트 고유의 캐릭터가 없는 일률적인 디자인과 같은 **일반적인 AI 생성 미학을 절대 사용하지 마세요.**\n\n창의적으로 해석하고, 해당 컨텍스트를 위해 진정으로 디자인된 느낌을 주는 예상치 못한 선택을 하세요. 어떤 디자인도 똑같아서는 안 됩니다. 라이트 테마와 다크 테마, 서로 다른 폰트, 서로 다른 미학을 다양하게 적용하세요. 생성 결과물들이 공통적인 선택지(예: Space Grotesk)에 머무르게 하지 마세요.\n\n**중요**: 구현 복잡도를 미적 비전에 맞추세요. 맥시멀리스트 디자인은 광범위한 애니메이션과 효과를 포함한 정교한 코드가 필요합니다. 미니멀리스트나 세련된 디자인은 절제, 정밀함, 그리고 간격, 타이포그래피, 미묘한 세부 사항에 대한 세심한 주의가 필요합니다. 우아함은 비전을 잘 실행하는 데서 나옵니다.\n\n기억하세요: Claude는 비범하고 창의적인 작업을 수행할 수 있습니다. 주저하지 말고 틀에서 벗어나 생각하며, 독특한 비전에 온전히 몰입했을 때 진정으로 무엇이 창조될 수 있는지 보여주세요.\n",
        "icartsh-plugin/skills/git-advanced/README.md": "# Git Advanced Skill\n\n복잡한 버전 관리 시나리오를 위한 고급 Git 작업 및 워크플로우.\n\n## 개요 (Overview)\n\n이 SKILL은 인터랙티브 리베이싱, 충돌 해결, 히스토리 조작 및 전략적인 브랜치 관리를 포함한 정교한 Git 작업에 대한 포괄적인 가이드를 제공합니다.\n\n## 빠른 시작 (Quick Start)\n\n```bash\n# 일반적인 작업을 위한 헬퍼 스크립트 사용\ncd /path/to/your/git/repo\nbash /path/to/git-advanced/scripts/git_helper.sh\n```\n\n## 포함 내용 (Contents)\n\n### 핵심 문서\n- **SKILL.md**: 워크플로우와 모범 사례를 포함한 전체 스킬 가이드\n\n### 헬퍼 스크립트 (Helper Scripts)\n- **scripts/git_helper.sh**: 다음을 위한 대화형 유틸리티:\n  - 브랜치 정리 (Branch cleanup)\n  - 충돌 해결 (Conflict resolution)\n  - 인터랙티브 리베이즈 보조\n  - 히스토리 시각화\n  - Stash 관리\n\n### 예시 (Examples)\n- **interactive_rebase.md**: 단계별 리베이즈 시나리오 및 패턴\n- **conflict_resolution.md**: 공통 충돌 패턴 및 해결 전략\n- **branch_strategies.md**: Git Flow, Trunk-based, GitHub Flow 등\n\n## 일반적인 유스케이스\n\n### 커밋 히스토리 정리\n```bash\ngit rebase -i HEAD~5\n# 커밋 병합(Squash), 순서 변경, 또는 삭제(Drop)\n```\n\n### 복잡한 충돌 해결\n```bash\nbash scripts/git_helper.sh resolve-conflicts\n# 대화형 충돌 해결 헬퍼\n```\n\n### 버그 도입 지점 찾기\n```bash\ngit bisect start\ngit bisect bad\ngit bisect good v1.0.0\n# 이진 탐색을 통해 문제 커밋 찾기\n```\n\n### 유실된 커밋 복구\n```bash\ngit reflog\ngit checkout -b recovery <lost-commit-hash>\n```\n\n### Git Flow 구현\n전체 워크플로우는 `examples/branch_strategies.md`를 참조하세요.\n\n## 학습 내용\n\n- 인터랙티브 리베이싱 (squash, reorder, edit, split commits)\n- 고급 머지 충돌 해결\n- 디버깅을 위한 Git bisect\n- 특정 커밋 cherry-picking\n- 커밋 복구를 위한 Reflog\n- 히스토리 재작성 및 정리\n- 브랜치 전략 구현\n- Stash 관리\n\n## 안전 제일 (Safety First)\n\n파괴적인 작업을 수행하기 전에는 항상 백업을 생성하세요:\n```bash\ngit branch backup-$(date +%Y%m%d-%H%M%S)\n```\n\n팀원과의 조율 없이 공유 브랜치에 포스 푸시(force push)하지 마세요.\n\n## 더 알아보기\n\n상세 워크플로우, 모범 사례 및 트러블슈팅 가이드는 `SKILL.md`에서 전체 스킬 문서를 확인하세요.\n",
        "icartsh-plugin/skills/git-advanced/SKILL.md": "---\nname: git-advanced\ndescription: 인터랙티브 리베이싱(interactive rebasing), 충돌 해결, 히스토리 조작, 버그 추적을 위한 bisect, cherry-picking, reflog 복구 및 브랜치 관리 전략을 포함한 고급 Git 작업 및 워크플로우입니다. 사용 사례: (1) 인터랙티브 리베이싱 및 커밋 정리, (2) 복잡한 머지 충돌 해결, (3) 버그 추적을 위한 Git bisect, (4) 히스토리 재작성 및 정리, (5) 브랜치 전략 구현 (Git Flow, trunk-based), (6) reflog를 이용한 유실된 커밋 복구\n---\n\n# Advanced Git Operations\n\n## 개요 (Overview)\n\n복잡한 버전 관리 시나리오를 위한 고급 Git 워크플로우를 마스터하세요. 이 SKILL은 기본적인 커밋과 머지를 넘어 인터랙티브 리베이싱, 고급 충돌 해결, 히스토리 조작 및 전략적 브랜치 관리를 포함한 정교한 작업들을 다룹니다.\n\n다음을 수행할 때 이 SKILL을 사용하세요:\n- 코드 리뷰 전 지저분한 커밋 히스토리 정리\n- 복잡한 머지 충돌의 전략적 해결\n- 이진 탐색(bisect)을 통한 버그 추적\n- 유실된 커밋 복구 또는 실수 되돌리기\n- 팀 브랜치 전략 구현\n- 안전한 히스토리 재작성\n\n## 핵심 역량 (Core Capabilities)\n\n### 인터랙티브 리베이싱 (Interactive Rebasing)\n- 논리적 히스토리를 위한 커밋 순서 재배치\n- 여러 커밋을 하나로 합치기 (Squash)\n- 과거 커밋 메시지 수정\n- 커밋을 더 작은 조각으로 분리\n- 히스토리에서 원치 않는 커밋 제거\n\n### 충돌 해결 (Conflict Resolution)\n- 전략적인 머지 충돌 처리\n- 3-way 머지 이해 및 활용\n- Ours vs. Theirs 전략 선택\n- 충돌 마커 해석\n- 충돌 중 파일의 부분적 스테이징(staging)\n\n### Git Bisect\n- 버그가 도입된 지점을 찾기 위한 이진 탐색\n- 스크립트를 이용한 자동 bisect\n- Good/Bad 커밋 식별\n- 회귀(regressions) 버그의 효율적인 디버깅\n\n### 히스토리 조작 (History Manipulation)\n- 안전한 커밋 수정 (Amend)\n- Filter-branch 작업\n- 대용량 파일을 위한 BFG Repo-Cleaner 활용\n- 주의 깊은 히스토리 재작성\n- 작성자 정보 및 날짜 보존\n\n### 브랜치 전략 (Branch Strategies)\n- Git Flow 워크플로우\n- Trunk-based 개발\n- Feature 브랜치 워크플로우\n- 릴리스(Release) 브랜치 관리\n- 핫픽스(Hotfix) 절차\n\n## 빠른 명령 참조 (Quick Command Reference)\n\n### 인터랙티브 리베이즈 (Interactive Rebase)\n```bash\n# 최근 5개 커밋을 인터랙티브하게 리베이즈\ngit rebase -i HEAD~5\n\n# main 브랜치를 대상으로 리베이즈\ngit rebase -i main\n\n# 충돌 해결 후 계속 진행\ngit rebase --continue\n\n# 중단하고 원래 상태로 복구\ngit rebase --abort\n```\n\n### 충돌 해결 (Conflict Resolution)\n```bash\n# '우리 것'(현재 브랜치) 선택\ngit checkout --ours <file>\n\n# '그들 것'(들어오는 브랜치) 선택\ngit checkout --theirs <file>\n\n# 충돌 발생 파일 목록 확인\ngit diff --name-only --diff-filter=U\n\n# 해결됨으로 표시\ngit add <file>\n```\n\n### Git Bisect\n```bash\n# bisect 세션 시작\ngit bisect start\ngit bisect bad\ngit bisect good <commit-hash>\n\n# 테스트 스크립트로 자동화\ngit bisect run ./test-script.sh\n\n# bisect 세션 종료\ngit bisect reset\n```\n\n### Cherry-Pick\n```bash\n# 특정 커밋 적용\ngit cherry-pick <commit-hash>\n\n# 커밋하지 않고 cherry-pick (스테이징만 수행)\ngit cherry-pick -n <commit-hash>\n\n# 커밋 범위 cherry-pick\ngit cherry-pick A^..B\n```\n\n### Reflog 복구 (Reflog Recovery)\n```bash\n# reflog 히스토리 확인\ngit reflog\n\n# 유실된 커밋 복구\ngit checkout -b recovery <commit-hash>\n\n# 이전 상태로 리셋\ngit reset --hard HEAD@{2}\n```\n\n## 핵심 워크플로우 (Core Workflows)\n\n### 1. 인터랙티브 리베이즈 워크플로우 (Interactive Rebase Workflow)\n\n**사용 시기:**\n- 푸시 전 지저분한 커밋 히스토리 정리\n- \"WIP\" 또는 \"fix typo\" 커밋 병합\n- 논리적 흐름에 맞춰 커밋 순서 재조정\n- 커다란 커밋을 전문적인 변경 사항들로 조각내기\n\n**단계:**\n\n```bash\n# 1. 최근 N개 커밋에 대해 인터랙티브 리베이즈 시작\ngit rebase -i HEAD~5\n\n# 커밋 목록이 포함된 인터랙티브 에디터가 열림\n# 히스토리 재구성을 위해 명령어를 수정\n```\n\n**리베이즈 명령어:**\n- `pick` (p): 커밋 그대로 유지\n- `reword` (r): 커밋은 유지하되 메시지 수정\n- `edit` (e): 커밋을 유지하되 수정을 위해 멈춤 (amend 가능)\n- `squash` (s): 이전 커밋과 합치고 메시지 유지\n- `fixup` (f): 이전 커밋과 합치되 메시지는 버림\n- `drop` (d): 커밋을 완전히 제거\n\n**예시:**\n```bash\n# 수정 전\npick abc1234 Add feature X\npick def5678 Fix typo\npick ghi9012 WIP commit\npick jkl3456 Update documentation\n\n# 정리 후\npick abc1234 Add feature X\nfixup def5678 Fix typo\ndrop ghi9012 WIP commit\nreword jkl3456 Update documentation\n```\n\n**안전 팁:**\n- 이미 공유 브랜치에 푸시된 커밋은 리베이즈하지 마세요.\n- 백업 브랜치 생성: `git branch backup`\n- 문제가 생기면 `git reflog`를 사용하세요.\n- 포스 푸시(force push) 시 주의: `git push --force-with-lease`\n\n상세 예시와 고급 기법은 `examples/interactive_rebase.md`를 참조하세요.\n\n### 2. 충돌 해결 워크플로우 (Conflict Resolution Workflow)\n\n**충돌 마커 이해하기:**\n```\n<<<<<<< HEAD (현재 변경 사항)\n현재 브랜치의 코드\n=======\n머지하려는 브랜치의 코드\n>>>>>>> branch-name (들어오는 변경 사항)\n```\n\n**해결 프로세스:**\n\n```bash\n# 1. 충돌 발생 확인\ngit status\n\n# 2. 해결 전략 선택:\n\n# 전략 A: 수동 해결\nvim <file>  # 마커 사이의 내용 수정\ngit add <file>\n\n# 전략 B: 한쪽 선택\ngit checkout --ours <file>    # 내 것 유지\ngit checkout --theirs <file>  # 상대 것 유지\ngit add <file>\n\n# 전략 C: 머지 도구 사용\ngit mergetool\n\n# 3. 작업 계속 진행\ngit merge --continue\n# 또는\ngit rebase --continue\n```\n\n**3-Way Diff 확인:**\n```bash\n# '내가' 변경한 내용 확인\ngit diff --ours <file>\n\n# '그들이' 변경한 내용 확인\ngit diff --theirs <file>\n\n# 공통 조상(Base) 확인\ngit diff --base <file>\n```\n\n공통적인 충돌 패턴과 해결책은 `examples/conflict_resolution.md`를 참조하세요.\n\n### 3. 버그 추적을 위한 Git Bisect (Git Bisect for Bug Hunting)\n\n**시나리오:** 현재 버전에 버그가 있을 때, 어떤 커밋에서 도입되었는지 찾습니다.\n\n**수동 Bisect:**\n```bash\n# 1. bisect 시작\ngit bisect start\n\n# 2. 현재 상태를 'bad'로 마킹\ngit bisect bad\n\n# 3. 버그가 없었던 과거 커밋을 'good'으로 마킹\ngit bisect good v1.0.0\n\n# 4. 코드 테스트 (Git이 중간 지점 커밋을 체크아웃함)\n# 앱을 실행하여 버그 존재 여부 확인\n\n# 5. 결과 마킹\ngit bisect bad   # 버그가 있는 경우\ngit bisect good  # 버그가 없는 경우\n\n# Git이 첫 번째 bad 커밋을 찾을 때까지 반복\n\n# 6. bisect 종료\ngit bisect reset\n```\n\n**자동화된 Bisect:**\n```bash\n# 테스트 스크립트 작성 (test.sh)\n#!/bin/bash\nnpm test\nexit $?\n\n# 자동화된 bisect 실행\ngit bisect start\ngit bisect bad\ngit bisect good v1.0.0\ngit bisect run ./test.sh\n\n# Git이 자동으로 bad 커밋을 찾아냄\ngit bisect reset\n```\n\n### 4. 브랜치 관리 (Branch Management)\n\n**빠른 정리:**\n```bash\n# 헬퍼 스크립트 사용\nbash scripts/git_helper.sh cleanup-branches\n\n# 또는 수동 정리\ngit branch --merged main | grep -v \"\\*\\|main\\|develop\" | xargs git branch -d\ngit fetch --prune\n```\n\n**오래된 브랜치 감지:**\n```bash\n# 마지막 커밋 날짜와 함께 브랜치 표시\nfor branch in $(git branch -r | grep -v HEAD); do\n    echo -e \"$(git show --format=\"%ci %cr\" $branch | head -n 1)\\t$branch\"\ndone | sort -r\n```\n\n상세한 브랜치 관리 전략은 `references/branch-management.md`를 참조하세요.\n\n### 5. Reflog를 이용한 복구 (Reflog Recovery)\n\n**삭제된 브랜치 복구:**\n```bash\n# reflog 확인\ngit reflog\n\n# 브랜치가 삭제된 시점의 커밋 찾기\n# 브랜치 복구\ngit checkout -b feature-x <commit-hash>\n```\n\n**잘못된 Reset 되돌리기:**\n```bash\n# 실수로 실행함: git reset --hard HEAD~5\n\n# reflog 확인\ngit reflog\n\n# 이전 상태로 복구\ngit reset --hard HEAD@{1}\n```\n\n**유실된 커밋 찾기:**\n```bash\n# 연결이 끊긴(dangling) 커밋 찾기\ngit fsck --lost-found\n\n# 찾은 커밋을 cherry-pick\ngit cherry-pick <lost-commit-hash>\n```\n\n포괄적인 복구 기법은 `references/reflog-recovery.md`를 참조하세요.\n\n## 브랜치 전략 구현 (Branch Strategy Implementation)\n\n이 SKILL은 다양한 브랜치 전략 구현을 지원합니다. 팀의 필요에 따라 선택하세요:\n\n### Git Flow\n**적합한 사례:** 정기적인 릴리스 일정이 있고, 여러 운영 버전을 유지해야 하는 프로젝트\n\n**브랜치 유형:**\n- `main`: 운영 환경에 배포 가능한 코드\n- `develop`: 개발 통합 브랜치\n- `feature/*`: 새로운 기능 개발\n- `release/*`: 릴리스 준비\n- `hotfix/*`: 운영 환경 긴급 수정\n\n**빠른 시작:**\n```bash\n# 기능 개발 시작\ngit checkout develop\ngit checkout -b feature/user-auth\n# 작업 완료 후\ngit checkout develop\ngit merge --no-ff feature/user-auth\n```\n\n### Trunk-Based Development\n**적합한 사례:** 지속적 배포(CI/CD), 빠른 속도를 지향하는 팀\n\n**원칙:**\n- 단일 메인 브랜치 사용\n- 짧은 수명의 피처 브랜치 (< 1일)\n- 잦은 통합\n- 미완성 작업을 위한 기능 플래그(Feature flags) 사용\n\n**빠른 시작:**\n```bash\n# 짧은 수명의 브랜치 생성\ngit checkout main\ngit checkout -b feature/quick-fix\n# 같은 날 작업 완료 및 머지\ngit checkout main\ngit merge feature/quick-fix\n```\n\n전체 브랜치 전략 워크플로우는 `examples/branch_strategies.md`를 참조하세요.\n\n## 모범 사례 (Best Practices)\n\n### 리베이즈 전\n- 백업 브랜치 생성: `git branch backup`\n- 작업 디렉토리가 깔끔한지 확인: `git status`\n- 커밋 히스토리 확인: `git log --oneline`\n- 공개/공유 브랜치는 절대 리베이즈하지 말 것\n\n### 충돌 발생 시\n- 충돌의 양쪽 변경 사항을 모두 이해할 것\n- 해결 후 철저히 테스트할 것\n- 의미 있는 머지 커밋 메시지 작성\n- 복잡한 해결 과정은 문서화할 것\n\n### 브랜치 관리\n- 머지된 브랜치는 즉시 삭제할 것\n- 설명적인 브랜치 이름 사용: `feature/user-login`\n- 브랜치는 목적에 집중하고 짧게 유지할 것\n- main 브랜치와 정기적으로 동기화할 것\n\n### 히스토리 관리\n- 푸시 전 \"fixup\" 커밋들은 합칠(squash) 것\n- 명확하고 설명적인 커밋 메시지 작성\n- 커밋은 원자적으로 유지 (하나의 논리적 변경 사항)\n- 가능한 경우 Conventional Commit 형식을 따를 것\n\n포괄적인 모범 사례는 `references/best-practices.md`를 참조하세요.\n\n## 트러블슈팅 (Troubleshooting)\n\n### 리베이즈 충돌\n```bash\n# 충돌이 너무 복잡한 경우\ngit rebase --abort\n\n# 다른 방식으로 다시 시작\ngit merge --no-ff <branch>\n```\n\n### 유실된 커밋\n```bash\n# 항상 reflog부터 확인\ngit reflog\n\n# 커밋을 찾아 복구\ngit checkout -b recovery <commit-hash>\n```\n\n### Detached HEAD 상태\n```bash\n# 현재 상태에서 브랜치 생성\ngit checkout -b new-branch-name\n\n# 또는 원래 브랜치로 복구\ngit checkout main\n```\n\n### 리베이즈 후 푸시 실패 (Failed Push)\n```bash\n# 확실한 경우에만, 그리고 공유 브랜치가 아닐 때만 실행\ngit push --force-with-lease\n\n# 더 안전한 방식: 새 브랜치 생성\ngit checkout -b feature-v2\ngit push origin feature-v2\n```\n\n상세한 트러블슈팅은 `references/troubleshooting.md`를 참조하세요.\n\n## 추가 자료\n\n### 상세 가이드\n- `examples/interactive_rebase.md`: 시나리오별 단계적 리베이즈 예시\n- `examples/conflict_resolution.md`: 공통 충돌 패턴 및 해결책\n- `examples/branch_strategies.md`: 전체 Git Flow 및 Trunk-based 워크플로우\n\n### 참조 문서\n- `references/branch-management.md`: 브랜치 정리 자동화 및 전략\n- `references/reflog-recovery.md`: 복구 기법 및 히스토리 재작성\n- `references/best-practices.md`: 포괄적인 모범 사례 및 품질 표준\n- `references/troubleshooting.md`: 일반적인 이슈 및 긴급 복구\n\n### 헬퍼 스크립트\n- `scripts/git_helper.sh`: 브랜치 정리 및 충돌 해결 유틸리티\n\n## 빠른 참조 명령 (Quick Reference Commands)\n\n### 필수 명령\n```bash\n# 인터랙티브 리베이즈\ngit rebase -i HEAD~N\n\n# 작업 중단\ngit rebase --abort\ngit merge --abort\ngit cherry-pick --abort\n\n# 히스토리 확인\ngit log --oneline --graph --all\ngit reflog\n\n# 충돌 해결\ngit checkout --ours <file>\ngit checkout --theirs <file>\n\n# 복구\ngit fsck --lost-found\ngit reflog\n\n# 정리\ngit branch --merged | xargs git branch -d\ngit fetch --prune\n```\n\n### 안전 제일 (Safety First)\n1. 히스토리 재작성 전 항상 백업: `git branch backup`\n2. 공유 브랜치에 포스 푸시 금지: `--force-with-lease` 사용\n3. 충돌 해결 후 반드시 테스트: 해결이 올바르다고 가정하지 말 것\n4. 복잡한 작업은 기록: 머지 커밋에 주석 남기기\n5. reflog 활용: 30일 이상 당신의 안전망이 되어줍니다.\n\n---\n\n깔끔한 히스토리를 유지하고, 충돌을 효율적으로 해결하며, 복잡한 개발 워크플로우를 자신 있게 관리하기 위해 이 고급 Git 작업들을 마스터하세요.\n",
        "icartsh-plugin/skills/git-advanced/examples/branch_strategies.md": "# Git Branch Strategies\n\n## Overview\n\nChoosing the right branching strategy is crucial for team productivity and code quality. This guide covers the most popular branching workflows with practical examples and best practices.\n\n## Table of Contents\n\n1. [Git Flow](#git-flow)\n2. [Trunk-Based Development](#trunk-based-development)\n3. [GitHub Flow](#github-flow)\n4. [GitLab Flow](#gitlab-flow)\n5. [Feature Branch Workflow](#feature-branch-workflow)\n6. [Comparison and Selection Guide](#comparison-and-selection-guide)\n\n## Git Flow\n\n### Overview\n\nGit Flow is a robust branching model for projects with scheduled releases. It defines strict branch roles and merge patterns.\n\n### Branch Types\n\n**Permanent Branches:**\n- `main` (or `master`): Production-ready code\n- `develop`: Integration branch for features\n\n**Temporary Branches:**\n- `feature/*`: New feature development\n- `release/*`: Release preparation\n- `hotfix/*`: Emergency production fixes\n\n### Complete Git Flow Example\n\n#### 1. Initial Setup\n\n```bash\n# Create and push develop branch\ngit checkout -b develop main\ngit push -u origin develop\n\n# Set up branch protection rules (on GitHub/GitLab)\n# - main: Require PR reviews, no direct commits\n# - develop: Require PR reviews\n```\n\n#### 2. Feature Development\n\n```bash\n# Start new feature\ngit checkout develop\ngit pull origin develop\ngit checkout -b feature/user-authentication\n\n# Work on feature\ngit add src/auth.js\ngit commit -m \"Add login endpoint\"\n\ngit add src/auth.js\ngit commit -m \"Add JWT token generation\"\n\ngit add tests/auth.test.js\ngit commit -m \"Add authentication tests\"\n\n# Push feature branch\ngit push -u origin feature/user-authentication\n\n# Create Pull Request to develop\n# After review and approval, merge to develop\n```\n\n**Merge Feature to Develop:**\n```bash\n# Option A: Via PR (recommended)\n# Use GitHub/GitLab UI\n\n# Option B: Manually\ngit checkout develop\ngit pull origin develop\ngit merge --no-ff feature/user-authentication\ngit push origin develop\ngit branch -d feature/user-authentication\ngit push origin --delete feature/user-authentication\n```\n\n#### 3. Release Process\n\n```bash\n# Create release branch from develop\ngit checkout develop\ngit pull origin develop\ngit checkout -b release/v1.2.0\n\n# Prepare release\n# - Update version numbers\n# - Update CHANGELOG.md\n# - Final bug fixes only\n\n# Version bump\nnpm version minor  # Updates package.json to 1.2.0\ngit add package.json package-lock.json\ngit commit -m \"Bump version to 1.2.0\"\n\n# Update changelog\ngit add CHANGELOG.md\ngit commit -m \"Update changelog for v1.2.0\"\n\n# Push release branch\ngit push -u origin release/v1.2.0\n\n# Merge to main (via PR or manually)\ngit checkout main\ngit pull origin main\ngit merge --no-ff release/v1.2.0\ngit tag -a v1.2.0 -m \"Release version 1.2.0\"\ngit push origin main --tags\n\n# Merge back to develop\ngit checkout develop\ngit pull origin develop\ngit merge --no-ff release/v1.2.0\ngit push origin develop\n\n# Delete release branch\ngit branch -d release/v1.2.0\ngit push origin --delete release/v1.2.0\n```\n\n#### 4. Hotfix Process\n\n```bash\n# Create hotfix from main\ngit checkout main\ngit pull origin main\ngit checkout -b hotfix/security-patch\n\n# Fix critical issue\ngit add src/security.js\ngit commit -m \"Fix XSS vulnerability in user input\"\n\n# Bump patch version\nnpm version patch  # 1.2.0 -> 1.2.1\ngit add package.json\ngit commit -m \"Bump version to 1.2.1\"\n\n# Merge to main\ngit checkout main\ngit merge --no-ff hotfix/security-patch\ngit tag -a v1.2.1 -m \"Hotfix v1.2.1 - Security patch\"\ngit push origin main --tags\n\n# Merge to develop\ngit checkout develop\ngit merge --no-ff hotfix/security-patch\ngit push origin develop\n\n# Delete hotfix branch\ngit branch -d hotfix/security-patch\ngit push origin --delete hotfix/security-patch\n```\n\n### Git Flow Advantages\n\n✅ Clear separation of concerns\n✅ Supports multiple versions in production\n✅ Explicit release preparation\n✅ Easy to track feature progress\n\n### Git Flow Disadvantages\n\n❌ Complex for continuous deployment\n❌ High ceremony for simple projects\n❌ Multiple long-lived branches to maintain\n❌ Can slow down fast-moving teams\n\n## Trunk-Based Development\n\n### Overview\n\nTrunk-based development focuses on a single main branch with short-lived feature branches and frequent integration.\n\n### Core Principles\n\n1. **Single Source of Truth**: `main` is always deployable\n2. **Short-lived Branches**: Feature branches live < 1 day\n3. **Frequent Integration**: Merge to main multiple times per day\n4. **Feature Flags**: Hide incomplete features\n5. **Continuous Integration**: Automated tests on every commit\n\n### Workflow Example\n\n#### Daily Development Flow\n\n```bash\n# Morning: Start with latest main\ngit checkout main\ngit pull origin main\n\n# Create short-lived feature branch\ngit checkout -b feature/quick-improvement\n\n# Work for a few hours\ngit add src/component.js\ngit commit -m \"Refactor data loading logic\"\n\n# Push and create PR\ngit push -u origin feature/quick-improvement\n\n# After quick review (same day)\n# Merge to main via PR\n# Automated tests run\n# Auto-deploy to staging\n\n# Delete branch immediately\ngit checkout main\ngit pull origin main\ngit branch -d feature/quick-improvement\n```\n\n#### Feature Flag Usage\n\n```javascript\n// Use feature flags for incomplete work\nimport { isFeatureEnabled } from './featureFlags';\n\nfunction UserDashboard() {\n    if (isFeatureEnabled('newDashboard')) {\n        return <NewDashboard />;  // Work in progress\n    }\n    return <OldDashboard />;  // Stable version\n}\n\n// Deploy to main with flag off\n// Enable flag when ready\n```\n\n#### Direct Commit to Main (Small Changes)\n\n```bash\n# For very small changes (typos, config)\ngit checkout main\ngit pull origin main\n\n# Make small fix\ngit add README.md\ngit commit -m \"Fix typo in documentation\"\n\n# Push directly (if allowed)\ngit push origin main\n# Or create quick PR for review\n```\n\n### Trunk-Based Advantages\n\n✅ Simple and fast\n✅ Always ready to deploy\n✅ Encourages small changes\n✅ No merge conflicts from long-lived branches\n✅ Supports continuous deployment\n\n### Trunk-Based Disadvantages\n\n❌ Requires discipline\n❌ Needs robust CI/CD\n❌ Feature flags add complexity\n❌ Challenging for large teams without coordination\n\n## GitHub Flow\n\n### Overview\n\nSimplified workflow optimized for continuous deployment. Main branch is always deployable, features developed in branches.\n\n### Workflow Steps\n\n1. Create branch from `main`\n2. Add commits\n3. Open Pull Request\n4. Discuss and review\n5. Deploy to production\n6. Merge to `main`\n\n### Complete Example\n\n```bash\n# 1. Create feature branch\ngit checkout main\ngit pull origin main\ngit checkout -b feature/add-search\n\n# 2. Implement feature\ngit add src/search.js\ngit commit -m \"Add search functionality\"\n\ngit add tests/search.test.js\ngit commit -m \"Add search tests\"\n\ngit add docs/search.md\ngit commit -m \"Document search API\"\n\n# 3. Push and create PR\ngit push -u origin feature/add-search\n\n# Create PR on GitHub\n# - Add description\n# - Request reviews\n# - Link related issues\n\n# 4. Review process\n# - Reviewers comment\n# - Make changes based on feedback\n\ngit add src/search.js\ngit commit -m \"Address review feedback\"\ngit push origin feature/add-search\n\n# 5. Deploy from PR branch (optional)\n# Some teams deploy PR branch to staging for testing\n\n# 6. After approval, deploy to production\n# Then merge PR on GitHub\n\n# 7. Cleanup\ngit checkout main\ngit pull origin main\ngit branch -d feature/add-search\n```\n\n### GitHub Flow Advantages\n\n✅ Very simple\n✅ Fast feedback loop\n✅ Always ready to ship\n✅ Great for web applications\n\n### GitHub Flow Disadvantages\n\n❌ No formal release process\n❌ Challenging for multiple versions\n❌ Requires continuous deployment\n❌ Less suitable for traditional release cycles\n\n## GitLab Flow\n\n### Overview\n\nCombines feature branches with environment branches for staged deployments.\n\n### Branch Structure\n\n- `main`: Development integration\n- `production`: Production code\n- `staging`: Staging environment (optional)\n- `feature/*`: Feature development\n\n### Workflow Example\n\n```bash\n# 1. Feature development\ngit checkout main\ngit pull origin main\ngit checkout -b feature/notification-system\n\n# Implement feature\ngit add src/notifications.js\ngit commit -m \"Implement notification system\"\n\n# 2. Merge to main\n# Create merge request to main\n# After review, merge\n\n# 3. Deploy to staging\ngit checkout staging\ngit merge main\ngit push origin staging\n# Automated deployment to staging environment\n\n# 4. Test on staging\n# QA team tests\n# If issues found, fix in new branch from main\n\n# 5. Deploy to production\ngit checkout production\ngit merge staging\ngit tag -a v1.3.0 -m \"Release v1.3.0\"\ngit push origin production --tags\n# Automated deployment to production\n\n# 6. Hotfix process (if needed)\ngit checkout production\ngit checkout -b hotfix/critical-bug\n# Fix bug\ngit commit -am \"Fix critical bug\"\n\n# Merge to production\ngit checkout production\ngit merge hotfix/critical-bug\ngit push origin production\n\n# Backport to main\ngit checkout main\ngit merge hotfix/critical-bug\ngit push origin main\n```\n\n### GitLab Flow Advantages\n\n✅ Supports multiple environments\n✅ Clear deployment pipeline\n✅ Works with both continuous and scheduled releases\n✅ Flexible for different workflows\n\n### GitLab Flow Disadvantages\n\n❌ More complex than GitHub Flow\n❌ Environment branches can drift\n❌ Requires discipline in backporting\n\n## Feature Branch Workflow\n\n### Overview\n\nSimple workflow where all feature development happens in dedicated branches merged to main.\n\n### Basic Example\n\n```bash\n# Create feature branch\ngit checkout -b feature/analytics-dashboard main\n\n# Develop feature\ngit add src/analytics.js\ngit commit -m \"Add analytics tracking\"\n\ngit add src/dashboard.js\ngit commit -m \"Create analytics dashboard UI\"\n\n# Rebase on main to stay updated\ngit fetch origin\ngit rebase origin/main\n\n# Push and create PR\ngit push -u origin feature/analytics-dashboard\n\n# After review and approval\n# Merge via PR (squash or merge commit)\n\n# Delete branch\ngit branch -d feature/analytics-dashboard\n```\n\n## Comparison and Selection Guide\n\n### Git Flow\n**Best For:**\n- Projects with scheduled releases\n- Multiple production versions\n- Enterprise software\n- Products with support for old versions\n\n**Team Size:** Medium to Large\n\n### Trunk-Based Development\n**Best For:**\n- Continuous deployment\n- Fast-moving teams\n- Web applications\n- Modern DevOps practices\n\n**Team Size:** Small to Medium (with discipline)\n\n### GitHub Flow\n**Best For:**\n- Web applications\n- SaaS products\n- Continuous deployment\n- Simple, fast iteration\n\n**Team Size:** Small to Medium\n\n### GitLab Flow\n**Best For:**\n- Multiple environments\n- Staged deployments\n- Hybrid approach needs\n- Organizations with both CD and scheduled releases\n\n**Team Size:** Any\n\n## Best Practices for All Strategies\n\n### 1. Branch Naming Conventions\n\n```bash\n# Features\nfeature/user-authentication\nfeature/payment-integration\nfeat/JIRA-123-add-export\n\n# Bugs\nbugfix/login-error\nfix/memory-leak\nbug/JIRA-456-fix-crash\n\n# Hotfixes\nhotfix/security-patch\nhotfix/critical-data-loss\n\n# Releases\nrelease/v1.2.0\nrelease/2023-Q4\n\n# Experiments\nexperiment/new-algorithm\npoc/machine-learning\n```\n\n### 2. Commit Message Format\n\n```bash\n# Use conventional commits\ngit commit -m \"feat: add user authentication\"\ngit commit -m \"fix: resolve memory leak in cache\"\ngit commit -m \"docs: update API documentation\"\ngit commit -m \"refactor: simplify data loading logic\"\ngit commit -m \"test: add unit tests for payment module\"\n\n# Include ticket numbers\ngit commit -m \"feat(auth): add SSO support [JIRA-123]\"\n```\n\n### 3. Pull Request Best Practices\n\n```markdown\n## Description\nBrief description of changes\n\n## Type of Change\n- [ ] Bug fix\n- [x] New feature\n- [ ] Breaking change\n- [ ] Documentation update\n\n## Testing\n- Unit tests added/updated\n- Integration tests passing\n- Manual testing completed\n\n## Screenshots (if applicable)\n[Add screenshots]\n\n## Checklist\n- [x] Code follows style guidelines\n- [x] Self-review completed\n- [x] Comments added for complex code\n- [x] Documentation updated\n- [x] No new warnings\n- [x] Tests added/updated\n```\n\n### 4. Branch Protection Rules\n\n```yaml\n# GitHub/GitLab Settings\nmain:\n  require_pull_request: true\n  required_approvals: 2\n  dismiss_stale_reviews: true\n  require_code_owner_reviews: true\n  require_status_checks: true\n  status_checks:\n    - continuous-integration\n    - code-quality\n    - security-scan\n  enforce_admins: true\n\ndevelop:\n  require_pull_request: true\n  required_approvals: 1\n  require_status_checks: true\n```\n\n### 5. Automation\n\n```yaml\n# Example CI/CD Pipeline\non:\n  pull_request:\n    branches: [main, develop]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Run tests\n        run: npm test\n      - name: Run linting\n        run: npm run lint\n      - name: Security scan\n        run: npm audit\n\n  deploy-staging:\n    if: github.ref == 'refs/heads/develop'\n    runs-on: ubuntu-latest\n    steps:\n      - name: Deploy to staging\n        run: ./deploy-staging.sh\n\n  deploy-production:\n    if: github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    steps:\n      - name: Deploy to production\n        run: ./deploy-production.sh\n```\n\n## Migration Between Strategies\n\n### From Git Flow to Trunk-Based\n\n```bash\n# 1. Merge all feature branches to develop\n# 2. Merge develop to main\ngit checkout main\ngit merge develop\n\n# 3. Make main the primary branch\n# 4. Archive develop branch\ngit branch -m develop develop-archived\ngit push origin :develop\n\n# 5. Adopt feature flags for incomplete work\n# 6. Shorten branch lifecycles to < 1 day\n```\n\n### From GitHub Flow to GitLab Flow\n\n```bash\n# 1. Create environment branches\ngit checkout -b staging main\ngit push -u origin staging\n\ngit checkout -b production main\ngit push -u origin production\n\n# 2. Update deployment pipelines\n# 3. Set up branch protection\n# 4. Update team processes\n```\n\n## Troubleshooting Common Issues\n\n### Long-Lived Branch Conflicts\n\n```bash\n# Regularly sync with base branch\ngit fetch origin\ngit rebase origin/main\n\n# Or merge if rebase is risky\ngit merge origin/main\n```\n\n### Accidentally Committed to Wrong Branch\n\n```bash\n# Move commit to correct branch\ngit checkout correct-branch\ngit cherry-pick <commit-hash>\n\n# Remove from wrong branch\ngit checkout wrong-branch\ngit reset --hard HEAD~1\n```\n\n### Need to Deploy Specific Features\n\n```bash\n# Use feature flags instead of branches\n# Or cherry-pick specific commits\ngit checkout release-branch\ngit cherry-pick <feature-commit-1>\ngit cherry-pick <feature-commit-2>\n```\n\n---\n\n**Choosing the right strategy depends on:**\n- Team size and distribution\n- Release cadence\n- Product type (web app vs. desktop vs. embedded)\n- CI/CD maturity\n- Team discipline and experience\n\nStart simple and evolve as needs change.\n",
        "icartsh-plugin/skills/git-advanced/examples/conflict_resolution.md": "# Git Conflict Resolution Patterns\n\n## Overview\n\nMerge conflicts are inevitable in collaborative development. This guide covers common conflict patterns, resolution strategies, and best practices for handling conflicts efficiently and safely.\n\n## Understanding Conflict Markers\n\n### Basic Conflict Structure\n\n```\n<<<<<<< HEAD (Current Change)\nThis is the code from your current branch\n=======\nThis is the code from the branch you're merging\n>>>>>>> feature-branch (Incoming Change)\n```\n\n### Anatomy of Conflict Markers\n\n- `<<<<<<< HEAD`: Start of your changes (current branch)\n- `=======`: Separator between the two versions\n- `>>>>>>> branch-name`: End of incoming changes\n- Everything between `<<<<<<<` and `=======` is YOUR code\n- Everything between `=======` and `>>>>>>>` is THEIR code\n\n## Common Conflict Patterns\n\n### Pattern 1: Simple Line Modification\n\n**Scenario:** Same line modified in both branches.\n\n**Conflict:**\n```javascript\n<<<<<<< HEAD\nconst API_URL = 'https://api.production.com';\n=======\nconst API_URL = 'https://api.staging.com';\n>>>>>>> feature-branch\n```\n\n**Resolution Options:**\n\n**Option A - Keep Yours:**\n```javascript\nconst API_URL = 'https://api.production.com';\n```\n\n**Option B - Keep Theirs:**\n```javascript\nconst API_URL = 'https://api.staging.com';\n```\n\n**Option C - Combine (Best):**\n```javascript\nconst API_URL = process.env.NODE_ENV === 'production'\n    ? 'https://api.production.com'\n    : 'https://api.staging.com';\n```\n\n**Commands:**\n```bash\n# Accept yours\ngit checkout --ours src/config.js\n\n# Accept theirs\ngit checkout --theirs src/config.js\n\n# Manual resolution\nvim src/config.js  # Edit manually\ngit add src/config.js\n```\n\n### Pattern 2: Adjacent Line Changes\n\n**Scenario:** Changes on adjacent lines.\n\n**Conflict:**\n```python\ndef calculate_total(items):\n<<<<<<< HEAD\n    subtotal = sum(item.price for item in items)\n    tax = subtotal * 0.08\n=======\n    subtotal = sum(item.price * item.quantity for item in items)\n    shipping = 5.99\n>>>>>>> feature-branch\n    return subtotal + tax\n```\n\n**Resolution (Combine Both):**\n```python\ndef calculate_total(items):\n    subtotal = sum(item.price * item.quantity for item in items)\n    tax = subtotal * 0.08\n    shipping = 5.99\n    return subtotal + tax + shipping\n```\n\n### Pattern 3: Function Signature Change\n\n**Scenario:** Function modified in different ways.\n\n**Conflict:**\n```javascript\n<<<<<<< HEAD\nfunction createUser(username, email, role) {\n    return {\n        username,\n        email,\n        role,\n        createdAt: new Date()\n    };\n}\n=======\nfunction createUser(username, email) {\n    return {\n        id: generateId(),\n        username,\n        email,\n        createdAt: new Date()\n    };\n}\n>>>>>>> feature-branch\n```\n\n**Resolution (Merge Features):**\n```javascript\nfunction createUser(username, email, role = 'user') {\n    return {\n        id: generateId(),\n        username,\n        email,\n        role,\n        createdAt: new Date()\n    };\n}\n```\n\n### Pattern 4: Import Conflicts\n\n**Scenario:** Different imports added.\n\n**Conflict:**\n```javascript\nimport React from 'react';\n<<<<<<< HEAD\nimport { useState, useEffect } from 'react';\nimport axios from 'axios';\n=======\nimport { useState, useCallback } from 'react';\nimport { api } from './api';\n>>>>>>> feature-branch\n```\n\n**Resolution:**\n```javascript\nimport React from 'react';\nimport { useState, useEffect, useCallback } from 'react';\nimport axios from 'axios';\nimport { api } from './api';\n```\n\n### Pattern 5: File Deleted vs Modified\n\n**Scenario:** One branch deleted file, other modified it.\n\n**Conflict Output:**\n```\nCONFLICT (modify/delete): src/old-component.js deleted in HEAD and modified in feature-branch.\n```\n\n**Resolution:**\n\n```bash\n# Keep deletion (remove file)\ngit rm src/old-component.js\n\n# Keep modification (restore file)\ngit checkout feature-branch -- src/old-component.js\ngit add src/old-component.js\n\n# Commit resolution\ngit commit\n```\n\n### Pattern 6: Rename Conflicts\n\n**Scenario:** File renamed differently in each branch.\n\n**Conflict:**\n```\nCONFLICT (rename/rename):\n    Rename src/utils.js -> src/helpers.js in HEAD\n    Rename src/utils.js -> src/utilities.js in feature-branch\n```\n\n**Resolution:**\n```bash\n# Choose one name\ngit mv src/helpers.js src/utilities.js\ngit add src/utilities.js\n\n# Or keep both\ngit add src/helpers.js src/utilities.js\n# Then update imports accordingly\n```\n\n## Resolution Strategies\n\n### Strategy 1: Accept All Ours\n\n**When to Use:**\n- Your branch is correct\n- Incoming changes should be discarded\n- Emergency hotfix takes precedence\n\n**Commands:**\n```bash\n# For all conflicts\ngit checkout --ours .\ngit add .\n\n# For specific file\ngit checkout --ours path/to/file.js\ngit add path/to/file.js\n```\n\n### Strategy 2: Accept All Theirs\n\n**When to Use:**\n- Main/develop is correct\n- Your changes are outdated\n- Rebasing onto updated base\n\n**Commands:**\n```bash\n# For all conflicts\ngit checkout --theirs .\ngit add .\n\n# For specific file\ngit checkout --theirs path/to/file.js\ngit add path/to/file.js\n```\n\n### Strategy 3: Manual Merge\n\n**When to Use:**\n- Need to combine both changes\n- Complex logic requires understanding\n- Most common scenario\n\n**Process:**\n```bash\n# 1. Identify conflicts\ngit status\ngit diff --name-only --diff-filter=U\n\n# 2. Open file in editor\nvim path/to/conflicted-file.js\n\n# 3. Find conflict markers\n/<<<<<<  # Search in vim\n\n# 4. Edit to combine changes\n# Remove markers, keep/combine code\n\n# 5. Test the resolution\nnpm test\n\n# 6. Stage resolved file\ngit add path/to/conflicted-file.js\n\n# 7. Verify no conflicts remain\ngit diff --check\n\n# 8. Continue merge\ngit merge --continue\n# or\ngit rebase --continue\n```\n\n### Strategy 4: Use Merge Tool\n\n**Configure Merge Tool:**\n```bash\n# One-time setup\ngit config --global merge.tool vimdiff\n# or: meld, kdiff3, p4merge, opendiff\n\ngit config --global mergetool.prompt false\ngit config --global mergetool.keepBackup false\n```\n\n**Use Merge Tool:**\n```bash\n# Launch for all conflicts\ngit mergetool\n\n# Launch for specific file\ngit mergetool path/to/file.js\n\n# After resolving, clean up\ngit clean -f *.orig  # Remove backup files\n```\n\n### Strategy 5: Three-Way Diff\n\n**Understand All Perspectives:**\n```bash\n# Show what YOU changed\ngit diff --ours path/to/file.js\n\n# Show what THEY changed\ngit diff --theirs path/to/file.js\n\n# Show common ancestor (base)\ngit diff --base path/to/file.js\n\n# All three in sequence\ngit show :1:path/to/file.js  # Base\ngit show :2:path/to/file.js  # Ours\ngit show :3:path/to/file.js  # Theirs\n```\n\n## Workflow Examples\n\n### Example 1: Merge Conflict During Pull\n\n**Scenario:**\n```bash\ngit pull origin main\n# Auto-merging src/app.js\n# CONFLICT (content): Merge conflict in src/app.js\n```\n\n**Resolution:**\n```bash\n# 1. Check what's conflicted\ngit status\n\n# 2. View the conflict\ngit diff src/app.js\n\n# 3. Edit file manually\nvim src/app.js\n# Resolve conflicts, remove markers\n\n# 4. Test changes\nnpm test\n\n# 5. Stage resolved file\ngit add src/app.js\n\n# 6. Complete merge\ngit commit  # Or git merge --continue\n\n# 7. Push\ngit push origin feature-branch\n```\n\n### Example 2: Rebase Conflict Resolution\n\n**Scenario:**\n```bash\ngit rebase main\n# CONFLICT (content): Merge conflict in src/api.js\n```\n\n**Resolution:**\n```bash\n# 1. Check status\ngit status\n# rebase in progress; onto abc1234\n\n# 2. View conflict\ngit diff src/api.js\n\n# 3. Resolve conflict\nvim src/api.js\n\n# 4. Stage resolution\ngit add src/api.js\n\n# 5. Continue rebase\ngit rebase --continue\n\n# 6. Repeat for each conflict until done\n\n# 7. Force push (use with caution)\ngit push --force-with-lease origin feature-branch\n```\n\n### Example 3: Cherry-Pick Conflict\n\n**Scenario:**\n```bash\ngit cherry-pick abc1234\n# CONFLICT (content): Merge conflict in src/utils.js\n```\n\n**Resolution:**\n```bash\n# 1. Resolve conflict\nvim src/utils.js\n\n# 2. Stage\ngit add src/utils.js\n\n# 3. Continue cherry-pick\ngit cherry-pick --continue\n\n# Or abort if too complex\ngit cherry-pick --abort\n```\n\n## Advanced Conflict Scenarios\n\n### Multiple File Conflicts\n\n**Handle Systematically:**\n```bash\n# List all conflicts\ngit diff --name-only --diff-filter=U > conflicts.txt\n\n# Process each file\nwhile read file; do\n    echo \"Resolving $file\"\n    # Edit file\n    vim \"$file\"\n    # Stage after manual resolution\n    git add \"$file\"\ndone < conflicts.txt\n\n# Verify all resolved\ngit diff --name-only --diff-filter=U\n# Should be empty\n\n# Continue operation\ngit merge --continue\n```\n\n### Conflict in Binary Files\n\n**Scenario:** Images, PDFs, or compiled files conflict.\n\n```bash\n# Check which version you want\ngit checkout --ours path/to/image.png\n# or\ngit checkout --theirs path/to/image.png\n\n# Stage decision\ngit add path/to/image.png\n```\n\n### Whitespace Conflicts\n\n**Prevent Whitespace Conflicts:**\n```bash\n# During merge\ngit merge -Xignore-space-change feature-branch\n\n# During rebase\ngit rebase -Xignore-space-change main\n\n# Configure globally\ngit config --global merge.renormalize true\n```\n\n## Best Practices\n\n### Before Merging\n\n```bash\n# 1. Ensure working directory is clean\ngit status\n\n# 2. Fetch latest changes\ngit fetch origin\n\n# 3. Review what you're merging\ngit log HEAD..origin/main\n\n# 4. Create backup branch\ngit branch backup-$(date +%Y%m%d-%H%M%S)\n\n# 5. Attempt merge\ngit merge origin/main\n```\n\n### During Conflict Resolution\n\n1. **Understand Both Changes**\n   - Read the code carefully\n   - Understand the intent of each change\n   - Check commit messages for context\n\n2. **Test Thoroughly**\n   - Run tests after resolving\n   - Manual testing of affected features\n   - Lint and format code\n\n3. **Document Complex Resolutions**\n   ```bash\n   git commit -m \"Merge main into feature-branch\n\n   Resolved conflicts in:\n   - src/api.js: Combined pagination and filtering\n   - src/utils.js: Kept new helper functions from both branches\n\n   All tests passing.\"\n   ```\n\n### After Resolution\n\n```bash\n# 1. Verify no conflicts remain\ngit diff --check\n\n# 2. Run full test suite\nnpm test\n\n# 3. Review changes\ngit diff HEAD~1\n\n# 4. Push with confidence\ngit push origin feature-branch\n```\n\n## Common Mistakes to Avoid\n\n### ❌ Don't: Remove Conflict Markers Without Understanding\n\n```javascript\n// BAD: Just deleted markers without reading\nfunction calculate() {\n    return value;\n}\n```\n\n### ✅ Do: Understand and Integrate Both Changes\n\n```javascript\n// GOOD: Combined both improvements\nfunction calculate(items) {\n    const subtotal = items.reduce((sum, item) => sum + item.price, 0);\n    const tax = subtotal * TAX_RATE;\n    return subtotal + tax;\n}\n```\n\n### ❌ Don't: Accept All Ours/Theirs Without Reviewing\n\n```bash\n# BAD: Blindly accepting\ngit checkout --ours .\ngit add .\ngit commit -m \"Resolved conflicts\"\n```\n\n### ✅ Do: Review Each Conflict\n\n```bash\n# GOOD: Selective resolution\ngit diff --name-only --diff-filter=U | while read file; do\n    echo \"Reviewing $file\"\n    git diff \"$file\"\n    # Decide per file\ndone\n```\n\n## Conflict Prevention\n\n### 1. Frequent Integration\n\n```bash\n# Regularly sync with main\ngit fetch origin\ngit merge origin/main\n# or\ngit rebase origin/main\n```\n\n### 2. Small, Focused Commits\n\n```bash\n# Better: Small commits\ngit add src/auth.js\ngit commit -m \"Add login function\"\n\ngit add tests/auth.test.js\ngit commit -m \"Add login tests\"\n```\n\n### 3. Communication\n\n- Coordinate on shared files\n- Use feature flags for large changes\n- Split work across different files/modules\n\n### 4. Code Review\n\n- Review PRs quickly\n- Merge frequently\n- Avoid long-lived branches\n\n## Troubleshooting\n\n### Conflict Resolution Went Wrong\n\n```bash\n# Abort and start over\ngit merge --abort\n# or\ngit rebase --abort\n\n# Check reflog for previous state\ngit reflog\ngit reset --hard HEAD@{1}\n```\n\n### Accidentally Committed Conflict Markers\n\n```bash\n# Check last commit\ngit show\n\n# If conflict markers present\ngit reset HEAD~1\n# Fix files\ngit add .\ngit commit\n```\n\n### Need Help Understanding Conflict\n\n```bash\n# Show commit that introduced change\ngit log --all --full-history -- path/to/file.js\n\n# Show who changed the line\ngit blame path/to/file.js\n\n# See change in context\ngit show <commit-hash>\n```\n\n---\n\n**Remember:** Conflicts are normal. Take time to understand both changes, test thoroughly, and document complex resolutions. When in doubt, consult with the other developer.\n",
        "icartsh-plugin/skills/git-advanced/examples/interactive_rebase.md": "# Interactive Rebase Workflow Guide\n\n## Overview\n\nInteractive rebasing is one of the most powerful features in Git for cleaning up commit history before sharing your work. This guide provides practical examples and workflows for common rebase scenarios.\n\n## Basic Interactive Rebase\n\n### Scenario 1: Squash Multiple \"WIP\" Commits\n\n**Starting History:**\n```\nabc1234 WIP: still working on feature\ndef5678 WIP: almost done\nghi9012 WIP: testing changes\njkl3456 Add user authentication feature\n```\n\n**Goal:** Combine all WIP commits into one clean commit.\n\n**Steps:**\n\n```bash\n# Start interactive rebase for last 4 commits\ngit rebase -i HEAD~4\n\n# Editor opens with:\npick jkl3456 Add user authentication feature\npick ghi9012 WIP: testing changes\npick def5678 WIP: almost done\npick abc1234 WIP: still working on feature\n\n# Change to:\npick jkl3456 Add user authentication feature\nsquash ghi9012 WIP: testing changes\nsquash def5678 WIP: almost done\nsquash abc1234 WIP: still working on feature\n\n# Save and close editor\n# New editor opens for combined commit message\n# Edit to single clean message:\nAdd user authentication feature\n\nImplemented JWT-based authentication with:\n- Login endpoint\n- Token validation middleware\n- Session management\n- Password hashing with bcrypt\n```\n\n**Result:**\n```\nmno7890 Add user authentication feature\n```\n\n### Scenario 2: Reorder Commits Logically\n\n**Starting History:**\n```\naaa1111 Update documentation\nbbb2222 Add tests for feature X\nccc3333 Implement feature X\nddd4444 Fix typo in README\n```\n\n**Goal:** Reorder so feature, tests, and docs are together.\n\n**Steps:**\n\n```bash\ngit rebase -i HEAD~4\n\n# Change order:\npick ccc3333 Implement feature X\npick bbb2222 Add tests for feature X\npick aaa1111 Update documentation\npick ddd4444 Fix typo in README\n```\n\n**Result:**\n```\nccc3333 Implement feature X\nbbb2222 Add tests for feature X\naaa1111 Update documentation\nddd4444 Fix typo in README\n```\n\n### Scenario 3: Edit a Commit in the Middle\n\n**Starting History:**\n```\n111aaaa Add feature A\n222bbbb Add feature B (forgot to add file)\n333cccc Add feature C\n```\n\n**Goal:** Add forgotten file to feature B commit.\n\n**Steps:**\n\n```bash\ngit rebase -i HEAD~3\n\n# Mark commit for editing:\npick 111aaaa Add feature A\nedit 222bbbb Add feature B\npick 333cccc Add feature C\n\n# Git stops at 222bbbb\n# Add the forgotten file\ngit add forgotten-file.js\ngit commit --amend --no-edit\n\n# Continue rebase\ngit rebase --continue\n```\n\n### Scenario 4: Split a Large Commit\n\n**Starting History:**\n```\nxyz9999 Implement user management and email notifications\n```\n\n**Goal:** Split into two focused commits.\n\n**Steps:**\n\n```bash\ngit rebase -i HEAD~1\n\n# Mark for editing:\nedit xyz9999 Implement user management and email notifications\n\n# Git stops at the commit\n# Reset to previous commit but keep changes\ngit reset HEAD~\n\n# Stage and commit user management\ngit add src/user-manager.js tests/user-manager.test.js\ngit commit -m \"Implement user management module\"\n\n# Stage and commit email notifications\ngit add src/email-notifier.js tests/email-notifier.test.js\ngit commit -m \"Add email notification system\"\n\n# Continue rebase\ngit rebase --continue\n```\n\n**Result:**\n```\naaa1111 Implement user management module\nbbb2222 Add email notification system\n```\n\n## Advanced Rebase Techniques\n\n### Using Fixup for Quick Fixes\n\n**Workflow:**\n\n```bash\n# Make initial commit\ngit add feature.js\ngit commit -m \"Add new feature\"\n\n# Continue working, find a typo in feature.js\ngit add feature.js\ngit commit --fixup HEAD  # or commit hash\n\n# Later, before pushing:\ngit rebase -i --autosquash HEAD~2\n# Automatically arranges fixup commits\n```\n\n### Exec Command for Testing\n\n**Ensure all commits pass tests:**\n\n```bash\ngit rebase -i HEAD~5\n\n# Add exec commands:\npick aaa1111 Add feature A\nexec npm test\npick bbb2222 Add feature B\nexec npm test\npick ccc3333 Add feature C\nexec npm test\n\n# Rebase stops if any test fails\n# Fix the issue, amend, and continue\n```\n\n### Rebase with Conflict Resolution\n\n**When conflicts occur during rebase:**\n\n```bash\ngit rebase -i main\n\n# Conflict in some-file.js\n# Output: CONFLICT (content): Merge conflict in some-file.js\n\n# Resolve conflict\nvim some-file.js  # Edit and resolve markers\n\n# Stage resolved file\ngit add some-file.js\n\n# Continue rebase\ngit rebase --continue\n\n# Repeat for any additional conflicts\n\n# If rebase becomes too complex:\ngit rebase --abort  # Start over\n```\n\n## Rebase Strategies by Scenario\n\n### Before Pull Request\n\n**Goal:** Clean history before code review.\n\n```bash\n# Fetch latest main\ngit fetch origin main\n\n# Interactive rebase onto main\ngit rebase -i origin/main\n\n# Common cleanup:\n# 1. Squash WIP/fixup commits\n# 2. Reword unclear messages\n# 3. Reorder related commits\n# 4. Drop debug commits\n```\n\n### After Code Review Feedback\n\n**Goal:** Incorporate review changes cleanly.\n\n```bash\n# Make changes based on feedback\ngit add .\ngit commit --fixup <original-commit-hash>\n\n# Before pushing updates:\ngit rebase -i --autosquash origin/main\n```\n\n### Syncing Feature Branch with Main\n\n**Goal:** Keep feature branch up to date.\n\n```bash\n# Fetch latest main\ngit fetch origin main\n\n# Rebase feature onto main\ngit checkout feature-branch\ngit rebase origin/main\n\n# Resolve any conflicts\n# Force push to update PR\ngit push --force-with-lease origin feature-branch\n```\n\n## Best Practices\n\n### DO:\n- ✅ Rebase local commits before pushing\n- ✅ Create backup branch: `git branch backup`\n- ✅ Use descriptive commit messages after squashing\n- ✅ Test after rebasing\n- ✅ Use `--force-with-lease` instead of `--force`\n\n### DON'T:\n- ❌ Rebase commits already pushed to shared branches\n- ❌ Rebase main/master branches\n- ❌ Rebase without backing up\n- ❌ Force push to branches others are working on\n- ❌ Rebase if you don't understand the consequences\n\n## Troubleshooting\n\n### Rebase Conflicts Are Too Complex\n\n```bash\n# Abort and try merge instead\ngit rebase --abort\ngit merge main\n```\n\n### Made a Mistake During Rebase\n\n```bash\n# Check reflog\ngit reflog\n\n# Find state before rebase started\n# Example: abc1234 HEAD@{5}: rebase -i (start)\n\n# Reset to that state\ngit reset --hard HEAD@{5}\n```\n\n### Want to Undo Last Rebase\n\n```bash\n# Immediately after rebase\ngit reflog\n\n# Find ORIG_HEAD\ngit reset --hard ORIG_HEAD\n```\n\n### Rebase in Progress, Not Sure How to Continue\n\n```bash\n# Check status\ngit status\n\n# Options:\ngit rebase --continue  # After resolving conflicts\ngit rebase --skip      # Skip current commit\ngit rebase --abort     # Cancel rebase\n```\n\n## Rebase Commands Quick Reference\n\n### Rebase Actions:\n- `pick` (p) - Use commit as-is\n- `reword` (r) - Use commit, edit message\n- `edit` (e) - Use commit, stop for amending\n- `squash` (s) - Combine with previous, keep both messages\n- `fixup` (f) - Combine with previous, discard this message\n- `drop` (d) - Remove commit\n- `exec` (x) - Run shell command\n\n### Useful Flags:\n- `-i` - Interactive mode\n- `--autosquash` - Auto-arrange fixup/squash commits\n- `--continue` - Continue after resolving conflicts\n- `--skip` - Skip current commit\n- `--abort` - Cancel rebase\n- `--onto` - Rebase onto different base\n\n## Example: Complete Feature Branch Cleanup\n\n**Starting point:**\n```bash\ngit log --oneline\nabc1234 Fix linting errors\ndef5678 WIP: debugging\nghi9012 Add tests\njkl3456 WIP: implement feature\nmno7890 Initial feature structure\npqr1234 Update README\n```\n\n**Cleanup process:**\n\n```bash\n# 1. Create backup\ngit branch backup-feature\n\n# 2. Start interactive rebase\ngit rebase -i HEAD~6\n\n# 3. Reorganize:\npick mno7890 Initial feature structure\nsquash jkl3456 WIP: implement feature\npick ghi9012 Add tests\ndrop def5678 WIP: debugging\nfixup abc1234 Fix linting errors\npick pqr1234 Update README\n\n# 4. Edit commit messages when prompted\n\n# 5. Verify result\ngit log --oneline\n# Result:\nzzz9999 Implement user authentication feature\nyyy8888 Add comprehensive tests for authentication\nxxx7777 Update README with authentication docs\n\n# 6. Force push (if already on remote)\ngit push --force-with-lease origin feature-branch\n```\n\n---\n\n**Remember:** Interactive rebase is a powerful tool. Use it to create clean, logical commit history that tells a clear story of your changes.\n",
        "icartsh-plugin/skills/git-advanced/references/best-practices.md": "# Git Advanced Operations Best Practices\n\n## Overview\n\nFollow these best practices to maintain clean Git history, avoid common pitfalls, and work effectively with advanced Git operations.\n\n## Before Rebase\n\n### Pre-Rebase Checklist\n\n```bash\n# 1. Create backup branch\ngit branch backup-$(date +%Y%m%d-%H%M%S)\n\n# 2. Ensure working directory is clean\ngit status\n# Should show: nothing to commit, working tree clean\n\n# 3. Know your commit history\ngit log --oneline --graph -10\n\n# 4. Verify you're not on a shared branch\ngit branch -vv\n```\n\n### Safety Measures\n\n- **Never rebase public/shared branches** - Only rebase local commits\n- **Check for remote tracking** - Ensure branch isn't being used by others\n- **Use backup branches** - Quick recovery if something goes wrong\n- **Understand the commits** - Know what you're rebasing\n\n```bash\n# Good: Rebasing local feature branch\ngit checkout feature/my-work\ngit rebase main\n\n# Bad: Rebasing shared branch\ngit checkout develop\ngit rebase main  # Don't do this if others are using develop!\n```\n\n## During Conflicts\n\n### Conflict Resolution Guidelines\n\n1. **Understand both sides of the conflict**\n   ```bash\n   # View what you changed\n   git diff --ours path/to/file.js\n\n   # View what they changed\n   git diff --theirs path/to/file.js\n\n   # View common ancestor\n   git diff --base path/to/file.js\n   ```\n\n2. **Test thoroughly after resolution**\n   ```bash\n   # Run tests\n   npm test\n\n   # Run linter\n   npm run lint\n\n   # Manual testing\n   npm start\n   ```\n\n3. **Use meaningful merge commit messages**\n   ```bash\n   git commit -m \"Merge main into feature-branch\n\n   Resolved conflicts in:\n   - src/api.js: Combined pagination and filtering features\n   - src/utils.js: Kept new helper functions from both branches\n   - tests/api.test.js: Merged test suites\n\n   All tests passing.\"\n   ```\n\n4. **Document complex resolutions**\n   - Add comments explaining non-obvious choices\n   - Reference related issues or discussions\n   - Explain why you chose one approach over another\n\n### Conflict Prevention\n\n```bash\n# Sync frequently with base branch\ngit fetch origin\ngit rebase origin/main  # or merge\n\n# Commit small, focused changes\ngit add specific-file.js\ngit commit -m \"Focused change description\"\n\n# Communicate with team\n# - Coordinate on shared files\n# - Review PRs quickly\n# - Merge frequently\n```\n\n## Branch Management\n\n### Branch Lifecycle\n\n```bash\n# 1. Create from up-to-date base\ngit checkout main\ngit pull origin main\ngit checkout -b feature/new-feature\n\n# 2. Work and commit regularly\ngit add .\ngit commit -m \"Clear, descriptive message\"\n\n# 3. Sync with base branch regularly\ngit fetch origin\ngit rebase origin/main\n\n# 4. Clean up before merging\ngit rebase -i origin/main  # Squash, reorder, cleanup\n\n# 5. Merge promptly after approval\n# (via PR or command line)\n\n# 6. Delete branch immediately after merge\ngit checkout main\ngit pull origin main\ngit branch -d feature/new-feature\ngit push origin --delete feature/new-feature\n```\n\n### Branch Naming Best Practices\n\n```bash\n# Use descriptive, categorized names\nfeature/user-authentication\nfeature/payment-integration\nbugfix/login-redirect\nfix/memory-leak\nhotfix/security-patch\nrelease/v1.2.0\nexperiment/new-algorithm\n\n# Include ticket numbers when relevant\nfeature/JIRA-123-add-export\nbugfix/GH-456-fix-crash\n\n# Avoid generic names\ntemp\ntest\nfix\nmy-branch\n```\n\n### Keep Branches Focused and Short-Lived\n\n- **Focused**: One feature or bug fix per branch\n- **Short-lived**: Merge within days, not weeks\n- **Synced**: Regularly update from base branch\n- **Clean**: Squash/cleanup before merging\n\n## History Hygiene\n\n### Commit Quality Standards\n\n```bash\n# Write clear, descriptive commit messages\ngit commit -m \"Add user authentication with JWT tokens\n\nImplemented:\n- Login endpoint with email/password\n- JWT token generation and validation\n- Refresh token mechanism\n- Password hashing with bcrypt\n\nTests added for all authentication flows.\"\n\n# Use conventional commit format\ngit commit -m \"feat: add user authentication\"\ngit commit -m \"fix: resolve memory leak in cache\"\ngit commit -m \"docs: update API documentation\"\ngit commit -m \"refactor: simplify data loading logic\"\ngit commit -m \"test: add unit tests for payment module\"\n```\n\n### Atomic Commits\n\n```bash\n# Good: Atomic commits (one logical change each)\ngit add src/auth.js\ngit commit -m \"Add login function\"\n\ngit add tests/auth.test.js\ngit commit -m \"Add login tests\"\n\ngit add docs/auth.md\ngit commit -m \"Document authentication API\"\n\n# Bad: Large, unfocused commit\ngit add .\ngit commit -m \"Add auth and fix bugs and update docs\"\n```\n\n### Pre-Push Cleanup\n\n```bash\n# Before pushing, clean up your commits\ngit rebase -i origin/main\n\n# Squash \"fixup\" commits\npick abc1234 Add feature\nfixup def5678 Fix typo\nfixup ghi9012 Fix linting\n\n# Reword unclear messages\nreword jkl3456 WIP commit  # Change to descriptive message\n\n# Drop debug commits\ndrop mno7890 Add console.log debugging\n```\n\n## Force Push Safety\n\n### When to Force Push\n\n✅ **Safe scenarios:**\n- Your own feature branch\n- After rebasing local commits\n- Cleaning up commit history before merge\n- Branch is not being used by others\n\n❌ **Never force push to:**\n- main/master branch\n- develop branch\n- Branches others are working on\n- Already released commits\n\n### Force Push Best Practices\n\n```bash\n# Use --force-with-lease instead of --force\ngit push --force-with-lease origin feature-branch\n\n# What --force-with-lease does:\n# - Checks if remote branch has unexpected commits\n# - Prevents overwriting others' work\n# - Fails safely if remote changed\n\n# Verify what you're pushing\ngit log origin/feature-branch..HEAD\ngit diff origin/feature-branch\n\n# Communicate before force pushing\n# - Notify team members\n# - Ensure no one is working on the branch\n# - Coordinate timing\n```\n\n## Code Review Integration\n\n### Preparing for Review\n\n```bash\n# 1. Clean up commit history\ngit rebase -i origin/main\n\n# 2. Ensure tests pass\nnpm test\n\n# 3. Run linting\nnpm run lint\n\n# 4. Self-review changes\ngit diff origin/main\n\n# 5. Write detailed PR description\n```\n\n### Addressing Review Feedback\n\n```bash\n# Option 1: Fixup commits (then squash before merge)\ngit add src/auth.js\ngit commit --fixup abc1234\n\n# Later, before final merge\ngit rebase -i --autosquash origin/main\n\n# Option 2: Regular commits (if history preservation matters)\ngit add src/auth.js\ngit commit -m \"Address review feedback: improve error handling\"\n\n# Option 3: Amend last commit (if feedback is immediate)\ngit add src/auth.js\ngit commit --amend --no-edit\ngit push --force-with-lease origin feature-branch\n```\n\n## Collaboration Patterns\n\n### Working on Shared Branch\n\n```bash\n# Pull before starting work\ngit pull --rebase origin feature-branch\n\n# Push frequently\ngit push origin feature-branch\n\n# Communicate changes\n# - Announce major refactors\n# - Coordinate on conflicts\n# - Use feature flags for breaking changes\n```\n\n### Reviewing Others' Work\n\n```bash\n# Fetch and checkout PR branch\ngit fetch origin pull/123/head:pr-123\ngit checkout pr-123\n\n# Test locally\nnpm install\nnpm test\n\n# Review changes\ngit diff main..pr-123\ngit log main..pr-123\n\n# Leave inline comments on GitHub/GitLab\n```\n\n## Emergency Procedures\n\n### Undo Last Operation\n\n```bash\n# Check reflog first\ngit reflog\n\n# Reset to previous state\ngit reset --hard HEAD@{1}\n\n# Or abort ongoing operation\ngit rebase --abort\ngit merge --abort\ngit cherry-pick --abort\n```\n\n### Recover Lost Work\n\n```bash\n# Find lost commits\ngit fsck --lost-found\ngit reflog\n\n# Recover specific commit\ngit checkout -b recovery <commit-hash>\n\n# Or cherry-pick\ngit cherry-pick <commit-hash>\n```\n\n### Fix Broken Branch\n\n```bash\n# If branch is in bad state\n# Option 1: Reset to known good state\ngit reset --hard origin/main\n\n# Option 2: Start fresh\ngit checkout main\ngit checkout -b feature/new-attempt\n# Cherry-pick good commits from broken branch\ngit cherry-pick <good-commit-1>\ngit cherry-pick <good-commit-2>\n```\n\n## Performance Optimization\n\n### Repository Maintenance\n\n```bash\n# Clean up repository\ngit gc --aggressive --prune=now\n\n# Remove unreachable objects\ngit prune\n\n# Optimize pack files\ngit repack -a -d --depth=250 --window=250\n\n# Verify repository integrity\ngit fsck --full\n```\n\n### Large Repository Handling\n\n```bash\n# Shallow clone for large repos\ngit clone --depth 1 https://github.com/user/repo.git\n\n# Fetch only specific branches\ngit clone --single-branch --branch main https://github.com/user/repo.git\n\n# Use sparse checkout\ngit sparse-checkout init\ngit sparse-checkout set src/ tests/\n```\n\n## Quality Checklist\n\n### Before Committing\n\n- [ ] Code compiles/runs without errors\n- [ ] Tests pass locally\n- [ ] Linting passes\n- [ ] Commit message is clear and descriptive\n- [ ] Commit is focused (one logical change)\n- [ ] No debug code or console.logs\n- [ ] No commented-out code\n\n### Before Pushing\n\n- [ ] Commits are clean and logical\n- [ ] No sensitive data in commits\n- [ ] Branch is up to date with base\n- [ ] All tests pass\n- [ ] Self-reviewed changes\n- [ ] Ready for code review\n\n### Before Merging\n\n- [ ] PR approved by required reviewers\n- [ ] All CI checks pass\n- [ ] No merge conflicts\n- [ ] Documentation updated\n- [ ] Changelog updated (if applicable)\n- [ ] Version bumped (if applicable)\n\n---\n\n**Remember:** Good Git hygiene prevents problems. Invest time in clean commits and clear history - your future self and teammates will thank you.\n",
        "icartsh-plugin/skills/git-advanced/references/branch-management.md": "# Branch Cleanup and Management\n\n## Overview\n\nEffective branch management prevents repository clutter and maintains a clean, navigable history. This guide covers both automated and manual approaches to branch cleanup.\n\n## Using Helper Script\n\n```bash\n# Run branch cleanup helper\nbash scripts/git_helper.sh cleanup-branches\n```\n\n## Manual Cleanup\n\n### List All Branches\n\n```bash\n# List all branches\ngit branch -a\n\n# List merged branches\ngit branch --merged main\n\n# Delete merged local branches\ngit branch -d feature/completed\n\n# Force delete unmerged branch\ngit branch -D feature/abandoned\n\n# Delete remote branch\ngit push origin --delete feature/old\n\n# Prune deleted remote branches\ngit fetch --prune\n\n# Remove all merged branches (except main/develop)\ngit branch --merged | grep -v \"\\*\\|main\\|develop\" | xargs -n 1 git branch -d\n```\n\n## Stale Branch Detection\n\n### Show Branches with Last Commit Date\n\n```bash\nfor branch in $(git branch -r | grep -v HEAD); do\n    echo -e \"$(git show --format=\"%ci %cr %an\" $branch | head -n 1)\\t$branch\"\ndone | sort -r\n```\n\n### Archive Old Branches as Tags\n\n```bash\n# Archive old branches as tags\ngit tag archive/feature-x feature/feature-x\ngit branch -D feature/feature-x\ngit push origin --delete feature/feature-x\n```\n\n## Branch Cleanup Strategies\n\n### By Merge Status\n\n```bash\n# List branches merged into main\ngit branch --merged main\n\n# Delete all merged branches\ngit branch --merged main | grep -v \"^\\*\\|main\\|develop\" | xargs git branch -d\n```\n\n### By Age\n\n```bash\n# Find branches older than 30 days\ngit for-each-ref --sort=-committerdate refs/heads/ \\\n    --format='%(committerdate:short) %(refname:short)' | \\\n    awk -v date=\"$(date -d '30 days ago' +%Y-%m-%d)\" '$1 < date {print $2}'\n```\n\n### By Author\n\n```bash\n# List your branches\ngit branch -r --format='%(authorname) %(refname:short)' | grep \"Your Name\"\n\n# Delete your old feature branches\ngit branch | grep \"feature/your-\" | xargs git branch -d\n```\n\n## Automation Scripts\n\n### Weekly Cleanup Script\n\n```bash\n#!/bin/bash\n# weekly-branch-cleanup.sh\n\necho \"Starting weekly branch cleanup...\"\n\n# Fetch latest\ngit fetch --prune\n\n# List merged branches\necho \"Merged branches:\"\ngit branch --merged main | grep -v \"^\\*\\|main\\|develop\"\n\n# Prompt for confirmation\nread -p \"Delete these branches? (y/n) \" -n 1 -r\necho\nif [[ $REPLY =~ ^[Yy]$ ]]; then\n    git branch --merged main | grep -v \"^\\*\\|main\\|develop\" | xargs git branch -d\n    echo \"Cleanup complete\"\nelse\n    echo \"Cleanup cancelled\"\nfi\n```\n\n### Archive Old Branches\n\n```bash\n#!/bin/bash\n# archive-old-branches.sh\n\n# Archive branches older than 90 days\ngit for-each-ref --sort=-committerdate refs/heads/ \\\n    --format='%(committerdate:short) %(refname:short)' | \\\n    awk -v date=\"$(date -d '90 days ago' +%Y-%m-%d)\" '$1 < date {print $2}' | \\\nwhile read branch; do\n    echo \"Archiving $branch\"\n    git tag archive/$branch $branch\n    git branch -D $branch\ndone\n\n# Push archive tags\ngit push origin --tags\n```\n\n## Best Practices\n\n### Regular Maintenance\n\n- Clean up merged branches immediately after merge\n- Archive old branches as tags before deletion\n- Use descriptive branch names for easier identification\n- Coordinate with team before deleting shared branches\n\n### Protection Rules\n\n- Protect main/master branches from deletion\n- Protect active development branches (develop, staging)\n- Protect release branches until officially deprecated\n- Archive instead of delete for audit trail\n\n### Naming Conventions\n\n```bash\n# Good branch names\nfeature/user-authentication\nbugfix/login-redirect\nhotfix/security-patch-v1.2.1\nrelease/v2.0.0\nexperiment/new-search-algorithm\n\n# Bad branch names\ntemp\nfix\ntest-branch\njohns-branch\n```\n\n## Recovery\n\n### Restore Deleted Branch\n\n```bash\n# Find deleted branch in reflog\ngit reflog | grep \"branch-name\"\n\n# Restore from commit hash\ngit checkout -b branch-name <commit-hash>\n```\n\n### Restore Deleted Remote Branch\n\n```bash\n# Find branch in old remote tracking\ngit branch -r --contains <commit-hash>\n\n# Recreate and push\ngit checkout -b branch-name <commit-hash>\ngit push -u origin branch-name\n```\n\n---\n\n**Remember:** Regular branch cleanup keeps your repository navigable and prevents accumulation of stale branches. Archive important history as tags before deletion.\n",
        "icartsh-plugin/skills/git-advanced/references/reflog-recovery.md": "# Reflog Recovery and History Rewriting\n\n## Overview\n\nGit reflog is your safety net for recovering lost commits and undoing mistakes. This guide covers recovery techniques and safe history rewriting practices.\n\n## Reflog Recovery\n\n### Understanding Reflog\n\nThe reflog records every change to HEAD and branch tips for 30-90 days (configurable). It's your time machine for Git operations.\n\n```bash\n# View reflog history\ngit reflog\n\n# View reflog for specific branch\ngit reflog show branch-name\n\n# View reflog with dates\ngit reflog --date=iso\n```\n\n### Recover Deleted Branch\n\n**Scenario:** Accidentally deleted a branch.\n\n```bash\n# View reflog\ngit reflog\n\n# Find commit where branch was deleted\n# Output shows: abc1234 HEAD@{5}: checkout: moving from feature-x to main\n\n# Restore branch\ngit checkout -b feature-x abc1234\n```\n\n### Undo Bad Reset\n\n**Scenario:** Ran `git reset --hard HEAD~5` and lost commits.\n\n```bash\n# Accidentally ran: git reset --hard HEAD~5\n\n# View reflog\ngit reflog\n\n# Find state before reset\n# Output: def5678 HEAD@{1}: reset: moving to HEAD~5\n\n# Restore previous state\ngit reset --hard HEAD@{1}\n```\n\n**Alternative approach:**\n\n```bash\n# Find exact commit before reset\ngit reflog | grep \"reset\"\n\n# Reset to specific reflog entry\ngit reset --hard HEAD@{3}\n```\n\n### Recover Lost Commits\n\n**Scenario:** Made commits on detached HEAD and lost them.\n\n```bash\n# Find dangling commits\ngit fsck --lost-found\n\n# Or use reflog\ngit reflog show --all\n\n# View commit content\ngit show <lost-commit-hash>\n\n# Cherry-pick recovered commit\ngit cherry-pick <lost-commit-hash>\n\n# Or create new branch from lost commit\ngit checkout -b recovered-work <lost-commit-hash>\n```\n\n### Recover Dropped Stash\n\n**Scenario:** Accidentally dropped a stash.\n\n```bash\n# Find dropped stashes\ngit fsck --unreachable | grep commit\n\n# View stash content\ngit show <commit-hash>\n\n# Apply recovered stash\ngit stash apply <commit-hash>\n\n# Or use reflog\ngit reflog show stash\ngit stash apply stash@{5}\n```\n\n## Safe History Rewriting\n\n### Amend Last Commit\n\n**Change Last Commit Message:**\n\n```bash\n# Change last commit message\ngit commit --amend -m \"New message\"\n```\n\n**Add Forgotten File:**\n\n```bash\n# Add forgotten file to last commit\ngit add forgotten-file.txt\ngit commit --amend --no-edit\n```\n\n**Change Author:**\n\n```bash\n# Change author of last commit\ngit commit --amend --author=\"Name <email@example.com>\"\n```\n\n### Filter Branch Operations\n\n**Remove File from Entire History:**\n\n```bash\n# Remove file from entire history\ngit filter-branch --tree-filter 'rm -f passwords.txt' HEAD\n\n# More efficient with index-filter\ngit filter-branch --index-filter 'git rm --cached --ignore-unmatch passwords.txt' HEAD\n```\n\n**Change Author in History:**\n\n```bash\ngit filter-branch --env-filter '\nif [ \"$GIT_COMMITTER_EMAIL\" = \"old@email.com\" ]; then\n    export GIT_COMMITTER_NAME=\"New Name\"\n    export GIT_COMMITTER_EMAIL=\"new@email.com\"\n    export GIT_AUTHOR_NAME=\"New Name\"\n    export GIT_AUTHOR_EMAIL=\"new@email.com\"\nfi\n' -- --all\n```\n\n### BFG Repo-Cleaner (Better Alternative)\n\n**Remove Sensitive Data:**\n\n```bash\n# Download BFG Repo-Cleaner\n# https://rtyley.github.io/bfg-repo-cleaner/\n\n# Remove specific file\njava -jar bfg.jar --delete-files passwords.txt\n\n# Remove files larger than 100M\njava -jar bfg.jar --strip-blobs-bigger-than 100M\n\n# Replace sensitive strings\njava -jar bfg.jar --replace-text passwords.txt\n\n# Clean up\ngit reflog expire --expire=now --all\ngit gc --prune=now --aggressive\n```\n\n**Example: Remove API Keys:**\n\n```bash\n# Create replacement file (replacements.txt)\n# API_KEY=.*  ==> API_KEY=REMOVED\n\njava -jar bfg.jar --replace-text replacements.txt\ngit reflog expire --expire=now --all\ngit gc --prune=now --aggressive\ngit push --force\n```\n\n## Advanced Recovery Scenarios\n\n### Recover After Force Push\n\n**Scenario:** Someone force-pushed and overwrote your work.\n\n```bash\n# Check reflog of remote tracking branch\ngit reflog show origin/main\n\n# Find commit before force push\n# Reset local branch to that commit\ngit reset --hard origin/main@{1}\n\n# Create backup branch\ngit branch backup-recovery\n\n# Push to new branch\ngit push origin backup-recovery\n```\n\n### Recover Entire Repository State\n\n**Scenario:** Need to restore repository to exact state from 2 days ago.\n\n```bash\n# Find reflog entry from 2 days ago\ngit reflog --date=iso | grep \"2 days ago\"\n\n# Reset to that state\ngit reset --hard HEAD@{yesterday}\n\n# Or specific reflog entry\ngit reset --hard HEAD@{10}\n```\n\n### Recover from Corrupted Repository\n\n```bash\n# Check repository integrity\ngit fsck --full\n\n# Attempt recovery\ngit fsck --lost-found\n\n# Restore from backup (if available)\n# Or clone fresh and apply recent work\n```\n\n## History Rewriting Best Practices\n\n### Before Rewriting\n\n1. **Create backup branch:**\n   ```bash\n   git branch backup-before-rewrite\n   ```\n\n2. **Ensure working directory is clean:**\n   ```bash\n   git status\n   ```\n\n3. **Know your commit history:**\n   ```bash\n   git log --oneline --graph --all\n   ```\n\n### During Rewriting\n\n1. **Never rewrite public/shared branches**\n2. **Use --force-with-lease instead of --force:**\n   ```bash\n   git push --force-with-lease origin feature-branch\n   ```\n\n3. **Test thoroughly after rewriting:**\n   ```bash\n   npm test\n   git log --oneline\n   git diff main\n   ```\n\n### After Rewriting\n\n1. **Communicate with team about force push**\n2. **Verify commit signatures (if using GPG)**\n3. **Check CI/CD pipeline status**\n4. **Monitor for issues**\n\n## Reflog Configuration\n\n### Extend Reflog Retention\n\n```bash\n# Keep reflog entries for 180 days (default is 90)\ngit config gc.reflogExpire 180\n\n# Keep unreachable reflog entries for 60 days (default is 30)\ngit config gc.reflogExpireUnreachable 60\n\n# Apply globally\ngit config --global gc.reflogExpire 180\ngit config --global gc.reflogExpireUnreachable 60\n```\n\n### Disable Garbage Collection Temporarily\n\n```bash\n# Disable automatic gc\ngit config gc.auto 0\n\n# Run manual gc when needed\ngit gc --prune=now\n```\n\n## Emergency Recovery Commands\n\n```bash\n# Abort ongoing operations\ngit merge --abort\ngit rebase --abort\ngit cherry-pick --abort\n\n# View reflog\ngit reflog\n\n# Reset to previous state\ngit reset --hard HEAD@{1}\n\n# Find lost commits\ngit fsck --lost-found\n\n# Recover specific commit\ngit checkout -b recovery <commit-hash>\n\n# Create patch from lost commit\ngit format-patch -1 <commit-hash>\ngit apply patch-file.patch\n```\n\n## Common Recovery Scenarios\n\n### Scenario 1: Committed to Wrong Branch\n\n```bash\n# Move commit to correct branch\ngit checkout correct-branch\ngit cherry-pick wrong-branch\n\n# Remove from wrong branch\ngit checkout wrong-branch\ngit reset --hard HEAD~1\n```\n\n### Scenario 2: Need to Split Recent Commits\n\n```bash\n# Reset but keep changes\ngit reset HEAD~3\n\n# Stage and commit selectively\ngit add file1.js\ngit commit -m \"Feature A\"\n\ngit add file2.js\ngit commit -m \"Feature B\"\n```\n\n### Scenario 3: Merge Gone Wrong\n\n```bash\n# Abort if still in progress\ngit merge --abort\n\n# Or reset if already committed\ngit reflog\ngit reset --hard HEAD@{1}\n```\n\n---\n\n**Remember:** Reflog is temporary (30-90 days). For long-term backup, use branches and tags. Always create a backup branch before risky operations.\n",
        "icartsh-plugin/skills/git-advanced/references/troubleshooting.md": "# Git Advanced Operations Troubleshooting\n\n## Overview\n\nCommon issues and solutions for advanced Git operations. This guide helps you quickly resolve problems with rebasing, merging, conflicts, and more.\n\n## Rebase Issues\n\n### Rebase Conflicts Too Complex\n\n**Problem:** Multiple conflicts during rebase, becoming unmanageable.\n\n**Solution:**\n\n```bash\n# Abort rebase\ngit rebase --abort\n\n# Try merge instead (preserves history differently)\ngit merge main\n\n# Or try interactive rebase with smaller chunks\ngit rebase -i HEAD~5  # Start with fewer commits\n```\n\n**Prevention:**\n\n```bash\n# Rebase more frequently to avoid large divergence\ngit fetch origin\ngit rebase origin/main  # Do this daily\n\n# Keep feature branches short-lived\n# Merge within a few days, not weeks\n```\n\n### Rebase Stops at Every Commit\n\n**Problem:** Rebase requires manual intervention for many commits.\n\n**Solution:**\n\n```bash\n# Use rerere (reuse recorded resolution)\ngit config --global rerere.enabled true\n\n# Git will remember conflict resolutions\n# and apply them automatically in subsequent commits\n\n# Or use merge strategy during rebase\ngit rebase -X ours origin/main  # Prefer your changes\ngit rebase -X theirs origin/main  # Prefer their changes\n```\n\n### Can't Continue Rebase\n\n**Problem:** `git rebase --continue` says nothing to commit.\n\n**Solution:**\n\n```bash\n# The conflict was already resolved, skip this commit\ngit rebase --skip\n\n# Or check if changes need staging\ngit status\ngit add <resolved-files>\ngit rebase --continue\n```\n\n### Detached HEAD After Rebase\n\n**Problem:** Ended up in detached HEAD state.\n\n**Solution:**\n\n```bash\n# Create branch from current state\ngit checkout -b recovered-branch\n\n# Or return to original branch\ngit checkout feature-branch\n\n# Check reflog to understand what happened\ngit reflog\n```\n\n## Merge Conflicts\n\n### Conflict Markers Left in Code\n\n**Problem:** Accidentally committed conflict markers.\n\n**Solution:**\n\n```bash\n# Check last commit\ngit show\n\n# If conflict markers present\ngit reset HEAD~1\n\n# Fix files (remove markers)\nvim src/file.js\n\n# Re-commit\ngit add src/file.js\ngit commit -m \"Fix: Remove conflict markers\"\n```\n\n**Prevention:**\n\n```bash\n# Search for conflict markers before committing\ngit diff --check\n\n# Or use pre-commit hook\n# .git/hooks/pre-commit\n#!/bin/sh\nif git diff --cached | grep -q '^<<<<<<<'; then\n    echo \"Error: Conflict markers found\"\n    exit 1\nfi\n```\n\n### Merge Went Wrong\n\n**Problem:** Merged but result is broken.\n\n**Solution:**\n\n```bash\n# If merge not committed yet\ngit merge --abort\n\n# If already committed\ngit reflog\ngit reset --hard HEAD@{1}  # State before merge\n\n# Or revert the merge commit\ngit revert -m 1 <merge-commit-hash>\n```\n\n### Binary File Conflict\n\n**Problem:** Can't merge binary files (images, PDFs).\n\n**Solution:**\n\n```bash\n# Choose one version\ngit checkout --ours path/to/image.png\n# or\ngit checkout --theirs path/to/image.png\n\n# Stage decision\ngit add path/to/image.png\n\n# Continue merge\ngit merge --continue\n```\n\n## Lost Commits\n\n### Can't Find Recent Commits\n\n**Problem:** Commits disappeared after reset or rebase.\n\n**Solution:**\n\n```bash\n# Check reflog\ngit reflog\n\n# Find lost commits\n# Look for: abc1234 HEAD@{5}: commit: Your message\n\n# Recover commit\ngit checkout -b recovery abc1234\n\n# Or cherry-pick it\ngit cherry-pick abc1234\n```\n\n### Deleted Branch with Important Work\n\n**Problem:** Deleted branch before merging.\n\n**Solution:**\n\n```bash\n# Find branch in reflog\ngit reflog | grep \"branch-name\"\n\n# Look for: checkout: moving from branch-name to main\n\n# Get commit hash before deletion\ngit reflog show --all | grep \"branch-name\"\n\n# Recreate branch\ngit checkout -b branch-name <commit-hash>\n```\n\n### Lost Stash\n\n**Problem:** Dropped stash accidentally.\n\n**Solution:**\n\n```bash\n# Find dropped stashes\ngit fsck --unreachable | grep commit\n\n# View stash content\ngit show <commit-hash>\n\n# Apply if it's the right one\ngit stash apply <commit-hash>\n\n# Or check stash reflog\ngit reflog show stash\ngit stash apply stash@{5}\n```\n\n## Force Push Problems\n\n### Force Push Rejected\n\n**Problem:** `git push --force-with-lease` rejected.\n\n**Solution:**\n\n```bash\n# Check what changed on remote\ngit fetch origin\ngit log HEAD..origin/feature-branch\n\n# If remote has new commits from others\n# Don't force push! Merge or rebase first\ngit rebase origin/feature-branch\n\n# If you're certain, use regular force push (risky)\ngit push --force origin feature-branch\n```\n\n### Overwrote Someone's Work\n\n**Problem:** Force pushed and lost colleague's commits.\n\n**Solution:**\n\n```bash\n# Contact colleague to get their commit hashes\n# Or check reflog of remote tracking branch\ngit reflog show origin/feature-branch\n\n# Reset to before your force push\ngit reset --hard origin/feature-branch@{1}\n\n# Cherry-pick your commits on top\ngit cherry-pick <your-commit-1>\ngit cherry-pick <your-commit-2>\n\n# Push normally\ngit push origin feature-branch\n```\n\n## Cherry-Pick Issues\n\n### Cherry-Pick Conflict\n\n**Problem:** Conflict when cherry-picking.\n\n**Solution:**\n\n```bash\n# Resolve conflict manually\nvim conflicted-file.js\n\n# Stage resolved file\ngit add conflicted-file.js\n\n# Continue cherry-pick\ngit cherry-pick --continue\n\n# Or skip this commit\ngit cherry-pick --skip\n\n# Or abort\ngit cherry-pick --abort\n```\n\n### Cherry-Picked Wrong Commit\n\n**Problem:** Picked wrong commit.\n\n**Solution:**\n\n```bash\n# Undo last commit (cherry-picked one)\ngit reset --hard HEAD~1\n\n# Cherry-pick correct commit\ngit cherry-pick <correct-commit-hash>\n```\n\n## Bisect Problems\n\n### Bisect Can't Find Bad Commit\n\n**Problem:** Bisect reports all commits as bad or good.\n\n**Solution:**\n\n```bash\n# Reset bisect\ngit bisect reset\n\n# Start over with correct good/bad markers\ngit bisect start\ngit bisect bad HEAD\ngit bisect good <earlier-commit>\n\n# Or check if issue is environmental\n# Ensure you're testing correctly at each step\n```\n\n### Bisect Test Script Fails\n\n**Problem:** Automated bisect script has errors.\n\n**Solution:**\n\n```bash\n# Make script more robust\n#!/bin/bash\nset -e  # Exit on error\n\n# Setup environment\nnpm install 2>/dev/null || true\n\n# Run test\nnpm test\n\n# Return appropriate exit code\nif [ $? -eq 0 ]; then\n    exit 0  # Good commit\nelse\n    exit 1  # Bad commit\nfi\n```\n\n## Submodule Issues\n\n### Submodule in Detached HEAD\n\n**Problem:** Submodule stuck in detached HEAD state.\n\n**Solution:**\n\n```bash\n# Navigate to submodule\ncd path/to/submodule\n\n# Checkout a branch\ngit checkout main\n\n# Or update submodule to tracked commit\ncd ..\ngit submodule update --init --recursive\n```\n\n### Submodule Update Conflicts\n\n**Problem:** Conflicts when updating submodules.\n\n**Solution:**\n\n```bash\n# Resolve submodule pointer conflict\ngit checkout --ours path/to/submodule  # Use your version\n# or\ngit checkout --theirs path/to/submodule  # Use their version\n\n# Update submodule\ncd path/to/submodule\ngit checkout <commit-hash>\n\n# Stage resolution\ncd ..\ngit add path/to/submodule\ngit merge --continue\n```\n\n## Large File Problems\n\n### Accidentally Committed Large File\n\n**Problem:** Committed large file, now repo is huge.\n\n**Solution:**\n\n```bash\n# Remove file from last commit (not pushed yet)\ngit rm --cached large-file.bin\ngit commit --amend --no-edit\n\n# If already pushed, use BFG Repo-Cleaner\njava -jar bfg.jar --strip-blobs-bigger-than 100M\ngit reflog expire --expire=now --all\ngit gc --prune=now --aggressive\ngit push --force\n```\n\n### Repository Too Large\n\n**Problem:** Clone/fetch is very slow due to size.\n\n**Solution:**\n\n```bash\n# Shallow clone\ngit clone --depth 1 https://github.com/user/repo.git\n\n# Or partial clone (Git 2.22+)\ngit clone --filter=blob:none https://github.com/user/repo.git\n\n# Clean up existing repo\ngit gc --aggressive --prune=now\ngit repack -a -d --depth=250 --window=250\n```\n\n## Performance Issues\n\n### Git Commands Are Slow\n\n**Problem:** All Git operations taking too long.\n\n**Solution:**\n\n```bash\n# Run garbage collection\ngit gc --aggressive\n\n# Remove unreachable objects\ngit prune\n\n# Check for repository corruption\ngit fsck --full\n\n# Optimize configuration\ngit config core.preloadindex true\ngit config core.fscache true\ngit config gc.auto 256\n```\n\n### Diff Takes Forever\n\n**Problem:** `git diff` or `git log` is very slow.\n\n**Solution:**\n\n```bash\n# Limit diff context\ngit diff -U3  # Only 3 lines of context\n\n# Skip binary files\ngit diff --no-binary\n\n# Use specific paths\ngit diff -- src/  # Only src directory\n\n# Cache git status\ngit config core.untrackedCache true\n```\n\n## Authentication Issues\n\n### Permission Denied (publickey)\n\n**Problem:** Can't push due to SSH key issues.\n\n**Solution:**\n\n```bash\n# Check SSH key\nssh -T git@github.com\n\n# Add SSH key to agent\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_rsa\n\n# Or use HTTPS instead\ngit remote set-url origin https://github.com/user/repo.git\n```\n\n### Remote Rejected\n\n**Problem:** Remote rejects push.\n\n**Solution:**\n\n```bash\n# Pull first (if behind)\ngit pull --rebase origin main\n\n# Check branch protection rules\n# May need to create PR instead of direct push\n\n# Verify remote URL\ngit remote -v\n\n# Check for hooks rejecting push\n# Contact repository admin if needed\n```\n\n## Emergency Recovery\n\n### Repository Corrupted\n\n**Problem:** Git reports corrupted objects.\n\n**Solution:**\n\n```bash\n# Check repository integrity\ngit fsck --full\n\n# Attempt recovery\ngit reflog expire --expire=now --all\ngit gc --prune=now\n\n# If still corrupted, clone fresh and apply recent work\ngit clone https://github.com/user/repo.git repo-fresh\ncd repo-fresh\n# Manually copy uncommitted work from corrupted repo\n```\n\n### Lost All Changes\n\n**Problem:** Accidentally ran destructive command.\n\n**Solution:**\n\n```bash\n# Check reflog immediately\ngit reflog\n\n# Find last good state\n# Look for: abc1234 HEAD@{10}: commit: Last good commit\n\n# Reset to that state\ngit reset --hard HEAD@{10}\n\n# If reflog doesn't help\ngit fsck --lost-found\n# Check .git/lost-found/ directory\n```\n\n### Need to Undo Everything\n\n**Problem:** Repository is in unknown state, need to start over.\n\n**Solution:**\n\n```bash\n# Save current state just in case\ngit stash\ngit branch backup-$(date +%Y%m%d-%H%M%S)\n\n# Reset to remote state\ngit fetch origin\ngit reset --hard origin/main\n\n# Clean all untracked files\ngit clean -fdx  # Be careful with this!\n\n# Or clone fresh\ncd ..\ngit clone https://github.com/user/repo.git repo-fresh\n```\n\n## Quick Reference: Abort Commands\n\n```bash\n# Abort ongoing operations\ngit merge --abort        # Abort merge\ngit rebase --abort       # Abort rebase\ngit cherry-pick --abort  # Abort cherry-pick\ngit bisect reset         # End bisect session\ngit am --abort           # Abort patch application\n\n# Reset to previous state\ngit reset --hard HEAD@{1}  # Undo last operation\ngit reflog                 # View history of HEAD changes\n\n# Clean up\ngit clean -fd              # Remove untracked files/dirs\ngit checkout .             # Discard all changes\ngit restore .              # Discard all changes (Git 2.23+)\n```\n\n## Prevention Checklist\n\nBefore risky operations:\n- [ ] Create backup branch: `git branch backup`\n- [ ] Ensure working tree is clean: `git status`\n- [ ] Know your reflog: `git reflog | head -20`\n- [ ] Verify you're on right branch: `git branch`\n- [ ] Check remote status: `git fetch && git status`\n\n---\n\n**Remember:** Most Git operations can be undone. Check reflog first, and don't panic. Take time to understand the problem before attempting fixes.\n",
        "icartsh-plugin/skills/markdown-pro/README.md": "# Professional Markdown Documentation\n\n세련된 README 파일, 변경 이력(changelog), 기여 가이드 및 기술 문서를 작성하기 위한 전문가 수준의 Markdown 문서화 SKILL입니다.\n\n## 개요 (Overview)\n\n이 SKILL은 전문적이고 잘 구조화된 Markdown 문서를 작성하기 위한 포괄적인 가이드를 제공합니다. 최신 포맷팅, 배지 및 모범 사례를 적용한 README 파일, 변경 이력, 기여 가이드 및 기술 문서를 다룹니다.\n\n배지와 섹션을 포함한 README 생성, git 히스토리를 이용한 자동 변경 이력 생성, 목차 생성, 기여 가이드라인 작성, 기술 문서 포맷팅, 구문 강조를 포함한 코드 문서화 시 이 SKILL을 사용하세요.\n\n## 설치 (Installation)\n\n기본적인 Markdown 문서화에는 설치가 필요하지 않습니다.\n\n선택 사항: 헬퍼 스크립트 종속성 설치:\n\n```bash\npip install markdown\n```\n\n## 포함 내용 (What's Included)\n\n### SKILL.md\nREADME 구조 모범 사례, 변경 이력 포맷, Markdown 포맷팅 모범 사례, 배지 생성, 코드 구문 강조, 표, 접기/펼치기 섹션, 알림 상자, 링크, 이미지 및 헬퍼 스크립트를 다루는 포괄적인 가이드입니다.\n\n### scripts/\n- `markdown_helper.py` - 다음을 위한 유틸리티 스크립트:\n  - 목차 자동 생성\n  - git 히스토리에서 변경 이력 생성\n  - 링크 유효성 검사\n\n### examples/\n- `README_template.md` - 운영 수준의 README 템플릿\n- `CHANGELOG_template.md` - Keep a Changelog 형식을 따르는 변경 이력 템플릿\n- `CONTRIBUTING.md` - 기여 가이드라인 템플릿\n\n## 빠른 시작 (Quick Start)\n\n### README 구조\n\n**필수 섹션:**\n\n```markdown\n# 프로젝트 이름\n\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n[![Version](https://img.shields.io/badge/version-1.0.0-green.svg)](releases)\n\n프로젝트가 무엇인지 설명하는 짧은 한 줄 설명.\n\n## 목차\n\n- [주요 특징](#features)\n- [설치 방법](#installation)\n- [사용법](#usage)\n- [API 참조](#api-reference)\n- [기여하기](#contributing)\n- [라이선스](#license)\n\n## 주요 특징\n\n- **특징 1**: 장점과 함께 명확한 설명 제공\n- **특징 2**: 어떤 문제를 해결하는지 기술\n- **특징 3**: 독특한 강점 강조\n\n## 설치 방법\n\n```bash\npip install package-name\n\\```\n\n## 사용법\n\n```python\nfrom package import Module\n\n# 기본 예시\nclient = Module(api_key=\"your-key\")\nresult = client.process(data)\n\\```\n\n## 라이선스\n\n이 프로젝트는 MIT 라이선스를 따릅니다. 자세한 내용은 [LICENSE](LICENSE) 파일을 참조하세요.\n```\n\n### 변경 이력 포맷\n\n```markdown\n# 변경 이력\n\n이 프로젝트의 모든 주목할 만한 변경 사항은 이 파일에 기록됩니다.\n\n형식은 [Keep a Changelog](https://keepachangelog.com/en/1.0.0/)를 따르며,\n이 프로젝트는 [시맨틱 버저닝](https://semver.org/spec/v2.0.0.html)을 준수합니다.\n\n## [Unreleased]\n\n### Added\n- 새로운 기능 설명\n\n### Changed\n- 기존 기능 수정 사항\n\n### Fixed\n- 버그 수정\n\n## [1.2.0] - 2025-01-15\n\n### Added\n- 사용자 인증 시스템 (#123)\n- CSV 내보내기 기능 (#145)\n\n### Fixed\n- 백그라운드 프로세서의 메모리 누수 수정 (#139)\n```\n\n## 핵심 기능 (Key Features)\n\n### README 생성\n- 프로젝트 개요 및 설명\n- 설치 지침\n- 코드 블록을 포함한 사용 예시\n- API 문서화\n- 배지 및 실드\n- 주요 특징 강조\n- 스크린샷 및 데모\n\n### 변경 이력 자동화\n- 시맨틱 버저닝 형식\n- Git 히스토리 파싱\n- 자동 릴리스 노트 생성\n- 주요 변경 사항 강조\n- 기여자 표시\n\n### 기술 문서화\n- 명확한 섹션 계층 구조\n- 코드 구문 강조\n- API 참조 포맷팅\n- 목차 생성\n- 상호 참조\n- 접기/펼치기 섹션\n\n## Markdown 포맷팅 모범 사례\n\n### 구문 강조를 포함한 코드 블록\n\n```markdown\n```python\ndef hello_world():\n    \"\"\"헬로 월드 메시지 출력.\"\"\"\n    print(\"Hello, World!\")\n\\```\n\n```javascript\nfunction helloWorld() {\n    console.log(\"Hello, World!\");\n}\n\\```\n\n```bash\n# 종속성 설치\nnpm install\n\\```\n```\n\n### 표 (Tables)\n\n```markdown\n| 기능 | 설명 | 상태 |\n|---------|-------------|--------|\n| 인증 | 사용자 인증 시스템 | ✅ 완료 |\n| API | RESTful API 엔드포인트 | ✅ 완료 |\n| 문서 | 문서화 작업 | 🚧 진행 중 |\n| 테스트 | 유닛 및 통합 테스트 | ❌ 계획됨 |\n```\n\n### 접기/펼치기 섹션\n\n```markdown\n<details>\n<summary>클릭하여 고급 설정 확인</summary>\n\n## 고급 옵션\n\n고급 설정을 구성합니다:\n\n```yaml\nadvanced:\n  cache_size: 1000\n  timeout: 30\n\\```\n\n</details>\n```\n\n### 알림 상자 (Alert Boxes)\n\n```markdown\n> **참고**: 이 기능은 Python 3.8 이상이 필요합니다.\n\n> **주의**: 이 작업은 되돌릴 수 없습니다!\n\n> **중요**: 업그레이드 전에는 항상 데이터를 백업하세요.\n```\n\n## 배지 생성\n\n### 공통 배지 패턴\n\n```markdown\n<!-- License -->\n![License](https://img.shields.io/badge/license-MIT-blue.svg)\n\n<!-- Version -->\n![Version](https://img.shields.io/badge/version-1.0.0-green.svg)\n\n<!-- Build Status -->\n![Build](https://img.shields.io/badge/build-passing-brightgreen.svg)\n\n<!-- Coverage -->\n![Coverage](https://img.shields.io/badge/coverage-95%25-brightgreen.svg)\n\n<!-- Language -->\n![Python](https://img.shields.io/badge/python-3.8+-blue.svg)\n\n<!-- Platform -->\n![Platform](https://img.shields.io/badge/platform-windows%20%7C%20macOS%20%7C%20linux-lightgrey.svg)\n```\n\n## 헬퍼 스크립트\n\n### 목차 생성\n```bash\npython scripts/markdown_helper.py toc README.md\n```\n\n### Git으로부터 변경 이력 생성\n```bash\npython scripts/markdown_helper.py changelog --since v1.0.0 --output CHANGELOG.md\n```\n\n### 링크 유효성 검사\n```bash\npython scripts/markdown_helper.py validate docs/\n```\n\n## 모범 사례 (Best Practices)\n\n### 수행할 작업 (Do's)\n- 명확하고 설명적인 헤더를 사용하세요.\n- 모든 주요 기능에 대해 코드 예시를 포함하세요.\n- 프로젝트 상태를 한눈에 볼 수 있도록 배지를 추가하세요.\n- 가독성을 위해 한 줄의 길이를 100자 이내로 유지하세요.\n- 코드 블록에는 구문 강조를 사용하세요.\n- 300행 이상의 문서에는 목차를 포함하세요.\n- 모든 이미지에 대체 텍스트를 추가하세요.\n- 관련 문서로의 링크를 제공하세요.\n\n### 피해야 할 작업 (Don'ts)\n- 일반적인 제목을 사용하지 마세요.\n- 텍스트 덩어리를 길게 나열하지 마세요 (섹션으로 나누세요).\n- 릴리스할 때 변경 이력을 업데이트하는 것을 잊지 마세요.\n- 단순 URL만 적지 마세요 (설명적인 링크 텍스트 사용).\n- 헤더 스타일을 혼용하지 마세요.\n- 설명 없는 스크린샷을 넣지 마세요.\n- 버전 번호를 하드코딩하지 마세요.\n\n## 헤더 계층 구조\n```markdown\n# H1 - 프로젝트 제목 (문서당 하나만 사용)\n## H2 - 주요 섹션\n### H3 - 하위 섹션\n#### H4 - 부가적인 포인트\n##### H5 - 드물게 사용되는 깊은 중첩\n```\n\n## 목록 포맷팅\n```markdown\n<!-- 순서 없는 목록 -->\n- 항목 1\n- 항목 2\n  - 중첩 항목\n\n<!-- 순서 있는 목록 -->\n1. 첫 번째 단계\n2. 두 번째 단계\n\n<!-- 작업 목록 -->\n- [x] 완료된 작업\n- [ ] 진행 예정 작업\n```\n\n## 강조 (Emphasis)\n```markdown\n*기울임* 또는 _기울임_\n**굵게** 또는 __굵게__\n***굵은 기울임***\n~~취소선~~\n`인라인 코드`\n```\n\n## API 문서 포맷\n```markdown\n### `Module.process(data, options=None)`\n\n선택적 설정을 사용하여 입력 데이터를 처리합니다.\n\n**매개변수:**\n- `data` (str|dict): 처리할 입력 데이터\n- `options` (dict, 선택 사항): 설정 옵션\n  - `verbose` (bool): 상세 출력 활성화 (기본값: False)\n  - `format` (str): 출력 형식 - 'json', 'yaml', 'xml' (기본값: 'json')\n\n**반환값:**\n- `dict`: 메타데이터가 포함된 처리 결과\n\n**예외:**\n- `ValueError`: 데이터가 유효하지 않은 경우\n\n**예시:**\n```python\nresult = client.process(\n    data={\"key\": \"value\"},\n    options={\"verbose\": True, \"format\": \"json\"}\n)\n\\```\n```\n\n## 링크 및 참조\n```markdown\n<!-- 외부 링크 -->\n[문서 보기](https://docs.example.com)\n\n<!-- 내부 링크 -->\n[설치 방법](#installation) 섹션을 참조하세요.\n\n<!-- 참조 스타일 링크 -->\n[프로젝트 홈페이지][homepage]와 [문서][docs]를 확인하세요.\n\n[homepage]: https://example.com\n[docs]: https://docs.example.com\n```\n\n## 이미지\n```markdown\n<!-- 표준 이미지 -->\n![프로젝트 로고](assets/logo.png)\n\n<!-- 대체 텍스트와 타이틀이 포함된 이미지 -->\n![대시보드 스크린샷](screenshots/dashboard.png \"메인 대시보드 화면\")\n\n<!-- 링크가 포함된 이미지 -->\n[![데모 비디오](thumbnail.jpg)](https://youtube.com/watch?v=example)\n```\n\n## 템플릿\n\n### 전문 README 템플릿\n모든 권장 섹션이 포함된 운영 수준의 README 템플릿은 `examples/README_template.md`를 참조하세요.\n\n### 변경 이력 템플릿\nKeep a Changelog 형식을 따르는 올바른 포맷의 변경 이력 템플릿은 `examples/CHANGELOG_template.md`를 참조하세요.\n\n### 기여 가이드라인\n행동 강령, 개발 환경 설정, PR 프로세스를 포함한 기여 가이드라인 템플릿은 `examples/CONTRIBUTING.md`를 참조하세요.\n\n## 일반적인 유스케이스\n\n### 프로젝트 문서 작성\nGitHub 프로젝트를 위해 적절한 구조와 포맷을 갖춘 포괄적인 README 파일을 생성합니다.\n\n### 변경 이력 관리\n시맨틱 버저닝을 따르며 git 커밋 히스토리에서 변경 이력 생성을 자동화합니다.\n\n### API 문서화\n코드 예시와 함께 명확하고 잘 포맷된 API 참조 문서를 작성합니다.\n\n### 기여 가이드라인 작성\n기여자들을 위해 설치 안내와 PR 프로세스를 포함한 명확한 가이드라인을 제공합니다.\n\n## 문서\n포괄적인 문서, 상세 포맷팅 패턴 및 고급 기법에 대해서는 `SKILL.md`를 참조하세요.\n\n## 요구 사항\n기본적인 Markdown 문서화에는 특별한 요구 사항이 없습니다.\n\n선택 사항:\n- Python 3.7+ (헬퍼 스크립트용)\n- markdown (고급 기능을 위한 Python 라이브러리)\n",
        "icartsh-plugin/skills/markdown-pro/SKILL.md": "---\nname: markdown-pro\ndescription: \"세련된 README 파일, 변경 이력(changelog), 기여 가이드(contribution guide) 및 기술 문서를 작성하기 위한 전문가 수준의 Markdown 문서화 SKILL입니다. 사용 사례: (1) 배지와 섹션을 포함한 README 생성, (2) git 히스토리를 이용한 자동 변경 이력 생성, (3) 목차(table of contents) 생성, (4) 기여 가이드라인 작성, (5) 기술 문서 포맷팅, (6) 구문 강조(syntax highlighting)를 포함한 코드 문서화\"\n---\n\n# Professional Markdown Documentation\n\n## 개요 (Overview)\n\n이 SKILL은 전문적이고 잘 구조화된 Markdown 문서를 작성하기 위한 포괄적인 가이드를 제공합니다. 최신 포맷팅, 배지 및 모범 사례를 적용한 README 파일, 변경 이력, 기여 가이드 및 기술 문서를 다룹니다.\n\n## 핵심 역량 (Core Capabilities)\n\n### README 생성\n- 프로젝트 개요 및 설명\n- 설치 지침\n- 코드 블록을 포함한 사용 예시\n- API 문서화\n- 배지(badges) 및 실드(shields)\n- 주요 특징 강조\n- 스크린샷 및 데모\n\n### 변경 이력(Changelog) 자동화\n- 시맨틱 버저닝(Semantic versioning) 형식\n- Git 히스토리 파싱\n- 자동 릴리스 노트 생성\n- 주요 변경 사항(Breaking changes) 강조\n- 기여자 표시 (attribution)\n\n### 기술 문서화\n- 명확한 섹션 계층 구조\n- 코드 구문 강조 (Syntax highlighting)\n- API 참조 포맷팅\n- 목차 (Table of contents)\n- 상호 참조 (Cross-referencing)\n- 접기/펼치기 섹션 (Collapsible sections)\n\n## README 구조 모범 사례\n\n### 필수 섹션\n\n**1. 배지를 포함한 헤더**\n```markdown\n# 프로젝트 이름\n\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n[![Version](https://img.shields.io/badge/version-1.0.0-green.svg)](releases)\n[![Build](https://img.shields.io/badge/build-passing-brightgreen.svg)](builds)\n\n프로젝트가 무엇인지 설명하는 짧은 한 줄 설명.\n```\n\n**2. 목차 (Table of Contents)** (내용이 긴 README의 경우)\n```markdown\n## 목차\n\n- [주요 특징](#features)\n- [설치 방법](#installation)\n- [사용법](#usage)\n- [API 참조](#api-reference)\n- [기여하기](#contributing)\n- [라이선스](#license)\n```\n\n**3. 주요 특징 섹션 (Features)**\n```markdown\n## 주요 특징\n\n- **특징 1**: 장점과 함께 명확한 설명 제공\n- **특징 2**: 어떤 문제를 해결하는지 기술\n- **특징 3**: 독특한 강점 강조\n- 크로스 플랫폼 지원 (Windows, macOS, Linux)\n- 포괄적인 테스트 커버리지 (>90%)\n```\n\n**4. 설치 방법 (Installation)**\n```markdown\n## 설치 방법\n\n### 사전 요구 사항\n- Python 3.8 이상\n- pip 패키지 매니저\n\n### 빠른 시작\n\n```bash\npip install package-name\n```\n\n### 소스에서 설치\n\n```bash\ngit clone https://github.com/username/repo.git\ncd repo\npip install -e .\n```\n```\n\n**5. 사용 예시 (Usage)**\n```markdown\n## 사용법\n\n### 기본 예시\n\n```python\nfrom package import Module\n\n# 초기화\nclient = Module(api_key=\"your-key\")\n\n# 작업 수행\nresult = client.process(data)\nprint(result)\n```\n\n### 고급 사용법\n\n더 자세한 사용 사례는 [examples/](examples/) 디렉토리를 참조하세요.\n```\n\n**6. API 문서화 (API Reference)**\n```markdown\n## API 참조\n\n### `Module.process(data, options=None)`\n\n선택적 설정을 사용하여 입력 데이터를 처리합니다.\n\n**매개변수:**\n- `data` (str|dict): 처리할 입력 데이터\n- `options` (dict, 선택 사항): 설정 옵션\n  - `verbose` (bool): 상세 출력 활성화 (기본값: False)\n  - `format` (str): 출력 형식 - 'json', 'yaml', 'xml' (기본값: 'json')\n\n**반환값:**\n- `dict`: 메타데이터가 포함된 처리 결과\n\n**예외:**\n- `ValueError`: 데이터가 유효하지 않은 경우\n- `APIError`: API 요청이 실패한 경우\n\n**예시:**\n```python\nresult = client.process(\n    data={\"key\": \"value\"},\n    options={\"verbose\": True, \"format\": \"json\"}\n)\n```\n```\n\n**7. 기여하기 섹션 (Contributing)**\n```markdown\n## 기여하기\n\n프로젝트 기여를 환영합니다! 가이드라인은 [CONTRIBUTING.md](CONTRIBUTING.md)를 참조하세요.\n\n### 빠른 기여 가이드\n1. 저장소 포크 (Fork)\n2. 피처 브랜치 생성 (`git checkout -b feature/amazing-feature`)\n3. 변경 사항 커밋 (`git commit -m 'Add amazing feature'`)\n4. 브랜치 푸시 (`git push origin feature/amazing-feature`)\n5. 풀 리퀘스트 (Pull Request) 오픈\n```\n\n**8. 라이선스 및 크레딧**\n```markdown\n## 라이선스\n\n이 프로젝트는 MIT 라이선스를 따릅니다. 자세한 내용은 [LICENSE](LICENSE) 파일을 참조하세요.\n\n## 감사의 글\n\n- 특징 X를 구현해준 [기여자 이름]님께 감사드립니다.\n- [Project Name](link)에서 영감을 얻었습니다.\n- [Technology Stack]으로 구축되었습니다.\n```\n\n## 변경 이력(Changelog) 포맷\n\n### 시맨틱 버저닝 구조\n\n```markdown\n# 변경 이력\n\n이 프로젝트의 모든 주목할 만한 변경 사항은 이 파일에 기록됩니다.\n\n형식은 [Keep a Changelog](https://keepachangelog.com/en/1.0.0/)를 따르며,\n이 프로젝트는 [시맨틱 버저닝(Semantic Versioning)](https://semver.org/spec/v2.0.0.html)을 준수합니다.\n\n## [Unreleased]\n\n### Added\n- 새로운 기능 설명\n\n### Changed\n- 기존 기능 수정 사항\n\n### Deprecated\n- 향후 삭제될 예정인 기능\n\n### Removed\n- 삭제된 기능\n\n### Fixed\n- 버그 수정\n\n### Security\n- 보안 개선 사항\n\n## [1.2.0] - 2025-01-15\n\n### Added\n- 사용자 인증 시스템 (#123)\n- CSV 내보내기 기능 (#145)\n- 다크 모드 지원 (#156)\n\n### Changed\n- 응답성 개선을 위한 UI 컴포넌트 업데이트 (#134)\n- 에러 메시지 개선 (#142)\n\n### Fixed\n- 백그라운드 프로세서의 메모리 누수 수정 (#139)\n- 로그인 타임아웃 이슈 해결 (#148)\n\n## [1.1.0] - 2024-12-01\n\n### Added\n- 핵심 기능을 포함한 초기 릴리스\n```\n\n## Markdown 포맷팅 모범 사례\n\n### 구문 강조를 포함한 코드 블록\n\n```markdown\n```python\ndef hello_world():\n    \"\"\"헬로 월드 메시지 출력.\"\"\"\n    print(\"Hello, World!\")\n```\n\n```javascript\nfunction helloWorld() {\n    console.log(\"Hello, World!\");\n}\n```\n\n```bash\n# 종속성 설치\nnpm install\n\n# 테스트 실행\nnpm test\n```\n```\n\n### 표 (Tables)\n\n```markdown\n| 기능 | 설명 | 상태 |\n|---------|-------------|--------|\n| 인증 | 사용자 인증 시스템 | ✅ 완료 |\n| API | RESTful API 엔드포인트 | ✅ 완료 |\n| 문서 | 문서화 작업 | 🚧 진행 중 |\n| 테스트 | 유닛 및 통합 테스트 | ❌ 계획됨 |\n```\n\n### 접기/펼치기 섹션 (Collapsible Sections)\n\n```markdown\n<details>\n<summary>클릭하여 고급 설정 확인</summary>\n\n## 고급 옵션\n\n고급 설정을 구성합니다:\n\n```yaml\nadvanced:\n  cache_size: 1000\n  timeout: 30\n  retry_attempts: 3\n```\n\n</details>\n```\n\n### 알림 상자 (Alert Boxes)\n\n```markdown\n> **참고**: 이 기능은 Python 3.8 이상이 필요합니다.\n\n> **주의**: 이 작업은 되돌릴 수 없습니다!\n\n> **중요**: 업그레이드 전에는 항상 데이터를 백업하세요.\n```\n\n### 링크 및 참조\n\n```markdown\n<!-- 외부 링크 -->\n[문서 보기](https://docs.example.com)\n\n<!-- 내부 링크 -->\n[설치 방법](#installation) 섹션을 참조하세요.\n\n<!-- 참조 스타일 링크 -->\n[프로젝트 홈페이지][homepage]와 [문서][docs]를 확인하세요.\n\n[homepage]: https://example.com\n[docs]: https://docs.example.com\n```\n\n### 이미지\n\n```markdown\n<!-- 표준 이미지 -->\n![프로젝트 로고](assets/logo.png)\n\n<!-- 대체 텍스트와 타이틀이 포함된 이미지 -->\n![대시보드 스크린샷](screenshots/dashboard.png \"메인 대시보드 화면\")\n\n<!-- 링크가 포함된 이미지 -->\n[![데모 비디오](thumbnail.jpg)](https://youtube.com/watch?v=example)\n```\n\n## 배지 생성\n\n### 공통 배지 패턴\n\n```markdown\n<!-- License -->\n![License](https://img.shields.io/badge/license-MIT-blue.svg)\n\n<!-- Version -->\n![Version](https://img.shields.io/badge/version-1.0.0-green.svg)\n\n<!-- Build Status -->\n![Build](https://img.shields.io/badge/build-passing-brightgreen.svg)\n\n<!-- Coverage -->\n![Coverage](https://img.shields.io/badge/coverage-95%25-brightgreen.svg)\n\n<!-- Language -->\n![Python](https://img.shields.io/badge/python-3.8+-blue.svg)\n\n<!-- Platform -->\n![Platform](https://img.shields.io/badge/platform-windows%20%7C%20macOS%20%7C%20linux-lightgrey.svg)\n```\n\n## 헬퍼 스크립트 (Helper Scripts)\n\n### 목차(TOC) 생성\n\n헤더로부터 목차를 자동으로 생성하려면 헬퍼 스크립트를 사용하세요:\n\n```bash\npython scripts/markdown_helper.py toc README.md\n```\n\n### Git으로부터 변경 이력 생성\n\ngit 히스토리에서 변경 이력 항목을 자동으로 생성합니다:\n\n```bash\npython scripts/markdown_helper.py changelog --since v1.0.0 --output CHANGELOG.md\n```\n\n### Markdown 링크 유효성 검사\n\n문서 내 깨진 링크가 있는지 확인합니다:\n\n```bash\npython scripts/markdown_helper.py validate docs/\n```\n\n## 템플릿 (Templates)\n\n### 전문 README 템플릿\n권장하는 모든 섹션이 포함된 운영 수준의 README 템플릿은 `examples/README_template.md`를 참조하세요.\n\n### 변경 이력 템플릿\nKeep a Changelog 형식을 따르는 올바른 포맷의 변경 이력 템플릿은 `examples/CHANGELOG_template.md`를 참조하세요.\n\n### 기여 가이드라인\n행동 강령(Code of conduct), 개발 환경 설정, PR 프로세스를 포함한 기여 가이드라인 템플릿은 `examples/CONTRIBUTING.md`를 참조하세요.\n\n## 모범 사례 요약 (Best Practices Summary)\n\n### 수행할 작업 (Do's)\n- 명확하고 설명적인 헤더를 사용하세요.\n- 모든 주요 기능에 대해 코드 예시를 포함하세요.\n- 프로젝트 상태를 한눈에 볼 수 있도록 배지를 추가하세요.\n- 가독성을 위해 한 줄의 길이를 100자 이내로 유지하세요.\n- 코드 블록에는 구문 강조를 사용하세요.\n- 300행 이상의 문서에는 목차를 포함하세요.\n- 모든 이미지에 대체 텍스트(alt text)를 추가하세요.\n- 관련 문서로의 링크를 제공하세요.\n\n### 피해야 할 작업 (Don'ts)\n- \"My Project\"와 같이 일반적인 제목을 사용하지 마세요.\n- 텍스트 덩어리를 길게 나열하지 마세요 (섹션으로 나누세요).\n- 릴리스할 때 변경 이력을 업데이트하는 것을 잊지 마세요.\n- 단순 URL만 적지 마세요 (항상 설명적인 링크 텍스트를 사용하세요).\n- 헤더 스타일을 혼용하지 마세요 (일관된 계층 구조 유지).\n- 설명 없는 스크린샷을 넣지 마세요.\n- 버전 번호를 여기저기 하드코딩하지 마세요 (변수나 배지 활용).\n\n## 빠른 참조 (Quick Reference)\n\n### 헤더 계층 구조\n```markdown\n# H1 - 프로젝트 제목 (문서당 하나만 사용)\n## H2 - 주요 섹션\n### H3 - 하위 섹션\n#### H4 - 부가적인 포인트\n##### H5 - 드물게 사용되는 깊은 중첩\n```\n\n### 목록 포맷팅\n```markdown\n<!-- 순서 없는 목록 -->\n- 항목 1\n- 항목 2\n  - 중첩 항목\n  - 또 다른 중첩 항목\n\n<!-- 순서 있는 목록 -->\n1. 첫 번째 단계\n2. 두 번째 단계\n3. 세 번째 단계\n\n<!-- 작업 목록 -->\n- [x] 완료된 작업\n- [ ] 진행 예정 작업\n- [ ] 또 다른 대기 작업\n```\n\n### 강조 (Emphasis)\n```markdown\n*기울임* 또는 _기울임_\n**굵게** 또는 __굵게__\n***굵은 기울임*** 또는 ___굵은 기울임___\n~~취소선~~\n`인라인 코드`\n```\n\n## 결론\n\n전문적인 Markdown 문서화는 프로젝트의 접근성을 높이고, 기여자를 유도하며, 사용자에게 명확한 가이드를 제공합니다. `examples/`의 템플릿을 시작점으로 삼고, `scripts/`의 헬퍼 스크립트로 커스터마이징하며, 세련되고 유지보수가 용이한 문서를 위해 이 모범 사례들을 따르세요.\n",
        "icartsh-plugin/skills/markdown-pro/examples/CHANGELOG_template.md": "# Changelog\n\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [Unreleased]\n\n### Added\n- Feature preview mode for testing new capabilities\n- Support for WebP image format\n- Experimental AI-powered suggestions\n\n### Changed\n- Improved error messages with actionable suggestions\n- Updated dependencies to latest stable versions\n\n### Fixed\n- Resolved race condition in concurrent requests\n\n## [2.1.0] - 2025-01-15\n\n### Added\n- Dark mode support with automatic theme switching (#234)\n- Export data to CSV, JSON, and XML formats (#245)\n- Real-time collaboration features (#256)\n- Keyboard shortcuts for common actions (#267)\n- Mobile app for iOS and Android (#278)\n\n### Changed\n- Redesigned user interface with improved navigation (#239)\n- Optimized database queries for 40% faster performance (#248)\n- Updated API to v2 with backward compatibility (#251)\n- Improved accessibility with ARIA labels (#263)\n\n### Fixed\n- Fixed memory leak in background processor (#241)\n- Resolved login timeout on slow connections (#249)\n- Fixed text encoding issues with special characters (#254)\n- Corrected timezone handling in date pickers (#261)\n\n### Security\n- Implemented rate limiting to prevent abuse (#243)\n- Added CSRF protection for all forms (#257)\n- Updated authentication to use OAuth 2.1 (#268)\n\n## [2.0.0] - 2024-12-01\n\n### Breaking Changes\n- **API v2 Migration**: API v1 endpoints are deprecated. Use v2 endpoints with `/api/v2/` prefix. v1 will be removed in v3.0.0\n- **Configuration Format**: Config file format changed from JSON to YAML. Run `migrate-config` tool to upgrade\n- **Python Support**: Dropped Python 3.7 support. Minimum version is now Python 3.8\n- **Database Schema**: Database schema updated. Run migrations with `alembic upgrade head`\n\n### Added\n- Complete API v2 with RESTful design (#201)\n- GraphQL endpoint for flexible queries (#212)\n- Webhook support for real-time notifications (#223)\n- Multi-factor authentication (TOTP, SMS) (#228)\n- Advanced search with filters and sorting (#235)\n\n### Changed\n- Migrated from SQLite to PostgreSQL for production (#206)\n- Switched to async/await pattern throughout codebase (#215)\n- Updated UI framework from React 16 to React 18 (#220)\n- Improved test coverage from 75% to 95% (#227)\n\n### Removed\n- Removed legacy API v1 endpoints (use v2) (#203)\n- Deprecated old authentication system (#210)\n- Removed jQuery dependency (#218)\n\n### Fixed\n- Fixed critical security vulnerability in auth flow (CVE-2024-XXXX) (#205)\n- Resolved data corruption issue with concurrent writes (#214)\n- Fixed session persistence across page reloads (#221)\n\n### Security\n- Implemented Content Security Policy headers (#208)\n- Added input sanitization for all user inputs (#216)\n- Updated all dependencies to patch known vulnerabilities (#226)\n\n## [1.5.2] - 2024-10-15\n\n### Fixed\n- Hotfix for login redirect loop (#198)\n- Fixed PDF export with special characters (#199)\n- Resolved cache invalidation issue (#200)\n\n## [1.5.1] - 2024-10-01\n\n### Fixed\n- Critical bug fix for payment processing (#195)\n- Fixed email notification delivery (#196)\n- Resolved UI rendering issue in Safari (#197)\n\n## [1.5.0] - 2024-09-15\n\n### Added\n- Bulk operations for managing multiple items (#178)\n- Template system for common workflows (#183)\n- Email notification preferences (#189)\n- Activity log and audit trail (#192)\n\n### Changed\n- Improved onboarding experience (#180)\n- Enhanced mobile responsiveness (#185)\n- Updated user profile page design (#188)\n\n### Fixed\n- Fixed drag-and-drop file upload (#181)\n- Resolved pagination bug with large datasets (#186)\n- Fixed date picker timezone issues (#190)\n\n### Performance\n- Reduced initial page load time by 35% (#179)\n- Optimized image loading with lazy loading (#184)\n- Implemented database query caching (#187)\n\n## [1.4.0] - 2024-08-01\n\n### Added\n- Integration with Slack for notifications (#156)\n- Custom field support for user profiles (#162)\n- Advanced filtering options (#168)\n- Password strength requirements (#173)\n\n### Changed\n- Updated dashboard layout for better usability (#159)\n- Improved error handling and user feedback (#165)\n- Enhanced search functionality (#170)\n\n### Fixed\n- Fixed memory leak in WebSocket connections (#158)\n- Resolved file upload size limit issue (#163)\n- Fixed CSV export formatting (#169)\n\n### Deprecated\n- Old notification system (to be removed in v2.0) (#175)\n- Legacy report format (use new format) (#176)\n\n## [1.3.0] - 2024-06-15\n\n### Added\n- User roles and permissions system (#134)\n- Two-factor authentication support (#142)\n- Data export functionality (#148)\n- Comprehensive logging system (#153)\n\n### Changed\n- Redesigned settings page (#137)\n- Improved API documentation (#145)\n- Updated dependencies to latest versions (#150)\n\n### Fixed\n- Fixed session timeout handling (#136)\n- Resolved CORS issues with API (#143)\n- Fixed image upload validation (#149)\n\n## [1.2.0] - 2024-05-01\n\n### Added\n- User profile customization (#112)\n- Search functionality across all modules (#119)\n- Email notifications for important events (#125)\n- API rate limiting (#131)\n\n### Changed\n- Updated UI components to Material Design 3 (#115)\n- Improved loading states and animations (#122)\n- Enhanced mobile navigation (#128)\n\n### Fixed\n- Fixed pagination in data tables (#114)\n- Resolved timezone conversion bugs (#120)\n- Fixed form validation edge cases (#126)\n\n## [1.1.0] - 2024-03-15\n\n### Added\n- User authentication and authorization (#89)\n- Dashboard with key metrics (#95)\n- File upload support (#101)\n- Basic reporting features (#107)\n\n### Changed\n- Improved error messages (#92)\n- Updated color scheme and branding (#98)\n- Enhanced form validation (#104)\n\n### Fixed\n- Fixed navigation menu on mobile devices (#91)\n- Resolved data synchronization issues (#97)\n- Fixed date formatting in reports (#103)\n\n## [1.0.0] - 2024-01-15\n\n### Added\n- Initial release with core functionality\n- User registration and login\n- Basic CRUD operations\n- Responsive web interface\n- RESTful API\n- SQLite database\n- Unit and integration tests\n- Documentation and examples\n\n### Changed\n- Migrated from prototype to production-ready code\n- Improved code organization and structure\n- Enhanced security measures\n\n### Fixed\n- Numerous bug fixes from beta testing\n- Performance optimizations\n- Memory leak fixes\n\n## [0.9.0-beta] - 2023-12-01\n\n### Added\n- Beta release for testing\n- Core features implementation\n- Basic UI and API\n\n---\n\n## Version Numbering\n\nThis project follows [Semantic Versioning](https://semver.org/):\n- **MAJOR** version for incompatible API changes\n- **MINOR** version for new functionality in a backward compatible manner\n- **PATCH** version for backward compatible bug fixes\n\n## Categories\n\n- **Added**: New features\n- **Changed**: Changes in existing functionality\n- **Deprecated**: Soon-to-be removed features\n- **Removed**: Now removed features\n- **Fixed**: Bug fixes\n- **Security**: Security vulnerability fixes\n- **Performance**: Performance improvements\n- **Breaking Changes**: Changes that break backward compatibility\n\n## Links\n\n- [Unreleased]: https://github.com/username/repo/compare/v2.1.0...HEAD\n- [2.1.0]: https://github.com/username/repo/compare/v2.0.0...v2.1.0\n- [2.0.0]: https://github.com/username/repo/compare/v1.5.2...v2.0.0\n- [1.5.2]: https://github.com/username/repo/compare/v1.5.1...v1.5.2\n- [1.5.1]: https://github.com/username/repo/compare/v1.5.0...v1.5.1\n- [1.5.0]: https://github.com/username/repo/compare/v1.4.0...v1.5.0\n- [1.4.0]: https://github.com/username/repo/compare/v1.3.0...v1.4.0\n- [1.3.0]: https://github.com/username/repo/compare/v1.2.0...v1.3.0\n- [1.2.0]: https://github.com/username/repo/compare/v1.1.0...v1.2.0\n- [1.1.0]: https://github.com/username/repo/compare/v1.0.0...v1.1.0\n- [1.0.0]: https://github.com/username/repo/releases/tag/v1.0.0\n",
        "icartsh-plugin/skills/markdown-pro/examples/CONTRIBUTING.md": "# Contributing to Project Name\n\nThank you for your interest in contributing! We welcome contributions from everyone and appreciate your help in making this project better.\n\n## Table of Contents\n\n- [Code of Conduct](#code-of-conduct)\n- [Getting Started](#getting-started)\n- [Development Setup](#development-setup)\n- [How to Contribute](#how-to-contribute)\n- [Coding Standards](#coding-standards)\n- [Commit Message Guidelines](#commit-message-guidelines)\n- [Pull Request Process](#pull-request-process)\n- [Testing Guidelines](#testing-guidelines)\n- [Documentation](#documentation)\n- [Community](#community)\n\n## Code of Conduct\n\nThis project follows a Code of Conduct to ensure a welcoming environment for all contributors. By participating, you agree to:\n\n- Be respectful and inclusive\n- Welcome newcomers and help them get started\n- Accept constructive criticism gracefully\n- Focus on what's best for the community\n- Show empathy towards other community members\n\nReport unacceptable behavior to [conduct@example.com](mailto:conduct@example.com).\n\n## Getting Started\n\n### Prerequisites\n\nBefore you begin, ensure you have:\n\n- Python 3.8 or higher\n- Git version control\n- A GitHub account\n- Familiarity with the project (read README.md)\n\n### Find an Issue\n\n1. Browse [open issues](https://github.com/username/repo/issues)\n2. Look for issues labeled `good first issue` or `help wanted`\n3. Comment on an issue you'd like to work on\n4. Wait for assignment before starting work\n\nNot sure where to start? Check out:\n- [Good First Issues](https://github.com/username/repo/labels/good%20first%20issue)\n- [Help Wanted](https://github.com/username/repo/labels/help%20wanted)\n- [Documentation](https://github.com/username/repo/labels/documentation)\n\n## Development Setup\n\n### 1. Fork and Clone\n\n```bash\n# Fork the repository on GitHub, then clone your fork\ngit clone https://github.com/your-username/repo.git\ncd repo\n\n# Add upstream remote\ngit remote add upstream https://github.com/username/repo.git\n```\n\n### 2. Create Virtual Environment\n\n```bash\n# Create virtual environment\npython -m venv venv\n\n# Activate virtual environment\n# On macOS/Linux:\nsource venv/bin/activate\n# On Windows:\nvenv\\Scripts\\activate\n```\n\n### 3. Install Dependencies\n\n```bash\n# Install package in editable mode with dev dependencies\npip install -e \".[dev]\"\n\n# Or install from requirements files\npip install -r requirements.txt\npip install -r requirements-dev.txt\n```\n\n### 4. Install Pre-commit Hooks\n\n```bash\n# Install pre-commit hooks\npre-commit install\n\n# Run hooks manually (optional)\npre-commit run --all-files\n```\n\n### 5. Verify Setup\n\n```bash\n# Run tests to verify setup\npytest\n\n# Check code quality\nflake8 src/\nblack --check src/\nmypy src/\n```\n\n## How to Contribute\n\n### Reporting Bugs\n\nBefore creating a bug report:\n\n1. Check existing issues to avoid duplicates\n2. Use the latest version of the software\n3. Determine which repository the issue belongs to\n\n**Create a bug report with:**\n\n- Clear, descriptive title\n- Steps to reproduce the issue\n- Expected vs actual behavior\n- Screenshots (if applicable)\n- Environment details (OS, Python version, etc.)\n- Relevant logs or error messages\n\n**Use the bug report template:**\n\n```markdown\n**Bug Description**\nA clear description of the bug.\n\n**To Reproduce**\n1. Go to '...'\n2. Click on '...'\n3. See error\n\n**Expected Behavior**\nWhat you expected to happen.\n\n**Screenshots**\nIf applicable, add screenshots.\n\n**Environment**\n- OS: [e.g., macOS 14.0]\n- Python: [e.g., 3.11.5]\n- Version: [e.g., 1.2.3]\n\n**Additional Context**\nAny other relevant information.\n```\n\n### Suggesting Features\n\nBefore creating a feature request:\n\n1. Check if the feature already exists\n2. Review open feature requests\n3. Ensure it aligns with project goals\n\n**Create a feature request with:**\n\n- Clear, descriptive title\n- Problem statement (what problem does this solve?)\n- Proposed solution\n- Alternative solutions considered\n- Additional context\n\n### Making Code Changes\n\n#### 1. Create a Branch\n\n```bash\n# Update your fork\ngit checkout main\ngit pull upstream main\n\n# Create a feature branch\ngit checkout -b feature/your-feature-name\n# or\ngit checkout -b fix/bug-description\n```\n\n**Branch naming convention:**\n- `feature/feature-name` - New features\n- `fix/bug-description` - Bug fixes\n- `docs/what-changed` - Documentation updates\n- `refactor/what-changed` - Code refactoring\n- `test/what-added` - Test additions/updates\n\n#### 2. Make Your Changes\n\n- Write clear, concise code\n- Follow the coding standards (see below)\n- Add tests for new functionality\n- Update documentation as needed\n- Keep commits focused and atomic\n\n#### 3. Test Your Changes\n\n```bash\n# Run all tests\npytest\n\n# Run specific test file\npytest tests/test_feature.py\n\n# Run with coverage\npytest --cov=src --cov-report=html\n\n# Run linting\nflake8 src/\nblack src/\nisort src/\nmypy src/\n```\n\n#### 4. Commit Your Changes\n\n```bash\n# Stage changes\ngit add .\n\n# Commit with descriptive message\ngit commit -m \"feat: add user authentication feature\"\n\n# Push to your fork\ngit push origin feature/your-feature-name\n```\n\n## Coding Standards\n\n### Python Style Guide\n\nWe follow [PEP 8](https://pep8.org/) with some modifications:\n\n- **Line length**: 100 characters (not 79)\n- **Indentation**: 4 spaces (no tabs)\n- **Quotes**: Double quotes for strings, single quotes for string keys\n- **Imports**: Organized with `isort`\n- **Type hints**: Use type hints for function signatures\n\n### Code Formatting\n\nWe use automated formatters:\n\n```bash\n# Format code with Black\nblack src/ tests/\n\n# Sort imports with isort\nisort src/ tests/\n\n# Check with flake8\nflake8 src/ tests/\n```\n\n### Code Structure\n\n```python\n\"\"\"Module docstring explaining the module purpose.\"\"\"\n\nimport standard_library\nimport third_party_library\n\nfrom package import local_module\n\n\nCONSTANT_NAME = \"value\"\n\n\nclass ClassName:\n    \"\"\"Class docstring with description.\n\n    Attributes:\n        attribute_name: Description of attribute\n    \"\"\"\n\n    def __init__(self, param: str) -> None:\n        \"\"\"Initialize class with parameters.\n\n        Args:\n            param: Description of parameter\n        \"\"\"\n        self.attribute = param\n\n    def method_name(self, arg: str) -> str:\n        \"\"\"Method with clear docstring.\n\n        Args:\n            arg: Description of argument\n\n        Returns:\n            Description of return value\n\n        Raises:\n            ValueError: When something goes wrong\n        \"\"\"\n        return f\"Result: {arg}\"\n\n\ndef function_name(param: str, optional: bool = False) -> dict:\n    \"\"\"Function with clear documentation.\n\n    Args:\n        param: Description of parameter\n        optional: Optional parameter description\n\n    Returns:\n        Dictionary with results\n\n    Example:\n        >>> function_name(\"test\")\n        {'result': 'test'}\n    \"\"\"\n    return {\"result\": param}\n```\n\n### Documentation Standards\n\n- Use Google-style docstrings\n- Document all public APIs\n- Include usage examples\n- Keep documentation up-to-date with code\n- Add inline comments for complex logic\n\n## Commit Message Guidelines\n\nWe follow [Conventional Commits](https://www.conventionalcommits.org/):\n\n### Format\n\n```\n<type>(<scope>): <subject>\n\n<body>\n\n<footer>\n```\n\n### Types\n\n- `feat`: New feature\n- `fix`: Bug fix\n- `docs`: Documentation changes\n- `style`: Formatting, missing semi-colons, etc.\n- `refactor`: Code refactoring\n- `test`: Adding or updating tests\n- `chore`: Maintenance tasks\n- `perf`: Performance improvements\n- `ci`: CI/CD changes\n\n### Examples\n\n```bash\n# Feature\ngit commit -m \"feat(auth): add OAuth2 authentication support\"\n\n# Bug fix\ngit commit -m \"fix(api): resolve timeout issue with large payloads\"\n\n# Documentation\ngit commit -m \"docs(readme): update installation instructions\"\n\n# Breaking change\ngit commit -m \"feat(api): redesign user endpoint\n\nBREAKING CHANGE: User endpoint now returns different format\"\n```\n\n### Guidelines\n\n- Use imperative mood (\"add\" not \"added\" or \"adds\")\n- Capitalize first letter of subject\n- No period at the end of subject\n- Limit subject line to 50 characters\n- Wrap body at 72 characters\n- Reference issues/PRs in footer\n\n## Pull Request Process\n\n### Before Submitting\n\n- [ ] All tests pass (`pytest`)\n- [ ] Code is formatted (`black`, `isort`)\n- [ ] Linting passes (`flake8`, `mypy`)\n- [ ] Documentation is updated\n- [ ] CHANGELOG.md is updated\n- [ ] No merge conflicts with main branch\n\n### Creating a Pull Request\n\n1. **Push your branch** to your fork\n2. **Navigate** to the original repository\n3. **Click** \"New Pull Request\"\n4. **Fill out** the PR template completely\n5. **Link** related issues (e.g., \"Closes #123\")\n\n### PR Template\n\n```markdown\n## Description\nBrief description of changes made.\n\n## Type of Change\n- [ ] Bug fix (non-breaking change)\n- [ ] New feature (non-breaking change)\n- [ ] Breaking change (fix or feature that would cause existing functionality to not work)\n- [ ] Documentation update\n\n## Related Issues\nCloses #123\n\n## Testing\nDescribe the tests you ran and how to reproduce them.\n\n## Screenshots\nIf applicable, add screenshots.\n\n## Checklist\n- [ ] My code follows the project's style guidelines\n- [ ] I have performed a self-review\n- [ ] I have commented my code where necessary\n- [ ] I have updated the documentation\n- [ ] My changes generate no new warnings\n- [ ] I have added tests that prove my fix/feature works\n- [ ] New and existing tests pass locally\n- [ ] I have updated CHANGELOG.md\n```\n\n### Review Process\n\n1. Maintainers will review your PR\n2. Address any requested changes\n3. Push updates to the same branch\n4. Once approved, your PR will be merged\n\n**Review criteria:**\n- Code quality and style\n- Test coverage\n- Documentation completeness\n- Performance impact\n- Backward compatibility\n\n## Testing Guidelines\n\n### Test Structure\n\n```python\nimport pytest\nfrom project import Module\n\n\nclass TestModule:\n    \"\"\"Test suite for Module class.\"\"\"\n\n    @pytest.fixture\n    def module(self):\n        \"\"\"Create module instance for testing.\"\"\"\n        return Module(config=\"test\")\n\n    def test_basic_functionality(self, module):\n        \"\"\"Test basic module functionality.\"\"\"\n        result = module.process(\"input\")\n        assert result.success is True\n        assert result.data == \"expected\"\n\n    def test_error_handling(self, module):\n        \"\"\"Test error handling.\"\"\"\n        with pytest.raises(ValueError):\n            module.process(None)\n\n    @pytest.mark.parametrize(\"input,expected\", [\n        (\"test1\", \"result1\"),\n        (\"test2\", \"result2\"),\n    ])\n    def test_multiple_inputs(self, module, input, expected):\n        \"\"\"Test with multiple input values.\"\"\"\n        result = module.process(input)\n        assert result.data == expected\n```\n\n### Test Coverage\n\n- Aim for >90% code coverage\n- Write tests for all new features\n- Test edge cases and error conditions\n- Include integration tests where appropriate\n\n```bash\n# Run with coverage report\npytest --cov=src --cov-report=html --cov-report=term\n\n# View coverage in browser\nopen htmlcov/index.html\n```\n\n## Documentation\n\n### Types of Documentation\n\n1. **Code Documentation**: Docstrings in code\n2. **API Documentation**: Generated from docstrings\n3. **User Guides**: How-to guides and tutorials\n4. **README**: Project overview and quick start\n\n### Building Documentation\n\n```bash\n# Install documentation dependencies\npip install -e \".[docs]\"\n\n# Build documentation\ncd docs\nmake html\n\n# View documentation\nopen _build/html/index.html\n```\n\n### Documentation Style\n\n- Write in clear, simple language\n- Include code examples\n- Use proper Markdown formatting\n- Add screenshots where helpful\n- Keep documentation up-to-date\n\n## Community\n\n### Communication Channels\n\n- **GitHub Issues**: Bug reports and feature requests\n- **GitHub Discussions**: Questions and general discussion\n- **Discord**: [Join our server](https://discord.gg/example)\n- **Email**: [dev@example.com](mailto:dev@example.com)\n\n### Getting Help\n\n- Read the documentation first\n- Search existing issues\n- Ask in GitHub Discussions\n- Join our Discord for real-time help\n\n### Recognition\n\nContributors are recognized in:\n- CONTRIBUTORS.md file\n- Release notes\n- Project README\n- Social media mentions\n\n## Thank You!\n\nYour contributions, no matter how small, are valuable and appreciated. Thank you for helping make this project better!\n\n---\n\n**Questions?** Open an issue or reach out on [Discord](https://discord.gg/example).\n\n**Need help?** Check our [FAQ](FAQ.md) or [documentation](https://docs.example.com).\n",
        "icartsh-plugin/skills/markdown-pro/examples/README_template.md": "# Project Name\n\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\n[![Version](https://img.shields.io/badge/version-1.0.0-green.svg)](https://github.com/username/repo/releases)\n[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org)\n[![Build Status](https://img.shields.io/badge/build-passing-brightgreen.svg)](https://github.com/username/repo/actions)\n[![Coverage](https://img.shields.io/badge/coverage-95%25-brightgreen.svg)](https://codecov.io/gh/username/repo)\n\n> A one-line description of what your project does and why it's awesome.\n\n![Project Screenshot](assets/screenshot.png)\n\n## Table of Contents\n\n- [Features](#features)\n- [Installation](#installation)\n- [Quick Start](#quick-start)\n- [Usage](#usage)\n- [API Reference](#api-reference)\n- [Configuration](#configuration)\n- [Examples](#examples)\n- [Testing](#testing)\n- [Contributing](#contributing)\n- [Roadmap](#roadmap)\n- [License](#license)\n- [Acknowledgments](#acknowledgments)\n\n## Features\n\n- **Feature 1**: Lightning-fast performance with optimized algorithms\n- **Feature 2**: Easy-to-use API with comprehensive documentation\n- **Feature 3**: Cross-platform support (Windows, macOS, Linux)\n- **Feature 4**: Extensive test coverage (>95%)\n- **Feature 5**: Active development and community support\n- Built with modern best practices\n- Zero configuration required for basic usage\n- Extensive plugin ecosystem\n\n## Installation\n\n### Prerequisites\n\n- Python 3.8 or higher\n- pip package manager\n- (Optional) Virtual environment tool\n\n### Install from PyPI\n\n```bash\npip install project-name\n```\n\n### Install from Source\n\n```bash\ngit clone https://github.com/username/repo.git\ncd repo\npip install -e .\n```\n\n### Using Docker\n\n```bash\ndocker pull username/project-name:latest\ndocker run -it username/project-name\n```\n\n## Quick Start\n\nGet up and running in less than 5 minutes:\n\n```python\nfrom project_name import Client\n\n# Initialize the client\nclient = Client(api_key=\"your-api-key\")\n\n# Perform basic operation\nresult = client.process(\"Hello, World!\")\nprint(result)\n```\n\nOutput:\n```json\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"processed\": \"Hello, World!\",\n    \"timestamp\": \"2025-01-15T10:30:00Z\"\n  }\n}\n```\n\n## Usage\n\n### Basic Usage\n\n```python\nfrom project_name import Client, Config\n\n# Configure with custom settings\nconfig = Config(\n    timeout=30,\n    retries=3,\n    verbose=True\n)\n\n# Initialize client with configuration\nclient = Client(config=config)\n\n# Process data\nresult = client.process(\n    data=\"Your input data\",\n    options={\n        \"format\": \"json\",\n        \"validate\": True\n    }\n)\n\n# Handle results\nif result.success:\n    print(f\"Processed: {result.data}\")\nelse:\n    print(f\"Error: {result.error}\")\n```\n\n### Advanced Usage\n\n#### Custom Processors\n\n```python\nfrom project_name import BaseProcessor\n\nclass CustomProcessor(BaseProcessor):\n    def process(self, data):\n        # Your custom processing logic\n        return self.transform(data)\n\n# Use custom processor\nclient = Client(processor=CustomProcessor())\nresult = client.process(data)\n```\n\n#### Batch Processing\n\n```python\n# Process multiple items\nitems = [\"item1\", \"item2\", \"item3\"]\nresults = client.batch_process(items, workers=4)\n\nfor item, result in zip(items, results):\n    print(f\"{item}: {result.status}\")\n```\n\n#### Async Support\n\n```python\nimport asyncio\n\nasync def process_async():\n    client = Client()\n    result = await client.process_async(data)\n    return result\n\n# Run async processing\nresult = asyncio.run(process_async())\n```\n\n## API Reference\n\n### Client\n\nMain client class for interacting with the service.\n\n#### `Client(config=None, api_key=None)`\n\nInitialize a new client instance.\n\n**Parameters:**\n- `config` (Config, optional): Configuration object\n- `api_key` (str, optional): API key for authentication\n\n**Example:**\n```python\nclient = Client(api_key=\"sk-123456\")\n```\n\n#### `process(data, options=None)`\n\nProcess input data with optional configuration.\n\n**Parameters:**\n- `data` (str|dict|list): Input data to process\n- `options` (dict, optional): Processing options\n  - `format` (str): Output format - 'json', 'yaml', 'xml' (default: 'json')\n  - `validate` (bool): Enable validation (default: True)\n  - `verbose` (bool): Enable verbose output (default: False)\n\n**Returns:**\n- `Result`: Processing result object\n\n**Raises:**\n- `ValueError`: If data format is invalid\n- `APIError`: If API request fails\n- `TimeoutError`: If request times out\n\n**Example:**\n```python\nresult = client.process(\n    data={\"key\": \"value\"},\n    options={\"format\": \"json\", \"validate\": True}\n)\n```\n\n### Config\n\nConfiguration class for client settings.\n\n#### `Config(timeout=30, retries=3, verbose=False)`\n\nCreate configuration object.\n\n**Parameters:**\n- `timeout` (int): Request timeout in seconds (default: 30)\n- `retries` (int): Number of retry attempts (default: 3)\n- `verbose` (bool): Enable verbose logging (default: False)\n\n## Configuration\n\n### Environment Variables\n\n```bash\n# API Configuration\nexport PROJECT_API_KEY=\"your-api-key\"\nexport PROJECT_API_URL=\"https://api.example.com\"\n\n# Client Configuration\nexport PROJECT_TIMEOUT=30\nexport PROJECT_RETRIES=3\nexport PROJECT_LOG_LEVEL=\"INFO\"\n```\n\n### Configuration File\n\nCreate a `config.yaml` file:\n\n```yaml\napi:\n  key: \"your-api-key\"\n  url: \"https://api.example.com\"\n  timeout: 30\n\nclient:\n  retries: 3\n  verbose: false\n  cache_enabled: true\n\nprocessing:\n  workers: 4\n  batch_size: 100\n  format: \"json\"\n```\n\nLoad configuration:\n\n```python\nfrom project_name import Client, load_config\n\nconfig = load_config(\"config.yaml\")\nclient = Client(config=config)\n```\n\n## Examples\n\nCheck out the [examples/](examples/) directory for more detailed use cases:\n\n- [Basic Usage](examples/basic_usage.py) - Simple examples to get started\n- [Advanced Features](examples/advanced_usage.py) - Complex scenarios\n- [Custom Processors](examples/custom_processor.py) - Build custom processors\n- [Batch Processing](examples/batch_processing.py) - Handle large datasets\n- [Integration Examples](examples/integrations/) - Third-party integrations\n\n## Testing\n\n### Run Tests\n\n```bash\n# Run all tests\npytest\n\n# Run with coverage\npytest --cov=project_name --cov-report=html\n\n# Run specific test file\npytest tests/test_client.py\n\n# Run with verbose output\npytest -v\n```\n\n### Test Coverage\n\nCurrent test coverage: **95%**\n\n```bash\n# Generate coverage report\npytest --cov=project_name --cov-report=term-missing\n```\n\n## Contributing\n\nWe welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines.\n\n### Quick Contribution Guide\n\n1. Fork the repository\n2. Create a feature branch (`git checkout -b feature/amazing-feature`)\n3. Make your changes\n4. Add tests for new functionality\n5. Ensure all tests pass (`pytest`)\n6. Commit your changes (`git commit -m 'Add amazing feature'`)\n7. Push to the branch (`git push origin feature/amazing-feature`)\n8. Open a Pull Request\n\n### Development Setup\n\n```bash\n# Clone the repository\ngit clone https://github.com/username/repo.git\ncd repo\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install development dependencies\npip install -e \".[dev]\"\n\n# Install pre-commit hooks\npre-commit install\n\n# Run tests\npytest\n```\n\n## Roadmap\n\n- [x] Core functionality\n- [x] API client\n- [x] Documentation\n- [ ] Additional processors\n- [ ] Web interface\n- [ ] Mobile app support\n- [ ] GraphQL API\n- [ ] Real-time streaming\n\nSee the [open issues](https://github.com/username/repo/issues) for a full list of proposed features and known issues.\n\n## Troubleshooting\n\n<details>\n<summary>Common Issues</summary>\n\n### Installation fails with \"No module named...\"\n\nEnsure you have Python 3.8+ installed:\n```bash\npython --version\npip install --upgrade pip\n```\n\n### API authentication errors\n\nCheck your API key is properly set:\n```python\nclient = Client(api_key=\"your-actual-key\")\n```\n\n### Performance issues with large datasets\n\nUse batch processing for better performance:\n```python\nresults = client.batch_process(items, workers=8)\n```\n\n</details>\n\n## FAQ\n\n**Q: Is this production-ready?**\nA: Yes, the project is stable and used in production by several companies.\n\n**Q: What's the pricing model?**\nA: The library is free and open-source. API usage may have separate pricing.\n\n**Q: How do I report bugs?**\nA: Please open an issue on GitHub with detailed reproduction steps.\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\n- Thanks to [@contributor1](https://github.com/contributor1) for the authentication module\n- Thanks to [@contributor2](https://github.com/contributor2) for performance optimizations\n- Inspired by [similar-project](https://github.com/org/similar-project)\n- Built with [Python](https://python.org), [FastAPI](https://fastapi.tiangolo.com), and [other-tool](https://example.com)\n\n## Support\n\n- Documentation: [https://docs.example.com](https://docs.example.com)\n- Issues: [GitHub Issues](https://github.com/username/repo/issues)\n- Discussions: [GitHub Discussions](https://github.com/username/repo/discussions)\n- Email: support@example.com\n- Discord: [Join our server](https://discord.gg/example)\n\n## Citation\n\nIf you use this project in your research, please cite:\n\n```bibtex\n@software{project_name,\n  author = {Your Name},\n  title = {Project Name},\n  year = {2025},\n  url = {https://github.com/username/repo}\n}\n```\n\n---\n\n**Made with ❤️ by [Your Name](https://github.com/username)**\n\n[⬆ Back to top](#project-name)\n",
        "icartsh-plugin/skills/mcp-builder/SKILL.md": "---\nname: mcp-builder\ndescription: LLM이 잘 설계된 도구를 통해 외부 서비스와 상호작용할 수 있게 해주는 고품질 MCP (Model Context Protocol) 서버를 만들기 위한 가이드입니다. Python (FastMCP) 또는 Node/TypeScript (MCP SDK)를 사용하여 외부 API나 서비스를 통합하는 MCP 서버를 구축할 때 사용하세요.\nlicense: LICENSE.txt의 전체 약관 참조\n---\n\n# MCP Server Development Guide\n\n## 개요 (Overview)\n\nLLM이 잘 설계된 도구를 통해 외부 서비스와 상호작용할 수 있게 해주는 MCP (Model Context Protocol) 서버를 생성하세요. MCP 서버의 품질은 LLM이 실제 작업을 얼마나 잘 수행할 수 있게 하는지에 따라 결정됩니다.\n\n---\n\n# 프로세스 (Process)\n\n## 🚀 워크플로우 개요 (High-Level Workflow)\n\n고품질 MCP 서버를 만드는 과정은 크게 네 단계로 나뉩니다:\n\n### Phase 1: 심층 조사 및 계획 (Deep Research and Planning)\n\n#### 1.1 최신 MCP 설계 이해\n\n**API 커버리지 vs. 워크플로우 도구:**\n포괄적인 API 엔드포인트 커버리지와 특화된 워크플로우 도구 사이의 균형을 맞추세요. 워크플로우 도구는 특정 작업에 더 편리할 수 있으며, 포괄적인 커버리지는 에이전트(agent)가 작업을 자유롭게 구성할 수 있는 유연성을 제공합니다. 성능은 클라이언트에 따라 다릅니다—일부 클라이언트는 기본 도구들을 조합하는 코드 실행 방식이 효율적이며, 다른 클라이언트는 상위 수준의 워크플로우 도구가 더 잘 작동합니다. 확실하지 않을 때는 포괄적인 API 커버리지를 우선시하세요.\n\n**도구 명명 및 발견 가능성 (Tool Naming and Discoverability):**\n명확하고 설명적인 도구 이름은 에이전트가 적절한 도구를 빠르게 찾는 데 도움이 됩니다. 일관된 접두사(예: `github_create_issue`, `github_list_repos`)를 사용하고 동작 중심의 이름을 지으세요.\n\n**컨텍스트 관리 (Context Management):**\n에이전트는 간결한 도구 설명과 결과 필터링/페이지네이션 기능이 있을 때 더 효율적으로 작동합니다. 집중적이고 관련성 높은 데이터를 반환하도록 도구를 설계하세요. 일부 클라이언트는 코드 실행을 지원하며, 이는 에이전트가 데이터를 효율적으로 필터링하고 처리하는 데 도움이 됩니다.\n\n**실행 가능한 에러 메시지 (Actionable Error Messages):**\n에러 메시지는 구체적인 제안과 다음 단계를 제시하여 에이전트가 해결책을 찾을 수 있도록 안내해야 합니다.\n\n#### 1.2 MCP 프로토콜 문서 학습\n\n**MCP 사양 탐색:**\n\n먼저 사이트맵에서 관련 페이지를 찾으세요: `https://modelcontextprotocol.io/sitemap.xml`\n\n그 다음, 마크다운 형식을 위해 `.md` 접미사가 붙은 특정 페이지를 가져오세요 (예: `https://modelcontextprotocol.io/specification/draft.md`).\n\n검토해야 할 주요 페이지:\n- 사양 개요 및 아키텍처\n- 전송 메커니즘 (streamable HTTP, stdio)\n- 도구(Tool), 리소스(Resource), 프롬프트(Prompt) 정의\n\n#### 1.3 프레임워크 문서 학습\n\n**권장 스택:**\n- **언어**: TypeScript (고품질 SDK 지원 및 MCPB 등 다양한 실행 환경에서 좋은 호환성 제공. 또한 AI 모델들이 광범위한 사용량, 정적 타이핑 및 우수한 린팅 도구 덕분에 TypeScript 코드를 생성하는 데 능숙함)\n- **전송(Transport)**: 원격 서버의 경우 상태 비저장(stateless) JSON을 사용하는 Streamable HTTP (상태 저장 세션 및 스트리밍 응답에 비해 확장 및 유지보수가 간단함). 로컬 서버의 경우 stdio 사용.\n\n**프레임워크 문서 로드:**\n\n- **MCP Best Practices**: [📋 Best Practices 보기](./reference/mcp_best_practices.md) - 핵심 가이드라인\n\n**TypeScript용 (권장):**\n- **TypeScript SDK**: WebFetch를 사용하여 `https://raw.githubusercontent.com/modelcontextprotocol/typescript-sdk/main/README.md` 로드\n- [⚡ TypeScript 가이드](./reference/node_mcp_server.md) - TypeScript 패턴 및 예시\n\n**Python용:**\n- **Python SDK**: WebFetch를 사용하여 `https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/README.md` 로드\n- [🐍 Python 가이드](./reference/python_mcp_server.md) - Python 패턴 및 예시\n\n#### 1.4 구현 계획 수립\n\n**API 이해:**\n핵심 엔드포인트, 인증 요구 사항 및 데이터 모델을 식별하기 위해 서비스의 API 문서를 검토하세요. 필요에 따라 웹 검색과 WebFetch를 활용하세요.\n\n**도구 선택:**\n포괄적인 API 커버리지를 우선시하세요. 가장 일반적인 작업부터 시작하여 구현할 엔드포인트 목록을 만듭니다.\n\n---\n\n### Phase 2: 구현 (Implementation)\n\n#### 2.1 프로젝트 구조 설정\n\n프로젝트 설정에 대해서는 언어별 가이드를 참조하세요:\n- [⚡ TypeScript 가이드](./reference/node_mcp_server.md) - 프로젝트 구조, package.json, tsconfig.json\n- [🐍 Python 가이드](./reference/python_mcp_server.md) - 모듈 조직, 종속성\n\n#### 2.2 핵심 인프라 구현\n\n공용 유틸리티 생성:\n- 인증 기능이 포함된 API 클라이언트\n- 에러 핸들링 헬퍼\n- 응답 포맷팅 (JSON/Markdown)\n- 페이지네이션 지원\n\n#### 2.3 도구 구현 (Implement Tools)\n\n각 도구별로 다음을 수행하세요:\n\n**입력 스키마 (Input Schema):**\n- Zod (TypeScript) 또는 Pydantic (Python) 사용\n- 제약 조건과 명확한 설명 포함\n- 필드 설명에 예시 추가\n\n**출력 스키마 (Output Schema):**\n- 구조화된 데이터를 위해 가능한 경우 `outputSchema` 정의\n- 도구 응답에서 `structuredContent` 사용 (TypeScript SDK 기능)\n- 클라이언트가 도구 출력을 이해하고 처리하는 데 도움을 줌\n\n**도구 설명 (Tool Description):**\n- 기능에 대한 간결한 요약\n- 파라미터 설명\n- 반환 타입 스키마\n\n**구현:**\n- I/O 작업을 위한 Async/await 사용\n- 실행 가능한 메시지를 포함한 적절한 에러 핸들링\n- 해당되는 경우 페이지네이션 지원\n- 최신 SDK 사용 시 텍스트 콘텐츠와 구조화된 데이터 모두 반환\n\n**어노테이션 (Annotations):**\n- `readOnlyHint`: true/false\n- `destructiveHint`: true/false\n- `idempotentHint`: true/false\n- `openWorldHint`: true/false\n\n---\n\n### Phase 3: 검토 및 테스트 (Review and Test)\n\n#### 3.1 코드 품질\n\n다음을 검토하세요:\n- 중복 코드 없음 (DRY 원칙)\n- 일관된 에러 핸들링\n- 완전한 타입 커버리지\n- 명확한 도구 설명\n\n#### 3.2 빌드 및 테스트\n\n**TypeScript:**\n- 컴파일 확인을 위해 `npm run build` 실행\n- MCP Inspector로 테스트: `npx @modelcontextprotocol/inspector`\n\n**Python:**\n- 구문 확인: `python -m py_compile your_server.py`\n- MCP Inspector로 테스트\n\n상세한 테스트 접근 방식과 품질 체크리스트는 언어별 가이드를 참조하세요.\n\n---\n\n### Phase 4: 평가 생성 (Create Evaluations)\n\nMCP 서버를 구현한 후, 그 효과를 테스트하기 위해 포괄적인 평가(evaluations)를 만드세요.\n\n**전체 평가 가이드라인을 위해 [✅ Evaluation 가이드](./reference/evaluation.md)를 로드하세요.**\n\n#### 4.1 평가 목적 이해\n\n평가를 통해 LLM이 실제적이고 복잡한 질문에 답하기 위해 당신의 MCP 서버를 효과적으로 사용할 수 있는지 테스트합니다.\n\n#### 4.2 10개의 평가 질문 작성\n\n효과적인 평가를 위해 가이드에 설명된 프로세스를 따르세요:\n\n1. **도구 점검 (Tool Inspection)**: 사용 가능한 도구를 나열하고 기능을 이해합니다.\n2. **콘텐츠 탐색 (Content Exploration)**: 읽기 전용(READ-ONLY) 작업을 사용하여 사용 가능한 데이터 탐색\n3. **질문 생성**: 10개의 복잡하고 실제적인 질문 생성\n4. **답변 검증**: 직접 질문을 해결하여 답변 확인\n\n#### 4.3 평가 요구 사항\n\n각 질문이 다음을 충족하는지 확인하세요:\n- **독립성 (Independent)**: 다른 질문에 의존하지 않음\n- **읽기 전용 (Read-only)**: 비파괴적인 작업만 필요함\n- **복잡성 (Complex)**: 여러 도구 호출 및 심층적인 탐색 필요\n- **실제성 (Realistic)**: 인간이 관심을 가질 만한 실제 유스케이스 기반\n- **검증 가능성 (Verifiable)**: 문자열 비교로 검증 가능한 명확한 단일 답변\n- **안정성 (Stable)**: 답변이 시간이 지나도 변하지 않음\n\n#### 4.4 출력 형식\n\n다음 구조의 XML 파일을 생성하세요:\n\n```xml\n<evaluation>\n  <qa_pair>\n    <question>동물 코드명을 가진 AI 모델 출시에 관한 토론을 찾으세요. 한 모델은 ASL-X 형식을 사용하는 특정 안전 지정이 필요했습니다. 점박이 야생 고양이의 이름을 딴 모델에 대해 결정된 숫자 X는 무엇입니까?</question>\n    <answer>3</answer>\n  </qa_pair>\n<!-- 더 많은 qa_pair... -->\n</evaluation>\n```\n\n---\n\n# 참조 파일 (Reference Files)\n\n## 📚 문서 라이브러리 (Documentation Library)\n\n개발 중에 필요에 따라 다음 리소스를 로드하세요:\n\n### 핵심 MCP 문서 (가장 먼저 로드)\n- **MCP Protocol**: `https://modelcontextprotocol.io/sitemap.xml`의 사이트맵부터 시작하여 `.md` 접미사가 붙은 특정 페이지를 가져오세요.\n- [📋 MCP Best Practices](./reference/mcp_best_practices.md) - 다음을 포함한 범용 MCP 가이드라인:\n  - 서버 및 도구 명명 규칙\n  - 응답 형식 가이드라인 (JSON vs Markdown)\n  - 페이지네이션 모범 사례\n  - 전송 방식 선택 (streamable HTTP vs stdio)\n  - 보안 및 에러 핸들링 표준\n\n### SDK 문서 (Phase 1/2 중에 로드)\n- **Python SDK**: `https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/README.md`에서 가져오기\n- **TypeScript SDK**: `https://raw.githubusercontent.com/modelcontextprotocol/typescript-sdk/main/README.md`에서 가져오기\n\n### 언어별 구현 가이드 (Phase 2 중에 로드)\n- [🐍 Python 구현 가이드](./reference/python_mcp_server.md) - 다음을 포함한 전체 Python/FastMCP 가이드:\n  - 서버 초기화 패턴\n  - Pydantic 모델 예시\n  - `@mcp.tool`을 이용한 도구 등록\n  - 전체 작동 예시 코드\n  - 품질 체크리스트\n\n- [⚡ TypeScript 구현 가이드](./reference/node_mcp_server.md) - 다음을 포함한 전체 TypeScript 가이드:\n  - 프로젝트 구조\n  - Zod 스키마 패턴\n  - `server.registerTool`을 이용한 도구 등록\n  - 전체 작동 예시 코드\n  - 품질 체크리스트\n\n### 평가 가이드 (Phase 4 중에 로드)\n- [✅ Evaluation 가이드](./reference/evaluation.md) - 다음을 포함한 전체 평가 생성 가이드:\n  - 질문 작성 가이드라인\n  - 답변 검증 전략\n  - XML 형식 사양\n  - 질문 및 답변 예시\n  - 제공된 스크립트를 이용한 평가 실행 방법\n",
        "icartsh-plugin/skills/mcp-builder/reference/evaluation.md": "# MCP Server Evaluation Guide\n\n## Overview\n\nThis document provides guidance on creating comprehensive evaluations for MCP servers. Evaluations test whether LLMs can effectively use your MCP server to answer realistic, complex questions using only the tools provided.\n\n---\n\n## Quick Reference\n\n### Evaluation Requirements\n- Create 10 human-readable questions\n- Questions must be READ-ONLY, INDEPENDENT, NON-DESTRUCTIVE\n- Each question requires multiple tool calls (potentially dozens)\n- Answers must be single, verifiable values\n- Answers must be STABLE (won't change over time)\n\n### Output Format\n```xml\n<evaluation>\n   <qa_pair>\n      <question>Your question here</question>\n      <answer>Single verifiable answer</answer>\n   </qa_pair>\n</evaluation>\n```\n\n---\n\n## Purpose of Evaluations\n\nThe measure of quality of an MCP server is NOT how well or comprehensively the server implements tools, but how well these implementations (input/output schemas, docstrings/descriptions, functionality) enable LLMs with no other context and access ONLY to the MCP servers to answer realistic and difficult questions.\n\n## Evaluation Overview\n\nCreate 10 human-readable questions requiring ONLY READ-ONLY, INDEPENDENT, NON-DESTRUCTIVE, and IDEMPOTENT operations to answer. Each question should be:\n- Realistic\n- Clear and concise\n- Unambiguous\n- Complex, requiring potentially dozens of tool calls or steps\n- Answerable with a single, verifiable value that you identify in advance\n\n## Question Guidelines\n\n### Core Requirements\n\n1. **Questions MUST be independent**\n   - Each question should NOT depend on the answer to any other question\n   - Should not assume prior write operations from processing another question\n\n2. **Questions MUST require ONLY NON-DESTRUCTIVE AND IDEMPOTENT tool use**\n   - Should not instruct or require modifying state to arrive at the correct answer\n\n3. **Questions must be REALISTIC, CLEAR, CONCISE, and COMPLEX**\n   - Must require another LLM to use multiple (potentially dozens of) tools or steps to answer\n\n### Complexity and Depth\n\n4. **Questions must require deep exploration**\n   - Consider multi-hop questions requiring multiple sub-questions and sequential tool calls\n   - Each step should benefit from information found in previous questions\n\n5. **Questions may require extensive paging**\n   - May need paging through multiple pages of results\n   - May require querying old data (1-2 years out-of-date) to find niche information\n   - The questions must be DIFFICULT\n\n6. **Questions must require deep understanding**\n   - Rather than surface-level knowledge\n   - May pose complex ideas as True/False questions requiring evidence\n   - May use multiple-choice format where LLM must search different hypotheses\n\n7. **Questions must not be solvable with straightforward keyword search**\n   - Do not include specific keywords from the target content\n   - Use synonyms, related concepts, or paraphrases\n   - Require multiple searches, analyzing multiple related items, extracting context, then deriving the answer\n\n### Tool Testing\n\n8. **Questions should stress-test tool return values**\n   - May elicit tools returning large JSON objects or lists, overwhelming the LLM\n   - Should require understanding multiple modalities of data:\n     - IDs and names\n     - Timestamps and datetimes (months, days, years, seconds)\n     - File IDs, names, extensions, and mimetypes\n     - URLs, GIDs, etc.\n   - Should probe the tool's ability to return all useful forms of data\n\n9. **Questions should MOSTLY reflect real human use cases**\n   - The kinds of information retrieval tasks that HUMANS assisted by an LLM would care about\n\n10. **Questions may require dozens of tool calls**\n    - This challenges LLMs with limited context\n    - Encourages MCP server tools to reduce information returned\n\n11. **Include ambiguous questions**\n    - May be ambiguous OR require difficult decisions on which tools to call\n    - Force the LLM to potentially make mistakes or misinterpret\n    - Ensure that despite AMBIGUITY, there is STILL A SINGLE VERIFIABLE ANSWER\n\n### Stability\n\n12. **Questions must be designed so the answer DOES NOT CHANGE**\n    - Do not ask questions that rely on \"current state\" which is dynamic\n    - For example, do not count:\n      - Number of reactions to a post\n      - Number of replies to a thread\n      - Number of members in a channel\n\n13. **DO NOT let the MCP server RESTRICT the kinds of questions you create**\n    - Create challenging and complex questions\n    - Some may not be solvable with the available MCP server tools\n    - Questions may require specific output formats (datetime vs. epoch time, JSON vs. MARKDOWN)\n    - Questions may require dozens of tool calls to complete\n\n## Answer Guidelines\n\n### Verification\n\n1. **Answers must be VERIFIABLE via direct string comparison**\n   - If the answer can be re-written in many formats, clearly specify the output format in the QUESTION\n   - Examples: \"Use YYYY/MM/DD.\", \"Respond True or False.\", \"Answer A, B, C, or D and nothing else.\"\n   - Answer should be a single VERIFIABLE value such as:\n     - User ID, user name, display name, first name, last name\n     - Channel ID, channel name\n     - Message ID, string\n     - URL, title\n     - Numerical quantity\n     - Timestamp, datetime\n     - Boolean (for True/False questions)\n     - Email address, phone number\n     - File ID, file name, file extension\n     - Multiple choice answer\n   - Answers must not require special formatting or complex, structured output\n   - Answer will be verified using DIRECT STRING COMPARISON\n\n### Readability\n\n2. **Answers should generally prefer HUMAN-READABLE formats**\n   - Examples: names, first name, last name, datetime, file name, message string, URL, yes/no, true/false, a/b/c/d\n   - Rather than opaque IDs (though IDs are acceptable)\n   - The VAST MAJORITY of answers should be human-readable\n\n### Stability\n\n3. **Answers must be STABLE/STATIONARY**\n   - Look at old content (e.g., conversations that have ended, projects that have launched, questions answered)\n   - Create QUESTIONS based on \"closed\" concepts that will always return the same answer\n   - Questions may ask to consider a fixed time window to insulate from non-stationary answers\n   - Rely on context UNLIKELY to change\n   - Example: if finding a paper name, be SPECIFIC enough so answer is not confused with papers published later\n\n4. **Answers must be CLEAR and UNAMBIGUOUS**\n   - Questions must be designed so there is a single, clear answer\n   - Answer can be derived from using the MCP server tools\n\n### Diversity\n\n5. **Answers must be DIVERSE**\n   - Answer should be a single VERIFIABLE value in diverse modalities and formats\n   - User concept: user ID, user name, display name, first name, last name, email address, phone number\n   - Channel concept: channel ID, channel name, channel topic\n   - Message concept: message ID, message string, timestamp, month, day, year\n\n6. **Answers must NOT be complex structures**\n   - Not a list of values\n   - Not a complex object\n   - Not a list of IDs or strings\n   - Not natural language text\n   - UNLESS the answer can be straightforwardly verified using DIRECT STRING COMPARISON\n   - And can be realistically reproduced\n   - It should be unlikely that an LLM would return the same list in any other order or format\n\n## Evaluation Process\n\n### Step 1: Documentation Inspection\n\nRead the documentation of the target API to understand:\n- Available endpoints and functionality\n- If ambiguity exists, fetch additional information from the web\n- Parallelize this step AS MUCH AS POSSIBLE\n- Ensure each subagent is ONLY examining documentation from the file system or on the web\n\n### Step 2: Tool Inspection\n\nList the tools available in the MCP server:\n- Inspect the MCP server directly\n- Understand input/output schemas, docstrings, and descriptions\n- WITHOUT calling the tools themselves at this stage\n\n### Step 3: Developing Understanding\n\nRepeat steps 1 & 2 until you have a good understanding:\n- Iterate multiple times\n- Think about the kinds of tasks you want to create\n- Refine your understanding\n- At NO stage should you READ the code of the MCP server implementation itself\n- Use your intuition and understanding to create reasonable, realistic, but VERY challenging tasks\n\n### Step 4: Read-Only Content Inspection\n\nAfter understanding the API and tools, USE the MCP server tools:\n- Inspect content using READ-ONLY and NON-DESTRUCTIVE operations ONLY\n- Goal: identify specific content (e.g., users, channels, messages, projects, tasks) for creating realistic questions\n- Should NOT call any tools that modify state\n- Will NOT read the code of the MCP server implementation itself\n- Parallelize this step with individual sub-agents pursuing independent explorations\n- Ensure each subagent is only performing READ-ONLY, NON-DESTRUCTIVE, and IDEMPOTENT operations\n- BE CAREFUL: SOME TOOLS may return LOTS OF DATA which would cause you to run out of CONTEXT\n- Make INCREMENTAL, SMALL, AND TARGETED tool calls for exploration\n- In all tool call requests, use the `limit` parameter to limit results (<10)\n- Use pagination\n\n### Step 5: Task Generation\n\nAfter inspecting the content, create 10 human-readable questions:\n- An LLM should be able to answer these with the MCP server\n- Follow all question and answer guidelines above\n\n## Output Format\n\nEach QA pair consists of a question and an answer. The output should be an XML file with this structure:\n\n```xml\n<evaluation>\n   <qa_pair>\n      <question>Find the project created in Q2 2024 with the highest number of completed tasks. What is the project name?</question>\n      <answer>Website Redesign</answer>\n   </qa_pair>\n   <qa_pair>\n      <question>Search for issues labeled as \"bug\" that were closed in March 2024. Which user closed the most issues? Provide their username.</question>\n      <answer>sarah_dev</answer>\n   </qa_pair>\n   <qa_pair>\n      <question>Look for pull requests that modified files in the /api directory and were merged between January 1 and January 31, 2024. How many different contributors worked on these PRs?</question>\n      <answer>7</answer>\n   </qa_pair>\n   <qa_pair>\n      <question>Find the repository with the most stars that was created before 2023. What is the repository name?</question>\n      <answer>data-pipeline</answer>\n   </qa_pair>\n</evaluation>\n```\n\n## Evaluation Examples\n\n### Good Questions\n\n**Example 1: Multi-hop question requiring deep exploration (GitHub MCP)**\n```xml\n<qa_pair>\n   <question>Find the repository that was archived in Q3 2023 and had previously been the most forked project in the organization. What was the primary programming language used in that repository?</question>\n   <answer>Python</answer>\n</qa_pair>\n```\n\nThis question is good because:\n- Requires multiple searches to find archived repositories\n- Needs to identify which had the most forks before archival\n- Requires examining repository details for the language\n- Answer is a simple, verifiable value\n- Based on historical (closed) data that won't change\n\n**Example 2: Requires understanding context without keyword matching (Project Management MCP)**\n```xml\n<qa_pair>\n   <question>Locate the initiative focused on improving customer onboarding that was completed in late 2023. The project lead created a retrospective document after completion. What was the lead's role title at that time?</question>\n   <answer>Product Manager</answer>\n</qa_pair>\n```\n\nThis question is good because:\n- Doesn't use specific project name (\"initiative focused on improving customer onboarding\")\n- Requires finding completed projects from specific timeframe\n- Needs to identify the project lead and their role\n- Requires understanding context from retrospective documents\n- Answer is human-readable and stable\n- Based on completed work (won't change)\n\n**Example 3: Complex aggregation requiring multiple steps (Issue Tracker MCP)**\n```xml\n<qa_pair>\n   <question>Among all bugs reported in January 2024 that were marked as critical priority, which assignee resolved the highest percentage of their assigned bugs within 48 hours? Provide the assignee's username.</question>\n   <answer>alex_eng</answer>\n</qa_pair>\n```\n\nThis question is good because:\n- Requires filtering bugs by date, priority, and status\n- Needs to group by assignee and calculate resolution rates\n- Requires understanding timestamps to determine 48-hour windows\n- Tests pagination (potentially many bugs to process)\n- Answer is a single username\n- Based on historical data from specific time period\n\n**Example 4: Requires synthesis across multiple data types (CRM MCP)**\n```xml\n<qa_pair>\n   <question>Find the account that upgraded from the Starter to Enterprise plan in Q4 2023 and had the highest annual contract value. What industry does this account operate in?</question>\n   <answer>Healthcare</answer>\n</qa_pair>\n```\n\nThis question is good because:\n- Requires understanding subscription tier changes\n- Needs to identify upgrade events in specific timeframe\n- Requires comparing contract values\n- Must access account industry information\n- Answer is simple and verifiable\n- Based on completed historical transactions\n\n### Poor Questions\n\n**Example 1: Answer changes over time**\n```xml\n<qa_pair>\n   <question>How many open issues are currently assigned to the engineering team?</question>\n   <answer>47</answer>\n</qa_pair>\n```\n\nThis question is poor because:\n- The answer will change as issues are created, closed, or reassigned\n- Not based on stable/stationary data\n- Relies on \"current state\" which is dynamic\n\n**Example 2: Too easy with keyword search**\n```xml\n<qa_pair>\n   <question>Find the pull request with title \"Add authentication feature\" and tell me who created it.</question>\n   <answer>developer123</answer>\n</qa_pair>\n```\n\nThis question is poor because:\n- Can be solved with a straightforward keyword search for exact title\n- Doesn't require deep exploration or understanding\n- No synthesis or analysis needed\n\n**Example 3: Ambiguous answer format**\n```xml\n<qa_pair>\n   <question>List all the repositories that have Python as their primary language.</question>\n   <answer>repo1, repo2, repo3, data-pipeline, ml-tools</answer>\n</qa_pair>\n```\n\nThis question is poor because:\n- Answer is a list that could be returned in any order\n- Difficult to verify with direct string comparison\n- LLM might format differently (JSON array, comma-separated, newline-separated)\n- Better to ask for a specific aggregate (count) or superlative (most stars)\n\n## Verification Process\n\nAfter creating evaluations:\n\n1. **Examine the XML file** to understand the schema\n2. **Load each task instruction** and in parallel using the MCP server and tools, identify the correct answer by attempting to solve the task YOURSELF\n3. **Flag any operations** that require WRITE or DESTRUCTIVE operations\n4. **Accumulate all CORRECT answers** and replace any incorrect answers in the document\n5. **Remove any `<qa_pair>`** that require WRITE or DESTRUCTIVE operations\n\nRemember to parallelize solving tasks to avoid running out of context, then accumulate all answers and make changes to the file at the end.\n\n## Tips for Creating Quality Evaluations\n\n1. **Think Hard and Plan Ahead** before generating tasks\n2. **Parallelize Where Opportunity Arises** to speed up the process and manage context\n3. **Focus on Realistic Use Cases** that humans would actually want to accomplish\n4. **Create Challenging Questions** that test the limits of the MCP server's capabilities\n5. **Ensure Stability** by using historical data and closed concepts\n6. **Verify Answers** by solving the questions yourself using the MCP server tools\n7. **Iterate and Refine** based on what you learn during the process\n\n---\n\n# Running Evaluations\n\nAfter creating your evaluation file, you can use the provided evaluation harness to test your MCP server.\n\n## Setup\n\n1. **Install Dependencies**\n\n   ```bash\n   pip install -r scripts/requirements.txt\n   ```\n\n   Or install manually:\n   ```bash\n   pip install anthropic mcp\n   ```\n\n2. **Set API Key**\n\n   ```bash\n   export ANTHROPIC_API_KEY=your_api_key_here\n   ```\n\n## Evaluation File Format\n\nEvaluation files use XML format with `<qa_pair>` elements:\n\n```xml\n<evaluation>\n   <qa_pair>\n      <question>Find the project created in Q2 2024 with the highest number of completed tasks. What is the project name?</question>\n      <answer>Website Redesign</answer>\n   </qa_pair>\n   <qa_pair>\n      <question>Search for issues labeled as \"bug\" that were closed in March 2024. Which user closed the most issues? Provide their username.</question>\n      <answer>sarah_dev</answer>\n   </qa_pair>\n</evaluation>\n```\n\n## Running Evaluations\n\nThe evaluation script (`scripts/evaluation.py`) supports three transport types:\n\n**Important:**\n- **stdio transport**: The evaluation script automatically launches and manages the MCP server process for you. Do not run the server manually.\n- **sse/http transports**: You must start the MCP server separately before running the evaluation. The script connects to the already-running server at the specified URL.\n\n### 1. Local STDIO Server\n\nFor locally-run MCP servers (script launches the server automatically):\n\n```bash\npython scripts/evaluation.py \\\n  -t stdio \\\n  -c python \\\n  -a my_mcp_server.py \\\n  evaluation.xml\n```\n\nWith environment variables:\n```bash\npython scripts/evaluation.py \\\n  -t stdio \\\n  -c python \\\n  -a my_mcp_server.py \\\n  -e API_KEY=abc123 \\\n  -e DEBUG=true \\\n  evaluation.xml\n```\n\n### 2. Server-Sent Events (SSE)\n\nFor SSE-based MCP servers (you must start the server first):\n\n```bash\npython scripts/evaluation.py \\\n  -t sse \\\n  -u https://example.com/mcp \\\n  -H \"Authorization: Bearer token123\" \\\n  -H \"X-Custom-Header: value\" \\\n  evaluation.xml\n```\n\n### 3. HTTP (Streamable HTTP)\n\nFor HTTP-based MCP servers (you must start the server first):\n\n```bash\npython scripts/evaluation.py \\\n  -t http \\\n  -u https://example.com/mcp \\\n  -H \"Authorization: Bearer token123\" \\\n  evaluation.xml\n```\n\n## Command-Line Options\n\n```\nusage: evaluation.py [-h] [-t {stdio,sse,http}] [-m MODEL] [-c COMMAND]\n                     [-a ARGS [ARGS ...]] [-e ENV [ENV ...]] [-u URL]\n                     [-H HEADERS [HEADERS ...]] [-o OUTPUT]\n                     eval_file\n\npositional arguments:\n  eval_file             Path to evaluation XML file\n\noptional arguments:\n  -h, --help            Show help message\n  -t, --transport       Transport type: stdio, sse, or http (default: stdio)\n  -m, --model           Claude model to use (default: claude-3-7-sonnet-20250219)\n  -o, --output          Output file for report (default: print to stdout)\n\nstdio options:\n  -c, --command         Command to run MCP server (e.g., python, node)\n  -a, --args            Arguments for the command (e.g., server.py)\n  -e, --env             Environment variables in KEY=VALUE format\n\nsse/http options:\n  -u, --url             MCP server URL\n  -H, --header          HTTP headers in 'Key: Value' format\n```\n\n## Output\n\nThe evaluation script generates a detailed report including:\n\n- **Summary Statistics**:\n  - Accuracy (correct/total)\n  - Average task duration\n  - Average tool calls per task\n  - Total tool calls\n\n- **Per-Task Results**:\n  - Prompt and expected response\n  - Actual response from the agent\n  - Whether the answer was correct (✅/❌)\n  - Duration and tool call details\n  - Agent's summary of its approach\n  - Agent's feedback on the tools\n\n### Save Report to File\n\n```bash\npython scripts/evaluation.py \\\n  -t stdio \\\n  -c python \\\n  -a my_server.py \\\n  -o evaluation_report.md \\\n  evaluation.xml\n```\n\n## Complete Example Workflow\n\nHere's a complete example of creating and running an evaluation:\n\n1. **Create your evaluation file** (`my_evaluation.xml`):\n\n```xml\n<evaluation>\n   <qa_pair>\n      <question>Find the user who created the most issues in January 2024. What is their username?</question>\n      <answer>alice_developer</answer>\n   </qa_pair>\n   <qa_pair>\n      <question>Among all pull requests merged in Q1 2024, which repository had the highest number? Provide the repository name.</question>\n      <answer>backend-api</answer>\n   </qa_pair>\n   <qa_pair>\n      <question>Find the project that was completed in December 2023 and had the longest duration from start to finish. How many days did it take?</question>\n      <answer>127</answer>\n   </qa_pair>\n</evaluation>\n```\n\n2. **Install dependencies**:\n\n```bash\npip install -r scripts/requirements.txt\nexport ANTHROPIC_API_KEY=your_api_key\n```\n\n3. **Run evaluation**:\n\n```bash\npython scripts/evaluation.py \\\n  -t stdio \\\n  -c python \\\n  -a github_mcp_server.py \\\n  -e GITHUB_TOKEN=ghp_xxx \\\n  -o github_eval_report.md \\\n  my_evaluation.xml\n```\n\n4. **Review the report** in `github_eval_report.md` to:\n   - See which questions passed/failed\n   - Read the agent's feedback on your tools\n   - Identify areas for improvement\n   - Iterate on your MCP server design\n\n## Troubleshooting\n\n### Connection Errors\n\nIf you get connection errors:\n- **STDIO**: Verify the command and arguments are correct\n- **SSE/HTTP**: Check the URL is accessible and headers are correct\n- Ensure any required API keys are set in environment variables or headers\n\n### Low Accuracy\n\nIf many evaluations fail:\n- Review the agent's feedback for each task\n- Check if tool descriptions are clear and comprehensive\n- Verify input parameters are well-documented\n- Consider whether tools return too much or too little data\n- Ensure error messages are actionable\n\n### Timeout Issues\n\nIf tasks are timing out:\n- Use a more capable model (e.g., `claude-3-7-sonnet-20250219`)\n- Check if tools are returning too much data\n- Verify pagination is working correctly\n- Consider simplifying complex questions",
        "icartsh-plugin/skills/mcp-builder/reference/mcp_best_practices.md": "# MCP Server Best Practices\n\n## Quick Reference\n\n### Server Naming\n- **Python**: `{service}_mcp` (e.g., `slack_mcp`)\n- **Node/TypeScript**: `{service}-mcp-server` (e.g., `slack-mcp-server`)\n\n### Tool Naming\n- Use snake_case with service prefix\n- Format: `{service}_{action}_{resource}`\n- Example: `slack_send_message`, `github_create_issue`\n\n### Response Formats\n- Support both JSON and Markdown formats\n- JSON for programmatic processing\n- Markdown for human readability\n\n### Pagination\n- Always respect `limit` parameter\n- Return `has_more`, `next_offset`, `total_count`\n- Default to 20-50 items\n\n### Transport\n- **Streamable HTTP**: For remote servers, multi-client scenarios\n- **stdio**: For local integrations, command-line tools\n- Avoid SSE (deprecated in favor of streamable HTTP)\n\n---\n\n## Server Naming Conventions\n\nFollow these standardized naming patterns:\n\n**Python**: Use format `{service}_mcp` (lowercase with underscores)\n- Examples: `slack_mcp`, `github_mcp`, `jira_mcp`\n\n**Node/TypeScript**: Use format `{service}-mcp-server` (lowercase with hyphens)\n- Examples: `slack-mcp-server`, `github-mcp-server`, `jira-mcp-server`\n\nThe name should be general, descriptive of the service being integrated, easy to infer from the task description, and without version numbers.\n\n---\n\n## Tool Naming and Design\n\n### Tool Naming\n\n1. **Use snake_case**: `search_users`, `create_project`, `get_channel_info`\n2. **Include service prefix**: Anticipate that your MCP server may be used alongside other MCP servers\n   - Use `slack_send_message` instead of just `send_message`\n   - Use `github_create_issue` instead of just `create_issue`\n3. **Be action-oriented**: Start with verbs (get, list, search, create, etc.)\n4. **Be specific**: Avoid generic names that could conflict with other servers\n\n### Tool Design\n\n- Tool descriptions must narrowly and unambiguously describe functionality\n- Descriptions must precisely match actual functionality\n- Provide tool annotations (readOnlyHint, destructiveHint, idempotentHint, openWorldHint)\n- Keep tool operations focused and atomic\n\n---\n\n## Response Formats\n\nAll tools that return data should support multiple formats:\n\n### JSON Format (`response_format=\"json\"`)\n- Machine-readable structured data\n- Include all available fields and metadata\n- Consistent field names and types\n- Use for programmatic processing\n\n### Markdown Format (`response_format=\"markdown\"`, typically default)\n- Human-readable formatted text\n- Use headers, lists, and formatting for clarity\n- Convert timestamps to human-readable format\n- Show display names with IDs in parentheses\n- Omit verbose metadata\n\n---\n\n## Pagination\n\nFor tools that list resources:\n\n- **Always respect the `limit` parameter**\n- **Implement pagination**: Use `offset` or cursor-based pagination\n- **Return pagination metadata**: Include `has_more`, `next_offset`/`next_cursor`, `total_count`\n- **Never load all results into memory**: Especially important for large datasets\n- **Default to reasonable limits**: 20-50 items is typical\n\nExample pagination response:\n```json\n{\n  \"total\": 150,\n  \"count\": 20,\n  \"offset\": 0,\n  \"items\": [...],\n  \"has_more\": true,\n  \"next_offset\": 20\n}\n```\n\n---\n\n## Transport Options\n\n### Streamable HTTP\n\n**Best for**: Remote servers, web services, multi-client scenarios\n\n**Characteristics**:\n- Bidirectional communication over HTTP\n- Supports multiple simultaneous clients\n- Can be deployed as a web service\n- Enables server-to-client notifications\n\n**Use when**:\n- Serving multiple clients simultaneously\n- Deploying as a cloud service\n- Integration with web applications\n\n### stdio\n\n**Best for**: Local integrations, command-line tools\n\n**Characteristics**:\n- Standard input/output stream communication\n- Simple setup, no network configuration needed\n- Runs as a subprocess of the client\n\n**Use when**:\n- Building tools for local development environments\n- Integrating with desktop applications\n- Single-user, single-session scenarios\n\n**Note**: stdio servers should NOT log to stdout (use stderr for logging)\n\n### Transport Selection\n\n| Criterion | stdio | Streamable HTTP |\n|-----------|-------|-----------------|\n| **Deployment** | Local | Remote |\n| **Clients** | Single | Multiple |\n| **Complexity** | Low | Medium |\n| **Real-time** | No | Yes |\n\n---\n\n## Security Best Practices\n\n### Authentication and Authorization\n\n**OAuth 2.1**:\n- Use secure OAuth 2.1 with certificates from recognized authorities\n- Validate access tokens before processing requests\n- Only accept tokens specifically intended for your server\n\n**API Keys**:\n- Store API keys in environment variables, never in code\n- Validate keys on server startup\n- Provide clear error messages when authentication fails\n\n### Input Validation\n\n- Sanitize file paths to prevent directory traversal\n- Validate URLs and external identifiers\n- Check parameter sizes and ranges\n- Prevent command injection in system calls\n- Use schema validation (Pydantic/Zod) for all inputs\n\n### Error Handling\n\n- Don't expose internal errors to clients\n- Log security-relevant errors server-side\n- Provide helpful but not revealing error messages\n- Clean up resources after errors\n\n### DNS Rebinding Protection\n\nFor streamable HTTP servers running locally:\n- Enable DNS rebinding protection\n- Validate the `Origin` header on all incoming connections\n- Bind to `127.0.0.1` rather than `0.0.0.0`\n\n---\n\n## Tool Annotations\n\nProvide annotations to help clients understand tool behavior:\n\n| Annotation | Type | Default | Description |\n|-----------|------|---------|-------------|\n| `readOnlyHint` | boolean | false | Tool does not modify its environment |\n| `destructiveHint` | boolean | true | Tool may perform destructive updates |\n| `idempotentHint` | boolean | false | Repeated calls with same args have no additional effect |\n| `openWorldHint` | boolean | true | Tool interacts with external entities |\n\n**Important**: Annotations are hints, not security guarantees. Clients should not make security-critical decisions based solely on annotations.\n\n---\n\n## Error Handling\n\n- Use standard JSON-RPC error codes\n- Report tool errors within result objects (not protocol-level errors)\n- Provide helpful, specific error messages with suggested next steps\n- Don't expose internal implementation details\n- Clean up resources properly on errors\n\nExample error handling:\n```typescript\ntry {\n  const result = performOperation();\n  return { content: [{ type: \"text\", text: result }] };\n} catch (error) {\n  return {\n    isError: true,\n    content: [{\n      type: \"text\",\n      text: `Error: ${error.message}. Try using filter='active_only' to reduce results.`\n    }]\n  };\n}\n```\n\n---\n\n## Testing Requirements\n\nComprehensive testing should cover:\n\n- **Functional testing**: Verify correct execution with valid/invalid inputs\n- **Integration testing**: Test interaction with external systems\n- **Security testing**: Validate auth, input sanitization, rate limiting\n- **Performance testing**: Check behavior under load, timeouts\n- **Error handling**: Ensure proper error reporting and cleanup\n\n---\n\n## Documentation Requirements\n\n- Provide clear documentation of all tools and capabilities\n- Include working examples (at least 3 per major feature)\n- Document security considerations\n- Specify required permissions and access levels\n- Document rate limits and performance characteristics\n",
        "icartsh-plugin/skills/mcp-builder/reference/node_mcp_server.md": "# Node/TypeScript MCP Server Implementation Guide\n\n## Overview\n\nThis document provides Node/TypeScript-specific best practices and examples for implementing MCP servers using the MCP TypeScript SDK. It covers project structure, server setup, tool registration patterns, input validation with Zod, error handling, and complete working examples.\n\n---\n\n## Quick Reference\n\n### Key Imports\n```typescript\nimport { McpServer } from \"@modelcontextprotocol/sdk/server/mcp.js\";\nimport { StreamableHTTPServerTransport } from \"@modelcontextprotocol/sdk/server/streamableHttp.js\";\nimport { StdioServerTransport } from \"@modelcontextprotocol/sdk/server/stdio.js\";\nimport express from \"express\";\nimport { z } from \"zod\";\n```\n\n### Server Initialization\n```typescript\nconst server = new McpServer({\n  name: \"service-mcp-server\",\n  version: \"1.0.0\"\n});\n```\n\n### Tool Registration Pattern\n```typescript\nserver.registerTool(\n  \"tool_name\",\n  {\n    title: \"Tool Display Name\",\n    description: \"What the tool does\",\n    inputSchema: { param: z.string() },\n    outputSchema: { result: z.string() }\n  },\n  async ({ param }) => {\n    const output = { result: `Processed: ${param}` };\n    return {\n      content: [{ type: \"text\", text: JSON.stringify(output) }],\n      structuredContent: output // Modern pattern for structured data\n    };\n  }\n);\n```\n\n---\n\n## MCP TypeScript SDK\n\nThe official MCP TypeScript SDK provides:\n- `McpServer` class for server initialization\n- `registerTool` method for tool registration\n- Zod schema integration for runtime input validation\n- Type-safe tool handler implementations\n\n**IMPORTANT - Use Modern APIs Only:**\n- **DO use**: `server.registerTool()`, `server.registerResource()`, `server.registerPrompt()`\n- **DO NOT use**: Old deprecated APIs such as `server.tool()`, `server.setRequestHandler(ListToolsRequestSchema, ...)`, or manual handler registration\n- The `register*` methods provide better type safety, automatic schema handling, and are the recommended approach\n\nSee the MCP SDK documentation in the references for complete details.\n\n## Server Naming Convention\n\nNode/TypeScript MCP servers must follow this naming pattern:\n- **Format**: `{service}-mcp-server` (lowercase with hyphens)\n- **Examples**: `github-mcp-server`, `jira-mcp-server`, `stripe-mcp-server`\n\nThe name should be:\n- General (not tied to specific features)\n- Descriptive of the service/API being integrated\n- Easy to infer from the task description\n- Without version numbers or dates\n\n## Project Structure\n\nCreate the following structure for Node/TypeScript MCP servers:\n\n```\n{service}-mcp-server/\n├── package.json\n├── tsconfig.json\n├── README.md\n├── src/\n│   ├── index.ts          # Main entry point with McpServer initialization\n│   ├── types.ts          # TypeScript type definitions and interfaces\n│   ├── tools/            # Tool implementations (one file per domain)\n│   ├── services/         # API clients and shared utilities\n│   ├── schemas/          # Zod validation schemas\n│   └── constants.ts      # Shared constants (API_URL, CHARACTER_LIMIT, etc.)\n└── dist/                 # Built JavaScript files (entry point: dist/index.js)\n```\n\n## Tool Implementation\n\n### Tool Naming\n\nUse snake_case for tool names (e.g., \"search_users\", \"create_project\", \"get_channel_info\") with clear, action-oriented names.\n\n**Avoid Naming Conflicts**: Include the service context to prevent overlaps:\n- Use \"slack_send_message\" instead of just \"send_message\"\n- Use \"github_create_issue\" instead of just \"create_issue\"\n- Use \"asana_list_tasks\" instead of just \"list_tasks\"\n\n### Tool Structure\n\nTools are registered using the `registerTool` method with the following requirements:\n- Use Zod schemas for runtime input validation and type safety\n- The `description` field must be explicitly provided - JSDoc comments are NOT automatically extracted\n- Explicitly provide `title`, `description`, `inputSchema`, and `annotations`\n- The `inputSchema` must be a Zod schema object (not a JSON schema)\n- Type all parameters and return values explicitly\n\n```typescript\nimport { McpServer } from \"@modelcontextprotocol/sdk/server/mcp.js\";\nimport { z } from \"zod\";\n\nconst server = new McpServer({\n  name: \"example-mcp\",\n  version: \"1.0.0\"\n});\n\n// Zod schema for input validation\nconst UserSearchInputSchema = z.object({\n  query: z.string()\n    .min(2, \"Query must be at least 2 characters\")\n    .max(200, \"Query must not exceed 200 characters\")\n    .describe(\"Search string to match against names/emails\"),\n  limit: z.number()\n    .int()\n    .min(1)\n    .max(100)\n    .default(20)\n    .describe(\"Maximum results to return\"),\n  offset: z.number()\n    .int()\n    .min(0)\n    .default(0)\n    .describe(\"Number of results to skip for pagination\"),\n  response_format: z.nativeEnum(ResponseFormat)\n    .default(ResponseFormat.MARKDOWN)\n    .describe(\"Output format: 'markdown' for human-readable or 'json' for machine-readable\")\n}).strict();\n\n// Type definition from Zod schema\ntype UserSearchInput = z.infer<typeof UserSearchInputSchema>;\n\nserver.registerTool(\n  \"example_search_users\",\n  {\n    title: \"Search Example Users\",\n    description: `Search for users in the Example system by name, email, or team.\n\nThis tool searches across all user profiles in the Example platform, supporting partial matches and various search filters. It does NOT create or modify users, only searches existing ones.\n\nArgs:\n  - query (string): Search string to match against names/emails\n  - limit (number): Maximum results to return, between 1-100 (default: 20)\n  - offset (number): Number of results to skip for pagination (default: 0)\n  - response_format ('markdown' | 'json'): Output format (default: 'markdown')\n\nReturns:\n  For JSON format: Structured data with schema:\n  {\n    \"total\": number,           // Total number of matches found\n    \"count\": number,           // Number of results in this response\n    \"offset\": number,          // Current pagination offset\n    \"users\": [\n      {\n        \"id\": string,          // User ID (e.g., \"U123456789\")\n        \"name\": string,        // Full name (e.g., \"John Doe\")\n        \"email\": string,       // Email address\n        \"team\": string,        // Team name (optional)\n        \"active\": boolean      // Whether user is active\n      }\n    ],\n    \"has_more\": boolean,       // Whether more results are available\n    \"next_offset\": number      // Offset for next page (if has_more is true)\n  }\n\nExamples:\n  - Use when: \"Find all marketing team members\" -> params with query=\"team:marketing\"\n  - Use when: \"Search for John's account\" -> params with query=\"john\"\n  - Don't use when: You need to create a user (use example_create_user instead)\n\nError Handling:\n  - Returns \"Error: Rate limit exceeded\" if too many requests (429 status)\n  - Returns \"No users found matching '<query>'\" if search returns empty`,\n    inputSchema: UserSearchInputSchema,\n    annotations: {\n      readOnlyHint: true,\n      destructiveHint: false,\n      idempotentHint: true,\n      openWorldHint: true\n    }\n  },\n  async (params: UserSearchInput) => {\n    try {\n      // Input validation is handled by Zod schema\n      // Make API request using validated parameters\n      const data = await makeApiRequest<any>(\n        \"users/search\",\n        \"GET\",\n        undefined,\n        {\n          q: params.query,\n          limit: params.limit,\n          offset: params.offset\n        }\n      );\n\n      const users = data.users || [];\n      const total = data.total || 0;\n\n      if (!users.length) {\n        return {\n          content: [{\n            type: \"text\",\n            text: `No users found matching '${params.query}'`\n          }]\n        };\n      }\n\n      // Prepare structured output\n      const output = {\n        total,\n        count: users.length,\n        offset: params.offset,\n        users: users.map((user: any) => ({\n          id: user.id,\n          name: user.name,\n          email: user.email,\n          ...(user.team ? { team: user.team } : {}),\n          active: user.active ?? true\n        })),\n        has_more: total > params.offset + users.length,\n        ...(total > params.offset + users.length ? {\n          next_offset: params.offset + users.length\n        } : {})\n      };\n\n      // Format text representation based on requested format\n      let textContent: string;\n      if (params.response_format === ResponseFormat.MARKDOWN) {\n        const lines = [`# User Search Results: '${params.query}'`, \"\",\n          `Found ${total} users (showing ${users.length})`, \"\"];\n        for (const user of users) {\n          lines.push(`## ${user.name} (${user.id})`);\n          lines.push(`- **Email**: ${user.email}`);\n          if (user.team) lines.push(`- **Team**: ${user.team}`);\n          lines.push(\"\");\n        }\n        textContent = lines.join(\"\\n\");\n      } else {\n        textContent = JSON.stringify(output, null, 2);\n      }\n\n      return {\n        content: [{ type: \"text\", text: textContent }],\n        structuredContent: output // Modern pattern for structured data\n      };\n    } catch (error) {\n      return {\n        content: [{\n          type: \"text\",\n          text: handleApiError(error)\n        }]\n      };\n    }\n  }\n);\n```\n\n## Zod Schemas for Input Validation\n\nZod provides runtime type validation:\n\n```typescript\nimport { z } from \"zod\";\n\n// Basic schema with validation\nconst CreateUserSchema = z.object({\n  name: z.string()\n    .min(1, \"Name is required\")\n    .max(100, \"Name must not exceed 100 characters\"),\n  email: z.string()\n    .email(\"Invalid email format\"),\n  age: z.number()\n    .int(\"Age must be a whole number\")\n    .min(0, \"Age cannot be negative\")\n    .max(150, \"Age cannot be greater than 150\")\n}).strict();  // Use .strict() to forbid extra fields\n\n// Enums\nenum ResponseFormat {\n  MARKDOWN = \"markdown\",\n  JSON = \"json\"\n}\n\nconst SearchSchema = z.object({\n  response_format: z.nativeEnum(ResponseFormat)\n    .default(ResponseFormat.MARKDOWN)\n    .describe(\"Output format\")\n});\n\n// Optional fields with defaults\nconst PaginationSchema = z.object({\n  limit: z.number()\n    .int()\n    .min(1)\n    .max(100)\n    .default(20)\n    .describe(\"Maximum results to return\"),\n  offset: z.number()\n    .int()\n    .min(0)\n    .default(0)\n    .describe(\"Number of results to skip\")\n});\n```\n\n## Response Format Options\n\nSupport multiple output formats for flexibility:\n\n```typescript\nenum ResponseFormat {\n  MARKDOWN = \"markdown\",\n  JSON = \"json\"\n}\n\nconst inputSchema = z.object({\n  query: z.string(),\n  response_format: z.nativeEnum(ResponseFormat)\n    .default(ResponseFormat.MARKDOWN)\n    .describe(\"Output format: 'markdown' for human-readable or 'json' for machine-readable\")\n});\n```\n\n**Markdown format**:\n- Use headers, lists, and formatting for clarity\n- Convert timestamps to human-readable format\n- Show display names with IDs in parentheses\n- Omit verbose metadata\n- Group related information logically\n\n**JSON format**:\n- Return complete, structured data suitable for programmatic processing\n- Include all available fields and metadata\n- Use consistent field names and types\n\n## Pagination Implementation\n\nFor tools that list resources:\n\n```typescript\nconst ListSchema = z.object({\n  limit: z.number().int().min(1).max(100).default(20),\n  offset: z.number().int().min(0).default(0)\n});\n\nasync function listItems(params: z.infer<typeof ListSchema>) {\n  const data = await apiRequest(params.limit, params.offset);\n\n  const response = {\n    total: data.total,\n    count: data.items.length,\n    offset: params.offset,\n    items: data.items,\n    has_more: data.total > params.offset + data.items.length,\n    next_offset: data.total > params.offset + data.items.length\n      ? params.offset + data.items.length\n      : undefined\n  };\n\n  return JSON.stringify(response, null, 2);\n}\n```\n\n## Character Limits and Truncation\n\nAdd a CHARACTER_LIMIT constant to prevent overwhelming responses:\n\n```typescript\n// At module level in constants.ts\nexport const CHARACTER_LIMIT = 25000;  // Maximum response size in characters\n\nasync function searchTool(params: SearchInput) {\n  let result = generateResponse(data);\n\n  // Check character limit and truncate if needed\n  if (result.length > CHARACTER_LIMIT) {\n    const truncatedData = data.slice(0, Math.max(1, data.length / 2));\n    response.data = truncatedData;\n    response.truncated = true;\n    response.truncation_message =\n      `Response truncated from ${data.length} to ${truncatedData.length} items. ` +\n      `Use 'offset' parameter or add filters to see more results.`;\n    result = JSON.stringify(response, null, 2);\n  }\n\n  return result;\n}\n```\n\n## Error Handling\n\nProvide clear, actionable error messages:\n\n```typescript\nimport axios, { AxiosError } from \"axios\";\n\nfunction handleApiError(error: unknown): string {\n  if (error instanceof AxiosError) {\n    if (error.response) {\n      switch (error.response.status) {\n        case 404:\n          return \"Error: Resource not found. Please check the ID is correct.\";\n        case 403:\n          return \"Error: Permission denied. You don't have access to this resource.\";\n        case 429:\n          return \"Error: Rate limit exceeded. Please wait before making more requests.\";\n        default:\n          return `Error: API request failed with status ${error.response.status}`;\n      }\n    } else if (error.code === \"ECONNABORTED\") {\n      return \"Error: Request timed out. Please try again.\";\n    }\n  }\n  return `Error: Unexpected error occurred: ${error instanceof Error ? error.message : String(error)}`;\n}\n```\n\n## Shared Utilities\n\nExtract common functionality into reusable functions:\n\n```typescript\n// Shared API request function\nasync function makeApiRequest<T>(\n  endpoint: string,\n  method: \"GET\" | \"POST\" | \"PUT\" | \"DELETE\" = \"GET\",\n  data?: any,\n  params?: any\n): Promise<T> {\n  try {\n    const response = await axios({\n      method,\n      url: `${API_BASE_URL}/${endpoint}`,\n      data,\n      params,\n      timeout: 30000,\n      headers: {\n        \"Content-Type\": \"application/json\",\n        \"Accept\": \"application/json\"\n      }\n    });\n    return response.data;\n  } catch (error) {\n    throw error;\n  }\n}\n```\n\n## Async/Await Best Practices\n\nAlways use async/await for network requests and I/O operations:\n\n```typescript\n// Good: Async network request\nasync function fetchData(resourceId: string): Promise<ResourceData> {\n  const response = await axios.get(`${API_URL}/resource/${resourceId}`);\n  return response.data;\n}\n\n// Bad: Promise chains\nfunction fetchData(resourceId: string): Promise<ResourceData> {\n  return axios.get(`${API_URL}/resource/${resourceId}`)\n    .then(response => response.data);  // Harder to read and maintain\n}\n```\n\n## TypeScript Best Practices\n\n1. **Use Strict TypeScript**: Enable strict mode in tsconfig.json\n2. **Define Interfaces**: Create clear interface definitions for all data structures\n3. **Avoid `any`**: Use proper types or `unknown` instead of `any`\n4. **Zod for Runtime Validation**: Use Zod schemas to validate external data\n5. **Type Guards**: Create type guard functions for complex type checking\n6. **Error Handling**: Always use try-catch with proper error type checking\n7. **Null Safety**: Use optional chaining (`?.`) and nullish coalescing (`??`)\n\n```typescript\n// Good: Type-safe with Zod and interfaces\ninterface UserResponse {\n  id: string;\n  name: string;\n  email: string;\n  team?: string;\n  active: boolean;\n}\n\nconst UserSchema = z.object({\n  id: z.string(),\n  name: z.string(),\n  email: z.string().email(),\n  team: z.string().optional(),\n  active: z.boolean()\n});\n\ntype User = z.infer<typeof UserSchema>;\n\nasync function getUser(id: string): Promise<User> {\n  const data = await apiCall(`/users/${id}`);\n  return UserSchema.parse(data);  // Runtime validation\n}\n\n// Bad: Using any\nasync function getUser(id: string): Promise<any> {\n  return await apiCall(`/users/${id}`);  // No type safety\n}\n```\n\n## Package Configuration\n\n### package.json\n\n```json\n{\n  \"name\": \"{service}-mcp-server\",\n  \"version\": \"1.0.0\",\n  \"description\": \"MCP server for {Service} API integration\",\n  \"type\": \"module\",\n  \"main\": \"dist/index.js\",\n  \"scripts\": {\n    \"start\": \"node dist/index.js\",\n    \"dev\": \"tsx watch src/index.ts\",\n    \"build\": \"tsc\",\n    \"clean\": \"rm -rf dist\"\n  },\n  \"engines\": {\n    \"node\": \">=18\"\n  },\n  \"dependencies\": {\n    \"@modelcontextprotocol/sdk\": \"^1.6.1\",\n    \"axios\": \"^1.7.9\",\n    \"zod\": \"^3.23.8\"\n  },\n  \"devDependencies\": {\n    \"@types/node\": \"^22.10.0\",\n    \"tsx\": \"^4.19.2\",\n    \"typescript\": \"^5.7.2\"\n  }\n}\n```\n\n### tsconfig.json\n\n```json\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2022\",\n    \"module\": \"Node16\",\n    \"moduleResolution\": \"Node16\",\n    \"lib\": [\"ES2022\"],\n    \"outDir\": \"./dist\",\n    \"rootDir\": \"./src\",\n    \"strict\": true,\n    \"esModuleInterop\": true,\n    \"skipLibCheck\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"declaration\": true,\n    \"declarationMap\": true,\n    \"sourceMap\": true,\n    \"allowSyntheticDefaultImports\": true\n  },\n  \"include\": [\"src/**/*\"],\n  \"exclude\": [\"node_modules\", \"dist\"]\n}\n```\n\n## Complete Example\n\n```typescript\n#!/usr/bin/env node\n/**\n * MCP Server for Example Service.\n *\n * This server provides tools to interact with Example API, including user search,\n * project management, and data export capabilities.\n */\n\nimport { McpServer } from \"@modelcontextprotocol/sdk/server/mcp.js\";\nimport { StdioServerTransport } from \"@modelcontextprotocol/sdk/server/stdio.js\";\nimport { z } from \"zod\";\nimport axios, { AxiosError } from \"axios\";\n\n// Constants\nconst API_BASE_URL = \"https://api.example.com/v1\";\nconst CHARACTER_LIMIT = 25000;\n\n// Enums\nenum ResponseFormat {\n  MARKDOWN = \"markdown\",\n  JSON = \"json\"\n}\n\n// Zod schemas\nconst UserSearchInputSchema = z.object({\n  query: z.string()\n    .min(2, \"Query must be at least 2 characters\")\n    .max(200, \"Query must not exceed 200 characters\")\n    .describe(\"Search string to match against names/emails\"),\n  limit: z.number()\n    .int()\n    .min(1)\n    .max(100)\n    .default(20)\n    .describe(\"Maximum results to return\"),\n  offset: z.number()\n    .int()\n    .min(0)\n    .default(0)\n    .describe(\"Number of results to skip for pagination\"),\n  response_format: z.nativeEnum(ResponseFormat)\n    .default(ResponseFormat.MARKDOWN)\n    .describe(\"Output format: 'markdown' for human-readable or 'json' for machine-readable\")\n}).strict();\n\ntype UserSearchInput = z.infer<typeof UserSearchInputSchema>;\n\n// Shared utility functions\nasync function makeApiRequest<T>(\n  endpoint: string,\n  method: \"GET\" | \"POST\" | \"PUT\" | \"DELETE\" = \"GET\",\n  data?: any,\n  params?: any\n): Promise<T> {\n  try {\n    const response = await axios({\n      method,\n      url: `${API_BASE_URL}/${endpoint}`,\n      data,\n      params,\n      timeout: 30000,\n      headers: {\n        \"Content-Type\": \"application/json\",\n        \"Accept\": \"application/json\"\n      }\n    });\n    return response.data;\n  } catch (error) {\n    throw error;\n  }\n}\n\nfunction handleApiError(error: unknown): string {\n  if (error instanceof AxiosError) {\n    if (error.response) {\n      switch (error.response.status) {\n        case 404:\n          return \"Error: Resource not found. Please check the ID is correct.\";\n        case 403:\n          return \"Error: Permission denied. You don't have access to this resource.\";\n        case 429:\n          return \"Error: Rate limit exceeded. Please wait before making more requests.\";\n        default:\n          return `Error: API request failed with status ${error.response.status}`;\n      }\n    } else if (error.code === \"ECONNABORTED\") {\n      return \"Error: Request timed out. Please try again.\";\n    }\n  }\n  return `Error: Unexpected error occurred: ${error instanceof Error ? error.message : String(error)}`;\n}\n\n// Create MCP server instance\nconst server = new McpServer({\n  name: \"example-mcp\",\n  version: \"1.0.0\"\n});\n\n// Register tools\nserver.registerTool(\n  \"example_search_users\",\n  {\n    title: \"Search Example Users\",\n    description: `[Full description as shown above]`,\n    inputSchema: UserSearchInputSchema,\n    annotations: {\n      readOnlyHint: true,\n      destructiveHint: false,\n      idempotentHint: true,\n      openWorldHint: true\n    }\n  },\n  async (params: UserSearchInput) => {\n    // Implementation as shown above\n  }\n);\n\n// Main function\n// For stdio (local):\nasync function runStdio() {\n  if (!process.env.EXAMPLE_API_KEY) {\n    console.error(\"ERROR: EXAMPLE_API_KEY environment variable is required\");\n    process.exit(1);\n  }\n\n  const transport = new StdioServerTransport();\n  await server.connect(transport);\n  console.error(\"MCP server running via stdio\");\n}\n\n// For streamable HTTP (remote):\nasync function runHTTP() {\n  if (!process.env.EXAMPLE_API_KEY) {\n    console.error(\"ERROR: EXAMPLE_API_KEY environment variable is required\");\n    process.exit(1);\n  }\n\n  const app = express();\n  app.use(express.json());\n\n  app.post('/mcp', async (req, res) => {\n    const transport = new StreamableHTTPServerTransport({\n      sessionIdGenerator: undefined,\n      enableJsonResponse: true\n    });\n    res.on('close', () => transport.close());\n    await server.connect(transport);\n    await transport.handleRequest(req, res, req.body);\n  });\n\n  const port = parseInt(process.env.PORT || '3000');\n  app.listen(port, () => {\n    console.error(`MCP server running on http://localhost:${port}/mcp`);\n  });\n}\n\n// Choose transport based on environment\nconst transport = process.env.TRANSPORT || 'stdio';\nif (transport === 'http') {\n  runHTTP().catch(error => {\n    console.error(\"Server error:\", error);\n    process.exit(1);\n  });\n} else {\n  runStdio().catch(error => {\n    console.error(\"Server error:\", error);\n    process.exit(1);\n  });\n}\n```\n\n---\n\n## Advanced MCP Features\n\n### Resource Registration\n\nExpose data as resources for efficient, URI-based access:\n\n```typescript\nimport { ResourceTemplate } from \"@modelcontextprotocol/sdk/types.js\";\n\n// Register a resource with URI template\nserver.registerResource(\n  {\n    uri: \"file://documents/{name}\",\n    name: \"Document Resource\",\n    description: \"Access documents by name\",\n    mimeType: \"text/plain\"\n  },\n  async (uri: string) => {\n    // Extract parameter from URI\n    const match = uri.match(/^file:\\/\\/documents\\/(.+)$/);\n    if (!match) {\n      throw new Error(\"Invalid URI format\");\n    }\n\n    const documentName = match[1];\n    const content = await loadDocument(documentName);\n\n    return {\n      contents: [{\n        uri,\n        mimeType: \"text/plain\",\n        text: content\n      }]\n    };\n  }\n);\n\n// List available resources dynamically\nserver.registerResourceList(async () => {\n  const documents = await getAvailableDocuments();\n  return {\n    resources: documents.map(doc => ({\n      uri: `file://documents/${doc.name}`,\n      name: doc.name,\n      mimeType: \"text/plain\",\n      description: doc.description\n    }))\n  };\n});\n```\n\n**When to use Resources vs Tools:**\n- **Resources**: For data access with simple URI-based parameters\n- **Tools**: For complex operations requiring validation and business logic\n- **Resources**: When data is relatively static or template-based\n- **Tools**: When operations have side effects or complex workflows\n\n### Transport Options\n\nThe TypeScript SDK supports two main transport mechanisms:\n\n#### Streamable HTTP (Recommended for Remote Servers)\n\n```typescript\nimport { StreamableHTTPServerTransport } from \"@modelcontextprotocol/sdk/server/streamableHttp.js\";\nimport express from \"express\";\n\nconst app = express();\napp.use(express.json());\n\napp.post('/mcp', async (req, res) => {\n  // Create new transport for each request (stateless, prevents request ID collisions)\n  const transport = new StreamableHTTPServerTransport({\n    sessionIdGenerator: undefined,\n    enableJsonResponse: true\n  });\n\n  res.on('close', () => transport.close());\n\n  await server.connect(transport);\n  await transport.handleRequest(req, res, req.body);\n});\n\napp.listen(3000);\n```\n\n#### stdio (For Local Integrations)\n\n```typescript\nimport { StdioServerTransport } from \"@modelcontextprotocol/sdk/server/stdio.js\";\n\nconst transport = new StdioServerTransport();\nawait server.connect(transport);\n```\n\n**Transport selection:**\n- **Streamable HTTP**: Web services, remote access, multiple clients\n- **stdio**: Command-line tools, local development, subprocess integration\n\n### Notification Support\n\nNotify clients when server state changes:\n\n```typescript\n// Notify when tools list changes\nserver.notification({\n  method: \"notifications/tools/list_changed\"\n});\n\n// Notify when resources change\nserver.notification({\n  method: \"notifications/resources/list_changed\"\n});\n```\n\nUse notifications sparingly - only when server capabilities genuinely change.\n\n---\n\n## Code Best Practices\n\n### Code Composability and Reusability\n\nYour implementation MUST prioritize composability and code reuse:\n\n1. **Extract Common Functionality**:\n   - Create reusable helper functions for operations used across multiple tools\n   - Build shared API clients for HTTP requests instead of duplicating code\n   - Centralize error handling logic in utility functions\n   - Extract business logic into dedicated functions that can be composed\n   - Extract shared markdown or JSON field selection & formatting functionality\n\n2. **Avoid Duplication**:\n   - NEVER copy-paste similar code between tools\n   - If you find yourself writing similar logic twice, extract it into a function\n   - Common operations like pagination, filtering, field selection, and formatting should be shared\n   - Authentication/authorization logic should be centralized\n\n## Building and Running\n\nAlways build your TypeScript code before running:\n\n```bash\n# Build the project\nnpm run build\n\n# Run the server\nnpm start\n\n# Development with auto-reload\nnpm run dev\n```\n\nAlways ensure `npm run build` completes successfully before considering the implementation complete.\n\n## Quality Checklist\n\nBefore finalizing your Node/TypeScript MCP server implementation, ensure:\n\n### Strategic Design\n- [ ] Tools enable complete workflows, not just API endpoint wrappers\n- [ ] Tool names reflect natural task subdivisions\n- [ ] Response formats optimize for agent context efficiency\n- [ ] Human-readable identifiers used where appropriate\n- [ ] Error messages guide agents toward correct usage\n\n### Implementation Quality\n- [ ] FOCUSED IMPLEMENTATION: Most important and valuable tools implemented\n- [ ] All tools registered using `registerTool` with complete configuration\n- [ ] All tools include `title`, `description`, `inputSchema`, and `annotations`\n- [ ] Annotations correctly set (readOnlyHint, destructiveHint, idempotentHint, openWorldHint)\n- [ ] All tools use Zod schemas for runtime input validation with `.strict()` enforcement\n- [ ] All Zod schemas have proper constraints and descriptive error messages\n- [ ] All tools have comprehensive descriptions with explicit input/output types\n- [ ] Descriptions include return value examples and complete schema documentation\n- [ ] Error messages are clear, actionable, and educational\n\n### TypeScript Quality\n- [ ] TypeScript interfaces are defined for all data structures\n- [ ] Strict TypeScript is enabled in tsconfig.json\n- [ ] No use of `any` type - use `unknown` or proper types instead\n- [ ] All async functions have explicit Promise<T> return types\n- [ ] Error handling uses proper type guards (e.g., `axios.isAxiosError`, `z.ZodError`)\n\n### Advanced Features (where applicable)\n- [ ] Resources registered for appropriate data endpoints\n- [ ] Appropriate transport configured (stdio or streamable HTTP)\n- [ ] Notifications implemented for dynamic server capabilities\n- [ ] Type-safe with SDK interfaces\n\n### Project Configuration\n- [ ] Package.json includes all necessary dependencies\n- [ ] Build script produces working JavaScript in dist/ directory\n- [ ] Main entry point is properly configured as dist/index.js\n- [ ] Server name follows format: `{service}-mcp-server`\n- [ ] tsconfig.json properly configured with strict mode\n\n### Code Quality\n- [ ] Pagination is properly implemented where applicable\n- [ ] Large responses check CHARACTER_LIMIT constant and truncate with clear messages\n- [ ] Filtering options are provided for potentially large result sets\n- [ ] All network operations handle timeouts and connection errors gracefully\n- [ ] Common functionality is extracted into reusable functions\n- [ ] Return types are consistent across similar operations\n\n### Testing and Build\n- [ ] `npm run build` completes successfully without errors\n- [ ] dist/index.js created and executable\n- [ ] Server runs: `node dist/index.js --help`\n- [ ] All imports resolve correctly\n- [ ] Sample tool calls work as expected",
        "icartsh-plugin/skills/mcp-builder/reference/python_mcp_server.md": "# Python MCP Server Implementation Guide\n\n## Overview\n\nThis document provides Python-specific best practices and examples for implementing MCP servers using the MCP Python SDK. It covers server setup, tool registration patterns, input validation with Pydantic, error handling, and complete working examples.\n\n---\n\n## Quick Reference\n\n### Key Imports\n```python\nfrom mcp.server.fastmcp import FastMCP\nfrom pydantic import BaseModel, Field, field_validator, ConfigDict\nfrom typing import Optional, List, Dict, Any\nfrom enum import Enum\nimport httpx\n```\n\n### Server Initialization\n```python\nmcp = FastMCP(\"service_mcp\")\n```\n\n### Tool Registration Pattern\n```python\n@mcp.tool(name=\"tool_name\", annotations={...})\nasync def tool_function(params: InputModel) -> str:\n    # Implementation\n    pass\n```\n\n---\n\n## MCP Python SDK and FastMCP\n\nThe official MCP Python SDK provides FastMCP, a high-level framework for building MCP servers. It provides:\n- Automatic description and inputSchema generation from function signatures and docstrings\n- Pydantic model integration for input validation\n- Decorator-based tool registration with `@mcp.tool`\n\n**For complete SDK documentation, use WebFetch to load:**\n`https://raw.githubusercontent.com/modelcontextprotocol/python-sdk/main/README.md`\n\n## Server Naming Convention\n\nPython MCP servers must follow this naming pattern:\n- **Format**: `{service}_mcp` (lowercase with underscores)\n- **Examples**: `github_mcp`, `jira_mcp`, `stripe_mcp`\n\nThe name should be:\n- General (not tied to specific features)\n- Descriptive of the service/API being integrated\n- Easy to infer from the task description\n- Without version numbers or dates\n\n## Tool Implementation\n\n### Tool Naming\n\nUse snake_case for tool names (e.g., \"search_users\", \"create_project\", \"get_channel_info\") with clear, action-oriented names.\n\n**Avoid Naming Conflicts**: Include the service context to prevent overlaps:\n- Use \"slack_send_message\" instead of just \"send_message\"\n- Use \"github_create_issue\" instead of just \"create_issue\"\n- Use \"asana_list_tasks\" instead of just \"list_tasks\"\n\n### Tool Structure with FastMCP\n\nTools are defined using the `@mcp.tool` decorator with Pydantic models for input validation:\n\n```python\nfrom pydantic import BaseModel, Field, ConfigDict\nfrom mcp.server.fastmcp import FastMCP\n\n# Initialize the MCP server\nmcp = FastMCP(\"example_mcp\")\n\n# Define Pydantic model for input validation\nclass ServiceToolInput(BaseModel):\n    '''Input model for service tool operation.'''\n    model_config = ConfigDict(\n        str_strip_whitespace=True,  # Auto-strip whitespace from strings\n        validate_assignment=True,    # Validate on assignment\n        extra='forbid'              # Forbid extra fields\n    )\n\n    param1: str = Field(..., description=\"First parameter description (e.g., 'user123', 'project-abc')\", min_length=1, max_length=100)\n    param2: Optional[int] = Field(default=None, description=\"Optional integer parameter with constraints\", ge=0, le=1000)\n    tags: Optional[List[str]] = Field(default_factory=list, description=\"List of tags to apply\", max_items=10)\n\n@mcp.tool(\n    name=\"service_tool_name\",\n    annotations={\n        \"title\": \"Human-Readable Tool Title\",\n        \"readOnlyHint\": True,     # Tool does not modify environment\n        \"destructiveHint\": False,  # Tool does not perform destructive operations\n        \"idempotentHint\": True,    # Repeated calls have no additional effect\n        \"openWorldHint\": False     # Tool does not interact with external entities\n    }\n)\nasync def service_tool_name(params: ServiceToolInput) -> str:\n    '''Tool description automatically becomes the 'description' field.\n\n    This tool performs a specific operation on the service. It validates all inputs\n    using the ServiceToolInput Pydantic model before processing.\n\n    Args:\n        params (ServiceToolInput): Validated input parameters containing:\n            - param1 (str): First parameter description\n            - param2 (Optional[int]): Optional parameter with default\n            - tags (Optional[List[str]]): List of tags\n\n    Returns:\n        str: JSON-formatted response containing operation results\n    '''\n    # Implementation here\n    pass\n```\n\n## Pydantic v2 Key Features\n\n- Use `model_config` instead of nested `Config` class\n- Use `field_validator` instead of deprecated `validator`\n- Use `model_dump()` instead of deprecated `dict()`\n- Validators require `@classmethod` decorator\n- Type hints are required for validator methods\n\n```python\nfrom pydantic import BaseModel, Field, field_validator, ConfigDict\n\nclass CreateUserInput(BaseModel):\n    model_config = ConfigDict(\n        str_strip_whitespace=True,\n        validate_assignment=True\n    )\n\n    name: str = Field(..., description=\"User's full name\", min_length=1, max_length=100)\n    email: str = Field(..., description=\"User's email address\", pattern=r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$')\n    age: int = Field(..., description=\"User's age\", ge=0, le=150)\n\n    @field_validator('email')\n    @classmethod\n    def validate_email(cls, v: str) -> str:\n        if not v.strip():\n            raise ValueError(\"Email cannot be empty\")\n        return v.lower()\n```\n\n## Response Format Options\n\nSupport multiple output formats for flexibility:\n\n```python\nfrom enum import Enum\n\nclass ResponseFormat(str, Enum):\n    '''Output format for tool responses.'''\n    MARKDOWN = \"markdown\"\n    JSON = \"json\"\n\nclass UserSearchInput(BaseModel):\n    query: str = Field(..., description=\"Search query\")\n    response_format: ResponseFormat = Field(\n        default=ResponseFormat.MARKDOWN,\n        description=\"Output format: 'markdown' for human-readable or 'json' for machine-readable\"\n    )\n```\n\n**Markdown format**:\n- Use headers, lists, and formatting for clarity\n- Convert timestamps to human-readable format (e.g., \"2024-01-15 10:30:00 UTC\" instead of epoch)\n- Show display names with IDs in parentheses (e.g., \"@john.doe (U123456)\")\n- Omit verbose metadata (e.g., show only one profile image URL, not all sizes)\n- Group related information logically\n\n**JSON format**:\n- Return complete, structured data suitable for programmatic processing\n- Include all available fields and metadata\n- Use consistent field names and types\n\n## Pagination Implementation\n\nFor tools that list resources:\n\n```python\nclass ListInput(BaseModel):\n    limit: Optional[int] = Field(default=20, description=\"Maximum results to return\", ge=1, le=100)\n    offset: Optional[int] = Field(default=0, description=\"Number of results to skip for pagination\", ge=0)\n\nasync def list_items(params: ListInput) -> str:\n    # Make API request with pagination\n    data = await api_request(limit=params.limit, offset=params.offset)\n\n    # Return pagination info\n    response = {\n        \"total\": data[\"total\"],\n        \"count\": len(data[\"items\"]),\n        \"offset\": params.offset,\n        \"items\": data[\"items\"],\n        \"has_more\": data[\"total\"] > params.offset + len(data[\"items\"]),\n        \"next_offset\": params.offset + len(data[\"items\"]) if data[\"total\"] > params.offset + len(data[\"items\"]) else None\n    }\n    return json.dumps(response, indent=2)\n```\n\n## Error Handling\n\nProvide clear, actionable error messages:\n\n```python\ndef _handle_api_error(e: Exception) -> str:\n    '''Consistent error formatting across all tools.'''\n    if isinstance(e, httpx.HTTPStatusError):\n        if e.response.status_code == 404:\n            return \"Error: Resource not found. Please check the ID is correct.\"\n        elif e.response.status_code == 403:\n            return \"Error: Permission denied. You don't have access to this resource.\"\n        elif e.response.status_code == 429:\n            return \"Error: Rate limit exceeded. Please wait before making more requests.\"\n        return f\"Error: API request failed with status {e.response.status_code}\"\n    elif isinstance(e, httpx.TimeoutException):\n        return \"Error: Request timed out. Please try again.\"\n    return f\"Error: Unexpected error occurred: {type(e).__name__}\"\n```\n\n## Shared Utilities\n\nExtract common functionality into reusable functions:\n\n```python\n# Shared API request function\nasync def _make_api_request(endpoint: str, method: str = \"GET\", **kwargs) -> dict:\n    '''Reusable function for all API calls.'''\n    async with httpx.AsyncClient() as client:\n        response = await client.request(\n            method,\n            f\"{API_BASE_URL}/{endpoint}\",\n            timeout=30.0,\n            **kwargs\n        )\n        response.raise_for_status()\n        return response.json()\n```\n\n## Async/Await Best Practices\n\nAlways use async/await for network requests and I/O operations:\n\n```python\n# Good: Async network request\nasync def fetch_data(resource_id: str) -> dict:\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"{API_URL}/resource/{resource_id}\")\n        response.raise_for_status()\n        return response.json()\n\n# Bad: Synchronous request\ndef fetch_data(resource_id: str) -> dict:\n    response = requests.get(f\"{API_URL}/resource/{resource_id}\")  # Blocks\n    return response.json()\n```\n\n## Type Hints\n\nUse type hints throughout:\n\n```python\nfrom typing import Optional, List, Dict, Any\n\nasync def get_user(user_id: str) -> Dict[str, Any]:\n    data = await fetch_user(user_id)\n    return {\"id\": data[\"id\"], \"name\": data[\"name\"]}\n```\n\n## Tool Docstrings\n\nEvery tool must have comprehensive docstrings with explicit type information:\n\n```python\nasync def search_users(params: UserSearchInput) -> str:\n    '''\n    Search for users in the Example system by name, email, or team.\n\n    This tool searches across all user profiles in the Example platform,\n    supporting partial matches and various search filters. It does NOT\n    create or modify users, only searches existing ones.\n\n    Args:\n        params (UserSearchInput): Validated input parameters containing:\n            - query (str): Search string to match against names/emails (e.g., \"john\", \"@example.com\", \"team:marketing\")\n            - limit (Optional[int]): Maximum results to return, between 1-100 (default: 20)\n            - offset (Optional[int]): Number of results to skip for pagination (default: 0)\n\n    Returns:\n        str: JSON-formatted string containing search results with the following schema:\n\n        Success response:\n        {\n            \"total\": int,           # Total number of matches found\n            \"count\": int,           # Number of results in this response\n            \"offset\": int,          # Current pagination offset\n            \"users\": [\n                {\n                    \"id\": str,      # User ID (e.g., \"U123456789\")\n                    \"name\": str,    # Full name (e.g., \"John Doe\")\n                    \"email\": str,   # Email address (e.g., \"john@example.com\")\n                    \"team\": str     # Team name (e.g., \"Marketing\") - optional\n                }\n            ]\n        }\n\n        Error response:\n        \"Error: <error message>\" or \"No users found matching '<query>'\"\n\n    Examples:\n        - Use when: \"Find all marketing team members\" -> params with query=\"team:marketing\"\n        - Use when: \"Search for John's account\" -> params with query=\"john\"\n        - Don't use when: You need to create a user (use example_create_user instead)\n        - Don't use when: You have a user ID and need full details (use example_get_user instead)\n\n    Error Handling:\n        - Input validation errors are handled by Pydantic model\n        - Returns \"Error: Rate limit exceeded\" if too many requests (429 status)\n        - Returns \"Error: Invalid API authentication\" if API key is invalid (401 status)\n        - Returns formatted list of results or \"No users found matching 'query'\"\n    '''\n```\n\n## Complete Example\n\nSee below for a complete Python MCP server example:\n\n```python\n#!/usr/bin/env python3\n'''\nMCP Server for Example Service.\n\nThis server provides tools to interact with Example API, including user search,\nproject management, and data export capabilities.\n'''\n\nfrom typing import Optional, List, Dict, Any\nfrom enum import Enum\nimport httpx\nfrom pydantic import BaseModel, Field, field_validator, ConfigDict\nfrom mcp.server.fastmcp import FastMCP\n\n# Initialize the MCP server\nmcp = FastMCP(\"example_mcp\")\n\n# Constants\nAPI_BASE_URL = \"https://api.example.com/v1\"\n\n# Enums\nclass ResponseFormat(str, Enum):\n    '''Output format for tool responses.'''\n    MARKDOWN = \"markdown\"\n    JSON = \"json\"\n\n# Pydantic Models for Input Validation\nclass UserSearchInput(BaseModel):\n    '''Input model for user search operations.'''\n    model_config = ConfigDict(\n        str_strip_whitespace=True,\n        validate_assignment=True\n    )\n\n    query: str = Field(..., description=\"Search string to match against names/emails\", min_length=2, max_length=200)\n    limit: Optional[int] = Field(default=20, description=\"Maximum results to return\", ge=1, le=100)\n    offset: Optional[int] = Field(default=0, description=\"Number of results to skip for pagination\", ge=0)\n    response_format: ResponseFormat = Field(default=ResponseFormat.MARKDOWN, description=\"Output format\")\n\n    @field_validator('query')\n    @classmethod\n    def validate_query(cls, v: str) -> str:\n        if not v.strip():\n            raise ValueError(\"Query cannot be empty or whitespace only\")\n        return v.strip()\n\n# Shared utility functions\nasync def _make_api_request(endpoint: str, method: str = \"GET\", **kwargs) -> dict:\n    '''Reusable function for all API calls.'''\n    async with httpx.AsyncClient() as client:\n        response = await client.request(\n            method,\n            f\"{API_BASE_URL}/{endpoint}\",\n            timeout=30.0,\n            **kwargs\n        )\n        response.raise_for_status()\n        return response.json()\n\ndef _handle_api_error(e: Exception) -> str:\n    '''Consistent error formatting across all tools.'''\n    if isinstance(e, httpx.HTTPStatusError):\n        if e.response.status_code == 404:\n            return \"Error: Resource not found. Please check the ID is correct.\"\n        elif e.response.status_code == 403:\n            return \"Error: Permission denied. You don't have access to this resource.\"\n        elif e.response.status_code == 429:\n            return \"Error: Rate limit exceeded. Please wait before making more requests.\"\n        return f\"Error: API request failed with status {e.response.status_code}\"\n    elif isinstance(e, httpx.TimeoutException):\n        return \"Error: Request timed out. Please try again.\"\n    return f\"Error: Unexpected error occurred: {type(e).__name__}\"\n\n# Tool definitions\n@mcp.tool(\n    name=\"example_search_users\",\n    annotations={\n        \"title\": \"Search Example Users\",\n        \"readOnlyHint\": True,\n        \"destructiveHint\": False,\n        \"idempotentHint\": True,\n        \"openWorldHint\": True\n    }\n)\nasync def example_search_users(params: UserSearchInput) -> str:\n    '''Search for users in the Example system by name, email, or team.\n\n    [Full docstring as shown above]\n    '''\n    try:\n        # Make API request using validated parameters\n        data = await _make_api_request(\n            \"users/search\",\n            params={\n                \"q\": params.query,\n                \"limit\": params.limit,\n                \"offset\": params.offset\n            }\n        )\n\n        users = data.get(\"users\", [])\n        total = data.get(\"total\", 0)\n\n        if not users:\n            return f\"No users found matching '{params.query}'\"\n\n        # Format response based on requested format\n        if params.response_format == ResponseFormat.MARKDOWN:\n            lines = [f\"# User Search Results: '{params.query}'\", \"\"]\n            lines.append(f\"Found {total} users (showing {len(users)})\")\n            lines.append(\"\")\n\n            for user in users:\n                lines.append(f\"## {user['name']} ({user['id']})\")\n                lines.append(f\"- **Email**: {user['email']}\")\n                if user.get('team'):\n                    lines.append(f\"- **Team**: {user['team']}\")\n                lines.append(\"\")\n\n            return \"\\n\".join(lines)\n\n        else:\n            # Machine-readable JSON format\n            import json\n            response = {\n                \"total\": total,\n                \"count\": len(users),\n                \"offset\": params.offset,\n                \"users\": users\n            }\n            return json.dumps(response, indent=2)\n\n    except Exception as e:\n        return _handle_api_error(e)\n\nif __name__ == \"__main__\":\n    mcp.run()\n```\n\n---\n\n## Advanced FastMCP Features\n\n### Context Parameter Injection\n\nFastMCP can automatically inject a `Context` parameter into tools for advanced capabilities like logging, progress reporting, resource reading, and user interaction:\n\n```python\nfrom mcp.server.fastmcp import FastMCP, Context\n\nmcp = FastMCP(\"example_mcp\")\n\n@mcp.tool()\nasync def advanced_search(query: str, ctx: Context) -> str:\n    '''Advanced tool with context access for logging and progress.'''\n\n    # Report progress for long operations\n    await ctx.report_progress(0.25, \"Starting search...\")\n\n    # Log information for debugging\n    await ctx.log_info(\"Processing query\", {\"query\": query, \"timestamp\": datetime.now()})\n\n    # Perform search\n    results = await search_api(query)\n    await ctx.report_progress(0.75, \"Formatting results...\")\n\n    # Access server configuration\n    server_name = ctx.fastmcp.name\n\n    return format_results(results)\n\n@mcp.tool()\nasync def interactive_tool(resource_id: str, ctx: Context) -> str:\n    '''Tool that can request additional input from users.'''\n\n    # Request sensitive information when needed\n    api_key = await ctx.elicit(\n        prompt=\"Please provide your API key:\",\n        input_type=\"password\"\n    )\n\n    # Use the provided key\n    return await api_call(resource_id, api_key)\n```\n\n**Context capabilities:**\n- `ctx.report_progress(progress, message)` - Report progress for long operations\n- `ctx.log_info(message, data)` / `ctx.log_error()` / `ctx.log_debug()` - Logging\n- `ctx.elicit(prompt, input_type)` - Request input from users\n- `ctx.fastmcp.name` - Access server configuration\n- `ctx.read_resource(uri)` - Read MCP resources\n\n### Resource Registration\n\nExpose data as resources for efficient, template-based access:\n\n```python\n@mcp.resource(\"file://documents/{name}\")\nasync def get_document(name: str) -> str:\n    '''Expose documents as MCP resources.\n\n    Resources are useful for static or semi-static data that doesn't\n    require complex parameters. They use URI templates for flexible access.\n    '''\n    document_path = f\"./docs/{name}\"\n    with open(document_path, \"r\") as f:\n        return f.read()\n\n@mcp.resource(\"config://settings/{key}\")\nasync def get_setting(key: str, ctx: Context) -> str:\n    '''Expose configuration as resources with context.'''\n    settings = await load_settings()\n    return json.dumps(settings.get(key, {}))\n```\n\n**When to use Resources vs Tools:**\n- **Resources**: For data access with simple parameters (URI templates)\n- **Tools**: For complex operations with validation and business logic\n\n### Structured Output Types\n\nFastMCP supports multiple return types beyond strings:\n\n```python\nfrom typing import TypedDict\nfrom dataclasses import dataclass\nfrom pydantic import BaseModel\n\n# TypedDict for structured returns\nclass UserData(TypedDict):\n    id: str\n    name: str\n    email: str\n\n@mcp.tool()\nasync def get_user_typed(user_id: str) -> UserData:\n    '''Returns structured data - FastMCP handles serialization.'''\n    return {\"id\": user_id, \"name\": \"John Doe\", \"email\": \"john@example.com\"}\n\n# Pydantic models for complex validation\nclass DetailedUser(BaseModel):\n    id: str\n    name: str\n    email: str\n    created_at: datetime\n    metadata: Dict[str, Any]\n\n@mcp.tool()\nasync def get_user_detailed(user_id: str) -> DetailedUser:\n    '''Returns Pydantic model - automatically generates schema.'''\n    user = await fetch_user(user_id)\n    return DetailedUser(**user)\n```\n\n### Lifespan Management\n\nInitialize resources that persist across requests:\n\n```python\nfrom contextlib import asynccontextmanager\n\n@asynccontextmanager\nasync def app_lifespan():\n    '''Manage resources that live for the server's lifetime.'''\n    # Initialize connections, load config, etc.\n    db = await connect_to_database()\n    config = load_configuration()\n\n    # Make available to all tools\n    yield {\"db\": db, \"config\": config}\n\n    # Cleanup on shutdown\n    await db.close()\n\nmcp = FastMCP(\"example_mcp\", lifespan=app_lifespan)\n\n@mcp.tool()\nasync def query_data(query: str, ctx: Context) -> str:\n    '''Access lifespan resources through context.'''\n    db = ctx.request_context.lifespan_state[\"db\"]\n    results = await db.query(query)\n    return format_results(results)\n```\n\n### Transport Options\n\nFastMCP supports two main transport mechanisms:\n\n```python\n# stdio transport (for local tools) - default\nif __name__ == \"__main__\":\n    mcp.run()\n\n# Streamable HTTP transport (for remote servers)\nif __name__ == \"__main__\":\n    mcp.run(transport=\"streamable_http\", port=8000)\n```\n\n**Transport selection:**\n- **stdio**: Command-line tools, local integrations, subprocess execution\n- **Streamable HTTP**: Web services, remote access, multiple clients\n\n---\n\n## Code Best Practices\n\n### Code Composability and Reusability\n\nYour implementation MUST prioritize composability and code reuse:\n\n1. **Extract Common Functionality**:\n   - Create reusable helper functions for operations used across multiple tools\n   - Build shared API clients for HTTP requests instead of duplicating code\n   - Centralize error handling logic in utility functions\n   - Extract business logic into dedicated functions that can be composed\n   - Extract shared markdown or JSON field selection & formatting functionality\n\n2. **Avoid Duplication**:\n   - NEVER copy-paste similar code between tools\n   - If you find yourself writing similar logic twice, extract it into a function\n   - Common operations like pagination, filtering, field selection, and formatting should be shared\n   - Authentication/authorization logic should be centralized\n\n### Python-Specific Best Practices\n\n1. **Use Type Hints**: Always include type annotations for function parameters and return values\n2. **Pydantic Models**: Define clear Pydantic models for all input validation\n3. **Avoid Manual Validation**: Let Pydantic handle input validation with constraints\n4. **Proper Imports**: Group imports (standard library, third-party, local)\n5. **Error Handling**: Use specific exception types (httpx.HTTPStatusError, not generic Exception)\n6. **Async Context Managers**: Use `async with` for resources that need cleanup\n7. **Constants**: Define module-level constants in UPPER_CASE\n\n## Quality Checklist\n\nBefore finalizing your Python MCP server implementation, ensure:\n\n### Strategic Design\n- [ ] Tools enable complete workflows, not just API endpoint wrappers\n- [ ] Tool names reflect natural task subdivisions\n- [ ] Response formats optimize for agent context efficiency\n- [ ] Human-readable identifiers used where appropriate\n- [ ] Error messages guide agents toward correct usage\n\n### Implementation Quality\n- [ ] FOCUSED IMPLEMENTATION: Most important and valuable tools implemented\n- [ ] All tools have descriptive names and documentation\n- [ ] Return types are consistent across similar operations\n- [ ] Error handling is implemented for all external calls\n- [ ] Server name follows format: `{service}_mcp`\n- [ ] All network operations use async/await\n- [ ] Common functionality is extracted into reusable functions\n- [ ] Error messages are clear, actionable, and educational\n- [ ] Outputs are properly validated and formatted\n\n### Tool Configuration\n- [ ] All tools implement 'name' and 'annotations' in the decorator\n- [ ] Annotations correctly set (readOnlyHint, destructiveHint, idempotentHint, openWorldHint)\n- [ ] All tools use Pydantic BaseModel for input validation with Field() definitions\n- [ ] All Pydantic Fields have explicit types and descriptions with constraints\n- [ ] All tools have comprehensive docstrings with explicit input/output types\n- [ ] Docstrings include complete schema structure for dict/JSON returns\n- [ ] Pydantic models handle input validation (no manual validation needed)\n\n### Advanced Features (where applicable)\n- [ ] Context injection used for logging, progress, or elicitation\n- [ ] Resources registered for appropriate data endpoints\n- [ ] Lifespan management implemented for persistent connections\n- [ ] Structured output types used (TypedDict, Pydantic models)\n- [ ] Appropriate transport configured (stdio or streamable HTTP)\n\n### Code Quality\n- [ ] File includes proper imports including Pydantic imports\n- [ ] Pagination is properly implemented where applicable\n- [ ] Filtering options are provided for potentially large result sets\n- [ ] All async functions are properly defined with `async def`\n- [ ] HTTP client usage follows async patterns with proper context managers\n- [ ] Type hints are used throughout the code\n- [ ] Constants are defined at module level in UPPER_CASE\n\n### Testing\n- [ ] Server runs successfully: `python your_server.py --help`\n- [ ] All imports resolve correctly\n- [ ] Sample tool calls work as expected\n- [ ] Error scenarios handled gracefully",
        "icartsh-plugin/skills/sequential-thinking/README.md": "# Sequential Thinking Agent Skill\n\nsequential-thinking MCP 서버의 구조화되고 성찰적인 문제 해결 방법론을 네이티브 에이전트 스킬(Agent Skill)로 변환한 것입니다.\n\n## 개요 (Overview)\n\n이 SKILL은 Claude가 외부 MCP 도구에 의존하지 않고도 복잡한 문제 해결을 위해 체계적인 순차적 사고 방법론을 적용하도록 가르칩니다. 다음이 가능해집니다:\n- 복잡한 문제를 관리 가능한 사고 시퀀스로 분해\n- 이해도가 높아짐에 따라 사고 횟수를 동적으로 조정\n- 새로운 통찰이 생겼을 때 이전 사고를 수정\n- 대안적인 추론 경로로의 분기(Branching)\n- 가설 생성 및 검증\n\n## 스킬 구조 (Skill Structure)\n\n```\nsequential-thinking/\n├── SKILL.md (105 lines)\n│   핵심 방법론, 적용 시기, 스크립트 사용법\n│\n├── package.json\n│   테스트 종속성 (jest)\n│\n├── .env.example\n│   설정 옵션\n│\n├── scripts/\n│   ├── process-thought.js (실행 파일)\n│   │   사고를 결정론적으로 검증하고 추적\n│   │\n│   └── format-thought.js (실행 파일)\n│       사고를 화면에 표시하기 위한 포맷팅 (박스/단순형/마크다운)\n│\n├── tests/\n│   ├── process-thought.test.js\n│   │   검증, 추적, 히스토리 테스트\n│   │\n│   └── format-thought.test.js\n│       포맷팅 테스트 (모든 형식)\n│\n└── references/\n    ├── core-patterns.md (95 lines)\n    │   필수 수정 및 분기 패턴\n    │\n    ├── examples-api.md (88 lines)\n    │   API 설계 예시 단계별 가이드\n    │\n    ├── examples-debug.md (90 lines)\n    │   성능 디버깅 예시\n    │\n    ├── examples-architecture.md (94 lines)\n    │   아키텍처 결정 예시\n    │\n    ├── advanced-techniques.md (76 lines)\n    │   나선형 정밀화, 가설 테스트, 수렴\n    │\n    └── advanced-strategies.md (79 lines)\n        불확실성 관리, 수정 케스케이드, 메타 사고\n```\n\n**문서**: 7개 파일에 걸친 627행 (모두 100행 미만)\n**스크립트**: 테스트가 포함된 2개의 실행 가능한 Node.js 스크립트\n\n## 주요 특징\n\n### 점진적 노출 설계 (Progressive Disclosure Design)\n각 파일은 특정 측면에 집중하며, 필요할 때만 로드됩니다:\n- **SKILL.md**: 핵심 방법론이 포함된 빠른 참조 문서\n- **core-patterns.md**: 일상적인 사용을 위한 보편적 패턴\n- **examples-*.md**: 학습을 위한 실제 사례 단계별 가이드\n- **advanced-*.md**: 복잡한 시나리오를 위한 정교한 기법\n\n### 토큰 효율성 (Token Efficiency)\n- 간결한 설명을 위해 문법보다 핵심 위주로 작성\n- 장황한 설명 대신 패턴을 보여주는 예시 활용\n- 파일 간 상호 참조를 통해 중복 방지\n\n### 방법론 변환 (Methodology Conversion)\nMCP 서버의 접근 방식을 추출하여 명령어(instructions)로 변환:\n- MCP 도구는 순차적 사고를 위한 **인터페이스**를 제공했습니다.\n- 에이전트 스킬은 순차적으로 사고하기 위한 **방법론**을 제공합니다.\n- 외부 도구에 대한 종속성 없이 순수한 지침 기반의 접근입니다.\n\n## 사용 모드\n\n**Explicit (명시적) 모드**: 가시적인 사고 마커 사용\n```\nThought 1/5: [분석]\nThought 2/5: [심층 분석]\n```\n\n**Implicit (암묵적) 모드**: 응답을 복잡하게 만들지 않고 내부적으로 방법론 적용\n\n## 이 SKILL을 사용해야 하는 경우\n\n다음에 대해 자동으로 활성화됩니다:\n- 복잡한 문제 분해\n- 수정을 동반할 수 있는 적응형 계획 수립\n- 디버깅 및 원인 분석\n- 아키텍처 및 디자인 결정\n- 범위가 불분명하거나 새로 형성되는 문제\n- 컨텍스트가 필요한 다단계 해결책\n\n## 스크립트 사용법 (Scripts Usage)\n\n### 사고 처리 (검증 및 추적)\n\n```bash\n# 사고 처리\nnode scripts/process-thought.js --thought \"Initial analysis\" --number 1 --total 5 --next true\n\n# 수정을 포함하여 처리\nnode scripts/process-thought.js --thought \"Corrected analysis\" --number 2 --total 5 --next true --revision 1\n\n# 분기를 포함하여 처리\nnode scripts/process-thought.js --thought \"Branch A\" --number 2 --total 5 --next true --branch 1 --branchId \"branch-a\"\n\n# 히스토리 보기\nnode scripts/process-thought.js --history\n\n# 히스토리 초기화\nnode scripts/process-thought.js --reset\n```\n\n### 사고 포맷팅 (표시용)\n\n```bash\n# 박스 형식 (기본값)\nnode scripts/format-thought.js --thought \"Analysis\" --number 1 --total 5\n\n# 단순 텍스트 형식\nnode scripts/format-thought.js --thought \"Analysis\" --number 1 --total 5 --format simple\n\n# 마크다운 형식\nnode scripts/format-thought.js --thought \"Analysis\" --number 1 --total 5 --format markdown\n\n# 수정을 포함한 경우\nnode scripts/format-thought.js --thought \"Revised\" --number 2 --total 5 --revision 1\n\n# 분기를 포함한 경우\nnode scripts/format-thought.js --thought \"Branch\" --number 2 --total 5 --branch 1 --branchId \"a\"\n```\n\n### 테스트 실행\n\n```bash\n# 종속성 설치 (최초 1회)\nnpm install\n\n# 모든 테스트 실행\nnpm test\n\n# watch 모드로 테스트 실행\nnpm run test:watch\n\n# 테스트 커버리지 확인\nnpm run test:coverage\n```\n\n## 스크립트 사용 시기\n\n**스크립트를 사용하는 경우**:\n- 사고 구조에 대한 결정론적인 검증이 필요할 때\n- 지속적인 사고 히스토리 추적이 필요할 때\n- 문서화를 위해 포맷팅된 출력이 필요할 때\n- 순차적 사고와 통합된 도구를 빌드할 때\n\n**스크립트를 사용하지 않는 경우**:\n- 응답 내에서 방법론을 직접 적용할 때\n- 가볍게 인라인 사고를 수행하고 싶을 때\n- 검증이나 추적이 필요 없을 때\n\n스크립트는 **선택적 도구**입니다 - 방법론은 스크립트 없이도 적용될 수 있습니다.\n\n## 출처 (Source)\n\n다음에서 변환됨: https://github.com/modelcontextprotocol/servers/tree/main/src/sequentialthinking\n\nAnthropic의 오리지널 MCP 서버 (MIT License).\n스킬 변환:\n- 방법론을 지침으로 추출\n- 결정론적 검증을 위한 실행 가능한 스크립트 추가\n- 기능을 보존하면서 도구 독립적으로 구현\n",
        "icartsh-plugin/skills/sequential-thinking/SKILL.md": "---\nname: sequential-thinking\ndescription: 다단계 분석, 수정 능력 및 가설 검증이 필요한 복잡한 작업을 위해 구조화되고 성찰적인 문제 해결 방식을 적용합니다. 복잡한 문제 분해, 적응형 계획 수립, 경로 수정이 필요한 분석, 범위가 불분명한 문제, 다단계 해결책 및 가설 기반 작업 시 사용합니다.\nversion: 1.0.0\nlicense: MIT\n---\n\n# Sequential Thinking\n\n역동적인 조정을 동반한 관리 가능하고 성찰적인 사고 시퀀스를 통해 구조화된 문제 해결을 수행합니다.\n\n## 적용 시기\n\n- 복잡한 문제의 분해\n- 수정을 포함한 적응형 계획 수립\n- 경로 수정(course correction)이 필요한 분석\n- 범위가 불분명하거나 새로 형성되는 문제\n- 컨텍스트 유지가 필요한 다단계 해결책\n- 가설 기반의 조사/디버깅\n\n## 핵심 프로세스 (Core Process)\n\n### 1. 개략적인 추정으로 시작\n```\nThought 1/5: [초기 분석]\n```\n이해도가 높아짐에 따라 역동적으로 조정하세요.\n\n### 2. 각 사고의 구조화\n- 이전 컨텍스트를 명시적으로 기반으로 삼음\n- 사고당 하나의 측면만 다룸\n- 가정, 불확실성, 깨달은 점을 명시\n- 다음 사고에서 다루어야 할 내용 예고\n\n### 3. 역동적인 조정 (Dynamic Adjustment) 적용\n- **확장 (Expand)**: 더 많은 복잡성 발견 → 총 사고 횟수 증가\n- **축소 (Contract)**: 예상보다 단순함 → 총 사고 횟수 감소\n- **수정 (Revise)**: 새로운 통찰이 이전을 무효화함 → 수정 마킹\n- **분기 (Branch)**: 여러 접근 방식 존재 → 대안 탐색\n\n### 4. 필요시 수정(Revision) 사용\n```\nThought 5/8 [Thought 2의 REVISION]: [수정된 이해]\n- 이전 내용: [언급되었던 내용]\n- 수정 이유: [새로운 통찰]\n- 영향: [변경되는 사항]\n```\n\n### 5. 대안을 위한 분기 (Branching)\n```\nThought 4/7 [Thought 2에서 시작된 BRANCH A]: [접근 방식 A]\nThought 4/7 [Thought 2에서 시작된 BRANCH B]: [접근 방식 B]\n```\n명시적으로 비교하고, 결정 근거와 함께 수렴(converge)시키세요.\n\n### 6. 가설 생성 및 검증\n```\nThought 6/9 [HYPOTHESIS]: [제안된 해결책]\nThought 7/9 [VERIFICATION]: [테스트 결과]\n```\n가설이 검증될 때까지 반복하세요.\n\n### 7. 준비가 되었을 때만 완료\n최종 마킹: `Thought N/N [FINAL]`\n\n완료 조건:\n- 해결책 검증 완료\n- 모든 핵심 측면 처리 완료\n- 확신(Confidence) 확보\n- 해결되지 않은 불확실성 없음\n\n## 적용 모드 (Application Modes)\n\n**Explicit (명시적)**: 복잡성이 가시적인 추론을 정당화하거나 사용자가 분석을 요청할 때 가시적인 사고 마커를 사용합니다.\n\n**Implicit (암묵적)**: 응답을 복잡하게 만들지 않으면서 정확도를 높이기 위해 일상적인 문제 해결 시 내부적으로 방법론을 적용합니다.\n\n## 스크립트 (선택 사항)\n\n결정론적 검증/추적을 위한 선택적 스크립트:\n- `scripts/process-thought.js` - 히스토리와 함께 사고를 검증하고 추적\n- `scripts/format-thought.js` - 표시용 포맷팅 (박스/마크다운/단순형)\n\n사용 예시는 README.md를 참조하세요. 검증이나 로그 보존이 필요할 때 사용하며, 그렇지 않으면 방법론을 직접 적용합니다.\n\n## 참조 문서 (References)\n\n더 깊은 이해가 필요할 때 로드하세요:\n- `references/core-patterns.md` - 수정 및 분기 패턴\n- `references/examples-api.md` - API 설계 예시\n- `references/examples-debug.md` - 디버깅 예시\n- `references/examples-architecture.md` - 아키텍처 결정 예시\n- `references/advanced-techniques.md` - 나선형 정밀화(spiral refinement), 가설 테스트, 수렴\n- `references/advanced-strategies.md` - 불확실성, 수정 케스케이드(revision cascades), 메타 사고(meta-thinking)\n",
        "icartsh-plugin/skills/sequential-thinking/references/advanced-strategies.md": "# Advanced Sequential Thinking Strategies\n\nAdditional sophisticated patterns for complex scenarios.\n\n## Uncertainty Management\n\nHandle incomplete information systematically.\n\n```\nThought 2/7: Need to decide X\nThought 3/7: Insufficient data—two scenarios possible\nThought 4/7 [SCENARIO A if P true]: Analysis for A\nThought 4/7 [SCENARIO B if P false]: Analysis for B\nThought 5/7: Decision that works for both scenarios\nThought 6/7: Or determine critical info needed\nThought 7/7 [FINAL]: Robust solution or clear info requirement\n```\n\n**Use for**: Decisions under uncertainty, incomplete requirements.\n\n**Strategies**:\n- Find solution robust to uncertainty\n- Identify minimal info needed to resolve\n- Make safe assumptions with clear documentation\n\n## Revision Cascade Management\n\nHandle revisions that invalidate multiple subsequent thoughts.\n\n```\nThought 1/8: Foundation assumption\nThought 2/8: Build on Thought 1\nThought 3/8: Further build\nThought 4/8: Discover Thought 1 invalid\nThought 5/8 [REVISION of Thought 1]: Corrected foundation\nThought 6/8 [REASSESSMENT]: Which of 2-3 still valid?\n  - Thought 2: Partially valid, needs adjustment\n  - Thought 3: Completely invalid\nThought 7/8: Rebuild from corrected Thought 5\nThought 8/8 [FINAL]: Solution on correct foundation\n```\n\n**Key**: After major revision, explicitly assess downstream impact.\n\n## Meta-Thinking Calibration\n\nMonitor and adjust thinking process itself.\n\n```\nThought 5/9: [Regular thought]\nThought 6/9 [META]: Past 3 thoughts circling without progress\n  Analysis: Missing key information\n  Adjustment: Need to research X before continuing\nThought 7/9: Research findings on X\nThought 8/9: Now can proceed with informed decision\nThought 9/9: [Resume productive path]\n```\n\n**Use when**: Stuck, circling, or unproductive pattern noticed.\n**Action**: Pause, identify issue, adjust strategy.\n\n## Parallel Constraint Satisfaction\n\nHandle multiple independent constraints simultaneously.\n\n```\nThought 2/10: Solution must satisfy A, B, C\nThought 3/10 [CONSTRAINT A]: Solutions satisfying A: {X, Y, Z}\nThought 4/10 [CONSTRAINT B]: Solutions satisfying B: {Y, Z, W}\nThought 5/10 [CONSTRAINT C]: Solutions satisfying C: {X, Z}\nThought 6/10 [INTERSECTION]: Z satisfies all\nThought 7/10: Verify Z feasible\nThought 8/10 [BRANCH if infeasible]: Relax which constraint?\nThought 9/10: Decision on constraint relaxation if needed\nThought 10/10 [FINAL]: Optimal solution given constraints\n```\n\n**Use for**: Optimization problems, multi-criteria decisions.\n**Pattern**: Analyze independently → Find intersection → Verify feasibility.\n",
        "icartsh-plugin/skills/sequential-thinking/references/advanced-techniques.md": "# Advanced Sequential Thinking Techniques\n\nComplex problem-solving patterns.\n\n## Spiral Refinement\n\nReturn to concepts with progressively deeper understanding.\n\n```\nThought 1/7: Initial design (surface)\nThought 2/7: Discover constraint A\nThought 3/7: Refine for A\nThought 4/7: Discover constraint B\nThought 5/7: Refine for both A and B\nThought 6/7: Integration reveals edge case\nThought 7/7: Final design addressing all constraints\n```\n\n**Use for**: Complex systems where constraints emerge iteratively.\n**Key**: Each return is refinement, not restart.\n\n## Hypothesis-Driven Investigation\n\nSystematic hypothesis generation and testing.\n\n```\nThought 1/6: Observe symptoms\nThought 2/6 [HYPOTHESIS]: Explanation X\nThought 3/6 [VERIFICATION]: Test X—partial match\nThought 4/6 [REFINED HYPOTHESIS]: Adjusted Y\nThought 5/6 [VERIFICATION]: Test Y—confirmed\nThought 6/6 [FINAL]: Solution based on verified Y\n```\n\n**Use for**: Debugging, root cause analysis, diagnostics.\n**Pattern**: Generate → Test → Refine → Re-test loop.\n\n## Multi-Branch Convergence\n\nExplore alternatives, then synthesize best approach.\n\n```\nThought 2/8: Multiple viable approaches\nThought 3/8 [BRANCH A]: Approach A benefits\nThought 4/8 [BRANCH A]: Approach A drawbacks\nThought 5/8 [BRANCH B]: Approach B benefits\nThought 6/8 [BRANCH B]: Approach B drawbacks\nThought 7/8 [CONVERGENCE]: Hybrid combining A's X with B's Y\nThought 8/8 [FINAL]: Hybrid superior to either alone\n```\n\n**Use for**: Complex decisions where neither option clearly best.\n**Key**: Convergence often yields better solution than either branch.\n\n## Progressive Context Deepening\n\nBuild understanding in layers from abstract to concrete.\n\n```\nThought 1/9: High-level problem\nThought 2/9: Identify major components\nThought 3/9: Zoom into component A (detailed)\nThought 4/9: Zoom into component B (detailed)\nThought 5/9: Identify A-B interactions\nThought 6/9: Discover emergent constraint\nThought 7/9 [REVISION of 3-4]: Adjust for interaction\nThought 8/9: Verify complete system\nThought 9/9 [FINAL]: Integrated solution\n```\n\n**Use for**: System design, architecture, integration problems.\n**Pattern**: Abstract → Components → Details → Interactions → Integration.\n\n## Reference\n\nSee `advanced-strategies.md` for: Uncertainty Management, Revision Cascade Management, Meta-Thinking Calibration, Parallel Constraint Satisfaction.\n",
        "icartsh-plugin/skills/sequential-thinking/references/core-patterns.md": "# Core Sequential Thinking Patterns\n\nEssential revision and branching patterns.\n\n## Revision Patterns\n\n### Assumption Challenge\nEarly assumption proves invalid with new data.\n```\nThought 1/5: Assume X is bottleneck\nThought 4/5 [REVISION of Thought 1]: X adequate; Y is actual bottleneck\n```\n\n### Scope Expansion\nProblem larger than initially understood.\n```\nThought 1/4: Fix bug\nThought 4/5 [REVISION of scope]: Architectural redesign needed, not patch\n```\n\n### Approach Shift\nInitial strategy inadequate for requirements.\n```\nThought 2/6: Optimize query\nThought 5/6 [REVISION of Thought 2]: Optimization + cache layer required\n```\n\n### Understanding Deepening\nLater insight fundamentally changes interpretation.\n```\nThought 1/5: Feature broken\nThought 4/5 [REVISION of Thought 1]: Not bug—UX confusion issue\n```\n\n## Branching Patterns\n\n### Trade-off Evaluation\nCompare approaches with different trade-offs.\n```\nThought 3/7: Choose between X and Y\nThought 4/7 [BRANCH A]: X—simpler, less scalable\nThought 4/7 [BRANCH B]: Y—complex, scales better\nThought 5/7: Choose Y for long-term needs\n```\n\n### Risk Mitigation\nPrepare backup for high-risk primary approach.\n```\nThought 2/6: Primary: API integration\nThought 3/6 [BRANCH A]: API details\nThought 3/6 [BRANCH B]: Fallback: webhook\nThought 4/6: Implement A with B contingency\n```\n\n### Parallel Exploration\nInvestigate independent concerns separately.\n```\nThought 3/8: Two unknowns—DB schema & API design\nThought 4/8 [BRANCH DB]: DB options\nThought 4/8 [BRANCH API]: API patterns\nThought 5/8: Integrate findings\n```\n\n### Hypothesis Testing\nTest multiple explanations systematically.\n```\nThought 2/6: Could be A, B, or C\nThought 3/6 [BRANCH A]: Test A—not cause\nThought 3/6 [BRANCH B]: Test B—confirmed\nThought 4/6: Root cause via Branch B\n```\n\n## Adjustment Guidelines\n\n**Expand when**: Complexity discovered, multiple aspects identified, verification needed, alternatives require exploration.\n\n**Contract when**: Key insight solves earlier, problem simpler, steps merge naturally.\n\n**Example**:\n```\nThought 1/5: Initial\nThought 3/7: Complexity (5→7)\nThought 5/8: Another aspect (7→8)\nThought 8/8 [FINAL]: Complete\n```\n\n## Anti-Patterns\n\n**Premature Completion**: Rushing without verification → Add verification thoughts.\n\n**Revision Cascade**: Repeated revisions without understanding why → Identify root cause.\n\n**Branching Explosion**: Too many branches → Limit to 2-3, converge before more.\n\n**Context Loss**: Ignoring earlier insights → Reference previous thoughts explicitly.\n",
        "icartsh-plugin/skills/sequential-thinking/references/examples-api.md": "# Example: API Design with Sequential Thinking\n\n**Problem**: Design authentication API for multi-tenant SaaS.\n\n## Without Sequential Thinking\n\"Use JWT with OAuth2. Add tenant ID to claims. Implement refresh tokens.\"\n(Misses edge cases, scaling, security)\n\n## With Sequential Thinking\n\n```\nThought 1/5: Requirements\n\nMulti-tenant SaaS authentication needs:\n- Tenant isolation (prevent cross-tenant access)\n- Scalability (thousands of tenants)\n- Security (industry standards)\n\nUncertainty: Session vs token? Stateful vs stateless?\nNext: Evaluate approaches\n\nThought 2/6: Approach evaluation (adjusted)\n\nComparing session vs JWT:\n- Session: Server state, easy revocation, harder scaling\n- JWT: Stateless, scales well, complex revocation\n\nMulti-tenancy complicates sessions (which DB?).\nJWT better for distributed systems.\nNext: JWT structure\n\nThought 3/6: JWT payload design\n\nJWT includes: User ID, Tenant ID, Permissions, Expiration\n\nUncertainty: Where verify tenant? Gateway vs service?\nNext: Token refresh\n\nThought 4/6 [REVISION of Thought 3]: Security fix\n\nOriginal: Tenant ID in JWT claims.\nIssue: Claims visible (base64), could read tenant IDs.\n\nCorrected:\n- Keep tenant ID (needed for routing)\n- Add gateway verification\n- Log access attempts\n\nImpact: Need gateway security layer\n\nThought 5/7: Refresh strategy (adjusted)\n\nAccess: 15 min\nRefresh: 7 days, rotate on use\n\nStorage:\n- Access: Client memory\n- Refresh: HttpOnly cookie + DB revocation\n\nEnables revocation + stateless benefits.\n\nThought 6/7: Gateway flow\n\n1. Verify JWT signature\n2. Extract tenant ID\n3. Verify user belongs to tenant (cache + DB)\n4. Forward with verified context\n\nAddresses Thought 4 security concern.\n\nThought 7/7 [FINAL]: Complete solution\n\n- JWT with refresh rotation\n- Tenant ID + gateway verification\n- 15-min access, 7-day refresh\n- HttpOnly cookies + revocation DB\n- Gateway enforces isolation\n\nConfidence: High\nAssumptions: Distributed, centralized gateway\nTrade-offs: Slight latency vs security\n```\n\n## Key Outcomes\n\n1. **Revision caught security issue**: Tenant ID visibility\n2. **Dynamic adjustment**: 5→7 thoughts as complexity emerged\n3. **Comprehensive**: Authentication, authorization, multi-tenancy, revocation\n",
        "icartsh-plugin/skills/sequential-thinking/references/examples-architecture.md": "# Example: Architecture Decision with Sequential Thinking\n\n**Problem**: Choose state management for growing React app.\n\n## Sequential Thinking Approach\n\n```\nThought 1/4: Initial assessment\n\nReact app growing complex. useState scattered across components.\nNeed centralized state management.\nOptions: Redux, MobX, Zustand, Context API, Jotai.\n\nNext: Define requirements\n\nThought 2/5: Requirements (adjusted up)\n\nApp needs:\n- Real-time updates (WebSocket)\n- Offline support\n- Complex nested state\n- Multiple devs (maintainability)\n- TypeScript (type safety)\n\nThese eliminate simpler solutions.\nNext: Evaluate remaining options\n\nThought 3/5 [REVISION of Thought 1]: Scope refinement\n\nOriginal assumption: Need global state management.\nRealization: Not all state needs centralization.\n\nBetter breakdown:\n- Server state (API data): React Query\n- UI state (modals, forms): Local useState\n- Auth state: Context API\n- App-wide settings: Simple global store\n\nThis reduces problem scope significantly.\nChanges evaluation criteria.\nNext: Re-evaluate with refined scope\n\nThought 4/6: Re-evaluation (adjusted)\n\nOnly need lightweight store for app-wide settings.\nZustand fits:\n- Minimal boilerplate\n- Excellent TypeScript support\n- No provider wrapper\n- ~1KB size\n\nReact Query handles server state (caching, refetch, offline).\nMost state stays local.\n\nNext: Verify this handles all requirements\n\nThought 5/6 [HYPOTHESIS]: Hybrid approach\n\nHypothesis: Zustand + React Query + local state solves needs.\n\n- Zustand: Theme, user prefs, global UI flags\n- React Query: All server data, caching, sync\n- useState: Component-local UI\n\nBenefits: Simple, performant, maintainable.\nUncertainty: Real-time updates?\n\nThought 6/6 [FINAL]: Verified solution\n\nReact Query supports WebSocket via custom hooks.\nZustand handles global UI simply.\nLocal state for component-specific needs.\n\nArchitecture decision:\n- Zustand for global UI state\n- React Query for all server interactions\n- Local useState for component state\n\nConfidence: High\nAssumptions: Team comfortable with React Query patterns\nBenefits: Simpler than Redux, more powerful than Context\nTrade-offs: Learning curve for React Query (but worthwhile)\n```\n\n## Key Insights\n\n1. **Scope revision crucial**: Original \"need global state\" reframed to specific state categories\n2. **Separation of concerns**: Different state types need different solutions\n3. **Hypothesis verification**: Confirmed WebSocket support before finalizing\n4. **Simpler is better**: Avoided over-engineering with heavy Redux solution\n\n## Impact of Revision\n\nWithout Thought 3 revision, might have chosen Redux for all state—significant over-engineering. Revision led to more appropriate, simpler solution.\n",
        "icartsh-plugin/skills/sequential-thinking/references/examples-debug.md": "# Example: Performance Debugging with Sequential Thinking\n\n**Problem**: API endpoint slow (2-3s instead of <200ms).\n\n## Sequential Thinking Approach\n\n```\nThought 1/5: Initial investigation\n\nEndpoint: GET /api/users/{id}/dashboard\nExpected: <200ms\nActual: 2-3s\n\nNeed to identify bottleneck.\nNext: Analyze endpoint logic\n\nThought 2/5: Endpoint analysis\n\nDashboard fetches:\n- User profile\n- Recent activities (last 50)\n- Notifications (unread)\n- Analytics summary\n\nEach is separate DB query. Could be N+1 problem.\nNext: Profile actual execution\n\nThought 3/6: Profiling results (adjusted up—more complex)\n\nProfiling:\n- User profile: 20ms\n- Activities: 1800ms (!!)\n- Notifications: 100ms\n- Analytics: 80ms\n\nActivities query is bottleneck (90% of time).\nNeed to examine that query.\nNext: Investigate activities query\n\nThought 4/6 [BRANCH A]: Hypothesis - N+1 query\n\nActivities table has user_id FK to users.\nQuery might fetch activities, then user for each (N+1).\nVerification: Check if uses joins or loops.\n\nThought 4/6 [BRANCH B]: Hypothesis - Missing index\n\nActivities table might lack index on user_id + created_at.\nWithout index, full table scan for user's activities.\nVerification: Check table indexes.\n\nThought 5/6 [VERIFICATION]: Testing hypotheses\n\nQuery examination: Uses proper joins (not N+1).\nIndex check: user_id indexed, NOT created_at.\nQuery filters by user_id AND sorts by created_at.\n\nBranch B confirmed: Missing composite index causes slow sort.\nBranch A eliminated: No N+1 issue.\n\nRoot cause identified.\nNext: Implement solution\n\nThought 6/6 [FINAL]: Solution and verification\n\nCreate composite index:\n```sql\nCREATE INDEX idx_activities_user_created\nON activities(user_id, created_at DESC);\n```\n\nTested: Response time now 120ms (target met).\n\nConfidence: High (verified with profiling)\nResult: 95% performance improvement\n```\n\n## Key Techniques Demonstrated\n\n1. **Branching for hypothesis testing**: Explored N+1 vs index hypotheses in parallel\n2. **Verification before solution**: Tested both hypotheses systematically\n3. **Data-driven decisions**: Used profiling data to guide investigation\n4. **Dynamic adjustment**: Expanded thought count when complexity emerged\n5. **Elimination method**: Ruled out N+1, confirmed index issue\n\n## Comparison\n\n**Without sequential thinking**: Might jump to N+1 conclusion (common issue), waste time optimizing wrong thing.\n\n**With sequential thinking**: Systematically tested hypotheses, identified actual root cause, implemented correct fix.\n",
        "icartsh-plugin/skills/skill-creator/SKILL.md": "---\nname: skill-creator\ndescription: 효과적인 SKILL을 제작하기 위한 가이드입니다. 특화된 지식, 워크플로우 또는 도구 통합을 통해 Claude의 기능을 확장하는 새로운 SKILL을 생성하거나 기존 SKILL을 업데이트하려는 경우 이 SKILL을 사용하세요.\nlicense: LICENSE.txt의 전체 약관 참조\n---\n\n# Skill Creator\n\n이 SKILL은 효과적인 SKILL을 생성하기 위한 지침을 제공합니다.\n\n## SKILL에 대하여\n\nSKILL은 특화된 지식, 워크플로우 및 도구를 제공하여 Claude의 기능을 확장하는 모듈화된 독립 패키지입니다. 특정 분야나 작업을 위한 \"온보딩 가이드\"라고 생각하세요. SKILL은 Claude를 일반 에이전트에서 특정 모델이 온전히 소유하기 어려운 절차적 지식을 갖춘 전문 에이전트로 변화시킵니다.\n\n### SKILL이 제공하는 것\n\n1. 특화된 워크플로우 - 특정 분야를 위한 다단계 절차\n2. 도구 통합 - 특정 파일 형식 또는 API 작업 지침\n3. 도메인 전문 지식 - 회사별 지식, 스키마, 비즈니스 로직\n4. 번들 리소스 - 복잡하고 반복적인 작업을 위한 스크립트, 참조 문서 및 에셋\n\n## 핵심 원칙 (Core Principles)\n\n### 간결함이 핵심 (Concise is Key)\n\n컨텍스트 윈도우(context window)는 공공재와 같습니다. SKILL은 시스템 프롬프트, 대화 히스토리, 다른 SKILL의 메타데이터, 그리고 실제 사용자 요청 등 Claude가 필요로 하는 모든 요소와 컨텍스트 공간을 공유합니다.\n\n**기본 전제: Claude는 이미 충분히 똑똑합니다.** Claude가 아직 가지고 있지 않은 컨텍스트만 추가하세요. 정보 하나하나에 대해 자문해 보세요: \"Claude에게 이 설명이 정말 필요한가?\", \"이 문단이 소모되는 토큰만큼의 가치가 있는가?\"\n\n장황한 설명보다는 간결한 예시를 선호하세요.\n\n### 적절한 자유도 설정 (Set Appropriate Degrees of Freedom)\n\n작업의 취약성과 변동성에 맞춰 구체화 수준을 조정하세요:\n\n**높은 자유도 (텍스트 기반 지침)**: 여러 접근 방식이 유효하거나, 결정이 컨텍스트에 달려 있거나, 휴리스틱(heuristics)이 접근 방식을 안내할 때 사용합니다.\n\n**중간 자유도 (의사코드 또는 파라미터가 있는 스크립트)**: 선호되는 패턴이 존재하거나, 약간의 변동이 허용되거나, 설정이 동작에 영향을 줄 때 사용합니다.\n\n**낮은 자유도 (특정 스크립트, 적은 파라미터)**: 작업이 취약하고 오류가 발생하기 쉽거나, 일관성이 중요하거나, 특정 순서를 반드시 지켜야 할 때 사용합니다.\n\nClaude가 길을 탐색하는 것을 상상해 보세요: 절벽이 있는 좁은 다리에는 구체적인 보호 난간(낮은 자유도)이 필요하고, 탁 트인 들판에서는 다양한 경로(높은 자유도)가 허용됩니다.\n\n### SKILL의 구조 (Anatomy of a Skill)\n\n모든 SKILL은 필수 파일인 SKILL.md와 선택적인 번들 리소스로 구성됩니다:\n\n```\nskill-name/\n├── SKILL.md (필수)\n│   ├── YAML frontmatter metadata (필수)\n│   │   ├── name: (필수)\n│   │   └── description: (필수)\n│   └── Markdown 지침 (필수)\n└── Bundled Resources (선택 사항)\n    ├── scripts/          - 실행 가능한 코드 (Python/Bash 등)\n    ├── references/       - 필요에 따라 컨텍스트에 로드할 문서\n    └── assets/           - 출력물에 사용될 파일 (템플릿, 아이콘, 폰트 등)\n```\n\n#### SKILL.md (필수)\n\n모든 SKILL.md는 다음으로 구성됩니다:\n\n- **Frontmatter** (YAML): `name`과 `description` 필드를 포함합니다. Claude가 SKILL의 사용 여부를 결정하기 위해 읽는 유일한 필드이므로, SKILL이 무엇인지와 언제 사용해야 하는지에 대해 명확하고 포괄적으로 설명하는 것이 매우 중요합니다.\n- **Body** (Markdown): SKILL을 사용하기 위한 지침과 안내입니다. SKILL이 트리거된 '이후'에만 로드됩니다.\n\n#### 번들 리소스 (선택 사항)\n\n##### 스크립트 (`scripts/`)\n\n결정론적인 신뢰성이 필요하거나 반복적으로 재작성되는 작업을 위한 실행 가능한 코드(Python/Bash 등)입니다.\n\n- **포함 시기**: 동일한 코드가 반복적으로 작성되거나 결정론적인(deterministic) 신뢰성이 필요한 경우\n- **예시**: PDF 회전 작업을 위한 `scripts/rotate_pdf.py`\n- **장점**: 토큰 효율적이며 결정론적이고, 컨텍스트에 로드하지 않고도 실행될 수 있음\n- **참고**: 패칭(patching)이나 환경별 조정을 위해 Claude가 스크립트를 읽어야 할 수도 있음\n\n##### 참조 문서 (`references/`)\n\nClaude의 프로세스와 사고를 돕기 위해 필요에 따라 컨텍스트에 로드하도록 의도된 문서 및 참조 자료입니다.\n\n- **포함 시기**: Claude가 작업 중에 참조해야 할 문서가 있는 경우\n- **예시**: 금융 스키마를 위한 `references/finance.md`, 회사 NDA 템플릿을 위한 `references/mnda.md`, 회사 정책을 위한 `references/policies.md`, API 사양을 위한 `references/api_docs.md`\n- **유스케이스**: 데이터베이스 스키마, API 문서, 도메인 지식, 회사 정책, 상세 워크플로우 가이드\n- **장점**: SKILL.md를 가볍게 유지하며, Claude가 필요하다고 판단할 때만 로드됨\n- **모범 사례**: 파일이 큰 경우(>10k 단어), SKILL.md에 grep 검색 패턴을 포함하세요.\n- **중복 방지**: 정보는 SKILL.md나 참조 파일 중 한 곳에만 있어야 합니다. 정보의 발견 가능성은 유지하면서 컨텍스트 윈도우 점유를 줄이기 위해, 정말 핵심적인 지침이 아니라면 상세 정보는 참조 파일에 두는 것을 선호하세요. SKILL.md에는 필수적인 절차 지침과 워크플로우 안내만 유지하고, 상세 참조 자료, 스키마, 예시는 참조 파일로 옮기세요.\n\n##### 에셋 (`assets/`)\n\n컨텍스트에 로드하도록 의도된 것이 아니라, Claude가 생성하는 출력물 내에서 사용하기 위한 파일입니다.\n\n- **포함 시기**: 최종 출력물에 사용될 파일이 SKILL에 필요한 경우\n- **예시**: 브랜드 에셋용 `assets/logo.png`, 파워포인트 템플릿용 `assets/slides.pptx`, HTML/React 보일러플레이트용 `assets/frontend-template/`, 타이포그래피용 `assets/font.ttf`\n- **유스케이스**: 템플릿, 이미지, 아이콘, 보일러플레이트 코드, 폰트, 복사하거나 수정하여 사용할 샘플 문서\n- **장점**: 출력용 리소스를 문서와 분리하며, Claude가 파일을 컨텍스트에 로드하지 않고도 사용할 수 있게 함\n\n#### SKILL에 포함하지 말아야 할 것\n\nSKILL은 그 기능을 직접적으로 지원하는 필수 파일들만 포함해야 합니다. 다음과 같은 불필요한 문서나 보조 파일들을 만들지 마세요:\n\n- README.md\n- INSTALLATION_GUIDE.md\n- QUICK_REFERENCE.md\n- CHANGELOG.md\n- 기타 등등\n\nSKILL에는 AI 에이전트가 주어진 작업을 수행하는 데 필요한 정보만 포함되어야 합니다. 제작 과정에 대한 부수적인 컨텍스트, 설정 및 테스트 절차, 사용자용 문서 등은 포함하지 마세요. 불필요한 문서 파일은 혼란만 가중시킵니다.\n\n### 점진적 노출 디자인 원칙 (Progressive Disclosure Design Principle)\n\nSKILL은 컨텍스트를 효율적으로 관리하기 위해 3단계 로딩 시스템을 사용합니다:\n\n1. **메타데이터 (name + description)** - 항상 컨텍스트에 상주 (~100 단어)\n2. **SKILL.md 본문** - SKILL이 트리거될 때 로드 (<5k 단어)\n3. **번들 리소스** - Claude가 필요할 때 로드 (스크립트는 컨텍스트 로드 없이 실행 가능하므로 용량 제한 없음)\n\n#### 점진적 노출 패턴\n\n컨텍스트 비대화를 최소화하기 위해 SKILL.md 본문은 필수 사항 위주로 500행 미만으로 유지하세요. 이 제한에 도달하면 콘텐츠를 별도 파일로 나누세요. 콘텐츠를 다른 파일로 분리할 때는, SKILL을 읽는 모델이 해당 파일의 존재와 사용 시점을 알 수 있도록 SKILL.md에서 이를 참조하고 언제 읽어야 하는지 명확히 설명하는 것이 매우 중요합니다.\n\n**핵심 원칙:** SKILL이 여러 변형(variations), 프레임워크 또는 옵션을 지원하는 경우, SKILL.md에는 핵심 워크플로우와 선택 가이드만 유지하세요. 특정 변형에 국한된 세부 사항(패턴, 예시, 설정)은 별도의 참조 파일로 옮기세요.\n\n**패턴 1: 참조 문서를 동반한 상위 수준 가이드**\n\n```markdown\n# PDF Processing\n\n## 빠른 시작\n\npdfplumber로 텍스트 추출:\n[코드 예시]\n\n## 고급 기능\n\n- **폼 채우기(Form filling)**: 전체 가이드는 [FORMS.md](FORMS.md) 참조\n- **API 참조**: 모든 메서드는 [REFERENCE.md](REFERENCE.md) 참조\n- **예시**: 일반적인 패턴은 [EXAMPLES.md](EXAMPLES.md) 참조\n```\n\nClaude는 필요할 때만 FORMS.md, REFERENCE.md 또는 EXAMPLES.md를 로드합니다.\n\n**패턴 2: 도메인별 조직화**\n\n여러 도메인을 가진 SKILL의 경우, 무관한 컨텍스트 로드를 피하기 위해 도메인별로 콘텐츠를 구성하세요:\n\n```\nbigquery-skill/\n├── SKILL.md (개요 및 탐색)\n└── reference/\n    ├── finance.md (매출, 청구 지표)\n    ├── sales.md (기회, 파이프라인)\n    ├── product.md (API 사용량, 기능)\n    └── marketing.md (캠페인, 어트리뷰션)\n```\n\n사용자가 매출 지표에 대해 물으면, Claude는 sales.md만 읽습니다.\n\n마찬가지로 여러 프레임워크나 변형을 지원하는 SKILL도 변형별로 정리하세요:\n\n```\ncloud-deploy/\n├── SKILL.md (워크플로우 + 프로바이더 선택)\n└── references/\n    ├── aws.md (AWS 배포 패턴)\n    ├── gcp.md (GCP 배포 패턴)\n    └── azure.md (Azure 배포 패턴)\n```\n\n사용자가 AWS를 선택하면, Claude는 aws.md만 읽습니다.\n\n**패턴 3: 조건부 세부 사항**\n\n기본 콘텐츠를 보여주고, 고급 콘텐츠로 링크를 제공하세요:\n\n```markdown\n# DOCX Processing\n\n## 문서 생성\n\n새 문서에는 docx-js를 사용하세요. [DOCX-JS.md](DOCX-JS.md) 참조.\n\n## 문서 편집\n\n단순한 편집의 경우 XML을 직접 수정하세요.\n\n**수정 내용 추적(tracked changes)의 경우**: [REDLINING.md](REDLINING.md) 참조\n**OOXML 세부 사항의 경우**: [OOXML.md](OOXML.md) 참조\n```\n\nClaude는 사용자가 해당 기능을 필요로 할 때만 REDLINING.md 또는 OOXML.md를 읽습니다.\n\n**중요 가이드라인:**\n\n- **깊은 중첩 참조 지양** - 참조는 SKILL.md에서 한 단계 깊이까지만 유지하세요. 모든 참조 파일은 SKILL.md에서 직접 링크되어야 합니다.\n- **긴 참조 파일 구조화** - 100행 이상의 파일은 상단에 목차를 포함하여 Claude가 미리보기 시 전체 범위를 파악할 수 있게 하세요.\n\n## SKILL 제작 프로세스 (Skill Creation Process)\n\nSKILL 제작은 다음 단계로 이루어집니다:\n\n1. 구체적인 예시를 통한 SKILL 이해 (Understand)\n2. 재사용 가능한 SKILL 콘텐츠 계획 (Plan)\n3. SKILL 초기화 (init_skill.py 실행)\n4. SKILL 편집 (리소스 구현 및 SKILL.md 작성)\n5. SKILL 패키징 (package_skill.py 실행)\n6. 실제 사용을 바탕으로 반복 개선 (Iterate)\n\n정당한 사유가 없는 한 이 단계를 순서대로 따르세요.\n\n### Step 1: 구체적인 예시를 통한 SKILL 이해\n\nSKILL의 사용 패턴이 이미 명확히 이해된 경우에만 이 단계를 생략하세요. 기존 SKILL을 개선할 때도 이 단계는 가치가 있습니다.\n\n효과적인 SKILL을 제작하려면 SKILL이 어떻게 사용될지에 대한 구체적인 예시를 명확히 이해해야 합니다. 이는 직접적인 사용자 예시나, 사용자 피드백을 통해 검증된 생성 예시를 통해 가능합니다.\n\n예를 들어, image-editor SKILL을 빌드할 때 다음과 같은 질문들이 도움이 됩니다:\n\n- \"image-editor SKILL은 어떤 기능을 지원해야 합니까? 편집, 회전, 그 외에 다른 기능은요?\"\n- \"이 SKILL이 어떻게 사용될지에 대한 몇 가지 예를 들어주실 수 있나요?\"\n- \"사용자들이 '이 이미지에서 적목 현상을 제거해줘'나 '이 이미지를 회전시켜줘'와 같은 요청을 할 것 같은데, 또 다른 유스케이스가 있을까요?\"\n- \"사용자가 어떤 말을 했을 때 이 SKILL이 트리거되어야 합니까?\"\n\n사용자에게 부담을 주지 않기 위해 한 메시지에 너무 많은 질문을 하지 마세요. 가장 중요한 질문부터 시작하고 효과를 높이기 위해 필요에 따라 후속 질문을 하세요.\n\nSKILL이 지원해야 할 기능에 대한 명확한 감이 잡히면 이 단계를 마칩니다.\n\n### Step 2: 재사용 가능한 SKILL 콘텐츠 계획\n\n구체적인 예시를 효과적인 SKILL로 바꾸기 위해, 각 예시를 다음과 같이 분석하세요:\n\n1. 해당 예시를 처음부터 어떻게 실행할지 고려\n2. 이 워크플로우를 반복적으로 실행할 때 도움이 될 스크립트, 참조 문서 및 에셋 식별\n\n예시: \"이 PDF를 회전시켜줘\"와 같은 쿼리를 처리하기 위한 `pdf-editor` SKILL을 빌드할 때의 분석:\n\n1. PDF를 회전하려면 매번 동일한 코드를 다시 작성해야 함\n2. `scripts/rotate_pdf.py` 스크립트를 SKILL에 저장해두면 도움이 됨\n\n예시: \"할 일 앱을 만들어줘\"나 \"내 걸음 수를 추적할 대시보드를 만들어줘\"와 같은 쿼리를 처리하기 위한 `frontend-webapp-builder` SKILL을 설계할 때의 분석:\n\n1. 프런트엔드 웹앱을 작성할 때마다 동일한 보일러플레이트 HTML/React가 필요함\n2. 보일러플레이트 HTML/React 프로젝트 파일이 포함된 `assets/hello-world/` 템플릿을 SKILL에 저장해두면 도움이 됨\n\n예시: \"오늘 몇 명의 사용자가 로그인했지?\"와 같은 쿼리를 처리하기 위한 `big-query` SKILL을 빌드할 때의 분석:\n\n1. BigQuery를 쿼리할 때마다 테이블 스키마와 관계를 매번 다시 확인해야 함\n2. 테이블 스키마를 문서화한 `references/schema.md` 파일을 SKILL에 저장해두면 도움이 됨\n\nSKILL의 콘텐츠를 확립하기 위해, 각 구체적인 예시를 분석하여 포함할 재사용 리소스(스크립트, 참조 문서, 에셋) 목록을 만드세요.\n\n### Step 3: SKILL 초기화\n\n이제 실제로 SKILL을 생성할 차례입니다.\n\n개발하려는 SKILL이 이미 존재하고 개선이나 패키징만 필요한 경우에는 이 단계를 생략하고 다음 단계로 진행하세요.\n\n새로운 SKILL을 처음부터 생성할 때는 항상 `init_skill.py` 스크립트를 실행하세요. 이 스크립트는 SKILL에 필요한 모든 것을 자동으로 포함하는 새 템플릿 SKILL 디렉토리를 생성하여, 제작 과정을 훨씬 효율적이고 안정적으로 만들어줍니다.\n\n사용법:\n\n```bash\nscripts/init_skill.py <skill-name> --path <output-directory>\n```\n\n이 스크립트는:\n\n- 지정된 경로에 SKILL 디렉토리 생성\n- 올바른 frontmatter와 TODO 위치 표시가 포함된 SKILL.md 템플릿 생성\n- 예시 리소스 디렉토리 생성: `scripts/`, `references/`, `assets/`\n- 커스터마이징하거나 삭제할 수 있는 각 디렉토리별 예시 파일 추가\n\n초기화 후에는 생성된 SKILL.md와 예시 파일들을 필요에 따라 커스터마이징하거나 삭제하세요.\n\n### Step 4: SKILL 편집\n\n(새로 생성되었거나 기존의) SKILL을 편집할 때, 이 SKILL은 Claude의 다른 인스턴스가 사용하기 위해 만들어진다는 점을 명심하세요. Claude에게 유익하면서도 자명하지 않은 정보를 포함하세요. 다른 Claude 인스턴스가 이러한 작업을 더 효과적으로 수행하도록 돕기 위해 어떤 절차적 지식, 도메인 세부 사항 또는 재사용 가능한 에셋이 도움이 될지 고려하세요.\n\n#### 검증된 디자인 패턴 학습\n\nSKILL의 필요에 따라 다음 가이드를 참조하세요:\n\n- **다단계 프로세스**: 순차적 워크플로우와 조건부 로직에 대해서는 references/workflows.md를 참조하세요.\n- **특정 출력 형식 또는 품질 표준**: 템플릿 및 예시 패턴에 대해서는 references/output-patterns.md를 참조하세요.\n\n이 파일들에는 효과적인 SKILL 디자인을 위한 검증된 모범 사례가 포함되어 있습니다.\n\n#### 재사용 가능한 SKILL 콘텐츠부터 시작\n\n구현을 시작하기 위해 위에서 식별한 재사용 가능한 리소스(`scripts/`, `references/`, `assets/`) 파일들부터 작성하세요. 이 단계에서 사용자의 입력이 필요할 수 있습니다. 예를 들어 `brand-guidelines` SKILL을 구현할 때, 사용자가 `assets/`에 저장할 브랜드 에셋이나 템플릿, 또는 `references/`에 저장할 문서를 제공해야 할 수 있습니다.\n\n추가된 스크립트는 실제로 실행하여 버그가 없는지, 출력이 예상과 일치하는지 반드시 테스트해야 합니다. 유사한 스크립트가 많은 경우, 완료 시간을 조절하면서도 신뢰성을 확보할 수 있도록 대표적인 샘플만 테스트하면 됩니다.\n\nSKILL에 필요하지 않은 예시 파일과 디렉토리는 삭제해야 합니다. 초기화 스크립트는 구조를 보여주기 위해 각 디렉토리에 예시 파일들을 생성하지만, 대부분의 SKILL에서 이들이 모두 필요하지는 않습니다.\n\n#### SKILL.md 업데이트\n\n**작성 가이드라인:** 항상 명령형/부정사(imperative/infinitive) 형식을 사용하세요.\n\n##### Frontmatter\n\n`name`과 `description` 필드가 포함된 YAML frontmatter를 작성하세요:\n\n- `name`: SKILL 이름\n- `description`: SKILL의 주요 트리거 메커니즘이며, Claude가 언제 이 SKILL을 사용해야 할지 이해하도록 돕습니다.\n  - SKILL이 무엇을 하는지와 이를 언제 사용해야 하는지에 대한 구체적인 트리거/컨텍스트를 모두 포함하세요.\n  - \"언제 사용할지\"에 대한 모든 정보는 본문이 아닌 이곳에 포함하세요. 본문은 트리거된 후에 로드되므로, 본문의 \"When to Use This Skill\" 섹션은 Claude에게 도움이 되지 않습니다.\n  - `docx` SKILL의 예시 설명: \"수정 내용 추적(tracked changes), 메모, 포맷 보존 및 텍스트 추출을 지원하는 포괄적인 문서 생성, 편집 및 분석입니다. Claude가 다음을 위해 전문적인 문서(.docx 파일) 작업이 필요할 때 사용하세요: (1) 새 문서 생성, (2) 내용 수정 또는 편집, (3) 수정 내용 추적 작업, (4) 메모 추가 또는 기타 문서 관련 작업\"\n\nYAML frontmatter에 다른 필드를 포함하지 마세요.\n\n##### 본문 (Body)\n\nSKILL과 번들 리소스를 사용하기 위한 지침을 작성하세요.\n\n### Step 5: SKILL 패키징\n\nSKILL 개발이 완료되면, 사용자에게 공유할 배포용 .skill 파일로 패키징해야 합니다. 패키징 프로세스는 먼저 모든 요구 사항을 충족하는지 SKILL을 자동으로 검증합니다:\n\n```bash\nscripts/package_skill.py <path/to/skill-folder>\n```\n\n선택적인 출력 디렉토리 지정:\n\n```bash\nscripts/package_skill.py <path/to/skill-folder> ./dist\n```\n\n패키징 스크립트는 다음을 수행합니다:\n\n1. **검증 (Validate)**: SKILL을 자동으로 검사하며 다음 사항들을 확인합니다:\n\n   - YAML frontmatter 형식 및 필수 필드\n   - SKILL 명명 규칙 및 디렉토리 구조\n   - 설명(description)의 완전성 및 품질\n   - 파일 조직 및 리소스 참조\n\n2. **패키징 (Package)**: 검증을 통과하면 배포를 위한 적절한 디렉토리 구조와 모든 파일을 포함하는, SKILL 이름으로 명명된 .skill 파일(예: `my-skill.skill`)을 생성합니다. .skill 파일은 .skill 확장자를 가진 zip 파일입니다.\n\n검증에 실패하면 스크립트는 에러를 보고하고 패키지를 생성하지 않은 채 종료됩니다. 검증 에러를 수정한 후 패키징 명령을 다시 실행하세요.\n\n### Step 6: 반복 개선 (Iterate)\n\nSKILL을 테스트한 후 사용자가 개선을 요청할 수 있습니다. 이는 대개 SKILL이 어떻게 작동했는지에 대한 생생한 컨텍스트가 남아 있는 사용 직후에 발생합니다.\n\n**반복 워크플로우:**\n\n1. 실제 작업에 SKILL을 사용\n2. 어려움이나 비효율적인 부분 발견\n3. SKILL.md나 번들 리소스를 어떻게 업데이트해야 할지 식별\n4. 변경 사항 구현 및 재테스트\n",
        "icartsh-plugin/skills/skill-creator/references/output-patterns.md": "# Output Patterns\n\nUse these patterns when skills need to produce consistent, high-quality output.\n\n## Template Pattern\n\nProvide templates for output format. Match the level of strictness to your needs.\n\n**For strict requirements (like API responses or data formats):**\n\n```markdown\n## Report structure\n\nALWAYS use this exact template structure:\n\n# [Analysis Title]\n\n## Executive summary\n[One-paragraph overview of key findings]\n\n## Key findings\n- Finding 1 with supporting data\n- Finding 2 with supporting data\n- Finding 3 with supporting data\n\n## Recommendations\n1. Specific actionable recommendation\n2. Specific actionable recommendation\n```\n\n**For flexible guidance (when adaptation is useful):**\n\n```markdown\n## Report structure\n\nHere is a sensible default format, but use your best judgment:\n\n# [Analysis Title]\n\n## Executive summary\n[Overview]\n\n## Key findings\n[Adapt sections based on what you discover]\n\n## Recommendations\n[Tailor to the specific context]\n\nAdjust sections as needed for the specific analysis type.\n```\n\n## Examples Pattern\n\nFor skills where output quality depends on seeing examples, provide input/output pairs:\n\n```markdown\n## Commit message format\n\nGenerate commit messages following these examples:\n\n**Example 1:**\nInput: Added user authentication with JWT tokens\nOutput:\n```\nfeat(auth): implement JWT-based authentication\n\nAdd login endpoint and token validation middleware\n```\n\n**Example 2:**\nInput: Fixed bug where dates displayed incorrectly in reports\nOutput:\n```\nfix(reports): correct date formatting in timezone conversion\n\nUse UTC timestamps consistently across report generation\n```\n\nFollow this style: type(scope): brief description, then detailed explanation.\n```\n\nExamples help Claude understand the desired style and level of detail more clearly than descriptions alone.\n",
        "icartsh-plugin/skills/skill-creator/references/workflows.md": "# Workflow Patterns\n\n## Sequential Workflows\n\nFor complex tasks, break operations into clear, sequential steps. It is often helpful to give Claude an overview of the process towards the beginning of SKILL.md:\n\n```markdown\nFilling a PDF form involves these steps:\n\n1. Analyze the form (run analyze_form.py)\n2. Create field mapping (edit fields.json)\n3. Validate mapping (run validate_fields.py)\n4. Fill the form (run fill_form.py)\n5. Verify output (run verify_output.py)\n```\n\n## Conditional Workflows\n\nFor tasks with branching logic, guide Claude through decision points:\n\n```markdown\n1. Determine the modification type:\n   **Creating new content?** → Follow \"Creation workflow\" below\n   **Editing existing content?** → Follow \"Editing workflow\" below\n\n2. Creation workflow: [steps]\n3. Editing workflow: [steps]\n```",
        "icartsh-plugin/skills/sql-expert/README.md": "# SQL Expert Skill\n\nPostgreSQL, MySQL, SQLite, 및 SQL Server를 지원하는 전문가 수준의 SQL 쿼리 작성, 최적화 및 데이터베이스 스키마 설계입니다.\n\n## 개요 (Overview)\n\nSQL 데이터베이스 작성, 최적화 및 관리를 위한 전문가 가이드입니다. 이 SKILL은 JOIN 및 윈도우 함수를 포함한 복잡한 쿼리, EXPLAIN 실행 계획을 활용한 쿼리 최적화, 올바른 정규화를 적용한 데이터베이스 스키마 설계, 성능을 위한 인덱스 생성, 안전한 마이그레이션 및 SQL 디버깅을 다룹니다.\n\n## 설치 (Installation)\n\n대상 데이터베이스에 맞는 데이터베이스 드라이버를 설치하세요:\n\n```bash\n# PostgreSQL\npip install psycopg2-binary sqlalchemy\n\n# MySQL/MariaDB\npip install mysql-connector-python sqlalchemy\n\n# SQLite (Python 내장)\npip install sqlite3\n\n# SQL Server\npip install pyodbc sqlalchemy\n```\n\n## 포함 내용 (What's Included)\n\n### SKILL.md\nSQL 쿼리 작성, 최적화 기법, 정규화를 포함한 스키마 설계, 인덱스 전략, 마이그레이션 패턴, 고급 패턴(CTE, 윈도우 함수, 재귀 쿼리), 모범 사례 및 자주 발생하는 문제를 다루는 포괄적인 가이드입니다.\n\n### scripts/\n- `sql_helper.py` - 다음을 위한 유틸리티 함수:\n  - 쿼리 빌딩 및 파라미터화\n  - 스키마 내성 조사(introspection)\n  - 인덱스 분석 및 추천\n  - 마이그레이션 헬퍼\n\n### examples/\n- `complex_queries.sql` - CTE, 윈도우 함수, 서브쿼리를 활용한 고급 쿼리 패턴\n- `schema_examples.sql` - 다양한 유스케이스를 위한 전체 스키마 설계 예시\n- `migrations.sql` - 안전한 마이그레이션 패턴 및 무중단 기법\n\n### references/\n- `query-optimization.md` - 포괄적인 최적화 기법 및 EXPLAIN 분석\n- `indexes-performance.md` - 상세 인덱스 전략, 유지보수, 모니터링\n- `advanced-patterns.md` - UPSERT, 벌크 작업, 피벗 테이블, JSON 작업, 재귀 쿼리\n- `best-practices.md` - SQL 모범 사례 가이드 전체\n- `common-pitfalls.md` - 일반적인 실수와 방지 방법\n\n## 빠른 시작 (Quick Start)\n\n### JOIN을 포함한 기본 SELECT\n\n```sql\n-- 필터링이 포함된 단순 SELECT\nSELECT\n    users.name,\n    orders.order_date,\n    orders.total_amount\nFROM\n    users\nINNER JOIN\n    orders ON users.id = orders.user_id\nWHERE\n    orders.status = 'completed'\nORDER BY\n    orders.order_date DESC\nLIMIT 10;\n\n-- LEFT JOIN (주문이 없는 사용자도 포함)\nSELECT\n    users.name,\n    COUNT(orders.id) as order_count,\n    COALESCE(SUM(orders.total_amount), 0) as total_spent\nFROM\n    users\nLEFT JOIN\n    orders ON users.id = orders.user_id\nGROUP BY\n    users.id, users.name;\n```\n\n### 공통 테이블 식별자 (CTEs)\n\n```sql\nWITH high_value_customers AS (\n    SELECT\n        user_id,\n        SUM(total_amount) as lifetime_value\n    FROM orders\n    GROUP BY user_id\n    HAVING SUM(total_amount) > 1000\n)\nSELECT\n    users.name,\n    users.email,\n    hvc.lifetime_value\nFROM users\nINNER JOIN high_value_customers hvc ON users.id = hvc.user_id;\n```\n\n### 윈도우 함수 (Window Functions)\n\n```sql\n-- 그룹 내 순위 지정\nSELECT\n    name,\n    department,\n    salary,\n    ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as salary_rank\nFROM\n    employees;\n\n-- 누계 (Running totals)\nSELECT\n    order_date,\n    total_amount,\n    SUM(total_amount) OVER (ORDER BY order_date) as running_total\nFROM\n    orders;\n```\n\n고급 패턴은 `examples/complex_queries.sql`을 참조하세요.\n\n## 핵심 역량 (Core Capabilities)\n\n### 쿼리 작성 (Query Writing)\n- JOIN, 서브쿼리, CTE 및 윈도우 함수를 포함한 복잡한 SQL 쿼리\n- GROUP BY 및 HAVING을 사용한 집계\n- 집합 연산 (UNION, INTERSECT, EXCEPT)\n- 계층형 데이터를 위한 재귀 CTE\n- JSON/JSONB 작업 (PostgreSQL)\n\n### 쿼리 최적화 (Query Optimization)\n- EXPLAIN 실행 계획 분석\n- 인덱스 권장 사항 도출\n- 성능 향상을 위한 쿼리 재작성\n- 실행 계획의 이해\n- 성능 병목 지점 식별\n\n### 스키마 설계 (Schema Design)\n- 데이터베이스 정규화 (1NF, 2NF, 3NF, BCNF)\n- 개체-관계(Entity-Relationship) 모델링\n- 외래 키(Foreign Key) 제약 조건\n- CHECK 제약 조건 및 유효성 검사\n- 기본값 및 트리거(Triggers)\n\n### 인덱스 관리 (Index Management)\n- 단일 컬럼 및 복합 인덱스\n- 고유(Unique) 인덱스\n- 부분(Partial) 인덱스 (PostgreSQL)\n- 인덱스 유지보수 및 모니터링\n- 인덱스 생성 또는 회피 시점 판단\n\n### 데이터베이스 마이그레이션 (Migrations)\n- 안전한 스키마 변경\n- 무중단 마이그레이션\n- 롤백 전략\n- 데이터 백필링(Backfilling)\n- 스키마 버전 관리\n\n### 디버깅 (Debugging)\n- SQL 에러 해석\n- 쿼리 트러블슈팅\n- 데이터 무결성 문제 해결\n- 성능 디버깅\n- 제약 조건 위반 해결\n\n## 쿼리 최적화\n\n### EXPLAIN 활용\n\n```sql\n-- 쿼리 성능 분석\nEXPLAIN ANALYZE\nSELECT\n    users.name,\n    COUNT(orders.id) as order_count\nFROM users\nLEFT JOIN orders ON users.id = orders.user_id\nGROUP BY users.id, users.name;\n\n-- 확인할 지표:\n-- - Seq Scan (나쁨) vs Index Scan (좋음)\n-- - 높은 비용(cost) 수치\n-- - 대량의 행 처리 여부\n```\n\n### 빠른 최적화 팁\n\n```sql\n-- 나쁨: 인덱스 컬럼에 함수 사용\nSELECT * FROM users WHERE LOWER(email) = 'user@example.com';\n\n-- 좋음: 인덱스 컬럼 가공 금지\nSELECT * FROM users WHERE email = LOWER('user@example.com');\n\n-- 나쁨: SELECT * 사용\nSELECT * FROM large_table WHERE id = 123;\n\n-- 좋음: 필요한 컬럼만 명시\nSELECT id, name, email FROM large_table WHERE id = 123;\n```\n\n상세 기법은 `references/query-optimization.md`를 참조하세요.\n\n## 스키마 설계\n\n### 정규화 예시\n\n```sql\n-- 좋음: 주문 항목 테이블 분리 (1NF)\nCREATE TABLE orders (\n    order_id INT PRIMARY KEY,\n    customer_name VARCHAR(100),\n    order_date DATE\n);\n\nCREATE TABLE order_items (\n    order_item_id INT PRIMARY KEY,\n    order_id INT REFERENCES orders(order_id),\n    product_name VARCHAR(100),\n    quantity INT,\n    price DECIMAL(10, 2)\n);\n```\n\n### 다대다 관계\n\n```sql\nCREATE TABLE students (\n    student_id INT PRIMARY KEY,\n    name VARCHAR(100)\n);\n\nCREATE TABLE courses (\n    course_id INT PRIMARY KEY,\n    course_name VARCHAR(100)\n);\n\n-- 교차(Junction) 테이블\nCREATE TABLE enrollments (\n    enrollment_id INT PRIMARY KEY,\n    student_id INT NOT NULL,\n    course_id INT NOT NULL,\n    enrollment_date DATE,\n    grade CHAR(2),\n    FOREIGN KEY (student_id) REFERENCES students(student_id),\n    FOREIGN KEY (course_id) REFERENCES courses(course_id),\n    UNIQUE (student_id, course_id)\n);\n```\n\n추가 패턴은 `examples/schema_examples.sql`을 참조하세요.\n\n## 인덱스와 성능\n\n### 인덱스 생성\n\n```sql\n-- 단일 컬럼 인덱스\nCREATE INDEX idx_users_email ON users(email);\n\n-- 복합 인덱스 (순서가 중요!)\nCREATE INDEX idx_orders_user_date ON orders(user_id, order_date);\n\n-- 고유 인덱스\nCREATE UNIQUE INDEX idx_users_username ON users(username);\n\n-- 부분 인덱스 (PostgreSQL)\nCREATE INDEX idx_active_users ON users(email) WHERE status = 'active';\n```\n\n### 인덱스 가이드라인\n\n**인덱스가 필요한 경우:**\n- ✅ WHERE 절에 사용되는 컬럼\n- ✅ JOIN 조건 컬럼\n- ✅ ORDER BY 컬럼\n- ✅ 외래 키 컬럼\n\n**인덱스를 피해야 하는 경우:**\n- ❌ 작은 테이블 (< 1000행)\n- ❌ 선택도가 낮은 컬럼 (예: boolean)\n- ❌ 업데이트가 빈번한 컬럼\n\n상세 전략은 `references/indexes-performance.md`를 참조하세요.\n\n## 마이그레이션 (Migrations)\n\n### 안전한 마이그레이션 패턴\n\n```sql\n-- 1단계: nullable 컬럼 추가\nALTER TABLE users ADD COLUMN status VARCHAR(20);\n\n-- 2단계: 기존 행 업데이트\nUPDATE users SET status = 'active' WHERE status IS NULL;\n\n-- 3단계: NOT NULL 설정\nALTER TABLE users ALTER COLUMN status SET NOT NULL;\n\n-- 4단계: 새 행을 위한 기본값 설정\nALTER TABLE users ALTER COLUMN status SET DEFAULT 'active';\n\n-- 롤백 계획\nALTER TABLE users DROP COLUMN status;\n```\n\n추가 패턴은 `examples/migrations.sql`을 참조하세요.\n\n## 고급 패턴\n\n### UPSERT (Insert or Update)\n\n```sql\n-- PostgreSQL\nINSERT INTO users (user_id, name, email, updated_at)\nVALUES (1, 'John Doe', 'john@example.com', NOW())\nON CONFLICT (user_id)\nDO UPDATE SET\n    name = EXCLUDED.name,\n    email = EXCLUDED.email,\n    updated_at = NOW();\n\n-- MySQL\nINSERT INTO users (user_id, name, email, updated_at)\nVALUES (1, 'John Doe', 'john@example.com', NOW())\nON DUPLICATE KEY UPDATE\n    name = VALUES(name),\n    email = VALUES(email),\n    updated_at = NOW();\n```\n\n### 재귀 CTE (Recursive CTEs)\n\n```sql\n-- 계층형 데이터 탐색\nWITH RECURSIVE employee_hierarchy AS (\n    -- Anchor: 최상위 직원\n    SELECT id, name, manager_id, 1 as level\n    FROM employees\n    WHERE manager_id IS NULL\n\n    UNION ALL\n\n    -- Recursive: 하위 직원\n    SELECT e.id, e.name, e.manager_id, eh.level + 1\n    FROM employees e\n    INNER JOIN employee_hierarchy eh ON e.manager_id = eh.id\n)\nSELECT * FROM employee_hierarchy ORDER BY level, name;\n```\n\n고급 패턴은 `references/advanced-patterns.md`를 참조하세요.\n\n## 모범 사례 (Best Practices)\n\n### 핵심 가이드라인\n\n1. **항상 파라미터화된 쿼리를 사용**하여 SQL 인젝션을 방지하세요.\n2. **연관된 작업은 트랜잭션을 사용**하여 원자성을 유지하세요.\n3. **적절한 제약 조건을 추가**하세요 (PK, FK, NOT NULL, CHECK).\n4. **타임스탬프** (created_at, updated_at)를 테이블에 포함하세요.\n5. 테이블과 컬럼에 **의미 있는 이름**을 사용하세요.\n6. **SELECT * 을 지양**하고 필요한 컬럼만 명시하세요.\n7. 조인 성능을 위해 **외래 키에 인덱스**를 고려하세요.\n8. 가변 길이 문자열에는 **VARCHAR**를 사용하세요.\n9. **NULL 값을 적절히 처리**하세요 (IS NULL / IS NOT NULL).\n10. **적절한 데이터 타입**을 사용하세요 (금액은 DECIMAL 권장).\n\n상세 내용은 `references/best-practices.md`를 참조하세요.\n\n## 자주 발생하는 문제 (Common Pitfalls)\n\n1. **N+1 쿼리 문제** - 반복문 대신 JOIN을 활용하세요.\n2. **LIMIT 부재** - 대량 테이블 조회 시 성능 저하를 방지하세요.\n3. **암시적 타입 변환** - 인덱스 사용을 방해할 수 있습니다.\n4. **COUNT(*) 남용** - 존재 여부 파악 시에는 EXISTS가 유리합니다.\n5. **NULL 처리 실수** - NULL 비교 연산의 특성을 이해하세요.\n6. **임시방편 SELECT DISTINCT** - 근본적인 쿼리 문제를 해결하세요.\n7. **트랜잭션 누락** - 연관 작업의 무결성을 보장하세요.\n8. **인덱스 컬럼 가공** - 함수 사용 시 인덱스를 타지 못할 수 있습니다.\n\n해결 방법 목록은 `references/common-pitfalls.md`를 참조하세요.\n\n## 지원하는 데이터베이스 시스템\n\n### PostgreSQL\n**강점**: 복잡한 쿼리, JSON 데이터, 고급 기능, ACID 준수\n\n### MySQL/MariaDB\n**강점**: 웹 애플리케이션, WordPress, 읽기 비중이 높은 워크로드\n\n### SQLite\n**강점**: 로컬 개발, 임베디드 DB, 테스트용\n\n### SQL Server\n**강점**: 엔터프라이즈 앱, Windows 환경\n\n## 워크플로우 (Workflow)\n\nSQL 데이터베이스 작업 시:\n\n1. **요구 사항 파악** - 어떤 데이터가 필요한가?\n2. **스키마 설계** - 정규화 및 데이터 타입 선택\n3. **인덱스 생성** - FK 및 주요 쿼리 컬럼 인덱싱\n4. **쿼리 작성** - 단순하게 시작하여 점진적으로 확장\n5. **최적화** - EXPLAIN으로 병목 지점 확인\n6. **테스트** - 샘플 데이터로 검증\n7. **문서화** - 복잡한 로직에 설명 추가\n\n마이그레이션 시:\n1. **변경 계획 수립** - 영향도 파악\n2. **마이그레이션 작성** - Up/Down 모두 작성\n3. **복사본 테스트** - 로컬/개발 DB 우선 검증\n4. **백업** - 실행 전 필수 단계\n5. **실행** - 트래픽이 적은 시간에 수행\n6. **검증** - 변경 후 무결성 확인\n\n## 문서 (Documentation)\n\n포괄적인 가이드, 상세 워크플로우 및 고급 기법은 `SKILL.md`를 확인하세요.\n\n## 요구 사항 (Requirements)\n\n- Python 3.7+ (헬퍼 스크립트용)\n- 데이터베이스 전용 드라이버 (psycopg2, mysql-connector-python, pyodbc 등)\n- SQLAlchemy (선택 사항, ORM용)\n- 데이터베이스 서버 접근 권한\n",
        "icartsh-plugin/skills/sql-expert/SKILL.md": "---\nname: sql-expert\ndescription: PostgreSQL, MySQL, SQLite, 및 SQL Server를 지원하는 전문가 수준의 SQL 쿼리 작성, 최적화 및 데이터베이스 스키마 설계입니다. 데이터베이스 작업 시 다음을 위해 사용하세요: (1) JOIN, 서브쿼리, 윈도우 함수를 포함한 복잡한 SQL 쿼리 작성, (2) 느린 쿼리 최적화 및 실행 계획 분석, (3) 올바른 정규화를 적용한 데이터베이스 스키마 설계, (4) 인덱스 생성 및 쿼리 성능 개선, (5) 마이그레이션 작성 및 스키마 변경 처리, (6) SQL 에러 및 쿼리 문제 디버깅\n---\n\n# SQL Expert Skill\n\nPostgreSQL, MySQL, SQLite 및 SQL Server 전반에 걸친 SQL 데이터베이스의 작성, 최적화 및 관리를 위한 전문가 가이드입니다.\n\n## 핵심 역량 (Core Capabilities)\n\n이 SKILL을 통해 다음을 수행할 수 있습니다:\n\n- JOIN, 서브쿼리, CTE 및 윈도우 함수를 포함한 **복잡한 SQL 쿼리 작성**\n- EXPLAIN 실행 계획 및 인덱스 권장 사항을 활용한 **느린 쿼리 최적화**\n- 올바른 정규화(1NF, 2NF, 3NF, BCNF)를 적용한 **데이터베이스 스키마 설계**\n- 쿼리 성능을 위한 **효과적인 인덱스 생성**\n- 롤백 지원을 포함한 안전한 **데이터베이스 마이그레이션 작성**\n- **SQL 에러 디버깅** 및 에러 메시지 해석\n- 적절한 격리 수준(isolation levels)을 적용한 **트랜잭션 처리**\n- **JSON/JSONB** 데이터 타입 활용\n- 테스트를 위한 **샘플 데이터 생성**\n- **데이터베이스 다이얼렉트 간 변환** (PostgreSQL ↔ MySQL ↔ SQLite)\n\n---\n\n## 지원하는 데이터베이스 시스템 (Supported Database Systems)\n\n### PostgreSQL\n**적합한 사례**: 복잡한 쿼리, JSON 데이터, 고급 기능, ACID 준수\n\n```bash\npip install psycopg2-binary sqlalchemy\n```\n\n### MySQL/MariaDB\n**적합한 사례**: 웹 애플리케이션, WordPress, 읽기 비중이 높은 워크로드\n\n```bash\npip install mysql-connector-python sqlalchemy\n```\n\n### SQLite\n**적합한 사례**: 로컬 개발, 임베디드 데이터베이스, 테스트\n\n```bash\npip install sqlite3  # Python 내장\n```\n\n### SQL Server\n**적합한 사례**: 엔터프라이즈 애플리케이션, Windows 환경\n\n```bash\npip install pyodbc sqlalchemy\n```\n\n---\n\n## 쿼리 작성 (Query Writing)\n\n### JOIN을 포함한 기본 SELECT\n\n```sql\n-- 필터링이 포함된 단순 SELECT\nSELECT\n    column1,\n    column2,\n    column3\nFROM\n    table_name\nWHERE\n    condition = 'value'\n    AND another_condition > 100\nORDER BY\n    column1 DESC\nLIMIT 10;\n\n-- INNER JOIN\nSELECT\n    users.name,\n    orders.order_date,\n    orders.total_amount\nFROM\n    users\nINNER JOIN\n    orders ON users.id = orders.user_id\nWHERE\n    orders.status = 'completed';\n\n-- LEFT JOIN (주문이 없는 사용자를 포함하여 모두 조회)\nSELECT\n    users.name,\n    COUNT(orders.id) as order_count,\n    COALESCE(SUM(orders.total_amount), 0) as total_spent\nFROM\n    users\nLEFT JOIN\n    orders ON users.id = orders.user_id\nGROUP BY\n    users.id, users.name;\n```\n\n### 서브쿼리 및 CTE (Common Table Expression)\n\n```sql\n-- WHERE 절의 서브쿼리\nSELECT name, salary\nFROM employees\nWHERE salary > (SELECT AVG(salary) FROM employees);\n\n-- CTE (가독성 향상을 위해 권장)\nWITH high_value_customers AS (\n    SELECT\n        user_id,\n        SUM(total_amount) as lifetime_value\n    FROM orders\n    GROUP BY user_id\n    HAVING SUM(total_amount) > 1000\n)\nSELECT\n    users.name,\n    users.email,\n    hvc.lifetime_value\nFROM users\nINNER JOIN high_value_customers hvc ON users.id = hvc.user_id;\n```\n\n### 윈도우 함수 (Window Functions)\n\n```sql\n-- 그룹 내 순위 지정\nSELECT\n    name,\n    department,\n    salary,\n    ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as salary_rank\nFROM\n    employees;\n\n-- 누계 (Running totals)\nSELECT\n    order_date,\n    total_amount,\n    SUM(total_amount) OVER (ORDER BY order_date) as running_total\nFROM\n    orders;\n\n-- 이동 평균 (Moving averages)\nSELECT\n    order_date,\n    total_amount,\n    AVG(total_amount) OVER (\n        ORDER BY order_date\n        ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n    ) as moving_avg_7days\nFROM\n    daily_sales;\n```\n\n더 복잡한 쿼리 패턴은 `examples/complex_queries.sql`을 참조하세요.\n\n---\n\n## 쿼리 최적화 (Query Optimization)\n\n### EXPLAIN 활용\n\n```sql\n-- 쿼리 성능 분석\nEXPLAIN ANALYZE\nSELECT\n    users.name,\n    COUNT(orders.id) as order_count\nFROM users\nLEFT JOIN orders ON users.id = orders.user_id\nGROUP BY users.id, users.name;\n\n-- 확인할 내용:\n-- - Seq Scan (나쁨) vs Index Scan (좋음)\n-- - 높은 비용(cost) 수치\n-- - 처리되는 대량의 행 수(row counts)\n```\n\n### 빠른 최적화 팁\n\n```sql\n-- 나쁨: 인덱스 컬럼에 함수 사용\nSELECT * FROM users WHERE LOWER(email) = 'user@example.com';\n\n-- 좋음: 인덱스 컬럼을 가공하지 않음\nSELECT * FROM users WHERE email = LOWER('user@example.com');\n\n-- 나쁨: SELECT * 사용\nSELECT * FROM large_table WHERE id = 123;\n\n-- 좋음: 필요한 컬럼만 선택\nSELECT id, name, email FROM large_table WHERE id = 123;\n```\n\n포괄적인 최적화 기법은 `references/query-optimization.md`를 참조하세요.\n\n---\n\n## 스키마 설계 (Schema Design)\n\n### 정규화 원칙 (Normalization Principles)\n\n**제1정규형 (1NF)**: 반복되는 그룹 제거, 원자성(atomic) 확보\n\n```sql\n-- 좋음: 주문 항목을 위한 별도 테이블 구성\nCREATE TABLE orders (\n    order_id INT PRIMARY KEY,\n    customer_name VARCHAR(100)\n);\n\nCREATE TABLE order_items (\n    order_item_id INT PRIMARY KEY,\n    order_id INT REFERENCES orders(order_id),\n    product_name VARCHAR(100)\n);\n```\n\n**제2정규형 (2NF)**: 기본키가 아닌 모든 속성이 기본키 전체에 의존해야 함\n\n```sql\n-- 좋음: 제품 정보를 분리하여 관리\nCREATE TABLE products (\n    product_id INT PRIMARY KEY,\n    product_name VARCHAR(100),\n    product_price DECIMAL(10, 2)\n);\n\nCREATE TABLE order_items (\n    order_id INT,\n    product_id INT,\n    quantity INT,\n    PRIMARY KEY (order_id, product_id),\n    FOREIGN KEY (product_id) REFERENCES products(product_id)\n);\n```\n\n**제3정규형 (3NF)**: 이행적 종속성(transitive dependency)이 없어야 함\n\n### 일반적인 스키마 패턴\n\n**일대다 (One-to-Many):**\n\n```sql\nCREATE TABLE authors (\n    author_id INT PRIMARY KEY,\n    name VARCHAR(100),\n    email VARCHAR(100) UNIQUE\n);\n\nCREATE TABLE books (\n    book_id INT PRIMARY KEY,\n    title VARCHAR(200),\n    author_id INT NOT NULL,\n    published_date DATE,\n    FOREIGN KEY (author_id) REFERENCES authors(author_id)\n);\n```\n\n**다대다 (Many-to-Many):**\n\n```sql\nCREATE TABLE students (\n    student_id INT PRIMARY KEY,\n    name VARCHAR(100)\n);\n\nCREATE TABLE courses (\n    course_id INT PRIMARY KEY,\n    course_name VARCHAR(100)\n);\n\n-- 교차(Junction) 테이블\nCREATE TABLE enrollments (\n    enrollment_id INT PRIMARY KEY,\n    student_id INT NOT NULL,\n    course_id INT NOT NULL,\n    enrollment_date DATE,\n    grade CHAR(2),\n    FOREIGN KEY (student_id) REFERENCES students(student_id),\n    FOREIGN KEY (course_id) REFERENCES courses(course_id),\n    UNIQUE (student_id, course_id)\n);\n```\n\n더 많은 스키마 패턴은 `examples/schema_examples.sql`을 참조하세요.\n\n---\n\n## 인덱스 및 성능 (Indexes and Performance)\n\n### 인덱스 생성\n\n```sql\n-- 단일 컬럼 인덱스\nCREATE INDEX idx_users_email ON users(email);\n\n-- 복합 인덱스 (컬럼 순서가 중요합니다!)\nCREATE INDEX idx_orders_user_date ON orders(user_id, order_date);\n\n-- 고유 인덱스\nCREATE UNIQUE INDEX idx_users_username ON users(username);\n\n-- 부분 인덱스 (PostgreSQL)\nCREATE INDEX idx_active_users ON users(email) WHERE status = 'active';\n```\n\n### 인덱스 가이드라인\n\n**인덱스가 필요한 경우:**\n- ✅ WHERE 절에 사용되는 컬럼\n- ✅ JOIN 조건에 사용되는 컬럼\n- ✅ ORDER BY에 사용되는 컬럼\n- ✅ 외래 키(Foreign key) 컬럼\n\n**인덱스를 피해야 하는 경우:**\n- ❌ 아주 작은 테이블 (< 1000행)\n- ❌ 선택도(selectivity)가 낮은 컬럼 (예: boolean 필드)\n- ❌ 업데이트가 매우 빈번한 컬럼\n\n상세한 인덱스 전략은 `references/indexes-performance.md`를 참조하세요.\n\n---\n\n## 마이그레이션 (Migrations)\n\n### 안전한 마이그레이션 패턴\n\n```sql\n-- 1단계: nullable 컬럼으로 추가\nALTER TABLE users ADD COLUMN status VARCHAR(20);\n\n-- 2단계: 기존 데이터 채우기\nUPDATE users SET status = 'active' WHERE status IS NULL;\n\n-- 3단계: NOT NULL 제약 조건 부여\nALTER TABLE users ALTER COLUMN status SET NOT NULL;\n\n-- 4단계: 새 행을 위한 기본값 설정\nALTER TABLE users ALTER COLUMN status SET DEFAULT 'active';\n\n-- 롤백 계획\nALTER TABLE users DROP COLUMN status;\n```\n\n### 무중단 마이그레이션 (Zero-Downtime Migrations)\n\n```sql\n-- 좋음: 컬럼을 먼저 nullable로 추가한 뒤 데이터 채움\nALTER TABLE large_table ADD COLUMN new_column VARCHAR(100);\n\n-- 대량 업데이트는 배치(batch) 단위로 수행\nUPDATE large_table SET new_column = 'value' WHERE new_column IS NULL LIMIT 1000;\n-- 완료될 때까지 반복\n\n-- 그 다음 NOT NULL 설정\nALTER TABLE large_table ALTER COLUMN new_column SET NOT NULL;\n```\n\n추가 마이그레이션 패턴은 `examples/migrations.sql`을 참조하세요.\n\n---\n\n## 고급 패턴 (Advanced Patterns)\n\n### UPSERT (Insert or Update)\n\n```sql\n-- PostgreSQL\nINSERT INTO users (user_id, name, email, updated_at)\nVALUES (1, 'John Doe', 'john@example.com', NOW())\nON CONFLICT (user_id)\nDO UPDATE SET\n    name = EXCLUDED.name,\n    email = EXCLUDED.email,\n    updated_at = NOW();\n\n-- MySQL\nINSERT INTO users (user_id, name, email, updated_at)\nVALUES (1, 'John Doe', 'john@example.com', NOW())\nON DUPLICATE KEY UPDATE\n    name = VALUES(name),\n    email = VALUES(email),\n    updated_at = NOW();\n```\n\n### 재귀 CTE (Recursive CTEs)\n\n```sql\n-- 계층형 데이터 탐색\nWITH RECURSIVE employee_hierarchy AS (\n    -- Anchor: 최상위 직원\n    SELECT id, name, manager_id, 1 as level\n    FROM employees\n    WHERE manager_id IS NULL\n\n    UNION ALL\n\n    -- Recursive: 이전 레벨에 보고하는 직원들\n    SELECT e.id, e.name, e.manager_id, eh.level + 1\n    FROM employees e\n    INNER JOIN employee_hierarchy eh ON e.manager_id = eh.id\n)\nSELECT * FROM employee_hierarchy ORDER BY level, name;\n```\n\n피벗 테이블, JSON 작업 및 벌크 작업을 포함한 고급 패턴은 `references/advanced-patterns.md`를 참조하세요.\n\n---\n\n## 모범 사례 (Best Practices)\n\n### 핵심 원칙\n\n1. **항상 파라미터화된 쿼리를 사용**하여 SQL 인젝션을 방지하세요.\n2. **연관된 작업은 트랜잭션을 사용**하여 원자성을 보장하세요.\n3. **적절한 제약 조건을 추가**하세요 (PRIMARY KEY, FOREIGN KEY, NOT NULL, CHECK).\n4. **타임스탬프 컬럼** (created_at, updated_at)을 테이블에 포함하세요.\n5. 테이블과 컬럼에 **의미 있는 이름**을 지으세요.\n6. **SELECT * 을 지양**하고 필요한 컬럼만 명시하세요.\n7. 조인 성능을 위해 **외래 키에 인덱스**를 고려하세요.\n8. 가변 길이 문자열에는 CHAR 대신 **VARCHAR**를 사용하세요.\n9. IS NULL / IS NOT NULL을 사용하여 **NULL 값을 적절히 처리**하세요.\n10. **적절한 데이터 타입**을 사용하세요 (금액은 FLOAT가 아닌 DECIMAL 권장).\n\n모범 사례 적용 예시:\n\n```sql\nCREATE TABLE orders (\n    order_id INT PRIMARY KEY,\n    user_id INT NOT NULL,\n    order_date DATE NOT NULL DEFAULT CURRENT_DATE,\n    total_amount DECIMAL(10, 2) CHECK (total_amount >= 0),\n    status VARCHAR(20) CHECK (status IN ('pending', 'completed', 'cancelled')),\n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    FOREIGN KEY (user_id) REFERENCES users(user_id)\n);\n\nCREATE INDEX idx_orders_user_id ON orders(user_id);\nCREATE INDEX idx_orders_status ON orders(status);\n```\n\n포괄적인 모범 사례는 `references/best-practices.md`를 참조하세요.\n\n---\n\n## 자주 발생하는 문제 (Common Pitfalls)\n\n다음을 주의하세요:\n\n1. **N+1 쿼리 문제** - 쿼리가 반복문 안에서 실행되지 않도록 JOIN을 활용하세요.\n2. **LIMIT 부재** - 큰 테이블 탐색 시 LIMIT을 잊지 마세요.\n3. **암시적 타입 변환** - 타입 불일치는 인덱스 사용을 방해할 수 있습니다.\n4. **EXISTS 대신 COUNT(*) 사용** - 존재 여부만 확인할 때는 EXISTS가 효율적입니다.\n5. **NULL 처리 실수** (NULL = NULL은 TRUE가 아니라 NULL입니다).\n6. **임시방편으로 SELECT DISTINCT 사용** - 쿼리의 근본적인 원인을 고치는 대신 DISTINCT로 중복만 가리는 것은 위험합니다.\n7. **연관 작업에서의 트랜잭션 누락**.\n8. **인덱스 컬럼 가공** - 인덱스 컬럼에 함수를 씌우면 인덱스를 탈 수 없습니다.\n\n예시 - N+1 문제 피하기:\n\n```python\n# 나쁨: N+1 쿼리\nusers = db.query(\"SELECT * FROM users\")\nfor user in users:\n    orders = db.query(\"SELECT * FROM orders WHERE user_id = ?\", user.id)\n\n# 좋음: JOIN을 이용한 단일 쿼리\nresult = db.query(\"\"\"\n    SELECT users.*, orders.*\n    FROM users\n    LEFT JOIN orders ON users.id = orders.user_id\n\"\"\")\n```\n\n문제 해결 방법의 전체 목록은 `references/common-pitfalls.md`를 참조하세요.\n\n---\n\n## 헬퍼 스크립트 및 예시\n\n### 활용 가능 리소스\n\n**헬퍼 스크립트** (`scripts/`):\n- `sql_helper.py` - 쿼리 빌딩, 스키마 내성 조사(introspection), 인덱스 분석 및 마이그레이션 보조 유틸리티\n\n**예시** (`examples/`):\n- `complex_queries.sql` - CTE, 윈도우 함수, 서브쿼리를 활용한 고급 쿼리 패턴\n- `schema_examples.sql` - 다양한 유스케이스를 위한 전체 스키마 설계 예시\n- `migrations.sql` - 안전한 마이그레이션 패턴 및 무중단 기법\n\n**참조 문서** (`references/`):\n- `query-optimization.md` - 포괄적인 쿼리 최적화 기법 및 EXPLAIN 분석\n- `indexes-performance.md` - 상세 인덱스 전략, 유지보수 및 모니터링\n- `advanced-patterns.md` - UPSERT, 벌크 작업, 피벗 테이블, JSON 작업, 재귀 쿼리\n- `best-practices.md` - 전체 SQL 모범 사례 가이드\n- `common-pitfalls.md` - 일반적인 실수와 방지 방법\n\n### 빠른 시작 가이드\n\n1. 기본 쿼리는 위에서 보여준 패턴을 사용하세요.\n2. 최적화가 필요한 경우 EXPLAIN으로 시작하고 `references/query-optimization.md`를 확인하세요.\n3. 스키마 설계 시 정규화 패턴을 검토하고 `examples/schema_examples.sql`을 참조하세요.\n4. 복잡한 시나리오는 `references/advanced-patterns.md`를 확인하세요.\n5. 유틸리티가 필요한 경우 `scripts/sql_helper.py`를 활용하세요.\n\n---\n\n## 워크플로우 (Workflow)\n\nSQL 데이터베이스 작업 시:\n\n1. **요구 사항 파악** - 어떤 데이터를 조회하거나 저장해야 하는가?\n2. **스키마 설계** - 정규화 적용 및 적절한 데이터 타입 선택\n3. **인덱스 생성** - 외래 키 및 자주 조회되는 컬럼 인덱싱\n4. **쿼리 작성** - 단순하게 시작하여 필요에 따라 복잡도 추가\n5. **최적화** - EXPLAIN을 사용하여 병목 지점 식별\n6. **테스트** - 샘플 데이터 및 에지 케이스(edge cases) 검증\n7. **문서화** - 복잡한 쿼리에는 주석 추가\n\n마이그레이션 시:\n1. **변경 계획 수립** - 영향을 받는 테이블과 의존성 식별\n2. **마이그레이션 작성** - Up/Down 마이그레이션 모두 작성\n3. **복사본 테스트** - 개발용 데이터베이스에서 먼저 테스트\n4. **백업** - 항상 마이그레이션 실행 전 백업 수행\n5. **실행** - 트래픽이 적은 시간에 실행\n6. **검증** - 실행 후 데이터 무결성 확인\n",
        "icartsh-plugin/skills/sql-expert/references/advanced-patterns.md": "# Advanced SQL Patterns\n\n## UPSERT (Insert or Update)\n\n### PostgreSQL: ON CONFLICT\n\n```sql\nINSERT INTO users (user_id, name, email, updated_at)\nVALUES (1, 'John Doe', 'john@example.com', NOW())\nON CONFLICT (user_id)\nDO UPDATE SET\n    name = EXCLUDED.name,\n    email = EXCLUDED.email,\n    updated_at = NOW();\n```\n\n### MySQL: ON DUPLICATE KEY UPDATE\n\n```sql\nINSERT INTO users (user_id, name, email, updated_at)\nVALUES (1, 'John Doe', 'john@example.com', NOW())\nON DUPLICATE KEY UPDATE\n    name = VALUES(name),\n    email = VALUES(email),\n    updated_at = NOW();\n```\n\n### SQLite: ON CONFLICT\n\n```sql\nINSERT INTO users (user_id, name, email, updated_at)\nVALUES (1, 'John Doe', 'john@example.com', datetime('now'))\nON CONFLICT(user_id) DO UPDATE SET\n    name = excluded.name,\n    email = excluded.email,\n    updated_at = datetime('now');\n```\n\n## Bulk Operations\n\n### Bulk INSERT\n\n```sql\nINSERT INTO users (name, email) VALUES\n    ('Alice', 'alice@example.com'),\n    ('Bob', 'bob@example.com'),\n    ('Charlie', 'charlie@example.com');\n```\n\n### Bulk UPDATE from Another Table\n\n```sql\nUPDATE products p\nSET price = new_prices.price\nFROM (\n    VALUES\n        (1, 19.99),\n        (2, 29.99),\n        (3, 39.99)\n) AS new_prices(product_id, price)\nWHERE p.product_id = new_prices.product_id;\n```\n\n### Bulk DELETE with JOIN\n\n```sql\nDELETE FROM orders\nWHERE order_id IN (\n    SELECT order_id\n    FROM orders o\n    INNER JOIN users u ON o.user_id = u.user_id\n    WHERE u.status = 'deleted'\n);\n```\n\n## Pivot Tables\n\n### Manual Pivot with CASE\n\n```sql\n-- Transform rows to columns\nSELECT\n    product_name,\n    SUM(CASE WHEN EXTRACT(MONTH FROM order_date) = 1 THEN quantity ELSE 0 END) as jan,\n    SUM(CASE WHEN EXTRACT(MONTH FROM order_date) = 2 THEN quantity ELSE 0 END) as feb,\n    SUM(CASE WHEN EXTRACT(MONTH FROM order_date) = 3 THEN quantity ELSE 0 END) as mar\nFROM\n    order_items oi\nINNER JOIN\n    products p ON oi.product_id = p.product_id\nGROUP BY\n    product_name;\n```\n\n### PostgreSQL Crosstab\n\n```sql\n-- Requires tablefunc extension\nCREATE EXTENSION IF NOT EXISTS tablefunc;\n\nSELECT * FROM crosstab(\n    'SELECT product_name, month, total_quantity\n     FROM monthly_sales\n     ORDER BY 1, 2',\n    'SELECT DISTINCT month FROM monthly_sales ORDER BY 1'\n) AS ct(product_name TEXT, jan INT, feb INT, mar INT);\n```\n\n## JSON Operations (PostgreSQL)\n\n### Query JSON Data\n\n```sql\nSELECT\n    user_id,\n    preferences->>'theme' as theme,\n    preferences->>'language' as language\nFROM\n    users\nWHERE\n    preferences->>'notifications' = 'true';\n```\n\n### Update JSON Field\n\n```sql\nUPDATE users\nSET preferences = jsonb_set(\n    preferences,\n    '{theme}',\n    '\"dark\"'\n)\nWHERE user_id = 123;\n```\n\n### JSON Aggregation\n\n```sql\nSELECT\n    department,\n    jsonb_agg(jsonb_build_object(\n        'name', name,\n        'salary', salary\n    )) as employees\nFROM\n    employees\nGROUP BY\n    department;\n```\n\n### JSON Array Operations\n\n```sql\n-- Check if JSON array contains value\nSELECT * FROM users\nWHERE preferences->'tags' ? 'premium';\n\n-- Get array element\nSELECT preferences->'tags'->0 as first_tag\nFROM users;\n\n-- Expand JSON array to rows\nSELECT\n    user_id,\n    jsonb_array_elements_text(preferences->'tags') as tag\nFROM\n    users;\n```\n\n## Recursive Queries\n\n### Hierarchical Data Traversal\n\n```sql\nWITH RECURSIVE employee_hierarchy AS (\n    -- Anchor member: top-level employees\n    SELECT\n        id,\n        name,\n        manager_id,\n        1 as level,\n        name::TEXT as path\n    FROM\n        employees\n    WHERE\n        manager_id IS NULL\n\n    UNION ALL\n\n    -- Recursive member: employees reporting to previous level\n    SELECT\n        e.id,\n        e.name,\n        e.manager_id,\n        eh.level + 1,\n        eh.path || ' > ' || e.name\n    FROM\n        employees e\n    INNER JOIN\n        employee_hierarchy eh ON e.manager_id = eh.id\n    WHERE\n        eh.level < 10  -- Prevent infinite recursion\n)\nSELECT * FROM employee_hierarchy ORDER BY path;\n```\n\n### Graph Traversal\n\n```sql\nWITH RECURSIVE connected_nodes AS (\n    -- Start node\n    SELECT\n        node_id,\n        1 as depth,\n        ARRAY[node_id] as path\n    FROM\n        nodes\n    WHERE\n        node_id = 1\n\n    UNION ALL\n\n    -- Connected nodes\n    SELECT\n        e.to_node_id,\n        cn.depth + 1,\n        cn.path || e.to_node_id\n    FROM\n        edges e\n    INNER JOIN\n        connected_nodes cn ON e.from_node_id = cn.node_id\n    WHERE\n        NOT e.to_node_id = ANY(cn.path)  -- Prevent cycles\n        AND cn.depth < 10\n)\nSELECT * FROM connected_nodes;\n```\n\n## Window Function Advanced Patterns\n\n### Percentile Rankings\n\n```sql\nSELECT\n    name,\n    salary,\n    department,\n    PERCENT_RANK() OVER (PARTITION BY department ORDER BY salary) as percentile\nFROM\n    employees;\n```\n\n### Running Aggregates with Frames\n\n```sql\nSELECT\n    order_date,\n    total_amount,\n    -- Last 7 days average\n    AVG(total_amount) OVER (\n        ORDER BY order_date\n        ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n    ) as moving_avg_7days,\n    -- Month to date total\n    SUM(total_amount) OVER (\n        PARTITION BY DATE_TRUNC('month', order_date)\n        ORDER BY order_date\n        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n    ) as mtd_total\nFROM\n    daily_sales;\n```\n\n### Gap and Island Detection\n\n```sql\n-- Find consecutive sequences\nWITH numbered AS (\n    SELECT\n        id,\n        value,\n        ROW_NUMBER() OVER (ORDER BY id) -\n        ROW_NUMBER() OVER (PARTITION BY value ORDER BY id) as grp\n    FROM\n        sequences\n)\nSELECT\n    value,\n    MIN(id) as start_id,\n    MAX(id) as end_id,\n    COUNT(*) as sequence_length\nFROM\n    numbered\nGROUP BY\n    value, grp\nORDER BY\n    start_id;\n```\n\n## Common Table Expressions (CTEs)\n\n### Multiple CTEs\n\n```sql\nWITH\n    high_value_customers AS (\n        SELECT user_id, SUM(total_amount) as lifetime_value\n        FROM orders\n        GROUP BY user_id\n        HAVING SUM(total_amount) > 1000\n    ),\n    recent_orders AS (\n        SELECT user_id, COUNT(*) as order_count\n        FROM orders\n        WHERE order_date > CURRENT_DATE - INTERVAL '30 days'\n        GROUP BY user_id\n    )\nSELECT\n    u.name,\n    hvc.lifetime_value,\n    COALESCE(ro.order_count, 0) as recent_order_count\nFROM\n    users u\nINNER JOIN high_value_customers hvc ON u.id = hvc.user_id\nLEFT JOIN recent_orders ro ON u.id = ro.user_id;\n```\n\n### Recursive CTE with Aggregation\n\n```sql\nWITH RECURSIVE category_totals AS (\n    -- Leaf categories\n    SELECT\n        category_id,\n        parent_category_id,\n        category_name,\n        (SELECT SUM(price) FROM products WHERE category_id = c.category_id) as total\n    FROM\n        categories c\n    WHERE\n        category_id NOT IN (SELECT parent_category_id FROM categories WHERE parent_category_id IS NOT NULL)\n\n    UNION ALL\n\n    -- Parent categories\n    SELECT\n        c.category_id,\n        c.parent_category_id,\n        c.category_name,\n        ct.total\n    FROM\n        categories c\n    INNER JOIN\n        category_totals ct ON c.category_id = ct.parent_category_id\n)\nSELECT\n    category_name,\n    SUM(total) as category_total\nFROM\n    category_totals\nGROUP BY\n    category_id, category_name;\n```\n",
        "icartsh-plugin/skills/sql-expert/references/best-practices.md": "# SQL Best Practices\n\n## 1. Always Use Parameterized Queries\n\nPrevent SQL injection by using parameterized queries:\n\n```python\n# BAD: SQL injection vulnerable\nquery = f\"SELECT * FROM users WHERE email = '{user_input}'\"\n\n# GOOD: Parameterized query\nquery = \"SELECT * FROM users WHERE email = %s\"\ncursor.execute(query, (user_input,))\n```\n\n**Why it matters:**\n- Prevents SQL injection attacks\n- Handles special characters correctly\n- Improves query plan caching\n\n## 2. Use Transactions for Related Operations\n\n```sql\nBEGIN TRANSACTION;\n\nUPDATE accounts SET balance = balance - 100 WHERE account_id = 1;\nUPDATE accounts SET balance = balance + 100 WHERE account_id = 2;\n\nCOMMIT;\n-- Or ROLLBACK if something goes wrong\n```\n\n**When to use transactions:**\n- Multiple related DML statements\n- Financial operations\n- Data consistency is critical\n- Need atomicity (all-or-nothing execution)\n\n## 3. Add Appropriate Constraints\n\n```sql\nCREATE TABLE orders (\n    order_id INT PRIMARY KEY,\n    user_id INT NOT NULL,\n    order_date DATE NOT NULL DEFAULT CURRENT_DATE,\n    total_amount DECIMAL(10, 2) CHECK (total_amount >= 0),\n    status VARCHAR(20) CHECK (status IN ('pending', 'completed', 'cancelled')),\n    FOREIGN KEY (user_id) REFERENCES users(user_id)\n);\n```\n\n**Essential constraints:**\n- `PRIMARY KEY`: Uniquely identifies rows\n- `FOREIGN KEY`: Maintains referential integrity\n- `NOT NULL`: Prevents missing data\n- `UNIQUE`: Prevents duplicates\n- `CHECK`: Validates data values\n- `DEFAULT`: Provides sensible defaults\n\n## 4. Use VARCHAR Instead of CHAR\n\n```sql\n-- BAD: Wastes space\nCREATE TABLE users (name CHAR(100));\n\n-- GOOD: Only uses needed space\nCREATE TABLE users (name VARCHAR(100));\n```\n\n**Why VARCHAR:**\n- Only stores the actual string length\n- More efficient for variable-length data\n- CHAR is only appropriate for fixed-length codes (like state abbreviations)\n\n## 5. Include Timestamps\n\n```sql\nCREATE TABLE posts (\n    post_id INT PRIMARY KEY,\n    title VARCHAR(200),\n    content TEXT,\n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP\n);\n```\n\n**Benefits:**\n- Track when records were created\n- Monitor when records were modified\n- Enable time-based queries and analytics\n- Support audit trails\n\n## 6. Use Meaningful Names\n\n```sql\n-- BAD\nCREATE TABLE t1 (id INT, n VARCHAR(100));\n\n-- GOOD\nCREATE TABLE customers (customer_id INT, customer_name VARCHAR(100));\n```\n\n**Naming conventions:**\n- Tables: Plural nouns (users, orders, products)\n- Columns: Descriptive names (email_address, not e)\n- Primary keys: table_name_id (user_id, order_id)\n- Foreign keys: referenced_table_id (user_id referencing users.user_id)\n- Indexes: idx_table_column (idx_users_email)\n\n## 7. Avoid SELECT *\n\n```sql\n-- BAD: Retrieves unnecessary data\nSELECT * FROM users WHERE user_id = 123;\n\n-- GOOD: Select only needed columns\nSELECT user_id, name, email FROM users WHERE user_id = 123;\n```\n\n**Why avoid SELECT *:**\n- Reduces network transfer\n- Improves query performance\n- Makes code more maintainable\n- Prevents issues when schema changes\n\n## 8. Use Explicit JOIN Syntax\n\n```sql\n-- BAD: Implicit joins (old style)\nSELECT u.name, o.total\nFROM users u, orders o\nWHERE u.id = o.user_id;\n\n-- GOOD: Explicit JOIN syntax\nSELECT u.name, o.total\nFROM users u\nINNER JOIN orders o ON u.id = o.user_id;\n```\n\n**Benefits:**\n- Clearer intent\n- Easier to spot missing join conditions\n- Separates join logic from filter logic\n- Industry standard\n\n## 9. Handle NULL Values Properly\n\n```sql\n-- BAD: This doesn't work as expected\nSELECT * FROM users WHERE deleted_at = NULL;\n\n-- GOOD: Use IS NULL / IS NOT NULL\nSELECT * FROM users WHERE deleted_at IS NULL;\n\n-- GOOD: Use COALESCE for defaults\nSELECT name, COALESCE(phone, 'No phone') as phone FROM users;\n```\n\n**NULL handling tips:**\n- Use `IS NULL` and `IS NOT NULL` for comparisons\n- Use `COALESCE()` to provide default values\n- Consider `NOT NULL` constraints when appropriate\n- Remember: NULL != NULL (it's unknown)\n\n## 10. Index Foreign Keys\n\n```sql\n-- Always index foreign key columns\nCREATE INDEX idx_orders_user_id ON orders(user_id);\nCREATE INDEX idx_order_items_order_id ON order_items(order_id);\nCREATE INDEX idx_order_items_product_id ON order_items(product_id);\n```\n\n**Why index foreign keys:**\n- Speeds up joins\n- Improves referential integrity checks\n- Essential for delete cascades\n- Often used in WHERE clauses\n\n## 11. Use Appropriate Data Types\n\n```sql\n-- BAD: Using wrong data types\nCREATE TABLE events (\n    event_date VARCHAR(50),  -- Should be DATE or TIMESTAMP\n    price VARCHAR(20),       -- Should be DECIMAL\n    is_active VARCHAR(5)     -- Should be BOOLEAN\n);\n\n-- GOOD: Use appropriate types\nCREATE TABLE events (\n    event_date TIMESTAMP NOT NULL,\n    price DECIMAL(10, 2) NOT NULL,\n    is_active BOOLEAN DEFAULT true\n);\n```\n\n**Type selection guidelines:**\n- Dates/times: DATE, TIMESTAMP, TIME\n- Money: DECIMAL (never FLOAT for currency)\n- Yes/No: BOOLEAN\n- Whole numbers: INT, BIGINT\n- Text: VARCHAR (with reasonable limits)\n\n## 12. Normalize to Third Normal Form (Usually)\n\nFollow normalization principles:\n\n1. **First Normal Form (1NF)**: No repeating groups\n2. **Second Normal Form (2NF)**: No partial dependencies\n3. **Third Normal Form (3NF)**: No transitive dependencies\n\n**When to denormalize:**\n- Read-heavy workloads\n- Reporting databases\n- When joins become too expensive\n- After measuring actual performance issues\n\n## 13. Add Appropriate Indexes\n\n```sql\n-- Index columns used in WHERE, JOIN, ORDER BY\nCREATE INDEX idx_users_email ON users(email);\nCREATE INDEX idx_orders_created_at ON orders(created_at);\nCREATE INDEX idx_products_category ON products(category_id);\n```\n\n**Indexing guidelines:**\n- Start with foreign keys\n- Add indexes for common WHERE clauses\n- Include ORDER BY columns\n- Monitor query performance and add as needed\n- Don't over-index (slows writes)\n\n## 14. Use Database-Level Defaults\n\n```sql\nCREATE TABLE users (\n    user_id SERIAL PRIMARY KEY,\n    status VARCHAR(20) DEFAULT 'active',\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    is_verified BOOLEAN DEFAULT false\n);\n```\n\n**Benefits:**\n- Ensures consistent defaults\n- Reduces application code complexity\n- Works across all applications accessing the database\n- Self-documenting\n\n## 15. Comment Complex Schema Elements\n\n```sql\nCOMMENT ON TABLE users IS 'Application user accounts';\nCOMMENT ON COLUMN users.verified_at IS 'Email verification timestamp, NULL if unverified';\n\n-- Or inline comments for complex queries\nSELECT\n    u.name,\n    COUNT(o.id) as order_count  -- Total lifetime orders\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nWHERE u.created_at > '2024-01-01'  -- New users only\nGROUP BY u.id, u.name;\n```\n\n## 16. Use Transactions Appropriately\n\n```sql\n-- Good: Multiple related operations\nBEGIN;\nINSERT INTO orders (user_id, total) VALUES (1, 100.00);\nINSERT INTO order_items (order_id, product_id, quantity) VALUES (LAST_INSERT_ID(), 5, 2);\nUPDATE products SET stock = stock - 2 WHERE product_id = 5;\nCOMMIT;\n\n-- Bad: Single operation doesn't need explicit transaction\nBEGIN;\nINSERT INTO logs (message) VALUES ('Test');\nCOMMIT;\n```\n\n## 17. Validate Data at Database Level\n\n```sql\nCREATE TABLE products (\n    product_id SERIAL PRIMARY KEY,\n    name VARCHAR(200) NOT NULL,\n    price DECIMAL(10, 2) CHECK (price > 0),\n    stock INT CHECK (stock >= 0),\n    email VARCHAR(255) CHECK (email ~* '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$')\n);\n```\n\n**Defense in depth:**\n- Validate in application AND database\n- Database validation catches bugs\n- Ensures data integrity even with multiple applications\n\n## 18. Plan for Soft Deletes When Appropriate\n\n```sql\nCREATE TABLE users (\n    user_id SERIAL PRIMARY KEY,\n    email VARCHAR(255) NOT NULL,\n    deleted_at TIMESTAMP NULL,\n    -- Other columns...\n);\n\n-- Query only active users\nSELECT * FROM users WHERE deleted_at IS NULL;\n\n-- Index for performance\nCREATE INDEX idx_users_deleted_at ON users(deleted_at);\n```\n\n**When to use soft deletes:**\n- Need to maintain referential integrity\n- Regulatory compliance requirements\n- Audit trail needed\n- Accidental deletes are common\n",
        "icartsh-plugin/skills/sql-expert/references/common-pitfalls.md": "# Common SQL Pitfalls\n\n## 1. N+1 Query Problem\n\n**The Issue:**\nExecuting one query to get a list, then N additional queries for each item in the list.\n\n```python\n# BAD: N+1 queries (1 + N queries total)\nusers = db.query(\"SELECT * FROM users\")\nfor user in users:\n    orders = db.query(\"SELECT * FROM orders WHERE user_id = ?\", user.id)\n    # This executes a query for EACH user\n\n# GOOD: Single query with JOIN\nresult = db.query(\"\"\"\n    SELECT\n        users.*,\n        orders.*\n    FROM users\n    LEFT JOIN orders ON users.id = orders.user_id\n\"\"\")\n```\n\n**Why it's bad:**\n- Multiplies database round trips\n- Scales poorly with data growth\n- Network latency compounds the problem\n\n## 2. Not Using LIMIT for Exploratory Queries\n\n```sql\n-- BAD: Returns all rows (could be millions)\nSELECT * FROM large_table WHERE status = 'active';\n\n-- GOOD: Limit results for exploratory queries\nSELECT * FROM large_table WHERE status = 'active' LIMIT 100;\n```\n\n**Why it matters:**\n- Prevents accidentally loading huge result sets\n- Faster query execution\n- Reduces memory usage in application\n\n## 3. Implicit Type Conversions\n\n```sql\n-- BAD: String comparison on INT column prevents index usage\nSELECT * FROM users WHERE user_id = '123';\n\n-- GOOD: Use correct type\nSELECT * FROM users WHERE user_id = 123;\n\n-- BAD: Comparing different numeric types\nSELECT * FROM products WHERE price = 19;  -- price is DECIMAL\n\n-- GOOD: Match the type\nSELECT * FROM products WHERE price = 19.00;\n```\n\n**Impact:**\n- Can prevent index usage\n- Slower query performance\n- Unexpected comparison results\n\n## 4. Using COUNT(*) When You Just Need EXISTS\n\n```sql\n-- BAD: Counts all rows (expensive)\nSELECT COUNT(*) FROM orders WHERE user_id = 123;\n\n-- GOOD: Just check existence (stops at first match)\nSELECT EXISTS(SELECT 1 FROM orders WHERE user_id = 123);\n```\n\n**Performance difference:**\n- COUNT(*) scans all matching rows\n- EXISTS stops at first match\n- Huge performance difference for large result sets\n\n## 5. Not Handling NULLs Properly\n\n```sql\n-- BAD: NULL comparisons always return NULL (not TRUE or FALSE)\nSELECT * FROM users WHERE deleted_at = NULL;  -- Returns no rows!\n\n-- GOOD: Use IS NULL / IS NOT NULL\nSELECT * FROM users WHERE deleted_at IS NULL;\n\n-- BAD: Forgetting NULL in aggregations\nSELECT SUM(total_amount) FROM orders;  -- NULLs are ignored, might be confusing\n\n-- GOOD: Be explicit about NULL handling\nSELECT COALESCE(SUM(total_amount), 0) as total FROM orders;\n```\n\n**NULL gotchas:**\n- NULL = NULL is NULL (not TRUE)\n- NULL != NULL is NULL (not TRUE)\n- NULL in arithmetic makes result NULL: 5 + NULL = NULL\n- COUNT(*) includes NULLs, COUNT(column) excludes NULLs\n\n## 6. Using SELECT DISTINCT as a Band-Aid\n\n```sql\n-- BAD: Using DISTINCT to hide a join problem\nSELECT DISTINCT user_id, name FROM users\nJOIN orders ON users.id = orders.user_id;\n\n-- GOOD: Fix the underlying issue\nSELECT users.id, users.name FROM users\nWHERE EXISTS (SELECT 1 FROM orders WHERE orders.user_id = users.id);\n\n-- Or if you want the orders too:\nSELECT DISTINCT ON (users.id) users.id, users.name, orders.*\nFROM users\nJOIN orders ON users.id = orders.user_id;\n```\n\n**Why DISTINCT is often wrong:**\n- Hides the real problem (wrong join or query structure)\n- Performance overhead\n- May mask bugs\n\n## 7. Forgetting Transactions for Related Operations\n\n```sql\n-- BAD: No transaction (money could be lost if second statement fails)\nUPDATE accounts SET balance = balance - 100 WHERE account_id = 1;\nUPDATE accounts SET balance = balance + 100 WHERE account_id = 2;\n\n-- GOOD: Use transaction for atomicity\nBEGIN TRANSACTION;\nUPDATE accounts SET balance = balance - 100 WHERE account_id = 1;\nUPDATE accounts SET balance = balance + 100 WHERE account_id = 2;\nCOMMIT;\n```\n\n**Critical for:**\n- Financial operations\n- Related record creation\n- Maintaining data consistency\n\n## 8. Using OR in WHERE with Different Columns\n\n```sql\n-- BAD: OR with different columns can't use indexes efficiently\nSELECT * FROM users WHERE first_name = 'John' OR last_name = 'Smith';\n\n-- GOOD: Use UNION if possible\nSELECT * FROM users WHERE first_name = 'John'\nUNION\nSELECT * FROM users WHERE last_name = 'Smith';\n```\n\n**Performance impact:**\n- OR often forces full table scan\n- UNION can use separate indexes\n\n## 9. Using Functions on Indexed Columns\n\n```sql\n-- BAD: Function on indexed column prevents index usage\nSELECT * FROM users WHERE LOWER(email) = 'user@example.com';\nSELECT * FROM orders WHERE YEAR(order_date) = 2024;\n\n-- GOOD: Avoid function on indexed column\nSELECT * FROM users WHERE email = LOWER('user@example.com');\nSELECT * FROM orders WHERE order_date >= '2024-01-01' AND order_date < '2025-01-01';\n\n-- ALTERNATIVE: Create functional index\nCREATE INDEX idx_email_lower ON users(LOWER(email));\n-- Now the first query can use the index\n```\n\n## 10. Cartesian Products (Missing JOIN Condition)\n\n```sql\n-- BAD: Missing join condition creates cartesian product\nSELECT users.name, orders.total\nFROM users, orders;  -- Every user paired with every order!\n\n-- GOOD: Always specify join condition\nSELECT users.name, orders.total\nFROM users\nINNER JOIN orders ON users.id = orders.user_id;\n```\n\n**Result:**\n- Returns user_count × order_count rows\n- Extremely slow\n- Wrong results\n\n## 11. Not Using LIMIT in DELETE/UPDATE on Large Tables\n\n```sql\n-- BAD: Can lock table for extended period\nDELETE FROM logs WHERE created_at < '2020-01-01';  -- Could be millions of rows\n\n-- GOOD: Delete in batches\nDELETE FROM logs\nWHERE log_id IN (\n    SELECT log_id FROM logs\n    WHERE created_at < '2020-01-01'\n    LIMIT 10000\n);\n-- Repeat until no more rows\n```\n\n**Why batch:**\n- Prevents long-running locks\n- Allows other queries to run\n- Can be paused/resumed\n\n## 12. Storing Encrypted/Hashed Data Without Length Consideration\n\n```sql\n-- BAD: Hash output is 64 chars but column is 50\nCREATE TABLE users (\n    password_hash VARCHAR(50)  -- SHA-256 outputs 64 characters!\n);\n\n-- GOOD: Know your data size requirements\nCREATE TABLE users (\n    password_hash VARCHAR(64),  -- Exact size for SHA-256\n    email_encrypted VARCHAR(255)  -- Allow for encryption overhead\n);\n```\n\n## 13. Using Float for Currency\n\n```sql\n-- BAD: Floating point precision errors\nCREATE TABLE orders (\n    total FLOAT\n);\n-- Can lead to: 10.10 being stored as 10.099999...\n\n-- GOOD: Use DECIMAL for exact precision\nCREATE TABLE orders (\n    total DECIMAL(10, 2)  -- 10 digits, 2 after decimal\n);\n```\n\n**Why DECIMAL:**\n- Exact precision\n- No rounding errors\n- Essential for financial calculations\n\n## 14. Not Considering NULL in Unique Constraints\n\n```sql\n-- In most databases, multiple NULLs are allowed in UNIQUE columns\nCREATE TABLE users (\n    email VARCHAR(255) UNIQUE  -- Multiple NULL emails allowed!\n);\n\n-- If you want only one NULL, use partial unique index (PostgreSQL)\nCREATE UNIQUE INDEX idx_users_email_unique ON users(email) WHERE email IS NOT NULL;\n```\n\n## 15. Inefficient Pagination\n\n```sql\n-- BAD: Gets slower as offset increases\nSELECT * FROM products\nORDER BY created_at\nLIMIT 20 OFFSET 100000;  -- Scans first 100,020 rows\n\n-- GOOD: Use keyset pagination\nSELECT * FROM products\nWHERE created_at > '2024-01-01 12:34:56'  -- Last seen value\nORDER BY created_at\nLIMIT 20;\n```\n\n## 16. Forgetting Index Column Order in Composite Indexes\n\n```sql\n-- Index on (user_id, created_at)\nCREATE INDEX idx_orders_user_created ON orders(user_id, created_at);\n\n-- CAN use index (leftmost column)\nSELECT * FROM orders WHERE user_id = 123;\n\n-- CANNOT use index efficiently (not leftmost)\nSELECT * FROM orders WHERE created_at > '2024-01-01';\n\n-- CAN use index (both columns)\nSELECT * FROM orders WHERE user_id = 123 AND created_at > '2024-01-01';\n```\n\n## 17. Using VARCHAR(255) for Everything\n\n```sql\n-- BAD: Unnecessarily large VARCHAR limits\nCREATE TABLE users (\n    email VARCHAR(255),        -- Email max is ~254\n    state VARCHAR(255),        -- US state is 2 chars\n    username VARCHAR(255)      -- Your app limits to 30\n);\n\n-- GOOD: Use appropriate sizes\nCREATE TABLE users (\n    email VARCHAR(254),        -- Industry standard max\n    state CHAR(2),            -- Fixed size\n    username VARCHAR(30)       -- Match business rules\n);\n```\n\n**Why it matters:**\n- Wastes space in indexes\n- Can allow invalid data\n- Self-documenting schema\n\n## 18. Not Handling Connection Pooling Properly\n\n```python\n# BAD: Creating new connection for each query\ndef get_user(user_id):\n    conn = create_connection()  # Expensive!\n    result = conn.execute(\"SELECT * FROM users WHERE id = ?\", user_id)\n    conn.close()\n    return result\n\n# GOOD: Use connection pooling\npool = create_connection_pool(min_size=5, max_size=20)\n\ndef get_user(user_id):\n    with pool.get_connection() as conn:\n        return conn.execute(\"SELECT * FROM users WHERE id = ?\", user_id)\n```\n\n## 19. Using String Concatenation Instead of Parameterized Queries\n\n```python\n# BAD: SQL injection vulnerability\nuser_input = \"admin'; DROP TABLE users; --\"\nquery = f\"SELECT * FROM users WHERE username = '{user_input}'\"\n# Result: SELECT * FROM users WHERE username = 'admin'; DROP TABLE users; --'\n\n# GOOD: Parameterized queries\nquery = \"SELECT * FROM users WHERE username = ?\"\ncursor.execute(query, (user_input,))\n```\n\n**Never concatenate user input into SQL!**\n\n## 20. Assuming Database Triggers Will Always Work\n\n```sql\n-- Problem: Triggers don't fire for bulk operations in some databases\nCREATE TRIGGER update_timestamp\nBEFORE UPDATE ON users\nFOR EACH ROW\nSET NEW.updated_at = NOW();\n\n-- Bulk operations might bypass trigger\nLOAD DATA INFILE 'users.csv' INTO TABLE users;  -- Trigger might not fire!\n```\n\n**Best practice:**\n- Don't rely solely on triggers\n- Validate in application too\n- Test bulk operations specifically\n",
        "icartsh-plugin/skills/sql-expert/references/indexes-performance.md": "# Indexes and Performance\n\n## Creating Indexes\n\n```sql\n-- Single column index\nCREATE INDEX idx_users_email ON users(email);\n\n-- Composite index (order matters!)\nCREATE INDEX idx_orders_user_date ON orders(user_id, order_date);\n\n-- Unique index\nCREATE UNIQUE INDEX idx_users_username ON users(username);\n\n-- Partial index (PostgreSQL)\nCREATE INDEX idx_active_users ON users(email) WHERE status = 'active';\n\n-- Functional index\nCREATE INDEX idx_users_email_lower ON users(LOWER(email));\n\n-- Full-text search index (PostgreSQL)\nCREATE INDEX idx_posts_search ON posts USING GIN(to_tsvector('english', title || ' ' || content));\n```\n\n## Index Guidelines\n\n### When to Create Indexes\n\n- ✅ Columns used in WHERE clauses\n- ✅ Columns used in JOIN conditions\n- ✅ Columns used in ORDER BY\n- ✅ Foreign key columns\n- ✅ Columns with high selectivity (many unique values)\n\n### When NOT to Create Indexes\n\n- ❌ Small tables (< 1000 rows)\n- ❌ Columns with low selectivity (few unique values like boolean)\n- ❌ Columns frequently updated\n- ❌ Too many indexes on one table (slows INSERTs/UPDATEs)\n\n## Index Maintenance\n\n```sql\n-- PostgreSQL: Rebuild index\nREINDEX INDEX idx_users_email;\n\n-- MySQL: Optimize table\nOPTIMIZE TABLE users;\n\n-- Check index usage (PostgreSQL)\nSELECT\n    schemaname,\n    tablename,\n    indexname,\n    idx_scan,\n    idx_tup_read,\n    idx_tup_fetch\nFROM\n    pg_stat_user_indexes\nORDER BY\n    idx_scan ASC;\n```\n\n## Composite Index Best Practices\n\nThe order of columns in a composite index matters significantly:\n\n1. **Most selective column first**: Put the column that narrows down results the most at the front\n2. **Equality before range**: Columns used with `=` should come before columns used with `>`, `<`, `BETWEEN`\n3. **Common queries first**: Order based on your most frequent query patterns\n\n```sql\n-- If you often query: WHERE user_id = ? AND order_date > ?\nCREATE INDEX idx_orders_user_date ON orders(user_id, order_date);\n\n-- If you often query: WHERE status = ? AND created_at BETWEEN ? AND ?\nCREATE INDEX idx_orders_status_created ON orders(status, created_at);\n```\n\n## Covering Indexes\n\nA covering index includes all columns needed by a query, allowing an \"Index Only Scan\":\n\n```sql\n-- Query needs: user_id, email, status\nCREATE INDEX idx_users_covering ON users(user_id, email, status);\n\n-- This query can use index-only scan\nSELECT user_id, email, status FROM users WHERE user_id = 123;\n```\n\n## Index Bloat and Cleanup\n\nOver time, indexes can become bloated (especially in PostgreSQL):\n\n```sql\n-- PostgreSQL: Check index bloat\nSELECT\n    schemaname,\n    tablename,\n    indexname,\n    pg_size_pretty(pg_relation_size(indexrelid)) as index_size\nFROM\n    pg_stat_user_indexes\nORDER BY\n    pg_relation_size(indexrelid) DESC;\n\n-- Rebuild bloated indexes\nREINDEX INDEX CONCURRENTLY idx_users_email;  -- PostgreSQL (doesn't lock table)\n```\n\n## Monitoring Index Effectiveness\n\n```sql\n-- PostgreSQL: Find unused indexes\nSELECT\n    schemaname,\n    tablename,\n    indexname,\n    idx_scan\nFROM\n    pg_stat_user_indexes\nWHERE\n    idx_scan = 0\n    AND indexrelname NOT LIKE 'pg_toast%'\nORDER BY\n    pg_relation_size(indexrelid) DESC;\n\n-- MySQL: Check index cardinality\nSHOW INDEX FROM users;\n```\n",
        "icartsh-plugin/skills/sql-expert/references/query-optimization.md": "# Query Optimization\n\n## Using EXPLAIN\n\n```sql\n-- PostgreSQL EXPLAIN\nEXPLAIN ANALYZE\nSELECT\n    users.name,\n    COUNT(orders.id) as order_count\nFROM\n    users\nLEFT JOIN\n    orders ON users.id = orders.user_id\nGROUP BY\n    users.id, users.name;\n\n-- Look for:\n-- - Seq Scan (bad) vs Index Scan (good)\n-- - High cost numbers\n-- - Large row counts being processed\n```\n\n## Key Performance Indicators\n\n- **Seq Scan**: Table scan without index (slow for large tables)\n- **Index Scan**: Using an index (fast)\n- **Index Only Scan**: Best case - all data from index\n- **Nested Loop**: Good for small datasets\n- **Hash Join**: Good for larger datasets\n- **Merge Join**: Good for sorted data\n\n## Optimization Techniques\n\n```sql\n-- BAD: Using OR with different columns\nSELECT * FROM users WHERE first_name = 'John' OR last_name = 'Smith';\n\n-- GOOD: Use UNION if possible\nSELECT * FROM users WHERE first_name = 'John'\nUNION\nSELECT * FROM users WHERE last_name = 'Smith';\n\n-- BAD: Function on indexed column prevents index usage\nSELECT * FROM users WHERE LOWER(email) = 'user@example.com';\n\n-- GOOD: Use functional index or store lowercase\nSELECT * FROM users WHERE email = LOWER('user@example.com');\n-- Create index: CREATE INDEX idx_email_lower ON users(LOWER(email));\n\n-- BAD: SELECT *\nSELECT * FROM large_table WHERE id = 123;\n\n-- GOOD: Select only needed columns\nSELECT id, name, email FROM large_table WHERE id = 123;\n\n-- BAD: Subquery in SELECT (executes for each row)\nSELECT\n    name,\n    (SELECT COUNT(*) FROM orders WHERE user_id = users.id) as order_count\nFROM\n    users;\n\n-- GOOD: Use JOIN instead\nSELECT\n    users.name,\n    COUNT(orders.id) as order_count\nFROM\n    users\nLEFT JOIN\n    orders ON users.id = orders.user_id\nGROUP BY\n    users.id, users.name;\n```\n\n## N+1 Query Problem\n\n```python\n# BAD: N+1 queries (1 + N queries total)\nusers = db.query(\"SELECT * FROM users\")\nfor user in users:\n    orders = db.query(\"SELECT * FROM orders WHERE user_id = ?\", user.id)\n    # This executes a query for EACH user\n\n# GOOD: Single query with JOIN\nresult = db.query(\"\"\"\n    SELECT\n        users.*,\n        orders.*\n    FROM users\n    LEFT JOIN orders ON users.id = orders.user_id\n\"\"\")\n```\n\n## Performance Tips\n\n### Use LIMIT Appropriately\n\n```sql\n-- BAD: Returns all rows (could be millions)\nSELECT * FROM large_table WHERE status = 'active';\n\n-- GOOD: Limit results for exploratory queries\nSELECT * FROM large_table WHERE status = 'active' LIMIT 100;\n```\n\n### Avoid Implicit Type Conversions\n\n```sql\n-- BAD: String comparison on INT column prevents index usage\nSELECT * FROM users WHERE user_id = '123';\n\n-- GOOD: Use correct type\nSELECT * FROM users WHERE user_id = 123;\n```\n\n### Use EXISTS Instead of COUNT(*)\n\n```sql\n-- BAD: Counts all rows\nSELECT COUNT(*) FROM orders WHERE user_id = 123;\n\n-- GOOD: Just check existence\nSELECT EXISTS(SELECT 1 FROM orders WHERE user_id = 123);\n```\n\n### Avoid SELECT DISTINCT as Band-Aid\n\n```sql\n-- BAD: Band-aid solution\nSELECT DISTINCT user_id, name FROM users\nJOIN orders ON users.id = orders.user_id;\n\n-- GOOD: Fix the underlying issue\nSELECT users.id, users.name FROM users\nWHERE EXISTS (SELECT 1 FROM orders WHERE orders.user_id = users.id);\n```\n",
        "icartsh-plugin/skills/sql-optimization-patterns/SKILL.md": "---\nname: sql-optimization-patterns\ndescription: SQL 쿼리 최적화, 인덱스 전략 및 EXPLAIN 분석을 마스터하여 데이터베이스 성능을 획기적으로 향상시키고 느린 쿼리를 제거합니다. 느린 쿼리 디버깅, 데이터베이스 스키마 설계 또는 애플리케이션 성능 최적화 시 사용하세요.\n---\n\n# SQL Optimization Patterns\n\n체계적인 최적화, 올바른 인덱싱 및 쿼리 실행 계획 분석을 통해 느린 데이터베이스 쿼리를 번개처럼 빠른 작업으로 변환하세요.\n\n## 적용 시기\n\n- 느리게 실행되는 쿼리 디버깅\n- 성능이 뛰어난 데이터베이스 스키마 설계\n- 애플리케이션 응답 시간 최적화\n- 데이터베이스 부하 및 비용 절감\n- 데이터 증가에 따른 확장성 개선\n- EXPLAIN 쿼리 실행 계획 분석\n- 효율적인 인덱스 구현\n- N+1 쿼리 문제 해결\n\n## 핵심 개념 (Core Concepts)\n\n### 1. 쿼리 실행 계획 (EXPLAIN)\n\nEXPLAIN 출력을 이해하는 것은 최적화의 기본입니다.\n\n**PostgreSQL EXPLAIN:**\n```sql\n-- 기본 실행 계획 확인\nEXPLAIN SELECT * FROM users WHERE email = 'user@example.com';\n\n-- 실제 실행 통계 포함\nEXPLAIN ANALYZE\nSELECT * FROM users WHERE email = 'user@example.com';\n\n-- 더 많은 세부 정보를 포함한 상세 출력\nEXPLAIN (ANALYZE, BUFFERS, VERBOSE)\nSELECT u.*, o.order_total\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE u.created_at > NOW() - INTERVAL '30 days';\n```\n\n**주의 깊게 봐야 할 주요 지표:**\n- **Seq Scan**: 전체 테이블 스캔 (대용량 테이블에서는 대개 느림)\n- **Index Scan**: 인덱스 사용 (좋음)\n- **Index Only Scan**: 테이블 접근 없이 인덱스만 사용 (가장 좋음)\n- **Nested Loop**: 조인 방식 (작은 데이터셋에는 괜찮음)\n- **Hash Join**: 조인 방식 (큰 데이터셋에 좋음)\n- **Merge Join**: 조인 방식 (정렬된 데이터에 좋음)\n- **Cost**: 추정된 쿼리 비용 (낮을수록 좋음)\n- **Rows**: 추정된 반환 행 수\n- **Actual Time**: 실제 실행 시간\n\n### 2. 인덱스 전략 (Index Strategies)\n\n인덱스는 가장 강력한 최적화 도구입니다.\n\n**인덱스 유형:**\n- **B-Tree**: 기본값, 등호(=) 및 범위 쿼리에 좋음\n- **Hash**: 등호(=) 비교에만 사용\n- **GIN**: 전체 텍스트 검색, 배열 쿼리, JSONB\n- **GiST**: 기하학적 데이터, 전체 텍스트 검색\n- **BRIN**: 데이터 간 상관관계가 있는 매우 큰 테이블을 위한 블록 범위 인덱스\n\n```sql\n-- 표준 B-Tree 인덱스\nCREATE INDEX idx_users_email ON users(email);\n\n-- 복합 인덱스 (순서가 중요합니다!)\nCREATE INDEX idx_orders_user_status ON orders(user_id, status);\n\n-- 부분 인덱스 (행의 일부만 인덱싱)\nCREATE INDEX idx_active_users ON users(email)\nWHERE status = 'active';\n\n-- 표현식 인덱스 (함수 기반 인덱스)\nCREATE INDEX idx_users_lower_email ON users(LOWER(email));\n\n-- 커버링 인덱스 (추가 컬럼 포함)\nCREATE INDEX idx_users_email_covering ON users(email)\nINCLUDE (name, created_at);\n\n-- 전체 텍스트 검색 인덱스\nCREATE INDEX idx_posts_search ON posts\nUSING GIN(to_tsvector('english', title || ' ' || body));\n\n-- JSONB 인덱스\nCREATE INDEX idx_metadata ON events USING GIN(metadata);\n```\n\n### 3. 쿼리 최적화 패턴\n\n**SELECT * 피하기:**\n```sql\n-- 나쁨: 불필요한 모든 컬럼을 가져옴\nSELECT * FROM users WHERE id = 123;\n\n-- 좋음: 필요한 컬럼만 명시\nSELECT id, email, name FROM users WHERE id = 123;\n```\n\n**WHERE 절의 효율적 사용:**\n```sql\n-- 나쁨: 함수 사용으로 인덱스 활용 불가\nSELECT * FROM users WHERE LOWER(email) = 'user@example.com';\n\n-- 좋음: 함수 기반 인덱스(functional index) 생성 또는 정확한 일치 사용\nCREATE INDEX idx_users_email_lower ON users(LOWER(email));\n-- 그 다음:\nSELECT * FROM users WHERE LOWER(email) = 'user@example.com';\n\n-- 또는 데이터를 정규화하여 저장\nSELECT * FROM users WHERE email = 'user@example.com';\n```\n\n**JOIN 최적화:**\n```sql\n-- 나쁨: 카테시안 곱 생성 후 필터링\nSELECT u.name, o.total\nFROM users u, orders o\nWHERE u.id = o.user_id AND u.created_at > '2024-01-01';\n\n-- 좋음: 조인 전 필터링\nSELECT u.name, o.total\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE u.created_at > '2024-01-01';\n\n-- 더 좋음: 두 테이블 모두 사전 필터링\nSELECT u.name, o.total\nFROM (SELECT * FROM users WHERE created_at > '2024-01-01') u\nJOIN orders o ON u.id = o.user_id;\n```\n\n## 최적화 패턴 (Optimization Patterns)\n\n### 패턴 1: N+1 쿼리 제거\n\n**문제: N+1 쿼리 안티 패턴**\n```python\n# 나쁨: N+1개의 쿼리를 실행함\nusers = db.query(\"SELECT * FROM users LIMIT 10\")\nfor user in users:\n    orders = db.query(\"SELECT * FROM orders WHERE user_id = ?\", user.id)\n    # orders 처리\n```\n\n**해결책: JOIN 또는 배치 로딩(Batch Loading) 사용**\n```sql\n-- 해결책 1: JOIN 사용\nSELECT\n    u.id, u.name,\n    o.id as order_id, o.total\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nWHERE u.id IN (1, 2, 3, 4, 5);\n\n-- 해결책 2: 배치 쿼리\nSELECT * FROM orders\nWHERE user_id IN (1, 2, 3, 4, 5);\n```\n\n```python\n# 좋음: JOIN 또는 배치 로드를 통한 단일 쿼리 실행\n# JOIN 사용 시\nresults = db.query(\"\"\"\n    SELECT u.id, u.name, o.id as order_id, o.total\n    FROM users u\n    LEFT JOIN orders o ON u.id = o.user_id\n    WHERE u.id IN (1, 2, 3, 4, 5)\n\"\"\")\n\n# 또는 배치 로드(Batch load)\nusers = db.query(\"SELECT * FROM users LIMIT 10\")\nuser_ids = [u.id for u in users]\norders = db.query(\n    \"SELECT * FROM orders WHERE user_id IN (?)\",\n    user_ids\n)\n# user_id별로 orders 그룹화\norders_by_user = {}\nfor order in orders:\n    orders_by_user.setdefault(order.user_id, []).append(order)\n```\n\n### 패턴 2: 페이지네이션(Pagination) 최적화\n\n**나쁨: 대용량 테이블에서의 OFFSET 사용**\n```sql\n-- 큰 offset 값에서 속도 저하 발생\nSELECT * FROM users\nORDER BY created_at DESC\nLIMIT 20 OFFSET 100000;  -- 매우 느림!\n```\n\n**좋음: 커서 기반 페이지네이션 (Cursor-Based Pagination)**\n```sql\n-- 훨씬 빠름: 커서(마지막 확인된 ID) 사용\nSELECT * FROM users\nWHERE created_at < '2024-01-15 10:30:00'  -- 마지막 커서\nORDER BY created_at DESC\nLIMIT 20;\n\n-- 복합 정렬 시\nSELECT * FROM users\nWHERE (created_at, id) < ('2024-01-15 10:30:00', 12345)\nORDER BY created_at DESC, id DESC\nLIMIT 20;\n\n-- 인덱스 필요\nCREATE INDEX idx_users_cursor ON users(created_at DESC, id DESC);\n```\n\n### 패턴 3: 효율적인 집계 (Aggregate Efficiently)\n\n**COUNT 쿼리 최적화:**\n```sql\n-- 나쁨: 모든 행을 카운트함\nSELECT COUNT(*) FROM orders;  -- 큰 테이블에서 느림\n\n-- 좋음: 근사치를 위한 추정치(estimates) 사용\nSELECT reltuples::bigint AS estimate\nFROM pg_class\nWHERE relname = 'orders';\n\n-- 좋음: 카운트 전 필터링 적용\nSELECT COUNT(*) FROM orders\nWHERE created_at > NOW() - INTERVAL '7 days';\n\n-- 더 좋음: 인덱스 전용 스캔(index-only scan) 활용\nCREATE INDEX idx_orders_created ON orders(created_at);\nSELECT COUNT(*) FROM orders\nWHERE created_at > NOW() - INTERVAL '7 days';\n```\n\n**GROUP BY 최적화:**\n```sql\n-- 나쁨: 그룹화 후 필터링\nSELECT user_id, COUNT(*) as order_count\nFROM orders\nGROUP BY user_id\nHAVING COUNT(*) > 10;\n\n-- 좋음: 가능한 경우 먼저 필터링 후 그룹화\nSELECT user_id, COUNT(*) as order_count\nFROM orders\nWHERE status = 'completed'\nGROUP BY user_id\nHAVING COUNT(*) > 10;\n\n-- 가장 좋음: 커버링 인덱스 활용\nCREATE INDEX idx_orders_user_status ON orders(user_id, status);\n```\n\n### 패턴 4: 서브쿼리 최적화\n\n**상관 서브쿼리(Correlated Subqueries) 변환:**\n```sql\n-- 나쁨: 상관 서브쿼리 (각 행마다 실행됨)\nSELECT u.name, u.email,\n    (SELECT COUNT(*) FROM orders o WHERE o.user_id = u.id) as order_count\nFROM users u;\n\n-- 좋음: 집계가 포함된 JOIN\nSELECT u.name, u.email, COUNT(o.id) as order_count\nFROM users u\nLEFT JOIN orders o ON o.user_id = u.id\nGROUP BY u.id, u.name, u.email;\n\n-- 더 좋음: 윈도우 함수 사용\nSELECT DISTINCT ON (u.id)\n    u.name, u.email,\n    COUNT(o.id) OVER (PARTITION BY u.id) as order_count\nFROM users u\nLEFT JOIN orders o ON o.user_id = u.id;\n```\n\n**가독성을 위한 CTE 사용:**\n```sql\n-- 공통 테이블 식별자(CTE) 활용\nWITH recent_users AS (\n    SELECT id, name, email\n    FROM users\n    WHERE created_at > NOW() - INTERVAL '30 days'\n),\nuser_order_counts AS (\n    SELECT user_id, COUNT(*) as order_count\n    FROM orders\n    WHERE created_at > NOW() - INTERVAL '30 days'\n    GROUP BY user_id\n)\nSELECT ru.name, ru.email, COALESCE(uoc.order_count, 0) as orders\nFROM recent_users ru\nLEFT JOIN user_order_counts uoc ON ru.id = uoc.user_id;\n```\n\n### 패턴 5: 배치 작업 (Batch Operations)\n\n**배치 INSERT:**\n```sql\n-- 나쁨: 다수의 개별 insert 수행\nINSERT INTO users (name, email) VALUES ('Alice', 'alice@example.com');\nINSERT INTO users (name, email) VALUES ('Bob', 'bob@example.com');\nINSERT INTO users (name, email) VALUES ('Carol', 'carol@example.com');\n\n-- 좋음: 배치 insert\nINSERT INTO users (name, email) VALUES\n    ('Alice', 'alice@example.com'),\n    ('Bob', 'bob@example.com'),\n    ('Carol', 'carol@example.com');\n\n-- 더 좋음: 대량 insert 시 COPY 활용 (PostgreSQL)\nCOPY users (name, email) FROM '/tmp/users.csv' CSV HEADER;\n```\n\n**배치 UPDATE:**\n```sql\n-- 나쁨: 반복문 내 업데이트\nUPDATE users SET status = 'active' WHERE id = 1;\nUPDATE users SET status = 'active' WHERE id = 2;\n-- ... 많은 ID를 반복\n\n-- 좋음: IN 절을 활용한 단일 UPDATE\nUPDATE users\nSET status = 'active'\nWHERE id IN (1, 2, 3, 4, 5, ...);\n\n-- 더 좋음: 대량 배치 시 임시 테이블 활용\nCREATE TEMP TABLE temp_user_updates (id INT, new_status VARCHAR);\nINSERT INTO temp_user_updates VALUES (1, 'active'), (2, 'active'), ...;\n\nUPDATE users u\nSET status = t.new_status\nFROM temp_user_updates t\nWHERE u.id = t.id;\n```\n\n## 고급 기범 (Advanced Techniques)\n\n### 구체화된 뷰 (Materialized Views)\n\n비용이 많이 드는 쿼리를 미리 계산해 둡니다.\n\n```sql\n-- 구체화된 뷰 생성\nCREATE MATERIALIZED VIEW user_order_summary AS\nSELECT\n    u.id,\n    u.name,\n    COUNT(o.id) as total_orders,\n    SUM(o.total) as total_spent,\n    MAX(o.created_at) as last_order_date\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nGROUP BY u.id, u.name;\n\n-- 구체화된 뷰에 인덱스 추가\nCREATE INDEX idx_user_summary_spent ON user_order_summary(total_spent DESC);\n\n-- 구체화된 뷰 갱신\nREFRESH MATERIALIZED VIEW user_order_summary;\n\n-- 동시 갱신 (PostgreSQL, 락 최소화)\nREFRESH MATERIALIZED VIEW CONCURRENTLY user_order_summary;\n\n-- 구체화된 뷰 쿼리 (매우 빠름)\nSELECT * FROM user_order_summary\nWHERE total_spent > 1000\nORDER BY total_spent DESC;\n```\n\n### 파티셔닝 (Partitioning)\n\n성능 향상을 위해 대형 테이블을 나눕니다.\n\n```sql\n-- 날짜별 범위 파티셔닝 (PostgreSQL)\nCREATE TABLE orders (\n    id SERIAL,\n    user_id INT,\n    total DECIMAL,\n    created_at TIMESTAMP\n) PARTITION BY RANGE (created_at);\n\n-- 파티션 생성\nCREATE TABLE orders_2024_q1 PARTITION OF orders\n    FOR VALUES FROM ('2024-01-01') TO ('2024-04-01');\n\nCREATE TABLE orders_2024_q2 PARTITION OF orders\n    FOR VALUES FROM ('2024-04-01') TO ('2024-07-01');\n\n-- 쿼리는 자동으로 적절한 파티션을 사용함\nSELECT * FROM orders\nWHERE created_at BETWEEN '2024-02-01' AND '2024-02-28';\n-- orders_2024_q1 파티션만 스캔함\n```\n\n### 쿼리 힌트 및 최적화\n\n```sql\n-- 인덱스 사용 강제 (MySQL)\nSELECT * FROM users\nUSE INDEX (idx_users_email)\nWHERE email = 'user@example.com';\n\n-- 병렬 쿼리 (PostgreSQL)\nSET max_parallel_workers_per_gather = 4;\nSELECT * FROM large_table WHERE condition;\n\n-- 조인 힌트 (PostgreSQL)\nSET enable_nestloop = OFF;  -- hash join 또는 merge join 강제\n```\n\n## 모범 사례 (Best Practices)\n\n1. **선별적인 인덱싱**: 인덱스가 너무 많으면 쓰기 작업이 느려집니다.\n2. **쿼리 성능 모니터링**: 느린 쿼리 로그(slow query logs)를 활용하세요.\n3. **통계 정보 업데이트 유지**: 정기적으로 ANALYZE를 실행하세요.\n4. **적절한 데이터 타입 사용**: 작은 타입일수록 성능이 좋습니다.\n5. **사려 깊은 정규화**: 정규화와 성능 사이의 균형을 맞추세요.\n6. **자주 접근하는 데이터 캐싱**: 애플리케이션 레벨 캐싱을 활용하세요.\n7. **커넥션 풀링 (Connection Pooling)**: 데이터베이스 연결을 재사용하세요.\n8. **정기적인 유지보수**: VACUUM, ANALYZE, 인덱스 재빌드 등을 수행하세요.\n\n```sql\n-- 통계 업데이트\nANALYZE users;\nANALYZE VERBOSE orders;\n\n-- Vacuum (PostgreSQL)\nVACUUM ANALYZE users;\nVACUUM FULL users;  -- 공간 회수 (테이블 락 발생)\n\n-- 인덱스 재구성\nREINDEX INDEX idx_users_email;\nREINDEX TABLE users;\n```\n\n## 자주 발생하는 문제 (Common Pitfalls)\n\n- **과도한 인덱싱**: 각 인덱스는 INSERT/UPDATE/DELETE 속도를 늦춥니다.\n- **사용되지 않는 인덱스**: 공간을 낭비하고 쓰기 성능을 저하시킵니다.\n- **인덱스 누락**: 쿼리 속도 저하, 전체 테이블 스캔 유발.\n- **암시적 타입 변환**: 인덱스 사용을 방해합니다.\n- **OR 조건**: 인덱스를 효율적으로 사용하기 어렵게 만들 수 있습니다.\n- **와일드카드가 앞에 붙은 LIKE**: `LIKE '%abc'`는 인덱스를 탈 수 없습니다.\n- **WHERE 절의 함수**: 기능 기반 인덱스가 없다면 인덱스 사용을 방해합니다.\n\n## 쿼리 모니터링\n\n```sql\n-- 느린 쿼리 찾기 (PostgreSQL)\nSELECT query, calls, total_time, mean_time\nFROM pg_stat_statements\nORDER BY mean_time DESC\nLIMIT 10;\n\n-- 누락된 인덱스 찾기 (PostgreSQL)\nSELECT\n    schemaname,\n    tablename,\n    seq_scan,\n    seq_tup_read,\n    idx_scan,\n    seq_tup_read / seq_scan AS avg_seq_tup_read\nFROM pg_stat_user_tables\nWHERE seq_scan > 0\nORDER BY seq_tup_read DESC\nLIMIT 10;\n\n-- 사용되지 않는 인덱스 찾기 (PostgreSQL)\nSELECT\n    schemaname,\n    tablename,\n    indexname,\n    idx_scan,\n    idx_tup_read,\n    idx_tup_fetch\nFROM pg_stat_user_indexes\nWHERE idx_scan = 0\nORDER BY pg_relation_size(indexrelid) DESC;\n```\n\n## 리소스\n\n- **references/postgres-optimization-guide.md**: PostgreSQL 전용 최적화\n- **references/mysql-optimization-guide.md**: MySQL/MariaDB 최적화\n- **references/query-plan-analysis.md**: EXPLAIN 실행 계획 심층 분석\n- **assets/index-strategy-checklist.md**: 인덱스 생성 시점 및 방법\n- **assets/query-optimization-checklist.md**: 단계별 최적화 가이드\n- **scripts/analyze-slow-queries.sql**: 데이터베이스 내 느린 쿼리 식별\n- **scripts/index-recommendations.sql**: 인덱스 권장 사항 생성\n",
        "icartsh-plugin/skills/web-artifacts-builder/SKILL.md": "---\nname: web-artifacts-builder\ndescription: 현대적인 프런트엔드 웹 기술(React, Tailwind CSS, shadcn/ui)을 사용하여 정교한 다중 컴포넌트 claude.ai용 HTML artifact를 제작하기 위한 툴킷입니다. 상태 관리, 라우팅 또는 shadcn/ui 컴포넌트가 필요한 복잡한 artifact에 사용하세요. 단순한 단일 파일 HTML/JSX artifact용이 아닙니다.\nlicense: LICENSE.txt의 전체 약관 참조\n---\n\n# Web Artifacts Builder\n\n강력한 프런트엔드 claude.ai artifact를 제작하려면 다음 단계를 따르세요:\n1. `scripts/init-artifact.sh`를 사용하여 프런트엔드 레포지토리를 초기화합니다.\n2. 생성된 코드를 편집하여 artifact를 개발합니다.\n3. `scripts/bundle-artifact.sh`를 사용하여 모든 코드를 단일 HTML 파일로 번들링합니다.\n4. 사용자에게 artifact를 표시합니다.\n5. (선택 사항) artifact를 테스트합니다.\n\n**기술 스택(Stack)**: React 18 + TypeScript + Vite + Parcel (번들링) + Tailwind CSS + shadcn/ui\n\n## 디자인 및 스타일 가이드라인\n\n매우 중요: \"AI가 만든 뻔한 느낌(AI slop)\"을 피하기 위해, 과도한 중앙 정렬 레이아웃, 보라색 그라데이션, 일률적인 둥근 모서리, Inter 폰트 사용을 지양하세요.\n\n## 빠른 시작 (Quick Start)\n\n### Step 1: 프로젝트 초기화\n\n초기화 스크립트를 실행하여 새로운 React 프로젝트를 생성합니다:\n```bash\nbash scripts/init-artifact.sh <project-name>\ncd <project-name>\n```\n\n이 명령은 다음이 구성된 프로젝트를 생성합니다:\n- ✅ React + TypeScript (Vite 기반)\n- ✅ Tailwind CSS 3.4.1 (shadcn/ui 테마 시스템 포함)\n- ✅ 경로 별칭(`@/`) 설정 완료\n- ✅ 40개 이상의 shadcn/ui 컴포넌트 사전 설치\n- ✅ 모든 Radix UI 종속성 포함\n- ✅ Parcel 번들링 설정 완료 (.parcelrc 사용)\n- ✅ Node 18+ 호환성 (Vite 버전 자동 정밀 감지)\n\n### Step 2: artifact 개발\n\n생성된 파일들을 편집하여 artifact를 빌드합니다. 안내가 필요한 경우 아래의 **일반적인 개발 작업**을 참조하세요.\n\n### Step 3: 단일 HTML 파일로 번들링\n\nReact 앱을 단일 HTML artifact로 번들링합니다:\n```bash\nbash scripts/bundle-artifact.sh\n```\n\n이 명령은 `bundle.html`을 생성합니다. 이는 모든 JavaScript, CSS 및 종속성이 인라인화된 독립적인 artifact 파일입니다. 이 파일은 Claude 대화에서 artifact로 직접 공유할 수 있습니다.\n\n**요구 사항**: 프로젝트 루트 디렉토리에 `index.html`이 있어야 합니다.\n\n**스크립트 동작 원리**:\n- 번들링 종속성 설치 (parcel, @parcel/config-default, parcel-resolver-tspaths, html-inline)\n- 경로 별칭을 지원하는 `.parcelrc` 설정 생성\n- Parcel로 빌드 (소스 맵 제외)\n- html-inline을 사용하여 모든 에셋을 단일 HTML로 인라인화\n\n### Step 4: 사용자에게 artifact 공유\n\n번들링된 HTML 파일을 사용자에게 공유하여 artifact로 볼 수 있게 합니다.\n\n### Step 5: artifact 테스트/시각화 (선택 사항)\n\n참고: 이 단계는 완전히 선택 사항입니다. 필요한 경우 또는 요청이 있을 때만 수행하세요.\n\nartifact를 테스트하거나 시각화하려면 가용한 도구들(다른 SKILL이나 Playwright, Puppeteer와 같은 내장 도구 포함)을 사용하세요. 일반적으로 artifact 테스트를 미리 수행하는 것은 요청과 결과 확인 사이의 지연 시간(latency)을 유발하므로 피하는 것이 좋습니다. 요청이 있거나 문제가 발생했을 때, artifact를 먼저 제시한 후에 테스트를 진행하세요.\n\n## 참조 (Reference)\n\n- **shadcn/ui components**: https://ui.shadcn.com/docs/components",
        "icartsh-plugin/skills/webapp-testing/SKILL.md": "---\nname: webapp-testing\ndescription: Playwright를 사용하여 로컬 웹 애플리케이션과 상호작용하고 테스트하기 위한 툴킷입니다. 프런트엔드 기능 검증, UI 동작 디버깅, 브라우저 스크린샷 캡처 및 브라우저 로그 확인을 지원합니다.\nlicense: LICENSE.txt의 전체 약관 참조\n---\n\n# Web Application Testing\n\n로컬 웹 애플리케이션을 테스트하려면 파이썬(Python) Playwright 스크립트를 작성하세요.\n\n**사용 가능한 헬퍼 스크립트**:\n- `scripts/with_server.py` - 서버 라이프사이클 관리 (다중 서버 지원)\n\n**항상 스크립트를 `--help`와 함께 먼저 실행**하여 사용법을 확인하세요. 스크립트를 직접 실행해보고 커스텀 솔루션이 절대적으로 필요하다고 판단되기 전까지는 소스 코드를 읽지 마세요. 이러한 스크립트들은 매우 방대하여 컨텍스트 윈도우를 오염시킬 수 있습니다. 이들은 컨텍스트에 담기보다는 블랙박스(black-box) 스크립트로 직접 호출하도록 만들어졌습니다.\n\n## 결정 트리: 접근 방식 선택 (Decision Tree)\n\n```\n사용자 작업 → 정적 HTML인가?\n    ├─ 예 → HTML 파일을 직접 읽어 셀렉터(selectors) 식별\n    │         ├─ 성공 → 식별된 셀렉터로 Playwright 스크립트 작성\n    │         └─ 실패/불충분 → 동적 앱으로 간주 (아래 참조)\n    │\n    └─ 아니오 (동적 웹앱) → 서버가 이미 실행 중인가?\n        ├─ 아니오 → 실행: python scripts/with_server.py --help\n        │            그다음 헬퍼를 사용하여 간소화된 Playwright 스크립트 작성\n        │\n        └─ 예 → 정찰 후 행동(Reconnaissance-then-action):\n            1. 이동(Navigate) 후 networkidle 상태까지 대기\n            2. 스크린샷 캡처 또는 DOM 검사\n            3. 렌더링된 상태에서 셀렉터 식별\n            4. 찾은 셀렉터로 작업 수행\n```\n\n## 예시: with_server.py 사용\n\n서버를 시작하려면 먼저 `--help`를 실행한 다음 헬퍼를 사용하세요:\n\n**단일 서버:**\n```bash\npython scripts/with_server.py --server \"npm run dev\" --port 5173 -- python your_automation.py\n```\n\n**다중 서버 (예: 백엔드 + 프런트엔드):**\n```bash\npython scripts/with_server.py \\\n  --server \"cd backend && python server.py\" --port 3000 \\\n  --server \"cd frontend && npm run dev\" --port 5173 \\\n  -- python your_automation.py\n```\n\n자동화 스크립트를 작성할 때는 Playwright 로직만 포함하세요 (서버는 자동으로 관리됩니다):\n```python\nfrom playwright.sync_api import sync_playwright\n\nwith sync_playwright() as p:\n    browser = p.chromium.launch(headless=True) # 항상 chromium을 headless 모드로 실행하세요\n    page = browser.new_page()\n    page.goto('http://localhost:5173') # 서버는 이미 실행되어 준비된 상태입니다\n    page.wait_for_load_state('networkidle') # 매우 중요: JS가 실행될 때까지 대기하세요\n    # ... 자동화 로직 작성\n    browser.close()\n```\n\n## 정찰 후 행동 패턴 (Reconnaissance-Then-Action Pattern)\n\n1. **렌더링된 DOM 검사**:\n   ```python\n   page.screenshot(path='/tmp/inspect.png', full_page=True)\n   content = page.content()\n   page.locator('button').all()\n   ```\n\n2. **검사 결과에서 셀렉터 식별**\n\n3. **찾은 셀렉터를 사용하여 작업 실행**\n\n## 주의해야 할 함정\n\n❌ **나쁨**: 동적 앱에서 `networkidle` 상태를 기다리지 않고 DOM을 검사하는 것\n✅ **좋음**: 검사 전에 `page.wait_for_load_state('networkidle')`를 호출하여 대기하는 것\n\n## 모범 사례 (Best Practices)\n\n- **번들링된 스크립트를 블랙박스로 활용하세요** - 작업을 수행할 때 `scripts/`에 있는 스크립트가 도움이 될 수 있는지 고려하세요. 이 스크립트들은 컨텍스트 윈도우를 어지럽히지 않으면서 복잡한 일반 워크플로우를 안정적으로 처리합니다. `--help`로 사용법을 확인한 후 직접 호출하세요.\n- 동기식 스크립트에는 `sync_playwright()`를 사용하세요.\n- 작업이 끝나면 항상 브라우저를 닫으세요.\n- 설명적인 셀렉터를 사용하세요: `text=`, `role=`, CSS 셀렉터 또는 ID.\n- 적절한 대기 시간을 추가하세요: `page.wait_for_selector()` 또는 `page.wait_for_timeout()`.\n\n## 참조 파일 (Reference Files)\n\n- **examples/** - 일반적인 패턴을 보여주는 예시:\n  - `element_discovery.py` - 페이지 내 버튼, 링크 및 입력 필드 찾기\n  - `static_html_automation.py` - 로컬 HTML에 file:// URL 사용하기\n  - `console_logging.py` - 자동화 중 콘솔 로그 캡처하기"
      },
      "plugins": [
        {
          "name": "icartsh-plugin",
          "source": "./icartsh-plugin",
          "description": "ICARTSH Plugin",
          "strict": false,
          "skills": [
            "./icartsh-plugin/skills/api-designer",
            "./icartsh-plugin/skills/brainstorming",
            "./icartsh-plugin/skills/code-analyze",
            "./icartsh-plugin/skills/code-format",
            "./icartsh-plugin/skills/code-reviewer",
            "./icartsh-plugin/skills/code-write",
            "./icartsh-plugin/skills/coding-conventions",
            "./icartsh-plugin/skills/csharp-async-patterns",
            "./icartsh-plugin/skills/csharp-developer",
            "./icartsh-plugin/skills/docker-workflow",
            "./icartsh-plugin/skills/dotnet-build",
            "./icartsh-plugin/skills/dotnet-test",
            "./icartsh-plugin/skills/error-detective",
            "./icartsh-plugin/skills/file-organizer",
            "./icartsh-plugin/skills/frontend-design",
            "./icartsh-plugin/skills/git-advanced",
            "./icartsh-plugin/skills/markdown-pro",
            "./icartsh-plugin/skills/mcp-builder",
            "./icartsh-plugin/skills/sequential-thinking",
            "./icartsh-plugin/skills/skill-creator",
            "./icartsh-plugin/skills/sql-expert",
            "./icartsh-plugin/skills/sql-optimization-patterns",
            "./icartsh-plugin/skills/web-artifacts-builder",
            "./icartsh-plugin/skills/webapp-testing"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add icartsh/icartsh_plugin",
            "/plugin install icartsh-plugin@icartsh-marketplace"
          ]
        }
      ]
    }
  ]
}