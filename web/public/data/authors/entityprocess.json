{
  "author": {
    "id": "EntityProcess",
    "display_name": "EntityProcess",
    "avatar_url": "https://avatars.githubusercontent.com/u/8837957?v=4"
  },
  "marketplaces": [
    {
      "name": "agentv",
      "version": null,
      "description": "Evaluate and optimize AI agents",
      "repo_full_name": "EntityProcess/agentv",
      "repo_url": "https://github.com/EntityProcess/agentv",
      "repo_description": "Light-weight AI agent evaluation and optimization framework",
      "signals": {
        "stars": 10,
        "forks": 0,
        "pushed_at": "2026-02-20T18:09:07Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"$schema\": \"https://anthropic.com/claude-code/marketplace.schema.json\",\n  \"name\": \"agentv\",\n  \"description\": \"Evaluate and optimize AI agents\",\n  \"owner\": {\n    \"name\": \"AgentV\",\n    \"email\": \"support@agentv.dev\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"agentv\",\n      \"description\": \"Automates AI agent evaluations and prompt optimization\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./skills/agentv-eval-builder\",\n        \"./skills/agentv-eval-orchestrator\",\n        \"./skills/agentv-prompt-optimizer\"\n      ],\n      \"category\": \"development\"\n    }\n  ]\n}\n",
        "README.md": "# AgentV\n\n**CLI-first AI agent evaluation. No server. No signup. No overhead.**\n\nAgentV evaluates your agents locally with multi-objective scoring (correctness, latency, cost, safety) from YAML specifications. Deterministic code judges + customizable LLM judges, all version-controlled in Git.\n\n## Installation\n\n**1. Install:**\n```bash\nnpm install -g agentv\n```\n\n**2. Initialize your workspace:**\n```bash\nagentv init\n```\n\n**3. Configure environment variables:**\n- The init command creates a `.env.example` file in your project root\n- Copy `.env.example` to `.env` and fill in your API keys, endpoints, and other configuration values\n- Update the environment variable names in `.agentv/targets.yaml` to match those defined in your `.env` file\n\n**4. Create an eval** (`./evals/example.yaml`):\n```yaml\ndescription: Math problem solving evaluation\nexecution:\n  target: default\n\ntests:\n  - id: addition\n    criteria: Correctly calculates 15 + 27 = 42\n\n    input: What is 15 + 27?\n\n    expected_output: \"42\"\n\n    execution:\n      evaluators:\n        - name: math_check\n          type: code_judge\n          script: ./validators/check_math.py\n```\n\n**5. Run the eval:**\n```bash\nagentv eval ./evals/example.yaml\n```\n\nResults appear in `.agentv/results/eval_<timestamp>.jsonl` with scores, reasoning, and execution traces.\n\nLearn more in the [examples/](examples/README.md) directory. For a detailed comparison with other frameworks, see [docs/COMPARISON.md](docs/COMPARISON.md).\n\n## Why AgentV?\n\n| Feature | AgentV | [LangWatch](https://github.com/langwatch/langwatch) | [LangSmith](https://github.com/langchain-ai/langsmith-sdk) | [LangFuse](https://github.com/langfuse/langfuse) |\n|---------|--------|-----------|-----------|----------|\n| **Setup** | `npm install` | Cloud account + API key | Cloud account + API key | Cloud account + API key |\n| **Server** | None (local) | Managed cloud | Managed cloud | Managed cloud |\n| **Privacy** | All local | Cloud-hosted | Cloud-hosted | Cloud-hosted |\n| **CLI-first** | ✓ | ✗ | Limited | Limited |\n| **CI/CD ready** | ✓ | Requires API calls | Requires API calls | Requires API calls |\n| **Version control** | ✓ (YAML in Git) | ✗ | ✗ | ✗ |\n| **Evaluators** | Code + LLM + Custom | LLM only | LLM + Code | LLM only |\n\n**Best for:** Developers who want evaluation in their workflow, not a separate dashboard. Teams prioritizing privacy and reproducibility.\n\n## Features\n\n- **Multi-objective scoring**: Correctness, latency, cost, safety in one run\n- **Multiple evaluator types**: Code validators, LLM judges, custom Python/TypeScript\n- **Built-in targets**: VS Code Copilot, Codex CLI, Pi Coding Agent, Azure OpenAI, local CLI agents\n- **Structured evaluation**: Rubric-based grading with weights and requirements\n- **Batch evaluation**: Run hundreds of test cases in parallel\n- **Export**: JSON, JSONL, YAML formats\n- **Compare results**: Compute deltas between evaluation runs for A/B testing\n\n## Development\n\nContributing to AgentV? Clone and set up the repository:\n\n```bash\ngit clone https://github.com/EntityProcess/agentv.git\ncd agentv\n\n# Install Bun if you don't have it\ncurl -fsSL https://bun.sh/install | bash\n\n# Install dependencies and build\nbun install && bun run build\n\n# Run tests\nbun test\n```\n\nSee [AGENTS.md](AGENTS.md) for development guidelines and design principles.\n\n### Releasing\n\nStable release:\n\n```bash\nbun run release          # patch bump\nbun run release minor\nbun run release major\nbun run publish          # publish to npm `latest`\n```\n\nPrerelease (`next`) channel:\n\n```bash\nbun run release:next         # bump/increment `-next.N`\nbun run release:next major   # start new major prerelease line\nbun run publish:next         # publish to npm `next`\n```\n\n## Core Concepts\n\n**Evaluation files** (`.yaml` or `.jsonl`) define test cases with expected outcomes. **Targets** specify which agent/provider to evaluate. **Judges** (code or LLM) score results. **Results** are written as JSONL/YAML for analysis and comparison.\n\n### JSONL Format Support\n\nFor large-scale evaluations, AgentV supports JSONL (JSON Lines) format as an alternative to YAML:\n\n```jsonl\n{\"id\": \"test-1\", \"criteria\": \"Calculates correctly\", \"input\": \"What is 2+2?\"}\n{\"id\": \"test-2\", \"criteria\": \"Provides explanation\", \"input\": \"Explain variables\"}\n```\n\nOptional sidecar YAML metadata file (`dataset.yaml` alongside `dataset.jsonl`):\n```yaml\ndescription: Math evaluation dataset\ndataset: math-tests\nexecution:\n  target: azure_base\nevaluator: llm_judge\n```\n\nBenefits: Streaming-friendly, Git-friendly diffs, programmatic generation, industry standard (DeepEval, LangWatch, Hugging Face).\n\n## Usage\n\n### Running Evaluations\n\n```bash\n# Validate evals\nagentv validate evals/my-eval.yaml\n\n# Run an eval with default target (from eval file or targets.yaml)\nagentv eval evals/my-eval.yaml\n\n# Override target\nagentv eval --target azure_base evals/**/*.yaml\n\n# Run specific test\nagentv eval --test-id case-123 evals/my-eval.yaml\n\n# Dry-run with mock provider\nagentv eval --dry-run evals/my-eval.yaml\n```\n\nSee `agentv eval --help` for all options: workers, timeouts, output formats, trace dumping, and more.\n\n### Create Custom Evaluators\n\nWrite code judges in Python or TypeScript:\n\n```python\n# validators/check_answer.py\nimport json, sys\ndata = json.load(sys.stdin)\ncandidate_answer = data.get(\"candidate_answer\", \"\")\n\nhits = []\nmisses = []\n\nif \"42\" in candidate_answer:\n    hits.append(\"Answer contains correct value (42)\")\nelse:\n    misses.append(\"Answer does not contain expected value (42)\")\n\nscore = 1.0 if hits else 0.0\n\nprint(json.dumps({\n    \"score\": score,\n    \"hits\": hits,\n    \"misses\": misses,\n    \"reasoning\": f\"Passed {len(hits)} check(s)\"\n}))\n```\n\nReference evaluators in your eval file:\n\n```yaml\nexecution:\n  evaluators:\n    - name: my_validator\n      type: code_judge\n      script: ./validators/check_answer.py\n```\n\nFor complete templates, examples, and evaluator patterns, see: [custom-evaluators](https://agentv.dev/evaluators/custom-evaluators/)\n\n### Compare Evaluation Results\n\nRun two evaluations and compare them:\n\n```bash\nagentv eval evals/my-eval.yaml --out before.jsonl\n# ... make changes to your agent ...\nagentv eval evals/my-eval.yaml --out after.jsonl\nagentv compare before.jsonl after.jsonl --threshold 0.1\n```\n\nOutput shows wins, losses, ties, and mean delta to identify improvements.\n\n## Targets Configuration\n\nDefine execution targets in `.agentv/targets.yaml` to decouple evals from providers:\n\n```yaml\ntargets:\n  - name: azure_base\n    provider: azure\n    endpoint: ${{ AZURE_OPENAI_ENDPOINT }}\n    api_key: ${{ AZURE_OPENAI_API_KEY }}\n    model: ${{ AZURE_DEPLOYMENT_NAME }}\n\n  - name: vscode_dev\n    provider: vscode\n    workspace_template: ${{ WORKSPACE_PATH }}\n    judge_target: azure_base\n\n  - name: local_agent\n    provider: cli\n    command_template: 'python agent.py --prompt {PROMPT}'\n    judge_target: azure_base\n```\n\nSupports: `azure`, `anthropic`, `gemini`, `codex`, `copilot`, `pi-coding-agent`, `claude`, `vscode`, `vscode-insiders`, `cli`, and `mock`.\n\nUse `${{ VARIABLE_NAME }}` syntax to reference your `.env` file. See `.agentv/targets.yaml` after `agentv init` for detailed examples and all provider-specific fields.\n\n## Evaluation Features\n\n### Code Judges\n\nWrite validators in any language (Python, TypeScript, Node, etc.):\n\n```bash\n# Input: stdin JSON with question, criteria, candidate_answer\n# Output: stdout JSON with score (0-1), hits, misses, reasoning\n```\n\nFor complete examples and patterns, see:\n- [custom-evaluators](https://agentv.dev/evaluators/custom-evaluators/)\n- [code-judge-sdk example](examples/features/code-judge-sdk)\n\n### LLM Judges\n\nCreate markdown judge files with evaluation criteria and scoring guidelines:\n\n```yaml\nexecution:\n  evaluators:\n    - name: semantic_check\n      type: llm_judge\n      prompt: ./judges/correctness.md\n```\n\nYour judge prompt file defines criteria and scoring guidelines.\n\n### Rubric-Based Evaluation\n\nDefine structured criteria directly in your test:\n\n```yaml\ntests:\n  - id: quicksort-explain\n    criteria: Explain how quicksort works\n\n    input: Explain quicksort algorithm\n\n    rubrics:\n      - Mentions divide-and-conquer approach\n      - Explains partition step\n      - States time complexity\n```\n\nScoring: `(satisfied weights) / (total weights)` → verdicts: `pass` (≥0.8), `borderline` (≥0.6), `fail`\n\nAuto-generate rubrics from expected outcomes:\n```bash\nagentv generate rubrics evals/my-eval.yaml\n```\n\nSee [rubric evaluator](https://agentv.dev/evaluation/rubrics/) for detailed patterns.\n\n## Advanced Configuration\n\n### Retry Behavior\n\nConfigure automatic retry with exponential backoff:\n\n```yaml\ntargets:\n  - name: azure_base\n    provider: azure\n    max_retries: 5\n    retry_initial_delay_ms: 2000\n    retry_max_delay_ms: 120000\n    retry_backoff_factor: 2\n    retry_status_codes: [500, 408, 429, 502, 503, 504]\n```\n\nAutomatically retries on rate limits, transient 5xx errors, and network failures with jitter.\n\n## Documentation & Learning\n\n**Getting Started:**\n- Run `agentv init` to set up your first evaluation workspace\n- Check [examples/README.md](examples/README.md) for demos (math, code generation, tool use)\n- AI agents: Ask Claude Code to `/agentv-eval-builder` to create and iterate on evals\n\n**Detailed Guides:**\n- [Evaluation format and structure](https://agentv.dev/evaluation/eval-files/)\n- [Custom evaluators](https://agentv.dev/evaluators/custom-evaluators/)\n- [Rubric evaluator](https://agentv.dev/evaluation/rubrics/)\n- [Composite evaluator](https://agentv.dev/evaluators/composite/)\n- [Tool trajectory evaluator](https://agentv.dev/evaluators/tool-trajectory/)\n- [Structured data evaluators](https://agentv.dev/evaluators/structured-data/)\n- [Batch CLI evaluation](https://agentv.dev/evaluation/batch-cli/)\n- [Compare results](https://agentv.dev/tools/compare/)\n- [Example evaluations](https://agentv.dev/evaluation/examples/)\n\n**Reference:**\n- Monorepo structure: `packages/core/` (engine), `packages/eval/` (evaluation logic), `apps/cli/` (commands)\n\n## Contributing\n\nSee [AGENTS.md](AGENTS.md) for development guidelines, design principles, and quality assurance workflow.\n\n## License\n\nMIT License - see [LICENSE](LICENSE) for details.\n"
      },
      "plugins": [
        {
          "name": "agentv",
          "description": "Automates AI agent evaluations and prompt optimization",
          "source": "./",
          "strict": false,
          "skills": [
            "./skills/agentv-eval-builder",
            "./skills/agentv-eval-orchestrator",
            "./skills/agentv-prompt-optimizer"
          ],
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add EntityProcess/agentv",
            "/plugin install agentv@agentv"
          ]
        }
      ]
    }
  ]
}