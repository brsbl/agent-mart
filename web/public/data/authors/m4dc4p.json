{
  "author": {
    "id": "m4dc4p",
    "display_name": "m4dc4p",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/18689?v=4",
    "url": "https://github.com/m4dc4p",
    "bio": null,
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 1,
      "total_commands": 2,
      "total_skills": 1,
      "total_stars": 0,
      "total_forks": 0
    }
  },
  "marketplaces": [
    {
      "name": "claude-hoogle",
      "version": "1.0.0",
      "description": "Hoogle integration for Claude Code - search Haskell APIs by name or type signature",
      "owner_info": {
        "name": "Justin Bailey",
        "email": "jgbailey@gmail.com"
      },
      "keywords": [],
      "repo_full_name": "m4dc4p/claude-hoogle",
      "repo_url": "https://github.com/m4dc4p/claude-hoogle",
      "repo_description": null,
      "homepage": null,
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2026-01-12T20:05:51Z",
        "created_at": "2026-01-11T07:34:22Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 520
        },
        {
          "path": ".claude-plugin/plugin.json",
          "type": "blob",
          "size": 193
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 1386
        },
        {
          "path": "commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "commands/remote.md",
          "type": "blob",
          "size": 1310
        },
        {
          "path": "commands/search.md",
          "type": "blob",
          "size": 1590
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/hoogle",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/hoogle/SKILL.md",
          "type": "blob",
          "size": 3502
        },
        {
          "path": "skills/hoogle/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/hoogle/references/hoogle-reference.md",
          "type": "blob",
          "size": 3636
        },
        {
          "path": "vendor",
          "type": "tree",
          "size": null
        },
        {
          "path": "vendor/dataframe",
          "type": "tree",
          "size": null
        },
        {
          "path": "vendor/dataframe/README.md",
          "type": "blob",
          "size": 3044
        },
        {
          "path": "vendor/dataframe/dataframe-persistent",
          "type": "tree",
          "size": null
        },
        {
          "path": "vendor/dataframe/dataframe-persistent/README.md",
          "type": "blob",
          "size": 5409
        },
        {
          "path": "vendor/dataframe/dataframe-symbolic-regression",
          "type": "tree",
          "size": null
        },
        {
          "path": "vendor/dataframe/dataframe-symbolic-regression/README.md",
          "type": "blob",
          "size": 4478
        },
        {
          "path": "vendor/dataframe/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "vendor/dataframe/examples/README.md",
          "type": "blob",
          "size": 3130
        },
        {
          "path": "vendor/dataframe/tests",
          "type": "tree",
          "size": null
        },
        {
          "path": "vendor/dataframe/tests/data",
          "type": "tree",
          "size": null
        },
        {
          "path": "vendor/dataframe/tests/data/README.md",
          "type": "blob",
          "size": 28985
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"$schema\": \"https://anthropic.com/claude-code/marketplace.schema.json\",\n  \"name\": \"claude-hoogle\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Hoogle integration for Claude Code - search Haskell APIs by name or type signature\",\n  \"owner\": {\n    \"name\": \"Justin Bailey\",\n    \"email\": \"jgbailey@gmail.com\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"hoogle\",\n      \"description\": \"Search Haskell APIs using Hoogle - find functions by name or type signature\",\n      \"source\": \"./\",\n      \"category\": \"development\"\n    }\n  ]\n}\n",
        ".claude-plugin/plugin.json": "{\n  \"name\": \"hoogle\",\n  \"description\": \"Search Haskell APIs using Hoogle - find functions by name or type signature\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Claude Code Plugin\"\n  }\n}\n",
        "README.md": "# Hoogle Plugin for Claude Code\n\nSearch Haskell APIs using [Hoogle](https://hoogle.haskell.org/) directly from Claude Code.\n\n## Commands\n\n### `/hoogle:search` - Local Search\n\nSearch your local Hoogle database:\n\n```\n/hoogle:search map\n/hoogle:search (a -> Bool) -> [a] -> [a]\n/hoogle:search +base foldl\n```\n\nRequires `hoogle` executable on PATH. Local database is generated automatically on first use.\n\n### `/hoogle:remote` - Online Search\n\nSearch the official Hoogle server at hoogle.haskell.org:\n\n```\n/hoogle:remote map\n/hoogle:remote (a -> Bool) -> [a] -> [a]\n```\n\nUse a custom Hoogle server:\n```\n/hoogle:remote map --url https://custom-hoogle.example.com\n```\n\n### Automatic Skill\n\nThe hoogle skill activates automatically when Claude is working with Haskell code and needs to look up functions, type signatures, or documentation.\n\n## Search Examples\n\n| Goal | Query |\n|------|-------|\n| Find by name | `map`, `filter`, `foldl` |\n| Find by type | `a -> b -> a`, `(a -> Bool) -> [a] -> [a]` |\n| Filter by package | `+base map`, `+containers lookup` |\n\n## Prerequisites\n\n- `hoogle` executable on PATH (`cabal install hoogle` or `stack install hoogle`)\n- `jq` for JSON parsing\n- For remote search: `curl`\n\n## Installation\n\n1. Add the marketplace:\n   ```\n   /plugin marketplace add m4dc4p/claude-hoogle\n   ```\n\n2. Install the plugin:\n   ```\n   /plugin install hoogle@claude-hoogle\n   ```\n",
        "commands/remote.md": "---\ndescription: Search online Hoogle at hoogle.haskell.org (or custom URL)\nallowed-tools: Bash(${CLAUDE_PLUGIN_ROOT}/scripts/hoogle-remote.sh:*)\n---\n\n# Remote Hoogle Search\n\nSearch the online Hoogle server for Haskell functions and types.\n\nSearch syntax:\n- **Function name**: `map`, `filter`, `foldl`\n- **Type signature**: `a -> b -> a`, `(a -> Bool) -> [a] -> [a]`\n- **Package-qualified**: `+base map`, `+containers lookup`\n\n## Context\n\nUser query: $ARGUMENTS\n\n## Instructions\n\n1. Parse the user's query. If they specify a custom URL with `--url`, use it. Otherwise use the default hoogle.haskell.org.\n\n2. Run the remote Hoogle search:\n   ```bash\n   ${CLAUDE_PLUGIN_ROOT}/scripts/hoogle-remote.sh \"$ARGUMENTS\" --count 10\n   ```\n\n3. Present results clearly:\n   - Show the function signature prominently\n   - Include the module and package\n   - Show relevant documentation\n   - Note the source URL if not the default\n\n4. For type signature searches, explain what the type means in plain language.\n\n## Output Format\n\nFor each relevant result, show:\n- **Function**: `functionName :: TypeSignature`\n- **From**: `Module.Name` (package-name)\n- **Description**: Brief doc summary\n\n## Custom Hoogle Servers\n\nUsers can specify a custom Hoogle server:\n```\n/hoogle:remote map --url https://custom-hoogle.example.com\n```\n",
        "commands/search.md": "---\ndescription: Search Haskell APIs with Hoogle - find functions by name or type signature\nallowed-tools: Bash(${CLAUDE_PLUGIN_ROOT}/scripts/hoogle-search.sh:*), Bash(${CLAUDE_PLUGIN_ROOT}/scripts/hoogle-init-db.sh:*)\n---\n\n# Hoogle Search\n\nSearch Haskell APIs using Hoogle. You can search by:\n- **Function name**: `map`, `filter`, `foldl`\n- **Type signature**: `a -> b -> a`, `(a -> Bool) -> [a] -> [a]`\n- **Package-qualified**: `+base map`, `+containers lookup`\n\n## Context\n\nUser query: $ARGUMENTS\n\n## Instructions\n\n1. First, check if this is a search query or if the user needs help:\n   - If the user asks for help, explain how Hoogle works\n   - Otherwise, proceed with the search\n\n2. Run the Hoogle search:\n   ```bash\n   ${CLAUDE_PLUGIN_ROOT}/scripts/hoogle-search.sh \"$ARGUMENTS\" --count 10\n   ```\n\n3. If the search fails with a database error:\n   - Inform the user that the database needs to be initialized\n   - Run `${CLAUDE_PLUGIN_ROOT}/scripts/hoogle-init-db.sh` to generate it\n   - This may take several minutes on first run\n\n4. Present results clearly:\n   - Show the function signature prominently\n   - Include the module and package\n   - Show relevant documentation\n   - If the results seem off, suggest alternative search terms\n\n5. For type signature searches, explain what the type means in plain language.\n\n## Output Format\n\nFor each relevant result, show:\n- **Function**: `functionName :: TypeSignature`\n- **From**: `Module.Name` (package-name)\n- **Description**: Brief doc summary\n\nIf the user asks for details on a specific result, use `--info` to get full documentation.\n",
        "skills/hoogle/SKILL.md": "---\nname: hoogle\ndescription: Search Haskell APIs using Hoogle. Use this skill when working with Haskell projects to look up function signatures, find functions by type, or discover library documentation. Activate proactively when you need to understand Haskell library APIs, find the right function for a task, or look up type signatures.\n---\n\n# Hoogle API Search\n\nHoogle is a Haskell API search engine. Use it to find functions by name or by type signature.\n\n## When to Use Hoogle\n\nUse Hoogle proactively when:\n- Working with Haskell code and need to find a function\n- Looking up the type signature of a function\n- Searching for functions that match a type signature (e.g., `a -> b -> a`)\n- Finding which module exports a particular function\n- Looking up documentation for Haskell functions\n\n## Search Methods\n\n### Search by Function Name\n\n```bash\n${CLAUDE_PLUGIN_ROOT}/scripts/hoogle-search.sh \"map\"\n```\n\nReturns functions named \"map\" or containing \"map\" in their name.\n\n### Search by Type Signature\n\n```bash\n${CLAUDE_PLUGIN_ROOT}/scripts/hoogle-search.sh \"a -> b -> a\"\n```\n\nReturns functions matching that type signature. Type variables are automatically generalized.\n\n### Search with Package Filter\n\nInclude package names in the query:\n```bash\n${CLAUDE_PLUGIN_ROOT}/scripts/hoogle-search.sh \"+base map\"\n${CLAUDE_PLUGIN_ROOT}/scripts/hoogle-search.sh \"+containers Data.Map.lookup\"\n```\n\n### Get Detailed Info\n\nUse `--info` for the first result's documentation:\n```bash\n${CLAUDE_PLUGIN_ROOT}/scripts/hoogle-search.sh \"foldl\" --info\n```\n\n## Understanding Results\n\nThe search returns JSON with this structure:\n```json\n{\n  \"results\": [\n    {\n      \"url\": \"https://hackage.haskell.org/package/base/docs/Prelude.html#v:map\",\n      \"module\": {\"name\": \"Prelude\", \"url\": \"...\"},\n      \"package\": {\"name\": \"base\", \"url\": \"...\"},\n      \"item\": \"map :: (a -> b) -> [a] -> [b]\",\n      \"docs\": \"map f xs is the list obtained by applying f to each element...\"\n    }\n  ],\n  \"query\": \"map\",\n  \"count\": 10\n}\n```\n\nKey fields:\n- `item`: The function signature\n- `docs`: Documentation/description\n- `module.name`: The module that exports this function\n- `package.name`: The package containing this function\n\n## Database Initialization\n\nBefore searching, ensure the Hoogle database exists:\n```bash\n${CLAUDE_PLUGIN_ROOT}/scripts/hoogle-init-db.sh\n```\n\nThis checks for a valid database and generates one from Stackage if needed. Generation takes several minutes on first run.\n\n### Local Database\n\nFor project-specific searches, generate a local database:\n```bash\n${CLAUDE_PLUGIN_ROOT}/scripts/hoogle-init-db.sh --local /path/to/haddock/docs\n```\n\n## Common Search Patterns\n\n| Goal | Query Example |\n|------|---------------|\n| Find function by name | `hoogle-search.sh \"filter\"` |\n| Find by exact type | `hoogle-search.sh \"(a -> Bool) -> [a] -> [a]\"` |\n| Find in specific package | `hoogle-search.sh \"+lens view\"` |\n| Find class methods | `hoogle-search.sh \"Monad m => m a -> m b\"` |\n| Find by partial type | `hoogle-search.sh \"Map k v -> k -> Maybe v\"` |\n\n## Tips\n\n1. **Type signatures are powerful**: Searching `a -> a` finds `id`, while `[a] -> a` finds `head`.\n2. **Use package filters**: Narrow results with `+packagename`.\n3. **Check multiple results**: The best match isn't always first.\n4. **Look at the module**: Tells you what to import.\n\n## Error Handling\n\nIf searches fail with database errors, run:\n```bash\n${CLAUDE_PLUGIN_ROOT}/scripts/hoogle-init-db.sh --force\n```\n\nThis regenerates the database from scratch.\n",
        "skills/hoogle/references/hoogle-reference.md": "# Hoogle Reference\n\nHoogle is a Haskell API search engine. This reference covers CLI usage and search syntax.\n\n## Installation\n\n```bash\ncabal install hoogle\n# or\nstack install hoogle\n```\n\n## Commands\n\n### Search\n\n```bash\nhoogle [search] [OPTIONS] [QUERY]\n```\n\n**Flags:**\n- `--json` - Output results as JSON\n- `--jsonl` - Output results as JSON Lines\n- `-l, --link` - Include URLs for each result\n- `--numbers` - Show counter for each result\n- `-i, --info` - Extended information about first result\n- `-d, --database=FILE` - Specify database file (.hoo extension)\n- `-n, --count=INT` - Maximum results (default: 10)\n\n**Examples:**\n```bash\nhoogle search map\nhoogle search \"a -> b -> a\" --count 5\nhoogle search \"+base filter\" --json\nhoogle search foldl --info\n```\n\n### Generate\n\n```bash\nhoogle generate [OPTIONS] [PACKAGE]\n```\n\n**Flags:**\n- `--download` - Download all files from the web\n- `--database=FILE` - Output database file\n- `--local[=ITEM]` - Index local packages\n- `--haddock=ITEM` - Use local haddock docs\n- `-n, --count=INT` - Maximum packages to index\n\n**Examples:**\n```bash\n# Generate from Stackage (all packages)\nhoogle generate\n\n# Generate for specific packages only\nhoogle generate base filepath\n\n# Generate from local ghc-pkg installed packages\nhoogle generate --local\n\n# Generate from a directory of .txt files\nhoogle generate --local=mydir\n```\n\n## Database\n\n- Default location: `~/.hoogle/default-haskell-VERSION.hoo`\n- Generated from `.txt` files (Hoogle input format)\n- Can be generated by Cabal: `cabal haddock --haddock-hoogle`\n\n## Search Syntax\n\n### By Name\n\n```bash\nhoogle map\nhoogle Data.Map.lookup\nhoogle \"Data.List\"\n```\n\n### By Type Signature\n\n```bash\nhoogle \"a -> b\"\nhoogle \"(a -> b) -> [a] -> [b]\"\nhoogle \"Monad m => m a -> (a -> m b) -> m b\"\n```\n\n### Package Filters\n\n```bash\nhoogle \"+base map\"           # Search in base package\nhoogle \"+containers lookup\"  # Search in containers\nhoogle \"+lens -base view\"    # Include lens, exclude base\n```\n\n### Module Filters\n\n```bash\nhoogle \"is:module Data.List\"\nhoogle \"is:package base\"\n```\n\n## JSON Output Format\n\n```json\n[\n  {\n    \"url\": \"https://hackage.haskell.org/package/base/docs/Prelude.html#v:map\",\n    \"module\": {\n      \"url\": \"https://hackage.haskell.org/package/base/docs/Prelude.html\",\n      \"name\": \"Prelude\"\n    },\n    \"package\": {\n      \"url\": \"https://hackage.haskell.org/package/base\",\n      \"name\": \"base\"\n    },\n    \"item\": \"map :: (a -> b) -> [a] -> [b]\",\n    \"type\": \"\",\n    \"docs\": \"map f xs is the list obtained by applying f to each element...\"\n  }\n]\n```\n\n## Type Search Algorithm\n\nHoogle matches types by applying rewrites:\n- **Arg reorder**: `x -> y -> z` matches `y -> x -> z`\n- **Arg delete**: Can drop one argument from either side\n- **Variable rename**: Type variables are unified\n- **Alias follow**: Follows type synonyms (`FilePath -> String`)\n- **Instance subtype**: Uses typeclass instances for matching\n\n## Tips\n\n1. **Type variables are flexible**: `a -> a` finds `id`, `Int -> Int` finds specific functions\n2. **Use parentheses for complex types**: `(a -> b) -> [a] -> [b]`\n3. **Package filters narrow results**: `+base` restricts to base package\n4. **Quotes help with operators**: `hoogle \">>=\"` or `hoogle \"bind\"`\n5. **Context is optional**: `m a -> m b` matches `Monad m => m a -> m b`\n\n## Common Searches\n\n| Goal | Query |\n|------|-------|\n| Find function by name | `map` |\n| Find by exact type | `(a -> Bool) -> [a] -> [a]` |\n| Find in package | `+lens view` |\n| Find monadic bind | `Monad m => m a -> (a -> m b) -> m b` |\n| Find by return type | `-> Maybe a` |\n| Find class methods | `Functor f => (a -> b) -> f a -> f b` |\n",
        "vendor/dataframe/README.md": "<h1 align=\"center\">\n  <a href=\"https://dataframe.readthedocs.io/en/latest/\">\n    <img width=\"100\" height=\"100\" src=\"https://raw.githubusercontent.com/mchav/dataframe/master/docs/_static/haskell-logo.svg\" alt=\"dataframe logo\">\n  </a>\n</h1>\n\n<div align=\"center\">\n  <a href=\"https://hackage.haskell.org/package/dataframe\">\n    <img src=\"https://img.shields.io/hackage/v/dataframe\" alt=\"hackage Latest Release\"/>\n  </a>\n  <a href=\"https://github.com/mchav/dataframe/actions/workflows/haskell-ci.yml\">\n    <img src=\"https://github.com/mchav/dataframe/actions/workflows/haskell-ci.yml/badge.svg\" alt=\"C/I\"/>\n  </a>\n</div>\n\n<p align=\"center\">\n  <a href=\"https://dataframe.readthedocs.io/en/latest/\">User guide</a>\n  |\n  <a href=\"https://discord.gg/8u8SCWfrNC\">Discord</a>\n</p>\n\n# DataFrame\n\nA fast, safe, and intuitive DataFrame library.\n\n## Why use this DataFrame library?\n\n* Encourages concise, declarative, and composable data pipelines.\n* Static typing makes code easier to reason about and catches many bugs at compile timeâ€”before your code ever runs.\n* Delivers high performance thanks to Haskellâ€™s optimizing compiler and efficient memory model.\n* Designed for interactivity: expressive syntax, helpful error messages, and sensible defaults.\n* Works seamlessly in both command-line and notebook environmentsâ€”great for exploration and scripting alike.\n\n## Features\n- Type-safe column operations with compile-time guarantees\n- Familiar, approachable API designed to feel easy coming from other languages.\n- Interactive REPL for data exploration and plotting.\n\n## Quick start\nBrowse through some examples in [binder](https://mybinder.org/v2/gh/mchav/ihaskell-dataframe/HEAD) or in our [playground](https://ulwazi-exh9dbh2exbzgbc9.westus-01.azurewebsites.net/lab).\n\n## Install\n\n### Cabal\nTo use the CLI tool:\n```bash\n$ cabal update\n$ cabal install dataframe\n$ dataframe\n```\n\nAs a prodject dependency add `dataframe` to your <project>.cabal file.\n\n### Stack (in stack.yaml add to extra-deps if needed)\nAdd to your package.yaml dependencies:\n```yaml\ndependencies:\n  - dataframe\n```\n\nOr manually to stack.yaml extra-deps if needed.\n\n## Example\n\n```haskell\ndataframe> df = D.fromNamedColumns [(\"product_id\", D.fromList [1,1,2,2,3,3]), (\"sales\", D.fromList [100,120,50,20,40,30])]\ndataframe> df\n------------------\nproduct_id | sales\n-----------|------\n   Int     |  Int \n-----------|------\n1          | 100  \n1          | 120  \n2          | 50   \n2          | 20   \n3          | 40   \n3          | 30   \n\ndataframe> :exposeColumns df\n\"product_id :: Expr Int\"\n\"sales :: Expr Int\"\ndataframe> df |> D.groupBy [F.name product_id] |> D.aggregate [F.sum sales `as` \"total_sales\"]\n------------------------\nproduct_id | total_sales\n-----------|------------\n   Int     |     Int    \n-----------|------------\n1          | 220        \n2          | 70         \n3          | 70         \n```\n\n## Documentation\n* ðŸ“š User guide: https://dataframe.readthedocs.io/en/latest/\n* ðŸ“– API reference: https://hackage.haskell.org/package/dataframe/docs/DataFrame.html\n",
        "vendor/dataframe/dataframe-persistent/README.md": "# dataframe-persistent\n\nPersistent database integration for the Haskell DataFrame library.\n\n## Overview\n\nThis package provides seamless integration between the `dataframe` library and the `persistent` database library, allowing you to:\n\n- Load database entities directly into DataFrames\n- Perform DataFrame operations on database data  \n- Save DataFrame results back to the database\n- Work with type-safe database entities\n\n## Installation\n\nAdd to your `package.yaml`:\n\n```yaml\ndependencies:\n- dataframe ^>= 0.4\n- dataframe-persistent ^>= 0.1\n- persistent >= 2.14\n- persistent-sqlite >= 2.13  # or your preferred backend\n```\n\nOr to your `.cabal` file:\n\n```cabal\nbuild-depends:\n  dataframe ^>= 0.4,\n  dataframe-persistent ^>= 0.1,\n  persistent >= 2.14,\n  persistent-sqlite >= 2.13\n```\n\n## Quick Start\n\n```haskell\n{-# LANGUAGE DataKinds #-}\n{-# LANGUAGE DerivingStrategies #-}\n{-# LANGUAGE FlexibleInstances #-}\n{-# LANGUAGE GADTs #-}\n{-# LANGUAGE GeneralizedNewtypeDeriving #-}\n{-# LANGUAGE MultiParamTypeClasses #-}\n{-# LANGUAGE OverloadedStrings #-}\n{-# LANGUAGE QuasiQuotes #-}\n{-# LANGUAGE StandaloneDeriving #-}\n{-# LANGUAGE TemplateHaskell #-}\n{-# LANGUAGE TypeApplications #-}\n{-# LANGUAGE TypeFamilies #-}\n{-# LANGUAGE UndecidableInstances #-}\n\nimport Control.Monad.IO.Class (liftIO)\nimport Database.Persist\nimport Database.Persist.Sqlite\nimport Database.Persist.TH\nimport qualified DataFrame as DF\nimport qualified DataFrame.Functions as F\nimport DataFrame.IO.Persistent\nimport DataFrame.IO.Persistent.TH\nimport qualified Data.Vector as V\n\nimport DataFrame.Functions ((.<))\n\n-- Define your entities\nshare [mkPersist sqlSettings, mkMigrate \"migrateAll\"] [persistLowerCase|\nTestUser\n    name Text\n    age Int\n    active Bool\n    deriving Show Eq\n|]\n\n-- Derive DataFrame instances\n$(derivePersistentDataFrame ''TestUser)\n\n-- Example usage\nmain :: IO ()\nmain = runSqlite \"example.db\" $ do\n    -- Run migrations\n    runMigration migrateAll\n    \n    -- Insert some test data\n    _ <- insert $ TestUser \"Alice\" 25 True\n    _ <- insert $ TestUser \"Bob\" 30 False\n    _ <- insert $ TestUser \"Charlie\" 35 True\n    \n    -- Load from database\n    allUsersDF <- fromPersistent @TestUser []\n    liftIO $ putStrLn $ \"Loaded \" ++ show (nRows allUsersDF) ++ \" users\"\n    \n    -- Load with filters\n    activeUsersDF <- fromPersistent @TestUser [TestUserActive ==. True]\n    liftIO $ putStrLn $ \"Active users: \" ++ show (nRows activeUsersDF)\n    \n    -- Process with DataFrame operations\n    -- Expressions are automaticaly generated.\n    let youngUsers = DF.filterWhere (test_user_age .< 30) allUsersDF\n        ages = DF.columnAsList test_user_age youngUsers\n    liftIO $ putStrLn $ \"Young user ages: \" ++ show ages\n    \n    -- Custom configuration\n    let config = defaultPersistentConfig \n                    { pcIdColumnName = \"user_id\"\n                    , pcIncludeId = True\n                    }\n    customDF <- fromPersistentWith @TestUser config []\n    liftIO $ putStrLn $ \"Columns with custom config: \" ++ show (DF.columnNames customDF)\n```\n\n## Features\n\n- **Type-safe conversions** between Persistent entities and DataFrames\n- **Template Haskell support** for automatic instance generation\n- **Configurable loading** with batch size and column selection\n- **Column name cleaning** - removes table prefixes automatically (e.g., `test_user_name` â†’ `name`)\n- **Automatically generate typed expressions** - creates expressions in snake case prefixed by table name (e.g `test_user_name`).\n- **Type preservation** - maintains proper types for Text, Int, Bool, Day, etc.\n- **Empty DataFrame support** - preserves column structure even with no data\n- **Support for all Persistent backends** (SQLite, PostgreSQL, MySQL, etc.)\n\n## Configuration Options\n\n```haskell\ndata PersistentConfig = PersistentConfig\n    { pcBatchSize :: Int        -- Number of records to fetch at once (default: 10000)\n    , pcIncludeId :: Bool       -- Whether to include entity ID as column (default: True)\n    , pcIdColumnName :: Text    -- Name for the ID column (default: \"id\")\n    }\n```\n\n## Advanced Usage\n\n### Custom Field Extraction\n\nYou can also extract fields from individual entities:\n\n```haskell\nlet user = TestUser \"Alice\" 25 True\n    columns = persistFieldsToColumns user\n-- Result: [(\"name\", SomeColumn [\"Alice\"]), (\"age\", SomeColumn [25]), (\"active\", SomeColumn [True])]\n```\n\n### Working with Vector Data\n\n```haskell\n-- Extract specific column data\nlet names = DF.columnAsList test_user_name df\n    ages = DF.columnAsList test_user_age df\n    activeFlags = DF.columnAsList test_user_active df\n```\n\n## Examples\n\nFor comprehensive examples and test cases, see:\n- [`tests/PersistentTests.hs`](tests/PersistentTests.hs) - Full test suite with examples\n- [`../docs/persistent_integration.md`](../docs/persistent_integration.md) - Detailed integration guide\n\n## Status\n\nThis package is **actively maintained** and tested. Current test coverage includes:\n- âœ… Entity loading with and without filters\n- âœ… Custom configuration options\n- âœ… DataFrame operations on Persistent data\n- âœ… Empty result set handling\n- âœ… Field extraction utilities\n- âœ… Multi-table relationships\n\n## Documentation\n\nFor detailed documentation, see:\n- [Main dataframe documentation](https://github.com/mchav/dataframe)\n- [Persistent integration guide](../docs/persistent_integration.md)\n\n## License\n\nGPL-3.0-or-later (same as the main dataframe package)",
        "vendor/dataframe/dataframe-symbolic-regression/README.md": "# DataFrame.SymbolicRegression\n\nA Haskell library (based on [eggp](https://github.com/folivetti/eggp) which is in turn based on [srtree](https://github.com/folivetti/srtree)) for symbolic regression on DataFrames. Automatically discover mathematical expressions that best fit your data using genetic programming with e-graph optimization.\n\n## Overview\n\nDataFrame.SymbolicRegression integrates symbolic regression capabilities into a DataFrame workflow. Given a target column and a dataset, it evolves mathematical expressions that predict the target variable, returning a Pareto front of expressions trading off complexity and accuracy.\n\n## Quick Start\n\n```haskell\nimport qualified DataFrame as D\nimport DataFrame.Functions ((.=))\nimport DataFrame.SymbolicRegression\n\n-- Load your data\ndf <- D.readParquet \"data/mtcars.parquet\"\n\n-- Run symbolic regression to predict 'mpg'\n-- NOTE: ALL COLUMNS MUST BE CONVERTED TO DOUBLE FIRST\n-- e.g df' = D.derive \"some_column\" (F.toDouble (F.col @Int \"some_column\")) df\nexprs <- fitSymbolicRegression defaultRegressionConfig mpg df\n\n-- View discovered expressions (Pareto front from simplest to most complex)\nexprs\n-- [(col @Double \"qsec\"),\n--  (divide (lit 57.33) (col @Double \"wt\")),\n--  (add (lit 10.75) (divide (lit 1557.67) (col @Double \"disp\")))]\n\n-- Create named expressions that we'll use in a dataframe.\nlevels = zipWith (.=) [\"level_1\", \"level_2\", \"level_3\"] exprs\n\n-- Show the various predictions in our dataframe.\nD.deriveMany levels df\n\n-- Or pick the best one\nD.derive \"prediction\" (last exprs) df\n```\n\n## Configuration\n\nCustomize the search with `RegressionConfig`:\n\n```haskell\ndata RegressionConfig = RegressionConfig\n    { generations              :: Int      -- Number of evolutionary generations (default: 100)\n    , maxExpressionSize        :: Int      -- Maximum tree depth/complexity (default: 5)\n    , numFolds                 :: Int      -- Cross-validation folds (default: 3)\n    , showTrace                :: Bool     -- Print progress during evolution (default: True)\n    , lossFunction             :: Distribution  -- MSE, Gaussian, Poisson, etc. (default: MSE)\n    , numOptimisationIterations :: Int     -- Parameter optimization iterations (default: 30)\n    , numParameterRetries      :: Int      -- Retries for parameter fitting (default: 2)\n    , populationSize           :: Int      -- Population size (default: 100)\n    , tournamentSize           :: Int      -- Tournament selection size (default: 3)\n    , crossoverProbability     :: Double   -- Crossover rate (default: 0.95)\n    , mutationProbability      :: Double   -- Mutation rate (default: 0.3)\n    , unaryFunctions           :: [...]    -- Unary operations to include\n    , binaryFunctions          :: [...]    -- Binary operations to include\n    , numParams                :: Int      -- Number of parameters (-1 for auto)\n    , generational             :: Bool     -- Use generational replacement (default: False)\n    , simplifyExpressions      :: Bool     -- Simplify output expressions (default: True)\n    , maxTime                  :: Int      -- Time limit in seconds (-1 for none)\n    , dumpTo                   :: String   -- Save e-graph state to file\n    , loadFrom                 :: String   -- Load e-graph state from file\n    }\n```\n\n### Example: Custom Configuration\n\n```haskell\nmyConfig :: RegressionConfig\nmyConfig = defaultRegressionConfig\n    { generations = 200\n    , maxExpressionSize = 7\n    , populationSize = 200\n    }\n\nexprs <- fitSymbolicRegression myConfig targetColumn df\n```\n\n## Output\n\n`fitSymbolicRegression` returns a list of expressions representing the Pareto front, ordered by complexity (simplest first). Each expression:\n\n- Is a valid `Expr Double` that can be used with DataFrame operations\n- Represents a different trade-off between simplicity and accuracy\n- Has optimized numerical constants\n\n## How It Works\n\n1. **Genetic Programming**: Evolves a population of expression trees through selection, crossover, and mutation\n2. **E-graph Optimization**: Uses equality saturation to discover equivalent expressions and simplify\n3. **Parameter Optimization**: Fits numerical constants using nonlinear optimization\n4. **Pareto Selection**: Returns expressions across the complexity-accuracy frontier\n\n## Dependencies\n\n### System dependencies\nTo install DataFrame.SymbolicRegression you'll need:\n* libz: `sudo apt install libz-dev`\n* libnlopt: `sudo apt install libnlopt-dev`\n* libgmp: `sudo apt install libgmp-dev`\n",
        "vendor/dataframe/examples/README.md": "# Running the examples\n\n## California housing\n\nPreparation:\nThis has a hasktorch integration that requires dynamic linking to be enabled.\n\n`./setup_torch` does just that. It creates a cabal.project.local file that has dunamic linking enabled.\n\nThis example also requires GHC <= 9.8.\n\nRunning:\n`cabal run california_housing`.\n\nExpected output:\n\n```\nTraining linear regression model...\nIteration: 10000 | Loss: Tensor Float []  5.0225e9   \nIteration: 20000 | Loss: Tensor Float []  4.9093e9   \nIteration: 30000 | Loss: Tensor Float []  4.8576e9   \nIteration: 40000 | Loss: Tensor Float []  4.8333e9   \nIteration: 50000 | Loss: Tensor Float []  4.8217e9   \nIteration: 60000 | Loss: Tensor Float []  4.8160e9   \nIteration: 70000 | Loss: Tensor Float []  4.8130e9   \nIteration: 80000 | Loss: Tensor Float []  4.8114e9   \nIteration: 90000 | Loss: Tensor Float []  4.8105e9   \nIteration: 100000 | Loss: Tensor Float []  4.8099e9   \n-------------------------------------------\n median_house_value | predicted_house_value\n--------------------|----------------------\n       Double       |         Float        \n--------------------|----------------------\n 452600.0           | 414079.94            \n 358500.0           | 423011.94            \n 352100.0           | 383239.06            \n 341300.0           | 324928.94            \n 342200.0           | 256934.23            \n 269700.0           | 264944.84            \n 299200.0           | 259094.13            \n 241400.0           | 257224.55            \n 226700.0           | 201753.69            \n 261100.0           | 268698.7\n```\n\n## Chipotle\n\nRunning:\n`cabal run chipotle`.\n\n## One billion row challenge\n\nRunning:\n`cabal run one_billion_row_challenge`.\n\n## Iris classification using Torch\n\nPreparation:\nThis has a hasktorch integration that requires dynamic linking to be enabled.\n\n`./setup_torch` does just that. It creates a cabal.project.local file that has dunamic linking enabled.\n\nThis example also requires GHC <= 9.8.\n\nRunning:\n`cabal run iris`.\n\nExpected output:\n\n```\n.....................................\n.....................................\nTraining Set Summary is as follows:\n====== Confusion Matrix ========\n          0     1     2\n    0 36.00  0.00  0.00\n    1  0.00 31.00  0.00\n    2  0.00  1.00 36.00\n\n=========== Classwise Metrics =============\n---------------------------------\n  variety   | precision | recall\n------------|-----------|--------\n    Iris    |   Float   |  Float\n------------|-----------|--------\n Setosa     | 1.0       | 1.0\n Versicolor | 1.0       | 0.96875\n Virginica  | 0.972973  | 1.0\n\n.....................................\n.....................................\nTest Set Summary is as follows:\n====== Confusion Matrix ========\n          0     1     2\n    0 14.00  0.00  0.00\n    1  0.00 16.00  0.00\n    2  0.00  2.00 14.00\n\n=========== Classwise Metrics =============\n-----------------------------------\n  variety   | precision |  recall\n------------|-----------|----------\n    Iris    |   Float   |   Float\n------------|-----------|----------\n Setosa     | 1.0       | 1.0\n Versicolor | 1.0       | 0.8888889\n Virginica  | 0.875     | 1.0\n```",
        "vendor/dataframe/tests/data/README.md": "<!--\n  ~ Licensed to the Apache Software Foundation (ASF) under one\n  ~ or more contributor license agreements.  See the NOTICE file\n  ~ distributed with this work for additional information\n  ~ regarding copyright ownership.  The ASF licenses this file\n  ~ to you under the Apache License, Version 2.0 (the\n  ~ \"License\"); you may not use this file except in compliance\n  ~ with the License.  You may obtain a copy of the License at\n  ~\n  ~   http://www.apache.org/licenses/LICENSE-2.0\n  ~\n  ~ Unless required by applicable law or agreed to in writing,\n  ~ software distributed under the License is distributed on an\n  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  ~ KIND, either express or implied.  See the License for the\n  ~ specific language governing permissions and limitations\n  ~ under the License.\n  -->\n\n# Test data files for Parquet compatibility and regression testing\n\n| File                                         | Description                                                                                                                                                      |\n|----------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| delta_byte_array.parquet                     | string columns with DELTA_BYTE_ARRAY encoding. See [delta_byte_array.md](delta_byte_array.md) for details.                                                       |\n| delta_length_byte_array.parquet              | string columns with DELTA_LENGTH_BYTE_ARRAY encoding.                                                                                                            |\n| delta_binary_packed.parquet                  | INT32 and INT64 columns with DELTA_BINARY_PACKED encoding. See [delta_binary_packed.md](delta_binary_packed.md) for details.                                     |\n| delta_encoding_required_column.parquet       | required INT32 and STRING columns with delta encoding. See [delta_encoding_required_column.md](delta_encoding_required_column.md) for details.                   |\n| delta_encoding_optional_column.parquet       | optional INT64 and STRING columns with delta encoding. See [delta_encoding_optional_column.md](delta_encoding_optional_column.md) for details.                   |\n| nested_structs.rust.parquet                  | Used to test that the Rust Arrow reader can lookup the correct field from a nested struct. See [ARROW-11452](https://issues.apache.org/jira/browse/ARROW-11452)  |\n| data_index_bloom_encoding_stats.parquet | optional STRING column. Contains optional metadata: bloom filters, column index, offset index and encoding stats.                                                |\n| data_index_bloom_encoding_with_length.parquet | Same as `data_index_bloom_encoding_stats.parquet` but has `bloom_filter_length` populated in the ColumnMetaData |\n| null_list.parquet                       | an empty list. Generated from this json `{\"emptylist\":[]}` and for the purposes of testing correct read/write behaviour of this base case.                       |\n| alltypes_tiny_pages.parquet             | small page sizes with dictionary encoding with page index from [impala](https://github.com/apache/impala/tree/master/testdata/data/alltypes_tiny_pages.parquet). |\n| alltypes_tiny_pages_plain.parquet       | small page sizes with plain encoding with page index [impala](https://github.com/apache/impala/tree/master/testdata/data/alltypes_tiny_pages.parquet).           |\n| rle_boolean_encoding.parquet            | option boolean columns with RLE encoding                                                                                                                         |\n| fixed_length_byte_array.parquet                | optional FIXED_LENGTH_BYTE_ARRAY column with page index. See [fixed_length_byte_array.md](fixed_length_byte_array.md) for details.                        |\n| int32_with_null_pages.parquet                  | optional INT32 column with random null pages. See [int32_with_null_pages.md](int32_with_null_pages.md) for details.                        |\n| datapage_v1-uncompressed-checksum.parquet      | uncompressed INT32 columns in v1 data pages with a matching CRC        |\n| datapage_v1-snappy-compressed-checksum.parquet | compressed INT32 columns in v1 data pages with a matching CRC          |\n| datapage_v1-corrupt-checksum.parquet           | uncompressed INT32 columns in v1 data pages with a mismatching CRC     |\n| overflow_i16_page_cnt.parquet                  | row group with more than INT16_MAX pages                   |\n| bloom_filter.bin                               | deprecated bloom filter binary with binary header and murmur3 hashing |\n| bloom_filter.xxhash.bin                        | bloom filter binary with thrift header and xxhash hashing    |\n| nan_in_stats.parquet                           | statistics contains NaN in max, from PyArrow 0.8.0. See note below on \"NaN in stats\".  |\n| rle-dict-snappy-checksum.parquet                 | compressed and dictionary-encoded INT32 and STRING columns in format v2 with a matching CRC |\n| plain-dict-uncompressed-checksum.parquet         | uncompressed and dictionary-encoded INT32 and STRING columns in format v1 with a matching CRC |\n| rle-dict-uncompressed-corrupt-checksum.parquet   | uncompressed and dictionary-encoded INT32 and STRING columns in format v2 with a mismatching CRC |\n| large_string_map.brotli.parquet       | MAP(STRING, INT32) with a string column chunk of more than 2GB. See [note](#large-string-map) below |\n| float16_nonzeros_and_nans.parquet | Float16 (logical type) column with NaNs and nonzero finite min/max values |\n| float16_zeros_and_nans.parquet    | Float16 (logical type) column with NaNs and zeros as min/max values. . See [note](#float16-files) below |\n| concatenated_gzip_members.parquet     | 513 UINT64 numbers compressed using 2 concatenated gzip members in a single data page |\n| byte_stream_split.zstd.parquet | Standard normals with `BYTE_STREAM_SPLIT` encoding. See [note](#byte-stream-split) below |\n| incorrect_map_schema.parquet | Contains a Map schema without explicitly required keys, produced by Presto. See [note](#incorrect-map-schema) |\n| column_chunk_key_value_metadata.parquet | two INT32 columns, one with column chunk key-value metadata {\"foo\": \"bar\", \"thisiskeywithoutvalue\": null} note that the second key \"thisiskeywithoutvalue\", does not have a value, but the value can be mapped to an empty string \"\" when read depending on the client |\n| sort_columns.parquet | INT64 and BYTE_ARRAY columns with first column with nulls first and descending, second column with nulls last and ascending. This file contains two row groups with same data and sorting columns. |\n| old_list_structure.parquet | Single LIST<LIST<INT32>> column with legacy two-level list structure. See [old_list_structure.md](old_list_structure.md) |\n| repeated_primitive_no_list.parquet | REPEATED INT32 and BYTE_ARRAY fields without LIST annotation. See [note](#REPEATED-primitive-fields-with-no-LIST-annotation) |\n| map_no_value.parquet | MAP with null values, MAP with INT32 keys and no values, and LIST<INT32> column with same values as the MAP keys. See [map_no_value.md](map_no_value.md) |\n| page_v2_empty_compressed.parquet | An INT32 column with DataPageV2, all values are null, the zero-sized data is compressed using ZSTD. This is a valid non-zero bytes ZSTD stream that uncompresses into 0 bytes. |\n| datapage_v2_empty_datapage.snappy.parquet | A compressed FLOAT column with DataPageV2, a single row, value is null, the file uses Snappy compression, but there is no data for uncompression (see [related issue](https://github.com/apache/arrow-rs/issues/7388)). The zero bytes must not be attempted to be uncompressed, as this is an invalid Snappy stream. |\n| unknown-logical-type.parquet | A file containing a column annotated with a LogicalType whose identifier has been set to an abitrary high value to check the behaviour of an old reader reading a file written by a new writer containing an unsupported type (see [related issue](https://github.com/apache/arrow/issues/41764)). |\n| int96_from_spark.parquet | Single column of (deprecated) int96 values that originated as Apache Spark microsecond-resolution timestamps. Some values are outside the range typically representable by 64-bit nanosecond-resolution timestamps. See [int96_from_spark.md](int96_from_spark.md) for details. |\n| binary_truncated_min_max.parquet | A file containing six columns with exact, fully-truncated and partially-truncated max and min statistics and with the expected is_{min/max}_value_exact.  (see [note](Binary-truncated-min-and-max-statistics)).|\n\nTODO: Document what each file is in the table above.\n\n## Encrypted Files\n\nTests files with .parquet.encrypted suffix are encrypted using Parquet Modular Encryption.\n\nA detailed description of the Parquet Modular Encryption specification can be found here:\n```\n https://github.com/apache/parquet-format/blob/encryption/Encryption.md\n```\n\nFollowing are the keys and key ids (when using key\\_retriever) used to encrypt\nthe encrypted columns and footer in all the encrypted files:\n* Encrypted/Signed Footer:\n  * key:   {0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5}\n  * key_id: \"kf\"\n* Encrypted column named double_field (including column and offset index):\n  * key:  {1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,0}\n  * key_id: \"kc1\"\n* Encrypted column named float_field (including column and offset index):\n  * key: {1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,1}\n  * key_id: \"kc2\"\n\nThe following files are encrypted with AAD prefix \"tester\":\n1. encrypt\\_columns\\_and\\_footer\\_disable\\_aad\\_storage.parquet.encrypted\n2. encrypt\\_columns\\_and\\_footer\\_aad.parquet.encrypted\n\n\nA sample that reads and checks these files can be found at the following tests\nin Parquet C++:\n```\ncpp/src/parquet/encryption/read-configurations-test.cc\ncpp/src/parquet/encryption/test-encryption-util.h\n```\n\nThe `external_key_material_java.parquet.encrypted` file was encrypted using parquet-mr with\nexternal key material enabled, so the key material is found in the\n`_KEY_MATERIAL_FOR_external_key_material_java.parquet.encrypted.json` file.\nThis data was written using the `org.apache.parquet.crypto.keytools.mocks.InMemoryKMS` KMS client,\nwhich is compatible with the `TestOnlyInServerWrapKms` KMS client used in C++ tests.\n\n## Checksum Files\n\nThe schema for the `datapage_v1-*-checksum.parquet` test files is:\n```\nmessage m {\n    required int32 a;\n    required int32 b;\n}\n```\n\nThe detailed structure for these files is as follows:\n\n* `data/datapage_v1-uncompressed-checksum.parquet`:\n  ```\n  [ Column \"a\" [ Page 0 [correct crc] | Uncompressed Contents ][ Page 1 [correct crc] | Uncompressed Contents ]]\n  [ Column \"b\" [ Page 0 [correct crc] | Uncompressed Contents ][ Page 1 [correct crc] | Uncompressed Contents ]]\n  ```\n\n* `data/datapage_v1-snappy-compressed-checksum.parquet`:\n  ```\n  [ Column \"a\" [ Page 0 [correct crc] | Snappy Contents ][ Page 1 [correct crc] | Snappy Contents ]]\n  [ Column \"b\" [ Page 0 [correct crc] | Snappy Contents ][ Page 1 [correct crc] | Snappy Contents ]]\n  ```\n\n* `data/datapage_v1-corrupt-checksum.parquet`:\n  ```\n  [ Column \"a\" [ Page 0 [bad crc] | Uncompressed Contents ][ Page 1 [correct crc] | Uncompressed Contents ]]\n  [ Column \"b\" [ Page 0 [correct crc] | Uncompressed Contents ][ Page 1 [bad crc] | Uncompressed Contents ]]\n  ```\n\nThe schema for the `*-dict-*-checksum.parquet` test files is:\n* `data/rle-dict-snappy-checksum.parquet`:\n  ```\n  [ Column \"long_field\" [ Dict Page [correct crc] | Compressed PLAIN Contents ][ Page 0 [correct crc] | Compressed RLE_DICTIONARY Contents ]]\n  [ Column \"binary_field\" [ Dict Page [correct crc] | Compressed PLAIN Contents ][ Page 0 [correct crc] | Compressed RLE_DICTIONARY Contents ]]\n  ```\n\n* `data/plain-dict-uncompressed-checksum.parquet`:\n  ```\n  [ Column \"long_field\" [ Dict Page [correct crc] | Uncompressed PLAIN_DICTIONARY(DICT) Contents ][ Page 0 [correct crc] | Uncompressed PLAIN_DICTIONARY Contents ]]\n  [ Column \"binary_field\" [ Dict Page [correct crc] | Uncompressed PLAIN_DICTIONARY(DICT) Contents ][ Page 0 [correct crc] | Uncompressed PLAIN_DICTIONARY Contents ]]\n  ```\n\n* `data/rle-dict-uncompressed-corrupt-checksum.parquet`:\n  ```\n  [ Column \"long_field\" [ Dict Page [bad crc] | Uncompressed PLAIN Contents ][ Page 0 [correct crc] | Uncompressed RLE_DICTIONARY Contents ]]\n  [ Column \"binary_field\" [ Dict Page [bad crc] | Uncompressed PLAIN Contents ][ Page 0 [correct crc] | Uncompressed RLE_DICTIONARY Contents ]]\n  ```\n\n## Bloom Filter Files\n\nBloom filter examples have been generated by parquet-mr.\nThey are not Parquet files but only contain the bloom filter header and payload.\n\nFor each of `bloom_filter.bin` and `bloom_filter.xxhash.bin`, the bloom filter\nwas generated by inserting the strings \"hello\", \"parquet\", \"bloom\", \"filter\".\n\n`bloom_filter.bin` uses the original Murmur3-based bloom filter format as of\nhttps://github.com/apache/parquet-format/commit/54839ad5e04314c944fed8aa4bc6cf15e4a58698.\n\n`bloom_filter.xxhash.bin` uses the newer xxHash-based bloom filter format as of\nhttps://github.com/apache/parquet-format/commit/3fb10e00c2204bf1c6cc91e094c59e84cefcee33.\n\n## NaN in stats\n\nPrior to version 1.4.0, the C++ Parquet writer would write NaN values in min and\nmax statistics. (Correction in [this issue](https://issues.apache.org/jira/browse/PARQUET-1225)).\nIt has been updated since to ignore NaN values when calculating\nstatistics, but for backwards compatibility the following rules were established\n(in [PARQUET-1222](https://github.com/apache/parquet-format/pull/185)):\n\n> For backwards compatibility when reading files:\n> * If the min is a NaN, it should be ignored.\n> * If the max is a NaN, it should be ignored.\n> * If the min is +0, the row group may contain -0 values as well.\n> * If the max is -0, the row group may contain +0 values as well.\n> * When looking for NaN values, min and max should be ignored.\n\nThe file `nan_in_stats.parquet` was generated with:\n\n```python\nimport pyarrow as pa # version 0.8.0\nimport pyarrow.parquet as pq\nfrom numpy import NaN\n\ntab = pa.Table.from_arrays(\n    [pa.array([1.0, NaN])],\n    names=\"x\"\n)\n\npq.write_table(tab, \"nan_in_stats.parquet\")\n\nmetadata = pq.read_metadata(\"nan_in_stats.parquet\")\nmetadata.row_group(0).column(0)\n# <pyarrow._parquet.ColumnChunkMetaData object at 0x7f28539e58f0>\n#   file_offset: 88\n#   file_path:\n#   type: DOUBLE\n#   num_values: 2\n#   path_in_schema: x\n#   is_stats_set: True\n#   statistics:\n#     <pyarrow._parquet.RowGroupStatistics object at 0x7f28539e5738>\n#       has_min_max: True\n#       min: 1\n#       max: nan\n#       null_count: 0\n#       distinct_count: 0\n#       num_values: 2\n#       physical_type: DOUBLE\n#   compression: 1\n#   encodings: <map object at 0x7f28539eb4e0>\n#   has_dictionary_page: True\n#   dictionary_page_offset: 4\n#   data_page_offset: 36\n#   index_page_offset: 0\n#   total_compressed_size: 84\n#   total_uncompressed_size: 80\n```\n\n## Large string map\n\nThe file `large_string_map.brotli.parquet` was generated with:\n```python\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\narr = pa.array([[(\"a\" * 2**30, 1)]], type = pa.map_(pa.string(), pa.int32()))\narr = pa.chunked_array([arr, arr])\ntab = pa.table({ \"arr\": arr })\n\npq.write_table(tab, \"test.parquet\", compression='BROTLI')\n```\n\nIt is meant to exercise reading of structured data where each value\nis smaller than 2GB but the combined uncompressed column chunk size\nis greater than 2GB.\n\n## Float16 Files\n\nThe files `float16_zeros_and_nans.parquet` and `float16_nonzeros_and_nans.parquet`\nare meant to exercise a variety of test cases regarding `Float16` columns (which\nare represented as 2-byte `FixedLenByteArray`s), including:\n* Basic binary representations of standard values, +/- zeros, and NaN\n* Comparisons between finite values\n* Exclusion of NaNs from statistics min/max\n* Normalizing min/max values when only zeros are present (i.e. `min` is always -0 and `max` is always +0)\n\nThe aforementioned files were generated with:\n\n```python\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport numpy as np\n\nt1 = pa.Table.from_arrays(\n    [pa.array([None,\n               np.float16(0.0),\n               np.float16(np.NaN)], type=pa.float16())],\n    names=\"x\")\nt2 = pa.Table.from_arrays(\n    [pa.array([None,\n               np.float16(1.0),\n               np.float16(-2.0),\n               np.float16(np.NaN),\n               np.float16(0.0),\n               np.float16(-1.0),\n               np.float16(-0.0),\n               np.float16(2.0)],\n              type=pa.float16())],\n    names=\"x\")\n\npq.write_table(t1, \"float16_zeros_and_nans.parquet\", compression='none')\npq.write_table(t2, \"float16_nonzeros_and_nans.parquet\", compression='none')\n\nm1 = pq.read_metadata(\"float16_zeros_and_nans.parquet\")\nm2 = pq.read_metadata(\"float16_nonzeros_and_nans.parquet\")\n\nprint(m1.row_group(0).column(0))\nprint(m2.row_group(0).column(0))\n# <pyarrow._parquet.ColumnChunkMetaData object at 0x7f79e9a3d850>\n#   file_offset: 68\n#   file_path:\n#   physical_type: FIXED_LEN_BYTE_ARRAY\n#   num_values: 3\n#   path_in_schema: x\n#   is_stats_set: True\n#   statistics:\n#     <pyarrow._parquet.Statistics object at 0x7f79e9a3d940>\n#       has_min_max: True\n#       min: b'\\x00\\x80'\n#       max: b'\\x00\\x00'\n#       null_count: 1\n#       distinct_count: None\n#       num_values: 2\n#       physical_type: FIXED_LEN_BYTE_ARRAY\n#       logical_type: Float16\n#       converted_type (legacy): NONE\n#   compression: UNCOMPRESSED\n#   encodings: ('PLAIN', 'RLE', 'RLE_DICTIONARY')\n#   has_dictionary_page: True\n#   dictionary_page_offset: 4\n#   data_page_offset: 22\n#   total_compressed_size: 64\n#   total_uncompressed_size: 64\n# <pyarrow._parquet.ColumnChunkMetaData object at 0x7f79ea003c40>\n#   file_offset: 80\n#   file_path:\n#   physical_type: FIXED_LEN_BYTE_ARRAY\n#   num_values: 8\n#   path_in_schema: x\n#   is_stats_set: True\n#   statistics:\n#     <pyarrow._parquet.Statistics object at 0x7f79e9a3d8a0>\n#       has_min_max: True\n#       min: b'\\x00\\xc0'\n#       max: b'\\x00@'\n#       null_count: 1\n#       distinct_count: None\n#       num_values: 7\n#       physical_type: FIXED_LEN_BYTE_ARRAY\n#       logical_type: Float16\n#       converted_type (legacy): NONE\n#   compression: UNCOMPRESSED\n#   encodings: ('PLAIN', 'RLE', 'RLE_DICTIONARY')\n#   has_dictionary_page: True\n#   dictionary_page_offset: 4\n#   data_page_offset: 32\n#   total_compressed_size: 76\n#   total_uncompressed_size: 76\n```\n\n## Byte Stream Split\n\n# FLOAT and DOUBLE data\n\n`byte_stream_split.zstd.parquet` is generated by pyarrow 14.0.2 using the following code:\n\n```python\nimport pyarrow as pa\nfrom pyarrow import parquet as pq\nimport numpy as np\n\nnp.random.seed(0)\ntable = pa.Table.from_pydict({\n  'f32': np.random.normal(size=300).astype(np.float32),\n  'f64': np.random.normal(size=300).astype(np.float64),\n})\n\npq.write_table(\n  table,\n  'byte_stream_split.parquet',\n  version='2.6',\n  compression='zstd',\n  compression_level=22,\n  column_encoding='BYTE_STREAM_SPLIT',\n  use_dictionary=False,\n)\n```\n\nThis is a practical case where `BYTE_STREAM_SPLIT` encoding obtains a smaller file size than `PLAIN` or dictionary.\nSince the distributions are random normals centered at 0, each byte has nontrivial behavior.\n\n# Additional types\n\n`byte_stream_split_extended.gzip.parquet` is generated by pyarrow 16.0.0.\nIt contains 7 pairs of columns, each in two variants containing the same\nvalues: one `PLAIN`-encoded and one `BYTE_STREAM_SPLIT`-encoded:\n```\nVersion: 2.6\nCreated By: parquet-cpp-arrow version 16.0.0-SNAPSHOT\nTotal rows: 200\nNumber of RowGroups: 1\nNumber of Real Columns: 14\nNumber of Columns: 14\nNumber of Selected Columns: 14\nColumn 0: float16_plain (FIXED_LEN_BYTE_ARRAY(2) / Float16)\nColumn 1: float16_byte_stream_split (FIXED_LEN_BYTE_ARRAY(2) / Float16)\nColumn 2: float_plain (FLOAT)\nColumn 3: float_byte_stream_split (FLOAT)\nColumn 4: double_plain (DOUBLE)\nColumn 5: double_byte_stream_split (DOUBLE)\nColumn 6: int32_plain (INT32)\nColumn 7: int32_byte_stream_split (INT32)\nColumn 8: int64_plain (INT64)\nColumn 9: int64_byte_stream_split (INT64)\nColumn 10: flba5_plain (FIXED_LEN_BYTE_ARRAY(5))\nColumn 11: flba5_byte_stream_split (FIXED_LEN_BYTE_ARRAY(5))\nColumn 12: decimal_plain (FIXED_LEN_BYTE_ARRAY(4) / Decimal(precision=7, scale=3) / DECIMAL(7,3))\nColumn 13: decimal_byte_stream_split (FIXED_LEN_BYTE_ARRAY(4) / Decimal(precision=7, scale=3) / DECIMAL(7,3))\n```\n\nTo check conformance of a `BYTE_STREAM_SPLIT` decoder, read each\n`BYTE_STREAM_SPLIT`-encoded column and compare the decoded values against\nthe values from the corresponding `PLAIN`-encoded column. The values should\nbe equal.\n\n## Incorrect Map Schema\n\nA number of producers, such as Presto/Trino/Athena, have been creating files with schemas\nwhere the Map key fields are marked as optional rather than required.\nThis is not spec-compliant, yet appears in a number of existing data files in the wild.\n\nThis issue has been fixed in:\n- [Trino v386+](https://github.com/trinodb/trino/commit/3247bd2e64d7422bd13e805cd67cfca3fa8ba520)\n- [Presto v0.274+](https://github.com/prestodb/presto/commit/842b46972c11534a7729d0a18e3abc5347922d1a)\n\nWe can recreate these problematic files for testing [arrow-rs #5630](https://github.com/apache/arrow-rs/pull/5630)\nwith relevant Presto/Trino CLI, or with AWS Athena Console:\n\n```sql\nCREATE TABLE my_catalog.my_table_name WITH (format = 'Parquet') AS (\n    SELECT MAP (\n        ARRAY['name', 'parent'],\n        ARRAY[\n            'report',\n            'another'\n        ]\n    ) my_map\n)\n```\n\nThe schema in the created file is:\n\n```\nmessage hive_schema {\n  OPTIONAL group my_map (MAP) {\n    REPEATED group key_value (MAP_KEY_VALUE) {\n      OPTIONAL BYTE_ARRAY key (STRING);\n      OPTIONAL BYTE_ARRAY value (STRING);\n    }\n  }\n}\n```\n\n## REPEATED primitive fields with no LIST annotation\n```\nMetadata for file: repeated_primitive_no_list.parquet\n\nversion: 1\nnum of rows: 4\ncreated by: parquet-rs version 53.2.0\nmessage schema {\n  REPEATED INT32 Int32_list;\n  REPEATED BYTE_ARRAY String_list (UTF8);\n  REQUIRED group group_of_lists {\n    REPEATED INT32 Int32_list_in_group;\n    REPEATED BYTE_ARRAY String_list_in_group (UTF8);\n  }\n}\n```\n\n## Binary truncated min and max statistics\n\nFor the file: binary_truncated_min_max.parquet\n\nThe file contains six columns written with parquet-rs 55.1.0 with `statistics_truncate_length=2`.\nThe contents are the following:\n\n|column_name                |min      |is_min_value_exact|max            |is_max_value_exact|\n| ------------------------- | ------- | ---------------- | ------------- | ---------------- |\n|utf8_full_truncation       |\"Al\"     |false             |\"Kf\"           |false             |\n|binary_full_truncation     |\"0x416C\" |false             |\"0x4B66\"       |false             |\n|utf8_partial_truncation    |\"Al\"     |false             |\"ðŸš€Kevin Bacon\"|true              |\n|binary_partial_truncation  |\"0x416C\" |false             |\"0xFFFF0102\"   |true              |\n|utf8_no_truncation         |\"Al\"     |true              |\"Ke\"           |true              |\n|binary_no_truncation       |\"0x416C\" |true              |\"0x4B65\"       |true              |\n\nColumns `utf8_full_truncation` and `binary_full_truncation` are truncating the min/max values and `is_{min/max}_value_exact` are false.\nColumns `utf8_partial_truncation` and `binary_partial_truncation` are truncating min value but can't truncate the maximum value. `is_min_value_exact` is false but `is_max_value_exact` is true.\nColumns `utf8_no_truncation` and `binary_no_truncation` contain min and max value that fit on min/max. Both `is_{min/max}_value_exact` are true.\n\nSome info:\n```\n$ java -jar parquet-cli/target/parquet-cli-1.16.0-SNAPSHOT-runtime.jar meta /home/raulcd/code/parquet_truncate_file_generator/binary_truncated_min_max.parquet\n\nFile path:  /home/raulcd/code/parquet_truncate_file_generator/binary_truncated_min_max.parquet\nCreated by: parquet-rs version 55.1.0\nProperties:\n  ARROW:schema: /////6wBAAAQAAAAAAAKAAwACgAJAAQACgAAABAAAAAAAQQACAAIAAAABAAIAAAABAAAAAYAAABAAQAA9AAAALgAAAB4AAAAQAAAAAQAAADo/v//FAAAAAwAAAAAAAAEDAAAAAAAAADY/v//FAAAAGJpbmFyeV9ub190cnVuY2F0aW9uAAAAACD///8UAAAADAAAAAAAAAUMAAAAAAAAABD///8SAAAAdXRmOF9ub190cnVuY2F0aW9uAABU////FAAAAAwAAAAAAAAEDAAAAAAAAABE////GQAAAGJpbmFyeV9wYXJ0aWFsX3RydW5jYXRpb24AAACQ////FAAAAAwAAAAAAAAFDAAAAAAAAACA////FwAAAHV0ZjhfcGFydGlhbF90cnVuY2F0aW9uAMj///8UAAAADAAAAAAAAAQMAAAAAAAAALj///8WAAAAYmluYXJ5X2Z1bGxfdHJ1bmNhdGlvbgAAEAAUABAAAAAPAAQAAAAIABAAAAAYAAAADAAAAAAAAAUQAAAAAAAAAAQABAAEAAAAFAAAAHV0ZjhfZnVsbF90cnVuY2F0aW9uAAAAAA==\nSchema:\nmessage arrow_schema {\n  required binary utf8_full_truncation (STRING);\n  required binary binary_full_truncation;\n  required binary utf8_partial_truncation (STRING);\n  required binary binary_partial_truncation;\n  required binary utf8_no_truncation (STRING);\n  required binary binary_no_truncation;\n}\n\n\nRow group 0:  count: 12  117.83 B records  start: 4  total(compressed): 1.381 kB total(uncompressed):1.381 kB\n--------------------------------------------------------------------------------\n                           type      encodings count     avg size   nulls   min / max\nutf8_full_truncation       BINARY    _ BB_     12        20.83 B    0       \"Al\" / \"Kf\"\nbinary_full_truncation     BINARY    _ BB_     12        20.83 B    0       \"0x416C\" / \"0x4B66\"\nutf8_partial_truncation    BINARY    _ BB_     12        21.50 B    0       \"Al\" / \"ðŸš€Kevin Bacon\"\nbinary_partial_truncation  BINARY    _ BB_     12        19.67 B    0       \"0x416C\" / \"0xFFFF0102\"\nutf8_no_truncation         BINARY    _ BB_     12        17.50 B    0       \"Al\" / \"Ke\"\nbinary_no_truncation       BINARY    _ BB_     12        17.50 B    0       \"0x416C\" / \"0x4B65\"\n```\nand\n```\njava -jar parquet-cli/target/parquet-cli-1.16.0-SNAPSHOT-runtime.jar cat /home/raulcd/code/parquet_truncate_file_generator/binary_truncated_min_max.parquet\n{\"utf8_full_truncation\": \"Blart Versenwald III\", \"binary_full_truncation\": \"Blart Versenwald III\", \"utf8_partial_truncation\": \"Blart Versenwald III\", \"binary_partial_truncation\": \"Blart Versenwald III\", \"utf8_no_truncation\": \"Blart Versenwald III\", \"binary_no_truncation\": \"Blart Versenwald III\"}\n{\"utf8_full_truncation\": \"Alice Johnson\", \"binary_full_truncation\": \"Alice Johnson\", \"utf8_partial_truncation\": \"Alice Johnson\", \"binary_partial_truncation\": \"Alice Johnson\", \"utf8_no_truncation\": \"Al\", \"binary_no_truncation\": \"Al\"}\n{\"utf8_full_truncation\": \"Bob Smith\", \"binary_full_truncation\": \"Bob Smith\", \"utf8_partial_truncation\": \"Bob Smith\", \"binary_partial_truncation\": \"Bob Smith\", \"utf8_no_truncation\": \"Bob Smith\", \"binary_no_truncation\": \"Bob Smith\"}\n{\"utf8_full_truncation\": \"Charlie Brown\", \"binary_full_truncation\": \"Charlie Brown\", \"utf8_partial_truncation\": \"Charlie Brown\", \"binary_partial_truncation\": \"Charlie Brown\", \"utf8_no_truncation\": \"Charlie Brown\", \"binary_no_truncation\": \"Charlie Brown\"}\n{\"utf8_full_truncation\": \"Diana Prince\", \"binary_full_truncation\": \"Diana Prince\", \"utf8_partial_truncation\": \"Diana Prince\", \"binary_partial_truncation\": \"Diana Prince\", \"utf8_no_truncation\": \"Diana Prince\", \"binary_no_truncation\": \"Diana Prince\"}\n{\"utf8_full_truncation\": \"Edward Norton\", \"binary_full_truncation\": \"Edward Norton\", \"utf8_partial_truncation\": \"Edward Norton\", \"binary_partial_truncation\": \"Edward Norton\", \"utf8_no_truncation\": \"Edward Norton\", \"binary_no_truncation\": \"Edward Norton\"}\n{\"utf8_full_truncation\": \"Fiona Apple\", \"binary_full_truncation\": \"Fiona Apple\", \"utf8_partial_truncation\": \"Fiona Apple\", \"binary_partial_truncation\": \"Fiona Apple\", \"utf8_no_truncation\": \"Fiona Apple\", \"binary_no_truncation\": \"Fiona Apple\"}\n{\"utf8_full_truncation\": \"George Lucas\", \"binary_full_truncation\": \"George Lucas\", \"utf8_partial_truncation\": \"George Lucas\", \"binary_partial_truncation\": \"George Lucas\", \"utf8_no_truncation\": \"George Lucas\", \"binary_no_truncation\": \"George Lucas\"}\n{\"utf8_full_truncation\": \"Helen Keller\", \"binary_full_truncation\": \"Helen Keller\", \"utf8_partial_truncation\": \"Helen Keller\", \"binary_partial_truncation\": \"Helen Keller\", \"utf8_no_truncation\": \"Helen Keller\", \"binary_no_truncation\": \"Helen Keller\"}\n{\"utf8_full_truncation\": \"Ivan Drago\", \"binary_full_truncation\": \"Ivan Drago\", \"utf8_partial_truncation\": \"Ivan Drago\", \"binary_partial_truncation\": \"Ivan Drago\", \"utf8_no_truncation\": \"Ivan Drago\", \"binary_no_truncation\": \"Ivan Drago\"}\n{\"utf8_full_truncation\": \"Julia Roberts\", \"binary_full_truncation\": \"Julia Roberts\", \"utf8_partial_truncation\": \"Julia Roberts\", \"binary_partial_truncation\": \"Julia Roberts\", \"utf8_no_truncation\": \"Julia Roberts\", \"binary_no_truncation\": \"Julia Roberts\"}\n{\"utf8_full_truncation\": \"Kevin Bacon\", \"binary_full_truncation\": \"Kevin Bacon\", \"utf8_partial_truncation\": \"ðŸš€Kevin Bacon\", \"binary_partial_truncation\": \"Ã¿Ã¿\\u0001\\u0002\", \"utf8_no_truncation\": \"Ke\", \"binary_no_truncation\": \"Ke\"}\n```\n"
      },
      "plugins": [
        {
          "name": "hoogle",
          "description": "Search Haskell APIs using Hoogle - find functions by name or type signature",
          "source": "./",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add m4dc4p/claude-hoogle",
            "/plugin install hoogle@claude-hoogle"
          ]
        }
      ]
    }
  ]
}