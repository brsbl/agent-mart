{
  "author": {
    "id": "XiaoConstantine",
    "display_name": "Xiao",
    "avatar_url": "https://avatars.githubusercontent.com/u/3046977?u=e3cb3c71bdab9a3e3f84b32acd0dcb6a9ae417c8&v=4"
  },
  "marketplaces": [
    {
      "name": "sgrep",
      "version": null,
      "description": "Smart semantic + hybrid code search",
      "repo_full_name": "XiaoConstantine/sgrep",
      "repo_url": "https://github.com/XiaoConstantine/sgrep",
      "repo_description": "CLI for semantic grep",
      "signals": {
        "stars": 8,
        "forks": 2,
        "pushed_at": "2026-01-20T00:10:40Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"$schema\": \"https://anthropic.com/claude-code/marketplace.schema.json\",\n  \"name\": \"sgrep\",\n  \"owner\": {\n    \"name\": \"XiaoConstantine\",\n    \"email\": \"constantine124@gmail.com\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"sgrep\",\n      \"source\": \"./plugins/sgrep\",\n      \"description\": \"Smart semantic + hybrid code search\",\n      \"version\": \"0.1.0\",\n      \"author\": {\n        \"name\": \"Xiao Cui\"\n      },\n      \"skills\": [\"./skills/sgrep\"]\n    }\n  ]\n}\n",
        "README.md": "# sgrep - Smart Grep for Code\n\n**Semantic + hybrid code search that complements `ripgrep` and `ast-grep`.**\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│  ripgrep (rg)     │  ast-grep (sg)    │  sgrep              │\n│  ─────────────    │  ──────────────   │  ──────             │\n│  Exact text/regex │  AST patterns     │  Semantic + hybrid  │\n│  \"findUser\"       │  $fn($args)       │  \"auth validation\"  │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n## Why sgrep?\n\nCoding agents (Amp, Claude Code, Cursor) waste tokens on failed `grep` attempts when searching for concepts rather than exact strings. `sgrep` understands **what you mean**, not just what you type.\n\n```bash\n# ❌ Agent tries 10+ grep patterns, burns 2000 tokens\nrg \"authenticate\" && rg \"auth\" && rg \"login\" && rg \"session\" ...\n\n# ✅ One semantic query, 50 tokens\nsgrep \"how does user authentication work\"\n```\n\n## Installation\n\n### Homebrew (macOS/Linux)\n\n```bash\nbrew tap XiaoConstantine/tap\nbrew install sgrep\n```\n\n### Quick Install (curl)\n\n```bash\ncurl -fsSL https://raw.githubusercontent.com/XiaoConstantine/sgrep/main/install.sh | bash\n```\n\n### Go Install\n\n```bash\ngo install github.com/XiaoConstantine/sgrep/cmd/sgrep@latest\n```\n\n### From Source\n\n```bash\ngit clone https://github.com/XiaoConstantine/sgrep.git\ncd sgrep\n\n# Default build (uses libSQL with DiskANN vector search)\ngo build -o sgrep ./cmd/sgrep\n\n# Alternative: sqlite-vec backend\ngo build -tags=sqlite_vec -o sgrep ./cmd/sgrep\n```\n\n**Requirements**: llama.cpp (for the embedding server)\n```bash\nbrew install llama.cpp   # macOS\n# or build from source: https://github.com/ggerganov/llama.cpp\n```\n\n### As Library\n\n```bash\ngo get github.com/XiaoConstantine/sgrep@latest\n```\n\n## Quick Start\n\n```bash\n# One-time setup: downloads embedding model (~130MB)\nsgrep setup\n\n# Index your codebase (auto-starts embedding server)\nsgrep index .\n\n# Semantic search (quick)\nsgrep \"error handling for database connections\"\n\n# Hybrid + ColBERT (recommended - best accuracy)\nsgrep --hybrid --colbert \"JWT token validation logic\"\nsgrep --hybrid --colbert \"how are API rate limits implemented\"\n\n# Hybrid with custom weights\nsgrep --hybrid --colbert \"authentication middleware\" --semantic-weight 0.5 --bm25-weight 0.5\n\n# Watch mode (background indexing)\nsgrep watch .\n```\n\nThe embedding server starts automatically when needed and stays running as a daemon.\n\n## Conversation Search\n\nSearch across conversations from Claude Code, Codex CLI, Cursor, and OpenCode.\n\n```bash\n# Index conversations (auto-starts embedding server)\nsgrep conv index\n\n# Index a single agent\nsgrep conv index --source claude\nsgrep conv index --source codex\nsgrep conv index --source cursor\nsgrep conv index --source opencode\n\n# Watch mode (auto-index new conversations)\nsgrep conv index --watch\n\n# Search conversations\nsgrep conv \"authentication\"\nsgrep conv \"JWT token\" --hybrid\nsgrep conv \"database migration\" --agent claude --since 7d\n\n# View, export, or resume a session\nsgrep conv view <session_id>\nsgrep conv export <session_id> -o conversation.md\nsgrep conv resume <session_id>\n\n# Extract context for injection into new session\nsgrep conv context <session_id>\n\n# Copy to clipboard\nsgrep conv copy <session_id>\n\n# Check index status\nsgrep conv status\n```\n\n**Watch mode** monitors conversation directories for all agents and automatically indexes new sessions as they're created. This ensures your conversation search stays up-to-date without manual re-indexing.\n\nConversations are stored at `~/.sgrep/conversations/conv.db`. Re-running\n`sgrep conv index` backfills missing embeddings for existing sessions.\n\n## Hybrid Search\n\nHybrid search combines **semantic understanding** with **lexical matching (BM25)** for improved accuracy. This helps when:\n- Searching for specific technical terms (e.g., \"JWT\", \"OAuth\", \"mutex\")\n- The query contains exact function/variable names\n- Semantic search alone misses exact keyword matches\n\n```bash\n# Default: semantic-only search\nsgrep \"authentication\"\n\n# Hybrid: semantic (60%) + BM25 (40%) - default weights\nsgrep --hybrid \"authentication\"\n\n# Custom weights: more emphasis on exact matches\nsgrep --hybrid --semantic-weight 0.4 --bm25-weight 0.6 \"parseAST\"\n```\n\n**Note**: Hybrid search requires building with FTS5 support (see [From Source](#from-source)). The FTS5 index is created automatically on first hybrid search - no re-indexing needed.\n\n## Multi-Stage Retrieval Pipeline\n\nsgrep uses a sophisticated multi-stage retrieval pipeline for maximum accuracy:\n\n```\nQuery: \"authentication middleware\"\n         ↓\n┌─────────────────────────────────────────────────────────────────┐\n│ Stage 1: Hybrid Retrieval (--hybrid)                            │\n│ ┌───────────────┐    ┌───────────────┐                         │\n│ │   Semantic    │    │     BM25      │                         │\n│ │  (DiskANN)    │    │    (FTS5)     │                         │\n│ │     60%       │    │     40%       │                         │\n│ └───────┬───────┘    └───────┬───────┘                         │\n│         └────────┬───────────┘                                  │\n│                  ↓                                              │\n│         Top 50 candidates                                       │\n└─────────────────────────────────────────────────────────────────┘\n                   ↓\n┌─────────────────────────────────────────────────────────────────┐\n│ Stage 2: ColBERT Late Interaction (--colbert)                   │\n│ ┌───────────────────────────────────────────────────────────┐  │\n│ │  Token-level similarity: MaxSim(query_tokens, doc_tokens) │  │\n│ │  Scores all 50 candidates with fine-grained matching      │  │\n│ └───────────────────────────────────────────────────────────┘  │\n│                  ↓                                              │\n│         Re-scored candidates                                    │\n└─────────────────────────────────────────────────────────────────┘\n                   ↓\n┌─────────────────────────────────────────────────────────────────┐\n│ Stage 3: Cross-Encoder Reranking (--rerank)                     │\n│ ┌───────────────────────────────────────────────────────────┐  │\n│ │  Full attention: query ⊗ document → relevance score       │  │\n│ │  Reranks top 20 ColBERT results (~300-700ms)              │  │\n│ └───────────────────────────────────────────────────────────┘  │\n│                  ↓                                              │\n│         Final ranked results                                    │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n### Retrieval Modes\n\n| Mode | Command | MRR | Latency | Best For |\n|------|---------|-----|---------|----------|\n| Semantic only | `sgrep \"query\"` | 0.61 | ~30ms | Quick searches |\n| **Hybrid + ColBERT** | `sgrep --hybrid --colbert \"query\"` | **0.70** | ~200ms | **Best accuracy for code** |\n| Hybrid | `sgrep --hybrid \"query\"` | 0.62 | ~50ms | Exact term matching |\n| Cascade (all 3 stages) | `sgrep --hybrid --colbert --rerank \"query\"` | 0.60 | ~500ms | General text (not code) |\n\n**Recommended for code**: Use `--hybrid --colbert`. ColBERT provides +13% MRR over plain hybrid.\n\n> **Note**: Cross-encoder reranking adds a third stage but currently hurts code search accuracy (MRR drops from 0.70 to 0.60). This is because available cross-encoder models (mxbai-rerank) are trained on general text, not code. Cross-encoder may help for non-code search tasks.\n\n```bash\n# Best accuracy (recommended)\nsgrep --hybrid --colbert \"authentication middleware\"\n\n# Quick search (semantic only)\nsgrep \"error handling\"\n\n# With custom weights\nsgrep --hybrid --colbert --semantic-weight 0.5 --bm25-weight 0.5 \"JWT token\"\n```\n\n### Setup\n\n```bash\n# Basic setup (embedding model only, ~130MB)\nsgrep setup\n\n# With cross-encoder reranking (~1.6GB additional)\nsgrep setup --with-rerank\n```\n\n**Note**: ColBERT scoring uses the same embedding model—no additional setup required. Cross-encoder reranking requires a separate model download.\n\n## Document-Level Search\n\nsgrep automatically handles meta-queries about your repository:\n\n```bash\n# These queries use document-level embeddings\nsgrep \"what does this repo do\"\nsgrep \"project overview\"\nsgrep \"purpose of this codebase\"\n```\n\nDocument-level embeddings (mean of chunk embeddings per file) are computed during indexing, enabling README.md and other overview files to rank highly for repository-level questions.\n\n## Agent-Optimized Output\n\nDefault output is minimal for token efficiency:\n\n```bash\n$ sgrep \"authentication middleware\"\nauth/middleware.go:45-67\nauth/jwt.go:12-38\nhandlers/login.go:89-112\n```\n\nUse `-c` for context (still concise):\n```bash\n$ sgrep -c \"authentication middleware\"\nauth/middleware.go:45-67\n  func AuthMiddleware(next http.Handler) http.Handler {\n      token := r.Header.Get(\"Authorization\")\n      ...\n\nauth/jwt.go:12-38\n  func ValidateJWT(token string) (*Claims, error) {\n      ...\n```\n\nJSON output for programmatic use:\n```bash\n$ sgrep --json \"authentication\"\n[{\"file\":\"auth/middleware.go\",\"start\":45,\"end\":67,\"score\":0.92}]\n```\n\n## Combining with ripgrep and ast-grep\n\n**The search hierarchy for agents:**\n\n1. **sgrep** - Find the right files/functions by intent\n2. **ast-grep** - Match structural patterns in those files  \n3. **ripgrep** - Exact text search for specific symbols\n\nExample workflow:\n```bash\n# Step 1: Semantic search to find relevant code\nsgrep \"rate limiting implementation\" \n# → api/ratelimit.go:20-80\n\n# Step 2: AST pattern to find all similar usages\nsg -p 'rateLimiter.Check($ctx, $key)' \n\n# Step 3: Exact search for specific constant\nrg \"RATE_LIMIT_MAX\"\n```\n\n## Storage\n\nAll data is stored in `~/.sgrep/`:\n```\n~/.sgrep/\n├── models/\n│   └── nomic-embed-text-v1.5.Q8_0.gguf   # Embedding model (~130MB)\n├── repos/\n│   ├── a1b2c3/              # Hash of /path/to/repo1\n│   │   ├── index.db         # libSQL database with DiskANN vectors\n│   │   └── metadata.json    # Repo path, index time\n│   └── d4e5f6/              # Hash of /path/to/repo2\n│       └── ...\n├── server.pid               # Embedding server PID\n└── server.log               # Embedding server logs\n```\n\nUse `sgrep list` to see all indexed repositories.\n\n## Storage Backends\n\nsgrep supports two vector storage backends:\n\n| Backend | Build Command | Storage Efficiency | Best For |\n|---------|--------------|-------------------|----------|\n| **libSQL** (default) | `go build ./cmd/sgrep` | ~5-10 KB/vector | Large repos, production |\n| sqlite-vec | `go build -tags=sqlite_vec ./cmd/sgrep` | ~780 KB/vector | Development, compatibility |\n\n**libSQL advantages:**\n- Uses DiskANN for approximate nearest neighbor search\n- 93-177x more space-efficient than sqlite-vec\n- Native F32_BLOB column type for vectors\n- Compress neighbors option for index compression\n\n## Commands\n\n| Command | Description |\n|---------|-------------|\n| `sgrep [query]` | Semantic search (default) |\n| `sgrep index [path]` | Index a directory |\n| `sgrep watch [path]` | Watch and auto-index |\n| `sgrep list` | List all indexed repos |\n| `sgrep status` | Show index status |\n| `sgrep clear` | Clear index |\n| `sgrep setup` | Download embedding model, verify llama-server |\n| `sgrep setup --with-rerank` | Also download reranker model (~636MB) |\n| `sgrep server start` | Manually start embedding server |\n| `sgrep server stop` | Stop embedding server |\n| `sgrep server status` | Show server status |\n| `sgrep install-claude-code` | Install Claude Code plugin |\n\n## Claude Code Integration\n\nInstall the sgrep plugin for Claude Code with one command:\n\n```bash\nsgrep install-claude-code\n```\n\nThis creates a plugin at `~/.claude/plugins/sgrep` that:\n- **Auto-indexes** your project when Claude Code starts\n- **Watch mode** keeps the index updated as you code\n- **Skill documentation** teaches Claude when to use sgrep vs ripgrep\n\nAfter installation, restart Claude Code to activate. The plugin works automatically—Claude will use sgrep for semantic searches like \"how does authentication work\" while using ripgrep for exact matches.\n\n## Flags\n\n| Flag | Description |\n|------|-------------|\n| `-n, --limit N` | Max results (default: 10) |\n| `-c, --context` | Show code context |\n| `--json` | JSON output for agents |\n| `-q, --quiet` | Minimal output (paths only) |\n| `--threshold F` | L2 distance threshold (default: 1.5, lower = stricter) |\n| `-t, --include-tests` | Include test files in results (excluded by default) |\n| `--all-chunks` | Show all matching chunks (disable deduplication) |\n| `--hybrid` | Enable hybrid search (semantic + BM25) |\n| `--colbert` | Enable ColBERT late interaction scoring (recommended with --hybrid) |\n| `--semantic-weight F` | Weight for semantic score in hybrid mode (default: 0.6) |\n| `--bm25-weight F` | Weight for BM25 score in hybrid mode (default: 0.4) |\n| `--rerank` | Enable cross-encoder reranking (requires `sgrep setup --with-rerank`) |\n| `-d, --debug` | Show debug timing information |\n\n## Configuration\n\nEnvironment variables:\n```bash\nSGREP_HOME=~/.sgrep                    # Data storage location\nSGREP_ENDPOINT=http://localhost:8080   # Override embedding server URL\nSGREP_PORT=8080                        # Embedding server port\nSGREP_DIMS=768                         # Vector dimensions\n```\n\n## How It Works\n\n1. **Setup**: `sgrep setup` downloads the embedding model and verifies llama-server\n2. **Indexing**: Files are chunked using AST-aware splitting (Go, TS, Python) or size-based fallback\n3. **Embedding**: Each chunk is embedded via llama.cpp (local, $0 cost, auto-started)\n4. **Storage**: Vectors stored in libSQL with DiskANN indexing\n5. **Search**: Query embedded → DiskANN approximate nearest neighbor → load matching documents\n\n**Smart skip for large repos**: When indexing repos with >1000 files, sgrep automatically filters out test files, generated code (*.pb.go, *.generated.go), and vendored directories to speed up indexing.\n\n## Architecture\n\n```\n┌──────────────────────────────────────────────────────────────┐\n│                         sgrep                                │\n├──────────────────────────────────────────────────────────────┤\n│  Query: \"error handling\"                                     │\n│         ↓                                                    │\n│  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐      │\n│  │ llama.cpp   │───▶│  DiskANN    │───▶│   libSQL    │      │\n│  │ Embedding   │    │ + BM25/FTS5 │    │  Documents  │      │\n│  │   (~15ms)   │    │   (~10ms)   │    │   (~5ms)    │      │\n│  └─────────────┘    └─────────────┘    └─────────────┘      │\n│       ▲                    │                                 │\n│       │                    ▼ (with --colbert)                │\n│       │              ┌─────────────┐                         │\n│       │              │  ColBERT    │                         │\n│       │              │ Late-Interx │                         │\n│       │              │  (~150ms)   │                         │\n│       │              └──────┬──────┘                         │\n│       │                     │                                │\n│       │                     ▼ (with --rerank)                │\n│       │              ┌─────────────┐                         │\n│       │              │Cross-Encoder│                         │\n│       │              │  Reranker   │                         │\n│       │              │ (~300-700ms)│                         │\n│       │              └─────────────┘                         │\n│       │                                                      │\n│       │ Auto-started by sgrep (16 parallel slots)           │\n│       │ (daemon mode, continuous batching)                  │\n│                                                              │\n│  Recommended: --hybrid --colbert (~200ms, MRR 0.70)         │\n└──────────────────────────────────────────────────────────────┘\n```\n\n### Hybrid Search Architecture\n\nWhen `--hybrid` is enabled, sgrep combines semantic and lexical search:\n\n```\nQuery: \"authentication middleware\"\n         ↓\n  ┌──────────────────────────────────────────────────────┐\n  │                                                      │\n  │  ┌─────────────┐         ┌─────────────┐           │\n  │  │  Semantic   │         │    BM25     │           │\n  │  │  (Vectors)  │         │   (FTS5)    │           │\n  │  │    60%      │         │    40%      │           │\n  │  └──────┬──────┘         └──────┬──────┘           │\n  │         │                       │                   │\n  │         └───────┬───────────────┘                   │\n  │                 ↓                                   │\n  │         ┌─────────────┐                            │\n  │         │   Hybrid    │                            │\n  │         │   Ranking   │                            │\n  │         └─────────────┘                            │\n  │                                                      │\n  └──────────────────────────────────────────────────────┘\n```\n\n- **Semantic**: Understands intent (\"auth\" matches \"authentication\", \"login\", \"session\")\n- **BM25**: Exact term matching with TF-IDF weighting (boosts exact \"authentication\" matches)\n\n## Performance\n\nBenchmarked on maestro codebase (102 files, 1572 chunks, 768-dim vectors):\n\n| Metric | sgrep | ripgrep | \n|--------|-------|---------|\n| Latency (avg) | **31ms** | 10ms |\n| Token usage | **57% less** | baseline |\n| Attempts needed | 1 | 3-7 |\n\n**Embedding server optimization:**\n\nThe llama.cpp server is configured for maximum throughput:\n- 16 parallel slots with continuous batching (`-cb`)\n- Dynamic thread count based on CPU cores\n- GPU acceleration (Metal on Mac, CUDA on Linux)\n\n## Chunk Size Limits\n\nThe embedding model (nomic-embed-text) has a 2048 token context limit. sgrep handles this by:\n\n1. Default chunk size: 1000 tokens (with AST-aware splitting)\n2. Safety truncation at 1500 tokens in embedder\n3. Large functions/types split into parts automatically\n\n## Library Usage\n\nUse sgrep as an embedded library in your Go application:\n\n```go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"log\"\n\n    \"github.com/XiaoConstantine/sgrep\"\n)\n\nfunc main() {\n    ctx := context.Background()\n    \n    // Create client for a codebase\n    client, err := sgrep.New(\"/path/to/codebase\")\n    if err != nil {\n        log.Fatal(err)\n    }\n    defer client.Close()\n\n    // Index the codebase (required before searching)\n    if err := client.Index(ctx); err != nil {\n        log.Fatal(err)\n    }\n\n    // Search for code by semantic intent\n    results, err := client.Search(ctx, \"authentication logic\", 10)\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    for _, r := range results {\n        fmt.Printf(\"%s:%d-%d (score: %.2f)\\n\", r.FilePath, r.StartLine, r.EndLine, r.Score)\n    }\n}\n```\n\nFor more control, use the `pkg/` subpackages directly:\n- `pkg/index` - Indexing and file watching\n- `pkg/search` - Search with caching\n- `pkg/embed` - Embedding generation\n- `pkg/store` - Vector storage\n- `pkg/chunk` - Code chunking with AST awareness\n\n## License\n\nApache-2.0\n"
      },
      "plugins": [
        {
          "name": "sgrep",
          "source": "./plugins/sgrep",
          "description": "Smart semantic + hybrid code search",
          "version": "0.1.0",
          "author": {
            "name": "Xiao Cui"
          },
          "skills": [
            "./skills/sgrep"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add XiaoConstantine/sgrep",
            "/plugin install sgrep@sgrep"
          ]
        }
      ]
    },
    {
      "name": "rlm",
      "version": null,
      "description": "Recursive Language Model for large context processing with token efficiency",
      "repo_full_name": "XiaoConstantine/rlm-go",
      "repo_url": "https://github.com/XiaoConstantine/rlm-go",
      "repo_description": "Go implementation of Recursive Language Models (RLM) - inference-time scaling for arbitrarily long contexts",
      "signals": {
        "stars": 6,
        "forks": 3,
        "pushed_at": "2026-01-09T16:40:00Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"$schema\": \"https://anthropic.com/claude-code/marketplace.schema.json\",\n  \"name\": \"rlm\",\n  \"owner\": {\n    \"name\": \"XiaoConstantine\",\n    \"email\": \"constantine124@gmail.com\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"rlm\",\n      \"source\": \"./plugins/rlm\",\n      \"description\": \"Recursive Language Model for large context processing with token efficiency\",\n      \"version\": \"0.1.0\",\n      \"author\": {\n        \"name\": \"Xiao Cui\"\n      },\n      \"skills\": [\"./skills/rlm\"]\n    }\n  ]\n}\n",
        "README.md": "# rlm-go\n\n[![CI](https://github.com/XiaoConstantine/rlm-go/actions/workflows/go.yml/badge.svg)](https://github.com/XiaoConstantine/rlm-go/actions/workflows/go.yml)\n[![Go Report Card](https://goreportcard.com/badge/github.com/XiaoConstantine/rlm-go)](https://goreportcard.com/report/github.com/XiaoConstantine/rlm-go)\n[![Go Reference](https://pkg.go.dev/badge/github.com/XiaoConstantine/rlm-go.svg)](https://pkg.go.dev/github.com/XiaoConstantine/rlm-go)\n\nA Go implementation of [Recursive Language Models (RLM)](https://github.com/alexzhang13/rlm) - an inference-time scaling strategy that enables LLMs to handle arbitrarily long contexts by treating prompts as external objects that can be programmatically examined and recursively processed.\n\n## Overview\n\nRLM-go provides a Go REPL environment where LLM-generated code can:\n- Access context stored as a variable\n- Make recursive sub-LLM calls via `Query()` and `QueryBatched()`\n- Use standard Go operations for text processing\n- Signal completion with `FINAL()` or `FINAL_VAR()`\n\n### Key Design Decisions\n\nUnlike the Python RLM which uses socket IPC, rlm-go uses **direct function injection** via [Yaegi](https://github.com/traefik/yaegi) - a Go interpreter. This eliminates:\n- Socket server overhead\n- Serialization/deserialization\n- Process boundaries\n\nThe result is ~100x less latency per sub-LLM call compared to socket IPC.\n\n## Requirements\n\n- Go 1.23 or later (for building from source)\n- An LLM API key:\n  - `ANTHROPIC_API_KEY` for Claude models (default)\n  - `GEMINI_API_KEY` for Gemini models\n  - `OPENAI_API_KEY` for OpenAI models\n- (Optional) Podman or Docker for isolated sandbox execution\n\n## Supported Models\n\n| Provider | Models | Env Variable |\n|----------|--------|--------------|\n| Anthropic | claude-sonnet-4-20250514, claude-opus-4-20250514, etc. | `ANTHROPIC_API_KEY` |\n| Google | gemini-3-flash-preview, gemini-3-pro-preview | `GEMINI_API_KEY` |\n| OpenAI | gpt-5, gpt-5-mini | `OPENAI_API_KEY` |\n\nThe provider is auto-detected based on model name. Anthropic is the default.\n\n## Installation\n\n### Quick Install (Recommended)\n\n```bash\n# Download and install the latest release\ncurl -fsSL https://raw.githubusercontent.com/XiaoConstantine/rlm-go/main/install.sh | bash\n```\n\nThis installs the `rlm` binary to `~/.local/bin/rlm`.\n\n### Go Install\n\n```bash\ngo install github.com/XiaoConstantine/rlm-go/cmd/rlm@latest\n```\n\n### From Source\n\n```bash\ngit clone https://github.com/XiaoConstantine/rlm-go.git\ncd rlm-go\ngo build -o rlm ./cmd/rlm\n```\n\n### As a Library\n\n```bash\ngo get github.com/XiaoConstantine/rlm-go\n```\n\n## Claude Code Integration\n\nRLM includes a skill for [Claude Code](https://claude.com/claude-code) that provides documentation and usage guidance for large context processing.\n\n### Install the Skill\n\n```bash\nrlm install-claude-code\n```\n\nThis creates a skill at `~/.claude/skills/rlm/SKILL.md` that teaches Claude Code:\n- When to use RLM (contexts >50KB, token efficiency needed)\n- Command usage and options\n- The Query() and FINAL() patterns\n- Token efficiency benefits (40% savings on large contexts)\n\nAfter installation, restart Claude Code to activate the skill.\n\n## CLI Usage\n\n```bash\n# Basic usage with Anthropic (default)\nrlm -context file.txt -query \"Summarize the key points\"\n\n# Use Gemini\nrlm -model gemini-3-flash-preview -context file.txt -query \"Analyze this data\"\n\n# Use OpenAI\nrlm -model gpt-5-mini -context file.txt -query \"Summarize this\"\n\n# Verbose output with iteration details\nrlm -context logs.json -query \"Find all errors\" -verbose\n\n# JSON output for programmatic use\nrlm -context data.csv -query \"Extract anomalies\" -json\n\n# Pipe context from stdin\ncat largefile.txt | rlm -query \"What patterns do you see?\"\n```\n\n### CLI Options\n\n| Flag | Description | Default |\n|------|-------------|---------|\n| `-context` | Path to context file | - |\n| `-context-string` | Context string directly | - |\n| `-query` | Query to run against context | Required |\n| `-model` | LLM model to use | claude-sonnet-4-20250514 |\n| `-max-iterations` | Maximum iterations | 30 |\n| `-verbose` | Enable verbose output | false |\n| `-json` | Output result as JSON | false |\n| `-log-dir` | Directory for JSONL logs | - |\n\n## Quick Start (Library)\n\n```go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"log\"\n    \"os\"\n\n    \"github.com/XiaoConstantine/rlm-go/pkg/rlm\"\n)\n\nfunc main() {\n    // Create your LLM client (implements rlm.LLMClient and repl.LLMClient)\n    client := NewAnthropicClient(os.Getenv(\"ANTHROPIC_API_KEY\"), \"claude-sonnet-4-20250514\")\n\n    // Create RLM instance\n    r := rlm.New(client, client,\n        rlm.WithMaxIterations(10),\n        rlm.WithVerbose(true),\n    )\n\n    // Run completion with long context\n    result, err := r.Complete(context.Background(), longDocument, \"What are the key findings?\")\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    fmt.Printf(\"Answer: %s\\n\", result.Response)\n    fmt.Printf(\"Iterations: %d\\n\", result.Iterations)\n    fmt.Printf(\"Total Tokens: %d\\n\", result.TotalUsage.TotalTokens)\n}\n```\n\n## Architecture\n\n```\n┌──────────────────────────────────────────────┐\n│              Single Go Process               │\n│                                              │\n│  ┌──────────────┐    ┌──────────────────┐   │\n│  │    Yaegi     │───►│   LLM Client     │   │\n│  │  Interpreter │    │ (your impl)      │   │\n│  │              │    └──────────────────┘   │\n│  │ - context    │                           │\n│  │ - Query()    │◄── direct function call   │\n│  │ - fmt.*      │                           │\n│  └──────────────┘                           │\n│         ▲                                   │\n│         │ Eval(code)                        │\n│  ┌──────┴──────┐                            │\n│  │  RLM Loop   │                            │\n│  └─────────────┘                            │\n└──────────────────────────────────────────────┘\n\nNo sockets. No IPC. No subprocess.\n```\n\n## Interfaces\n\nYou need to implement two interfaces:\n\n```go\n// For root LLM orchestration\ntype LLMClient interface {\n    Complete(ctx context.Context, messages []core.Message) (core.LLMResponse, error)\n}\n\n// For sub-LLM calls from REPL\ntype REPLClient interface {\n    Query(ctx context.Context, prompt string) (repl.QueryResponse, error)\n    QueryBatched(ctx context.Context, prompts []string) ([]repl.QueryResponse, error)\n}\n```\n\nSee [examples/basic](examples/basic/main.go) for a complete Anthropic client implementation.\n\n## How It Works\n\n1. **Context Loading**: Your context is injected into a Yaegi interpreter as a `context` variable\n2. **Iteration Loop**: The root LLM generates Go code in ` ```go ` blocks\n3. **Code Execution**: Yaegi executes the code with access to `Query()`, `QueryBatched()`, `fmt`, `strings`, `regexp`\n4. **Sub-LLM Calls**: `Query()` calls your LLM client directly (no IPC)\n5. **Completion**: LLM signals done with `FINAL(\"answer\")` or `FINAL_VAR(varName)`\n\n## Available in REPL\n\n```go\n// Pre-imported packages\nfmt, strings, regexp\n\n// RLM functions\nQuery(prompt string) string              // Single sub-LLM call\nQueryBatched(prompts []string) []string  // Concurrent sub-LLM calls\n\n// Multi-depth recursion (when enabled)\nQueryWithRLM(prompt string, depth int) string  // Spawn nested RLM\nCurrentDepth() int                              // Get current recursion depth\nMaxDepth() int                                  // Get max allowed depth\nCanRecurse() bool                               // Check if more recursion allowed\n\n// Your context\ncontext  // string variable with your data\n```\n\n### Multi-Depth Recursion\n\nEnable sub-LLMs to spawn their own sub-LLMs for complex decomposition tasks:\n\n```go\nr := rlm.New(client, replClient,\n    rlm.WithMaxRecursionDepth(3),  // Allow 3 levels of nesting\n    rlm.WithRecursionCallback(func(depth int, prompt string) {\n        log.Printf(\"Recursive call at depth %d\", depth)\n    }),\n)\n```\n\nIn the REPL, use `QueryWithRLM()` to spawn a nested RLM that can itself use `Query()`:\n```go\n// Depth 0 (root)\nresult := QueryWithRLM(\"Analyze each section in detail\", 1)\n\n// The sub-RLM (depth 1) can use Query() or QueryWithRLM() up to MaxDepth\n```\n\n## Configuration\n\n```go\nrlm.New(client, replClient,\n    rlm.WithMaxIterations(30),      // Default: 30\n    rlm.WithSystemPrompt(custom),   // Override system prompt\n    rlm.WithVerbose(true),          // Enable console logging\n    rlm.WithLogger(logger),         // Attach JSONL logger for session recording\n)\n```\n\n## Sandbox Execution (Podman/Docker)\n\nBy default, rlm-go executes LLM-generated code in-process using Yaegi for maximum performance. For production environments or when running untrusted code, you can enable isolated sandbox execution using Podman (recommended) or Docker.\n\n### Why Sandbox?\n\n| Mode | Isolation | Latency | Security |\n|------|-----------|---------|----------|\n| Local (default) | None | ~0ms | Trusted code only |\n| Podman/Docker | Full container | 50-200ms | Untrusted code safe |\n\n### Setup\n\n**Podman (Recommended - Open Source, Daemonless)**\n```bash\n# macOS\nbrew install podman\npodman machine init\npodman machine start\n\n# Linux (Fedora/RHEL)\nsudo dnf install podman\n\n# Linux (Ubuntu/Debian)\nsudo apt install podman\n```\n\n**Docker (Alternative)**\n```bash\n# macOS\nbrew install --cask docker\n# Start Docker Desktop\n\n# Linux\nsudo apt install docker.io\nsudo systemctl start docker\n```\n\n### Usage\n\n```go\nimport (\n    \"github.com/XiaoConstantine/rlm-go/pkg/rlm\"\n    \"github.com/XiaoConstantine/rlm-go/pkg/sandbox\"\n)\n\n// Auto-detect best available backend (podman > docker > local)\nr := rlm.New(client, replClient, rlm.WithSandbox())\n\n// Use specific backend\nr := rlm.New(client, replClient,\n    rlm.WithSandboxBackend(sandbox.BackendPodman))\n\n// Custom configuration\ncfg := sandbox.Config{\n    Backend:     sandbox.BackendPodman,\n    Image:       \"golang:1.23-alpine\",  // Container image\n    Memory:      \"512m\",                // Memory limit\n    CPUs:        1.0,                   // CPU limit\n    Timeout:     60 * time.Second,      // Execution timeout\n    NetworkMode: sandbox.NetworkNone,   // Disable network (secure)\n}\nr := rlm.New(client, replClient, rlm.WithSandboxConfig(cfg))\n```\n\n### Container Behavior\n\nWhen sandbox mode is enabled:\n- Code runs in an isolated container with resource limits\n- Network is disabled by default (`--network=none`)\n- Containers are auto-removed after execution (`--rm`)\n- `Query()` and `QueryBatched()` work via JSON IPC protocol\n- First execution pulls the Go image (~250MB, cached after)\n\n### Verifying Setup\n\n```bash\n# Check if Podman/Docker is available\npodman --version  # or: docker --version\n\n# Test container execution\npodman run --rm golang:1.23-alpine go version\n\n# Run sandbox tests\ngo test ./pkg/sandbox/... -v -run TestContainer\n```\n\n## Token Tracking\n\nRLM-go provides complete token usage accounting across all LLM calls:\n\n```go\nresult, _ := r.Complete(ctx, context, query)\n\n// Aggregated token usage\nfmt.Printf(\"Prompt tokens: %d\\n\", result.TotalUsage.PromptTokens)\nfmt.Printf(\"Completion tokens: %d\\n\", result.TotalUsage.CompletionTokens)\nfmt.Printf(\"Total tokens: %d\\n\", result.TotalUsage.TotalTokens)\n```\n\nToken counts include both root LLM calls and all sub-LLM calls made via `Query()` and `QueryBatched()`.\n\n## JSONL Logging\n\nRecord sessions for analysis and visualization:\n\n```go\nimport \"github.com/XiaoConstantine/rlm-go/pkg/logger\"\n\n// Create logger\nlog, _ := logger.New(\"./logs\", \"session-001\")\ndefer log.Close()\n\n// Attach to RLM\nr := rlm.New(client, replClient, rlm.WithLogger(log))\n```\n\nLog format includes:\n- Session metadata (model, max iterations, context info)\n- Per-iteration details (prompts, responses, executed code)\n- Sub-LLM call records with token counts\n- Compatible with the [Python RLM visualizer](https://github.com/alexzhang13/rlm)\n\n## CLI Tools\n\n### Benchmark Tool\n\nCompare RLM accuracy against baseline direct LLM calls:\n\n```bash\ngo run ./cmd/benchmark/main.go \\\n  -tasks tasks.json \\\n  -model claude-sonnet-4-20250514 \\\n  -num-tasks 10 \\\n  -log-dir ./logs \\\n  -output results.json \\\n  -verbose\n```\n\nFeatures:\n- Load tasks from JSON or generate samples\n- Track accuracy, execution time, token usage\n- Flexible answer matching (exact, word-boundary, numeric)\n\n### Log Viewer\n\nInteractive CLI viewer for JSONL session logs:\n\n```bash\ngo run ./cmd/rlm-viewer/main.go ./logs/session.jsonl\n\n# Watch mode for real-time viewing\ngo run ./cmd/rlm-viewer/main.go -watch ./logs/session.jsonl\n\n# Filter by iteration\ngo run ./cmd/rlm-viewer/main.go -iter 3 ./logs/session.jsonl\n\n# Interactive navigation mode\ngo run ./cmd/rlm-viewer/main.go -interactive ./logs/session.jsonl\n```\n\nFeatures:\n- Color-coded output (system, user, assistant messages)\n- Code block display with execution results\n- Token usage tracking per LLM call\n- Interactive navigation\n\n## Package Structure\n\n```\nrlm-go/\n├── pkg/\n│   ├── core/      # Core types (Message, CompletionResult, UsageStats)\n│   ├── rlm/       # Main RLM orchestration engine\n│   ├── repl/      # Yaegi-based Go interpreter\n│   ├── sandbox/   # Isolated execution (Podman/Docker)\n│   ├── parsing/   # LLM response parsing utilities\n│   └── logger/    # JSONL session logging\n├── cmd/\n│   ├── benchmark/ # RLM vs baseline comparison tool\n│   └── rlm-viewer/ # JSONL log viewer\n└── examples/\n    └── basic/     # Complete Anthropic client example\n```\n\n## Testing\n\n```bash\n# Run all tests\ngo test -v ./...\n\n# Run with race detection and coverage\ngo test -race -v ./... -coverprofile coverage.txt\n```\n\n## References\n\n- [Recursive Language Models Paper](https://arxiv.org/abs/2512.24601) (Zhang, Kraska, Khattab - MIT CSAIL)\n- [Python RLM Implementation](https://github.com/alexzhang13/rlm)\n- [Yaegi Go Interpreter](https://github.com/traefik/yaegi)\n\n## License\n\nMIT License - see [LICENSE](LICENSE)\n"
      },
      "plugins": [
        {
          "name": "rlm",
          "source": "./plugins/rlm",
          "description": "Recursive Language Model for large context processing with token efficiency",
          "version": "0.1.0",
          "author": {
            "name": "Xiao Cui"
          },
          "skills": [
            "./skills/rlm"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add XiaoConstantine/rlm-go",
            "/plugin install rlm@rlm"
          ]
        }
      ]
    }
  ]
}