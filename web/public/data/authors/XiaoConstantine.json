{
  "author": {
    "id": "XiaoConstantine",
    "display_name": "Xiao",
    "avatar_url": "https://avatars.githubusercontent.com/u/3046977?u=e3cb3c71bdab9a3e3f84b32acd0dcb6a9ae417c8&v=4"
  },
  "marketplaces": [
    {
      "name": "rlm",
      "version": null,
      "description": "Recursive Language Model for large context processing with token efficiency",
      "repo_full_name": "XiaoConstantine/rlm-go",
      "repo_url": "https://github.com/XiaoConstantine/rlm-go",
      "repo_description": "Go implementation of Recursive Language Models (RLM) - inference-time scaling for arbitrarily long contexts",
      "signals": {
        "stars": 5,
        "forks": 3,
        "pushed_at": "2026-01-09T16:40:00Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"$schema\": \"https://anthropic.com/claude-code/marketplace.schema.json\",\n  \"name\": \"rlm\",\n  \"owner\": {\n    \"name\": \"XiaoConstantine\",\n    \"email\": \"constantine124@gmail.com\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"rlm\",\n      \"source\": \"./plugins/rlm\",\n      \"description\": \"Recursive Language Model for large context processing with token efficiency\",\n      \"version\": \"0.1.0\",\n      \"author\": {\n        \"name\": \"Xiao Cui\"\n      },\n      \"skills\": [\"./skills/rlm\"]\n    }\n  ]\n}\n",
        "README.md": "# rlm-go\n\n[![CI](https://github.com/XiaoConstantine/rlm-go/actions/workflows/go.yml/badge.svg)](https://github.com/XiaoConstantine/rlm-go/actions/workflows/go.yml)\n[![Go Report Card](https://goreportcard.com/badge/github.com/XiaoConstantine/rlm-go)](https://goreportcard.com/report/github.com/XiaoConstantine/rlm-go)\n[![Go Reference](https://pkg.go.dev/badge/github.com/XiaoConstantine/rlm-go.svg)](https://pkg.go.dev/github.com/XiaoConstantine/rlm-go)\n\nA Go implementation of [Recursive Language Models (RLM)](https://github.com/alexzhang13/rlm) - an inference-time scaling strategy that enables LLMs to handle arbitrarily long contexts by treating prompts as external objects that can be programmatically examined and recursively processed.\n\n## Overview\n\nRLM-go provides a Go REPL environment where LLM-generated code can:\n- Access context stored as a variable\n- Make recursive sub-LLM calls via `Query()` and `QueryBatched()`\n- Use standard Go operations for text processing\n- Signal completion with `FINAL()` or `FINAL_VAR()`\n\n### Key Design Decisions\n\nUnlike the Python RLM which uses socket IPC, rlm-go uses **direct function injection** via [Yaegi](https://github.com/traefik/yaegi) - a Go interpreter. This eliminates:\n- Socket server overhead\n- Serialization/deserialization\n- Process boundaries\n\nThe result is ~100x less latency per sub-LLM call compared to socket IPC.\n\n## Requirements\n\n- Go 1.23 or later (for building from source)\n- An LLM API key:\n  - `ANTHROPIC_API_KEY` for Claude models (default)\n  - `GEMINI_API_KEY` for Gemini models\n  - `OPENAI_API_KEY` for OpenAI models\n- (Optional) Podman or Docker for isolated sandbox execution\n\n## Supported Models\n\n| Provider | Models | Env Variable |\n|----------|--------|--------------|\n| Anthropic | claude-sonnet-4-20250514, claude-opus-4-20250514, etc. | `ANTHROPIC_API_KEY` |\n| Google | gemini-3-flash-preview, gemini-3-pro-preview | `GEMINI_API_KEY` |\n| OpenAI | gpt-5, gpt-5-mini | `OPENAI_API_KEY` |\n\nThe provider is auto-detected based on model name. Anthropic is the default.\n\n## Installation\n\n### Quick Install (Recommended)\n\n```bash\n# Download and install the latest release\ncurl -fsSL https://raw.githubusercontent.com/XiaoConstantine/rlm-go/main/install.sh | bash\n```\n\nThis installs the `rlm` binary to `~/.local/bin/rlm`.\n\n### Go Install\n\n```bash\ngo install github.com/XiaoConstantine/rlm-go/cmd/rlm@latest\n```\n\n### From Source\n\n```bash\ngit clone https://github.com/XiaoConstantine/rlm-go.git\ncd rlm-go\ngo build -o rlm ./cmd/rlm\n```\n\n### As a Library\n\n```bash\ngo get github.com/XiaoConstantine/rlm-go\n```\n\n## Claude Code Integration\n\nRLM includes a skill for [Claude Code](https://claude.com/claude-code) that provides documentation and usage guidance for large context processing.\n\n### Install the Skill\n\n```bash\nrlm install-claude-code\n```\n\nThis creates a skill at `~/.claude/skills/rlm/SKILL.md` that teaches Claude Code:\n- When to use RLM (contexts >50KB, token efficiency needed)\n- Command usage and options\n- The Query() and FINAL() patterns\n- Token efficiency benefits (40% savings on large contexts)\n\nAfter installation, restart Claude Code to activate the skill.\n\n## CLI Usage\n\n```bash\n# Basic usage with Anthropic (default)\nrlm -context file.txt -query \"Summarize the key points\"\n\n# Use Gemini\nrlm -model gemini-3-flash-preview -context file.txt -query \"Analyze this data\"\n\n# Use OpenAI\nrlm -model gpt-5-mini -context file.txt -query \"Summarize this\"\n\n# Verbose output with iteration details\nrlm -context logs.json -query \"Find all errors\" -verbose\n\n# JSON output for programmatic use\nrlm -context data.csv -query \"Extract anomalies\" -json\n\n# Pipe context from stdin\ncat largefile.txt | rlm -query \"What patterns do you see?\"\n```\n\n### CLI Options\n\n| Flag | Description | Default |\n|------|-------------|---------|\n| `-context` | Path to context file | - |\n| `-context-string` | Context string directly | - |\n| `-query` | Query to run against context | Required |\n| `-model` | LLM model to use | claude-sonnet-4-20250514 |\n| `-max-iterations` | Maximum iterations | 30 |\n| `-verbose` | Enable verbose output | false |\n| `-json` | Output result as JSON | false |\n| `-log-dir` | Directory for JSONL logs | - |\n\n## Quick Start (Library)\n\n```go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"log\"\n    \"os\"\n\n    \"github.com/XiaoConstantine/rlm-go/pkg/rlm\"\n)\n\nfunc main() {\n    // Create your LLM client (implements rlm.LLMClient and repl.LLMClient)\n    client := NewAnthropicClient(os.Getenv(\"ANTHROPIC_API_KEY\"), \"claude-sonnet-4-20250514\")\n\n    // Create RLM instance\n    r := rlm.New(client, client,\n        rlm.WithMaxIterations(10),\n        rlm.WithVerbose(true),\n    )\n\n    // Run completion with long context\n    result, err := r.Complete(context.Background(), longDocument, \"What are the key findings?\")\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    fmt.Printf(\"Answer: %s\\n\", result.Response)\n    fmt.Printf(\"Iterations: %d\\n\", result.Iterations)\n    fmt.Printf(\"Total Tokens: %d\\n\", result.TotalUsage.TotalTokens)\n}\n```\n\n## Architecture\n\n```\n┌──────────────────────────────────────────────┐\n│              Single Go Process               │\n│                                              │\n│  ┌──────────────┐    ┌──────────────────┐   │\n│  │    Yaegi     │───►│   LLM Client     │   │\n│  │  Interpreter │    │ (your impl)      │   │\n│  │              │    └──────────────────┘   │\n│  │ - context    │                           │\n│  │ - Query()    │◄── direct function call   │\n│  │ - fmt.*      │                           │\n│  └──────────────┘                           │\n│         ▲                                   │\n│         │ Eval(code)                        │\n│  ┌──────┴──────┐                            │\n│  │  RLM Loop   │                            │\n│  └─────────────┘                            │\n└──────────────────────────────────────────────┘\n\nNo sockets. No IPC. No subprocess.\n```\n\n## Interfaces\n\nYou need to implement two interfaces:\n\n```go\n// For root LLM orchestration\ntype LLMClient interface {\n    Complete(ctx context.Context, messages []core.Message) (core.LLMResponse, error)\n}\n\n// For sub-LLM calls from REPL\ntype REPLClient interface {\n    Query(ctx context.Context, prompt string) (repl.QueryResponse, error)\n    QueryBatched(ctx context.Context, prompts []string) ([]repl.QueryResponse, error)\n}\n```\n\nSee [examples/basic](examples/basic/main.go) for a complete Anthropic client implementation.\n\n## How It Works\n\n1. **Context Loading**: Your context is injected into a Yaegi interpreter as a `context` variable\n2. **Iteration Loop**: The root LLM generates Go code in ` ```go ` blocks\n3. **Code Execution**: Yaegi executes the code with access to `Query()`, `QueryBatched()`, `fmt`, `strings`, `regexp`\n4. **Sub-LLM Calls**: `Query()` calls your LLM client directly (no IPC)\n5. **Completion**: LLM signals done with `FINAL(\"answer\")` or `FINAL_VAR(varName)`\n\n## Available in REPL\n\n```go\n// Pre-imported packages\nfmt, strings, regexp\n\n// RLM functions\nQuery(prompt string) string              // Single sub-LLM call\nQueryBatched(prompts []string) []string  // Concurrent sub-LLM calls\n\n// Multi-depth recursion (when enabled)\nQueryWithRLM(prompt string, depth int) string  // Spawn nested RLM\nCurrentDepth() int                              // Get current recursion depth\nMaxDepth() int                                  // Get max allowed depth\nCanRecurse() bool                               // Check if more recursion allowed\n\n// Your context\ncontext  // string variable with your data\n```\n\n### Multi-Depth Recursion\n\nEnable sub-LLMs to spawn their own sub-LLMs for complex decomposition tasks:\n\n```go\nr := rlm.New(client, replClient,\n    rlm.WithMaxRecursionDepth(3),  // Allow 3 levels of nesting\n    rlm.WithRecursionCallback(func(depth int, prompt string) {\n        log.Printf(\"Recursive call at depth %d\", depth)\n    }),\n)\n```\n\nIn the REPL, use `QueryWithRLM()` to spawn a nested RLM that can itself use `Query()`:\n```go\n// Depth 0 (root)\nresult := QueryWithRLM(\"Analyze each section in detail\", 1)\n\n// The sub-RLM (depth 1) can use Query() or QueryWithRLM() up to MaxDepth\n```\n\n## Configuration\n\n```go\nrlm.New(client, replClient,\n    rlm.WithMaxIterations(30),      // Default: 30\n    rlm.WithSystemPrompt(custom),   // Override system prompt\n    rlm.WithVerbose(true),          // Enable console logging\n    rlm.WithLogger(logger),         // Attach JSONL logger for session recording\n)\n```\n\n## Sandbox Execution (Podman/Docker)\n\nBy default, rlm-go executes LLM-generated code in-process using Yaegi for maximum performance. For production environments or when running untrusted code, you can enable isolated sandbox execution using Podman (recommended) or Docker.\n\n### Why Sandbox?\n\n| Mode | Isolation | Latency | Security |\n|------|-----------|---------|----------|\n| Local (default) | None | ~0ms | Trusted code only |\n| Podman/Docker | Full container | 50-200ms | Untrusted code safe |\n\n### Setup\n\n**Podman (Recommended - Open Source, Daemonless)**\n```bash\n# macOS\nbrew install podman\npodman machine init\npodman machine start\n\n# Linux (Fedora/RHEL)\nsudo dnf install podman\n\n# Linux (Ubuntu/Debian)\nsudo apt install podman\n```\n\n**Docker (Alternative)**\n```bash\n# macOS\nbrew install --cask docker\n# Start Docker Desktop\n\n# Linux\nsudo apt install docker.io\nsudo systemctl start docker\n```\n\n### Usage\n\n```go\nimport (\n    \"github.com/XiaoConstantine/rlm-go/pkg/rlm\"\n    \"github.com/XiaoConstantine/rlm-go/pkg/sandbox\"\n)\n\n// Auto-detect best available backend (podman > docker > local)\nr := rlm.New(client, replClient, rlm.WithSandbox())\n\n// Use specific backend\nr := rlm.New(client, replClient,\n    rlm.WithSandboxBackend(sandbox.BackendPodman))\n\n// Custom configuration\ncfg := sandbox.Config{\n    Backend:     sandbox.BackendPodman,\n    Image:       \"golang:1.23-alpine\",  // Container image\n    Memory:      \"512m\",                // Memory limit\n    CPUs:        1.0,                   // CPU limit\n    Timeout:     60 * time.Second,      // Execution timeout\n    NetworkMode: sandbox.NetworkNone,   // Disable network (secure)\n}\nr := rlm.New(client, replClient, rlm.WithSandboxConfig(cfg))\n```\n\n### Container Behavior\n\nWhen sandbox mode is enabled:\n- Code runs in an isolated container with resource limits\n- Network is disabled by default (`--network=none`)\n- Containers are auto-removed after execution (`--rm`)\n- `Query()` and `QueryBatched()` work via JSON IPC protocol\n- First execution pulls the Go image (~250MB, cached after)\n\n### Verifying Setup\n\n```bash\n# Check if Podman/Docker is available\npodman --version  # or: docker --version\n\n# Test container execution\npodman run --rm golang:1.23-alpine go version\n\n# Run sandbox tests\ngo test ./pkg/sandbox/... -v -run TestContainer\n```\n\n## Token Tracking\n\nRLM-go provides complete token usage accounting across all LLM calls:\n\n```go\nresult, _ := r.Complete(ctx, context, query)\n\n// Aggregated token usage\nfmt.Printf(\"Prompt tokens: %d\\n\", result.TotalUsage.PromptTokens)\nfmt.Printf(\"Completion tokens: %d\\n\", result.TotalUsage.CompletionTokens)\nfmt.Printf(\"Total tokens: %d\\n\", result.TotalUsage.TotalTokens)\n```\n\nToken counts include both root LLM calls and all sub-LLM calls made via `Query()` and `QueryBatched()`.\n\n## JSONL Logging\n\nRecord sessions for analysis and visualization:\n\n```go\nimport \"github.com/XiaoConstantine/rlm-go/pkg/logger\"\n\n// Create logger\nlog, _ := logger.New(\"./logs\", \"session-001\")\ndefer log.Close()\n\n// Attach to RLM\nr := rlm.New(client, replClient, rlm.WithLogger(log))\n```\n\nLog format includes:\n- Session metadata (model, max iterations, context info)\n- Per-iteration details (prompts, responses, executed code)\n- Sub-LLM call records with token counts\n- Compatible with the [Python RLM visualizer](https://github.com/alexzhang13/rlm)\n\n## CLI Tools\n\n### Benchmark Tool\n\nCompare RLM accuracy against baseline direct LLM calls:\n\n```bash\ngo run ./cmd/benchmark/main.go \\\n  -tasks tasks.json \\\n  -model claude-sonnet-4-20250514 \\\n  -num-tasks 10 \\\n  -log-dir ./logs \\\n  -output results.json \\\n  -verbose\n```\n\nFeatures:\n- Load tasks from JSON or generate samples\n- Track accuracy, execution time, token usage\n- Flexible answer matching (exact, word-boundary, numeric)\n\n### Log Viewer\n\nInteractive CLI viewer for JSONL session logs:\n\n```bash\ngo run ./cmd/rlm-viewer/main.go ./logs/session.jsonl\n\n# Watch mode for real-time viewing\ngo run ./cmd/rlm-viewer/main.go -watch ./logs/session.jsonl\n\n# Filter by iteration\ngo run ./cmd/rlm-viewer/main.go -iter 3 ./logs/session.jsonl\n\n# Interactive navigation mode\ngo run ./cmd/rlm-viewer/main.go -interactive ./logs/session.jsonl\n```\n\nFeatures:\n- Color-coded output (system, user, assistant messages)\n- Code block display with execution results\n- Token usage tracking per LLM call\n- Interactive navigation\n\n## Package Structure\n\n```\nrlm-go/\n├── pkg/\n│   ├── core/      # Core types (Message, CompletionResult, UsageStats)\n│   ├── rlm/       # Main RLM orchestration engine\n│   ├── repl/      # Yaegi-based Go interpreter\n│   ├── sandbox/   # Isolated execution (Podman/Docker)\n│   ├── parsing/   # LLM response parsing utilities\n│   └── logger/    # JSONL session logging\n├── cmd/\n│   ├── benchmark/ # RLM vs baseline comparison tool\n│   └── rlm-viewer/ # JSONL log viewer\n└── examples/\n    └── basic/     # Complete Anthropic client example\n```\n\n## Testing\n\n```bash\n# Run all tests\ngo test -v ./...\n\n# Run with race detection and coverage\ngo test -race -v ./... -coverprofile coverage.txt\n```\n\n## References\n\n- [Recursive Language Models Paper](https://arxiv.org/abs/2512.24601) (Zhang, Kraska, Khattab - MIT CSAIL)\n- [Python RLM Implementation](https://github.com/alexzhang13/rlm)\n- [Yaegi Go Interpreter](https://github.com/traefik/yaegi)\n\n## License\n\nMIT License - see [LICENSE](LICENSE)\n"
      },
      "plugins": [
        {
          "name": "rlm",
          "source": "./plugins/rlm",
          "description": "Recursive Language Model for large context processing with token efficiency",
          "version": "0.1.0",
          "author": {
            "name": "Xiao Cui"
          },
          "skills": [
            "./skills/rlm"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add XiaoConstantine/rlm-go",
            "/plugin install rlm@rlm"
          ]
        }
      ]
    }
  ]
}