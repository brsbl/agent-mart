{
  "author": {
    "id": "hyperion-git",
    "display_name": "Alex Friedrich",
    "avatar_url": "https://avatars.githubusercontent.com/u/7377256?u=d66d6bd8868dcea467fdfe525aa8132268af45ee&v=4"
  },
  "marketplaces": [
    {
      "name": "alethic",
      "version": null,
      "description": "Reasoning agent for mathematics and physics with Generate-Verify-Revise loop, plus scientific figure generation",
      "repo_full_name": "hyperion-git/alethic",
      "repo_url": "https://github.com/hyperion-git/alethic",
      "repo_description": "An implementation of the Google Aletheia architecture based on publications for use as a skill in Claude Code or via API",
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2026-02-16T19:33:41Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"alethic\",\n  \"description\": \"Reasoning agent for mathematics and physics with Generate-Verify-Revise loop, plus scientific figure generation\",\n  \"owner\": {\n    \"name\": \"Alexander Friedrich\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"alethic\",\n      \"description\": \"Reasoning agent for mathematics and physics with Generate-Verify-Revise loop (inspired by DeepMind's Aletheia), plus scientific figure generation\",\n      \"version\": \"1.0.0\",\n      \"source\": \"./\",\n      \"author\": {\n        \"name\": \"Alexander Friedrich\"\n      }\n    }\n  ]\n}\n",
        ".claude-plugin/plugin.json": "{\n  \"name\": \"alethic\",\n  \"description\": \"Reasoning agent for mathematics and physics with Generate-Verify-Revise loop (inspired by DeepMind's Aletheia), textbook-style proof/derivation converter, plus publication-quality scientific figure generation\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Alexander Friedrich\"\n  },\n  \"homepage\": \"https://github.com/hyperion-git/alethic\",\n  \"repository\": \"https://github.com/hyperion-git/alethic\",\n  \"license\": \"MIT\",\n  \"keywords\": [\"math\", \"physics\", \"reasoning\", \"verification\", \"theorem-proving\", \"aletheia\", \"figures\", \"plotting\"]\n}\n",
        "README.md": "# Alethic\n\nA reasoning agent for mathematics and physics inspired by [Google DeepMind's Aletheia](https://arxiv.org/abs/2602.10177), built on **Claude (Opus 4.6)**. Alethic implements a Generate-Verify-Revise loop with decoupled verification — a key architectural insight from DeepMind's design — to produce rigorous mathematical proofs and physics derivations with high confidence.\n\nAvailable as Claude Code skills (`/alethic-solve` for math, `/alethic-derive` for physics, `/alethic-scientific-figure` for scientific figures) or as a standalone **Python library** with CLI.\n\n## Background and Motivation\n\nIn February 2026, Google DeepMind introduced Aletheia, a multi-agent system that achieved 95% accuracy on IMO-ProofBench Advanced and autonomously resolved open Erdős conjectures. The system's central innovation lies in its separation of solution generation from solution verification: by preventing the verifier from observing the generator's intermediate reasoning traces, Aletheia avoids the confidence inflation that arises when a model evaluates its own chain of thought.\n\nWhen a verifier has access to the generator's internal reasoning, it tends to follow the same logical path and confirm flawed steps with unwarranted certainty. Decoupling forces the verifier to reconstruct and independently assess each argument from the final output alone.\n\nAlethic translates this decoupled verification approach to Claude's API. The project implements the same three-subagent loop — Generator, Verifier, and Reviser — with each role instantiated as an independent API call (in the Python library) or a separate Task sub-agent with a fresh context window (in the Claude Code skill). The orchestrator logic is domain-neutral; only the prompt templates differ between math (`MathAgent`, `/alethic-solve`) and physics (`PhysicsAgent`, `/alethic-derive`). The result is a system that can solve mathematical problems and derive physics results with verified confidence, or honestly admit failure when it cannot.\n\n### Key References\n\n- T. Shoeybi et al., \"Towards Autonomous Mathematics Research,\" [arXiv:2602.10177](https://arxiv.org/abs/2602.10177) (Feb 2026). The primary Aletheia paper describing the Generate-Verify-Revise architecture.\n- J. Huang et al., \"Accelerating Scientific Research with Gemini,\" [arXiv:2602.03837](https://arxiv.org/abs/2602.03837) (Feb 2026). Companion paper on Gemini's scientific reasoning capabilities.\n- R. Anil et al., \"Semi-Autonomous Mathematics Discovery,\" [arXiv:2601.22401](https://arxiv.org/abs/2601.22401) (Jan 2026). The Erdős conjecture study demonstrating autonomous mathematical discovery.\n\n## Architecture\n\nAlethic's reasoning loop proceeds through three distinct phases that repeat until the solution is verified or the iteration budget is exhausted.\n\n```\n┌─────────────────────────────────────────────────────────┐\n│                    Orchestrator Loop                     │\n│                                                         │\n│   ┌───────────┐    ┌──────────┐    ┌──────────┐       │\n│   │ Generator  │───▶│ Verifier │───▶│ Reviser  │──┐    │\n│   │  (T=1.0)  │    │  (T=0.2) │    │  (T=0.7) │  │    │\n│   └───────────┘    └──────────┘    └──────────┘  │    │\n│        ▲                                          │    │\n│        └──────────────────────────────────────────┘    │\n│                                                         │\n│   Terminates when: CORRECT (≥ threshold) OR max iters   │\n└─────────────────────────────────────────────────────────┘\n```\n\nThe **Generator** produces a candidate solution at high temperature (T=1.0) to encourage creative exploration of proof strategies. The **Verifier** then evaluates that solution at low temperature (T=0.2) for strict, deterministic assessment. Critically, the Verifier receives only the problem statement and the final written solution — never the Generator's thinking traces, tool outputs, or intermediate reasoning. If the Verifier identifies issues, the **Reviser** receives both the solution and the Verifier's structured critique, producing an improved version at moderate temperature (T=0.7) that balances faithfulness to the original with the flexibility to restructure flawed arguments.\n\nThe loop terminates under one of three conditions: the Verifier issues a `CORRECT` verdict with confidence at or above the configured threshold (default 90%), the maximum number of iterations is reached (strategic failure admission), or the Verifier detects that the problem's premise is false and halts early with an explanation.\n\n### Information Flow and Decoupling\n\nThe following sequence diagram illustrates the critical decoupling boundary. The Generator's internal reasoning — thinking traces, tool call results, intermediate drafts — never crosses to the Verifier. Only the final solution text is passed, forcing the Verifier to evaluate the argument on its own merits.\n\n```mermaid\nsequenceDiagram\n    participant O as Orchestrator\n    participant G as Generator\n    participant V as Verifier\n    participant R as Reviser\n\n    O->>G: problem statement\n    Note right of G: Reasoning traces,<br/>tool calls, thinking\n    G->>O: solution text\n\n    Note over O: Only solution text<br/>crosses to Verifier\n\n    O->>V: problem + solution text\n    Note right of V: Independent evaluation<br/>(no Generator context)\n    V->>O: verdict + confidence + critique\n\n    alt CORRECT with confidence >= threshold\n        O->>O: Accept solution\n    else Needs revision\n        O->>R: solution + critique\n        R->>O: revised solution\n        O->>V: problem + revised solution\n        Note right of V: Fresh evaluation<br/>(no prior context)\n        V->>O: verdict + confidence + critique\n    end\n```\n\n### The Three Subagents\n\nEach subagent is instantiated as an independent Claude API call with role-specific system prompts, temperature settings, and tool access. This separation ensures that no subagent can observe another's internal state.\n\n**Generator.** The Generator's task is to produce a complete, self-contained solution — a mathematical proof (in `/alethic-solve` / `MathAgent`) or a physics derivation (in `/alethic-derive` / `PhysicsAgent`). Its system prompt instructs it to restate the problem, select a strategy explicitly (proof techniques for math, derivation methods like Lagrangian mechanics or perturbation theory for physics), justify every inference, and use precise notation. When balanced prompting is enabled (the default), an addendum directs the Generator to first check whether the problem might be ill-posed: for math, this means testing small cases and boundary conditions; for physics, checking dimensional consistency and known limiting cases. This anti-confirmation-bias technique, adapted from the Aletheia design, reduces the risk of the model anchoring prematurely on a flawed approach. The Generator has access to a sandboxed Python environment for computational verification of intermediate results.\n\n**Verifier.** The Verifier is the architectural cornerstone of the system. Its system prompt establishes strict independence: it must evaluate the solution purely on its written merits, checking every logical step, re-deriving computations independently, and flagging common mathematical errors including sign mistakes, off-by-one errors, vacuous truth claims, circular reasoning, non-exhaustive case analysis, and incorrect theorem application. The Verifier produces a structured output containing a verdict (`correct`, `minor_issues`, `major_flaw`, or `unsolved`), a numerical confidence score calibrated against explicit benchmarks (0.95-1.0 for fully verified solutions, below 0.50 for likely errors), a step-by-step critique, a reason field for false-premise detection, and a list of specific issues. The confidence threshold (default 90%, configurable via `confidence_threshold`) means that even a `correct` verdict at lower confidence is treated as requiring revision — the Verifier must be genuinely certain.\n\n**Reviser.** When the Verifier identifies issues, the Reviser receives the original solution alongside the full structured critique. Its prompt instructs it to distinguish between minor issues (which can be patched in place) and major flaws (which require a fundamentally different proof strategy). The Reviser preserves parts of the solution that the Verifier confirmed as sound and provides explicit justification for each change. If it believes the critique itself is incorrect, it may argue back with evidence — but the subsequent re-verification by a fresh Verifier instance has the final word.\n\n### Verdict Types and Actions\n\n| Verdict | Condition | Action |\n|---------|-----------|--------|\n| `CORRECT` | Confidence ≥ threshold | Accept the solution and return it to the user |\n| `CORRECT` | Confidence < threshold | Treat as uncertain — send to Reviser for strengthening |\n| `MINOR_ISSUES` | — | Send to Reviser with the critique |\n| `MAJOR_FLAW` | — | Revise if attempts remain, otherwise restart from Generator |\n| `UNSOLVED` | Reason field populated | Problem premise is false — halt and explain |\n| `UNSOLVED` | No reason | Cannot solve — restart from Generator or admit failure |\n\n### Decision Flowchart\n\nThe full control flow, including revision loops, iteration restarts, and termination conditions:\n\n```mermaid\nflowchart TD\n    Start([Problem]) --> Gen[\"Generate (T=1.0)\"]\n    Gen --> Ver[\"Verify (T=0.2)\"]\n    Ver --> D{Verdict?}\n\n    D -->|\"CORRECT ≥ threshold\"| Accept[Accept solution]\n    D -->|\"CORRECT < threshold\"| Rev[\"Revise (T=0.7)\"]\n    D -->|MINOR_ISSUES| Rev\n    D -->|MAJOR_FLAW| MF{Revisions left?}\n    D -->|\"UNSOLVED + reason\"| FP[Premise is false]\n    D -->|UNSOLVED| Iter{Iterations left?}\n\n    MF -->|Yes| Rev\n    MF -->|No| Iter\n\n    Rev --> ReVer[\"Re-verify (T=0.2)\"]\n    ReVer --> RD{Verdict?}\n\n    RD -->|\"CORRECT ≥ threshold\"| Accept\n    RD -->|\"CORRECT < threshold\"| MoreRev{Revisions left?}\n    RD -->|MINOR_ISSUES| MoreRev\n    RD -->|MAJOR_FLAW| Iter\n    RD -->|\"UNSOLVED + reason\"| FP\n    RD -->|UNSOLVED| Iter\n\n    MoreRev -->|Yes| Rev\n    MoreRev -->|No| Iter\n\n    Iter -->|Yes| Gen\n    Iter -->|No| Fail[Admit failure]\n\n    Accept --> B[\"Beautify (skill only)\"]\n    B --> Solved([SOLVED])\n    FP --> Halt([HALT - premise false])\n    Fail --> BF[\"Beautify best effort (skill only)\"]\n    BF --> Unsolved([UNSOLVED - best effort])\n```\n\n### Verifier Output Format\n\nThe Verifier produces structured output that is parsed via regex with independent extraction per field. This design ensures that partial or malformed output still yields usable information — if the verdict is parseable but the confidence is not, the system defaults to 0.5 rather than failing entirely.\n\n```\nVERDICT: correct | minor_issues | major_flaw | unsolved\nCONFIDENCE: 0.0 to 1.0\n\nCRITIQUE:\n[Step-by-step evaluation of the solution]\n\nREASON: [Why the premise is false, or \"N/A\"]\n\nISSUES:\n- [Issue 1]\n- [Issue 2]\n```\n\n## Design Principles\n\n**Decoupled verification.** The separation of Generator and Verifier contexts is the single most important architectural decision. In the Python library, decoupling is enforced at the API level: the `verify()` function receives only the problem string and the solution text, never the Generator's response object or thinking blocks. In the Claude Code skill, decoupling is enforced structurally — each Verifier runs as a separate Task sub-agent that launches with a fresh context window and physically cannot access the Generator's reasoning.\n\n**Balanced prompting.** Before committing to a strategy, the Generator is instructed to actively look for reasons the problem might be ill-posed or the approach might fail. For math, this means searching for counterexamples and testing boundary conditions; for physics, checking dimensional consistency and verifying known limiting cases. This technique, adapted from the Aletheia paper, counteracts the confirmation bias that arises when a language model generates a solution: once a model begins pursuing a particular approach, it tends to rationalize intermediate steps rather than question the approach itself. Balanced prompting is enabled by default and can be disabled via `--no-balanced` (CLI) or `balanced=False` (Python API).\n\n**Strategic failure admission.** When the Verifier cannot approve any solution after exhausting the iteration budget, the system returns `Verdict.UNSOLVED` along with the highest-confidence solution encountered during the run. This honest failure mode prevents the agent from hallucinating confidence in an unverified answer. The `admitted_failure` flag on the result object distinguishes between problems that were identified as having a false premise (a valid finding) and problems that the agent simply could not solve.\n\n**Confidence calibration.** The Verifier outputs a numerical confidence score between 0.0 and 1.0. The system prompt instructs the Verifier to be skeptical and to assign confidence proportional to the rigor of its own verification. The configurable confidence threshold (default 90%) means that even a `CORRECT` verdict at lower confidence is treated as uncertain and sent for revision, preventing the common failure mode where a model assigns high confidence to every output regardless of actual certainty.\n\n**Sandboxed code execution.** Both the Generator and Verifier have access to a Python sandbox for computational verification. The sandbox restricts available builtins to a safe subset, limits importable modules to mathematical and scientific libraries (math, sympy, numpy, scipy, mpmath, and related packages), and enforces execution timeouts via `SIGALRM`. This allows the subagents to test conjectures numerically, verify algebraic manipulations symbolically, evaluate special functions, and check edge cases computationally — all without the security risks of unrestricted code execution.\n\n**File-based state (skill only).** In the Claude Code skill, all solutions, verifications, and revisions are written to files in a session directory (`.alethic/{slug}-{date}-{hex}/` in the project directory, falling back to `/tmp/alethic-*` outside git repos). The orchestrator tracks only summary metrics — verdict strings, confidence floats, and file paths — in its own context. This prevents the exponential context growth that would occur if full solution texts accumulated across iterations, enabling the system to run for many iterations without approaching context limits. Each session contains `session.json` (metadata), `problem.md`, `output.md` (final deliverable), and a `worklog/` subdirectory for intermediate files. An append-only `sessions.jsonl` index at the `.alethic/` root enables querying across sessions.\n\n## Presets\n\nAlethic provides four named presets that control the speed-vs-rigor tradeoff. Each preset configures the iteration budget, revision limit, confidence threshold, and whether extended thinking is enabled. Use `quick` for simple problems where speed matters, `default` for general use, `thorough` for competition-level problems requiring deep verification, and `extreme` for research-grade proofs demanding the highest confidence.\n\n| Preset | Iterations | Revisions | Threshold | Thinking | Think budget | Max tokens |\n|--------|-----------|-----------|-----------|----------|-------------|------------|\n| `quick` | 2 | 1 | 0.85 | off | — | 16,384 |\n| `default` | 5 | 3 | 0.90 | off | — | 16,384 |\n| `thorough` | 8 | 5 | 0.95 | on | 15,000 | 32,768 |\n| `extreme` | 12 | 5 | 0.97 | on | 40,000 | 65,536 |\n\nExplicit flags (CLI) or keyword arguments (Python API) override preset values, so `--preset quick --iterations 4` uses the `quick` preset but with 4 iterations instead of 2.\n\n> **Skill note:** Both `/alethic-solve` and `/alethic-derive` support presets via `-p`/`--preset` for iterations, revisions, budget, and confidence threshold. Temperature and extended thinking are not controllable through the skills (Task sub-agent limitation).\n\n## Claude Code Skills (Recommended)\n\nAlethic provides two Claude Code skills that run natively inside Claude Code, using Task sub-agents for true architectural decoupling. Each Verifier launches as an independent Task with a fresh context window, providing the strongest possible guarantee that it cannot observe the Generator's reasoning process. Both skills include a Beautifier stage that formats accepted outputs into clean LaTeX/Markdown.\n\n- **`/alethic-solve`** — Mathematical problem solving (proofs, computations, theorems)\n- **`/alethic-derive`** — Physics derivations (with physics-specific strategies, error checking, and notation)\n- **`/alethic-scientific-figure`** — Publication-quality scientific figures with AFP color palette and Tufte principles\n\n### Install\n\n#### Via marketplace (recommended)\n\n```bash\nclaude plugins add hyperion-git/alethic\n```\n\n#### Manual installation\n\n```bash\ngit clone https://github.com/hyperion-git/alethic.git\nDEST=~/.claude/plugins/cache/local/alethic/1.0.0\nmkdir -p \"$DEST\"\ncp -r alethic/.claude-plugin alethic/skills \"$DEST/\"\n```\n\nRestart Claude Code. The `/alethic-solve`, `/alethic-derive`, and `/alethic-scientific-figure` commands are now available.\n\n### Usage\n\n```\n# Math\n/alethic-solve \"Prove that sqrt(2) is irrational\"\n/alethic-solve -p thorough \"Prove the Fundamental Theorem of Algebra\"\n/alethic-solve -p quick -i 4 \"Is 17 prime?\"\n/alethic-solve -t 0.95 \"Prove the AM-GM inequality for n variables\"\n\n# Physics\n/alethic-derive \"Derive the energy levels of the quantum harmonic oscillator\"\n/alethic-derive -p thorough \"Derive the hydrogen atom energy spectrum\"\n/alethic-derive -i 8 -r 5 \"Derive the Dirac equation from relativistic quantum mechanics\"\n/alethic-derive \"Show that the Euler-Lagrange equations follow from Hamilton's principle\"\n```\n\n| Flag | Short | Default | Description |\n|------|-------|---------|-------------|\n| `--preset` | `-p` | `default` | Named preset (`quick`, `default`, `thorough`, `extreme`) |\n| `--threshold` | `-t` | 0.90 | Confidence threshold for accepting a solution |\n| `--iterations` | `-i` | 5 | Maximum generate-verify-revise iterations |\n| `--revisions` | `-r` | 3 | Maximum revision attempts per iteration |\n| `--budget` | `-b` | 50 | Total sub-agent call budget |\n\n### How `/alethic-derive` Differs from `/alethic-solve`\n\nBoth skills share the same orchestrator logic (iteration loop, verdict parsing, file-based state, budget tracking). The differences are entirely in the prompt templates:\n\n| Component | `/alethic-solve` | `/alethic-derive` |\n|-----------|----------|-----------|\n| Generator role | \"mathematical problem solver\" | \"theoretical physics derivation solver\" |\n| Strategy catalog | Proof strategies (induction, contradiction, pigeonhole, ...) | Derivation techniques (Lagrangian, perturbation theory, WKB, Feynman diagrams, ...) |\n| Balanced approach | Test counterexamples, boundary cases | Check dimensional consistency, verify limiting cases (ħ→0, c→∞) |\n| Verifier errors | Math errors (sign, off-by-one, circular reasoning) | Math errors + physics errors (dimensional inconsistency, violated conservation laws, wrong sign convention, unjustified approximation) |\n| Correct = | \"Mathematically sound\" | \"Physically and mathematically sound\" |\n| Beautifier symbols | Standard LaTeX math | + `\\hbar`, `\\nabla`, `\\partial`, `\\langle\\rangle`, `\\mathcal{H}`, `\\mathcal{L}`, `\\dagger`, `\\mathrm{d}`, bra-ket |\n| Document structure | Proof strategy → Body → Conclusion ∎ | Setup (system, assumptions) → Derivation → Result → Limiting cases |\n\n### Skill Execution Flow\n\nBoth skills follow the same four-stage flow. First, the **Generator** (an Opus Task sub-agent) reads the problem file, uses Bash for Python computation and WebSearch for theorem/identity lookup, and writes a complete solution to disk. Second, the **Verifier** (a separate Opus Task sub-agent with a fresh context) reads only the problem file and the solution file, performs its independent evaluation, and writes a structured verification report. If issues are found, the **Reviser** (another Opus Task sub-agent) reads the solution and the critique, writes a revised solution, and the cycle repeats with a fresh Verifier. Finally, the **Beautifier** formats the accepted solution into clean LaTeX/Markdown with proper typesetting. All intermediate state lives in `.alethic/{session}/worklog/` (or `/tmp/alethic-*/` outside git repos), and the orchestrator tracks only verdicts and confidence scores in its own context window.\n\n## Python Library\n\nThe Python library provides programmatic access for batch benchmarking, integration into larger pipelines, and fine-grained configuration of the reasoning loop. It requires an `ANTHROPIC_API_KEY` environment variable.\n\n### Install\n\n```bash\npip install -e \".[dev]\"  # from source\n```\n\n### Quick Start\n\n```python\nfrom alethic import MathAgent, PhysicsAgent, AgentConfig\n\n# Math\nagent = MathAgent()  # uses ANTHROPIC_API_KEY env var\nresult = agent.solve(\"Prove that the square root of 2 is irrational.\")\n\nprint(result)            # Full formatted output\nprint(result.solved)     # True/False\nprint(result.confidence) # 0.0-1.0\n\n# Physics\nagent = PhysicsAgent()\nresult = agent.solve(\"Derive the energy levels of the quantum harmonic oscillator.\")\n```\n\n`PhysicsAgent` is a thin subclass of `MathAgent` that injects physics-specific prompt templates into the same Generate-Verify-Revise loop. All orchestrator logic is inherited — only the prompts differ.\n\nThe `AgentResult` returned by `solve()` exposes:\n\n| Property | Type | Description |\n|----------|------|-------------|\n| `solved` | `bool` | `True` if verdict is `CORRECT` and solution exists |\n| `solution` | `str \\| None` | The solution text (best attempt if unsolved) |\n| `verdict` | `Verdict` | `CORRECT`, `MINOR_ISSUES`, `MAJOR_FLAW`, or `UNSOLVED` |\n| `confidence` | `float` | Verifier's confidence (0.0–1.0) |\n| `iterations_used` | `int` | Number of generate-verify cycles used |\n| `total_revisions` | `int` | Total revision attempts across all iterations |\n| `admitted_failure` | `bool` | `True` if all iterations exhausted without success |\n| `elapsed_seconds` | `float` | Wall-clock time for the solve call |\n| `history` | `list[dict]` | Per-phase log entries for debugging |\n\n### CLI\n\n```bash\n# Math (default — no subcommand needed)\nalethic \"Prove that there are infinitely many primes\"\nalethic solve \"Prove that there are infinitely many primes\"  # explicit\n\n# Physics derivations\nalethic derive \"Derive the energy levels of the quantum harmonic oscillator\"\nalethic derive --preset thorough \"Derive the hydrogen atom energy spectrum\"\n\n# Presets\nalethic --preset quick \"Is 17 prime?\"\nalethic --preset thorough \"Prove the Cayley-Hamilton theorem\"\n\n# Override a preset value\nalethic --preset quick --iterations 4 \"Prove the AM-GM inequality\"\n\n# Set confidence threshold\nalethic --confidence-threshold 0.95 \"Prove the Basel problem\"\n\n# From file\nalethic --file problem.txt\nalethic derive --file derivation.txt\n\n# JSON output for pipeline integration\nalethic --json \"Solve x^2 - 5x + 6 = 0\"\n\n# Control iteration budget\nalethic --iterations 3 \"Prove the AM-GM inequality\"\n\n# Extended thinking (deeper reasoning, more tokens per call)\nalethic --thinking --thinking-budget 20000 \"Prove the Basel problem\"\n\n# Disable code execution (pure reasoning mode)\nalethic --no-code \"Prove Euler's identity\"\n```\n\n### Configuration\n\nThe `AgentConfig` dataclass exposes all tunable parameters. The easiest way to get started is with a named preset:\n\n```python\nfrom alethic import MathAgent, PhysicsAgent, AgentConfig\n\n# Quick preset for simple problems\nconfig = AgentConfig.from_preset(\"quick\")\n\n# Thorough preset with a custom iteration limit\nconfig = AgentConfig.from_preset(\"thorough\", max_iterations=10)\n\n# Same config works for both agents\nagent = MathAgent(config=config)\nagent = PhysicsAgent(config=config)  # same presets, physics prompts\n```\n\nFor full control, construct `AgentConfig` directly. Temperature settings follow the Aletheia design: high for creative generation, low for strict verification, moderate for targeted revision.\n\n```python\nconfig = AgentConfig(\n    model=\"claude-opus-4-6\",           # Anthropic model ID\n    max_iterations=5,                   # Max generate-verify-revise cycles\n    max_revisions_per_cycle=3,          # Max revisions before restarting\n    confidence_threshold=0.90,           # Minimum confidence to accept\n    enable_code_execution=True,         # Python sandbox for computation\n    temperature_generator=1.0,          # Creative exploration\n    temperature_verifier=0.2,           # Strict, deterministic evaluation\n    temperature_reviser=0.7,            # Balanced revision\n    max_tokens=16384,                   # Max tokens per API call\n    extended_thinking=False,            # Enable extended thinking\n    thinking_budget=10000,              # Token budget for thinking blocks\n    verbose=True,                       # Print progress to stdout\n)\n\nagent = MathAgent(config=config)\n```\n\n### Extended Thinking\n\nWhen `extended_thinking=True`, the API enables Claude's internal reasoning budget, allowing the model to \"think longer\" on difficult problems before producing output. The Aletheia paper attributes significant performance gains to Gemini's Deep Think mode, which scales inference-time compute for harder problems. Claude's extended thinking provides an analogous capability. Note that the API requires `temperature=1` when thinking is enabled, so the per-subagent temperature settings are overridden in this mode.\n\n### Bundled Examples\n\nThe library includes six example problems spanning undergraduate to intermediate difficulty for quick testing and demonstration.\n\n```bash\n# List available examples\npython -m alethic.examples --list\n\n# Run a specific example\npython -m alethic.examples --pick 1\n\n# Run all examples with custom iteration limit\npython -m alethic.examples --iterations 3\n```\n\n## Module Reference\n\nThe Python library is organized into the following modules, each with a single clear responsibility.\n\n| Module | Purpose |\n|--------|---------|\n| `agent.py` | `MathAgent` orchestrator — runs the full Generate-Verify-Revise loop with false-premise detection and strategic failure admission |\n| `physics_agent.py` | `PhysicsAgent` — thin subclass of `MathAgent` that injects physics-specific prompt templates |\n| `subagents.py` | `generate()`, `verify()`, `revise()` — each wraps a Claude API call with role-specific prompts, temperature, and tool configuration; accepts optional prompt kwargs for domain specialization; includes the tool-use loop (up to 5 rounds) and structured output parsing |\n| `models.py` | Dataclasses: `AgentConfig` (with `PRESETS` and `from_preset()`), `Solution`, `VerificationResult`, `Revision`, `AgentResult`, and the `Verdict` enum |\n| `prompts.py` | System and user prompt templates for the math subagents, plus the balanced prompting addendum |\n| `physics_prompts.py` | Physics-specific prompt templates: derivation strategies, physics error checklist, dimensional/limiting-case balanced addendum |\n| `tools.py` | `execute_python()` sandbox with restricted builtins and module allowlist, `PYTHON_TOOL` schema for the Anthropic API, and `process_tool_calls()` for the tool-use loop |\n| `cli.py` | `argparse`-based CLI entry point (`alethic`) with `solve`/`derive` subcommands, `--preset`, `--confidence-threshold`, `--thinking`, `--json`, and `--file` support |\n| `examples.py` | Six bundled example problems (`python -m alethic.examples`) |\n\n| Skill file | Purpose |\n|------------|---------|\n| `skills/alethic-solve/SKILL.md` | `/alethic-solve` command orchestrator — spawns Opus Task sub-agents with file-based state |\n| `skills/alethic-derive/SKILL.md` | `/alethic-derive` command orchestrator — physics derivations with physics-specific prompts |\n| `skills/alethic-scientific-figure/SKILL.md` | `/alethic-scientific-figure` — publication-quality scientific figures with AFP palette |\n| `.claude-plugin/plugin.json` | Plugin metadata for Claude Code |\n| `.claude-plugin/marketplace.json` | Marketplace manifest for `hyperion-git/alethic` |\n| `skills/alethic-solve/references/*.md` | Standalone math prompt references (generator, verifier, reviser, beautifier) |\n| `skills/alethic-derive/references/*.md` | Standalone physics prompt references (generator, verifier, reviser, beautifier) |\n| `skills/alethic-scientific-figure/references/*.md` | Color palette guide and presentation override rcParams |\n| `skills/alethic-scientific-figure/scripts/*.py` | AFP colormap registration for matplotlib |\n\n## Project Structure\n\n```\nalethic/\n├── .claude-plugin/                 # Claude Code marketplace plugin\n│   ├── plugin.json                 # Plugin metadata (v1.0.0)\n│   └── marketplace.json            # Marketplace manifest\n├── skills/                         # Claude Code skills\n│   ├── alethic-solve/\n│   │   ├── SKILL.md                # /alethic-solve command orchestrator\n│   │   └── references/             # Math prompt references\n│   │       ├── generator.md\n│   │       ├── verifier.md\n│   │       ├── reviser.md\n│   │       └── beautifier.md\n│   ├── alethic-derive/\n│   │   ├── SKILL.md                # /alethic-derive command orchestrator\n│   │   └── references/             # Physics prompt references\n│   │       ├── generator.md\n│   │       ├── verifier.md\n│   │       ├── reviser.md\n│   │       └── beautifier.md\n│   └── alethic-scientific-figure/\n│       ├── SKILL.md                # /alethic-scientific-figure command\n│       ├── evals.json              # Evaluation scenarios\n│       ├── references/             # Color palette + presentation overrides\n│       │   ├── color-palette.md\n│       │   └── presentation-override.md\n│       └── scripts/\n│           └── register_colormaps.py  # AFP colormap registration\n├── src/alethic/                    # Python library\n│   ├── agent.py                    # MathAgent orchestrator\n│   ├── physics_agent.py            # PhysicsAgent (subclass of MathAgent)\n│   ├── subagents.py                # generate(), verify(), revise()\n│   ├── models.py                   # Data models + Verdict enum\n│   ├── prompts.py                  # Math prompt templates\n│   ├── physics_prompts.py          # Physics prompt templates\n│   ├── tools.py                    # Python sandbox + tool-use loop\n│   ├── cli.py                      # CLI entry point (solve/derive subcommands)\n│   └── examples.py                 # Bundled example problems\n└── tests/\n    ├── test_alethic.py             # Core tests (43)\n    ├── test_physics.py             # Physics tests (35)\n    └── test_adversarial_*.py       # Adversarial tests (185)\n```\n\n## Testing\n\nAll tests use mocked API responses and require no API key. The test suite covers data models, prompt content validation (math and physics), sandbox execution (including timeout enforcement and import restrictions), structured output parsing for all verdict types, preset creation and overrides, configurable confidence thresholds, CLI argument parsing (including `solve`/`derive` subcommands, `--preset`, and `--confidence-threshold`), physics prompt injection via kwargs, `PhysicsAgent` instantiation and integration, and end-to-end flows for solved, revised, and failed problems. Adversarial tests cover edge cases in CLI parsing, backward compatibility, prompt kwargs, sandbox allowlist, and skill file structure.\n\n```bash\n# Run all 263 tests\npytest\n\n# With coverage report\npytest --cov=alethic\n\n# Lint and format\nruff check src tests\nruff format src tests\n```\n\n## Known Limitations\n\n**Single-model verification.** Both the Generator and Verifier use the same underlying model (Claude Opus). While decoupling prevents the Verifier from following the Generator's specific reasoning path, both models share the same training data and potential blind spots. The Aletheia paper uses the same single-model approach (Gemini for all roles) and notes that decoupling alone provides substantial gains, but cross-model verification could further improve robustness.\n\n**No best-of-N sampling.** Each iteration generates a single candidate solution. The Aletheia system benefits from sampling multiple candidates in parallel and selecting the best. The Python library could be extended with parallel generation; the skill architecture does not currently support this.\n\n**Temperature override with extended thinking.** The Anthropic API requires `temperature=1` when extended thinking is enabled, which overrides the carefully tuned per-subagent temperatures. In this mode, the Verifier loses its low-temperature strictness, potentially reducing verification precision. The prompt instructions partially compensate for this by emphasizing skepticism and rigor.\n\n**Skill temperature limitations.** Claude Code Task sub-agents run at the default temperature. The per-subagent temperature tuning (T=1.0, T=0.2, T=0.7) is only available through the Python library. The skill relies on prompt instructions to approximate these behavioral differences.\n\n**Context accumulation in skill mode.** Without `context:fork`, all Task call/response pairs accumulate in the main conversation. The file-based state design mitigates this by keeping solution text out of the orchestrator's context, but very long runs (8+ iterations) may approach context limits.\n\n**Beautifier runs post-verification.** The Beautifier formats the accepted solution after the final verification pass. While it is constrained to formatting-only changes (converting text math to LaTeX, adding section headers), there is no re-verification of the beautified output. The raw verified solution is preserved at `worklog/best_solution.md` as a fallback.\n\n**Session storage.** Sessions are stored in `.alethic/` in the project directory (falls back to `/tmp/alethic-*` outside git repos). Intermediate files live in `worklog/` subdirectories and can be pruned with `rm -rf .alethic/*/worklog/`. Add `.alethic/` to your `.gitignore`.\n\n## License\n\nMIT\n"
      },
      "plugins": [
        {
          "name": "alethic",
          "description": "Reasoning agent for mathematics and physics with Generate-Verify-Revise loop (inspired by DeepMind's Aletheia), plus scientific figure generation",
          "version": "1.0.0",
          "source": "./",
          "author": {
            "name": "Alexander Friedrich"
          },
          "categories": [],
          "install_commands": [
            "/plugin marketplace add hyperion-git/alethic",
            "/plugin install alethic@alethic"
          ]
        }
      ]
    }
  ]
}