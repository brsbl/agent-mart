{
  "author": {
    "id": "hyperion-git",
    "display_name": "Alex Friedrich",
    "avatar_url": "https://avatars.githubusercontent.com/u/7377256?u=d66d6bd8868dcea467fdfe525aa8132268af45ee&v=4"
  },
  "marketplaces": [
    {
      "name": "alethic",
      "version": null,
      "description": "Reasoning agent for mathematics and physics with Generate-Verify-Revise loop, plus scientific figure generation",
      "repo_full_name": "hyperion-git/alethic",
      "repo_url": "https://github.com/hyperion-git/alethic",
      "repo_description": "An implementation of the Google Aletheia architecture based on publications for use as a skill in Claude Code or via API",
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2026-02-17T20:52:22Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"alethic\",\n  \"description\": \"Reasoning agent for mathematics and physics with Generate-Verify-Revise loop, plus scientific figure generation\",\n  \"owner\": {\n    \"name\": \"Alexander Friedrich\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"alethic\",\n      \"description\": \"Reasoning agent for mathematics and physics with Generate-Verify-Revise loop (inspired by DeepMind's Aletheia), plus scientific figure generation\",\n      \"version\": \"2.0.0\",\n      \"source\": \"./\",\n      \"author\": {\n        \"name\": \"Alexander Friedrich\"\n      }\n    }\n  ]\n}\n",
        ".claude-plugin/plugin.json": "{\n  \"name\": \"alethic\",\n  \"description\": \"Reasoning agent for mathematics and physics with Generate-Verify-Revise loop (inspired by DeepMind's Aletheia), textbook-style proof/derivation converter, plus publication-quality scientific figure generation\",\n  \"version\": \"2.0.0\",\n  \"author\": {\n    \"name\": \"Alexander Friedrich\"\n  },\n  \"homepage\": \"https://github.com/hyperion-git/alethic\",\n  \"repository\": \"https://github.com/hyperion-git/alethic\",\n  \"license\": \"MIT\",\n  \"keywords\": [\"math\", \"physics\", \"reasoning\", \"verification\", \"theorem-proving\", \"aletheia\", \"figures\", \"plotting\"]\n}\n",
        "README.md": "# Alethic\n\nA reasoning agent for mathematics and physics inspired by [Google DeepMind's Aletheia](https://arxiv.org/abs/2602.10177), built on **Claude (Opus 4.6)**. Alethic implements a Generate-Verify-Revise loop with decoupled verification — a key architectural insight from DeepMind's design — to produce rigorous mathematical proofs and physics derivations with high confidence.\n\nAvailable as Claude Code skills (`/alethic-solve` for math, `/alethic-derive` for physics, `/alethic-scientific-figure` for scientific figures) or as a standalone **Python library** with CLI.\n\n## Background and Motivation\n\nIn February 2026, Google DeepMind introduced Aletheia, a multi-agent system that achieved 95% accuracy on IMO-ProofBench Advanced and autonomously resolved open Erdős conjectures. The system's central innovation lies in its separation of solution generation from solution verification: by preventing the verifier from observing the generator's intermediate reasoning traces, Aletheia avoids the confidence inflation that arises when a model evaluates its own chain of thought.\n\nWhen a verifier has access to the generator's internal reasoning, it tends to follow the same logical path and confirm flawed steps with unwarranted certainty. Decoupling forces the verifier to reconstruct and independently assess each argument from the final output alone.\n\nAlethic translates this decoupled verification approach to Claude's API. The project implements the same three-subagent loop — Generator, Verifier, and Reviser — with each role instantiated as an independent API call (in the Python library) or a separate Task sub-agent with a fresh context window (in the Claude Code skill). The orchestrator logic is domain-neutral; only the prompt templates differ between math (`MathAgent`, `/alethic-solve`) and physics (`PhysicsAgent`, `/alethic-derive`). The result is a system that can solve mathematical problems and derive physics results with verified confidence, or honestly admit failure when it cannot.\n\n### Key References\n\n- T. Shoeybi et al., \"Towards Autonomous Mathematics Research,\" [arXiv:2602.10177](https://arxiv.org/abs/2602.10177) (Feb 2026). The primary Aletheia paper describing the Generate-Verify-Revise architecture.\n- J. Huang et al., \"Accelerating Scientific Research with Gemini,\" [arXiv:2602.03837](https://arxiv.org/abs/2602.03837) (Feb 2026). Companion paper on Gemini's scientific reasoning capabilities.\n- R. Anil et al., \"Semi-Autonomous Mathematics Discovery,\" [arXiv:2601.22401](https://arxiv.org/abs/2601.22401) (Jan 2026). The Erdős conjecture study demonstrating autonomous mathematical discovery.\n\n## Architecture\n\nAlethic's reasoning loop proceeds through three distinct phases that repeat until the solution is verified or the iteration budget is exhausted.\n\n```\n┌─────────────────────────────────────────────────────────┐\n│                    Orchestrator Loop                     │\n│                                                         │\n│   ┌───────────┐    ┌──────────┐    ┌──────────┐       │\n│   │ Generator  │───▶│ Verifier │───▶│ Reviser  │──┐    │\n│   │  (T=1.0)  │    │  (T=0.2) │    │  (T=0.7) │  │    │\n│   └───────────┘    └──────────┘    └──────────┘  │    │\n│        ▲                                          │    │\n│        └──────────────────────────────────────────┘    │\n│                                                         │\n│   Terminates when: CORRECT (≥ threshold) OR max iters   │\n└─────────────────────────────────────────────────────────┘\n```\n\nThe **Generator** produces a candidate solution at high temperature (T=1.0) to encourage creative exploration of proof strategies. The **Verifier** then evaluates that solution at low temperature (T=0.2) for strict, deterministic assessment. Critically, the Verifier receives only the problem statement and the final written solution — never the Generator's thinking traces, tool outputs, or intermediate reasoning. If the Verifier identifies issues, the **Reviser** receives both the solution and the Verifier's structured critique, producing an improved version at moderate temperature (T=0.7) that balances faithfulness to the original with the flexibility to restructure flawed arguments.\n\nThe loop terminates under one of three conditions: the Verifier issues a `CORRECT` verdict with confidence at or above the configured threshold (default 90%), the maximum number of iterations is reached (strategic failure admission), or the Verifier detects that the problem's premise is false and halts early with an explanation.\n\n### Information Flow and Decoupling\n\nThe following sequence diagram illustrates the critical decoupling boundary. The Generator's internal reasoning — thinking traces, tool call results, intermediate drafts — never crosses to the Verifier. Only the final solution text is passed, forcing the Verifier to evaluate the argument on its own merits.\n\n```mermaid\nsequenceDiagram\n    participant O as Orchestrator\n    participant G as Generator\n    participant V as Verifier\n    participant R as Reviser\n\n    O->>G: problem statement\n    Note right of G: Reasoning traces,<br/>tool calls, thinking\n    G->>O: solution text\n\n    Note over O: Only solution text<br/>crosses to Verifier\n\n    O->>V: problem + solution text\n    Note right of V: Independent evaluation<br/>(no Generator context)\n    V->>O: verdict + confidence + critique\n\n    alt CORRECT with confidence >= threshold\n        O->>O: Accept solution\n    else Needs revision\n        O->>R: solution + critique\n        R->>O: revised solution\n        O->>V: problem + revised solution\n        Note right of V: Fresh evaluation<br/>(no prior context)\n        V->>O: verdict + confidence + critique\n    end\n```\n\n### The Three Subagents\n\nEach subagent is instantiated as an independent Claude API call with role-specific system prompts, temperature settings, and tool access. This separation ensures that no subagent can observe another's internal state.\n\n**Generator.** The Generator's task is to produce a complete, self-contained solution — a mathematical proof (in `/alethic-solve` / `MathAgent`) or a physics derivation (in `/alethic-derive` / `PhysicsAgent`). Its system prompt instructs it to restate the problem, select a strategy explicitly (proof techniques for math, derivation methods like Lagrangian mechanics or perturbation theory for physics), justify every inference, and use precise notation. When balanced prompting is enabled (the default), an addendum directs the Generator to first check whether the problem might be ill-posed: for math, this means testing small cases and boundary conditions; for physics, checking dimensional consistency and known limiting cases. This anti-confirmation-bias technique, adapted from the Aletheia design, reduces the risk of the model anchoring prematurely on a flawed approach. The Generator has access to a sandboxed Python environment with SymPy (pre-imported as `sp`) for computational verification of intermediate results. A **SymPy Verification Toolkit** section in the prompt provides domain-specific recipes: math generators are guided toward `sp.simplify`, `sp.integrate`, `sp.series`, `sp.solve`; physics generators additionally use `sp.dsolve`, `sympy.physics.units`, `sympy.physics.quantum`, and special functions.\n\n**Verifier.** The Verifier is the architectural cornerstone of the system. Its system prompt establishes strict independence: it must evaluate the solution purely on its written merits, checking every logical step, re-deriving computations independently, and flagging common mathematical errors including sign mistakes, off-by-one errors, vacuous truth claims, circular reasoning, non-exhaustive case analysis, and incorrect theorem application. A **Mandatory SymPy Re-derivation** section requires the Verifier to independently verify every non-trivial algebraic step using SymPy (`sp.simplify(claimed - rederived) == 0`), re-compute integrals and sums, and check equation solutions. If SymPy cannot confirm a claimed result, this is treated as a RED FLAG warranting at least `[MAJOR]` severity. Physics verifiers additionally check ODE solutions (`sp.dsolve`), eigenvalue problems, limiting cases via symbolic substitution, and dimensional consistency (`sympy.physics.units`). The Verifier produces a structured output containing a verdict (`correct`, `minor_issues`, `major_flaw`, or `unsolved`), a numerical confidence score calibrated against explicit benchmarks (0.95-1.0 for fully verified solutions, below 0.50 for likely errors), a step-by-step critique, a reason field for false-premise detection, and a list of specific issues. The confidence threshold (default 90%, configurable via `confidence_threshold`) means that even a `correct` verdict at lower confidence is treated as requiring revision — the Verifier must be genuinely certain.\n\n**Reviser.** When the Verifier identifies issues, the Reviser receives the original solution alongside the full structured critique. Its prompt instructs it to distinguish between minor issues (which can be patched in place) and major flaws (which require a fundamentally different proof strategy). The Reviser preserves parts of the solution that the Verifier confirmed as sound and provides explicit justification for each change. If it believes the critique itself is incorrect, it may argue back with evidence — but the subsequent re-verification by a fresh Verifier instance has the final word.\n\n### Verdict Types and Actions\n\n| Verdict | Condition | Action |\n|---------|-----------|--------|\n| `CORRECT` | Confidence ≥ threshold | Accept the solution and return it to the user |\n| `CORRECT` | Confidence < threshold | Treat as uncertain — send to Reviser for strengthening |\n| `MINOR_ISSUES` | — | Send to Reviser with the critique |\n| `MAJOR_FLAW` | — | Revise if attempts remain, otherwise restart from Generator |\n| `UNSOLVED` | Reason field populated | Problem premise is false — halt and explain |\n| `UNSOLVED` | No reason | Cannot solve — restart from Generator or admit failure |\n\n### Decision Flowchart\n\nThe full control flow, including revision loops, iteration restarts, and termination conditions:\n\n```mermaid\nflowchart TD\n    Start([Problem]) --> Gen[\"Generate (T=1.0)\"]\n    Gen --> Ver[\"Verify (T=0.2)\"]\n    Ver --> D{Verdict?}\n\n    D -->|\"CORRECT ≥ threshold\"| Accept[Accept solution]\n    D -->|\"CORRECT < threshold\"| Rev[\"Revise (T=0.7)\"]\n    D -->|MINOR_ISSUES| Rev\n    D -->|MAJOR_FLAW| MF{Revisions left?}\n    D -->|\"UNSOLVED + reason\"| FP[Premise is false]\n    D -->|UNSOLVED| Iter{Iterations left?}\n\n    MF -->|Yes| Rev\n    MF -->|No| Iter\n\n    Rev --> ReVer[\"Re-verify (T=0.2)\"]\n    ReVer --> RD{Verdict?}\n\n    RD -->|\"CORRECT ≥ threshold\"| Accept\n    RD -->|\"CORRECT < threshold\"| MoreRev{Revisions left?}\n    RD -->|MINOR_ISSUES| MoreRev\n    RD -->|MAJOR_FLAW| Iter\n    RD -->|\"UNSOLVED + reason\"| FP\n    RD -->|UNSOLVED| Iter\n\n    MoreRev -->|Yes| Rev\n    MoreRev -->|No| Iter\n\n    Iter -->|Yes| Gen\n    Iter -->|No| Fail[Admit failure]\n\n    Accept --> B[\"Beautify (skill only)\"]\n    B --> Solved([SOLVED])\n    FP --> Halt([HALT - premise false])\n    Fail --> BF[\"Beautify best effort (skill only)\"]\n    BF --> Unsolved([UNSOLVED - best effort])\n```\n\n### Verifier Output Format\n\nThe Verifier produces structured output that is parsed via regex with independent extraction per field. This design ensures that partial or malformed output still yields usable information — if the verdict is parseable but the confidence is not, the system defaults to 0.5 rather than failing entirely.\n\n```\nVERDICT: correct | minor_issues | major_flaw | unsolved\nCONFIDENCE: 0.0 to 1.0\n\nCRITIQUE:\n[Step-by-step evaluation of the solution]\n\nREASON: [Why the premise is false, or \"N/A\"]\n\nISSUES:\n- [CRITICAL] Issue requiring fundamental rework\n- [MAJOR] Serious gap or error\n- [MINOR] Small imprecision or stylistic concern\n(Tag each issue with severity. Write \"None\" if there are no issues)\n\nSECTION CONFIDENCES:\n- [section name]: [0.0-1.0] [optional note]\n(Omit this section if the solution is too short to decompose into sections)\n```\n\n## Design Principles\n\n**Decoupled verification.** The separation of Generator and Verifier contexts is the single most important architectural decision. In the Python library, decoupling is enforced at the API level: the `verify()` function receives only the problem string and the solution text, never the Generator's response object or thinking blocks. In the Claude Code skill, decoupling is enforced structurally — each Verifier runs as a separate Task sub-agent that launches with a fresh context window and physically cannot access the Generator's reasoning.\n\n**Balanced prompting.** Before committing to a strategy, the Generator is instructed to actively look for reasons the problem might be ill-posed or the approach might fail. For math, this means searching for counterexamples and testing boundary conditions; for physics, checking dimensional consistency and verifying known limiting cases. This technique, adapted from the Aletheia paper, counteracts the confirmation bias that arises when a language model generates a solution: once a model begins pursuing a particular approach, it tends to rationalize intermediate steps rather than question the approach itself. Balanced prompting is enabled by default and can be disabled via `--no-balanced` (CLI) or `balanced=False` (Python API).\n\n**Strategic failure admission.** When the Verifier cannot approve any solution after exhausting the iteration budget, the system returns `Verdict.UNSOLVED` along with the highest-confidence solution encountered during the run. This honest failure mode prevents the agent from hallucinating confidence in an unverified answer. The `admitted_failure` flag on the result object distinguishes between problems that were identified as having a false premise (a valid finding) and problems that the agent simply could not solve.\n\n**Confidence calibration.** The Verifier outputs a numerical confidence score between 0.0 and 1.0. The system prompt instructs the Verifier to be skeptical and to assign confidence proportional to the rigor of its own verification. The configurable confidence threshold (default 90%) means that even a `CORRECT` verdict at lower confidence is treated as uncertain and sent for revision, preventing the common failure mode where a model assigns high confidence to every output regardless of actual certainty.\n\n**Sandboxed code execution with SymPy.** Both the Generator and Verifier have access to a Python sandbox for computational verification. Code runs in a **child subprocess** for process-level isolation; restricted builtins and an allowlist of importable modules (math, sympy, numpy, scipy, mpmath, and related packages) provide defense-in-depth inside the child. SymPy is pre-imported as `sp`, and the subagent prompts include domain-specific SymPy recipes: Generators get an advisory \"SymPy Verification Toolkit\" (verify key algebraic steps symbolically); Verifiers get an imperative \"Mandatory SymPy Re-derivation\" (must independently re-derive with SymPy, RED FLAG escalation if SymPy disagrees). Physics prompts additionally reference `sympy.physics.units`, `sympy.physics.quantum`, `sp.dsolve`, and special functions. Timeouts are enforced at two levels: `signal.SIGALRM` in the child process and `subprocess.run(timeout=)` in the parent. The sandbox is thread-safe and works from both main threads and `ThreadPoolExecutor` workers.\n\n**File-based state (skill only).** In the Claude Code skill, all solutions, verifications, and revisions are written to files in a session directory (`.alethic/{slug}-{date}-{hex}/` in the project directory, falling back to `/tmp/alethic-*` outside git repos). The orchestrator tracks only summary metrics — verdict strings, confidence floats, and file paths — in its own context. This prevents the exponential context growth that would occur if full solution texts accumulated across iterations, enabling the system to run for many iterations without approaching context limits. Each session contains `session.json` (metadata), `problem.md`, `output.md` (final deliverable), and a `worklog/` subdirectory for intermediate files. An append-only `sessions.jsonl` index at the `.alethic/` root enables querying across sessions.\n\n## Presets\n\nAlethic provides four named presets that control the speed-vs-rigor tradeoff. Each preset configures the iteration budget, revision limit, confidence threshold, and whether extended thinking is enabled. Use `quick` for simple problems where speed matters, `default` for general use, `thorough` for competition-level problems requiring deep verification, and `extreme` for research-grade proofs demanding the highest confidence.\n\n| Preset | Iterations | Revisions | Threshold | Thinking | Think budget | Max tokens |\n|--------|-----------|-----------|-----------|----------|-------------|------------|\n| `quick` | 2 | 1 | 0.85 | off | — | 16,384 |\n| `default` | 5 | 3 | 0.90 | off | — | 16,384 |\n| `thorough` | 8 | 5 | 0.95 | on | 15,000 | 32,768 |\n| `extreme` | 12 | 5 | 0.97 | on | 40,000 | 65,536 |\n\nExplicit flags (CLI) or keyword arguments (Python API) override preset values, so `--preset quick --iterations 4` uses the `quick` preset but with 4 iterations instead of 2.\n\n> **Skill note:** Both `/alethic-solve` and `/alethic-derive` support presets via `-p`/`--preset` for iterations, revisions, budget, and confidence threshold. Temperature and extended thinking are not controllable through the skills (Task sub-agent limitation).\n\n## Claude Code Skills (Recommended)\n\nAlethic provides two Claude Code skills that run natively inside Claude Code, using Task sub-agents for true architectural decoupling. Each Verifier launches as an independent Task with a fresh context window, providing the strongest possible guarantee that it cannot observe the Generator's reasoning process. Both skills include a Beautifier stage that formats accepted outputs into clean LaTeX/Markdown.\n\n- **`/alethic-solve`** — Mathematical problem solving (proofs, computations, theorems)\n- **`/alethic-derive`** — Physics derivations (with physics-specific strategies, error checking, and notation)\n- **`/alethic-scientific-figure`** — Publication-quality scientific figures with AFP color palette and Tufte principles\n\n### Install\n\n#### Via marketplace (recommended)\n\n```bash\nclaude plugins add hyperion-git/alethic\n```\n\n#### Manual installation\n\n```bash\ngit clone https://github.com/hyperion-git/alethic.git\nDEST=~/.claude/plugins/cache/local/alethic/2.0.0\nmkdir -p \"$DEST\"\ncp -r alethic/.claude-plugin alethic/skills \"$DEST/\"\n```\n\nRestart Claude Code. The `/alethic-solve`, `/alethic-derive`, and `/alethic-scientific-figure` commands are now available.\n\n### Usage\n\n```\n# Math\n/alethic-solve \"Prove that sqrt(2) is irrational\"\n/alethic-solve -p thorough \"Prove the Fundamental Theorem of Algebra\"\n/alethic-solve -p quick -i 4 \"Is 17 prime?\"\n/alethic-solve -t 0.95 \"Prove the AM-GM inequality for n variables\"\n/alethic-solve -B 3 \"Prove the Cayley-Hamilton theorem\"\n/alethic-solve --textbook \"Prove sqrt(2) is irrational\"\n\n# Physics\n/alethic-derive \"Derive the energy levels of the quantum harmonic oscillator\"\n/alethic-derive -p thorough \"Derive the hydrogen atom energy spectrum\"\n/alethic-derive -i 8 -r 5 \"Derive the Dirac equation from relativistic quantum mechanics\"\n/alethic-derive --textbook \"Derive harmonic oscillator energy levels\"\n\n# Additional flags\n/alethic-solve --no-balanced \"Prove sqrt(2) is irrational\"      # skip counterexample check\n/alethic-solve --file problem.md                                 # read problem from file\n/alethic-solve -q -p thorough \"Prove the Cayley-Hamilton theorem\" # quiet mode (no dashboard)\n/alethic-solve --json \"Is 17 prime?\"                             # JSON output\n/alethic-solve --model sonnet \"Prove Fermat's little theorem\"    # use Sonnet for sub-agents\n```\n\n| Flag | Short | Default | Description |\n|------|-------|---------|-------------|\n| `--preset` | `-p` | `default` | Named preset (`quick`, `default`, `thorough`, `extreme`) |\n| `--threshold` | `-t` | 0.90 | Confidence threshold for accepting a solution |\n| `--iterations` | `-i` | 5 | Maximum generate-verify-revise iterations |\n| `--revisions` | `-r` | 3 | Maximum revision attempts per iteration |\n| `--budget` | `-b` | 50 | Total sub-agent call budget |\n| `--best-of` | `-B` | 2 | Candidates per iteration (best-of-N sampling) |\n| `--textbook` | | off | Textbook-style output (Planner → Writer × N → Fidelity) |\n| `--no-balanced` | `-n` | off | Disable balanced prompting addendum in Generator |\n| `--file` | `-f` | — | Read problem from file instead of argument |\n| `--quiet` | `-q` | off | Suppress monitoring dashboard and iteration output |\n| `--json` | `-j` | off | JSON output (for pipeline integration) |\n| `--model` | `-m` | `opus` | Model for sub-agents (`opus`, `sonnet`, `haiku`) |\n\n### How `/alethic-derive` Differs from `/alethic-solve`\n\nBoth skills are thin ~73-line configurators that load a shared orchestrator (`skills/alethic-common/orchestrator.md`, ~729 lines). The orchestrator uses domain-variable placeholders (`{noun}`, `{domain}`, `{verb}`, etc.) and reads prompt templates from each skill's `references/*.md` at runtime. The differences are entirely in the domain configuration and prompt templates:\n\n| Component | `/alethic-solve` | `/alethic-derive` |\n|-----------|----------|-----------|\n| Generator role | \"mathematical problem solver\" | \"theoretical physics derivation solver\" |\n| Strategy catalog | Proof strategies (induction, contradiction, pigeonhole, ...) | Derivation techniques (Lagrangian, perturbation theory, WKB, Feynman diagrams, ...) |\n| Balanced approach | Test counterexamples, boundary cases | Check dimensional consistency, verify limiting cases (ħ→0, c→∞) |\n| Verifier errors | Math errors (sign, off-by-one, circular reasoning) | Math errors + physics errors (dimensional inconsistency, violated conservation laws, wrong sign convention, unjustified approximation) |\n| Correct = | \"Mathematically sound\" | \"Physically and mathematically sound\" |\n| Beautifier symbols | Standard LaTeX math | + `\\hbar`, `\\nabla`, `\\partial`, `\\langle\\rangle`, `\\mathcal{H}`, `\\mathcal{L}`, `\\dagger`, `\\mathrm{d}`, bra-ket |\n| Document structure | Proof strategy → Body → Conclusion ∎ | Setup (system, assumptions) → Derivation → Result → Limiting cases |\n\n### Skill Execution Flow\n\nBoth skills follow the same six-step flow defined in the shared orchestrator. The thin SKILL.md sets domain variables and loads `skills/alethic-common/orchestrator.md`, which drives the entire session. **Step 1** (Setup) parses flags, creates the session directory, and writes `session.json`. **Step 2** (Main Loop) iterates: the **Generator** (an Opus Task sub-agent) reads the problem and writes a solution; the **Verifier** (a separate Opus Task with a fresh context) reads only the problem and solution files, producing a structured verdict with `HAS_CRITICAL` tracking; the **Reviser** handles critique-based fixes with section-targeted revision. When best-of-N > 1, multiple candidates are generated and verified per iteration, with the best selected. **Step 3** handles failure admission. **Step 4** formats output (simple Beautifier or textbook pipeline). **Step 5** presents results (including `--json` mode). **Step 6** finalizes session metadata (events.jsonl, elapsed_seconds, failed_approaches). All intermediate state lives in `.alethic/{session}/worklog/`, and the orchestrator tracks only verdicts and confidence scores in its own context window.\n\n## Python Library\n\nThe Python library provides programmatic access for batch benchmarking, integration into larger pipelines, and fine-grained configuration of the reasoning loop. It requires an `ANTHROPIC_API_KEY` environment variable.\n\n### Install\n\n```bash\npip install -e \".[dev]\"  # from source\n```\n\n### Quick Start\n\n```python\nfrom alethic import MathAgent, PhysicsAgent, AgentConfig\n\n# Math\nagent = MathAgent()  # uses ANTHROPIC_API_KEY env var\nresult = agent.solve(\"Prove that the square root of 2 is irrational.\")\n\nprint(result)            # Full formatted output\nprint(result.solved)     # True/False\nprint(result.confidence) # 0.0-1.0\n\n# Physics\nagent = PhysicsAgent()\nresult = agent.solve(\"Derive the energy levels of the quantum harmonic oscillator.\")\n```\n\n`PhysicsAgent` is a thin subclass of `MathAgent` that injects physics-specific prompt templates into the same Generate-Verify-Revise loop. All orchestrator logic is inherited — only the prompts differ.\n\nThe `AgentResult` returned by `solve()` exposes:\n\n| Property | Type | Description |\n|----------|------|-------------|\n| `solved` | `bool` | `True` if verdict is `CORRECT` and solution exists |\n| `solution` | `str \\| None` | The solution text (best attempt if unsolved) |\n| `verdict` | `Verdict` | `CORRECT`, `MINOR_ISSUES`, `MAJOR_FLAW`, or `UNSOLVED` |\n| `confidence` | `float` | Verifier's confidence (0.0–1.0) |\n| `iterations_used` | `int` | Number of generate-verify cycles used |\n| `total_revisions` | `int` | Total revision attempts across all iterations |\n| `admitted_failure` | `bool` | `True` if all iterations exhausted without success |\n| `elapsed_seconds` | `float` | Wall-clock time for the solve call |\n| `events` | `list[AgentEvent]` | Structured event log for debugging and analysis |\n| `failed_approaches` | `list[str]` | One-line summaries of strategies that failed |\n| `history` | `list[dict]` | *Deprecated* -- backward-compatible dict view of events |\n\n### CLI\n\n```bash\n# Math (default — no subcommand needed)\nalethic \"Prove that there are infinitely many primes\"\nalethic solve \"Prove that there are infinitely many primes\"  # explicit\n\n# Physics derivations\nalethic derive \"Derive the energy levels of the quantum harmonic oscillator\"\nalethic derive --preset thorough \"Derive the hydrogen atom energy spectrum\"\n\n# Presets\nalethic --preset quick \"Is 17 prime?\"\nalethic --preset thorough \"Prove the Cayley-Hamilton theorem\"\n\n# Override a preset value\nalethic --preset quick --iterations 4 \"Prove the AM-GM inequality\"\n\n# Set confidence threshold\nalethic --confidence-threshold 0.95 \"Prove the Basel problem\"\n\n# From file\nalethic --file problem.txt\nalethic derive --file derivation.txt\n\n# JSON output for pipeline integration\nalethic --json \"Solve x^2 - 5x + 6 = 0\"\n\n# Control iteration budget\nalethic --iterations 3 \"Prove the AM-GM inequality\"\n\n# Extended thinking (deeper reasoning, more tokens per call)\nalethic --thinking --thinking-budget 20000 \"Prove the Basel problem\"\n\n# Disable code execution (pure reasoning mode)\nalethic --no-code \"Prove Euler's identity\"\n```\n\n### Configuration\n\nThe `AgentConfig` dataclass exposes all tunable parameters. The easiest way to get started is with a named preset:\n\n```python\nfrom alethic import MathAgent, PhysicsAgent, AgentConfig\n\n# Quick preset for simple problems\nconfig = AgentConfig.from_preset(\"quick\")\n\n# Thorough preset with a custom iteration limit\nconfig = AgentConfig.from_preset(\"thorough\", max_iterations=10)\n\n# Same config works for both agents\nagent = MathAgent(config=config)\nagent = PhysicsAgent(config=config)  # same presets, physics prompts\n```\n\nFor full control, construct `AgentConfig` directly. Temperature settings follow the Aletheia design: high for creative generation, low for strict verification, moderate for targeted revision.\n\n```python\nconfig = AgentConfig(\n    model=\"claude-opus-4-6\",           # Anthropic model ID\n    max_iterations=5,                   # Max generate-verify-revise cycles\n    max_revisions_per_cycle=3,          # Max revisions before restarting\n    confidence_threshold=0.90,           # Minimum confidence to accept\n    enable_code_execution=True,         # Python sandbox for computation\n    temperature_generator=1.0,          # Creative exploration\n    temperature_verifier=0.2,           # Strict, deterministic evaluation\n    temperature_reviser=0.7,            # Balanced revision\n    max_tokens=16384,                   # Max tokens per API call\n    extended_thinking=False,            # Enable extended thinking\n    thinking_budget=10000,              # Token budget for thinking blocks\n    verbose=True,                       # Print progress to stdout\n)\n\nagent = MathAgent(config=config)\n```\n\n### Extended Thinking\n\nWhen `extended_thinking=True`, the API enables Claude's internal reasoning budget, allowing the model to \"think longer\" on difficult problems before producing output. The Aletheia paper attributes significant performance gains to Gemini's Deep Think mode, which scales inference-time compute for harder problems. Claude's extended thinking provides an analogous capability. Note that the API requires `temperature=1` when thinking is enabled, so the per-subagent temperature settings are overridden in this mode.\n\n### Bundled Examples\n\nThe library includes six example problems spanning undergraduate to intermediate difficulty for quick testing and demonstration.\n\n```bash\n# List available examples\npython -m alethic.examples --list\n\n# Run a specific example\npython -m alethic.examples --pick 1\n\n# Run all examples with custom iteration limit\npython -m alethic.examples --iterations 3\n```\n\n## Module Reference\n\nThe Python library is organized into the following modules, each with a single clear responsibility.\n\n| Module | Purpose |\n|--------|---------|\n| `agent.py` | `MathAgent` orchestrator — runs the full Generate-Verify-Revise loop with false-premise detection, strategic failure admission, `RunState`/`EventLog` tracking, and failed approach tracking |\n| `physics_agent.py` | `PhysicsAgent` — thin subclass of `MathAgent` that injects physics-specific prompt templates |\n| `subagents.py` | `generate()`, `verify()`, `revise()` — each wraps a Claude API call with role-specific prompts, temperature, and tool configuration; accepts optional prompt kwargs for domain specialization; includes the tool-use loop (up to 5 rounds) and structured output parsing |\n| `models.py` | Dataclasses: `AgentConfig` (with `PRESETS` and `from_preset()`), `Solution`, `VerificationResult`, `Revision`, `AgentResult`, and the `Verdict` enum; also `IssueSeverity`, `Issue`, `SectionConfidence`, `EventType`, and `AgentEvent` types |\n| `prompts.py` | System and user prompt templates for the math subagents, plus balanced prompting addendum and SymPy guidance |\n| `physics_prompts.py` | Physics-specific prompt templates: derivation strategies, physics error checklist, dimensional/limiting-case balanced addendum, and SymPy/`sympy.physics` guidance |\n| `tools.py` | `execute_python()` sandbox with restricted builtins and module allowlist, `PYTHON_TOOL` schema (highlights SymPy as `sp`), and `process_tool_calls()` for the tool-use loop |\n| `cli.py` | `argparse`-based CLI entry point (`alethic`) with `solve`/`derive` subcommands, `--preset`, `--confidence-threshold`, `--thinking`, `--json`, and `--file` support |\n| `examples.py` | Six bundled example problems (`python -m alethic.examples`) |\n\n| Skill file | Purpose |\n|------------|---------|\n| `skills/alethic-common/orchestrator.md` | Shared GVR loop orchestrator — parameterized by domain, reads prompts from `references/*.md`, handles session management, dashboard, textbook pipeline, event logging, and all CLI flags |\n| `skills/alethic-solve/SKILL.md` | `/alethic-solve` thin configurator — sets math domain variables, balanced approach addendum, loads shared orchestrator |\n| `skills/alethic-derive/SKILL.md` | `/alethic-derive` thin configurator — sets physics domain variables, balanced approach addendum, loads shared orchestrator |\n| `skills/alethic-textbook/SKILL.md` | `/alethic-textbook` — standalone textbook-style converter for existing sessions or raw .md files |\n| `skills/alethic-scientific-figure/SKILL.md` | `/alethic-scientific-figure` — publication-quality scientific figures with AFP palette |\n| `.claude-plugin/plugin.json` | Plugin metadata for Claude Code |\n| `.claude-plugin/marketplace.json` | Marketplace manifest for `hyperion-git/alethic` |\n| `skills/alethic-solve/references/*.md` | Authoritative math prompt templates (generator, verifier, reviser, beautifier, textbook planner/writer/fidelity) with SymPy verification toolkit/mandatory re-derivation — read by orchestrator at runtime |\n| `skills/alethic-derive/references/*.md` | Authoritative physics prompt templates (generator, verifier, reviser, beautifier, textbook planner/writer/fidelity) with SymPy verification toolkit/mandatory re-derivation + `sympy.physics.*` — read by orchestrator at runtime |\n| `skills/alethic-scientific-figure/references/*.md` | Color palette guide and presentation override rcParams |\n| `skills/alethic-scientific-figure/scripts/*.py` | AFP colormap registration for matplotlib |\n\n## Project Structure\n\n```\nalethic/\n├── .claude-plugin/                 # Claude Code marketplace plugin\n│   ├── plugin.json                 # Plugin metadata (v1.0.0)\n│   └── marketplace.json            # Marketplace manifest\n├── skills/                         # Claude Code skills\n│   ├── alethic-common/\n│   │   └── orchestrator.md         # Shared GVR loop (parameterized by domain)\n│   ├── alethic-solve/\n│   │   ├── SKILL.md                # Thin configurator (math domain variables)\n│   │   └── references/             # Authoritative math prompt templates\n│   │       ├── generator.md\n│   │       ├── verifier.md\n│   │       ├── reviser.md\n│   │       ├── beautifier.md\n│   │       ├── textbook_planner.md\n│   │       ├── textbook_writer.md\n│   │       └── fidelity_verifier.md\n│   ├── alethic-derive/\n│   │   ├── SKILL.md                # Thin configurator (physics domain variables)\n│   │   └── references/             # Authoritative physics prompt templates\n│   │       ├── generator.md\n│   │       ├── verifier.md\n│   │       ├── reviser.md\n│   │       ├── beautifier.md\n│   │       ├── textbook_planner.md\n│   │       ├── textbook_writer.md\n│   │       └── fidelity_verifier.md\n│   ├── alethic-textbook/\n│   │   └── SKILL.md                # Standalone textbook converter\n│   └── alethic-scientific-figure/\n│       ├── SKILL.md                # /alethic-scientific-figure command\n│       ├── evals.json              # Evaluation scenarios\n│       ├── references/             # Color palette + presentation overrides\n│       │   ├── color-palette.md\n│       │   └── presentation-override.md\n│       └── scripts/\n│           └── register_colormaps.py  # AFP colormap registration\n├── src/alethic/                    # Python library\n│   ├── agent.py                    # MathAgent orchestrator\n│   ├── physics_agent.py            # PhysicsAgent (subclass of MathAgent)\n│   ├── subagents.py                # generate(), verify(), revise()\n│   ├── models.py                   # Data models + Verdict enum\n│   ├── prompts.py                  # Math prompt templates\n│   ├── physics_prompts.py          # Physics prompt templates\n│   ├── tools.py                    # Python sandbox + tool-use loop\n│   ├── cli.py                      # CLI entry point (solve/derive subcommands)\n│   └── examples.py                 # Bundled example problems\n└── tests/\n    ├── test_alethic.py             # Core tests (62)\n    ├── test_physics.py             # Physics tests (40)\n    ├── test_new_types.py           # IssueSeverity, Issue, SectionConfidence, EventType, AgentEvent tests\n    ├── test_best_of_n.py           # Best-of-N sampling tests\n    └── test_adversarial_*.py       # Adversarial tests (skill structure, domain config, orchestrator)\n```\n\n## Testing\n\nAll tests use mocked API responses and require no API key. The test suite covers data models, prompt content validation (math and physics), sandbox execution (including timeout enforcement and import restrictions), structured output parsing for all verdict types, preset creation and overrides, configurable confidence thresholds, CLI argument parsing (including `solve`/`derive` subcommands, `--preset`, and `--confidence-threshold`), physics prompt injection via kwargs, `PhysicsAgent` instantiation and integration, and end-to-end flows for solved, revised, and failed problems. Adversarial tests validate the skill architecture: domain configuration symmetry, shared orchestrator structure and parameterization, reference file authority headers, physics-specific prompts and symbols, extended verifier return lines (HAS_CRITICAL, TOP_ISSUE), CLI flag coverage, event logging, balanced addendum placement, and SymPy guidance coverage (toolkit sections in generators, mandatory re-derivation in verifiers, domain-specific `sympy.physics` modules in physics files only).\n\n```bash\n# Run all tests\npytest\n\n# With coverage report\npytest --cov=alethic\n\n# Lint and format\nruff check src tests\nruff format src tests\n```\n\n## Known Limitations\n\n**Single-model verification.** Both the Generator and Verifier use the same underlying model (Claude Opus). While decoupling prevents the Verifier from following the Generator's specific reasoning path, both models share the same training data and potential blind spots. The Aletheia paper uses the same single-model approach (Gemini for all roles) and notes that decoupling alone provides substantial gains, but cross-model verification could further improve robustness.\n\n**Temperature override with extended thinking.** The Anthropic API requires `temperature=1` when extended thinking is enabled, which overrides the carefully tuned per-subagent temperatures. In this mode, the Verifier loses its low-temperature strictness, potentially reducing verification precision. The prompt instructions partially compensate for this by emphasizing skepticism and rigor.\n\n**Skill temperature limitations.** Claude Code Task sub-agents run at the default temperature. The per-subagent temperature tuning (T=1.0, T=0.2, T=0.7) is only available through the Python library. The skill relies on prompt instructions to approximate these behavioral differences.\n\n**Context accumulation in skill mode.** Without `context:fork`, all Task call/response pairs accumulate in the main conversation. The file-based state design mitigates this by keeping solution text out of the orchestrator's context, but very long runs (8+ iterations) may approach context limits.\n\n**Beautifier runs post-verification.** The Beautifier formats the accepted solution after the final verification pass. While it is constrained to formatting-only changes (converting text math to LaTeX, adding section headers), there is no re-verification of the beautified output. The raw verified solution is preserved at `worklog/best_solution.md` as a fallback.\n\n**Issue severity depends on prompt compliance.** The Verifier is prompted to tag issues with severity levels ([CRITICAL], [MAJOR], [MINOR]). When the model does not produce tags, issues default to MAJOR severity. Critical issues block solution acceptance regardless of confidence score.\n\n**Session storage.** Sessions are stored in `.alethic/` in the project directory (falls back to `/tmp/alethic-*` outside git repos). Intermediate files live in `worklog/` subdirectories and can be pruned with `rm -rf .alethic/*/worklog/`. Add `.alethic/` to your `.gitignore`.\n\n## Related Work\n\nAlethic is one of several AI-assisted mathematical reasoning systems. For a detailed architectural comparison with Tobias Osborne's **Alethfeld** (Clojure, Lean 4 formalization) and **Vibefeld** (Go, adversarial proof framework), see [docs/comparison.md](docs/comparison.md).\n\n## License\n\nMIT\n"
      },
      "plugins": [
        {
          "name": "alethic",
          "description": "Reasoning agent for mathematics and physics with Generate-Verify-Revise loop (inspired by DeepMind's Aletheia), plus scientific figure generation",
          "version": "2.0.0",
          "source": "./",
          "author": {
            "name": "Alexander Friedrich"
          },
          "categories": [],
          "install_commands": [
            "/plugin marketplace add hyperion-git/alethic",
            "/plugin install alethic@alethic"
          ]
        }
      ]
    }
  ]
}