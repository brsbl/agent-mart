{
  "author": {
    "id": "alijilani-dev",
    "display_name": "alijilani-dev",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/190165159?v=4",
    "url": "https://github.com/alijilani-dev",
    "bio": null,
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 12,
      "total_commands": 0,
      "total_skills": 45,
      "total_stars": 0,
      "total_forks": 0
    }
  },
  "marketplaces": [
    {
      "name": "alijilani-claude-skills",
      "version": "1.0.0",
      "description": "A collection of Claude Code skills for developers - includes GitHub assistant, Docker tools, FastAPI helpers, TDD workflows, and productivity boosters",
      "owner_info": {
        "name": "Ali Jilani",
        "email": "alijilani.dev@gmail.com"
      },
      "keywords": [],
      "repo_full_name": "alijilani-dev/Claude",
      "repo_url": "https://github.com/alijilani-dev/Claude",
      "repo_description": "Claude code developments.",
      "homepage": null,
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2026-01-28T02:08:25Z",
        "created_at": "2026-01-06T05:27:40Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/.claude-plugin.txt",
          "type": "blob",
          "size": 589
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 4414
        },
        {
          "path": "skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 312
        },
        {
          "path": "skills/cover_letter_generator/.claude",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/browser-use",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/browser-use/SKILL.md",
          "type": "blob",
          "size": 4295
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/browser-use/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/browser-use/references/playwright-tools.md",
          "type": "blob",
          "size": 20315
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/browsing-with-playwright",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/browsing-with-playwright/SKILL.md",
          "type": "blob",
          "size": 4753
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/browsing-with-playwright/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/browsing-with-playwright/references/playwright-tools.md",
          "type": "blob",
          "size": 20315
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/context7-efficient",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/context7-efficient/SKILL.md",
          "type": "blob",
          "size": 5987
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/context7-efficient/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/context7-efficient/references/context7-tools.md",
          "type": "blob",
          "size": 8082
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/cover-letter-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/cover-letter-generator/SKILL.md",
          "type": "blob",
          "size": 3796
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/cover-letter-generator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/cover-letter-generator/references/psi_template.md",
          "type": "blob",
          "size": 2548
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/cover-letter-generator/references/quality_pillars.md",
          "type": "blob",
          "size": 1474
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/doc-coauthoring",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/doc-coauthoring/SKILL.md",
          "type": "blob",
          "size": 15815
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/docx",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/docx/SKILL.md",
          "type": "blob",
          "size": 10150
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/docx/docx-js.md",
          "type": "blob",
          "size": 16509
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/docx/ooxml.md",
          "type": "blob",
          "size": 23572
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/fetch-library-docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/fetch-library-docs/SKILL.md",
          "type": "blob",
          "size": 3742
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/fetch-library-docs/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/fetch-library-docs/references/context7-tools.md",
          "type": "blob",
          "size": 9062
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/fetch-library-docs/references/library-ids.md",
          "type": "blob",
          "size": 1591
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/fetch-library-docs/references/patterns.md",
          "type": "blob",
          "size": 2246
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/github-assistant",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/github-assistant/SKILL.md",
          "type": "blob",
          "size": 22111
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/internal-comms",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/internal-comms/SKILL.md",
          "type": "blob",
          "size": 1511
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/internal-comms/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/internal-comms/examples/3p-updates.md",
          "type": "blob",
          "size": 3274
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/internal-comms/examples/company-newsletter.md",
          "type": "blob",
          "size": 3295
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/internal-comms/examples/faq-answers.md",
          "type": "blob",
          "size": 2366
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/internal-comms/examples/general-comms.md",
          "type": "blob",
          "size": 602
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/interview",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/interview/SKILL.md",
          "type": "blob",
          "size": 8305
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/interview/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/interview/references/anti-patterns.md",
          "type": "blob",
          "size": 5757
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/interview/references/question-patterns.md",
          "type": "blob",
          "size": 4555
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/interview/references/summary-templates.md",
          "type": "blob",
          "size": 3050
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/linkedin_job_resume_optimizer",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/linkedin_job_resume_optimizer/SKILL.md",
          "type": "blob",
          "size": 14607
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/linkedin_job_resume_optimizer/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/linkedin_job_resume_optimizer/references/ats-optimization-guide.md",
          "type": "blob",
          "size": 10517
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/linkedin_job_resume_optimizer/references/interview-prep-patterns.md",
          "type": "blob",
          "size": 14411
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/linkedin_job_resume_optimizer/references/linkedin-automation.md",
          "type": "blob",
          "size": 11449
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/pdf",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/pdf/SKILL.md",
          "type": "blob",
          "size": 7068
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/pdf/forms.md",
          "type": "blob",
          "size": 9438
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/pdf/reference.md",
          "type": "blob",
          "size": 16692
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/pptx",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/pptx/SKILL.md",
          "type": "blob",
          "size": 25551
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/pptx/html2pptx.md",
          "type": "blob",
          "size": 19859
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/pptx/ooxml.md",
          "type": "blob",
          "size": 10388
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/skill-creator-pro",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/skill-creator-pro/SKILL.md",
          "type": "blob",
          "size": 13692
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/skill-creator-pro/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/skill-creator-pro/references/creation-workflow.md",
          "type": "blob",
          "size": 10240
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/skill-creator-pro/references/output-patterns.md",
          "type": "blob",
          "size": 6769
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/skill-creator-pro/references/quality-patterns.md",
          "type": "blob",
          "size": 6086
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/skill-creator-pro/references/reusability-patterns.md",
          "type": "blob",
          "size": 8864
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/skill-creator-pro/references/skill-patterns.md",
          "type": "blob",
          "size": 15188
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/skill-creator-pro/references/technical-patterns.md",
          "type": "blob",
          "size": 6864
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/skill-creator-pro/references/workflows.md",
          "type": "blob",
          "size": 4990
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/skill-creator",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/skill-creator/SKILL.md",
          "type": "blob",
          "size": 17837
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/skill-creator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/skill-creator/references/output-patterns.md",
          "type": "blob",
          "size": 1813
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/skill-creator/references/workflows.md",
          "type": "blob",
          "size": 818
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/skill-validator",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/skill-validator/SKILL.md",
          "type": "blob",
          "size": 11245
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/skill-validator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/skill-validator/references/detailed-criteria.md",
          "type": "blob",
          "size": 13271
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/skill-validator/references/improvement-patterns.md",
          "type": "blob",
          "size": 13742
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/skill-validator/references/scoring-examples.md",
          "type": "blob",
          "size": 7147
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/theme-factory",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/theme-factory/SKILL.md",
          "type": "blob",
          "size": 3124
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/theme-factory/themes",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/theme-factory/themes/arctic-frost.md",
          "type": "blob",
          "size": 544
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/theme-factory/themes/botanical-garden.md",
          "type": "blob",
          "size": 519
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/theme-factory/themes/desert-rose.md",
          "type": "blob",
          "size": 496
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/theme-factory/themes/forest-canopy.md",
          "type": "blob",
          "size": 506
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/theme-factory/themes/golden-hour.md",
          "type": "blob",
          "size": 528
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/theme-factory/themes/midnight-galaxy.md",
          "type": "blob",
          "size": 513
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/theme-factory/themes/modern-minimalist.md",
          "type": "blob",
          "size": 549
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/theme-factory/themes/ocean-depths.md",
          "type": "blob",
          "size": 555
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/theme-factory/themes/sunset-boulevard.md",
          "type": "blob",
          "size": 558
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/theme-factory/themes/tech-innovation.md",
          "type": "blob",
          "size": 547
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/xlsx",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/.claude/skills/xlsx/SKILL.md",
          "type": "blob",
          "size": 10632
        },
        {
          "path": "skills/cover_letter_generator/documentation",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/documentation/cover_letter_criteria.md",
          "type": "blob",
          "size": 2610
        },
        {
          "path": "skills/cover_letter_generator/documentation/gemini_readme_generator.md",
          "type": "blob",
          "size": 2914
        },
        {
          "path": "skills/cover_letter_generator/output",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/cover_letter_generator/output/cover_letter_anthropic.md",
          "type": "blob",
          "size": 2964
        },
        {
          "path": "skills/cover_letter_generator/output/cover_letter_deloitte.md",
          "type": "blob",
          "size": 3073
        },
        {
          "path": "skills/cover_letter_generator/readme.md",
          "type": "blob",
          "size": 5116
        },
        {
          "path": "skills/docker-rocker",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/docker-rocker/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/docker-rocker/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 361
        },
        {
          "path": "skills/docker-rocker/SKILL.md",
          "type": "blob",
          "size": 7862
        },
        {
          "path": "skills/docker-rocker/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/docker-rocker/references/database-connections.md",
          "type": "blob",
          "size": 7706
        },
        {
          "path": "skills/docker-rocker/references/fastapi-patterns.md",
          "type": "blob",
          "size": 8632
        },
        {
          "path": "skills/docker-rocker/references/multi-stage-builds.md",
          "type": "blob",
          "size": 8607
        },
        {
          "path": "skills/docker-rocker/references/optimization-strategies.md",
          "type": "blob",
          "size": 6055
        },
        {
          "path": "skills/docker-rocker/references/security-best-practices.md",
          "type": "blob",
          "size": 6754
        },
        {
          "path": "skills/docker-rocker/skill_validator_report.md",
          "type": "blob",
          "size": 11007
        },
        {
          "path": "skills/fastapi-helper",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/fastapi-helper/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/fastapi-helper/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 329
        },
        {
          "path": "skills/fastapi-helper/SKILL.md",
          "type": "blob",
          "size": 5657
        },
        {
          "path": "skills/fastapi-helper/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/fastapi-helper/references/dependencies.md",
          "type": "blob",
          "size": 3033
        },
        {
          "path": "skills/fastapi-helper/references/exceptions.md",
          "type": "blob",
          "size": 3127
        },
        {
          "path": "skills/fastapi-helper/references/middleware-events.md",
          "type": "blob",
          "size": 3932
        },
        {
          "path": "skills/fastapi-helper/references/parameters.md",
          "type": "blob",
          "size": 3713
        },
        {
          "path": "skills/fastapi-helper/references/pydantic-models.md",
          "type": "blob",
          "size": 4248
        },
        {
          "path": "skills/fastapi-helper/references/responses.md",
          "type": "blob",
          "size": 3049
        },
        {
          "path": "skills/fastapi-helper/references/routing.md",
          "type": "blob",
          "size": 1911
        },
        {
          "path": "skills/fastapi-helper/references/testing.md",
          "type": "blob",
          "size": 3850
        },
        {
          "path": "skills/fastapi-helper/references/websockets.md",
          "type": "blob",
          "size": 2998
        },
        {
          "path": "skills/fastapi-pytest-tddhelper",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/fastapi-pytest-tddhelper/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/fastapi-pytest-tddhelper/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 345
        },
        {
          "path": "skills/fastapi-pytest-tddhelper/SKILL.md",
          "type": "blob",
          "size": 6608
        },
        {
          "path": "skills/fastapi-pytest-tddhelper/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/fastapi-pytest-tddhelper/references/conftest-patterns.md",
          "type": "blob",
          "size": 6295
        },
        {
          "path": "skills/fastapi-pytest-tddhelper/references/pydantic-validation.md",
          "type": "blob",
          "size": 6039
        },
        {
          "path": "skills/fastapi-pytest-tddhelper/references/pytest-optimization.md",
          "type": "blob",
          "size": 5473
        },
        {
          "path": "skills/fastapi-pytest-tddhelper/references/tdd-workflow.md",
          "type": "blob",
          "size": 7931
        },
        {
          "path": "skills/fastapi-pytest-tddhelper/references/testing-patterns.md",
          "type": "blob",
          "size": 8644
        },
        {
          "path": "skills/github-assistant",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/github-assistant/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/github-assistant/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 361
        },
        {
          "path": "skills/github-assistant/SKILL.md",
          "type": "blob",
          "size": 22111
        },
        {
          "path": "skills/linkedin-post-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/linkedin-post-generator/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/linkedin-post-generator/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 314
        },
        {
          "path": "skills/linkedin-post-generator/.claude",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/linkedin-post-generator/.claude/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/linkedin-post-generator/.claude/skills/linkedin-post-maker",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/linkedin-post-generator/.claude/skills/linkedin-post-maker/BEST-PRACTICES.md",
          "type": "blob",
          "size": 5892
        },
        {
          "path": "skills/linkedin-post-generator/.claude/skills/linkedin-post-maker/POST-TEMPLATE.md",
          "type": "blob",
          "size": 4810
        },
        {
          "path": "skills/linkedin-post-generator/.claude/skills/linkedin-post-maker/README.md",
          "type": "blob",
          "size": 6193
        },
        {
          "path": "skills/linkedin-post-generator/.claude/skills/linkedin-post-maker/SKILL.md",
          "type": "blob",
          "size": 8865
        },
        {
          "path": "skills/linkedin-post-generator/LinkedIn-Posts",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/linkedin-post-generator/LinkedIn-Posts/2025-12-21_ai-agents-changing-work.md",
          "type": "blob",
          "size": 2028
        },
        {
          "path": "skills/pytest-python",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/pytest-python/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/pytest-python/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 291
        },
        {
          "path": "skills/pytest-python/SKILL.md",
          "type": "blob",
          "size": 7024
        },
        {
          "path": "skills/pytest-python/readme.md",
          "type": "blob",
          "size": 2860
        },
        {
          "path": "skills/pytest-python/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/pytest-python/references/fixtures.md",
          "type": "blob",
          "size": 5441
        },
        {
          "path": "skills/pytest-python/references/patterns.md",
          "type": "blob",
          "size": 8782
        },
        {
          "path": "skills/recipes",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/recipes/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/recipes/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 267
        },
        {
          "path": "skills/recipes/CLAUDE.md",
          "type": "blob",
          "size": 400
        },
        {
          "path": "skills/recipes/oat-juice-breakfast.md",
          "type": "blob",
          "size": 2147
        },
        {
          "path": "skills/resume_optimizer",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 307
        },
        {
          "path": "skills/resume_optimizer/.claude",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude/command",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude/command/job_optimize_resume.md",
          "type": "blob",
          "size": 460
        },
        {
          "path": "skills/resume_optimizer/.claude/command/job_resume_optimizer.md",
          "type": "blob",
          "size": 1128
        },
        {
          "path": "skills/resume_optimizer/.claude/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/browser-use",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/browser-use/SKILL.md",
          "type": "blob",
          "size": 4295
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/browser-use/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/browser-use/references/playwright-tools.md",
          "type": "blob",
          "size": 20315
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/browsing-with-playwright",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/browsing-with-playwright/SKILL.md",
          "type": "blob",
          "size": 4753
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/browsing-with-playwright/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/browsing-with-playwright/references/playwright-tools.md",
          "type": "blob",
          "size": 20315
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/context7-efficient",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/context7-efficient/SKILL.md",
          "type": "blob",
          "size": 5987
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/context7-efficient/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/context7-efficient/references/context7-tools.md",
          "type": "blob",
          "size": 8082
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/doc-coauthoring",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/doc-coauthoring/SKILL.md",
          "type": "blob",
          "size": 15815
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/docx",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/docx/SKILL.md",
          "type": "blob",
          "size": 10150
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/docx/docx-js.md",
          "type": "blob",
          "size": 16509
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/docx/ooxml.md",
          "type": "blob",
          "size": 23572
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/fetch-library-docs",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/fetch-library-docs/SKILL.md",
          "type": "blob",
          "size": 3742
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/fetch-library-docs/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/fetch-library-docs/references/context7-tools.md",
          "type": "blob",
          "size": 9062
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/fetch-library-docs/references/library-ids.md",
          "type": "blob",
          "size": 1591
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/fetch-library-docs/references/patterns.md",
          "type": "blob",
          "size": 2246
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/internal-comms",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/internal-comms/SKILL.md",
          "type": "blob",
          "size": 1511
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/internal-comms/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/internal-comms/examples/3p-updates.md",
          "type": "blob",
          "size": 3274
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/internal-comms/examples/company-newsletter.md",
          "type": "blob",
          "size": 3295
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/internal-comms/examples/faq-answers.md",
          "type": "blob",
          "size": 2366
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/internal-comms/examples/general-comms.md",
          "type": "blob",
          "size": 602
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/interview",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/interview/SKILL.md",
          "type": "blob",
          "size": 8305
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/interview/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/interview/references/anti-patterns.md",
          "type": "blob",
          "size": 5757
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/interview/references/question-patterns.md",
          "type": "blob",
          "size": 4555
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/interview/references/summary-templates.md",
          "type": "blob",
          "size": 3050
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/linkedin_job_resume_optimizer",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/linkedin_job_resume_optimizer/SKILL.md",
          "type": "blob",
          "size": 14607
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/linkedin_job_resume_optimizer/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/linkedin_job_resume_optimizer/references/ats-optimization-guide.md",
          "type": "blob",
          "size": 10517
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/linkedin_job_resume_optimizer/references/interview-prep-patterns.md",
          "type": "blob",
          "size": 14411
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/linkedin_job_resume_optimizer/references/linkedin-automation.md",
          "type": "blob",
          "size": 11449
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/pdf",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/pdf/SKILL.md",
          "type": "blob",
          "size": 7068
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/pdf/forms.md",
          "type": "blob",
          "size": 9438
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/pdf/reference.md",
          "type": "blob",
          "size": 16692
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/pptx",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/pptx/SKILL.md",
          "type": "blob",
          "size": 25551
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/pptx/html2pptx.md",
          "type": "blob",
          "size": 19859
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/pptx/ooxml.md",
          "type": "blob",
          "size": 10388
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/skill-creator-pro",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/skill-creator-pro/SKILL.md",
          "type": "blob",
          "size": 13692
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/skill-creator-pro/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/skill-creator-pro/references/creation-workflow.md",
          "type": "blob",
          "size": 10240
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/skill-creator-pro/references/output-patterns.md",
          "type": "blob",
          "size": 6769
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/skill-creator-pro/references/quality-patterns.md",
          "type": "blob",
          "size": 6086
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/skill-creator-pro/references/reusability-patterns.md",
          "type": "blob",
          "size": 8864
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/skill-creator-pro/references/skill-patterns.md",
          "type": "blob",
          "size": 15188
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/skill-creator-pro/references/technical-patterns.md",
          "type": "blob",
          "size": 6864
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/skill-creator-pro/references/workflows.md",
          "type": "blob",
          "size": 4990
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/skill-creator",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/skill-creator/SKILL.md",
          "type": "blob",
          "size": 17837
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/skill-creator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/skill-creator/references/output-patterns.md",
          "type": "blob",
          "size": 1813
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/skill-creator/references/workflows.md",
          "type": "blob",
          "size": 818
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/skill-validator",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/skill-validator/SKILL.md",
          "type": "blob",
          "size": 11245
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/skill-validator/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/skill-validator/references/detailed-criteria.md",
          "type": "blob",
          "size": 13271
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/skill-validator/references/improvement-patterns.md",
          "type": "blob",
          "size": 13742
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/skill-validator/references/scoring-examples.md",
          "type": "blob",
          "size": 7147
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/theme-factory",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/theme-factory/SKILL.md",
          "type": "blob",
          "size": 3124
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/theme-factory/themes",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/theme-factory/themes/arctic-frost.md",
          "type": "blob",
          "size": 544
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/theme-factory/themes/botanical-garden.md",
          "type": "blob",
          "size": 519
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/theme-factory/themes/desert-rose.md",
          "type": "blob",
          "size": 496
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/theme-factory/themes/forest-canopy.md",
          "type": "blob",
          "size": 506
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/theme-factory/themes/golden-hour.md",
          "type": "blob",
          "size": 528
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/theme-factory/themes/midnight-galaxy.md",
          "type": "blob",
          "size": 513
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/theme-factory/themes/modern-minimalist.md",
          "type": "blob",
          "size": 549
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/theme-factory/themes/ocean-depths.md",
          "type": "blob",
          "size": 555
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/theme-factory/themes/sunset-boulevard.md",
          "type": "blob",
          "size": 558
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/theme-factory/themes/tech-innovation.md",
          "type": "blob",
          "size": 547
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/xlsx",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/resume_optimizer/.claude/skills/xlsx/SKILL.md",
          "type": "blob",
          "size": 10632
        },
        {
          "path": "skills/resume_optimizer/readme.md",
          "type": "blob",
          "size": 4695
        },
        {
          "path": "skills/resume_optimizer/resume_optimizer.md",
          "type": "blob",
          "size": 1417
        },
        {
          "path": "skills/sqlmodel-orm-dbhelper",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/sqlmodel-orm-dbhelper/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/sqlmodel-orm-dbhelper/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 313
        },
        {
          "path": "skills/sqlmodel-orm-dbhelper/SKILL.md",
          "type": "blob",
          "size": 9742
        },
        {
          "path": "skills/sqlmodel-orm-dbhelper/readme.md",
          "type": "blob",
          "size": 2893
        },
        {
          "path": "skills/sqlmodel-orm-dbhelper/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/sqlmodel-orm-dbhelper/references/engine-config.md",
          "type": "blob",
          "size": 6863
        },
        {
          "path": "skills/sqlmodel-orm-dbhelper/references/field-types.md",
          "type": "blob",
          "size": 9986
        },
        {
          "path": "skills/sqlmodel-orm-dbhelper/references/migrations.md",
          "type": "blob",
          "size": 9612
        },
        {
          "path": "skills/sqlmodel-orm-dbhelper/references/performance.md",
          "type": "blob",
          "size": 9345
        },
        {
          "path": "skills/sqlmodel-orm-dbhelper/references/relationships.md",
          "type": "blob",
          "size": 8894
        },
        {
          "path": "skills/study_notes",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/study_notes/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/study_notes/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 259
        },
        {
          "path": "skills/study_notes/CLAUDE.md",
          "type": "blob",
          "size": 2844
        },
        {
          "path": "skills/study_notes/study-system",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/study_notes/study-system/CLAUDE.md",
          "type": "blob",
          "size": 840
        },
        {
          "path": "skills/study_notes/study-system/lessons",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/study_notes/study-system/lessons/lesson1.md",
          "type": "blob",
          "size": 21848
        },
        {
          "path": "skills/study_system",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/study_system/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/study_system/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 287
        },
        {
          "path": "skills/study_system/.claude",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/study_system/.claude/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/study_system/.claude/skills/flashcards-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/study_system/.claude/skills/flashcards-generator/SKILL.md",
          "type": "blob",
          "size": 10687
        },
        {
          "path": "skills/study_system/.claude/skills/flashcards-generator/examples.md",
          "type": "blob",
          "size": 35006
        },
        {
          "path": "skills/study_system/.claude/skills/flashcards-generator/reference.md",
          "type": "blob",
          "size": 18722
        },
        {
          "path": "skills/study_system/.claude/skills/notes-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/study_system/.claude/skills/notes-generator/SKILL.md",
          "type": "blob",
          "size": 13793
        },
        {
          "path": "skills/study_system/.claude/skills/notes-generator/examples.md",
          "type": "blob",
          "size": 41972
        },
        {
          "path": "skills/study_system/.claude/skills/notes-generator/reference.md",
          "type": "blob",
          "size": 18682
        },
        {
          "path": "skills/study_system/.claude/skills/notes-generator/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/study_system/.claude/skills/notes-generator/templates/note-template.md",
          "type": "blob",
          "size": 11178
        },
        {
          "path": "skills/study_system/.claude/skills/quiz-generator",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/study_system/.claude/skills/quiz-generator/SKILL.md",
          "type": "blob",
          "size": 17126
        },
        {
          "path": "skills/study_system/.claude/skills/quiz-generator/examples.md",
          "type": "blob",
          "size": 36438
        },
        {
          "path": "skills/study_system/.claude/skills/quiz-generator/reference.md",
          "type": "blob",
          "size": 28208
        },
        {
          "path": "skills/study_system/.claude/skills/skill-maker",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/study_system/.claude/skills/skill-maker/SKILL.md",
          "type": "blob",
          "size": 9293
        },
        {
          "path": "skills/study_system/.claude/skills/skill-maker/examples.md",
          "type": "blob",
          "size": 22236
        },
        {
          "path": "skills/study_system/.claude/skills/skill-maker/reference.md",
          "type": "blob",
          "size": 11686
        },
        {
          "path": "skills/study_system/CLAUDE.md",
          "type": "blob",
          "size": 2844
        },
        {
          "path": "skills/study_system/lessons",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/study_system/lessons/lesson1.md",
          "type": "blob",
          "size": 21848
        },
        {
          "path": "skills/study_system/study-system",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/study_system/study-system/CLAUDE.md",
          "type": "blob",
          "size": 822
        },
        {
          "path": "skills/study_system/study-system/lessons",
          "type": "tree",
          "size": null
        },
        {
          "path": "skills/study_system/study-system/lessons/lesson1.md",
          "type": "blob",
          "size": 21848
        }
      ],
      "files": {
        ".claude-plugin/.claude-plugin.txt": "Use fetch-library-docs skill to read the webpage at https://claudemarketplaces.com/about what this community is and to discover the process of creating my    \r\n  own marketplace. My claude repository on GitHub is at [https://github.com/alijilani-dev/Claude/tree/main]. Explain briefly what claudemarketplaces.com is     \r\n  and guide me step-by-step as a teenager to be able to create my marketplace so my marketplace gets listed on claudemarketplaces.com and I am able to\r\n  contribute to the community and earn from it by selling my claude skills like github-assistant and docker-rocker.",
        ".claude-plugin/marketplace.json": "{\n  \"$schema\": \"https://anthropic.com/claude-code/marketplace.schema.json\",\n  \"name\": \"alijilani-claude-skills\",\n  \"version\": \"1.0.0\",\n  \"description\": \"A collection of Claude Code skills for developers - includes GitHub assistant, Docker tools, FastAPI helpers, TDD workflows, and productivity boosters\",\n  \"owner\": {\n    \"name\": \"Ali Jilani\",\n    \"email\": \"alijilani.dev@gmail.com\"\n  },\n  \"metadata\": {\n    \"description\": \"Community-built Claude Code skills to supercharge your development workflow\",\n    \"version\": \"1.0.0\",\n    \"pluginRoot\": \"./skills\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"github-assistant\",\n      \"description\": \"Interactive troubleshooting for GitHub problems - authentication, sync issues, merge conflicts, and sensitive data exposure\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Ali Jilani\"\n      },\n      \"source\": \"./skills/github-assistant\",\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"docker-rocker\",\n      \"description\": \"Docker containerization expert for FastAPI projects - Dockerfiles, docker-compose, multi-stage builds, and security best practices\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Ali Jilani\"\n      },\n      \"source\": \"./skills/docker-rocker\",\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"fastapi-helper\",\n      \"description\": \"FastAPI development assistant with routing, dependencies, Pydantic models, middleware, and testing patterns\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Ali Jilani\"\n      },\n      \"source\": \"./skills/fastapi-helper\",\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"fastapi-pytest-tddhelper\",\n      \"description\": \"Test-Driven Development workflow helper for FastAPI with pytest - fixtures, conftest patterns, and testing best practices\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Ali Jilani\"\n      },\n      \"source\": \"./skills/fastapi-pytest-tddhelper\",\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"sqlmodel-orm-dbhelper\",\n      \"description\": \"SQLModel ORM database helper for Python - models, relationships, migrations, and query patterns\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Ali Jilani\"\n      },\n      \"source\": \"./skills/sqlmodel-orm-dbhelper\",\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"cover-letter-generator\",\n      \"description\": \"Professional cover letter generator tailored to job descriptions and your experience\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Ali Jilani\"\n      },\n      \"source\": \"./skills/cover_letter_generator\",\n      \"category\": \"productivity\"\n    },\n    {\n      \"name\": \"resume-optimizer\",\n      \"description\": \"Resume optimization tool to match job requirements and highlight relevant skills\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Ali Jilani\"\n      },\n      \"source\": \"./skills/resume_optimizer\",\n      \"category\": \"productivity\"\n    },\n    {\n      \"name\": \"linkedin-post-generator\",\n      \"description\": \"LinkedIn post generator for professional content creation and engagement\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Ali Jilani\"\n      },\n      \"source\": \"./skills/linkedin-post-generator\",\n      \"category\": \"productivity\"\n    },\n    {\n      \"name\": \"study-notes\",\n      \"description\": \"Study notes generator for learning and revision\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Ali Jilani\"\n      },\n      \"source\": \"./skills/study_notes\",\n      \"category\": \"learning\"\n    },\n    {\n      \"name\": \"study-system\",\n      \"description\": \"Complete study system for organized learning and knowledge management\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Ali Jilani\"\n      },\n      \"source\": \"./skills/study_system\",\n      \"category\": \"learning\"\n    },\n    {\n      \"name\": \"pytest-python\",\n      \"description\": \"Python testing helper with pytest patterns, fixtures, and best practices\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Ali Jilani\"\n      },\n      \"source\": \"./skills/pytest-python\",\n      \"category\": \"development\"\n    },\n    {\n      \"name\": \"recipes\",\n      \"description\": \"Recipe and cooking assistant for meal planning and instructions\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Ali Jilani\"\n      },\n      \"source\": \"./skills/recipes\",\n      \"category\": \"productivity\"\n    }\n  ]\n}\n",
        "skills/cover_letter_generator/.claude-plugin/plugin.json": "{\n  \"name\": \"cover-letter-generator\",\n  \"description\": \"Professional cover letter generator tailored to job descriptions and your experience\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Ali Jilani\"\n  },\n  \"category\": \"productivity\",\n  \"keywords\": [\"cover-letter\", \"job-application\", \"career\", \"writing\"]\n}\n",
        "skills/cover_letter_generator/.claude/skills/browser-use/SKILL.md": "---\nname: browser-use\ndescription: Browser automation using Playwright MCP. Navigate websites, fill forms, click elements, take screenshots, and extract data. Use when tasks require web browsing, form submission, web scraping, UI testing, or any browser interaction.\n---\n\n# Browser Automation\n\nAutomate browser interactions via Playwright MCP server.\n\n## Server Lifecycle\n\n### Start Server\n```bash\n# Using helper script (recommended)\nbash scripts/start-server.sh\n\n# Or manually\nnpx @playwright/mcp@latest --port 8808 --shared-browser-context &\n```\n\n### Stop Server\n```bash\n# Using helper script (closes browser first)\nbash scripts/stop-server.sh\n\n# Or manually\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_close -p '{}'\npkill -f \"@playwright/mcp\"\n```\n\n### When to Stop\n- **End of task**: Stop when browser work is complete\n- **Long sessions**: Keep running if doing multiple browser tasks\n- **Errors**: Stop and restart if browser becomes unresponsive\n\n**Important:** The `--shared-browser-context` flag is required to maintain browser state across multiple mcp-client.py calls. Without it, each call gets a fresh browser context.\n\n## Quick Reference\n\n### Navigation\n\n```bash\n# Go to URL\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_navigate \\\n  -p '{\"url\": \"https://example.com\"}'\n\n# Go back\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_navigate_back -p '{}'\n```\n\n### Get Page State\n\n```bash\n# Accessibility snapshot (returns element refs for clicking/typing)\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_snapshot -p '{}'\n\n# Screenshot\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_take_screenshot \\\n  -p '{\"type\": \"png\", \"fullPage\": true}'\n```\n\n### Interact with Elements\n\nUse `ref` from snapshot output to target elements:\n\n```bash\n# Click element\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_click \\\n  -p '{\"element\": \"Submit button\", \"ref\": \"e42\"}'\n\n# Type text\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_type \\\n  -p '{\"element\": \"Search input\", \"ref\": \"e15\", \"text\": \"hello world\", \"submit\": true}'\n\n# Fill form (multiple fields)\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_fill_form \\\n  -p '{\"fields\": [{\"ref\": \"e10\", \"value\": \"john@example.com\"}, {\"ref\": \"e12\", \"value\": \"password123\"}]}'\n\n# Select dropdown\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_select_option \\\n  -p '{\"element\": \"Country dropdown\", \"ref\": \"e20\", \"values\": [\"US\"]}'\n```\n\n### Wait for Conditions\n\n```bash\n# Wait for text to appear\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_wait_for \\\n  -p '{\"text\": \"Success\"}'\n\n# Wait for time (ms)\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_wait_for \\\n  -p '{\"time\": 2000}'\n```\n\n### Execute JavaScript\n\n```bash\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_evaluate \\\n  -p '{\"function\": \"return document.title\"}'\n```\n\n### Multi-Step Playwright Code\n\nFor complex workflows, use `browser_run_code` to run multiple actions in one call:\n\n```bash\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_run_code \\\n  -p '{\"code\": \"async (page) => { await page.goto(\\\"https://example.com\\\"); await page.click(\\\"text=Learn more\\\"); return await page.title(); }\"}'\n```\n\n**Tip:** Use `browser_run_code` for complex multi-step operations that should be atomic (all-or-nothing).\n\n## Workflow: Form Submission\n\n1. Navigate to page\n2. Get snapshot to find element refs\n3. Fill form fields using refs\n4. Click submit\n5. Wait for confirmation\n6. Screenshot result\n\n## Workflow: Data Extraction\n\n1. Navigate to page\n2. Get snapshot (contains text content)\n3. Use browser_evaluate for complex extraction\n4. Process results\n\n## Tool Reference\n\nSee [references/playwright-tools.md](references/playwright-tools.md) for complete tool documentation.\n\n## Troubleshooting\n\n| Issue | Solution |\n|-------|----------|\n| Element not found | Run browser_snapshot first to get current refs |\n| Click fails | Try browser_hover first, then click |\n| Form not submitting | Use `\"submit\": true` with browser_type |\n| Page not loading | Increase wait time or use browser_wait_for |\n",
        "skills/cover_letter_generator/.claude/skills/browser-use/references/playwright-tools.md": "# MCP Server Tools\n\n*22 tools available*\n\n## `browser_close`\n\nClose the page\n\n*Flags: destructive*\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {},\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_resize`\n\nResize the browser window\n\n*Flags: destructive*\n\n### Parameters\n\n- **`width`** (`number`) *(required)*: Width of the browser window\n- **`height`** (`number`) *(required)*: Height of the browser window\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"width\": {\n      \"type\": \"number\",\n      \"description\": \"Width of the browser window\"\n    },\n    \"height\": {\n      \"type\": \"number\",\n      \"description\": \"Height of the browser window\"\n    }\n  },\n  \"required\": [\n    \"width\",\n    \"height\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_console_messages`\n\nReturns all console messages\n\n*Flags: read-only*\n\n### Parameters\n\n- **`level`** (`string`): Level of the console messages to return. Each level includes the messages of more severe levels. Defaults to \"info\".\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"level\": {\n      \"type\": \"string\",\n      \"enum\": [\n        \"error\",\n        \"warning\",\n        \"info\",\n        \"debug\"\n      ],\n      \"default\": \"info\",\n      \"description\": \"Level of the console messages to return. Each level includes the messages of more severe levels. Defaults to \\\"info\\\".\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_handle_dialog`\n\nHandle a dialog\n\n*Flags: destructive*\n\n### Parameters\n\n- **`accept`** (`boolean`) *(required)*: Whether to accept the dialog.\n- **`promptText`** (`string`): The text of the prompt in case of a prompt dialog.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"accept\": {\n      \"type\": \"boolean\",\n      \"description\": \"Whether to accept the dialog.\"\n    },\n    \"promptText\": {\n      \"type\": \"string\",\n      \"description\": \"The text of the prompt in case of a prompt dialog.\"\n    }\n  },\n  \"required\": [\n    \"accept\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_evaluate`\n\nEvaluate JavaScript expression on page or element\n\n*Flags: destructive*\n\n### Parameters\n\n- **`function`** (`string`) *(required)*: () => { /* code */ } or (element) => { /* code */ } when element is provided\n- **`element`** (`string`): Human-readable element description used to obtain permission to interact with the element\n- **`ref`** (`string`): Exact target element reference from the page snapshot\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"function\": {\n      \"type\": \"string\",\n      \"description\": \"() => { /* code */ } or (element) => { /* code */ } when element is provided\"\n    },\n    \"element\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable element description used to obtain permission to interact with the element\"\n    },\n    \"ref\": {\n      \"type\": \"string\",\n      \"description\": \"Exact target element reference from the page snapshot\"\n    }\n  },\n  \"required\": [\n    \"function\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_file_upload`\n\nUpload one or multiple files\n\n*Flags: destructive*\n\n### Parameters\n\n- **`paths`** (`array`): The absolute paths to the files to upload. Can be single file or multiple files. If omitted, file chooser is cancelled.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"paths\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"description\": \"The absolute paths to the files to upload. Can be single file or multiple files. If omitted, file chooser is cancelled.\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_fill_form`\n\nFill multiple form fields\n\n*Flags: destructive*\n\n### Parameters\n\n- **`fields`** (`array`) *(required)*: Fields to fill in\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"fields\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"name\": {\n            \"type\": \"string\",\n            \"description\": \"Human-readable field name\"\n          },\n          \"type\": {\n            \"type\": \"string\",\n            \"enum\": [\n              \"textbox\",\n              \"checkbox\",\n              \"radio\",\n              \"combobox\",\n              \"slider\"\n            ],\n            \"description\": \"Type of the field\"\n          },\n          \"ref\": {\n            \"type\": \"string\",\n            \"description\": \"Exact target field reference from the page snapshot\"\n          },\n          \"value\": {\n            \"type\": \"string\",\n            \"description\": \"Value to fill in the field. If the field is a checkbox, the value should be `true` or `false`. If the field is a combobox, the value should be the text of the option.\"\n          }\n        },\n        \"required\": [\n          \"name\",\n          \"type\",\n          \"ref\",\n          \"value\"\n        ],\n        \"additionalProperties\": false\n      },\n      \"description\": \"Fields to fill in\"\n    }\n  },\n  \"required\": [\n    \"fields\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_install`\n\nInstall the browser specified in the config. Call this if you get an error about the browser not being installed.\n\n*Flags: destructive*\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {},\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_press_key`\n\nPress a key on the keyboard\n\n*Flags: destructive*\n\n### Parameters\n\n- **`key`** (`string`) *(required)*: Name of the key to press or a character to generate, such as `ArrowLeft` or `a`\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"key\": {\n      \"type\": \"string\",\n      \"description\": \"Name of the key to press or a character to generate, such as `ArrowLeft` or `a`\"\n    }\n  },\n  \"required\": [\n    \"key\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_type`\n\nType text into editable element\n\n*Flags: destructive*\n\n### Parameters\n\n- **`element`** (`string`) *(required)*: Human-readable element description used to obtain permission to interact with the element\n- **`ref`** (`string`) *(required)*: Exact target element reference from the page snapshot\n- **`text`** (`string`) *(required)*: Text to type into the element\n- **`submit`** (`boolean`): Whether to submit entered text (press Enter after)\n- **`slowly`** (`boolean`): Whether to type one character at a time. Useful for triggering key handlers in the page. By default entire text is filled in at once.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"element\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable element description used to obtain permission to interact with the element\"\n    },\n    \"ref\": {\n      \"type\": \"string\",\n      \"description\": \"Exact target element reference from the page snapshot\"\n    },\n    \"text\": {\n      \"type\": \"string\",\n      \"description\": \"Text to type into the element\"\n    },\n    \"submit\": {\n      \"type\": \"boolean\",\n      \"description\": \"Whether to submit entered text (press Enter after)\"\n    },\n    \"slowly\": {\n      \"type\": \"boolean\",\n      \"description\": \"Whether to type one character at a time. Useful for triggering key handlers in the page. By default entire text is filled in at once.\"\n    }\n  },\n  \"required\": [\n    \"element\",\n    \"ref\",\n    \"text\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_navigate`\n\nNavigate to a URL\n\n*Flags: destructive*\n\n### Parameters\n\n- **`url`** (`string`) *(required)*: The URL to navigate to\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"url\": {\n      \"type\": \"string\",\n      \"description\": \"The URL to navigate to\"\n    }\n  },\n  \"required\": [\n    \"url\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_navigate_back`\n\nGo back to the previous page\n\n*Flags: destructive*\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {},\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_network_requests`\n\nReturns all network requests since loading the page\n\n*Flags: read-only*\n\n### Parameters\n\n- **`includeStatic`** (`boolean`): Whether to include successful static resources like images, fonts, scripts, etc. Defaults to false.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"includeStatic\": {\n      \"type\": \"boolean\",\n      \"default\": false,\n      \"description\": \"Whether to include successful static resources like images, fonts, scripts, etc. Defaults to false.\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_run_code`\n\nRun Playwright code snippet\n\n*Flags: destructive*\n\n### Parameters\n\n- **`code`** (`string`) *(required)*: A JavaScript function containing Playwright code to execute. It will be invoked with a single argument, page, which you can use for any page interaction. For example: `async (page) => { await page.getByRole('button', { name: 'Submit' }).click(); return await page.title(); }`\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"code\": {\n      \"type\": \"string\",\n      \"description\": \"A JavaScript function containing Playwright code to execute. It will be invoked with a single argument, page, which you can use for any page interaction. For example: `async (page) => { await page.getByRole('button', { name: 'Submit' }).click(); return await page.title(); }`\"\n    }\n  },\n  \"required\": [\n    \"code\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_take_screenshot`\n\nTake a screenshot of the current page. You can't perform actions based on the screenshot, use browser_snapshot for actions.\n\n*Flags: read-only*\n\n### Parameters\n\n- **`type`** (`string`): Image format for the screenshot. Default is png.\n- **`filename`** (`string`): File name to save the screenshot to. Defaults to `page-{timestamp}.{png|jpeg}` if not specified. Prefer relative file names to stay within the output directory.\n- **`element`** (`string`): Human-readable element description used to obtain permission to screenshot the element. If not provided, the screenshot will be taken of viewport. If element is provided, ref must be provided too.\n- **`ref`** (`string`): Exact target element reference from the page snapshot. If not provided, the screenshot will be taken of viewport. If ref is provided, element must be provided too.\n- **`fullPage`** (`boolean`): When true, takes a screenshot of the full scrollable page, instead of the currently visible viewport. Cannot be used with element screenshots.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"type\": {\n      \"type\": \"string\",\n      \"enum\": [\n        \"png\",\n        \"jpeg\"\n      ],\n      \"default\": \"png\",\n      \"description\": \"Image format for the screenshot. Default is png.\"\n    },\n    \"filename\": {\n      \"type\": \"string\",\n      \"description\": \"File name to save the screenshot to. Defaults to `page-{timestamp}.{png|jpeg}` if not specified. Prefer relative file names to stay within the output directory.\"\n    },\n    \"element\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable element description used to obtain permission to screenshot the element. If not provided, the screenshot will be taken of viewport. If element is provided, ref must be provided too.\"\n    },\n    \"ref\": {\n      \"type\": \"string\",\n      \"description\": \"Exact target element reference from the page snapshot. If not provided, the screenshot will be taken of viewport. If ref is provided, element must be provided too.\"\n    },\n    \"fullPage\": {\n      \"type\": \"boolean\",\n      \"description\": \"When true, takes a screenshot of the full scrollable page, instead of the currently visible viewport. Cannot be used with element screenshots.\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_snapshot`\n\nCapture accessibility snapshot of the current page, this is better than screenshot\n\n*Flags: read-only*\n\n### Parameters\n\n- **`filename`** (`string`): Save snapshot to markdown file instead of returning it in the response.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"filename\": {\n      \"type\": \"string\",\n      \"description\": \"Save snapshot to markdown file instead of returning it in the response.\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_click`\n\nPerform click on a web page\n\n*Flags: destructive*\n\n### Parameters\n\n- **`element`** (`string`) *(required)*: Human-readable element description used to obtain permission to interact with the element\n- **`ref`** (`string`) *(required)*: Exact target element reference from the page snapshot\n- **`doubleClick`** (`boolean`): Whether to perform a double click instead of a single click\n- **`button`** (`string`): Button to click, defaults to left\n- **`modifiers`** (`array`): Modifier keys to press\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"element\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable element description used to obtain permission to interact with the element\"\n    },\n    \"ref\": {\n      \"type\": \"string\",\n      \"description\": \"Exact target element reference from the page snapshot\"\n    },\n    \"doubleClick\": {\n      \"type\": \"boolean\",\n      \"description\": \"Whether to perform a double click instead of a single click\"\n    },\n    \"button\": {\n      \"type\": \"string\",\n      \"enum\": [\n        \"left\",\n        \"right\",\n        \"middle\"\n      ],\n      \"description\": \"Button to click, defaults to left\"\n    },\n    \"modifiers\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\",\n        \"enum\": [\n          \"Alt\",\n          \"Control\",\n          \"ControlOrMeta\",\n          \"Meta\",\n          \"Shift\"\n        ]\n      },\n      \"description\": \"Modifier keys to press\"\n    }\n  },\n  \"required\": [\n    \"element\",\n    \"ref\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_drag`\n\nPerform drag and drop between two elements\n\n*Flags: destructive*\n\n### Parameters\n\n- **`startElement`** (`string`) *(required)*: Human-readable source element description used to obtain the permission to interact with the element\n- **`startRef`** (`string`) *(required)*: Exact source element reference from the page snapshot\n- **`endElement`** (`string`) *(required)*: Human-readable target element description used to obtain the permission to interact with the element\n- **`endRef`** (`string`) *(required)*: Exact target element reference from the page snapshot\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"startElement\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable source element description used to obtain the permission to interact with the element\"\n    },\n    \"startRef\": {\n      \"type\": \"string\",\n      \"description\": \"Exact source element reference from the page snapshot\"\n    },\n    \"endElement\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable target element description used to obtain the permission to interact with the element\"\n    },\n    \"endRef\": {\n      \"type\": \"string\",\n      \"description\": \"Exact target element reference from the page snapshot\"\n    }\n  },\n  \"required\": [\n    \"startElement\",\n    \"startRef\",\n    \"endElement\",\n    \"endRef\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_hover`\n\nHover over element on page\n\n*Flags: destructive*\n\n### Parameters\n\n- **`element`** (`string`) *(required)*: Human-readable element description used to obtain permission to interact with the element\n- **`ref`** (`string`) *(required)*: Exact target element reference from the page snapshot\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"element\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable element description used to obtain permission to interact with the element\"\n    },\n    \"ref\": {\n      \"type\": \"string\",\n      \"description\": \"Exact target element reference from the page snapshot\"\n    }\n  },\n  \"required\": [\n    \"element\",\n    \"ref\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_select_option`\n\nSelect an option in a dropdown\n\n*Flags: destructive*\n\n### Parameters\n\n- **`element`** (`string`) *(required)*: Human-readable element description used to obtain permission to interact with the element\n- **`ref`** (`string`) *(required)*: Exact target element reference from the page snapshot\n- **`values`** (`array`) *(required)*: Array of values to select in the dropdown. This can be a single value or multiple values.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"element\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable element description used to obtain permission to interact with the element\"\n    },\n    \"ref\": {\n      \"type\": \"string\",\n      \"description\": \"Exact target element reference from the page snapshot\"\n    },\n    \"values\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"description\": \"Array of values to select in the dropdown. This can be a single value or multiple values.\"\n    }\n  },\n  \"required\": [\n    \"element\",\n    \"ref\",\n    \"values\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_tabs`\n\nList, create, close, or select a browser tab.\n\n*Flags: destructive*\n\n### Parameters\n\n- **`action`** (`string`) *(required)*: Operation to perform\n- **`index`** (`number`): Tab index, used for close/select. If omitted for close, current tab is closed.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"action\": {\n      \"type\": \"string\",\n      \"enum\": [\n        \"list\",\n        \"new\",\n        \"close\",\n        \"select\"\n      ],\n      \"description\": \"Operation to perform\"\n    },\n    \"index\": {\n      \"type\": \"number\",\n      \"description\": \"Tab index, used for close/select. If omitted for close, current tab is closed.\"\n    }\n  },\n  \"required\": [\n    \"action\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_wait_for`\n\nWait for text to appear or disappear or a specified time to pass\n\n*Flags: read-only*\n\n### Parameters\n\n- **`time`** (`number`): The time to wait in seconds\n- **`text`** (`string`): The text to wait for\n- **`textGone`** (`string`): The text to wait for to disappear\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"time\": {\n      \"type\": \"number\",\n      \"description\": \"The time to wait in seconds\"\n    },\n    \"text\": {\n      \"type\": \"string\",\n      \"description\": \"The text to wait for\"\n    },\n    \"textGone\": {\n      \"type\": \"string\",\n      \"description\": \"The text to wait for to disappear\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n",
        "skills/cover_letter_generator/.claude/skills/browsing-with-playwright/SKILL.md": "---\nname: browsing-with-playwright\ndescription: Browser automation using Playwright MCP. Navigate websites, fill forms, click elements, take screenshots, and extract data. Use for web browsing, form submission, web scraping, or UI testing. NOT for static content (use curl/wget).\n---\n\n# Browser Automation\n\nAutomate browser interactions via Playwright MCP server.\n\n## Server Lifecycle\n\n### Start Server\n```bash\n# Using helper script (recommended)\nbash scripts/start-server.sh\n\n# Or manually\nnpx @playwright/mcp@latest --port 8808 --shared-browser-context &\n```\n\n### Stop Server\n```bash\n# Using helper script (closes browser first)\nbash scripts/stop-server.sh\n\n# Or manually\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_close -p '{}'\npkill -f \"@playwright/mcp\"\n```\n\n### When to Stop\n- **End of task**: Stop when browser work is complete\n- **Long sessions**: Keep running if doing multiple browser tasks\n- **Errors**: Stop and restart if browser becomes unresponsive\n\n**Important:** The `--shared-browser-context` flag is required to maintain browser state across multiple mcp-client.py calls. Without it, each call gets a fresh browser context.\n\n## Quick Reference\n\n### Navigation\n\n```bash\n# Go to URL\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_navigate \\\n  -p '{\"url\": \"https://example.com\"}'\n\n# Go back\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_navigate_back -p '{}'\n```\n\n### Get Page State\n\n```bash\n# Accessibility snapshot (returns element refs for clicking/typing)\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_snapshot -p '{}'\n\n# Screenshot\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_take_screenshot \\\n  -p '{\"type\": \"png\", \"fullPage\": true}'\n```\n\n### Interact with Elements\n\nUse `ref` from snapshot output to target elements:\n\n```bash\n# Click element\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_click \\\n  -p '{\"element\": \"Submit button\", \"ref\": \"e42\"}'\n\n# Type text\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_type \\\n  -p '{\"element\": \"Search input\", \"ref\": \"e15\", \"text\": \"hello world\", \"submit\": true}'\n\n# Fill form (multiple fields)\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_fill_form \\\n  -p '{\"fields\": [{\"ref\": \"e10\", \"value\": \"john@example.com\"}, {\"ref\": \"e12\", \"value\": \"password123\"}]}'\n\n# Select dropdown\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_select_option \\\n  -p '{\"element\": \"Country dropdown\", \"ref\": \"e20\", \"values\": [\"US\"]}'\n```\n\n### Wait for Conditions\n\n```bash\n# Wait for text to appear\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_wait_for \\\n  -p '{\"text\": \"Success\"}'\n\n# Wait for time (ms)\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_wait_for \\\n  -p '{\"time\": 2000}'\n```\n\n### Execute JavaScript\n\n```bash\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_evaluate \\\n  -p '{\"function\": \"return document.title\"}'\n```\n\n### Multi-Step Playwright Code\n\nFor complex workflows, use `browser_run_code` to run multiple actions in one call:\n\n```bash\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_run_code \\\n  -p '{\"code\": \"async (page) => { await page.goto(\\\"https://example.com\\\"); await page.click(\\\"text=Learn more\\\"); return await page.title(); }\"}'\n```\n\n**Tip:** Use `browser_run_code` for complex multi-step operations that should be atomic (all-or-nothing).\n\n## Workflow: Form Submission\n\n1. Navigate to page\n2. Get snapshot to find element refs\n3. Fill form fields using refs\n4. Click submit\n5. Wait for confirmation\n6. Screenshot result\n\n## Workflow: Data Extraction\n\n1. Navigate to page\n2. Get snapshot (contains text content)\n3. Use browser_evaluate for complex extraction\n4. Process results\n\n## Verification\n\nRun: `python3 scripts/verify.py`\n\nExpected: ` Playwright MCP server running`\n\n## If Verification Fails\n\n1. Run diagnostic: `pgrep -f \"@playwright/mcp\"`\n2. Check: Server process running on port 8808\n3. Try: `bash scripts/start-server.sh`\n4. **Stop and report** if still failing - do not proceed with downstream steps\n\n## Tool Reference\n\nSee [references/playwright-tools.md](references/playwright-tools.md) for complete tool documentation.\n\n## Troubleshooting\n\n| Issue | Solution |\n|-------|----------|\n| Element not found | Run browser_snapshot first to get current refs |\n| Click fails | Try browser_hover first, then click |\n| Form not submitting | Use `\"submit\": true` with browser_type |\n| Page not loading | Increase wait time or use browser_wait_for |\n| Server not responding | Stop and restart: `bash scripts/stop-server.sh && bash scripts/start-server.sh` |\n",
        "skills/cover_letter_generator/.claude/skills/browsing-with-playwright/references/playwright-tools.md": "# MCP Server Tools\n\n*22 tools available*\n\n## `browser_close`\n\nClose the page\n\n*Flags: destructive*\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {},\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_resize`\n\nResize the browser window\n\n*Flags: destructive*\n\n### Parameters\n\n- **`width`** (`number`) *(required)*: Width of the browser window\n- **`height`** (`number`) *(required)*: Height of the browser window\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"width\": {\n      \"type\": \"number\",\n      \"description\": \"Width of the browser window\"\n    },\n    \"height\": {\n      \"type\": \"number\",\n      \"description\": \"Height of the browser window\"\n    }\n  },\n  \"required\": [\n    \"width\",\n    \"height\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_console_messages`\n\nReturns all console messages\n\n*Flags: read-only*\n\n### Parameters\n\n- **`level`** (`string`): Level of the console messages to return. Each level includes the messages of more severe levels. Defaults to \"info\".\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"level\": {\n      \"type\": \"string\",\n      \"enum\": [\n        \"error\",\n        \"warning\",\n        \"info\",\n        \"debug\"\n      ],\n      \"default\": \"info\",\n      \"description\": \"Level of the console messages to return. Each level includes the messages of more severe levels. Defaults to \\\"info\\\".\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_handle_dialog`\n\nHandle a dialog\n\n*Flags: destructive*\n\n### Parameters\n\n- **`accept`** (`boolean`) *(required)*: Whether to accept the dialog.\n- **`promptText`** (`string`): The text of the prompt in case of a prompt dialog.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"accept\": {\n      \"type\": \"boolean\",\n      \"description\": \"Whether to accept the dialog.\"\n    },\n    \"promptText\": {\n      \"type\": \"string\",\n      \"description\": \"The text of the prompt in case of a prompt dialog.\"\n    }\n  },\n  \"required\": [\n    \"accept\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_evaluate`\n\nEvaluate JavaScript expression on page or element\n\n*Flags: destructive*\n\n### Parameters\n\n- **`function`** (`string`) *(required)*: () => { /* code */ } or (element) => { /* code */ } when element is provided\n- **`element`** (`string`): Human-readable element description used to obtain permission to interact with the element\n- **`ref`** (`string`): Exact target element reference from the page snapshot\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"function\": {\n      \"type\": \"string\",\n      \"description\": \"() => { /* code */ } or (element) => { /* code */ } when element is provided\"\n    },\n    \"element\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable element description used to obtain permission to interact with the element\"\n    },\n    \"ref\": {\n      \"type\": \"string\",\n      \"description\": \"Exact target element reference from the page snapshot\"\n    }\n  },\n  \"required\": [\n    \"function\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_file_upload`\n\nUpload one or multiple files\n\n*Flags: destructive*\n\n### Parameters\n\n- **`paths`** (`array`): The absolute paths to the files to upload. Can be single file or multiple files. If omitted, file chooser is cancelled.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"paths\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"description\": \"The absolute paths to the files to upload. Can be single file or multiple files. If omitted, file chooser is cancelled.\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_fill_form`\n\nFill multiple form fields\n\n*Flags: destructive*\n\n### Parameters\n\n- **`fields`** (`array`) *(required)*: Fields to fill in\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"fields\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"name\": {\n            \"type\": \"string\",\n            \"description\": \"Human-readable field name\"\n          },\n          \"type\": {\n            \"type\": \"string\",\n            \"enum\": [\n              \"textbox\",\n              \"checkbox\",\n              \"radio\",\n              \"combobox\",\n              \"slider\"\n            ],\n            \"description\": \"Type of the field\"\n          },\n          \"ref\": {\n            \"type\": \"string\",\n            \"description\": \"Exact target field reference from the page snapshot\"\n          },\n          \"value\": {\n            \"type\": \"string\",\n            \"description\": \"Value to fill in the field. If the field is a checkbox, the value should be `true` or `false`. If the field is a combobox, the value should be the text of the option.\"\n          }\n        },\n        \"required\": [\n          \"name\",\n          \"type\",\n          \"ref\",\n          \"value\"\n        ],\n        \"additionalProperties\": false\n      },\n      \"description\": \"Fields to fill in\"\n    }\n  },\n  \"required\": [\n    \"fields\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_install`\n\nInstall the browser specified in the config. Call this if you get an error about the browser not being installed.\n\n*Flags: destructive*\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {},\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_press_key`\n\nPress a key on the keyboard\n\n*Flags: destructive*\n\n### Parameters\n\n- **`key`** (`string`) *(required)*: Name of the key to press or a character to generate, such as `ArrowLeft` or `a`\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"key\": {\n      \"type\": \"string\",\n      \"description\": \"Name of the key to press or a character to generate, such as `ArrowLeft` or `a`\"\n    }\n  },\n  \"required\": [\n    \"key\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_type`\n\nType text into editable element\n\n*Flags: destructive*\n\n### Parameters\n\n- **`element`** (`string`) *(required)*: Human-readable element description used to obtain permission to interact with the element\n- **`ref`** (`string`) *(required)*: Exact target element reference from the page snapshot\n- **`text`** (`string`) *(required)*: Text to type into the element\n- **`submit`** (`boolean`): Whether to submit entered text (press Enter after)\n- **`slowly`** (`boolean`): Whether to type one character at a time. Useful for triggering key handlers in the page. By default entire text is filled in at once.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"element\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable element description used to obtain permission to interact with the element\"\n    },\n    \"ref\": {\n      \"type\": \"string\",\n      \"description\": \"Exact target element reference from the page snapshot\"\n    },\n    \"text\": {\n      \"type\": \"string\",\n      \"description\": \"Text to type into the element\"\n    },\n    \"submit\": {\n      \"type\": \"boolean\",\n      \"description\": \"Whether to submit entered text (press Enter after)\"\n    },\n    \"slowly\": {\n      \"type\": \"boolean\",\n      \"description\": \"Whether to type one character at a time. Useful for triggering key handlers in the page. By default entire text is filled in at once.\"\n    }\n  },\n  \"required\": [\n    \"element\",\n    \"ref\",\n    \"text\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_navigate`\n\nNavigate to a URL\n\n*Flags: destructive*\n\n### Parameters\n\n- **`url`** (`string`) *(required)*: The URL to navigate to\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"url\": {\n      \"type\": \"string\",\n      \"description\": \"The URL to navigate to\"\n    }\n  },\n  \"required\": [\n    \"url\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_navigate_back`\n\nGo back to the previous page\n\n*Flags: destructive*\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {},\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_network_requests`\n\nReturns all network requests since loading the page\n\n*Flags: read-only*\n\n### Parameters\n\n- **`includeStatic`** (`boolean`): Whether to include successful static resources like images, fonts, scripts, etc. Defaults to false.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"includeStatic\": {\n      \"type\": \"boolean\",\n      \"default\": false,\n      \"description\": \"Whether to include successful static resources like images, fonts, scripts, etc. Defaults to false.\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_run_code`\n\nRun Playwright code snippet\n\n*Flags: destructive*\n\n### Parameters\n\n- **`code`** (`string`) *(required)*: A JavaScript function containing Playwright code to execute. It will be invoked with a single argument, page, which you can use for any page interaction. For example: `async (page) => { await page.getByRole('button', { name: 'Submit' }).click(); return await page.title(); }`\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"code\": {\n      \"type\": \"string\",\n      \"description\": \"A JavaScript function containing Playwright code to execute. It will be invoked with a single argument, page, which you can use for any page interaction. For example: `async (page) => { await page.getByRole('button', { name: 'Submit' }).click(); return await page.title(); }`\"\n    }\n  },\n  \"required\": [\n    \"code\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_take_screenshot`\n\nTake a screenshot of the current page. You can't perform actions based on the screenshot, use browser_snapshot for actions.\n\n*Flags: read-only*\n\n### Parameters\n\n- **`type`** (`string`): Image format for the screenshot. Default is png.\n- **`filename`** (`string`): File name to save the screenshot to. Defaults to `page-{timestamp}.{png|jpeg}` if not specified. Prefer relative file names to stay within the output directory.\n- **`element`** (`string`): Human-readable element description used to obtain permission to screenshot the element. If not provided, the screenshot will be taken of viewport. If element is provided, ref must be provided too.\n- **`ref`** (`string`): Exact target element reference from the page snapshot. If not provided, the screenshot will be taken of viewport. If ref is provided, element must be provided too.\n- **`fullPage`** (`boolean`): When true, takes a screenshot of the full scrollable page, instead of the currently visible viewport. Cannot be used with element screenshots.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"type\": {\n      \"type\": \"string\",\n      \"enum\": [\n        \"png\",\n        \"jpeg\"\n      ],\n      \"default\": \"png\",\n      \"description\": \"Image format for the screenshot. Default is png.\"\n    },\n    \"filename\": {\n      \"type\": \"string\",\n      \"description\": \"File name to save the screenshot to. Defaults to `page-{timestamp}.{png|jpeg}` if not specified. Prefer relative file names to stay within the output directory.\"\n    },\n    \"element\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable element description used to obtain permission to screenshot the element. If not provided, the screenshot will be taken of viewport. If element is provided, ref must be provided too.\"\n    },\n    \"ref\": {\n      \"type\": \"string\",\n      \"description\": \"Exact target element reference from the page snapshot. If not provided, the screenshot will be taken of viewport. If ref is provided, element must be provided too.\"\n    },\n    \"fullPage\": {\n      \"type\": \"boolean\",\n      \"description\": \"When true, takes a screenshot of the full scrollable page, instead of the currently visible viewport. Cannot be used with element screenshots.\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_snapshot`\n\nCapture accessibility snapshot of the current page, this is better than screenshot\n\n*Flags: read-only*\n\n### Parameters\n\n- **`filename`** (`string`): Save snapshot to markdown file instead of returning it in the response.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"filename\": {\n      \"type\": \"string\",\n      \"description\": \"Save snapshot to markdown file instead of returning it in the response.\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_click`\n\nPerform click on a web page\n\n*Flags: destructive*\n\n### Parameters\n\n- **`element`** (`string`) *(required)*: Human-readable element description used to obtain permission to interact with the element\n- **`ref`** (`string`) *(required)*: Exact target element reference from the page snapshot\n- **`doubleClick`** (`boolean`): Whether to perform a double click instead of a single click\n- **`button`** (`string`): Button to click, defaults to left\n- **`modifiers`** (`array`): Modifier keys to press\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"element\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable element description used to obtain permission to interact with the element\"\n    },\n    \"ref\": {\n      \"type\": \"string\",\n      \"description\": \"Exact target element reference from the page snapshot\"\n    },\n    \"doubleClick\": {\n      \"type\": \"boolean\",\n      \"description\": \"Whether to perform a double click instead of a single click\"\n    },\n    \"button\": {\n      \"type\": \"string\",\n      \"enum\": [\n        \"left\",\n        \"right\",\n        \"middle\"\n      ],\n      \"description\": \"Button to click, defaults to left\"\n    },\n    \"modifiers\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\",\n        \"enum\": [\n          \"Alt\",\n          \"Control\",\n          \"ControlOrMeta\",\n          \"Meta\",\n          \"Shift\"\n        ]\n      },\n      \"description\": \"Modifier keys to press\"\n    }\n  },\n  \"required\": [\n    \"element\",\n    \"ref\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_drag`\n\nPerform drag and drop between two elements\n\n*Flags: destructive*\n\n### Parameters\n\n- **`startElement`** (`string`) *(required)*: Human-readable source element description used to obtain the permission to interact with the element\n- **`startRef`** (`string`) *(required)*: Exact source element reference from the page snapshot\n- **`endElement`** (`string`) *(required)*: Human-readable target element description used to obtain the permission to interact with the element\n- **`endRef`** (`string`) *(required)*: Exact target element reference from the page snapshot\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"startElement\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable source element description used to obtain the permission to interact with the element\"\n    },\n    \"startRef\": {\n      \"type\": \"string\",\n      \"description\": \"Exact source element reference from the page snapshot\"\n    },\n    \"endElement\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable target element description used to obtain the permission to interact with the element\"\n    },\n    \"endRef\": {\n      \"type\": \"string\",\n      \"description\": \"Exact target element reference from the page snapshot\"\n    }\n  },\n  \"required\": [\n    \"startElement\",\n    \"startRef\",\n    \"endElement\",\n    \"endRef\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_hover`\n\nHover over element on page\n\n*Flags: destructive*\n\n### Parameters\n\n- **`element`** (`string`) *(required)*: Human-readable element description used to obtain permission to interact with the element\n- **`ref`** (`string`) *(required)*: Exact target element reference from the page snapshot\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"element\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable element description used to obtain permission to interact with the element\"\n    },\n    \"ref\": {\n      \"type\": \"string\",\n      \"description\": \"Exact target element reference from the page snapshot\"\n    }\n  },\n  \"required\": [\n    \"element\",\n    \"ref\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_select_option`\n\nSelect an option in a dropdown\n\n*Flags: destructive*\n\n### Parameters\n\n- **`element`** (`string`) *(required)*: Human-readable element description used to obtain permission to interact with the element\n- **`ref`** (`string`) *(required)*: Exact target element reference from the page snapshot\n- **`values`** (`array`) *(required)*: Array of values to select in the dropdown. This can be a single value or multiple values.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"element\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable element description used to obtain permission to interact with the element\"\n    },\n    \"ref\": {\n      \"type\": \"string\",\n      \"description\": \"Exact target element reference from the page snapshot\"\n    },\n    \"values\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"description\": \"Array of values to select in the dropdown. This can be a single value or multiple values.\"\n    }\n  },\n  \"required\": [\n    \"element\",\n    \"ref\",\n    \"values\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_tabs`\n\nList, create, close, or select a browser tab.\n\n*Flags: destructive*\n\n### Parameters\n\n- **`action`** (`string`) *(required)*: Operation to perform\n- **`index`** (`number`): Tab index, used for close/select. If omitted for close, current tab is closed.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"action\": {\n      \"type\": \"string\",\n      \"enum\": [\n        \"list\",\n        \"new\",\n        \"close\",\n        \"select\"\n      ],\n      \"description\": \"Operation to perform\"\n    },\n    \"index\": {\n      \"type\": \"number\",\n      \"description\": \"Tab index, used for close/select. If omitted for close, current tab is closed.\"\n    }\n  },\n  \"required\": [\n    \"action\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_wait_for`\n\nWait for text to appear or disappear or a specified time to pass\n\n*Flags: read-only*\n\n### Parameters\n\n- **`time`** (`number`): The time to wait in seconds\n- **`text`** (`string`): The text to wait for\n- **`textGone`** (`string`): The text to wait for to disappear\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"time\": {\n      \"type\": \"number\",\n      \"description\": \"The time to wait in seconds\"\n    },\n    \"text\": {\n      \"type\": \"string\",\n      \"description\": \"The text to wait for\"\n    },\n    \"textGone\": {\n      \"type\": \"string\",\n      \"description\": \"The text to wait for to disappear\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n",
        "skills/cover_letter_generator/.claude/skills/context7-efficient/SKILL.md": "---\nname: context7-efficient\ndescription: Token-efficient library documentation fetcher using Context7 MCP with 86.8% token savings through intelligent shell pipeline filtering. Fetches code examples, API references, and best practices for JavaScript, Python, Go, Rust, and other libraries. Use when users ask about library documentation, need code examples, want API usage patterns, are learning a new framework, need syntax reference, or troubleshooting with library-specific information. Triggers include questions like \"Show me React hooks\", \"How do I use Prisma\", \"What's the Next.js routing syntax\", or any request for library/framework documentation.\n---\n\n# Context7 Efficient Documentation Fetcher\n\nFetch library documentation with automatic 77% token reduction via shell pipeline.\n\n## Quick Start\n\n**Always use the token-efficient shell pipeline:**\n\n```bash\n# Automatic library resolution + filtering\nbash scripts/fetch-docs.sh --library <library-name> --topic <topic>\n\n# Examples:\nbash scripts/fetch-docs.sh --library react --topic useState\nbash scripts/fetch-docs.sh --library nextjs --topic routing\nbash scripts/fetch-docs.sh --library prisma --topic queries\n```\n\n**Result:** Returns ~205 tokens instead of ~934 tokens (77% savings).\n\n## Standard Workflow\n\nFor any documentation request, follow this workflow:\n\n### 1. Identify Library and Topic\n\nExtract from user query:\n- **Library:** React, Next.js, Prisma, Express, etc.\n- **Topic:** Specific feature (hooks, routing, queries, etc.)\n\n### 2. Fetch with Shell Pipeline\n\n```bash\nbash scripts/fetch-docs.sh --library <library> --topic <topic> --verbose\n```\n\nThe `--verbose` flag shows token savings statistics.\n\n### 3. Use Filtered Output\n\nThe script automatically:\n- Fetches full documentation (934 tokens, stays in subprocess)\n- Filters to code examples + API signatures + key notes\n- Returns only essential content (205 tokens to Claude)\n\n## Parameters\n\n### Basic Usage\n\n```bash\nbash scripts/fetch-docs.sh [OPTIONS]\n```\n\n**Required (pick one):**\n- `--library <name>` - Library name (e.g., \"react\", \"nextjs\")\n- `--library-id <id>` - Direct Context7 ID (faster, skips resolution)\n\n**Optional:**\n- `--topic <topic>` - Specific feature to focus on\n- `--mode <code|info>` - code for examples (default), info for concepts\n- `--page <1-10>` - Pagination for more results\n- `--verbose` - Show token savings statistics\n\n### Mode Selection\n\n**Code Mode (default):** Returns code examples + API signatures\n```bash\n--mode code\n```\n\n**Info Mode:** Returns conceptual explanations + fewer examples\n```bash\n--mode info\n```\n\n## Common Library IDs\n\nUse `--library-id` for faster lookup (skips resolution):\n\n```bash\nReact:      /reactjs/react.dev\nNext.js:    /vercel/next.js\nExpress:    /expressjs/express\nPrisma:     /prisma/docs\nMongoDB:    /mongodb/docs\nFastify:    /fastify/fastify\nNestJS:     /nestjs/docs\nVue.js:     /vuejs/docs\nSvelte:     /sveltejs/site\n```\n\n## Workflow Patterns\n\n### Pattern 1: Quick Code Examples\n\nUser asks: \"Show me React useState examples\"\n\n```bash\nbash scripts/fetch-docs.sh --library react --topic useState --verbose\n```\n\nReturns: 5 code examples + API signatures + notes (~205 tokens)\n\n### Pattern 2: Learning New Library\n\nUser asks: \"How do I get started with Prisma?\"\n\n```bash\n# Step 1: Get overview\nbash scripts/fetch-docs.sh --library prisma --topic \"getting started\" --mode info\n\n# Step 2: Get code examples\nbash scripts/fetch-docs.sh --library prisma --topic queries --mode code\n```\n\n### Pattern 3: Specific Feature Lookup\n\nUser asks: \"How does Next.js routing work?\"\n\n```bash\nbash scripts/fetch-docs.sh --library-id /vercel/next.js --topic routing\n```\n\nUsing `--library-id` is faster when you know the exact ID.\n\n### Pattern 4: Deep Exploration\n\nUser needs comprehensive information:\n\n```bash\n# Page 1: Basic examples\nbash scripts/fetch-docs.sh --library react --topic hooks --page 1\n\n# Page 2: Advanced patterns\nbash scripts/fetch-docs.sh --library react --topic hooks --page 2\n```\n\n## Token Efficiency\n\n**How it works:**\n\n1. `fetch-docs.sh` calls `fetch-raw.sh` (which uses `mcp-client.py`)\n2. Full response (934 tokens) stays in subprocess memory\n3. Shell filters (awk/grep/sed) extract essentials (0 LLM tokens used)\n4. Returns filtered output (205 tokens) to Claude\n\n**Savings:**\n- Direct MCP: 934 tokens per query\n- This approach: 205 tokens per query\n- **77% reduction**\n\n**Do NOT use `mcp-client.py` directly** - it bypasses filtering and wastes tokens.\n\n## Advanced: Library Resolution\n\nIf library name fails, try variations:\n\n```bash\n# Try different formats\n--library \"next.js\"    # with dot\n--library \"nextjs\"     # without dot\n--library \"next\"       # short form\n\n# Or search manually\nbash scripts/fetch-docs.sh --library \"your-library\" --verbose\n# Check output for suggested library IDs\n```\n\n## Troubleshooting\n\n| Issue | Solution |\n|-------|----------|\n| Library not found | Try name variations or use broader search term |\n| No results | Use `--mode info` or broader topic |\n| Need more examples | Increase page: `--page 2` |\n| Want full context | Use `--mode info` for explanations |\n\n## References\n\nFor detailed Context7 MCP tool documentation, see:\n- [references/context7-tools.md](references/context7-tools.md) - Complete tool reference\n\n## Implementation Notes\n\n**Components (for reference only, use fetch-docs.sh):**\n- `mcp-client.py` - Universal MCP client (foundation)\n- `fetch-raw.sh` - MCP wrapper\n- `extract-code-blocks.sh` - Code example filter (awk)\n- `extract-signatures.sh` - API signature filter (awk)\n- `extract-notes.sh` - Important notes filter (grep)\n- `fetch-docs.sh` - **Main orchestrator (ALWAYS USE THIS)**\n\n**Architecture:**\nShell pipeline processes documentation in subprocess, keeping full response out of Claude's context. Only filtered essentials enter the LLM context, achieving 77% token savings with 100% functionality preserved.\n\nBased on [Anthropic's \"Code Execution with MCP\" blog post](https://www.anthropic.com/engineering/code-execution-with-mcp).\n",
        "skills/cover_letter_generator/.claude/skills/context7-efficient/references/context7-tools.md": "# Context7 MCP Tools\n\n*2 tools available*\n\n## `resolve-library-id`\n\nResolves a package/product name to a Context7-compatible library ID and returns a list of matching libraries.\n\nYou MUST call this function before 'get-library-docs' to obtain a valid Context7-compatible library ID UNLESS the user explicitly provides a library ID in the format '/org/project' or '/org/project/version' in their query.\n\n### Selection Process\n\n1. Analyze the query to understand what library/package the user is looking for\n2. Return the most relevant match based on:\n   - Name similarity to the query (exact matches prioritized)\n   - Description relevance to the query's intent\n   - Documentation coverage (prioritize libraries with higher Code Snippet counts)\n   - Source reputation (consider libraries with High or Medium reputation more authoritative)\n   - Benchmark Score: Quality indicator (100 is the highest score)\n\n### Parameters\n\n- **`libraryName`** (`string`) *(required)*: Library name to search for and retrieve a Context7-compatible library ID\n\n### Response Format\n\nReturns a list of matching libraries with:\n- Title\n- Context7-compatible library ID (e.g., `/reactjs/react.dev`)\n- Code Snippets count\n- Source Reputation (High/Medium/Low)\n- Benchmark Score\n- Description\n\n### Examples\n\n```bash\n# Find React library\npython3 scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t resolve-library-id \\\n  -p '{\"libraryName\": \"react\"}'\n\n# Find Next.js library\npython3 scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t resolve-library-id \\\n  -p '{\"libraryName\": \"next.js\"}'\n\n# Find MongoDB library\npython3 scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t resolve-library-id \\\n  -p '{\"libraryName\": \"mongodb\"}'\n```\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"libraryName\": {\n      \"type\": \"string\",\n      \"description\": \"Library name to search for and retrieve a Context7-compatible library ID.\"\n    }\n  },\n  \"required\": [\"libraryName\"]\n}\n```\n</details>\n\n## `get-library-docs`\n\nFetches up-to-date documentation for a library. You must call 'resolve-library-id' first to obtain the exact Context7-compatible library ID required to use this tool, UNLESS the user explicitly provides a library ID in the format '/org/project' or '/org/project/version' in their query.\n\nUse mode='code' (default) for API references and code examples, or mode='info' for conceptual guides, narrative information, and architectural questions.\n\n### Parameters\n\n- **`context7CompatibleLibraryID`** (`string`) *(required)*: Exact Context7-compatible library ID (e.g., '/mongodb/docs', '/vercel/next.js', '/supabase/supabase', '/vercel/next.js/v14.3.0-canary.87') retrieved from 'resolve-library-id' or directly from user query in the format '/org/project' or '/org/project/version'\n\n- **`topic`** (`string`) *(optional)*: Topic to focus documentation on (e.g., 'hooks', 'routing')\n\n- **`mode`** (`string`) *(optional, default: \"code\")*: Documentation mode\n  - `code`: API references and code examples (default)\n  - `info`: Conceptual guides, narrative information, and architectural questions\n\n- **`page`** (`integer`) *(optional, default: 1)*: Page number for pagination (start: 1, default: 1). If the context is not sufficient, try page=2, page=3, page=4, etc. with the same topic. Range: 1-10\n\n### Examples\n\n```bash\n# Get React hooks documentation (code mode)\npython3 scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t get-library-docs \\\n  -p '{\"context7CompatibleLibraryID\": \"/reactjs/react.dev\", \"topic\": \"hooks\", \"mode\": \"code\", \"page\": 1}'\n\n# Get conceptual information about Next.js routing\npython3 scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t get-library-docs \\\n  -p '{\"context7CompatibleLibraryID\": \"/vercel/nextjs.org\", \"topic\": \"routing\", \"mode\": \"info\"}'\n\n# Get MongoDB aggregation examples\npython3 scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t get-library-docs \\\n  -p '{\"context7CompatibleLibraryID\": \"/mongodb/docs\", \"topic\": \"aggregation\", \"mode\": \"code\"}'\n\n# Get additional pages for more details\npython3 scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t get-library-docs \\\n  -p '{\"context7CompatibleLibraryID\": \"/reactjs/react.dev\", \"topic\": \"hooks\", \"mode\": \"code\", \"page\": 2}'\n```\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"context7CompatibleLibraryID\": {\n      \"type\": \"string\",\n      \"description\": \"Exact Context7-compatible library ID (e.g., '/mongodb/docs', '/vercel/next.js', '/supabase/supabase', '/vercel/next.js/v14.3.0-canary.87') retrieved from 'resolve-library-id' or directly from user query in the format '/org/project' or '/org/project/version'.\"\n    },\n    \"topic\": {\n      \"type\": \"string\",\n      \"description\": \"Topic to focus documentation on (e.g., 'hooks', 'routing').\"\n    },\n    \"mode\": {\n      \"type\": \"string\",\n      \"enum\": [\"code\", \"info\"],\n      \"default\": \"code\",\n      \"description\": \"Documentation mode: 'code' for API references and code examples (default), 'info' for conceptual guides, narrative information, and architectural questions.\"\n    },\n    \"page\": {\n      \"type\": \"integer\",\n      \"minimum\": 1,\n      \"maximum\": 10,\n      \"default\": 1,\n      \"description\": \"Page number for pagination (start: 1, default: 1). If the context is not sufficient, try page=2, page=3, page=4, etc. with the same topic.\"\n    }\n  },\n  \"required\": [\"context7CompatibleLibraryID\"]\n}\n```\n</details>\n\n## Usage Patterns\n\n### Pattern 1: Unknown Library\n\nWhen you don't know the exact library ID:\n\n```bash\n# Step 1: Resolve library name\npython3 scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t resolve-library-id -p '{\"libraryName\": \"express\"}'\n\n# Step 2: Use returned ID to fetch docs\npython3 scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t get-library-docs \\\n  -p '{\"context7CompatibleLibraryID\": \"/expressjs/expressjs.com\", \"topic\": \"middleware\"}'\n```\n\n### Pattern 2: Known Library ID\n\nWhen you know the library ID:\n\n```bash\n# Direct fetch (skip resolve step)\npython3 scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t get-library-docs \\\n  -p '{\"context7CompatibleLibraryID\": \"/reactjs/react.dev\", \"topic\": \"useState\"}'\n```\n\n### Pattern 3: Exploring Multiple Topics\n\n```bash\n# Get overview first\npython3 scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t get-library-docs \\\n  -p '{\"context7CompatibleLibraryID\": \"/prisma/docs\", \"topic\": \"getting started\", \"mode\": \"info\"}'\n\n# Then drill into specifics\npython3 scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t get-library-docs \\\n  -p '{\"context7CompatibleLibraryID\": \"/prisma/docs\", \"topic\": \"queries\", \"mode\": \"code\"}'\n```\n\n### Pattern 4: Pagination for Deep Research\n\n```bash\n# Get first page\npython3 scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t get-library-docs \\\n  -p '{\"context7CompatibleLibraryID\": \"/reactjs/react.dev\", \"topic\": \"hooks\", \"page\": 1}'\n\n# Get additional pages as needed\npython3 scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t get-library-docs \\\n  -p '{\"context7CompatibleLibraryID\": \"/reactjs/react.dev\", \"topic\": \"hooks\", \"page\": 2}'\n```\n\n## Common Library IDs\n\nQuick reference for popular libraries:\n\n| Library | Context7 ID |\n|---------|-------------|\n| React | `/reactjs/react.dev` |\n| Next.js | `/vercel/nextjs.org` |\n| Express | `/expressjs/expressjs.com` |\n| MongoDB | `/mongodb/docs` |\n| Prisma | `/prisma/docs` |\n| Vue | `/vuejs/docs` |\n| Svelte | `/sveltejs/svelte.dev` |\n| FastAPI | `/tiangolo/fastapi` |\n| Django | `/django/docs` |\n\n## Tips\n\n1. **Library Resolution**: Always use `resolve-library-id` first unless you have the exact ID\n2. **Mode Selection**: Use `code` mode for examples, `info` mode for concepts\n3. **Topic Specificity**: More specific topics yield better results\n4. **Pagination**: If results are insufficient, try `page: 2` or refine the topic\n5. **Fallback**: If no results, try broader topics or switch modes\n",
        "skills/cover_letter_generator/.claude/skills/cover-letter-generator/SKILL.md": "---\nname: cover-letter-generator\ndescription: |\n  Generate tailored AI-focused cover letters using the PSI (Problem-Solution-Impact) methodology.\n  Use when: (1) User wants to create cover letters for AI/ML job applications, (2) User provides a resume and wants LinkedIn job matching, (3) User asks for personalized cover letters based on job postings, (4) User mentions applying for AI Engineer, ML Engineer, or similar technical roles.\n  Integrates market intelligence, LinkedIn research via Playwright, and professional writing standards.\n---\n\n# Cover Letter Generator\n\nGenerate PSI-formatted cover letters tailored to LinkedIn AI job postings.\n\n## Workflow\n\n### Step 1: Analyze Resume\n\nExtract and analyze the applicant's resume:\n\n```bash\npython3 scripts/extract_resume.py \"<path_to_resume.docx>\"\n```\n\nIdentify from the resume:\n- **Core technical stack**: Languages, frameworks, platforms\n- **Quantified achievements**: Metrics, percentages, business outcomes\n- **Domain experience**: Industries, project types, team sizes\n- **AI/ML specific skills**: Models, pipelines, tools\n\n### Step 2: Market Intelligence\n\nBased on the resume profile, identify the **top 3 AI skills currently in demand**:\n\nCommon high-demand AI skills (2024-2025):\n- RAG (Retrieval-Augmented Generation) pipelines\n- Agentic AI workflows (LangGraph, AutoGen, CrewAI)\n- LLMOps / MLOps (deployment, monitoring, fine-tuning)\n- Prompt engineering & context optimization\n- Vector databases & semantic search\n- Multi-modal AI systems\n\nMatch resume skills to market demand to identify positioning strategy.\n\n### Step 3: LinkedIn Research\n\nUse `browsing-with-playwright` skill or Playwright MCP to search LinkedIn for relevant jobs:\n\n**Search Strategy:**\n1. Navigate to LinkedIn Jobs: `https://www.linkedin.com/jobs/`\n2. Search terms combining: `[Primary Skill] + [Secondary Skill] + [Location/Remote]`\n   - Example: \"Agentic AI Developer Remote\"\n   - Example: \"RAG Engineer LLMOps\"\n3. Find **2 relevant job postings** matching the profile\n4. For each job, extract:\n   - Company name and job title\n   - Key technical requirements\n   - Company's AI focus/challenges (from description)\n   - Hiring manager name (if visible)\n\n### Step 4: Bridge the Capability Gap\n\nFor each job posting, create a **PSI mapping**:\n\n| Component | Source | Action |\n|-----------|--------|--------|\n| **Problem** | Job posting | Identify the organization's technical bottleneck |\n| **Solution** | Resume | Map applicant's skills as the solution |\n| **Impact** | Resume | Extract metrics proving ROI capability |\n\n**Constraint**: Never fabricate experience. Reframe existing resume data to address the job's specific challenges.\n\n### Step 5: Generate Cover Letters\n\nCreate 2 cover letters using the PSI template. See [references/psi_template.md](references/psi_template.md).\n\n**Requirements:**\n- Follow PSI format strictly (Problem -> Solution -> Impact)\n- Integrate all 5 quality pillars from [references/quality_pillars.md](references/quality_pillars.md)\n- Maintain professional, technical, impact-oriented tone\n- Ensure \"Translation Layer\" is evident (explaining AI to stakeholders)\n- Hyper-personalize to each company's context\n\n**Quality Checklist:**\n- [ ] Problem identifies company's specific AI challenge\n- [ ] Solution uses concrete tools/methods from resume\n- [ ] Impact includes quantified metrics\n- [ ] Mentions Responsible AI / ethics alignment\n- [ ] Demonstrates learning velocity (current tech awareness)\n- [ ] References company-specific information\n- [ ] Written with clarity for non-technical readers\n\n## Output Format\n\nDeliver 2 complete cover letters, each with:\n1. Header (name, LinkedIn, GitHub)\n2. Subject line targeting company's challenge\n3. PSI body paragraphs\n4. Professional closing\n\nSave as: `cover_letter_[company_name].md`\n",
        "skills/cover_letter_generator/.claude/skills/cover-letter-generator/references/psi_template.md": "# PSI Cover Letter Template\n\nProblem-Solution-Impact (PSI) format integrating the five core qualities: impact, ethics, communication, learning velocity, and personalization.\n\n## Format\n\n```\n[Your Name]\n[Your LinkedIn Profile URL] | [Your Portfolio/GitHub]\n\nSubject: [Job Title]  Solving [Company Name]'s [Specific AI Challenge]\n\nDear [Hiring Manager Name or \"AI Talent Team\"],\n\n[PROBLEM] I noticed in [Company Name]'s recent technical blog post that you are currently scaling your [specific system, e.g., Agentic workflow or RAG pipeline], which often introduces significant challenges in model latency and hallucination management. In the rapidly evolving AI landscape, many organizations struggle to move past the \"prototype phase\" because they lack models that are both high-performing and ethically transparent. I am writing to you because I specialize in bridging this gapensuring that cutting-edge SOTA architectures aren't just technically impressive, but are production-ready, explainable, and aligned with business ROI.\n\n[SOLUTION] To address similar bottlenecks in my previous role, I didn't just deploy standard models; I engineered a [Specific Solution, e.g., custom fine-tuning strategy or hybrid retrieval-augmented system] using [Specific Tools, e.g., PyTorch and LangGraph]. My approach centers on Responsible AI; I prioritize building \"The Translation Layer\" by creating visualization dashboards that explain model decision-making to non-technical stakeholders. Furthermore, I maintain high learning velocity by actively implementing recent research from arXiv, such as [mention a recent trend like \"Mamba architectures\" or \"Speculative Decoding\"], ensuring your stack remains future-proof.\n\n[IMPACT] The result of this methodology was a [Quantitative Impact, e.g., 35% reduction in inference costs and a 20% boost in retrieval accuracy] for [Previous Project/Company]. By hyper-personalizing AI solutions to meet specific user needs while maintaining strict ethical audits, I helped the organization realize immediate ROI. I am eager to bring this same focus on quantified impact and technical agility to the [Job Title] role at [Company Name]. I look forward to discussing how my background in [Your Niche] can help your team dominate the [Specific Industry] market.\n\nBest regards,\n\n[Your Name]\n```\n\n## Key Sections\n\n1. **PROBLEM**: Identify the company's specific technical bottleneck\n2. **SOLUTION**: Map your skills/experience as the solution using specific tools\n3. **IMPACT**: Provide quantitative results proving ROI\n",
        "skills/cover_letter_generator/.claude/skills/cover-letter-generator/references/quality_pillars.md": "# Five Quality Pillars for AI Cover Letters\n\n## 1. Evidence of AI Problem-Solving (Not Just Tools)\n\nFocus on business impact, not tool lists. Instead of \"I know NLP,\" say \"I implemented a transformer-based model that reduced customer support response times by 40%.\"\n\n**Why**: Proves you understand AI is a tool for ROI, not a laboratory experiment.\n\n## 2. Alignment with Company's AI Ethics\n\nMention commitment to Responsible AI or Explainability (XAI). Show understanding of how to audit models for bias.\n\n**Why**: Shows you're a mature professional who understands societal risks of the technology, not just a coder.\n\n## 3. The Translation Layer (Communication Skills)\n\nWrite with high-level clarity. Mention experience \"presenting model performance metrics to non-technical executive leadership.\"\n\n**Why**: Most AI projects fail due to poor communication between data and business teams. Prove you bridge that gap.\n\n## 4. Technical Agility & Learning Velocity\n\nDemonstrate staying current with research papers (arXiv) or new architectures (Diffusion models, Mamba, etc.).\n\n**Why**: Reassures hiring managers you won't become technically stagnant as the field evolves.\n\n## 5. Hyper-Personalization to LinkedIn Context\n\nReference a recent post by the company's Head of AI or a specific technical challenge the company blogged about.\n\n**Why**: Proves you've done research and are genuinely interested in their specific stack and missionnot sending a mass-blast template.\n",
        "skills/cover_letter_generator/.claude/skills/doc-coauthoring/SKILL.md": "---\nname: doc-coauthoring\ndescription: Guide users through a structured workflow for co-authoring documentation. Use when user wants to write documentation, proposals, technical specs, decision docs, or similar structured content. This workflow helps users efficiently transfer context, refine content through iteration, and verify the doc works for readers. Trigger when user mentions writing docs, creating proposals, drafting specs, or similar documentation tasks.\n---\n\n# Doc Co-Authoring Workflow\n\nThis skill provides a structured workflow for guiding users through collaborative document creation. Act as an active guide, walking users through three stages: Context Gathering, Refinement & Structure, and Reader Testing.\n\n## When to Offer This Workflow\n\n**Trigger conditions:**\n- User mentions writing documentation: \"write a doc\", \"draft a proposal\", \"create a spec\", \"write up\"\n- User mentions specific doc types: \"PRD\", \"design doc\", \"decision doc\", \"RFC\"\n- User seems to be starting a substantial writing task\n\n**Initial offer:**\nOffer the user a structured workflow for co-authoring the document. Explain the three stages:\n\n1. **Context Gathering**: User provides all relevant context while Claude asks clarifying questions\n2. **Refinement & Structure**: Iteratively build each section through brainstorming and editing\n3. **Reader Testing**: Test the doc with a fresh Claude (no context) to catch blind spots before others read it\n\nExplain that this approach helps ensure the doc works well when others read it (including when they paste it into Claude). Ask if they want to try this workflow or prefer to work freeform.\n\nIf user declines, work freeform. If user accepts, proceed to Stage 1.\n\n## Stage 1: Context Gathering\n\n**Goal:** Close the gap between what the user knows and what Claude knows, enabling smart guidance later.\n\n### Initial Questions\n\nStart by asking the user for meta-context about the document:\n\n1. What type of document is this? (e.g., technical spec, decision doc, proposal)\n2. Who's the primary audience?\n3. What's the desired impact when someone reads this?\n4. Is there a template or specific format to follow?\n5. Any other constraints or context to know?\n\nInform them they can answer in shorthand or dump information however works best for them.\n\n**If user provides a template or mentions a doc type:**\n- Ask if they have a template document to share\n- If they provide a link to a shared document, use the appropriate integration to fetch it\n- If they provide a file, read it\n\n**If user mentions editing an existing shared document:**\n- Use the appropriate integration to read the current state\n- Check for images without alt-text\n- If images exist without alt-text, explain that when others use Claude to understand the doc, Claude won't be able to see them. Ask if they want alt-text generated. If so, request they paste each image into chat for descriptive alt-text generation.\n\n### Info Dumping\n\nOnce initial questions are answered, encourage the user to dump all the context they have. Request information such as:\n- Background on the project/problem\n- Related team discussions or shared documents\n- Why alternative solutions aren't being used\n- Organizational context (team dynamics, past incidents, politics)\n- Timeline pressures or constraints\n- Technical architecture or dependencies\n- Stakeholder concerns\n\nAdvise them not to worry about organizing it - just get it all out. Offer multiple ways to provide context:\n- Info dump stream-of-consciousness\n- Point to team channels or threads to read\n- Link to shared documents\n\n**If integrations are available** (e.g., Slack, Teams, Google Drive, SharePoint, or other MCP servers), mention that these can be used to pull in context directly.\n\n**If no integrations are detected and in Claude.ai or Claude app:** Suggest they can enable connectors in their Claude settings to allow pulling context from messaging apps and document storage directly.\n\nInform them clarifying questions will be asked once they've done their initial dump.\n\n**During context gathering:**\n\n- If user mentions team channels or shared documents:\n  - If integrations available: Inform them the content will be read now, then use the appropriate integration\n  - If integrations not available: Explain lack of access. Suggest they enable connectors in Claude settings, or paste the relevant content directly.\n\n- If user mentions entities/projects that are unknown:\n  - Ask if connected tools should be searched to learn more\n  - Wait for user confirmation before searching\n\n- As user provides context, track what's being learned and what's still unclear\n\n**Asking clarifying questions:**\n\nWhen user signals they've done their initial dump (or after substantial context provided), ask clarifying questions to ensure understanding:\n\nGenerate 5-10 numbered questions based on gaps in the context.\n\nInform them they can use shorthand to answer (e.g., \"1: yes, 2: see #channel, 3: no because backwards compat\"), link to more docs, point to channels to read, or just keep info-dumping. Whatever's most efficient for them.\n\n**Exit condition:**\nSufficient context has been gathered when questions show understanding - when edge cases and trade-offs can be asked about without needing basics explained.\n\n**Transition:**\nAsk if there's any more context they want to provide at this stage, or if it's time to move on to drafting the document.\n\nIf user wants to add more, let them. When ready, proceed to Stage 2.\n\n## Stage 2: Refinement & Structure\n\n**Goal:** Build the document section by section through brainstorming, curation, and iterative refinement.\n\n**Instructions to user:**\nExplain that the document will be built section by section. For each section:\n1. Clarifying questions will be asked about what to include\n2. 5-20 options will be brainstormed\n3. User will indicate what to keep/remove/combine\n4. The section will be drafted\n5. It will be refined through surgical edits\n\nStart with whichever section has the most unknowns (usually the core decision/proposal), then work through the rest.\n\n**Section ordering:**\n\nIf the document structure is clear:\nAsk which section they'd like to start with.\n\nSuggest starting with whichever section has the most unknowns. For decision docs, that's usually the core proposal. For specs, it's typically the technical approach. Summary sections are best left for last.\n\nIf user doesn't know what sections they need:\nBased on the type of document and template, suggest 3-5 sections appropriate for the doc type.\n\nAsk if this structure works, or if they want to adjust it.\n\n**Once structure is agreed:**\n\nCreate the initial document structure with placeholder text for all sections.\n\n**If access to artifacts is available:**\nUse `create_file` to create an artifact. This gives both Claude and the user a scaffold to work from.\n\nInform them that the initial structure with placeholders for all sections will be created.\n\nCreate artifact with all section headers and brief placeholder text like \"[To be written]\" or \"[Content here]\".\n\nProvide the scaffold link and indicate it's time to fill in each section.\n\n**If no access to artifacts:**\nCreate a markdown file in the working directory. Name it appropriately (e.g., `decision-doc.md`, `technical-spec.md`).\n\nInform them that the initial structure with placeholders for all sections will be created.\n\nCreate file with all section headers and placeholder text.\n\nConfirm the filename has been created and indicate it's time to fill in each section.\n\n**For each section:**\n\n### Step 1: Clarifying Questions\n\nAnnounce work will begin on the [SECTION NAME] section. Ask 5-10 clarifying questions about what should be included:\n\nGenerate 5-10 specific questions based on context and section purpose.\n\nInform them they can answer in shorthand or just indicate what's important to cover.\n\n### Step 2: Brainstorming\n\nFor the [SECTION NAME] section, brainstorm [5-20] things that might be included, depending on the section's complexity. Look for:\n- Context shared that might have been forgotten\n- Angles or considerations not yet mentioned\n\nGenerate 5-20 numbered options based on section complexity. At the end, offer to brainstorm more if they want additional options.\n\n### Step 3: Curation\n\nAsk which points should be kept, removed, or combined. Request brief justifications to help learn priorities for the next sections.\n\nProvide examples:\n- \"Keep 1,4,7,9\"\n- \"Remove 3 (duplicates 1)\"\n- \"Remove 6 (audience already knows this)\"\n- \"Combine 11 and 12\"\n\n**If user gives freeform feedback** (e.g., \"looks good\" or \"I like most of it but...\") instead of numbered selections, extract their preferences and proceed. Parse what they want kept/removed/changed and apply it.\n\n### Step 4: Gap Check\n\nBased on what they've selected, ask if there's anything important missing for the [SECTION NAME] section.\n\n### Step 5: Drafting\n\nUse `str_replace` to replace the placeholder text for this section with the actual drafted content.\n\nAnnounce the [SECTION NAME] section will be drafted now based on what they've selected.\n\n**If using artifacts:**\nAfter drafting, provide a link to the artifact.\n\nAsk them to read through it and indicate what to change. Note that being specific helps learning for the next sections.\n\n**If using a file (no artifacts):**\nAfter drafting, confirm completion.\n\nInform them the [SECTION NAME] section has been drafted in [filename]. Ask them to read through it and indicate what to change. Note that being specific helps learning for the next sections.\n\n**Key instruction for user (include when drafting the first section):**\nProvide a note: Instead of editing the doc directly, ask them to indicate what to change. This helps learning of their style for future sections. For example: \"Remove the X bullet - already covered by Y\" or \"Make the third paragraph more concise\".\n\n### Step 6: Iterative Refinement\n\nAs user provides feedback:\n- Use `str_replace` to make edits (never reprint the whole doc)\n- **If using artifacts:** Provide link to artifact after each edit\n- **If using files:** Just confirm edits are complete\n- If user edits doc directly and asks to read it: mentally note the changes they made and keep them in mind for future sections (this shows their preferences)\n\n**Continue iterating** until user is satisfied with the section.\n\n### Quality Checking\n\nAfter 3 consecutive iterations with no substantial changes, ask if anything can be removed without losing important information.\n\nWhen section is done, confirm [SECTION NAME] is complete. Ask if ready to move to the next section.\n\n**Repeat for all sections.**\n\n### Near Completion\n\nAs approaching completion (80%+ of sections done), announce intention to re-read the entire document and check for:\n- Flow and consistency across sections\n- Redundancy or contradictions\n- Anything that feels like \"slop\" or generic filler\n- Whether every sentence carries weight\n\nRead entire document and provide feedback.\n\n**When all sections are drafted and refined:**\nAnnounce all sections are drafted. Indicate intention to review the complete document one more time.\n\nReview for overall coherence, flow, completeness.\n\nProvide any final suggestions.\n\nAsk if ready to move to Reader Testing, or if they want to refine anything else.\n\n## Stage 3: Reader Testing\n\n**Goal:** Test the document with a fresh Claude (no context bleed) to verify it works for readers.\n\n**Instructions to user:**\nExplain that testing will now occur to see if the document actually works for readers. This catches blind spots - things that make sense to the authors but might confuse others.\n\n### Testing Approach\n\n**If access to sub-agents is available (e.g., in Claude Code):**\n\nPerform the testing directly without user involvement.\n\n### Step 1: Predict Reader Questions\n\nAnnounce intention to predict what questions readers might ask when trying to discover this document.\n\nGenerate 5-10 questions that readers would realistically ask.\n\n### Step 2: Test with Sub-Agent\n\nAnnounce that these questions will be tested with a fresh Claude instance (no context from this conversation).\n\nFor each question, invoke a sub-agent with just the document content and the question.\n\nSummarize what Reader Claude got right/wrong for each question.\n\n### Step 3: Run Additional Checks\n\nAnnounce additional checks will be performed.\n\nInvoke sub-agent to check for ambiguity, false assumptions, contradictions.\n\nSummarize any issues found.\n\n### Step 4: Report and Fix\n\nIf issues found:\nReport that Reader Claude struggled with specific issues.\n\nList the specific issues.\n\nIndicate intention to fix these gaps.\n\nLoop back to refinement for problematic sections.\n\n---\n\n**If no access to sub-agents (e.g., claude.ai web interface):**\n\nThe user will need to do the testing manually.\n\n### Step 1: Predict Reader Questions\n\nAsk what questions people might ask when trying to discover this document. What would they type into Claude.ai?\n\nGenerate 5-10 questions that readers would realistically ask.\n\n### Step 2: Setup Testing\n\nProvide testing instructions:\n1. Open a fresh Claude conversation: https://claude.ai\n2. Paste or share the document content (if using a shared doc platform with connectors enabled, provide the link)\n3. Ask Reader Claude the generated questions\n\nFor each question, instruct Reader Claude to provide:\n- The answer\n- Whether anything was ambiguous or unclear\n- What knowledge/context the doc assumes is already known\n\nCheck if Reader Claude gives correct answers or misinterprets anything.\n\n### Step 3: Additional Checks\n\nAlso ask Reader Claude:\n- \"What in this doc might be ambiguous or unclear to readers?\"\n- \"What knowledge or context does this doc assume readers already have?\"\n- \"Are there any internal contradictions or inconsistencies?\"\n\n### Step 4: Iterate Based on Results\n\nAsk what Reader Claude got wrong or struggled with. Indicate intention to fix those gaps.\n\nLoop back to refinement for any problematic sections.\n\n---\n\n### Exit Condition (Both Approaches)\n\nWhen Reader Claude consistently answers questions correctly and doesn't surface new gaps or ambiguities, the doc is ready.\n\n## Final Review\n\nWhen Reader Testing passes:\nAnnounce the doc has passed Reader Claude testing. Before completion:\n\n1. Recommend they do a final read-through themselves - they own this document and are responsible for its quality\n2. Suggest double-checking any facts, links, or technical details\n3. Ask them to verify it achieves the impact they wanted\n\nAsk if they want one more review, or if the work is done.\n\n**If user wants final review, provide it. Otherwise:**\nAnnounce document completion. Provide a few final tips:\n- Consider linking this conversation in an appendix so readers can see how the doc was developed\n- Use appendices to provide depth without bloating the main doc\n- Update the doc as feedback is received from real readers\n\n## Tips for Effective Guidance\n\n**Tone:**\n- Be direct and procedural\n- Explain rationale briefly when it affects user behavior\n- Don't try to \"sell\" the approach - just execute it\n\n**Handling Deviations:**\n- If user wants to skip a stage: Ask if they want to skip this and write freeform\n- If user seems frustrated: Acknowledge this is taking longer than expected. Suggest ways to move faster\n- Always give user agency to adjust the process\n\n**Context Management:**\n- Throughout, if context is missing on something mentioned, proactively ask\n- Don't let gaps accumulate - address them as they come up\n\n**Artifact Management:**\n- Use `create_file` for drafting full sections\n- Use `str_replace` for all edits\n- Provide artifact link after every change\n- Never use artifacts for brainstorming lists - that's just conversation\n\n**Quality over Speed:**\n- Don't rush through stages\n- Each iteration should make meaningful improvements\n- The goal is a document that actually works for readers\n",
        "skills/cover_letter_generator/.claude/skills/docx/SKILL.md": "---\nname: docx\ndescription: \"Comprehensive document creation, editing, and analysis with support for tracked changes, comments, formatting preservation, and text extraction. When Claude needs to work with professional documents (.docx files) for: (1) Creating new documents, (2) Modifying or editing content, (3) Working with tracked changes, (4) Adding comments, or any other document tasks\"\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# DOCX creation, editing, and analysis\n\n## Overview\n\nA user may ask you to create, edit, or analyze the contents of a .docx file. A .docx file is essentially a ZIP archive containing XML files and other resources that you can read or edit. You have different tools and workflows available for different tasks.\n\n## Workflow Decision Tree\n\n### Reading/Analyzing Content\nUse \"Text extraction\" or \"Raw XML access\" sections below\n\n### Creating New Document\nUse \"Creating a new Word document\" workflow\n\n### Editing Existing Document\n- **Your own document + simple changes**\n  Use \"Basic OOXML editing\" workflow\n\n- **Someone else's document**\n  Use **\"Redlining workflow\"** (recommended default)\n\n- **Legal, academic, business, or government docs**\n  Use **\"Redlining workflow\"** (required)\n\n## Reading and analyzing content\n\n### Text extraction\nIf you just need to read the text contents of a document, you should convert the document to markdown using pandoc. Pandoc provides excellent support for preserving document structure and can show tracked changes:\n\n```bash\n# Convert document to markdown with tracked changes\npandoc --track-changes=all path-to-file.docx -o output.md\n# Options: --track-changes=accept/reject/all\n```\n\n### Raw XML access\nYou need raw XML access for: comments, complex formatting, document structure, embedded media, and metadata. For any of these features, you'll need to unpack a document and read its raw XML contents.\n\n#### Unpacking a file\n`python ooxml/scripts/unpack.py <office_file> <output_directory>`\n\n#### Key file structures\n* `word/document.xml` - Main document contents\n* `word/comments.xml` - Comments referenced in document.xml\n* `word/media/` - Embedded images and media files\n* Tracked changes use `<w:ins>` (insertions) and `<w:del>` (deletions) tags\n\n## Creating a new Word document\n\nWhen creating a new Word document from scratch, use **docx-js**, which allows you to create Word documents using JavaScript/TypeScript.\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`docx-js.md`](docx-js.md) (~500 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for detailed syntax, critical formatting rules, and best practices before proceeding with document creation.\n2. Create a JavaScript/TypeScript file using Document, Paragraph, TextRun components (You can assume all dependencies are installed, but if not, refer to the dependencies section below)\n3. Export as .docx using Packer.toBuffer()\n\n## Editing an existing Word document\n\nWhen editing an existing Word document, use the **Document library** (a Python library for OOXML manipulation). The library automatically handles infrastructure setup and provides methods for document manipulation. For complex scenarios, you can access the underlying DOM directly through the library.\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~600 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for the Document library API and XML patterns for directly editing document files.\n2. Unpack the document: `python ooxml/scripts/unpack.py <office_file> <output_directory>`\n3. Create and run a Python script using the Document library (see \"Document Library\" section in ooxml.md)\n4. Pack the final document: `python ooxml/scripts/pack.py <input_directory> <office_file>`\n\nThe Document library provides both high-level methods for common operations and direct DOM access for complex scenarios.\n\n## Redlining workflow for document review\n\nThis workflow allows you to plan comprehensive tracked changes using markdown before implementing them in OOXML. **CRITICAL**: For complete tracked changes, you must implement ALL changes systematically.\n\n**Batching Strategy**: Group related changes into batches of 3-10 changes. This makes debugging manageable while maintaining efficiency. Test each batch before moving to the next.\n\n**Principle: Minimal, Precise Edits**\nWhen implementing tracked changes, only mark text that actually changes. Repeating unchanged text makes edits harder to review and appears unprofessional. Break replacements into: [unchanged text] + [deletion] + [insertion] + [unchanged text]. Preserve the original run's RSID for unchanged text by extracting the `<w:r>` element from the original and reusing it.\n\nExample - Changing \"30 days\" to \"60 days\" in a sentence:\n```python\n# BAD - Replaces entire sentence\n'<w:del><w:r><w:delText>The term is 30 days.</w:delText></w:r></w:del><w:ins><w:r><w:t>The term is 60 days.</w:t></w:r></w:ins>'\n\n# GOOD - Only marks what changed, preserves original <w:r> for unchanged text\n'<w:r w:rsidR=\"00AB12CD\"><w:t>The term is </w:t></w:r><w:del><w:r><w:delText>30</w:delText></w:r></w:del><w:ins><w:r><w:t>60</w:t></w:r></w:ins><w:r w:rsidR=\"00AB12CD\"><w:t> days.</w:t></w:r>'\n```\n\n### Tracked changes workflow\n\n1. **Get markdown representation**: Convert document to markdown with tracked changes preserved:\n   ```bash\n   pandoc --track-changes=all path-to-file.docx -o current.md\n   ```\n\n2. **Identify and group changes**: Review the document and identify ALL changes needed, organizing them into logical batches:\n\n   **Location methods** (for finding changes in XML):\n   - Section/heading numbers (e.g., \"Section 3.2\", \"Article IV\")\n   - Paragraph identifiers if numbered\n   - Grep patterns with unique surrounding text\n   - Document structure (e.g., \"first paragraph\", \"signature block\")\n   - **DO NOT use markdown line numbers** - they don't map to XML structure\n\n   **Batch organization** (group 3-10 related changes per batch):\n   - By section: \"Batch 1: Section 2 amendments\", \"Batch 2: Section 5 updates\"\n   - By type: \"Batch 1: Date corrections\", \"Batch 2: Party name changes\"\n   - By complexity: Start with simple text replacements, then tackle complex structural changes\n   - Sequential: \"Batch 1: Pages 1-3\", \"Batch 2: Pages 4-6\"\n\n3. **Read documentation and unpack**:\n   - **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~600 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Pay special attention to the \"Document Library\" and \"Tracked Change Patterns\" sections.\n   - **Unpack the document**: `python ooxml/scripts/unpack.py <file.docx> <dir>`\n   - **Note the suggested RSID**: The unpack script will suggest an RSID to use for your tracked changes. Copy this RSID for use in step 4b.\n\n4. **Implement changes in batches**: Group changes logically (by section, by type, or by proximity) and implement them together in a single script. This approach:\n   - Makes debugging easier (smaller batch = easier to isolate errors)\n   - Allows incremental progress\n   - Maintains efficiency (batch size of 3-10 changes works well)\n\n   **Suggested batch groupings:**\n   - By document section (e.g., \"Section 3 changes\", \"Definitions\", \"Termination clause\")\n   - By change type (e.g., \"Date changes\", \"Party name updates\", \"Legal term replacements\")\n   - By proximity (e.g., \"Changes on pages 1-3\", \"Changes in first half of document\")\n\n   For each batch of related changes:\n\n   **a. Map text to XML**: Grep for text in `word/document.xml` to verify how text is split across `<w:r>` elements.\n\n   **b. Create and run script**: Use `get_node` to find nodes, implement changes, then `doc.save()`. See **\"Document Library\"** section in ooxml.md for patterns.\n\n   **Note**: Always grep `word/document.xml` immediately before writing a script to get current line numbers and verify text content. Line numbers change after each script run.\n\n5. **Pack the document**: After all batches are complete, convert the unpacked directory back to .docx:\n   ```bash\n   python ooxml/scripts/pack.py unpacked reviewed-document.docx\n   ```\n\n6. **Final verification**: Do a comprehensive check of the complete document:\n   - Convert final document to markdown:\n     ```bash\n     pandoc --track-changes=all reviewed-document.docx -o verification.md\n     ```\n   - Verify ALL changes were applied correctly:\n     ```bash\n     grep \"original phrase\" verification.md  # Should NOT find it\n     grep \"replacement phrase\" verification.md  # Should find it\n     ```\n   - Check that no unintended changes were introduced\n\n\n## Converting Documents to Images\n\nTo visually analyze Word documents, convert them to images using a two-step process:\n\n1. **Convert DOCX to PDF**:\n   ```bash\n   soffice --headless --convert-to pdf document.docx\n   ```\n\n2. **Convert PDF pages to JPEG images**:\n   ```bash\n   pdftoppm -jpeg -r 150 document.pdf page\n   ```\n   This creates files like `page-1.jpg`, `page-2.jpg`, etc.\n\nOptions:\n- `-r 150`: Sets resolution to 150 DPI (adjust for quality/size balance)\n- `-jpeg`: Output JPEG format (use `-png` for PNG if preferred)\n- `-f N`: First page to convert (e.g., `-f 2` starts from page 2)\n- `-l N`: Last page to convert (e.g., `-l 5` stops at page 5)\n- `page`: Prefix for output files\n\nExample for specific range:\n```bash\npdftoppm -jpeg -r 150 -f 2 -l 5 document.pdf page  # Converts only pages 2-5\n```\n\n## Code Style Guidelines\n**IMPORTANT**: When generating code for DOCX operations:\n- Write concise code\n- Avoid verbose variable names and redundant operations\n- Avoid unnecessary print statements\n\n## Dependencies\n\nRequired dependencies (install if not available):\n\n- **pandoc**: `sudo apt-get install pandoc` (for text extraction)\n- **docx**: `npm install -g docx` (for creating new documents)\n- **LibreOffice**: `sudo apt-get install libreoffice` (for PDF conversion)\n- **Poppler**: `sudo apt-get install poppler-utils` (for pdftoppm to convert PDF to images)\n- **defusedxml**: `pip install defusedxml` (for secure XML parsing)",
        "skills/cover_letter_generator/.claude/skills/docx/docx-js.md": "# DOCX Library Tutorial\n\nGenerate .docx files with JavaScript/TypeScript.\n\n**Important: Read this entire document before starting.** Critical formatting rules and common pitfalls are covered throughout - skipping sections may result in corrupted files or rendering issues.\n\n## Setup\nAssumes docx is already installed globally\nIf not installed: `npm install -g docx`\n\n```javascript\nconst { Document, Packer, Paragraph, TextRun, Table, TableRow, TableCell, ImageRun, Media, \n        Header, Footer, AlignmentType, PageOrientation, LevelFormat, ExternalHyperlink, \n        InternalHyperlink, TableOfContents, HeadingLevel, BorderStyle, WidthType, TabStopType, \n        TabStopPosition, UnderlineType, ShadingType, VerticalAlign, SymbolRun, PageNumber,\n        FootnoteReferenceRun, Footnote, PageBreak } = require('docx');\n\n// Create & Save\nconst doc = new Document({ sections: [{ children: [/* content */] }] });\nPacker.toBuffer(doc).then(buffer => fs.writeFileSync(\"doc.docx\", buffer)); // Node.js\nPacker.toBlob(doc).then(blob => { /* download logic */ }); // Browser\n```\n\n## Text & Formatting\n```javascript\n// IMPORTANT: Never use \\n for line breaks - always use separate Paragraph elements\n//  WRONG: new TextRun(\"Line 1\\nLine 2\")\n//  CORRECT: new Paragraph({ children: [new TextRun(\"Line 1\")] }), new Paragraph({ children: [new TextRun(\"Line 2\")] })\n\n// Basic text with all formatting options\nnew Paragraph({\n  alignment: AlignmentType.CENTER,\n  spacing: { before: 200, after: 200 },\n  indent: { left: 720, right: 720 },\n  children: [\n    new TextRun({ text: \"Bold\", bold: true }),\n    new TextRun({ text: \"Italic\", italics: true }),\n    new TextRun({ text: \"Underlined\", underline: { type: UnderlineType.DOUBLE, color: \"FF0000\" } }),\n    new TextRun({ text: \"Colored\", color: \"FF0000\", size: 28, font: \"Arial\" }), // Arial default\n    new TextRun({ text: \"Highlighted\", highlight: \"yellow\" }),\n    new TextRun({ text: \"Strikethrough\", strike: true }),\n    new TextRun({ text: \"x2\", superScript: true }),\n    new TextRun({ text: \"H2O\", subScript: true }),\n    new TextRun({ text: \"SMALL CAPS\", smallCaps: true }),\n    new SymbolRun({ char: \"2022\", font: \"Symbol\" }), // Bullet \n    new SymbolRun({ char: \"00A9\", font: \"Arial\" })   // Copyright  - Arial for symbols\n  ]\n})\n```\n\n## Styles & Professional Formatting\n\n```javascript\nconst doc = new Document({\n  styles: {\n    default: { document: { run: { font: \"Arial\", size: 24 } } }, // 12pt default\n    paragraphStyles: [\n      // Document title style - override built-in Title style\n      { id: \"Title\", name: \"Title\", basedOn: \"Normal\",\n        run: { size: 56, bold: true, color: \"000000\", font: \"Arial\" },\n        paragraph: { spacing: { before: 240, after: 120 }, alignment: AlignmentType.CENTER } },\n      // IMPORTANT: Override built-in heading styles by using their exact IDs\n      { id: \"Heading1\", name: \"Heading 1\", basedOn: \"Normal\", next: \"Normal\", quickFormat: true,\n        run: { size: 32, bold: true, color: \"000000\", font: \"Arial\" }, // 16pt\n        paragraph: { spacing: { before: 240, after: 240 }, outlineLevel: 0 } }, // Required for TOC\n      { id: \"Heading2\", name: \"Heading 2\", basedOn: \"Normal\", next: \"Normal\", quickFormat: true,\n        run: { size: 28, bold: true, color: \"000000\", font: \"Arial\" }, // 14pt\n        paragraph: { spacing: { before: 180, after: 180 }, outlineLevel: 1 } },\n      // Custom styles use your own IDs\n      { id: \"myStyle\", name: \"My Style\", basedOn: \"Normal\",\n        run: { size: 28, bold: true, color: \"000000\" },\n        paragraph: { spacing: { after: 120 }, alignment: AlignmentType.CENTER } }\n    ],\n    characterStyles: [{ id: \"myCharStyle\", name: \"My Char Style\",\n      run: { color: \"FF0000\", bold: true, underline: { type: UnderlineType.SINGLE } } }]\n  },\n  sections: [{\n    properties: { page: { margin: { top: 1440, right: 1440, bottom: 1440, left: 1440 } } },\n    children: [\n      new Paragraph({ heading: HeadingLevel.TITLE, children: [new TextRun(\"Document Title\")] }), // Uses overridden Title style\n      new Paragraph({ heading: HeadingLevel.HEADING_1, children: [new TextRun(\"Heading 1\")] }), // Uses overridden Heading1 style\n      new Paragraph({ style: \"myStyle\", children: [new TextRun(\"Custom paragraph style\")] }),\n      new Paragraph({ children: [\n        new TextRun(\"Normal with \"),\n        new TextRun({ text: \"custom char style\", style: \"myCharStyle\" })\n      ]})\n    ]\n  }]\n});\n```\n\n**Professional Font Combinations:**\n- **Arial (Headers) + Arial (Body)** - Most universally supported, clean and professional\n- **Times New Roman (Headers) + Arial (Body)** - Classic serif headers with modern sans-serif body\n- **Georgia (Headers) + Verdana (Body)** - Optimized for screen reading, elegant contrast\n\n**Key Styling Principles:**\n- **Override built-in styles**: Use exact IDs like \"Heading1\", \"Heading2\", \"Heading3\" to override Word's built-in heading styles\n- **HeadingLevel constants**: `HeadingLevel.HEADING_1` uses \"Heading1\" style, `HeadingLevel.HEADING_2` uses \"Heading2\" style, etc.\n- **Include outlineLevel**: Set `outlineLevel: 0` for H1, `outlineLevel: 1` for H2, etc. to ensure TOC works correctly\n- **Use custom styles** instead of inline formatting for consistency\n- **Set a default font** using `styles.default.document.run.font` - Arial is universally supported\n- **Establish visual hierarchy** with different font sizes (titles > headers > body)\n- **Add proper spacing** with `before` and `after` paragraph spacing\n- **Use colors sparingly**: Default to black (000000) and shades of gray for titles and headings (heading 1, heading 2, etc.)\n- **Set consistent margins** (1440 = 1 inch is standard)\n\n\n## Lists (ALWAYS USE PROPER LISTS - NEVER USE UNICODE BULLETS)\n```javascript\n// Bullets - ALWAYS use the numbering config, NOT unicode symbols\n// CRITICAL: Use LevelFormat.BULLET constant, NOT the string \"bullet\"\nconst doc = new Document({\n  numbering: {\n    config: [\n      { reference: \"bullet-list\",\n        levels: [{ level: 0, format: LevelFormat.BULLET, text: \"\", alignment: AlignmentType.LEFT,\n          style: { paragraph: { indent: { left: 720, hanging: 360 } } } }] },\n      { reference: \"first-numbered-list\",\n        levels: [{ level: 0, format: LevelFormat.DECIMAL, text: \"%1.\", alignment: AlignmentType.LEFT,\n          style: { paragraph: { indent: { left: 720, hanging: 360 } } } }] },\n      { reference: \"second-numbered-list\", // Different reference = restarts at 1\n        levels: [{ level: 0, format: LevelFormat.DECIMAL, text: \"%1.\", alignment: AlignmentType.LEFT,\n          style: { paragraph: { indent: { left: 720, hanging: 360 } } } }] }\n    ]\n  },\n  sections: [{\n    children: [\n      // Bullet list items\n      new Paragraph({ numbering: { reference: \"bullet-list\", level: 0 },\n        children: [new TextRun(\"First bullet point\")] }),\n      new Paragraph({ numbering: { reference: \"bullet-list\", level: 0 },\n        children: [new TextRun(\"Second bullet point\")] }),\n      // Numbered list items\n      new Paragraph({ numbering: { reference: \"first-numbered-list\", level: 0 },\n        children: [new TextRun(\"First numbered item\")] }),\n      new Paragraph({ numbering: { reference: \"first-numbered-list\", level: 0 },\n        children: [new TextRun(\"Second numbered item\")] }),\n      //  CRITICAL: Different reference = INDEPENDENT list that restarts at 1\n      // Same reference = CONTINUES previous numbering\n      new Paragraph({ numbering: { reference: \"second-numbered-list\", level: 0 },\n        children: [new TextRun(\"Starts at 1 again (because different reference)\")] })\n    ]\n  }]\n});\n\n//  CRITICAL NUMBERING RULE: Each reference creates an INDEPENDENT numbered list\n// - Same reference = continues numbering (1, 2, 3... then 4, 5, 6...)\n// - Different reference = restarts at 1 (1, 2, 3... then 1, 2, 3...)\n// Use unique reference names for each separate numbered section!\n\n//  CRITICAL: NEVER use unicode bullets - they create fake lists that don't work properly\n// new TextRun(\" Item\")           // WRONG\n// new SymbolRun({ char: \"2022\" }) // WRONG\n//  ALWAYS use numbering config with LevelFormat.BULLET for real Word lists\n```\n\n## Tables\n```javascript\n// Complete table with margins, borders, headers, and bullet points\nconst tableBorder = { style: BorderStyle.SINGLE, size: 1, color: \"CCCCCC\" };\nconst cellBorders = { top: tableBorder, bottom: tableBorder, left: tableBorder, right: tableBorder };\n\nnew Table({\n  columnWidths: [4680, 4680], //  CRITICAL: Set column widths at table level - values in DXA (twentieths of a point)\n  margins: { top: 100, bottom: 100, left: 180, right: 180 }, // Set once for all cells\n  rows: [\n    new TableRow({\n      tableHeader: true,\n      children: [\n        new TableCell({\n          borders: cellBorders,\n          width: { size: 4680, type: WidthType.DXA }, // ALSO set width on each cell\n          //  CRITICAL: Always use ShadingType.CLEAR to prevent black backgrounds in Word.\n          shading: { fill: \"D5E8F0\", type: ShadingType.CLEAR }, \n          verticalAlign: VerticalAlign.CENTER,\n          children: [new Paragraph({ \n            alignment: AlignmentType.CENTER,\n            children: [new TextRun({ text: \"Header\", bold: true, size: 22 })]\n          })]\n        }),\n        new TableCell({\n          borders: cellBorders,\n          width: { size: 4680, type: WidthType.DXA }, // ALSO set width on each cell\n          shading: { fill: \"D5E8F0\", type: ShadingType.CLEAR },\n          children: [new Paragraph({ \n            alignment: AlignmentType.CENTER,\n            children: [new TextRun({ text: \"Bullet Points\", bold: true, size: 22 })]\n          })]\n        })\n      ]\n    }),\n    new TableRow({\n      children: [\n        new TableCell({\n          borders: cellBorders,\n          width: { size: 4680, type: WidthType.DXA }, // ALSO set width on each cell\n          children: [new Paragraph({ children: [new TextRun(\"Regular data\")] })]\n        }),\n        new TableCell({\n          borders: cellBorders,\n          width: { size: 4680, type: WidthType.DXA }, // ALSO set width on each cell\n          children: [\n            new Paragraph({ \n              numbering: { reference: \"bullet-list\", level: 0 },\n              children: [new TextRun(\"First bullet point\")] \n            }),\n            new Paragraph({ \n              numbering: { reference: \"bullet-list\", level: 0 },\n              children: [new TextRun(\"Second bullet point\")] \n            })\n          ]\n        })\n      ]\n    })\n  ]\n})\n```\n\n**IMPORTANT: Table Width & Borders**\n- Use BOTH `columnWidths: [width1, width2, ...]` array AND `width: { size: X, type: WidthType.DXA }` on each cell\n- Values in DXA (twentieths of a point): 1440 = 1 inch, Letter usable width = 9360 DXA (with 1\" margins)\n- Apply borders to individual `TableCell` elements, NOT the `Table` itself\n\n**Precomputed Column Widths (Letter size with 1\" margins = 9360 DXA total):**\n- **2 columns:** `columnWidths: [4680, 4680]` (equal width)\n- **3 columns:** `columnWidths: [3120, 3120, 3120]` (equal width)\n\n## Links & Navigation\n```javascript\n// TOC (requires headings) - CRITICAL: Use HeadingLevel only, NOT custom styles\n//  WRONG: new Paragraph({ heading: HeadingLevel.HEADING_1, style: \"customHeader\", children: [new TextRun(\"Title\")] })\n//  CORRECT: new Paragraph({ heading: HeadingLevel.HEADING_1, children: [new TextRun(\"Title\")] })\nnew TableOfContents(\"Table of Contents\", { hyperlink: true, headingStyleRange: \"1-3\" }),\n\n// External link\nnew Paragraph({\n  children: [new ExternalHyperlink({\n    children: [new TextRun({ text: \"Google\", style: \"Hyperlink\" })],\n    link: \"https://www.google.com\"\n  })]\n}),\n\n// Internal link & bookmark\nnew Paragraph({\n  children: [new InternalHyperlink({\n    children: [new TextRun({ text: \"Go to Section\", style: \"Hyperlink\" })],\n    anchor: \"section1\"\n  })]\n}),\nnew Paragraph({\n  children: [new TextRun(\"Section Content\")],\n  bookmark: { id: \"section1\", name: \"section1\" }\n}),\n```\n\n## Images & Media\n```javascript\n// Basic image with sizing & positioning\n// CRITICAL: Always specify 'type' parameter - it's REQUIRED for ImageRun\nnew Paragraph({\n  alignment: AlignmentType.CENTER,\n  children: [new ImageRun({\n    type: \"png\", // NEW REQUIREMENT: Must specify image type (png, jpg, jpeg, gif, bmp, svg)\n    data: fs.readFileSync(\"image.png\"),\n    transformation: { width: 200, height: 150, rotation: 0 }, // rotation in degrees\n    altText: { title: \"Logo\", description: \"Company logo\", name: \"Name\" } // IMPORTANT: All three fields are required\n  })]\n})\n```\n\n## Page Breaks\n```javascript\n// Manual page break\nnew Paragraph({ children: [new PageBreak()] }),\n\n// Page break before paragraph\nnew Paragraph({\n  pageBreakBefore: true,\n  children: [new TextRun(\"This starts on a new page\")]\n})\n\n//  CRITICAL: NEVER use PageBreak standalone - it will create invalid XML that Word cannot open\n//  WRONG: new PageBreak() \n//  CORRECT: new Paragraph({ children: [new PageBreak()] })\n```\n\n## Headers/Footers & Page Setup\n```javascript\nconst doc = new Document({\n  sections: [{\n    properties: {\n      page: {\n        margin: { top: 1440, right: 1440, bottom: 1440, left: 1440 }, // 1440 = 1 inch\n        size: { orientation: PageOrientation.LANDSCAPE },\n        pageNumbers: { start: 1, formatType: \"decimal\" } // \"upperRoman\", \"lowerRoman\", \"upperLetter\", \"lowerLetter\"\n      }\n    },\n    headers: {\n      default: new Header({ children: [new Paragraph({ \n        alignment: AlignmentType.RIGHT,\n        children: [new TextRun(\"Header Text\")]\n      })] })\n    },\n    footers: {\n      default: new Footer({ children: [new Paragraph({ \n        alignment: AlignmentType.CENTER,\n        children: [new TextRun(\"Page \"), new TextRun({ children: [PageNumber.CURRENT] }), new TextRun(\" of \"), new TextRun({ children: [PageNumber.TOTAL_PAGES] })]\n      })] })\n    },\n    children: [/* content */]\n  }]\n});\n```\n\n## Tabs\n```javascript\nnew Paragraph({\n  tabStops: [\n    { type: TabStopType.LEFT, position: TabStopPosition.MAX / 4 },\n    { type: TabStopType.CENTER, position: TabStopPosition.MAX / 2 },\n    { type: TabStopType.RIGHT, position: TabStopPosition.MAX * 3 / 4 }\n  ],\n  children: [new TextRun(\"Left\\tCenter\\tRight\")]\n})\n```\n\n## Constants & Quick Reference\n- **Underlines:** `SINGLE`, `DOUBLE`, `WAVY`, `DASH`\n- **Borders:** `SINGLE`, `DOUBLE`, `DASHED`, `DOTTED`  \n- **Numbering:** `DECIMAL` (1,2,3), `UPPER_ROMAN` (I,II,III), `LOWER_LETTER` (a,b,c)\n- **Tabs:** `LEFT`, `CENTER`, `RIGHT`, `DECIMAL`\n- **Symbols:** `\"2022\"` (), `\"00A9\"` (), `\"00AE\"` (), `\"2122\"` (), `\"00B0\"` (), `\"F070\"` (), `\"F0FC\"` ()\n\n## Critical Issues & Common Mistakes\n- **CRITICAL: PageBreak must ALWAYS be inside a Paragraph** - standalone PageBreak creates invalid XML that Word cannot open\n- **ALWAYS use ShadingType.CLEAR for table cell shading** - Never use ShadingType.SOLID (causes black background).\n- Measurements in DXA (1440 = 1 inch) | Each table cell needs 1 Paragraph | TOC requires HeadingLevel styles only\n- **ALWAYS use custom styles** with Arial font for professional appearance and proper visual hierarchy\n- **ALWAYS set a default font** using `styles.default.document.run.font` - Arial recommended\n- **ALWAYS use columnWidths array for tables** + individual cell widths for compatibility\n- **NEVER use unicode symbols for bullets** - always use proper numbering configuration with `LevelFormat.BULLET` constant (NOT the string \"bullet\")\n- **NEVER use \\n for line breaks anywhere** - always use separate Paragraph elements for each line\n- **ALWAYS use TextRun objects within Paragraph children** - never use text property directly on Paragraph\n- **CRITICAL for images**: ImageRun REQUIRES `type` parameter - always specify \"png\", \"jpg\", \"jpeg\", \"gif\", \"bmp\", or \"svg\"\n- **CRITICAL for bullets**: Must use `LevelFormat.BULLET` constant, not string \"bullet\", and include `text: \"\"` for the bullet character\n- **CRITICAL for numbering**: Each numbering reference creates an INDEPENDENT list. Same reference = continues numbering (1,2,3 then 4,5,6). Different reference = restarts at 1 (1,2,3 then 1,2,3). Use unique reference names for each separate numbered section!\n- **CRITICAL for TOC**: When using TableOfContents, headings must use HeadingLevel ONLY - do NOT add custom styles to heading paragraphs or TOC will break\n- **Tables**: Set `columnWidths` array + individual cell widths, apply borders to cells not table\n- **Set table margins at TABLE level** for consistent cell padding (avoids repetition per cell)",
        "skills/cover_letter_generator/.claude/skills/docx/ooxml.md": "# Office Open XML Technical Reference\n\n**Important: Read this entire document before starting.** This document covers:\n- [Technical Guidelines](#technical-guidelines) - Schema compliance rules and validation requirements\n- [Document Content Patterns](#document-content-patterns) - XML patterns for headings, lists, tables, formatting, etc.\n- [Document Library (Python)](#document-library-python) - Recommended approach for OOXML manipulation with automatic infrastructure setup\n- [Tracked Changes (Redlining)](#tracked-changes-redlining) - XML patterns for implementing tracked changes\n\n## Technical Guidelines\n\n### Schema Compliance\n- **Element ordering in `<w:pPr>`**: `<w:pStyle>`, `<w:numPr>`, `<w:spacing>`, `<w:ind>`, `<w:jc>`\n- **Whitespace**: Add `xml:space='preserve'` to `<w:t>` elements with leading/trailing spaces\n- **Unicode**: Escape characters in ASCII content: `\"` becomes `&#8220;`\n  - **Character encoding reference**: Curly quotes `\"\"` become `&#8220;&#8221;`, apostrophe `'` becomes `&#8217;`, em-dash `` becomes `&#8212;`\n- **Tracked changes**: Use `<w:del>` and `<w:ins>` tags with `w:author=\"Claude\"` outside `<w:r>` elements\n  - **Critical**: `<w:ins>` closes with `</w:ins>`, `<w:del>` closes with `</w:del>` - never mix\n  - **RSIDs must be 8-digit hex**: Use values like `00AB1234` (only 0-9, A-F characters)\n  - **trackRevisions placement**: Add `<w:trackRevisions/>` after `<w:proofState>` in settings.xml\n- **Images**: Add to `word/media/`, reference in `document.xml`, set dimensions to prevent overflow\n\n## Document Content Patterns\n\n### Basic Structure\n```xml\n<w:p>\n  <w:r><w:t>Text content</w:t></w:r>\n</w:p>\n```\n\n### Headings and Styles\n```xml\n<w:p>\n  <w:pPr>\n    <w:pStyle w:val=\"Title\"/>\n    <w:jc w:val=\"center\"/>\n  </w:pPr>\n  <w:r><w:t>Document Title</w:t></w:r>\n</w:p>\n\n<w:p>\n  <w:pPr><w:pStyle w:val=\"Heading2\"/></w:pPr>\n  <w:r><w:t>Section Heading</w:t></w:r>\n</w:p>\n```\n\n### Text Formatting\n```xml\n<!-- Bold -->\n<w:r><w:rPr><w:b/><w:bCs/></w:rPr><w:t>Bold</w:t></w:r>\n<!-- Italic -->\n<w:r><w:rPr><w:i/><w:iCs/></w:rPr><w:t>Italic</w:t></w:r>\n<!-- Underline -->\n<w:r><w:rPr><w:u w:val=\"single\"/></w:rPr><w:t>Underlined</w:t></w:r>\n<!-- Highlight -->\n<w:r><w:rPr><w:highlight w:val=\"yellow\"/></w:rPr><w:t>Highlighted</w:t></w:r>\n```\n\n### Lists\n```xml\n<!-- Numbered list -->\n<w:p>\n  <w:pPr>\n    <w:pStyle w:val=\"ListParagraph\"/>\n    <w:numPr><w:ilvl w:val=\"0\"/><w:numId w:val=\"1\"/></w:numPr>\n    <w:spacing w:before=\"240\"/>\n  </w:pPr>\n  <w:r><w:t>First item</w:t></w:r>\n</w:p>\n\n<!-- Restart numbered list at 1 - use different numId -->\n<w:p>\n  <w:pPr>\n    <w:pStyle w:val=\"ListParagraph\"/>\n    <w:numPr><w:ilvl w:val=\"0\"/><w:numId w:val=\"2\"/></w:numPr>\n    <w:spacing w:before=\"240\"/>\n  </w:pPr>\n  <w:r><w:t>New list item 1</w:t></w:r>\n</w:p>\n\n<!-- Bullet list (level 2) -->\n<w:p>\n  <w:pPr>\n    <w:pStyle w:val=\"ListParagraph\"/>\n    <w:numPr><w:ilvl w:val=\"1\"/><w:numId w:val=\"1\"/></w:numPr>\n    <w:spacing w:before=\"240\"/>\n    <w:ind w:left=\"900\"/>\n  </w:pPr>\n  <w:r><w:t>Bullet item</w:t></w:r>\n</w:p>\n```\n\n### Tables\n```xml\n<w:tbl>\n  <w:tblPr>\n    <w:tblStyle w:val=\"TableGrid\"/>\n    <w:tblW w:w=\"0\" w:type=\"auto\"/>\n  </w:tblPr>\n  <w:tblGrid>\n    <w:gridCol w:w=\"4675\"/><w:gridCol w:w=\"4675\"/>\n  </w:tblGrid>\n  <w:tr>\n    <w:tc>\n      <w:tcPr><w:tcW w:w=\"4675\" w:type=\"dxa\"/></w:tcPr>\n      <w:p><w:r><w:t>Cell 1</w:t></w:r></w:p>\n    </w:tc>\n    <w:tc>\n      <w:tcPr><w:tcW w:w=\"4675\" w:type=\"dxa\"/></w:tcPr>\n      <w:p><w:r><w:t>Cell 2</w:t></w:r></w:p>\n    </w:tc>\n  </w:tr>\n</w:tbl>\n```\n\n### Layout\n```xml\n<!-- Page break before new section (common pattern) -->\n<w:p>\n  <w:r>\n    <w:br w:type=\"page\"/>\n  </w:r>\n</w:p>\n<w:p>\n  <w:pPr>\n    <w:pStyle w:val=\"Heading1\"/>\n  </w:pPr>\n  <w:r>\n    <w:t>New Section Title</w:t>\n  </w:r>\n</w:p>\n\n<!-- Centered paragraph -->\n<w:p>\n  <w:pPr>\n    <w:spacing w:before=\"240\" w:after=\"0\"/>\n    <w:jc w:val=\"center\"/>\n  </w:pPr>\n  <w:r><w:t>Centered text</w:t></w:r>\n</w:p>\n\n<!-- Font change - paragraph level (applies to all runs) -->\n<w:p>\n  <w:pPr>\n    <w:rPr><w:rFonts w:ascii=\"Courier New\" w:hAnsi=\"Courier New\"/></w:rPr>\n  </w:pPr>\n  <w:r><w:t>Monospace text</w:t></w:r>\n</w:p>\n\n<!-- Font change - run level (specific to this text) -->\n<w:p>\n  <w:r>\n    <w:rPr><w:rFonts w:ascii=\"Courier New\" w:hAnsi=\"Courier New\"/></w:rPr>\n    <w:t>This text is Courier New</w:t>\n  </w:r>\n  <w:r><w:t> and this text uses default font</w:t></w:r>\n</w:p>\n```\n\n## File Updates\n\nWhen adding content, update these files:\n\n**`word/_rels/document.xml.rels`:**\n```xml\n<Relationship Id=\"rId1\" Type=\"http://schemas.openxmlformats.org/officeDocument/2006/relationships/numbering\" Target=\"numbering.xml\"/>\n<Relationship Id=\"rId5\" Type=\"http://schemas.openxmlformats.org/officeDocument/2006/relationships/image\" Target=\"media/image1.png\"/>\n```\n\n**`[Content_Types].xml`:**\n```xml\n<Default Extension=\"png\" ContentType=\"image/png\"/>\n<Override PartName=\"/word/numbering.xml\" ContentType=\"application/vnd.openxmlformats-officedocument.wordprocessingml.numbering+xml\"/>\n```\n\n### Images\n**CRITICAL**: Calculate dimensions to prevent page overflow and maintain aspect ratio.\n\n```xml\n<!-- Minimal required structure -->\n<w:p>\n  <w:r>\n    <w:drawing>\n      <wp:inline>\n        <wp:extent cx=\"2743200\" cy=\"1828800\"/>\n        <wp:docPr id=\"1\" name=\"Picture 1\"/>\n        <a:graphic xmlns:a=\"http://schemas.openxmlformats.org/drawingml/2006/main\">\n          <a:graphicData uri=\"http://schemas.openxmlformats.org/drawingml/2006/picture\">\n            <pic:pic xmlns:pic=\"http://schemas.openxmlformats.org/drawingml/2006/picture\">\n              <pic:nvPicPr>\n                <pic:cNvPr id=\"0\" name=\"image1.png\"/>\n                <pic:cNvPicPr/>\n              </pic:nvPicPr>\n              <pic:blipFill>\n                <a:blip r:embed=\"rId5\"/>\n                <!-- Add for stretch fill with aspect ratio preservation -->\n                <a:stretch>\n                  <a:fillRect/>\n                </a:stretch>\n              </pic:blipFill>\n              <pic:spPr>\n                <a:xfrm>\n                  <a:ext cx=\"2743200\" cy=\"1828800\"/>\n                </a:xfrm>\n                <a:prstGeom prst=\"rect\"/>\n              </pic:spPr>\n            </pic:pic>\n          </a:graphicData>\n        </a:graphic>\n      </wp:inline>\n    </w:drawing>\n  </w:r>\n</w:p>\n```\n\n### Links (Hyperlinks)\n\n**IMPORTANT**: All hyperlinks (both internal and external) require the Hyperlink style to be defined in styles.xml. Without this style, links will look like regular text instead of blue underlined clickable links.\n\n**External Links:**\n```xml\n<!-- In document.xml -->\n<w:hyperlink r:id=\"rId5\">\n  <w:r>\n    <w:rPr><w:rStyle w:val=\"Hyperlink\"/></w:rPr>\n    <w:t>Link Text</w:t>\n  </w:r>\n</w:hyperlink>\n\n<!-- In word/_rels/document.xml.rels -->\n<Relationship Id=\"rId5\" Type=\"http://schemas.openxmlformats.org/officeDocument/2006/relationships/hyperlink\" \n              Target=\"https://www.example.com/\" TargetMode=\"External\"/>\n```\n\n**Internal Links:**\n\n```xml\n<!-- Link to bookmark -->\n<w:hyperlink w:anchor=\"myBookmark\">\n  <w:r>\n    <w:rPr><w:rStyle w:val=\"Hyperlink\"/></w:rPr>\n    <w:t>Link Text</w:t>\n  </w:r>\n</w:hyperlink>\n\n<!-- Bookmark target -->\n<w:bookmarkStart w:id=\"0\" w:name=\"myBookmark\"/>\n<w:r><w:t>Target content</w:t></w:r>\n<w:bookmarkEnd w:id=\"0\"/>\n```\n\n**Hyperlink Style (required in styles.xml):**\n```xml\n<w:style w:type=\"character\" w:styleId=\"Hyperlink\">\n  <w:name w:val=\"Hyperlink\"/>\n  <w:basedOn w:val=\"DefaultParagraphFont\"/>\n  <w:uiPriority w:val=\"99\"/>\n  <w:unhideWhenUsed/>\n  <w:rPr>\n    <w:color w:val=\"467886\" w:themeColor=\"hyperlink\"/>\n    <w:u w:val=\"single\"/>\n  </w:rPr>\n</w:style>\n```\n\n## Document Library (Python)\n\nUse the Document class from `scripts/document.py` for all tracked changes and comments. It automatically handles infrastructure setup (people.xml, RSIDs, settings.xml, comment files, relationships, content types). Only use direct XML manipulation for complex scenarios not supported by the library.\n\n**Working with Unicode and Entities:**\n- **Searching**: Both entity notation and Unicode characters work - `contains=\"&#8220;Company\"` and `contains=\"\\u201cCompany\"` find the same text\n- **Replacing**: Use either entities (`&#8220;`) or Unicode (`\\u201c`) - both work and will be converted appropriately based on the file's encoding (ascii  entities, utf-8  Unicode)\n\n### Initialization\n\n**Find the docx skill root** (directory containing `scripts/` and `ooxml/`):\n```bash\n# Search for document.py to locate the skill root\n# Note: /mnt/skills is used here as an example; check your context for the actual location\nfind /mnt/skills -name \"document.py\" -path \"*/docx/scripts/*\" 2>/dev/null | head -1\n# Example output: /mnt/skills/docx/scripts/document.py\n# Skill root is: /mnt/skills/docx\n```\n\n**Run your script with PYTHONPATH** set to the docx skill root:\n```bash\nPYTHONPATH=/mnt/skills/docx python your_script.py\n```\n\n**In your script**, import from the skill root:\n```python\nfrom scripts.document import Document, DocxXMLEditor\n\n# Basic initialization (automatically creates temp copy and sets up infrastructure)\ndoc = Document('unpacked')\n\n# Customize author and initials\ndoc = Document('unpacked', author=\"John Doe\", initials=\"JD\")\n\n# Enable track revisions mode\ndoc = Document('unpacked', track_revisions=True)\n\n# Specify custom RSID (auto-generated if not provided)\ndoc = Document('unpacked', rsid=\"07DC5ECB\")\n```\n\n### Creating Tracked Changes\n\n**CRITICAL**: Only mark text that actually changes. Keep ALL unchanged text outside `<w:del>`/`<w:ins>` tags. Marking unchanged text makes edits unprofessional and harder to review.\n\n**Attribute Handling**: The Document class auto-injects attributes (w:id, w:date, w:rsidR, w:rsidDel, w16du:dateUtc, xml:space) into new elements. When preserving unchanged text from the original document, copy the original `<w:r>` element with its existing attributes to maintain document integrity.\n\n**Method Selection Guide**:\n- **Adding your own changes to regular text**: Use `replace_node()` with `<w:del>`/`<w:ins>` tags, or `suggest_deletion()` for removing entire `<w:r>` or `<w:p>` elements\n- **Partially modifying another author's tracked change**: Use `replace_node()` to nest your changes inside their `<w:ins>`/`<w:del>`\n- **Completely rejecting another author's insertion**: Use `revert_insertion()` on the `<w:ins>` element (NOT `suggest_deletion()`)\n- **Completely rejecting another author's deletion**: Use `revert_deletion()` on the `<w:del>` element to restore deleted content using tracked changes\n\n```python\n# Minimal edit - change one word: \"The report is monthly\"  \"The report is quarterly\"\n# Original: <w:r w:rsidR=\"00AB12CD\"><w:rPr><w:rFonts w:ascii=\"Calibri\"/></w:rPr><w:t>The report is monthly</w:t></w:r>\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:r\", contains=\"The report is monthly\")\nrpr = tags[0].toxml() if (tags := node.getElementsByTagName(\"w:rPr\")) else \"\"\nreplacement = f'<w:r w:rsidR=\"00AB12CD\">{rpr}<w:t>The report is </w:t></w:r><w:del><w:r>{rpr}<w:delText>monthly</w:delText></w:r></w:del><w:ins><w:r>{rpr}<w:t>quarterly</w:t></w:r></w:ins>'\ndoc[\"word/document.xml\"].replace_node(node, replacement)\n\n# Minimal edit - change number: \"within 30 days\"  \"within 45 days\"\n# Original: <w:r w:rsidR=\"00XYZ789\"><w:rPr><w:rFonts w:ascii=\"Calibri\"/></w:rPr><w:t>within 30 days</w:t></w:r>\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:r\", contains=\"within 30 days\")\nrpr = tags[0].toxml() if (tags := node.getElementsByTagName(\"w:rPr\")) else \"\"\nreplacement = f'<w:r w:rsidR=\"00XYZ789\">{rpr}<w:t>within </w:t></w:r><w:del><w:r>{rpr}<w:delText>30</w:delText></w:r></w:del><w:ins><w:r>{rpr}<w:t>45</w:t></w:r></w:ins><w:r w:rsidR=\"00XYZ789\">{rpr}<w:t> days</w:t></w:r>'\ndoc[\"word/document.xml\"].replace_node(node, replacement)\n\n# Complete replacement - preserve formatting even when replacing all text\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:r\", contains=\"apple\")\nrpr = tags[0].toxml() if (tags := node.getElementsByTagName(\"w:rPr\")) else \"\"\nreplacement = f'<w:del><w:r>{rpr}<w:delText>apple</w:delText></w:r></w:del><w:ins><w:r>{rpr}<w:t>banana orange</w:t></w:r></w:ins>'\ndoc[\"word/document.xml\"].replace_node(node, replacement)\n\n# Insert new content (no attributes needed - auto-injected)\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:r\", contains=\"existing text\")\ndoc[\"word/document.xml\"].insert_after(node, '<w:ins><w:r><w:t>new text</w:t></w:r></w:ins>')\n\n# Partially delete another author's insertion\n# Original: <w:ins w:author=\"Jane Smith\" w:date=\"...\"><w:r><w:t>quarterly financial report</w:t></w:r></w:ins>\n# Goal: Delete only \"financial\" to make it \"quarterly report\"\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:ins\", attrs={\"w:id\": \"5\"})\n# IMPORTANT: Preserve w:author=\"Jane Smith\" on the outer <w:ins> to maintain authorship\nreplacement = '''<w:ins w:author=\"Jane Smith\" w:date=\"2025-01-15T10:00:00Z\">\n  <w:r><w:t>quarterly </w:t></w:r>\n  <w:del><w:r><w:delText>financial </w:delText></w:r></w:del>\n  <w:r><w:t>report</w:t></w:r>\n</w:ins>'''\ndoc[\"word/document.xml\"].replace_node(node, replacement)\n\n# Change part of another author's insertion\n# Original: <w:ins w:author=\"Jane Smith\"><w:r><w:t>in silence, safe and sound</w:t></w:r></w:ins>\n# Goal: Change \"safe and sound\" to \"soft and unbound\"\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:ins\", attrs={\"w:id\": \"8\"})\nreplacement = f'''<w:ins w:author=\"Jane Smith\" w:date=\"2025-01-15T10:00:00Z\">\n  <w:r><w:t>in silence, </w:t></w:r>\n</w:ins>\n<w:ins>\n  <w:r><w:t>soft and unbound</w:t></w:r>\n</w:ins>\n<w:ins w:author=\"Jane Smith\" w:date=\"2025-01-15T10:00:00Z\">\n  <w:del><w:r><w:delText>safe and sound</w:delText></w:r></w:del>\n</w:ins>'''\ndoc[\"word/document.xml\"].replace_node(node, replacement)\n\n# Delete entire run (use only when deleting all content; use replace_node for partial deletions)\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:r\", contains=\"text to delete\")\ndoc[\"word/document.xml\"].suggest_deletion(node)\n\n# Delete entire paragraph (in-place, handles both regular and numbered list paragraphs)\npara = doc[\"word/document.xml\"].get_node(tag=\"w:p\", contains=\"paragraph to delete\")\ndoc[\"word/document.xml\"].suggest_deletion(para)\n\n# Add new numbered list item\ntarget_para = doc[\"word/document.xml\"].get_node(tag=\"w:p\", contains=\"existing list item\")\npPr = tags[0].toxml() if (tags := target_para.getElementsByTagName(\"w:pPr\")) else \"\"\nnew_item = f'<w:p>{pPr}<w:r><w:t>New item</w:t></w:r></w:p>'\ntracked_para = DocxXMLEditor.suggest_paragraph(new_item)\ndoc[\"word/document.xml\"].insert_after(target_para, tracked_para)\n# Optional: add spacing paragraph before content for better visual separation\n# spacing = DocxXMLEditor.suggest_paragraph('<w:p><w:pPr><w:pStyle w:val=\"ListParagraph\"/></w:pPr></w:p>')\n# doc[\"word/document.xml\"].insert_after(target_para, spacing + tracked_para)\n```\n\n### Adding Comments\n\n```python\n# Add comment spanning two existing tracked changes\n# Note: w:id is auto-generated. Only search by w:id if you know it from XML inspection\nstart_node = doc[\"word/document.xml\"].get_node(tag=\"w:del\", attrs={\"w:id\": \"1\"})\nend_node = doc[\"word/document.xml\"].get_node(tag=\"w:ins\", attrs={\"w:id\": \"2\"})\ndoc.add_comment(start=start_node, end=end_node, text=\"Explanation of this change\")\n\n# Add comment on a paragraph\npara = doc[\"word/document.xml\"].get_node(tag=\"w:p\", contains=\"paragraph text\")\ndoc.add_comment(start=para, end=para, text=\"Comment on this paragraph\")\n\n# Add comment on newly created tracked change\n# First create the tracked change\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:r\", contains=\"old\")\nnew_nodes = doc[\"word/document.xml\"].replace_node(\n    node,\n    '<w:del><w:r><w:delText>old</w:delText></w:r></w:del><w:ins><w:r><w:t>new</w:t></w:r></w:ins>'\n)\n# Then add comment on the newly created elements\n# new_nodes[0] is the <w:del>, new_nodes[1] is the <w:ins>\ndoc.add_comment(start=new_nodes[0], end=new_nodes[1], text=\"Changed old to new per requirements\")\n\n# Reply to existing comment\ndoc.reply_to_comment(parent_comment_id=0, text=\"I agree with this change\")\n```\n\n### Rejecting Tracked Changes\n\n**IMPORTANT**: Use `revert_insertion()` to reject insertions and `revert_deletion()` to restore deletions using tracked changes. Use `suggest_deletion()` only for regular unmarked content.\n\n```python\n# Reject insertion (wraps it in deletion)\n# Use this when another author inserted text that you want to delete\nins = doc[\"word/document.xml\"].get_node(tag=\"w:ins\", attrs={\"w:id\": \"5\"})\nnodes = doc[\"word/document.xml\"].revert_insertion(ins)  # Returns [ins]\n\n# Reject deletion (creates insertion to restore deleted content)\n# Use this when another author deleted text that you want to restore\ndel_elem = doc[\"word/document.xml\"].get_node(tag=\"w:del\", attrs={\"w:id\": \"3\"})\nnodes = doc[\"word/document.xml\"].revert_deletion(del_elem)  # Returns [del_elem, new_ins]\n\n# Reject all insertions in a paragraph\npara = doc[\"word/document.xml\"].get_node(tag=\"w:p\", contains=\"paragraph text\")\nnodes = doc[\"word/document.xml\"].revert_insertion(para)  # Returns [para]\n\n# Reject all deletions in a paragraph\npara = doc[\"word/document.xml\"].get_node(tag=\"w:p\", contains=\"paragraph text\")\nnodes = doc[\"word/document.xml\"].revert_deletion(para)  # Returns [para]\n```\n\n### Inserting Images\n\n**CRITICAL**: The Document class works with a temporary copy at `doc.unpacked_path`. Always copy images to this temp directory, not the original unpacked folder.\n\n```python\nfrom PIL import Image\nimport shutil, os\n\n# Initialize document first\ndoc = Document('unpacked')\n\n# Copy image and calculate full-width dimensions with aspect ratio\nmedia_dir = os.path.join(doc.unpacked_path, 'word/media')\nos.makedirs(media_dir, exist_ok=True)\nshutil.copy('image.png', os.path.join(media_dir, 'image1.png'))\nimg = Image.open(os.path.join(media_dir, 'image1.png'))\nwidth_emus = int(6.5 * 914400)  # 6.5\" usable width, 914400 EMUs/inch\nheight_emus = int(width_emus * img.size[1] / img.size[0])\n\n# Add relationship and content type\nrels_editor = doc['word/_rels/document.xml.rels']\nnext_rid = rels_editor.get_next_rid()\nrels_editor.append_to(rels_editor.dom.documentElement,\n    f'<Relationship Id=\"{next_rid}\" Type=\"http://schemas.openxmlformats.org/officeDocument/2006/relationships/image\" Target=\"media/image1.png\"/>')\ndoc['[Content_Types].xml'].append_to(doc['[Content_Types].xml'].dom.documentElement,\n    '<Default Extension=\"png\" ContentType=\"image/png\"/>')\n\n# Insert image\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:p\", line_number=100)\ndoc[\"word/document.xml\"].insert_after(node, f'''<w:p>\n  <w:r>\n    <w:drawing>\n      <wp:inline distT=\"0\" distB=\"0\" distL=\"0\" distR=\"0\">\n        <wp:extent cx=\"{width_emus}\" cy=\"{height_emus}\"/>\n        <wp:docPr id=\"1\" name=\"Picture 1\"/>\n        <a:graphic xmlns:a=\"http://schemas.openxmlformats.org/drawingml/2006/main\">\n          <a:graphicData uri=\"http://schemas.openxmlformats.org/drawingml/2006/picture\">\n            <pic:pic xmlns:pic=\"http://schemas.openxmlformats.org/drawingml/2006/picture\">\n              <pic:nvPicPr><pic:cNvPr id=\"1\" name=\"image1.png\"/><pic:cNvPicPr/></pic:nvPicPr>\n              <pic:blipFill><a:blip r:embed=\"{next_rid}\"/><a:stretch><a:fillRect/></a:stretch></pic:blipFill>\n              <pic:spPr><a:xfrm><a:ext cx=\"{width_emus}\" cy=\"{height_emus}\"/></a:xfrm><a:prstGeom prst=\"rect\"><a:avLst/></a:prstGeom></pic:spPr>\n            </pic:pic>\n          </a:graphicData>\n        </a:graphic>\n      </wp:inline>\n    </w:drawing>\n  </w:r>\n</w:p>''')\n```\n\n### Getting Nodes\n\n```python\n# By text content\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:p\", contains=\"specific text\")\n\n# By line range\npara = doc[\"word/document.xml\"].get_node(tag=\"w:p\", line_number=range(100, 150))\n\n# By attributes\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:del\", attrs={\"w:id\": \"1\"})\n\n# By exact line number (must be line number where tag opens)\npara = doc[\"word/document.xml\"].get_node(tag=\"w:p\", line_number=42)\n\n# Combine filters\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:r\", line_number=range(40, 60), contains=\"text\")\n\n# Disambiguate when text appears multiple times - add line_number range\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:r\", contains=\"Section\", line_number=range(2400, 2500))\n```\n\n### Saving\n\n```python\n# Save with automatic validation (copies back to original directory)\ndoc.save()  # Validates by default, raises error if validation fails\n\n# Save to different location\ndoc.save('modified-unpacked')\n\n# Skip validation (debugging only - needing this in production indicates XML issues)\ndoc.save(validate=False)\n```\n\n### Direct DOM Manipulation\n\nFor complex scenarios not covered by the library:\n\n```python\n# Access any XML file\neditor = doc[\"word/document.xml\"]\neditor = doc[\"word/comments.xml\"]\n\n# Direct DOM access (defusedxml.minidom.Document)\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:p\", line_number=5)\nparent = node.parentNode\nparent.removeChild(node)\nparent.appendChild(node)  # Move to end\n\n# General document manipulation (without tracked changes)\nold_node = doc[\"word/document.xml\"].get_node(tag=\"w:p\", contains=\"original text\")\ndoc[\"word/document.xml\"].replace_node(old_node, \"<w:p><w:r><w:t>replacement text</w:t></w:r></w:p>\")\n\n# Multiple insertions - use return value to maintain order\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:r\", line_number=100)\nnodes = doc[\"word/document.xml\"].insert_after(node, \"<w:r><w:t>A</w:t></w:r>\")\nnodes = doc[\"word/document.xml\"].insert_after(nodes[-1], \"<w:r><w:t>B</w:t></w:r>\")\nnodes = doc[\"word/document.xml\"].insert_after(nodes[-1], \"<w:r><w:t>C</w:t></w:r>\")\n# Results in: original_node, A, B, C\n```\n\n## Tracked Changes (Redlining)\n\n**Use the Document class above for all tracked changes.** The patterns below are for reference when constructing replacement XML strings.\n\n### Validation Rules\nThe validator checks that the document text matches the original after reverting Claude's changes. This means:\n- **NEVER modify text inside another author's `<w:ins>` or `<w:del>` tags**\n- **ALWAYS use nested deletions** to remove another author's insertions\n- **Every edit must be properly tracked** with `<w:ins>` or `<w:del>` tags\n\n### Tracked Change Patterns\n\n**CRITICAL RULES**:\n1. Never modify the content inside another author's tracked changes. Always use nested deletions.\n2. **XML Structure**: Always place `<w:del>` and `<w:ins>` at paragraph level containing complete `<w:r>` elements. Never nest inside `<w:r>` elements - this creates invalid XML that breaks document processing.\n\n**Text Insertion:**\n```xml\n<w:ins w:id=\"1\" w:author=\"Claude\" w:date=\"2025-07-30T23:05:00Z\" w16du:dateUtc=\"2025-07-31T06:05:00Z\">\n  <w:r w:rsidR=\"00792858\">\n    <w:t>inserted text</w:t>\n  </w:r>\n</w:ins>\n```\n\n**Text Deletion:**\n```xml\n<w:del w:id=\"2\" w:author=\"Claude\" w:date=\"2025-07-30T23:05:00Z\" w16du:dateUtc=\"2025-07-31T06:05:00Z\">\n  <w:r w:rsidDel=\"00792858\">\n    <w:delText>deleted text</w:delText>\n  </w:r>\n</w:del>\n```\n\n**Deleting Another Author's Insertion (MUST use nested structure):**\n```xml\n<!-- Nest deletion inside the original insertion -->\n<w:ins w:author=\"Jane Smith\" w:id=\"16\">\n  <w:del w:author=\"Claude\" w:id=\"40\">\n    <w:r><w:delText>monthly</w:delText></w:r>\n  </w:del>\n</w:ins>\n<w:ins w:author=\"Claude\" w:id=\"41\">\n  <w:r><w:t>weekly</w:t></w:r>\n</w:ins>\n```\n\n**Restoring Another Author's Deletion:**\n```xml\n<!-- Leave their deletion unchanged, add new insertion after it -->\n<w:del w:author=\"Jane Smith\" w:id=\"50\">\n  <w:r><w:delText>within 30 days</w:delText></w:r>\n</w:del>\n<w:ins w:author=\"Claude\" w:id=\"51\">\n  <w:r><w:t>within 30 days</w:t></w:r>\n</w:ins>\n```",
        "skills/cover_letter_generator/.claude/skills/fetch-library-docs/SKILL.md": "---\nname: fetch-library-docs\ndescription: Fetches official documentation for external libraries and frameworks (React, Next.js, Prisma, FastAPI, Express, Tailwind, MongoDB, etc.) with 60-90% token savings via content-type filtering. Use this skill when implementing features using library APIs, debugging library-specific errors, troubleshooting configuration issues, installing or setting up frameworks, integrating third-party packages, upgrading between library versions, or looking up correct API patterns and best practices. Triggers automatically during coding work - fetch docs before writing library code to get correct patterns, not after guessing wrong.\n---\n\n# Library Documentation Skill\n\nFetches official library documentation with 60-90% token savings.\n\n---\n\n## WHEN TO INVOKE (Auto-Detection)\n\n**INVOKE AUTOMATICALLY when:**\n\n| Context | Detection Signal | Content Type |\n|---------|------------------|--------------|\n| **Implementing** | About to write code using library API | `examples,api-ref` |\n| **Debugging** | Error contains library name (e.g., `PrismaClientError`) | `troubleshooting` |\n| **Installing** | Adding new package, `npm install`, setup task | `setup` |\n| **Integrating** | Connecting libraries (\"use X with Y\") | `examples,setup` |\n| **Upgrading** | Version migration, breaking changes | `migration` |\n| **Uncertain** | First use of library feature, unsure of pattern | `examples` |\n\n**DO NOT INVOKE when:**\n- Already have sufficient knowledge from training\n- User pasted docs or has them open\n- Task is about local/private code (use codebase search)\n- Comparing libraries (use web search)\n\n---\n\n## DECISION LOGIC\n\n### 1. Identify Library\n\n```\nPriority: User mention  Error message  File imports  package.json  Ask user\n```\n\nExamples:\n- `PrismaClientKnownRequestError`  library = \"prisma\"\n- `import { useState } from 'react'`  library = \"react\"\n- `from fastapi import FastAPI`  library = \"fastapi\"\n\n### 2. Identify Topic\n\n```\nPriority: User specifies  Error message  Feature being implemented  \"getting started\"\n```\n\n### 3. Select Content Type\n\n| Task | Content Type |\n|------|--------------|\n| Implementing code | `examples,api-ref` |\n| Debugging error | `troubleshooting,examples` |\n| Installing/setup | `setup` |\n| Integrating libs | `examples,setup` |\n| Upgrading version | `migration` |\n| Understanding why | `concepts` |\n| Best practices | `patterns` |\n\n---\n\n## EXECUTION\n\n```bash\n# With known library ID (faster - saves 1 API call)\nbash scripts/fetch-docs.sh --library-id <id> --topic \"<topic>\" --content-type <types>\n\n# With library name (auto-resolves)\nbash scripts/fetch-docs.sh --library <name> --topic \"<topic>\" --content-type <types>\n```\n\n### Quick Library IDs\n\n| Library | ID |\n|---------|----|\n| React | `/reactjs/react.dev` |\n| Next.js | `/vercel/next.js` |\n| Prisma | `/prisma/docs` |\n| Tailwind | `/tailwindlabs/tailwindcss.com` |\n| FastAPI | `/tiangolo/fastapi` |\n\nSee [references/library-ids.md](references/library-ids.md) for complete list.\n\n---\n\n## ERROR HANDLING (Quick Reference)\n\n| Error | Action |\n|-------|--------|\n| `[LIBRARY_NOT_FOUND]` | Try spelling variations |\n| `[LIBRARY_MISMATCH]` | Use --library-id directly |\n| `[EMPTY_RESULTS]` | Broaden topic or use `--content-type all` |\n| `[RATE_LIMIT_ERROR]` | Check API key setup |\n\n**Call Budget**: Context7 allows 3 calls/question. Use `--library-id` to save 1 call.\n\nSee [references/context7-tools.md](references/context7-tools.md) for full error handling.\n\n---\n\n## REFERENCES\n\n- [Library IDs](references/library-ids.md) - Complete library ID list\n- [Usage Patterns](references/patterns.md) - Real-world examples\n- [Context7 Tools](references/context7-tools.md) - API details, error codes, setup\n",
        "skills/cover_letter_generator/.claude/skills/fetch-library-docs/references/context7-tools.md": "# Context7 MCP Tools\n\n*2 tools available*\n\n** Context7 has a 3-call limit per question.** Plan your calls carefully:\n- Use `--library-id` when you know the ID (saves 1 call)\n- The fetch-docs.sh skill uses max 2 calls, leaving 1 for your retry\n\n## `resolve-library-id`\n\nResolves a package/product name to a Context7-compatible library ID and returns a list of matching libraries.\n\nYou MUST call this function before 'query-docs' to obtain a valid Context7-compatible library ID UNLESS the user explicitly provides a library ID in the format '/org/project' or '/org/project/version' in their query.\n\n### Selection Process\n\n1. Analyze the query to understand what library/package the user is looking for\n2. Return the most relevant match based on:\n   - Name similarity to the query (exact matches prioritized)\n   - Description relevance to the query's intent\n   - Documentation coverage (prioritize libraries with higher Code Snippet counts)\n   - Source reputation (consider libraries with High or Medium reputation more authoritative)\n   - Benchmark Score: Quality indicator (100 is the highest score)\n\n### Parameters\n\n- **`query`** (`string`) *(required)*: The user's original question or task. Used to rank library results by relevance.\n\n- **`libraryName`** (`string`) *(required)*: Library name to search for and retrieve a Context7-compatible library ID.\n\n### Response Format\n\nReturns a list of matching libraries with:\n- Title\n- Context7-compatible library ID (e.g., `/reactjs/react.dev`)\n- Code Snippets count\n- Source Reputation (High/Medium/Low)\n- Benchmark Score\n- Description\n\n### Examples\n\n```bash\n# Find React library\npython scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t resolve-library-id \\\n  -p '{\"query\": \"useState hooks\", \"libraryName\": \"react\"}'\n\n# Find Next.js library\npython scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t resolve-library-id \\\n  -p '{\"query\": \"routing\", \"libraryName\": \"next.js\"}'\n```\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"query\": {\n      \"type\": \"string\",\n      \"description\": \"The user's original question or task. Used to rank library results by relevance.\"\n    },\n    \"libraryName\": {\n      \"type\": \"string\",\n      \"description\": \"Library name to search for and retrieve a Context7-compatible library ID.\"\n    }\n  },\n  \"required\": [\"query\", \"libraryName\"]\n}\n```\n</details>\n\n## `query-docs`\n\nRetrieves and queries up-to-date documentation and code examples from Context7 for any programming library or framework.\n\nYou must call 'resolve-library-id' first to obtain the exact Context7-compatible library ID required to use this tool, UNLESS the user explicitly provides a library ID in the format '/org/project' or '/org/project/version' in their query.\n\n### Parameters\n\n- **`libraryId`** (`string`) *(required)*: Exact Context7-compatible library ID (e.g., '/mongodb/docs', '/vercel/next.js') retrieved from 'resolve-library-id' or directly from user query.\n\n- **`query`** (`string`) *(required)*: The question or task you need help with. Be specific and include relevant details. Good: 'How to set up authentication with JWT in Express.js'. Bad: 'auth'.\n\n### Examples\n\n```bash\n# Get React hooks documentation\npython scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t query-docs \\\n  -p '{\"libraryId\": \"/reactjs/react.dev\", \"query\": \"useState hooks examples\"}'\n\n# Get Next.js routing information\npython scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t query-docs \\\n  -p '{\"libraryId\": \"/vercel/next.js\", \"query\": \"how does routing work\"}'\n\n# Get MongoDB aggregation examples\npython scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t query-docs \\\n  -p '{\"libraryId\": \"/mongodb/docs\", \"query\": \"aggregation pipeline examples\"}'\n```\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"libraryId\": {\n      \"type\": \"string\",\n      \"description\": \"Exact Context7-compatible library ID (e.g., '/mongodb/docs', '/vercel/next.js') retrieved from 'resolve-library-id' or directly from user query.\"\n    },\n    \"query\": {\n      \"type\": \"string\",\n      \"description\": \"The question or task you need help with. Be specific and include relevant details.\"\n    }\n  },\n  \"required\": [\"libraryId\", \"query\"]\n}\n```\n</details>\n\n## Usage Patterns\n\n### Pattern 1: Unknown Library\n\nWhen you don't know the exact library ID:\n\n```bash\n# Step 1: Resolve library name\npython scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t resolve-library-id -p '{\"query\": \"middleware\", \"libraryName\": \"express\"}'\n\n# Step 2: Use returned ID to fetch docs\npython scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t query-docs \\\n  -p '{\"libraryId\": \"/expressjs/express\", \"query\": \"how to use middleware\"}'\n```\n\n### Pattern 2: Known Library ID\n\nWhen you know the library ID:\n\n```bash\n# Direct fetch (skip resolve step)\npython scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t query-docs \\\n  -p '{\"libraryId\": \"/reactjs/react.dev\", \"query\": \"useState examples\"}'\n```\n\n### Pattern 3: Using the Shell Pipeline (Recommended)\n\nFor token-efficient documentation fetching:\n\n```bash\n# Code examples (default, 60-70% token savings)\nbash scripts/fetch-docs.sh --library react --topic useState --content-type examples\n\n# Multiple content types\nbash scripts/fetch-docs.sh --library react --topic useState --content-type examples,api-ref\n\n# With verbose output to see savings\nbash scripts/fetch-docs.sh --library react --topic useState --content-type examples --verbose\n```\n\n**Content types available:** `examples`, `api-ref`, `setup`, `concepts`, `migration`, `troubleshooting`, `patterns`, `all`\n\n## Common Library IDs\n\nQuick reference for popular libraries:\n\n| Library | Context7 ID |\n|---------|-------------|\n| React | `/reactjs/react.dev` or `/websites/react_dev` |\n| Next.js | `/vercel/next.js` |\n| Express | `/expressjs/express` |\n| MongoDB | `/mongodb/docs` |\n| Prisma | `/prisma/docs` |\n| Vue | `/vuejs/docs` |\n| Svelte | `/sveltejs/svelte.dev` |\n| FastAPI | `/tiangolo/fastapi` |\n| Django | `/django/docs` |\n\n## Error Handling\n\n### Error Codes\n\n| Error Code | Meaning | Call Cost | Action |\n|------------|---------|-----------|--------|\n| `[LIBRARY_NOT_FOUND]` | Library name didn't resolve | 1 call | Try spelling variations |\n| `[LIBRARY_MISMATCH]` | Resolved to wrong library | 1 call | Use --library-id directly |\n| `[INVALID_LIBRARY_ID]` | Bad ID format | 0 calls | Fix to `/org/project` format |\n| `[EMPTY_RESULTS]` | No docs for query | 1-2 calls | Broaden topic or --content-type all |\n| `[RATE_LIMIT_ERROR]` | Context7 limit hit | N/A | Wait, check API key |\n| `[FETCH_FAILED_AFTER_RETRIES]` | Network issues | 0 calls | Safe to retry |\n\n### Retry Logic\n\n- **Infrastructure failures** (timeout, network): Auto-retries with exponential backoff (2s, 5s, 10s)\n- **API errors** (rate limit, auth): No retry (would waste call budget)\n\n---\n\n## API Key Setup\n\nRequired for reliable access. Get free key at [context7.com/dashboard](https://context7.com/dashboard).\n\n### Setup Methods\n\n**Method 1: Config File** (Recommended - persistent)\n```bash\n# User-level (applies to all projects)\necho \"CONTEXT7_API_KEY=ctx7sk_your_key\" > ~/.context7.env\n\n# Project-level (add to .gitignore!)\necho \"CONTEXT7_API_KEY=ctx7sk_your_key\" > .context7.env\n```\n\n**Method 2: Environment Variable**\n```bash\n# macOS/Linux\nexport CONTEXT7_API_KEY=\"ctx7sk_your_key\"\n\n# Windows (PowerShell)\n$env:CONTEXT7_API_KEY = \"ctx7sk_your_key\"\n```\n\n### Check Status\n\n```bash\nbash scripts/fetch-docs.sh --api-status\n```\n\n### Priority Order\n\n1. `CONTEXT7_API_KEY` environment variable (highest)\n2. `.context7.env` in current directory (project-level)\n3. `~/.context7.env` in home directory (user-level)\n\n---\n\n## Parameters Reference\n\n```bash\nbash scripts/fetch-docs.sh [OPTIONS]\n```\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `--library <name>` | One of these | Library name (uses 2 calls for resolution) |\n| `--library-id <id>` | required | Direct ID like `/vercel/next.js` (uses 1 call) |\n| `--topic <topic>` | Recommended | Feature to focus on |\n| `--content-type <types>` | Optional | Comma-separated: examples, api-ref, setup, concepts, migration, troubleshooting, patterns, all |\n| `--max-items <num>` | Optional | Max items per type (default: 5) |\n| `--verbose` | Optional | Show token savings stats |\n| `--api-status` | Optional | Check API key configuration |\n\n---\n\n## Tips\n\n1. **Save API Calls**: Use `--library-id` when you know the ID (saves 1 of your 3 calls)\n2. **Specific Queries**: More specific queries yield better results\n3. **Use Shell Pipeline**: `fetch-docs.sh` provides 60-90% token savings through content-type filtering\n4. **Match Content Type**: Use `examples` for code, `api-ref` for parameters, `setup` for installation\n5. **Combine Types**: Use comma-separated types like `examples,api-ref` for comprehensive output\n6. **Fallback**: If no results, try `--content-type all` or broader topics\n",
        "skills/cover_letter_generator/.claude/skills/fetch-library-docs/references/library-ids.md": "# Common Library IDs\n\nUse `--library-id` instead of `--library` to save 1 API call.\n\n## JavaScript / TypeScript\n\n| Library | Context7 ID |\n|---------|-------------|\n| React | `/reactjs/react.dev` |\n| Next.js | `/vercel/next.js` |\n| Vue | `/vuejs/docs` |\n| Svelte | `/sveltejs/svelte.dev` |\n| Angular | `/angular/angular` |\n| Express | `/expressjs/express` |\n| Fastify | `/fastify/fastify` |\n| NestJS | `/nestjs/docs` |\n| Prisma | `/prisma/docs` |\n| Drizzle | `/drizzle-team/drizzle-orm` |\n| tRPC | `/trpc/trpc` |\n| Zod | `/colinhacks/zod` |\n| Tailwind CSS | `/tailwindlabs/tailwindcss.com` |\n| TypeScript | `/microsoft/typescript` |\n\n## Python\n\n| Library | Context7 ID |\n|---------|-------------|\n| FastAPI | `/tiangolo/fastapi` |\n| Django | `/django/docs` |\n| Flask | `/pallets/flask` |\n| SQLAlchemy | `/sqlalchemy/sqlalchemy` |\n| Pydantic | `/pydantic/pydantic` |\n| LangChain | `/langchain-ai/langchain` |\n\n## Databases\n\n| Library | Context7 ID |\n|---------|-------------|\n| MongoDB | `/mongodb/docs` |\n| PostgreSQL | `/postgres/postgres` |\n| Redis | `/redis/redis-doc` |\n| Supabase | `/supabase/supabase` |\n\n## Cloud / DevOps\n\n| Library | Context7 ID |\n|---------|-------------|\n| Docker | `/docker/docs` |\n| Kubernetes | `/kubernetes/website` |\n| Terraform | `/hashicorp/terraform` |\n| AWS CDK | `/aws/aws-cdk` |\n\n## Finding New Library IDs\n\nIf a library isn't listed, use `--library <name>` and the skill will resolve it. Check the verbose output for the resolved ID to use next time:\n\n```bash\nbash scripts/fetch-docs.sh --library \"your-library\" --topic \"getting started\" --verbose\n```\n",
        "skills/cover_letter_generator/.claude/skills/fetch-library-docs/references/patterns.md": "# Usage Patterns\n\nReal-world examples for common scenarios.\n\n## Pattern A: Implementing React Feature\n\nBuilding a form component with hooks:\n\n```bash\nbash scripts/fetch-docs.sh --library-id /reactjs/react.dev \\\n  --topic \"form handling useState\" \\\n  --content-type examples,api-ref\n```\n\n## Pattern B: Debugging Prisma Error\n\nError: `PrismaClientKnownRequestError: Unique constraint failed`\n\n```bash\nbash scripts/fetch-docs.sh --library-id /prisma/docs \\\n  --topic \"unique constraint error\" \\\n  --content-type troubleshooting,examples\n```\n\n## Pattern C: Setting Up Next.js Auth\n\nAdding authentication to Next.js app:\n\n```bash\nbash scripts/fetch-docs.sh --library-id /vercel/next.js \\\n  --topic \"authentication middleware\" \\\n  --content-type setup,examples\n```\n\n## Pattern D: Upgrading Tailwind\n\nMigrating from Tailwind v3 to v4:\n\n```bash\nbash scripts/fetch-docs.sh --library tailwind \\\n  --topic \"upgrade v3 v4\" \\\n  --content-type migration\n```\n\n## Pattern E: Best Practices Query\n\nShould I use server components?\n\n```bash\nbash scripts/fetch-docs.sh --library-id /vercel/next.js \\\n  --topic \"server components\" \\\n  --content-type patterns,concepts\n```\n\n## Pattern F: Library Integration\n\nConnecting Prisma with Next.js API routes:\n\n```bash\nbash scripts/fetch-docs.sh --library-id /prisma/docs \\\n  --topic \"next.js integration api routes\" \\\n  --content-type examples,setup\n```\n\n## Pattern G: Installing Framework\n\nSetting up Tailwind in new project:\n\n```bash\nbash scripts/fetch-docs.sh --library-id /tailwindlabs/tailwindcss.com \\\n  --topic \"installation next.js\" \\\n  --content-type setup\n```\n\n## Pattern H: API Reference Lookup\n\nQuick lookup for function parameters:\n\n```bash\nbash scripts/fetch-docs.sh --library-id /reactjs/react.dev \\\n  --topic \"useEffect\" \\\n  --content-type api-ref\n```\n\n## Content Type Quick Reference\n\n| Content Type | Use For |\n|--------------|---------|\n| `examples` | Code samples, syntax patterns |\n| `api-ref` | Function signatures, parameters |\n| `setup` | Installation, configuration |\n| `concepts` | Understanding how/why |\n| `troubleshooting` | Debugging, error fixes |\n| `migration` | Version upgrades, breaking changes |\n| `patterns` | Best practices, recommendations |\n| `all` | Full exploration (no filtering) |\n",
        "skills/cover_letter_generator/.claude/skills/github-assistant/SKILL.md": "---\nname: github-assistant\ndescription: Interactive troubleshooting assistant for the top 5 GitHub problems faced by new users. Diagnoses authentication failures, Git vs GitHub confusion, local-remote sync issues, merge conflicts, and accidental sensitive data commits. Provides step-by-step solutions with command execution. Use when users mention GitHub errors, password issues, sync problems, merge conflicts, or accidentally committing secrets.\n---\n\n# GitHub Assistant Skill\n\nThis skill provides interactive, diagnostic-driven troubleshooting for the most common GitHub problems faced by new users. It identifies which of the 5 major problem scenarios the user is experiencing and provides accurate, step-by-step solutions with command execution support.\n\n## When to Activate This Skill\n\n**Trigger conditions:**\n- User mentions GitHub authentication errors: \"password doesn't work\", \"authentication failed\", \"can't push\"\n- User is confused about Git vs GitHub: \"is Git the same as GitHub?\", \"which commands are Git?\"\n- User wants to sync local folder with GitHub: \"sync my folder\", \"automatically update GitHub\", \"keep in sync\"\n- User encounters merge conflicts: \"merge conflict\", \"conflicting changes\", \"pull failed\"\n- User accidentally committed sensitive data: \"committed password\", \"exposed API key\", \"sensitive file in GitHub\"\n- User asks general GitHub help questions for beginners\n\n**Initial offer:**\nGreet the user and explain this skill will help diagnose and solve their GitHub issue. Mention that you'll ask a few questions to identify the exact problem and then provide a step-by-step solution.\n\n## Diagnostic Workflow\n\n### Step 1: Problem Identification\n\nAsk the user to describe their issue or choose from common scenarios:\n\nPresent these options:\n1. **Authentication/Password Issues** - Can't push or clone, password doesn't work\n2. **Understanding Git vs GitHub** - Confused about what commands to use where\n3. **Syncing Local Folder with GitHub** - Want to keep local files in sync with repository\n4. **Merge Conflicts** - Getting conflict errors when pulling or merging\n5. **Sensitive Data Exposure** - Accidentally committed passwords, API keys, or secrets\n6. **Other/Not Sure** - Describe the issue in your own words\n\nWait for user response and route to appropriate solution workflow.\n\n**If user chooses \"Other/Not Sure\":**\nAsk them to describe:\n- What they were trying to do\n- What command they ran (if any)\n- The exact error message they received\n- Their current situation\n\nBased on their description, identify which of the 5 scenarios best matches and proceed to that workflow.\n\n### Step 2: Route to Appropriate Solution\n\nBased on user selection, proceed to the corresponding solution workflow below.\n\n## Solution Workflow 1: Authentication Failures\n\n**Problem confirmed:** User is experiencing \"Support for password authentication was removed\" or similar authentication errors.\n\n### Step 1: Explain the Issue\n\nInform the user:\n- GitHub removed password authentication on August 13, 2021\n- They now need to use either Personal Access Token (PAT) or SSH keys\n- Their GitHub account password will not work for Git operations\n\n### Step 2: Recommend Approach\n\nAsk which authentication method they prefer:\n- **Personal Access Token (Recommended for beginners)** - Use a token instead of password\n- **SSH Keys (More advanced)** - Set up SSH key authentication\n\n### Step 3a: Personal Access Token Setup (if chosen)\n\nProvide step-by-step instructions:\n\n1. **Generate a PAT on GitHub:**\n   - Go to: https://github.com/settings/tokens\n   - Click \"Generate new token\"  \"Generate new token (classic)\"\n   - Give it a descriptive name (e.g., \"My Laptop Git Access\")\n   - Set expiration (recommend 90 days for security)\n   - Select scopes: Check \"repo\" for full repository access\n   - Click \"Generate token\" at the bottom\n   - **IMPORTANT:** Copy the token immediately - you won't see it again!\n\n2. **Use the token:**\n   - When Git prompts for a password, paste the token instead\n   - The token should start with `ghp_` or `github_pat_`\n\n3. **Store credentials (optional but recommended):**\n\nAsk if they want help setting up credential caching so they don't have to enter the token every time.\n\nIf yes, detect their operating system and provide appropriate command:\n\n**For Windows:**\n```bash\ngit config --global credential.helper wincred\n```\n\n**For macOS:**\n```bash\ngit config --global credential.helper osxkeychain\n```\n\n**For Linux:**\n```bash\ngit config --global credential.helper cache\n# Or for permanent storage:\ngit config --global credential.helper store\n```\n\nOffer to run the appropriate command if they're in a terminal environment.\n\n### Step 3b: SSH Key Setup (if chosen)\n\nProvide step-by-step instructions:\n\n1. **Check for existing SSH keys:**\n```bash\nls -al ~/.ssh\n```\n\nOffer to run this command for them.\n\n2. **Generate new SSH key (if needed):**\n```bash\nssh-keygen -t ed25519 -C \"their-email@example.com\"\n```\n\nAsk for their GitHub email and offer to run this command.\n\n3. **Add SSH key to ssh-agent:**\n```bash\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_ed25519\n```\n\n4. **Copy public key:**\n```bash\ncat ~/.ssh/id_ed25519.pub\n```\n\nOffer to run this and display the public key for them to copy.\n\n5. **Add to GitHub:**\n   - Go to: https://github.com/settings/keys\n   - Click \"New SSH key\"\n   - Paste the public key\n   - Give it a title and save\n\n6. **Test connection:**\n```bash\nssh -T git@github.com\n```\n\nOffer to run this test.\n\n7. **Update remote URL (if needed):**\n\nCheck their current remote:\n```bash\ngit remote -v\n```\n\nIf it shows HTTPS (https://github.com/...), offer to convert to SSH:\n```bash\ngit remote set-url origin git@github.com:username/repo.git\n```\n\n### Step 4: Verify Solution\n\nAsk user to try their original operation (push, pull, clone) and confirm it works.\n\nIf still having issues, ask for the error message and troubleshoot further.\n\n## Solution Workflow 2: Git vs GitHub Confusion\n\n**Problem confirmed:** User is confused about the difference between Git and GitHub.\n\n### Step 1: Explain the Difference\n\nProvide clear explanation:\n\n**Git** = The version control tool installed on your computer\n- Command-line tool you run locally\n- Commands: `git init`, `git add`, `git commit`, `git status`, etc.\n- Works entirely offline\n- Manages your local repository\n\n**GitHub** = The hosting platform (website) for your code\n- Remote server accessible at github.com\n- Stores your code in the cloud\n- Provides web interface for collaboration\n- Your code lives here so others can access it\n\n**The Connection:**\n- You use Git (locally) to interact with GitHub (remotely)\n- Git commands like `git push` send your local changes to GitHub\n- Git commands like `git pull` download changes from GitHub\n\n### Step 2: Common Command Breakdown\n\nShow which commands are used for what:\n\n**Local Git Operations (no internet needed):**\n- `git init` - Initialize a new repository\n- `git add <file>` - Stage files for commit\n- `git commit -m \"message\"` - Save changes locally\n- `git status` - Check what's changed\n- `git log` - View commit history\n- `git branch` - Manage branches\n\n**GitHub Operations (require internet):**\n- `git clone <url>` - Download repository from GitHub\n- `git push` - Upload your commits to GitHub\n- `git pull` - Download changes from GitHub\n- `git fetch` - Check for remote changes\n\n**GitHub Web Interface:**\n- Creating repositories\n- Managing collaborators\n- Viewing pull requests\n- Adjusting settings\n\n### Step 3: Practical Example\n\nOffer to demonstrate a typical workflow:\n\n1. Create or modify files  **You do this in your editor**\n2. `git add .`  **Git command (local)**\n3. `git commit -m \"description\"`  **Git command (local)**\n4. `git push`  **Git command that talks to GitHub (remote)**\n\n### Step 4: Address Specific Questions\n\nAsk if they have any specific questions about:\n- Which tool to use for their current task\n- Where to find certain features\n- How to accomplish a specific goal\n\nProvide targeted answers based on their questions.\n\n## Solution Workflow 3: Syncing Local Folder with GitHub\n\n**Problem confirmed:** User wants to keep their local folder synchronized with a GitHub repository.\n\n### Step 1: Set Expectations\n\nExplain that Git is not automatic cloud storage:\n- Unlike Dropbox or Google Drive, Git requires manual synchronization\n- This is intentional - you control exactly what gets saved and when\n- The workflow has three steps: stage  commit  push\n\n### Step 2: Check Current Setup\n\nAsk: \"Do you already have a GitHub repository created for this folder?\"\n\n**If NO:**\n\n1. **Create repository on GitHub:**\n   - Go to: https://github.com/new\n   - Give it a name matching your local folder\n   - Choose public or private\n   - **Do NOT** initialize with README, .gitignore, or license (since you have local files)\n   - Click \"Create repository\"\n\n2. **Initialize local folder:**\n\nAsk for their local folder path and offer to run:\n```bash\ncd /path/to/their/folder\ngit init\ngit remote add origin https://github.com/username/repo-name.git\n```\n\n**If YES:**\n\nCheck if folder is already initialized:\n```bash\ngit status\n```\n\nOffer to run this. If it shows \"not a git repository\", proceed with initialization.\n\n### Step 3: Configure .gitignore\n\nBefore first commit, ask about files they DON'T want synced:\n\nCommon files to exclude:\n- System files (.DS_Store, Thumbs.db)\n- Dependencies (node_modules/, venv/, .venv/)\n- Build outputs (dist/, build/, *.pyc)\n- Environment files (.env, .env.local)\n- IDE settings (.vscode/, .idea/)\n- Logs (*.log)\n\nOffer to create a .gitignore file with appropriate exclusions based on their project type.\n\nAsk: \"What type of project is this?\" (Python, JavaScript, Java, etc.)\n\nGenerate appropriate .gitignore template and offer to create the file.\n\n### Step 4: Initial Sync\n\nWalk through the first synchronization:\n\n```bash\n# 1. Stage all files\ngit add .\n\n# 2. Create first commit\ngit commit -m \"Initial commit\"\n\n# 3. Push to GitHub\ngit branch -M main\ngit push -u origin main\n```\n\nOffer to run these commands step by step, explaining what each does.\n\n### Step 5: Regular Workflow\n\nTeach the ongoing synchronization routine:\n\n**Before starting work each day:**\n```bash\ngit pull origin main\n```\nThis downloads any changes from GitHub (in case you worked elsewhere or teammates contributed).\n\n**After making changes:**\n```bash\n# 1. Check what changed\ngit status\n\n# 2. Review changes (optional but recommended)\ngit diff\n\n# 3. Stage specific files or all changes\ngit add <file>  # for specific file\n# or\ngit add .  # for all changes\n\n# 4. Commit with descriptive message\ngit commit -m \"Describe what you changed and why\"\n\n# 5. Push to GitHub\ngit push origin main\n```\n\n### Step 6: Best Practices\n\nShare important tips:\n- **Commit frequently:** Small, logical commits are better than large ones\n- **Write good messages:** Describe WHAT and WHY, not just what files changed\n- **Pull before push:** Always pull latest changes before pushing\n- **Review before commit:** Use `git status` and `git diff` to see what you're committing\n\n### Step 7: Create Quick Reference\n\nOffer to create a cheat sheet file in their directory with the common commands.\n\nIf accepted, create `GIT_WORKFLOW.md`:\n```markdown\n# Git Sync Workflow\n\n## Daily Routine\n\n### Before Starting Work\ngit pull origin main\n\n### After Making Changes\ngit status              # See what changed\ngit diff                # Review changes\ngit add .               # Stage all changes\ngit commit -m \"message\" # Commit with description\ngit push origin main    # Upload to GitHub\n\n## Tips\n- Commit frequently (multiple times per day)\n- Pull before starting work\n- Write clear commit messages\n- Review changes before committing\n```\n\n## Solution Workflow 4: Merge Conflicts\n\n**Problem confirmed:** User encountered merge conflict errors.\n\n### Step 1: Calm and Explain\n\nReassure the user:\n- Merge conflicts are normal and expected in collaborative work\n- They happen when the same lines of code are edited in different ways\n- They're not dangerous - just need manual resolution\n- Git is asking you to choose which version to keep\n\n### Step 2: Understand the Situation\n\nAsk clarifying questions:\n- What were you doing when this happened? (pulling, merging, rebasing)\n- Are you working alone or with others?\n- Do you know what changes exist on GitHub vs your local files?\n\n### Step 3: Show Current Status\n\nOffer to run:\n```bash\ngit status\n```\n\nThis shows which files have conflicts.\n\n### Step 4: Explain Conflict Markers\n\nExplain what they'll see in conflicted files:\n\n```\n<<<<<<< HEAD\nYour local changes are here\n=======\nThe changes from GitHub are here\n>>>>>>> branch-name\n```\n\n- `<<<<<<< HEAD` marks the start of your local version\n- `=======` separates the two versions\n- `>>>>>>>` marks the end of the remote version\n\n### Step 5: Resolution Strategy\n\nAsk which approach they prefer:\n\n**Option A: Manual Resolution (Recommended)**\n1. Open each conflicted file\n2. Find the conflict markers\n3. Decide what to keep:\n   - Keep your version (delete the other and markers)\n   - Keep their version (delete yours and markers)\n   - Combine both (merge the logic and delete markers)\n4. Save the file\n5. Stage the resolved file: `git add <file>`\n6. Complete the merge: `git commit`\n\n**Option B: Choose All Yours**\n```bash\ngit checkout --ours <file>\ngit add <file>\n```\n\n**Option C: Choose All Theirs**\n```bash\ngit checkout --theirs <file>\ngit add <file>\n```\n\n**Option D: Abort the Merge**\n```bash\ngit merge --abort\n# or\ngit rebase --abort\n```\n\n### Step 6: Guide Through Resolution\n\nBased on their choice, guide step-by-step:\n\n**For Manual Resolution:**\n1. Offer to read the conflicted file and show them the conflicts\n2. For each conflict, ask: \"Do you want to keep your version, their version, or combine them?\"\n3. Offer to make the edits for them\n4. After all conflicts resolved, run:\n```bash\ngit add <resolved-file>\n```\n5. Complete the merge:\n```bash\ngit commit -m \"Resolved merge conflicts\"\n```\n\n### Step 7: Verify and Complete\n\nAfter resolution:\n```bash\ngit status  # Should show no conflicts\ngit push origin main  # Upload the resolution\n```\n\nOffer to run these commands.\n\n### Step 8: Prevention Tips\n\nShare strategies to minimize future conflicts:\n- **Pull frequently:** Get changes before they pile up\n- **Communicate:** Let teammates know what files you're working on\n- **Keep commits small:** Easier to resolve smaller conflicts\n- **Work on different files:** When possible, divide work to avoid same-file edits\n\n## Solution Workflow 5: Sensitive Data Exposure\n\n**Problem confirmed:** User accidentally committed sensitive information (passwords, API keys, tokens, etc.)\n\n### Step 1: Assess Urgency\n\nAsk critical questions:\n- Have you pushed this to GitHub yet?\n- Is the repository public or private?\n- What type of sensitive data? (password, API key, private key, credentials)\n\n**CRITICAL: If pushed to public repository:**\nEmphasize extreme urgency:\n- Assume the secrets are already compromised\n- They need to rotate/change all exposed credentials IMMEDIATELY\n- Bots scan public GitHub commits for secrets within minutes\n\n### Step 2: Immediate Action - Rotate Credentials\n\n**This is the most important step:**\n\nInstruct user to immediately:\n1. **Change the exposed password** (if it's a password)\n2. **Revoke and regenerate API keys** (if API keys)\n3. **Rotate tokens** (if tokens/secrets)\n4. **Disable compromised credentials** (if service accounts)\n\nExplain: Removing the file from Git history does NOT undo the exposure. Anyone who saw the commit still has the secret.\n\n### Step 3: Identify Affected Files\n\nOffer to run:\n```bash\ngit status\n```\n\nAsk them to identify which files contain sensitive data.\n\n### Step 4: Removal Strategy\n\n**If NOT yet pushed:**\n\nExplain this is much simpler since it's only local.\n\n**Option A: Remove from last commit (if just committed):**\n```bash\n# Remove file from tracking but keep locally\ngit rm --cached <sensitive-file>\n\n# Amend the previous commit\ngit commit --amend -m \"Remove sensitive file\"\n```\n\n**Option B: Completely remove file:**\n```bash\ngit rm --cached <sensitive-file>\ngit commit -m \"Remove sensitive file\"\n```\n\nOffer to run these commands.\n\n**If ALREADY pushed:**\n\nExplain this is more complex and requires rewriting history.\n\n**Important warnings:**\n- This will rewrite commit history\n- If others have pulled the commits, they'll need to re-clone\n- This should only be done if absolutely necessary\n\n**Option A: Use BFG Repo-Cleaner (Recommended for large repos):**\n\nProvide instructions:\n1. Download BFG: https://rtyley.github.io/bfg-repo-cleaner/\n2. Clone a fresh mirror:\n```bash\ngit clone --mirror https://github.com/username/repo.git\n```\n3. Run BFG:\n```bash\nbfg --delete-files <filename> repo.git\n# or for text replacement\nbfg --replace-text passwords.txt repo.git\n```\n4. Clean up and push:\n```bash\ncd repo.git\ngit reflog expire --expire=now --all\ngit gc --prune=now --aggressive\ngit push --force\n```\n\n**Option B: Use git filter-branch (Built-in but slower):**\n```bash\ngit filter-branch --force --index-filter \\\n  \"git rm --cached --ignore-unmatch <path-to-file>\" \\\n  --prune-empty --tag-name-filter cat -- --all\n\ngit push --force --all\n```\n\nOffer to help run these commands but warn about the implications.\n\n### Step 5: Prevent Future Exposure\n\n**Create/Update .gitignore:**\n\nOffer to create or update .gitignore with common sensitive file patterns:\n```\n# Environment variables\n.env\n.env.local\n.env.*.local\n\n# Credentials\n**/credentials.json\n**/secrets.yml\n**/*secret*\n**/*credentials*\n\n# Private keys\n*.pem\n*.key\n*.p12\n*.pfx\n\n# API keys\n**/apikeys.txt\n\n# Config files with secrets\nconfig/database.yml\nconfig/secrets.yml\n```\n\nAsk about their specific project needs and customize accordingly.\n\n### Step 6: Implement Pre-commit Checks\n\nSuggest using tools to prevent future accidents:\n\n**Option 1: git-secrets**\n```bash\n# Install git-secrets\n# macOS\nbrew install git-secrets\n\n# Configure for repo\ngit secrets --install\ngit secrets --register-aws\n```\n\n**Option 2: detect-secrets**\n```bash\npip install detect-secrets\ndetect-secrets scan > .secrets.baseline\n```\n\nOffer to help set these up.\n\n### Step 7: Verify Removal\n\nAfter cleanup, verify the sensitive data is gone:\n```bash\ngit log --all --full-history -- <path-to-file>\n```\n\nShould show no commits.\n\nSearch for the sensitive string in history:\n```bash\ngit log -S \"sensitive-string\" --all\n```\n\nShould return no results.\n\n### Step 8: Final Checklist\n\nConfirm with user they've completed:\n- [ ] Rotated/changed all exposed credentials\n- [ ] Removed sensitive files from Git history\n- [ ] Added files to .gitignore\n- [ ] Pushed the cleanup (if applicable)\n- [ ] Verified removal with git log\n- [ ] Set up pre-commit hooks (optional)\n- [ ] Documented which credentials were exposed for their records\n\n## Ongoing Support and Follow-up\n\nAfter completing any solution workflow:\n\n1. **Verify Success:** Ask user to confirm the problem is resolved\n2. **Test the Solution:** Encourage them to test the operation that was failing\n3. **Provide Resources:** Share relevant GitHub documentation links\n4. **Offer Additional Help:** Ask if they have other GitHub-related questions\n5. **Teach Prevention:** Share best practices to avoid the issue in the future\n\n## Additional Troubleshooting\n\nIf the user's issue doesn't fit the 5 main scenarios or solution didn't work:\n\n### Common Additional Issues\n\n**Permission Denied Errors:**\n- Check repository access permissions on GitHub\n- Verify correct username/organization\n- Confirm repository exists and URL is correct\n\n**\"Repository not found\" Errors:**\n- Verify repository URL spelling\n- Check if repository is private (requires authentication)\n- Confirm user has access to the repository\n\n**Detached HEAD State:**\n- Explain what detached HEAD means\n- Guide back to branch: `git checkout main`\n\n**Large File Errors:**\n- Explain GitHub's file size limits (100MB)\n- Suggest Git LFS for large files\n- Help remove large files if needed\n\n**Untracked Files Overwrite Errors:**\n- Use `git stash` to save local changes\n- Pull, then apply stash: `git stash pop`\n\nFor any issue, maintain the diagnostic approach:\n1. Ask for exact error message\n2. Check current state with `git status`\n3. Understand what they were trying to do\n4. Provide step-by-step solution\n5. Verify resolution\n\n## Best Practices for This Skill\n\n**Tone:**\n- Patient and encouraging - users are learning\n- Non-judgmental about mistakes (everyone makes them)\n- Clear and specific with instructions\n- Celebrate successes when issues are resolved\n\n**Command Execution:**\n- Always explain what a command does before running it\n- Offer to run commands but let user decide\n- Show expected output so they know what's normal\n- If a command fails, read the error and adjust approach\n\n**Teaching Approach:**\n- Explain the \"why\" not just the \"how\"\n- Connect actions to concepts\n- Build mental models of how Git/GitHub work\n- Empower users to solve similar issues themselves\n\n**Safety:**\n- Warn before destructive operations (force push, filter-branch)\n- Recommend backups when rewriting history\n- Double-check before rotating credentials\n- Verify commands in safe environments first\n\n**Follow-through:**\n- Don't leave users hanging at any step\n- Verify each step completed before moving to next\n- Offer alternatives if primary solution doesn't work\n- Ensure problem is fully resolved, not just partially\n\n## Quick Reference Commands\n\nFor easy copy-paste during troubleshooting:\n\n**Status Checks:**\n```bash\ngit status\ngit remote -v\ngit branch -a\ngit log --oneline -5\n```\n\n**Common Fixes:**\n```bash\n# Undo last commit (keep changes)\ngit reset --soft HEAD~1\n\n# Discard all local changes\ngit reset --hard HEAD\n\n# View what will be pushed\ngit diff origin/main main\n\n# Update remote tracking\ngit fetch --prune\n\n# Sync fork with upstream\ngit fetch upstream\ngit merge upstream/main\n```\n\n**Config:**\n```bash\n# Set username\ngit config --global user.name \"Your Name\"\n\n# Set email\ngit config --global user.email \"your.email@example.com\"\n\n# View all config\ngit config --list\n```\n\nThis comprehensive skill should handle the vast majority of GitHub issues faced by new users with accuracy and clarity.\n",
        "skills/cover_letter_generator/.claude/skills/internal-comms/SKILL.md": "---\nname: internal-comms\ndescription: A set of resources to help me write all kinds of internal communications, using the formats that my company likes to use. Claude should use this skill whenever asked to write some sort of internal communications (status reports, leadership updates, 3P updates, company newsletters, FAQs, incident reports, project updates, etc.).\nlicense: Complete terms in LICENSE.txt\n---\n\n## When to use this skill\nTo write internal communications, use this skill for:\n- 3P updates (Progress, Plans, Problems)\n- Company newsletters\n- FAQ responses\n- Status reports\n- Leadership updates\n- Project updates\n- Incident reports\n\n## How to use this skill\n\nTo write any internal communication:\n\n1. **Identify the communication type** from the request\n2. **Load the appropriate guideline file** from the `examples/` directory:\n    - `examples/3p-updates.md` - For Progress/Plans/Problems team updates\n    - `examples/company-newsletter.md` - For company-wide newsletters\n    - `examples/faq-answers.md` - For answering frequently asked questions\n    - `examples/general-comms.md` - For anything else that doesn't explicitly match one of the above\n3. **Follow the specific instructions** in that file for formatting, tone, and content gathering\n\nIf the communication type doesn't match any existing guideline, ask for clarification or more context about the desired format.\n\n## Keywords\n3P updates, company newsletter, company comms, weekly update, faqs, common questions, updates, internal comms\n",
        "skills/cover_letter_generator/.claude/skills/internal-comms/examples/3p-updates.md": "## Instructions\nYou are being asked to write a 3P update. 3P updates stand for \"Progress, Plans, Problems.\" The main audience is for executives, leadership, other teammates, etc. They're meant to be very succinct and to-the-point: think something you can read in 30-60sec or less. They're also for people with some, but not a lot of context on what the team does.\n\n3Ps can cover a team of any size, ranging all the way up to the entire company. The bigger the team, the less granular the tasks should be. For example, \"mobile team\" might have \"shipped feature\" or \"fixed bugs,\" whereas the company might have really meaty 3Ps, like \"hired 20 new people\" or \"closed 10 new deals.\" \n\nThey represent the work of the team across a time period, almost always one week. They include three sections:\n1) Progress: what the team has accomplished over the next time period. Focus mainly on things shipped, milestones achieved, tasks created, etc.\n2) Plans: what the team plans to do over the next time period. Focus on what things are top-of-mind, really high priority, etc. for the team.\n3) Problems: anything that is slowing the team down. This could be things like too few people, bugs or blockers that are preventing the team from moving forward, some deal that fell through, etc.\n\nBefore writing them, make sure that you know the team name. If it's not specified, you can ask explicitly what the team name you're writing for is.\n\n\n## Tools Available\nWhenever possible, try to pull from available sources to get the information you need:\n- Slack: posts from team members with their updates - ideally look for posts in large channels with lots of reactions\n- Google Drive: docs written from critical team members with lots of views\n- Email: emails with lots of responses of lots of content that seems relevant\n- Calendar: non-recurring meetings that have a lot of importance, like product reviews, etc.\n\n\nTry to gather as much context as you can, focusing on the things that covered the time period you're writing for:\n- Progress: anything between a week ago and today\n- Plans: anything from today to the next week\n- Problems: anything between a week ago and today\n\n\nIf you don't have access, you can ask the user for things they want to cover. They might also include these things to you directly, in which case you're mostly just formatting for this particular format.\n\n## Workflow\n\n1. **Clarify scope**: Confirm the team name and time period (usually past week for Progress/Problems, next\nweek for Plans)\n2. **Gather information**: Use available tools or ask the user directly\n3. **Draft the update**: Follow the strict formatting guidelines\n4. **Review**: Ensure it's concise (30-60 seconds to read) and data-driven\n\n## Formatting\n\nThe format is always the same, very strict formatting. Never use any formatting other than this. Pick an emoji that is fun and captures the vibe of the team and update.\n\n[pick an emoji] [Team Name] (Dates Covered, usually a week)\nProgress: [1-3 sentences of content]\nPlans: [1-3 sentences of content]\nProblems: [1-3 sentences of content]\n\nEach section should be no more than 1-3 sentences: clear, to the point. It should be data-driven, and generally include metrics where possible. The tone should be very matter-of-fact, not super prose-heavy.",
        "skills/cover_letter_generator/.claude/skills/internal-comms/examples/company-newsletter.md": "## Instructions\nYou are being asked to write a company-wide newsletter update. You are meant to summarize the past week/month of a company in the form of a newsletter that the entire company will read. It should be maybe ~20-25 bullet points long. It will be sent via Slack and email, so make it consumable for that.\n\nIdeally it includes the following attributes:\n- Lots of links: pulling documents from Google Drive that are very relevant, linking to prominent Slack messages in announce channels and from executives, perhgaps referencing emails that went company-wide, highlighting significant things that have happened in the company.\n- Short and to-the-point: each bullet should probably be no longer than ~1-2 sentences\n- Use the \"we\" tense, as you are part of the company. Many of the bullets should say \"we did this\" or \"we did that\"\n\n## Tools to use\nIf you have access to the following tools, please try to use them. If not, you can also let the user know directly that their responses would be better if they gave them access.\n\n- Slack: look for messages in channels with lots of people, with lots of reactions or lots of responses within the thread\n- Email: look for things from executives that discuss company-wide announcements\n- Calendar: if there were meetings with large attendee lists, particularly things like All-Hands meetings, big company announcements, etc. If there were documents attached to those meetings, those are great links to include.\n- Documents: if there were new docs published in the last week or two that got a lot of attention, you can link them. These should be things like company-wide vision docs, plans for the upcoming quarter or half, things authored by critical executives, etc.\n- External press: if you see references to articles or press we've received over the past week, that could be really cool too.\n\nIf you don't have access to any of these things, you can ask the user for things they want to cover. In this case, you'll mostly just be polishing up and fitting to this format more directly.\n\n## Sections\nThe company is pretty big: 1000+ people. There are a variety of different teams and initiatives going on across the company. To make sure the update works well, try breaking it into sections of similar things. You might break into clusters like {product development, go to market, finance} or {recruiting, execution, vision}, or {external news, internal news} etc. Try to make sure the different areas of the company are highlighted well.\n\n## Prioritization\nFocus on:\n- Company-wide impact (not team-specific details)\n- Announcements from leadership\n- Major milestones and achievements\n- Information that affects most employees\n- External recognition or press\n\nAvoid:\n- Overly granular team updates (save those for 3Ps)\n- Information only relevant to small groups\n- Duplicate information already communicated\n\n## Example Formats\n\n:megaphone: Company Announcements\n- Announcement 1\n- Announcement 2\n- Announcement 3\n\n:dart: Progress on Priorities\n- Area 1\n    - Sub-area 1\n    - Sub-area 2\n    - Sub-area 3\n- Area 2\n    - Sub-area 1\n    - Sub-area 2\n    - Sub-area 3\n- Area 3\n    - Sub-area 1\n    - Sub-area 2\n    - Sub-area 3\n\n:pillar: Leadership Updates\n- Post 1\n- Post 2\n- Post 3\n\n:thread: Social Updates\n- Update 1\n- Update 2\n- Update 3\n",
        "skills/cover_letter_generator/.claude/skills/internal-comms/examples/faq-answers.md": "## Instructions\nYou are an assistant for answering questions that are being asked across the company. Every week, there are lots of questions that get asked across the company, and your goal is to try to summarize what those questions are. We want our company to be well-informed and on the same page, so your job is to produce a set of frequently asked questions that our employees are asking and attempt to answer them. Your singular job is to do two things:\n\n- Find questions that are big sources of confusion for lots of employees at the company, generally about things that affect a large portion of the employee base\n- Attempt to give a nice summarized answer to that question in order to minimize confusion.\n\nSome examples of areas that may be interesting to folks: recent corporate events (fundraising, new executives, etc.), upcoming launches, hiring progress, changes to vision or focus, etc.\n\n\n## Tools Available\nYou should use the company's available tools, where communication and work happens. For most companies, it looks something like this:\n- Slack: questions being asked across the company - it could be questions in response to posts with lots of responses, questions being asked with lots of reactions or thumbs up to show support, or anything else to show that a large number of employees want to ask the same things\n- Email: emails with FAQs written directly in them can be a good source as well\n- Documents: docs in places like Google Drive, linked on calendar events, etc. can also be a good source of FAQs, either directly added or inferred based on the contents of the doc\n\n## Formatting\nThe formatting should be pretty basic:\n\n- *Question*: [insert question - 1 sentence]\n- *Answer*: [insert answer - 1-2 sentence]\n\n## Guidance\nMake sure you're being holistic in your questions. Don't focus too much on just the user in question or the team they are a part of, but try to capture the entire company. Try to be as holistic as you can in reading all the tools available, producing responses that are relevant to all at the company.\n\n## Answer Guidelines\n- Base answers on official company communications when possible\n- If information is uncertain, indicate that clearly\n- Link to authoritative sources (docs, announcements, emails)\n- Keep tone professional but approachable\n- Flag if a question requires executive input or official response",
        "skills/cover_letter_generator/.claude/skills/internal-comms/examples/general-comms.md": "  ## Instructions\n  You are being asked to write internal company communication that doesn't fit into the standard formats (3P\n  updates, newsletters, or FAQs).\n\n  Before proceeding:\n  1. Ask the user about their target audience\n  2. Understand the communication's purpose\n  3. Clarify the desired tone (formal, casual, urgent, informational)\n  4. Confirm any specific formatting requirements\n\n  Use these general principles:\n  - Be clear and concise\n  - Use active voice\n  - Put the most important information first\n  - Include relevant links and references\n  - Match the company's communication style",
        "skills/cover_letter_generator/.claude/skills/interview/SKILL.md": "---\nname: interview\ndescription: |\n  This skill conducts discovery conversations to understand user intent and agree on approach before taking action. It should be used when the user explicitly calls /interview, asks for recommendations, needs brainstorming, wants to clarify, or when the request could be misunderstood. Prevents building the wrong thing by uncovering WHY behind WHAT.\n---\n\n# Interview Skill\n\nPrevent building the wrong thing. Discover user's intent (WHY), validate assumptions, and agree on approach (WHAT) before taking action.\n\n## What This Skill Does\n\n- Discovers INTENT behind surface requests (WHY they want it)\n- Surfaces and validates AI's assumptions before acting\n- Explores solution options informed by intent\n- Reaches mutual agreement on both problem and solution\n- Works for any context: software, documents, brainstorming, automation\n\n## What This Skill Does NOT Do\n\n- Follow rigid scripts\n- Skip to implementation without understanding\n- Accept surface requests without exploring intent\n- Make assumptions without validating them\n\n---\n\n## Core Problem This Skill Solves\n\n**AI builds the wrong thing because it:**\n1. Takes surface requests literally without understanding intent\n2. Makes hidden assumptions it never validates\n3. Proceeds without confirming alignment\n\n**This skill ensures:**\n1. Intent (WHY) is discovered, not just request (WHAT)\n2. Assumptions are surfaced and validated\n3. Both problem and solution are agreed before proceeding\n\n---\n\n## The WHY + WHAT Model\n\n```\nSurface WHAT  Discover WHY  Surface Assumptions \nInformed WHAT  Agree on Both  Proceed\n```\n\n| Phase | Purpose | Example |\n|-------|---------|---------|\n| **Surface WHAT** | Capture initial request | \"Add dark mode\" |\n| **Discover WHY** | Uncover intent/problem | \"Eye strain for night workers\" |\n| **Surface Assumptions** | Expose AI's hidden assumptions | \"Assuming web app, not mobile\" |\n| **Informed WHAT** | Solution options based on WHY | \"Dark mode + auto-brightness + schedule\" |\n| **Agree on Both** | Confirm problem AND solution | \"Solving eye strain via dark mode with auto-switch\" |\n\n---\n\n## When to Trigger\n\n| Trigger | Example |\n|---------|---------|\n| Explicit invocation | `/interview`, \"let's clarify\" |\n| Request could be misunderstood | Ambiguous, complex, or multi-part requests |\n| Recommendations needed | \"What should I use for...\" |\n| Brainstorming | \"Help me think through...\" |\n| High-stakes work | Where wrong output wastes significant effort |\n\n**Don't over-trigger**: Simple, clear requests don't need full discovery.\n\n---\n\n## Discovery Flow\n\n### Before Starting\n\nGather available context before asking questions:\n\n| Source | Gather |\n|--------|--------|\n| **Conversation** | User's stated request, prior context |\n| **Available Context** | Information already shared in session |\n| **Skill References** | Question patterns from `references/` |\n\n### 1. Surface WHAT\n\nCapture the initial request clearly.\n\n```\n\"Let me make sure I understand - you're asking for [X]?\"\n```\n\n### 2. Discover WHY\n\n**This is the critical step most AI skips.**\n\nGo beyond WHAT to understand WHY:\n\n| Ask | To Discover |\n|-----|-------------|\n| \"What problem does this solve?\" | The real need |\n| \"Why now?\" | Urgency and context |\n| \"What happens if we don't do this?\" | Stakes and priority |\n| \"Who benefits and how?\" | Users and value |\n| \"What led to this request?\" | Background and triggers |\n\n**Techniques for WHY:**\n\n**Laddering** - Dig into abstract goals:\n```\n\"Dark mode\"  \"Why?\"  \"Eye strain\"  \"Why an issue?\"  \"Night shift workers\"\n```\n\n**5 Whys** - Uncover root need:\n```\n\"Export feature\"  Why?  \"Share reports\"  Why?  \"Stakeholder reviews\"  Root need\n```\n\n**Structuring Clarifications**:\n\nWhen presenting multiple questions, distinguish must-know from nice-to-know:\n\n```\n## Required Clarifications\n1. [Critical question - blocks progress]\n2. [Critical question - affects core approach]\n\n## Optional Clarifications (if relevant)\n3. [Nice-to-know - can assume reasonable default]\n\nNote: Keep to 1-4 questions per round. Build on answers.\n```\n\n### 3. Surface Assumptions\n\n**This prevents \"builds wrong thing.\"**\n\nAI always makes assumptions. Surface them explicitly:\n\n```\n\"I'm assuming:\n- This is for [platform/context]\n- Users are [type]\n- We need to support [X] but not [Y]\n- [Other assumption]\n\nAre these correct?\"\n```\n\n**Common hidden assumptions:**\n- Technology/platform\n- User expertise level\n- Scale/performance needs\n- Integration requirements\n- What's in vs out of scope\n\n### 4. Informed WHAT\n\nNow that WHY is clear, explore WHAT options:\n\n```\n\"Given that you need [WHY], we could:\n1. [Option A] - [trade-off]\n2. [Option B] - [trade-off]\n3. [Option C] - [trade-off]\n\nWhich fits your intent best?\"\n```\n\n**Key**: Options should address the WHY, not just the surface WHAT.\n\n### 5. Agree on Both\n\nConfirm understanding of BOTH problem and solution:\n\n```\n## Understanding\n\n**Problem (WHY)**: [What we're solving and why it matters]\n\n**Solution (WHAT)**: [What we'll build/do]\n\n**Key decisions**:\n- [Decision 1]\n- [Decision 2]\n\n**Not included**: [Explicit scope boundaries]\n\nDoes this capture it correctly?\n```\n\n**Only proceed after explicit confirmation.**\n\n---\n\n## Depth Check\n\nHow do you know understanding is deep enough?\n\n### Surface Understanding (NOT enough)\n- Can repeat what user asked for\n- Know the immediate request\n- Haven't explored why\n\n### Deep Understanding (ENOUGH)\n- [ ] Know WHY they want it, not just WHAT\n- [ ] Know what problem it solves\n- [ ] Assumptions are surfaced and validated\n- [ ] Know who benefits and how\n- [ ] Know what's explicitly out of scope\n- [ ] Could explain it to someone else accurately\n- [ ] User confirmed understanding is correct\n\n**Test**: If you proceeded now and built something, would user say \"yes, that's what I meant\" or \"no, you misunderstood\"?\n\n---\n\n## Assumption Categories\n\nSurface assumptions in these areas:\n\n| Category | Example Assumptions |\n|----------|---------------------|\n| **Context** | Platform, environment, existing systems |\n| **Users** | Who they are, expertise level, needs |\n| **Scale** | Volume, performance requirements |\n| **Scope** | What's included vs excluded |\n| **Quality** | Standards, constraints, requirements |\n| **Timeline** | Urgency, phases, dependencies |\n\n---\n\n## Anti-Patterns\n\n| Anti-Pattern | What Happens | Fix |\n|--------------|--------------|-----|\n| Skip WHY | Build wrong solution | Always ask why before how |\n| Hidden assumptions | Surprise misalignment | Surface and validate explicitly |\n| Accept surface request | Miss real need | Dig deeper with laddering/5 whys |\n| Proceed without confirm | Waste effort | Get explicit \"yes, proceed\" |\n| Over-question simple requests | Annoy user | Match depth to complexity |\n\n---\n\n## Tool Adaptation\n\nUse whatever tools are available:\n\n| Goal | Approach |\n|------|----------|\n| Ask questions | Interactive tools if available, otherwise conversation |\n| Research context | Web search if needed and available |\n| Present options | Structured choices if available |\n\nThe skill describes WHAT to do. The agent uses available tools.\n\n---\n\n## Output: Understanding Summary\n\nMatch formality to situation:\n\n**Quick** (simple requests):\n```\nGot it: [WHAT] to solve [WHY]\nProceeding with [approach]. Confirm?\n```\n\n**Standard** (most cases):\n```\n## Understanding\n\n**Problem (WHY)**: [Intent and problem being solved]\n**Solution (WHAT)**: [What we'll do]\n**Key points**: [Important details]\n**Not included**: [Scope boundaries]\n\nReady to proceed?\n```\n\n**Detailed** (complex work):\nSee `references/summary-templates.md`\n\n---\n\n## Quick Reference\n\n```\n1. Surface WHAT  \"You're asking for X?\"\n2. Discover WHY  \"What problem does this solve?\"\n3. Surface assumptions  \"I'm assuming A, B, C - correct?\"\n4. Informed WHAT  \"Given WHY, we could do X, Y, or Z\"\n5. Confirm both  \"So we're solving [WHY] by doing [WHAT]?\"\n6. Proceed  Only after explicit confirmation\n```\n\n---\n\n## Reference Files\n\n| File | Purpose |\n|------|---------|\n| `references/question-patterns.md` | Techniques for discovering WHY and surfacing assumptions |\n| `references/anti-patterns.md` | Common mistakes that lead to building wrong thing |\n| `references/summary-templates.md` | Output formats for different situations |\n",
        "skills/cover_letter_generator/.claude/skills/interview/references/anti-patterns.md": "# Anti-Patterns\n\nCommon mistakes in discovery conversations and how to avoid them.\n\n---\n\n## Discovery Anti-Patterns\n\n### 1. Scripted Questioning\n\n**Symptom**: Following a fixed list of questions regardless of context.\n\n**Problem**: Misses the specific gaps in THIS situation. Wastes time on irrelevant questions.\n\n**Fix**:\n```\nIdentify what's ACTUALLY unclear in this request.\nGenerate questions to fill THOSE specific gaps.\nAdapt based on answers received.\n```\n\n### 2. Assuming User Lacks Expertise\n\n**Symptom**: Researching and explaining domain basics when user is an expert.\n\n**Problem**: Wastes user's time, feels condescending.\n\n**Fix**:\n```\nAsk: \"Do you have experience with [domain]?\"\nIf yes: Leverage their expertise, ask for their knowledge\nIf no: Then research and explain\n```\n\n### 3. Jumping to Implementation\n\n**Symptom**: Starting work before understanding is confirmed.\n\n**Problem**: Builds wrong thing, wastes effort, frustrates user.\n\n**Fix**:\n```\nAlways summarize understanding first.\nGet explicit confirmation: \"Does this capture what you want?\"\nOnly proceed after user confirms.\n```\n\n### 4. Solution Anchoring\n\n**Symptom**: Asking questions that validate a pre-decided solution.\n\n**Problem**: Misses better alternatives, biases the discovery.\n\n**Fix**:\n```\nExplore the problem space first.\nPresent multiple options when they exist.\nLet user's needs drive the solution.\n```\n\n---\n\n## Questioning Anti-Patterns\n\n### 5. Question Overload\n\n**Symptom**: Asking 5+ questions at once.\n\n**Problem**: Overwhelms user, gets rushed/incomplete answers.\n\n**Fix**:\n```\n1-4 questions maximum per round.\nBuild on answers before asking more.\nMost important questions first.\n```\n\n### 6. Leading Questions\n\n**Symptom**: \"Don't you think we should use X?\"\n\n**Problem**: User agrees without genuine consideration.\n\n**Fix**:\n```\nPresent options neutrally.\n\"We could use X, Y, or Z. What fits your needs?\"\nLet user choose, don't push.\n```\n\n### 7. Accepting Vagueness\n\n**Symptom**: User says \"make it fast\"  \"Okay, got it.\"\n\n**Problem**: \"Fast\" means different things. Undefined requirements cause problems later.\n\n**Fix**:\n```\nProbe vague terms:\n\"When you say 'fast', what response time is acceptable?\"\n\"What does 'easy to use' mean for your users?\"\nQuantify whenever possible.\n```\n\n### 8. Re-asking the Obvious\n\n**Symptom**: Asking about things user already explained.\n\n**Problem**: Shows AI wasn't listening. Wastes time.\n\n**Fix**:\n```\nTrack what's already clear from context.\nOnly ask about actual gaps.\nReference what user said: \"You mentioned X - should that include Y?\"\n```\n\n---\n\n## Context Anti-Patterns\n\n### 9. Codebase Assumption\n\n**Symptom**: Asking about code/files when user is brainstorming or starting fresh.\n\n**Problem**: Wrong context assumed. Confuses user.\n\n**Fix**:\n```\nAssess what context exists FIRST.\nFresh project?  Focus on goals, constraints\nExisting work?  Then examine what exists\nBrainstorming?  Explore possibilities\n```\n\n### 10. Specs-Only Mindset\n\n**Symptom**: Treating every discovery as \"requirements gathering.\"\n\n**Problem**: Not all work needs formal specs. Overkill for simple tasks.\n\n**Fix**:\n```\nMatch formality to situation:\n- Simple request  Quick clarification, proceed\n- Complex work  Thorough discovery\n- Brainstorming  Exploratory conversation\nGoal is understanding, not documentation.\n```\n\n### 11. Ignoring User's Time\n\n**Symptom**: Extensive questioning for simple requests.\n\n**Problem**: User wanted quick help, not an interview.\n\n**Fix**:\n```\nGauge complexity first.\nSimple + clear  Just do it\nAmbiguous  Clarify specific gaps only\nComplex  Full discovery warranted\n```\n\n---\n\n## Research Anti-Patterns\n\n### 12. Researching When User Knows\n\n**Symptom**: Web searching domain basics when user is the expert.\n\n**Problem**: Wastes time, ignores valuable user knowledge.\n\n**Fix**:\n```\nAsk about user's expertise first.\nExpert user  Ask for their knowledge\nNovice user  Then research helps\n```\n\n### 13. Skipping Research When Needed\n\n**Symptom**: Making recommendations without checking current best practices.\n\n**Problem**: Outdated or suboptimal advice.\n\n**Fix**:\n```\nIf user needs industry context  Research\nIf comparing approaches  Research current options\nIf user asks \"what's best\"  Research before recommending\n```\n\n---\n\n## Confirmation Anti-Patterns\n\n### 14. No Summary\n\n**Symptom**: Proceeding without confirming understanding.\n\n**Problem**: Misalignment discovered too late.\n\n**Fix**:\n```\nAlways summarize: \"Here's what I understand...\"\nAlways confirm: \"Does this capture what you want?\"\nOnly proceed after explicit confirmation.\n```\n\n### 15. Vague Summary\n\n**Symptom**: Summary is as vague as original request.\n\n**Problem**: Hasn't actually achieved clarity.\n\n**Fix**:\n```\nSummary should be SPECIFIC enough to act on.\nIf you can't be specific, you need more questions.\nTest: Could someone else implement from this summary?\n```\n\n---\n\n## Summary Table\n\n| Anti-Pattern | Key Fix |\n|--------------|---------|\n| Scripted questioning | Generate for THIS situation |\n| Assuming no expertise | Ask user first |\n| Jumping to implementation | Confirm understanding first |\n| Solution anchoring | Explore problem space first |\n| Question overload | 1-4 questions at a time |\n| Leading questions | Present options neutrally |\n| Accepting vagueness | Probe for specifics |\n| Re-asking obvious | Track what's clear |\n| Codebase assumption | Assess context first |\n| Specs-only mindset | Match formality to situation |\n| Ignoring user's time | Gauge complexity first |\n| Researching when user knows | Ask about expertise |\n| Skipping needed research | Research when context needed |\n| No summary | Always summarize and confirm |\n| Vague summary | Be specific enough to act on |\n",
        "skills/cover_letter_generator/.claude/skills/interview/references/question-patterns.md": "# Question Patterns\n\nTechniques for discovering WHY (intent) and surfacing assumptions.\n\n---\n\n## Core Purpose\n\nQuestions serve two goals:\n1. **Discover WHY** - Uncover intent behind surface requests\n2. **Surface assumptions** - Expose and validate what AI is assuming\n\n---\n\n## Discovering WHY\n\n### The WHY Questions\n\n| Question | Discovers |\n|----------|-----------|\n| \"What problem does this solve?\" | Real need |\n| \"Why is this needed now?\" | Urgency, context |\n| \"What happens if we don't do this?\" | Stakes, priority |\n| \"Who benefits and how?\" | Users, value |\n| \"What led to this request?\" | Background, triggers |\n| \"What does success look like?\" | Goals, criteria |\n\n### Laddering Technique\n\nDig progressively deeper into abstract requests:\n\n```\nSurface request\n   \"Why do you need that?\"\n     Reason\n       \"Why is that important?\"\n         Deeper reason\n           Root intent\n```\n\n**Example:**\n```\n\"Add dark mode\"\n   Why?  \"Reduce eye strain\"\n     Why an issue?  \"Users work at night\"\n       Why at night?  \"Support team is 24/7\"\n         Intent: Accessibility for shift workers\n```\n\n### 5 Whys Technique\n\nKeep asking \"why\" until you hit root cause:\n\n```\nRequest  Why?  Reason  Why?  Deeper  Why?  Root\n```\n\n**Example:**\n```\n\"Add export to PDF\"\n   Why?  \"Need to share reports\"\n     Why share?  \"Stakeholders review weekly\"\n       Why PDF?  \"They print them\"\n         Root: Stakeholders need printable weekly reports\n```\n\n### Intent vs Solution\n\nUsers often state solutions, not problems:\n\n| User Says (Solution) | Ask to Find (Intent) |\n|---------------------|----------------------|\n| \"Add a button for X\" | \"What should happen when users do X?\" |\n| \"Make it faster\" | \"What's slow? What speed is acceptable?\" |\n| \"Use Redis\" | \"What problem are you solving with caching?\" |\n\n---\n\n## Surfacing Assumptions\n\n### Why Assumptions Matter\n\nAI always makes assumptions. Unvalidated assumptions  wrong output.\n\n### The Assumption Declaration\n\nAfter initial understanding, explicitly state assumptions:\n\n```\n\"I'm assuming:\n- [Assumption 1]\n- [Assumption 2]\n- [Assumption 3]\n\nAre these correct?\"\n```\n\n### Assumption Categories\n\n| Category | Example Questions |\n|----------|-------------------|\n| **Context** | \"Is this for web, mobile, or both?\" |\n| **Users** | \"Who are the users? Technical or non-technical?\" |\n| **Scale** | \"How many users/requests/items are we talking?\" |\n| **Scope** | \"Should this include X, or is that separate?\" |\n| **Integration** | \"Does this need to work with existing system Y?\" |\n| **Quality** | \"Any specific performance/security requirements?\" |\n| **Timeline** | \"Is there a deadline? Can this be phased?\" |\n\n### Probing Hidden Assumptions\n\nWhen user gives vague terms, probe:\n\n| Vague Term | Probe |\n|------------|-------|\n| \"Fast\" | \"What response time is acceptable?\" |\n| \"Scalable\" | \"What scale? 100 users? 1 million?\" |\n| \"Simple\" | \"Simple for whom? What complexity is okay?\" |\n| \"Secure\" | \"What threats? What compliance requirements?\" |\n| \"Good UX\" | \"What makes UX good for your users?\" |\n\n---\n\n## Question Types\n\n### Open Questions\n**Purpose**: Explore, gather context\n**Pattern**: \"What...\", \"How...\", \"Tell me about...\"\n**Use when**: Starting discovery, exploring WHY\n\n### Closed Questions\n**Purpose**: Confirm, validate\n**Pattern**: \"Is...\", \"Should...\", \"Do you want...\"\n**Use when**: Validating assumptions, confirming understanding\n\n### Choice Questions\n**Purpose**: Help decide between options\n**Pattern**: \"Would you prefer A, B, or C?\"\n**Use when**: Multiple valid approaches exist\n\n### Probing Questions\n**Purpose**: Dig deeper into vague answers\n**Pattern**: \"When you say X, what do you mean?\"\n**Use when**: Answer contains undefined terms\n\n---\n\n## Question Pacing\n\n**Batch size**: 1-4 questions maximum\n\n**Flow**:\n1. Start with WHY questions (most important)\n2. Surface assumptions based on answers\n3. Probe vague terms\n4. Confirm understanding\n\n**Adapt depth**:\n- Simple request  Quick check, proceed\n- Complex/ambiguous  Full discovery\n- High stakes  Thorough validation\n\n---\n\n## Signs You've Discovered Enough\n\n### Surface Understanding (NOT ready)\n- Know WHAT user asked for\n- Don't know WHY they want it\n- Have unvalidated assumptions\n\n### Deep Understanding (READY)\n- Know WHY (intent/problem)\n- Know WHAT (agreed solution)\n- Assumptions validated\n- Scope boundaries clear\n- User confirmed accuracy\n\n**Test**: Could you explain to someone else WHY user wants this and WHAT you'll do, without guessing?\n",
        "skills/cover_letter_generator/.claude/skills/interview/references/summary-templates.md": "# Summary Templates\n\nFormats for capturing mutual understanding. Match formality to situation.\n\n---\n\n## Core Principle\n\n**Summaries confirm understanding, not create documentation.**\n\nThe goal is ensuring AI and user are aligned before proceeding. Keep it as simple as the situation allows.\n\n---\n\n## Standard Understanding Summary\n\nUse for most situations:\n\n```markdown\n## Understanding Summary\n\n**What we're doing**: [One clear statement]\n\n**Goals**: [What success looks like]\n\n**Key Points**:\n- [Important point 1]\n- [Important point 2]\n- [Important point 3]\n\n**Approach**: [How we'll proceed]\n\n**Boundaries**: [What's NOT included]\n\n**Open items**: [Any remaining uncertainties, or \"None\"]\n\n---\nReady to proceed?\n```\n\n---\n\n## Quick Summary\n\nFor simple, clear requests:\n\n```markdown\n**Got it**: [What user wants in one sentence]\n\n**I'll**: [What AI will do]\n\n**Confirm**: Ready to proceed?\n```\n\n---\n\n## Brainstorming Summary\n\nAfter exploratory conversations:\n\n```markdown\n## Where We Landed\n\n**Exploring**: [Topic/problem space]\n\n**Options Considered**:\n1. [Option] - [Trade-off]\n2. [Option] - [Trade-off]\n3. [Option] - [Trade-off]\n\n**Direction**: [What user is leaning toward]\n\n**Next Step**: [What happens now]\n\n**Still Open**: [Unresolved questions, if any]\n```\n\n---\n\n## Complex Work Summary\n\nFor multi-faceted projects:\n\n```markdown\n## Understanding Summary\n\n**Project**: [What this is]\n\n**Goal**: [What success looks like]\n\n### Key Elements\n1. [Element 1]: [Brief description]\n2. [Element 2]: [Brief description]\n3. [Element 3]: [Brief description]\n\n### Decisions Made\n| Decision | Choice |\n|----------|--------|\n| [Decision 1] | [What we chose] |\n| [Decision 2] | [What we chose] |\n\n### Constraints\n- [Constraint 1]\n- [Constraint 2]\n\n### Approach\n[How we'll proceed]\n\n### Not Included\n- [Out of scope 1]\n- [Out of scope 2]\n\n### Open Items\n- [Remaining question, if any]\n\n---\nDoes this capture what you want?\n```\n\n---\n\n## Decision Summary\n\nWhen helping user make a choice:\n\n```markdown\n## Decision Summary\n\n**Question**: [What we were deciding]\n\n**Options Evaluated**:\n| Option | Pros | Cons |\n|--------|------|------|\n| [A] | [Pros] | [Cons] |\n| [B] | [Pros] | [Cons] |\n\n**Decision**: [What user chose]\n\n**Rationale**: [Why this choice]\n\n**Next**: [What happens now]\n```\n\n---\n\n## Formality Guide\n\n| Situation | Template | Why |\n|-----------|----------|-----|\n| Simple request | Quick Summary | Don't over-formalize simple things |\n| Brainstorming | Brainstorming Summary | Capture exploration, not specs |\n| Clear medium task | Standard Summary | Right balance |\n| Complex/multi-part | Complex Summary | Need more structure |\n| Decision-making | Decision Summary | Document the choice |\n\n---\n\n## Summary Quality Check\n\nBefore presenting summary, verify:\n\n- [ ] **Specific**: Could someone act on this?\n- [ ] **Complete**: All key points captured?\n- [ ] **Accurate**: Reflects what user said?\n- [ ] **Scoped**: Boundaries are clear?\n- [ ] **Actionable**: Clear what happens next?\n\nIf you can't make it specific, you need more questions first.\n",
        "skills/cover_letter_generator/.claude/skills/linkedin_job_resume_optimizer/SKILL.md": "---\nname: linkedin_job_resume_optimizer\ndescription: |\n  Automates end-to-end job search workflow: searches LinkedIn for AI-related remote jobs,\n  extracts job descriptions, reads base resume, generates ATS-optimized tailored resumes,\n  performs skill gap analysis, suggests interview prep questions, and provides LinkedIn\n  profile optimization recommendations. Use when: (1) User needs job-specific resume\n  tailoring, (2) Preparing for job applications with targeted optimization, (3) Requires\n  skill gap analysis between resume and job requirements, (4) Needs interview preparation\n  questions based on skill gaps.\nallowed-tools:\n  - Bash\n  - Read\n  - Write\n  - Glob\n  - Grep\nmodel: claude-sonnet-4-5\n---\n\n# LinkedIn Job Resume Optimizer\n\nComprehensive job search automation with resume tailoring and interview preparation.\n\n## Overview\n\nThis skill orchestrates a complete job application workflow:\n1. Search LinkedIn for relevant remote AI jobs\n2. Extract full job descriptions and requirements\n3. Analyze base resume to extract skills and experience\n4. Generate ATS-optimized, job-tailored resumes\n5. Perform skill gap analysis\n6. Generate targeted interview preparation questions\n7. Suggest LinkedIn profile optimizations\n\n## Workflow Diagram\n\n```\nUser Request\n    \nWorkflow Orchestrator\n    \n\n LinkedIn         Resume            ATS             \n Scraper          Analyzer          Optimizer       \n (Playwright)     (Pandoc+Docx)     (NLP)           \n\n    \nGap Analyzer (Compare skills)\n    \nQuestion Generator (Interview prep)\n    \nFinal Report (Summary + File locations)\n```\n\n## Prerequisites\n\n### 1. Playwright MCP Server (for LinkedIn automation)\n\nStart the server before running this skill:\n```bash\nbash /mnt/d/Ali_Home/Learning/AgenticAI/AI-P009/Assignments/ProjectA1/resume_optimizer/.claude/skills/browsing-with-playwright/scripts/start-server.sh\n```\n\n### 2. Base Resume\n\nEnsure resume exists at configured path (default: `/mnt/d/Ali_Home/Learning/AgenticAI/AI-P009/Assignments/ProjectA1/resume_optimizer/resume/resume06012026.docx`)\n\n### 3. System Dependencies\n\n```bash\n# Core tools\nsudo apt-get install pandoc python3 python3-pip nodejs npm\n\n# Optional (for PDF export)\nsudo apt-get install libreoffice poppler-utils\n```\n\n### 4. Python Packages\n\n```bash\npip install python-docx spacy nltk pandas requests\npython -m spacy download en_core_web_sm\npython -m nltk.downloader punkt stopwords\n```\n\n### 5. Node Packages\n\n```bash\nnpm install -g @playwright/mcp\n```\n\n## Quick Start\n\n### Basic Usage\n\n```bash\npython3 scripts/workflow_orchestrator.py \\\n  --job-keywords \"AI Engineer remote\" \\\n  --job-count 2\n```\n\n### With Custom Resume Path\n\n```bash\npython3 scripts/workflow_orchestrator.py \\\n  --resume-path \"/path/to/your/resume.docx\" \\\n  --job-keywords \"Machine Learning Engineer\" \\\n  --job-count 2 \\\n  --output-dir \"./custom_output\"\n```\n\n## Step-by-Step Workflow\n\n### Phase 1: Initialize Playwright Server\n\n```bash\nbash /mnt/d/Ali_Home/Learning/AgenticAI/AI-P009/Assignments/ProjectA1/resume_optimizer/.claude/skills/browsing-with-playwright/scripts/start-server.sh\n```\n\nVerify server is running:\n```bash\ncurl http://localhost:8808\n```\n\n### Phase 2: Search LinkedIn Jobs\n\n```bash\npython3 scripts/linkedin_scraper.py \\\n  --keywords \"AI Engineer remote\" \\\n  --count 2 \\\n  --output jobs.json\n```\n\n**Output**: `jobs.json` containing:\n```json\n[\n  {\n    \"title\": \"Senior AI Engineer\",\n    \"company\": \"TechCorp\",\n    \"url\": \"https://linkedin.com/jobs/view/12345\",\n    \"description\": \"Full job description...\",\n    \"required_skills\": [\"Python\", \"TensorFlow\", \"MLOps\"],\n    \"preferred_skills\": [\"Kubernetes\", \"AWS\"]\n  }\n]\n```\n\n### Phase 3: Extract Base Resume Skills\n\n```bash\n# Convert DOCX to markdown\npandoc --track-changes=accept /mnt/d/Ali_Home/Learning/AgenticAI/AI-P009/Assignments/ProjectA1/resume_optimizer/resume/resume06012026.docx -o resume.md\n\n# Extract skills\npython3 scripts/resume_analyzer.py --resume-md resume.md --output base_skills.json\n```\n\n**Output**: `base_skills.json` with skills inventory\n\n### Phase 4: Generate Tailored Resumes\n\nFor each job, generate ATS-optimized resume:\n```bash\npython3 scripts/ats_optimizer.py \\\n  --base-resume /mnt/d/Ali_Home/Learning/AgenticAI/AI-P009/Assignments/ProjectA1/resume_optimizer/resume/resume06012026.docx \\\n  --job-description jobs.json \\\n  --job-index 0 \\\n  --output \"resume_tailored_Senior_AI_Engineer.docx\"\n```\n\n### Phase 5: Perform Gap Analysis\n\n```bash\npython3 scripts/gap_analyzer.py \\\n  --base-skills base_skills.json \\\n  --job-requirements jobs.json \\\n  --output gap_analysis.json\n```\n\n**Output**: `gap_analysis.json`\n```json\n[\n  {\n    \"job_title\": \"Senior AI Engineer\",\n    \"skill_gaps\": [\n      {\n        \"skill\": \"Kubernetes\",\n        \"gap_type\": \"missing\",\n        \"importance\": \"high\",\n        \"related_experience\": [\"Docker experience in current role\"]\n      }\n    ],\n    \"interview_questions\": [\n      \"Explain your understanding of Kubernetes orchestration...\",\n      \"How would you design a Kubernetes deployment for ML models?\",\n      \"...\"\n    ],\n    \"linkedin_updates\": {\n      \"about_section\": \"Add: 'Experienced in containerization with Docker, transitioning to Kubernetes for production ML deployments'\",\n      \"skills_to_add\": [\"Kubernetes\", \"Container Orchestration\", \"MLOps\"]\n    }\n  }\n]\n```\n\n### Phase 6: Generate Interview Questions\n\n```bash\npython3 scripts/question_generator.py \\\n  --gap-analysis gap_analysis.json \\\n  --questions-per-gap 10 \\\n  --output interview_prep.md\n```\n\n### Phase 7: Cleanup\n\n```bash\nbash /mnt/d/Ali_Home/Learning/AgenticAI/AI-P009/Assignments/ProjectA1/resume_optimizer/.claude/skills/browsing-with-playwright/scripts/stop-server.sh\n```\n\n## Configuration\n\nEdit `config.json` in skill directory:\n\n```json\n{\n  \"resume_path\": \"/mnt/d/Ali_Home/Learning/AgenticAI/AI-P009/Assignments/ProjectA1/resume_optimizer/resume/resume06012026.docx\",\n  \"output_directory\": \"./resume_optimizer/output\",\n  \"playwright_port\": 8808,\n  \"job_search\": {\n    \"default_keywords\": \"AI Engineer remote\",\n    \"default_count\": 2,\n    \"location\": \"Remote\"\n  },\n  \"ats_optimization\": {\n    \"keyword_density_target\": 0.75,\n    \"exact_match_priority\": true,\n    \"preserve_formatting\": true\n  },\n  \"interview_prep\": {\n    \"questions_per_gap\": 10,\n    \"include_behavioral\": true,\n    \"include_technical\": true\n  }\n}\n```\n\n## ATS Optimization Strategies\n\nSee [references/ats-optimization-guide.md](references/ats-optimization-guide.md) for detailed strategies including:\n\n### Keyword Density\n- **Required Skills**: 70-80% coverage\n- **Preferred Skills**: 40-50% coverage\n- **Exact Match**: Use exact phrases from job description\n- **Context Integration**: Weave keywords naturally into experience descriptions\n\n### Keyword Placement Priority\n1. **Professional Summary** (highest ATS weight)\n   - Include job title keyword\n   - Top 3-5 required skills\n   - Industry-specific terminology\n\n2. **Skills Section** (exact match critical)\n   - Mirror job posting's skills terminology exactly\n   - Group by category if job posting does\n\n3. **Experience Descriptions** (context + keywords)\n   - Integrate keywords into achievement statements\n   - Use action verbs + keyword + quantifiable result\n\n### Natural Integration Techniques\n\n**Action Verb + Keyword + Result**:\n```\nBefore: \"Worked on machine learning projects\"\nAfter: \"Developed TensorFlow-based ML models, reducing inference time by 30%\"\n```\n\n**Technology Stack Enumeration**:\n```\nBefore: \"Built backend systems\"\nAfter: \"Built scalable backend systems using Python, Docker, and Kubernetes on AWS infrastructure\"\n```\n\n## LinkedIn Automation\n\nSee [references/linkedin-automation.md](references/linkedin-automation.md) for:\n\n- Navigation patterns for LinkedIn job search\n- Element selectors and snapshot references\n- Rate limiting and anti-bot detection strategies\n- Fallback strategies if elements change\n\n**Key Points**:\n- Implement random delays (1-3 seconds between actions)\n- Use `--shared-browser-context` flag for Playwright MCP\n- Handle rate limiting with exponential backoff\n- Parse snapshots for job card refs\n\n## Interview Question Generation\n\nSee [references/interview-prep-patterns.md](references/interview-prep-patterns.md) for:\n\n- STAR method question templates\n- Technical depth assessment questions\n- Behavioral questions for skill gaps\n- System design scenarios\n\n**Question Distribution**:\n- **Missing Skills**: 40% fundamentals, 30% transferable experience, 30% scenarios\n- **Weak Skills**: 40% depth assessment, 30% practical application, 30% troubleshooting\n\n## Error Handling\n\n### No Jobs Found\n```python\nif len(jobs) == 0:\n    print(\"No jobs found matching criteria. Suggestions:\")\n    print(\"- Broaden search keywords\")\n    print(\"- Remove 'remote' filter\")\n    print(\"- Try alternative job titles\")\n    sys.exit(1)\n```\n\n### Resume Read Errors\n```python\ntry:\n    resume_text = extract_resume(resume_path)\nexcept FileNotFoundError:\n    print(f\"Resume not found at {resume_path}\")\n    print(\"Please provide valid resume path with --resume-path\")\n    sys.exit(1)\nexcept Exception as e:\n    print(f\"Error reading resume: {e}\")\n    print(\"Ensure resume is valid .docx format\")\n    sys.exit(1)\n```\n\n### LinkedIn Rate Limiting\n```python\n# Implement exponential backoff\nfor attempt in range(3):\n    try:\n        job_data = scrape_job(url)\n        break\n    except RateLimitError:\n        wait_time = 2 ** attempt * 5  # 5s, 10s, 20s\n        time.sleep(wait_time)\n```\n\n### Playwright Connection Failures\n- Pre-flight check: Verify server running before job search\n- Auto-restart server if connection fails\n- Retry with exponential backoff (3 attempts)\n\n## Troubleshooting\n\n| Issue | Solution |\n|-------|----------|\n| Playwright not responding | Restart: `bash scripts/stop-server.sh && bash scripts/start-server.sh` |\n| LinkedIn blocks automation | Add wait times, use headless:false, rotate user agents |\n| Resume conversion fails | Verify pandoc installed: `pandoc --version` |\n| ATS optimization weak | Review keyword extraction, increase density target in config.json |\n| No skill gaps detected | Lower similarity threshold in gap_analyzer.py |\n| Python packages missing | Run: `pip install python-docx spacy nltk pandas requests` |\n| spaCy model not found | Run: `python -m spacy download en_core_web_sm` |\n\n## Output Summary Format\n\nAfter running the workflow, you'll receive a summary report:\n\n```markdown\n# Job Search Results Summary\n\n## Jobs Found: 2\n\n### Job 1: Senior AI Engineer - TechCorp\n- **LinkedIn URL**: https://linkedin.com/jobs/view/12345\n- **Tailored Resume**: ./resume_optimizer/output/resume_tailored_Senior_AI_Engineer.docx\n- **Key Requirements**: Python, TensorFlow, MLOps, Kubernetes\n- **Skill Gaps**: Kubernetes (High priority), AWS Sagemaker (Medium)\n- **Interview Prep Questions**: 20 questions generated (see interview_prep.md)\n\n### Job 2: Machine Learning Engineer - AI Startup\n- **LinkedIn URL**: https://linkedin.com/jobs/view/67890\n- **Tailored Resume**: ./resume_optimizer/output/resume_tailored_Machine_Learning_Engineer.docx\n- **Key Requirements**: PyTorch, Docker, CI/CD, Model deployment\n- **Skill Gaps**: PyTorch (High priority - similar TensorFlow experience)\n- **Interview Prep Questions**: 15 questions generated (see interview_prep.md)\n\n## LinkedIn Profile Recommendations\n\n### About Section Updates\n- Add: \"Specialized in MLOps with focus on production-scale deployments\"\n- Emphasize: Kubernetes orchestration experience\n- Highlight: Cross-functional collaboration in AI product development\n\n### Skills to Add\n1. Kubernetes (High priority - both jobs)\n2. MLOps (Critical keyword)\n3. Model Deployment\n4. CI/CD for ML\n\n## Next Steps\n1. Review tailored resumes in ./resume_optimizer/output/\n2. Study interview prep questions in interview_prep.md\n3. Update LinkedIn profile per recommendations above\n4. Apply to jobs with tailored resumes\n```\n\n## Integration with Existing Skills\n\nThis skill leverages:\n\n### browsing-with-playwright\n- LinkedIn job search automation\n- Browser navigation and snapshot parsing\n- Start server: `bash scripts/start-server.sh`\n- Stop server: `bash scripts/stop-server.sh`\n\n### docx\n- Resume reading using pandoc\n- Tailored resume generation\n- Convert: `pandoc --track-changes=accept resume.docx -o resume.md`\n\n## Advanced Usage\n\n### Manual Job URLs (Skip LinkedIn Scraping)\n\nIf you prefer to provide job URLs manually:\n\n1. Create `jobs_manual.json`:\n```json\n[\n  {\n    \"title\": \"AI Engineer\",\n    \"company\": \"CompanyName\",\n    \"url\": \"https://linkedin.com/jobs/view/12345\",\n    \"description\": \"Paste full job description here...\"\n  }\n]\n```\n\n2. Run workflow with manual jobs:\n```bash\npython3 scripts/workflow_orchestrator.py \\\n  --manual-jobs jobs_manual.json \\\n  --skip-linkedin\n```\n\n### Custom Keyword Extraction\n\nOverride automatic skill extraction:\n\n```bash\npython3 scripts/ats_optimizer.py \\\n  --base-resume resume.docx \\\n  --job-description jobs.json \\\n  --job-index 0 \\\n  --custom-keywords \"Python,TensorFlow,Kubernetes,AWS\" \\\n  --output tailored_resume.docx\n```\n\n## Dependencies Summary\n\n### Required\n- Python 3.8+\n- Node.js 14+\n- pandoc\n- browsing-with-playwright skill\n- docx skill\n\n### Python Packages\n- python-docx\n- spacy\n- nltk\n- pandas\n- requests\n\n### Node Packages\n- @playwright/mcp\n\n## Performance\n\n- **LinkedIn Search**: 30-90 seconds (2 jobs)\n- **Resume Analysis**: 10-20 seconds\n- **Resume Tailoring**: 30-60 seconds per job\n- **Gap Analysis**: 20-30 seconds\n- **Interview Prep**: 40-60 seconds\n- **Total Estimated Time**: 2-4 minutes for 2 jobs\n\n## Limitations\n\n1. **LinkedIn Access**: Requires LinkedIn to be accessible (may need login for some content)\n2. **Job Count**: Recommended maximum 5 jobs per session to avoid rate limiting\n3. **Resume Format**: Only supports .docx format (not .doc or PDF)\n4. **Language**: Optimized for English resumes and job descriptions\n5. **Technical Roles**: Best suited for technical roles (AI, ML, Engineering)\n\n## Security & Privacy\n\n- **Local Processing**: All resume processing happens locally\n- **No Storage**: No resume data is stored remotely\n- **Browser Automation**: Uses local Playwright instance\n- **Data Privacy**: Job descriptions and resumes remain on your machine\n\n## License\n\nThis skill is part of the Claude Code skills ecosystem.\n",
        "skills/cover_letter_generator/.claude/skills/linkedin_job_resume_optimizer/references/ats-optimization-guide.md": "# ATS Optimization Guide\n\nComprehensive guide to optimizing resumes for Applicant Tracking Systems (ATS).\n\n## What is ATS?\n\nApplicant Tracking Systems (ATS) are software applications that parse, store, and rank resumes based on specific job requirements. Most large companies use ATS to filter candidates before human review.\n\n### How ATS Works\n\n1. **Parsing**: Extracts text and data from resume files\n2. **Keyword Matching**: Searches for required skills and qualifications\n3. **Scoring**: Ranks candidates based on keyword matches and relevance\n4. **Filtering**: Presents top-ranked resumes to hiring managers\n\n## Keyword Strategy\n\n### Keyword Density Targets\n\n- **Required Skills**: 70-80% coverage\n  - If a job lists 10 required skills, include 7-8 in your resume\n  - Prioritize exact phrase matches\n\n- **Preferred Skills**: 40-50% coverage\n  - Lower priority but still valuable\n  - Use synonyms if exact phrase doesn't fit naturally\n\n### Keyword Sources\n\n1. **Job Description**: Primary source of keywords\n2. **Required Qualifications Section**: Highest priority\n3. **Preferred/Nice-to-Have Section**: Secondary priority\n4. **Company Website/Culture Pages**: Additional context\n\n### Keyword Placement Priority\n\nRanked by ATS weighting:\n\n1. **Professional Summary** (Highest Weight)\n   - Include job title keyword\n   - Top 3-5 required skills\n   - Industry-specific terminology\n\n2. **Skills Section** (Exact Match Critical)\n   - Mirror job posting's skills terminology exactly\n   - Use same capitalization (AWS not aws)\n   - Group by category if job posting does\n\n3. **Experience Descriptions** (Context + Keywords)\n   - Integrate keywords into achievement statements\n   - Use action verbs + keyword + quantifiable result\n\n4. **Job Titles** (Match if Applicable)\n   - If your title matches job posting, use exact wording\n   - Add clarification in parentheses if needed\n\n## Natural Keyword Integration Techniques\n\n### Technique 1: Action Verb + Keyword + Result\n\n```\n Before: \"Worked on machine learning projects\"\n After: \"Developed TensorFlow-based ML models, reducing inference time by 30%\"\n```\n\n### Technique 2: Technology Stack Enumeration\n\n```\n Before: \"Built backend systems\"\n After: \"Built scalable backend systems using Python, Docker, and Kubernetes on AWS infrastructure\"\n```\n\n### Technique 3: Problem-Solution-Technology\n\n```\n Before: \"Improved system performance\"\n After: \"Diagnosed system bottlenecks and implemented caching using Redis, improving response time by 40%\"\n```\n\n### Technique 4: Project Context Integration\n\n```\n Before: \"Collaborated with team\"\n After: \"Collaborated with cross-functional team on MLOps pipeline using Kubernetes and Jenkins CI/CD\"\n```\n\n### Technique 5: Skills in Bullet Points\n\n```\n Before: \"Led development initiatives\"\n After: \"Led development of microservices architecture using React, Node.js, and MongoDB, deployed on AWS ECS\"\n```\n\n## Formatting Best Practices\n\n### DO:\n\n-  Use standard section headings: \"Professional Summary\", \"Skills\", \"Experience\", \"Education\"\n-  Use simple bullet points (, -, *)\n-  Include exact dates (Month Year - Month Year)\n-  Use standard fonts: Arial, Calibri, Times New Roman (10-12pt)\n-  Save as .docx (most ATS-compatible format)\n-  Use clear hierarchy (H1 for name, H2 for sections)\n-  Left-align text for best parsing\n-  Include phone number and email in standard format\n\n### DON'T:\n\n-  Tables for layout (often parsed incorrectly)\n-  Text boxes (content may not be extracted)\n-  Images or logos (ignored by ATS)\n-  Headers/footers with critical info (often not parsed)\n-  Unusual formatting (columns, graphics, shapes)\n-  PDF (unless explicitly requested; .docx is more parseable)\n-  Fancy fonts or decorative elements\n-  Using the word \"resume\" in filename (use Name_JobTitle.docx)\n\n## Section-Specific Optimization\n\n### Professional Summary\n\n**Purpose**: High-impact keyword placement\n\n**Structure**:\n```\n[Job Title] with [X] years of experience in [Top 3 Keywords].\nProven track record of [Achievement with Keywords].\nSpecialized in [Technology Stack] with focus on [Business Value].\n```\n\n**Example**:\n```\nSenior AI Engineer with 5+ years of experience in Machine Learning, MLOps, and Python.\nProven track record of deploying production-scale models using TensorFlow and Kubernetes.\nSpecialized in NLP and Computer Vision with focus on driving business impact through AI innovation.\n```\n\n### Skills Section\n\n**Format Options**:\n\n1. **Categorized** (Best for Technical Roles):\n```\nProgramming Languages: Python, Java, JavaScript, SQL\nML Frameworks: TensorFlow, PyTorch, Scikit-learn\nCloud & DevOps: AWS, Docker, Kubernetes, CI/CD\nDatabases: PostgreSQL, MongoDB, Redis\n```\n\n2. **Flat List** (Simple):\n```\nPython  TensorFlow  AWS  Kubernetes  Docker  PostgreSQL  Git  Jenkins\n```\n\n**Tips**:\n- Use exact terminology from job posting\n- Include both acronyms and full names (ML and Machine Learning)\n- List proficiency levels if space permits\n- Prioritize required skills first\n\n### Experience Section\n\n**Bullet Point Formula**:\n```\n[Action Verb] + [What] + [Using/With Keywords] + [Quantifiable Result]\n```\n\n**Examples**:\n```\n Developed microservices architecture using Node.js and Docker, reducing deployment time by 60%\n Implemented CI/CD pipeline with Jenkins and Kubernetes, enabling 20+ daily deployments\n Built recommendation system using PyTorch and AWS SageMaker, increasing user engagement by 35%\n Architected data pipeline processing 10TB daily with Spark and Airflow on AWS EMR\n```\n\n**Action Verbs by Category**:\n\n- **Leadership**: Led, Directed, Managed, Coordinated, Supervised\n- **Development**: Developed, Built, Implemented, Designed, Architected, Engineered\n- **Improvement**: Optimized, Enhanced, Streamlined, Improved, Increased, Reduced\n- **Analysis**: Analyzed, Researched, Evaluated, Assessed, Investigated\n- **Collaboration**: Collaborated, Partnered, Worked with, Coordinated with\n\n### Education Section\n\n**Format**:\n```\n[Degree], [Field of Study]\n[University Name] | [City, State] | [Graduation Date]\nGPA: [X.XX] (if 3.5+) | Relevant Coursework: [Keywords]\n```\n\n## ATS Testing Methods\n\n### Method 1: Manual Review\n\n1. Copy resume text into plain text editor\n2. Check if structure is preserved\n3. Verify all keywords are present and readable\n4. Look for garbled text or missing sections\n\n### Method 2: Online ATS Simulators\n\nFree/Paid Tools:\n- **Jobscan.co**: Compare resume to job description\n- **Resume Worded**: ATS compatibility score\n- **TopResume**: Free ATS scan\n- **SkillSyncer**: Keyword matching tool\n\n### Method 3: Conversion Test\n\n```bash\n# Convert DOCX to text and verify\npandoc resume.docx -o resume.txt\ncat resume.txt  # Review for formatting issues\n```\n\n### Method 4: ATS-Friendly Checklist\n\n- [ ] Filename: FirstName_LastName_JobTitle.docx\n- [ ] Standard sections in logical order\n- [ ] No tables, text boxes, or images\n- [ ] Simple bullet points throughout\n- [ ] Keywords from job posting included\n- [ ] Contact info clearly visible at top\n- [ ] Dates in consistent format\n- [ ] No spelling or grammar errors\n\n## Common ATS Mistakes\n\n### Mistake 1: Keyword Stuffing\n\n```\n Bad: \"Python Python Python expert with Python skills in Python development\"\n Good: \"Python developer with expertise in Django, Flask, and data analysis libraries\"\n```\n\n### Mistake 2: Using Graphics for Text\n\n```\n Bad: Skills listed in chart/graph format\n Good: Skills listed as bullet points or comma-separated\n```\n\n### Mistake 3: Non-Standard Section Names\n\n```\n Bad: \"My Journey\", \"What I'm Good At\", \"Places I've Worked\"\n Good: \"Experience\", \"Skills\", \"Professional Summary\"\n```\n\n### Mistake 4: Acronym-Only\n\n```\n Bad: \"ML, NLP, CI/CD\"\n Good: \"Machine Learning (ML), Natural Language Processing (NLP), CI/CD\"\n```\n\n### Mistake 5: Missing Dates\n\n```\n Bad: \"Software Engineer at TechCorp\"\n Good: \"Software Engineer at TechCorp | June 2020 - Present\"\n```\n\n## Advanced ATS Strategies\n\n### Strategy 1: Exact Phrase Matching\n\nIf job posting says \"5+ years of Python experience\", use that exact phrase rather than \"experienced Python developer\"\n\n### Strategy 2: Keyword Variations\n\nInclude variations:\n- Machine Learning / ML / Machine-Learning\n- Artificial Intelligence / AI\n- Full Stack / Full-Stack / Fullstack\n\n### Strategy 3: Hidden Keywords (Use Sparingly)\n\nWhite text on white background is detectable and not recommended. Instead:\n- Include keywords naturally in bullet points\n- Use a \"Relevant Coursework\" or \"Technologies\" section\n\n### Strategy 4: Tailoring Intensity Levels\n\n- **High Priority Job**: 80%+ keyword match, extensive tailoring\n- **Medium Priority**: 60-70% keyword match, moderate tailoring\n- **Exploratory**: 50%+ keyword match, light tailoring\n\n## Industry-Specific Considerations\n\n### Tech/Software Engineering\n- Emphasize: Programming languages, frameworks, cloud platforms\n- Include: GitHub profile, technical blog, portfolio projects\n- Keywords: Agile, Scrum, CI/CD, Test-Driven Development\n\n### Data Science/ML\n- Emphasize: ML frameworks, statistical methods, data tools\n- Include: Kaggle profile, publications, conference presentations\n- Keywords: A/B testing, Model deployment, Feature engineering\n\n### Cloud/DevOps\n- Emphasize: Cloud platforms, automation tools, infrastructure as code\n- Include: Certifications (AWS, Azure, Kubernetes)\n- Keywords: High availability, Scalability, Disaster recovery\n\n### Management/Leadership\n- Emphasize: Team size, budget managed, strategic initiatives\n- Include: Agile/Scrum certifications, change management\n- Keywords: Cross-functional, Stakeholder management, P&L\n\n## Final Checklist\n\nBefore submitting your ATS-optimized resume:\n\n- [ ] Job title keyword in professional summary\n- [ ] 70-80% required skills coverage\n- [ ] 40-50% preferred skills coverage\n- [ ] Standard .docx format (not PDF unless requested)\n- [ ] No tables, text boxes, or images\n- [ ] Simple, clean formatting\n- [ ] Standard section headings\n- [ ] Exact dates for all positions\n- [ ] Keywords integrated naturally (no stuffing)\n- [ ] Tested with online ATS checker\n- [ ] Proofread for spelling/grammar\n- [ ] Filename: FirstName_LastName_JobTitle.docx\n\n## Conclusion\n\nATS optimization is about making your resume machine-readable while keeping it human-readable. Focus on natural keyword integration, clean formatting, and exact matches to job requirements. Remember: the goal is to pass the ATS filter AND impress the human reviewer.\n",
        "skills/cover_letter_generator/.claude/skills/linkedin_job_resume_optimizer/references/interview-prep-patterns.md": "# Interview Preparation Patterns\n\nComprehensive guide for generating and answering interview questions based on skill gaps.\n\n## Question Generation Framework\n\n### Question Types\n\n1. **Fundamental Knowledge** (30-40% for missing skills)\n   - Core concepts and principles\n   - Basic definitions and terminology\n   - Standard use cases\n\n2. **Depth Assessment** (40% for weak skills)\n   - Advanced features and optimizations\n   - Trade-offs and design decisions\n   - Performance considerations\n\n3. **Scenario-Based** (30% for all skills)\n   - Problem-solving in realistic contexts\n   - Troubleshooting and debugging\n   - System design challenges\n\n4. **Behavioral/STAR** (20% overall)\n   - Past experience and learnings\n   - Team collaboration\n   - Handling challenges\n\n5. **Transferable Experience** (30% for missing skills with related experience)\n   - Leveraging similar technologies\n   - Learning approach and adaptability\n   - Comparative analysis\n\n## Progressive Difficulty Levels\n\n### Entry-Level Questions\n\nFocus: Definitions, basic usage, standard patterns\n\n**Examples**:\n- \"What is [technology] and when would you use it?\"\n- \"Explain the difference between [A] and [B].\"\n- \"What are the main components of [technology]?\"\n\n### Mid-Level Questions\n\nFocus: Practical application, best practices, common scenarios\n\n**Examples**:\n- \"How would you implement [feature] using [technology]?\"\n- \"What are common pitfalls when working with [technology]?\"\n- \"Describe your approach to debugging [specific issue].\"\n\n### Senior-Level Questions\n\nFocus: Architecture, optimization, leadership, trade-offs\n\n**Examples**:\n- \"Design a [system] that handles [scale/complexity].\"\n- \"What are the trade-offs between [approach A] and [approach B]?\"\n- \"How would you mentor a junior developer learning [technology]?\"\n\n## STAR Method Framework\n\n### Structure\n\n**S**ituation: Set the context\n**T**ask: Describe the challenge or objective\n**A**ction: Explain what you did\n**R**esult: Share the outcome with metrics\n\n### Example Question Types\n\n1. **Achievement Questions**\n   - \"Tell me about a time you successfully implemented [technology].\"\n   - \"Describe a project where you improved [metric] using [technology].\"\n\n2. **Challenge Questions**\n   - \"Tell me about a difficult technical problem you solved.\"\n   - \"Describe a time when you had to learn a new technology quickly.\"\n\n3. **Collaboration Questions**\n   - \"Tell me about a time you worked with a cross-functional team.\"\n   - \"Describe how you handled a disagreement with a colleague.\"\n\n4. **Failure/Learning Questions**\n   - \"Tell me about a time something went wrong. What did you learn?\"\n   - \"Describe a project that didn't go as planned.\"\n\n### Answer Template\n\n```\nSituation: \"At my previous role at [Company], we had [problem/context]...\"\n\nTask: \"I was responsible for [specific objective]...\"\n\nAction: \"I approached this by:\n  1. [First action with technology/method]\n  2. [Second action with details]\n  3. [Third action highlighting skills]\"\n\nResult: \"This led to [quantifiable outcome]:\n  - [Metric 1]: Improved by X%\n  - [Metric 2]: Reduced by Y%\n  - [Business impact]: [Result]\"\n```\n\n## Question Templates by Technology\n\n### Cloud Platforms (AWS, Azure, GCP)\n\n**Fundamental**:\n1. Explain the core services and their use cases.\n2. What is the shared responsibility model?\n3. How does IAM work for access control?\n4. Describe the networking concepts (VPC, subnets, security groups).\n\n**Scenario**:\n1. Design a highly available, scalable web application architecture.\n2. An instance is experiencing high CPU. How do you diagnose and resolve?\n3. How would you implement disaster recovery?\n4. Design a cost-effective data processing pipeline.\n\n### Container Orchestration (Kubernetes, Docker)\n\n**Fundamental**:\n1. Explain containers vs. virtual machines.\n2. What are the key components (pods, services, deployments)?\n3. How does container networking work?\n4. What is container orchestration and why is it needed?\n\n**Scenario**:\n1. A pod keeps crashing. Walk through your debugging process.\n2. Design a deployment strategy for zero-downtime updates.\n3. How would you scale an application to handle 10x traffic?\n4. Implement monitoring and logging for containers in production.\n\n### Machine Learning Frameworks (TensorFlow, PyTorch)\n\n**Fundamental**:\n1. Explain the difference between TensorFlow and PyTorch.\n2. What is automatic differentiation?\n3. How does backpropagation work?\n4. What are common regularization techniques?\n\n**Scenario**:\n1. Your model is overfitting. What approaches would you try?\n2. How would you deploy an ML model to production?\n3. Optimize a model for faster inference.\n4. Debug a model that's not converging during training.\n\n### Programming Languages (Python, Java, JavaScript)\n\n**Fundamental**:\n1. Explain key language features and paradigms.\n2. What are common data structures and when to use them?\n3. How does memory management work?\n4. Describe the concurrency model.\n\n**Scenario**:\n1. Code a solution to [specific problem] (live coding).\n2. Review this code and identify improvements.\n3. How would you optimize this function for performance?\n4. Debug this code that's producing incorrect output.\n\n## Skill-Specific Question Banks\n\n### Kubernetes\n\n**Fundamental** (Pick 4):\n1. Explain Kubernetes architecture (control plane, nodes, pods).\n2. What is the difference between Deployment and StatefulSet?\n3. How does service discovery work in Kubernetes?\n4. What are ConfigMaps and Secrets?\n5. Describe the pod lifecycle.\n6. How does the Kubernetes scheduler work?\n7. What is a namespace and why is it important?\n8. Explain horizontal pod autoscaling.\n\n**Scenario** (Pick 3):\n1. A pod keeps crashing. How do you troubleshoot?\n2. Design a deployment for a stateful application (database).\n3. Implement blue-green deployment strategy.\n4. A service has high latency. Investigate and resolve.\n5. Set up monitoring for a Kubernetes cluster.\n6. Implement RBAC for multi-tenant cluster.\n\n### AWS\n\n**Fundamental** (Pick 4):\n1. Explain EC2, S3, RDS, Lambda and their use cases.\n2. What is the difference between S3 and EBS?\n3. How does IAM enable secure access control?\n4. What are VPCs and why are they important?\n5. Explain Auto Scaling groups.\n6. What is CloudFormation?\n7. Describe AWS security best practices.\n8. How does AWS Lambda pricing work?\n\n**Scenario** (Pick 3):\n1. Design a scalable, highly available web application.\n2. Migrate a legacy application to AWS.\n3. Implement disaster recovery with RTO < 1 hour.\n4. Optimize AWS costs for a production workload.\n5. Secure an S3 bucket with sensitive data.\n6. Design a data pipeline processing TBs of data daily.\n\n### Python\n\n**Fundamental** (Pick 4):\n1. Explain Python's memory management and garbage collection.\n2. What is the Global Interpreter Lock (GIL)?\n3. Describe decorators and their use cases.\n4. What are generators and why use them?\n5. Explain list comprehensions vs. generator expressions.\n6. How does Python handle multiple inheritance?\n7. What are context managers (with statement)?\n8. Describe asyncio and when to use it.\n\n**Scenario** (Pick 3):\n1. Optimize this code for performance (given slow code).\n2. Debug a memory leak in a Python application.\n3. Implement a multi-threaded/async solution for [problem].\n4. Design a Python package with proper structure.\n5. Handle exceptions and errors gracefully in production code.\n6. Write unit tests for this function with edge cases.\n\n## Answer Frameworks\n\n### Technical Explanation Pattern\n\n```\n1. **Definition**: Start with a clear, concise definition\n2. **Purpose**: Explain why it exists / what problem it solves\n3. **Key Components**: Break down into main parts\n4. **Example**: Provide a concrete use case\n5. **Trade-offs**: Discuss advantages and limitations\n```\n\n**Example**:\n> Q: \"What is Docker?\"\n>\n> A: \"Docker is a containerization platform that packages applications\n> and their dependencies into isolated containers. It solves the\n> 'works on my machine' problem by ensuring consistency across\n> environments. Key components include the Docker Engine (runtime),\n> Images (templates), and Containers (running instances). For example,\n> you can package a Python app with all its libraries into a Docker\n> image and run it identically on dev, staging, and production.\n> The main advantage is portability, but the trade-off is added\n> complexity in orchestration at scale.\"\n\n### System Design Pattern\n\n```\n1. **Clarify Requirements**:\n   - Functional requirements\n   - Non-functional requirements (scale, latency, availability)\n\n2. **High-Level Architecture**:\n   - Draw/describe main components\n   - Data flow\n\n3. **Deep Dive**:\n   - Database design\n   - API design\n   - Key algorithms\n\n4. **Scalability**:\n   - How to handle growth\n   - Bottlenecks and solutions\n\n5. **Trade-offs**:\n   - CAP theorem considerations\n   - Consistency vs. availability\n\n6. **Monitoring & Ops**:\n   - Metrics to track\n   - Failure scenarios\n```\n\n### Debugging Pattern\n\n```\n1. **Reproduce**: Ensure problem is reproducible\n2. **Isolate**: Narrow down the source\n3. **Hypothesize**: Form theories about the cause\n4. **Test**: Verify hypothesis with experiments\n5. **Fix**: Implement solution\n6. **Verify**: Confirm fix works\n7. **Prevent**: Add tests/monitoring to prevent recurrence\n```\n\n## Preparation Strategies\n\n### For Missing Skills (High Priority)\n\n**Week 1-2: Foundation**\n- Take online course or tutorial\n- Read official documentation\n- Complete basic exercises\n\n**Week 3-4: Practice**\n- Build small project using technology\n- Answer fundamental questions out loud\n- Study common interview questions\n\n**Interview Prep**:\n- Be honest: \"I haven't used X in production, but I've studied it and built [project]\"\n- Emphasize transferable skills: \"I have experience with Y which is similar\"\n- Show learning ability: \"I learned Z in 2 weeks for a recent project\"\n\n### For Weak Skills (Medium Priority)\n\n**Depth Study**:\n- Read advanced tutorials and best practices\n- Study production use cases\n- Review design patterns\n\n**Practice**:\n- Explain concepts to others\n- Write blog posts or documentation\n- Contribute to open source\n\n**Interview Prep**:\n- Prepare specific examples from your experience\n- Be ready for deep technical questions\n- Know limitations and trade-offs\n\n### For Strong Skills (Confidence Building)\n\n**Advanced Topics**:\n- Performance optimization\n- Architecture and design patterns\n- Leadership and mentorship\n\n**Interview Prep**:\n- Prepare multiple detailed examples\n- Be ready for system design questions\n- Practice explaining to non-technical audience\n\n## Common Interview Mistakes\n\n### Mistake 1: Overconfidence\n\n```\n Bad: \"I know everything about Kubernetes\"\n Good: \"I have 3 years of production Kubernetes experience managing\n20+ microservices. I'm always learning about new features like\nOperators and service mesh integration.\"\n```\n\n### Mistake 2: Saying \"I Don't Know\" and Stopping\n\n```\n Bad: \"I don't know.\"\n Good: \"I haven't worked with that specific feature, but based on\nmy understanding of [related concept], I would approach it by [reasoning].\"\n```\n\n### Mistake 3: Not Asking Clarifying Questions\n\n```\n Bad: Immediately jumping to solution\n Good: \"To design this system, I need to clarify: What's the\nexpected scale? What are the latency requirements? What's the\navailability target?\"\n```\n\n### Mistake 4: Overly Theoretical Answers\n\n```\n Bad: Long explanation with no real-world context\n Good: \"In my previous role, we implemented [technology] for\n[specific use case]. Here's what we learned...\"\n```\n\n## Interview Day Tips\n\n### Before\n\n- [ ] Research the company and role thoroughly\n- [ ] Review job description and your resume\n- [ ] Practice answering questions out loud\n- [ ] Prepare questions to ask interviewer\n- [ ] Get good sleep and eat well\n\n### During\n\n- [ ] Listen carefully to the full question\n- [ ] Ask clarifying questions\n- [ ] Think before answering (pause is okay)\n- [ ] Use STAR method for behavioral questions\n- [ ] Provide specific examples with metrics\n- [ ] Admit when you don't know something\n- [ ] Show enthusiasm for learning\n\n### After\n\n- [ ] Send thank-you email within 24 hours\n- [ ] Reflect on what went well and what to improve\n- [ ] Make notes about questions asked\n- [ ] Follow up if promised information\n\n## Sample Question-Answer Pairs\n\n### Q: \"You have Docker experience but the job requires Kubernetes. How quickly can you learn it?\"\n\n**Good Answer**:\n> \"I have 2 years of production Docker experience, which gives me a\n> strong foundation since Kubernetes orchestrates Docker containers.\n> I've already started learning Kubernetes - I've completed a Udemy\n> course and deployed a small personal project to a local Kubernetes\n> cluster using Minikube. The concepts of containers, networking, and\n> resource management transfer directly. Based on my experience\n> learning Docker in just 3 weeks for a critical project, I'm confident\n> I can become productive with Kubernetes within a month of hands-on use.\n> I'm actually excited about this opportunity to deepen my container\n> orchestration skills.\"\n\n### Q: \"Walk me through debugging a production issue.\"\n\n**Good Answer (STAR)**:\n> \"At my previous company, we had a production API that started returning\n> 500 errors for 10% of requests (Situation). I was on-call and responsible\n> for diagnosing and fixing the issue within our 15-minute SLA (Task).\n>\n> I started by checking our monitoring dashboard and noticed high database\n> connection pool exhaustion. I then reviewed recent deployments and found\n> we'd increased a query timeout from 5s to 30s, causing connections to\n> be held longer. I quickly rolled back that change while the team worked\n> on a proper fix: adding connection pooling limits and query optimization\n> (Action).\n>\n> The rollback resolved the issue in 8 minutes, well within SLA. We then\n> implemented the optimized query with proper connection management,\n> reducing average response time by 40% and eliminating connection pool\n> issues. I also added monitoring alerts for connection pool utilization\n> to catch this earlier in the future (Result).\"\n\n## Conclusion\n\nEffective interview preparation combines understanding technical concepts,\npracticing articulate explanations, preparing specific examples from your\nexperience, and showing genuine enthusiasm for learning. Focus on honest\ncommunication, structured thinking, and demonstrating your problem-solving\napproach.\n",
        "skills/cover_letter_generator/.claude/skills/linkedin_job_resume_optimizer/references/linkedin-automation.md": "# LinkedIn Automation Guide\n\nGuide for automating LinkedIn job searches using Playwright MCP.\n\n## LinkedIn Job Search URL Structure\n\n### Basic Search URL\n\n```\nhttps://www.linkedin.com/jobs/search/\n```\n\n### Query Parameters\n\n| Parameter | Purpose | Example |\n|-----------|---------|---------|\n| `keywords` | Search keywords | `?keywords=AI%20Engineer` |\n| `location` | Job location | `&location=Remote` |\n| `f_WT` | Remote filter | `&f_WT=2` (Remote only) |\n| `f_TPR` | Time posted | `&f_TPR=r86400` (Past 24 hours) |\n| `f_E` | Experience level | `&f_E=3` (Mid-Senior level) |\n| `f_JT` | Job type | `&f_JT=F` (Full-time) |\n| `start` | Pagination offset | `&start=25` (Page 2) |\n\n### Complete Example URL\n\n```\nhttps://www.linkedin.com/jobs/search/?keywords=Machine%20Learning%20Engineer&location=Remote&f_WT=2&f_TPR=r604800&f_E=3\n```\n\nThis searches for \"Machine Learning Engineer\", Remote, Posted in last 7 days, Mid-Senior level.\n\n## Navigation Patterns\n\n### Step-by-Step Workflow\n\n1. **Navigate to Search URL**\n   ```python\n   mcp_client.call('browser_navigate', {\n       'url': 'https://www.linkedin.com/jobs/search/?keywords=AI%20Engineer&location=Remote'\n   })\n   ```\n\n2. **Wait for Page Load**\n   ```python\n   mcp_client.call('browser_wait_for', {'time': 3000})  # 3 seconds\n   ```\n\n3. **Get Page Snapshot**\n   ```python\n   snapshot = mcp_client.call('browser_snapshot', {})\n   ```\n\n4. **Parse Job Cards**\n   - Look for job listing elements in snapshot\n   - Extract refs for clicking\n\n5. **Click Job Card**\n   ```python\n   mcp_client.call('browser_click', {\n       'element': 'Job title text',\n       'ref': 'e42'  # From snapshot\n   })\n   ```\n\n6. **Wait for Job Details**\n   ```python\n   mcp_client.call('browser_wait_for', {'time': 2000})\n   ```\n\n7. **Get Job Details Snapshot**\n   ```python\n   job_snapshot = mcp_client.call('browser_snapshot', {})\n   ```\n\n## Element Selectors and Patterns\n\n### Job List Page\n\n**Job Cards**: Look for patterns like:\n- \"Senior AI Engineer\" [ref]\n- \"TechCorp \" [company]\n- \"Remote\" [location]\n- \"Posted X days ago\"\n\n**Navigation Elements**:\n- \"Next\" button for pagination\n- Filter dropdowns (Date posted, Experience level, etc.)\n\n### Job Detail Page\n\n**Key Sections**:\n- Job title (usually in large font at top)\n- Company name (after \"at\" or \"\")\n- \"About the job\" section (main description)\n- \"Show more\" button (expand full description)\n- \"Apply\" button\n\n**Parsing Strategy**:\n```python\ndef extract_job_details(snapshot_text):\n    # Title: First line or large text element\n    title = extract_first_line(snapshot_text)\n\n    # Company: Look for \"at CompanyName\" or \" CompanyName\"\n    company_match = re.search(r'(?:at|)\\s+(.+?)(?:\\n|)', snapshot_text)\n\n    # Description: After \"About the job\" or \"Job Description\"\n    desc_start = snapshot_text.find('About the job')\n    description = snapshot_text[desc_start:desc_start+3000]\n\n    return {'title': title, 'company': company, 'description': description}\n```\n\n## Anti-Bot Detection Strategies\n\n### 1. Random Delays\n\n```python\nimport random\nimport time\n\n# Between actions\ntime.sleep(random.uniform(1, 3))\n\n# After page loads\ntime.sleep(random.uniform(2, 5))\n```\n\n### 2. Human-Like Patterns\n\n- Don't scrape too fast (max 10-15 jobs per session)\n- Vary wait times between requests\n- Occasionally scroll or hover before clicking\n- Don't follow predictable patterns\n\n### 3. Browser Settings\n\n```bash\n# Use shared browser context\nnpx @playwright/mcp@latest --port 8808 --shared-browser-context\n\n# Disable headless mode if needed (more human-like)\n# (May require modifying start-server.sh)\n```\n\n### 4. User Agent Rotation\n\nIf detection occurs, consider rotating user agents:\n```python\nuser_agents = [\n    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36...',\n    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36...',\n    # Add more variations\n]\n```\n\n### 5. Session Management\n\n- Login manually before automation (if needed)\n- Use cookies from authenticated session\n- Clear cookies if rate limited\n\n## Rate Limiting Handling\n\n### Detection Signs\n\n- HTTP 429 (Too Many Requests) status\n- \"You've been rate limited\" message\n- Blank snapshots or error pages\n- Login wall appearing\n\n### Response Strategy\n\n```python\ndef search_with_retry(keywords, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            jobs = search_linkedin_jobs(keywords)\n            return jobs\n        except RateLimitError:\n            wait_time = (2 ** attempt) * 5  # 5s, 10s, 20s\n            print(f\"Rate limited. Waiting {wait_time}s...\")\n            time.sleep(wait_time)\n    return []\n```\n\n### Prevention\n\n- Limit scraping to 2-5 jobs per session\n- Add delays between actions (2-5 seconds)\n- Don't run multiple sessions simultaneously\n- Use `--shared-browser-context` to maintain session state\n\n## Snapshot Parsing Techniques\n\n### Extracting Job Listings\n\n```python\ndef extract_job_cards_from_snapshot(snapshot_text):\n    job_cards = []\n    lines = snapshot_text.split('\\n')\n\n    for i, line in enumerate(lines):\n        # Look for job title patterns\n        if any(keyword in line.lower() for keyword in ['engineer', 'developer', 'scientist']):\n            # Extract ref number\n            ref_match = re.search(r'\\[(\\d+)\\]', line)\n            if ref_match:\n                ref = ref_match.group(1)\n                title = re.sub(r'\\[\\d+\\]', '', line).strip()\n                job_cards.append({'ref': ref, 'title': title})\n\n    return job_cards\n```\n\n### Extracting Job Description\n\n```python\ndef extract_description(snapshot_text):\n    # Strategy 1: Look for \"About the job\" section\n    desc_markers = ['About the job', 'Job Description', 'Description']\n\n    for marker in desc_markers:\n        if marker in snapshot_text:\n            start = snapshot_text.find(marker) + len(marker)\n            # Take next 3000 chars or until next major section\n            end_markers = ['Seniority level', 'Employment type', 'How you match']\n            end = start + 3000\n            for end_marker in end_markers:\n                pos = snapshot_text.find(end_marker, start)\n                if pos != -1 and pos < end:\n                    end = pos\n            return snapshot_text[start:end].strip()\n\n    # Strategy 2: If no marker found, use heuristics\n    # Description is usually the longest text block\n    return snapshot_text[:3000]\n```\n\n### Extracting Required Skills\n\n```python\ndef extract_required_skills(description):\n    # Look for \"Required\" or \"Qualifications\" section\n    required_section = \"\"\n    patterns = [\n        r'(?:Required|Must have|Requirements)[:\\s]+(.+?)(?:Preferred|Nice to have|Plus|$)',\n        r'(?:Qualifications)[:\\s]+(.+?)(?:Preferred|Nice to have|Plus|$)'\n    ]\n\n    for pattern in patterns:\n        match = re.search(pattern, description, re.DOTALL | re.IGNORECASE)\n        if match:\n            required_section = match.group(1)\n            break\n\n    # Extract technical keywords from section\n    tech_keywords = ['Python', 'Java', 'AWS', 'Docker', ...]  # Full list\n    found_skills = []\n\n    for keyword in tech_keywords:\n        if keyword.lower() in required_section.lower():\n            found_skills.append(keyword)\n\n    return found_skills\n```\n\n## Error Handling\n\n### Login Wall\n\n```python\ndef handle_login_wall(snapshot_text):\n    if 'sign in' in snapshot_text.lower() or 'join now' in snapshot_text.lower():\n        print(\"LinkedIn requires login\")\n        print(\"Options:\")\n        print(\"  1. Login manually in browser\")\n        print(\"  2. Provide job URLs manually\")\n        print(\"  3. Use alternative job boards\")\n        return True\n    return False\n```\n\n### Stale Elements\n\n```python\ndef safe_click(element_desc, ref, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            mcp_client.call('browser_click', {'element': element_desc, 'ref': ref})\n            return True\n        except StaleElementError:\n            print(f\"Stale element, retry {attempt+1}/{max_retries}\")\n            time.sleep(1)\n            # Re-get snapshot\n            snapshot = mcp_client.call('browser_snapshot', {})\n            # Re-find element\n    return False\n```\n\n### Connection Failures\n\n```python\ndef check_playwright_server():\n    try:\n        response = mcp_client.call('browser_snapshot', {})\n        return response is not None\n    except Exception:\n        return False\n\nif not check_playwright_server():\n    print(\"Playwright server not responding\")\n    print(\"Restarting server...\")\n    restart_playwright_server()\n```\n\n## Alternative Strategies\n\n### If LinkedIn Automation Fails\n\n1. **Manual Job URL Input**\n   ```python\n   jobs = [\n       {'url': 'https://linkedin.com/jobs/view/12345', 'title': '...'},\n       {'url': 'https://linkedin.com/jobs/view/67890', 'title': '...'}\n   ]\n   ```\n\n2. **Use LinkedIn API** (If you have access)\n   - OAuth authentication required\n   - Rate limits apply\n   - May require partnership\n\n3. **Alternative Job Boards**\n   - Indeed: Often more automation-friendly\n   - Glassdoor: Similar structure to LinkedIn\n   - Company career pages directly\n\n4. **RSS Feeds**\n   - Some companies offer RSS feeds for jobs\n   - Can be parsed without web automation\n\n## Debugging Tips\n\n### Save Snapshots for Analysis\n\n```python\n# Save snapshot to file\nwith open('/tmp/linkedin_snapshot.txt', 'w') as f:\n    f.write(snapshot_text)\n\n# Review manually to understand structure\ncat /tmp/linkedin_snapshot.txt | less\n```\n\n### Take Screenshots\n\n```python\nscreenshot = mcp_client.call('browser_take_screenshot', {\n    'type': 'png',\n    'fullPage': True\n})\n# Screenshot saved to file, check path in output\n```\n\n### Log All Actions\n\n```python\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nlogger.info(f\"Navigating to {url}\")\nlogger.info(f\"Clicking element: {element_desc}\")\nlogger.info(f\"Extracted {len(jobs)} jobs\")\n```\n\n## Best Practices\n\n1. **Start Small**: Test with 1-2 jobs before scaling\n2. **Monitor Output**: Check snapshot files and logs\n3. **Respect Rate Limits**: Don't scrape aggressively\n4. **Handle Failures Gracefully**: Provide fallback options\n5. **Keep Updated**: LinkedIn structure changes; update selectors\n6. **User Privacy**: Don't store personal data unnecessarily\n7. **Terms of Service**: Review LinkedIn's ToS and robots.txt\n\n## Example Complete Workflow\n\n```python\n# 1. Start server\nstart_playwright_server()\n\n# 2. Navigate to LinkedIn jobs\nurl = \"https://www.linkedin.com/jobs/search/?keywords=AI%20Engineer&location=Remote\"\nmcp_client.call('browser_navigate', {'url': url})\n\n# 3. Wait and get snapshot\ntime.sleep(random.uniform(3, 5))\nsnapshot = mcp_client.call('browser_snapshot', {})\n\n# 4. Extract job cards\njob_cards = extract_job_cards(snapshot['content'])\n\n# 5. For each job\njobs = []\nfor card in job_cards[:2]:  # Limit to 2\n    # Click job\n    mcp_client.call('browser_click', {'element': card['title'], 'ref': card['ref']})\n\n    # Wait and get details\n    time.sleep(random.uniform(2, 4))\n    job_snapshot = mcp_client.call('browser_snapshot', {})\n\n    # Extract and store\n    job_data = extract_job_details(job_snapshot['content'])\n    jobs.append(job_data)\n\n# 6. Stop server\nstop_playwright_server()\n\n# 7. Return jobs\nreturn jobs\n```\n\n## Conclusion\n\nLinkedIn automation requires careful attention to rate limits, bot detection, and changing page structures. Always have fallback strategies (manual URLs, alternative sources) and respect the platform's terms of service.\n",
        "skills/cover_letter_generator/.claude/skills/pdf/SKILL.md": "---\nname: pdf\ndescription: Comprehensive PDF manipulation toolkit for extracting text and tables, creating new PDFs, merging/splitting documents, and handling forms. When Claude needs to fill in a PDF form or programmatically process, generate, or analyze PDF documents at scale.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# PDF Processing Guide\n\n## Overview\n\nThis guide covers essential PDF processing operations using Python libraries and command-line tools. For advanced features, JavaScript libraries, and detailed examples, see reference.md. If you need to fill out a PDF form, read forms.md and follow its instructions.\n\n## Quick Start\n\n```python\nfrom pypdf import PdfReader, PdfWriter\n\n# Read a PDF\nreader = PdfReader(\"document.pdf\")\nprint(f\"Pages: {len(reader.pages)}\")\n\n# Extract text\ntext = \"\"\nfor page in reader.pages:\n    text += page.extract_text()\n```\n\n## Python Libraries\n\n### pypdf - Basic Operations\n\n#### Merge PDFs\n```python\nfrom pypdf import PdfWriter, PdfReader\n\nwriter = PdfWriter()\nfor pdf_file in [\"doc1.pdf\", \"doc2.pdf\", \"doc3.pdf\"]:\n    reader = PdfReader(pdf_file)\n    for page in reader.pages:\n        writer.add_page(page)\n\nwith open(\"merged.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n#### Split PDF\n```python\nreader = PdfReader(\"input.pdf\")\nfor i, page in enumerate(reader.pages):\n    writer = PdfWriter()\n    writer.add_page(page)\n    with open(f\"page_{i+1}.pdf\", \"wb\") as output:\n        writer.write(output)\n```\n\n#### Extract Metadata\n```python\nreader = PdfReader(\"document.pdf\")\nmeta = reader.metadata\nprint(f\"Title: {meta.title}\")\nprint(f\"Author: {meta.author}\")\nprint(f\"Subject: {meta.subject}\")\nprint(f\"Creator: {meta.creator}\")\n```\n\n#### Rotate Pages\n```python\nreader = PdfReader(\"input.pdf\")\nwriter = PdfWriter()\n\npage = reader.pages[0]\npage.rotate(90)  # Rotate 90 degrees clockwise\nwriter.add_page(page)\n\nwith open(\"rotated.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n### pdfplumber - Text and Table Extraction\n\n#### Extract Text with Layout\n```python\nimport pdfplumber\n\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    for page in pdf.pages:\n        text = page.extract_text()\n        print(text)\n```\n\n#### Extract Tables\n```python\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    for i, page in enumerate(pdf.pages):\n        tables = page.extract_tables()\n        for j, table in enumerate(tables):\n            print(f\"Table {j+1} on page {i+1}:\")\n            for row in table:\n                print(row)\n```\n\n#### Advanced Table Extraction\n```python\nimport pandas as pd\n\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    all_tables = []\n    for page in pdf.pages:\n        tables = page.extract_tables()\n        for table in tables:\n            if table:  # Check if table is not empty\n                df = pd.DataFrame(table[1:], columns=table[0])\n                all_tables.append(df)\n\n# Combine all tables\nif all_tables:\n    combined_df = pd.concat(all_tables, ignore_index=True)\n    combined_df.to_excel(\"extracted_tables.xlsx\", index=False)\n```\n\n### reportlab - Create PDFs\n\n#### Basic PDF Creation\n```python\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.pdfgen import canvas\n\nc = canvas.Canvas(\"hello.pdf\", pagesize=letter)\nwidth, height = letter\n\n# Add text\nc.drawString(100, height - 100, \"Hello World!\")\nc.drawString(100, height - 120, \"This is a PDF created with reportlab\")\n\n# Add a line\nc.line(100, height - 140, 400, height - 140)\n\n# Save\nc.save()\n```\n\n#### Create PDF with Multiple Pages\n```python\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak\nfrom reportlab.lib.styles import getSampleStyleSheet\n\ndoc = SimpleDocTemplate(\"report.pdf\", pagesize=letter)\nstyles = getSampleStyleSheet()\nstory = []\n\n# Add content\ntitle = Paragraph(\"Report Title\", styles['Title'])\nstory.append(title)\nstory.append(Spacer(1, 12))\n\nbody = Paragraph(\"This is the body of the report. \" * 20, styles['Normal'])\nstory.append(body)\nstory.append(PageBreak())\n\n# Page 2\nstory.append(Paragraph(\"Page 2\", styles['Heading1']))\nstory.append(Paragraph(\"Content for page 2\", styles['Normal']))\n\n# Build PDF\ndoc.build(story)\n```\n\n## Command-Line Tools\n\n### pdftotext (poppler-utils)\n```bash\n# Extract text\npdftotext input.pdf output.txt\n\n# Extract text preserving layout\npdftotext -layout input.pdf output.txt\n\n# Extract specific pages\npdftotext -f 1 -l 5 input.pdf output.txt  # Pages 1-5\n```\n\n### qpdf\n```bash\n# Merge PDFs\nqpdf --empty --pages file1.pdf file2.pdf -- merged.pdf\n\n# Split pages\nqpdf input.pdf --pages . 1-5 -- pages1-5.pdf\nqpdf input.pdf --pages . 6-10 -- pages6-10.pdf\n\n# Rotate pages\nqpdf input.pdf output.pdf --rotate=+90:1  # Rotate page 1 by 90 degrees\n\n# Remove password\nqpdf --password=mypassword --decrypt encrypted.pdf decrypted.pdf\n```\n\n### pdftk (if available)\n```bash\n# Merge\npdftk file1.pdf file2.pdf cat output merged.pdf\n\n# Split\npdftk input.pdf burst\n\n# Rotate\npdftk input.pdf rotate 1east output rotated.pdf\n```\n\n## Common Tasks\n\n### Extract Text from Scanned PDFs\n```python\n# Requires: pip install pytesseract pdf2image\nimport pytesseract\nfrom pdf2image import convert_from_path\n\n# Convert PDF to images\nimages = convert_from_path('scanned.pdf')\n\n# OCR each page\ntext = \"\"\nfor i, image in enumerate(images):\n    text += f\"Page {i+1}:\\n\"\n    text += pytesseract.image_to_string(image)\n    text += \"\\n\\n\"\n\nprint(text)\n```\n\n### Add Watermark\n```python\nfrom pypdf import PdfReader, PdfWriter\n\n# Create watermark (or load existing)\nwatermark = PdfReader(\"watermark.pdf\").pages[0]\n\n# Apply to all pages\nreader = PdfReader(\"document.pdf\")\nwriter = PdfWriter()\n\nfor page in reader.pages:\n    page.merge_page(watermark)\n    writer.add_page(page)\n\nwith open(\"watermarked.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n### Extract Images\n```bash\n# Using pdfimages (poppler-utils)\npdfimages -j input.pdf output_prefix\n\n# This extracts all images as output_prefix-000.jpg, output_prefix-001.jpg, etc.\n```\n\n### Password Protection\n```python\nfrom pypdf import PdfReader, PdfWriter\n\nreader = PdfReader(\"input.pdf\")\nwriter = PdfWriter()\n\nfor page in reader.pages:\n    writer.add_page(page)\n\n# Add password\nwriter.encrypt(\"userpassword\", \"ownerpassword\")\n\nwith open(\"encrypted.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n## Quick Reference\n\n| Task | Best Tool | Command/Code |\n|------|-----------|--------------|\n| Merge PDFs | pypdf | `writer.add_page(page)` |\n| Split PDFs | pypdf | One page per file |\n| Extract text | pdfplumber | `page.extract_text()` |\n| Extract tables | pdfplumber | `page.extract_tables()` |\n| Create PDFs | reportlab | Canvas or Platypus |\n| Command line merge | qpdf | `qpdf --empty --pages ...` |\n| OCR scanned PDFs | pytesseract | Convert to image first |\n| Fill PDF forms | pdf-lib or pypdf (see forms.md) | See forms.md |\n\n## Next Steps\n\n- For advanced pypdfium2 usage, see reference.md\n- For JavaScript libraries (pdf-lib), see reference.md\n- If you need to fill out a PDF form, follow the instructions in forms.md\n- For troubleshooting guides, see reference.md\n",
        "skills/cover_letter_generator/.claude/skills/pdf/forms.md": "**CRITICAL: You MUST complete these steps in order. Do not skip ahead to writing code.**\n\nIf you need to fill out a PDF form, first check to see if the PDF has fillable form fields. Run this script from this file's directory:\n `python scripts/check_fillable_fields <file.pdf>`, and depending on the result go to either the \"Fillable fields\" or \"Non-fillable fields\" and follow those instructions.\n\n# Fillable fields\nIf the PDF has fillable form fields:\n- Run this script from this file's directory: `python scripts/extract_form_field_info.py <input.pdf> <field_info.json>`. It will create a JSON file with a list of fields in this format:\n```\n[\n  {\n    \"field_id\": (unique ID for the field),\n    \"page\": (page number, 1-based),\n    \"rect\": ([left, bottom, right, top] bounding box in PDF coordinates, y=0 is the bottom of the page),\n    \"type\": (\"text\", \"checkbox\", \"radio_group\", or \"choice\"),\n  },\n  // Checkboxes have \"checked_value\" and \"unchecked_value\" properties:\n  {\n    \"field_id\": (unique ID for the field),\n    \"page\": (page number, 1-based),\n    \"type\": \"checkbox\",\n    \"checked_value\": (Set the field to this value to check the checkbox),\n    \"unchecked_value\": (Set the field to this value to uncheck the checkbox),\n  },\n  // Radio groups have a \"radio_options\" list with the possible choices.\n  {\n    \"field_id\": (unique ID for the field),\n    \"page\": (page number, 1-based),\n    \"type\": \"radio_group\",\n    \"radio_options\": [\n      {\n        \"value\": (set the field to this value to select this radio option),\n        \"rect\": (bounding box for the radio button for this option)\n      },\n      // Other radio options\n    ]\n  },\n  // Multiple choice fields have a \"choice_options\" list with the possible choices:\n  {\n    \"field_id\": (unique ID for the field),\n    \"page\": (page number, 1-based),\n    \"type\": \"choice\",\n    \"choice_options\": [\n      {\n        \"value\": (set the field to this value to select this option),\n        \"text\": (display text of the option)\n      },\n      // Other choice options\n    ],\n  }\n]\n```\n- Convert the PDF to PNGs (one image for each page) with this script (run from this file's directory):\n`python scripts/convert_pdf_to_images.py <file.pdf> <output_directory>`\nThen analyze the images to determine the purpose of each form field (make sure to convert the bounding box PDF coordinates to image coordinates).\n- Create a `field_values.json` file in this format with the values to be entered for each field:\n```\n[\n  {\n    \"field_id\": \"last_name\", // Must match the field_id from `extract_form_field_info.py`\n    \"description\": \"The user's last name\",\n    \"page\": 1, // Must match the \"page\" value in field_info.json\n    \"value\": \"Simpson\"\n  },\n  {\n    \"field_id\": \"Checkbox12\",\n    \"description\": \"Checkbox to be checked if the user is 18 or over\",\n    \"page\": 1,\n    \"value\": \"/On\" // If this is a checkbox, use its \"checked_value\" value to check it. If it's a radio button group, use one of the \"value\" values in \"radio_options\".\n  },\n  // more fields\n]\n```\n- Run the `fill_fillable_fields.py` script from this file's directory to create a filled-in PDF:\n`python scripts/fill_fillable_fields.py <input pdf> <field_values.json> <output pdf>`\nThis script will verify that the field IDs and values you provide are valid; if it prints error messages, correct the appropriate fields and try again.\n\n# Non-fillable fields\nIf the PDF doesn't have fillable form fields, you'll need to visually determine where the data should be added and create text annotations. Follow the below steps *exactly*. You MUST perform all of these steps to ensure that the the form is accurately completed. Details for each step are below.\n- Convert the PDF to PNG images and determine field bounding boxes.\n- Create a JSON file with field information and validation images showing the bounding boxes.\n- Validate the the bounding boxes.\n- Use the bounding boxes to fill in the form.\n\n## Step 1: Visual Analysis (REQUIRED)\n- Convert the PDF to PNG images. Run this script from this file's directory:\n`python scripts/convert_pdf_to_images.py <file.pdf> <output_directory>`\nThe script will create a PNG image for each page in the PDF.\n- Carefully examine each PNG image and identify all form fields and areas where the user should enter data. For each form field where the user should enter text, determine bounding boxes for both the form field label, and the area where the user should enter text. The label and entry bounding boxes MUST NOT INTERSECT; the text entry box should only include the area where data should be entered. Usually this area will be immediately to the side, above, or below its label. Entry bounding boxes must be tall and wide enough to contain their text.\n\nThese are some examples of form structures that you might see:\n\n*Label inside box*\n```\n\n Name:                  \n\n```\nThe input area should be to the right of the \"Name\" label and extend to the edge of the box.\n\n*Label before line*\n```\nEmail: _______________________\n```\nThe input area should be above the line and include its entire width.\n\n*Label under line*\n```\n_________________________\nName\n```\nThe input area should be above the line and include the entire width of the line. This is common for signature and date fields.\n\n*Label above line*\n```\nPlease enter any special requests:\n________________________________________________\n```\nThe input area should extend from the bottom of the label to the line, and should include the entire width of the line.\n\n*Checkboxes*\n```\nAre you a US citizen? Yes   No \n```\nFor checkboxes:\n- Look for small square boxes () - these are the actual checkboxes to target. They may be to the left or right of their labels.\n- Distinguish between label text (\"Yes\", \"No\") and the clickable checkbox squares.\n- The entry bounding box should cover ONLY the small square, not the text label.\n\n### Step 2: Create fields.json and validation images (REQUIRED)\n- Create a file named `fields.json` with information for the form fields and bounding boxes in this format:\n```\n{\n  \"pages\": [\n    {\n      \"page_number\": 1,\n      \"image_width\": (first page image width in pixels),\n      \"image_height\": (first page image height in pixels),\n    },\n    {\n      \"page_number\": 2,\n      \"image_width\": (second page image width in pixels),\n      \"image_height\": (second page image height in pixels),\n    }\n    // additional pages\n  ],\n  \"form_fields\": [\n    // Example for a text field.\n    {\n      \"page_number\": 1,\n      \"description\": \"The user's last name should be entered here\",\n      // Bounding boxes are [left, top, right, bottom]. The bounding boxes for the label and text entry should not overlap.\n      \"field_label\": \"Last name\",\n      \"label_bounding_box\": [30, 125, 95, 142],\n      \"entry_bounding_box\": [100, 125, 280, 142],\n      \"entry_text\": {\n        \"text\": \"Johnson\", // This text will be added as an annotation at the entry_bounding_box location\n        \"font_size\": 14, // optional, defaults to 14\n        \"font_color\": \"000000\", // optional, RRGGBB format, defaults to 000000 (black)\n      }\n    },\n    // Example for a checkbox. TARGET THE SQUARE for the entry bounding box, NOT THE TEXT\n    {\n      \"page_number\": 2,\n      \"description\": \"Checkbox that should be checked if the user is over 18\",\n      \"entry_bounding_box\": [140, 525, 155, 540],  // Small box over checkbox square\n      \"field_label\": \"Yes\",\n      \"label_bounding_box\": [100, 525, 132, 540],  // Box containing \"Yes\" text\n      // Use \"X\" to check a checkbox.\n      \"entry_text\": {\n        \"text\": \"X\",\n      }\n    }\n    // additional form field entries\n  ]\n}\n```\n\nCreate validation images by running this script from this file's directory for each page:\n`python scripts/create_validation_image.py <page_number> <path_to_fields.json> <input_image_path> <output_image_path>\n\nThe validation images will have red rectangles where text should be entered, and blue rectangles covering label text.\n\n### Step 3: Validate Bounding Boxes (REQUIRED)\n#### Automated intersection check\n- Verify that none of bounding boxes intersect and that the entry bounding boxes are tall enough by checking the fields.json file with the `check_bounding_boxes.py` script (run from this file's directory):\n`python scripts/check_bounding_boxes.py <JSON file>`\n\nIf there are errors, reanalyze the relevant fields, adjust the bounding boxes, and iterate until there are no remaining errors. Remember: label (blue) bounding boxes should contain text labels, entry (red) boxes should not.\n\n#### Manual image inspection\n**CRITICAL: Do not proceed without visually inspecting validation images**\n- Red rectangles must ONLY cover input areas\n- Red rectangles MUST NOT contain any text\n- Blue rectangles should contain label text\n- For checkboxes:\n  - Red rectangle MUST be centered on the checkbox square\n  - Blue rectangle should cover the text label for the checkbox\n\n- If any rectangles look wrong, fix fields.json, regenerate the validation images, and verify again. Repeat this process until the bounding boxes are fully accurate.\n\n\n### Step 4: Add annotations to the PDF\nRun this script from this file's directory to create a filled-out PDF using the information in fields.json:\n`python scripts/fill_pdf_form_with_annotations.py <input_pdf_path> <path_to_fields.json> <output_pdf_path>\n",
        "skills/cover_letter_generator/.claude/skills/pdf/reference.md": "# PDF Processing Advanced Reference\n\nThis document contains advanced PDF processing features, detailed examples, and additional libraries not covered in the main skill instructions.\n\n## pypdfium2 Library (Apache/BSD License)\n\n### Overview\npypdfium2 is a Python binding for PDFium (Chromium's PDF library). It's excellent for fast PDF rendering, image generation, and serves as a PyMuPDF replacement.\n\n### Render PDF to Images\n```python\nimport pypdfium2 as pdfium\nfrom PIL import Image\n\n# Load PDF\npdf = pdfium.PdfDocument(\"document.pdf\")\n\n# Render page to image\npage = pdf[0]  # First page\nbitmap = page.render(\n    scale=2.0,  # Higher resolution\n    rotation=0  # No rotation\n)\n\n# Convert to PIL Image\nimg = bitmap.to_pil()\nimg.save(\"page_1.png\", \"PNG\")\n\n# Process multiple pages\nfor i, page in enumerate(pdf):\n    bitmap = page.render(scale=1.5)\n    img = bitmap.to_pil()\n    img.save(f\"page_{i+1}.jpg\", \"JPEG\", quality=90)\n```\n\n### Extract Text with pypdfium2\n```python\nimport pypdfium2 as pdfium\n\npdf = pdfium.PdfDocument(\"document.pdf\")\nfor i, page in enumerate(pdf):\n    text = page.get_text()\n    print(f\"Page {i+1} text length: {len(text)} chars\")\n```\n\n## JavaScript Libraries\n\n### pdf-lib (MIT License)\n\npdf-lib is a powerful JavaScript library for creating and modifying PDF documents in any JavaScript environment.\n\n#### Load and Manipulate Existing PDF\n```javascript\nimport { PDFDocument } from 'pdf-lib';\nimport fs from 'fs';\n\nasync function manipulatePDF() {\n    // Load existing PDF\n    const existingPdfBytes = fs.readFileSync('input.pdf');\n    const pdfDoc = await PDFDocument.load(existingPdfBytes);\n\n    // Get page count\n    const pageCount = pdfDoc.getPageCount();\n    console.log(`Document has ${pageCount} pages`);\n\n    // Add new page\n    const newPage = pdfDoc.addPage([600, 400]);\n    newPage.drawText('Added by pdf-lib', {\n        x: 100,\n        y: 300,\n        size: 16\n    });\n\n    // Save modified PDF\n    const pdfBytes = await pdfDoc.save();\n    fs.writeFileSync('modified.pdf', pdfBytes);\n}\n```\n\n#### Create Complex PDFs from Scratch\n```javascript\nimport { PDFDocument, rgb, StandardFonts } from 'pdf-lib';\nimport fs from 'fs';\n\nasync function createPDF() {\n    const pdfDoc = await PDFDocument.create();\n\n    // Add fonts\n    const helveticaFont = await pdfDoc.embedFont(StandardFonts.Helvetica);\n    const helveticaBold = await pdfDoc.embedFont(StandardFonts.HelveticaBold);\n\n    // Add page\n    const page = pdfDoc.addPage([595, 842]); // A4 size\n    const { width, height } = page.getSize();\n\n    // Add text with styling\n    page.drawText('Invoice #12345', {\n        x: 50,\n        y: height - 50,\n        size: 18,\n        font: helveticaBold,\n        color: rgb(0.2, 0.2, 0.8)\n    });\n\n    // Add rectangle (header background)\n    page.drawRectangle({\n        x: 40,\n        y: height - 100,\n        width: width - 80,\n        height: 30,\n        color: rgb(0.9, 0.9, 0.9)\n    });\n\n    // Add table-like content\n    const items = [\n        ['Item', 'Qty', 'Price', 'Total'],\n        ['Widget', '2', '$50', '$100'],\n        ['Gadget', '1', '$75', '$75']\n    ];\n\n    let yPos = height - 150;\n    items.forEach(row => {\n        let xPos = 50;\n        row.forEach(cell => {\n            page.drawText(cell, {\n                x: xPos,\n                y: yPos,\n                size: 12,\n                font: helveticaFont\n            });\n            xPos += 120;\n        });\n        yPos -= 25;\n    });\n\n    const pdfBytes = await pdfDoc.save();\n    fs.writeFileSync('created.pdf', pdfBytes);\n}\n```\n\n#### Advanced Merge and Split Operations\n```javascript\nimport { PDFDocument } from 'pdf-lib';\nimport fs from 'fs';\n\nasync function mergePDFs() {\n    // Create new document\n    const mergedPdf = await PDFDocument.create();\n\n    // Load source PDFs\n    const pdf1Bytes = fs.readFileSync('doc1.pdf');\n    const pdf2Bytes = fs.readFileSync('doc2.pdf');\n\n    const pdf1 = await PDFDocument.load(pdf1Bytes);\n    const pdf2 = await PDFDocument.load(pdf2Bytes);\n\n    // Copy pages from first PDF\n    const pdf1Pages = await mergedPdf.copyPages(pdf1, pdf1.getPageIndices());\n    pdf1Pages.forEach(page => mergedPdf.addPage(page));\n\n    // Copy specific pages from second PDF (pages 0, 2, 4)\n    const pdf2Pages = await mergedPdf.copyPages(pdf2, [0, 2, 4]);\n    pdf2Pages.forEach(page => mergedPdf.addPage(page));\n\n    const mergedPdfBytes = await mergedPdf.save();\n    fs.writeFileSync('merged.pdf', mergedPdfBytes);\n}\n```\n\n### pdfjs-dist (Apache License)\n\nPDF.js is Mozilla's JavaScript library for rendering PDFs in the browser.\n\n#### Basic PDF Loading and Rendering\n```javascript\nimport * as pdfjsLib from 'pdfjs-dist';\n\n// Configure worker (important for performance)\npdfjsLib.GlobalWorkerOptions.workerSrc = './pdf.worker.js';\n\nasync function renderPDF() {\n    // Load PDF\n    const loadingTask = pdfjsLib.getDocument('document.pdf');\n    const pdf = await loadingTask.promise;\n\n    console.log(`Loaded PDF with ${pdf.numPages} pages`);\n\n    // Get first page\n    const page = await pdf.getPage(1);\n    const viewport = page.getViewport({ scale: 1.5 });\n\n    // Render to canvas\n    const canvas = document.createElement('canvas');\n    const context = canvas.getContext('2d');\n    canvas.height = viewport.height;\n    canvas.width = viewport.width;\n\n    const renderContext = {\n        canvasContext: context,\n        viewport: viewport\n    };\n\n    await page.render(renderContext).promise;\n    document.body.appendChild(canvas);\n}\n```\n\n#### Extract Text with Coordinates\n```javascript\nimport * as pdfjsLib from 'pdfjs-dist';\n\nasync function extractText() {\n    const loadingTask = pdfjsLib.getDocument('document.pdf');\n    const pdf = await loadingTask.promise;\n\n    let fullText = '';\n\n    // Extract text from all pages\n    for (let i = 1; i <= pdf.numPages; i++) {\n        const page = await pdf.getPage(i);\n        const textContent = await page.getTextContent();\n\n        const pageText = textContent.items\n            .map(item => item.str)\n            .join(' ');\n\n        fullText += `\\n--- Page ${i} ---\\n${pageText}`;\n\n        // Get text with coordinates for advanced processing\n        const textWithCoords = textContent.items.map(item => ({\n            text: item.str,\n            x: item.transform[4],\n            y: item.transform[5],\n            width: item.width,\n            height: item.height\n        }));\n    }\n\n    console.log(fullText);\n    return fullText;\n}\n```\n\n#### Extract Annotations and Forms\n```javascript\nimport * as pdfjsLib from 'pdfjs-dist';\n\nasync function extractAnnotations() {\n    const loadingTask = pdfjsLib.getDocument('annotated.pdf');\n    const pdf = await loadingTask.promise;\n\n    for (let i = 1; i <= pdf.numPages; i++) {\n        const page = await pdf.getPage(i);\n        const annotations = await page.getAnnotations();\n\n        annotations.forEach(annotation => {\n            console.log(`Annotation type: ${annotation.subtype}`);\n            console.log(`Content: ${annotation.contents}`);\n            console.log(`Coordinates: ${JSON.stringify(annotation.rect)}`);\n        });\n    }\n}\n```\n\n## Advanced Command-Line Operations\n\n### poppler-utils Advanced Features\n\n#### Extract Text with Bounding Box Coordinates\n```bash\n# Extract text with bounding box coordinates (essential for structured data)\npdftotext -bbox-layout document.pdf output.xml\n\n# The XML output contains precise coordinates for each text element\n```\n\n#### Advanced Image Conversion\n```bash\n# Convert to PNG images with specific resolution\npdftoppm -png -r 300 document.pdf output_prefix\n\n# Convert specific page range with high resolution\npdftoppm -png -r 600 -f 1 -l 3 document.pdf high_res_pages\n\n# Convert to JPEG with quality setting\npdftoppm -jpeg -jpegopt quality=85 -r 200 document.pdf jpeg_output\n```\n\n#### Extract Embedded Images\n```bash\n# Extract all embedded images with metadata\npdfimages -j -p document.pdf page_images\n\n# List image info without extracting\npdfimages -list document.pdf\n\n# Extract images in their original format\npdfimages -all document.pdf images/img\n```\n\n### qpdf Advanced Features\n\n#### Complex Page Manipulation\n```bash\n# Split PDF into groups of pages\nqpdf --split-pages=3 input.pdf output_group_%02d.pdf\n\n# Extract specific pages with complex ranges\nqpdf input.pdf --pages input.pdf 1,3-5,8,10-end -- extracted.pdf\n\n# Merge specific pages from multiple PDFs\nqpdf --empty --pages doc1.pdf 1-3 doc2.pdf 5-7 doc3.pdf 2,4 -- combined.pdf\n```\n\n#### PDF Optimization and Repair\n```bash\n# Optimize PDF for web (linearize for streaming)\nqpdf --linearize input.pdf optimized.pdf\n\n# Remove unused objects and compress\nqpdf --optimize-level=all input.pdf compressed.pdf\n\n# Attempt to repair corrupted PDF structure\nqpdf --check input.pdf\nqpdf --fix-qdf damaged.pdf repaired.pdf\n\n# Show detailed PDF structure for debugging\nqpdf --show-all-pages input.pdf > structure.txt\n```\n\n#### Advanced Encryption\n```bash\n# Add password protection with specific permissions\nqpdf --encrypt user_pass owner_pass 256 --print=none --modify=none -- input.pdf encrypted.pdf\n\n# Check encryption status\nqpdf --show-encryption encrypted.pdf\n\n# Remove password protection (requires password)\nqpdf --password=secret123 --decrypt encrypted.pdf decrypted.pdf\n```\n\n## Advanced Python Techniques\n\n### pdfplumber Advanced Features\n\n#### Extract Text with Precise Coordinates\n```python\nimport pdfplumber\n\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    page = pdf.pages[0]\n    \n    # Extract all text with coordinates\n    chars = page.chars\n    for char in chars[:10]:  # First 10 characters\n        print(f\"Char: '{char['text']}' at x:{char['x0']:.1f} y:{char['y0']:.1f}\")\n    \n    # Extract text by bounding box (left, top, right, bottom)\n    bbox_text = page.within_bbox((100, 100, 400, 200)).extract_text()\n```\n\n#### Advanced Table Extraction with Custom Settings\n```python\nimport pdfplumber\nimport pandas as pd\n\nwith pdfplumber.open(\"complex_table.pdf\") as pdf:\n    page = pdf.pages[0]\n    \n    # Extract tables with custom settings for complex layouts\n    table_settings = {\n        \"vertical_strategy\": \"lines\",\n        \"horizontal_strategy\": \"lines\",\n        \"snap_tolerance\": 3,\n        \"intersection_tolerance\": 15\n    }\n    tables = page.extract_tables(table_settings)\n    \n    # Visual debugging for table extraction\n    img = page.to_image(resolution=150)\n    img.save(\"debug_layout.png\")\n```\n\n### reportlab Advanced Features\n\n#### Create Professional Reports with Tables\n```python\nfrom reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph\nfrom reportlab.lib.styles import getSampleStyleSheet\nfrom reportlab.lib import colors\n\n# Sample data\ndata = [\n    ['Product', 'Q1', 'Q2', 'Q3', 'Q4'],\n    ['Widgets', '120', '135', '142', '158'],\n    ['Gadgets', '85', '92', '98', '105']\n]\n\n# Create PDF with table\ndoc = SimpleDocTemplate(\"report.pdf\")\nelements = []\n\n# Add title\nstyles = getSampleStyleSheet()\ntitle = Paragraph(\"Quarterly Sales Report\", styles['Title'])\nelements.append(title)\n\n# Add table with advanced styling\ntable = Table(data)\ntable.setStyle(TableStyle([\n    ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n    ('FONTSIZE', (0, 0), (-1, 0), 14),\n    ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n    ('BACKGROUND', (0, 1), (-1, -1), colors.beige),\n    ('GRID', (0, 0), (-1, -1), 1, colors.black)\n]))\nelements.append(table)\n\ndoc.build(elements)\n```\n\n## Complex Workflows\n\n### Extract Figures/Images from PDF\n\n#### Method 1: Using pdfimages (fastest)\n```bash\n# Extract all images with original quality\npdfimages -all document.pdf images/img\n```\n\n#### Method 2: Using pypdfium2 + Image Processing\n```python\nimport pypdfium2 as pdfium\nfrom PIL import Image\nimport numpy as np\n\ndef extract_figures(pdf_path, output_dir):\n    pdf = pdfium.PdfDocument(pdf_path)\n    \n    for page_num, page in enumerate(pdf):\n        # Render high-resolution page\n        bitmap = page.render(scale=3.0)\n        img = bitmap.to_pil()\n        \n        # Convert to numpy for processing\n        img_array = np.array(img)\n        \n        # Simple figure detection (non-white regions)\n        mask = np.any(img_array != [255, 255, 255], axis=2)\n        \n        # Find contours and extract bounding boxes\n        # (This is simplified - real implementation would need more sophisticated detection)\n        \n        # Save detected figures\n        # ... implementation depends on specific needs\n```\n\n### Batch PDF Processing with Error Handling\n```python\nimport os\nimport glob\nfrom pypdf import PdfReader, PdfWriter\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef batch_process_pdfs(input_dir, operation='merge'):\n    pdf_files = glob.glob(os.path.join(input_dir, \"*.pdf\"))\n    \n    if operation == 'merge':\n        writer = PdfWriter()\n        for pdf_file in pdf_files:\n            try:\n                reader = PdfReader(pdf_file)\n                for page in reader.pages:\n                    writer.add_page(page)\n                logger.info(f\"Processed: {pdf_file}\")\n            except Exception as e:\n                logger.error(f\"Failed to process {pdf_file}: {e}\")\n                continue\n        \n        with open(\"batch_merged.pdf\", \"wb\") as output:\n            writer.write(output)\n    \n    elif operation == 'extract_text':\n        for pdf_file in pdf_files:\n            try:\n                reader = PdfReader(pdf_file)\n                text = \"\"\n                for page in reader.pages:\n                    text += page.extract_text()\n                \n                output_file = pdf_file.replace('.pdf', '.txt')\n                with open(output_file, 'w', encoding='utf-8') as f:\n                    f.write(text)\n                logger.info(f\"Extracted text from: {pdf_file}\")\n                \n            except Exception as e:\n                logger.error(f\"Failed to extract text from {pdf_file}: {e}\")\n                continue\n```\n\n### Advanced PDF Cropping\n```python\nfrom pypdf import PdfWriter, PdfReader\n\nreader = PdfReader(\"input.pdf\")\nwriter = PdfWriter()\n\n# Crop page (left, bottom, right, top in points)\npage = reader.pages[0]\npage.mediabox.left = 50\npage.mediabox.bottom = 50\npage.mediabox.right = 550\npage.mediabox.top = 750\n\nwriter.add_page(page)\nwith open(\"cropped.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n## Performance Optimization Tips\n\n### 1. For Large PDFs\n- Use streaming approaches instead of loading entire PDF in memory\n- Use `qpdf --split-pages` for splitting large files\n- Process pages individually with pypdfium2\n\n### 2. For Text Extraction\n- `pdftotext -bbox-layout` is fastest for plain text extraction\n- Use pdfplumber for structured data and tables\n- Avoid `pypdf.extract_text()` for very large documents\n\n### 3. For Image Extraction\n- `pdfimages` is much faster than rendering pages\n- Use low resolution for previews, high resolution for final output\n\n### 4. For Form Filling\n- pdf-lib maintains form structure better than most alternatives\n- Pre-validate form fields before processing\n\n### 5. Memory Management\n```python\n# Process PDFs in chunks\ndef process_large_pdf(pdf_path, chunk_size=10):\n    reader = PdfReader(pdf_path)\n    total_pages = len(reader.pages)\n    \n    for start_idx in range(0, total_pages, chunk_size):\n        end_idx = min(start_idx + chunk_size, total_pages)\n        writer = PdfWriter()\n        \n        for i in range(start_idx, end_idx):\n            writer.add_page(reader.pages[i])\n        \n        # Process chunk\n        with open(f\"chunk_{start_idx//chunk_size}.pdf\", \"wb\") as output:\n            writer.write(output)\n```\n\n## Troubleshooting Common Issues\n\n### Encrypted PDFs\n```python\n# Handle password-protected PDFs\nfrom pypdf import PdfReader\n\ntry:\n    reader = PdfReader(\"encrypted.pdf\")\n    if reader.is_encrypted:\n        reader.decrypt(\"password\")\nexcept Exception as e:\n    print(f\"Failed to decrypt: {e}\")\n```\n\n### Corrupted PDFs\n```bash\n# Use qpdf to repair\nqpdf --check corrupted.pdf\nqpdf --replace-input corrupted.pdf\n```\n\n### Text Extraction Issues\n```python\n# Fallback to OCR for scanned PDFs\nimport pytesseract\nfrom pdf2image import convert_from_path\n\ndef extract_text_with_ocr(pdf_path):\n    images = convert_from_path(pdf_path)\n    text = \"\"\n    for i, image in enumerate(images):\n        text += pytesseract.image_to_string(image)\n    return text\n```\n\n## License Information\n\n- **pypdf**: BSD License\n- **pdfplumber**: MIT License\n- **pypdfium2**: Apache/BSD License\n- **reportlab**: BSD License\n- **poppler-utils**: GPL-2 License\n- **qpdf**: Apache License\n- **pdf-lib**: MIT License\n- **pdfjs-dist**: Apache License",
        "skills/cover_letter_generator/.claude/skills/pptx/SKILL.md": "---\nname: pptx\ndescription: \"Presentation creation, editing, and analysis. When Claude needs to work with presentations (.pptx files) for: (1) Creating new presentations, (2) Modifying or editing content, (3) Working with layouts, (4) Adding comments or speaker notes, or any other presentation tasks\"\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# PPTX creation, editing, and analysis\n\n## Overview\n\nA user may ask you to create, edit, or analyze the contents of a .pptx file. A .pptx file is essentially a ZIP archive containing XML files and other resources that you can read or edit. You have different tools and workflows available for different tasks.\n\n## Reading and analyzing content\n\n### Text extraction\nIf you just need to read the text contents of a presentation, you should convert the document to markdown:\n\n```bash\n# Convert document to markdown\npython -m markitdown path-to-file.pptx\n```\n\n### Raw XML access\nYou need raw XML access for: comments, speaker notes, slide layouts, animations, design elements, and complex formatting. For any of these features, you'll need to unpack a presentation and read its raw XML contents.\n\n#### Unpacking a file\n`python ooxml/scripts/unpack.py <office_file> <output_dir>`\n\n**Note**: The unpack.py script is located at `skills/pptx/ooxml/scripts/unpack.py` relative to the project root. If the script doesn't exist at this path, use `find . -name \"unpack.py\"` to locate it.\n\n#### Key file structures\n* `ppt/presentation.xml` - Main presentation metadata and slide references\n* `ppt/slides/slide{N}.xml` - Individual slide contents (slide1.xml, slide2.xml, etc.)\n* `ppt/notesSlides/notesSlide{N}.xml` - Speaker notes for each slide\n* `ppt/comments/modernComment_*.xml` - Comments for specific slides\n* `ppt/slideLayouts/` - Layout templates for slides\n* `ppt/slideMasters/` - Master slide templates\n* `ppt/theme/` - Theme and styling information\n* `ppt/media/` - Images and other media files\n\n#### Typography and color extraction\n**When given an example design to emulate**: Always analyze the presentation's typography and colors first using the methods below:\n1. **Read theme file**: Check `ppt/theme/theme1.xml` for colors (`<a:clrScheme>`) and fonts (`<a:fontScheme>`)\n2. **Sample slide content**: Examine `ppt/slides/slide1.xml` for actual font usage (`<a:rPr>`) and colors\n3. **Search for patterns**: Use grep to find color (`<a:solidFill>`, `<a:srgbClr>`) and font references across all XML files\n\n## Creating a new PowerPoint presentation **without a template**\n\nWhen creating a new PowerPoint presentation from scratch, use the **html2pptx** workflow to convert HTML slides to PowerPoint with accurate positioning.\n\n### Design Principles\n\n**CRITICAL**: Before creating any presentation, analyze the content and choose appropriate design elements:\n1. **Consider the subject matter**: What is this presentation about? What tone, industry, or mood does it suggest?\n2. **Check for branding**: If the user mentions a company/organization, consider their brand colors and identity\n3. **Match palette to content**: Select colors that reflect the subject\n4. **State your approach**: Explain your design choices before writing code\n\n**Requirements**:\n-  State your content-informed design approach BEFORE writing code\n-  Use web-safe fonts only: Arial, Helvetica, Times New Roman, Georgia, Courier New, Verdana, Tahoma, Trebuchet MS, Impact\n-  Create clear visual hierarchy through size, weight, and color\n-  Ensure readability: strong contrast, appropriately sized text, clean alignment\n-  Be consistent: repeat patterns, spacing, and visual language across slides\n\n#### Color Palette Selection\n\n**Choosing colors creatively**:\n- **Think beyond defaults**: What colors genuinely match this specific topic? Avoid autopilot choices.\n- **Consider multiple angles**: Topic, industry, mood, energy level, target audience, brand identity (if mentioned)\n- **Be adventurous**: Try unexpected combinations - a healthcare presentation doesn't have to be green, finance doesn't have to be navy\n- **Build your palette**: Pick 3-5 colors that work together (dominant colors + supporting tones + accent)\n- **Ensure contrast**: Text must be clearly readable on backgrounds\n\n**Example color palettes** (use these to spark creativity - choose one, adapt it, or create your own):\n\n1. **Classic Blue**: Deep navy (#1C2833), slate gray (#2E4053), silver (#AAB7B8), off-white (#F4F6F6)\n2. **Teal & Coral**: Teal (#5EA8A7), deep teal (#277884), coral (#FE4447), white (#FFFFFF)\n3. **Bold Red**: Red (#C0392B), bright red (#E74C3C), orange (#F39C12), yellow (#F1C40F), green (#2ECC71)\n4. **Warm Blush**: Mauve (#A49393), blush (#EED6D3), rose (#E8B4B8), cream (#FAF7F2)\n5. **Burgundy Luxury**: Burgundy (#5D1D2E), crimson (#951233), rust (#C15937), gold (#997929)\n6. **Deep Purple & Emerald**: Purple (#B165FB), dark blue (#181B24), emerald (#40695B), white (#FFFFFF)\n7. **Cream & Forest Green**: Cream (#FFE1C7), forest green (#40695B), white (#FCFCFC)\n8. **Pink & Purple**: Pink (#F8275B), coral (#FF574A), rose (#FF737D), purple (#3D2F68)\n9. **Lime & Plum**: Lime (#C5DE82), plum (#7C3A5F), coral (#FD8C6E), blue-gray (#98ACB5)\n10. **Black & Gold**: Gold (#BF9A4A), black (#000000), cream (#F4F6F6)\n11. **Sage & Terracotta**: Sage (#87A96B), terracotta (#E07A5F), cream (#F4F1DE), charcoal (#2C2C2C)\n12. **Charcoal & Red**: Charcoal (#292929), red (#E33737), light gray (#CCCBCB)\n13. **Vibrant Orange**: Orange (#F96D00), light gray (#F2F2F2), charcoal (#222831)\n14. **Forest Green**: Black (#191A19), green (#4E9F3D), dark green (#1E5128), white (#FFFFFF)\n15. **Retro Rainbow**: Purple (#722880), pink (#D72D51), orange (#EB5C18), amber (#F08800), gold (#DEB600)\n16. **Vintage Earthy**: Mustard (#E3B448), sage (#CBD18F), forest green (#3A6B35), cream (#F4F1DE)\n17. **Coastal Rose**: Old rose (#AD7670), beaver (#B49886), eggshell (#F3ECDC), ash gray (#BFD5BE)\n18. **Orange & Turquoise**: Light orange (#FC993E), grayish turquoise (#667C6F), white (#FCFCFC)\n\n#### Visual Details Options\n\n**Geometric Patterns**:\n- Diagonal section dividers instead of horizontal\n- Asymmetric column widths (30/70, 40/60, 25/75)\n- Rotated text headers at 90 or 270\n- Circular/hexagonal frames for images\n- Triangular accent shapes in corners\n- Overlapping shapes for depth\n\n**Border & Frame Treatments**:\n- Thick single-color borders (10-20pt) on one side only\n- Double-line borders with contrasting colors\n- Corner brackets instead of full frames\n- L-shaped borders (top+left or bottom+right)\n- Underline accents beneath headers (3-5pt thick)\n\n**Typography Treatments**:\n- Extreme size contrast (72pt headlines vs 11pt body)\n- All-caps headers with wide letter spacing\n- Numbered sections in oversized display type\n- Monospace (Courier New) for data/stats/technical content\n- Condensed fonts (Arial Narrow) for dense information\n- Outlined text for emphasis\n\n**Chart & Data Styling**:\n- Monochrome charts with single accent color for key data\n- Horizontal bar charts instead of vertical\n- Dot plots instead of bar charts\n- Minimal gridlines or none at all\n- Data labels directly on elements (no legends)\n- Oversized numbers for key metrics\n\n**Layout Innovations**:\n- Full-bleed images with text overlays\n- Sidebar column (20-30% width) for navigation/context\n- Modular grid systems (33, 44 blocks)\n- Z-pattern or F-pattern content flow\n- Floating text boxes over colored shapes\n- Magazine-style multi-column layouts\n\n**Background Treatments**:\n- Solid color blocks occupying 40-60% of slide\n- Gradient fills (vertical or diagonal only)\n- Split backgrounds (two colors, diagonal or vertical)\n- Edge-to-edge color bands\n- Negative space as a design element\n\n### Layout Tips\n**When creating slides with charts or tables:**\n- **Two-column layout (PREFERRED)**: Use a header spanning the full width, then two columns below - text/bullets in one column and the featured content in the other. This provides better balance and makes charts/tables more readable. Use flexbox with unequal column widths (e.g., 40%/60% split) to optimize space for each content type.\n- **Full-slide layout**: Let the featured content (chart/table) take up the entire slide for maximum impact and readability\n- **NEVER vertically stack**: Do not place charts/tables below text in a single column - this causes poor readability and layout issues\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`html2pptx.md`](html2pptx.md) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for detailed syntax, critical formatting rules, and best practices before proceeding with presentation creation.\n2. Create an HTML file for each slide with proper dimensions (e.g., 720pt  405pt for 16:9)\n   - Use `<p>`, `<h1>`-`<h6>`, `<ul>`, `<ol>` for all text content\n   - Use `class=\"placeholder\"` for areas where charts/tables will be added (render with gray background for visibility)\n   - **CRITICAL**: Rasterize gradients and icons as PNG images FIRST using Sharp, then reference in HTML\n   - **LAYOUT**: For slides with charts/tables/images, use either full-slide layout or two-column layout for better readability\n3. Create and run a JavaScript file using the [`html2pptx.js`](scripts/html2pptx.js) library to convert HTML slides to PowerPoint and save the presentation\n   - Use the `html2pptx()` function to process each HTML file\n   - Add charts and tables to placeholder areas using PptxGenJS API\n   - Save the presentation using `pptx.writeFile()`\n4. **Visual validation**: Generate thumbnails and inspect for layout issues\n   - Create thumbnail grid: `python scripts/thumbnail.py output.pptx workspace/thumbnails --cols 4`\n   - Read and carefully examine the thumbnail image for:\n     - **Text cutoff**: Text being cut off by header bars, shapes, or slide edges\n     - **Text overlap**: Text overlapping with other text or shapes\n     - **Positioning issues**: Content too close to slide boundaries or other elements\n     - **Contrast issues**: Insufficient contrast between text and backgrounds\n   - If issues found, adjust HTML margins/spacing/colors and regenerate the presentation\n   - Repeat until all slides are visually correct\n\n## Editing an existing PowerPoint presentation\n\nWhen edit slides in an existing PowerPoint presentation, you need to work with the raw Office Open XML (OOXML) format. This involves unpacking the .pptx file, editing the XML content, and repacking it.\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~500 lines) completely from start to finish.  **NEVER set any range limits when reading this file.**  Read the full file content for detailed guidance on OOXML structure and editing workflows before any presentation editing.\n2. Unpack the presentation: `python ooxml/scripts/unpack.py <office_file> <output_dir>`\n3. Edit the XML files (primarily `ppt/slides/slide{N}.xml` and related files)\n4. **CRITICAL**: Validate immediately after each edit and fix any validation errors before proceeding: `python ooxml/scripts/validate.py <dir> --original <file>`\n5. Pack the final presentation: `python ooxml/scripts/pack.py <input_directory> <office_file>`\n\n## Creating a new PowerPoint presentation **using a template**\n\nWhen you need to create a presentation that follows an existing template's design, you'll need to duplicate and re-arrange template slides before then replacing placeholder context.\n\n### Workflow\n1. **Extract template text AND create visual thumbnail grid**:\n   * Extract text: `python -m markitdown template.pptx > template-content.md`\n   * Read `template-content.md`: Read the entire file to understand the contents of the template presentation. **NEVER set any range limits when reading this file.**\n   * Create thumbnail grids: `python scripts/thumbnail.py template.pptx`\n   * See [Creating Thumbnail Grids](#creating-thumbnail-grids) section for more details\n\n2. **Analyze template and save inventory to a file**:\n   * **Visual Analysis**: Review thumbnail grid(s) to understand slide layouts, design patterns, and visual structure\n   * Create and save a template inventory file at `template-inventory.md` containing:\n     ```markdown\n     # Template Inventory Analysis\n     **Total Slides: [count]**\n     **IMPORTANT: Slides are 0-indexed (first slide = 0, last slide = count-1)**\n\n     ## [Category Name]\n     - Slide 0: [Layout code if available] - Description/purpose\n     - Slide 1: [Layout code] - Description/purpose\n     - Slide 2: [Layout code] - Description/purpose\n     [... EVERY slide must be listed individually with its index ...]\n     ```\n   * **Using the thumbnail grid**: Reference the visual thumbnails to identify:\n     - Layout patterns (title slides, content layouts, section dividers)\n     - Image placeholder locations and counts\n     - Design consistency across slide groups\n     - Visual hierarchy and structure\n   * This inventory file is REQUIRED for selecting appropriate templates in the next step\n\n3. **Create presentation outline based on template inventory**:\n   * Review available templates from step 2.\n   * Choose an intro or title template for the first slide. This should be one of the first templates.\n   * Choose safe, text-based layouts for the other slides.\n   * **CRITICAL: Match layout structure to actual content**:\n     - Single-column layouts: Use for unified narrative or single topic\n     - Two-column layouts: Use ONLY when you have exactly 2 distinct items/concepts\n     - Three-column layouts: Use ONLY when you have exactly 3 distinct items/concepts\n     - Image + text layouts: Use ONLY when you have actual images to insert\n     - Quote layouts: Use ONLY for actual quotes from people (with attribution), never for emphasis\n     - Never use layouts with more placeholders than you have content\n     - If you have 2 items, don't force them into a 3-column layout\n     - If you have 4+ items, consider breaking into multiple slides or using a list format\n   * Count your actual content pieces BEFORE selecting the layout\n   * Verify each placeholder in the chosen layout will be filled with meaningful content\n   * Select one option representing the **best** layout for each content section.\n   * Save `outline.md` with content AND template mapping that leverages available designs\n   * Example template mapping:\n      ```\n      # Template slides to use (0-based indexing)\n      # WARNING: Verify indices are within range! Template with 73 slides has indices 0-72\n      # Mapping: slide numbers from outline -> template slide indices\n      template_mapping = [\n          0,   # Use slide 0 (Title/Cover)\n          34,  # Use slide 34 (B1: Title and body)\n          34,  # Use slide 34 again (duplicate for second B1)\n          50,  # Use slide 50 (E1: Quote)\n          54,  # Use slide 54 (F2: Closing + Text)\n      ]\n      ```\n\n4. **Duplicate, reorder, and delete slides using `rearrange.py`**:\n   * Use the `scripts/rearrange.py` script to create a new presentation with slides in the desired order:\n     ```bash\n     python scripts/rearrange.py template.pptx working.pptx 0,34,34,50,52\n     ```\n   * The script handles duplicating repeated slides, deleting unused slides, and reordering automatically\n   * Slide indices are 0-based (first slide is 0, second is 1, etc.)\n   * The same slide index can appear multiple times to duplicate that slide\n\n5. **Extract ALL text using the `inventory.py` script**:\n   * **Run inventory extraction**:\n     ```bash\n     python scripts/inventory.py working.pptx text-inventory.json\n     ```\n   * **Read text-inventory.json**: Read the entire text-inventory.json file to understand all shapes and their properties. **NEVER set any range limits when reading this file.**\n\n   * The inventory JSON structure:\n      ```json\n        {\n          \"slide-0\": {\n            \"shape-0\": {\n              \"placeholder_type\": \"TITLE\",  // or null for non-placeholders\n              \"left\": 1.5,                  // position in inches\n              \"top\": 2.0,\n              \"width\": 7.5,\n              \"height\": 1.2,\n              \"paragraphs\": [\n                {\n                  \"text\": \"Paragraph text\",\n                  // Optional properties (only included when non-default):\n                  \"bullet\": true,           // explicit bullet detected\n                  \"level\": 0,               // only included when bullet is true\n                  \"alignment\": \"CENTER\",    // CENTER, RIGHT (not LEFT)\n                  \"space_before\": 10.0,     // space before paragraph in points\n                  \"space_after\": 6.0,       // space after paragraph in points\n                  \"line_spacing\": 22.4,     // line spacing in points\n                  \"font_name\": \"Arial\",     // from first run\n                  \"font_size\": 14.0,        // in points\n                  \"bold\": true,\n                  \"italic\": false,\n                  \"underline\": false,\n                  \"color\": \"FF0000\"         // RGB color\n                }\n              ]\n            }\n          }\n        }\n      ```\n\n   * Key features:\n     - **Slides**: Named as \"slide-0\", \"slide-1\", etc.\n     - **Shapes**: Ordered by visual position (top-to-bottom, left-to-right) as \"shape-0\", \"shape-1\", etc.\n     - **Placeholder types**: TITLE, CENTER_TITLE, SUBTITLE, BODY, OBJECT, or null\n     - **Default font size**: `default_font_size` in points extracted from layout placeholders (when available)\n     - **Slide numbers are filtered**: Shapes with SLIDE_NUMBER placeholder type are automatically excluded from inventory\n     - **Bullets**: When `bullet: true`, `level` is always included (even if 0)\n     - **Spacing**: `space_before`, `space_after`, and `line_spacing` in points (only included when set)\n     - **Colors**: `color` for RGB (e.g., \"FF0000\"), `theme_color` for theme colors (e.g., \"DARK_1\")\n     - **Properties**: Only non-default values are included in the output\n\n6. **Generate replacement text and save the data to a JSON file**\n   Based on the text inventory from the previous step:\n   - **CRITICAL**: First verify which shapes exist in the inventory - only reference shapes that are actually present\n   - **VALIDATION**: The replace.py script will validate that all shapes in your replacement JSON exist in the inventory\n     - If you reference a non-existent shape, you'll get an error showing available shapes\n     - If you reference a non-existent slide, you'll get an error indicating the slide doesn't exist\n     - All validation errors are shown at once before the script exits\n   - **IMPORTANT**: The replace.py script uses inventory.py internally to identify ALL text shapes\n   - **AUTOMATIC CLEARING**: ALL text shapes from the inventory will be cleared unless you provide \"paragraphs\" for them\n   - Add a \"paragraphs\" field to shapes that need content (not \"replacement_paragraphs\")\n   - Shapes without \"paragraphs\" in the replacement JSON will have their text cleared automatically\n   - Paragraphs with bullets will be automatically left aligned. Don't set the `alignment` property on when `\"bullet\": true`\n   - Generate appropriate replacement content for placeholder text\n   - Use shape size to determine appropriate content length\n   - **CRITICAL**: Include paragraph properties from the original inventory - don't just provide text\n   - **IMPORTANT**: When bullet: true, do NOT include bullet symbols (, -, *) in text - they're added automatically\n   - **ESSENTIAL FORMATTING RULES**:\n     - Headers/titles should typically have `\"bold\": true`\n     - List items should have `\"bullet\": true, \"level\": 0` (level is required when bullet is true)\n     - Preserve any alignment properties (e.g., `\"alignment\": \"CENTER\"` for centered text)\n     - Include font properties when different from default (e.g., `\"font_size\": 14.0`, `\"font_name\": \"Lora\"`)\n     - Colors: Use `\"color\": \"FF0000\"` for RGB or `\"theme_color\": \"DARK_1\"` for theme colors\n     - The replacement script expects **properly formatted paragraphs**, not just text strings\n     - **Overlapping shapes**: Prefer shapes with larger default_font_size or more appropriate placeholder_type\n   - Save the updated inventory with replacements to `replacement-text.json`\n   - **WARNING**: Different template layouts have different shape counts - always check the actual inventory before creating replacements\n\n   Example paragraphs field showing proper formatting:\n   ```json\n   \"paragraphs\": [\n     {\n       \"text\": \"New presentation title text\",\n       \"alignment\": \"CENTER\",\n       \"bold\": true\n     },\n     {\n       \"text\": \"Section Header\",\n       \"bold\": true\n     },\n     {\n       \"text\": \"First bullet point without bullet symbol\",\n       \"bullet\": true,\n       \"level\": 0\n     },\n     {\n       \"text\": \"Red colored text\",\n       \"color\": \"FF0000\"\n     },\n     {\n       \"text\": \"Theme colored text\",\n       \"theme_color\": \"DARK_1\"\n     },\n     {\n       \"text\": \"Regular paragraph text without special formatting\"\n     }\n   ]\n   ```\n\n   **Shapes not listed in the replacement JSON are automatically cleared**:\n   ```json\n   {\n     \"slide-0\": {\n       \"shape-0\": {\n         \"paragraphs\": [...] // This shape gets new text\n       }\n       // shape-1 and shape-2 from inventory will be cleared automatically\n     }\n   }\n   ```\n\n   **Common formatting patterns for presentations**:\n   - Title slides: Bold text, sometimes centered\n   - Section headers within slides: Bold text\n   - Bullet lists: Each item needs `\"bullet\": true, \"level\": 0`\n   - Body text: Usually no special properties needed\n   - Quotes: May have special alignment or font properties\n\n7. **Apply replacements using the `replace.py` script**\n   ```bash\n   python scripts/replace.py working.pptx replacement-text.json output.pptx\n   ```\n\n   The script will:\n   - First extract the inventory of ALL text shapes using functions from inventory.py\n   - Validate that all shapes in the replacement JSON exist in the inventory\n   - Clear text from ALL shapes identified in the inventory\n   - Apply new text only to shapes with \"paragraphs\" defined in the replacement JSON\n   - Preserve formatting by applying paragraph properties from the JSON\n   - Handle bullets, alignment, font properties, and colors automatically\n   - Save the updated presentation\n\n   Example validation errors:\n   ```\n   ERROR: Invalid shapes in replacement JSON:\n     - Shape 'shape-99' not found on 'slide-0'. Available shapes: shape-0, shape-1, shape-4\n     - Slide 'slide-999' not found in inventory\n   ```\n\n   ```\n   ERROR: Replacement text made overflow worse in these shapes:\n     - slide-0/shape-2: overflow worsened by 1.25\" (was 0.00\", now 1.25\")\n   ```\n\n## Creating Thumbnail Grids\n\nTo create visual thumbnail grids of PowerPoint slides for quick analysis and reference:\n\n```bash\npython scripts/thumbnail.py template.pptx [output_prefix]\n```\n\n**Features**:\n- Creates: `thumbnails.jpg` (or `thumbnails-1.jpg`, `thumbnails-2.jpg`, etc. for large decks)\n- Default: 5 columns, max 30 slides per grid (56)\n- Custom prefix: `python scripts/thumbnail.py template.pptx my-grid`\n  - Note: The output prefix should include the path if you want output in a specific directory (e.g., `workspace/my-grid`)\n- Adjust columns: `--cols 4` (range: 3-6, affects slides per grid)\n- Grid limits: 3 cols = 12 slides/grid, 4 cols = 20, 5 cols = 30, 6 cols = 42\n- Slides are zero-indexed (Slide 0, Slide 1, etc.)\n\n**Use cases**:\n- Template analysis: Quickly understand slide layouts and design patterns\n- Content review: Visual overview of entire presentation\n- Navigation reference: Find specific slides by their visual appearance\n- Quality check: Verify all slides are properly formatted\n\n**Examples**:\n```bash\n# Basic usage\npython scripts/thumbnail.py presentation.pptx\n\n# Combine options: custom name, columns\npython scripts/thumbnail.py template.pptx analysis --cols 4\n```\n\n## Converting Slides to Images\n\nTo visually analyze PowerPoint slides, convert them to images using a two-step process:\n\n1. **Convert PPTX to PDF**:\n   ```bash\n   soffice --headless --convert-to pdf template.pptx\n   ```\n\n2. **Convert PDF pages to JPEG images**:\n   ```bash\n   pdftoppm -jpeg -r 150 template.pdf slide\n   ```\n   This creates files like `slide-1.jpg`, `slide-2.jpg`, etc.\n\nOptions:\n- `-r 150`: Sets resolution to 150 DPI (adjust for quality/size balance)\n- `-jpeg`: Output JPEG format (use `-png` for PNG if preferred)\n- `-f N`: First page to convert (e.g., `-f 2` starts from page 2)\n- `-l N`: Last page to convert (e.g., `-l 5` stops at page 5)\n- `slide`: Prefix for output files\n\nExample for specific range:\n```bash\npdftoppm -jpeg -r 150 -f 2 -l 5 template.pdf slide  # Converts only pages 2-5\n```\n\n## Code Style Guidelines\n**IMPORTANT**: When generating code for PPTX operations:\n- Write concise code\n- Avoid verbose variable names and redundant operations\n- Avoid unnecessary print statements\n\n## Dependencies\n\nRequired dependencies (should already be installed):\n\n- **markitdown**: `pip install \"markitdown[pptx]\"` (for text extraction from presentations)\n- **pptxgenjs**: `npm install -g pptxgenjs` (for creating presentations via html2pptx)\n- **playwright**: `npm install -g playwright` (for HTML rendering in html2pptx)\n- **react-icons**: `npm install -g react-icons react react-dom` (for icons)\n- **sharp**: `npm install -g sharp` (for SVG rasterization and image processing)\n- **LibreOffice**: `sudo apt-get install libreoffice` (for PDF conversion)\n- **Poppler**: `sudo apt-get install poppler-utils` (for pdftoppm to convert PDF to images)\n- **defusedxml**: `pip install defusedxml` (for secure XML parsing)",
        "skills/cover_letter_generator/.claude/skills/pptx/html2pptx.md": "# HTML to PowerPoint Guide\n\nConvert HTML slides to PowerPoint presentations with accurate positioning using the `html2pptx.js` library.\n\n## Table of Contents\n\n1. [Creating HTML Slides](#creating-html-slides)\n2. [Using the html2pptx Library](#using-the-html2pptx-library)\n3. [Using PptxGenJS](#using-pptxgenjs)\n\n---\n\n## Creating HTML Slides\n\nEvery HTML slide must include proper body dimensions:\n\n### Layout Dimensions\n\n- **16:9** (default): `width: 720pt; height: 405pt`\n- **4:3**: `width: 720pt; height: 540pt`\n- **16:10**: `width: 720pt; height: 450pt`\n\n### Supported Elements\n\n- `<p>`, `<h1>`-`<h6>` - Text with styling\n- `<ul>`, `<ol>` - Lists (never use manual bullets , -, *)\n- `<b>`, `<strong>` - Bold text (inline formatting)\n- `<i>`, `<em>` - Italic text (inline formatting)\n- `<u>` - Underlined text (inline formatting)\n- `<span>` - Inline formatting with CSS styles (bold, italic, underline, color)\n- `<br>` - Line breaks\n- `<div>` with bg/border - Becomes shape\n- `<img>` - Images\n- `class=\"placeholder\"` - Reserved space for charts (returns `{ id, x, y, w, h }`)\n\n### Critical Text Rules\n\n**ALL text MUST be inside `<p>`, `<h1>`-`<h6>`, `<ul>`, or `<ol>` tags:**\n-  Correct: `<div><p>Text here</p></div>`\n-  Wrong: `<div>Text here</div>` - **Text will NOT appear in PowerPoint**\n-  Wrong: `<span>Text</span>` - **Text will NOT appear in PowerPoint**\n- Text in `<div>` or `<span>` without a text tag will be silently ignored\n\n**NEVER use manual bullet symbols (, -, *, etc.)** - Use `<ul>` or `<ol>` lists instead\n\n**ONLY use web-safe fonts that are universally available:**\n-  Web-safe fonts: `Arial`, `Helvetica`, `Times New Roman`, `Georgia`, `Courier New`, `Verdana`, `Tahoma`, `Trebuchet MS`, `Impact`, `Comic Sans MS`\n-  Wrong: `'Segoe UI'`, `'SF Pro'`, `'Roboto'`, custom fonts - **Might cause rendering issues**\n\n### Styling\n\n- Use `display: flex` on body to prevent margin collapse from breaking overflow validation\n- Use `margin` for spacing (padding included in size)\n- Inline formatting: Use `<b>`, `<i>`, `<u>` tags OR `<span>` with CSS styles\n  - `<span>` supports: `font-weight: bold`, `font-style: italic`, `text-decoration: underline`, `color: #rrggbb`\n  - `<span>` does NOT support: `margin`, `padding` (not supported in PowerPoint text runs)\n  - Example: `<span style=\"font-weight: bold; color: #667eea;\">Bold blue text</span>`\n- Flexbox works - positions calculated from rendered layout\n- Use hex colors with `#` prefix in CSS\n- **Text alignment**: Use CSS `text-align` (`center`, `right`, etc.) when needed as a hint to PptxGenJS for text formatting if text lengths are slightly off\n\n### Shape Styling (DIV elements only)\n\n**IMPORTANT: Backgrounds, borders, and shadows only work on `<div>` elements, NOT on text elements (`<p>`, `<h1>`-`<h6>`, `<ul>`, `<ol>`)**\n\n- **Backgrounds**: CSS `background` or `background-color` on `<div>` elements only\n  - Example: `<div style=\"background: #f0f0f0;\">` - Creates a shape with background\n- **Borders**: CSS `border` on `<div>` elements converts to PowerPoint shape borders\n  - Supports uniform borders: `border: 2px solid #333333`\n  - Supports partial borders: `border-left`, `border-right`, `border-top`, `border-bottom` (rendered as line shapes)\n  - Example: `<div style=\"border-left: 8pt solid #E76F51;\">`\n- **Border radius**: CSS `border-radius` on `<div>` elements for rounded corners\n  - `border-radius: 50%` or higher creates circular shape\n  - Percentages <50% calculated relative to shape's smaller dimension\n  - Supports px and pt units (e.g., `border-radius: 8pt;`, `border-radius: 12px;`)\n  - Example: `<div style=\"border-radius: 25%;\">` on 100x200px box = 25% of 100px = 25px radius\n- **Box shadows**: CSS `box-shadow` on `<div>` elements converts to PowerPoint shadows\n  - Supports outer shadows only (inset shadows are ignored to prevent corruption)\n  - Example: `<div style=\"box-shadow: 2px 2px 8px rgba(0, 0, 0, 0.3);\">`\n  - Note: Inset/inner shadows are not supported by PowerPoint and will be skipped\n\n### Icons & Gradients\n\n- **CRITICAL: Never use CSS gradients (`linear-gradient`, `radial-gradient`)** - They don't convert to PowerPoint\n- **ALWAYS create gradient/icon PNGs FIRST using Sharp, then reference in HTML**\n- For gradients: Rasterize SVG to PNG background images\n- For icons: Rasterize react-icons SVG to PNG images\n- All visual effects must be pre-rendered as raster images before HTML rendering\n\n**Rasterizing Icons with Sharp:**\n\n```javascript\nconst React = require('react');\nconst ReactDOMServer = require('react-dom/server');\nconst sharp = require('sharp');\nconst { FaHome } = require('react-icons/fa');\n\nasync function rasterizeIconPng(IconComponent, color, size = \"256\", filename) {\n  const svgString = ReactDOMServer.renderToStaticMarkup(\n    React.createElement(IconComponent, { color: `#${color}`, size: size })\n  );\n\n  // Convert SVG to PNG using Sharp\n  await sharp(Buffer.from(svgString))\n    .png()\n    .toFile(filename);\n\n  return filename;\n}\n\n// Usage: Rasterize icon before using in HTML\nconst iconPath = await rasterizeIconPng(FaHome, \"4472c4\", \"256\", \"home-icon.png\");\n// Then reference in HTML: <img src=\"home-icon.png\" style=\"width: 40pt; height: 40pt;\">\n```\n\n**Rasterizing Gradients with Sharp:**\n\n```javascript\nconst sharp = require('sharp');\n\nasync function createGradientBackground(filename) {\n  const svg = `<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1000\" height=\"562.5\">\n    <defs>\n      <linearGradient id=\"g\" x1=\"0%\" y1=\"0%\" x2=\"100%\" y2=\"100%\">\n        <stop offset=\"0%\" style=\"stop-color:#COLOR1\"/>\n        <stop offset=\"100%\" style=\"stop-color:#COLOR2\"/>\n      </linearGradient>\n    </defs>\n    <rect width=\"100%\" height=\"100%\" fill=\"url(#g)\"/>\n  </svg>`;\n\n  await sharp(Buffer.from(svg))\n    .png()\n    .toFile(filename);\n\n  return filename;\n}\n\n// Usage: Create gradient background before HTML\nconst bgPath = await createGradientBackground(\"gradient-bg.png\");\n// Then in HTML: <body style=\"background-image: url('gradient-bg.png');\">\n```\n\n### Example\n\n```html\n<!DOCTYPE html>\n<html>\n<head>\n<style>\nhtml { background: #ffffff; }\nbody {\n  width: 720pt; height: 405pt; margin: 0; padding: 0;\n  background: #f5f5f5; font-family: Arial, sans-serif;\n  display: flex;\n}\n.content { margin: 30pt; padding: 40pt; background: #ffffff; border-radius: 8pt; }\nh1 { color: #2d3748; font-size: 32pt; }\n.box {\n  background: #70ad47; padding: 20pt; border: 3px solid #5a8f37;\n  border-radius: 12pt; box-shadow: 3px 3px 10px rgba(0, 0, 0, 0.25);\n}\n</style>\n</head>\n<body>\n<div class=\"content\">\n  <h1>Recipe Title</h1>\n  <ul>\n    <li><b>Item:</b> Description</li>\n  </ul>\n  <p>Text with <b>bold</b>, <i>italic</i>, <u>underline</u>.</p>\n  <div id=\"chart\" class=\"placeholder\" style=\"width: 350pt; height: 200pt;\"></div>\n\n  <!-- Text MUST be in <p> tags -->\n  <div class=\"box\">\n    <p>5</p>\n  </div>\n</div>\n</body>\n</html>\n```\n\n## Using the html2pptx Library\n\n### Dependencies\n\nThese libraries have been globally installed and are available to use:\n- `pptxgenjs`\n- `playwright`\n- `sharp`\n\n### Basic Usage\n\n```javascript\nconst pptxgen = require('pptxgenjs');\nconst html2pptx = require('./html2pptx');\n\nconst pptx = new pptxgen();\npptx.layout = 'LAYOUT_16x9';  // Must match HTML body dimensions\n\nconst { slide, placeholders } = await html2pptx('slide1.html', pptx);\n\n// Add chart to placeholder area\nif (placeholders.length > 0) {\n    slide.addChart(pptx.charts.LINE, chartData, placeholders[0]);\n}\n\nawait pptx.writeFile('output.pptx');\n```\n\n### API Reference\n\n#### Function Signature\n```javascript\nawait html2pptx(htmlFile, pres, options)\n```\n\n#### Parameters\n- `htmlFile` (string): Path to HTML file (absolute or relative)\n- `pres` (pptxgen): PptxGenJS presentation instance with layout already set\n- `options` (object, optional):\n  - `tmpDir` (string): Temporary directory for generated files (default: `process.env.TMPDIR || '/tmp'`)\n  - `slide` (object): Existing slide to reuse (default: creates new slide)\n\n#### Returns\n```javascript\n{\n    slide: pptxgenSlide,           // The created/updated slide\n    placeholders: [                 // Array of placeholder positions\n        { id: string, x: number, y: number, w: number, h: number },\n        ...\n    ]\n}\n```\n\n### Validation\n\nThe library automatically validates and collects all errors before throwing:\n\n1. **HTML dimensions must match presentation layout** - Reports dimension mismatches\n2. **Content must not overflow body** - Reports overflow with exact measurements\n3. **CSS gradients** - Reports unsupported gradient usage\n4. **Text element styling** - Reports backgrounds/borders/shadows on text elements (only allowed on divs)\n\n**All validation errors are collected and reported together** in a single error message, allowing you to fix all issues at once instead of one at a time.\n\n### Working with Placeholders\n\n```javascript\nconst { slide, placeholders } = await html2pptx('slide.html', pptx);\n\n// Use first placeholder\nslide.addChart(pptx.charts.BAR, data, placeholders[0]);\n\n// Find by ID\nconst chartArea = placeholders.find(p => p.id === 'chart-area');\nslide.addChart(pptx.charts.LINE, data, chartArea);\n```\n\n### Complete Example\n\n```javascript\nconst pptxgen = require('pptxgenjs');\nconst html2pptx = require('./html2pptx');\n\nasync function createPresentation() {\n    const pptx = new pptxgen();\n    pptx.layout = 'LAYOUT_16x9';\n    pptx.author = 'Your Name';\n    pptx.title = 'My Presentation';\n\n    // Slide 1: Title\n    const { slide: slide1 } = await html2pptx('slides/title.html', pptx);\n\n    // Slide 2: Content with chart\n    const { slide: slide2, placeholders } = await html2pptx('slides/data.html', pptx);\n\n    const chartData = [{\n        name: 'Sales',\n        labels: ['Q1', 'Q2', 'Q3', 'Q4'],\n        values: [4500, 5500, 6200, 7100]\n    }];\n\n    slide2.addChart(pptx.charts.BAR, chartData, {\n        ...placeholders[0],\n        showTitle: true,\n        title: 'Quarterly Sales',\n        showCatAxisTitle: true,\n        catAxisTitle: 'Quarter',\n        showValAxisTitle: true,\n        valAxisTitle: 'Sales ($000s)'\n    });\n\n    // Save\n    await pptx.writeFile({ fileName: 'presentation.pptx' });\n    console.log('Presentation created successfully!');\n}\n\ncreatePresentation().catch(console.error);\n```\n\n## Using PptxGenJS\n\nAfter converting HTML to slides with `html2pptx`, you'll use PptxGenJS to add dynamic content like charts, images, and additional elements.\n\n###  Critical Rules\n\n#### Colors\n- **NEVER use `#` prefix** with hex colors in PptxGenJS - causes file corruption\n-  Correct: `color: \"FF0000\"`, `fill: { color: \"0066CC\" }`\n-  Wrong: `color: \"#FF0000\"` (breaks document)\n\n### Adding Images\n\nAlways calculate aspect ratios from actual image dimensions:\n\n```javascript\n// Get image dimensions: identify image.png | grep -o '[0-9]* x [0-9]*'\nconst imgWidth = 1860, imgHeight = 1519;  // From actual file\nconst aspectRatio = imgWidth / imgHeight;\n\nconst h = 3;  // Max height\nconst w = h * aspectRatio;\nconst x = (10 - w) / 2;  // Center on 16:9 slide\n\nslide.addImage({ path: \"chart.png\", x, y: 1.5, w, h });\n```\n\n### Adding Text\n\n```javascript\n// Rich text with formatting\nslide.addText([\n    { text: \"Bold \", options: { bold: true } },\n    { text: \"Italic \", options: { italic: true } },\n    { text: \"Normal\" }\n], {\n    x: 1, y: 2, w: 8, h: 1\n});\n```\n\n### Adding Shapes\n\n```javascript\n// Rectangle\nslide.addShape(pptx.shapes.RECTANGLE, {\n    x: 1, y: 1, w: 3, h: 2,\n    fill: { color: \"4472C4\" },\n    line: { color: \"000000\", width: 2 }\n});\n\n// Circle\nslide.addShape(pptx.shapes.OVAL, {\n    x: 5, y: 1, w: 2, h: 2,\n    fill: { color: \"ED7D31\" }\n});\n\n// Rounded rectangle\nslide.addShape(pptx.shapes.ROUNDED_RECTANGLE, {\n    x: 1, y: 4, w: 3, h: 1.5,\n    fill: { color: \"70AD47\" },\n    rectRadius: 0.2\n});\n```\n\n### Adding Charts\n\n**Required for most charts:** Axis labels using `catAxisTitle` (category) and `valAxisTitle` (value).\n\n**Chart Data Format:**\n- Use **single series with all labels** for simple bar/line charts\n- Each series creates a separate legend entry\n- Labels array defines X-axis values\n\n**Time Series Data - Choose Correct Granularity:**\n- **< 30 days**: Use daily grouping (e.g., \"10-01\", \"10-02\") - avoid monthly aggregation that creates single-point charts\n- **30-365 days**: Use monthly grouping (e.g., \"2024-01\", \"2024-02\")\n- **> 365 days**: Use yearly grouping (e.g., \"2023\", \"2024\")\n- **Validate**: Charts with only 1 data point likely indicate incorrect aggregation for the time period\n\n```javascript\nconst { slide, placeholders } = await html2pptx('slide.html', pptx);\n\n// CORRECT: Single series with all labels\nslide.addChart(pptx.charts.BAR, [{\n    name: \"Sales 2024\",\n    labels: [\"Q1\", \"Q2\", \"Q3\", \"Q4\"],\n    values: [4500, 5500, 6200, 7100]\n}], {\n    ...placeholders[0],  // Use placeholder position\n    barDir: 'col',       // 'col' = vertical bars, 'bar' = horizontal\n    showTitle: true,\n    title: 'Quarterly Sales',\n    showLegend: false,   // No legend needed for single series\n    // Required axis labels\n    showCatAxisTitle: true,\n    catAxisTitle: 'Quarter',\n    showValAxisTitle: true,\n    valAxisTitle: 'Sales ($000s)',\n    // Optional: Control scaling (adjust min based on data range for better visualization)\n    valAxisMaxVal: 8000,\n    valAxisMinVal: 0,  // Use 0 for counts/amounts; for clustered data (e.g., 4500-7100), consider starting closer to min value\n    valAxisMajorUnit: 2000,  // Control y-axis label spacing to prevent crowding\n    catAxisLabelRotate: 45,  // Rotate labels if crowded\n    dataLabelPosition: 'outEnd',\n    dataLabelColor: '000000',\n    // Use single color for single-series charts\n    chartColors: [\"4472C4\"]  // All bars same color\n});\n```\n\n#### Scatter Chart\n\n**IMPORTANT**: Scatter chart data format is unusual - first series contains X-axis values, subsequent series contain Y-values:\n\n```javascript\n// Prepare data\nconst data1 = [{ x: 10, y: 20 }, { x: 15, y: 25 }, { x: 20, y: 30 }];\nconst data2 = [{ x: 12, y: 18 }, { x: 18, y: 22 }];\n\nconst allXValues = [...data1.map(d => d.x), ...data2.map(d => d.x)];\n\nslide.addChart(pptx.charts.SCATTER, [\n    { name: 'X-Axis', values: allXValues },  // First series = X values\n    { name: 'Series 1', values: data1.map(d => d.y) },  // Y values only\n    { name: 'Series 2', values: data2.map(d => d.y) }   // Y values only\n], {\n    x: 1, y: 1, w: 8, h: 4,\n    lineSize: 0,  // 0 = no connecting lines\n    lineDataSymbol: 'circle',\n    lineDataSymbolSize: 6,\n    showCatAxisTitle: true,\n    catAxisTitle: 'X Axis',\n    showValAxisTitle: true,\n    valAxisTitle: 'Y Axis',\n    chartColors: [\"4472C4\", \"ED7D31\"]\n});\n```\n\n#### Line Chart\n\n```javascript\nslide.addChart(pptx.charts.LINE, [{\n    name: \"Temperature\",\n    labels: [\"Jan\", \"Feb\", \"Mar\", \"Apr\"],\n    values: [32, 35, 42, 55]\n}], {\n    x: 1, y: 1, w: 8, h: 4,\n    lineSize: 4,\n    lineSmooth: true,\n    // Required axis labels\n    showCatAxisTitle: true,\n    catAxisTitle: 'Month',\n    showValAxisTitle: true,\n    valAxisTitle: 'Temperature (F)',\n    // Optional: Y-axis range (set min based on data range for better visualization)\n    valAxisMinVal: 0,     // For ranges starting at 0 (counts, percentages, etc.)\n    valAxisMaxVal: 60,\n    valAxisMajorUnit: 20,  // Control y-axis label spacing to prevent crowding (e.g., 10, 20, 25)\n    // valAxisMinVal: 30,  // PREFERRED: For data clustered in a range (e.g., 32-55 or ratings 3-5), start axis closer to min value to show variation\n    // Optional: Chart colors\n    chartColors: [\"4472C4\", \"ED7D31\", \"A5A5A5\"]\n});\n```\n\n#### Pie Chart (No Axis Labels Required)\n\n**CRITICAL**: Pie charts require a **single data series** with all categories in the `labels` array and corresponding values in the `values` array.\n\n```javascript\nslide.addChart(pptx.charts.PIE, [{\n    name: \"Market Share\",\n    labels: [\"Product A\", \"Product B\", \"Other\"],  // All categories in one array\n    values: [35, 45, 20]  // All values in one array\n}], {\n    x: 2, y: 1, w: 6, h: 4,\n    showPercent: true,\n    showLegend: true,\n    legendPos: 'r',  // right\n    chartColors: [\"4472C4\", \"ED7D31\", \"A5A5A5\"]\n});\n```\n\n#### Multiple Data Series\n\n```javascript\nslide.addChart(pptx.charts.LINE, [\n    {\n        name: \"Product A\",\n        labels: [\"Q1\", \"Q2\", \"Q3\", \"Q4\"],\n        values: [10, 20, 30, 40]\n    },\n    {\n        name: \"Product B\",\n        labels: [\"Q1\", \"Q2\", \"Q3\", \"Q4\"],\n        values: [15, 25, 20, 35]\n    }\n], {\n    x: 1, y: 1, w: 8, h: 4,\n    showCatAxisTitle: true,\n    catAxisTitle: 'Quarter',\n    showValAxisTitle: true,\n    valAxisTitle: 'Revenue ($M)'\n});\n```\n\n### Chart Colors\n\n**CRITICAL**: Use hex colors **without** the `#` prefix - including `#` causes file corruption.\n\n**Align chart colors with your chosen design palette**, ensuring sufficient contrast and distinctiveness for data visualization. Adjust colors for:\n- Strong contrast between adjacent series\n- Readability against slide backgrounds\n- Accessibility (avoid red-green only combinations)\n\n```javascript\n// Example: Ocean palette-inspired chart colors (adjusted for contrast)\nconst chartColors = [\"16A085\", \"FF6B9D\", \"2C3E50\", \"F39C12\", \"9B59B6\"];\n\n// Single-series chart: Use one color for all bars/points\nslide.addChart(pptx.charts.BAR, [{\n    name: \"Sales\",\n    labels: [\"Q1\", \"Q2\", \"Q3\", \"Q4\"],\n    values: [4500, 5500, 6200, 7100]\n}], {\n    ...placeholders[0],\n    chartColors: [\"16A085\"],  // All bars same color\n    showLegend: false\n});\n\n// Multi-series chart: Each series gets a different color\nslide.addChart(pptx.charts.LINE, [\n    { name: \"Product A\", labels: [\"Q1\", \"Q2\", \"Q3\"], values: [10, 20, 30] },\n    { name: \"Product B\", labels: [\"Q1\", \"Q2\", \"Q3\"], values: [15, 25, 20] }\n], {\n    ...placeholders[0],\n    chartColors: [\"16A085\", \"FF6B9D\"]  // One color per series\n});\n```\n\n### Adding Tables\n\nTables can be added with basic or advanced formatting:\n\n#### Basic Table\n\n```javascript\nslide.addTable([\n    [\"Header 1\", \"Header 2\", \"Header 3\"],\n    [\"Row 1, Col 1\", \"Row 1, Col 2\", \"Row 1, Col 3\"],\n    [\"Row 2, Col 1\", \"Row 2, Col 2\", \"Row 2, Col 3\"]\n], {\n    x: 0.5,\n    y: 1,\n    w: 9,\n    h: 3,\n    border: { pt: 1, color: \"999999\" },\n    fill: { color: \"F1F1F1\" }\n});\n```\n\n#### Table with Custom Formatting\n\n```javascript\nconst tableData = [\n    // Header row with custom styling\n    [\n        { text: \"Product\", options: { fill: { color: \"4472C4\" }, color: \"FFFFFF\", bold: true } },\n        { text: \"Revenue\", options: { fill: { color: \"4472C4\" }, color: \"FFFFFF\", bold: true } },\n        { text: \"Growth\", options: { fill: { color: \"4472C4\" }, color: \"FFFFFF\", bold: true } }\n    ],\n    // Data rows\n    [\"Product A\", \"$50M\", \"+15%\"],\n    [\"Product B\", \"$35M\", \"+22%\"],\n    [\"Product C\", \"$28M\", \"+8%\"]\n];\n\nslide.addTable(tableData, {\n    x: 1,\n    y: 1.5,\n    w: 8,\n    h: 3,\n    colW: [3, 2.5, 2.5],  // Column widths\n    rowH: [0.5, 0.6, 0.6, 0.6],  // Row heights\n    border: { pt: 1, color: \"CCCCCC\" },\n    align: \"center\",\n    valign: \"middle\",\n    fontSize: 14\n});\n```\n\n#### Table with Merged Cells\n\n```javascript\nconst mergedTableData = [\n    [\n        { text: \"Q1 Results\", options: { colspan: 3, fill: { color: \"4472C4\" }, color: \"FFFFFF\", bold: true } }\n    ],\n    [\"Product\", \"Sales\", \"Market Share\"],\n    [\"Product A\", \"$25M\", \"35%\"],\n    [\"Product B\", \"$18M\", \"25%\"]\n];\n\nslide.addTable(mergedTableData, {\n    x: 1,\n    y: 1,\n    w: 8,\n    h: 2.5,\n    colW: [3, 2.5, 2.5],\n    border: { pt: 1, color: \"DDDDDD\" }\n});\n```\n\n### Table Options\n\nCommon table options:\n- `x, y, w, h` - Position and size\n- `colW` - Array of column widths (in inches)\n- `rowH` - Array of row heights (in inches)\n- `border` - Border style: `{ pt: 1, color: \"999999\" }`\n- `fill` - Background color (no # prefix)\n- `align` - Text alignment: \"left\", \"center\", \"right\"\n- `valign` - Vertical alignment: \"top\", \"middle\", \"bottom\"\n- `fontSize` - Text size\n- `autoPage` - Auto-create new slides if content overflows",
        "skills/cover_letter_generator/.claude/skills/pptx/ooxml.md": "# Office Open XML Technical Reference for PowerPoint\n\n**Important: Read this entire document before starting.** Critical XML schema rules and formatting requirements are covered throughout. Incorrect implementation can create invalid PPTX files that PowerPoint cannot open.\n\n## Technical Guidelines\n\n### Schema Compliance\n- **Element ordering in `<p:txBody>`**: `<a:bodyPr>`, `<a:lstStyle>`, `<a:p>`\n- **Whitespace**: Add `xml:space='preserve'` to `<a:t>` elements with leading/trailing spaces\n- **Unicode**: Escape characters in ASCII content: `\"` becomes `&#8220;`\n- **Images**: Add to `ppt/media/`, reference in slide XML, set dimensions to fit slide bounds\n- **Relationships**: Update `ppt/slides/_rels/slideN.xml.rels` for each slide's resources\n- **Dirty attribute**: Add `dirty=\"0\"` to `<a:rPr>` and `<a:endParaRPr>` elements to indicate clean state\n\n## Presentation Structure\n\n### Basic Slide Structure\n```xml\n<!-- ppt/slides/slide1.xml -->\n<p:sld>\n  <p:cSld>\n    <p:spTree>\n      <p:nvGrpSpPr>...</p:nvGrpSpPr>\n      <p:grpSpPr>...</p:grpSpPr>\n      <!-- Shapes go here -->\n    </p:spTree>\n  </p:cSld>\n</p:sld>\n```\n\n### Text Box / Shape with Text\n```xml\n<p:sp>\n  <p:nvSpPr>\n    <p:cNvPr id=\"2\" name=\"Title\"/>\n    <p:cNvSpPr>\n      <a:spLocks noGrp=\"1\"/>\n    </p:cNvSpPr>\n    <p:nvPr>\n      <p:ph type=\"ctrTitle\"/>\n    </p:nvPr>\n  </p:nvSpPr>\n  <p:spPr>\n    <a:xfrm>\n      <a:off x=\"838200\" y=\"365125\"/>\n      <a:ext cx=\"7772400\" cy=\"1470025\"/>\n    </a:xfrm>\n  </p:spPr>\n  <p:txBody>\n    <a:bodyPr/>\n    <a:lstStyle/>\n    <a:p>\n      <a:r>\n        <a:t>Slide Title</a:t>\n      </a:r>\n    </a:p>\n  </p:txBody>\n</p:sp>\n```\n\n### Text Formatting\n```xml\n<!-- Bold -->\n<a:r>\n  <a:rPr b=\"1\"/>\n  <a:t>Bold Text</a:t>\n</a:r>\n\n<!-- Italic -->\n<a:r>\n  <a:rPr i=\"1\"/>\n  <a:t>Italic Text</a:t>\n</a:r>\n\n<!-- Underline -->\n<a:r>\n  <a:rPr u=\"sng\"/>\n  <a:t>Underlined</a:t>\n</a:r>\n\n<!-- Highlight -->\n<a:r>\n  <a:rPr>\n    <a:highlight>\n      <a:srgbClr val=\"FFFF00\"/>\n    </a:highlight>\n  </a:rPr>\n  <a:t>Highlighted Text</a:t>\n</a:r>\n\n<!-- Font and Size -->\n<a:r>\n  <a:rPr sz=\"2400\" typeface=\"Arial\">\n    <a:solidFill>\n      <a:srgbClr val=\"FF0000\"/>\n    </a:solidFill>\n  </a:rPr>\n  <a:t>Colored Arial 24pt</a:t>\n</a:r>\n\n<!-- Complete formatting example -->\n<a:r>\n  <a:rPr lang=\"en-US\" sz=\"1400\" b=\"1\" dirty=\"0\">\n    <a:solidFill>\n      <a:srgbClr val=\"FAFAFA\"/>\n    </a:solidFill>\n  </a:rPr>\n  <a:t>Formatted text</a:t>\n</a:r>\n```\n\n### Lists\n```xml\n<!-- Bullet list -->\n<a:p>\n  <a:pPr lvl=\"0\">\n    <a:buChar char=\"\"/>\n  </a:pPr>\n  <a:r>\n    <a:t>First bullet point</a:t>\n  </a:r>\n</a:p>\n\n<!-- Numbered list -->\n<a:p>\n  <a:pPr lvl=\"0\">\n    <a:buAutoNum type=\"arabicPeriod\"/>\n  </a:pPr>\n  <a:r>\n    <a:t>First numbered item</a:t>\n  </a:r>\n</a:p>\n\n<!-- Second level indent -->\n<a:p>\n  <a:pPr lvl=\"1\">\n    <a:buChar char=\"\"/>\n  </a:pPr>\n  <a:r>\n    <a:t>Indented bullet</a:t>\n  </a:r>\n</a:p>\n```\n\n### Shapes\n```xml\n<!-- Rectangle -->\n<p:sp>\n  <p:nvSpPr>\n    <p:cNvPr id=\"3\" name=\"Rectangle\"/>\n    <p:cNvSpPr/>\n    <p:nvPr/>\n  </p:nvSpPr>\n  <p:spPr>\n    <a:xfrm>\n      <a:off x=\"1000000\" y=\"1000000\"/>\n      <a:ext cx=\"3000000\" cy=\"2000000\"/>\n    </a:xfrm>\n    <a:prstGeom prst=\"rect\">\n      <a:avLst/>\n    </a:prstGeom>\n    <a:solidFill>\n      <a:srgbClr val=\"FF0000\"/>\n    </a:solidFill>\n    <a:ln w=\"25400\">\n      <a:solidFill>\n        <a:srgbClr val=\"000000\"/>\n      </a:solidFill>\n    </a:ln>\n  </p:spPr>\n</p:sp>\n\n<!-- Rounded Rectangle -->\n<p:sp>\n  <p:spPr>\n    <a:prstGeom prst=\"roundRect\">\n      <a:avLst/>\n    </a:prstGeom>\n  </p:spPr>\n</p:sp>\n\n<!-- Circle/Ellipse -->\n<p:sp>\n  <p:spPr>\n    <a:prstGeom prst=\"ellipse\">\n      <a:avLst/>\n    </a:prstGeom>\n  </p:spPr>\n</p:sp>\n```\n\n### Images\n```xml\n<p:pic>\n  <p:nvPicPr>\n    <p:cNvPr id=\"4\" name=\"Picture\">\n      <a:hlinkClick r:id=\"\" action=\"ppaction://media\"/>\n    </p:cNvPr>\n    <p:cNvPicPr>\n      <a:picLocks noChangeAspect=\"1\"/>\n    </p:cNvPicPr>\n    <p:nvPr/>\n  </p:nvPicPr>\n  <p:blipFill>\n    <a:blip r:embed=\"rId2\"/>\n    <a:stretch>\n      <a:fillRect/>\n    </a:stretch>\n  </p:blipFill>\n  <p:spPr>\n    <a:xfrm>\n      <a:off x=\"1000000\" y=\"1000000\"/>\n      <a:ext cx=\"3000000\" cy=\"2000000\"/>\n    </a:xfrm>\n    <a:prstGeom prst=\"rect\">\n      <a:avLst/>\n    </a:prstGeom>\n  </p:spPr>\n</p:pic>\n```\n\n### Tables\n```xml\n<p:graphicFrame>\n  <p:nvGraphicFramePr>\n    <p:cNvPr id=\"5\" name=\"Table\"/>\n    <p:cNvGraphicFramePr>\n      <a:graphicFrameLocks noGrp=\"1\"/>\n    </p:cNvGraphicFramePr>\n    <p:nvPr/>\n  </p:nvGraphicFramePr>\n  <p:xfrm>\n    <a:off x=\"1000000\" y=\"1000000\"/>\n    <a:ext cx=\"6000000\" cy=\"2000000\"/>\n  </p:xfrm>\n  <a:graphic>\n    <a:graphicData uri=\"http://schemas.openxmlformats.org/drawingml/2006/table\">\n      <a:tbl>\n        <a:tblGrid>\n          <a:gridCol w=\"3000000\"/>\n          <a:gridCol w=\"3000000\"/>\n        </a:tblGrid>\n        <a:tr h=\"500000\">\n          <a:tc>\n            <a:txBody>\n              <a:bodyPr/>\n              <a:lstStyle/>\n              <a:p>\n                <a:r>\n                  <a:t>Cell 1</a:t>\n                </a:r>\n              </a:p>\n            </a:txBody>\n          </a:tc>\n          <a:tc>\n            <a:txBody>\n              <a:bodyPr/>\n              <a:lstStyle/>\n              <a:p>\n                <a:r>\n                  <a:t>Cell 2</a:t>\n                </a:r>\n              </a:p>\n            </a:txBody>\n          </a:tc>\n        </a:tr>\n      </a:tbl>\n    </a:graphicData>\n  </a:graphic>\n</p:graphicFrame>\n```\n\n### Slide Layouts\n\n```xml\n<!-- Title Slide Layout -->\n<p:sp>\n  <p:nvSpPr>\n    <p:nvPr>\n      <p:ph type=\"ctrTitle\"/>\n    </p:nvPr>\n  </p:nvSpPr>\n  <!-- Title content -->\n</p:sp>\n\n<p:sp>\n  <p:nvSpPr>\n    <p:nvPr>\n      <p:ph type=\"subTitle\" idx=\"1\"/>\n    </p:nvPr>\n  </p:nvSpPr>\n  <!-- Subtitle content -->\n</p:sp>\n\n<!-- Content Slide Layout -->\n<p:sp>\n  <p:nvSpPr>\n    <p:nvPr>\n      <p:ph type=\"title\"/>\n    </p:nvPr>\n  </p:nvSpPr>\n  <!-- Slide title -->\n</p:sp>\n\n<p:sp>\n  <p:nvSpPr>\n    <p:nvPr>\n      <p:ph type=\"body\" idx=\"1\"/>\n    </p:nvPr>\n  </p:nvSpPr>\n  <!-- Content body -->\n</p:sp>\n```\n\n## File Updates\n\nWhen adding content, update these files:\n\n**`ppt/_rels/presentation.xml.rels`:**\n```xml\n<Relationship Id=\"rId1\" Type=\"http://schemas.openxmlformats.org/officeDocument/2006/relationships/slide\" Target=\"slides/slide1.xml\"/>\n<Relationship Id=\"rId2\" Type=\"http://schemas.openxmlformats.org/officeDocument/2006/relationships/slideMaster\" Target=\"slideMasters/slideMaster1.xml\"/>\n```\n\n**`ppt/slides/_rels/slide1.xml.rels`:**\n```xml\n<Relationship Id=\"rId1\" Type=\"http://schemas.openxmlformats.org/officeDocument/2006/relationships/slideLayout\" Target=\"../slideLayouts/slideLayout1.xml\"/>\n<Relationship Id=\"rId2\" Type=\"http://schemas.openxmlformats.org/officeDocument/2006/relationships/image\" Target=\"../media/image1.png\"/>\n```\n\n**`[Content_Types].xml`:**\n```xml\n<Default Extension=\"png\" ContentType=\"image/png\"/>\n<Default Extension=\"jpg\" ContentType=\"image/jpeg\"/>\n<Override PartName=\"/ppt/slides/slide1.xml\" ContentType=\"application/vnd.openxmlformats-officedocument.presentationml.slide+xml\"/>\n```\n\n**`ppt/presentation.xml`:**\n```xml\n<p:sldIdLst>\n  <p:sldId id=\"256\" r:id=\"rId1\"/>\n  <p:sldId id=\"257\" r:id=\"rId2\"/>\n</p:sldIdLst>\n```\n\n**`docProps/app.xml`:** Update slide count and statistics\n```xml\n<Slides>2</Slides>\n<Paragraphs>10</Paragraphs>\n<Words>50</Words>\n```\n\n## Slide Operations\n\n### Adding a New Slide\nWhen adding a slide to the end of the presentation:\n\n1. **Create the slide file** (`ppt/slides/slideN.xml`)\n2. **Update `[Content_Types].xml`**: Add Override for the new slide\n3. **Update `ppt/_rels/presentation.xml.rels`**: Add relationship for the new slide\n4. **Update `ppt/presentation.xml`**: Add slide ID to `<p:sldIdLst>`\n5. **Create slide relationships** (`ppt/slides/_rels/slideN.xml.rels`) if needed\n6. **Update `docProps/app.xml`**: Increment slide count and update statistics (if present)\n\n### Duplicating a Slide\n1. Copy the source slide XML file with a new name\n2. Update all IDs in the new slide to be unique\n3. Follow the \"Adding a New Slide\" steps above\n4. **CRITICAL**: Remove or update any notes slide references in `_rels` files\n5. Remove references to unused media files\n\n### Reordering Slides\n1. **Update `ppt/presentation.xml`**: Reorder `<p:sldId>` elements in `<p:sldIdLst>`\n2. The order of `<p:sldId>` elements determines slide order\n3. Keep slide IDs and relationship IDs unchanged\n\nExample:\n```xml\n<!-- Original order -->\n<p:sldIdLst>\n  <p:sldId id=\"256\" r:id=\"rId2\"/>\n  <p:sldId id=\"257\" r:id=\"rId3\"/>\n  <p:sldId id=\"258\" r:id=\"rId4\"/>\n</p:sldIdLst>\n\n<!-- After moving slide 3 to position 2 -->\n<p:sldIdLst>\n  <p:sldId id=\"256\" r:id=\"rId2\"/>\n  <p:sldId id=\"258\" r:id=\"rId4\"/>\n  <p:sldId id=\"257\" r:id=\"rId3\"/>\n</p:sldIdLst>\n```\n\n### Deleting a Slide\n1. **Remove from `ppt/presentation.xml`**: Delete the `<p:sldId>` entry\n2. **Remove from `ppt/_rels/presentation.xml.rels`**: Delete the relationship\n3. **Remove from `[Content_Types].xml`**: Delete the Override entry\n4. **Delete files**: Remove `ppt/slides/slideN.xml` and `ppt/slides/_rels/slideN.xml.rels`\n5. **Update `docProps/app.xml`**: Decrement slide count and update statistics\n6. **Clean up unused media**: Remove orphaned images from `ppt/media/`\n\nNote: Don't renumber remaining slides - keep their original IDs and filenames.\n\n\n## Common Errors to Avoid\n\n- **Encodings**: Escape unicode characters in ASCII content: `\"` becomes `&#8220;`\n- **Images**: Add to `ppt/media/` and update relationship files\n- **Lists**: Omit bullets from list headers\n- **IDs**: Use valid hexadecimal values for UUIDs\n- **Themes**: Check all themes in `theme` directory for colors\n\n## Validation Checklist for Template-Based Presentations\n\n### Before Packing, Always:\n- **Clean unused resources**: Remove unreferenced media, fonts, and notes directories\n- **Fix Content_Types.xml**: Declare ALL slides, layouts, and themes present in the package\n- **Fix relationship IDs**: \n   - Remove font embed references if not using embedded fonts\n- **Remove broken references**: Check all `_rels` files for references to deleted resources\n\n### Common Template Duplication Pitfalls:\n- Multiple slides referencing the same notes slide after duplication\n- Image/media references from template slides that no longer exist\n- Font embedding references when fonts aren't included\n- Missing slideLayout declarations for layouts 12-25\n- docProps directory may not unpack - this is optional",
        "skills/cover_letter_generator/.claude/skills/skill-creator-pro/SKILL.md": "---\nname: skill-creator-pro\ndescription: |\n  Creates production-grade, reusable skills that extend Claude's capabilities.\n  This skill should be used when users want to create a new skill, improve an\n  existing skill, or build domain-specific intelligence. Gathers context from\n  codebase, conversation, and authentic sources before creating adaptable skills.\n---\n\n# Skill Creator Pro\n\nCreate production-grade skills that extend Claude's capabilities.\n\n## How This Skill Works\n\n```\nUser: \"Create a skill for X\"\n       \nClaude Code uses this meta-skill as guidance\n       \nFollow Domain Discovery  Ask user clarifying questions  Create skill\n       \nGenerated skill with embedded domain expertise\n```\n\nThis skill provides **guidance and structure** for creating skills. Claude Code:\n1. Uses this skill's framework to discover domain knowledge\n2. Asks user for clarifications about THEIR specific requirements\n3. Decides how to structure the generated skill based on domain needs\n\n## What This Skill Does\n\n- Guides creation of new skills from scratch\n- Helps improve existing skills to production quality\n- Provides patterns for 5 skill types (Builder, Guide, Automation, Analyzer, Validator)\n- Ensures skills encode procedural knowledge + domain expertise\n\n## What This Skill Does NOT Do\n\n- Test skills in production environments\n- Deploy or distribute skills\n- Handle skill versioning/updates after creation\n- Create requirement-specific skills (always create reusable intelligence)\n\n---\n\n## Domain Discovery Framework\n\n**Key Principle**: Users want domain expertise IN the skill. They may not BE domain experts.\n\n### Phase 1: Automatic Discovery (No User Input)\n\nProactively research the domain before asking anything:\n\n| Discover | How | Example: \"Kafka integration\" |\n|----------|-----|------------------------------|\n| Core concepts | Official docs, Context7 | Producers, consumers, topics, partitions |\n| Standards/compliance | Search \"[domain] standards\" | Kafka security, exactly-once semantics |\n| Best practices | Search \"[domain] best practices 2025\" | Partitioning strategies, consumer groups |\n| Anti-patterns | Search \"[domain] common mistakes\" | Too many partitions, no monitoring |\n| Security | Search \"[domain] security\" | SASL, SSL, ACLs, encryption |\n| Ecosystem | Search \"[domain] ecosystem tools\" | Confluent, Schema Registry, Connect |\n\n**Sources priority**: Official docs  Library docs (Context7)  GitHub  Community  WebSearch\n\n### Phase 2: Knowledge Sufficiency Check\n\nBefore asking user anything, verify internally:\n\n```\n- [ ] Core concepts understood?\n- [ ] Best practices identified?\n- [ ] Anti-patterns known?\n- [ ] Security considerations covered?\n- [ ] Official sources found?\n\nIf ANY gap  Research more (don't ask user for domain knowledge)\nOnly if CANNOT discover (proprietary/internal)  Ask user\n```\n\n### Phase 3: User Requirements (NOT Domain Knowledge)\n\nOnly ask about user's SPECIFIC context:\n\n| Ask | Don't Ask |\n|-----|-----------|\n| \"What's YOUR use case?\" | \"What is Kafka?\" |\n| \"What's YOUR tech stack?\" | \"What options exist?\" |\n| \"Any existing resources?\" | \"How does it work?\" |\n| \"Specific constraints?\" | \"What are best practices?\" |\n\nThe skill contains domain expertise. User provides requirements.\n\n---\n\n## Required Clarifications\n\nAsk about SKILL METADATA and USER REQUIREMENTS (not domain knowledge):\n\n### Skill Metadata\n\n**1. Skill Type** - \"What type of skill?\"\n\n| Type | Purpose | Example |\n|------|---------|---------|\n| **Builder** | Create artifacts | Widgets, code, documents |\n| **Guide** | Provide instructions | How-to, tutorials |\n| **Automation** | Execute workflows | File processing, deployments |\n| **Analyzer** | Extract insights | Code review, data analysis |\n| **Validator** | Enforce quality | Compliance checks, scoring |\n\n**2. Domain** - \"What domain or technology?\"\n\n### User Requirements (After Domain Discovery)\n\n**3. Use Case** - \"What's YOUR specific use case?\"\n- Not \"what can it do\" but \"what do YOU need\"\n\n**4. Tech Stack** - \"What's YOUR environment?\"\n- Languages, frameworks, existing infrastructure\n\n**5. Existing Resources** - \"Any scripts, templates, configs to include?\"\n\n**6. Constraints** - \"Any specific requirements or limitations?\"\n- Performance, security, compliance specific to user's context\n\n### Note\n- Questions 1-2: Ask immediately\n- Domain Discovery: Research automatically after knowing domain\n- Questions 3-6: Ask after discovery, informed by domain knowledge\n- **Question pacing**: Avoid asking too many questions in a single message. Start with most important, follow up as needed.\n\n---\n\n## Core Principles\n\n### Reusable Intelligence, Not Requirement-Specific\n\nSkills must handle VARIATIONS, not single requirements:\n\n```\n Bad: \"Create bar chart with sales data using Recharts\"\n Good: \"Create visualizations - adaptable to data shape, chart type, library\"\n\n Bad: \"Deploy to AWS EKS with Helm\"\n Good: \"Deploy applications - adaptable to platform, orchestration, environment\"\n```\n\nIdentify what VARIES vs what's CONSTANT in the domain. See `references/reusability-patterns.md`.\n\n### Concise is Key\n\nContext window is a public good (~1,500+ tokens per skill activation). Challenge each piece:\n- \"Does Claude really need this explanation?\"\n- \"Does this paragraph justify its token cost?\"\n\nPrefer concise examples over verbose explanations.\n\n### Appropriate Freedom\n\nMatch specificity to task fragility:\n\n| Freedom Level | When to Use | Example |\n|---------------|-------------|---------|\n| **High** | Multiple approaches valid | \"Choose your preferred style\" |\n| **Medium** | Preferred pattern exists | Pseudocode with parameters |\n| **Low** | Operations are fragile | Exact scripts, few parameters |\n\n### Progressive Disclosure\n\nThree-level loading system:\n\n1. **Metadata** (~100 tokens) - Always in context (description 1024 chars)\n2. **SKILL.md body** (<500 lines) - When skill triggers\n3. **References** (unlimited) - Loaded as needed by Claude\n\n---\n\n## Anatomy of a Skill\n\nGenerated skills are **zero-shot domain experts** with embedded knowledge.\n\n```\nskill-name/\n SKILL.md (required)\n    YAML frontmatter (name, description, allowed-tools?, model?)\n    Procedural knowledge (workflows, steps, decision trees)\n Bundled Resources\n     references/   - Domain expertise (structure based on domain needs)\n     scripts/      - Executable code (tested, reliable)\n     assets/       - Templates, boilerplate, images\n```\n\n### SKILL.md Requirements\n\n| Component | Requirement |\n|-----------|-------------|\n| Line count | <500 lines (extract to references/) |\n| Frontmatter | See `references/skill-patterns.md` for complete spec |\n| `name` | Lowercase, numbers, hyphens; 64 chars; match directory |\n| `description` | [What] + [When]; 1024 chars; third-person style |\n| Description style | \"This skill should be used when...\" (not \"Use when...\") |\n| Form | Imperative (\"Do X\" not \"You should X\") |\n| Scope | What it does AND does not do |\n\n### What Goes in references/\n\nEmbed domain knowledge gathered during discovery:\n\n| Gathered Knowledge | Purpose in Skill |\n|--------------------|------------------|\n| Library/API documentation | Enable correct implementation |\n| Best practices | Guide quality decisions |\n| Code examples | Provide reference patterns |\n| Anti-patterns | Prevent common mistakes |\n| Domain-specific details | Support edge cases |\n\n**Structure references/ based on what the domain needs.**\n\n**Large files**: If references >10k words, include grep search patterns in SKILL.md for efficient discovery.\n\n### When to Generate scripts/\n\nGenerate scripts when domain requires **deterministic, executable procedures**:\n\n| Domain Need | Example Scripts |\n|-------------|-----------------|\n| Setup/installation | Install dependencies, initialize project |\n| Processing | Transform data, process files |\n| Validation | Check compliance, verify output |\n| Deployment | Deploy services, configure infrastructure |\n\n**Decision**: If procedure is complex, error-prone, or needs to be exactly repeatable  create script. Otherwise  document in SKILL.md or references/.\n\n### When to Generate assets/\n\nGenerate assets when domain requires **exact templates or boilerplate**:\n\n| Domain Need | Example Assets |\n|-------------|----------------|\n| Starting templates | HTML boilerplate, component scaffolds |\n| Configuration files | Config templates, schema definitions |\n| Code boilerplate | Base classes, starter code |\n\n### What NOT to Include\n\n- README.md (SKILL.md IS the readme)\n- CHANGELOG.md\n- LICENSE (inherited from repo)\n- Duplicate information\n\n### What Generated Skill Does at Runtime\n\n```\nUser invokes skill  Gather context from:\n  1. Codebase (if existing project)\n  2. Conversation (user's requirements)\n  3. Own references/ (embedded domain expertise)\n  4. User-specific guidelines\n Ensure all information gathered  Implement ZERO-SHOT\n```\n\n### Include in Generated Skills\n\nEvery generated skill should include:\n\n```markdown\n## Before Implementation\n\nGather context to ensure successful implementation:\n\n| Source | Gather |\n|--------|--------|\n| **Codebase** | Existing structure, patterns, conventions to integrate with |\n| **Conversation** | User's specific requirements, constraints, preferences |\n| **Skill References** | Domain patterns from `references/` (library docs, best practices, examples) |\n| **User Guidelines** | Project-specific conventions, team standards |\n\nEnsure all required context is gathered before implementing.\nOnly ask user for THEIR specific requirements (domain expertise is in this skill).\n```\n\n---\n\n## Type-Aware Creation\n\nAfter determining skill type, follow type-specific patterns:\n\n| Type | Key Sections | Reference |\n|------|--------------|-----------|\n| **Builder** | Clarifications  Output Spec  Standards  Checklist | `skill-patterns.md#builder` |\n| **Guide** | Workflow  Examples  Official Docs | `skill-patterns.md#guide` |\n| **Automation** | Scripts  Dependencies  Error Handling | `skill-patterns.md#automation` |\n| **Analyzer** | Scope  Criteria  Output Format | `skill-patterns.md#analyzer` |\n| **Validator** | Criteria  Scoring  Thresholds  Remediation | `skill-patterns.md#validator` |\n\n---\n\n## Skill Creation Process\n\n```\nMetadata  Discovery  Requirements  Analyze  Embed  Structure  Implement  Validate\n```\n\nSee `references/creation-workflow.md` for detailed steps.\n\n### Quick Steps\n\n1. **Metadata**: Ask skill type + domain (Questions 1-2)\n2. **Discovery**: Research domain automatically (Phase 1-2 above)\n3. **Requirements**: Ask user's specific needs (Questions 3-6)\n4. **Analyze**: Identify procedural (HOW) + domain (WHAT) knowledge\n5. **Embed**: Put gathered domain expertise into `references/`\n6. **Structure**: Initialize skill directory\n7. **Implement**: Write SKILL.md + resources following type patterns\n8. **Validate**: Run `scripts/package_skill.py` and test\n\n### SKILL.md Template\n\n```yaml\n---\nname: skill-name                    # lowercase, hyphens, 64 chars\ndescription: |                      # 1024 chars\n  [What] Capability statement.\n  [When] Use when users ask to <triggers>.\nallowed-tools: Read, Grep, Glob     # optional: restrict tools\n---\n```\n\nSee `references/skill-patterns.md` for complete frontmatter spec and body patterns.\n\n---\n\n## Output Checklist\n\nBefore delivering a skill, verify:\n\n### Domain Discovery Complete\n- [ ] Core concepts discovered and understood\n- [ ] Best practices identified from authentic sources\n- [ ] Anti-patterns documented\n- [ ] Security considerations covered\n- [ ] Official documentation linked\n- [ ] User was NOT asked for domain knowledge\n\n### Frontmatter\n- [ ] `name`: lowercase, hyphens, 64 chars, matches directory\n- [ ] `description`: [What]+[When], 1024 chars, clear triggers\n- [ ] `allowed-tools`: Set if restricted access needed\n\n### Structure\n- [ ] SKILL.md <500 lines\n- [ ] Progressive disclosure (details in references/)\n\n### Knowledge Coverage\n- [ ] **Procedural** (HOW): Workflows, decision trees, error handling\n- [ ] **Domain** (WHAT): Concepts, best practices, anti-patterns\n\n### Zero-Shot Implementation (in generated skill)\n- [ ] Includes \"Before Implementation\" section\n- [ ] Gathers runtime context (codebase, conversation, user guidelines)\n- [ ] Domain expertise embedded in `references/` (structured per domain needs)\n- [ ] Only asks user for THEIR requirements (not domain knowledge)\n\n### Reusability\n- [ ] Handles variations (not requirement-specific)\n- [ ] Clarifications capture variable elements (user's context)\n- [ ] Constants encoded (domain patterns, best practices)\n\n### Type-Specific (see `references/skill-patterns.md`)\n- [ ] Builder: Clarifications, output spec, standards, checklist\n- [ ] Guide: Workflow, examples, official docs\n- [ ] Automation: Scripts, dependencies, error handling\n- [ ] Analyzer: Scope, criteria, output format\n- [ ] Validator: Criteria, scoring, thresholds, remediation\n\n---\n\n## Reference Files\n| File | When to Read |\n|------|--------------|\n| `references/creation-workflow.md` | Detailed step-by-step creation process |\n| `references/skill-patterns.md` | Frontmatter spec, type-specific patterns, assets guidance |\n| `references/reusability-patterns.md` | Procedural+domain knowledge, varies vs constant |\n| `references/quality-patterns.md` | Clarifications, enforcement, checklists |\n| `references/technical-patterns.md` | Error handling, security, dependencies |\n| `references/workflows.md` | Sequential and conditional workflow patterns |\n| `references/output-patterns.md` | Template and example patterns |\n",
        "skills/cover_letter_generator/.claude/skills/skill-creator-pro/references/creation-workflow.md": "# Skill Creation Workflow\n\nDetailed step-by-step process for creating production-grade skills.\n\n---\n\n## Overview\n\n```\nMetadata  Discovery  Requirements  Analyze  Embed  Structure  Implement  Validate\n```\n\n**Key Principle**: Users want domain expertise IN the skill. They may not BE domain experts.\n\n---\n\n## Step 1: Get Skill Metadata\n\nAsk ONLY these two questions first:\n\n| Question | Purpose |\n|----------|---------|\n| **Skill type?** | Route to appropriate patterns |\n| **Domain/technology?** | Focus domain discovery |\n\n**Do NOT ask**: Usage examples, tech stack, constraints (these come AFTER discovery)\n\n**Exit criteria**: Clear skill type + domain identified\n\n---\n\n## Step 2: Domain Discovery (Automatic)\n\nResearch the domain BEFORE asking user anything else.\n\n### What to Discover\n\n| Knowledge Area | Search Strategy | Example: \"Kafka\" |\n|----------------|-----------------|------------------|\n| Core concepts | Official docs, Context7 | Topics, partitions, consumers, producers |\n| Standards/compliance | \"[domain] standards compliance\" | Exactly-once semantics, ordering guarantees |\n| Best practices | \"[domain] best practices 2025\" | Partition strategies, consumer groups |\n| Anti-patterns | \"[domain] common mistakes pitfalls\" | Over-partitioning, no dead letter queue |\n| Security | \"[domain] security requirements\" | SASL, SSL, ACLs, encryption at rest |\n| Ecosystem | \"[domain] ecosystem tools\" | Schema Registry, Kafka Connect, ksqlDB |\n| Official sources | \"[domain] official documentation\" | kafka.apache.org, Confluent docs |\n\n### Source Priority\n\n1. **Official documentation** - Always authoritative\n2. **Library docs (Context7)** - Structured, current\n3. **GitHub** - Real implementations, discussions\n4. **Community** - Stack Overflow, forums (verify accuracy)\n5. **WebSearch** - Last resort, cross-reference\n\n### Knowledge Sufficiency Check\n\nBefore proceeding, verify internally:\n\n```\n- [ ] Core concepts: Can I explain the fundamentals?\n- [ ] Best practices: Do I know the recommended approaches?\n- [ ] Anti-patterns: Do I know what to avoid?\n- [ ] Security: Do I know the security considerations?\n- [ ] Ecosystem: Do I know the related tools/options?\n- [ ] Official sources: Do I have authoritative references?\n\nIf ANY is incomplete  Research more\nOnly if CANNOT discover (proprietary/internal)  Ask user\n```\n\n**Exit criteria**: Sufficient domain knowledge to create expert-level skill\n\n---\n\n## Step 3: Get User Requirements\n\nNOW ask about user's SPECIFIC context (not domain knowledge):\n\n| Ask | Purpose | NOT This |\n|-----|---------|----------|\n| \"What's YOUR use case?\" | Understand their need | \"What can [domain] do?\" |\n| \"What's YOUR tech stack?\" | Know their environment | \"What options exist?\" |\n| \"Existing resources?\" | Include their scripts/templates | \"What tools are available?\" |\n| \"Specific constraints?\" | Their limitations | \"What are best practices?\" |\n\n### Good vs Bad Questions\n\n```\n \"What's your use case - event sourcing, messaging, or streaming?\"\n   (Informed by discovery, asking about THEIR need)\n\n \"What is Kafka used for?\"\n   (Asking user for domain knowledge we should discover)\n\n \"Are you using Java, Python, or another language?\"\n   (Asking about THEIR environment)\n\n \"What languages does Kafka support?\"\n   (Asking user for domain knowledge we should discover)\n```\n\n**Exit criteria**: Clear user requirements + existing resources identified\n\n---\n\n## Step 4: Analyze Domain\n\nCombine discovered knowledge + user requirements:\n\n### Procedural Knowledge (HOW)\n- Step-by-step workflows\n- Decision trees and branching logic\n- Error handling sequences\n- Validation procedures\n\n### Domain Expertise (WHAT)\n- Core concepts and terminology\n- Best practices and patterns\n- Anti-patterns to avoid\n- Standards and compliance requirements\n\n### Variability Analysis\n| What VARIES | What's CONSTANT |\n|-------------|-----------------|\n| (User-specific inputs) | (Domain patterns) |\n| (Tool/library choices) | (Best practices) |\n| (Configuration options) | (Error handling) |\n\nSee `reusability-patterns.md` for detailed guidance.\n\n---\n\n## Step 5: Embed Domain Knowledge\n\n**Critical**: Take the knowledge gathered in Step 2 and EMBED it into the skill.\n\n### What Goes in references/\n\n| Gathered Knowledge | Purpose in Generated Skill |\n|--------------------|---------------------------|\n| Library/API documentation | Enable correct implementation |\n| Best practices | Guide quality decisions |\n| Code examples | Provide reference patterns |\n| Anti-patterns | Prevent common mistakes |\n| Domain-specific details | Support edge cases |\n\n**Structure `references/` based on what the domain needs** - file organization depends on the gathered knowledge and skill's purpose.\n\n### When to Generate scripts/\n\nGenerate scripts when the domain requires **deterministic, executable procedures**:\n\n| Domain Need | Example Scripts |\n|-------------|-----------------|\n| Setup/installation | Install dependencies, initialize project |\n| Processing | Transform data, process files |\n| Validation | Check compliance, verify output |\n| Deployment | Deploy services, configure infrastructure |\n| Automation | Batch operations, scheduled tasks |\n\n**Decision**: If a procedure is complex, error-prone, or needs to be exactly repeatable  create a script. Otherwise  document in SKILL.md or references/.\n\n### When to Generate assets/\n\nGenerate assets when the domain requires **exact templates or boilerplate**:\n\n| Domain Need | Example Assets |\n|-------------|----------------|\n| Starting templates | HTML boilerplate, component scaffolds |\n| Configuration files | Config templates, schema definitions |\n| Code boilerplate | Base classes, starter code |\n\n### Why This Matters\n\nGenerated skills are **zero-shot domain experts**. The expertise gathered in Step 2 must be embedded so the skill can implement without runtime discovery.\n\n---\n\n## Step 6: Initialize Structure\n\n```bash\nscripts/init_skill.py <skill-name> --path <output-directory>\n```\n\nOr manually create:\n```\nskill-name/\n SKILL.md\n references/\n scripts/\n assets/\n```\n\n---\n\n## Step 7: Implement by Type\n\n### Builder Skills\n1. Write Required Clarifications (critical questions before building)\n2. Define output specification (what artifact looks like)\n3. Add domain standards enforcement (Must Follow/Must Avoid)\n4. Create templates in assets/\n5. Add output checklist\n\n**Key sections**:\n- Required Clarifications\n- Output Specification\n- Domain Standards\n- Output Checklist\n\n### Guide Skills\n1. Write step-by-step workflow\n2. Add good/bad examples\n3. Link to official documentation\n4. Create decision trees for branching paths\n\n**Key sections**:\n- Workflow Steps\n- Examples (Good/Bad)\n- Official Documentation table\n- When to Use / When NOT to Use\n\n### Automation Skills\n1. Create tested scripts in scripts/\n2. Document dependencies\n3. Add error handling guidance\n4. Define input/output contracts\n\n**Key sections**:\n- Available Scripts table\n- Dependencies\n- Error Handling\n- Input/Output Specification\n\n### Analyzer Skills\n1. Define analysis scope and criteria\n2. Create extraction/review patterns\n3. Add output format specification\n4. Include validation rules\n\n**Key sections**:\n- Analysis Scope\n- Evaluation Criteria\n- Output Format\n- Validation Rules\n\n### Validator Skills\n1. Define quality criteria\n2. Create scoring/evaluation rubrics\n3. Add pass/fail thresholds\n4. Include remediation guidance\n\n**Key sections**:\n- Quality Criteria\n- Scoring Rubric\n- Thresholds\n- Remediation Patterns\n\n---\n\n## Step 8: Write SKILL.md\n\nFollow this structure (includes Before Implementation for zero-shot execution):\n\n```markdown\n---\nname: skill-name\ndescription: |\n  [What] Capability statement.\n  [When] Use when users ask to <triggers>.\nallowed-tools: Tool1, Tool2    # if restricted\n---\n\n# Skill Name\n\nBrief description.\n\n## What This Skill Does\n- Capability 1\n- Capability 2\n\n## What This Skill Does NOT Do\n- Exclusion 1\n- Exclusion 2\n\n---\n\n## Before Implementation\n\nGather context to ensure successful implementation:\n\n| Source | Gather |\n|--------|--------|\n| **Codebase** | Existing structure, patterns, conventions to integrate with |\n| **Conversation** | User's specific requirements, constraints, preferences |\n| **Skill References** | Domain patterns from `references/` (library docs, best practices, examples) |\n| **User Guidelines** | Project-specific conventions, team standards |\n\nEnsure all required context is gathered before implementing.\nOnly ask user for THEIR specific requirements (domain expertise is in this skill).\n\n---\n\n## [Type-specific sections]\n(See Step 6 for sections by type)\n\n## Reference Files\n| File | When to Read |\n|------|--------------|\n| `references/file.md` | When X |\n\n## Output Checklist\n- [ ] Requirement 1\n- [ ] Requirement 2\n```\n\n**CRITICAL**: Generated skills are zero-shot domain experts. The \"Before Implementation\" section ensures they gather runtime context (codebase, user requirements) and apply their embedded domain expertise (from `references/`) to implement successfully in a single interaction.\n\n---\n\n## Step 9: Validate\n\nRun validation before delivery:\n\n```bash\nscripts/package_skill.py <path/to/skill-folder>\n```\n\n### Manual Validation Checklist\n\n**Frontmatter**:\n- [ ] name: lowercase, hyphens, 64 chars, matches directory\n- [ ] description: [What]+[When], 1024 chars, clear triggers\n\n**Structure**:\n- [ ] SKILL.md <500 lines\n- [ ] No README.md, CHANGELOG.md\n- [ ] Progressive disclosure to references/\n\n**Reusability**:\n- [ ] Handles variations (not requirement-specific)\n- [ ] Clarifications capture variable elements\n- [ ] Encodes constant patterns\n\n**Zero-Shot Implementation**:\n- [ ] Includes \"Before Implementation\" section\n- [ ] Gathers runtime context (codebase, conversation, user guidelines)\n- [ ] Domain expertise embedded in `references/` (library docs, best practices, examples)\n- [ ] Only asks user for THEIR requirements (not domain knowledge)\n\n**Knowledge**:\n- [ ] Procedural knowledge (HOW) documented\n- [ ] Domain expertise (WHAT) captured\n- [ ] Official sources referenced\n\n---\n\n## Step 10: Iterate\n\n1. Use skill on real tasks\n2. Notice gaps or issues\n3. Update and revalidate\n4. Repeat until production-ready\n",
        "skills/cover_letter_generator/.claude/skills/skill-creator-pro/references/output-patterns.md": "# Output Patterns\n\nTemplate patterns and example-driven guidance for consistent output.\n\n---\n\n## Template Pattern\n\nProvide templates for output format. Match strictness to requirements.\n\n### Strict Template (Exact Format Required)\n\n```markdown\n## Report Structure\n\nALWAYS use this exact template:\n\n```markdown\n# [Analysis Title]\n\n## Executive Summary\n[One-paragraph overview of key findings - 3-5 sentences]\n\n## Key Findings\n1. **Finding 1**: [Description with supporting data]\n2. **Finding 2**: [Description with supporting data]\n3. **Finding 3**: [Description with supporting data]\n\n## Recommendations\n1. [Specific actionable recommendation]\n2. [Specific actionable recommendation]\n3. [Specific actionable recommendation]\n\n## Appendix\n[Supporting data, charts, or detailed analysis]\n```\n```\n\n### Flexible Template (Guidance with Room for Adaptation)\n\n```markdown\n## Report Structure\n\nDefault format (adapt sections as needed):\n\n```markdown\n# [Analysis Title]\n\n## Executive Summary\n[Overview - adjust length based on complexity]\n\n## Key Findings\n[Organize by theme, priority, or chronology as appropriate]\n\n## Recommendations\n[Tailor to specific context and audience]\n```\n\nAdjust sections based on:\n- Audience (technical vs executive)\n- Complexity (simple vs comprehensive)\n- Purpose (decision support vs documentation)\n```\n\n---\n\n## Examples Pattern\n\nShow input/output pairs for quality guidance.\n\n### Code Generation Examples\n\n```markdown\n## Function Naming\n\n**Example 1:**\nInput: Function that validates email format\nOutput:\n```typescript\nfunction validateEmail(email: string): boolean {\n  const pattern = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n  return pattern.test(email);\n}\n```\n\n**Example 2:**\nInput: Function that fetches user by ID\nOutput:\n```typescript\nasync function getUserById(id: string): Promise<User | null> {\n  const response = await db.users.findUnique({ where: { id } });\n  return response;\n}\n```\n\nFollow this style: verb + noun, camelCase, explicit return types.\n```\n\n### Commit Message Examples\n\n```markdown\n## Commit Message Format\n\n**Example 1:**\nInput: Added user authentication with JWT tokens\nOutput:\n```\nfeat(auth): implement JWT-based authentication\n\nAdd login endpoint and token validation middleware.\nIncludes refresh token rotation for security.\n```\n\n**Example 2:**\nInput: Fixed bug where dates displayed incorrectly\nOutput:\n```\nfix(reports): correct date formatting in timezone conversion\n\nUse UTC timestamps consistently across report generation.\nFixes #234.\n```\n\nFormat: `type(scope): brief description`\n\nTypes: feat, fix, docs, style, refactor, test, chore\n```\n\n---\n\n## Good/Bad Examples Pattern\n\nShow correct and incorrect patterns with explanations.\n\n### Error Handling\n\n```markdown\n## Error Handling Examples\n\n### Good Example\n```typescript\nasync function fetchUser(id: string): Promise<Result<User>> {\n  try {\n    const user = await api.getUser(id);\n    if (!user) {\n      return { ok: false, error: 'User not found' };\n    }\n    return { ok: true, data: user };\n  } catch (error) {\n    logger.error('Failed to fetch user', { id, error });\n    return { ok: false, error: 'Failed to fetch user' };\n  }\n}\n```\n\n**Why it's good:**\n- Explicit error handling\n- Logging for debugging\n- Typed return value\n- User-friendly error message\n\n### Bad Example (Don't Do This)\n```typescript\nasync function fetchUser(id: string) {\n  const user = await api.getUser(id);  // Unhandled rejection\n  return user;  // Returns undefined if not found\n}\n```\n\n**Why it's bad:**\n- No error handling\n- Caller can't distinguish \"not found\" from error\n- No logging for debugging\n```\n\n### Input Validation\n\n```markdown\n## Input Validation Examples\n\n### Good Example\n```typescript\nconst CreateUserSchema = z.object({\n  name: z.string().min(1, 'Name required').max(100),\n  email: z.string().email('Invalid email format'),\n  age: z.number().int().min(0).max(150).optional(),\n});\n\nfunction createUser(input: unknown) {\n  const result = CreateUserSchema.safeParse(input);\n  if (!result.success) {\n    return { error: result.error.format() };\n  }\n  return saveUser(result.data);\n}\n```\n\n### Bad Example (Don't Do This)\n```typescript\nfunction createUser(input: any) {\n  // No validation - vulnerable to injection, crashes\n  return saveUser(input);\n}\n```\n```\n\n---\n\n## Output Specification Pattern\n\nDefine expected output characteristics.\n\n### Widget Output Specification\n\n```markdown\n## Widget Output Specification\n\nEvery generated widget includes:\n\n### Required Elements\n- [ ] Container with theme-aware styles\n- [ ] Data binding to window.openai.toolOutput\n- [ ] Loading state (shown before data arrives)\n- [ ] Error state (shown when data.isError)\n- [ ] Empty state (shown when no data)\n\n### Code Structure\n```typescript\n// Required structure\nfunction Widget() {\n  const data = window.openai?.toolOutput;\n  const theme = window.openai?.theme ?? 'light';\n\n  if (!data) return <LoadingState />;\n  if (data.isError) return <ErrorState message={data.error} />;\n  if (isEmpty(data)) return <EmptyState />;\n\n  return <MainContent data={data} theme={theme} />;\n}\n```\n\n### Style Requirements\n- System fonts only (no custom fonts)\n- Theme-aware colors (light/dark)\n- Responsive breakpoints (mobile + desktop)\n- WCAG AA contrast ratios\n```\n\n### API Response Specification\n\n```markdown\n## API Response Specification\n\nAll responses follow this structure:\n\n### Success Response\n```json\n{\n  \"success\": true,\n  \"data\": { ... },\n  \"meta\": {\n    \"timestamp\": \"ISO-8601\",\n    \"requestId\": \"uuid\"\n  }\n}\n```\n\n### Error Response\n```json\n{\n  \"success\": false,\n  \"error\": {\n    \"code\": \"ERROR_CODE\",\n    \"message\": \"Human-readable message\",\n    \"details\": { ... }\n  },\n  \"meta\": {\n    \"timestamp\": \"ISO-8601\",\n    \"requestId\": \"uuid\"\n  }\n}\n```\n\n### Pagination\n```json\n{\n  \"success\": true,\n  \"data\": [...],\n  \"pagination\": {\n    \"page\": 1,\n    \"pageSize\": 20,\n    \"total\": 100,\n    \"hasMore\": true\n  }\n}\n```\n```\n\n---\n\n## Consistency Patterns\n\n### Naming Conventions\n\n```markdown\n## Naming Conventions\n\n| Type | Convention | Example |\n|------|------------|---------|\n| Files | kebab-case | `user-service.ts` |\n| Functions | camelCase | `getUserById` |\n| Classes | PascalCase | `UserService` |\n| Constants | UPPER_SNAKE | `MAX_RETRIES` |\n| Types | PascalCase | `UserResponse` |\n| CSS classes | kebab-case | `user-card` |\n```\n\n### File Structure\n\n```markdown\n## File Structure Convention\n\n```\nsrc/\n components/       # React components\n    Button/\n       Button.tsx\n       Button.test.tsx\n       index.ts\n hooks/           # Custom hooks\n services/        # API/business logic\n utils/           # Helper functions\n types/           # TypeScript types\n index.ts         # Public exports\n```\n```\n",
        "skills/cover_letter_generator/.claude/skills/skill-creator-pro/references/quality-patterns.md": "# Quality Patterns\n\nPatterns for clarifications, enforcement checklists, and quality gates.\n\n---\n\n## User Interaction Patterns\n\n### Clarification Questions\n\nStructure clarifications as Required vs Optional:\n\n```markdown\n## Required Clarifications\n\nBefore proceeding, ask:\n\n1. **Data shape**: \"What structure will the input have?\"\n   ```json\n   Example: { items: [...], total: 10 }\n   ```\n\n2. **Action type**: \"Read-only or write operations?\"\n   - Read  Simple rendering\n   - Write  Need form handling, validation\n\n3. **Output format**: \"What should the result look like?\"\n\n## Optional Clarifications\n\n4. **Styling**: \"Any design preferences?\" (ask if complex UI)\n5. **Performance**: \"Expected data volume?\" (ask if potentially large)\n```\n\n### Context Awareness\n\nBefore asking, check existing context:\n\n```markdown\n## Before Asking\n\n1. Review conversation history for prior answers\n2. Infer from file names/content when possible\n3. Check available data structures\n4. Only ask what cannot be determined\n\nExample: If user says \"create a progress widget for my course app\",\ninfer it's a Builder skill for education domain before asking.\n```\n\n### Graceful Handling\n\nWhat to do when user doesn't answer:\n\n```markdown\n## If User Skips Clarifications\n\n- **Required questions**: Explain why needed, ask again simply\n- **Optional questions**: Proceed with sensible defaults\n- **Ambiguous answers**: Confirm interpretation before proceeding\n```\n\n---\n\n## Official Documentation Links\n\n### Documentation Table Pattern\n\n```markdown\n## Official Documentation\n\n| Resource | URL | Use For |\n|----------|-----|---------|\n| Getting Started | https://docs.example.com/start | Basic setup |\n| API Reference | https://docs.example.com/api | Method details |\n| Best Practices | https://docs.example.com/best | Patterns |\n| Examples | https://github.com/example/examples | Reference code |\n\nFor patterns not covered here, fetch from official docs.\n```\n\n### Fetch Guidance Pattern\n\n```markdown\n## Unlisted Scenarios\n\nFor patterns not documented in this skill:\n\n1. Fetch from official docs: [primary URL]\n2. Apply same quality standards as this skill\n3. Follow established patterns\n\nExamples of when to fetch:\n- Complex authentication flows\n- Third-party integrations\n- Platform-specific features\n- New API versions\n```\n\n### Version Awareness\n\n```markdown\n## Keeping Current\n\n- Official docs: [URL]\n- Changelog: [URL]\n- Last verified: 2024-12\n\nWhen docs update:\n1. Check for breaking changes\n2. Update affected references\n3. Verify examples still work\n```\n\n---\n\n## Domain Standards Enforcement\n\n### Must Follow / Must Avoid Pattern\n\n```markdown\n## Standards Enforcement\n\n### Must Follow\n- [ ] WCAG AA contrast (4.5:1 for text, 3:1 for UI)\n- [ ] Keyboard navigation for all interactions\n- [ ] Focus indicators visible\n- [ ] Screen reader labels for icons\n- [ ] Error states with clear messages\n\n### Must Avoid\n- Color as only indicator\n- Mouse-only interactions\n- Auto-playing media without controls\n- Nested scrolling\n- Hardcoded strings\n```\n\n### Domain-Specific Examples\n\n**Web Accessibility (WCAG)**:\n```markdown\n### Must Follow\n- [ ] 4.5:1 contrast ratio for text\n- [ ] 3:1 contrast ratio for UI components\n- [ ] Keyboard accessible\n- [ ] Focus visible\n- [ ] Alt text for images\n\n### Must Avoid\n- Color-only indicators\n- Keyboard traps\n- Auto-playing audio\n```\n\n**API Security (OWASP)**:\n```markdown\n### Must Follow\n- [ ] Input validation on all endpoints\n- [ ] Parameterized queries (no SQL injection)\n- [ ] Authentication on protected routes\n- [ ] Rate limiting\n- [ ] HTTPS only\n\n### Must Avoid\n- Hardcoded secrets\n- SQL string concatenation\n- Verbose error messages to clients\n- Sensitive data in logs\n```\n\n**Code Quality**:\n```markdown\n### Must Follow\n- [ ] TypeScript strict mode\n- [ ] Explicit return types\n- [ ] Error handling with try/catch\n- [ ] Meaningful variable names\n\n### Must Avoid\n- `any` type\n- Ignored errors\n- Magic numbers\n- Deep nesting (>3 levels)\n```\n\n---\n\n## Quality Gates\n\n### Output Checklist Pattern\n\n```markdown\n## Output Checklist\n\nBefore delivering, verify ALL items:\n\n### Functional\n- [ ] Core feature works as specified\n- [ ] Error states handled gracefully\n- [ ] Loading states present (if async)\n- [ ] Edge cases covered\n\n### Quality\n- [ ] Follows naming conventions\n- [ ] No hardcoded values\n- [ ] Comments where logic is non-obvious\n- [ ] Consistent formatting\n\n### Standards\n- [ ] Passes domain requirements (above)\n- [ ] Tested against acceptance criteria\n- [ ] No security vulnerabilities\n```\n\n### Skill-Specific Checklists\n\n**Widget Output Checklist**:\n```markdown\n## Widget Output Checklist\n\n### Functional\n- [ ] window.openai data access with null checks\n- [ ] Event listener for openai:set_globals\n- [ ] Loading state before data\n- [ ] Error state when data.isError\n- [ ] Empty state when no data\n\n### Visual\n- [ ] Theme support (light/dark)\n- [ ] System fonts\n- [ ] WCAG AA contrast\n- [ ] Responsive layout\n- [ ] Focus indicators\n\n### UX\n- [ ] Follows \"Extract, Don't Port\"\n- [ ] Inline mode unless justified\n- [ ] 2 actions per card\n```\n\n**Script Output Checklist**:\n```markdown\n## Script Output Checklist\n\n### Functional\n- [ ] Runs without errors on sample input\n- [ ] Handles missing/invalid input gracefully\n- [ ] Produces expected output format\n- [ ] Cleans up temporary files\n\n### Quality\n- [ ] Clear usage instructions\n- [ ] Documented parameters\n- [ ] Meaningful error messages\n- [ ] Exit codes for automation\n```\n\n---\n\n## Good/Bad Examples Pattern\n\nAlways include both correct and incorrect patterns:\n\n```markdown\n### Good Example\n```python\n# Correct: Explicit error handling\ntry:\n    result = process_file(path)\n    return {\"success\": True, \"data\": result}\nexcept FileNotFoundError:\n    return {\"success\": False, \"error\": \"File not found\"}\nexcept Exception as e:\n    logger.error(f\"Processing failed: {e}\")\n    return {\"success\": False, \"error\": \"Processing failed\"}\n```\n\n### Bad Example (Don't Do This)\n```python\n# Wrong: Silently ignores errors\ntry:\n    result = process_file(path)\nexcept:\n    pass  # Never do this - errors are hidden\n```\n```\n",
        "skills/cover_letter_generator/.claude/skills/skill-creator-pro/references/reusability-patterns.md": "# Reusability Patterns\n\nPatterns for creating adaptable skills that handle variations across requirements.\n\n---\n\n## Skills = Procedural Knowledge + Domain Expertise\n\nEvery production-grade skill encodes TWO types of knowledge:\n\n### Procedural Knowledge (HOW)\nStep-by-step processes, decision trees, workflows that guide execution.\n\n| Examples |\n|----------|\n| \"First validate input, then process, then verify output\" |\n| \"If authentication fails, retry with backoff, then prompt for credentials\" |\n| \"Check for null  validate format  transform  return\" |\n\n### Domain Expertise (WHAT)\nConcepts, best practices, patterns, anti-patterns specific to the domain.\n\n| Examples |\n|----------|\n| \"WCAG AA requires 4.5:1 contrast ratio for text\" |\n| \"React components should be pure functions\" |\n| \"API rate limits are typically 1000 req/min\" |\n\n### How They Work Together\n\n```\nUser Request\n     \n     \n\n Domain Expertise (WHAT)         \n - Understand the domain         \n - Apply best practices          \n - Avoid anti-patterns           \n\n     \n     \n\n Procedural Knowledge (HOW)      \n - Follow step-by-step workflow  \n - Make decisions at branches    \n - Handle errors appropriately   \n\n     \n     \nQuality Output\n```\n\n### Encoding in Skills (Embedded, Not Discovered at Runtime)\n\n| Knowledge Type | Where to Embed |\n|----------------|----------------|\n| Procedural (workflows) | SKILL.md main sections |\n| Domain expertise | `references/` (structured per domain needs) |\n| Complex procedures | `scripts/` for executable code |\n| Templates | `assets/` for boilerplate |\n\n**What goes in references/**:\n- Library/API documentation\n- Best practices\n- Code examples\n- Patterns and anti-patterns\n- Domain-specific details\n\n**Key**: Domain expertise is EMBEDDED during skill creation. Generated skills are zero-shot experts. Structure `references/` based on what the domain needs.\n\n---\n\n## Core Concept: Varies vs Constant\n\nEvery domain has elements that VARY across use cases and elements that remain CONSTANT.\n\n**Skills encode the CONSTANT patterns and ask clarifying questions for what VARIES.**\n\n---\n\n## Domain Analysis Pattern\n\nBefore creating a skill, analyze the domain:\n\n| Question | Purpose |\n|----------|---------|\n| What changes between use cases? | Identify clarification questions needed |\n| What stays the same? | Identify patterns to encode in skill |\n| What are the common variations? | Identify options to present |\n| What are the boundaries? | Identify scope (does/does not do) |\n\n---\n\n## Examples by Domain\n\n### Visualization Skills\n\n| Varies | Constant |\n|--------|----------|\n| Data shape/structure | Rendering lifecycle |\n| Chart type (bar, line, pie) | Accessibility requirements |\n| Library (Recharts, D3, Chart.js) | Responsive patterns |\n| Color scheme | Loading/error states |\n| Interactivity level | Data validation |\n\n**Skill should**: Ask for data shape, chart type, library preference\n**Skill should NOT**: Hardcode specific data fields or single chart type\n\n### Web Framework Skills (Next.js, React, etc.)\n\n| Varies | Constant |\n|--------|----------|\n| Database (Postgres, MongoDB, Prisma) | Project structure patterns |\n| CSS framework (Tailwind, CSS Modules) | Component architecture |\n| Auth provider (NextAuth, Clerk) | Error handling patterns |\n| Deployment target (Vercel, AWS) | Performance best practices |\n| Features (specific pages, APIs) | Security patterns |\n\n**Skill should**: Ask for tech stack preferences, feature requirements\n**Skill should NOT**: Hardcode database schema or specific routes\n\n### Deployment Skills\n\n| Varies | Constant |\n|--------|----------|\n| Platform (AWS, GCP, Azure) | CI/CD principles |\n| Orchestration (Kubernetes, ECS, serverless) | Rollback strategies |\n| Configuration (Helm, Terraform, CDK) | Health check patterns |\n| Environment (dev, staging, prod) | Secret management principles |\n| Scale requirements | Monitoring patterns |\n\n**Skill should**: Ask for platform, orchestration preference, environment\n**Skill should NOT**: Hardcode specific cloud resources or configs\n\n### API Integration Skills\n\n| Varies | Constant |\n|--------|----------|\n| Endpoint URLs | Error handling patterns |\n| Authentication method | Retry with backoff |\n| Request/response shape | Rate limiting handling |\n| Third-party service | Timeout management |\n| Rate limits | Response validation |\n\n**Skill should**: Ask for API details, auth method\n**Skill should NOT**: Hardcode specific API endpoints\n\n### Data Processing Skills\n\n| Varies | Constant |\n|--------|----------|\n| Input format (CSV, JSON, XML) | Validation patterns |\n| Output format | Error recovery |\n| Transformation rules | Streaming for large files |\n| Data schema | Progress reporting |\n| Volume/scale | Cleanup procedures |\n\n**Skill should**: Ask for input/output formats, transformation needs\n**Skill should NOT**: Hardcode specific field names or schemas\n\n---\n\n## Abstraction Levels\n\n### Level 1: Domain-Agnostic (Highest Reuse)\n\nSkills that work across many domains:\n- Error handling patterns\n- Logging and monitoring\n- Testing strategies\n- Documentation generation\n\n### Level 2: Domain-Specific, Tool-Agnostic (High Reuse)\n\nSkills that work within a domain but across tools:\n- \"Visualization\" (not \"Recharts visualization\")\n- \"Deployment\" (not \"Kubernetes deployment\")\n- \"API integration\" (not \"Stripe integration\")\n\n### Level 3: Tool-Specific (Moderate Reuse)\n\nSkills for specific tools but adaptable workflows:\n- \"Next.js applications\" (adaptable to different features)\n- \"PostgreSQL databases\" (adaptable to different schemas)\n\n### Level 4: Requirement-Specific (Avoid)\n\nSkills tied to single requirements:\n- \"Sales dashboard with bar chart\" - TOO SPECIFIC\n- \"User auth with email/password\" - TOO SPECIFIC\n\n---\n\n## Clarification Questions for Reusability\n\nStructure questions to capture variations:\n\n```markdown\n## Required Clarifications\n\n1. **[Variable Element]**: \"What [element] will you use?\"\n   - Option A  implications\n   - Option B  implications\n   - Other  ask for details\n\n2. **[Another Variable]**: \"What are your requirements for [aspect]?\"\n```\n\n### Good Examples\n\n```markdown\n## Required Clarifications\n\n1. **Data source**: \"What shape will the input data have?\"\n   - Provide example structure\n\n2. **Output format**: \"What type of visualization?\"\n   - Chart (bar, line, pie, etc.)\n   - Table\n   - Dashboard\n   - Other\n\n3. **Library preference**: \"Any preferred visualization library?\"\n   - Recharts (React)\n   - D3.js (vanilla)\n   - Chart.js\n   - No preference (recommend based on use case)\n```\n\n### Bad Examples (Too Specific)\n\n```markdown\n## This skill creates:\n- A bar chart showing monthly sales\n- Using Recharts\n- With blue color scheme\n```\n\n---\n\n## Reusability Checklist\n\nBefore finalizing a skill, verify:\n\n### Scope\n- [ ] Does NOT hardcode specific data fields/schemas\n- [ ] Does NOT hardcode specific tool/library (unless tool-specific skill)\n- [ ] Does NOT hardcode specific configurations\n- [ ] DOES handle common variations via clarifications\n\n### Clarifications\n- [ ] Asks for variable elements (data shape, tool preference, etc.)\n- [ ] Provides reasonable options for common choices\n- [ ] Allows \"other\" for uncommon variations\n\n### Patterns\n- [ ] Encodes CONSTANT domain patterns (best practices, error handling)\n- [ ] Separates concerns (what varies vs what's constant)\n- [ ] Works for multiple use cases within the domain\n\n### Boundaries\n- [ ] Clear scope (what it does / does not do)\n- [ ] Explicit about abstraction level (domain vs tool-specific)\n\n---\n\n## Anti-Patterns\n\n### Hardcoded Specifics\n\n```markdown\n# Bad: Hardcoded data fields\nThe widget displays:\n- product.name\n- product.price\n- product.quantity\n```\n\n```markdown\n# Good: Adaptable to data shape\nAsk for data structure, then map fields dynamically.\n```\n\n### Single Tool Lock-in (when domain-level skill)\n\n```markdown\n# Bad: Tool lock-in for domain skill\nThis visualization skill uses Recharts exclusively.\n```\n\n```markdown\n# Good: Tool-agnostic with preference\nAsk for library preference. Support Recharts, D3, Chart.js, or recommend based on use case.\n```\n\n### Feature Enumeration\n\n```markdown\n# Bad: Listing specific features\nCreates:\n- Login page\n- Dashboard\n- Settings page\n```\n\n```markdown\n# Good: Feature categories\nAsk what features are needed. Provide patterns for:\n- Authentication flows\n- Data display pages\n- Configuration interfaces\n```\n",
        "skills/cover_letter_generator/.claude/skills/skill-creator-pro/references/skill-patterns.md": "# Skill Patterns\n\nSKILL.md structure and examples for different skill types.\n\n---\n\n## Frontmatter Specification\n\n### Complete Frontmatter Template\n\n```yaml\n---\nname: skill-name                          # Required\ndescription: |                            # Required\n  [What] Brief statement of capability.\n  [When] Use when users ask to <triggers>.\nallowed-tools: Read, Grep, Glob           # Optional: restrict tool access\nmodel: claude-sonnet-4-20250514           # Optional: model override\n---\n```\n\n### Field Requirements\n\n| Field | Required | Constraints | Purpose |\n|-------|----------|-------------|---------|\n| `name` | Yes | Lowercase, numbers, hyphens only; 64 chars; must match directory name | Skill identifier |\n| `description` | Yes | 1024 characters; must include What + When | Claude Code uses this to decide when to trigger |\n| `allowed-tools` | No | Comma-separated tool names | Restricts tool access during skill execution |\n| `model` | No | Valid model ID | Override model for complex reasoning |\n\n### Name Constraints\n\n```\n Valid: pdf-processor, data-viz, api-v2\n Invalid: PDF_Processor, dataViz, my skill\n```\n\n- Lowercase letters, numbers, hyphens only\n- Maximum 64 characters\n- Must match the directory name exactly\n\n### Description Format\n\n**Structure**: `[What it does] + [When to use/triggers]`\n\n**Limit**: 1024 characters (truncated if exceeded)\n\n**Purpose**: Claude Code reads this to decide when to activate the skill. Include specific trigger phrases users would say.\n\n```yaml\n# Good: Clear what + when with triggers (third-person style)\ndescription: |\n  Create data visualizations with charts and graphs.\n  This skill should be used when users ask to visualize data, create charts,\n  build dashboards, or display metrics graphically.\n\n# Bad: Vague, no triggers\ndescription: Helps with charts\n```\n\n### allowed-tools Usage\n\nRestrict tool access for security or scope:\n\n```yaml\n# Read-only skill (no file modifications)\nallowed-tools: Read, Grep, Glob\n\n# Analysis skill with web access\nallowed-tools: Read, Grep, WebFetch, WebSearch\n\n# Full access (default if omitted)\n# allowed-tools: (omit field)\n```\n\n### model Override\n\nSpecify when skill needs different model capabilities:\n\n```yaml\n# For complex reasoning tasks\nmodel: claude-sonnet-4-20250514\n\n# For simple, fast operations\nmodel: claude-haiku-3-20250514\n```\n\n---\n\n## SKILL.md Structure\n\n### Complete Template\n\n```markdown\n---\nname: skill-name\ndescription: |\n  [What] Capability statement.\n  [When] Use when users ask to <specific triggers>.\n---\n\n# Skill Name\n\nBrief one-line description.\n\n## What This Skill Does\n- Capability 1\n- Capability 2\n- Capability 3\n\n## What This Skill Does NOT Do\n- Exclusion 1\n- Exclusion 2\n\n---\n\n## Before Implementation\n\nGather context to ensure successful implementation:\n\n| Source | Gather |\n|--------|--------|\n| **Codebase** | Existing structure, patterns, conventions to integrate with |\n| **Conversation** | User's specific requirements, constraints, preferences |\n| **Skill References** | Domain patterns from `references/` (library docs, best practices, examples) |\n| **User Guidelines** | Project-specific conventions, team standards |\n\nEnsure all required context is gathered before implementing.\nOnly ask user for THEIR specific requirements (domain expertise is in this skill).\n\n---\n\n## Required Clarifications\n\nAsk about USER'S context (not domain knowledge):\n\n1. **Use case**: \"What's YOUR specific need?\"\n2. **Constraints**: \"Any specific requirements?\"\n\n---\n\n## Workflow\n\n1. Step one\n2. Step two\n3. Step three\n\n---\n\n## [Domain-Specific Section]\n\nContent specific to what the skill does.\n\n---\n\n## Output Checklist\n\nBefore delivering, verify:\n- [ ] Requirement 1\n- [ ] Requirement 2\n- [ ] Requirement 3\n\n---\n\n## Reference Files\n\n| File | When to Read |\n|------|--------------|\n| `references/file1.md` | When X |\n| `references/file2.md` | When Y |\n```\n\n---\n\n## Skill Types Taxonomy\n\n### Overview\n\n| Type | Purpose | Key Output |\n|------|---------|------------|\n| **Builder** | Creates new artifacts | Code, documents, widgets, configs |\n| **Guide** | Provides instructions | Step-by-step workflows, tutorials |\n| **Automation** | Executes workflows | Processed files, transformed data |\n| **Analyzer** | Extracts insights | Reports, summaries, reviews |\n| **Validator** | Enforces quality | Pass/fail assessments, scores |\n\n### Type Selection Guide\n\n```\nWhat does the skill primarily do?\n\nCreates NEW artifacts (code, docs, widgets)?\n   Builder\n\nTeaches HOW to do something?\n   Guide\n\nExecutes multi-step PROCESSES automatically?\n   Automation\n\nEXTRACTS information or provides ANALYSIS?\n   Analyzer\n\nCHECKS quality or ENFORCES standards?\n   Validator\n```\n\n---\n\n## Builder Skills (Create Artifacts)\n\n**Purpose**: Generate new code, documents, widgets, configurations\n\n**Key elements**:\n- Required Clarifications (MUST ask before building)\n- Output specification\n- Domain standards enforcement\n- Templates in assets/\n\n**Example frontmatter**:\n```yaml\n---\nname: widget-creator\ndescription: |\n  Create production widgets for ChatGPT Apps.\n  Use when users ask to build UI components, visual interfaces,\n  or interactive elements.\n---\n```\n\n**Required sections**:\n```markdown\n## Before Implementation\n\nGather context to ensure successful implementation:\n\n| Source | Gather |\n|--------|--------|\n| **Codebase** | Existing structure, patterns, conventions to integrate with |\n| **Conversation** | User's specific requirements, constraints, preferences |\n| **Skill References** | Domain patterns from `references/` |\n| **User Guidelines** | Project-specific conventions, team standards |\n\n## Required Clarifications\n1. **Data shape**: \"What structure will input have?\"\n2. **Output type**: \"What artifact to create?\"\n3. **Constraints**: \"Any specific requirements?\"\n\n## Output Specification\n[Define what the artifact looks like]\n\n## Domain Standards\n### Must Follow\n- [ ] Standard 1\n### Must Avoid\n- Anti-pattern 1\n\n## Output Checklist\n- [ ] Artifact meets requirements\n```\n\n---\n\n## Guide Skills (Provide Instructions)\n\n**Purpose**: Teach procedures, provide tutorials, explain how-to\n\n**Key elements**:\n- Step-by-step workflow\n- Good/bad examples\n- Official documentation links\n- Decision trees for branching\n\n**Example frontmatter**:\n```yaml\n---\nname: api-integration-guide\ndescription: |\n  Guide for integrating external APIs.\n  Use when users need to connect to third-party services,\n  handle authentication, or manage API responses.\n---\n```\n\n**Required sections**:\n```markdown\n## Before Implementation\n\nGather context to ensure successful implementation:\n\n| Source | Gather |\n|--------|--------|\n| **Codebase** | Existing structure, patterns, conventions to integrate with |\n| **Conversation** | User's specific requirements, constraints, preferences |\n| **Skill References** | Domain patterns from `references/` |\n| **User Guidelines** | Project-specific conventions, team standards |\n\n## Workflow\n1. **Step 1** - Action\n2. **Step 2** - Action\n3. **Step 3** - Action\n\n## Examples\n### Good Example\n[Correct pattern with explanation]\n\n### Bad Example (Avoid)\n[Incorrect pattern with explanation]\n\n## Official Documentation\n| Resource | URL | Use For |\n|----------|-----|---------|\n| Docs | https://... | Reference |\n```\n\n---\n\n## Automation Skills (Execute Workflows)\n\n**Purpose**: Process files, deploy systems, execute multi-step operations\n\n**Key elements**:\n- Tested scripts in scripts/\n- Error handling guidance\n- Dependencies documented\n- Input/output contracts\n\n**Example frontmatter**:\n```yaml\n---\nname: pdf-processor\ndescription: |\n  Process PDF files with extraction, rotation, and form filling.\n  Use when users need to manipulate PDF documents programmatically.\n---\n```\n\n**Required sections**:\n```markdown\n## Before Implementation\n\nGather context to ensure successful implementation:\n\n| Source | Gather |\n|--------|--------|\n| **Codebase** | Existing structure, patterns, conventions to integrate with |\n| **Conversation** | User's specific requirements, constraints, preferences |\n| **Skill References** | Domain patterns from `references/` |\n| **User Guidelines** | Project-specific conventions, team standards |\n\n## Available Scripts\n| Script | Purpose | Usage |\n|--------|---------|-------|\n| `scripts/process.py` | Main processing | `python process.py input output` |\n\n## Dependencies\n- Python 3.10+\n- Required packages: [list]\n\n## Error Handling\n| Error | Recovery |\n|-------|----------|\n| Invalid input | [Action] |\n\n## Input/Output\n- **Input**: [Format, constraints]\n- **Output**: [Format, location]\n```\n\n---\n\n## Analyzer Skills (Extract Insights)\n\n**Purpose**: Review documents, analyze data, extract information, summarize\n\n**Key elements**:\n- Analysis scope and criteria\n- Extraction patterns\n- Output format specification\n- Synthesis guidance\n\n**Example frontmatter**:\n```yaml\n---\nname: code-analyzer\ndescription: |\n  Analyze codebases for patterns, issues, and improvements.\n  Use when users ask to review code, find patterns,\n  assess quality, or understand architecture.\n---\n```\n\n**Required sections**:\n```markdown\n## Before Implementation\n\nGather context to ensure successful implementation:\n\n| Source | Gather |\n|--------|--------|\n| **Codebase** | Existing structure, patterns, conventions to integrate with |\n| **Conversation** | User's specific requirements, constraints, preferences |\n| **Skill References** | Domain patterns from `references/` |\n| **User Guidelines** | Project-specific conventions, team standards |\n\n## Analysis Scope\n- What to analyze\n- What to ignore\n\n## Evaluation Criteria\n| Criterion | Weight | How to Assess |\n|-----------|--------|---------------|\n| Criterion 1 | X% | [Method] |\n\n## Output Format\n[Specify report structure]\n\n## Synthesis\n- Combine findings into actionable insights\n- Prioritize by impact\n```\n\n---\n\n## Validator Skills (Enforce Quality)\n\n**Purpose**: Check compliance, score quality, enforce standards\n\n**Key elements**:\n- Quality criteria with scoring\n- Pass/fail thresholds\n- Remediation guidance\n- Evidence collection\n\n**Example frontmatter**:\n```yaml\n---\nname: accessibility-validator\ndescription: |\n  Validate web content for WCAG accessibility compliance.\n  Use when users ask to check accessibility, audit for\n  compliance, or verify standards adherence.\n---\n```\n\n**Required sections**:\n```markdown\n## Before Implementation\n\nGather context to ensure successful implementation:\n\n| Source | Gather |\n|--------|--------|\n| **Codebase** | Existing structure, patterns, conventions to integrate with |\n| **Conversation** | User's specific requirements, constraints, preferences |\n| **Skill References** | Domain patterns from `references/` |\n| **User Guidelines** | Project-specific conventions, team standards |\n\n## Quality Criteria\n| Criterion | Weight | Pass Threshold |\n|-----------|--------|----------------|\n| Criterion 1 | X% | [Threshold] |\n\n## Scoring Rubric\n- **3 (Excellent)**: [Definition]\n- **2 (Good)**: [Definition]\n- **1 (Needs Work)**: [Definition]\n- **0 (Fail)**: [Definition]\n\n## Thresholds\n- **Pass**: Score  X\n- **Conditional**: Score X-Y\n- **Fail**: Score < Y\n\n## Remediation\n| Issue | Fix |\n|-------|-----|\n| Issue 1 | [How to fix] |\n```\n\n---\n\n## Assets Directory Patterns\n\n### When to Use assets/\n\n| Use Case | Example |\n|----------|---------|\n| Output templates | HTML boilerplate, component scaffolds |\n| Exact boilerplate | Config files that must be precise |\n| Visual assets | Images, icons, fonts |\n| Data templates | JSON schemas, sample data |\n\n### When NOT to Use assets/\n\n| Avoid | Instead |\n|-------|---------|\n| Code that varies per use | Describe pattern in SKILL.md |\n| Large files (>50KB) | Reference external URLs |\n| Generated content | Create dynamically |\n\n### Asset Types\n\n```\nassets/\n templates/           # Output scaffolds\n    component.tsx    # React component template\n    page.html        # HTML page template\n    config.json      # Configuration template\n schemas/             # Data structures\n    input.schema.json\n    output.schema.json\n examples/            # Reference implementations\n     simple.tsx\n     advanced.tsx\n```\n\n### Referencing Assets in SKILL.md\n\n**Template with placeholders**:\n```markdown\nUse `assets/templates/component.tsx` as base template.\n\nReplace placeholders:\n- `{{COMPONENT_NAME}}`  PascalCase component name\n- `{{PROPS_INTERFACE}}`  TypeScript props interface\n- `{{RENDER_CONTENT}}`  JSX content\n```\n\n**Exact copy**:\n```markdown\nCopy `assets/config.json` to project root.\nDo not modify structure; only update values.\n```\n\n**Reference example**:\n```markdown\nFollow pattern in `assets/examples/simple.tsx` for basic usage.\nFor advanced features, see `assets/examples/advanced.tsx`.\n```\n\n### Template Design Principles\n\n1. **Self-documenting placeholders**: `{{DESCRIPTIVE_NAME}}`\n2. **Minimal structure**: Only essential boilerplate\n3. **Clear boundaries**: Mark customizable sections\n4. **Valid syntax**: Template should be syntactically valid\n\n**Good template**:\n```typescript\n// assets/templates/component.tsx\nimport React from 'react';\n\ninterface {{COMPONENT_NAME}}Props {\n  {{PROPS}}\n}\n\nexport function {{COMPONENT_NAME}}({ {{DESTRUCTURED_PROPS}} }: {{COMPONENT_NAME}}Props) {\n  {{HOOKS}}\n\n  return (\n    {{JSX_CONTENT}}\n  );\n}\n```\n\n---\n\n## Frontmatter Best Practices\n\n### Good Description (triggers reliably)\n\n```yaml\ndescription: |\n  Create production widgets for ChatGPT Apps using OpenAI Apps SDK.\n  Use when users ask to build UI components, visual interfaces,\n  progress trackers, quiz interfaces, or interactive elements.\n```\n\n**Why it works**:\n- [What] Clear capability statement first\n- [When] Specific trigger phrases users would say\n- 1024 characters\n- Claude Code can match user intent\n\n### Bad Description (won't trigger)\n\n```yaml\ndescription: Widget stuff\n```\n\n**Why it fails**:\n- No [What] capability statement\n- No [When] trigger phrases\n- Too vague for Claude Code to match\n\n---\n\n## Scope Clarity Examples\n\n### Good Scope\n\n```markdown\n## What This Skill Does\n- Creates ChatGPT widgets with window.openai integration\n- Supports all display modes (inline, fullscreen, pip)\n- Implements theme support (light/dark)\n- Follows OpenAI UX/UI guidelines\n\n## What This Skill Does NOT Do\n- Create native mobile apps\n- Handle backend server logic\n- Manage user authentication\n- Deploy widgets to production\n```\n\n### Bad Scope (missing exclusions)\n\n```markdown\n## What This Skill Does\n- Creates widgets\n```\n\n---\n\n## Reference Organization\n\n### By Domain (for multi-domain skills)\n\n```\nreferences/\n aws.md        # AWS-specific patterns\n gcp.md        # GCP-specific patterns\n azure.md      # Azure-specific patterns\n```\n\n### By Complexity (for single-domain skills)\n\n```\nreferences/\n quick-start.md     # Basic usage\n advanced.md        # Complex scenarios\n troubleshooting.md # Common issues\n```\n\n### By Feature (for feature-rich skills)\n\n```\nreferences/\n authentication.md  # Auth patterns\n state-management.md # State handling\n error-handling.md   # Error patterns\n```\n",
        "skills/cover_letter_generator/.claude/skills/skill-creator-pro/references/technical-patterns.md": "# Technical Patterns\n\nError handling, security considerations, and dependency documentation.\n\n---\n\n## Error Handling\n\n### Error Handling Table\n\nDocument error scenarios and actions:\n\n```markdown\n## Error Handling\n\n| Scenario | Detection | Action |\n|----------|-----------|--------|\n| Invalid input | Validation fails | Return error with specifics |\n| File not found | FileNotFoundError | Clear message, suggest fix |\n| Network failure | Timeout/ConnectionError | Retry 3x with backoff |\n| Auth failure | 401/403 response | Prompt re-authentication |\n| Unknown error | Catch-all exception | Log context, safe default |\n```\n\n### Retry Pattern\n\n```python\nimport time\nimport random\n\ndef retry_with_backoff(func, max_retries=3, base_delay=0.1):\n    \"\"\"Retry with exponential backoff and jitter.\"\"\"\n    for attempt in range(max_retries):\n        try:\n            return func()\n        except Exception as e:\n            if attempt == max_retries - 1:\n                raise\n            delay = base_delay * (2 ** attempt) + random.uniform(0, 0.1)\n            time.sleep(delay)\n```\n\n### Error Response Format\n\n```typescript\n// Consistent error response structure\ninterface ErrorResponse {\n  isError: true;\n  content: [{ type: 'text', text: string }];  // User-friendly message\n  _meta?: {\n    errorCode: string;      // Machine-readable code\n    details: unknown;       // Debug info (not shown to user)\n    retryable: boolean;     // Can user retry?\n  };\n}\n\n// Example\nreturn {\n  isError: true,\n  content: [{ type: 'text', text: 'Could not process file. Please check the format.' }],\n  _meta: {\n    errorCode: 'INVALID_FORMAT',\n    details: { expected: 'PDF', received: 'PNG' },\n    retryable: false\n  }\n};\n```\n\n### Graceful Degradation\n\n```markdown\n## Fallback Strategy\n\n1. **Primary**: Execute main logic\n2. **Retry**: On transient failure, retry with backoff\n3. **Cache**: If available, serve cached result\n4. **Fallback**: Return safe default or partial result\n5. **Fail**: Clear error message with next steps\n```\n\n---\n\n## Security Considerations\n\n### Secrets Management\n\n```markdown\n## Secrets Handling\n\n### Never Do\n- Hardcode API keys, tokens, passwords in code\n- Commit .env files to version control\n- Log sensitive values\n- Include secrets in error messages\n\n### Always Do\n- Use environment variables\n- Keep .env in .gitignore\n- Provide .env.example template\n- Rotate compromised credentials immediately\n```\n\n### Input Validation\n\n```markdown\n## Input Validation\n\nValidate ALL external input:\n\n### File Paths\n```python\nimport os\n\ndef safe_path(base_dir: str, user_path: str) -> str:\n    \"\"\"Prevent path traversal attacks.\"\"\"\n    full_path = os.path.normpath(os.path.join(base_dir, user_path))\n    if not full_path.startswith(os.path.normpath(base_dir)):\n        raise ValueError(\"Invalid path: traversal detected\")\n    return full_path\n```\n\n### User Input\n```typescript\nimport { z } from 'zod';\n\nconst InputSchema = z.object({\n  name: z.string().min(1).max(100),\n  email: z.string().email(),\n  count: z.number().int().positive().max(1000),\n});\n\n// Validate before use\nconst result = InputSchema.safeParse(userInput);\nif (!result.success) {\n  return { error: result.error.format() };\n}\n```\n\n### SQL Queries\n```python\n# NEVER: String concatenation\nquery = f\"SELECT * FROM users WHERE id = {user_id}\"  # SQL injection!\n\n# ALWAYS: Parameterized queries\ncursor.execute(\"SELECT * FROM users WHERE id = ?\", (user_id,))\n```\n```\n\n### Output Escaping\n\n```markdown\n## Output Escaping\n\nPrevent XSS in generated content:\n\n```typescript\nfunction escapeHtml(text: string): string {\n  const map: Record<string, string> = {\n    '&': '&amp;',\n    '<': '&lt;',\n    '>': '&gt;',\n    '\"': '&quot;',\n    \"'\": '&#039;'\n  };\n  return text.replace(/[&<>\"']/g, m => map[m]);\n}\n\n// Use when inserting user content into HTML\nconst safeContent = escapeHtml(userInput);\n```\n```\n\n---\n\n## Dependency Documentation\n\n### Dependencies Section\n\n```markdown\n## Dependencies\n\n### Required\n- Python 3.10+ (for match statements)\n- Node.js 18+ (for fetch API)\n- TypeScript 5.0+ (for satisfies operator)\n\n### Optional\n- Redis 7+ (for caching, improves performance)\n- Docker (for containerized deployment)\n\n### External APIs\n| API | Purpose | Rate Limit | Auth |\n|-----|---------|------------|------|\n| OpenAI | Embeddings | 3000/min | API key |\n| GitHub | Repo access | 5000/hour | OAuth |\n\n### System Requirements\n- Memory: 512MB minimum, 2GB recommended\n- Disk: 100MB for installation\n- Network: Outbound HTTPS access\n```\n\n### Version Compatibility\n\n```markdown\n## Version Compatibility\n\n| Component | Minimum | Recommended | Notes |\n|-----------|---------|-------------|-------|\n| Python | 3.10 | 3.11+ | Match statements required |\n| Node.js | 18 | 20 LTS | Native fetch required |\n| npm | 9 | 10 | Workspaces support |\n```\n\n### Installation Notes\n\n```markdown\n## Installation\n\n### Prerequisites\n```bash\n# Verify versions\npython --version  # 3.10+\nnode --version    # 18+\n```\n\n### Setup\n```bash\n# Install dependencies\npip install -r requirements.txt\nnpm install\n\n# Configure environment\ncp .env.example .env\n# Edit .env with your values\n```\n\n### Verification\n```bash\n# Run tests to verify installation\npytest tests/\nnpm test\n```\n```\n\n---\n\n## Testing Guidance\n\n### Script Testing\n\n```markdown\n## Testing Scripts\n\nAll scripts in `scripts/` must be tested before inclusion.\n\n### Test Process\n1. Run with sample input\n2. Verify output matches expected\n3. Test error cases (invalid input, missing files)\n4. Check cleanup (no temp files left behind)\n\n### Test Documentation\n```bash\n# scripts/extract_text.py\n# Tested with:\n# - Single page PDF \n# - Multi-page PDF \n# - Scanned PDF (OCR) \n# - Encrypted PDF  Returns clear error \n# - Non-PDF file  Returns clear error \n```\n```\n\n### Edge Cases\n\n```markdown\n## Common Edge Cases\n\nAlways handle:\n\n| Category | Edge Cases |\n|----------|------------|\n| **Input** | Empty, null, undefined, very large |\n| **Files** | Missing, locked, wrong format, empty |\n| **Network** | Timeout, DNS failure, rate limited |\n| **Data** | Unicode, special chars, injection attempts |\n| **State** | Concurrent access, stale data, race conditions |\n```\n\n---\n\n## Performance Considerations\n\n### Timeout Protection\n\n```typescript\nasync function withTimeout<T>(\n  promise: Promise<T>,\n  ms: number,\n  message = 'Operation timed out'\n): Promise<T> {\n  const timeout = new Promise<never>((_, reject) =>\n    setTimeout(() => reject(new Error(message)), ms)\n  );\n  return Promise.race([promise, timeout]);\n}\n\n// Usage\nconst result = await withTimeout(fetchData(), 5000);\n```\n\n### Resource Limits\n\n```markdown\n## Resource Limits\n\n| Resource | Limit | Reason |\n|----------|-------|--------|\n| Request timeout | 25s | Platform limit |\n| File size | 10MB | Memory constraints |\n| Batch size | 100 items | API rate limits |\n| Concurrent requests | 5 | Prevent overload |\n```\n",
        "skills/cover_letter_generator/.claude/skills/skill-creator-pro/references/workflows.md": "# Workflow Patterns\n\nSequential, conditional, and complex workflow patterns for skills.\n\n---\n\n## Sequential Workflows\n\nBreak complex tasks into clear, sequential steps with overview:\n\n```markdown\n## Workflow Overview\n\nProcessing a document involves:\n\n1. **Validate** - Check file format and permissions\n2. **Parse** - Extract content and structure\n3. **Transform** - Apply requested changes\n4. **Verify** - Confirm output matches expectations\n5. **Deliver** - Return result to user\n```\n\n### With Scripts\n\n```markdown\nFilling a PDF form involves these steps:\n\n1. Analyze the form (run `scripts/analyze_form.py`)\n2. Create field mapping (edit `fields.json`)\n3. Validate mapping (run `scripts/validate_fields.py`)\n4. Fill the form (run `scripts/fill_form.py`)\n5. Verify output (run `scripts/verify_output.py`)\n```\n\n### With Checkpoints\n\n```markdown\n## Data Migration Workflow\n\n### Phase 1: Preparation\n1. [ ] Backup existing data\n2. [ ] Verify backup integrity\n3. [ ] Document current schema\n\n### Phase 2: Migration\n4. [ ] Run migration script\n5. [ ] Verify row counts match\n6. [ ] Validate data integrity\n\n### Phase 3: Verification\n7. [ ] Run integration tests\n8. [ ] Compare sample records\n9. [ ] Sign off on migration\n```\n\n---\n\n## Conditional Workflows\n\nGuide through decision points with clear branching:\n\n### Simple Branch\n\n```markdown\n## Workflow\n\n1. Determine the operation type:\n   - **Creating new content?**  Follow \"Creation Workflow\" below\n   - **Editing existing content?**  Follow \"Editing Workflow\" below\n\n---\n\n## Creation Workflow\n1. Generate template\n2. Fill with content\n3. Validate structure\n4. Save output\n\n---\n\n## Editing Workflow\n1. Load existing file\n2. Parse current content\n3. Apply modifications\n4. Validate changes\n5. Save updated file\n```\n\n### Decision Tree\n\n```markdown\n## Authentication Flow\n\n1. Check authentication method:\n\n   **OAuth?**\n    Get authorization URL\n    Redirect user\n    Handle callback\n    Store tokens\n\n   **API Key?**\n    Validate key format\n    Check permissions\n    Store in secure config\n\n   **Username/Password?**\n    Validate credentials\n    Generate session token\n    Set secure cookie\n```\n\n### With Fallbacks\n\n```markdown\n## Content Retrieval\n\n1. Try primary source:\n   - **Success**  Return content, done\n   - **Failure**  Continue to step 2\n\n2. Try cache:\n   - **Hit**  Return cached, done\n   - **Miss**  Continue to step 3\n\n3. Try fallback source:\n   - **Success**  Return content, update cache\n   - **Failure**  Return error with guidance\n```\n\n---\n\n## Complex Workflows\n\n### Parallel Steps\n\n```markdown\n## Deployment Workflow\n\n### Sequential: Preparation\n1. Run tests\n2. Build artifacts\n\n### Parallel: Deployment (run simultaneously)\n- Deploy to Region A\n- Deploy to Region B\n- Deploy to Region C\n\n### Sequential: Verification\n3. Run smoke tests\n4. Enable traffic\n```\n\n### State Machine\n\n```markdown\n## Order Processing States\n\n```\n[Created]  [Validated]  [Processing]  [Completed]\n                            \n[Cancelled] [Rejected]    [Failed]\n```\n\n### Transitions\n\n| From | To | Trigger |\n|------|-----|---------|\n| Created | Validated | Validation passes |\n| Created | Cancelled | User cancels |\n| Validated | Processing | Payment confirmed |\n| Validated | Rejected | Validation fails |\n| Processing | Completed | Fulfillment done |\n| Processing | Failed | Error during fulfillment |\n```\n\n### Iterative Workflows\n\n```markdown\n## Refinement Workflow\n\n1. Generate initial output\n2. Evaluate against criteria\n3. If criteria not met:\n   - Identify gaps\n   - Apply improvements\n   - Return to step 2\n4. If criteria met:\n   - Finalize output\n   - Deliver result\n\n**Maximum iterations**: 3 (prevent infinite loops)\n```\n\n---\n\n## Workflow Documentation Best Practices\n\n### DO: Clear Entry/Exit Criteria\n\n```markdown\n## Workflow: Code Review\n\n### Entry Criteria\n- [ ] PR created with description\n- [ ] Tests passing\n- [ ] No merge conflicts\n\n### Steps\n1. Review code changes\n2. Check test coverage\n3. Verify documentation\n4. Provide feedback\n\n### Exit Criteria\n- [ ] All comments resolved\n- [ ] Approved by reviewer\n- [ ] CI/CD passing\n```\n\n### DO: Error Recovery\n\n```markdown\n## If Step Fails\n\n| Step | Failure | Recovery |\n|------|---------|----------|\n| 1. Parse | Invalid format | Show format requirements, ask for correct file |\n| 2. Validate | Missing fields | List missing fields, request completion |\n| 3. Process | Timeout | Retry once, then report with partial results |\n| 4. Save | Permission denied | Suggest alternative location |\n```\n\n### DON'T: Vague Steps\n\n```markdown\n## Bad Example\n\n1. Do the thing\n2. Check if it worked\n3. Fix problems\n4. Finish up\n```\n\n### DO: Specific Steps\n\n```markdown\n## Good Example\n\n1. Run `validate.py input.json` to check format\n2. Verify output shows \"Valid: true\"\n3. If errors shown, fix issues listed in output\n4. Run `process.py input.json output.json`\n```\n",
        "skills/cover_letter_generator/.claude/skills/skill-creator/SKILL.md": "---\nname: skill-creator\ndescription: Guide for creating effective skills. This skill should be used when users want to create a new skill (or update an existing skill) that extends Claude's capabilities with specialized knowledge, workflows, or tool integrations.\nlicense: Complete terms in LICENSE.txt\n---\n\n# Skill Creator\n\nThis skill provides guidance for creating effective skills.\n\n## About Skills\n\nSkills are modular, self-contained packages that extend Claude's capabilities by providing\nspecialized knowledge, workflows, and tools. Think of them as \"onboarding guides\" for specific\ndomains or tasksthey transform Claude from a general-purpose agent into a specialized agent\nequipped with procedural knowledge that no model can fully possess.\n\n### What Skills Provide\n\n1. Specialized workflows - Multi-step procedures for specific domains\n2. Tool integrations - Instructions for working with specific file formats or APIs\n3. Domain expertise - Company-specific knowledge, schemas, business logic\n4. Bundled resources - Scripts, references, and assets for complex and repetitive tasks\n\n## Core Principles\n\n### Concise is Key\n\nThe context window is a public good. Skills share the context window with everything else Claude needs: system prompt, conversation history, other Skills' metadata, and the actual user request.\n\n**Default assumption: Claude is already very smart.** Only add context Claude doesn't already have. Challenge each piece of information: \"Does Claude really need this explanation?\" and \"Does this paragraph justify its token cost?\"\n\nPrefer concise examples over verbose explanations.\n\n### Set Appropriate Degrees of Freedom\n\nMatch the level of specificity to the task's fragility and variability:\n\n**High freedom (text-based instructions)**: Use when multiple approaches are valid, decisions depend on context, or heuristics guide the approach.\n\n**Medium freedom (pseudocode or scripts with parameters)**: Use when a preferred pattern exists, some variation is acceptable, or configuration affects behavior.\n\n**Low freedom (specific scripts, few parameters)**: Use when operations are fragile and error-prone, consistency is critical, or a specific sequence must be followed.\n\nThink of Claude as exploring a path: a narrow bridge with cliffs needs specific guardrails (low freedom), while an open field allows many routes (high freedom).\n\n### Anatomy of a Skill\n\nEvery skill consists of a required SKILL.md file and optional bundled resources:\n\n```\nskill-name/\n SKILL.md (required)\n    YAML frontmatter metadata (required)\n       name: (required)\n       description: (required)\n    Markdown instructions (required)\n Bundled Resources (optional)\n     scripts/          - Executable code (Python/Bash/etc.)\n     references/       - Documentation intended to be loaded into context as needed\n     assets/           - Files used in output (templates, icons, fonts, etc.)\n```\n\n#### SKILL.md (required)\n\nEvery SKILL.md consists of:\n\n- **Frontmatter** (YAML): Contains `name` and `description` fields. These are the only fields that Claude reads to determine when the skill gets used, thus it is very important to be clear and comprehensive in describing what the skill is, and when it should be used.\n- **Body** (Markdown): Instructions and guidance for using the skill. Only loaded AFTER the skill triggers (if at all).\n\n#### Bundled Resources (optional)\n\n##### Scripts (`scripts/`)\n\nExecutable code (Python/Bash/etc.) for tasks that require deterministic reliability or are repeatedly rewritten.\n\n- **When to include**: When the same code is being rewritten repeatedly or deterministic reliability is needed\n- **Example**: `scripts/rotate_pdf.py` for PDF rotation tasks\n- **Benefits**: Token efficient, deterministic, may be executed without loading into context\n- **Note**: Scripts may still need to be read by Claude for patching or environment-specific adjustments\n\n##### References (`references/`)\n\nDocumentation and reference material intended to be loaded as needed into context to inform Claude's process and thinking.\n\n- **When to include**: For documentation that Claude should reference while working\n- **Examples**: `references/finance.md` for financial schemas, `references/mnda.md` for company NDA template, `references/policies.md` for company policies, `references/api_docs.md` for API specifications\n- **Use cases**: Database schemas, API documentation, domain knowledge, company policies, detailed workflow guides\n- **Benefits**: Keeps SKILL.md lean, loaded only when Claude determines it's needed\n- **Best practice**: If files are large (>10k words), include grep search patterns in SKILL.md\n- **Avoid duplication**: Information should live in either SKILL.md or references files, not both. Prefer references files for detailed information unless it's truly core to the skillthis keeps SKILL.md lean while making information discoverable without hogging the context window. Keep only essential procedural instructions and workflow guidance in SKILL.md; move detailed reference material, schemas, and examples to references files.\n\n##### Assets (`assets/`)\n\nFiles not intended to be loaded into context, but rather used within the output Claude produces.\n\n- **When to include**: When the skill needs files that will be used in the final output\n- **Examples**: `assets/logo.png` for brand assets, `assets/slides.pptx` for PowerPoint templates, `assets/frontend-template/` for HTML/React boilerplate, `assets/font.ttf` for typography\n- **Use cases**: Templates, images, icons, boilerplate code, fonts, sample documents that get copied or modified\n- **Benefits**: Separates output resources from documentation, enables Claude to use files without loading them into context\n\n#### What to Not Include in a Skill\n\nA skill should only contain essential files that directly support its functionality. Do NOT create extraneous documentation or auxiliary files, including:\n\n- README.md\n- INSTALLATION_GUIDE.md\n- QUICK_REFERENCE.md\n- CHANGELOG.md\n- etc.\n\nThe skill should only contain the information needed for an AI agent to do the job at hand. It should not contain auxilary context about the process that went into creating it, setup and testing procedures, user-facing documentation, etc. Creating additional documentation files just adds clutter and confusion.\n\n### Progressive Disclosure Design Principle\n\nSkills use a three-level loading system to manage context efficiently:\n\n1. **Metadata (name + description)** - Always in context (~100 words)\n2. **SKILL.md body** - When skill triggers (<5k words)\n3. **Bundled resources** - As needed by Claude (Unlimited because scripts can be executed without reading into context window)\n\n#### Progressive Disclosure Patterns\n\nKeep SKILL.md body to the essentials and under 500 lines to minimize context bloat. Split content into separate files when approaching this limit. When splitting out content into other files, it is very important to reference them from SKILL.md and describe clearly when to read them, to ensure the reader of the skill knows they exist and when to use them.\n\n**Key principle:** When a skill supports multiple variations, frameworks, or options, keep only the core workflow and selection guidance in SKILL.md. Move variant-specific details (patterns, examples, configuration) into separate reference files.\n\n**Pattern 1: High-level guide with references**\n\n```markdown\n# PDF Processing\n\n## Quick start\n\nExtract text with pdfplumber:\n[code example]\n\n## Advanced features\n\n- **Form filling**: See [FORMS.md](FORMS.md) for complete guide\n- **API reference**: See [REFERENCE.md](REFERENCE.md) for all methods\n- **Examples**: See [EXAMPLES.md](EXAMPLES.md) for common patterns\n```\n\nClaude loads FORMS.md, REFERENCE.md, or EXAMPLES.md only when needed.\n\n**Pattern 2: Domain-specific organization**\n\nFor Skills with multiple domains, organize content by domain to avoid loading irrelevant context:\n\n```\nbigquery-skill/\n SKILL.md (overview and navigation)\n reference/\n     finance.md (revenue, billing metrics)\n     sales.md (opportunities, pipeline)\n     product.md (API usage, features)\n     marketing.md (campaigns, attribution)\n```\n\nWhen a user asks about sales metrics, Claude only reads sales.md.\n\nSimilarly, for skills supporting multiple frameworks or variants, organize by variant:\n\n```\ncloud-deploy/\n SKILL.md (workflow + provider selection)\n references/\n     aws.md (AWS deployment patterns)\n     gcp.md (GCP deployment patterns)\n     azure.md (Azure deployment patterns)\n```\n\nWhen the user chooses AWS, Claude only reads aws.md.\n\n**Pattern 3: Conditional details**\n\nShow basic content, link to advanced content:\n\n```markdown\n# DOCX Processing\n\n## Creating documents\n\nUse docx-js for new documents. See [DOCX-JS.md](DOCX-JS.md).\n\n## Editing documents\n\nFor simple edits, modify the XML directly.\n\n**For tracked changes**: See [REDLINING.md](REDLINING.md)\n**For OOXML details**: See [OOXML.md](OOXML.md)\n```\n\nClaude reads REDLINING.md or OOXML.md only when the user needs those features.\n\n**Important guidelines:**\n\n- **Avoid deeply nested references** - Keep references one level deep from SKILL.md. All reference files should link directly from SKILL.md.\n- **Structure longer reference files** - For files longer than 100 lines, include a table of contents at the top so Claude can see the full scope when previewing.\n\n## Skill Creation Process\n\nSkill creation involves these steps:\n\n1. Understand the skill with concrete examples\n2. Plan reusable skill contents (scripts, references, assets)\n3. Initialize the skill (run init_skill.py)\n4. Edit the skill (implement resources and write SKILL.md)\n5. Package the skill (run package_skill.py)\n6. Iterate based on real usage\n\nFollow these steps in order, skipping only if there is a clear reason why they are not applicable.\n\n### Step 1: Understanding the Skill with Concrete Examples\n\nSkip this step only when the skill's usage patterns are already clearly understood. It remains valuable even when working with an existing skill.\n\nTo create an effective skill, clearly understand concrete examples of how the skill will be used. This understanding can come from either direct user examples or generated examples that are validated with user feedback.\n\nFor example, when building an image-editor skill, relevant questions include:\n\n- \"What functionality should the image-editor skill support? Editing, rotating, anything else?\"\n- \"Can you give some examples of how this skill would be used?\"\n- \"I can imagine users asking for things like 'Remove the red-eye from this image' or 'Rotate this image'. Are there other ways you imagine this skill being used?\"\n- \"What would a user say that should trigger this skill?\"\n\nTo avoid overwhelming users, avoid asking too many questions in a single message. Start with the most important questions and follow up as needed for better effectiveness.\n\nConclude this step when there is a clear sense of the functionality the skill should support.\n\n### Step 2: Planning the Reusable Skill Contents\n\nTo turn concrete examples into an effective skill, analyze each example by:\n\n1. Considering how to execute on the example from scratch\n2. Identifying what scripts, references, and assets would be helpful when executing these workflows repeatedly\n\nExample: When building a `pdf-editor` skill to handle queries like \"Help me rotate this PDF,\" the analysis shows:\n\n1. Rotating a PDF requires re-writing the same code each time\n2. A `scripts/rotate_pdf.py` script would be helpful to store in the skill\n\nExample: When designing a `frontend-webapp-builder` skill for queries like \"Build me a todo app\" or \"Build me a dashboard to track my steps,\" the analysis shows:\n\n1. Writing a frontend webapp requires the same boilerplate HTML/React each time\n2. An `assets/hello-world/` template containing the boilerplate HTML/React project files would be helpful to store in the skill\n\nExample: When building a `big-query` skill to handle queries like \"How many users have logged in today?\" the analysis shows:\n\n1. Querying BigQuery requires re-discovering the table schemas and relationships each time\n2. A `references/schema.md` file documenting the table schemas would be helpful to store in the skill\n\nTo establish the skill's contents, analyze each concrete example to create a list of the reusable resources to include: scripts, references, and assets.\n\n### Step 3: Initializing the Skill\n\nAt this point, it is time to actually create the skill.\n\nSkip this step only if the skill being developed already exists, and iteration or packaging is needed. In this case, continue to the next step.\n\nWhen creating a new skill from scratch, always run the `init_skill.py` script. The script conveniently generates a new template skill directory that automatically includes everything a skill requires, making the skill creation process much more efficient and reliable.\n\nUsage:\n\n```bash\nscripts/init_skill.py <skill-name> --path <output-directory>\n```\n\nThe script:\n\n- Creates the skill directory at the specified path\n- Generates a SKILL.md template with proper frontmatter and TODO placeholders\n- Creates example resource directories: `scripts/`, `references/`, and `assets/`\n- Adds example files in each directory that can be customized or deleted\n\nAfter initialization, customize or remove the generated SKILL.md and example files as needed.\n\n### Step 4: Edit the Skill\n\nWhen editing the (newly-generated or existing) skill, remember that the skill is being created for another instance of Claude to use. Include information that would be beneficial and non-obvious to Claude. Consider what procedural knowledge, domain-specific details, or reusable assets would help another Claude instance execute these tasks more effectively.\n\n#### Learn Proven Design Patterns\n\nConsult these helpful guides based on your skill's needs:\n\n- **Multi-step processes**: See references/workflows.md for sequential workflows and conditional logic\n- **Specific output formats or quality standards**: See references/output-patterns.md for template and example patterns\n\nThese files contain established best practices for effective skill design.\n\n#### Start with Reusable Skill Contents\n\nTo begin implementation, start with the reusable resources identified above: `scripts/`, `references/`, and `assets/` files. Note that this step may require user input. For example, when implementing a `brand-guidelines` skill, the user may need to provide brand assets or templates to store in `assets/`, or documentation to store in `references/`.\n\nAdded scripts must be tested by actually running them to ensure there are no bugs and that the output matches what is expected. If there are many similar scripts, only a representative sample needs to be tested to ensure confidence that they all work while balancing time to completion.\n\nAny example files and directories not needed for the skill should be deleted. The initialization script creates example files in `scripts/`, `references/`, and `assets/` to demonstrate structure, but most skills won't need all of them.\n\n#### Update SKILL.md\n\n**Writing Guidelines:** Always use imperative/infinitive form.\n\n##### Frontmatter\n\nWrite the YAML frontmatter with `name` and `description`:\n\n- `name`: The skill name\n- `description`: This is the primary triggering mechanism for your skill, and helps Claude understand when to use the skill.\n  - Include both what the Skill does and specific triggers/contexts for when to use it.\n  - Include all \"when to use\" information here - Not in the body. The body is only loaded after triggering, so \"When to Use This Skill\" sections in the body are not helpful to Claude.\n  - Example description for a `docx` skill: \"Comprehensive document creation, editing, and analysis with support for tracked changes, comments, formatting preservation, and text extraction. Use when Claude needs to work with professional documents (.docx files) for: (1) Creating new documents, (2) Modifying or editing content, (3) Working with tracked changes, (4) Adding comments, or any other document tasks\"\n\nDo not include any other fields in YAML frontmatter.\n\n##### Body\n\nWrite instructions for using the skill and its bundled resources.\n\n### Step 5: Packaging a Skill\n\nOnce development of the skill is complete, it must be packaged into a distributable .skill file that gets shared with the user. The packaging process automatically validates the skill first to ensure it meets all requirements:\n\n```bash\nscripts/package_skill.py <path/to/skill-folder>\n```\n\nOptional output directory specification:\n\n```bash\nscripts/package_skill.py <path/to/skill-folder> ./dist\n```\n\nThe packaging script will:\n\n1. **Validate** the skill automatically, checking:\n\n   - YAML frontmatter format and required fields\n   - Skill naming conventions and directory structure\n   - Description completeness and quality\n   - File organization and resource references\n\n2. **Package** the skill if validation passes, creating a .skill file named after the skill (e.g., `my-skill.skill`) that includes all files and maintains the proper directory structure for distribution. The .skill file is a zip file with a .skill extension.\n\nIf validation fails, the script will report the errors and exit without creating a package. Fix any validation errors and run the packaging command again.\n\n### Step 6: Iterate\n\nAfter testing the skill, users may request improvements. Often this happens right after using the skill, with fresh context of how the skill performed.\n\n**Iteration workflow:**\n\n1. Use the skill on real tasks\n2. Notice struggles or inefficiencies\n3. Identify how SKILL.md or bundled resources should be updated\n4. Implement changes and test again\n",
        "skills/cover_letter_generator/.claude/skills/skill-creator/references/output-patterns.md": "# Output Patterns\n\nUse these patterns when skills need to produce consistent, high-quality output.\n\n## Template Pattern\n\nProvide templates for output format. Match the level of strictness to your needs.\n\n**For strict requirements (like API responses or data formats):**\n\n```markdown\n## Report structure\n\nALWAYS use this exact template structure:\n\n# [Analysis Title]\n\n## Executive summary\n[One-paragraph overview of key findings]\n\n## Key findings\n- Finding 1 with supporting data\n- Finding 2 with supporting data\n- Finding 3 with supporting data\n\n## Recommendations\n1. Specific actionable recommendation\n2. Specific actionable recommendation\n```\n\n**For flexible guidance (when adaptation is useful):**\n\n```markdown\n## Report structure\n\nHere is a sensible default format, but use your best judgment:\n\n# [Analysis Title]\n\n## Executive summary\n[Overview]\n\n## Key findings\n[Adapt sections based on what you discover]\n\n## Recommendations\n[Tailor to the specific context]\n\nAdjust sections as needed for the specific analysis type.\n```\n\n## Examples Pattern\n\nFor skills where output quality depends on seeing examples, provide input/output pairs:\n\n```markdown\n## Commit message format\n\nGenerate commit messages following these examples:\n\n**Example 1:**\nInput: Added user authentication with JWT tokens\nOutput:\n```\nfeat(auth): implement JWT-based authentication\n\nAdd login endpoint and token validation middleware\n```\n\n**Example 2:**\nInput: Fixed bug where dates displayed incorrectly in reports\nOutput:\n```\nfix(reports): correct date formatting in timezone conversion\n\nUse UTC timestamps consistently across report generation\n```\n\nFollow this style: type(scope): brief description, then detailed explanation.\n```\n\nExamples help Claude understand the desired style and level of detail more clearly than descriptions alone.\n",
        "skills/cover_letter_generator/.claude/skills/skill-creator/references/workflows.md": "# Workflow Patterns\n\n## Sequential Workflows\n\nFor complex tasks, break operations into clear, sequential steps. It is often helpful to give Claude an overview of the process towards the beginning of SKILL.md:\n\n```markdown\nFilling a PDF form involves these steps:\n\n1. Analyze the form (run analyze_form.py)\n2. Create field mapping (edit fields.json)\n3. Validate mapping (run validate_fields.py)\n4. Fill the form (run fill_form.py)\n5. Verify output (run verify_output.py)\n```\n\n## Conditional Workflows\n\nFor tasks with branching logic, guide Claude through decision points:\n\n```markdown\n1. Determine the modification type:\n   **Creating new content?**  Follow \"Creation workflow\" below\n   **Editing existing content?**  Follow \"Editing workflow\" below\n\n2. Creation workflow: [steps]\n3. Editing workflow: [steps]\n```",
        "skills/cover_letter_generator/.claude/skills/skill-validator/SKILL.md": "---\nname: skill-validator\ndescription: |\n  Validates skills against production-level criteria with 9-category scoring.\n  This skill should be used when reviewing, auditing, or improving skills to\n  ensure quality standards. Evaluates structure, content, user interaction,\n  documentation, domain standards, technical robustness, maintainability,\n  zero-shot implementation, and reusability. Returns actionable validation\n  report with scores and improvement recommendations.\n---\n\n# Skill Validator\n\nValidate any skill against production-level quality criteria.\n\n## Validation Workflow\n\n### Phase 1: Gather Context\n\n1. **Read the skill's SKILL.md** completely\n2. **Identify skill type** from frontmatter description:\n   - Builder skill (creates artifacts)\n   - Guide skill (provides instructions)\n   - Automation skill (executes workflows)\n   - Analyzer skill (extracts insights)\n   - Validator skill (enforces quality)\n   - Hybrid skill (combination of above)\n3. **Read all reference files** in `references/` directory\n4. **Check for assets/scripts** directories\n5. **Note frontmatter fields** (`name`, `description`, `allowed-tools`, `model`)\n\n### Phase 2: Apply Criteria\n\nEvaluate against **9 criteria categories**. Each criterion scores 0-3:\n- **0**: Missing/Absent\n- **1**: Present but inadequate\n- **2**: Adequate implementation\n- **3**: Excellent implementation\n\n---\n\n## Criteria Categories\n\n### 1. Structure & Anatomy (Weight: 12%)\n\n| Criterion | What to Check |\n|-----------|---------------|\n| **SKILL.md exists** | Root file present |\n| **Line count** | <500 lines (context is precious) |\n| **Frontmatter complete** | `name` and `description` present in YAML |\n| **Name constraints** | Lowercase, numbers, hyphens only; 64 chars; matches directory |\n| **Description format** | [What] + [When] format; 1024 chars |\n| **Description style** | Third-person: \"This skill should be used when...\" |\n| **No extraneous files** | No README.md, CHANGELOG.md, LICENSE in skill dir |\n| **Progressive disclosure** | Details in `references/`, not bloated SKILL.md |\n| **Asset organization** | Templates in `assets/`, scripts in `scripts/` |\n| **Large file guidance** | If references >10k words, grep patterns in SKILL.md |\n\n**Fail condition**: Missing SKILL.md or >800 lines = automatic fail\n\n### 2. Content Quality (Weight: 15%)\n\n| Criterion | What to Check |\n|-----------|---------------|\n| **Conciseness** | No verbose explanations, context is public good |\n| **Imperative form** | Instructions use \"Do X\" not \"You should do X\" |\n| **Appropriate freedom** | Constraints where needed, flexibility where safe |\n| **Scope clarity** | Clear what skill does AND does not do |\n| **No hallucination risk** | No instructions that encourage making up info |\n| **Output specification** | Clear expected outputs defined |\n\n### 3. User Interaction (Weight: 12%)\n\n| Criterion | What to Check |\n|-----------|---------------|\n| **Clarification triggers** | Asks questions before acting on ambiguity |\n| **Required vs optional** | Distinguishes must-know from nice-to-know |\n| **Graceful handling** | What to do when user doesn't answer |\n| **No over-asking** | Doesn't ask obvious or inferrable questions |\n| **Question pacing** | Avoids too many questions in single message |\n| **Context awareness** | Uses available context before asking |\n\n**Key pattern to look for**:\n```markdown\n## Required Clarifications\n1. Question about X\n2. Question about Y\n\n## Optional Clarifications\n3. Question about Z (if relevant)\n\nNote: Avoid asking too many questions in a single message.\n```\n\n### 4. Documentation & References (Weight: 10%)\n\n| Criterion | What to Check |\n|-----------|---------------|\n| **Source URLs** | Official documentation links provided |\n| **Reference files** | Complex details in `references/` not main file |\n| **Fetch guidance** | Instructions to fetch docs for unlisted patterns |\n| **Version awareness** | Notes about checking for latest patterns |\n| **Example coverage** | Good/bad examples for key patterns |\n\n**Key pattern to look for**:\n```markdown\n| Resource | URL | Use For |\n|----------|-----|---------|\n| Official Docs | https://... | Complex cases |\n```\n\n### 5. Domain Standards (Weight: 10%)\n\n| Criterion | What to Check |\n|-----------|---------------|\n| **Best practices** | Follows domain conventions (e.g., WCAG, OWASP) |\n| **Enforcement mechanism** | Checklists, validation steps, must-verify items |\n| **Anti-patterns** | Lists what NOT to do |\n| **Quality gates** | Output checklist before delivery |\n\n**Key pattern to look for**:\n```markdown\n### Must Follow\n- [ ] Requirement 1\n- [ ] Requirement 2\n\n### Must Avoid\n- Antipattern 1\n- Antipattern 2\n```\n\n### 6. Technical Robustness (Weight: 8%)\n\n| Criterion | What to Check |\n|-----------|---------------|\n| **Error handling** | Guidance for failure scenarios |\n| **Security considerations** | Input validation, secrets handling if relevant |\n| **Dependencies** | External tools/APIs documented |\n| **Edge cases** | Common edge cases addressed |\n| **Testability** | Can outputs be verified? |\n\n### 7. Maintainability (Weight: 8%)\n\n| Criterion | What to Check |\n|-----------|---------------|\n| **Modularity** | References are self-contained topics |\n| **Update path** | Easy to update when standards change |\n| **No hardcoded values** | Uses placeholders/variables where appropriate |\n| **Clear organization** | Logical section ordering |\n\n### 8. Zero-Shot Implementation (Weight: 12%)\n\nSkills should enable single-interaction implementation with embedded expertise.\n\n| Criterion | What to Check |\n|-----------|---------------|\n| **Before Implementation section** | Context gathering guidance present |\n| **Codebase context** | Guidance to scan existing structure/patterns |\n| **Conversation context** | Uses discussed requirements/decisions |\n| **Embedded expertise** | Domain knowledge in `references/`, not runtime discovery |\n| **User-only questions** | Only asks for USER requirements, not domain knowledge |\n\n**Key pattern to look for**:\n```markdown\n## Before Implementation\n\nGather context to ensure successful implementation:\n\n| Source | Gather |\n|--------|--------|\n| **Codebase** | Existing structure, patterns, conventions |\n| **Conversation** | User's specific requirements |\n| **Skill References** | Domain patterns from `references/` |\n| **User Guidelines** | Project-specific conventions |\n```\n\n**Red flag**: Skill instructs to \"research\" or \"discover\" domain knowledge at runtime instead of embedding it.\n\n### 9. Reusability (Weight: 13%)\n\nSkills should handle variations, not single requirements.\n\n| Criterion | What to Check |\n|-----------|---------------|\n| **Handles variations** | Not hardcoded to single use case |\n| **Variable elements** | Clarifications capture what VARIES |\n| **Constant patterns** | Domain best practices encoded as constants |\n| **Not requirement-specific** | Avoids hardcoded data, tools, configs |\n| **Abstraction level** | Appropriate generalization for domain |\n\n**Good example**:\n```markdown\n\"Create visualizations - adaptable to data shape, chart type, library\"\n```\n\n**Bad example (too specific)**:\n```markdown\n\"Create bar chart with sales data using Recharts\"\n```\n\n**Key check**: Does the skill work for multiple use cases within its domain?\n\n---\n\n## Type-Specific Validation\n\nAfter scoring general criteria, verify type-specific requirements:\n\n| Type | Must Have |\n|------|-----------|\n| **Builder** | Clarifications, Output Spec, Domain Standards, Output Checklist |\n| **Guide** | Workflow Steps, Examples (Good/Bad), Official Docs links |\n| **Automation** | Scripts in `scripts/`, Dependencies, Error Handling, I/O Spec |\n| **Analyzer** | Analysis Scope, Evaluation Criteria, Output Format, Synthesis |\n| **Validator** | Quality Criteria, Scoring Rubric, Thresholds, Remediation |\n\n**Scoring**: Deduct 10 points if type-specific requirements missing for identified type.\n\n---\n\n## Scoring Guide\n\n### Category Scores\n\nCalculate each category score:\n```\nCategory Score = (Sum of criterion scores) / (Max possible) * 100\n```\n\n### Overall Score\n\n```\nOverall = (Category Score  Weight)\n```\n\n### Rating Thresholds\n\n| Score | Rating | Meaning |\n|-------|--------|---------|\n| 90-100 | **Production** | Ready for wide use |\n| 75-89 | **Good** | Minor improvements needed |\n| 60-74 | **Adequate** | Functional but needs work |\n| 40-59 | **Developing** | Significant gaps |\n| 0-39 | **Incomplete** | Major rework required |\n\n---\n\n## Output Format\n\nGenerate validation report:\n\n```markdown\n# Skill Validation Report: [skill-name]\n\n**Rating**: [Production/Good/Adequate/Developing/Incomplete]\n**Overall Score**: [X]/100\n\n## Summary\n[2-3 sentence assessment]\n\n## Category Scores\n\n| Category | Score | Weight | Weighted |\n|----------|-------|--------|----------|\n| Structure & Anatomy | X/100 | 12% | X |\n| Content Quality | X/100 | 15% | X |\n| User Interaction | X/100 | 12% | X |\n| Documentation | X/100 | 10% | X |\n| Domain Standards | X/100 | 10% | X |\n| Technical Robustness | X/100 | 8% | X |\n| Maintainability | X/100 | 8% | X |\n| Zero-Shot Implementation | X/100 | 12% | X |\n| Reusability | X/100 | 13% | X |\n| **Type-Specific Deduction** | -X | - | -X |\n\n## Critical Issues (if any)\n- [Issue requiring immediate fix]\n\n## Improvement Recommendations\n1. **High Priority**: [Specific action]\n2. **Medium Priority**: [Specific action]\n3. **Low Priority**: [Specific action]\n\n## Strengths\n- [What skill does well]\n```\n\n---\n\n## Quick Validation Checklist\n\nFor rapid assessment, check these critical items:\n\n### Structure & Frontmatter\n- [ ] SKILL.md <500 lines\n- [ ] Frontmatter: name (64 chars, lowercase, hyphens) + description (1024 chars)\n- [ ] Description uses third-person style (\"This skill should be used when...\")\n- [ ] No README.md/CHANGELOG.md in skill directory\n\n### Content & Interaction\n- [ ] Has clarification questions (Required vs Optional)\n- [ ] Has output specification\n- [ ] Has official documentation links\n\n### Zero-Shot & Reusability\n- [ ] Has \"Before Implementation\" section (context gathering)\n- [ ] Domain expertise embedded in `references/` (not runtime discovery)\n- [ ] Handles variations (not requirement-specific)\n\n### Type-Specific (check based on skill type)\n- [ ] Builder: Clarifications + Output Spec + Standards + Checklist\n- [ ] Guide: Workflow + Examples + Docs\n- [ ] Automation: Scripts + Dependencies + Error Handling\n- [ ] Analyzer: Scope + Criteria + Output Format\n- [ ] Validator: Criteria + Scoring + Thresholds + Remediation\n\n**If 10+ checked**: Likely Production (90+)\n**If 7-9 checked**: Likely Good (75-89)\n**If 5-6 checked**: Likely Adequate (60-74)\n**If <5 checked**: Needs significant work\n\n---\n\n## Reference Files\n\n| File | When to Read |\n|------|--------------|\n| `references/detailed-criteria.md` | Deep evaluation of specific criterion |\n| `references/scoring-examples.md` | Example validations for calibration |\n| `references/improvement-patterns.md` | Common fixes for common issues |\n\n---\n\n## Usage Examples\n\n### Validate a skill\n```\nValidate the chatgpt-widget-creator skill against production criteria\n```\n\n### Quick audit\n```\nQuick validation check on mcp-builder skill\n```\n\n### Focused review\n```\nCheck if skill-creator skill has proper user interaction patterns\n```\n",
        "skills/cover_letter_generator/.claude/skills/skill-validator/references/detailed-criteria.md": "# Detailed Validation Criteria\n\nDeep evaluation guidance for each criterion.\n\n---\n\n## 1. Structure & Anatomy\n\n### 1.1 SKILL.md Exists\n- **Score 3**: Present at root of skill directory\n- **Score 0**: Missing = automatic fail\n\n### 1.2 Line Count\n- **Score 3**: <300 lines (lean and focused)\n- **Score 2**: 300-500 lines (acceptable)\n- **Score 1**: 500-800 lines (bloated, needs refactoring)\n- **Score 0**: >800 lines (automatic fail - split into references)\n\n**Why it matters**: Context window is a shared resource. Every line in SKILL.md competes with user's actual work.\n\n### 1.3 Frontmatter Quality\n\n**Score 3** - Complete, triggerable, proper format:\n```yaml\n---\nname: skill-name  # lowercase, hyphens, 64 chars, matches directory\ndescription: |    # 1024 chars, third-person style\n  This skill creates X and validates Y.\n  This skill should be used when users need to build widgets,\n  generate reports, or process data.\n---\n```\n\n**Score 2** - Present but format issues:\n```yaml\n---\nname: skill-name\ndescription: Creates X. Use when doing Y.  # Not third-person\n---\n```\n\n**Score 1** - Minimal:\n```yaml\n---\nname: skill-name\ndescription: X skill\n---\n```\n\n**Score 0**: Missing frontmatter\n\n### 1.4 Name Constraints\n\n**Score 3**: All constraints met\n- Lowercase letters only\n- Numbers allowed\n- Hyphens for separation\n- 64 characters\n- Matches directory name exactly\n\n**Score 2**: Minor violation (e.g., slightly over 64 chars)\n\n**Score 1**: Multiple violations (e.g., uppercase, underscores)\n\n**Score 0**: Name missing or completely invalid\n\n### 1.5 Description Format\n\n**Score 3** - [What] + [When] + Third-person:\n```yaml\ndescription: |\n  Validates skills against production criteria.\n  This skill should be used when reviewing, auditing, or\n  improving skills to ensure quality standards.\n```\n\n**Score 2** - Has content but wrong style:\n```yaml\ndescription: |\n  Validate skills against criteria.\n  Use when reviewing skills.  # Not third-person\n```\n\n**Score 1** - Vague:\n```yaml\ndescription: Helps with skill stuff\n```\n\n**Score 0**: Missing or single word\n\n### 1.6 No Extraneous Files\n\n**Should NOT exist in skill directory**:\n- README.md (SKILL.md IS the readme)\n- CHANGELOG.md (version in frontmatter if needed)\n- LICENSE (inherited from repo)\n- package.json (unless skill runs scripts)\n- .gitignore\n\n**Exception**: `assets/`, `references/`, `scripts/` directories are expected.\n\n### 1.7 Progressive Disclosure\n\n**Score 3**:\n- SKILL.md has overview + quick reference\n- Complex patterns in `references/`\n- Table of reference files with \"when to read\"\n\n**Score 2**:\n- Some content in references\n- SKILL.md still has most detail\n\n**Score 1**:\n- References exist but poorly organized\n\n**Score 0**:\n- Everything in SKILL.md or no references\n\n---\n\n## 2. Content Quality\n\n### 2.1 Conciseness\n\n**Score 3** - Every sentence earns its place:\n```markdown\n## Quick Start\n1. Define input schema\n2. Implement handler\n3. Register tool\n```\n\n**Score 1** - Verbose:\n```markdown\n## Quick Start\nIn this section, we will walk through the steps needed to get started with creating your tool. First, you will need to define an input schema. This schema specifies what parameters your tool accepts...\n```\n\n### 2.2 Imperative Form\n\n**Score 3**:\n```markdown\n- Validate all inputs with Zod\n- Return structured responses\n- Handle errors gracefully\n```\n\n**Score 1**:\n```markdown\n- You should validate all inputs with Zod\n- It would be good to return structured responses\n- Try to handle errors gracefully\n```\n\n### 2.3 Appropriate Freedom\n\n**Score 3** - Constraints where needed, flexibility elsewhere:\n```markdown\n## Required (No Exceptions)\n- Use TypeScript strict mode\n- Validate inputs at boundaries\n\n## Flexible (Your Choice)\n- Error message format\n- Logging verbosity\n```\n\n**Score 1** - Either too rigid or too loose\n\n### 2.4 Scope Clarity\n\n**Score 3**:\n```markdown\n## What This Skill Does\n- Creates X\n- Validates Y\n- Generates Z\n\n## What This Skill Does NOT Do\n- Production deployment\n- Performance optimization\n- Security auditing\n```\n\n**Score 1**: No explicit scope boundaries\n\n### 2.5 Output Specification\n\n**Score 3**:\n```markdown\n## Output Format\nEvery generated [artifact] includes:\n- [ ] Component A with X property\n- [ ] Component B following Y pattern\n- [ ] Component C validated against Z\n```\n\n**Score 0**: No clear output definition\n\n---\n\n## 3. User Interaction\n\n### 3.1 Clarification Triggers\n\n**Score 3** - Structured clarification section:\n```markdown\n## Required Clarifications\nBefore proceeding, ask:\n1. **Data shape**: \"What structure will the input have?\"\n2. **Action type**: \"Read-only or write?\"\n\n## Optional Clarifications\n3. **Styling**: \"Any design preferences?\" (if relevant)\n```\n\n**Score 2**: Questions mentioned but unstructured\n\n**Score 1**: \"Ask questions if needed\" without specifics\n\n**Score 0**: No clarification guidance\n\n### 3.2 Required vs Optional\n\n**Score 3**: Clearly labeled required vs optional questions\n**Score 1**: All questions treated equally\n\n### 3.3 Context Awareness\n\n**Score 3**:\n```markdown\nBefore asking:\n- Check conversation history for prior answers\n- Infer from file names/content when possible\n- Only ask what cannot be determined\n```\n\n**Score 0**: No guidance on using existing context\n\n---\n\n## 4. Documentation & References\n\n### 4.1 Source URLs\n\n**Score 3** - Table with official sources:\n```markdown\n| Resource | URL | Use For |\n|----------|-----|---------|\n| Official Guide | https://... | Core patterns |\n| API Reference | https://... | Method details |\n```\n\n**Score 2**: URLs mentioned but scattered\n\n**Score 1**: One or two links buried in text\n\n**Score 0**: No external references\n\n### 4.2 Fetch Guidance\n\n**Score 3**:\n```markdown\nFor patterns not covered here, fetch from official docs:\n- Complex widget: https://...\n- Advanced auth: https://...\n```\n\n**Score 0**: No guidance for unlisted scenarios\n\n### 4.3 Example Coverage\n\n**Score 3**:\n```markdown\n### Good Example\n[working code]\n\n### Bad Example (Don't Do This)\n[antipattern with explanation why]\n```\n\n**Score 1**: Only good examples, no antipatterns\n\n---\n\n## 5. Domain Standards\n\n### 5.1 Best Practices Enforcement\n\n**Score 3** - Mandatory checklist:\n```markdown\n### Must Follow\n- [ ] WCAG AA contrast (4.5:1)\n- [ ] Keyboard navigation\n- [ ] Error states\n\n### Must Avoid\n- Nested scrolling\n- Hardcoded strings\n```\n\n**Score 1**: Mentions best practices without enforcement\n\n### 5.2 Quality Gates\n\n**Score 3** - Output checklist before delivery:\n```markdown\n## Output Checklist\nBefore delivering, verify:\n- [ ] All required components present\n- [ ] Follows naming convention\n- [ ] No hardcoded values\n- [ ] Tested against criteria\n```\n\n**Score 0**: No quality gate\n\n---\n\n## 6. Technical Robustness\n\n### 6.1 Error Handling\n\n**Score 3**:\n```markdown\n## Error Handling\n- Invalid input: Return validation error with specifics\n- Network failure: Retry with backoff, then graceful fallback\n- Unknown error: Log context, return safe default\n```\n\n**Score 0**: No error handling guidance\n\n### 6.2 Security Considerations\n\n**Score 3** (when relevant):\n```markdown\n## Security\n- Never hardcode secrets\n- Validate/sanitize all user input\n- Use parameterized queries\n```\n\n**Score 2**: Mentions security generally\n**Score 0**: No security guidance (when relevant)\n\n### 6.3 Dependencies\n\n**Score 3**:\n```markdown\n## Dependencies\n- Requires: Node.js 18+, TypeScript 5+\n- Optional: Redis for caching\n- External APIs: None / List with rate limits\n```\n\n**Score 0**: Dependencies not documented\n\n---\n\n## 7. Maintainability\n\n### 7.1 Modularity\n\n**Score 3** - Self-contained reference files:\n```\nreferences/\n core-patterns.md      # Can be read independently\n advanced-usage.md     # Self-contained advanced topic\n troubleshooting.md    # Standalone help\n```\n\n**Score 1**: References that require reading other files first\n\n### 7.2 Update Path\n\n**Score 3**:\n```markdown\n## Keeping Current\n- Fetch latest from [URL] for new patterns\n- Check [changelog URL] for breaking changes\n- Last verified: 2024-12\n```\n\n**Score 0**: No update guidance\n\n### 7.3 Clear Organization\n\n**Score 3** - Logical flow:\n1. What (overview)\n2. When (triggers)\n3. How (workflow)\n4. Reference (details)\n5. Examples\n\n**Score 1**: Scattered organization, hard to navigate\n\n---\n\n## 8. Zero-Shot Implementation\n\nSkills should enable single-interaction implementation with embedded expertise.\n\n### 8.1 Before Implementation Section\n\n**Score 3** - Complete context gathering:\n```markdown\n## Before Implementation\n\nGather context to ensure successful implementation:\n\n| Source | Gather |\n|--------|--------|\n| **Codebase** | Existing structure, patterns, conventions |\n| **Conversation** | User's specific requirements |\n| **Skill References** | Domain patterns from `references/` |\n| **User Guidelines** | Project-specific conventions |\n\nEnsure all required context is gathered before implementing.\n```\n\n**Score 2**: Mentions gathering context but incomplete sources\n\n**Score 1**: Brief \"check context first\" note\n\n**Score 0**: No context gathering guidance\n\n### 8.2 Embedded Expertise\n\n**Score 3** - Domain knowledge IN the skill:\n- Best practices documented in `references/`\n- Code examples included\n- Anti-patterns listed\n- Library/API documentation embedded\n\n**Score 2**: Some expertise embedded, some requires runtime discovery\n\n**Score 1**: Mostly relies on runtime discovery (\"search for...\" instructions)\n\n**Score 0**: No embedded domain expertise\n\n**Red flags**:\n- \"Research the domain...\"\n- \"Fetch documentation for...\"\n- \"Discover best practices...\"\n\nThese indicate runtime discovery instead of embedded expertise.\n\n### 8.3 User-Only Questions\n\n**Score 3** - Only asks about USER's context:\n```markdown\n| Ask | Don't Ask |\n|-----|-----------|\n| \"What's YOUR use case?\" | \"What is [technology]?\" |\n| \"What's YOUR tech stack?\" | \"What options exist?\" |\n| \"Specific constraints?\" | \"What are best practices?\" |\n```\n\n**Score 2**: Mostly user questions, some domain questions\n\n**Score 1**: Mixes user and domain questions\n\n**Score 0**: Asks user for domain knowledge skill should contain\n\n---\n\n## 9. Reusability\n\nSkills should handle variations, not single requirements.\n\n### 9.1 Handles Variations\n\n**Score 3** - Explicitly adaptable:\n```markdown\nThis skill adapts to:\n- Different data shapes\n- Multiple output formats\n- Various libraries/frameworks\n```\n\n**Score 2**: Some variability but limited\n\n**Score 1**: Works for narrow range of cases\n\n**Score 0**: Hardcoded to single requirement\n\n### 9.2 Variable vs Constant Analysis\n\n**Score 3** - Clear separation:\n```markdown\n## What VARIES (ask user)\n- Data shape/structure\n- Tool/library preference\n- Output format\n\n## What's CONSTANT (encoded in skill)\n- Best practices\n- Error handling patterns\n- Security requirements\n```\n\n**Score 2**: Implicit separation\n\n**Score 1**: Mixed without clear distinction\n\n**Score 0**: No consideration of variability\n\n### 9.3 Not Requirement-Specific\n\n**Score 3** - Generic within domain:\n```markdown\n# Good: \"Create visualizations\"\nAdapts to chart type, library, data shape\n\n# Good: \"Deploy applications\"\nAdapts to platform, orchestration, environment\n```\n\n**Score 0** - Too specific:\n```markdown\n# Bad: \"Create bar chart with sales data using Recharts\"\n# Bad: \"Deploy to AWS EKS with Helm\"\n```\n\n### 9.4 Abstraction Level\n\n| Level | Description | Score |\n|-------|-------------|-------|\n| Domain-agnostic | Works across domains | 3 |\n| Domain-specific, tool-agnostic | Within domain, any tool | 3 |\n| Tool-specific, workflow-agnostic | Specific tool, flexible workflow | 2 |\n| Requirement-specific | Single use case | 0 |\n\n---\n\n## 10. Type-Specific Criteria\n\n### 10.1 Builder Skills\n\nMust have ALL of:\n- [ ] Required Clarifications section\n- [ ] Output Specification\n- [ ] Domain Standards (Must Follow / Must Avoid)\n- [ ] Output Checklist\n\n**Score 3**: All present and detailed\n**Score 2**: All present, some minimal\n**Score 1**: Missing 1-2 elements\n**Score 0**: Missing 3+ elements\n\n### 10.2 Guide Skills\n\nMust have ALL of:\n- [ ] Workflow Steps (numbered, sequential)\n- [ ] Good/Bad Examples\n- [ ] Official Documentation links\n\n**Score 3**: All present with clear examples\n**Score 2**: All present, examples minimal\n**Score 1**: Missing 1 element\n**Score 0**: Missing 2+ elements\n\n### 10.3 Automation Skills\n\nMust have ALL of:\n- [ ] Scripts in `scripts/` directory\n- [ ] Dependencies documented\n- [ ] Error Handling guidance\n- [ ] Input/Output Specification\n\n**Score 3**: All present, scripts tested\n**Score 2**: All present, minimal documentation\n**Score 1**: Missing 1-2 elements\n**Score 0**: Missing 3+ elements or no scripts\n\n### 10.4 Analyzer Skills\n\nMust have ALL of:\n- [ ] Analysis Scope (what to analyze, what to ignore)\n- [ ] Evaluation Criteria\n- [ ] Output Format specification\n- [ ] Synthesis guidance\n\n**Score 3**: All present with clear criteria\n**Score 2**: All present, criteria vague\n**Score 1**: Missing 1-2 elements\n**Score 0**: Missing 3+ elements\n\n### 10.5 Validator Skills\n\nMust have ALL of:\n- [ ] Quality Criteria with weights\n- [ ] Scoring Rubric (0-3 or similar)\n- [ ] Pass/Fail Thresholds\n- [ ] Remediation guidance\n\n**Score 3**: All present with clear thresholds\n**Score 2**: All present, thresholds vague\n**Score 1**: Missing 1-2 elements\n**Score 0**: Missing 3+ elements\n",
        "skills/cover_letter_generator/.claude/skills/skill-validator/references/improvement-patterns.md": "# Improvement Patterns\n\nCommon fixes for common skill validation issues.\n\n---\n\n## Structure Issues\n\n### Problem: SKILL.md Too Long (>500 lines)\n\n**Fix**: Extract to references\n\n```\nBefore:\nSKILL.md (800 lines with everything)\n\nAfter:\nSKILL.md (200 lines - overview + quick ref)\nreferences/\n detailed-patterns.md (300 lines)\n advanced-usage.md (200 lines)\n troubleshooting.md (100 lines)\n```\n\n**Add reference table**:\n```markdown\n## Reference Files\n\n| File | When to Read |\n|------|--------------|\n| `references/detailed-patterns.md` | Complex implementations |\n| `references/advanced-usage.md` | Non-standard scenarios |\n```\n\n### Problem: README.md Exists\n\n**Fix**: Delete it. SKILL.md IS the readme.\n\n### Problem: No Progressive Disclosure\n\n**Fix**: Create references/ directory with topical files:\n1. Identify independent topics in SKILL.md\n2. Extract each to `references/[topic].md`\n3. Add \"when to read\" guidance in SKILL.md\n\n---\n\n## Content Issues\n\n### Problem: Verbose Content\n\n**Before**:\n```markdown\nIn this section, we will walk through the process of creating a new widget.\nFirst, you will want to understand what data your widget needs to display.\nThis is important because the data shape determines how you structure your component.\n```\n\n**After**:\n```markdown\n## Creating a Widget\n\n1. Define data shape (what `toolOutput` contains)\n2. Choose display mode (inline/fullscreen/pip)\n3. Implement with theme support\n```\n\n### Problem: \"You should\" Instead of Imperatives\n\n**Before**:\n```markdown\n- You should validate all inputs\n- It would be good to handle errors\n- Try to follow the patterns\n```\n\n**After**:\n```markdown\n- Validate all inputs with Zod\n- Handle errors with try/catch\n- Follow these patterns:\n```\n\n### Problem: No Scope Boundaries\n\n**Fix**: Add explicit sections:\n\n```markdown\n## What This Skill Does\n- Creates X\n- Validates Y\n- Generates Z\n\n## What This Skill Does NOT Do\n- Deploy to production\n- Optimize performance\n- Handle authentication\n```\n\n---\n\n## User Interaction Issues\n\n### Problem: No Clarification Questions\n\n**Fix**: Add structured clarification section:\n\n```markdown\n## Required Clarifications\n\nBefore building, ask:\n\n1. **Data shape**: \"What will `toolOutput` contain?\"\n   ```json\n   Example: { items: [...], total: 10 }\n   ```\n\n2. **Action type**: \"Display only or interactive?\"\n   - Display  No callTool needed\n   - Interactive  Need tool name and params\n\n3. **Display mode**: \"Inline, fullscreen, or pip?\"\n\n## Optional Clarifications\n\n4. **Styling**: \"Any design preferences?\" (ask if complex UI)\n```\n\n### Problem: All Questions Treated Equally\n\n**Fix**: Separate required from optional:\n\n```markdown\n## Required (Must Ask)\n1. Core question A\n2. Core question B\n\n## Optional (Ask if Relevant)\n3. Nice-to-know question\n4. Edge case question\n```\n\n### Problem: No Context Awareness\n\n**Fix**: Add context-checking guidance:\n\n```markdown\n## Before Asking\n\nCheck existing context:\n- Review conversation history for prior answers\n- Infer from file names when possible\n- Check toolOutput structure if available\n\nOnly ask what cannot be determined.\n```\n\n---\n\n## Documentation Issues\n\n### Problem: No Official Source Links\n\n**Fix**: Add documentation table:\n\n```markdown\n## Official Documentation\n\n| Resource | URL | Use For |\n|----------|-----|---------|\n| Getting Started | https://... | Basic setup |\n| API Reference | https://... | Method details |\n| Best Practices | https://... | Pattern guidance |\n\nFor patterns not covered here, fetch from official docs.\n```\n\n### Problem: No Fetch Guidance\n\n**Fix**: Add on-the-go learning section:\n\n```markdown\n## Unlisted Scenarios\n\nFor patterns not documented here:\n\n1. Fetch from official docs: [URL]\n2. Apply same validation criteria\n3. Follow established patterns in this skill\n\nExamples of when to fetch:\n- Complex authentication flows\n- Third-party integrations\n- Platform-specific features\n```\n\n### Problem: No Examples\n\n**Fix**: Add good/bad examples:\n\n```markdown\n### Good Example\n```typescript\n// Correct pattern with explanation\nconst data = window.openai?.toolOutput ?? defaultValue;\n```\n\n### Bad Example (Don't Do This)\n```typescript\n// Why this is wrong\nconst data = window.openai.toolOutput; // Crashes if undefined\n```\n```\n\n---\n\n## Domain Standards Issues\n\n### Problem: Best Practices Mentioned But Not Enforced\n\n**Before**:\n```markdown\nFollow accessibility best practices.\n```\n\n**After**:\n```markdown\n## Accessibility Requirements\n\n### Must Follow\n- [ ] WCAG AA contrast (4.5:1 for text)\n- [ ] Keyboard navigation for all interactions\n- [ ] Focus indicators visible\n- [ ] Screen reader labels for icons\n\n### Must Avoid\n- Color as only indicator\n- Mouse-only interactions\n- Auto-playing media without controls\n```\n\n### Problem: No Output Checklist\n\n**Fix**: Add quality gate:\n\n```markdown\n## Output Checklist\n\nBefore delivering, verify ALL items:\n\n### Functional\n- [ ] Core feature works\n- [ ] Error states handled\n- [ ] Loading states present\n\n### Quality\n- [ ] Follows naming conventions\n- [ ] No hardcoded values\n- [ ] Comments where non-obvious\n\n### Standards\n- [ ] Passes domain requirements (above)\n- [ ] Tested against criteria\n```\n\n---\n\n## Technical Issues\n\n### Problem: No Error Handling Guidance\n\n**Fix**: Add error handling section:\n\n```markdown\n## Error Handling\n\n| Scenario | Action |\n|----------|--------|\n| Invalid input | Return validation error with specifics |\n| Network failure | Retry 3x with backoff, then fallback |\n| Unknown error | Log context, return safe default |\n\n### Error Response Format\n```typescript\nreturn {\n  isError: true,\n  content: [{ type: 'text', text: 'User-friendly message' }],\n  _meta: { errorCode: 'VALIDATION_FAILED', details: {...} }\n};\n```\n```\n\n### Problem: No Security Guidance\n\n**Fix**: Add security section (when relevant):\n\n```markdown\n## Security Considerations\n\n- **Never hardcode**: Secrets, API keys, tokens\n- **Always validate**: User input, file paths, URLs\n- **Escape output**: Prevent XSS in generated HTML\n- **Use parameterized**: Queries to prevent injection\n```\n\n### Problem: Dependencies Not Listed\n\n**Fix**: Add dependencies section:\n\n```markdown\n## Dependencies\n\n### Required\n- Node.js 18+\n- TypeScript 5.0+\n\n### Optional\n- Redis (for caching)\n\n### External APIs\n- OpenAI Apps SDK (via window.openai)\n- No rate limits apply to widget\n```\n\n---\n\n## Maintainability Issues\n\n### Problem: Monolithic SKILL.md\n\n**Fix**: Modularize into references:\n\n1. Identify 3-5 independent topics\n2. Create `references/[topic].md` for each\n3. Keep SKILL.md as entry point with \"when to read\" table\n\n### Problem: No Update Guidance\n\n**Fix**: Add versioning section:\n\n```markdown\n## Keeping Current\n\n- Official docs: [URL]\n- Changelog: [URL]\n- Last verified: 2024-12\n\nWhen official docs update:\n1. Check for breaking changes\n2. Update affected references\n3. Test against validation criteria\n```\n\n### Problem: Hardcoded Values\n\n**Before**:\n```markdown\nSet timeout to 5000ms.\nUse port 3000.\n```\n\n**After**:\n```markdown\nSet timeout (default: 5000ms, adjust for your use case).\nUse configured port (default: 3000).\n```\n\n---\n\n## Zero-Shot Implementation Issues\n\n### Problem: Runtime Discovery Instead of Embedded Expertise\n\n**Before (Bad)**:\n```markdown\n## Implementation\n1. Research the domain best practices\n2. Fetch library documentation\n3. Discover common patterns\n4. Implement based on findings\n```\n\n**After (Good)**:\n```markdown\n## Before Implementation\n\nGather context to ensure successful implementation:\n\n| Source | Gather |\n|--------|--------|\n| **Codebase** | Existing structure, patterns, conventions |\n| **Conversation** | User's specific requirements |\n| **Skill References** | Domain patterns from `references/` |\n| **User Guidelines** | Project-specific conventions |\n\n## Implementation\n1. Apply patterns from `references/best-practices.md`\n2. Follow examples in `references/examples.md`\n3. Avoid anti-patterns in `references/anti-patterns.md`\n```\n\n### Problem: Asking User for Domain Knowledge\n\n**Before (Bad)**:\n```markdown\n## Clarifications\n1. What is Kafka and how does it work?\n2. What are Kafka best practices?\n3. What libraries are available?\n```\n\n**After (Good)**:\n```markdown\n## Required Clarifications\n\nAsk about USER's context (domain expertise is in this skill):\n\n1. **Use case**: \"What's YOUR specific use case?\"\n2. **Tech stack**: \"What's YOUR environment?\"\n3. **Constraints**: \"Any specific requirements?\"\n\nNote: Domain best practices are embedded in `references/`.\n```\n\n### Problem: No Context Gathering Guidance\n\n**Fix**: Add Before Implementation section:\n\n```markdown\n## Before Implementation\n\nGather context from available sources:\n1. **Codebase** (if exists): Scan structure, patterns, conventions\n2. **Conversation**: Review discussed requirements and decisions\n3. **Skill References**: Apply embedded domain expertise\n4. **User Guidelines**: Follow project-specific conventions\n\nAdapt approach based on available context.\nOnly ask user for what cannot be discovered.\n```\n\n---\n\n## Reusability Issues\n\n### Problem: Hardcoded to Single Requirement\n\n**Before (Bad)**:\n```markdown\n# Sales Dashboard Creator\n\nCreates a bar chart showing monthly sales using Recharts.\n\n## Output\n- Bar chart component\n- Uses blue color scheme\n- Shows last 12 months\n```\n\n**After (Good)**:\n```markdown\n# Data Visualization Creator\n\nCreates visualizations adaptable to data shape, chart type, and library.\n\n## Required Clarifications\n1. **Data shape**: \"What structure will input have?\"\n2. **Chart type**: \"Bar, line, pie, or other?\"\n3. **Library**: \"Recharts, D3, Chart.js, or no preference?\"\n\n## Output\nAdapts based on clarifications.\n```\n\n### Problem: No Variability Analysis\n\n**Fix**: Add Varies vs Constant section:\n\n```markdown\n## Variability Analysis\n\n| What VARIES (ask user) | What's CONSTANT (encoded) |\n|------------------------|---------------------------|\n| Data shape/structure | Accessibility requirements |\n| Chart type preference | Responsive patterns |\n| Library choice | Error handling |\n| Color scheme | Loading states |\n```\n\n### Problem: Too Specific Abstraction Level\n\n**Before (Bad)**:\n```markdown\n# AWS EKS Helm Deployment\n\nDeploys to AWS EKS using Helm charts.\n```\n\n**After (Good)**:\n```markdown\n# Application Deployment\n\nDeploys applications with adaptable platform, orchestration, and configuration.\n\n## Required Clarifications\n1. **Platform**: \"AWS, GCP, Azure, or other?\"\n2. **Orchestration**: \"Kubernetes, ECS, serverless?\"\n3. **Configuration**: \"Helm, Terraform, CDK?\"\n```\n\n---\n\n## Frontmatter Issues\n\n### Problem: Wrong Description Style\n\n**Before (Bad)**:\n```yaml\ndescription: |\n  Create widgets for apps.\n  Use when building UI components.\n```\n\n**After (Good)**:\n```yaml\ndescription: |\n  Creates production widgets for applications.\n  This skill should be used when users want to build\n  UI components, visual interfaces, or interactive elements.\n```\n\n### Problem: Name Constraint Violations\n\n**Before (Bad)**:\n```yaml\nname: Widget_Creator    # Uppercase, underscores\nname: my-super-long-skill-name-that-exceeds-the-sixty-four-character-limit  # Too long\n```\n\n**After (Good)**:\n```yaml\nname: widget-creator    # Lowercase, hyphens, 64 chars\n```\n\n### Problem: Description Too Long or Missing Format\n\n**Fix**: Use [What] + [When] format, 1024 chars:\n\n```yaml\ndescription: |\n  [What] Creates X, validates Y, generates Z.\n  [When] This skill should be used when users need to\n  build widgets, review code, or process data.\n```\n\n---\n\n## Type-Specific Issues\n\n### Problem: Builder Skill Missing Required Sections\n\n**Fix**: Add all required sections:\n\n```markdown\n## Required Clarifications\n1. Question about input\n2. Question about output\n\n## Output Specification\n[Define what artifact looks like]\n\n## Domain Standards\n### Must Follow\n- [ ] Requirement 1\n### Must Avoid\n- Anti-pattern 1\n\n## Output Checklist\n- [ ] Meets requirements\n- [ ] Follows standards\n```\n\n### Problem: Analyzer Skill Missing Scope\n\n**Fix**: Add analysis scope:\n\n```markdown\n## Analysis Scope\n\n### What to Analyze\n- Code structure\n- Naming conventions\n- Error handling\n\n### What to Ignore\n- Comments and formatting\n- Third-party libraries\n- Generated code\n\n## Evaluation Criteria\n| Criterion | Weight | How to Assess |\n|-----------|--------|---------------|\n| Readability | 30% | Naming, structure |\n| Robustness | 40% | Error handling |\n| Performance | 30% | Complexity |\n```\n\n### Problem: Validator Skill Missing Thresholds\n\n**Fix**: Add clear thresholds:\n\n```markdown\n## Scoring Rubric\n- **3**: Excellent\n- **2**: Adequate\n- **1**: Needs improvement\n- **0**: Missing/Fail\n\n## Thresholds\n| Score | Rating | Action |\n|-------|--------|--------|\n| 90-100 | Pass | Ready for use |\n| 70-89 | Conditional | Minor fixes needed |\n| <70 | Fail | Major rework required |\n\n## Remediation\n| Issue | Fix |\n|-------|-----|\n| Missing X | Add X section |\n| Weak Y | Strengthen with examples |\n```\n\n---\n\n## Quick Improvement Checklist\n\nWhen improving a skill, address in this order:\n\n1. **Critical** (blocks usage):\n   - [ ] SKILL.md exists and <500 lines\n   - [ ] Frontmatter: name (64, lowercase) + description (1024, third-person)\n   - [ ] Core workflow documented\n\n2. **High Priority** (major quality):\n   - [ ] Before Implementation section (context gathering)\n   - [ ] Domain expertise embedded in `references/`\n   - [ ] Clarification questions (for builder skills)\n   - [ ] Output specification\n\n3. **Medium Priority** (polish):\n   - [ ] Handles variations (not requirement-specific)\n   - [ ] Progressive disclosure to references\n   - [ ] Error handling guidance\n   - [ ] Good/bad examples\n\n4. **Low Priority** (excellence):\n   - [ ] Type-specific sections complete\n   - [ ] Update path documented\n   - [ ] All edge cases covered\n   - [ ] Templates in assets/\n",
        "skills/cover_letter_generator/.claude/skills/skill-validator/references/scoring-examples.md": "# Scoring Examples\n\nCalibration examples showing how to score skills consistently.\n\n---\n\n## Example 1: Production-Level Skill\n\n**Skill**: chatgpt-widget-creator\n**Type**: Builder\n\n### Category Scores\n\n| Category | Score | Weight | Weighted |\n|----------|-------|--------|----------|\n| Structure & Anatomy | 95/100 | 15% | 14.25 |\n| Content Quality | 90/100 | 20% | 18.00 |\n| User Interaction | 95/100 | 15% | 14.25 |\n| Documentation | 100/100 | 15% | 15.00 |\n| Domain Standards | 90/100 | 15% | 13.50 |\n| Technical Robustness | 75/100 | 10% | 7.50 |\n| Maintainability | 85/100 | 10% | 8.50 |\n| **Total** | | | **91.00** |\n\n**Rating**: Production\n\n### Breakdown\n\n**Structure & Anatomy (95/100)**:\n- SKILL.md exists: 3\n- Line count (262): 3\n- Frontmatter: 3 (complete with triggers)\n- No extraneous files: 3\n- Progressive disclosure: 3 (6 reference files)\n- Asset organization: 3 (templates in assets/)\n\n**Content Quality (90/100)**:\n- Conciseness: 3 (lean, table-driven)\n- Imperative form: 3\n- Appropriate freedom: 3 (Required + Optional)\n- Scope clarity: 2 (could add \"What this does NOT do\")\n- Output specification: 3 (full checklist)\n\n**User Interaction (95/100)**:\n- Clarification triggers: 3 (\"STOP: Clarify Before Building\")\n- Required vs optional: 3 (clearly separated)\n- Graceful handling: 2 (missing \"if user declines\")\n- Context awareness: 3\n\n**Documentation (100/100)**:\n- Source URLs: 3 (8 official links in table)\n- Reference files: 3 (6 topical files)\n- Fetch guidance: 3 (\"fetch from docs\" instruction)\n- Version awareness: 3\n- Example coverage: 3 (templates provided)\n\n**Domain Standards (90/100)**:\n- Best practices: 3 (UX Principles section)\n- Enforcement mechanism: 3 (checkbox checklist)\n- Anti-patterns: 3 (\"Must Avoid\" section)\n- Quality gates: 2 (good but could be stricter)\n\n**Technical Robustness (75/100)**:\n- Error handling: 2 (states in checklist)\n- Security: 2 (CSP mentioned, basic)\n- Dependencies: 2 (implicit)\n- Edge cases: 2 (some covered)\n- Testability: 2 (mock pattern provided)\n\n**Maintainability (85/100)**:\n- Modularity: 3 (self-contained refs)\n- Update path: 3 (fetch pattern)\n- Clear organization: 2 (good, minor flow issues)\n\n---\n\n## Example 2: Adequate Skill (Needs Work)\n\n**Skill**: hypothetical-basic-skill\n**Type**: Guide\n\n### Category Scores\n\n| Category | Score | Weight | Weighted |\n|----------|-------|--------|----------|\n| Structure & Anatomy | 60/100 | 15% | 9.00 |\n| Content Quality | 55/100 | 20% | 11.00 |\n| User Interaction | 30/100 | 15% | 4.50 |\n| Documentation | 40/100 | 15% | 6.00 |\n| Domain Standards | 50/100 | 15% | 7.50 |\n| Technical Robustness | 45/100 | 10% | 4.50 |\n| Maintainability | 50/100 | 10% | 5.00 |\n| **Total** | | | **47.50** |\n\n**Rating**: Developing\n\n### Key Issues\n\n- **Structure**: 450 lines (borderline), README.md exists\n- **Content**: Verbose, mixed imperative form\n- **User Interaction**: \"Ask if needed\" without specifics\n- **Documentation**: One link buried in text\n- **Domain Standards**: Mentions best practices without enforcement\n- **Technical**: \"Handle errors\" without specifics\n- **Maintainability**: Everything in one file\n\n---\n\n## Example 3: Incomplete Skill\n\n**Skill**: hypothetical-minimal-skill\n\n```yaml\n---\nname: do-stuff\ndescription: Does stuff\n---\n\n# Do Stuff\n\nThis skill does stuff with things.\n\n## Usage\nJust ask it to do stuff.\n```\n\n### Category Scores\n\n| Category | Score | Weight | Weighted |\n|----------|-------|--------|----------|\n| Structure & Anatomy | 40/100 | 15% | 6.00 |\n| Content Quality | 20/100 | 20% | 4.00 |\n| User Interaction | 0/100 | 15% | 0.00 |\n| Documentation | 0/100 | 15% | 0.00 |\n| Domain Standards | 0/100 | 15% | 0.00 |\n| Technical Robustness | 0/100 | 10% | 0.00 |\n| Maintainability | 20/100 | 10% | 2.00 |\n| **Total** | | | **12.00** |\n\n**Rating**: Incomplete\n\n---\n\n## Example 4: Official skill-creator (Honest Assessment)\n\n**Skill**: skill-creator (Anthropic official)\n**Type**: Guide\n\n### Category Scores\n\n| Category | Score | Weight | Weighted |\n|----------|-------|--------|----------|\n| Structure & Anatomy | 90/100 | 15% | 13.50 |\n| Content Quality | 95/100 | 20% | 19.00 |\n| User Interaction | 50/100 | 15% | 7.50 |\n| Documentation | 30/100 | 15% | 4.50 |\n| Domain Standards | 40/100 | 15% | 6.00 |\n| Technical Robustness | 50/100 | 10% | 5.00 |\n| Maintainability | 85/100 | 10% | 8.50 |\n| **Total** | | | **64.00** |\n\n**Rating**: Adequate\n\n### Honest Breakdown\n\n**Structure & Anatomy (90/100)** - Excellent:\n- 357 lines (well under 500)\n- Complete frontmatter\n- 2 reference files\n- No extraneous files\n\n**Content Quality (95/100)** - Excellent:\n- Very concise\n- Consistent imperative form\n- Clear 6-step workflow\n- Good appropriate freedom guidance\n\n**User Interaction (50/100)** - Gaps:\n- Step 1 mentions asking users questions during skill creation process\n- But no structured \"Required Clarifications\" pattern\n- No guidance on context awareness\n- Missing \"what if user doesn't answer\"\n\n**Documentation (30/100)** - Missing:\n- No official documentation links\n- No fetch guidance for unlisted scenarios\n- References exist but no external sources\n- No version awareness\n\n**Domain Standards (40/100)** - Partial:\n- Has output-patterns.md reference\n- But no enforcement checklist\n- No \"Must Follow / Must Avoid\" pattern\n- No quality gates before delivery\n\n**Technical Robustness (50/100)** - Partial:\n- Scripts mentioned with testing requirement\n- But no error handling guidance\n- No security considerations\n- Dependencies implicit\n\n**Maintainability (85/100)** - Good:\n- Modular references\n- Clear organization\n- But no update path guidance\n\n### Improvement Recommendations for skill-creator\n\n1. **High Priority**: Add official documentation links (if Anthropic has docs)\n2. **High Priority**: Add structured clarification questions pattern\n3. **Medium Priority**: Add \"Must Follow / Must Avoid\" enforcement pattern\n4. **Medium Priority**: Add output checklist / quality gate\n5. **Low Priority**: Add error handling guidance for scripts\n6. **Low Priority**: Add update path / version awareness\n\n**Note**: This is an honest assessment. Even official skills can have room for improvement to reach production-level standards.\n\n---\n\n## Calibration Notes\n\n### Common Scoring Mistakes\n\n1. **Over-scoring structure**: A skill with clean structure but no content shouldn't score high overall\n\n2. **Under-scoring interaction**: Missing clarification patterns is a major gap for builder skills\n\n3. **Ignoring domain specificity**: A widget skill without UX standards is incomplete; a data skill without validation patterns is incomplete\n\n4. **Binary scoring**: Use the full 0-3 range, not just 0 or 3\n\n### Skill Type Adjustments\n\n**Builder Skills** (create artifacts):\n- User Interaction weight: Higher importance\n- Documentation weight: Higher importance\n- Clarification questions: Essential\n\n**Automation Skills** (run workflows):\n- Technical Robustness: Higher importance\n- Error handling: Critical\n- User Interaction: Lower unless ambiguous inputs\n\n**Guide Skills** (provide instructions):\n- Content Quality: Higher importance\n- Documentation: Higher importance\n- User Interaction: Medium (depends on complexity)\n",
        "skills/cover_letter_generator/.claude/skills/theme-factory/SKILL.md": "---\nname: theme-factory\ndescription: Toolkit for styling artifacts with a theme. These artifacts can be slides, docs, reportings, HTML landing pages, etc. There are 10 pre-set themes with colors/fonts that you can apply to any artifact that has been creating, or can generate a new theme on-the-fly.\nlicense: Complete terms in LICENSE.txt\n---\n\n\n# Theme Factory Skill\n\nThis skill provides a curated collection of professional font and color themes themes, each with carefully selected color palettes and font pairings. Once a theme is chosen, it can be applied to any artifact.\n\n## Purpose\n\nTo apply consistent, professional styling to presentation slide decks, use this skill. Each theme includes:\n- A cohesive color palette with hex codes\n- Complementary font pairings for headers and body text\n- A distinct visual identity suitable for different contexts and audiences\n\n## Usage Instructions\n\nTo apply styling to a slide deck or other artifact:\n\n1. **Show the theme showcase**: Display the `theme-showcase.pdf` file to allow users to see all available themes visually. Do not make any modifications to it; simply show the file for viewing.\n2. **Ask for their choice**: Ask which theme to apply to the deck\n3. **Wait for selection**: Get explicit confirmation about the chosen theme\n4. **Apply the theme**: Once a theme has been chosen, apply the selected theme's colors and fonts to the deck/artifact\n\n## Themes Available\n\nThe following 10 themes are available, each showcased in `theme-showcase.pdf`:\n\n1. **Ocean Depths** - Professional and calming maritime theme\n2. **Sunset Boulevard** - Warm and vibrant sunset colors\n3. **Forest Canopy** - Natural and grounded earth tones\n4. **Modern Minimalist** - Clean and contemporary grayscale\n5. **Golden Hour** - Rich and warm autumnal palette\n6. **Arctic Frost** - Cool and crisp winter-inspired theme\n7. **Desert Rose** - Soft and sophisticated dusty tones\n8. **Tech Innovation** - Bold and modern tech aesthetic\n9. **Botanical Garden** - Fresh and organic garden colors\n10. **Midnight Galaxy** - Dramatic and cosmic deep tones\n\n## Theme Details\n\nEach theme is defined in the `themes/` directory with complete specifications including:\n- Cohesive color palette with hex codes\n- Complementary font pairings for headers and body text\n- Distinct visual identity suitable for different contexts and audiences\n\n## Application Process\n\nAfter a preferred theme is selected:\n1. Read the corresponding theme file from the `themes/` directory\n2. Apply the specified colors and fonts consistently throughout the deck\n3. Ensure proper contrast and readability\n4. Maintain the theme's visual identity across all slides\n\n## Create your Own Theme\nTo handle cases where none of the existing themes work for an artifact, create a custom theme. Based on provided inputs, generate a new theme similar to the ones above. Give the theme a similar name describing what the font/color combinations represent. Use any basic description provided to choose appropriate colors/fonts. After generating the theme, show it for review and verification. Following that, apply the theme as described above.\n",
        "skills/cover_letter_generator/.claude/skills/theme-factory/themes/arctic-frost.md": "# Arctic Frost\n\nA cool and crisp winter-inspired theme that conveys clarity, precision, and professionalism.\n\n## Color Palette\n\n- **Ice Blue**: `#d4e4f7` - Light backgrounds and highlights\n- **Steel Blue**: `#4a6fa5` - Primary accent color\n- **Silver**: `#c0c0c0` - Metallic accent elements\n- **Crisp White**: `#fafafa` - Clean backgrounds and text\n\n## Typography\n\n- **Headers**: DejaVu Sans Bold\n- **Body Text**: DejaVu Sans\n\n## Best Used For\n\nHealthcare presentations, technology solutions, winter sports, clean tech, pharmaceutical content.\n",
        "skills/cover_letter_generator/.claude/skills/theme-factory/themes/botanical-garden.md": "# Botanical Garden\n\nA fresh and organic theme featuring vibrant garden-inspired colors for lively presentations.\n\n## Color Palette\n\n- **Fern Green**: `#4a7c59` - Rich natural green\n- **Marigold**: `#f9a620` - Bright floral accent\n- **Terracotta**: `#b7472a` - Earthy warm tone\n- **Cream**: `#f5f3ed` - Soft neutral backgrounds\n\n## Typography\n\n- **Headers**: DejaVu Serif Bold\n- **Body Text**: DejaVu Sans\n\n## Best Used For\n\nGarden centers, food presentations, farm-to-table content, botanical brands, natural products.\n",
        "skills/cover_letter_generator/.claude/skills/theme-factory/themes/desert-rose.md": "# Desert Rose\n\nA soft and sophisticated theme with dusty, muted tones perfect for elegant presentations.\n\n## Color Palette\n\n- **Dusty Rose**: `#d4a5a5` - Soft primary color\n- **Clay**: `#b87d6d` - Earthy accent\n- **Sand**: `#e8d5c4` - Warm neutral backgrounds\n- **Deep Burgundy**: `#5d2e46` - Rich dark contrast\n\n## Typography\n\n- **Headers**: FreeSans Bold\n- **Body Text**: FreeSans\n\n## Best Used For\n\nFashion presentations, beauty brands, wedding planning, interior design, boutique businesses.\n",
        "skills/cover_letter_generator/.claude/skills/theme-factory/themes/forest-canopy.md": "# Forest Canopy\n\nA natural and grounded theme featuring earth tones inspired by dense forest environments.\n\n## Color Palette\n\n- **Forest Green**: `#2d4a2b` - Primary dark green\n- **Sage**: `#7d8471` - Muted green accent\n- **Olive**: `#a4ac86` - Light accent color\n- **Ivory**: `#faf9f6` - Backgrounds and text\n\n## Typography\n\n- **Headers**: FreeSerif Bold\n- **Body Text**: FreeSans\n\n## Best Used For\n\nEnvironmental presentations, sustainability reports, outdoor brands, wellness content, organic products.\n",
        "skills/cover_letter_generator/.claude/skills/theme-factory/themes/golden-hour.md": "# Golden Hour\n\nA rich and warm autumnal palette that creates an inviting and sophisticated atmosphere.\n\n## Color Palette\n\n- **Mustard Yellow**: `#f4a900` - Bold primary accent\n- **Terracotta**: `#c1666b` - Warm secondary color\n- **Warm Beige**: `#d4b896` - Neutral backgrounds\n- **Chocolate Brown**: `#4a403a` - Dark text and anchors\n\n## Typography\n\n- **Headers**: FreeSans Bold\n- **Body Text**: FreeSans\n\n## Best Used For\n\nRestaurant presentations, hospitality brands, fall campaigns, cozy lifestyle content, artisan products.\n",
        "skills/cover_letter_generator/.claude/skills/theme-factory/themes/midnight-galaxy.md": "# Midnight Galaxy\n\nA dramatic and cosmic theme with deep purples and mystical tones for impactful presentations.\n\n## Color Palette\n\n- **Deep Purple**: `#2b1e3e` - Rich dark base\n- **Cosmic Blue**: `#4a4e8f` - Mystical mid-tone\n- **Lavender**: `#a490c2` - Soft accent color\n- **Silver**: `#e6e6fa` - Light highlights and text\n\n## Typography\n\n- **Headers**: FreeSans Bold\n- **Body Text**: FreeSans\n\n## Best Used For\n\nEntertainment industry, gaming presentations, nightlife venues, luxury brands, creative agencies.\n",
        "skills/cover_letter_generator/.claude/skills/theme-factory/themes/modern-minimalist.md": "# Modern Minimalist\n\nA clean and contemporary theme with a sophisticated grayscale palette for maximum versatility.\n\n## Color Palette\n\n- **Charcoal**: `#36454f` - Primary dark color\n- **Slate Gray**: `#708090` - Medium gray for accents\n- **Light Gray**: `#d3d3d3` - Backgrounds and dividers\n- **White**: `#ffffff` - Text and clean backgrounds\n\n## Typography\n\n- **Headers**: DejaVu Sans Bold\n- **Body Text**: DejaVu Sans\n\n## Best Used For\n\nTech presentations, architecture portfolios, design showcases, modern business proposals, data visualization.\n",
        "skills/cover_letter_generator/.claude/skills/theme-factory/themes/ocean-depths.md": "# Ocean Depths\n\nA professional and calming maritime theme that evokes the serenity of deep ocean waters.\n\n## Color Palette\n\n- **Deep Navy**: `#1a2332` - Primary background color\n- **Teal**: `#2d8b8b` - Accent color for highlights and emphasis\n- **Seafoam**: `#a8dadc` - Secondary accent for lighter elements\n- **Cream**: `#f1faee` - Text and light backgrounds\n\n## Typography\n\n- **Headers**: DejaVu Sans Bold\n- **Body Text**: DejaVu Sans\n\n## Best Used For\n\nCorporate presentations, financial reports, professional consulting decks, trust-building content.\n",
        "skills/cover_letter_generator/.claude/skills/theme-factory/themes/sunset-boulevard.md": "# Sunset Boulevard\n\nA warm and vibrant theme inspired by golden hour sunsets, perfect for energetic and creative presentations.\n\n## Color Palette\n\n- **Burnt Orange**: `#e76f51` - Primary accent color\n- **Coral**: `#f4a261` - Secondary warm accent\n- **Warm Sand**: `#e9c46a` - Highlighting and backgrounds\n- **Deep Purple**: `#264653` - Dark contrast and text\n\n## Typography\n\n- **Headers**: DejaVu Serif Bold\n- **Body Text**: DejaVu Sans\n\n## Best Used For\n\nCreative pitches, marketing presentations, lifestyle brands, event promotions, inspirational content.\n",
        "skills/cover_letter_generator/.claude/skills/theme-factory/themes/tech-innovation.md": "# Tech Innovation\n\nA bold and modern theme with high-contrast colors perfect for cutting-edge technology presentations.\n\n## Color Palette\n\n- **Electric Blue**: `#0066ff` - Vibrant primary accent\n- **Neon Cyan**: `#00ffff` - Bright highlight color\n- **Dark Gray**: `#1e1e1e` - Deep backgrounds\n- **White**: `#ffffff` - Clean text and contrast\n\n## Typography\n\n- **Headers**: DejaVu Sans Bold\n- **Body Text**: DejaVu Sans\n\n## Best Used For\n\nTech startups, software launches, innovation showcases, AI/ML presentations, digital transformation content.\n",
        "skills/cover_letter_generator/.claude/skills/xlsx/SKILL.md": "---\nname: xlsx\ndescription: \"Comprehensive spreadsheet creation, editing, and analysis with support for formulas, formatting, data analysis, and visualization. When Claude needs to work with spreadsheets (.xlsx, .xlsm, .csv, .tsv, etc) for: (1) Creating new spreadsheets with formulas and formatting, (2) Reading or analyzing data, (3) Modify existing spreadsheets while preserving formulas, (4) Data analysis and visualization in spreadsheets, or (5) Recalculating formulas\"\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Requirements for Outputs\n\n## All Excel files\n\n### Zero Formula Errors\n- Every Excel model MUST be delivered with ZERO formula errors (#REF!, #DIV/0!, #VALUE!, #N/A, #NAME?)\n\n### Preserve Existing Templates (when updating templates)\n- Study and EXACTLY match existing format, style, and conventions when modifying files\n- Never impose standardized formatting on files with established patterns\n- Existing template conventions ALWAYS override these guidelines\n\n## Financial models\n\n### Color Coding Standards\nUnless otherwise stated by the user or existing template\n\n#### Industry-Standard Color Conventions\n- **Blue text (RGB: 0,0,255)**: Hardcoded inputs, and numbers users will change for scenarios\n- **Black text (RGB: 0,0,0)**: ALL formulas and calculations\n- **Green text (RGB: 0,128,0)**: Links pulling from other worksheets within same workbook\n- **Red text (RGB: 255,0,0)**: External links to other files\n- **Yellow background (RGB: 255,255,0)**: Key assumptions needing attention or cells that need to be updated\n\n### Number Formatting Standards\n\n#### Required Format Rules\n- **Years**: Format as text strings (e.g., \"2024\" not \"2,024\")\n- **Currency**: Use $#,##0 format; ALWAYS specify units in headers (\"Revenue ($mm)\")\n- **Zeros**: Use number formatting to make all zeros \"-\", including percentages (e.g., \"$#,##0;($#,##0);-\")\n- **Percentages**: Default to 0.0% format (one decimal)\n- **Multiples**: Format as 0.0x for valuation multiples (EV/EBITDA, P/E)\n- **Negative numbers**: Use parentheses (123) not minus -123\n\n### Formula Construction Rules\n\n#### Assumptions Placement\n- Place ALL assumptions (growth rates, margins, multiples, etc.) in separate assumption cells\n- Use cell references instead of hardcoded values in formulas\n- Example: Use =B5*(1+$B$6) instead of =B5*1.05\n\n#### Formula Error Prevention\n- Verify all cell references are correct\n- Check for off-by-one errors in ranges\n- Ensure consistent formulas across all projection periods\n- Test with edge cases (zero values, negative numbers)\n- Verify no unintended circular references\n\n#### Documentation Requirements for Hardcodes\n- Comment or in cells beside (if end of table). Format: \"Source: [System/Document], [Date], [Specific Reference], [URL if applicable]\"\n- Examples:\n  - \"Source: Company 10-K, FY2024, Page 45, Revenue Note, [SEC EDGAR URL]\"\n  - \"Source: Company 10-Q, Q2 2025, Exhibit 99.1, [SEC EDGAR URL]\"\n  - \"Source: Bloomberg Terminal, 8/15/2025, AAPL US Equity\"\n  - \"Source: FactSet, 8/20/2025, Consensus Estimates Screen\"\n\n# XLSX creation, editing, and analysis\n\n## Overview\n\nA user may ask you to create, edit, or analyze the contents of an .xlsx file. You have different tools and workflows available for different tasks.\n\n## Important Requirements\n\n**LibreOffice Required for Formula Recalculation**: You can assume LibreOffice is installed for recalculating formula values using the `recalc.py` script. The script automatically configures LibreOffice on first run\n\n## Reading and analyzing data\n\n### Data analysis with pandas\nFor data analysis, visualization, and basic operations, use **pandas** which provides powerful data manipulation capabilities:\n\n```python\nimport pandas as pd\n\n# Read Excel\ndf = pd.read_excel('file.xlsx')  # Default: first sheet\nall_sheets = pd.read_excel('file.xlsx', sheet_name=None)  # All sheets as dict\n\n# Analyze\ndf.head()      # Preview data\ndf.info()      # Column info\ndf.describe()  # Statistics\n\n# Write Excel\ndf.to_excel('output.xlsx', index=False)\n```\n\n## Excel File Workflows\n\n## CRITICAL: Use Formulas, Not Hardcoded Values\n\n**Always use Excel formulas instead of calculating values in Python and hardcoding them.** This ensures the spreadsheet remains dynamic and updateable.\n\n###  WRONG - Hardcoding Calculated Values\n```python\n# Bad: Calculating in Python and hardcoding result\ntotal = df['Sales'].sum()\nsheet['B10'] = total  # Hardcodes 5000\n\n# Bad: Computing growth rate in Python\ngrowth = (df.iloc[-1]['Revenue'] - df.iloc[0]['Revenue']) / df.iloc[0]['Revenue']\nsheet['C5'] = growth  # Hardcodes 0.15\n\n# Bad: Python calculation for average\navg = sum(values) / len(values)\nsheet['D20'] = avg  # Hardcodes 42.5\n```\n\n###  CORRECT - Using Excel Formulas\n```python\n# Good: Let Excel calculate the sum\nsheet['B10'] = '=SUM(B2:B9)'\n\n# Good: Growth rate as Excel formula\nsheet['C5'] = '=(C4-C2)/C2'\n\n# Good: Average using Excel function\nsheet['D20'] = '=AVERAGE(D2:D19)'\n```\n\nThis applies to ALL calculations - totals, percentages, ratios, differences, etc. The spreadsheet should be able to recalculate when source data changes.\n\n## Common Workflow\n1. **Choose tool**: pandas for data, openpyxl for formulas/formatting\n2. **Create/Load**: Create new workbook or load existing file\n3. **Modify**: Add/edit data, formulas, and formatting\n4. **Save**: Write to file\n5. **Recalculate formulas (MANDATORY IF USING FORMULAS)**: Use the recalc.py script\n   ```bash\n   python recalc.py output.xlsx\n   ```\n6. **Verify and fix any errors**: \n   - The script returns JSON with error details\n   - If `status` is `errors_found`, check `error_summary` for specific error types and locations\n   - Fix the identified errors and recalculate again\n   - Common errors to fix:\n     - `#REF!`: Invalid cell references\n     - `#DIV/0!`: Division by zero\n     - `#VALUE!`: Wrong data type in formula\n     - `#NAME?`: Unrecognized formula name\n\n### Creating new Excel files\n\n```python\n# Using openpyxl for formulas and formatting\nfrom openpyxl import Workbook\nfrom openpyxl.styles import Font, PatternFill, Alignment\n\nwb = Workbook()\nsheet = wb.active\n\n# Add data\nsheet['A1'] = 'Hello'\nsheet['B1'] = 'World'\nsheet.append(['Row', 'of', 'data'])\n\n# Add formula\nsheet['B2'] = '=SUM(A1:A10)'\n\n# Formatting\nsheet['A1'].font = Font(bold=True, color='FF0000')\nsheet['A1'].fill = PatternFill('solid', start_color='FFFF00')\nsheet['A1'].alignment = Alignment(horizontal='center')\n\n# Column width\nsheet.column_dimensions['A'].width = 20\n\nwb.save('output.xlsx')\n```\n\n### Editing existing Excel files\n\n```python\n# Using openpyxl to preserve formulas and formatting\nfrom openpyxl import load_workbook\n\n# Load existing file\nwb = load_workbook('existing.xlsx')\nsheet = wb.active  # or wb['SheetName'] for specific sheet\n\n# Working with multiple sheets\nfor sheet_name in wb.sheetnames:\n    sheet = wb[sheet_name]\n    print(f\"Sheet: {sheet_name}\")\n\n# Modify cells\nsheet['A1'] = 'New Value'\nsheet.insert_rows(2)  # Insert row at position 2\nsheet.delete_cols(3)  # Delete column 3\n\n# Add new sheet\nnew_sheet = wb.create_sheet('NewSheet')\nnew_sheet['A1'] = 'Data'\n\nwb.save('modified.xlsx')\n```\n\n## Recalculating formulas\n\nExcel files created or modified by openpyxl contain formulas as strings but not calculated values. Use the provided `recalc.py` script to recalculate formulas:\n\n```bash\npython recalc.py <excel_file> [timeout_seconds]\n```\n\nExample:\n```bash\npython recalc.py output.xlsx 30\n```\n\nThe script:\n- Automatically sets up LibreOffice macro on first run\n- Recalculates all formulas in all sheets\n- Scans ALL cells for Excel errors (#REF!, #DIV/0!, etc.)\n- Returns JSON with detailed error locations and counts\n- Works on both Linux and macOS\n\n## Formula Verification Checklist\n\nQuick checks to ensure formulas work correctly:\n\n### Essential Verification\n- [ ] **Test 2-3 sample references**: Verify they pull correct values before building full model\n- [ ] **Column mapping**: Confirm Excel columns match (e.g., column 64 = BL, not BK)\n- [ ] **Row offset**: Remember Excel rows are 1-indexed (DataFrame row 5 = Excel row 6)\n\n### Common Pitfalls\n- [ ] **NaN handling**: Check for null values with `pd.notna()`\n- [ ] **Far-right columns**: FY data often in columns 50+ \n- [ ] **Multiple matches**: Search all occurrences, not just first\n- [ ] **Division by zero**: Check denominators before using `/` in formulas (#DIV/0!)\n- [ ] **Wrong references**: Verify all cell references point to intended cells (#REF!)\n- [ ] **Cross-sheet references**: Use correct format (Sheet1!A1) for linking sheets\n\n### Formula Testing Strategy\n- [ ] **Start small**: Test formulas on 2-3 cells before applying broadly\n- [ ] **Verify dependencies**: Check all cells referenced in formulas exist\n- [ ] **Test edge cases**: Include zero, negative, and very large values\n\n### Interpreting recalc.py Output\nThe script returns JSON with error details:\n```json\n{\n  \"status\": \"success\",           // or \"errors_found\"\n  \"total_errors\": 0,              // Total error count\n  \"total_formulas\": 42,           // Number of formulas in file\n  \"error_summary\": {              // Only present if errors found\n    \"#REF!\": {\n      \"count\": 2,\n      \"locations\": [\"Sheet1!B5\", \"Sheet1!C10\"]\n    }\n  }\n}\n```\n\n## Best Practices\n\n### Library Selection\n- **pandas**: Best for data analysis, bulk operations, and simple data export\n- **openpyxl**: Best for complex formatting, formulas, and Excel-specific features\n\n### Working with openpyxl\n- Cell indices are 1-based (row=1, column=1 refers to cell A1)\n- Use `data_only=True` to read calculated values: `load_workbook('file.xlsx', data_only=True)`\n- **Warning**: If opened with `data_only=True` and saved, formulas are replaced with values and permanently lost\n- For large files: Use `read_only=True` for reading or `write_only=True` for writing\n- Formulas are preserved but not evaluated - use recalc.py to update values\n\n### Working with pandas\n- Specify data types to avoid inference issues: `pd.read_excel('file.xlsx', dtype={'id': str})`\n- For large files, read specific columns: `pd.read_excel('file.xlsx', usecols=['A', 'C', 'E'])`\n- Handle dates properly: `pd.read_excel('file.xlsx', parse_dates=['date_column'])`\n\n## Code Style Guidelines\n**IMPORTANT**: When generating Python code for Excel operations:\n- Write minimal, concise Python code without unnecessary comments\n- Avoid verbose variable names and redundant operations\n- Avoid unnecessary print statements\n\n**For Excel files themselves**:\n- Add comments to cells with complex formulas or important assumptions\n- Document data sources for hardcoded values\n- Include notes for key calculations and model sections",
        "skills/cover_letter_generator/documentation/cover_letter_criteria.md": "When applying for a specialized AI role on LinkedIn, your cover letter shouldn't just repeat your resumeit needs to prove you have the \"human\" insight to manage machine intelligence.\r\n\r\nHere are the top 5 qualities of an outstanding AI professional cover letter:\r\n\r\n1. Evidence of \"AI Problem-Solving\" (Not Just Tools)\r\nAn outstanding letter doesn't just list Python or PyTorch; it describes a specific business problem you solved using AI.\r\n\r\n\tThe Quality: Focus on the impact ($Output$). Instead of saying \"I know NLP,\" say \"I implemented a transformer-based model that reduced customer support response times by 40%.\"\r\n\tWhy it works: It proves you understand that AI is a tool for ROI, not just a laboratory experiment.\r\n\r\n2. Alignment with the Companys Specific AI Ethics\r\nAI roles are increasingly scrutinized for safety and bias.\r\n\r\n\tThe Quality: Mentioning your commitment to Responsible AI or Explainability (XAI). If the company (e.g., Anthropic or Google) emphasizes safety, show you understand how to audit models for bias.\r\n\tWhy it works: It shows you aren't just a \"coder,\" but a mature professional who understands the societal risks of the technology.\r\n\r\n3. \"The Translation Layer\" (Communication Skills)\r\nOutstanding AI professionals must explain complex neural networks to stakeholders who don't know what a \"weight\" or \"bias\" is.\r\n\r\n\tThe Quality: The letter itself should be written with high-level clarity. Mentioning experience \"presenting model performance metrics to non-technical executive leadership\" is a major green flag.\r\n\tWhy it works: Most AI projects fail due to poor communication between the data team and the business team. You are proving you can bridge that gap.\r\n\r\n4. Technical Agility & Learning Velocity\r\nThe AI field moves faster than any other industry. What was \"State of the Art\" (SOTA) six months ago is often obsolete today.\r\n\r\n\tThe Quality: Demonstrating that you stay current with research papers (arXiv) or specific new architectures (like Diffusion models or Mamba).\r\n\tWhy it works: It reassures the hiring manager that you won't become technically stagnant as the field evolves.\r\n\r\n5. Hyper-Personalization to the \"LinkedIn Context\"\r\nSince you found the job on LinkedIn, use the \"intelligence\" available there.\r\n\r\n\tThe Quality: Mentioning a recent post by the company's Head of AI or a specific technical challenge the company recently blogged about.\r\n\tWhy it works: It proves you aren't sending a \"mass-blast\" template. It shows you have done your research and are genuinely interested in their specific stack and mission.\r\n",
        "skills/cover_letter_generator/documentation/gemini_readme_generator.md": "Create a powerful markdown file called \"readme.md\" with useful details about the top 5 qualities of a successful professional cover letter for a specific AI job on LinkedIn as described in the PSI method. Also use the following template to briefly describe how the solution is to use these top 5 qualities to relate our key skills from the resume to company's technical problem recently blogged on company's LinkedIn page.  Start this readme.md file exactly with following code to place a header image at the top.\r\n\"![Banner](./header.jpg)\"\r\n\r\n\r\n\r\n\"This template uses the Problem-Solution-Impact (PSI) method while weaving in the five core qualities: impact, ethics, communication, learning velocity, and personalization.\r\n\r\n\r\n\r\n[Your Name]\r\n\r\n[Your LinkedIn Profile URL] | [Your Portfolio/GitHub]\r\n\r\n\r\n\r\nSubject: [Job Title]  Solving [Company Name]s [Specific AI Challenge]\r\n\r\n\r\n\r\nDear [Hiring Manager Name or \"AI Talent Team\"],\r\n\r\n\r\n\r\n[PROBLEM] I noticed in [Company Name]s recent technical blog post that you are currently scaling your [specific system, e.g., Agentic workflow or RAG pipeline], which often introduces significant challenges in model latency and hallucination management. In the rapidly evolving AI landscape, many organizations struggle to move past the \"prototype phase\" because they lack models that are both high-performing and ethically transparent. I am writing to you because I specialize in bridging this gapensuring that cutting-edge SOTA architectures aren't just technically impressive, but are production-ready, explainable, and aligned with business ROI.\r\n\r\n\r\n\r\n[SOLUTION] To address similar bottlenecks in my previous role, I didnt just deploy standard models; I engineered a [Specific Solution, e.g., custom fine-tuning strategy or hybrid retrieval-augmented system] using [Specific Tools, e.g., PyTorch and LangGraph]. My approach centers on Responsible AI; I prioritize building \"The Translation Layer\" by creating visualization dashboards that explain model decision-making to non-technical stakeholders. Furthermore, I maintain high learning velocity by actively implementing recent research from arXiv, such as [mention a recent trend like \"Mamba architectures\" or \"Speculative Decoding\"], ensuring your stack remains future-proof.\r\n\r\n\r\n\r\n[IMPACT] The result of this methodology was a [Quantitative Impact, e.g., 35% reduction in inference costs and a 20% boost in retrieval accuracy] for [Previous Project/Company]. By hyper-personalizing AI solutions to meet specific user needs while maintaining strict ethical audits, I helped the organization realize immediate ROI. I am eager to bring this same focus on quantified impact and technical agility to the [Job Title] role at [Company Name]. I look forward to discussing how my background in [Your Niche] can help your team dominate the [Specific Industry] market.\r\n\r\n\r\n\r\nBest regards,\r\n\r\n\r\n\r\n[Your Name]\"\r\n\r\n",
        "skills/cover_letter_generator/output/cover_letter_anthropic.md": "# Cover Letter - Anthropic\n\n**Ali Jilani**\n[LinkedIn](https://linkedin.com/in/alijilani) | [GitHub](https://github.com/alijilani-dev)\n\n---\n\n**Subject:** Software Engineer, Claude Code  Bridging Agentic AI Development with Enterprise-Scale Reliability\n\nDear Claude Code Talent Team,\n\n**[PROBLEM]** I've observed that Anthropic is pioneering the next generation of agentic coding tools with Claude Code, tackling one of AI's most complex challenges: creating developer tools that enhance productivity while maintaining the secure, reliable foundations that distinguish responsible AI from experimental prototypes. In a landscape where AI-assisted development is rapidly evolving, the challenge isn't merely building intelligent featuresit's ensuring these tools operate within stringent safety and compliance frameworks while remaining genuinely useful across diverse programming environments.\n\n**[SOLUTION]** My approach to this challenge comes from hands-on experience building production-grade AI systems. As an Agentic AI Developer, I work daily with Claude Code alongside Python, n8n, and Make.com to construct autonomous workflows that solve real business problems. What sets my methodology apart is my commitment to \"The Translation Layer\"throughout my 18+ years in IT, I've consistently bridged the gap between complex technical implementations and stakeholder understanding. At WorkPlains, I architected a Document Management System with version control, security classification, and WebDAV integrationdemonstrating the same full-stack thinking and attention to secure foundations that Claude Code demands. My experience spans regulated environments including KYC/AML compliance systems at ARY Speed Remit, where reliability and security weren't optionalthey were mandatory. I maintain high learning velocity by actively following advances in agentic AI architectures, prompt engineering patterns, and LLM orchestration techniques, ensuring my implementations leverage current best practices rather than yesterday's patterns.\n\n**[IMPACT]** The quantifiable results of this methodology include building and maintaining trading platforms serving 500+ international traders across the UK, Germany, and beyondsystems requiring the scalability, security, and reliability that enterprise users demand. I delivered B2B data exchange systems with FTPS integration, proving my ability to work across unfamiliar domains and security-sensitive protocols. By combining DevOps expertise (AWS EC2, Docker, CI/CD pipelines) with hands-on AI development, I've helped organizations move past the prototype phase into production-ready deployments. I am eager to bring this same focus on secure foundations, developer empathy, and measurable impact to the Claude Code team at Anthropic, helping build tools that genuinely enhance developer creativity rather than replace human ingenuity.\n\nBest regards,\n\n**Ali Jilani**\nProject Manager & Agentic AI Developer\n",
        "skills/cover_letter_generator/output/cover_letter_deloitte.md": "# Cover Letter - Deloitte\n\n**Ali Jilani**\n[LinkedIn](https://linkedin.com/in/alijilani) | [GitHub](https://github.com/alijilani-dev)\n\n---\n\n**Subject:** Consultant-Gen AI  Delivering Production-Ready Agentic AI Solutions Across Industries\n\nDear Deloitte AI & Data Talent Team,\n\n**[PROBLEM]** Deloitte's AI & Data practice is at the forefront of helping enterprises transition from AI experimentation to production-grade deployment. Organizations across sectors struggle with a critical bottleneck: they have access to powerful LLM models like GPT-4, Gemini, and Bedrock, yet lack the architectural expertise to implement RAG pipelines and agentic workflows that deliver measurable business ROI. The gap between a promising AI prototype and a scalable, compliant enterprise solution requires consultants who understand both the cutting-edge technology and the practical realities of regulated industries.\n\n**[SOLUTION]** I specialize in bridging exactly this gap. As a Project Manager & Agentic AI Developer with 18+ years of multidisciplinary IT experience, I bring hands-on expertise with Claude Code, Python, and NLP, combined with practical agentic workflow implementation using n8n and Make.com. My approach centers on Responsible AII prioritize explainability and \"The Translation Layer,\" having spent years presenting complex technical implementations to non-technical executive leadership across fintech, telecom, and international trade sectors. At ARY Speed Remit, I re-engineered web portals to meet KYC/AML compliance requirements, demonstrating my ability to deliver AI-adjacent solutions in heavily regulated environments. My DevOps foundation (AWS EC2, RDS, Docker, CI/CD pipelines) ensures that the Gen AI solutions I architect don't just work in developmentthey scale reliably in production. I stay current with transformer architectures, diffusion models, and the latest research from arXiv, ensuring my recommendations reflect today's best practices, not last year's patterns.\n\n**[IMPACT]** This methodology has produced quantifiable results: I built and maintained trading platforms serving 500+ international traders with users spanning 250+ from the UK and 175+ from Germany, requiring robust scalability and security. My Document Management System implementations at WorkPlains featured enterprise-grade capabilities including document classification, version control, and secure WebDAV integrationthe same architectural thinking required for vector database implementations and RAG pipelines. Across my career spanning Ireland, UAE, and Pakistan, I've delivered AI and software solutions in telecom, fintech, education, and logisticsthe cross-industry experience that Deloitte's diverse client portfolio demands. I am eager to bring this combination of technical depth, consulting acumen, and proven delivery capability to the Gen AI Consultant role, helping Deloitte's clients realize immediate ROI from their AI investments.\n\nBest regards,\n\n**Ali Jilani**\nProject Manager & Agentic AI Developer\nMasters in Enterprise Software Systems\n",
        "skills/cover_letter_generator/readme.md": "![Banner](./header.jpg)\r\n\r\n#  Mastering the AI Cover Letter: The PSI Method\r\n\r\nThis guide outlines the strategic framework for crafting high-impact cover letters for **Artificial Intelligence** roles. By using the **Problem-Solution-Impact (PSI)** method, you can effectively bridge the gap between your technical resume and a company's real-world business challenges.\r\n\r\n---\r\n\r\n##  Top 5 Qualities of an Outstanding AI Cover Letter\r\n\r\nTo land an interview in the competitive AI landscape, your cover letter must go beyond a simple list of skills. It must embody these five professional pillars:\r\n\r\n1. **Quantified Impact:** Move beyond \"knowing\" tools to \"delivering\" results. Always lead with measurable outcomes (e.g.,  reduction in inference latency).\r\n2. **Ethics & Explainability:** Demonstrate a commitment to **Responsible AI**. Show that you don't just build \"black boxes,\" but prioritize model transparency and bias mitigation.\r\n3. **The Translation Layer:** Prove you can explain complex neural network logic to non-technical stakeholders. This \"bilingual\" ability is the #1 soft skill for AI engineers.\r\n4. **Learning Velocity:** The field moves at light speed. Mentioning your ability to implement the latest SOTA (State of the Art) research from arXiv shows you are future-proof.\r\n5. **Hyper-Personalization:** Use LinkedIn \"Intelligence.\" Reference a specific technical blog post or a challenge mentioned by the company's Engineering Lead to show genuine interest.\r\n\r\n---\r\n\r\n##  The PSI Method: Relating Skills to LinkedIn Insights\r\n\r\nThe **Problem-Solution-Impact (PSI)** method is a specialized framework designed to connect your key skills directly to a technical problem the company has recently publicized (e.g., on their LinkedIn Engineering blog).\r\n\r\n* **Problem:** Identify a specific technical bottleneck the company is facing.\r\n* **Solution:** Explain how your specific skill set (e.g., RAG, LLMOps, Fine-tuning) provides a direct remedy.\r\n* **Impact:** Share a past success story that proves your solution delivers concrete value.\r\n\r\n---\r\n\r\n##  The PSI Cover Letter Template\r\n\r\nThis template uses the Problem-Solution-Impact (PSI) method while weaving in the five core qualities: impact, ethics, communication, learning velocity, and personalization.\r\n\r\n---\r\n\r\n**[Your Name]** **[Your LinkedIn Profile URL] | [Your Portfolio/GitHub]**\r\n\r\n**Subject:** [Job Title]  Solving [Company Name]s [Specific AI Challenge]\r\n\r\n**Dear [Hiring Manager Name or \"AI Talent Team\"],**\r\n\r\n**[PROBLEM]** I noticed in [Company Name]s recent technical blog post that you are currently scaling your [specific system, e.g., Agentic workflow or RAG pipeline], which often introduces significant challenges in model latency and hallucination management. In the rapidly evolving AI landscape, many organizations struggle to move past the \"prototype phase\" because they lack models that are both high-performing and ethically transparent. I am writing to you because I specialize in bridging this gapensuring that cutting-edge SOTA architectures aren't just technically impressive, but are production-ready, explainable, and aligned with business ROI.\r\n\r\n**[SOLUTION]** To address similar bottlenecks in my previous role, I didnt just deploy standard models; I engineered a [Specific Solution, e.g., custom fine-tuning strategy or hybrid retrieval-augmented system] using [Specific Tools, e.g., PyTorch and LangGraph]. My approach centers on Responsible AI; I prioritize building \"The Translation Layer\" by creating visualization dashboards that explain model decision-making to non-technical stakeholders. Furthermore, I maintain high learning velocity by actively implementing recent research from arXiv, such as [mention a recent trend like \"Mamba architectures\" or \"Speculative Decoding\"], ensuring your stack remains future-proof.\r\n\r\n**[IMPACT]** The result of this methodology was a [Quantitative Impact, e.g., 35% reduction in inference costs and a 20% boost in retrieval accuracy] for [Previous Project/Company]. By hyper-personalizing AI solutions to meet specific user needs while maintaining strict ethical audits, I helped the organization realize immediate ROI. I am eager to bring this same focus on quantified impact and technical agility to the [Job Title] role at [Company Name]. I look forward to discussing how my background in [Your Niche] can help your team dominate the [Specific Industry] market.\r\n\r\n**Best regards,**\r\n\r\n**[Your Name]**\r\n\r\n---\r\n\r\n##  How to Use This Template\r\n\r\n1. **Monitor LinkedIn:** Follow the company's Engineering or \"Life\" page to find recent technical updates.\r\n2. **Select a \"Problem\":** Find a technical hurdle they mentioned (e.g., \"Improving data quality for LLMs\").\r\n3. **Customize Brackets:** Replace the `[bracketed]` text with your actual skills and metrics.\r\n4. **Proofread:** Ensure the \"Translation Layer\" (your writing style) is clear, professional, and concise.\r\n\r\n**Would you like me to help you find a specific technical problem from a company's recent LinkedIn post to fill out the [PROBLEM] section of this template?**",
        "skills/docker-rocker/.claude-plugin/plugin.json": "{\n  \"name\": \"docker-rocker\",\n  \"description\": \"Docker containerization expert for FastAPI projects - Dockerfiles, docker-compose, multi-stage builds, and security best practices\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Ali Jilani\"\n  },\n  \"category\": \"development\",\n  \"keywords\": [\"docker\", \"containerization\", \"fastapi\", \"devops\", \"docker-compose\"]\n}\n",
        "skills/docker-rocker/SKILL.md": "---\nname: docker-rocker\ndescription: |\n  Production-grade Docker containerization for FastAPI, Pytest, SQLModel, and Pydantic projects.\n  This skill should be used when users ask to dockerize FastAPI applications, create multi-stage\n  Docker builds, optimize Docker images for Python APIs, set up CI/CD Docker pipelines, or\n  deploy containerized Python web APIs with maximum performance and minimal image size.\n---\n\n# Docker Rocker\n\nProduction-grade Docker automation for Python FastAPI web API projects using multi-stage builds, UV package manager, and performance optimization strategies.\n\n## What This Skill Does\n\n- Creates optimized multi-stage Dockerfiles (Builder  Runtime  Testing stages)\n- Configures UV package manager for 50-70% faster dependency installation\n- Implements layer caching strategies for rapid rebuilds\n- Sets up non-root user security configurations\n- Generates docker-compose files for development and production\n- Optimizes for SQLModel/Neon database connections in containers\n- Configures Pydantic settings for container environments\n\n## What This Skill Does NOT Do\n\n- Manage Kubernetes deployments (use dedicated k8s skills)\n- Handle cloud-specific container registries (AWS ECR, GCR, ACR)\n- Configure reverse proxies (Nginx, Traefik)\n- Manage container orchestration at scale\n- Handle secrets management beyond environment variables\n\n---\n\n## Before Implementation\n\nGather context to ensure successful implementation:\n\n| Source | Gather |\n|--------|--------|\n| **Codebase** | Project structure, existing Dockerfile, pyproject.toml/requirements.txt, app entry point |\n| **Conversation** | Target deployment (dev/staging/prod), database connection requirements, special dependencies |\n| **Skill References** | Multi-stage patterns from `references/`, optimization strategies, security practices |\n| **User Guidelines** | Team Docker conventions, registry requirements, CI/CD integration needs |\n\nEnsure all required context is gathered before implementing.\nOnly ask user for THEIR specific requirements (domain expertise is in this skill).\n\n---\n\n## Required Clarifications\n\nAsk about USER's context before generating Dockerfiles:\n\n| Question | Purpose |\n|----------|---------|\n| **Deployment target?** | Dev (hot-reload) vs Production (optimized) vs CI/CD (testing) |\n| **Package manager?** | UV (recommended) vs pip vs Poetry |\n| **Database type?** | Neon PostgreSQL, local PostgreSQL, SQLite, other |\n| **Special system deps?** | Libraries requiring apt packages (Pillow, psycopg2, etc.) |\n| **Port configuration?** | Default 80 or custom port |\n\n---\n\n## Docker Deployment Stages\n\n### Stage Overview\n\n| Stage | Purpose | Base Image | Final Size |\n|-------|---------|------------|------------|\n| **Builder** | Install dependencies, compile packages | python:3.12-slim | ~400MB (discarded) |\n| **Runtime** | Production application | python:3.12-slim | ~150MB |\n| **Testing** | CI/CD test execution | extends Builder | ~450MB |\n| **Development** | Hot-reload for local dev | python:3.12-slim | ~300MB |\n\n### Stage Selection Logic\n\n```\nWhat is the deployment target?\n\nProduction deployment?\n   Use Builder + Runtime stages\n   Minimal image, no dev dependencies\n\nCI/CD pipeline?\n   Use Builder + Testing stage\n   Include pytest, dev dependencies\n\nLocal development?\n   Use Development stage\n   Volume mounts, hot-reload enabled\n\nAll of the above?\n   Generate complete multi-stage Dockerfile with all targets\n```\n\n---\n\n## Workflow\n\n### 1. Analyze Project Structure\n\n```bash\n# Check for existing configurations\nls -la pyproject.toml uv.lock requirements*.txt Dockerfile docker-compose.yml\n```\n\nIdentify:\n- Package manager (UV if uv.lock exists, Poetry if poetry.lock, pip otherwise)\n- Entry point (app/main.py, src/main.py, main.py)\n- Dependencies requiring system packages\n\n### 2. Select Dockerfile Template\n\nBased on user's deployment target, use appropriate template from `assets/templates/`:\n\n| Target | Template |\n|--------|----------|\n| Production only | `Dockerfile.production` |\n| With testing | `Dockerfile.complete` |\n| Development | `Dockerfile.dev` |\n| Full multi-stage | `Dockerfile.multistage` |\n\n### 3. Configure Environment Variables\n\n```dockerfile\n# Performance optimizations\nENV UV_LINK_MODE=copy \\\n    UV_COMPILE_BYTECODE=1 \\\n    PYTHONDONTWRITEBYTECODE=1 \\\n    PYTHONUNBUFFERED=1\n\n# Application settings\nENV APP_MODULE=app.main:app \\\n    PORT=80\n```\n\n### 4. Optimize Layer Caching\n\nOrder Dockerfile instructions by change frequency:\n1. Base image (rarely changes)\n2. System dependencies (rarely changes)\n3. Python dependencies (changes occasionally)\n4. Application code (changes frequently)\n\n### 5. Configure Database Connections\n\nFor Neon/PostgreSQL in containers:\n\n```python\n# Use NullPool - let external pooler handle connections\nfrom sqlalchemy.pool import NullPool\n\nengine = create_async_engine(\n    DATABASE_URL,\n    poolclass=NullPool,\n    pool_pre_ping=True,\n)\n```\n\n### 6. Generate docker-compose\n\nCreate appropriate compose file for target environment.\n\n### 7. Validate Build\n\n```bash\n# Build and verify\ndocker build --target runtime -t app:latest .\ndocker run --rm app:latest python -c \"import app; print('OK')\"\n```\n\n---\n\n## Quick Commands\n\n| Task | Command |\n|------|---------|\n| Build production image | `docker build --target runtime -t app:prod .` |\n| Build test image | `docker build --target testing -t app:test .` |\n| Run tests in container | `docker run --rm app:test pytest -v` |\n| Build with no cache | `docker build --no-cache -t app:latest .` |\n| Check image size | `docker images app:latest` |\n| Scan for vulnerabilities | `docker scout cves app:latest` |\n\n---\n\n## Error Handling\n\n| Error | Cause | Solution |\n|-------|-------|----------|\n| `uv: command not found` | UV not installed in image | Add `COPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/uv` |\n| `ModuleNotFoundError` | Dependencies not in runtime | Verify COPY of .venv from builder |\n| `Permission denied` | Non-root user can't write | Use `--chown=app:app` on COPY |\n| `Connection refused` to DB | Network isolation | Use docker-compose networks or host.docker.internal |\n| `OOM killed` | Container memory limit | Increase memory limit or optimize workers |\n| Build cache not working | Layer order wrong | Reorder: deps before code |\n\n---\n\n## Output Checklist\n\nBefore delivering Dockerfile, verify:\n\n- [ ] Multi-stage build separates builder and runtime\n- [ ] UV cache mounts configured for fast rebuilds\n- [ ] `UV_COMPILE_BYTECODE=1` set for startup speed\n- [ ] Non-root user created and used\n- [ ] `.dockerignore` includes .venv, __pycache__, .git\n- [ ] Layer order optimized (deps before code)\n- [ ] Health check configured\n- [ ] `--proxy-headers` flag for FastAPI behind load balancer\n- [ ] Database connection uses NullPool for external pooling\n- [ ] Secrets not hardcoded (use environment variables)\n\n---\n\n## Reference Files\n\n| File | When to Read |\n|------|--------------|\n| `references/multi-stage-builds.md` | Understanding stage architecture and patterns |\n| `references/optimization-strategies.md` | Reducing image size and build time |\n| `references/fastapi-patterns.md` | FastAPI-specific Docker configurations |\n| `references/security-best-practices.md` | Container security hardening |\n| `references/database-connections.md` | SQLModel/Neon connection patterns in containers |\n\n## Asset Templates\n\n| Template | Use Case |\n|----------|----------|\n| `assets/templates/Dockerfile.multistage` | Complete multi-stage (Builder + Runtime + Testing) |\n| `assets/templates/Dockerfile.production` | Production-only optimized build |\n| `assets/templates/Dockerfile.dev` | Development with hot-reload |\n| `assets/templates/docker-compose.yml` | Full development stack |\n| `assets/templates/docker-compose.prod.yml` | Production compose |\n| `assets/templates/.dockerignore` | Standard ignore patterns |\n",
        "skills/docker-rocker/references/database-connections.md": "# Database Connections in Docker\n\nPatterns for SQLModel, SQLAlchemy, and Neon PostgreSQL in containerized environments.\n\n---\n\n## Connection Pool Strategies\n\n### NullPool (Recommended for Neon/PgBouncer)\n\n```python\nfrom sqlalchemy.ext.asyncio import create_async_engine\nfrom sqlalchemy.pool import NullPool\n\nengine = create_async_engine(\n    DATABASE_URL,\n    poolclass=NullPool,\n    pool_pre_ping=True,\n)\n```\n\n**When to use**:\n- Neon PostgreSQL (has built-in PgBouncer)\n- External PgBouncer\n- Serverless/Lambda deployments\n- Kubernetes with many pods\n\n**Why**: External pooler manages connections. Application-level pooling conflicts with external poolers.\n\n### QueuePool (Default for Direct Connections)\n\n```python\nfrom sqlalchemy.ext.asyncio import create_async_engine\n\nengine = create_async_engine(\n    DATABASE_URL,\n    pool_size=5,\n    max_overflow=10,\n    pool_timeout=30,\n    pool_recycle=1800,\n    pool_pre_ping=True,\n)\n```\n\n**When to use**:\n- Direct PostgreSQL connection\n- No external connection pooler\n- Single container deployment\n\n---\n\n## Neon PostgreSQL Configuration\n\n### Connection Strings\n\n```bash\n# Pooled connection (for application)\nDATABASE_URL=postgresql+asyncpg://user:pass@ep-xxx-pooler.region.aws.neon.tech/db?sslmode=require\n\n# Direct connection (for migrations)\nDATABASE_URL_DIRECT=postgresql+asyncpg://user:pass@ep-xxx.region.aws.neon.tech/db?sslmode=require\n```\n\n### Docker Environment\n\n```yaml\n# docker-compose.yml\nservices:\n  api:\n    environment:\n      - DATABASE_URL=${NEON_POOLED_URL}\n\n  migrate:\n    environment:\n      # Use direct connection for Alembic\n      - DATABASE_URL=${NEON_DIRECT_URL}\n```\n\n### Neon-Specific Settings\n\n```python\n# Disable statement caching (Neon recommendation)\nengine = create_async_engine(\n    DATABASE_URL,\n    poolclass=NullPool,\n    pool_pre_ping=True,\n    connect_args={\n        \"statement_cache_size\": 0,  # Neon requirement\n        \"prepared_statement_cache_size\": 0,\n    },\n)\n```\n\n---\n\n## SQLModel Session Pattern\n\n### Async Session Factory\n\n```python\n# app/core/database.py\nfrom sqlmodel import SQLModel\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\nfrom sqlalchemy.orm import sessionmaker\nfrom sqlalchemy.pool import NullPool\n\nfrom app.core.config import settings\n\nengine = create_async_engine(\n    settings.database_url,\n    poolclass=NullPool,\n    pool_pre_ping=True,\n    echo=settings.debug,\n)\n\nasync_session = sessionmaker(\n    engine,\n    class_=AsyncSession,\n    expire_on_commit=False,\n)\n\nasync def get_session():\n    async with async_session() as session:\n        try:\n            yield session\n            await session.commit()\n        except Exception:\n            await session.rollback()\n            raise\n```\n\n### Dependency Injection\n\n```python\n# app/routers/users.py\nfrom fastapi import Depends\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom app.core.database import get_session\n\n@router.get(\"/users\")\nasync def get_users(session: AsyncSession = Depends(get_session)):\n    result = await session.execute(select(User))\n    return result.scalars().all()\n```\n\n---\n\n## Health Checks\n\n### Database Health Endpoint\n\n```python\n# app/routers/health.py\nfrom fastapi import APIRouter, Depends\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy import text\n\nrouter = APIRouter()\n\n@router.get(\"/health/db\")\nasync def db_health(session: AsyncSession = Depends(get_session)):\n    try:\n        result = await session.execute(text(\"SELECT 1\"))\n        return {\"status\": \"healthy\", \"database\": \"connected\"}\n    except Exception as e:\n        return {\"status\": \"unhealthy\", \"error\": str(e)}\n```\n\n### docker-compose Health Check\n\n```yaml\nservices:\n  api:\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:80/health/db\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 10s\n    depends_on:\n      db:\n        condition: service_healthy\n```\n\n---\n\n## Connection Handling\n\n### Stale Connection Recovery\n\n```python\n# pool_pre_ping tests connection before use\nengine = create_async_engine(\n    DATABASE_URL,\n    pool_pre_ping=True,  # Required for containerized apps\n)\n```\n\n### Connection Timeout\n\n```python\nfrom sqlalchemy import event\n\nengine = create_async_engine(\n    DATABASE_URL,\n    connect_args={\n        \"timeout\": 10,  # Connection timeout\n        \"command_timeout\": 30,  # Query timeout\n    },\n)\n```\n\n### Graceful Shutdown\n\n```python\n# app/main.py\nfrom contextlib import asynccontextmanager\nfrom app.core.database import engine\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup\n    yield\n    # Shutdown: close all connections\n    await engine.dispose()\n\napp = FastAPI(lifespan=lifespan)\n```\n\n---\n\n## Migrations in Containers\n\n### Alembic Configuration\n\n```python\n# alembic/env.py\nimport asyncio\nfrom sqlalchemy.ext.asyncio import create_async_engine\nfrom sqlalchemy.pool import NullPool\n\ndef run_migrations_online():\n    # Use direct connection (not pooled)\n    connectable = create_async_engine(\n        config.get_main_option(\"sqlalchemy.url\"),\n        poolclass=NullPool,\n    )\n\n    async def do_run_migrations():\n        async with connectable.connect() as connection:\n            await connection.run_sync(do_run_migrations)\n\n    asyncio.run(do_run_migrations())\n```\n\n### Migration Container\n\n```yaml\n# docker-compose.yml\nservices:\n  migrate:\n    build:\n      context: .\n      target: runtime\n    command: [\"alembic\", \"upgrade\", \"head\"]\n    environment:\n      # CRITICAL: Use direct connection for migrations\n      - DATABASE_URL=${DATABASE_URL_DIRECT}\n    profiles:\n      - migrate\n\n# Run: docker compose --profile migrate up migrate\n```\n\n### Migration Script\n\n```bash\n#!/bin/bash\n# scripts/migrate.sh\n\nset -e\n\necho \"Running database migrations...\"\nalembic upgrade head\n\necho \"Migrations complete!\"\n```\n\n```dockerfile\n# Add to testing stage\nCOPY scripts/migrate.sh /app/scripts/\nRUN chmod +x /app/scripts/migrate.sh\n```\n\n---\n\n## Testing Database Connections\n\n### Test Configuration\n\n```python\n# tests/conftest.py\nimport pytest\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\nfrom sqlalchemy.pool import StaticPool\nfrom sqlmodel import SQLModel\n\nTEST_DATABASE_URL = \"sqlite+aiosqlite:///:memory:\"\n\n@pytest.fixture\nasync def engine():\n    engine = create_async_engine(\n        TEST_DATABASE_URL,\n        poolclass=StaticPool,  # Shared connection for tests\n    )\n    async with engine.begin() as conn:\n        await conn.run_sync(SQLModel.metadata.create_all)\n    yield engine\n    await engine.dispose()\n\n@pytest.fixture\nasync def session(engine):\n    async with AsyncSession(engine) as session:\n        yield session\n```\n\n### Integration Test with Real Database\n\n```yaml\n# docker-compose.test.yml\nservices:\n  test:\n    build:\n      target: testing\n    environment:\n      - DATABASE_URL=postgresql+asyncpg://test:test@db:5432/test\n    depends_on:\n      db:\n        condition: service_healthy\n\n  db:\n    image: postgres:15-alpine\n    environment:\n      POSTGRES_USER: test\n      POSTGRES_PASSWORD: test\n      POSTGRES_DB: test\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U test\"]\n      interval: 5s\n      timeout: 5s\n      retries: 5\n```\n\n---\n\n## Troubleshooting\n\n| Issue | Cause | Solution |\n|-------|-------|----------|\n| `Connection refused` | DB not ready | Use `depends_on` with health check |\n| `Too many connections` | No pooling | Use NullPool with external pooler |\n| `Connection reset` | Stale connection | Enable `pool_pre_ping=True` |\n| `SSL required` | Missing sslmode | Add `?sslmode=require` to URL |\n| `Prepared statement` error | Statement caching | Disable with `statement_cache_size=0` |\n| Slow migrations | Using pooled connection | Use direct connection for Alembic |\n",
        "skills/docker-rocker/references/fastapi-patterns.md": "# FastAPI Docker Patterns\n\nDocker configurations specific to FastAPI, Pydantic, SQLModel, and async Python applications.\n\n---\n\n## FastAPI Server Configuration\n\n### Development vs Production\n\n```dockerfile\n# Development (with hot-reload)\nCMD [\"fastapi\", \"dev\", \"app/main.py\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n\n# Production (optimized)\nCMD [\"fastapi\", \"run\", \"app/main.py\", \"--port\", \"80\", \"--proxy-headers\"]\n\n# Production with multiple workers\nCMD [\"fastapi\", \"run\", \"app/main.py\", \"--port\", \"80\", \"--workers\", \"4\", \"--proxy-headers\"]\n```\n\n### Exec Form Required\n\n```dockerfile\n# CORRECT: Exec form (array) - receives signals properly\nCMD [\"fastapi\", \"run\", \"app/main.py\", \"--port\", \"80\"]\n\n# WRONG: Shell form - breaks signal handling\nCMD fastapi run app/main.py --port 80\n```\n\n### Proxy Headers\n\nWhen running behind a reverse proxy (Nginx, Traefik, load balancer):\n\n```dockerfile\n# Enable proxy header forwarding\nCMD [\"fastapi\", \"run\", \"app/main.py\", \"--port\", \"80\", \"--proxy-headers\"]\n```\n\nThis ensures:\n- Correct client IP in logs\n- Proper HTTPS detection\n- Accurate host headers\n\n---\n\n## Pydantic Settings in Containers\n\n### Environment-Based Configuration\n\n```python\n# app/core/config.py\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\n\nclass Settings(BaseSettings):\n    model_config = SettingsConfigDict(\n        env_file=\".env\",\n        env_file_encoding=\"utf-8\",\n        case_sensitive=False,\n    )\n\n    # Database\n    database_url: str\n\n    # Server\n    port: int = 80\n    workers: int = 1\n\n    # Security\n    secret_key: str\n    allowed_hosts: list[str] = [\"*\"]\n\n    # Environment\n    environment: str = \"production\"\n    debug: bool = False\n\nsettings = Settings()\n```\n\n### Docker Environment Variables\n\n```dockerfile\n# Set defaults, override at runtime\nENV DATABASE_URL=\"\" \\\n    PORT=80 \\\n    WORKERS=1 \\\n    ENVIRONMENT=production \\\n    DEBUG=false\n```\n\n### docker-compose Environment\n\n```yaml\nservices:\n  api:\n    build: .\n    environment:\n      - DATABASE_URL=postgresql+asyncpg://user:pass@db:5432/app\n      - SECRET_KEY=${SECRET_KEY}\n      - ENVIRONMENT=production\n    env_file:\n      - .env.production\n```\n\n---\n\n## SQLModel/Database Connections\n\n### Async Engine for Containers\n\n```python\n# app/core/database.py\nfrom sqlmodel import SQLModel\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\nfrom sqlalchemy.orm import sessionmaker\nfrom sqlalchemy.pool import NullPool\n\nfrom app.core.config import settings\n\n# NullPool for external connection pooling (Neon, PgBouncer)\nengine = create_async_engine(\n    settings.database_url,\n    poolclass=NullPool,\n    pool_pre_ping=True,\n    echo=settings.debug,\n)\n\nasync_session = sessionmaker(\n    engine,\n    class_=AsyncSession,\n    expire_on_commit=False,\n)\n\nasync def get_session() -> AsyncSession:\n    async with async_session() as session:\n        yield session\n```\n\n### Why NullPool?\n\n| Scenario | Pool Type | Reason |\n|----------|-----------|--------|\n| Neon PostgreSQL | NullPool | Neon has built-in PgBouncer |\n| PgBouncer | NullPool | External pooler manages connections |\n| Direct PostgreSQL | QueuePool | Local pooling beneficial |\n| Serverless/Lambda | NullPool | Connections don't persist |\n\n### Connection String Formats\n\n```python\n# Neon PostgreSQL (pooled endpoint)\nDATABASE_URL=postgresql+asyncpg://user:pass@ep-xxx.neon.tech/db?sslmode=require\n\n# Neon (direct - for migrations only)\nDATABASE_URL_DIRECT=postgresql+asyncpg://user:pass@ep-xxx.neon.tech/db?sslmode=require&options=endpoint%3Dep-xxx\n\n# Local PostgreSQL\nDATABASE_URL=postgresql+asyncpg://user:pass@localhost:5432/db\n```\n\n---\n\n## Health Checks\n\n### FastAPI Health Endpoint\n\n```python\n# app/routers/health.py\nfrom fastapi import APIRouter, Depends\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy import text\n\nfrom app.core.database import get_session\n\nrouter = APIRouter(tags=[\"health\"])\n\n@router.get(\"/health\")\nasync def health_check():\n    return {\"status\": \"healthy\"}\n\n@router.get(\"/health/db\")\nasync def db_health_check(session: AsyncSession = Depends(get_session)):\n    try:\n        await session.execute(text(\"SELECT 1\"))\n        return {\"status\": \"healthy\", \"database\": \"connected\"}\n    except Exception as e:\n        return {\"status\": \"unhealthy\", \"database\": str(e)}\n```\n\n### Dockerfile Health Check\n\n```dockerfile\nHEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\n    CMD python -c \"import urllib.request; urllib.request.urlopen('http://localhost:80/health')\" || exit 1\n```\n\n### docker-compose Health Check\n\n```yaml\nservices:\n  api:\n    build: .\n    healthcheck:\n      test: [\"CMD\", \"python\", \"-c\", \"import urllib.request; urllib.request.urlopen('http://localhost:80/health')\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 10s\n```\n\n---\n\n## Lifespan Events\n\n### Startup/Shutdown in Containers\n\n```python\n# app/main.py\nfrom contextlib import asynccontextmanager\nfrom fastapi import FastAPI\n\nfrom app.core.database import engine\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup: create tables, warm caches\n    async with engine.begin() as conn:\n        # await conn.run_sync(SQLModel.metadata.create_all)\n        pass\n\n    yield\n\n    # Shutdown: close connections\n    await engine.dispose()\n\napp = FastAPI(lifespan=lifespan)\n```\n\n### Graceful Shutdown\n\nThe exec form `CMD [\"fastapi\", ...]` ensures:\n- SIGTERM is received by the process\n- Lifespan shutdown runs\n- Connections close gracefully\n\n```yaml\n# docker-compose.yml\nservices:\n  api:\n    stop_grace_period: 30s  # Time for graceful shutdown\n```\n\n---\n\n## Testing Configuration\n\n### Test Dockerfile Stage\n\n```dockerfile\nFROM builder AS testing\n\n# Install dev dependencies\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv sync --locked\n\nCOPY . .\n\n# Override for test database\nENV DATABASE_URL=sqlite+aiosqlite:///./test.db \\\n    ENVIRONMENT=testing\n\nCMD [\"pytest\", \"-v\", \"--cov=app\"]\n```\n\n### pytest with Docker\n\n```python\n# tests/conftest.py\nimport pytest\nfrom httpx import AsyncClient\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\nfrom sqlalchemy.pool import StaticPool\nfrom sqlmodel import SQLModel\n\nfrom app.main import app\nfrom app.core.database import get_session\n\n# Test database\nTEST_DATABASE_URL = \"sqlite+aiosqlite:///:memory:\"\n\n@pytest.fixture\nasync def session():\n    engine = create_async_engine(\n        TEST_DATABASE_URL,\n        poolclass=StaticPool,\n    )\n    async with engine.begin() as conn:\n        await conn.run_sync(SQLModel.metadata.create_all)\n\n    async with AsyncSession(engine) as session:\n        yield session\n\n    await engine.dispose()\n\n@pytest.fixture\nasync def client(session):\n    async def override_session():\n        yield session\n\n    app.dependency_overrides[get_session] = override_session\n\n    async with AsyncClient(app=app, base_url=\"http://test\") as client:\n        yield client\n\n    app.dependency_overrides.clear()\n```\n\n---\n\n## Worker Configuration\n\n### CPU-Based Workers\n\n```dockerfile\n# Auto-detect CPU count\nCMD [\"sh\", \"-c\", \"fastapi run app/main.py --port 80 --workers $(nproc)\"]\n\n# Or use Gunicorn for more control\nCMD [\"gunicorn\", \"app.main:app\", \"-w\", \"4\", \"-k\", \"uvicorn.workers.UvicornWorker\", \"-b\", \"0.0.0.0:80\"]\n```\n\n### Worker Recommendations\n\n| Deployment | Workers | Memory per Worker |\n|------------|---------|-------------------|\n| Kubernetes pod | 1 | 256-512MB |\n| Single container | CPU cores | 256MB each |\n| Development | 1 | 512MB |\n\n### Memory Considerations\n\n```yaml\n# docker-compose.yml\nservices:\n  api:\n    deploy:\n      resources:\n        limits:\n          memory: 1G\n        reservations:\n          memory: 512M\n    environment:\n      - WORKERS=4  # 4 workers  256MB = 1GB limit\n```\n\n---\n\n## Common Patterns\n\n### API Behind Reverse Proxy\n\n```yaml\n# docker-compose.yml\nservices:\n  api:\n    build: .\n    expose:\n      - \"80\"  # Internal only\n    environment:\n      - ROOT_PATH=/api/v1\n    command: [\"fastapi\", \"run\", \"app/main.py\", \"--port\", \"80\", \"--proxy-headers\"]\n\n  nginx:\n    image: nginx:alpine\n    ports:\n      - \"443:443\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf\n```\n\n```python\n# app/main.py\nfrom app.core.config import settings\n\napp = FastAPI(root_path=settings.root_path)\n```\n\n### Migrations in Containers\n\n```yaml\n# docker-compose.yml\nservices:\n  migrate:\n    build: .\n    command: [\"alembic\", \"upgrade\", \"head\"]\n    environment:\n      # Use direct connection for migrations\n      - DATABASE_URL=${DATABASE_URL_DIRECT}\n    depends_on:\n      db:\n        condition: service_healthy\n```\n\n**Important**: Use direct (non-pooled) connection for Alembic migrations with Neon/PgBouncer.\n",
        "skills/docker-rocker/references/multi-stage-builds.md": "# Multi-Stage Docker Builds\n\nComprehensive guide to multi-stage Docker builds for Python FastAPI applications.\n\n---\n\n## Stage Architecture\n\n```\n\n                        BUILDER STAGE                             \n  python:3.12-slim + UV + dependencies + compile                  \n  Size: ~400MB (discarded after build)                            \n\n                              \n                              \n\n                        RUNTIME STAGE                             \n  python:3.12-slim + .venv (copied) + app code                    \n  Size: ~150MB (production image)                                 \n\n                              \n                              \n\n                        TESTING STAGE                             \n  Extends builder + dev dependencies + pytest                     \n  Size: ~450MB (CI/CD only)                                       \n\n```\n\n---\n\n## Stage 1: Builder Stage\n\n### Purpose\n- Install UV package manager\n- Download and cache dependencies\n- Compile Python bytecode\n- Build any native extensions\n\n### Key Configuration\n\n```dockerfile\nFROM python:3.12-slim AS builder\n\n# Install UV from official image\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/uv\n\n# Performance environment variables\nENV UV_LINK_MODE=copy \\\n    UV_COMPILE_BYTECODE=1 \\\n    UV_NO_PROGRESS=1\n\nWORKDIR /app\n\n# Copy dependency files first (layer caching)\nCOPY pyproject.toml uv.lock ./\n\n# Install dependencies with cache mount\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv sync --locked --no-dev --no-editable\n```\n\n### Why These Settings?\n\n| Setting | Purpose |\n|---------|---------|\n| `UV_LINK_MODE=copy` | Avoids hardlink issues between container layers |\n| `UV_COMPILE_BYTECODE=1` | Pre-compiles .pyc for faster startup |\n| `UV_NO_PROGRESS=1` | Cleaner build logs |\n| `--mount=type=cache` | Persists UV cache across builds |\n| `--locked` | Ensures exact versions from lockfile |\n| `--no-dev` | Excludes development dependencies |\n| `--no-editable` | Allows copying venv without source |\n\n---\n\n## Stage 2: Runtime Stage\n\n### Purpose\n- Minimal production image\n- Only runtime artifacts\n- Non-root user for security\n- Health checks\n\n### Key Configuration\n\n```dockerfile\nFROM python:3.12-slim AS runtime\n\n# Create non-root user\nRUN groupadd --gid 1000 app && \\\n    useradd --uid 1000 --gid app --shell /bin/bash --create-home app\n\nWORKDIR /app\n\n# Copy virtual environment from builder\nCOPY --from=builder --chown=app:app /app/.venv /app/.venv\n\n# Copy application code\nCOPY --chown=app:app ./app ./app\n\n# Set PATH to use virtual environment\nENV PATH=\"/app/.venv/bin:$PATH\" \\\n    PYTHONDONTWRITEBYTECODE=1 \\\n    PYTHONUNBUFFERED=1\n\n# Switch to non-root user\nUSER app\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\n    CMD python -c \"import urllib.request; urllib.request.urlopen('http://localhost:80/health')\"\n\n# Expose port\nEXPOSE 80\n\n# Run FastAPI\nCMD [\"fastapi\", \"run\", \"app/main.py\", \"--port\", \"80\", \"--proxy-headers\"]\n```\n\n### Runtime Environment Variables\n\n| Variable | Value | Purpose |\n|----------|-------|---------|\n| `PYTHONDONTWRITEBYTECODE=1` | Skip .pyc at runtime (already compiled) |\n| `PYTHONUNBUFFERED=1` | Real-time log output |\n| `PATH` includes .venv/bin | Use installed packages |\n\n---\n\n## Stage 3: Testing Stage\n\n### Purpose\n- Run tests in CI/CD\n- Include dev dependencies\n- Same base as builder for consistency\n\n### Key Configuration\n\n```dockerfile\nFROM builder AS testing\n\n# Install dev dependencies including pytest\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv sync --locked\n\n# Copy all source code including tests\nCOPY --chown=app:app . .\n\n# Default command runs tests\nCMD [\"pytest\", \"-v\", \"--cov=app\", \"--cov-report=term-missing\"]\n```\n\n### CI/CD Usage\n\n```yaml\n# GitHub Actions example\n- name: Run tests\n  run: |\n    docker build --target testing -t app:test .\n    docker run --rm app:test pytest -v\n```\n\n---\n\n## Stage 4: Development Stage (Optional)\n\n### Purpose\n- Local development with hot-reload\n- Volume mounting for live code changes\n- Debug capabilities\n\n### Key Configuration\n\n```dockerfile\nFROM python:3.12-slim AS development\n\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/uv\n\nWORKDIR /app\n\n# Install all dependencies including dev\nCOPY pyproject.toml uv.lock ./\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv sync --locked\n\n# Source code mounted as volume at runtime\nENV PATH=\"/app/.venv/bin:$PATH\"\n\n# Development server with reload\nCMD [\"fastapi\", \"dev\", \"app/main.py\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n```\n\n### docker-compose for Development\n\n```yaml\nservices:\n  api:\n    build:\n      context: .\n      target: development\n    volumes:\n      - .:/app\n      - /app/.venv  # Exclude venv from mount\n    ports:\n      - \"8000:8000\"\n    environment:\n      - DATABASE_URL=postgresql://...\n```\n\n---\n\n## Complete Multi-Stage Dockerfile\n\n```dockerfile\n# ============================================\n# STAGE 1: Builder\n# ============================================\nFROM python:3.12-slim AS builder\n\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/uv\n\nENV UV_LINK_MODE=copy \\\n    UV_COMPILE_BYTECODE=1\n\nWORKDIR /app\n\nCOPY pyproject.toml uv.lock ./\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv sync --locked --no-dev --no-editable\n\n# ============================================\n# STAGE 2: Runtime (Production)\n# ============================================\nFROM python:3.12-slim AS runtime\n\nRUN groupadd --gid 1000 app && \\\n    useradd --uid 1000 --gid app --shell /bin/bash --create-home app\n\nWORKDIR /app\n\nCOPY --from=builder --chown=app:app /app/.venv /app/.venv\nCOPY --chown=app:app ./app ./app\n\nENV PATH=\"/app/.venv/bin:$PATH\" \\\n    PYTHONDONTWRITEBYTECODE=1 \\\n    PYTHONUNBUFFERED=1\n\nUSER app\n\nEXPOSE 80\n\nCMD [\"fastapi\", \"run\", \"app/main.py\", \"--port\", \"80\", \"--proxy-headers\"]\n\n# ============================================\n# STAGE 3: Testing (CI/CD)\n# ============================================\nFROM builder AS testing\n\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv sync --locked\n\nCOPY . .\n\nCMD [\"pytest\", \"-v\"]\n```\n\n---\n\n## Build Commands\n\n```bash\n# Production image\ndocker build --target runtime -t myapp:prod .\n\n# Testing image\ndocker build --target testing -t myapp:test .\n\n# Development image\ndocker build --target development -t myapp:dev .\n\n# Build with build args\ndocker build --target runtime \\\n    --build-arg PYTHON_VERSION=3.12 \\\n    -t myapp:prod .\n```\n\n---\n\n## Layer Caching Best Practices\n\n### Order of Operations (Most Stable  Most Volatile)\n\n1. Base image selection\n2. System package installation\n3. UV/package manager installation\n4. Dependency file copy (pyproject.toml, uv.lock)\n5. Dependency installation\n6. Application code copy\n7. Configuration\n\n### Cache Invalidation\n\n```dockerfile\n# BAD: Any file change invalidates cache\nCOPY . .\nRUN uv sync\n\n# GOOD: Only dependency changes invalidate sync\nCOPY pyproject.toml uv.lock ./\nRUN uv sync\nCOPY ./app ./app\n```\n\n---\n\n## Troubleshooting\n\n| Issue | Solution |\n|-------|----------|\n| Cache not working | Verify layer order, use `--mount=type=cache` |\n| Large image size | Check you're copying from builder, not including source |\n| Module not found | Verify PATH includes .venv/bin |\n| Permission denied | Use `--chown` on COPY, run as non-root |\n| Slow builds | Add cache mounts, order layers by volatility |\n",
        "skills/docker-rocker/references/optimization-strategies.md": "# Docker Optimization Strategies\n\nStrategies to minimize image size, reduce build time, and maximize performance.\n\n---\n\n## Image Size Optimization\n\n### Base Image Selection\n\n| Image | Size | Use Case |\n|-------|------|----------|\n| `python:3.12` | ~1GB | Never use in production |\n| `python:3.12-slim` | ~150MB | Recommended for most cases |\n| `python:3.12-alpine` | ~50MB | Smaller but compatibility issues |\n\n**Recommendation**: Use `python:3.12-slim` for balance of size and compatibility.\n\n### Size Reduction Techniques\n\n```dockerfile\n# 1. Multi-stage builds (50-70% reduction)\nFROM python:3.12-slim AS builder\n# ... build steps\nFROM python:3.12-slim AS runtime\nCOPY --from=builder /app/.venv /app/.venv\n\n# 2. Exclude dev dependencies\nRUN uv sync --locked --no-dev\n\n# 3. Clean package manager cache\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    gcc libpq-dev \\\n    && rm -rf /var/lib/apt/lists/*\n\n# 4. Use .dockerignore\n# See assets/templates/.dockerignore\n```\n\n### Measuring Image Size\n\n```bash\n# Check image size\ndocker images myapp:latest\n\n# Analyze layers\ndocker history myapp:latest\n\n# Detailed analysis with dive\ndocker run --rm -it -v /var/run/docker.sock:/var/run/docker.sock wagoodman/dive myapp:latest\n```\n\n---\n\n## Build Time Optimization\n\n### UV Cache Mounts\n\n```dockerfile\n# Cache UV downloads across builds (50-70% faster)\nRUN --mount=type=cache,target=/root/.cache/uv \\\n    uv sync --locked --no-dev\n```\n\n### BuildKit Features\n\n```bash\n# Enable BuildKit (faster, parallel builds)\nexport DOCKER_BUILDKIT=1\n\n# Or in docker-compose\n# compose.yml\nservices:\n  app:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    x-bake:\n      cache-from:\n        - type=local,src=/tmp/.buildx-cache\n      cache-to:\n        - type=local,dest=/tmp/.buildx-cache\n```\n\n### Layer Order Optimization\n\n```dockerfile\n# CORRECT ORDER (fastest rebuilds)\n\n# 1. Rarely changes\nFROM python:3.12-slim AS builder\n\n# 2. Rarely changes\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /bin/uv\n\n# 3. Rarely changes\nENV UV_LINK_MODE=copy\n\n# 4. Occasionally changes\nCOPY pyproject.toml uv.lock ./\n\n# 5. Occasionally changes\nRUN uv sync --locked --no-dev\n\n# 6. Frequently changes (last!)\nCOPY ./app ./app\n```\n\n### Parallel Stage Builds\n\n```bash\n# Build multiple targets in parallel\ndocker buildx bake --file docker-bake.hcl\n\n# docker-bake.hcl\ngroup \"default\" {\n  targets = [\"runtime\", \"testing\"]\n}\n\ntarget \"runtime\" {\n  dockerfile = \"Dockerfile\"\n  target = \"runtime\"\n  tags = [\"myapp:prod\"]\n}\n\ntarget \"testing\" {\n  dockerfile = \"Dockerfile\"\n  target = \"testing\"\n  tags = [\"myapp:test\"]\n}\n```\n\n---\n\n## Runtime Performance\n\n### Bytecode Compilation\n\n```dockerfile\n# Compile at build time (faster startup)\nENV UV_COMPILE_BYTECODE=1\n\n# Or manually\nRUN python -m compileall -q /app\n```\n\n### Worker Configuration\n\n```dockerfile\n# Single worker (container orchestration handles scaling)\nCMD [\"fastapi\", \"run\", \"app/main.py\", \"--port\", \"80\"]\n\n# Multiple workers (standalone deployment)\nCMD [\"fastapi\", \"run\", \"app/main.py\", \"--port\", \"80\", \"--workers\", \"4\"]\n\n# Auto-detect workers based on CPU\nCMD [\"gunicorn\", \"-w\", \"$(nproc)\", \"-k\", \"uvicorn.workers.UvicornWorker\", \"app.main:app\"]\n```\n\n### Memory Optimization\n\n```dockerfile\n# Limit Python memory\nENV PYTHONMALLOC=malloc\nENV MALLOC_TRIM_THRESHOLD_=100000\n\n# In docker-compose or runtime\ndeploy:\n  resources:\n    limits:\n      memory: 512M\n    reservations:\n      memory: 256M\n```\n\n---\n\n## UV-Specific Optimizations\n\n### Environment Variables\n\n```dockerfile\nENV UV_LINK_MODE=copy \\           # Avoid hardlink issues\n    UV_COMPILE_BYTECODE=1 \\       # Pre-compile for speed\n    UV_NO_PROGRESS=1 \\            # Clean logs\n    UV_FROZEN=1 \\                 # Fail if lockfile outdated\n    UV_NO_CACHE=0                 # Enable caching\n```\n\n### Lockfile Validation\n\n```dockerfile\n# Fail build if lockfile is outdated\nRUN uv sync --locked --no-dev\n\n# Or strict mode\nRUN uv sync --frozen --no-dev\n```\n\n### Non-Editable Install\n\n```dockerfile\n# Allows copying venv without source code\nRUN uv sync --locked --no-dev --no-editable\n\n# In runtime stage, only copy venv (not source)\nCOPY --from=builder /app/.venv /app/.venv\n```\n\n---\n\n## .dockerignore Best Practices\n\n```dockerignore\n# Version control\n.git\n.gitignore\n\n# Python artifacts\n__pycache__\n*.py[cod]\n*$py.class\n*.so\n.Python\n*.egg-info\n.eggs\n\n# Virtual environments\n.venv\nvenv\nENV\n\n# IDE\n.idea\n.vscode\n*.swp\n*.swo\n\n# Testing\n.pytest_cache\n.coverage\nhtmlcov\n.tox\n\n# Build artifacts\nbuild\ndist\n*.egg\n\n# Documentation\ndocs/_build\n\n# Local config\n.env\n.env.*\n*.local\n\n# Docker\nDockerfile*\ndocker-compose*\n.docker\n\n# CI/CD\n.github\n.gitlab-ci.yml\n\n# Misc\n*.log\n*.tmp\n.DS_Store\nThumbs.db\n```\n\n---\n\n## CI/CD Caching\n\n### GitHub Actions\n\n```yaml\n- name: Set up Docker Buildx\n  uses: docker/setup-buildx-action@v3\n\n- name: Build and push\n  uses: docker/build-push-action@v5\n  with:\n    context: .\n    target: runtime\n    push: true\n    tags: myapp:latest\n    cache-from: type=gha\n    cache-to: type=gha,mode=max\n```\n\n### GitLab CI\n\n```yaml\nbuild:\n  script:\n    - docker build\n        --cache-from $CI_REGISTRY_IMAGE:cache\n        --build-arg BUILDKIT_INLINE_CACHE=1\n        -t $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA\n        .\n    - docker push $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA\n```\n\n---\n\n## Size Comparison Examples\n\n| Configuration | Image Size |\n|---------------|------------|\n| python:3.12 + pip install | ~1.2GB |\n| python:3.12-slim + pip | ~400MB |\n| Multi-stage + UV + slim | ~150MB |\n| Multi-stage + UV + alpine | ~80MB |\n\n**Target**: Production images should be <200MB for fast deployments.\n\n---\n\n## Optimization Checklist\n\n- [ ] Using python:3.12-slim base image\n- [ ] Multi-stage build separating builder and runtime\n- [ ] UV cache mounts enabled (`--mount=type=cache`)\n- [ ] Bytecode compilation enabled (`UV_COMPILE_BYTECODE=1`)\n- [ ] Dev dependencies excluded (`--no-dev`)\n- [ ] Layer order optimized (deps before code)\n- [ ] .dockerignore configured\n- [ ] BuildKit enabled\n- [ ] No unnecessary files in final image\n- [ ] apt cache cleaned after installs\n",
        "skills/docker-rocker/references/security-best-practices.md": "# Docker Security Best Practices\n\nSecurity hardening for containerized FastAPI applications.\n\n---\n\n## Non-Root User\n\n### Why Non-Root?\n\nRunning as root in containers:\n- Allows container escape vulnerabilities\n- Gives unnecessary filesystem access\n- Violates principle of least privilege\n\n### Implementation\n\n```dockerfile\n# Create user with specific UID/GID\nRUN groupadd --gid 1000 app && \\\n    useradd --uid 1000 --gid app --shell /bin/bash --create-home app\n\n# Set ownership when copying files\nCOPY --from=builder --chown=app:app /app/.venv /app/.venv\nCOPY --chown=app:app ./app ./app\n\n# Switch to non-root user\nUSER app\n```\n\n### Verification\n\n```bash\n# Check running user\ndocker run --rm myapp:prod whoami\n# Output: app\n\n# Check process user\ndocker run --rm myapp:prod ps aux\n# Processes should show 'app' not 'root'\n```\n\n---\n\n## Secrets Management\n\n### Never Do\n\n```dockerfile\n# NEVER hardcode secrets\nENV DATABASE_URL=postgresql://user:password@host/db\nENV SECRET_KEY=mysecretkey123\n\n# NEVER copy .env files\nCOPY .env /app/.env\n```\n\n### Always Do\n\n```dockerfile\n# Set placeholders, inject at runtime\nENV DATABASE_URL=\"\" \\\n    SECRET_KEY=\"\"\n```\n\n```yaml\n# docker-compose.yml\nservices:\n  api:\n    environment:\n      - DATABASE_URL=${DATABASE_URL}  # From host environment\n      - SECRET_KEY=${SECRET_KEY}\n    secrets:\n      - db_password\n\nsecrets:\n  db_password:\n    file: ./secrets/db_password.txt\n```\n\n### Docker Secrets (Swarm/Compose)\n\n```yaml\nservices:\n  api:\n    secrets:\n      - source: db_password\n        target: /run/secrets/db_password\n        mode: 0400\n\nsecrets:\n  db_password:\n    external: true  # Created via `docker secret create`\n```\n\n```python\n# Read secret in application\nfrom pathlib import Path\n\ndef get_secret(name: str) -> str:\n    secret_file = Path(f\"/run/secrets/{name}\")\n    if secret_file.exists():\n        return secret_file.read_text().strip()\n    return os.getenv(name.upper(), \"\")\n```\n\n---\n\n## Image Security\n\n### Minimal Base Images\n\n```dockerfile\n# Prefer slim images\nFROM python:3.12-slim\n\n# If using alpine, be aware of compatibility\nFROM python:3.12-alpine  # Smaller but may have issues\n```\n\n### Vulnerability Scanning\n\n```bash\n# Docker Scout (built-in)\ndocker scout cves myapp:latest\n\n# Trivy\ntrivy image myapp:latest\n\n# Snyk\nsnyk container test myapp:latest\n```\n\n### Pin Image Versions\n\n```dockerfile\n# BAD: Latest can change unexpectedly\nFROM python:latest\n\n# BETTER: Pin major.minor\nFROM python:3.12-slim\n\n# BEST: Pin digest for reproducibility\nFROM python:3.12-slim@sha256:abc123...\n```\n\n### Multi-Stage Security\n\n```dockerfile\n# Builder has build tools (potential vulnerabilities)\nFROM python:3.12-slim AS builder\nRUN apt-get update && apt-get install -y gcc\n\n# Runtime has minimal surface area\nFROM python:3.12-slim AS runtime\n# No gcc, no apt cache, no build tools\n```\n\n---\n\n## Network Security\n\n### Expose vs Ports\n\n```yaml\n# INTERNAL only (accessed by other containers)\nservices:\n  api:\n    expose:\n      - \"80\"\n\n# EXTERNAL (accessed from host)\nservices:\n  api:\n    ports:\n      - \"8080:80\"\n```\n\n### Network Isolation\n\n```yaml\nservices:\n  api:\n    networks:\n      - frontend\n      - backend\n\n  db:\n    networks:\n      - backend  # Only accessible from backend network\n\nnetworks:\n  frontend:\n  backend:\n    internal: true  # No external access\n```\n\n### TLS/HTTPS\n\n```dockerfile\n# FastAPI handles HTTP, terminate TLS at proxy\nCMD [\"fastapi\", \"run\", \"app/main.py\", \"--port\", \"80\", \"--proxy-headers\"]\n```\n\n```yaml\n# Nginx handles TLS\nservices:\n  nginx:\n    image: nginx:alpine\n    ports:\n      - \"443:443\"\n    volumes:\n      - ./certs:/etc/nginx/certs:ro\n```\n\n---\n\n## Filesystem Security\n\n### Read-Only Root\n\n```yaml\nservices:\n  api:\n    read_only: true\n    tmpfs:\n      - /tmp\n      - /app/cache\n```\n\n### No New Privileges\n\n```yaml\nservices:\n  api:\n    security_opt:\n      - no-new-privileges:true\n```\n\n### Drop Capabilities\n\n```yaml\nservices:\n  api:\n    cap_drop:\n      - ALL\n    cap_add:\n      - NET_BIND_SERVICE  # Only if needed for port < 1024\n```\n\n---\n\n## Environment Hardening\n\n### Python Security Settings\n\n```dockerfile\nENV PYTHONDONTWRITEBYTECODE=1 \\\n    PYTHONUNBUFFERED=1 \\\n    PYTHONHASHSEED=random \\\n    PIP_NO_CACHE_DIR=1 \\\n    PIP_DISABLE_PIP_VERSION_CHECK=1\n```\n\n### Disable Debug in Production\n\n```dockerfile\nENV DEBUG=false \\\n    ENVIRONMENT=production\n```\n\n```python\n# app/core/config.py\nclass Settings(BaseSettings):\n    debug: bool = False\n    environment: str = \"production\"\n\n# app/main.py\napp = FastAPI(\n    debug=settings.debug,\n    docs_url=\"/docs\" if settings.environment != \"production\" else None,\n    redoc_url=None,\n)\n```\n\n---\n\n## Health & Resource Limits\n\n### Health Checks\n\n```dockerfile\nHEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\n    CMD python -c \"import urllib.request; urllib.request.urlopen('http://localhost:80/health')\" || exit 1\n```\n\n### Resource Limits\n\n```yaml\nservices:\n  api:\n    deploy:\n      resources:\n        limits:\n          cpus: '1.0'\n          memory: 512M\n          pids: 100\n        reservations:\n          cpus: '0.25'\n          memory: 256M\n```\n\n### Restart Policy\n\n```yaml\nservices:\n  api:\n    restart: unless-stopped\n    # or for production\n    deploy:\n      restart_policy:\n        condition: on-failure\n        delay: 5s\n        max_attempts: 3\n```\n\n---\n\n## .dockerignore Security\n\nPrevent sensitive files from entering image:\n\n```dockerignore\n# Secrets\n.env\n.env.*\n*.pem\n*.key\nsecrets/\ncredentials/\n\n# Git (may contain sensitive history)\n.git\n.gitignore\n\n# IDE (may contain local configs)\n.idea\n.vscode\n\n# Tests (may contain test credentials)\ntests/fixtures/secrets/\n```\n\n---\n\n## Security Checklist\n\n### Build Time\n- [ ] Using slim/minimal base image\n- [ ] Pinned image versions\n- [ ] Multi-stage build (no build tools in runtime)\n- [ ] No secrets in Dockerfile or image\n- [ ] .dockerignore excludes sensitive files\n- [ ] Scanned for vulnerabilities\n\n### Runtime\n- [ ] Running as non-root user\n- [ ] Read-only filesystem (where possible)\n- [ ] Dropped unnecessary capabilities\n- [ ] Resource limits configured\n- [ ] Health checks enabled\n- [ ] Debug disabled in production\n- [ ] Network isolation configured\n\n### Secrets\n- [ ] No hardcoded secrets\n- [ ] Using environment variables or Docker secrets\n- [ ] Secrets not logged or exposed in errors\n- [ ] .env files not in image\n\n---\n\n## Compliance Considerations\n\n### OWASP Docker Security\n\n1. Keep images minimal\n2. Use non-root users\n3. Sign and verify images\n4. Scan for vulnerabilities\n5. Limit resource usage\n6. Use read-only filesystems\n7. Enable logging and monitoring\n\n### CIS Docker Benchmark\n\nKey recommendations:\n- Don't run containers as root\n- Don't expose unnecessary ports\n- Use trusted base images\n- Implement resource limits\n- Enable content trust\n",
        "skills/docker-rocker/skill_validator_report.md": "---\n\n  Skill Validation Report: docker-rocker\n\n  Rating: Good\n  Overall Score: 87/100\n\n  Summary\n\n  The docker-rocker skill is a well-structured Automation skill with comprehensive domain expertise embedded in references. It        \n  excels in zero-shot implementation capability, reusability, and maintainability. Minor improvements needed in user interaction      \n  patterns and official documentation links.\n\n  ---\n\n  Category Scores\n\n| Category                 | Score   | Weight | Weighted |\n| ------------------------ | ------- | ------ | -------- |\n| Structure & Anatomy      | 97/100  | 12%    | 11.64    |\n| Content Quality          | 100/100 | 15%    | 15.00    |\n| User Interaction         | 78/100  | 12%    | 9.36     |\n| Documentation            | 67/100  | 10%    | 6.70     |\n| Domain Standards         | 92/100  | 10%    | 9.20     |\n| Technical Robustness     | 93/100  | 8%     | 7.44     |\n| Maintainability          | 100/100 | 8%     | 8.00     |\n| Zero-Shot Implementation | 100/100 | 12%    | 12.00    |\n| Reusability              | 100/100 | 13%    | 13.00    |\n| Type-Specific Deduction  | -       | -      | -5       |\n| TOTAL                    |         |        | 87.34    |\n\n  ---\n\n  Detailed Criterion Analysis\n\n  Structure & Anatomy (97/100)\n\n| Criterion              | Score | Notes                                          |\n| ---------------------- | ----- | ---------------------------------------------- |\n| SKILL.md exists        | 3/3   | Present at root                                |\n| Line count             | 3/3   | 236 lines (well under 500)                     |\n| Frontmatter complete   | 3/3   | name and description present                   |\n| Name constraints       | 3/3   | \"docker-rocker\" - lowercase, hyphens, 13 chars |\n| Description format     | 3/3   | [What] + [When] format, ~350 chars             |\n| Description style      | 3/3   | \"This skill should be used when...\"            |\n| No extraneous files    | 3/3   | No README.md, CHANGELOG.md                     |\n| Progressive disclosure | 3/3   | 5 reference files, organized templates         |\n| Asset organization     | 3/3   | Templates in assets/, scripts in scripts/      |\n| Large file guidance    | 2/3   | 1833 lines in references, no grep patterns     |\n\n  Content Quality (100/100)\n\n| Criterion             | Score | Notes                                |\n| --------------------- | ----- | ------------------------------------ |\n| Conciseness           | 3/3   | No verbose explanations              |\n| Imperative form       | 3/3   | \"Do X\" style throughout              |\n| Appropriate freedom   | 3/3   | Good constraint/flexibility balance  |\n| Scope clarity         | 3/3   | Clear \"Does\" and \"Does NOT\" sections |\n| No hallucination risk | 3/3   | Relies on embedded knowledge         |\n| Output specification  | 3/3   | Comprehensive Output Checklist       |\n\n  User Interaction (78/100)\n\n| Criterion              | Score | Notes                                |\n| ---------------------- | ----- | ------------------------------------ |\n| Clarification triggers | 3/3   | \"Required Clarifications\" section    |\n| Required vs optional   | 2/3   | All required, no optional section    |\n| Graceful handling      | 1/3   | No fallback when user doesn't answer |\n| No over-asking         | 3/3   | 5 reasonable questions               |\n| Question pacing        | 2/3   | No explicit pacing note              |\n| Context awareness      | 3/3   | Strong \"Before Implementation\"       |\n\n  Documentation & References (67/100)\n\n| Criterion         | Score | Notes                                     |\n| ----------------- | ----- | ----------------------------------------- |\n| Source URLs       | 1/3   | Missing Docker/FastAPI official doc links |\n| Reference files   | 3/3   | 5 comprehensive files (1833 lines)        |\n| Fetch guidance    | 1/3   | No instructions for unlisted patterns     |\n| Version awareness | 2/3   | Python 3.12 mentioned                     |\n| Example coverage  | 3/3   | Good examples in references               |\n\n  Domain Standards (92/100)\n\n| Criterion             | Score | Notes                                     |\n| --------------------- | ----- | ----------------------------------------- |\n| Best practices        | 3/3   | UV, multi-stage, security                 |\n| Enforcement mechanism | 3/3   | Output Checklist with checkboxes          |\n| Anti-patterns         | 2/3   | Error table, but no explicit \"Must Avoid\" |\n| Quality gates         | 3/3   | Output checklist before delivery          |\n\n  Technical Robustness (93/100)\n\n| Criterion               | Score | Notes                              |\n| ----------------------- | ----- | ---------------------------------- |\n| Error handling          | 3/3   | Comprehensive Error Handling table |\n| Security considerations | 3/3   | Dedicated security reference       |\n| Dependencies            | 3/3   | Docker, UV documented              |\n| Edge cases              | 2/3   | Covered in error handling          |\n| Testability             | 3/3   | Validation commands provided       |\n\n  Maintainability (100/100)\n\n| Criterion           | Score | Notes                             |\n| ------------------- | ----- | --------------------------------- |\n| Modularity          | 3/3   | 5 self-contained reference topics |\n| Update path         | 3/3   | Easy to update independently      |\n| No hardcoded values | 3/3   | Uses placeholders throughout      |\n| Clear organization  | 3/3   | Logical section ordering          |\n\n  Zero-Shot Implementation (100/100)\n\n| Criterion                     | Score | Notes                              |\n| ----------------------------- | ----- | ---------------------------------- |\n| Before Implementation section | 3/3   | Comprehensive context table        |\n| Codebase context              | 3/3   | Project structure, existing files  |\n| Conversation context          | 3/3   | Requirements, dependencies         |\n| Embedded expertise            | 3/3   | All domain knowledge in references |\n| User-only questions           | 3/3   | Only asks for USER context         |\n\n  Reusability (100/100)\n\n| Criterion                | Score | Notes                             |\n| ------------------------ | ----- | --------------------------------- |\n| Handles variations       | 3/3   | Dev/Prod/Testing/CI-CD            |\n| Variable elements        | 3/3   | Target, package manager, DB, port |\n| Constant patterns        | 3/3   | Best practices encoded            |\n| Not requirement-specific | 3/3   | Generic FastAPI+Docker domain     |\n| Abstraction level        | 3/3   | Appropriate generalization        |\n\n  Type-Specific (Automation) - Deduction: -5\n\n| Requirement             | Status  | Notes                                                |\n| ----------------------- | ------- | ---------------------------------------------------- |\n| Scripts in scripts/     |        | 6 scripts (build, test, dev, scan, migrate, analyze) |\n| Dependencies documented |        | Docker, UV, Python 3.12                              |\n| Error Handling          |        | Table with cause/solution                            |\n| I/O Specification       | Partial | Templates specified, no formal I/O contract          |\n\n  ---\n\n  Critical Issues\n\n  None. The skill meets all critical requirements.\n\n  ---\n\n  Improvement Recommendations\n\n  High Priority\n\n1. Add Official Documentation Links\n   \n   ## Official Documentation\n   \n   | Resource        | URL                                                  | Use For          |\n   | --------------- | ---------------------------------------------------- | ---------------- |\n   | Docker Docs     | https://docs.docker.com/                             | Reference        |\n   | FastAPI Docker  | https://fastapi.tiangolo.com/deployment/docker/      | FastAPI patterns |\n   | UV Docker Guide | https://docs.astral.sh/uv/guides/integration/docker/ | UV configuration |\n\n2. Add Graceful Handling for Unanswered Questions\n   \n   ## Default Behaviors (when user doesn't specify)\n   \n   | Question          | Default          |\n   | ----------------- | ---------------- |\n   | Deployment target | Production       |\n   | Package manager   | UV (recommended) |\n   | Database          | Neon PostgreSQL  |\n   | Port              | 80               |\n   \n   Medium Priority\n\n3. Add Optional Clarifications Section\n   \n   ## Optional Clarifications\n- Registry push requirements?\n\n- CI/CD platform (GitHub Actions, GitLab CI)?\n\n- Multi-architecture build (amd64, arm64)?\n  Note: Avoid asking too many questions in a single message.\n4. Add \"Must Avoid\" Anti-patterns Section\n   \n   ### Must Avoid\n- Running as root in production\n\n- Copying .env files into image\n\n- Using `latest` tag in production\n\n- Hardcoding secrets in Dockerfile\n  Low Priority\n5. Add Grep Patterns for Large References\n   \n   ## Finding Patterns in References\n   \n   | Pattern       | Grep Command                                  |\n   | ------------- | --------------------------------------------- |\n   | Cache mounts  | `grep -n \"mount=type=cache\" references/*.md`  |\n   | Health checks | `grep -n \"HEALTHCHECK\" references/*.md`       |\n   | Non-root user | `grep -n \"useradd\\|USER app\" references/*.md` |\n\n6. Add Formal I/O Specification\n   \n   ## Input/Output Specification\n   \n   **Inputs:**\n- pyproject.toml or requirements.txt\n\n- App entry point (app/main.py)\n\n- User deployment preferences\n  **Outputs:**\n\n- Dockerfile (multi-stage)\n\n- docker-compose.yml\n\n- .dockerignore\n\n  ---\n\n  Strengths\n\n- Excellent Zero-Shot Capability: \"Before Implementation\" section ensures proper context gathering\n- Comprehensive Domain Expertise: 5 well-organized reference files (1833 lines of embedded knowledge)\n- Strong Reusability: Handles multiple deployment targets (dev/prod/test/CI-CD)\n- Production-Ready Templates: 7 ready-to-use templates in assets/\n- Automation Scripts: 6 helper scripts for common operations\n- Clear Scope Definition: Explicit \"Does\" and \"Does NOT\" sections prevent scope creep\n- Practical Output Checklist: 10-item verification before delivery\n- Good Error Handling: Cause/solution table for common issues\n\n  ---\n\n  Validation Summary\n\n| Aspect           | Status                  |\n| ---------------- | ----------------------- |\n| Structure        |  Excellent             |\n| Content          |  Excellent             |\n| User Interaction |  Minor gaps           |\n| Documentation    |  Needs official links |\n| Domain Standards |  Strong                |\n| Technical        |  Robust                |\n| Maintainability  |  Excellent             |\n| Zero-Shot        |  Excellent             |\n| Reusability      |  Excellent             |\n\n  Verdict: The docker-rocker skill is production-ready with minor enhancements recommended. It successfully embeds Docker\n  deployment expertise and handles variations across development, testing, and production environments.\n",
        "skills/fastapi-helper/.claude-plugin/plugin.json": "{\n  \"name\": \"fastapi-helper\",\n  \"description\": \"FastAPI development assistant with routing, dependencies, Pydantic models, middleware, and testing patterns\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Ali Jilani\"\n  },\n  \"category\": \"development\",\n  \"keywords\": [\"fastapi\", \"python\", \"api\", \"pydantic\", \"web-development\"]\n}\n",
        "skills/fastapi-helper/SKILL.md": "---\nname: fastapi-helper\ndescription: |\n  FastAPI development assistant for building modern Python web APIs. Provides guidance on routing, request/response handling, dependency injection, authentication, middleware, WebSockets, testing, and Pydantic models. Use when: (1) Creating FastAPI applications or endpoints, (2) Implementing CRUD operations, (3) Setting up authentication/authorization, (4) Working with request parameters (path, query, body, headers, cookies, forms, files), (5) Configuring middleware or CORS, (6) Implementing WebSocket connections, (7) Writing tests for FastAPI apps, (8) Defining Pydantic models for validation.\n---\n\n# FastAPI Helper\n\nBuild modern, high-performance Python APIs with FastAPI.\n\n## Quick Start\n\n```python\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"Hello World\"}\n```\n\nRun: `uvicorn main:app --reload`\n\n## Core Concepts\n\n### Route Decorators\n\n| Decorator | HTTP Method |\n|-----------|-------------|\n| `@app.get()` | GET |\n| `@app.post()` | POST |\n| `@app.put()` | PUT |\n| `@app.delete()` | DELETE |\n| `@app.patch()` | PATCH |\n\n### Parameter Functions\n\n| Function | Source |\n|----------|--------|\n| `Path()` | URL path `/items/{id}` |\n| `Query()` | Query string `?q=foo` |\n| `Body()` | JSON body |\n| `Header()` | HTTP headers |\n| `Cookie()` | Cookies |\n| `Form()` | Form data |\n| `File()` / `UploadFile` | File uploads |\n\n### Dependency Injection\n\n```python\nfrom fastapi import Depends\n\nasync def get_db():\n    db = SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n\n@app.get(\"/items/\")\nasync def read_items(db: Session = Depends(get_db)):\n    return db.query(Item).all()\n```\n\n### Common Imports\n\n```python\nfrom fastapi import (\n    FastAPI, APIRouter, Depends, HTTPException, status,\n    Request, Response, BackgroundTasks, WebSocket,\n    Path, Query, Body, Header, Cookie, Form, File, UploadFile\n)\nfrom fastapi.responses import JSONResponse, HTMLResponse, FileResponse, RedirectResponse\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel, Field, EmailStr\n```\n\n## Reference Documentation\n\nLoad these based on task:\n\n| Task | Reference File |\n|------|----------------|\n| Routes, APIRouter, decorators | [references/routing.md](references/routing.md) |\n| Path, Query, Body, Form, File params | [references/parameters.md](references/parameters.md) |\n| Response types, status codes, headers | [references/responses.md](references/responses.md) |\n| Depends, Security, OAuth2 | [references/dependencies.md](references/dependencies.md) |\n| Middleware, CORS, lifespan, BackgroundTasks | [references/middleware-events.md](references/middleware-events.md) |\n| WebSocket connections | [references/websockets.md](references/websockets.md) |\n| HTTPException, error handlers | [references/exceptions.md](references/exceptions.md) |\n| Pydantic models, validation | [references/pydantic-models.md](references/pydantic-models.md) |\n| TestClient, pytest fixtures | [references/testing.md](references/testing.md) |\n\n## Common Patterns\n\n### CRUD Endpoint Structure\n\n```python\nfrom fastapi import APIRouter, Depends, HTTPException, status\nfrom sqlalchemy.orm import Session\n\nrouter = APIRouter(prefix=\"/items\", tags=[\"items\"])\n\n@router.get(\"/\", response_model=list[ItemOut])\nasync def list_items(skip: int = 0, limit: int = 100, db: Session = Depends(get_db)):\n    return db.query(Item).offset(skip).limit(limit).all()\n\n@router.get(\"/{item_id}\", response_model=ItemOut)\nasync def get_item(item_id: int, db: Session = Depends(get_db)):\n    item = db.query(Item).filter(Item.id == item_id).first()\n    if not item:\n        raise HTTPException(status_code=404, detail=\"Item not found\")\n    return item\n\n@router.post(\"/\", response_model=ItemOut, status_code=status.HTTP_201_CREATED)\nasync def create_item(item: ItemCreate, db: Session = Depends(get_db)):\n    db_item = Item(**item.model_dump())\n    db.add(db_item)\n    db.commit()\n    db.refresh(db_item)\n    return db_item\n\n@router.put(\"/{item_id}\", response_model=ItemOut)\nasync def update_item(item_id: int, item: ItemUpdate, db: Session = Depends(get_db)):\n    db_item = db.query(Item).filter(Item.id == item_id).first()\n    if not db_item:\n        raise HTTPException(status_code=404, detail=\"Item not found\")\n    for key, value in item.model_dump(exclude_unset=True).items():\n        setattr(db_item, key, value)\n    db.commit()\n    return db_item\n\n@router.delete(\"/{item_id}\", status_code=status.HTTP_204_NO_CONTENT)\nasync def delete_item(item_id: int, db: Session = Depends(get_db)):\n    db_item = db.query(Item).filter(Item.id == item_id).first()\n    if not db_item:\n        raise HTTPException(status_code=404, detail=\"Item not found\")\n    db.delete(db_item)\n    db.commit()\n```\n\n### CORS Setup\n\n```python\nfrom fastapi.middleware.cors import CORSMiddleware\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"http://localhost:3000\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n```\n\n### JWT Authentication Pattern\n\n```python\nfrom fastapi import Depends, HTTPException, status\nfrom fastapi.security import OAuth2PasswordBearer\n\noauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"token\")\n\nasync def get_current_user(token: str = Depends(oauth2_scheme)):\n    user = decode_token(token)\n    if not user:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid credentials\",\n            headers={\"WWW-Authenticate\": \"Bearer\"},\n        )\n    return user\n\n@app.get(\"/users/me\")\nasync def read_users_me(current_user: User = Depends(get_current_user)):\n    return current_user\n```\n",
        "skills/fastapi-helper/references/dependencies.md": "# FastAPI Dependencies Reference\n\n## Basic Dependencies\n\n```python\nfrom fastapi import Depends\n\n# Function dependency\nasync def common_parameters(q: str | None = None, skip: int = 0, limit: int = 100):\n    return {\"q\": q, \"skip\": skip, \"limit\": limit}\n\n@app.get(\"/items/\")\nasync def read_items(commons: dict = Depends(common_parameters)):\n    return commons\n\n# Class dependency\nclass CommonQueryParams:\n    def __init__(self, q: str | None = None, skip: int = 0, limit: int = 100):\n        self.q = q\n        self.skip = skip\n        self.limit = limit\n\n@app.get(\"/items/\")\nasync def read_items(commons: CommonQueryParams = Depends(CommonQueryParams)):\n    return {\"q\": commons.q, \"skip\": commons.skip}\n\n# Shorthand for class\n@app.get(\"/items/\")\nasync def read_items(commons: CommonQueryParams = Depends()):\n    return commons\n```\n\n## Nested Dependencies\n\n```python\ndef query_extractor(q: str | None = None):\n    return q\n\ndef query_or_cookie_extractor(\n    q: str = Depends(query_extractor),\n    last_query: str | None = Cookie(default=None)\n):\n    return q or last_query\n\n@app.get(\"/items/\")\nasync def read_items(query: str = Depends(query_or_cookie_extractor)):\n    return {\"query\": query}\n```\n\n## Dependencies with Yield (Context Managers)\n\n```python\nasync def get_db():\n    db = SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n\n@app.get(\"/items/\")\nasync def read_items(db: Session = Depends(get_db)):\n    return db.query(Item).all()\n```\n\n## Global Dependencies\n\n```python\n# On app\napp = FastAPI(dependencies=[Depends(verify_token), Depends(verify_key)])\n\n# On router\nrouter = APIRouter(dependencies=[Depends(verify_token)])\n\n# On specific route\n@app.get(\"/items/\", dependencies=[Depends(verify_token)])\nasync def read_items():\n    return []\n```\n\n## Security Dependencies\n\n```python\nfrom fastapi import Security\nfrom fastapi.security import (\n    OAuth2PasswordBearer,\n    OAuth2PasswordRequestForm,\n    HTTPBasic,\n    HTTPBasicCredentials,\n    APIKeyHeader,\n    APIKeyQuery,\n    APIKeyCookie\n)\n\n# OAuth2 Password Bearer\noauth2_scheme = OAuth2PasswordBearer(tokenUrl=\"token\")\n\n@app.get(\"/users/me\")\nasync def read_users_me(token: str = Depends(oauth2_scheme)):\n    return decode_token(token)\n\n# Login endpoint\n@app.post(\"/token\")\nasync def login(form_data: OAuth2PasswordRequestForm = Depends()):\n    user = authenticate(form_data.username, form_data.password)\n    return {\"access_token\": token, \"token_type\": \"bearer\"}\n\n# HTTP Basic Auth\nsecurity = HTTPBasic()\n\n@app.get(\"/users/me\")\nasync def read_current_user(credentials: HTTPBasicCredentials = Depends(security)):\n    return {\"username\": credentials.username}\n\n# API Key\napi_key_header = APIKeyHeader(name=\"X-API-Key\")\n\n@app.get(\"/items/\")\nasync def read_items(api_key: str = Security(api_key_header)):\n    return {\"api_key\": api_key}\n```\n\n## Dependency Override (Testing)\n\n```python\n# In tests\ndef override_get_db():\n    return TestingSessionLocal()\n\napp.dependency_overrides[get_db] = override_get_db\n\n# Clean up\napp.dependency_overrides = {}\n```\n",
        "skills/fastapi-helper/references/exceptions.md": "# FastAPI Exceptions Reference\n\n## HTTPException\n\n```python\nfrom fastapi import HTTPException\n\n@app.get(\"/items/{item_id}\")\nasync def read_item(item_id: int):\n    if item_id not in items:\n        raise HTTPException(\n            status_code=404,\n            detail=\"Item not found\",\n            headers={\"X-Error\": \"Item missing\"}\n        )\n    return items[item_id]\n\n# With custom detail object\nraise HTTPException(\n    status_code=400,\n    detail={\"error\": \"validation_error\", \"fields\": [\"name\", \"email\"]}\n)\n```\n\n## Custom Exception Handlers\n\n```python\nfrom fastapi import Request\nfrom fastapi.responses import JSONResponse\n\nclass UnicornException(Exception):\n    def __init__(self, name: str):\n        self.name = name\n\n@app.exception_handler(UnicornException)\nasync def unicorn_exception_handler(request: Request, exc: UnicornException):\n    return JSONResponse(\n        status_code=418,\n        content={\"message\": f\"Oops! {exc.name} did something wrong.\"}\n    )\n\n@app.get(\"/unicorns/{name}\")\nasync def read_unicorn(name: str):\n    if name == \"yolo\":\n        raise UnicornException(name=name)\n    return {\"unicorn_name\": name}\n```\n\n## Override Default Exception Handlers\n\n```python\nfrom fastapi.exceptions import RequestValidationError\nfrom starlette.exceptions import HTTPException as StarletteHTTPException\n\n# Override validation errors\n@app.exception_handler(RequestValidationError)\nasync def validation_exception_handler(request: Request, exc: RequestValidationError):\n    return JSONResponse(\n        status_code=422,\n        content={\n            \"detail\": exc.errors(),\n            \"body\": exc.body\n        }\n    )\n\n# Override HTTP exceptions\n@app.exception_handler(StarletteHTTPException)\nasync def http_exception_handler(request: Request, exc: StarletteHTTPException):\n    return JSONResponse(\n        status_code=exc.status_code,\n        content={\"detail\": exc.detail}\n    )\n```\n\n## Common HTTP Exceptions Pattern\n\n```python\nfrom fastapi import HTTPException, status\n\ndef get_item_or_404(item_id: int):\n    item = db.get(item_id)\n    if not item:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"Item {item_id} not found\"\n        )\n    return item\n\ndef require_admin(user: User):\n    if not user.is_admin:\n        raise HTTPException(\n            status_code=status.HTTP_403_FORBIDDEN,\n            detail=\"Admin access required\"\n        )\n\ndef validate_token(token: str):\n    if not is_valid(token):\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid token\",\n            headers={\"WWW-Authenticate\": \"Bearer\"}\n        )\n```\n\n## Pydantic Validation Errors\n\n```python\nfrom pydantic import BaseModel, field_validator, ValidationError\n\nclass Item(BaseModel):\n    name: str\n    price: float\n\n    @field_validator(\"price\")\n    @classmethod\n    def price_must_be_positive(cls, v):\n        if v <= 0:\n            raise ValueError(\"Price must be positive\")\n        return v\n\n# Validation errors automatically return 422 with details\n@app.post(\"/items/\")\nasync def create_item(item: Item):\n    return item\n```\n",
        "skills/fastapi-helper/references/middleware-events.md": "# FastAPI Middleware & Events Reference\n\n## Middleware\n\n### Custom Middleware\n\n```python\nfrom fastapi import Request\nfrom starlette.middleware.base import BaseHTTPMiddleware\nimport time\n\n# Function-based middleware\n@app.middleware(\"http\")\nasync def add_process_time_header(request: Request, call_next):\n    start_time = time.time()\n    response = await call_next(request)\n    process_time = time.time() - start_time\n    response.headers[\"X-Process-Time\"] = str(process_time)\n    return response\n\n# Class-based middleware\nclass CustomMiddleware(BaseHTTPMiddleware):\n    async def dispatch(self, request: Request, call_next):\n        # Before request\n        response = await call_next(request)\n        # After request\n        return response\n\napp.add_middleware(CustomMiddleware)\n```\n\n### Built-in Middleware\n\n```python\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.middleware.gzip import GZipMiddleware\nfrom fastapi.middleware.trustedhost import TrustedHostMiddleware\nfrom fastapi.middleware.httpsredirect import HTTPSRedirectMiddleware\n\n# CORS\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"http://localhost:3000\", \"https://example.com\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n    expose_headers=[\"X-Custom-Header\"],\n    max_age=600\n)\n\n# GZip compression\napp.add_middleware(GZipMiddleware, minimum_size=1000)\n\n# Trusted hosts\napp.add_middleware(TrustedHostMiddleware, allowed_hosts=[\"example.com\", \"*.example.com\"])\n\n# HTTPS redirect\napp.add_middleware(HTTPSRedirectMiddleware)\n```\n\n## Startup & Shutdown Events\n\n### Lifespan Context Manager (Recommended)\n\n```python\nfrom contextlib import asynccontextmanager\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup: runs before accepting requests\n    db_pool = await create_db_pool()\n    app.state.db = db_pool\n    yield\n    # Shutdown: runs after finishing requests\n    await db_pool.close()\n\napp = FastAPI(lifespan=lifespan)\n```\n\n### Event Decorators (Legacy)\n\n```python\n@app.on_event(\"startup\")\nasync def startup_event():\n    await database.connect()\n    app.state.config = load_config()\n\n@app.on_event(\"shutdown\")\nasync def shutdown_event():\n    await database.disconnect()\n```\n\n## Background Tasks\n\n```python\nfrom fastapi import BackgroundTasks\n\ndef write_log(message: str):\n    with open(\"log.txt\", \"a\") as f:\n        f.write(message)\n\nasync def send_email(email: str, message: str):\n    # Async email sending\n    await email_client.send(email, message)\n\n@app.post(\"/send-notification/\")\nasync def send_notification(\n    email: str,\n    background_tasks: BackgroundTasks\n):\n    background_tasks.add_task(write_log, f\"Notification sent to {email}\")\n    background_tasks.add_task(send_email, email, \"Hello!\")\n    return {\"message\": \"Notification sent in the background\"}\n\n# In dependencies\ndef write_notification(background_tasks: BackgroundTasks, q: str | None = None):\n    if q:\n        background_tasks.add_task(write_log, f\"Query: {q}\")\n    return q\n\n@app.post(\"/items/\")\nasync def create_item(query: str = Depends(write_notification)):\n    return {\"query\": query}\n```\n\n## Request Object\n\n```python\nfrom fastapi import Request\n\n@app.get(\"/items/\")\nasync def read_items(request: Request):\n    # Request properties\n    request.method          # \"GET\"\n    request.url             # URL object\n    request.url.path        # \"/items/\"\n    request.url.query       # \"q=foo&limit=10\"\n    request.headers         # Headers dict-like\n    request.query_params    # QueryParams object\n    request.path_params     # Dict of path params\n    request.cookies         # Dict of cookies\n    request.client          # Client info (host, port)\n    request.state           # Custom state object\n\n    # Body methods\n    body = await request.body()      # Raw bytes\n    json = await request.json()      # Parsed JSON\n    form = await request.form()      # Form data\n\n    return {\"path\": request.url.path}\n```\n",
        "skills/fastapi-helper/references/parameters.md": "# FastAPI Parameters Reference\n\n## Path Parameters\n\n```python\nfrom fastapi import Path\n\n@app.get(\"/items/{item_id}\")\nasync def read_item(\n    item_id: int = Path(\n        ...,                    # Required (... = Ellipsis)\n        title=\"Item ID\",\n        description=\"The ID of the item\",\n        ge=1,                   # Greater than or equal\n        le=1000,                # Less than or equal\n        example=42\n    )\n):\n    return {\"item_id\": item_id}\n```\n\n## Query Parameters\n\n```python\nfrom fastapi import Query\n\n@app.get(\"/items/\")\nasync def read_items(\n    q: str | None = Query(\n        default=None,\n        min_length=3,\n        max_length=50,\n        pattern=\"^fixedquery$\",  # Regex pattern\n        title=\"Query string\",\n        description=\"Query for items\",\n        alias=\"item-query\",      # Use different name in URL\n        deprecated=False,\n        include_in_schema=True\n    ),\n    skip: int = Query(default=0, ge=0),\n    limit: int = Query(default=10, le=100)\n):\n    return {\"q\": q, \"skip\": skip, \"limit\": limit}\n\n# Multiple values\n@app.get(\"/items/\")\nasync def read_items(q: list[str] = Query(default=[])):\n    return {\"q\": q}  # ?q=foo&q=bar -> {\"q\": [\"foo\", \"bar\"]}\n```\n\n## Request Body\n\n```python\nfrom fastapi import Body\nfrom pydantic import BaseModel\n\nclass Item(BaseModel):\n    name: str\n    price: float\n    description: str | None = None\n\n@app.post(\"/items/\")\nasync def create_item(item: Item):\n    return item\n\n# Multiple bodies\n@app.put(\"/items/{item_id}\")\nasync def update_item(\n    item_id: int,\n    item: Item,\n    user: User,\n    importance: int = Body(default=1, ge=1, le=5)\n):\n    return {\"item_id\": item_id, \"item\": item, \"user\": user}\n\n# Embed single body\n@app.post(\"/items/\")\nasync def create_item(item: Item = Body(embed=True)):\n    # Expects: {\"item\": {\"name\": \"...\", \"price\": ...}}\n    return item\n```\n\n## Headers\n\n```python\nfrom fastapi import Header\n\n@app.get(\"/items/\")\nasync def read_items(\n    user_agent: str | None = Header(default=None),\n    x_token: str = Header(..., alias=\"X-Token\"),\n    x_tokens: list[str] = Header(default=[])\n):\n    return {\"User-Agent\": user_agent, \"X-Token\": x_token}\n```\n\n## Cookies\n\n```python\nfrom fastapi import Cookie\n\n@app.get(\"/items/\")\nasync def read_items(\n    session_id: str | None = Cookie(default=None),\n    tracking_id: str = Cookie(...)\n):\n    return {\"session_id\": session_id}\n```\n\n## Form Data\n\n```python\nfrom fastapi import Form\n\n@app.post(\"/login/\")\nasync def login(\n    username: str = Form(...),\n    password: str = Form(...)\n):\n    return {\"username\": username}\n```\n\n## File Uploads\n\n```python\nfrom fastapi import File, UploadFile\n\n# Simple file (bytes)\n@app.post(\"/files/\")\nasync def create_file(file: bytes = File(...)):\n    return {\"file_size\": len(file)}\n\n# UploadFile (recommended for large files)\n@app.post(\"/uploadfile/\")\nasync def create_upload_file(file: UploadFile):\n    contents = await file.read()\n    return {\n        \"filename\": file.filename,\n        \"content_type\": file.content_type,\n        \"size\": len(contents)\n    }\n\n# Multiple files\n@app.post(\"/files/\")\nasync def create_files(files: list[UploadFile]):\n    return {\"filenames\": [f.filename for f in files]}\n\n# File with form data\n@app.post(\"/files/\")\nasync def create_file(\n    file: UploadFile,\n    description: str = Form(...)\n):\n    return {\"filename\": file.filename, \"description\": description}\n```\n\n### UploadFile Attributes & Methods\n\n```python\nfile.filename       # Original filename\nfile.content_type   # MIME type (e.g., \"image/png\")\nfile.file           # SpooledTemporaryFile object\nfile.headers        # Headers\n\nawait file.read()   # Read contents\nawait file.write(data)\nawait file.seek(0)  # Go to start\nawait file.close()\n```\n",
        "skills/fastapi-helper/references/pydantic-models.md": "# FastAPI Pydantic Models Reference\n\n## Basic Models\n\n```python\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\nfrom datetime import datetime\n\nclass Item(BaseModel):\n    name: str\n    description: str | None = None\n    price: float\n    tax: float = 10.5\n    tags: list[str] = []\n\n# Usage\nitem = Item(name=\"Foo\", price=42.0)\nitem_dict = item.model_dump()  # Convert to dict\nitem_json = item.model_dump_json()  # Convert to JSON string\n```\n\n## Field Validation\n\n```python\nfrom pydantic import BaseModel, Field, field_validator, model_validator\n\nclass Item(BaseModel):\n    name: str = Field(\n        ...,  # Required\n        min_length=1,\n        max_length=100,\n        description=\"Item name\",\n        examples=[\"Widget\"]\n    )\n    price: float = Field(\n        ...,\n        gt=0,           # Greater than\n        le=10000,       # Less than or equal\n        description=\"Item price\"\n    )\n    quantity: int = Field(default=1, ge=0)\n\n    @field_validator(\"name\")\n    @classmethod\n    def name_must_not_be_empty(cls, v: str) -> str:\n        if not v.strip():\n            raise ValueError(\"Name cannot be empty\")\n        return v.title()\n\n    @model_validator(mode=\"after\")\n    def check_total(self) -> \"Item\":\n        if self.price * self.quantity > 100000:\n            raise ValueError(\"Total value too high\")\n        return self\n```\n\n## Nested Models\n\n```python\nclass Address(BaseModel):\n    street: str\n    city: str\n    country: str = \"USA\"\n\nclass User(BaseModel):\n    name: str\n    address: Address\n    tags: list[str] = []\n\n# Usage\nuser = User(\n    name=\"John\",\n    address={\"street\": \"123 Main\", \"city\": \"NYC\"}\n)\n```\n\n## Request/Response Models Pattern\n\n```python\n# Base model with shared fields\nclass ItemBase(BaseModel):\n    name: str\n    description: str | None = None\n    price: float\n\n# For creating (no ID)\nclass ItemCreate(ItemBase):\n    pass\n\n# For updating (all optional)\nclass ItemUpdate(BaseModel):\n    name: str | None = None\n    description: str | None = None\n    price: float | None = None\n\n# For response (includes ID, timestamps)\nclass Item(ItemBase):\n    id: int\n    created_at: datetime\n\n    model_config = {\"from_attributes\": True}  # For ORM mode\n\n# Usage\n@app.post(\"/items/\", response_model=Item)\nasync def create_item(item: ItemCreate):\n    db_item = crud.create_item(item)\n    return db_item\n\n@app.patch(\"/items/{item_id}\", response_model=Item)\nasync def update_item(item_id: int, item: ItemUpdate):\n    return crud.update_item(item_id, item.model_dump(exclude_unset=True))\n```\n\n## Common Field Types\n\n```python\nfrom pydantic import BaseModel, EmailStr, HttpUrl, SecretStr\nfrom typing import Literal\nfrom enum import Enum\nfrom uuid import UUID\nfrom datetime import datetime, date, time\n\nclass Status(str, Enum):\n    pending = \"pending\"\n    active = \"active\"\n    completed = \"completed\"\n\nclass User(BaseModel):\n    id: UUID\n    email: EmailStr\n    password: SecretStr\n    website: HttpUrl | None = None\n    status: Status = Status.pending\n    role: Literal[\"admin\", \"user\", \"guest\"] = \"user\"\n    created_at: datetime\n    birth_date: date | None = None\n```\n\n## Model Configuration\n\n```python\nfrom pydantic import BaseModel, ConfigDict\n\nclass Item(BaseModel):\n    model_config = ConfigDict(\n        str_strip_whitespace=True,  # Strip whitespace from strings\n        str_min_length=1,           # Min string length\n        from_attributes=True,       # Enable ORM mode\n        extra=\"forbid\",             # Forbid extra fields\n        populate_by_name=True,      # Allow population by field name\n        use_enum_values=True        # Use enum values instead of enum objects\n    )\n\n    name: str\n    price: float\n```\n\n## Generic Response Models\n\n```python\nfrom typing import Generic, TypeVar\nfrom pydantic import BaseModel\n\nT = TypeVar(\"T\")\n\nclass Response(BaseModel, Generic[T]):\n    data: T\n    message: str = \"Success\"\n\nclass PaginatedResponse(BaseModel, Generic[T]):\n    items: list[T]\n    total: int\n    page: int\n    size: int\n\n# Usage\n@app.get(\"/items/\", response_model=PaginatedResponse[Item])\nasync def list_items(page: int = 1, size: int = 10):\n    items = get_items(page, size)\n    return PaginatedResponse(\n        items=items,\n        total=100,\n        page=page,\n        size=size\n    )\n```\n",
        "skills/fastapi-helper/references/responses.md": "# FastAPI Responses Reference\n\n## Response Classes\n\n```python\nfrom fastapi import Response\nfrom fastapi.responses import (\n    JSONResponse,\n    HTMLResponse,\n    PlainTextResponse,\n    RedirectResponse,\n    StreamingResponse,\n    FileResponse\n)\n\n# Default JSON response\n@app.get(\"/items/\")\nasync def read_items():\n    return {\"items\": []}  # Automatically JSONResponse\n\n# Explicit JSONResponse\n@app.get(\"/items/\")\nasync def read_items():\n    return JSONResponse(\n        content={\"items\": []},\n        status_code=200,\n        headers={\"X-Custom\": \"value\"},\n        media_type=\"application/json\"\n    )\n\n# HTML Response\n@app.get(\"/html/\", response_class=HTMLResponse)\nasync def get_html():\n    return \"<html><body><h1>Hello</h1></body></html>\"\n\n# Plain Text\n@app.get(\"/text/\", response_class=PlainTextResponse)\nasync def get_text():\n    return \"Hello, World!\"\n\n# Redirect\n@app.get(\"/redirect/\")\nasync def redirect():\n    return RedirectResponse(url=\"/items/\", status_code=307)\n\n# File Response\n@app.get(\"/file/\")\nasync def get_file():\n    return FileResponse(\n        path=\"file.pdf\",\n        filename=\"download.pdf\",\n        media_type=\"application/pdf\"\n    )\n\n# Streaming Response\n@app.get(\"/stream/\")\nasync def stream():\n    async def generate():\n        for i in range(10):\n            yield f\"data: {i}\\n\\n\"\n    return StreamingResponse(generate(), media_type=\"text/event-stream\")\n```\n\n## Response Model\n\n```python\nfrom pydantic import BaseModel\n\nclass ItemOut(BaseModel):\n    name: str\n    price: float\n    # password field excluded from response\n\n@app.post(\"/items/\", response_model=ItemOut)\nasync def create_item(item: ItemIn):\n    return item  # Filters to only ItemOut fields\n\n# Exclude unset values\n@app.get(\"/items/\", response_model=Item, response_model_exclude_unset=True)\nasync def read_item():\n    return item\n\n# Include/exclude specific fields\n@app.get(\"/items/\", response_model=Item, response_model_include={\"name\", \"price\"})\n@app.get(\"/items/\", response_model=Item, response_model_exclude={\"password\"})\n```\n\n## Custom Response Headers & Cookies\n\n```python\nfrom fastapi import Response\n\n@app.get(\"/items/\")\nasync def read_items(response: Response):\n    response.headers[\"X-Custom-Header\"] = \"custom-value\"\n    response.set_cookie(key=\"session\", value=\"abc123\", httponly=True)\n    return {\"items\": []}\n\n# Delete cookie\n@app.get(\"/logout/\")\nasync def logout(response: Response):\n    response.delete_cookie(key=\"session\")\n    return {\"message\": \"logged out\"}\n```\n\n## Status Codes\n\n```python\nfrom fastapi import status\n\n@app.post(\"/items/\", status_code=status.HTTP_201_CREATED)\nasync def create_item(item: Item):\n    return item\n\n@app.delete(\"/items/{item_id}\", status_code=status.HTTP_204_NO_CONTENT)\nasync def delete_item(item_id: int):\n    return None\n\n# Common status codes\nstatus.HTTP_200_OK\nstatus.HTTP_201_CREATED\nstatus.HTTP_204_NO_CONTENT\nstatus.HTTP_400_BAD_REQUEST\nstatus.HTTP_401_UNAUTHORIZED\nstatus.HTTP_403_FORBIDDEN\nstatus.HTTP_404_NOT_FOUND\nstatus.HTTP_422_UNPROCESSABLE_ENTITY\nstatus.HTTP_500_INTERNAL_SERVER_ERROR\n```\n",
        "skills/fastapi-helper/references/routing.md": "# FastAPI Routing Reference\n\n## Application & Router\n\n### FastAPI Application\n\n```python\nfrom fastapi import FastAPI\n\napp = FastAPI(\n    title=\"My API\",\n    description=\"API description\",\n    version=\"1.0.0\",\n    docs_url=\"/docs\",        # Swagger UI\n    redoc_url=\"/redoc\",      # ReDoc\n    openapi_url=\"/openapi.json\"\n)\n```\n\n### APIRouter (Modular Routes)\n\n```python\nfrom fastapi import APIRouter\n\nrouter = APIRouter(\n    prefix=\"/items\",\n    tags=[\"items\"],\n    dependencies=[Depends(verify_token)],\n    responses={404: {\"description\": \"Not found\"}}\n)\n\n@router.get(\"/\")\nasync def read_items():\n    return [{\"item_id\": \"Foo\"}]\n\n# Include in main app\napp.include_router(router)\n```\n\n## Route Decorators\n\n### HTTP Methods\n\n```python\n@app.get(\"/items/{item_id}\")\nasync def read_item(item_id: int):\n    return {\"item_id\": item_id}\n\n@app.post(\"/items/\", status_code=201)\nasync def create_item(item: Item):\n    return item\n\n@app.put(\"/items/{item_id}\")\nasync def update_item(item_id: int, item: Item):\n    return {\"item_id\": item_id, **item.dict()}\n\n@app.delete(\"/items/{item_id}\", status_code=204)\nasync def delete_item(item_id: int):\n    return None\n\n@app.patch(\"/items/{item_id}\")\nasync def partial_update(item_id: int, item: ItemUpdate):\n    return {\"item_id\": item_id}\n\n@app.options(\"/items/\")\nasync def options_items():\n    return {\"methods\": [\"GET\", \"POST\"]}\n\n@app.head(\"/items/{item_id}\")\nasync def head_item(item_id: int):\n    return None\n```\n\n### Route Parameters\n\n```python\n@app.get(\n    \"/items/{item_id}\",\n    response_model=Item,\n    status_code=200,\n    tags=[\"items\"],\n    summary=\"Get an item\",\n    description=\"Get item by ID\",\n    response_description=\"The item\",\n    deprecated=False,\n    operation_id=\"get_item\",\n    responses={\n        200: {\"description\": \"Successful\"},\n        404: {\"description\": \"Not found\"}\n    }\n)\nasync def read_item(item_id: int):\n    return {\"item_id\": item_id}\n```\n",
        "skills/fastapi-helper/references/testing.md": "# FastAPI Testing Reference\n\n## TestClient Setup\n\n```python\nfrom fastapi.testclient import TestClient\nfrom main import app\n\nclient = TestClient(app)\n\ndef test_read_main():\n    response = client.get(\"/\")\n    assert response.status_code == 200\n    assert response.json() == {\"message\": \"Hello World\"}\n```\n\n## Async Testing\n\n```python\nimport pytest\nfrom httpx import AsyncClient, ASGITransport\nfrom main import app\n\n@pytest.mark.anyio\nasync def test_root():\n    async with AsyncClient(\n        transport=ASGITransport(app=app),\n        base_url=\"http://test\"\n    ) as ac:\n        response = await ac.get(\"/\")\n    assert response.status_code == 200\n```\n\n## Testing with Dependencies Override\n\n```python\nfrom fastapi.testclient import TestClient\nfrom main import app, get_db\n\ndef override_get_db():\n    db = TestingSessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n\napp.dependency_overrides[get_db] = override_get_db\n\nclient = TestClient(app)\n\ndef test_create_item():\n    response = client.post(\"/items/\", json={\"name\": \"Test\", \"price\": 10.0})\n    assert response.status_code == 201\n\n# Clean up after tests\ndef teardown_module():\n    app.dependency_overrides = {}\n```\n\n## Testing Different Request Types\n\n```python\n# GET with query params\ndef test_read_items():\n    response = client.get(\"/items/?skip=0&limit=10\")\n    assert response.status_code == 200\n\n# POST with JSON body\ndef test_create_item():\n    response = client.post(\n        \"/items/\",\n        json={\"name\": \"Foo\", \"price\": 42.0}\n    )\n    assert response.status_code == 201\n    assert response.json()[\"name\"] == \"Foo\"\n\n# PUT with path params\ndef test_update_item():\n    response = client.put(\n        \"/items/1\",\n        json={\"name\": \"Updated\", \"price\": 50.0}\n    )\n    assert response.status_code == 200\n\n# DELETE\ndef test_delete_item():\n    response = client.delete(\"/items/1\")\n    assert response.status_code == 204\n\n# With headers\ndef test_with_auth():\n    response = client.get(\n        \"/users/me\",\n        headers={\"Authorization\": \"Bearer test-token\"}\n    )\n    assert response.status_code == 200\n\n# With cookies\ndef test_with_cookies():\n    response = client.get(\n        \"/items/\",\n        cookies={\"session\": \"abc123\"}\n    )\n    assert response.status_code == 200\n\n# File upload\ndef test_upload_file():\n    response = client.post(\n        \"/uploadfile/\",\n        files={\"file\": (\"test.txt\", b\"file content\", \"text/plain\")}\n    )\n    assert response.status_code == 200\n\n# Form data\ndef test_login():\n    response = client.post(\n        \"/login/\",\n        data={\"username\": \"test\", \"password\": \"secret\"}\n    )\n    assert response.status_code == 200\n```\n\n## Testing WebSockets\n\n```python\ndef test_websocket():\n    with client.websocket_connect(\"/ws\") as websocket:\n        websocket.send_text(\"Hello\")\n        data = websocket.receive_text()\n        assert data == \"Message received: Hello\"\n```\n\n## Pytest Fixtures\n\n```python\nimport pytest\nfrom fastapi.testclient import TestClient\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\nfrom main import app, get_db\nfrom database import Base\n\nSQLALCHEMY_DATABASE_URL = \"sqlite:///./test.db\"\nengine = create_engine(SQLALCHEMY_DATABASE_URL)\nTestingSessionLocal = sessionmaker(bind=engine)\n\n@pytest.fixture(scope=\"function\")\ndef db():\n    Base.metadata.create_all(bind=engine)\n    db = TestingSessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n        Base.metadata.drop_all(bind=engine)\n\n@pytest.fixture(scope=\"function\")\ndef client(db):\n    def override_get_db():\n        yield db\n    app.dependency_overrides[get_db] = override_get_db\n    with TestClient(app) as c:\n        yield c\n    app.dependency_overrides = {}\n\ndef test_create_item(client):\n    response = client.post(\"/items/\", json={\"name\": \"Test\", \"price\": 10.0})\n    assert response.status_code == 201\n```\n",
        "skills/fastapi-helper/references/websockets.md": "# FastAPI WebSockets Reference\n\n## Basic WebSocket\n\n```python\nfrom fastapi import WebSocket, WebSocketDisconnect\n\n@app.websocket(\"/ws\")\nasync def websocket_endpoint(websocket: WebSocket):\n    await websocket.accept()\n    try:\n        while True:\n            data = await websocket.receive_text()\n            await websocket.send_text(f\"Message received: {data}\")\n    except WebSocketDisconnect:\n        print(\"Client disconnected\")\n```\n\n## WebSocket Methods\n\n```python\n@app.websocket(\"/ws\")\nasync def websocket_endpoint(websocket: WebSocket):\n    await websocket.accept()\n\n    # Receive data\n    text = await websocket.receive_text()\n    data = await websocket.receive_bytes()\n    json_data = await websocket.receive_json()\n\n    # Send data\n    await websocket.send_text(\"Hello\")\n    await websocket.send_bytes(b\"bytes\")\n    await websocket.send_json({\"message\": \"hello\"})\n\n    # Close connection\n    await websocket.close(code=1000)\n```\n\n## WebSocket with Parameters\n\n```python\n@app.websocket(\"/ws/{client_id}\")\nasync def websocket_endpoint(\n    websocket: WebSocket,\n    client_id: int,\n    q: str | None = None,\n    token: str = Depends(get_token)\n):\n    await websocket.accept()\n    await websocket.send_text(f\"Client {client_id} connected\")\n```\n\n## Connection Manager Pattern\n\n```python\nfrom fastapi import WebSocket, WebSocketDisconnect\n\nclass ConnectionManager:\n    def __init__(self):\n        self.active_connections: list[WebSocket] = []\n\n    async def connect(self, websocket: WebSocket):\n        await websocket.accept()\n        self.active_connections.append(websocket)\n\n    def disconnect(self, websocket: WebSocket):\n        self.active_connections.remove(websocket)\n\n    async def send_personal_message(self, message: str, websocket: WebSocket):\n        await websocket.send_text(message)\n\n    async def broadcast(self, message: str):\n        for connection in self.active_connections:\n            await connection.send_text(message)\n\nmanager = ConnectionManager()\n\n@app.websocket(\"/ws/{client_id}\")\nasync def websocket_endpoint(websocket: WebSocket, client_id: int):\n    await manager.connect(websocket)\n    try:\n        while True:\n            data = await websocket.receive_text()\n            await manager.broadcast(f\"Client #{client_id}: {data}\")\n    except WebSocketDisconnect:\n        manager.disconnect(websocket)\n        await manager.broadcast(f\"Client #{client_id} left\")\n```\n\n## WebSocket with Authentication\n\n```python\nfrom fastapi import WebSocket, Query, status\n\nasync def get_cookie_or_token(\n    websocket: WebSocket,\n    session: str | None = Cookie(default=None),\n    token: str | None = Query(default=None)\n):\n    if session or token:\n        return session or token\n    await websocket.close(code=status.WS_1008_POLICY_VIOLATION)\n\n@app.websocket(\"/ws\")\nasync def websocket_endpoint(\n    websocket: WebSocket,\n    cookie_or_token: str = Depends(get_cookie_or_token)\n):\n    await websocket.accept()\n    await websocket.send_text(f\"Session: {cookie_or_token}\")\n```\n",
        "skills/fastapi-pytest-tddhelper/.claude-plugin/plugin.json": "{\n  \"name\": \"fastapi-pytest-tddhelper\",\n  \"description\": \"Test-Driven Development workflow helper for FastAPI with pytest - fixtures, conftest patterns, and testing best practices\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Ali Jilani\"\n  },\n  \"category\": \"development\",\n  \"keywords\": [\"pytest\", \"tdd\", \"testing\", \"fastapi\", \"fixtures\"]\n}\n",
        "skills/fastapi-pytest-tddhelper/SKILL.md": "---\nname: FastAPI_Pytest_TDDHelper\ndescription: |\n  Senior Backend Architect and Principal QA Engineer skill for Test-Driven Development (TDD) with FastAPI and Pytest. Provides high-performance testing blueprints prioritizing execution speed and memory efficiency. Use when: (1) Setting up pytest for FastAPI projects, (2) Writing tests following Red-Green-Refactor TDD cycle, (3) Creating high-performance conftest.py with async fixtures, (4) Implementing transaction rollback patterns for fast test isolation, (5) Using httpx.AsyncClient for async endpoint testing, (6) Validating responses with Pydantic models, (7) Creating factory fixtures and dependency overrides, (8) Optimizing test execution speed and parallelization. Applies to FastAPI v0.100+ and Pytest v8.0+.\n---\n\n# FastAPI Pytest TDD Helper\n\nHigh-performance TDD blueprint for FastAPI projects.\n\n## Core Principles\n\n| Principle | Implementation |\n|-----------|----------------|\n| **Speed** | AsyncClient over TestClient (~20% faster) |\n| **Isolation** | Transaction rollback, not schema recreation |\n| **TDD** | Red-Green-Refactor cycle strictly |\n| **Validation** | Pydantic models, not just status codes |\n\n## Quick Start\n\n### 1. Install Dependencies\n\n```bash\npip install pytest pytest-asyncio httpx aiosqlite pytest-cov\n```\n\n### 2. Configure pytest (pyproject.toml)\n\n```toml\n[tool.pytest.ini_options]\nasyncio_mode = \"auto\"\ntestpaths = [\"tests\"]\naddopts = [\"-v\", \"--tb=short\", \"-x\"]\n```\n\n### 3. Create conftest.py\n\n```python\nimport pytest\nfrom httpx import AsyncClient, ASGITransport\nfrom sqlalchemy.ext.asyncio import AsyncSession, create_async_engine, async_sessionmaker\nfrom sqlalchemy.pool import NullPool\nfrom app.main import app\nfrom app.database import Base, get_db\n\n@pytest.fixture(scope=\"session\")\nasync def async_engine():\n    engine = create_async_engine(\"sqlite+aiosqlite:///./test.db\", poolclass=NullPool)\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n    yield engine\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.drop_all)\n\n@pytest.fixture(scope=\"function\")\nasync def db_session(async_engine):\n    async_session = async_sessionmaker(async_engine, class_=AsyncSession)\n    async with async_session() as session:\n        async with session.begin():\n            yield session\n            await session.rollback()  # Fast isolation!\n\n@pytest.fixture\nasync def client(db_session):\n    app.dependency_overrides[get_db] = lambda: db_session\n    async with AsyncClient(transport=ASGITransport(app=app), base_url=\"http://test\") as ac:\n        yield ac\n    app.dependency_overrides.clear()\n```\n\n## TDD Workflow: Red-Green-Refactor\n\n### Step 1: RED - Write Failing Test\n\n```python\nfrom pydantic import BaseModel\n\nclass ItemResponse(BaseModel):\n    id: int\n    name: str\n    price: float\n\nasync def test_create_item(client):\n    response = await client.post(\"/items/\", json={\"name\": \"Widget\", \"price\": 10.0})\n\n    assert response.status_code == 201\n    item = ItemResponse(**response.json())  # Validate shape!\n    assert item.name == \"Widget\"\n```\n\nRun: `pytest -x` (fails - endpoint doesn't exist)\n\n### Step 2: GREEN - Minimal Implementation\n\n```python\n@app.post(\"/items/\", status_code=201, response_model=ItemResponse)\nasync def create_item(item: ItemCreate, db: Session = Depends(get_db)):\n    db_item = Item(**item.model_dump())\n    db.add(db_item)\n    db.commit()\n    return db_item\n```\n\nRun: `pytest -x` (passes)\n\n### Step 3: REFACTOR - Optimize\n\nImprove code quality, run tests to verify nothing breaks.\n\n## Reference Documentation\n\n| Task | Reference |\n|------|-----------|\n| conftest.py patterns, fixture scopes | [references/conftest-patterns.md](references/conftest-patterns.md) |\n| Red-Green-Refactor examples | [references/tdd-workflow.md](references/tdd-workflow.md) |\n| pyproject.toml, parallel execution | [references/pytest-optimization.md](references/pytest-optimization.md) |\n| Response validation with Pydantic | [references/pydantic-validation.md](references/pydantic-validation.md) |\n| CRUD tests, mocking, overrides | [references/testing-patterns.md](references/testing-patterns.md) |\n\n## Assets\n\n| Template | Description |\n|----------|-------------|\n| [assets/conftest_template.py](assets/conftest_template.py) | Complete conftest.py ready to customize |\n| [assets/pyproject_template.toml](assets/pyproject_template.toml) | Optimized pytest configuration |\n\n## Performance Decisions\n\n### Why AsyncClient Over TestClient\n\n```python\n# AVOID: Sync-to-async bridge overhead\nfrom fastapi.testclient import TestClient\nclient = TestClient(app)\n\n# USE: Native async, ~20% faster\nfrom httpx import AsyncClient, ASGITransport\nasync with AsyncClient(transport=ASGITransport(app=app), base_url=\"http://test\") as client:\n    response = await client.get(\"/\")\n```\n\n### Why Transaction Rollback Over Schema Recreation\n\n| Approach | 100 tests | 1000 tests |\n|----------|-----------|------------|\n| Schema recreation | ~60s | ~600s |\n| Transaction rollback | ~5s | ~50s |\n\n```python\n# FAST: Rollback at end of each test\nasync with session.begin():\n    yield session\n    await session.rollback()\n```\n\n### Fixture Scoping Strategy\n\n| Scope | Use For | Example |\n|-------|---------|---------|\n| `session` | Expensive setup | DB engine, app instance |\n| `function` | Test isolation | DB session with rollback |\n\n## Common Commands\n\n```bash\n# Run all tests\npytest\n\n# Run with coverage\npytest --cov=app --cov-report=term-missing\n\n# Run specific test\npytest tests/test_items.py::test_create_item -v\n\n# Run excluding slow tests\npytest -m \"not slow\"\n\n# Parallel execution\npytest -n auto\n\n# Stop on first failure (TDD mode)\npytest -x\n\n# Run failed tests first\npytest --ff\n```\n\n## Parametrize Pattern\n\n```python\n@pytest.mark.parametrize(\"name,price,status\", [\n    (\"Valid\", 10.0, 201),\n    (\"\", 10.0, 422),      # Empty name\n    (\"Item\", -5.0, 422),  # Negative price\n])\nasync def test_create_item_validation(client, name, price, status):\n    response = await client.post(\"/items/\", json={\"name\": name, \"price\": price})\n    assert response.status_code == status\n```\n\n## Factory Fixture Pattern\n\n```python\n@pytest.fixture\ndef item_factory(db_session):\n    async def _create(name=\"Item\", price=10.0, **kwargs):\n        item = Item(name=name, price=price, **kwargs)\n        db_session.add(item)\n        await db_session.flush()\n        return item\n    return _create\n\nasync def test_get_item(client, item_factory):\n    item = await item_factory(name=\"Widget\")\n    response = await client.get(f\"/items/{item.id}\")\n    assert response.json()[\"name\"] == \"Widget\"\n```\n",
        "skills/fastapi-pytest-tddhelper/references/conftest-patterns.md": "# High-Performance Conftest Patterns\n\n## Fixture Scoping Strategy\n\n| Scope | Use Case | Performance Impact |\n|-------|----------|-------------------|\n| `session` | DB engine, app instance | Create once, reuse across all tests |\n| `function` | DB session with rollback | Isolation without schema recreation |\n| `class` | Shared state within test class | Moderate reuse |\n\n## Async Database Engine (Session Scope)\n\n```python\n# tests/conftest.py\nimport pytest\nimport asyncio\nfrom typing import AsyncGenerator\nfrom httpx import AsyncClient, ASGITransport\nfrom sqlalchemy.ext.asyncio import AsyncSession, create_async_engine, async_sessionmaker\nfrom sqlalchemy.pool import NullPool\n\nfrom app.main import app\nfrom app.database import Base, get_db\n\n# Use NullPool for testing - avoids connection pool issues with async\nTEST_DATABASE_URL = \"sqlite+aiosqlite:///./test.db\"\n\n@pytest.fixture(scope=\"session\")\ndef event_loop():\n    \"\"\"Create event loop for session-scoped async fixtures.\"\"\"\n    loop = asyncio.get_event_loop_policy().new_event_loop()\n    yield loop\n    loop.close()\n\n@pytest.fixture(scope=\"session\")\nasync def async_engine():\n    \"\"\"Session-scoped engine - created once, reused across all tests.\"\"\"\n    engine = create_async_engine(\n        TEST_DATABASE_URL,\n        poolclass=NullPool,  # Prevents connection pool issues in tests\n        echo=False  # Set True for debugging SQL\n    )\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n    yield engine\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.drop_all)\n    await engine.dispose()\n```\n\n## Transaction Rollback Pattern (Function Scope)\n\n```python\n@pytest.fixture(scope=\"function\")\nasync def db_session(async_engine) -> AsyncGenerator[AsyncSession, None]:\n    \"\"\"\n    Function-scoped session with transaction rollback.\n\n    Performance: Avoids schema recreation per test.\n    Each test runs in a transaction that rolls back.\n    \"\"\"\n    async_session = async_sessionmaker(\n        async_engine,\n        class_=AsyncSession,\n        expire_on_commit=False\n    )\n    async with async_session() as session:\n        async with session.begin():\n            yield session\n            await session.rollback()  # Rollback after each test\n```\n\n## AsyncClient Fixture\n\n```python\n@pytest.fixture(scope=\"function\")\nasync def client(db_session: AsyncSession) -> AsyncGenerator[AsyncClient, None]:\n    \"\"\"\n    Async HTTP client with dependency override.\n\n    Why AsyncClient over TestClient:\n    - Native async support (no sync/async bridging overhead)\n    - ~20% faster for async endpoints\n    - Proper handling of async context managers\n    \"\"\"\n    async def override_get_db():\n        yield db_session\n\n    app.dependency_overrides[get_db] = override_get_db\n\n    async with AsyncClient(\n        transport=ASGITransport(app=app),\n        base_url=\"http://test\"\n    ) as ac:\n        yield ac\n\n    app.dependency_overrides.clear()\n```\n\n## Authenticated Client Fixture\n\n```python\n@pytest.fixture\nasync def auth_client(client: AsyncClient, db_session: AsyncSession):\n    \"\"\"Client with authentication headers.\"\"\"\n    # Create test user\n    user = User(email=\"test@example.com\", hashed_password=\"...\")\n    db_session.add(user)\n    await db_session.flush()\n\n    # Generate token\n    token = create_access_token({\"sub\": str(user.id)})\n    client.headers[\"Authorization\"] = f\"Bearer {token}\"\n\n    yield client\n```\n\n## Factory Fixtures\n\n```python\n@pytest.fixture\ndef item_factory(db_session: AsyncSession):\n    \"\"\"Factory for creating test items with defaults.\"\"\"\n    async def _create_item(\n        name: str = \"Test Item\",\n        price: float = 10.0,\n        quantity: int = 1\n    ) -> Item:\n        item = Item(name=name, price=price, quantity=quantity)\n        db_session.add(item)\n        await db_session.flush()\n        await db_session.refresh(item)\n        return item\n    return _create_item\n```\n\n## Complete conftest.py Template\n\n```python\n# tests/conftest.py\nimport pytest\nimport asyncio\nfrom typing import AsyncGenerator, Callable\nfrom httpx import AsyncClient, ASGITransport\nfrom sqlalchemy.ext.asyncio import AsyncSession, create_async_engine, async_sessionmaker\nfrom sqlalchemy.pool import NullPool\n\nfrom app.main import app\nfrom app.database import Base, get_db\nfrom app.models import User, Item\n\nTEST_DATABASE_URL = \"sqlite+aiosqlite:///./test.db\"\n\n@pytest.fixture(scope=\"session\")\ndef event_loop():\n    loop = asyncio.get_event_loop_policy().new_event_loop()\n    yield loop\n    loop.close()\n\n@pytest.fixture(scope=\"session\")\nasync def async_engine():\n    engine = create_async_engine(TEST_DATABASE_URL, poolclass=NullPool)\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n    yield engine\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.drop_all)\n    await engine.dispose()\n\n@pytest.fixture(scope=\"function\")\nasync def db_session(async_engine) -> AsyncGenerator[AsyncSession, None]:\n    async_session = async_sessionmaker(async_engine, class_=AsyncSession, expire_on_commit=False)\n    async with async_session() as session:\n        async with session.begin():\n            yield session\n            await session.rollback()\n\n@pytest.fixture(scope=\"function\")\nasync def client(db_session: AsyncSession) -> AsyncGenerator[AsyncClient, None]:\n    async def override_get_db():\n        yield db_session\n    app.dependency_overrides[get_db] = override_get_db\n    async with AsyncClient(transport=ASGITransport(app=app), base_url=\"http://test\") as ac:\n        yield ac\n    app.dependency_overrides.clear()\n\n@pytest.fixture\ndef user_factory(db_session: AsyncSession) -> Callable:\n    async def _create(email: str = \"test@example.com\", **kwargs) -> User:\n        user = User(email=email, **kwargs)\n        db_session.add(user)\n        await db_session.flush()\n        await db_session.refresh(user)\n        return user\n    return _create\n\n@pytest.fixture\ndef item_factory(db_session: AsyncSession) -> Callable:\n    async def _create(name: str = \"Item\", price: float = 10.0, **kwargs) -> Item:\n        item = Item(name=name, price=price, **kwargs)\n        db_session.add(item)\n        await db_session.flush()\n        await db_session.refresh(item)\n        return item\n    return _create\n```\n",
        "skills/fastapi-pytest-tddhelper/references/pydantic-validation.md": "# Pydantic Validation in Tests\n\n## Why Validate with Pydantic (Not Just Status Codes)\n\n```python\n# BAD: Only checks status code\ndef test_get_item(client):\n    response = client.get(\"/items/1\")\n    assert response.status_code == 200\n    # Response could have wrong shape, missing fields, wrong types!\n\n# GOOD: Validates entire response shape\nfrom app.schemas import ItemResponse\n\ndef test_get_item(client):\n    response = client.get(\"/items/1\")\n    assert response.status_code == 200\n    item = ItemResponse(**response.json())  # Raises ValidationError if wrong shape\n    assert item.name == \"Expected Name\"\n```\n\n## Response Schema Definitions\n\n```python\n# tests/schemas.py - Test-specific response schemas\nfrom pydantic import BaseModel, Field\nfrom datetime import datetime\n\nclass ErrorResponse(BaseModel):\n    detail: str\n\nclass ValidationErrorDetail(BaseModel):\n    loc: list[str | int]\n    msg: str\n    type: str\n\nclass ValidationErrorResponse(BaseModel):\n    detail: list[ValidationErrorDetail]\n\nclass ItemResponse(BaseModel):\n    id: int\n    name: str\n    price: float = Field(ge=0)\n    quantity: int = Field(ge=0)\n    created_at: datetime\n\n    model_config = {\"extra\": \"forbid\"}  # Fail on unexpected fields\n\nclass PaginatedResponse(BaseModel):\n    items: list[ItemResponse]\n    total: int = Field(ge=0)\n    page: int = Field(ge=1)\n    size: int = Field(ge=1)\n    pages: int = Field(ge=0)\n```\n\n## Validation Patterns\n\n### Validate Success Responses\n\n```python\n@pytest.mark.asyncio\nasync def test_create_item_returns_valid_response(client: AsyncClient):\n    response = await client.post(\"/items/\", json={\"name\": \"Widget\", \"price\": 10.0})\n\n    assert response.status_code == 201\n\n    # Validate response matches expected schema\n    item = ItemResponse(**response.json())\n    assert item.name == \"Widget\"\n    assert item.price == 10.0\n    assert item.id is not None  # Auto-generated\n```\n\n### Validate Error Responses\n\n```python\n@pytest.mark.asyncio\nasync def test_get_nonexistent_item_returns_404(client: AsyncClient):\n    response = await client.get(\"/items/99999\")\n\n    assert response.status_code == 404\n\n    error = ErrorResponse(**response.json())\n    assert \"not found\" in error.detail.lower()\n\n@pytest.mark.asyncio\nasync def test_create_item_invalid_data_returns_422(client: AsyncClient):\n    response = await client.post(\"/items/\", json={\"name\": \"\", \"price\": -10})\n\n    assert response.status_code == 422\n\n    error = ValidationErrorResponse(**response.json())\n    assert len(error.detail) >= 1\n\n    # Check specific validation errors\n    error_fields = [e.loc[-1] for e in error.detail]\n    assert \"price\" in error_fields or \"name\" in error_fields\n```\n\n### Validate List Responses\n\n```python\n@pytest.mark.asyncio\nasync def test_list_items_returns_valid_paginated_response(\n    client: AsyncClient,\n    item_factory\n):\n    # Create test data\n    for i in range(15):\n        await item_factory(name=f\"Item {i}\", price=float(i))\n\n    response = await client.get(\"/items/\", params={\"page\": 1, \"size\": 10})\n\n    assert response.status_code == 200\n\n    # Validate pagination structure\n    data = PaginatedResponse(**response.json())\n    assert data.total == 15\n    assert data.page == 1\n    assert data.size == 10\n    assert len(data.items) == 10\n    assert data.pages == 2\n\n    # Validate each item in list\n    for item in data.items:\n        assert isinstance(item, ItemResponse)\n        assert item.id is not None\n```\n\n### Validate Computed/Derived Fields\n\n```python\nfrom pydantic import computed_field\n\nclass ItemWithTotal(BaseModel):\n    id: int\n    name: str\n    price: float\n    quantity: int\n\n    @computed_field\n    @property\n    def total_value(self) -> float:\n        return self.price * self.quantity\n\n@pytest.mark.asyncio\nasync def test_item_response_includes_computed_total(client: AsyncClient, item_factory):\n    await item_factory(name=\"Widget\", price=25.0, quantity=4)\n\n    response = await client.get(\"/items/\")\n    items = [ItemWithTotal(**i) for i in response.json()[\"items\"]]\n\n    assert items[0].total_value == 100.0  # 25.0 * 4\n```\n\n## Generic Response Validator\n\n```python\n# tests/utils.py\nfrom typing import TypeVar, Type\nfrom pydantic import BaseModel\nfrom httpx import Response\n\nT = TypeVar(\"T\", bound=BaseModel)\n\ndef validate_response(response: Response, schema: Type[T], status_code: int = 200) -> T:\n    \"\"\"\n    Validate response status and parse body with Pydantic schema.\n\n    Usage:\n        item = validate_response(response, ItemResponse, 201)\n    \"\"\"\n    assert response.status_code == status_code, (\n        f\"Expected {status_code}, got {response.status_code}: {response.text}\"\n    )\n    return schema(**response.json())\n\ndef validate_list_response(\n    response: Response,\n    item_schema: Type[T],\n    status_code: int = 200\n) -> list[T]:\n    \"\"\"Validate list response and parse each item.\"\"\"\n    assert response.status_code == status_code\n    return [item_schema(**item) for item in response.json()]\n```\n\n```python\n# Usage in tests\nfrom tests.utils import validate_response, validate_list_response\n\n@pytest.mark.asyncio\nasync def test_create_and_get_item(client: AsyncClient):\n    # Create\n    create_response = await client.post(\"/items/\", json={\"name\": \"Test\", \"price\": 10.0})\n    created = validate_response(create_response, ItemResponse, 201)\n\n    # Get\n    get_response = await client.get(f\"/items/{created.id}\")\n    fetched = validate_response(get_response, ItemResponse)\n\n    assert fetched.id == created.id\n    assert fetched.name == created.name\n```\n\n## Strict Mode for Extra Fields\n\n```python\nclass StrictItemResponse(BaseModel):\n    \"\"\"Fails if API returns unexpected fields.\"\"\"\n    id: int\n    name: str\n    price: float\n\n    model_config = {\"extra\": \"forbid\"}\n\n@pytest.mark.asyncio\nasync def test_response_has_no_extra_fields(client: AsyncClient, item_factory):\n    await item_factory()\n    response = await client.get(\"/items/1\")\n\n    # This will raise ValidationError if response has fields\n    # not defined in StrictItemResponse\n    StrictItemResponse(**response.json())\n```\n",
        "skills/fastapi-pytest-tddhelper/references/pytest-optimization.md": "# Pytest Optimization Patterns\n\n## pyproject.toml Configuration\n\n```toml\n[tool.pytest.ini_options]\n# Async mode - auto-detect async tests\nasyncio_mode = \"auto\"\n\n# Test discovery\ntestpaths = [\"tests\"]\npython_files = [\"test_*.py\"]\npython_functions = [\"test_*\"]\n\n# Performance options\naddopts = [\n    \"-v\",                    # Verbose output\n    \"--tb=short\",            # Shorter tracebacks\n    \"-x\",                    # Stop on first failure (fast feedback)\n    \"--strict-markers\",      # Enforce marker registration\n    \"-p\", \"no:warnings\",     # Disable warnings (cleaner output)\n]\n\n# Markers\nmarkers = [\n    \"slow: marks tests as slow (deselect with '-m \\\"not slow\\\"')\",\n    \"integration: marks integration tests\",\n    \"unit: marks unit tests\",\n]\n\n# Async configuration\nasyncio_default_fixture_loop_scope = \"function\"\n```\n\n## Requirements (Optimized Versions)\n\n```txt\n# requirements-test.txt\npytest>=8.0.0\npytest-asyncio>=0.23.0\npytest-cov>=4.1.0\nhttpx>=0.27.0\naiosqlite>=0.19.0\n```\n\n## Why AsyncClient Over TestClient\n\n| Aspect | TestClient (sync) | AsyncClient |\n|--------|-------------------|-------------|\n| **Speed** | Sync-to-async bridge overhead | Native async, ~20% faster |\n| **Memory** | Thread pool for async | Single event loop |\n| **Compatibility** | May cause event loop issues | Works with async fixtures |\n| **Recommended** | Simple sync endpoints | Async endpoints (standard) |\n\n```python\n# AVOID: Sync TestClient with async endpoints\nfrom fastapi.testclient import TestClient\nclient = TestClient(app)  # Creates new event loop, bridge overhead\n\n# PREFER: AsyncClient\nfrom httpx import AsyncClient, ASGITransport\nasync with AsyncClient(transport=ASGITransport(app=app), base_url=\"http://test\") as client:\n    response = await client.get(\"/\")  # Native async\n```\n\n## Transaction Rollback vs Schema Recreation\n\n```python\n# SLOW: Recreate schema per test\n@pytest.fixture(scope=\"function\")\nasync def db_session(async_engine):\n    async with async_engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)  # Slow!\n\n    async_session = async_sessionmaker(async_engine)\n    async with async_session() as session:\n        yield session\n\n    async with async_engine.begin() as conn:\n        await conn.run_sync(Base.metadata.drop_all)  # Slow!\n\n# FAST: Transaction rollback per test\n@pytest.fixture(scope=\"function\")\nasync def db_session(async_engine):  # Engine created once at session scope\n    async_session = async_sessionmaker(async_engine, class_=AsyncSession)\n    async with async_session() as session:\n        async with session.begin():\n            yield session\n            await session.rollback()  # Fast! Just rollback\n```\n\n**Performance Comparison:**\n| Approach | 100 tests | 1000 tests |\n|----------|-----------|------------|\n| Schema recreation | ~60s | ~600s |\n| Transaction rollback | ~5s | ~50s |\n\n## Parallel Test Execution\n\n```bash\n# Install pytest-xdist\npip install pytest-xdist\n\n# Run tests in parallel (auto-detect CPU cores)\npytest -n auto\n\n# Run with specific worker count\npytest -n 4\n```\n\n```toml\n# pyproject.toml for parallel\n[tool.pytest.ini_options]\naddopts = [\"-n\", \"auto\"]\n```\n\n**Note:** Requires isolated database per worker for parallel execution.\n\n## Selective Test Running\n\n```bash\n# Run specific test\npytest tests/test_items.py::test_create_item -v\n\n# Run tests matching pattern\npytest -k \"test_create\" -v\n\n# Run by marker\npytest -m \"not slow\" -v\n\n# Stop on first failure\npytest -x\n\n# Run last failed tests only\npytest --lf\n\n# Run failed first, then rest\npytest --ff\n```\n\n## Coverage Configuration\n\n```toml\n# pyproject.toml\n[tool.coverage.run]\nsource = [\"app\"]\nbranch = true\nomit = [\"*/tests/*\", \"*/__pycache__/*\"]\n\n[tool.coverage.report]\nexclude_lines = [\n    \"pragma: no cover\",\n    \"if TYPE_CHECKING:\",\n    \"raise NotImplementedError\",\n]\nfail_under = 80\nshow_missing = true\n```\n\n```bash\n# Run with coverage\npytest --cov=app --cov-report=term-missing --cov-report=html\n```\n\n## Fixture Caching\n\n```python\n# Expensive fixture - cache at session level\n@pytest.fixture(scope=\"session\")\nasync def expensive_resource():\n    \"\"\"Created once, cached for entire test session.\"\"\"\n    resource = await create_expensive_resource()\n    yield resource\n    await resource.cleanup()\n\n# Use lru_cache for non-fixture helpers\nfrom functools import lru_cache\n\n@lru_cache(maxsize=1)\ndef get_test_settings():\n    return Settings(_env_file=\".env.test\")\n```\n\n## Async Fixture Best Practices\n\n```python\n# Correct: async generator fixture\n@pytest.fixture\nasync def db_session() -> AsyncGenerator[AsyncSession, None]:\n    async with session_factory() as session:\n        yield session\n\n# Correct: async fixture with cleanup\n@pytest.fixture\nasync def temp_file():\n    path = Path(\"/tmp/test_file.txt\")\n    path.write_text(\"test\")\n    yield path\n    path.unlink()  # Cleanup\n\n# Avoid: Mixing sync and async improperly\n@pytest.fixture\ndef bad_fixture():  # Sync fixture\n    return asyncio.run(async_operation())  # Creates new event loop!\n```\n\n## Performance Monitoring\n\n```python\n# conftest.py - Add timing to slow tests\nimport time\n\n@pytest.fixture(autouse=True)\ndef test_timing(request):\n    start = time.perf_counter()\n    yield\n    duration = time.perf_counter() - start\n    if duration > 1.0:  # Log tests taking > 1 second\n        print(f\"\\n[SLOW] {request.node.name}: {duration:.2f}s\")\n```\n\n```bash\n# Show test durations\npytest --durations=10  # Show 10 slowest tests\npytest --durations=0   # Show all test durations\n```\n",
        "skills/fastapi-pytest-tddhelper/references/tdd-workflow.md": "# TDD Workflow: Red-Green-Refactor\n\n## The TDD Cycle\n\n```\n    \n                                         \n                                         \n            \n  RED     GREEN    REFACTOR  \n          \nWrite         Write          Optimize\nfailing       minimal        without\ntest          code           breaking\n```\n\n## Step 1: RED - Write Failing Test First\n\nWrite a test that defines expected behavior. It MUST fail initially.\n\n```python\n# tests/test_items.py\nimport pytest\nfrom httpx import AsyncClient\nfrom pydantic import BaseModel\n\n# Define expected response shape with Pydantic\nclass ItemResponse(BaseModel):\n    id: int\n    name: str\n    price: float\n    quantity: int\n    total_value: float  # Computed field\n\nclass PaginatedItems(BaseModel):\n    items: list[ItemResponse]\n    total: int\n    page: int\n    size: int\n    pages: int\n\n@pytest.mark.asyncio\nasync def test_get_items_with_pagination_and_filters(client: AsyncClient, item_factory):\n    \"\"\"\n    RED: Test complex route with query params.\n    Expected: GET /items?min_price=5&max_price=50&page=1&size=10\n    \"\"\"\n    # Arrange: Create test data\n    await item_factory(name=\"Cheap\", price=3.0, quantity=10)\n    await item_factory(name=\"Mid\", price=25.0, quantity=5)\n    await item_factory(name=\"Expensive\", price=100.0, quantity=2)\n\n    # Act: Call endpoint with filters\n    response = await client.get(\n        \"/items\",\n        params={\"min_price\": 5, \"max_price\": 50, \"page\": 1, \"size\": 10}\n    )\n\n    # Assert: Validate with Pydantic (not just status code)\n    assert response.status_code == 200\n\n    # Validate response shape\n    data = PaginatedItems(**response.json())\n    assert data.total == 1  # Only \"Mid\" matches\n    assert data.page == 1\n    assert len(data.items) == 1\n    assert data.items[0].name == \"Mid\"\n    assert data.items[0].total_value == 125.0  # price * quantity\n```\n\nRun test - it MUST fail:\n```bash\npytest tests/test_items.py::test_get_items_with_pagination_and_filters -v\n# Expected: FAILED (404 or AttributeError)\n```\n\n## Step 2: GREEN - Minimal Implementation\n\nWrite the minimum code to make the test pass. No optimization yet.\n\n```python\n# app/schemas.py\nfrom pydantic import BaseModel, computed_field\n\nclass ItemBase(BaseModel):\n    name: str\n    price: float\n    quantity: int = 1\n\nclass ItemCreate(ItemBase):\n    pass\n\nclass ItemResponse(ItemBase):\n    id: int\n\n    @computed_field\n    @property\n    def total_value(self) -> float:\n        return self.price * self.quantity\n\n    model_config = {\"from_attributes\": True}\n\nclass PaginatedItems(BaseModel):\n    items: list[ItemResponse]\n    total: int\n    page: int\n    size: int\n    pages: int\n```\n\n```python\n# app/routers/items.py\nfrom fastapi import APIRouter, Depends, Query\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy import select, func\nfrom math import ceil\n\nfrom app.database import get_db\nfrom app.models import Item\nfrom app.schemas import ItemResponse, PaginatedItems\n\nrouter = APIRouter(prefix=\"/items\", tags=[\"items\"])\n\n@router.get(\"\", response_model=PaginatedItems)\nasync def get_items(\n    min_price: float | None = Query(default=None, ge=0),\n    max_price: float | None = Query(default=None, ge=0),\n    page: int = Query(default=1, ge=1),\n    size: int = Query(default=10, ge=1, le=100),\n    db: AsyncSession = Depends(get_db)\n):\n    # Build query with filters\n    query = select(Item)\n\n    if min_price is not None:\n        query = query.where(Item.price >= min_price)\n    if max_price is not None:\n        query = query.where(Item.price <= max_price)\n\n    # Count total\n    count_query = select(func.count()).select_from(query.subquery())\n    total = (await db.execute(count_query)).scalar()\n\n    # Paginate\n    offset = (page - 1) * size\n    query = query.offset(offset).limit(size)\n\n    result = await db.execute(query)\n    items = result.scalars().all()\n\n    return PaginatedItems(\n        items=items,\n        total=total,\n        page=page,\n        size=size,\n        pages=ceil(total / size) if total > 0 else 0\n    )\n```\n\nRun test - it MUST pass:\n```bash\npytest tests/test_items.py::test_get_items_with_pagination_and_filters -v\n# Expected: PASSED\n```\n\n## Step 3: REFACTOR - Optimize\n\nNow improve code quality and performance without breaking tests.\n\n```python\n# app/routers/items.py (refactored)\nfrom fastapi import APIRouter, Depends, Query\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy import select, func, and_\nfrom math import ceil\nfrom typing import Annotated\n\nfrom app.database import get_db\nfrom app.models import Item\nfrom app.schemas import PaginatedItems\n\nrouter = APIRouter(prefix=\"/items\", tags=[\"items\"])\n\n# Reusable dependencies\nPaginationParams = Annotated[\n    tuple[int, int],\n    Depends(lambda page: int = Query(1, ge=1), size: int = Query(10, ge=1, le=100): (page, size))\n]\n\nasync def get_pagination(\n    page: int = Query(default=1, ge=1),\n    size: int = Query(default=10, ge=1, le=100)\n) -> tuple[int, int]:\n    return page, size\n\n@router.get(\"\", response_model=PaginatedItems)\nasync def get_items(\n    min_price: float | None = Query(default=None, ge=0),\n    max_price: float | None = Query(default=None, ge=0),\n    pagination: tuple[int, int] = Depends(get_pagination),\n    db: AsyncSession = Depends(get_db)\n):\n    page, size = pagination\n\n    # Build filters once\n    filters = []\n    if min_price is not None:\n        filters.append(Item.price >= min_price)\n    if max_price is not None:\n        filters.append(Item.price <= max_price)\n\n    where_clause = and_(*filters) if filters else True\n\n    # Single optimized query with count\n    base_query = select(Item).where(where_clause)\n\n    # Execute count and data in parallel-friendly way\n    count_result = await db.execute(\n        select(func.count(Item.id)).where(where_clause)\n    )\n    total = count_result.scalar() or 0\n\n    # Fetch paginated results\n    offset = (page - 1) * size\n    result = await db.execute(base_query.offset(offset).limit(size))\n    items = result.scalars().all()\n\n    return PaginatedItems(\n        items=items,\n        total=total,\n        page=page,\n        size=size,\n        pages=ceil(total / size) if total > 0 else 0\n    )\n```\n\nRun all tests to ensure refactor didn't break anything:\n```bash\npytest tests/ -v\n# All tests should pass\n```\n\n## Parametrized Testing for Edge Cases\n\n```python\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\"min_price,max_price,expected_count\", [\n    (None, None, 3),      # No filter - all items\n    (0, 5, 1),            # Only cheap\n    (20, 30, 1),          # Only mid-range\n    (50, 200, 1),         # Only expensive\n    (1000, 2000, 0),      # No matches\n])\nasync def test_items_price_filter_combinations(\n    client: AsyncClient,\n    item_factory,\n    min_price,\n    max_price,\n    expected_count\n):\n    # Arrange\n    await item_factory(name=\"Cheap\", price=3.0)\n    await item_factory(name=\"Mid\", price=25.0)\n    await item_factory(name=\"Expensive\", price=100.0)\n\n    # Act\n    params = {}\n    if min_price is not None:\n        params[\"min_price\"] = min_price\n    if max_price is not None:\n        params[\"max_price\"] = max_price\n\n    response = await client.get(\"/items\", params=params)\n\n    # Assert\n    assert response.status_code == 200\n    assert response.json()[\"total\"] == expected_count\n```\n\n## TDD Checklist\n\n| Phase | Action | Verification |\n|-------|--------|--------------|\n| RED | Write test with assertions | Test fails with expected error |\n| GREEN | Implement minimal code | Test passes |\n| REFACTOR | Optimize/clean code | All tests still pass |\n| REPEAT | Next feature/edge case | Cycle continues |\n",
        "skills/fastapi-pytest-tddhelper/references/testing-patterns.md": "# Advanced Testing Patterns\n\n## Dependency Override Patterns\n\n### Database Override\n\n```python\n@pytest.fixture\nasync def client(db_session: AsyncSession):\n    async def override_get_db():\n        yield db_session\n\n    app.dependency_overrides[get_db] = override_get_db\n    async with AsyncClient(transport=ASGITransport(app=app), base_url=\"http://test\") as ac:\n        yield ac\n    app.dependency_overrides.clear()\n```\n\n### Authentication Override\n\n```python\nfrom app.auth import get_current_user\n\n@pytest.fixture\ndef mock_user():\n    return User(id=1, email=\"test@example.com\", is_active=True)\n\n@pytest.fixture\nasync def authenticated_client(client: AsyncClient, mock_user: User):\n    app.dependency_overrides[get_current_user] = lambda: mock_user\n    yield client\n    app.dependency_overrides.pop(get_current_user, None)\n\n@pytest.fixture\nasync def admin_client(client: AsyncClient):\n    admin = User(id=1, email=\"admin@example.com\", is_admin=True)\n    app.dependency_overrides[get_current_user] = lambda: admin\n    yield client\n    app.dependency_overrides.pop(get_current_user, None)\n```\n\n### External Service Override\n\n```python\nfrom app.services import EmailService, get_email_service\n\nclass MockEmailService:\n    def __init__(self):\n        self.sent_emails = []\n\n    async def send(self, to: str, subject: str, body: str):\n        self.sent_emails.append({\"to\": to, \"subject\": subject, \"body\": body})\n\n@pytest.fixture\ndef mock_email_service():\n    return MockEmailService()\n\n@pytest.fixture\nasync def client_with_mock_email(client: AsyncClient, mock_email_service):\n    app.dependency_overrides[get_email_service] = lambda: mock_email_service\n    yield client, mock_email_service\n    app.dependency_overrides.pop(get_email_service, None)\n\n@pytest.mark.asyncio\nasync def test_registration_sends_welcome_email(client_with_mock_email):\n    client, email_service = client_with_mock_email\n\n    response = await client.post(\"/users/\", json={\n        \"email\": \"new@example.com\",\n        \"password\": \"secret123\"\n    })\n\n    assert response.status_code == 201\n    assert len(email_service.sent_emails) == 1\n    assert email_service.sent_emails[0][\"to\"] == \"new@example.com\"\n```\n\n## Parametrize Patterns\n\n### Basic Parametrize\n\n```python\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\"name,price,expected_status\", [\n    (\"Valid Item\", 10.0, 201),\n    (\"\", 10.0, 422),           # Empty name\n    (\"Item\", -5.0, 422),       # Negative price\n    (\"A\" * 256, 10.0, 422),    # Name too long\n])\nasync def test_create_item_validation(client: AsyncClient, name, price, expected_status):\n    response = await client.post(\"/items/\", json={\"name\": name, \"price\": price})\n    assert response.status_code == expected_status\n```\n\n### Parametrize with IDs\n\n```python\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\"endpoint,method,expected_status\", [\n    (\"/items/\", \"GET\", 200),\n    (\"/items/1\", \"GET\", 404),\n    (\"/items/\", \"POST\", 401),\n    (\"/users/me\", \"GET\", 401),\n], ids=[\n    \"list_items_ok\",\n    \"get_missing_item\",\n    \"create_requires_auth\",\n    \"me_requires_auth\",\n])\nasync def test_endpoint_access(client: AsyncClient, endpoint, method, expected_status):\n    response = await getattr(client, method.lower())(endpoint)\n    assert response.status_code == expected_status\n```\n\n### Parametrize with Fixtures\n\n```python\n@pytest.fixture(params=[\"sqlite\", \"postgres\"])\ndef database_url(request):\n    urls = {\n        \"sqlite\": \"sqlite+aiosqlite:///./test.db\",\n        \"postgres\": \"postgresql+asyncpg://test:test@localhost/test\"\n    }\n    return urls[request.param]\n```\n\n## CRUD Test Template\n\n```python\n# tests/test_crud_items.py\nimport pytest\nfrom httpx import AsyncClient\nfrom app.schemas import ItemResponse, ItemCreate\n\nclass TestItemsCRUD:\n    \"\"\"Complete CRUD test suite for Items.\"\"\"\n\n    @pytest.mark.asyncio\n    async def test_create_item(self, client: AsyncClient):\n        \"\"\"POST /items/ - Create new item.\"\"\"\n        payload = {\"name\": \"Widget\", \"price\": 29.99, \"quantity\": 10}\n\n        response = await client.post(\"/items/\", json=payload)\n\n        assert response.status_code == 201\n        item = ItemResponse(**response.json())\n        assert item.name == \"Widget\"\n        assert item.price == 29.99\n\n    @pytest.mark.asyncio\n    async def test_read_item(self, client: AsyncClient, item_factory):\n        \"\"\"GET /items/{id} - Read existing item.\"\"\"\n        created = await item_factory(name=\"Test Item\")\n\n        response = await client.get(f\"/items/{created.id}\")\n\n        assert response.status_code == 200\n        item = ItemResponse(**response.json())\n        assert item.id == created.id\n\n    @pytest.mark.asyncio\n    async def test_read_item_not_found(self, client: AsyncClient):\n        \"\"\"GET /items/{id} - 404 for non-existent item.\"\"\"\n        response = await client.get(\"/items/99999\")\n        assert response.status_code == 404\n\n    @pytest.mark.asyncio\n    async def test_update_item(self, client: AsyncClient, item_factory):\n        \"\"\"PUT /items/{id} - Update existing item.\"\"\"\n        created = await item_factory(name=\"Original\", price=10.0)\n\n        response = await client.put(\n            f\"/items/{created.id}\",\n            json={\"name\": \"Updated\", \"price\": 20.0}\n        )\n\n        assert response.status_code == 200\n        item = ItemResponse(**response.json())\n        assert item.name == \"Updated\"\n        assert item.price == 20.0\n\n    @pytest.mark.asyncio\n    async def test_partial_update_item(self, client: AsyncClient, item_factory):\n        \"\"\"PATCH /items/{id} - Partial update.\"\"\"\n        created = await item_factory(name=\"Original\", price=10.0)\n\n        response = await client.patch(\n            f\"/items/{created.id}\",\n            json={\"price\": 15.0}\n        )\n\n        assert response.status_code == 200\n        item = ItemResponse(**response.json())\n        assert item.name == \"Original\"  # Unchanged\n        assert item.price == 15.0       # Updated\n\n    @pytest.mark.asyncio\n    async def test_delete_item(self, client: AsyncClient, item_factory):\n        \"\"\"DELETE /items/{id} - Delete existing item.\"\"\"\n        created = await item_factory()\n\n        response = await client.delete(f\"/items/{created.id}\")\n        assert response.status_code == 204\n\n        # Verify deleted\n        get_response = await client.get(f\"/items/{created.id}\")\n        assert get_response.status_code == 404\n\n    @pytest.mark.asyncio\n    async def test_list_items(self, client: AsyncClient, item_factory):\n        \"\"\"GET /items/ - List all items with pagination.\"\"\"\n        for i in range(5):\n            await item_factory(name=f\"Item {i}\")\n\n        response = await client.get(\"/items/\", params={\"page\": 1, \"size\": 3})\n\n        assert response.status_code == 200\n        data = response.json()\n        assert len(data[\"items\"]) == 3\n        assert data[\"total\"] == 5\n```\n\n## Testing WebSockets\n\n```python\n@pytest.mark.asyncio\nasync def test_websocket_connection():\n    async with AsyncClient(transport=ASGITransport(app=app), base_url=\"http://test\") as client:\n        async with client.stream(\"GET\", \"/ws\") as response:\n            # For actual WebSocket, use websockets library\n            pass\n\n# Using websockets library\nimport websockets\n\n@pytest.mark.asyncio\nasync def test_websocket_echo():\n    async with websockets.connect(\"ws://localhost:8000/ws\") as ws:\n        await ws.send(\"Hello\")\n        response = await ws.recv()\n        assert response == \"Message received: Hello\"\n```\n\n## Testing Background Tasks\n\n```python\nfrom unittest.mock import AsyncMock, patch\n\n@pytest.mark.asyncio\nasync def test_endpoint_triggers_background_task(client: AsyncClient):\n    with patch(\"app.tasks.send_notification\", new_callable=AsyncMock) as mock_task:\n        response = await client.post(\"/orders/\", json={\"product_id\": 1})\n\n        assert response.status_code == 201\n        mock_task.assert_called_once()\n```\n\n## Testing File Uploads\n\n```python\n@pytest.mark.asyncio\nasync def test_upload_file(client: AsyncClient):\n    file_content = b\"test file content\"\n\n    response = await client.post(\n        \"/upload/\",\n        files={\"file\": (\"test.txt\", file_content, \"text/plain\")}\n    )\n\n    assert response.status_code == 200\n    assert response.json()[\"filename\"] == \"test.txt\"\n    assert response.json()[\"size\"] == len(file_content)\n\n@pytest.mark.asyncio\nasync def test_upload_multiple_files(client: AsyncClient):\n    files = [\n        (\"files\", (\"file1.txt\", b\"content1\", \"text/plain\")),\n        (\"files\", (\"file2.txt\", b\"content2\", \"text/plain\")),\n    ]\n\n    response = await client.post(\"/upload-multiple/\", files=files)\n\n    assert response.status_code == 200\n    assert len(response.json()[\"filenames\"]) == 2\n```\n",
        "skills/github-assistant/.claude-plugin/plugin.json": "{\n  \"name\": \"github-assistant\",\n  \"description\": \"Interactive troubleshooting for GitHub problems - authentication, sync issues, merge conflicts, and sensitive data exposure\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Ali Jilani\"\n  },\n  \"category\": \"development\",\n  \"keywords\": [\"github\", \"git\", \"authentication\", \"merge-conflicts\", \"troubleshooting\"]\n}\n",
        "skills/github-assistant/SKILL.md": "---\nname: github-assistant\ndescription: Interactive troubleshooting assistant for the top 5 GitHub problems faced by new users. Diagnoses authentication failures, Git vs GitHub confusion, local-remote sync issues, merge conflicts, and accidental sensitive data commits. Provides step-by-step solutions with command execution. Use when users mention GitHub errors, password issues, sync problems, merge conflicts, or accidentally committing secrets.\n---\n\n# GitHub Assistant Skill\n\nThis skill provides interactive, diagnostic-driven troubleshooting for the most common GitHub problems faced by new users. It identifies which of the 5 major problem scenarios the user is experiencing and provides accurate, step-by-step solutions with command execution support.\n\n## When to Activate This Skill\n\n**Trigger conditions:**\n- User mentions GitHub authentication errors: \"password doesn't work\", \"authentication failed\", \"can't push\"\n- User is confused about Git vs GitHub: \"is Git the same as GitHub?\", \"which commands are Git?\"\n- User wants to sync local folder with GitHub: \"sync my folder\", \"automatically update GitHub\", \"keep in sync\"\n- User encounters merge conflicts: \"merge conflict\", \"conflicting changes\", \"pull failed\"\n- User accidentally committed sensitive data: \"committed password\", \"exposed API key\", \"sensitive file in GitHub\"\n- User asks general GitHub help questions for beginners\n\n**Initial offer:**\nGreet the user and explain this skill will help diagnose and solve their GitHub issue. Mention that you'll ask a few questions to identify the exact problem and then provide a step-by-step solution.\n\n## Diagnostic Workflow\n\n### Step 1: Problem Identification\n\nAsk the user to describe their issue or choose from common scenarios:\n\nPresent these options:\n1. **Authentication/Password Issues** - Can't push or clone, password doesn't work\n2. **Understanding Git vs GitHub** - Confused about what commands to use where\n3. **Syncing Local Folder with GitHub** - Want to keep local files in sync with repository\n4. **Merge Conflicts** - Getting conflict errors when pulling or merging\n5. **Sensitive Data Exposure** - Accidentally committed passwords, API keys, or secrets\n6. **Other/Not Sure** - Describe the issue in your own words\n\nWait for user response and route to appropriate solution workflow.\n\n**If user chooses \"Other/Not Sure\":**\nAsk them to describe:\n- What they were trying to do\n- What command they ran (if any)\n- The exact error message they received\n- Their current situation\n\nBased on their description, identify which of the 5 scenarios best matches and proceed to that workflow.\n\n### Step 2: Route to Appropriate Solution\n\nBased on user selection, proceed to the corresponding solution workflow below.\n\n## Solution Workflow 1: Authentication Failures\n\n**Problem confirmed:** User is experiencing \"Support for password authentication was removed\" or similar authentication errors.\n\n### Step 1: Explain the Issue\n\nInform the user:\n- GitHub removed password authentication on August 13, 2021\n- They now need to use either Personal Access Token (PAT) or SSH keys\n- Their GitHub account password will not work for Git operations\n\n### Step 2: Recommend Approach\n\nAsk which authentication method they prefer:\n- **Personal Access Token (Recommended for beginners)** - Use a token instead of password\n- **SSH Keys (More advanced)** - Set up SSH key authentication\n\n### Step 3a: Personal Access Token Setup (if chosen)\n\nProvide step-by-step instructions:\n\n1. **Generate a PAT on GitHub:**\n   - Go to: https://github.com/settings/tokens\n   - Click \"Generate new token\"  \"Generate new token (classic)\"\n   - Give it a descriptive name (e.g., \"My Laptop Git Access\")\n   - Set expiration (recommend 90 days for security)\n   - Select scopes: Check \"repo\" for full repository access\n   - Click \"Generate token\" at the bottom\n   - **IMPORTANT:** Copy the token immediately - you won't see it again!\n\n2. **Use the token:**\n   - When Git prompts for a password, paste the token instead\n   - The token should start with `ghp_` or `github_pat_`\n\n3. **Store credentials (optional but recommended):**\n\nAsk if they want help setting up credential caching so they don't have to enter the token every time.\n\nIf yes, detect their operating system and provide appropriate command:\n\n**For Windows:**\n```bash\ngit config --global credential.helper wincred\n```\n\n**For macOS:**\n```bash\ngit config --global credential.helper osxkeychain\n```\n\n**For Linux:**\n```bash\ngit config --global credential.helper cache\n# Or for permanent storage:\ngit config --global credential.helper store\n```\n\nOffer to run the appropriate command if they're in a terminal environment.\n\n### Step 3b: SSH Key Setup (if chosen)\n\nProvide step-by-step instructions:\n\n1. **Check for existing SSH keys:**\n```bash\nls -al ~/.ssh\n```\n\nOffer to run this command for them.\n\n2. **Generate new SSH key (if needed):**\n```bash\nssh-keygen -t ed25519 -C \"their-email@example.com\"\n```\n\nAsk for their GitHub email and offer to run this command.\n\n3. **Add SSH key to ssh-agent:**\n```bash\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_ed25519\n```\n\n4. **Copy public key:**\n```bash\ncat ~/.ssh/id_ed25519.pub\n```\n\nOffer to run this and display the public key for them to copy.\n\n5. **Add to GitHub:**\n   - Go to: https://github.com/settings/keys\n   - Click \"New SSH key\"\n   - Paste the public key\n   - Give it a title and save\n\n6. **Test connection:**\n```bash\nssh -T git@github.com\n```\n\nOffer to run this test.\n\n7. **Update remote URL (if needed):**\n\nCheck their current remote:\n```bash\ngit remote -v\n```\n\nIf it shows HTTPS (https://github.com/...), offer to convert to SSH:\n```bash\ngit remote set-url origin git@github.com:username/repo.git\n```\n\n### Step 4: Verify Solution\n\nAsk user to try their original operation (push, pull, clone) and confirm it works.\n\nIf still having issues, ask for the error message and troubleshoot further.\n\n## Solution Workflow 2: Git vs GitHub Confusion\n\n**Problem confirmed:** User is confused about the difference between Git and GitHub.\n\n### Step 1: Explain the Difference\n\nProvide clear explanation:\n\n**Git** = The version control tool installed on your computer\n- Command-line tool you run locally\n- Commands: `git init`, `git add`, `git commit`, `git status`, etc.\n- Works entirely offline\n- Manages your local repository\n\n**GitHub** = The hosting platform (website) for your code\n- Remote server accessible at github.com\n- Stores your code in the cloud\n- Provides web interface for collaboration\n- Your code lives here so others can access it\n\n**The Connection:**\n- You use Git (locally) to interact with GitHub (remotely)\n- Git commands like `git push` send your local changes to GitHub\n- Git commands like `git pull` download changes from GitHub\n\n### Step 2: Common Command Breakdown\n\nShow which commands are used for what:\n\n**Local Git Operations (no internet needed):**\n- `git init` - Initialize a new repository\n- `git add <file>` - Stage files for commit\n- `git commit -m \"message\"` - Save changes locally\n- `git status` - Check what's changed\n- `git log` - View commit history\n- `git branch` - Manage branches\n\n**GitHub Operations (require internet):**\n- `git clone <url>` - Download repository from GitHub\n- `git push` - Upload your commits to GitHub\n- `git pull` - Download changes from GitHub\n- `git fetch` - Check for remote changes\n\n**GitHub Web Interface:**\n- Creating repositories\n- Managing collaborators\n- Viewing pull requests\n- Adjusting settings\n\n### Step 3: Practical Example\n\nOffer to demonstrate a typical workflow:\n\n1. Create or modify files  **You do this in your editor**\n2. `git add .`  **Git command (local)**\n3. `git commit -m \"description\"`  **Git command (local)**\n4. `git push`  **Git command that talks to GitHub (remote)**\n\n### Step 4: Address Specific Questions\n\nAsk if they have any specific questions about:\n- Which tool to use for their current task\n- Where to find certain features\n- How to accomplish a specific goal\n\nProvide targeted answers based on their questions.\n\n## Solution Workflow 3: Syncing Local Folder with GitHub\n\n**Problem confirmed:** User wants to keep their local folder synchronized with a GitHub repository.\n\n### Step 1: Set Expectations\n\nExplain that Git is not automatic cloud storage:\n- Unlike Dropbox or Google Drive, Git requires manual synchronization\n- This is intentional - you control exactly what gets saved and when\n- The workflow has three steps: stage  commit  push\n\n### Step 2: Check Current Setup\n\nAsk: \"Do you already have a GitHub repository created for this folder?\"\n\n**If NO:**\n\n1. **Create repository on GitHub:**\n   - Go to: https://github.com/new\n   - Give it a name matching your local folder\n   - Choose public or private\n   - **Do NOT** initialize with README, .gitignore, or license (since you have local files)\n   - Click \"Create repository\"\n\n2. **Initialize local folder:**\n\nAsk for their local folder path and offer to run:\n```bash\ncd /path/to/their/folder\ngit init\ngit remote add origin https://github.com/username/repo-name.git\n```\n\n**If YES:**\n\nCheck if folder is already initialized:\n```bash\ngit status\n```\n\nOffer to run this. If it shows \"not a git repository\", proceed with initialization.\n\n### Step 3: Configure .gitignore\n\nBefore first commit, ask about files they DON'T want synced:\n\nCommon files to exclude:\n- System files (.DS_Store, Thumbs.db)\n- Dependencies (node_modules/, venv/, .venv/)\n- Build outputs (dist/, build/, *.pyc)\n- Environment files (.env, .env.local)\n- IDE settings (.vscode/, .idea/)\n- Logs (*.log)\n\nOffer to create a .gitignore file with appropriate exclusions based on their project type.\n\nAsk: \"What type of project is this?\" (Python, JavaScript, Java, etc.)\n\nGenerate appropriate .gitignore template and offer to create the file.\n\n### Step 4: Initial Sync\n\nWalk through the first synchronization:\n\n```bash\n# 1. Stage all files\ngit add .\n\n# 2. Create first commit\ngit commit -m \"Initial commit\"\n\n# 3. Push to GitHub\ngit branch -M main\ngit push -u origin main\n```\n\nOffer to run these commands step by step, explaining what each does.\n\n### Step 5: Regular Workflow\n\nTeach the ongoing synchronization routine:\n\n**Before starting work each day:**\n```bash\ngit pull origin main\n```\nThis downloads any changes from GitHub (in case you worked elsewhere or teammates contributed).\n\n**After making changes:**\n```bash\n# 1. Check what changed\ngit status\n\n# 2. Review changes (optional but recommended)\ngit diff\n\n# 3. Stage specific files or all changes\ngit add <file>  # for specific file\n# or\ngit add .  # for all changes\n\n# 4. Commit with descriptive message\ngit commit -m \"Describe what you changed and why\"\n\n# 5. Push to GitHub\ngit push origin main\n```\n\n### Step 6: Best Practices\n\nShare important tips:\n- **Commit frequently:** Small, logical commits are better than large ones\n- **Write good messages:** Describe WHAT and WHY, not just what files changed\n- **Pull before push:** Always pull latest changes before pushing\n- **Review before commit:** Use `git status` and `git diff` to see what you're committing\n\n### Step 7: Create Quick Reference\n\nOffer to create a cheat sheet file in their directory with the common commands.\n\nIf accepted, create `GIT_WORKFLOW.md`:\n```markdown\n# Git Sync Workflow\n\n## Daily Routine\n\n### Before Starting Work\ngit pull origin main\n\n### After Making Changes\ngit status              # See what changed\ngit diff                # Review changes\ngit add .               # Stage all changes\ngit commit -m \"message\" # Commit with description\ngit push origin main    # Upload to GitHub\n\n## Tips\n- Commit frequently (multiple times per day)\n- Pull before starting work\n- Write clear commit messages\n- Review changes before committing\n```\n\n## Solution Workflow 4: Merge Conflicts\n\n**Problem confirmed:** User encountered merge conflict errors.\n\n### Step 1: Calm and Explain\n\nReassure the user:\n- Merge conflicts are normal and expected in collaborative work\n- They happen when the same lines of code are edited in different ways\n- They're not dangerous - just need manual resolution\n- Git is asking you to choose which version to keep\n\n### Step 2: Understand the Situation\n\nAsk clarifying questions:\n- What were you doing when this happened? (pulling, merging, rebasing)\n- Are you working alone or with others?\n- Do you know what changes exist on GitHub vs your local files?\n\n### Step 3: Show Current Status\n\nOffer to run:\n```bash\ngit status\n```\n\nThis shows which files have conflicts.\n\n### Step 4: Explain Conflict Markers\n\nExplain what they'll see in conflicted files:\n\n```\n<<<<<<< HEAD\nYour local changes are here\n=======\nThe changes from GitHub are here\n>>>>>>> branch-name\n```\n\n- `<<<<<<< HEAD` marks the start of your local version\n- `=======` separates the two versions\n- `>>>>>>>` marks the end of the remote version\n\n### Step 5: Resolution Strategy\n\nAsk which approach they prefer:\n\n**Option A: Manual Resolution (Recommended)**\n1. Open each conflicted file\n2. Find the conflict markers\n3. Decide what to keep:\n   - Keep your version (delete the other and markers)\n   - Keep their version (delete yours and markers)\n   - Combine both (merge the logic and delete markers)\n4. Save the file\n5. Stage the resolved file: `git add <file>`\n6. Complete the merge: `git commit`\n\n**Option B: Choose All Yours**\n```bash\ngit checkout --ours <file>\ngit add <file>\n```\n\n**Option C: Choose All Theirs**\n```bash\ngit checkout --theirs <file>\ngit add <file>\n```\n\n**Option D: Abort the Merge**\n```bash\ngit merge --abort\n# or\ngit rebase --abort\n```\n\n### Step 6: Guide Through Resolution\n\nBased on their choice, guide step-by-step:\n\n**For Manual Resolution:**\n1. Offer to read the conflicted file and show them the conflicts\n2. For each conflict, ask: \"Do you want to keep your version, their version, or combine them?\"\n3. Offer to make the edits for them\n4. After all conflicts resolved, run:\n```bash\ngit add <resolved-file>\n```\n5. Complete the merge:\n```bash\ngit commit -m \"Resolved merge conflicts\"\n```\n\n### Step 7: Verify and Complete\n\nAfter resolution:\n```bash\ngit status  # Should show no conflicts\ngit push origin main  # Upload the resolution\n```\n\nOffer to run these commands.\n\n### Step 8: Prevention Tips\n\nShare strategies to minimize future conflicts:\n- **Pull frequently:** Get changes before they pile up\n- **Communicate:** Let teammates know what files you're working on\n- **Keep commits small:** Easier to resolve smaller conflicts\n- **Work on different files:** When possible, divide work to avoid same-file edits\n\n## Solution Workflow 5: Sensitive Data Exposure\n\n**Problem confirmed:** User accidentally committed sensitive information (passwords, API keys, tokens, etc.)\n\n### Step 1: Assess Urgency\n\nAsk critical questions:\n- Have you pushed this to GitHub yet?\n- Is the repository public or private?\n- What type of sensitive data? (password, API key, private key, credentials)\n\n**CRITICAL: If pushed to public repository:**\nEmphasize extreme urgency:\n- Assume the secrets are already compromised\n- They need to rotate/change all exposed credentials IMMEDIATELY\n- Bots scan public GitHub commits for secrets within minutes\n\n### Step 2: Immediate Action - Rotate Credentials\n\n**This is the most important step:**\n\nInstruct user to immediately:\n1. **Change the exposed password** (if it's a password)\n2. **Revoke and regenerate API keys** (if API keys)\n3. **Rotate tokens** (if tokens/secrets)\n4. **Disable compromised credentials** (if service accounts)\n\nExplain: Removing the file from Git history does NOT undo the exposure. Anyone who saw the commit still has the secret.\n\n### Step 3: Identify Affected Files\n\nOffer to run:\n```bash\ngit status\n```\n\nAsk them to identify which files contain sensitive data.\n\n### Step 4: Removal Strategy\n\n**If NOT yet pushed:**\n\nExplain this is much simpler since it's only local.\n\n**Option A: Remove from last commit (if just committed):**\n```bash\n# Remove file from tracking but keep locally\ngit rm --cached <sensitive-file>\n\n# Amend the previous commit\ngit commit --amend -m \"Remove sensitive file\"\n```\n\n**Option B: Completely remove file:**\n```bash\ngit rm --cached <sensitive-file>\ngit commit -m \"Remove sensitive file\"\n```\n\nOffer to run these commands.\n\n**If ALREADY pushed:**\n\nExplain this is more complex and requires rewriting history.\n\n**Important warnings:**\n- This will rewrite commit history\n- If others have pulled the commits, they'll need to re-clone\n- This should only be done if absolutely necessary\n\n**Option A: Use BFG Repo-Cleaner (Recommended for large repos):**\n\nProvide instructions:\n1. Download BFG: https://rtyley.github.io/bfg-repo-cleaner/\n2. Clone a fresh mirror:\n```bash\ngit clone --mirror https://github.com/username/repo.git\n```\n3. Run BFG:\n```bash\nbfg --delete-files <filename> repo.git\n# or for text replacement\nbfg --replace-text passwords.txt repo.git\n```\n4. Clean up and push:\n```bash\ncd repo.git\ngit reflog expire --expire=now --all\ngit gc --prune=now --aggressive\ngit push --force\n```\n\n**Option B: Use git filter-branch (Built-in but slower):**\n```bash\ngit filter-branch --force --index-filter \\\n  \"git rm --cached --ignore-unmatch <path-to-file>\" \\\n  --prune-empty --tag-name-filter cat -- --all\n\ngit push --force --all\n```\n\nOffer to help run these commands but warn about the implications.\n\n### Step 5: Prevent Future Exposure\n\n**Create/Update .gitignore:**\n\nOffer to create or update .gitignore with common sensitive file patterns:\n```\n# Environment variables\n.env\n.env.local\n.env.*.local\n\n# Credentials\n**/credentials.json\n**/secrets.yml\n**/*secret*\n**/*credentials*\n\n# Private keys\n*.pem\n*.key\n*.p12\n*.pfx\n\n# API keys\n**/apikeys.txt\n\n# Config files with secrets\nconfig/database.yml\nconfig/secrets.yml\n```\n\nAsk about their specific project needs and customize accordingly.\n\n### Step 6: Implement Pre-commit Checks\n\nSuggest using tools to prevent future accidents:\n\n**Option 1: git-secrets**\n```bash\n# Install git-secrets\n# macOS\nbrew install git-secrets\n\n# Configure for repo\ngit secrets --install\ngit secrets --register-aws\n```\n\n**Option 2: detect-secrets**\n```bash\npip install detect-secrets\ndetect-secrets scan > .secrets.baseline\n```\n\nOffer to help set these up.\n\n### Step 7: Verify Removal\n\nAfter cleanup, verify the sensitive data is gone:\n```bash\ngit log --all --full-history -- <path-to-file>\n```\n\nShould show no commits.\n\nSearch for the sensitive string in history:\n```bash\ngit log -S \"sensitive-string\" --all\n```\n\nShould return no results.\n\n### Step 8: Final Checklist\n\nConfirm with user they've completed:\n- [ ] Rotated/changed all exposed credentials\n- [ ] Removed sensitive files from Git history\n- [ ] Added files to .gitignore\n- [ ] Pushed the cleanup (if applicable)\n- [ ] Verified removal with git log\n- [ ] Set up pre-commit hooks (optional)\n- [ ] Documented which credentials were exposed for their records\n\n## Ongoing Support and Follow-up\n\nAfter completing any solution workflow:\n\n1. **Verify Success:** Ask user to confirm the problem is resolved\n2. **Test the Solution:** Encourage them to test the operation that was failing\n3. **Provide Resources:** Share relevant GitHub documentation links\n4. **Offer Additional Help:** Ask if they have other GitHub-related questions\n5. **Teach Prevention:** Share best practices to avoid the issue in the future\n\n## Additional Troubleshooting\n\nIf the user's issue doesn't fit the 5 main scenarios or solution didn't work:\n\n### Common Additional Issues\n\n**Permission Denied Errors:**\n- Check repository access permissions on GitHub\n- Verify correct username/organization\n- Confirm repository exists and URL is correct\n\n**\"Repository not found\" Errors:**\n- Verify repository URL spelling\n- Check if repository is private (requires authentication)\n- Confirm user has access to the repository\n\n**Detached HEAD State:**\n- Explain what detached HEAD means\n- Guide back to branch: `git checkout main`\n\n**Large File Errors:**\n- Explain GitHub's file size limits (100MB)\n- Suggest Git LFS for large files\n- Help remove large files if needed\n\n**Untracked Files Overwrite Errors:**\n- Use `git stash` to save local changes\n- Pull, then apply stash: `git stash pop`\n\nFor any issue, maintain the diagnostic approach:\n1. Ask for exact error message\n2. Check current state with `git status`\n3. Understand what they were trying to do\n4. Provide step-by-step solution\n5. Verify resolution\n\n## Best Practices for This Skill\n\n**Tone:**\n- Patient and encouraging - users are learning\n- Non-judgmental about mistakes (everyone makes them)\n- Clear and specific with instructions\n- Celebrate successes when issues are resolved\n\n**Command Execution:**\n- Always explain what a command does before running it\n- Offer to run commands but let user decide\n- Show expected output so they know what's normal\n- If a command fails, read the error and adjust approach\n\n**Teaching Approach:**\n- Explain the \"why\" not just the \"how\"\n- Connect actions to concepts\n- Build mental models of how Git/GitHub work\n- Empower users to solve similar issues themselves\n\n**Safety:**\n- Warn before destructive operations (force push, filter-branch)\n- Recommend backups when rewriting history\n- Double-check before rotating credentials\n- Verify commands in safe environments first\n\n**Follow-through:**\n- Don't leave users hanging at any step\n- Verify each step completed before moving to next\n- Offer alternatives if primary solution doesn't work\n- Ensure problem is fully resolved, not just partially\n\n## Quick Reference Commands\n\nFor easy copy-paste during troubleshooting:\n\n**Status Checks:**\n```bash\ngit status\ngit remote -v\ngit branch -a\ngit log --oneline -5\n```\n\n**Common Fixes:**\n```bash\n# Undo last commit (keep changes)\ngit reset --soft HEAD~1\n\n# Discard all local changes\ngit reset --hard HEAD\n\n# View what will be pushed\ngit diff origin/main main\n\n# Update remote tracking\ngit fetch --prune\n\n# Sync fork with upstream\ngit fetch upstream\ngit merge upstream/main\n```\n\n**Config:**\n```bash\n# Set username\ngit config --global user.name \"Your Name\"\n\n# Set email\ngit config --global user.email \"your.email@example.com\"\n\n# View all config\ngit config --list\n```\n\nThis comprehensive skill should handle the vast majority of GitHub issues faced by new users with accuracy and clarity.\n",
        "skills/linkedin-post-generator/.claude-plugin/plugin.json": "{\n  \"name\": \"linkedin-post-generator\",\n  \"description\": \"LinkedIn post generator for professional content creation and engagement\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Ali Jilani\"\n  },\n  \"category\": \"productivity\",\n  \"keywords\": [\"linkedin\", \"social-media\", \"content\", \"professional\", \"networking\"]\n}\n",
        "skills/linkedin-post-generator/.claude/skills/linkedin-post-maker/BEST-PRACTICES.md": "# LinkedIn Post Best Practices Reference\n\n## Optimal Post Length\n- **Character sweet spot:** 1,200-1,500 characters\n- **Engagement drop-off:** After 2,000 characters\n- **Short posts (under 500 chars):** Good for quick insights, quotes\n- **Long posts (1,000+ chars):** Better for stories, tutorials, thought leadership\n\n## Best Posting Times\n- **Tuesday-Thursday:** 8-10 AM, 12 PM, 5-6 PM (local time)\n- **Avoid:** Weekends and Monday mornings\n- **Pro tip:** Test different times and track engagement\n\n## Hook Formulas That Work\n\n### Pattern 1: Surprising Statement\n```\n\"The best code I ever wrote was code I deleted.\"\n\"I got rejected by 47 companies before landing my dream job.\"\n```\n\n### Pattern 2: Relatable Pain Point\n```\n\"You spend 3 hours debugging. The issue? A missing semicolon.\"\n\"Your calendar is full but you feel unproductive. Here's why:\"\n```\n\n### Pattern 3: Bold Question\n```\n\"What if everything you know about productivity is wrong?\"\n\"Are you building features or solving problems?\"\n```\n\n### Pattern 4: Numbered Promise\n```\n\"5 mistakes that cost me $100K in my first startup.\"\n\"3 things I wish I knew before becoming a manager.\"\n```\n\n### Pattern 5: Contrarian Take\n```\n\"Unpopular opinion: Passion is overrated.\"\n\"Hot take: Your MVP is too complicated.\"\n```\n\n## Content Types That Perform Well\n\n### 1. Personal Stories (Highest Engagement)\n- Career transitions\n- Lessons from failures\n- Breakthrough moments\n- Behind-the-scenes experiences\n\n### 2. Actionable Lists\n- \"X ways to...\"\n- \"X mistakes to avoid...\"\n- \"X tools that...\"\n- \"X lessons learned...\"\n\n### 3. Frameworks and Mental Models\n- Decision-making frameworks\n- Problem-solving approaches\n- Career development models\n- Productivity systems\n\n### 4. Industry Insights\n- Trend analysis\n- Market observations\n- Tool comparisons\n- Best practice guides\n\n### 5. Thought Leadership\n- Contrarian perspectives\n- Future predictions\n- Industry commentary\n- Philosophy on work\n\n## Formatting Techniques\n\n### Use Visual Breaks\n```\nShort sentence.\n\nBlank line creates breathing room.\n\nAnother short thought.\n```\n\n### Leverage Symbols\n-  Arrow for points\n-  Checkmark for completions\n-  Bullet for lists\n-  Em dash for emphasis\n- \" \" Quotes for highlighting\n\n### Create Scannable Structure\n```\n**Bold for key phrases**\n\n Bullet points for lists\n\nNumbers for sequences:\n1. First point\n2. Second point\n3. Third point\n\nCAPS for EMPHASIS (use sparingly)\n```\n\n## Hashtag Strategy\n\n### The 3-5 Rule\n- **2 popular hashtags** (100K+ posts) - #Marketing #AI\n- **2-3 medium hashtags** (10K-100K) - #ContentStrategy #TechLeadership\n- **0-1 niche hashtag** (under 10K) - #B2BSaaS #DevOpsTools\n\n### Placement Options\n1. **End of post** (recommended - doesn't break reading flow)\n2. **In comments** (keeps post cleaner)\n3. **Integrated in text** (use sparingly)\n\n### Find Good Hashtags\n- Check what industry leaders use\n- Look at competitors' popular posts\n- Use LinkedIn's hashtag suggestions\n- Track which tags drive your engagement\n\n## Call-to-Action Examples\n\n### For Engagement\n- \"What's your experience with this?\"\n- \"Agree or disagree?\"\n- \"What would you add to this list?\"\n- \"Have you faced this challenge?\"\n\n### For Discussion\n- \"What's your biggest challenge with [topic]?\"\n- \"How do you handle this in your role?\"\n- \"What's worked for you?\"\n\n### For Sharing\n- \"Tag someone who needs to see this\"\n- \"Share if you found this helpful\"\n- \"Pass this along to your team\"\n\n### For Connection\n- \"DM me if you want to discuss this further\"\n- \"Connect if you're interested in [topic]\"\n- \"Let's continue this conversation\"\n\n## Engagement Boosters\n\n### Reply to Comments Quickly\n- First hour is critical\n- Ask follow-up questions\n- Show genuine interest\n- Keep conversation going\n\n### Tag Strategically\n- Mention people you quote\n- Tag companies when relevant\n- Don't over-tag (max 2-3)\n\n### Post Consistently\n- Find sustainable cadence (2-3x per week is good)\n- Quality > Quantity\n- Batch create content\n\n### Repurpose Top Performers\n- Reframe popular posts after 3-6 months\n- Update with new insights\n- Different angle on same topic\n\n## Common Engagement Killers\n\n **Too corporate/salesy**\n- Sounds like a press release\n- Heavy product promotion\n- No personal voice\n\n **Wall of text**\n- No paragraph breaks\n- Hard to scan\n- Readers scroll past\n\n **Vague or generic**\n- \"Success takes hard work\" (no insight)\n- Motivational platitudes\n- No specific value\n\n **No clear point**\n- Rambling thoughts\n- Multiple unrelated ideas\n- Missing takeaway\n\n **Clickbait without delivery**\n- Promising what you don't deliver\n- Misleading hooks\n- Disappointing content\n\n## Platform Algorithm Factors\n\n### LinkedIn Favors:\n Native content (not external links in post)\n Conversation in comments\n Quick engagement (first 60 minutes)\n Dwell time (how long people read)\n Shares and reshares\n\n### LinkedIn Penalizes:\n External links in post body (put in comments instead)\n Spammy behavior\n Very short posts with just links\n Excessive tagging\n\n## Content Calendar Ideas\n\n### Weekly Themes\n- **Monday:** Motivational/Start of week\n- **Tuesday:** Tips & How-to\n- **Wednesday:** Industry insights\n- **Thursday:** Personal story/lessons\n- **Friday:** Recap/reflection\n\n### Monthly Themes\n- Week 1: Personal development\n- Week 2: Industry trends\n- Week 3: Tools & resources\n- Week 4: Case studies/examples\n\n## Measuring Success\n\n### Key Metrics\n- **Engagement rate:** (Likes + Comments + Shares) / Impressions\n- **Comment quality:** Depth of discussion\n- **Profile views:** Are people checking you out?\n- **Connection requests:** Growing your network?\n\n### What Good Looks Like\n- 3-5% engagement rate is solid\n- 5-10 meaningful comments\n- Several profile views\n- 1-3 connection requests\n\n### Iterate Based on Data\n- Track what topics resonate\n- Note which formats work\n- Test different times\n- Double down on what works\n",
        "skills/linkedin-post-generator/.claude/skills/linkedin-post-maker/POST-TEMPLATE.md": "# LinkedIn Post Template\n\nUse this template as a quick reference when generating posts.\n\n## Basic Template Structure\n\n```markdown\n[HOOK - 1-2 punchy lines that grab attention]\n\n[Blank line]\n\n[CONTEXT - Set up the story/insight]\n\n[Blank line]\n\n[VALUE/BODY - Main content with specific insights]\n\nUse formatting:\n Bullet points for clarity\n Short paragraphs (1-3 lines)\n White space for readability\n\n[Blank line]\n\n[KEY TAKEAWAY - What's the main point?]\n\n[Blank line]\n\n[CALL TO ACTION - Question or request for engagement]\n\n[Blank line]\n\n#Hashtag1 #Hashtag2 #Hashtag3 #Hashtag4 #Hashtag5\n```\n\n## Template Variations\n\n### Personal Story Template\n\n```\n[Opening: The moment that changed everything]\n\n[What happened: Brief context]\n\n[The struggle or challenge:]\n Problem 1\n Problem 2\n Problem 3\n\n[What changed: The turning point]\n\n[Lessons learned:]\n1. First lesson\n2. Second lesson\n3. Third lesson\n\n[Reflective closing statement]\n\n[CTA: Ask readers about their experience]\n\n#RelevantHashtags\n```\n\n### Educational/Tips Template\n\n```\n[Hook: Common mistake or surprising fact]\n\n[Why it matters]\n\nHere's what actually works:\n\n**Tip 1: [Title]**\n Explanation\n Why it works\n\n**Tip 2: [Title]**\n Explanation\n Why it works\n\n**Tip 3: [Title]**\n Explanation\n Why it works\n\n[Summary statement]\n\n[CTA: Ask which tip they'll try first]\n\n#RelevantHashtags\n```\n\n### Thought Leadership Template\n\n```\n[Controversial or contrarian statement]\n\nEveryone thinks [common belief].\n\nBut here's what they're missing:\n\n[Your perspective]\n\nThe data shows:\n Point 1\n Point 2\n Point 3\n\nThis is why [industry/practice] needs to change:\n\n[Your argument with examples]\n\n**The future isn't [old way].**\n**It's [new way].**\n\n[CTA: Ask for agreement/disagreement]\n\n#RelevantHashtags\n```\n\n### List/Framework Template\n\n```\n[Hook: Promise of value]\n\nAfter [X years/experiences], here are the [number] [things]:\n\n1 [First item]\n[Brief explanation]\n\n2 [Second item]\n[Brief explanation]\n\n3 [Third item]\n[Brief explanation]\n\n4 [Fourth item]\n[Brief explanation]\n\n5 [Fifth item]\n[Brief explanation]\n\n[Closing insight]\n\n[CTA: Ask which resonated most]\n\n#RelevantHashtags\n```\n\n### Problem-Solution Template\n\n```\n[Hook: Relatable problem]\n\nSound familiar?\n\n[Describe the pain point in detail]\n\nI used to struggle with this too.\n\nHere's what changed everything:\n\n**The Problem:**\n[Root cause explanation]\n\n**The Solution:**\n[Your framework/approach]\n\n**How to implement it:**\n Step 1\n Step 2\n Step 3\n\n**The result:**\n[What changes/improves]\n\n[CTA: Ask about their approach]\n\n#RelevantHashtags\n```\n\n### Short & Punchy Template\n\n```\n[One powerful statement]\n\n[Supporting line]\n\n[Key insight or question]\n\n[CTA]\n\n#Hashtags\n```\n\nExample:\n```\nThe best leaders I know ask more questions than they answer.\n\nBecause giving all the answers creates dependence.\nAsking the right questions creates leaders.\n\nWhat question has had the biggest impact on your growth?\n\n#Leadership #Management #GrowthMindset\n```\n\n## File Metadata Template\n\nWhen saving posts, use this structure:\n\n```markdown\n# LinkedIn Post: [Descriptive Title]\n\n**Date Created:** YYYY-MM-DD\n**Target Audience:** [specific audience]\n**Tone:** [Professional/Casual/Inspirational/Educational]\n**Length:** [word count] words\n**Topic:** [main topic]\n\n---\n\n## Post Content\n\n[The actual post goes here]\n\n---\n\n## Metadata\n- **Main Topic:** [topic]\n- **Key Themes:** [theme1, theme2, theme3]\n- **Hashtags:** #hashtag1 #hashtag2 #hashtag3 #hashtag4 #hashtag5\n- **Estimated Reading Time:** [X] seconds\n- **Target Engagement:** [Comments/Shares/Discussion]\n\n## Performance Notes\n[Add notes after posting about what worked/didn't work]\n```\n\n## Character Count Guidelines\n\n- **Hook:** 50-100 characters\n- **Context/Setup:** 100-200 characters\n- **Main Content:** 400-800 characters\n- **Call to Action:** 50-100 characters\n- **Hashtags:** 50-150 characters\n- **Total Sweet Spot:** 1,200-1,500 characters\n\n## Quick Reference: Power Words\n\n### For Hooks\n- Surprising, Unexpected, Shocking\n- Mistake, Failure, Lesson\n- Secret, Truth, Reality\n- Stop, Avoid, Never\n- Better, Best, Worst\n\n### For Authority\n- Research shows, Data proves\n- In my experience, I've learned\n- Studies indicate, Experts agree\n- The reality is, The truth is\n\n### For Engagement\n- What's your, How do you\n- Agree or disagree, Thoughts?\n- Share your, Tell me about\n- Have you ever, Do you\n\n## Emoji Guide (Use Sparingly)\n\n### Professional Context\n-  (arrow for points)\n-  (checkmark for completed items)\n-  (charts/data)\n-  (ideas/insights)\n-  (goals/targets)\n\n### Numbered Lists\n- 1 2 3 (for numbered points)\n\n### Avoid in Professional Posts\n-    (too casual for most B2B content)\n- Excessive emojis (looks unprofessional)\n",
        "skills/linkedin-post-generator/.claude/skills/linkedin-post-maker/README.md": "# LinkedIn Post Maker Skill\n\n## Overview\nThe `linkedin-post-maker` skill automatically generates high-quality, engaging LinkedIn posts on any topic while maintaining professional standards and best practices.\n\n## Features\n\n- **Smart Clarification**: Asks targeted questions when topic details are vague or missing\n- **Multi-Format Support**: Creates different post types (personal stories, educational content, thought leadership, etc.)\n- **Professional Standards**: Follows LinkedIn best practices for formatting, engagement, and readability\n- **Automatic Organization**: Saves all posts to `LinkedIn-Posts/` directory with proper naming\n- **Comprehensive Templates**: Includes reference materials and templates for consistent quality\n\n## How It Works\n\n### Automatic Activation\nClaude will automatically use this skill when you ask to:\n- \"Create a LinkedIn post about...\"\n- \"Write a social media post for LinkedIn on...\"\n- \"Generate LinkedIn content about...\"\n- \"Help me write a professional post about...\"\n\n### The Workflow\n\n1. **Information Gathering**: If your topic is vague or missing key details, Claude will ask clarifying questions about:\n   - Target audience\n   - Desired tone (professional, casual, inspirational, etc.)\n   - Post length\n   - Key message and takeaways\n   - Call-to-action preferences\n\n2. **Content Generation**: Creates a well-structured post with:\n   - Attention-grabbing hook\n   - Valuable content body\n   - Clear takeaways\n   - Engaging call-to-action\n   - Relevant hashtags (3-5)\n\n3. **File Organization**: Automatically saves the post to:\n   ```\n   LinkedIn-Posts/YYYY-MM-DD_topic-keywords.md\n   ```\n\n## File Structure\n\n```\n.claude/skills/linkedin-post-maker/\n SKILL.md              # Main skill definition and instructions\n BEST-PRACTICES.md     # LinkedIn best practices reference\n POST-TEMPLATE.md      # Quick reference templates\n README.md            # This file\n```\n\n## Usage Examples\n\n### Simple Request\n```\nYou: \"Create a LinkedIn post about remote work benefits\"\n\nClaude: Will ask 1-2 clarifying questions about audience and tone,\n        then generate a tailored post\n```\n\n### Detailed Request (No Questions Needed)\n```\nYou: \"Write a casual LinkedIn post for software developers about the\n     importance of code reviews. Make it around 400 words with a\n     conversational tone and ask them to share their experiences.\"\n\nClaude: Proceeds directly to generate the post based on your specifications\n```\n\n### Vague Request (More Clarification)\n```\nYou: \"Make a post about technology\"\n\nClaude: Will ask about:\n        - Specific technology topic/angle\n        - Target audience\n        - Main message\n        Then generates the post\n```\n\n## Post Quality Standards\n\nEvery generated post includes:\n\n **Hook** - Attention-grabbing opening that stops scrolling\n **Value** - Clear insights, lessons, or actionable information\n **Structure** - Short paragraphs, visual breaks, scannable format\n **Engagement** - Call-to-action that encourages comments/discussion\n **Discoverability** - 3-5 relevant, strategic hashtags\n **Professional formatting** - Proper use of white space, bullets, emphasis\n\n## Supporting Resources\n\n### BEST-PRACTICES.md\nComprehensive reference including:\n- Hook formulas that work\n- Content types that perform well\n- Hashtag strategy (3-5 rule)\n- Engagement optimization tips\n- Common mistakes to avoid\n- Platform algorithm factors\n\n### POST-TEMPLATE.md\nQuick reference templates for:\n- Personal stories\n- Educational/tips posts\n- Thought leadership\n- List/framework posts\n- Problem-solution format\n- Short & punchy posts\n\n## Post Output Format\n\nEach saved post includes:\n\n```markdown\n# LinkedIn Post: [Topic Title]\n\n**Date Created:** YYYY-MM-DD\n**Target Audience:** [audience]\n**Tone:** [style]\n**Length:** [word count] words\n\n---\n\n## Post Content\n\n[The actual LinkedIn post ready to copy and paste]\n\n---\n\n## Metadata\n- Main Topic: [topic]\n- Key Themes: [themes]\n- Hashtags: [tags]\n- Estimated Reading Time: [time]\n```\n\n## Customization\n\nYou can customize posts by specifying:\n- **Tone**: Professional, Casual, Inspirational, Educational, Story-driven\n- **Length**: Short (150-300 words), Medium (300-600), Long (600-1000)\n- **Audience**: Developers, Entrepreneurs, Managers, Industry-specific\n- **Style**: Personal story, How-to, Thought leadership, List format\n- **CTA Type**: Question, Share request, Discussion prompt, Connection invite\n\n## Tips for Best Results\n\n1. **Be Specific**: The more context you provide, the better the output\n2. **Know Your Audience**: Mention who you're targeting for tailored content\n3. **Define Success**: Tell Claude what engagement you want (comments, shares, etc.)\n4. **Iterate**: Ask for revisions if the first version isn't quite right\n5. **Add Personal Touch**: Request personal anecdotes or specific examples to include\n\n## Examples of Good Prompts\n\n \"Create a LinkedIn post for CTOs about AI adoption challenges, professional tone, around 500 words\"\n\n \"Write an inspirational post about overcoming imposter syndrome for junior developers\"\n\n \"Generate a thought leadership post on the future of remote work, contrarian perspective\"\n\n \"Make a short, casual post about productivity tips for busy founders with a question CTA\"\n\n## Troubleshooting\n\n**Claude isn't using the skill:**\n- Make sure you mention \"LinkedIn post\" or \"social media content\"\n- Be explicit: \"Use the linkedin-post-maker skill to...\"\n\n**Too many questions:**\n- Provide more details upfront about audience, tone, and topic specifics\n\n**Post doesn't match expectations:**\n- Ask for specific revisions: \"Make it more casual\" or \"Add a personal story\"\n- Request a different template: \"Rewrite using the list format\"\n\n## Version History\n- v1.0.0 (2025-12-21): Initial release with core functionality\n\n## Feedback and Improvements\n\nAs you use this skill, note what works well and what could be improved. The skill can be updated with:\n- Additional templates\n- New hook formulas\n- Industry-specific guidance\n- More examples\n- Enhanced clarification questions\n\n---\n\n**Ready to create your first LinkedIn post?** Just ask Claude to create a LinkedIn post on your topic!\n",
        "skills/linkedin-post-generator/.claude/skills/linkedin-post-maker/SKILL.md": "---\nname: linkedin-post-maker\ndescription: Creates engaging LinkedIn posts on any topic with proper formatting, hooks, and CTAs. Use when the user asks to create, generate, or write a LinkedIn post, social media content for LinkedIn, or professional social content. Asks clarifying questions to ensure quality and relevance.\nallowed-tools: Read, Write, AskUserQuestion, Grep, Glob\n---\n\n# LinkedIn Post Maker\n\n## Purpose\nThis skill generates high-quality, engaging LinkedIn posts on any topic while maintaining professional standards and best practices for the platform.\n\n## Supporting Resources\n- [BEST-PRACTICES.md](BEST-PRACTICES.md) - Comprehensive LinkedIn best practices, hook formulas, and engagement strategies\n- [POST-TEMPLATE.md](POST-TEMPLATE.md) - Quick reference templates for different post types\n\n## Instructions\n\n### 1. Gather Information and Clarify Requirements\n\nBefore generating the post, assess if you have enough information. If any of the following are unclear or missing, use AskUserQuestion to clarify:\n\n**Required Clarifications:**\n- **Topic Depth**: If the topic is vague or too broad, ask for specific angles, key points, or main message\n- **Target Audience**: Who is this post for? (e.g., software developers, entrepreneurs, HR professionals, general business audience)\n- **Post Tone**: What style should the post have?\n  - Professional/Corporate\n  - Casual/Conversational\n  - Thought Leadership\n  - Educational/Tutorial\n  - Inspirational/Motivational\n  - Story-driven/Personal\n- **Post Length**: What length is preferred?\n  - Short (150-300 words) - Quick insights\n  - Medium (300-600 words) - Standard engagement\n  - Long (600-1000 words) - In-depth analysis\n- **Key Message**: What's the main takeaway readers should have?\n- **Call to Action**: What should readers do after reading? (e.g., comment, share experience, visit link, engage in discussion)\n\n**Use AskUserQuestion strategically:**\n- If topic is clear and specific: Ask 1-2 questions maximum (audience + tone)\n- If topic is vague: Ask about topic specifics, target audience, and desired message\n- If user provides detailed context: Proceed without questions\n\n### 2. LinkedIn Post Structure\n\nEvery post should follow this proven structure:\n\n**A. Hook (First 1-2 lines)**\n- Grab attention immediately\n- Make readers want to click \"see more\"\n- Use patterns like:\n  - Surprising statement\n  - Relatable pain point\n  - Bold claim or question\n  - Personal story opening\n  - Contrarian viewpoint\n\n**B. Value/Body (Main content)**\n- Deliver on the hook's promise\n- Use short paragraphs (1-3 lines each)\n- Include white space for readability\n- Use formatting:\n  -  Bullet points with arrows\n  -  Checkmarks for lists\n  - Numbers for sequences\n  - Line breaks between sections\n\n**C. Key Takeaways (Optional for longer posts)**\n- Summarize main points\n- Use bullet format\n- Make them actionable\n\n**D. Call to Action**\n- Ask engaging question\n- Invite discussion\n- Request shares or comments\n- Encourage connection\n\n**E. Hashtags (3-5 relevant tags)**\n- Mix of popular and niche hashtags\n- Industry-specific terms\n- Topic-related keywords\n\n### 3. Writing Best Practices\n\n**Tone and Style:**\n- Write conversationally (use \"you\", \"I\", \"we\")\n- Keep sentences short and punchy\n- Avoid jargon unless audience-appropriate\n- Use active voice\n- Show personality and authenticity\n\n**Formatting:**\n- Short paragraphs (1-3 lines)\n- Blank lines between paragraphs\n- Use emojis sparingly (1-3 max, only if tone-appropriate)\n- Bold key phrases with asterisks: **like this**\n- Create visual hierarchy\n\n**Engagement Optimization:**\n- Start with a hook that stops scrolling\n- Provide genuine value or insight\n- Make it relatable to target audience\n- End with clear CTA\n- Spark conversation in comments\n\n### 4. Generate and Save the Post\n\n**File Naming Convention:**\n```\nLinkedIn-Posts/YYYY-MM-DD_topic-keywords.md\n```\n\nExample: `LinkedIn-Posts/2025-12-21_ai-product-development.md`\n\n**File Content Structure:**\n```markdown\n# LinkedIn Post: [Topic Title]\n\n**Date Created:** YYYY-MM-DD\n**Target Audience:** [audience]\n**Tone:** [tone style]\n**Length:** [word count] words\n\n---\n\n## Post Content\n\n[The actual LinkedIn post goes here with proper formatting]\n\n---\n\n## Metadata\n- **Main Topic:** [topic]\n- **Key Themes:** [theme1, theme2, theme3]\n- **Hashtags:** #hashtag1 #hashtag2 #hashtag3\n- **Estimated Reading Time:** [X] seconds\n```\n\n### 5. Quality Checklist\n\nBefore finalizing, ensure:\n- [ ] Hook is compelling and makes reader want more\n- [ ] Content delivers value (insight, education, inspiration, or entertainment)\n- [ ] Paragraphs are short and scannable\n- [ ] Message is clear and focused\n- [ ] Tone matches audience and purpose\n- [ ] CTA encourages engagement\n- [ ] 3-5 relevant hashtags included\n- [ ] Proper formatting with white space\n- [ ] No spelling or grammar errors\n- [ ] File saved to LinkedIn-Posts directory\n\n## Examples\n\n### Example 1: Tech Topic - Short Casual Post\n\n**User Request:** \"Create a LinkedIn post about clean code\"\n\n**Clarifying Questions:** (if needed)\n- Target audience: software developers, tech leads\n- Tone: casual/conversational\n- Length: short\n\n**Generated Post:**\n```\nYour code might work perfectly.\n\nBut that doesn't mean it's good code.\n\nHere's the difference:\n\nWorking code = solves the problem\nGood code = solves the problem AND is easy to maintain\n\n3 signs your code needs cleanup:\n\n You can't explain what it does in 1 sentence\n You need comments to understand your own logic\n Making a small change breaks 3 other things\n\nClean code isn't about being fancy.\nIt's about being kind to your future self.\n\nWhat's your #1 rule for writing clean code?\n\n#SoftwareDevelopment #CleanCode #Programming #CodingBestPractices\n```\n\n### Example 2: Business Topic - Professional Tone\n\n**User Request:** \"Write about leadership and delegation\"\n\n**Generated Post:**\n```\nThe hardest lesson I learned as a leader:\n\nDelegation isn't about offloading work.\nIt's about multiplying impact.\n\nEarly in my career, I thought being a good leader meant:\n- Being the smartest person in the room\n- Having all the answers\n- Doing the important work myself\n\nI was wrong.\n\n**Real delegation means:**\n\n Trusting your team with meaningful work\n Providing context, not just tasks\n Creating space for others to grow\n Being okay with different approaches\n\nThe result?\nYour team becomes more capable. You create future leaders. And you free yourself to focus on what only you can do.\n\n**The question isn't: \"Can I do this better?\"\nThe question is: \"Who can I develop by giving this opportunity?\"**\n\nWhat's been your biggest challenge with delegation?\n\n#Leadership #Management #TeamDevelopment #GrowthMindset #ProfessionalGrowth\n```\n\n### Example 3: Personal Story - Inspirational\n\n**User Request:** \"Post about overcoming failure in startup\"\n\n**Generated Post:**\n```\nTwo years ago, I shut down my startup.\n\n$200K invested. 18 months of work. Zero revenue.\n\nI felt like a complete failure.\n\nToday? I'm grateful it happened.\n\nHere's what that \"failure\" taught me:\n\n**About Product:**\n Building what users want > building what you think is cool\n Talk to customers BEFORE writing code\n MVPs should be embarrassingly simple\n\n**About Business:**\n Revenue isn't optionalit's oxygen\n Funding doesn't validate your idea\n Burn rate kills dreams faster than bad products\n\n**About Myself:**\n I'm more resilient than I thought\n Failure is data, not identity\n Every setback teaches something valuable\n\nThe startup died. But I didn't.\n\nNow I'm building againsmarter, humbler, and more focused.\n\nSometimes you need to fail at the wrong thing to succeed at the right thing.\n\nHave you had a \"failure\" that became your best teacher?\n\n#Entrepreneurship #StartupJourney #FailureIsPartOfSuccess #GrowthMindset #StartupLessons\n```\n\n## Advanced Tips\n\n### When Topic Requires Research:\n1. Use Grep/Glob to search existing posts for similar topics\n2. Check for previous content to avoid repetition\n3. Build on previous insights\n\n### Hashtag Strategy:\n- **Popular (100K+ posts):** 1-2 max (#Leadership, #Technology)\n- **Medium (10K-100K):** 2-3 tags (#CleanCode, #ProductManagement)\n- **Niche (Under 10K):** 1-2 tags (specific to topic)\n\n### Engagement Patterns:\n- Posts with questions get 50% more comments\n- Posts with personal stories get higher engagement\n- Lists and frameworks are highly shareable\n- Contrarian takes spark discussion\n\n## Common Mistakes to Avoid\n\n Starting with generic statements (\"In today's fast-paced world...\")\n Writing long paragraphs (makes readers scroll away)\n Using too much corporate jargon\n No clear takeaway or CTA\n Overusing hashtags (more than 5)\n Writing without considering target audience\n Being too salesy or promotional\n\n## Version History\n- v1.0.0 (2025-12-21): Initial release with core functionality\n",
        "skills/linkedin-post-generator/LinkedIn-Posts/2025-12-21_ai-agents-changing-work.md": "# LinkedIn Post: AI Agents and the Changing Work Horizon\n\n**Date Created:** 2025-12-21\n**Target Audience:** Business professionals, tech leaders, knowledge workers\n**Tone:** Professional\n**Length:** 186 words (928 characters)\n**Topic:** AI Agents transforming the workplace\n\n---\n\n## Post Content\n\nAI agents aren't replacing usthey're unlocking our potential.\n\nThe narrative around AI often focuses on fear. But here's what's actually happening in forward-thinking organizations:\n\n**What used to take hours now takes minutes.**\n\nAI agents are handling the repetitive work that drains our energy:\n Scheduling meetings across time zones\n Summarizing lengthy reports\n Drafting initial responses to routine queries\n Analyzing data patterns at scale\n\nThe result? We're getting back what matters most: time to think, create, and solve complex problems.\n\n**This isn't about working lessit's about working smarter.**\n\nTeams using AI agents report higher job satisfaction because they're finally doing the work they were hired to do. The strategic thinking. The creative problem-solving. The human connections that drive real innovation.\n\nThe future of work isn't humans vs. machines.\n\nIt's humans empowered by intelligent tools, focusing on what we do best.\n\nHow is AI changing your daily work for the better?\n\n#ArtificialIntelligence #FutureOfWork #AIAgents #DigitalTransformation #WorkplaceInnovation\n\n---\n\n## Metadata\n- **Main Topic:** AI Agents in the workplace\n- **Key Themes:** Productivity, automation, human potential, positive transformation\n- **Hashtags:** #ArtificialIntelligence #FutureOfWork #AIAgents #DigitalTransformation #WorkplaceInnovation\n- **Estimated Reading Time:** 45 seconds\n- **Target Engagement:** Comments about personal AI experiences\n- **Character Count:** 928 characters (under 950 limit)\n\n## Performance Notes\n- Hook emphasizes opportunity over threat\n- Uses concrete examples of time-saving tasks\n- Balances optimism with practical reality\n- CTA invites positive sharing of experiences\n",
        "skills/pytest-python/.claude-plugin/plugin.json": "{\n  \"name\": \"pytest-python\",\n  \"description\": \"Python testing helper with pytest patterns, fixtures, and best practices\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Ali Jilani\"\n  },\n  \"category\": \"development\",\n  \"keywords\": [\"pytest\", \"python\", \"testing\", \"unit-tests\", \"fixtures\"]\n}\n",
        "skills/pytest-python/SKILL.md": "---\nname: pytest-python\ndescription: |\n  Comprehensive pytest testing skill for Python projects. Write efficient, maintainable tests with fixtures, parametrization, markers, mocking, and assertions. Use when: (1) Writing new tests for Python code, (2) Setting up pytest in a project, (3) Creating fixtures for test dependencies, (4) Parametrizing tests for multiple inputs, (5) Mocking/patching with monkeypatch, (6) Debugging test failures, (7) Organizing test suites with markers, (8) Any Python testing task.\n---\n\n# Pytest Testing for Python\n\n## Quick Reference\n\n### Test Discovery\n\nPytest auto-discovers tests matching:\n- Files: `test_*.py` or `*_test.py`\n- Functions: `test_*` prefix\n- Classes: `Test*` prefix (no `__init__`)\n\n### Running Tests\n\n```bash\npytest                           # Run all tests\npytest test_mod.py               # Single module\npytest tests/                    # Directory\npytest -k \"name\"                 # By keyword\npytest -m slow                   # By marker\npytest test_mod.py::test_func    # Specific test\npytest --durations=10            # Show slowest tests\n```\n\n## Fixtures\n\nFixtures provide reusable test dependencies.\n\n### Basic Fixture\n\n```python\nimport pytest\n\n@pytest.fixture\ndef sample_data():\n    return {\"key\": \"value\"}\n\ndef test_example(sample_data):\n    assert sample_data[\"key\"] == \"value\"\n```\n\n### Fixture Scopes\n\n```python\n@pytest.fixture(scope=\"function\")  # Default: per test\n@pytest.fixture(scope=\"class\")     # Per test class\n@pytest.fixture(scope=\"module\")    # Per module\n@pytest.fixture(scope=\"session\")   # Entire session\n```\n\n### Fixture with Teardown (yield)\n\n```python\n@pytest.fixture\ndef db_connection():\n    conn = create_connection()\n    yield conn\n    conn.close()  # Cleanup after test\n```\n\n### Factory Fixture\n\n```python\n@pytest.fixture\ndef make_user():\n    def _make_user(name, role=\"user\"):\n        return {\"name\": name, \"role\": role}\n    return _make_user\n\ndef test_users(make_user):\n    admin = make_user(\"Alice\", role=\"admin\")\n    user = make_user(\"Bob\")\n```\n\n### Parametrized Fixture\n\n```python\n@pytest.fixture(params=[\"mysql\", \"postgres\", \"sqlite\"])\ndef database(request):\n    return create_db(request.param)\n```\n\n## Parametrization\n\nRun tests with multiple inputs.\n\n```python\n@pytest.mark.parametrize(\"input,expected\", [\n    (1, 2),\n    (2, 4),\n    (3, 6),\n])\ndef test_double(input, expected):\n    assert input * 2 == expected\n```\n\n### Multiple Parameters (Combinations)\n\n```python\n@pytest.mark.parametrize(\"x\", [1, 2])\n@pytest.mark.parametrize(\"y\", [10, 20])\ndef test_multiply(x, y):  # Runs 4 combinations\n    assert x * y > 0\n```\n\n### With Expected Failures\n\n```python\n@pytest.mark.parametrize(\"input,expected\", [\n    (1, 1),\n    pytest.param(0, 1, marks=pytest.mark.xfail),\n])\ndef test_factorial(input, expected):\n    assert factorial(input) == expected\n```\n\n## Markers\n\n### Built-in Markers\n\n```python\n@pytest.mark.skip(reason=\"Not implemented\")\ndef test_feature(): ...\n\n@pytest.mark.skipif(sys.platform == \"win32\", reason=\"Unix only\")\ndef test_unix(): ...\n\n@pytest.mark.xfail(reason=\"Known bug\")\ndef test_buggy(): ...\n```\n\n### Custom Markers\n\nRegister in `pytest.ini` or `pyproject.toml`:\n\n```ini\n[pytest]\nmarkers =\n    slow: marks tests as slow\n    integration: integration tests\n```\n\n```python\n@pytest.mark.slow\ndef test_slow_operation(): ...\n```\n\nRun: `pytest -m slow` or `pytest -m \"not slow\"`\n\n## Assertions\n\n### Basic Assertions\n\n```python\nassert value == expected\nassert value != other\nassert value is None\nassert value is not None\nassert value in collection\nassert isinstance(obj, MyClass)\n```\n\n### Floating Point\n\n```python\nassert 0.1 + 0.2 == pytest.approx(0.3)\nassert result == pytest.approx(expected, rel=1e-3)\n```\n\n### Exception Testing\n\n```python\ndef test_raises():\n    with pytest.raises(ValueError):\n        int(\"invalid\")\n\ndef test_raises_with_match():\n    with pytest.raises(ValueError, match=r\"invalid.*\"):\n        raise ValueError(\"invalid input\")\n\ndef test_raises_inspect():\n    with pytest.raises(ValueError) as exc_info:\n        raise ValueError(\"test error\")\n    assert \"test\" in str(exc_info.value)\n```\n\n## Monkeypatch (Mocking)\n\n### Patching Functions\n\n```python\ndef test_api_call(monkeypatch):\n    def mock_get(*args, **kwargs):\n        return {\"status\": \"ok\"}\n\n    monkeypatch.setattr(\"mymodule.api.get\", mock_get)\n    result = mymodule.fetch_data()\n    assert result[\"status\"] == \"ok\"\n```\n\n### Environment Variables\n\n```python\ndef test_with_env(monkeypatch):\n    monkeypatch.setenv(\"API_KEY\", \"test-key\")\n    assert os.environ[\"API_KEY\"] == \"test-key\"\n\ndef test_without_env(monkeypatch):\n    monkeypatch.delenv(\"API_KEY\", raising=False)\n```\n\n### Dictionary Values\n\n```python\ndef test_config(monkeypatch):\n    monkeypatch.setitem(app.config, \"DEBUG\", True)\n```\n\n## Built-in Fixtures\n\n| Fixture | Purpose |\n|---------|---------|\n| `tmp_path` | Temporary directory (pathlib.Path) |\n| `tmp_path_factory` | Session-scoped temp directories |\n| `capsys` | Capture stdout/stderr |\n| `caplog` | Capture log messages |\n| `monkeypatch` | Dynamic patching |\n| `request` | Fixture/test metadata |\n\n### Examples\n\n```python\ndef test_output(capsys):\n    print(\"hello\")\n    captured = capsys.readouterr()\n    assert captured.out == \"hello\\n\"\n\ndef test_logging(caplog):\n    import logging\n    logging.warning(\"test warning\")\n    assert \"test warning\" in caplog.text\n\ndef test_temp_file(tmp_path):\n    file = tmp_path / \"test.txt\"\n    file.write_text(\"content\")\n    assert file.read_text() == \"content\"\n```\n\n## Project Structure\n\nRecommended layout:\n\n```\nproject/\n pyproject.toml\n src/\n    mypackage/\n        __init__.py\n        module.py\n tests/\n     conftest.py      # Shared fixtures\n     test_module.py\n     unit/\n         test_specific.py\n```\n\n### conftest.py\n\nShared fixtures available to all tests in directory:\n\n```python\n# tests/conftest.py\nimport pytest\n\n@pytest.fixture\ndef app():\n    return create_app(testing=True)\n\n@pytest.fixture\ndef client(app):\n    return app.test_client()\n```\n\n## Configuration\n\n### pyproject.toml\n\n```toml\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\npython_files = [\"test_*.py\"]\npython_functions = [\"test_*\"]\naddopts = \"-v --strict-markers\"\nmarkers = [\n    \"slow: marks tests as slow\",\n    \"integration: integration tests\",\n]\nfilterwarnings = [\n    \"ignore::DeprecationWarning\",\n]\n```\n\n## Best Practices\n\n1. **One assertion focus per test** - Test one behavior per function\n2. **Descriptive names** - `test_user_creation_with_invalid_email_raises_error`\n3. **Use fixtures** - Avoid setup duplication\n4. **Isolate tests** - No shared state between tests\n5. **Fast unit tests** - Mark slow tests with `@pytest.mark.slow`\n6. **Parametrize** - Use parametrize over copy-paste tests\n7. **Test edge cases** - Empty inputs, boundaries, errors\n\n## References\n\n- [Fixtures Guide](references/fixtures.md) - Advanced fixture patterns\n- [Patterns Guide](references/patterns.md) - Common testing patterns\n",
        "skills/pytest-python/readme.md": "![Banner](./header.jpg)\r\n\r\n# Pytest-Python Skill \r\n\r\nA comprehensive testing skill for **Claude Code** designed to automate the creation, management, and debugging of production-grade Python test suites using `pytest`.\r\n\r\n---\r\n\r\n##  Overview\r\n\r\nThe `pytest-python` skill enables Claude to act as a specialized QA Engineer within your terminal. It streamlines the Test-Driven Development (TDD) workflow by providing deep integration with the `pytest` ecosystem, allowing for automated test generation, complex mocking, and rapid failure analysis.\r\n\r\n##  Key Features\r\n\r\n* **Efficient Test Generation:** Quickly write maintainable tests for any Python code, following industry best practices.\r\n* **Advanced Mocking:** Seamlessly handle external dependencies using `monkeypatch` and mocking/patching strategies.\r\n* **Parametrization:** Create dry, efficient test suites by testing multiple inputs with a single test function.\r\n* **Fixture Management:** Design and implement robust fixtures for complex test dependencies and setup/teardown logic.\r\n* **Suite Organization:** Use custom markers and structured directories to keep your test suites organized and scalable.\r\n* **Rapid Debugging:** Automated analysis of test failures to identify root causes and suggest immediate fixes.\r\n\r\n##  When to Use\r\n\r\nThis skill should be triggered when you need to:\r\n1.  **Initialize** pytest in a new or existing Python project.\r\n2.  **Write new test cases** for logic-heavy functions or classes.\r\n3.  **Refactor tests** to use parametrization or shared fixtures.\r\n4.  **Isolate tests** by mocking API calls, databases, or file systems.\r\n5.  **Fix broken tests** during a CI/CD pipeline failure.\r\n\r\n##  How to Use with Claude Code\r\n\r\n### Step 1: Trigger the Skill\r\nAsk Claude to perform a testing task using the skill name:\r\n`claude \"Using the pytest-python skill, write unit tests for the user authentication module with 100% coverage.\"`\r\n\r\n### Step 2: Implementation\r\nClaude will:\r\n* Analyze your existing codebase.\r\n* Identify edge cases and potential failure points.\r\n* Generate a `tests/` directory (if not present) with structured test files.\r\n\r\n### Step 3: Run & Debug\r\n`claude \"Run the pytest-python suite and debug any failures in the database connection tests.\"`\r\n\r\n##  Best Practices (Subagent Delegation)\r\n* **Keep Tests Isolated:** Use subagents for heavy test exploration to avoid bloating the main session context.\r\n* **Parallel Execution:** Launch multiple test agents simultaneously to save time, as work done by subagents does not consume the main session's context.\r\n* **Synthesize Results:** Use the synthesis pattern to return only compact summaries of test results to the orchestrator, preventing auto-compaction.\r\n\r\n---\r\n*Created by [alijilani-dev] based on the Pytest-Python Skill definition and Subagent Delegation Strategy.*",
        "skills/pytest-python/references/fixtures.md": "# Advanced Fixture Patterns\n\n## Table of Contents\n\n1. [Fixture Dependencies](#fixture-dependencies)\n2. [Dynamic Fixtures](#dynamic-fixtures)\n3. [Autouse Fixtures](#autouse-fixtures)\n4. [Fixture Finalization](#fixture-finalization)\n5. [Request Object](#request-object)\n6. [Override Patterns](#override-patterns)\n\n## Fixture Dependencies\n\nFixtures can request other fixtures, creating dependency chains:\n\n```python\n@pytest.fixture\ndef database():\n    db = Database()\n    db.connect()\n    yield db\n    db.disconnect()\n\n@pytest.fixture\ndef user_repo(database):\n    return UserRepository(database)\n\n@pytest.fixture\ndef user_service(user_repo):\n    return UserService(user_repo)\n\ndef test_user_creation(user_service):\n    user = user_service.create(\"test@example.com\")\n    assert user.email == \"test@example.com\"\n```\n\n## Dynamic Fixtures\n\n### Using pytest_generate_tests\n\nGenerate test parameters dynamically:\n\n```python\n# conftest.py\ndef pytest_generate_tests(metafunc):\n    if \"db_engine\" in metafunc.fixturenames:\n        engines = [\"sqlite\", \"postgres\"]\n        if metafunc.config.getoption(\"--mysql\"):\n            engines.append(\"mysql\")\n        metafunc.parametrize(\"db_engine\", engines)\n```\n\n### Dynamic Scope\n\nDetermine fixture scope at runtime:\n\n```python\ndef determine_scope(fixture_name, config):\n    if config.getoption(\"--expensive-setup\"):\n        return \"session\"\n    return \"function\"\n\n@pytest.fixture(scope=determine_scope)\ndef expensive_resource():\n    return create_expensive_resource()\n```\n\n## Autouse Fixtures\n\nAutomatically apply fixtures without explicit request:\n\n```python\n@pytest.fixture(autouse=True)\ndef reset_database(database):\n    \"\"\"Clean database before each test.\"\"\"\n    database.truncate_all()\n    yield\n    database.truncate_all()\n\n@pytest.fixture(autouse=True, scope=\"module\")\ndef setup_logging():\n    \"\"\"Configure logging for test module.\"\"\"\n    logging.basicConfig(level=logging.DEBUG)\n```\n\n### Class-Scoped Autouse\n\n```python\nclass TestUserFeatures:\n    @pytest.fixture(autouse=True)\n    def setup_user(self, database):\n        self.user = database.create_user(\"test@example.com\")\n        yield\n        database.delete_user(self.user.id)\n\n    def test_user_name(self):\n        assert self.user.email == \"test@example.com\"\n```\n\n## Fixture Finalization\n\n### Multiple Finalizers\n\n```python\n@pytest.fixture\ndef resource_with_cleanup(request):\n    resources = []\n\n    def create_resource(name):\n        r = Resource(name)\n        resources.append(r)\n        request.addfinalizer(lambda: r.cleanup())\n        return r\n\n    yield create_resource\n    # All finalizers run in reverse order\n```\n\n### Safe Teardown Pattern\n\n```python\n@pytest.fixture\ndef database():\n    db = None\n    try:\n        db = Database()\n        db.connect()\n        yield db\n    finally:\n        if db is not None:\n            db.disconnect()\n```\n\n## Request Object\n\nAccess test context from fixtures:\n\n```python\n@pytest.fixture\ndef resource(request):\n    # Access test function name\n    test_name = request.node.name\n\n    # Access module-level attribute\n    config = getattr(request.module, \"RESOURCE_CONFIG\", {})\n\n    # Access markers\n    marker = request.node.get_closest_marker(\"resource_type\")\n    resource_type = marker.args[0] if marker else \"default\"\n\n    return create_resource(test_name, resource_type, config)\n```\n\n### Indirect Parametrization\n\n```python\n@pytest.fixture\ndef user(request):\n    return User(role=request.param)\n\n@pytest.mark.parametrize(\"user\", [\"admin\", \"member\"], indirect=True)\ndef test_permissions(user):\n    if user.role == \"admin\":\n        assert user.can_delete()\n```\n\n## Override Patterns\n\n### Conftest Hierarchy\n\n```\ntests/\n conftest.py              # Base fixture\n unit/\n    conftest.py          # Override for unit tests\n    test_service.py\n integration/\n     conftest.py          # Override for integration tests\n     test_api.py\n```\n\n```python\n# tests/conftest.py\n@pytest.fixture\ndef database():\n    return MockDatabase()\n\n# tests/integration/conftest.py\n@pytest.fixture\ndef database():\n    return RealDatabase()  # Overrides parent\n```\n\n### Module-Level Override\n\n```python\n# test_specific.py\nimport pytest\n\n@pytest.fixture\ndef database():\n    \"\"\"Override conftest fixture for this module.\"\"\"\n    return SpecialDatabase()\n\ndef test_with_special_db(database):\n    assert isinstance(database, SpecialDatabase)\n```\n\n### Parametrized Override\n\n```python\n# conftest.py\n@pytest.fixture(params=[\"small\", \"medium\", \"large\"])\ndef dataset(request):\n    return load_dataset(request.param)\n\n# test_specific.py\n@pytest.fixture\ndef dataset():\n    \"\"\"Use only large dataset for this module.\"\"\"\n    return load_dataset(\"large\")\n```\n\n## Fixture Caching\n\nFixtures are cached per scope:\n\n```python\n@pytest.fixture(scope=\"session\")\ndef expensive_data():\n    # Computed once per test session\n    return compute_expensive_data()\n\n@pytest.fixture(scope=\"module\")\ndef module_data(expensive_data):\n    # New instance per module, reuses expensive_data\n    return process_data(expensive_data)\n```\n\n## Async Fixtures (pytest-asyncio)\n\n```python\nimport pytest_asyncio\n\n@pytest_asyncio.fixture\nasync def async_client():\n    client = AsyncClient()\n    await client.connect()\n    yield client\n    await client.disconnect()\n\n@pytest.mark.asyncio\nasync def test_async_operation(async_client):\n    result = await async_client.fetch()\n    assert result is not None\n```\n",
        "skills/pytest-python/references/patterns.md": "# Common Testing Patterns\n\n## Table of Contents\n\n1. [Test Organization](#test-organization)\n2. [Mocking Patterns](#mocking-patterns)\n3. [Database Testing](#database-testing)\n4. [API Testing](#api-testing)\n5. [File System Testing](#file-system-testing)\n6. [Time-Dependent Tests](#time-dependent-tests)\n7. [Error Handling Tests](#error-handling-tests)\n8. [Performance Testing](#performance-testing)\n\n## Test Organization\n\n### Arrange-Act-Assert (AAA)\n\n```python\ndef test_user_discount():\n    # Arrange\n    user = User(membership=\"premium\")\n    product = Product(price=100)\n\n    # Act\n    total = calculate_total(user, product)\n\n    # Assert\n    assert total == 90  # 10% discount\n```\n\n### Given-When-Then (BDD Style)\n\n```python\ndef test_premium_user_gets_discount():\n    # Given a premium user\n    user = User(membership=\"premium\")\n    product = Product(price=100)\n\n    # When calculating total\n    total = calculate_total(user, product)\n\n    # Then 10% discount is applied\n    assert total == 90\n```\n\n### Test Classes for Grouping\n\n```python\nclass TestUserAuthentication:\n    \"\"\"Group related authentication tests.\"\"\"\n\n    def test_valid_credentials(self, auth_service):\n        result = auth_service.login(\"user\", \"password\")\n        assert result.success\n\n    def test_invalid_password(self, auth_service):\n        result = auth_service.login(\"user\", \"wrong\")\n        assert not result.success\n\n    def test_locked_account(self, auth_service, locked_user):\n        result = auth_service.login(locked_user.email, \"password\")\n        assert result.error == \"account_locked\"\n```\n\n## Mocking Patterns\n\n### External API Calls\n\n```python\ndef test_weather_service(monkeypatch):\n    mock_response = {\"temp\": 72, \"condition\": \"sunny\"}\n\n    def mock_get(*args, **kwargs):\n        class MockResponse:\n            def json(self):\n                return mock_response\n        return MockResponse()\n\n    monkeypatch.setattr(\"requests.get\", mock_get)\n\n    weather = get_weather(\"NYC\")\n    assert weather[\"temp\"] == 72\n```\n\n### Class Methods\n\n```python\ndef test_email_notification(monkeypatch):\n    sent_emails = []\n\n    def mock_send(self, to, subject, body):\n        sent_emails.append({\"to\": to, \"subject\": subject})\n\n    monkeypatch.setattr(EmailService, \"send\", mock_send)\n\n    notify_user(User(email=\"test@example.com\"))\n    assert len(sent_emails) == 1\n    assert sent_emails[0][\"to\"] == \"test@example.com\"\n```\n\n### Using unittest.mock\n\n```python\nfrom unittest.mock import Mock, patch, MagicMock\n\ndef test_with_mock():\n    mock_db = Mock()\n    mock_db.query.return_value = [{\"id\": 1, \"name\": \"Test\"}]\n\n    service = UserService(mock_db)\n    users = service.get_all()\n\n    mock_db.query.assert_called_once()\n    assert len(users) == 1\n\n@patch(\"mymodule.external_api\")\ndef test_with_patch(mock_api):\n    mock_api.fetch.return_value = {\"data\": \"test\"}\n    result = process_data()\n    assert result == \"test\"\n```\n\n## Database Testing\n\n### Transaction Rollback\n\n```python\n@pytest.fixture\ndef db_session(database):\n    \"\"\"Rollback after each test.\"\"\"\n    session = database.create_session()\n    session.begin_nested()  # Savepoint\n    yield session\n    session.rollback()\n    session.close()\n\ndef test_user_creation(db_session):\n    user = User(email=\"test@example.com\")\n    db_session.add(user)\n    db_session.flush()\n    assert user.id is not None\n    # Rolled back automatically\n```\n\n### Test Data Builders\n\n```python\nclass UserBuilder:\n    def __init__(self):\n        self._email = \"default@example.com\"\n        self._role = \"user\"\n\n    def with_email(self, email):\n        self._email = email\n        return self\n\n    def with_role(self, role):\n        self._role = role\n        return self\n\n    def build(self):\n        return User(email=self._email, role=self._role)\n\n@pytest.fixture\ndef user_builder():\n    return UserBuilder()\n\ndef test_admin_permissions(user_builder):\n    admin = user_builder.with_role(\"admin\").build()\n    assert admin.can_delete_users()\n```\n\n## API Testing\n\n### HTTP Client Testing\n\n```python\n@pytest.fixture\ndef client(app):\n    return app.test_client()\n\ndef test_get_users(client):\n    response = client.get(\"/api/users\")\n    assert response.status_code == 200\n    assert len(response.json) > 0\n\ndef test_create_user(client):\n    response = client.post(\"/api/users\", json={\n        \"email\": \"new@example.com\",\n        \"name\": \"New User\"\n    })\n    assert response.status_code == 201\n    assert response.json[\"email\"] == \"new@example.com\"\n\ndef test_authentication_required(client):\n    response = client.get(\"/api/protected\")\n    assert response.status_code == 401\n```\n\n### Response Validation\n\n```python\ndef test_user_response_schema(client):\n    response = client.get(\"/api/users/1\")\n\n    assert response.status_code == 200\n    data = response.json\n\n    # Validate structure\n    assert \"id\" in data\n    assert \"email\" in data\n    assert \"created_at\" in data\n\n    # Validate types\n    assert isinstance(data[\"id\"], int)\n    assert \"@\" in data[\"email\"]\n```\n\n## File System Testing\n\n### Using tmp_path\n\n```python\ndef test_file_processing(tmp_path):\n    # Create test file\n    input_file = tmp_path / \"input.txt\"\n    input_file.write_text(\"line1\\nline2\\nline3\")\n\n    # Process\n    output_file = tmp_path / \"output.txt\"\n    process_file(input_file, output_file)\n\n    # Verify\n    assert output_file.exists()\n    content = output_file.read_text()\n    assert \"PROCESSED\" in content\n\ndef test_directory_creation(tmp_path):\n    config_dir = tmp_path / \"config\"\n    initialize_config(config_dir)\n\n    assert config_dir.exists()\n    assert (config_dir / \"settings.json\").exists()\n```\n\n### File Fixtures\n\n```python\n@pytest.fixture\ndef sample_csv(tmp_path):\n    csv_file = tmp_path / \"data.csv\"\n    csv_file.write_text(\"name,value\\ntest,123\\n\")\n    return csv_file\n\ndef test_csv_import(sample_csv, database):\n    import_csv(sample_csv, database)\n    assert database.count(\"records\") == 1\n```\n\n## Time-Dependent Tests\n\n### Freezing Time\n\n```python\nfrom freezegun import freeze_time\n\n@freeze_time(\"2024-01-15 12:00:00\")\ndef test_subscription_expiry():\n    user = User(subscription_end=\"2024-01-14\")\n    assert user.is_expired()\n\n@freeze_time(\"2024-01-13 12:00:00\")\ndef test_subscription_active():\n    user = User(subscription_end=\"2024-01-14\")\n    assert not user.is_expired()\n```\n\n### Mocking datetime\n\n```python\ndef test_daily_report(monkeypatch):\n    import datetime\n\n    class MockDatetime:\n        @classmethod\n        def now(cls):\n            return datetime.datetime(2024, 1, 15, 9, 0, 0)\n\n    monkeypatch.setattr(\"mymodule.datetime\", MockDatetime)\n\n    report = generate_daily_report()\n    assert report.date == \"2024-01-15\"\n```\n\n## Error Handling Tests\n\n### Exception Messages\n\n```python\ndef test_validation_error_message():\n    with pytest.raises(ValidationError) as exc_info:\n        validate_email(\"invalid\")\n\n    assert \"Invalid email format\" in str(exc_info.value)\n    assert exc_info.value.field == \"email\"\n```\n\n### Multiple Exception Types\n\n```python\n@pytest.mark.parametrize(\"input,expected_error\", [\n    (\"\", ValueError),\n    (None, TypeError),\n    (-1, ValueError),\n])\ndef test_invalid_inputs(input, expected_error):\n    with pytest.raises(expected_error):\n        process_input(input)\n```\n\n### Error Recovery\n\n```python\ndef test_graceful_degradation(monkeypatch):\n    def mock_fail(*args):\n        raise ConnectionError(\"Service unavailable\")\n\n    monkeypatch.setattr(\"mymodule.external_service.call\", mock_fail)\n\n    # Should return cached/default value instead of crashing\n    result = get_data_with_fallback()\n    assert result == DEFAULT_VALUE\n```\n\n## Performance Testing\n\n### Timing Tests\n\n```python\nimport time\n\ndef test_query_performance():\n    start = time.perf_counter()\n    result = expensive_query()\n    duration = time.perf_counter() - start\n\n    assert duration < 1.0  # Should complete in under 1 second\n    assert len(result) > 0\n```\n\n### Memory Testing\n\n```python\nimport tracemalloc\n\ndef test_memory_efficiency():\n    tracemalloc.start()\n\n    process_large_dataset()\n\n    current, peak = tracemalloc.get_traced_memory()\n    tracemalloc.stop()\n\n    # Peak memory should be under 100MB\n    assert peak < 100 * 1024 * 1024\n```\n\n### Benchmark with pytest-benchmark\n\n```python\ndef test_sorting_performance(benchmark):\n    data = list(range(10000, 0, -1))\n\n    result = benchmark(sorted, data)\n\n    assert result == list(range(1, 10001))\n```\n\n## Parametrized Edge Cases\n\n```python\n@pytest.mark.parametrize(\"input,expected\", [\n    # Normal cases\n    (\"hello\", \"HELLO\"),\n    (\"World\", \"WORLD\"),\n    # Edge cases\n    (\"\", \"\"),\n    (\"a\", \"A\"),\n    # Unicode\n    (\"cafe\", \"CAFE\"),\n    # Already uppercase\n    (\"HELLO\", \"HELLO\"),\n    # Mixed\n    (\"HeLLo\", \"HELLO\"),\n])\ndef test_uppercase(input, expected):\n    assert to_uppercase(input) == expected\n```\n",
        "skills/recipes/.claude-plugin/plugin.json": "{\n  \"name\": \"recipes\",\n  \"description\": \"Recipe and cooking assistant for meal planning and instructions\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Ali Jilani\"\n  },\n  \"category\": \"productivity\",\n  \"keywords\": [\"recipes\", \"cooking\", \"food\", \"meal-planning\"]\n}\n",
        "skills/recipes/CLAUDE.md": "# The healthy recipe book\n\n## Key directives (Goals)\n\n- `Primary Goal: Make all food high-quality and allergy-safe.`\n- `Style Goal: Every dish must use fresh, local ingredients.`\n\n## Non-Negotiable Rules (Constraints)\n\n**Constraint:** Absolutely no peanuts of tree nuts are allowed in any recipe.\n\n## Key Ingredients Locations (File Context)\n\n/recipes/ : All new recipes must be saved in this folder.",
        "skills/recipes/oat-juice-breakfast.md": "# Oat Juice Breakfast Bowl\n\n## Description\nA refreshing, nutritious breakfast featuring homemade oat juice with colorful toppings. This recipe is completely nut-free, allergy-safe, and packed with wholesome ingredients.\n\n## Servings\n2 servings\n\n## Prep Time\n10 minutes\n\n## Ingredients\n\n### For the Oat Juice:\n- 1 cup rolled oats\n- 4 cups cold water\n- 2 tablespoons maple syrup\n- 1 teaspoon vanilla extract\n- Pinch of salt\n\n### For the Breakfast Bowl:\n- 1 cup fresh strawberries, sliced\n- 1 banana, sliced\n- 1/2 cup fresh blueberries\n- 1/4 cup granola (nut-free)\n- 2 tablespoons chia seeds\n- 2 tablespoons shredded coconut\n- Fresh mint leaves for garnish\n\n## Instructions\n\n### Making the Oat Juice:\n1. Add rolled oats and cold water to a blender\n2. Blend on high speed for 30-45 seconds until smooth\n3. Strain through a fine-mesh sieve or cheesecloth into a pitcher\n4. Stir in maple syrup, vanilla extract, and a pinch of salt\n5. Refrigerate until ready to serve\n\n### Assembling the Breakfast Bowl:\n1. Pour 1 cup of oat juice into each bowl\n2. Arrange strawberries on one side (bright red)\n3. Place banana slices in the center (bright yellow)\n4. Add blueberries on the other side (vibrant blue/purple)\n5. Sprinkle chia seeds and granola on top\n6. Add shredded coconut for texture\n7. Garnish with fresh mint leaves for a pop of green\n\n## Plating Style\nUse **bright, simple colors** for visual appeal:\n- **Red** from strawberries\n- **Yellow** from bananas\n- **Blue/Purple** from blueberries\n- **Green** from mint\n- **White** from coconut and oat juice\n\nArrange fruits in separate sections for a colorful, eye-catching presentation.\n\n## Nutritional Benefits\n- High in fiber from oats and chia seeds\n- Rich in antioxidants from berries\n- Natural energy from bananas\n- Plant-based and dairy-free\n- Heart-healthy and cholesterol-free\n\n## Allergy Information\n **Nut-free** - No peanuts or tree nuts\n **Dairy-free**\n **Vegan-friendly**\n Safe for common allergies\n\n## Storage\nOat juice can be stored in the refrigerator for up to 3 days. Shake well before serving.\n\n---\n\n*Created: 2025-12-10*\n*Category: Breakfast, Beverages*\n*Difficulty: Easy*\n",
        "skills/resume_optimizer/.claude-plugin/plugin.json": "{\n  \"name\": \"resume-optimizer\",\n  \"description\": \"Resume optimization tool to match job requirements and highlight relevant skills\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Ali Jilani\"\n  },\n  \"category\": \"productivity\",\n  \"keywords\": [\"resume\", \"cv\", \"job-application\", \"career\", \"optimization\"]\n}\n",
        "skills/resume_optimizer/.claude/command/job_optimize_resume.md": "Use the linkedin_job_resume_optimizer skill to identify relevant LinkedIn job postings that match the candidates background.\r\nThen optimize the resume located at\r\n/mnt/d/Ali_Home/Learning/AgenticAI/AI-P009/Assignments/ProjectA1/resume_optimizer/resume/resume06012026.docx\r\nto truthfully align with the required skills and keywords.\r\nFinally, generate targeted interview questions and recommended updates for the LinkedIn profile based on the selected roles.",
        "skills/resume_optimizer/.claude/command/job_resume_optimizer.md": "Create a skill called \"linkedin_job_resume_optimizer\" with the following strengths\r\n\r\nInitialize a job search and resume tailoring workflow:\r\n\r\n1. Use 'playwright' to search for 2 remote AI-related job postings on LinkedIn (linkedin.com/jobs) that match the skills found in 'D:\\Ali_Home\\Learning\\AgenticAI\\AI-P009\\claude_projects\\skills\\docx\\resume.docx'.\r\n2. For each job found\r\n   - Share link to the exact job on linkedin.\r\n   - Extract the full job description.\r\n   - Use 'docx' to read the base resume at 'D:\\Ali_Home\\Learning\\AgenticAI\\AI-P009\\claude_projects\\skills\\docx\\resume.docx'.\r\n   - Generate a new, ATS-optimized .docx resume tailored specifically to that job's keywords and requirements. Save these as 'resume_tailored_[job_title].docx'.\r\n3. Perform a Gap Analysis:\r\n   - Identify skill gaps between the base resume and the job requirements.\r\n   - For each gap, suggest 10 targeted interview preparation questions.\r\n   - Provide specific LinkedIn 'About' and 'Skills' section updates to address these missing keywords.\r\n4. Finalize by summarizing the 2 jobs found and the location of the newly generated resumes.",
        "skills/resume_optimizer/.claude/skills/browser-use/SKILL.md": "---\nname: browser-use\ndescription: Browser automation using Playwright MCP. Navigate websites, fill forms, click elements, take screenshots, and extract data. Use when tasks require web browsing, form submission, web scraping, UI testing, or any browser interaction.\n---\n\n# Browser Automation\n\nAutomate browser interactions via Playwright MCP server.\n\n## Server Lifecycle\n\n### Start Server\n```bash\n# Using helper script (recommended)\nbash scripts/start-server.sh\n\n# Or manually\nnpx @playwright/mcp@latest --port 8808 --shared-browser-context &\n```\n\n### Stop Server\n```bash\n# Using helper script (closes browser first)\nbash scripts/stop-server.sh\n\n# Or manually\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_close -p '{}'\npkill -f \"@playwright/mcp\"\n```\n\n### When to Stop\n- **End of task**: Stop when browser work is complete\n- **Long sessions**: Keep running if doing multiple browser tasks\n- **Errors**: Stop and restart if browser becomes unresponsive\n\n**Important:** The `--shared-browser-context` flag is required to maintain browser state across multiple mcp-client.py calls. Without it, each call gets a fresh browser context.\n\n## Quick Reference\n\n### Navigation\n\n```bash\n# Go to URL\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_navigate \\\n  -p '{\"url\": \"https://example.com\"}'\n\n# Go back\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_navigate_back -p '{}'\n```\n\n### Get Page State\n\n```bash\n# Accessibility snapshot (returns element refs for clicking/typing)\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_snapshot -p '{}'\n\n# Screenshot\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_take_screenshot \\\n  -p '{\"type\": \"png\", \"fullPage\": true}'\n```\n\n### Interact with Elements\n\nUse `ref` from snapshot output to target elements:\n\n```bash\n# Click element\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_click \\\n  -p '{\"element\": \"Submit button\", \"ref\": \"e42\"}'\n\n# Type text\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_type \\\n  -p '{\"element\": \"Search input\", \"ref\": \"e15\", \"text\": \"hello world\", \"submit\": true}'\n\n# Fill form (multiple fields)\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_fill_form \\\n  -p '{\"fields\": [{\"ref\": \"e10\", \"value\": \"john@example.com\"}, {\"ref\": \"e12\", \"value\": \"password123\"}]}'\n\n# Select dropdown\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_select_option \\\n  -p '{\"element\": \"Country dropdown\", \"ref\": \"e20\", \"values\": [\"US\"]}'\n```\n\n### Wait for Conditions\n\n```bash\n# Wait for text to appear\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_wait_for \\\n  -p '{\"text\": \"Success\"}'\n\n# Wait for time (ms)\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_wait_for \\\n  -p '{\"time\": 2000}'\n```\n\n### Execute JavaScript\n\n```bash\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_evaluate \\\n  -p '{\"function\": \"return document.title\"}'\n```\n\n### Multi-Step Playwright Code\n\nFor complex workflows, use `browser_run_code` to run multiple actions in one call:\n\n```bash\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_run_code \\\n  -p '{\"code\": \"async (page) => { await page.goto(\\\"https://example.com\\\"); await page.click(\\\"text=Learn more\\\"); return await page.title(); }\"}'\n```\n\n**Tip:** Use `browser_run_code` for complex multi-step operations that should be atomic (all-or-nothing).\n\n## Workflow: Form Submission\n\n1. Navigate to page\n2. Get snapshot to find element refs\n3. Fill form fields using refs\n4. Click submit\n5. Wait for confirmation\n6. Screenshot result\n\n## Workflow: Data Extraction\n\n1. Navigate to page\n2. Get snapshot (contains text content)\n3. Use browser_evaluate for complex extraction\n4. Process results\n\n## Tool Reference\n\nSee [references/playwright-tools.md](references/playwright-tools.md) for complete tool documentation.\n\n## Troubleshooting\n\n| Issue | Solution |\n|-------|----------|\n| Element not found | Run browser_snapshot first to get current refs |\n| Click fails | Try browser_hover first, then click |\n| Form not submitting | Use `\"submit\": true` with browser_type |\n| Page not loading | Increase wait time or use browser_wait_for |\n",
        "skills/resume_optimizer/.claude/skills/browser-use/references/playwright-tools.md": "# MCP Server Tools\n\n*22 tools available*\n\n## `browser_close`\n\nClose the page\n\n*Flags: destructive*\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {},\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_resize`\n\nResize the browser window\n\n*Flags: destructive*\n\n### Parameters\n\n- **`width`** (`number`) *(required)*: Width of the browser window\n- **`height`** (`number`) *(required)*: Height of the browser window\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"width\": {\n      \"type\": \"number\",\n      \"description\": \"Width of the browser window\"\n    },\n    \"height\": {\n      \"type\": \"number\",\n      \"description\": \"Height of the browser window\"\n    }\n  },\n  \"required\": [\n    \"width\",\n    \"height\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_console_messages`\n\nReturns all console messages\n\n*Flags: read-only*\n\n### Parameters\n\n- **`level`** (`string`): Level of the console messages to return. Each level includes the messages of more severe levels. Defaults to \"info\".\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"level\": {\n      \"type\": \"string\",\n      \"enum\": [\n        \"error\",\n        \"warning\",\n        \"info\",\n        \"debug\"\n      ],\n      \"default\": \"info\",\n      \"description\": \"Level of the console messages to return. Each level includes the messages of more severe levels. Defaults to \\\"info\\\".\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_handle_dialog`\n\nHandle a dialog\n\n*Flags: destructive*\n\n### Parameters\n\n- **`accept`** (`boolean`) *(required)*: Whether to accept the dialog.\n- **`promptText`** (`string`): The text of the prompt in case of a prompt dialog.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"accept\": {\n      \"type\": \"boolean\",\n      \"description\": \"Whether to accept the dialog.\"\n    },\n    \"promptText\": {\n      \"type\": \"string\",\n      \"description\": \"The text of the prompt in case of a prompt dialog.\"\n    }\n  },\n  \"required\": [\n    \"accept\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_evaluate`\n\nEvaluate JavaScript expression on page or element\n\n*Flags: destructive*\n\n### Parameters\n\n- **`function`** (`string`) *(required)*: () => { /* code */ } or (element) => { /* code */ } when element is provided\n- **`element`** (`string`): Human-readable element description used to obtain permission to interact with the element\n- **`ref`** (`string`): Exact target element reference from the page snapshot\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"function\": {\n      \"type\": \"string\",\n      \"description\": \"() => { /* code */ } or (element) => { /* code */ } when element is provided\"\n    },\n    \"element\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable element description used to obtain permission to interact with the element\"\n    },\n    \"ref\": {\n      \"type\": \"string\",\n      \"description\": \"Exact target element reference from the page snapshot\"\n    }\n  },\n  \"required\": [\n    \"function\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_file_upload`\n\nUpload one or multiple files\n\n*Flags: destructive*\n\n### Parameters\n\n- **`paths`** (`array`): The absolute paths to the files to upload. Can be single file or multiple files. If omitted, file chooser is cancelled.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"paths\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"description\": \"The absolute paths to the files to upload. Can be single file or multiple files. If omitted, file chooser is cancelled.\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_fill_form`\n\nFill multiple form fields\n\n*Flags: destructive*\n\n### Parameters\n\n- **`fields`** (`array`) *(required)*: Fields to fill in\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"fields\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"name\": {\n            \"type\": \"string\",\n            \"description\": \"Human-readable field name\"\n          },\n          \"type\": {\n            \"type\": \"string\",\n            \"enum\": [\n              \"textbox\",\n              \"checkbox\",\n              \"radio\",\n              \"combobox\",\n              \"slider\"\n            ],\n            \"description\": \"Type of the field\"\n          },\n          \"ref\": {\n            \"type\": \"string\",\n            \"description\": \"Exact target field reference from the page snapshot\"\n          },\n          \"value\": {\n            \"type\": \"string\",\n            \"description\": \"Value to fill in the field. If the field is a checkbox, the value should be `true` or `false`. If the field is a combobox, the value should be the text of the option.\"\n          }\n        },\n        \"required\": [\n          \"name\",\n          \"type\",\n          \"ref\",\n          \"value\"\n        ],\n        \"additionalProperties\": false\n      },\n      \"description\": \"Fields to fill in\"\n    }\n  },\n  \"required\": [\n    \"fields\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_install`\n\nInstall the browser specified in the config. Call this if you get an error about the browser not being installed.\n\n*Flags: destructive*\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {},\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_press_key`\n\nPress a key on the keyboard\n\n*Flags: destructive*\n\n### Parameters\n\n- **`key`** (`string`) *(required)*: Name of the key to press or a character to generate, such as `ArrowLeft` or `a`\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"key\": {\n      \"type\": \"string\",\n      \"description\": \"Name of the key to press or a character to generate, such as `ArrowLeft` or `a`\"\n    }\n  },\n  \"required\": [\n    \"key\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_type`\n\nType text into editable element\n\n*Flags: destructive*\n\n### Parameters\n\n- **`element`** (`string`) *(required)*: Human-readable element description used to obtain permission to interact with the element\n- **`ref`** (`string`) *(required)*: Exact target element reference from the page snapshot\n- **`text`** (`string`) *(required)*: Text to type into the element\n- **`submit`** (`boolean`): Whether to submit entered text (press Enter after)\n- **`slowly`** (`boolean`): Whether to type one character at a time. Useful for triggering key handlers in the page. By default entire text is filled in at once.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"element\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable element description used to obtain permission to interact with the element\"\n    },\n    \"ref\": {\n      \"type\": \"string\",\n      \"description\": \"Exact target element reference from the page snapshot\"\n    },\n    \"text\": {\n      \"type\": \"string\",\n      \"description\": \"Text to type into the element\"\n    },\n    \"submit\": {\n      \"type\": \"boolean\",\n      \"description\": \"Whether to submit entered text (press Enter after)\"\n    },\n    \"slowly\": {\n      \"type\": \"boolean\",\n      \"description\": \"Whether to type one character at a time. Useful for triggering key handlers in the page. By default entire text is filled in at once.\"\n    }\n  },\n  \"required\": [\n    \"element\",\n    \"ref\",\n    \"text\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_navigate`\n\nNavigate to a URL\n\n*Flags: destructive*\n\n### Parameters\n\n- **`url`** (`string`) *(required)*: The URL to navigate to\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"url\": {\n      \"type\": \"string\",\n      \"description\": \"The URL to navigate to\"\n    }\n  },\n  \"required\": [\n    \"url\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_navigate_back`\n\nGo back to the previous page\n\n*Flags: destructive*\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {},\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_network_requests`\n\nReturns all network requests since loading the page\n\n*Flags: read-only*\n\n### Parameters\n\n- **`includeStatic`** (`boolean`): Whether to include successful static resources like images, fonts, scripts, etc. Defaults to false.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"includeStatic\": {\n      \"type\": \"boolean\",\n      \"default\": false,\n      \"description\": \"Whether to include successful static resources like images, fonts, scripts, etc. Defaults to false.\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_run_code`\n\nRun Playwright code snippet\n\n*Flags: destructive*\n\n### Parameters\n\n- **`code`** (`string`) *(required)*: A JavaScript function containing Playwright code to execute. It will be invoked with a single argument, page, which you can use for any page interaction. For example: `async (page) => { await page.getByRole('button', { name: 'Submit' }).click(); return await page.title(); }`\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"code\": {\n      \"type\": \"string\",\n      \"description\": \"A JavaScript function containing Playwright code to execute. It will be invoked with a single argument, page, which you can use for any page interaction. For example: `async (page) => { await page.getByRole('button', { name: 'Submit' }).click(); return await page.title(); }`\"\n    }\n  },\n  \"required\": [\n    \"code\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_take_screenshot`\n\nTake a screenshot of the current page. You can't perform actions based on the screenshot, use browser_snapshot for actions.\n\n*Flags: read-only*\n\n### Parameters\n\n- **`type`** (`string`): Image format for the screenshot. Default is png.\n- **`filename`** (`string`): File name to save the screenshot to. Defaults to `page-{timestamp}.{png|jpeg}` if not specified. Prefer relative file names to stay within the output directory.\n- **`element`** (`string`): Human-readable element description used to obtain permission to screenshot the element. If not provided, the screenshot will be taken of viewport. If element is provided, ref must be provided too.\n- **`ref`** (`string`): Exact target element reference from the page snapshot. If not provided, the screenshot will be taken of viewport. If ref is provided, element must be provided too.\n- **`fullPage`** (`boolean`): When true, takes a screenshot of the full scrollable page, instead of the currently visible viewport. Cannot be used with element screenshots.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"type\": {\n      \"type\": \"string\",\n      \"enum\": [\n        \"png\",\n        \"jpeg\"\n      ],\n      \"default\": \"png\",\n      \"description\": \"Image format for the screenshot. Default is png.\"\n    },\n    \"filename\": {\n      \"type\": \"string\",\n      \"description\": \"File name to save the screenshot to. Defaults to `page-{timestamp}.{png|jpeg}` if not specified. Prefer relative file names to stay within the output directory.\"\n    },\n    \"element\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable element description used to obtain permission to screenshot the element. If not provided, the screenshot will be taken of viewport. If element is provided, ref must be provided too.\"\n    },\n    \"ref\": {\n      \"type\": \"string\",\n      \"description\": \"Exact target element reference from the page snapshot. If not provided, the screenshot will be taken of viewport. If ref is provided, element must be provided too.\"\n    },\n    \"fullPage\": {\n      \"type\": \"boolean\",\n      \"description\": \"When true, takes a screenshot of the full scrollable page, instead of the currently visible viewport. Cannot be used with element screenshots.\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_snapshot`\n\nCapture accessibility snapshot of the current page, this is better than screenshot\n\n*Flags: read-only*\n\n### Parameters\n\n- **`filename`** (`string`): Save snapshot to markdown file instead of returning it in the response.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"filename\": {\n      \"type\": \"string\",\n      \"description\": \"Save snapshot to markdown file instead of returning it in the response.\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_click`\n\nPerform click on a web page\n\n*Flags: destructive*\n\n### Parameters\n\n- **`element`** (`string`) *(required)*: Human-readable element description used to obtain permission to interact with the element\n- **`ref`** (`string`) *(required)*: Exact target element reference from the page snapshot\n- **`doubleClick`** (`boolean`): Whether to perform a double click instead of a single click\n- **`button`** (`string`): Button to click, defaults to left\n- **`modifiers`** (`array`): Modifier keys to press\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"element\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable element description used to obtain permission to interact with the element\"\n    },\n    \"ref\": {\n      \"type\": \"string\",\n      \"description\": \"Exact target element reference from the page snapshot\"\n    },\n    \"doubleClick\": {\n      \"type\": \"boolean\",\n      \"description\": \"Whether to perform a double click instead of a single click\"\n    },\n    \"button\": {\n      \"type\": \"string\",\n      \"enum\": [\n        \"left\",\n        \"right\",\n        \"middle\"\n      ],\n      \"description\": \"Button to click, defaults to left\"\n    },\n    \"modifiers\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\",\n        \"enum\": [\n          \"Alt\",\n          \"Control\",\n          \"ControlOrMeta\",\n          \"Meta\",\n          \"Shift\"\n        ]\n      },\n      \"description\": \"Modifier keys to press\"\n    }\n  },\n  \"required\": [\n    \"element\",\n    \"ref\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_drag`\n\nPerform drag and drop between two elements\n\n*Flags: destructive*\n\n### Parameters\n\n- **`startElement`** (`string`) *(required)*: Human-readable source element description used to obtain the permission to interact with the element\n- **`startRef`** (`string`) *(required)*: Exact source element reference from the page snapshot\n- **`endElement`** (`string`) *(required)*: Human-readable target element description used to obtain the permission to interact with the element\n- **`endRef`** (`string`) *(required)*: Exact target element reference from the page snapshot\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"startElement\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable source element description used to obtain the permission to interact with the element\"\n    },\n    \"startRef\": {\n      \"type\": \"string\",\n      \"description\": \"Exact source element reference from the page snapshot\"\n    },\n    \"endElement\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable target element description used to obtain the permission to interact with the element\"\n    },\n    \"endRef\": {\n      \"type\": \"string\",\n      \"description\": \"Exact target element reference from the page snapshot\"\n    }\n  },\n  \"required\": [\n    \"startElement\",\n    \"startRef\",\n    \"endElement\",\n    \"endRef\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_hover`\n\nHover over element on page\n\n*Flags: destructive*\n\n### Parameters\n\n- **`element`** (`string`) *(required)*: Human-readable element description used to obtain permission to interact with the element\n- **`ref`** (`string`) *(required)*: Exact target element reference from the page snapshot\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"element\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable element description used to obtain permission to interact with the element\"\n    },\n    \"ref\": {\n      \"type\": \"string\",\n      \"description\": \"Exact target element reference from the page snapshot\"\n    }\n  },\n  \"required\": [\n    \"element\",\n    \"ref\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_select_option`\n\nSelect an option in a dropdown\n\n*Flags: destructive*\n\n### Parameters\n\n- **`element`** (`string`) *(required)*: Human-readable element description used to obtain permission to interact with the element\n- **`ref`** (`string`) *(required)*: Exact target element reference from the page snapshot\n- **`values`** (`array`) *(required)*: Array of values to select in the dropdown. This can be a single value or multiple values.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"element\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable element description used to obtain permission to interact with the element\"\n    },\n    \"ref\": {\n      \"type\": \"string\",\n      \"description\": \"Exact target element reference from the page snapshot\"\n    },\n    \"values\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"description\": \"Array of values to select in the dropdown. This can be a single value or multiple values.\"\n    }\n  },\n  \"required\": [\n    \"element\",\n    \"ref\",\n    \"values\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_tabs`\n\nList, create, close, or select a browser tab.\n\n*Flags: destructive*\n\n### Parameters\n\n- **`action`** (`string`) *(required)*: Operation to perform\n- **`index`** (`number`): Tab index, used for close/select. If omitted for close, current tab is closed.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"action\": {\n      \"type\": \"string\",\n      \"enum\": [\n        \"list\",\n        \"new\",\n        \"close\",\n        \"select\"\n      ],\n      \"description\": \"Operation to perform\"\n    },\n    \"index\": {\n      \"type\": \"number\",\n      \"description\": \"Tab index, used for close/select. If omitted for close, current tab is closed.\"\n    }\n  },\n  \"required\": [\n    \"action\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_wait_for`\n\nWait for text to appear or disappear or a specified time to pass\n\n*Flags: read-only*\n\n### Parameters\n\n- **`time`** (`number`): The time to wait in seconds\n- **`text`** (`string`): The text to wait for\n- **`textGone`** (`string`): The text to wait for to disappear\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"time\": {\n      \"type\": \"number\",\n      \"description\": \"The time to wait in seconds\"\n    },\n    \"text\": {\n      \"type\": \"string\",\n      \"description\": \"The text to wait for\"\n    },\n    \"textGone\": {\n      \"type\": \"string\",\n      \"description\": \"The text to wait for to disappear\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n",
        "skills/resume_optimizer/.claude/skills/browsing-with-playwright/SKILL.md": "---\nname: browsing-with-playwright\ndescription: Browser automation using Playwright MCP. Navigate websites, fill forms, click elements, take screenshots, and extract data. Use for web browsing, form submission, web scraping, or UI testing. NOT for static content (use curl/wget).\n---\n\n# Browser Automation\n\nAutomate browser interactions via Playwright MCP server.\n\n## Server Lifecycle\n\n### Start Server\n```bash\n# Using helper script (recommended)\nbash scripts/start-server.sh\n\n# Or manually\nnpx @playwright/mcp@latest --port 8808 --shared-browser-context &\n```\n\n### Stop Server\n```bash\n# Using helper script (closes browser first)\nbash scripts/stop-server.sh\n\n# Or manually\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_close -p '{}'\npkill -f \"@playwright/mcp\"\n```\n\n### When to Stop\n- **End of task**: Stop when browser work is complete\n- **Long sessions**: Keep running if doing multiple browser tasks\n- **Errors**: Stop and restart if browser becomes unresponsive\n\n**Important:** The `--shared-browser-context` flag is required to maintain browser state across multiple mcp-client.py calls. Without it, each call gets a fresh browser context.\n\n## Quick Reference\n\n### Navigation\n\n```bash\n# Go to URL\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_navigate \\\n  -p '{\"url\": \"https://example.com\"}'\n\n# Go back\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_navigate_back -p '{}'\n```\n\n### Get Page State\n\n```bash\n# Accessibility snapshot (returns element refs for clicking/typing)\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_snapshot -p '{}'\n\n# Screenshot\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_take_screenshot \\\n  -p '{\"type\": \"png\", \"fullPage\": true}'\n```\n\n### Interact with Elements\n\nUse `ref` from snapshot output to target elements:\n\n```bash\n# Click element\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_click \\\n  -p '{\"element\": \"Submit button\", \"ref\": \"e42\"}'\n\n# Type text\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_type \\\n  -p '{\"element\": \"Search input\", \"ref\": \"e15\", \"text\": \"hello world\", \"submit\": true}'\n\n# Fill form (multiple fields)\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_fill_form \\\n  -p '{\"fields\": [{\"ref\": \"e10\", \"value\": \"john@example.com\"}, {\"ref\": \"e12\", \"value\": \"password123\"}]}'\n\n# Select dropdown\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_select_option \\\n  -p '{\"element\": \"Country dropdown\", \"ref\": \"e20\", \"values\": [\"US\"]}'\n```\n\n### Wait for Conditions\n\n```bash\n# Wait for text to appear\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_wait_for \\\n  -p '{\"text\": \"Success\"}'\n\n# Wait for time (ms)\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_wait_for \\\n  -p '{\"time\": 2000}'\n```\n\n### Execute JavaScript\n\n```bash\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_evaluate \\\n  -p '{\"function\": \"return document.title\"}'\n```\n\n### Multi-Step Playwright Code\n\nFor complex workflows, use `browser_run_code` to run multiple actions in one call:\n\n```bash\npython3 scripts/mcp-client.py call -u http://localhost:8808 -t browser_run_code \\\n  -p '{\"code\": \"async (page) => { await page.goto(\\\"https://example.com\\\"); await page.click(\\\"text=Learn more\\\"); return await page.title(); }\"}'\n```\n\n**Tip:** Use `browser_run_code` for complex multi-step operations that should be atomic (all-or-nothing).\n\n## Workflow: Form Submission\n\n1. Navigate to page\n2. Get snapshot to find element refs\n3. Fill form fields using refs\n4. Click submit\n5. Wait for confirmation\n6. Screenshot result\n\n## Workflow: Data Extraction\n\n1. Navigate to page\n2. Get snapshot (contains text content)\n3. Use browser_evaluate for complex extraction\n4. Process results\n\n## Verification\n\nRun: `python3 scripts/verify.py`\n\nExpected: ` Playwright MCP server running`\n\n## If Verification Fails\n\n1. Run diagnostic: `pgrep -f \"@playwright/mcp\"`\n2. Check: Server process running on port 8808\n3. Try: `bash scripts/start-server.sh`\n4. **Stop and report** if still failing - do not proceed with downstream steps\n\n## Tool Reference\n\nSee [references/playwright-tools.md](references/playwright-tools.md) for complete tool documentation.\n\n## Troubleshooting\n\n| Issue | Solution |\n|-------|----------|\n| Element not found | Run browser_snapshot first to get current refs |\n| Click fails | Try browser_hover first, then click |\n| Form not submitting | Use `\"submit\": true` with browser_type |\n| Page not loading | Increase wait time or use browser_wait_for |\n| Server not responding | Stop and restart: `bash scripts/stop-server.sh && bash scripts/start-server.sh` |\n",
        "skills/resume_optimizer/.claude/skills/browsing-with-playwright/references/playwright-tools.md": "# MCP Server Tools\n\n*22 tools available*\n\n## `browser_close`\n\nClose the page\n\n*Flags: destructive*\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {},\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_resize`\n\nResize the browser window\n\n*Flags: destructive*\n\n### Parameters\n\n- **`width`** (`number`) *(required)*: Width of the browser window\n- **`height`** (`number`) *(required)*: Height of the browser window\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"width\": {\n      \"type\": \"number\",\n      \"description\": \"Width of the browser window\"\n    },\n    \"height\": {\n      \"type\": \"number\",\n      \"description\": \"Height of the browser window\"\n    }\n  },\n  \"required\": [\n    \"width\",\n    \"height\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_console_messages`\n\nReturns all console messages\n\n*Flags: read-only*\n\n### Parameters\n\n- **`level`** (`string`): Level of the console messages to return. Each level includes the messages of more severe levels. Defaults to \"info\".\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"level\": {\n      \"type\": \"string\",\n      \"enum\": [\n        \"error\",\n        \"warning\",\n        \"info\",\n        \"debug\"\n      ],\n      \"default\": \"info\",\n      \"description\": \"Level of the console messages to return. Each level includes the messages of more severe levels. Defaults to \\\"info\\\".\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_handle_dialog`\n\nHandle a dialog\n\n*Flags: destructive*\n\n### Parameters\n\n- **`accept`** (`boolean`) *(required)*: Whether to accept the dialog.\n- **`promptText`** (`string`): The text of the prompt in case of a prompt dialog.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"accept\": {\n      \"type\": \"boolean\",\n      \"description\": \"Whether to accept the dialog.\"\n    },\n    \"promptText\": {\n      \"type\": \"string\",\n      \"description\": \"The text of the prompt in case of a prompt dialog.\"\n    }\n  },\n  \"required\": [\n    \"accept\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_evaluate`\n\nEvaluate JavaScript expression on page or element\n\n*Flags: destructive*\n\n### Parameters\n\n- **`function`** (`string`) *(required)*: () => { /* code */ } or (element) => { /* code */ } when element is provided\n- **`element`** (`string`): Human-readable element description used to obtain permission to interact with the element\n- **`ref`** (`string`): Exact target element reference from the page snapshot\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"function\": {\n      \"type\": \"string\",\n      \"description\": \"() => { /* code */ } or (element) => { /* code */ } when element is provided\"\n    },\n    \"element\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable element description used to obtain permission to interact with the element\"\n    },\n    \"ref\": {\n      \"type\": \"string\",\n      \"description\": \"Exact target element reference from the page snapshot\"\n    }\n  },\n  \"required\": [\n    \"function\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_file_upload`\n\nUpload one or multiple files\n\n*Flags: destructive*\n\n### Parameters\n\n- **`paths`** (`array`): The absolute paths to the files to upload. Can be single file or multiple files. If omitted, file chooser is cancelled.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"paths\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"description\": \"The absolute paths to the files to upload. Can be single file or multiple files. If omitted, file chooser is cancelled.\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_fill_form`\n\nFill multiple form fields\n\n*Flags: destructive*\n\n### Parameters\n\n- **`fields`** (`array`) *(required)*: Fields to fill in\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"fields\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"name\": {\n            \"type\": \"string\",\n            \"description\": \"Human-readable field name\"\n          },\n          \"type\": {\n            \"type\": \"string\",\n            \"enum\": [\n              \"textbox\",\n              \"checkbox\",\n              \"radio\",\n              \"combobox\",\n              \"slider\"\n            ],\n            \"description\": \"Type of the field\"\n          },\n          \"ref\": {\n            \"type\": \"string\",\n            \"description\": \"Exact target field reference from the page snapshot\"\n          },\n          \"value\": {\n            \"type\": \"string\",\n            \"description\": \"Value to fill in the field. If the field is a checkbox, the value should be `true` or `false`. If the field is a combobox, the value should be the text of the option.\"\n          }\n        },\n        \"required\": [\n          \"name\",\n          \"type\",\n          \"ref\",\n          \"value\"\n        ],\n        \"additionalProperties\": false\n      },\n      \"description\": \"Fields to fill in\"\n    }\n  },\n  \"required\": [\n    \"fields\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_install`\n\nInstall the browser specified in the config. Call this if you get an error about the browser not being installed.\n\n*Flags: destructive*\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {},\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_press_key`\n\nPress a key on the keyboard\n\n*Flags: destructive*\n\n### Parameters\n\n- **`key`** (`string`) *(required)*: Name of the key to press or a character to generate, such as `ArrowLeft` or `a`\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"key\": {\n      \"type\": \"string\",\n      \"description\": \"Name of the key to press or a character to generate, such as `ArrowLeft` or `a`\"\n    }\n  },\n  \"required\": [\n    \"key\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_type`\n\nType text into editable element\n\n*Flags: destructive*\n\n### Parameters\n\n- **`element`** (`string`) *(required)*: Human-readable element description used to obtain permission to interact with the element\n- **`ref`** (`string`) *(required)*: Exact target element reference from the page snapshot\n- **`text`** (`string`) *(required)*: Text to type into the element\n- **`submit`** (`boolean`): Whether to submit entered text (press Enter after)\n- **`slowly`** (`boolean`): Whether to type one character at a time. Useful for triggering key handlers in the page. By default entire text is filled in at once.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"element\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable element description used to obtain permission to interact with the element\"\n    },\n    \"ref\": {\n      \"type\": \"string\",\n      \"description\": \"Exact target element reference from the page snapshot\"\n    },\n    \"text\": {\n      \"type\": \"string\",\n      \"description\": \"Text to type into the element\"\n    },\n    \"submit\": {\n      \"type\": \"boolean\",\n      \"description\": \"Whether to submit entered text (press Enter after)\"\n    },\n    \"slowly\": {\n      \"type\": \"boolean\",\n      \"description\": \"Whether to type one character at a time. Useful for triggering key handlers in the page. By default entire text is filled in at once.\"\n    }\n  },\n  \"required\": [\n    \"element\",\n    \"ref\",\n    \"text\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_navigate`\n\nNavigate to a URL\n\n*Flags: destructive*\n\n### Parameters\n\n- **`url`** (`string`) *(required)*: The URL to navigate to\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"url\": {\n      \"type\": \"string\",\n      \"description\": \"The URL to navigate to\"\n    }\n  },\n  \"required\": [\n    \"url\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_navigate_back`\n\nGo back to the previous page\n\n*Flags: destructive*\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {},\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_network_requests`\n\nReturns all network requests since loading the page\n\n*Flags: read-only*\n\n### Parameters\n\n- **`includeStatic`** (`boolean`): Whether to include successful static resources like images, fonts, scripts, etc. Defaults to false.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"includeStatic\": {\n      \"type\": \"boolean\",\n      \"default\": false,\n      \"description\": \"Whether to include successful static resources like images, fonts, scripts, etc. Defaults to false.\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_run_code`\n\nRun Playwright code snippet\n\n*Flags: destructive*\n\n### Parameters\n\n- **`code`** (`string`) *(required)*: A JavaScript function containing Playwright code to execute. It will be invoked with a single argument, page, which you can use for any page interaction. For example: `async (page) => { await page.getByRole('button', { name: 'Submit' }).click(); return await page.title(); }`\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"code\": {\n      \"type\": \"string\",\n      \"description\": \"A JavaScript function containing Playwright code to execute. It will be invoked with a single argument, page, which you can use for any page interaction. For example: `async (page) => { await page.getByRole('button', { name: 'Submit' }).click(); return await page.title(); }`\"\n    }\n  },\n  \"required\": [\n    \"code\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_take_screenshot`\n\nTake a screenshot of the current page. You can't perform actions based on the screenshot, use browser_snapshot for actions.\n\n*Flags: read-only*\n\n### Parameters\n\n- **`type`** (`string`): Image format for the screenshot. Default is png.\n- **`filename`** (`string`): File name to save the screenshot to. Defaults to `page-{timestamp}.{png|jpeg}` if not specified. Prefer relative file names to stay within the output directory.\n- **`element`** (`string`): Human-readable element description used to obtain permission to screenshot the element. If not provided, the screenshot will be taken of viewport. If element is provided, ref must be provided too.\n- **`ref`** (`string`): Exact target element reference from the page snapshot. If not provided, the screenshot will be taken of viewport. If ref is provided, element must be provided too.\n- **`fullPage`** (`boolean`): When true, takes a screenshot of the full scrollable page, instead of the currently visible viewport. Cannot be used with element screenshots.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"type\": {\n      \"type\": \"string\",\n      \"enum\": [\n        \"png\",\n        \"jpeg\"\n      ],\n      \"default\": \"png\",\n      \"description\": \"Image format for the screenshot. Default is png.\"\n    },\n    \"filename\": {\n      \"type\": \"string\",\n      \"description\": \"File name to save the screenshot to. Defaults to `page-{timestamp}.{png|jpeg}` if not specified. Prefer relative file names to stay within the output directory.\"\n    },\n    \"element\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable element description used to obtain permission to screenshot the element. If not provided, the screenshot will be taken of viewport. If element is provided, ref must be provided too.\"\n    },\n    \"ref\": {\n      \"type\": \"string\",\n      \"description\": \"Exact target element reference from the page snapshot. If not provided, the screenshot will be taken of viewport. If ref is provided, element must be provided too.\"\n    },\n    \"fullPage\": {\n      \"type\": \"boolean\",\n      \"description\": \"When true, takes a screenshot of the full scrollable page, instead of the currently visible viewport. Cannot be used with element screenshots.\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_snapshot`\n\nCapture accessibility snapshot of the current page, this is better than screenshot\n\n*Flags: read-only*\n\n### Parameters\n\n- **`filename`** (`string`): Save snapshot to markdown file instead of returning it in the response.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"filename\": {\n      \"type\": \"string\",\n      \"description\": \"Save snapshot to markdown file instead of returning it in the response.\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_click`\n\nPerform click on a web page\n\n*Flags: destructive*\n\n### Parameters\n\n- **`element`** (`string`) *(required)*: Human-readable element description used to obtain permission to interact with the element\n- **`ref`** (`string`) *(required)*: Exact target element reference from the page snapshot\n- **`doubleClick`** (`boolean`): Whether to perform a double click instead of a single click\n- **`button`** (`string`): Button to click, defaults to left\n- **`modifiers`** (`array`): Modifier keys to press\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"element\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable element description used to obtain permission to interact with the element\"\n    },\n    \"ref\": {\n      \"type\": \"string\",\n      \"description\": \"Exact target element reference from the page snapshot\"\n    },\n    \"doubleClick\": {\n      \"type\": \"boolean\",\n      \"description\": \"Whether to perform a double click instead of a single click\"\n    },\n    \"button\": {\n      \"type\": \"string\",\n      \"enum\": [\n        \"left\",\n        \"right\",\n        \"middle\"\n      ],\n      \"description\": \"Button to click, defaults to left\"\n    },\n    \"modifiers\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\",\n        \"enum\": [\n          \"Alt\",\n          \"Control\",\n          \"ControlOrMeta\",\n          \"Meta\",\n          \"Shift\"\n        ]\n      },\n      \"description\": \"Modifier keys to press\"\n    }\n  },\n  \"required\": [\n    \"element\",\n    \"ref\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_drag`\n\nPerform drag and drop between two elements\n\n*Flags: destructive*\n\n### Parameters\n\n- **`startElement`** (`string`) *(required)*: Human-readable source element description used to obtain the permission to interact with the element\n- **`startRef`** (`string`) *(required)*: Exact source element reference from the page snapshot\n- **`endElement`** (`string`) *(required)*: Human-readable target element description used to obtain the permission to interact with the element\n- **`endRef`** (`string`) *(required)*: Exact target element reference from the page snapshot\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"startElement\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable source element description used to obtain the permission to interact with the element\"\n    },\n    \"startRef\": {\n      \"type\": \"string\",\n      \"description\": \"Exact source element reference from the page snapshot\"\n    },\n    \"endElement\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable target element description used to obtain the permission to interact with the element\"\n    },\n    \"endRef\": {\n      \"type\": \"string\",\n      \"description\": \"Exact target element reference from the page snapshot\"\n    }\n  },\n  \"required\": [\n    \"startElement\",\n    \"startRef\",\n    \"endElement\",\n    \"endRef\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_hover`\n\nHover over element on page\n\n*Flags: destructive*\n\n### Parameters\n\n- **`element`** (`string`) *(required)*: Human-readable element description used to obtain permission to interact with the element\n- **`ref`** (`string`) *(required)*: Exact target element reference from the page snapshot\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"element\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable element description used to obtain permission to interact with the element\"\n    },\n    \"ref\": {\n      \"type\": \"string\",\n      \"description\": \"Exact target element reference from the page snapshot\"\n    }\n  },\n  \"required\": [\n    \"element\",\n    \"ref\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_select_option`\n\nSelect an option in a dropdown\n\n*Flags: destructive*\n\n### Parameters\n\n- **`element`** (`string`) *(required)*: Human-readable element description used to obtain permission to interact with the element\n- **`ref`** (`string`) *(required)*: Exact target element reference from the page snapshot\n- **`values`** (`array`) *(required)*: Array of values to select in the dropdown. This can be a single value or multiple values.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"element\": {\n      \"type\": \"string\",\n      \"description\": \"Human-readable element description used to obtain permission to interact with the element\"\n    },\n    \"ref\": {\n      \"type\": \"string\",\n      \"description\": \"Exact target element reference from the page snapshot\"\n    },\n    \"values\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"string\"\n      },\n      \"description\": \"Array of values to select in the dropdown. This can be a single value or multiple values.\"\n    }\n  },\n  \"required\": [\n    \"element\",\n    \"ref\",\n    \"values\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_tabs`\n\nList, create, close, or select a browser tab.\n\n*Flags: destructive*\n\n### Parameters\n\n- **`action`** (`string`) *(required)*: Operation to perform\n- **`index`** (`number`): Tab index, used for close/select. If omitted for close, current tab is closed.\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"action\": {\n      \"type\": \"string\",\n      \"enum\": [\n        \"list\",\n        \"new\",\n        \"close\",\n        \"select\"\n      ],\n      \"description\": \"Operation to perform\"\n    },\n    \"index\": {\n      \"type\": \"number\",\n      \"description\": \"Tab index, used for close/select. If omitted for close, current tab is closed.\"\n    }\n  },\n  \"required\": [\n    \"action\"\n  ],\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n## `browser_wait_for`\n\nWait for text to appear or disappear or a specified time to pass\n\n*Flags: read-only*\n\n### Parameters\n\n- **`time`** (`number`): The time to wait in seconds\n- **`text`** (`string`): The text to wait for\n- **`textGone`** (`string`): The text to wait for to disappear\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"time\": {\n      \"type\": \"number\",\n      \"description\": \"The time to wait in seconds\"\n    },\n    \"text\": {\n      \"type\": \"string\",\n      \"description\": \"The text to wait for\"\n    },\n    \"textGone\": {\n      \"type\": \"string\",\n      \"description\": \"The text to wait for to disappear\"\n    }\n  },\n  \"additionalProperties\": false,\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\"\n}\n```\n</details>\n\n",
        "skills/resume_optimizer/.claude/skills/context7-efficient/SKILL.md": "---\nname: context7-efficient\ndescription: Token-efficient library documentation fetcher using Context7 MCP with 86.8% token savings through intelligent shell pipeline filtering. Fetches code examples, API references, and best practices for JavaScript, Python, Go, Rust, and other libraries. Use when users ask about library documentation, need code examples, want API usage patterns, are learning a new framework, need syntax reference, or troubleshooting with library-specific information. Triggers include questions like \"Show me React hooks\", \"How do I use Prisma\", \"What's the Next.js routing syntax\", or any request for library/framework documentation.\n---\n\n# Context7 Efficient Documentation Fetcher\n\nFetch library documentation with automatic 77% token reduction via shell pipeline.\n\n## Quick Start\n\n**Always use the token-efficient shell pipeline:**\n\n```bash\n# Automatic library resolution + filtering\nbash scripts/fetch-docs.sh --library <library-name> --topic <topic>\n\n# Examples:\nbash scripts/fetch-docs.sh --library react --topic useState\nbash scripts/fetch-docs.sh --library nextjs --topic routing\nbash scripts/fetch-docs.sh --library prisma --topic queries\n```\n\n**Result:** Returns ~205 tokens instead of ~934 tokens (77% savings).\n\n## Standard Workflow\n\nFor any documentation request, follow this workflow:\n\n### 1. Identify Library and Topic\n\nExtract from user query:\n- **Library:** React, Next.js, Prisma, Express, etc.\n- **Topic:** Specific feature (hooks, routing, queries, etc.)\n\n### 2. Fetch with Shell Pipeline\n\n```bash\nbash scripts/fetch-docs.sh --library <library> --topic <topic> --verbose\n```\n\nThe `--verbose` flag shows token savings statistics.\n\n### 3. Use Filtered Output\n\nThe script automatically:\n- Fetches full documentation (934 tokens, stays in subprocess)\n- Filters to code examples + API signatures + key notes\n- Returns only essential content (205 tokens to Claude)\n\n## Parameters\n\n### Basic Usage\n\n```bash\nbash scripts/fetch-docs.sh [OPTIONS]\n```\n\n**Required (pick one):**\n- `--library <name>` - Library name (e.g., \"react\", \"nextjs\")\n- `--library-id <id>` - Direct Context7 ID (faster, skips resolution)\n\n**Optional:**\n- `--topic <topic>` - Specific feature to focus on\n- `--mode <code|info>` - code for examples (default), info for concepts\n- `--page <1-10>` - Pagination for more results\n- `--verbose` - Show token savings statistics\n\n### Mode Selection\n\n**Code Mode (default):** Returns code examples + API signatures\n```bash\n--mode code\n```\n\n**Info Mode:** Returns conceptual explanations + fewer examples\n```bash\n--mode info\n```\n\n## Common Library IDs\n\nUse `--library-id` for faster lookup (skips resolution):\n\n```bash\nReact:      /reactjs/react.dev\nNext.js:    /vercel/next.js\nExpress:    /expressjs/express\nPrisma:     /prisma/docs\nMongoDB:    /mongodb/docs\nFastify:    /fastify/fastify\nNestJS:     /nestjs/docs\nVue.js:     /vuejs/docs\nSvelte:     /sveltejs/site\n```\n\n## Workflow Patterns\n\n### Pattern 1: Quick Code Examples\n\nUser asks: \"Show me React useState examples\"\n\n```bash\nbash scripts/fetch-docs.sh --library react --topic useState --verbose\n```\n\nReturns: 5 code examples + API signatures + notes (~205 tokens)\n\n### Pattern 2: Learning New Library\n\nUser asks: \"How do I get started with Prisma?\"\n\n```bash\n# Step 1: Get overview\nbash scripts/fetch-docs.sh --library prisma --topic \"getting started\" --mode info\n\n# Step 2: Get code examples\nbash scripts/fetch-docs.sh --library prisma --topic queries --mode code\n```\n\n### Pattern 3: Specific Feature Lookup\n\nUser asks: \"How does Next.js routing work?\"\n\n```bash\nbash scripts/fetch-docs.sh --library-id /vercel/next.js --topic routing\n```\n\nUsing `--library-id` is faster when you know the exact ID.\n\n### Pattern 4: Deep Exploration\n\nUser needs comprehensive information:\n\n```bash\n# Page 1: Basic examples\nbash scripts/fetch-docs.sh --library react --topic hooks --page 1\n\n# Page 2: Advanced patterns\nbash scripts/fetch-docs.sh --library react --topic hooks --page 2\n```\n\n## Token Efficiency\n\n**How it works:**\n\n1. `fetch-docs.sh` calls `fetch-raw.sh` (which uses `mcp-client.py`)\n2. Full response (934 tokens) stays in subprocess memory\n3. Shell filters (awk/grep/sed) extract essentials (0 LLM tokens used)\n4. Returns filtered output (205 tokens) to Claude\n\n**Savings:**\n- Direct MCP: 934 tokens per query\n- This approach: 205 tokens per query\n- **77% reduction**\n\n**Do NOT use `mcp-client.py` directly** - it bypasses filtering and wastes tokens.\n\n## Advanced: Library Resolution\n\nIf library name fails, try variations:\n\n```bash\n# Try different formats\n--library \"next.js\"    # with dot\n--library \"nextjs\"     # without dot\n--library \"next\"       # short form\n\n# Or search manually\nbash scripts/fetch-docs.sh --library \"your-library\" --verbose\n# Check output for suggested library IDs\n```\n\n## Troubleshooting\n\n| Issue | Solution |\n|-------|----------|\n| Library not found | Try name variations or use broader search term |\n| No results | Use `--mode info` or broader topic |\n| Need more examples | Increase page: `--page 2` |\n| Want full context | Use `--mode info` for explanations |\n\n## References\n\nFor detailed Context7 MCP tool documentation, see:\n- [references/context7-tools.md](references/context7-tools.md) - Complete tool reference\n\n## Implementation Notes\n\n**Components (for reference only, use fetch-docs.sh):**\n- `mcp-client.py` - Universal MCP client (foundation)\n- `fetch-raw.sh` - MCP wrapper\n- `extract-code-blocks.sh` - Code example filter (awk)\n- `extract-signatures.sh` - API signature filter (awk)\n- `extract-notes.sh` - Important notes filter (grep)\n- `fetch-docs.sh` - **Main orchestrator (ALWAYS USE THIS)**\n\n**Architecture:**\nShell pipeline processes documentation in subprocess, keeping full response out of Claude's context. Only filtered essentials enter the LLM context, achieving 77% token savings with 100% functionality preserved.\n\nBased on [Anthropic's \"Code Execution with MCP\" blog post](https://www.anthropic.com/engineering/code-execution-with-mcp).\n",
        "skills/resume_optimizer/.claude/skills/context7-efficient/references/context7-tools.md": "# Context7 MCP Tools\n\n*2 tools available*\n\n## `resolve-library-id`\n\nResolves a package/product name to a Context7-compatible library ID and returns a list of matching libraries.\n\nYou MUST call this function before 'get-library-docs' to obtain a valid Context7-compatible library ID UNLESS the user explicitly provides a library ID in the format '/org/project' or '/org/project/version' in their query.\n\n### Selection Process\n\n1. Analyze the query to understand what library/package the user is looking for\n2. Return the most relevant match based on:\n   - Name similarity to the query (exact matches prioritized)\n   - Description relevance to the query's intent\n   - Documentation coverage (prioritize libraries with higher Code Snippet counts)\n   - Source reputation (consider libraries with High or Medium reputation more authoritative)\n   - Benchmark Score: Quality indicator (100 is the highest score)\n\n### Parameters\n\n- **`libraryName`** (`string`) *(required)*: Library name to search for and retrieve a Context7-compatible library ID\n\n### Response Format\n\nReturns a list of matching libraries with:\n- Title\n- Context7-compatible library ID (e.g., `/reactjs/react.dev`)\n- Code Snippets count\n- Source Reputation (High/Medium/Low)\n- Benchmark Score\n- Description\n\n### Examples\n\n```bash\n# Find React library\npython3 scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t resolve-library-id \\\n  -p '{\"libraryName\": \"react\"}'\n\n# Find Next.js library\npython3 scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t resolve-library-id \\\n  -p '{\"libraryName\": \"next.js\"}'\n\n# Find MongoDB library\npython3 scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t resolve-library-id \\\n  -p '{\"libraryName\": \"mongodb\"}'\n```\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"libraryName\": {\n      \"type\": \"string\",\n      \"description\": \"Library name to search for and retrieve a Context7-compatible library ID.\"\n    }\n  },\n  \"required\": [\"libraryName\"]\n}\n```\n</details>\n\n## `get-library-docs`\n\nFetches up-to-date documentation for a library. You must call 'resolve-library-id' first to obtain the exact Context7-compatible library ID required to use this tool, UNLESS the user explicitly provides a library ID in the format '/org/project' or '/org/project/version' in their query.\n\nUse mode='code' (default) for API references and code examples, or mode='info' for conceptual guides, narrative information, and architectural questions.\n\n### Parameters\n\n- **`context7CompatibleLibraryID`** (`string`) *(required)*: Exact Context7-compatible library ID (e.g., '/mongodb/docs', '/vercel/next.js', '/supabase/supabase', '/vercel/next.js/v14.3.0-canary.87') retrieved from 'resolve-library-id' or directly from user query in the format '/org/project' or '/org/project/version'\n\n- **`topic`** (`string`) *(optional)*: Topic to focus documentation on (e.g., 'hooks', 'routing')\n\n- **`mode`** (`string`) *(optional, default: \"code\")*: Documentation mode\n  - `code`: API references and code examples (default)\n  - `info`: Conceptual guides, narrative information, and architectural questions\n\n- **`page`** (`integer`) *(optional, default: 1)*: Page number for pagination (start: 1, default: 1). If the context is not sufficient, try page=2, page=3, page=4, etc. with the same topic. Range: 1-10\n\n### Examples\n\n```bash\n# Get React hooks documentation (code mode)\npython3 scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t get-library-docs \\\n  -p '{\"context7CompatibleLibraryID\": \"/reactjs/react.dev\", \"topic\": \"hooks\", \"mode\": \"code\", \"page\": 1}'\n\n# Get conceptual information about Next.js routing\npython3 scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t get-library-docs \\\n  -p '{\"context7CompatibleLibraryID\": \"/vercel/nextjs.org\", \"topic\": \"routing\", \"mode\": \"info\"}'\n\n# Get MongoDB aggregation examples\npython3 scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t get-library-docs \\\n  -p '{\"context7CompatibleLibraryID\": \"/mongodb/docs\", \"topic\": \"aggregation\", \"mode\": \"code\"}'\n\n# Get additional pages for more details\npython3 scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t get-library-docs \\\n  -p '{\"context7CompatibleLibraryID\": \"/reactjs/react.dev\", \"topic\": \"hooks\", \"mode\": \"code\", \"page\": 2}'\n```\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"context7CompatibleLibraryID\": {\n      \"type\": \"string\",\n      \"description\": \"Exact Context7-compatible library ID (e.g., '/mongodb/docs', '/vercel/next.js', '/supabase/supabase', '/vercel/next.js/v14.3.0-canary.87') retrieved from 'resolve-library-id' or directly from user query in the format '/org/project' or '/org/project/version'.\"\n    },\n    \"topic\": {\n      \"type\": \"string\",\n      \"description\": \"Topic to focus documentation on (e.g., 'hooks', 'routing').\"\n    },\n    \"mode\": {\n      \"type\": \"string\",\n      \"enum\": [\"code\", \"info\"],\n      \"default\": \"code\",\n      \"description\": \"Documentation mode: 'code' for API references and code examples (default), 'info' for conceptual guides, narrative information, and architectural questions.\"\n    },\n    \"page\": {\n      \"type\": \"integer\",\n      \"minimum\": 1,\n      \"maximum\": 10,\n      \"default\": 1,\n      \"description\": \"Page number for pagination (start: 1, default: 1). If the context is not sufficient, try page=2, page=3, page=4, etc. with the same topic.\"\n    }\n  },\n  \"required\": [\"context7CompatibleLibraryID\"]\n}\n```\n</details>\n\n## Usage Patterns\n\n### Pattern 1: Unknown Library\n\nWhen you don't know the exact library ID:\n\n```bash\n# Step 1: Resolve library name\npython3 scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t resolve-library-id -p '{\"libraryName\": \"express\"}'\n\n# Step 2: Use returned ID to fetch docs\npython3 scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t get-library-docs \\\n  -p '{\"context7CompatibleLibraryID\": \"/expressjs/expressjs.com\", \"topic\": \"middleware\"}'\n```\n\n### Pattern 2: Known Library ID\n\nWhen you know the library ID:\n\n```bash\n# Direct fetch (skip resolve step)\npython3 scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t get-library-docs \\\n  -p '{\"context7CompatibleLibraryID\": \"/reactjs/react.dev\", \"topic\": \"useState\"}'\n```\n\n### Pattern 3: Exploring Multiple Topics\n\n```bash\n# Get overview first\npython3 scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t get-library-docs \\\n  -p '{\"context7CompatibleLibraryID\": \"/prisma/docs\", \"topic\": \"getting started\", \"mode\": \"info\"}'\n\n# Then drill into specifics\npython3 scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t get-library-docs \\\n  -p '{\"context7CompatibleLibraryID\": \"/prisma/docs\", \"topic\": \"queries\", \"mode\": \"code\"}'\n```\n\n### Pattern 4: Pagination for Deep Research\n\n```bash\n# Get first page\npython3 scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t get-library-docs \\\n  -p '{\"context7CompatibleLibraryID\": \"/reactjs/react.dev\", \"topic\": \"hooks\", \"page\": 1}'\n\n# Get additional pages as needed\npython3 scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t get-library-docs \\\n  -p '{\"context7CompatibleLibraryID\": \"/reactjs/react.dev\", \"topic\": \"hooks\", \"page\": 2}'\n```\n\n## Common Library IDs\n\nQuick reference for popular libraries:\n\n| Library | Context7 ID |\n|---------|-------------|\n| React | `/reactjs/react.dev` |\n| Next.js | `/vercel/nextjs.org` |\n| Express | `/expressjs/expressjs.com` |\n| MongoDB | `/mongodb/docs` |\n| Prisma | `/prisma/docs` |\n| Vue | `/vuejs/docs` |\n| Svelte | `/sveltejs/svelte.dev` |\n| FastAPI | `/tiangolo/fastapi` |\n| Django | `/django/docs` |\n\n## Tips\n\n1. **Library Resolution**: Always use `resolve-library-id` first unless you have the exact ID\n2. **Mode Selection**: Use `code` mode for examples, `info` mode for concepts\n3. **Topic Specificity**: More specific topics yield better results\n4. **Pagination**: If results are insufficient, try `page: 2` or refine the topic\n5. **Fallback**: If no results, try broader topics or switch modes\n",
        "skills/resume_optimizer/.claude/skills/doc-coauthoring/SKILL.md": "---\nname: doc-coauthoring\ndescription: Guide users through a structured workflow for co-authoring documentation. Use when user wants to write documentation, proposals, technical specs, decision docs, or similar structured content. This workflow helps users efficiently transfer context, refine content through iteration, and verify the doc works for readers. Trigger when user mentions writing docs, creating proposals, drafting specs, or similar documentation tasks.\n---\n\n# Doc Co-Authoring Workflow\n\nThis skill provides a structured workflow for guiding users through collaborative document creation. Act as an active guide, walking users through three stages: Context Gathering, Refinement & Structure, and Reader Testing.\n\n## When to Offer This Workflow\n\n**Trigger conditions:**\n- User mentions writing documentation: \"write a doc\", \"draft a proposal\", \"create a spec\", \"write up\"\n- User mentions specific doc types: \"PRD\", \"design doc\", \"decision doc\", \"RFC\"\n- User seems to be starting a substantial writing task\n\n**Initial offer:**\nOffer the user a structured workflow for co-authoring the document. Explain the three stages:\n\n1. **Context Gathering**: User provides all relevant context while Claude asks clarifying questions\n2. **Refinement & Structure**: Iteratively build each section through brainstorming and editing\n3. **Reader Testing**: Test the doc with a fresh Claude (no context) to catch blind spots before others read it\n\nExplain that this approach helps ensure the doc works well when others read it (including when they paste it into Claude). Ask if they want to try this workflow or prefer to work freeform.\n\nIf user declines, work freeform. If user accepts, proceed to Stage 1.\n\n## Stage 1: Context Gathering\n\n**Goal:** Close the gap between what the user knows and what Claude knows, enabling smart guidance later.\n\n### Initial Questions\n\nStart by asking the user for meta-context about the document:\n\n1. What type of document is this? (e.g., technical spec, decision doc, proposal)\n2. Who's the primary audience?\n3. What's the desired impact when someone reads this?\n4. Is there a template or specific format to follow?\n5. Any other constraints or context to know?\n\nInform them they can answer in shorthand or dump information however works best for them.\n\n**If user provides a template or mentions a doc type:**\n- Ask if they have a template document to share\n- If they provide a link to a shared document, use the appropriate integration to fetch it\n- If they provide a file, read it\n\n**If user mentions editing an existing shared document:**\n- Use the appropriate integration to read the current state\n- Check for images without alt-text\n- If images exist without alt-text, explain that when others use Claude to understand the doc, Claude won't be able to see them. Ask if they want alt-text generated. If so, request they paste each image into chat for descriptive alt-text generation.\n\n### Info Dumping\n\nOnce initial questions are answered, encourage the user to dump all the context they have. Request information such as:\n- Background on the project/problem\n- Related team discussions or shared documents\n- Why alternative solutions aren't being used\n- Organizational context (team dynamics, past incidents, politics)\n- Timeline pressures or constraints\n- Technical architecture or dependencies\n- Stakeholder concerns\n\nAdvise them not to worry about organizing it - just get it all out. Offer multiple ways to provide context:\n- Info dump stream-of-consciousness\n- Point to team channels or threads to read\n- Link to shared documents\n\n**If integrations are available** (e.g., Slack, Teams, Google Drive, SharePoint, or other MCP servers), mention that these can be used to pull in context directly.\n\n**If no integrations are detected and in Claude.ai or Claude app:** Suggest they can enable connectors in their Claude settings to allow pulling context from messaging apps and document storage directly.\n\nInform them clarifying questions will be asked once they've done their initial dump.\n\n**During context gathering:**\n\n- If user mentions team channels or shared documents:\n  - If integrations available: Inform them the content will be read now, then use the appropriate integration\n  - If integrations not available: Explain lack of access. Suggest they enable connectors in Claude settings, or paste the relevant content directly.\n\n- If user mentions entities/projects that are unknown:\n  - Ask if connected tools should be searched to learn more\n  - Wait for user confirmation before searching\n\n- As user provides context, track what's being learned and what's still unclear\n\n**Asking clarifying questions:**\n\nWhen user signals they've done their initial dump (or after substantial context provided), ask clarifying questions to ensure understanding:\n\nGenerate 5-10 numbered questions based on gaps in the context.\n\nInform them they can use shorthand to answer (e.g., \"1: yes, 2: see #channel, 3: no because backwards compat\"), link to more docs, point to channels to read, or just keep info-dumping. Whatever's most efficient for them.\n\n**Exit condition:**\nSufficient context has been gathered when questions show understanding - when edge cases and trade-offs can be asked about without needing basics explained.\n\n**Transition:**\nAsk if there's any more context they want to provide at this stage, or if it's time to move on to drafting the document.\n\nIf user wants to add more, let them. When ready, proceed to Stage 2.\n\n## Stage 2: Refinement & Structure\n\n**Goal:** Build the document section by section through brainstorming, curation, and iterative refinement.\n\n**Instructions to user:**\nExplain that the document will be built section by section. For each section:\n1. Clarifying questions will be asked about what to include\n2. 5-20 options will be brainstormed\n3. User will indicate what to keep/remove/combine\n4. The section will be drafted\n5. It will be refined through surgical edits\n\nStart with whichever section has the most unknowns (usually the core decision/proposal), then work through the rest.\n\n**Section ordering:**\n\nIf the document structure is clear:\nAsk which section they'd like to start with.\n\nSuggest starting with whichever section has the most unknowns. For decision docs, that's usually the core proposal. For specs, it's typically the technical approach. Summary sections are best left for last.\n\nIf user doesn't know what sections they need:\nBased on the type of document and template, suggest 3-5 sections appropriate for the doc type.\n\nAsk if this structure works, or if they want to adjust it.\n\n**Once structure is agreed:**\n\nCreate the initial document structure with placeholder text for all sections.\n\n**If access to artifacts is available:**\nUse `create_file` to create an artifact. This gives both Claude and the user a scaffold to work from.\n\nInform them that the initial structure with placeholders for all sections will be created.\n\nCreate artifact with all section headers and brief placeholder text like \"[To be written]\" or \"[Content here]\".\n\nProvide the scaffold link and indicate it's time to fill in each section.\n\n**If no access to artifacts:**\nCreate a markdown file in the working directory. Name it appropriately (e.g., `decision-doc.md`, `technical-spec.md`).\n\nInform them that the initial structure with placeholders for all sections will be created.\n\nCreate file with all section headers and placeholder text.\n\nConfirm the filename has been created and indicate it's time to fill in each section.\n\n**For each section:**\n\n### Step 1: Clarifying Questions\n\nAnnounce work will begin on the [SECTION NAME] section. Ask 5-10 clarifying questions about what should be included:\n\nGenerate 5-10 specific questions based on context and section purpose.\n\nInform them they can answer in shorthand or just indicate what's important to cover.\n\n### Step 2: Brainstorming\n\nFor the [SECTION NAME] section, brainstorm [5-20] things that might be included, depending on the section's complexity. Look for:\n- Context shared that might have been forgotten\n- Angles or considerations not yet mentioned\n\nGenerate 5-20 numbered options based on section complexity. At the end, offer to brainstorm more if they want additional options.\n\n### Step 3: Curation\n\nAsk which points should be kept, removed, or combined. Request brief justifications to help learn priorities for the next sections.\n\nProvide examples:\n- \"Keep 1,4,7,9\"\n- \"Remove 3 (duplicates 1)\"\n- \"Remove 6 (audience already knows this)\"\n- \"Combine 11 and 12\"\n\n**If user gives freeform feedback** (e.g., \"looks good\" or \"I like most of it but...\") instead of numbered selections, extract their preferences and proceed. Parse what they want kept/removed/changed and apply it.\n\n### Step 4: Gap Check\n\nBased on what they've selected, ask if there's anything important missing for the [SECTION NAME] section.\n\n### Step 5: Drafting\n\nUse `str_replace` to replace the placeholder text for this section with the actual drafted content.\n\nAnnounce the [SECTION NAME] section will be drafted now based on what they've selected.\n\n**If using artifacts:**\nAfter drafting, provide a link to the artifact.\n\nAsk them to read through it and indicate what to change. Note that being specific helps learning for the next sections.\n\n**If using a file (no artifacts):**\nAfter drafting, confirm completion.\n\nInform them the [SECTION NAME] section has been drafted in [filename]. Ask them to read through it and indicate what to change. Note that being specific helps learning for the next sections.\n\n**Key instruction for user (include when drafting the first section):**\nProvide a note: Instead of editing the doc directly, ask them to indicate what to change. This helps learning of their style for future sections. For example: \"Remove the X bullet - already covered by Y\" or \"Make the third paragraph more concise\".\n\n### Step 6: Iterative Refinement\n\nAs user provides feedback:\n- Use `str_replace` to make edits (never reprint the whole doc)\n- **If using artifacts:** Provide link to artifact after each edit\n- **If using files:** Just confirm edits are complete\n- If user edits doc directly and asks to read it: mentally note the changes they made and keep them in mind for future sections (this shows their preferences)\n\n**Continue iterating** until user is satisfied with the section.\n\n### Quality Checking\n\nAfter 3 consecutive iterations with no substantial changes, ask if anything can be removed without losing important information.\n\nWhen section is done, confirm [SECTION NAME] is complete. Ask if ready to move to the next section.\n\n**Repeat for all sections.**\n\n### Near Completion\n\nAs approaching completion (80%+ of sections done), announce intention to re-read the entire document and check for:\n- Flow and consistency across sections\n- Redundancy or contradictions\n- Anything that feels like \"slop\" or generic filler\n- Whether every sentence carries weight\n\nRead entire document and provide feedback.\n\n**When all sections are drafted and refined:**\nAnnounce all sections are drafted. Indicate intention to review the complete document one more time.\n\nReview for overall coherence, flow, completeness.\n\nProvide any final suggestions.\n\nAsk if ready to move to Reader Testing, or if they want to refine anything else.\n\n## Stage 3: Reader Testing\n\n**Goal:** Test the document with a fresh Claude (no context bleed) to verify it works for readers.\n\n**Instructions to user:**\nExplain that testing will now occur to see if the document actually works for readers. This catches blind spots - things that make sense to the authors but might confuse others.\n\n### Testing Approach\n\n**If access to sub-agents is available (e.g., in Claude Code):**\n\nPerform the testing directly without user involvement.\n\n### Step 1: Predict Reader Questions\n\nAnnounce intention to predict what questions readers might ask when trying to discover this document.\n\nGenerate 5-10 questions that readers would realistically ask.\n\n### Step 2: Test with Sub-Agent\n\nAnnounce that these questions will be tested with a fresh Claude instance (no context from this conversation).\n\nFor each question, invoke a sub-agent with just the document content and the question.\n\nSummarize what Reader Claude got right/wrong for each question.\n\n### Step 3: Run Additional Checks\n\nAnnounce additional checks will be performed.\n\nInvoke sub-agent to check for ambiguity, false assumptions, contradictions.\n\nSummarize any issues found.\n\n### Step 4: Report and Fix\n\nIf issues found:\nReport that Reader Claude struggled with specific issues.\n\nList the specific issues.\n\nIndicate intention to fix these gaps.\n\nLoop back to refinement for problematic sections.\n\n---\n\n**If no access to sub-agents (e.g., claude.ai web interface):**\n\nThe user will need to do the testing manually.\n\n### Step 1: Predict Reader Questions\n\nAsk what questions people might ask when trying to discover this document. What would they type into Claude.ai?\n\nGenerate 5-10 questions that readers would realistically ask.\n\n### Step 2: Setup Testing\n\nProvide testing instructions:\n1. Open a fresh Claude conversation: https://claude.ai\n2. Paste or share the document content (if using a shared doc platform with connectors enabled, provide the link)\n3. Ask Reader Claude the generated questions\n\nFor each question, instruct Reader Claude to provide:\n- The answer\n- Whether anything was ambiguous or unclear\n- What knowledge/context the doc assumes is already known\n\nCheck if Reader Claude gives correct answers or misinterprets anything.\n\n### Step 3: Additional Checks\n\nAlso ask Reader Claude:\n- \"What in this doc might be ambiguous or unclear to readers?\"\n- \"What knowledge or context does this doc assume readers already have?\"\n- \"Are there any internal contradictions or inconsistencies?\"\n\n### Step 4: Iterate Based on Results\n\nAsk what Reader Claude got wrong or struggled with. Indicate intention to fix those gaps.\n\nLoop back to refinement for any problematic sections.\n\n---\n\n### Exit Condition (Both Approaches)\n\nWhen Reader Claude consistently answers questions correctly and doesn't surface new gaps or ambiguities, the doc is ready.\n\n## Final Review\n\nWhen Reader Testing passes:\nAnnounce the doc has passed Reader Claude testing. Before completion:\n\n1. Recommend they do a final read-through themselves - they own this document and are responsible for its quality\n2. Suggest double-checking any facts, links, or technical details\n3. Ask them to verify it achieves the impact they wanted\n\nAsk if they want one more review, or if the work is done.\n\n**If user wants final review, provide it. Otherwise:**\nAnnounce document completion. Provide a few final tips:\n- Consider linking this conversation in an appendix so readers can see how the doc was developed\n- Use appendices to provide depth without bloating the main doc\n- Update the doc as feedback is received from real readers\n\n## Tips for Effective Guidance\n\n**Tone:**\n- Be direct and procedural\n- Explain rationale briefly when it affects user behavior\n- Don't try to \"sell\" the approach - just execute it\n\n**Handling Deviations:**\n- If user wants to skip a stage: Ask if they want to skip this and write freeform\n- If user seems frustrated: Acknowledge this is taking longer than expected. Suggest ways to move faster\n- Always give user agency to adjust the process\n\n**Context Management:**\n- Throughout, if context is missing on something mentioned, proactively ask\n- Don't let gaps accumulate - address them as they come up\n\n**Artifact Management:**\n- Use `create_file` for drafting full sections\n- Use `str_replace` for all edits\n- Provide artifact link after every change\n- Never use artifacts for brainstorming lists - that's just conversation\n\n**Quality over Speed:**\n- Don't rush through stages\n- Each iteration should make meaningful improvements\n- The goal is a document that actually works for readers\n",
        "skills/resume_optimizer/.claude/skills/docx/SKILL.md": "---\nname: docx\ndescription: \"Comprehensive document creation, editing, and analysis with support for tracked changes, comments, formatting preservation, and text extraction. When Claude needs to work with professional documents (.docx files) for: (1) Creating new documents, (2) Modifying or editing content, (3) Working with tracked changes, (4) Adding comments, or any other document tasks\"\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# DOCX creation, editing, and analysis\n\n## Overview\n\nA user may ask you to create, edit, or analyze the contents of a .docx file. A .docx file is essentially a ZIP archive containing XML files and other resources that you can read or edit. You have different tools and workflows available for different tasks.\n\n## Workflow Decision Tree\n\n### Reading/Analyzing Content\nUse \"Text extraction\" or \"Raw XML access\" sections below\n\n### Creating New Document\nUse \"Creating a new Word document\" workflow\n\n### Editing Existing Document\n- **Your own document + simple changes**\n  Use \"Basic OOXML editing\" workflow\n\n- **Someone else's document**\n  Use **\"Redlining workflow\"** (recommended default)\n\n- **Legal, academic, business, or government docs**\n  Use **\"Redlining workflow\"** (required)\n\n## Reading and analyzing content\n\n### Text extraction\nIf you just need to read the text contents of a document, you should convert the document to markdown using pandoc. Pandoc provides excellent support for preserving document structure and can show tracked changes:\n\n```bash\n# Convert document to markdown with tracked changes\npandoc --track-changes=all path-to-file.docx -o output.md\n# Options: --track-changes=accept/reject/all\n```\n\n### Raw XML access\nYou need raw XML access for: comments, complex formatting, document structure, embedded media, and metadata. For any of these features, you'll need to unpack a document and read its raw XML contents.\n\n#### Unpacking a file\n`python ooxml/scripts/unpack.py <office_file> <output_directory>`\n\n#### Key file structures\n* `word/document.xml` - Main document contents\n* `word/comments.xml` - Comments referenced in document.xml\n* `word/media/` - Embedded images and media files\n* Tracked changes use `<w:ins>` (insertions) and `<w:del>` (deletions) tags\n\n## Creating a new Word document\n\nWhen creating a new Word document from scratch, use **docx-js**, which allows you to create Word documents using JavaScript/TypeScript.\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`docx-js.md`](docx-js.md) (~500 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for detailed syntax, critical formatting rules, and best practices before proceeding with document creation.\n2. Create a JavaScript/TypeScript file using Document, Paragraph, TextRun components (You can assume all dependencies are installed, but if not, refer to the dependencies section below)\n3. Export as .docx using Packer.toBuffer()\n\n## Editing an existing Word document\n\nWhen editing an existing Word document, use the **Document library** (a Python library for OOXML manipulation). The library automatically handles infrastructure setup and provides methods for document manipulation. For complex scenarios, you can access the underlying DOM directly through the library.\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~600 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for the Document library API and XML patterns for directly editing document files.\n2. Unpack the document: `python ooxml/scripts/unpack.py <office_file> <output_directory>`\n3. Create and run a Python script using the Document library (see \"Document Library\" section in ooxml.md)\n4. Pack the final document: `python ooxml/scripts/pack.py <input_directory> <office_file>`\n\nThe Document library provides both high-level methods for common operations and direct DOM access for complex scenarios.\n\n## Redlining workflow for document review\n\nThis workflow allows you to plan comprehensive tracked changes using markdown before implementing them in OOXML. **CRITICAL**: For complete tracked changes, you must implement ALL changes systematically.\n\n**Batching Strategy**: Group related changes into batches of 3-10 changes. This makes debugging manageable while maintaining efficiency. Test each batch before moving to the next.\n\n**Principle: Minimal, Precise Edits**\nWhen implementing tracked changes, only mark text that actually changes. Repeating unchanged text makes edits harder to review and appears unprofessional. Break replacements into: [unchanged text] + [deletion] + [insertion] + [unchanged text]. Preserve the original run's RSID for unchanged text by extracting the `<w:r>` element from the original and reusing it.\n\nExample - Changing \"30 days\" to \"60 days\" in a sentence:\n```python\n# BAD - Replaces entire sentence\n'<w:del><w:r><w:delText>The term is 30 days.</w:delText></w:r></w:del><w:ins><w:r><w:t>The term is 60 days.</w:t></w:r></w:ins>'\n\n# GOOD - Only marks what changed, preserves original <w:r> for unchanged text\n'<w:r w:rsidR=\"00AB12CD\"><w:t>The term is </w:t></w:r><w:del><w:r><w:delText>30</w:delText></w:r></w:del><w:ins><w:r><w:t>60</w:t></w:r></w:ins><w:r w:rsidR=\"00AB12CD\"><w:t> days.</w:t></w:r>'\n```\n\n### Tracked changes workflow\n\n1. **Get markdown representation**: Convert document to markdown with tracked changes preserved:\n   ```bash\n   pandoc --track-changes=all path-to-file.docx -o current.md\n   ```\n\n2. **Identify and group changes**: Review the document and identify ALL changes needed, organizing them into logical batches:\n\n   **Location methods** (for finding changes in XML):\n   - Section/heading numbers (e.g., \"Section 3.2\", \"Article IV\")\n   - Paragraph identifiers if numbered\n   - Grep patterns with unique surrounding text\n   - Document structure (e.g., \"first paragraph\", \"signature block\")\n   - **DO NOT use markdown line numbers** - they don't map to XML structure\n\n   **Batch organization** (group 3-10 related changes per batch):\n   - By section: \"Batch 1: Section 2 amendments\", \"Batch 2: Section 5 updates\"\n   - By type: \"Batch 1: Date corrections\", \"Batch 2: Party name changes\"\n   - By complexity: Start with simple text replacements, then tackle complex structural changes\n   - Sequential: \"Batch 1: Pages 1-3\", \"Batch 2: Pages 4-6\"\n\n3. **Read documentation and unpack**:\n   - **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~600 lines) completely from start to finish. **NEVER set any range limits when reading this file.** Pay special attention to the \"Document Library\" and \"Tracked Change Patterns\" sections.\n   - **Unpack the document**: `python ooxml/scripts/unpack.py <file.docx> <dir>`\n   - **Note the suggested RSID**: The unpack script will suggest an RSID to use for your tracked changes. Copy this RSID for use in step 4b.\n\n4. **Implement changes in batches**: Group changes logically (by section, by type, or by proximity) and implement them together in a single script. This approach:\n   - Makes debugging easier (smaller batch = easier to isolate errors)\n   - Allows incremental progress\n   - Maintains efficiency (batch size of 3-10 changes works well)\n\n   **Suggested batch groupings:**\n   - By document section (e.g., \"Section 3 changes\", \"Definitions\", \"Termination clause\")\n   - By change type (e.g., \"Date changes\", \"Party name updates\", \"Legal term replacements\")\n   - By proximity (e.g., \"Changes on pages 1-3\", \"Changes in first half of document\")\n\n   For each batch of related changes:\n\n   **a. Map text to XML**: Grep for text in `word/document.xml` to verify how text is split across `<w:r>` elements.\n\n   **b. Create and run script**: Use `get_node` to find nodes, implement changes, then `doc.save()`. See **\"Document Library\"** section in ooxml.md for patterns.\n\n   **Note**: Always grep `word/document.xml` immediately before writing a script to get current line numbers and verify text content. Line numbers change after each script run.\n\n5. **Pack the document**: After all batches are complete, convert the unpacked directory back to .docx:\n   ```bash\n   python ooxml/scripts/pack.py unpacked reviewed-document.docx\n   ```\n\n6. **Final verification**: Do a comprehensive check of the complete document:\n   - Convert final document to markdown:\n     ```bash\n     pandoc --track-changes=all reviewed-document.docx -o verification.md\n     ```\n   - Verify ALL changes were applied correctly:\n     ```bash\n     grep \"original phrase\" verification.md  # Should NOT find it\n     grep \"replacement phrase\" verification.md  # Should find it\n     ```\n   - Check that no unintended changes were introduced\n\n\n## Converting Documents to Images\n\nTo visually analyze Word documents, convert them to images using a two-step process:\n\n1. **Convert DOCX to PDF**:\n   ```bash\n   soffice --headless --convert-to pdf document.docx\n   ```\n\n2. **Convert PDF pages to JPEG images**:\n   ```bash\n   pdftoppm -jpeg -r 150 document.pdf page\n   ```\n   This creates files like `page-1.jpg`, `page-2.jpg`, etc.\n\nOptions:\n- `-r 150`: Sets resolution to 150 DPI (adjust for quality/size balance)\n- `-jpeg`: Output JPEG format (use `-png` for PNG if preferred)\n- `-f N`: First page to convert (e.g., `-f 2` starts from page 2)\n- `-l N`: Last page to convert (e.g., `-l 5` stops at page 5)\n- `page`: Prefix for output files\n\nExample for specific range:\n```bash\npdftoppm -jpeg -r 150 -f 2 -l 5 document.pdf page  # Converts only pages 2-5\n```\n\n## Code Style Guidelines\n**IMPORTANT**: When generating code for DOCX operations:\n- Write concise code\n- Avoid verbose variable names and redundant operations\n- Avoid unnecessary print statements\n\n## Dependencies\n\nRequired dependencies (install if not available):\n\n- **pandoc**: `sudo apt-get install pandoc` (for text extraction)\n- **docx**: `npm install -g docx` (for creating new documents)\n- **LibreOffice**: `sudo apt-get install libreoffice` (for PDF conversion)\n- **Poppler**: `sudo apt-get install poppler-utils` (for pdftoppm to convert PDF to images)\n- **defusedxml**: `pip install defusedxml` (for secure XML parsing)",
        "skills/resume_optimizer/.claude/skills/docx/docx-js.md": "# DOCX Library Tutorial\n\nGenerate .docx files with JavaScript/TypeScript.\n\n**Important: Read this entire document before starting.** Critical formatting rules and common pitfalls are covered throughout - skipping sections may result in corrupted files or rendering issues.\n\n## Setup\nAssumes docx is already installed globally\nIf not installed: `npm install -g docx`\n\n```javascript\nconst { Document, Packer, Paragraph, TextRun, Table, TableRow, TableCell, ImageRun, Media, \n        Header, Footer, AlignmentType, PageOrientation, LevelFormat, ExternalHyperlink, \n        InternalHyperlink, TableOfContents, HeadingLevel, BorderStyle, WidthType, TabStopType, \n        TabStopPosition, UnderlineType, ShadingType, VerticalAlign, SymbolRun, PageNumber,\n        FootnoteReferenceRun, Footnote, PageBreak } = require('docx');\n\n// Create & Save\nconst doc = new Document({ sections: [{ children: [/* content */] }] });\nPacker.toBuffer(doc).then(buffer => fs.writeFileSync(\"doc.docx\", buffer)); // Node.js\nPacker.toBlob(doc).then(blob => { /* download logic */ }); // Browser\n```\n\n## Text & Formatting\n```javascript\n// IMPORTANT: Never use \\n for line breaks - always use separate Paragraph elements\n//  WRONG: new TextRun(\"Line 1\\nLine 2\")\n//  CORRECT: new Paragraph({ children: [new TextRun(\"Line 1\")] }), new Paragraph({ children: [new TextRun(\"Line 2\")] })\n\n// Basic text with all formatting options\nnew Paragraph({\n  alignment: AlignmentType.CENTER,\n  spacing: { before: 200, after: 200 },\n  indent: { left: 720, right: 720 },\n  children: [\n    new TextRun({ text: \"Bold\", bold: true }),\n    new TextRun({ text: \"Italic\", italics: true }),\n    new TextRun({ text: \"Underlined\", underline: { type: UnderlineType.DOUBLE, color: \"FF0000\" } }),\n    new TextRun({ text: \"Colored\", color: \"FF0000\", size: 28, font: \"Arial\" }), // Arial default\n    new TextRun({ text: \"Highlighted\", highlight: \"yellow\" }),\n    new TextRun({ text: \"Strikethrough\", strike: true }),\n    new TextRun({ text: \"x2\", superScript: true }),\n    new TextRun({ text: \"H2O\", subScript: true }),\n    new TextRun({ text: \"SMALL CAPS\", smallCaps: true }),\n    new SymbolRun({ char: \"2022\", font: \"Symbol\" }), // Bullet \n    new SymbolRun({ char: \"00A9\", font: \"Arial\" })   // Copyright  - Arial for symbols\n  ]\n})\n```\n\n## Styles & Professional Formatting\n\n```javascript\nconst doc = new Document({\n  styles: {\n    default: { document: { run: { font: \"Arial\", size: 24 } } }, // 12pt default\n    paragraphStyles: [\n      // Document title style - override built-in Title style\n      { id: \"Title\", name: \"Title\", basedOn: \"Normal\",\n        run: { size: 56, bold: true, color: \"000000\", font: \"Arial\" },\n        paragraph: { spacing: { before: 240, after: 120 }, alignment: AlignmentType.CENTER } },\n      // IMPORTANT: Override built-in heading styles by using their exact IDs\n      { id: \"Heading1\", name: \"Heading 1\", basedOn: \"Normal\", next: \"Normal\", quickFormat: true,\n        run: { size: 32, bold: true, color: \"000000\", font: \"Arial\" }, // 16pt\n        paragraph: { spacing: { before: 240, after: 240 }, outlineLevel: 0 } }, // Required for TOC\n      { id: \"Heading2\", name: \"Heading 2\", basedOn: \"Normal\", next: \"Normal\", quickFormat: true,\n        run: { size: 28, bold: true, color: \"000000\", font: \"Arial\" }, // 14pt\n        paragraph: { spacing: { before: 180, after: 180 }, outlineLevel: 1 } },\n      // Custom styles use your own IDs\n      { id: \"myStyle\", name: \"My Style\", basedOn: \"Normal\",\n        run: { size: 28, bold: true, color: \"000000\" },\n        paragraph: { spacing: { after: 120 }, alignment: AlignmentType.CENTER } }\n    ],\n    characterStyles: [{ id: \"myCharStyle\", name: \"My Char Style\",\n      run: { color: \"FF0000\", bold: true, underline: { type: UnderlineType.SINGLE } } }]\n  },\n  sections: [{\n    properties: { page: { margin: { top: 1440, right: 1440, bottom: 1440, left: 1440 } } },\n    children: [\n      new Paragraph({ heading: HeadingLevel.TITLE, children: [new TextRun(\"Document Title\")] }), // Uses overridden Title style\n      new Paragraph({ heading: HeadingLevel.HEADING_1, children: [new TextRun(\"Heading 1\")] }), // Uses overridden Heading1 style\n      new Paragraph({ style: \"myStyle\", children: [new TextRun(\"Custom paragraph style\")] }),\n      new Paragraph({ children: [\n        new TextRun(\"Normal with \"),\n        new TextRun({ text: \"custom char style\", style: \"myCharStyle\" })\n      ]})\n    ]\n  }]\n});\n```\n\n**Professional Font Combinations:**\n- **Arial (Headers) + Arial (Body)** - Most universally supported, clean and professional\n- **Times New Roman (Headers) + Arial (Body)** - Classic serif headers with modern sans-serif body\n- **Georgia (Headers) + Verdana (Body)** - Optimized for screen reading, elegant contrast\n\n**Key Styling Principles:**\n- **Override built-in styles**: Use exact IDs like \"Heading1\", \"Heading2\", \"Heading3\" to override Word's built-in heading styles\n- **HeadingLevel constants**: `HeadingLevel.HEADING_1` uses \"Heading1\" style, `HeadingLevel.HEADING_2` uses \"Heading2\" style, etc.\n- **Include outlineLevel**: Set `outlineLevel: 0` for H1, `outlineLevel: 1` for H2, etc. to ensure TOC works correctly\n- **Use custom styles** instead of inline formatting for consistency\n- **Set a default font** using `styles.default.document.run.font` - Arial is universally supported\n- **Establish visual hierarchy** with different font sizes (titles > headers > body)\n- **Add proper spacing** with `before` and `after` paragraph spacing\n- **Use colors sparingly**: Default to black (000000) and shades of gray for titles and headings (heading 1, heading 2, etc.)\n- **Set consistent margins** (1440 = 1 inch is standard)\n\n\n## Lists (ALWAYS USE PROPER LISTS - NEVER USE UNICODE BULLETS)\n```javascript\n// Bullets - ALWAYS use the numbering config, NOT unicode symbols\n// CRITICAL: Use LevelFormat.BULLET constant, NOT the string \"bullet\"\nconst doc = new Document({\n  numbering: {\n    config: [\n      { reference: \"bullet-list\",\n        levels: [{ level: 0, format: LevelFormat.BULLET, text: \"\", alignment: AlignmentType.LEFT,\n          style: { paragraph: { indent: { left: 720, hanging: 360 } } } }] },\n      { reference: \"first-numbered-list\",\n        levels: [{ level: 0, format: LevelFormat.DECIMAL, text: \"%1.\", alignment: AlignmentType.LEFT,\n          style: { paragraph: { indent: { left: 720, hanging: 360 } } } }] },\n      { reference: \"second-numbered-list\", // Different reference = restarts at 1\n        levels: [{ level: 0, format: LevelFormat.DECIMAL, text: \"%1.\", alignment: AlignmentType.LEFT,\n          style: { paragraph: { indent: { left: 720, hanging: 360 } } } }] }\n    ]\n  },\n  sections: [{\n    children: [\n      // Bullet list items\n      new Paragraph({ numbering: { reference: \"bullet-list\", level: 0 },\n        children: [new TextRun(\"First bullet point\")] }),\n      new Paragraph({ numbering: { reference: \"bullet-list\", level: 0 },\n        children: [new TextRun(\"Second bullet point\")] }),\n      // Numbered list items\n      new Paragraph({ numbering: { reference: \"first-numbered-list\", level: 0 },\n        children: [new TextRun(\"First numbered item\")] }),\n      new Paragraph({ numbering: { reference: \"first-numbered-list\", level: 0 },\n        children: [new TextRun(\"Second numbered item\")] }),\n      //  CRITICAL: Different reference = INDEPENDENT list that restarts at 1\n      // Same reference = CONTINUES previous numbering\n      new Paragraph({ numbering: { reference: \"second-numbered-list\", level: 0 },\n        children: [new TextRun(\"Starts at 1 again (because different reference)\")] })\n    ]\n  }]\n});\n\n//  CRITICAL NUMBERING RULE: Each reference creates an INDEPENDENT numbered list\n// - Same reference = continues numbering (1, 2, 3... then 4, 5, 6...)\n// - Different reference = restarts at 1 (1, 2, 3... then 1, 2, 3...)\n// Use unique reference names for each separate numbered section!\n\n//  CRITICAL: NEVER use unicode bullets - they create fake lists that don't work properly\n// new TextRun(\" Item\")           // WRONG\n// new SymbolRun({ char: \"2022\" }) // WRONG\n//  ALWAYS use numbering config with LevelFormat.BULLET for real Word lists\n```\n\n## Tables\n```javascript\n// Complete table with margins, borders, headers, and bullet points\nconst tableBorder = { style: BorderStyle.SINGLE, size: 1, color: \"CCCCCC\" };\nconst cellBorders = { top: tableBorder, bottom: tableBorder, left: tableBorder, right: tableBorder };\n\nnew Table({\n  columnWidths: [4680, 4680], //  CRITICAL: Set column widths at table level - values in DXA (twentieths of a point)\n  margins: { top: 100, bottom: 100, left: 180, right: 180 }, // Set once for all cells\n  rows: [\n    new TableRow({\n      tableHeader: true,\n      children: [\n        new TableCell({\n          borders: cellBorders,\n          width: { size: 4680, type: WidthType.DXA }, // ALSO set width on each cell\n          //  CRITICAL: Always use ShadingType.CLEAR to prevent black backgrounds in Word.\n          shading: { fill: \"D5E8F0\", type: ShadingType.CLEAR }, \n          verticalAlign: VerticalAlign.CENTER,\n          children: [new Paragraph({ \n            alignment: AlignmentType.CENTER,\n            children: [new TextRun({ text: \"Header\", bold: true, size: 22 })]\n          })]\n        }),\n        new TableCell({\n          borders: cellBorders,\n          width: { size: 4680, type: WidthType.DXA }, // ALSO set width on each cell\n          shading: { fill: \"D5E8F0\", type: ShadingType.CLEAR },\n          children: [new Paragraph({ \n            alignment: AlignmentType.CENTER,\n            children: [new TextRun({ text: \"Bullet Points\", bold: true, size: 22 })]\n          })]\n        })\n      ]\n    }),\n    new TableRow({\n      children: [\n        new TableCell({\n          borders: cellBorders,\n          width: { size: 4680, type: WidthType.DXA }, // ALSO set width on each cell\n          children: [new Paragraph({ children: [new TextRun(\"Regular data\")] })]\n        }),\n        new TableCell({\n          borders: cellBorders,\n          width: { size: 4680, type: WidthType.DXA }, // ALSO set width on each cell\n          children: [\n            new Paragraph({ \n              numbering: { reference: \"bullet-list\", level: 0 },\n              children: [new TextRun(\"First bullet point\")] \n            }),\n            new Paragraph({ \n              numbering: { reference: \"bullet-list\", level: 0 },\n              children: [new TextRun(\"Second bullet point\")] \n            })\n          ]\n        })\n      ]\n    })\n  ]\n})\n```\n\n**IMPORTANT: Table Width & Borders**\n- Use BOTH `columnWidths: [width1, width2, ...]` array AND `width: { size: X, type: WidthType.DXA }` on each cell\n- Values in DXA (twentieths of a point): 1440 = 1 inch, Letter usable width = 9360 DXA (with 1\" margins)\n- Apply borders to individual `TableCell` elements, NOT the `Table` itself\n\n**Precomputed Column Widths (Letter size with 1\" margins = 9360 DXA total):**\n- **2 columns:** `columnWidths: [4680, 4680]` (equal width)\n- **3 columns:** `columnWidths: [3120, 3120, 3120]` (equal width)\n\n## Links & Navigation\n```javascript\n// TOC (requires headings) - CRITICAL: Use HeadingLevel only, NOT custom styles\n//  WRONG: new Paragraph({ heading: HeadingLevel.HEADING_1, style: \"customHeader\", children: [new TextRun(\"Title\")] })\n//  CORRECT: new Paragraph({ heading: HeadingLevel.HEADING_1, children: [new TextRun(\"Title\")] })\nnew TableOfContents(\"Table of Contents\", { hyperlink: true, headingStyleRange: \"1-3\" }),\n\n// External link\nnew Paragraph({\n  children: [new ExternalHyperlink({\n    children: [new TextRun({ text: \"Google\", style: \"Hyperlink\" })],\n    link: \"https://www.google.com\"\n  })]\n}),\n\n// Internal link & bookmark\nnew Paragraph({\n  children: [new InternalHyperlink({\n    children: [new TextRun({ text: \"Go to Section\", style: \"Hyperlink\" })],\n    anchor: \"section1\"\n  })]\n}),\nnew Paragraph({\n  children: [new TextRun(\"Section Content\")],\n  bookmark: { id: \"section1\", name: \"section1\" }\n}),\n```\n\n## Images & Media\n```javascript\n// Basic image with sizing & positioning\n// CRITICAL: Always specify 'type' parameter - it's REQUIRED for ImageRun\nnew Paragraph({\n  alignment: AlignmentType.CENTER,\n  children: [new ImageRun({\n    type: \"png\", // NEW REQUIREMENT: Must specify image type (png, jpg, jpeg, gif, bmp, svg)\n    data: fs.readFileSync(\"image.png\"),\n    transformation: { width: 200, height: 150, rotation: 0 }, // rotation in degrees\n    altText: { title: \"Logo\", description: \"Company logo\", name: \"Name\" } // IMPORTANT: All three fields are required\n  })]\n})\n```\n\n## Page Breaks\n```javascript\n// Manual page break\nnew Paragraph({ children: [new PageBreak()] }),\n\n// Page break before paragraph\nnew Paragraph({\n  pageBreakBefore: true,\n  children: [new TextRun(\"This starts on a new page\")]\n})\n\n//  CRITICAL: NEVER use PageBreak standalone - it will create invalid XML that Word cannot open\n//  WRONG: new PageBreak() \n//  CORRECT: new Paragraph({ children: [new PageBreak()] })\n```\n\n## Headers/Footers & Page Setup\n```javascript\nconst doc = new Document({\n  sections: [{\n    properties: {\n      page: {\n        margin: { top: 1440, right: 1440, bottom: 1440, left: 1440 }, // 1440 = 1 inch\n        size: { orientation: PageOrientation.LANDSCAPE },\n        pageNumbers: { start: 1, formatType: \"decimal\" } // \"upperRoman\", \"lowerRoman\", \"upperLetter\", \"lowerLetter\"\n      }\n    },\n    headers: {\n      default: new Header({ children: [new Paragraph({ \n        alignment: AlignmentType.RIGHT,\n        children: [new TextRun(\"Header Text\")]\n      })] })\n    },\n    footers: {\n      default: new Footer({ children: [new Paragraph({ \n        alignment: AlignmentType.CENTER,\n        children: [new TextRun(\"Page \"), new TextRun({ children: [PageNumber.CURRENT] }), new TextRun(\" of \"), new TextRun({ children: [PageNumber.TOTAL_PAGES] })]\n      })] })\n    },\n    children: [/* content */]\n  }]\n});\n```\n\n## Tabs\n```javascript\nnew Paragraph({\n  tabStops: [\n    { type: TabStopType.LEFT, position: TabStopPosition.MAX / 4 },\n    { type: TabStopType.CENTER, position: TabStopPosition.MAX / 2 },\n    { type: TabStopType.RIGHT, position: TabStopPosition.MAX * 3 / 4 }\n  ],\n  children: [new TextRun(\"Left\\tCenter\\tRight\")]\n})\n```\n\n## Constants & Quick Reference\n- **Underlines:** `SINGLE`, `DOUBLE`, `WAVY`, `DASH`\n- **Borders:** `SINGLE`, `DOUBLE`, `DASHED`, `DOTTED`  \n- **Numbering:** `DECIMAL` (1,2,3), `UPPER_ROMAN` (I,II,III), `LOWER_LETTER` (a,b,c)\n- **Tabs:** `LEFT`, `CENTER`, `RIGHT`, `DECIMAL`\n- **Symbols:** `\"2022\"` (), `\"00A9\"` (), `\"00AE\"` (), `\"2122\"` (), `\"00B0\"` (), `\"F070\"` (), `\"F0FC\"` ()\n\n## Critical Issues & Common Mistakes\n- **CRITICAL: PageBreak must ALWAYS be inside a Paragraph** - standalone PageBreak creates invalid XML that Word cannot open\n- **ALWAYS use ShadingType.CLEAR for table cell shading** - Never use ShadingType.SOLID (causes black background).\n- Measurements in DXA (1440 = 1 inch) | Each table cell needs 1 Paragraph | TOC requires HeadingLevel styles only\n- **ALWAYS use custom styles** with Arial font for professional appearance and proper visual hierarchy\n- **ALWAYS set a default font** using `styles.default.document.run.font` - Arial recommended\n- **ALWAYS use columnWidths array for tables** + individual cell widths for compatibility\n- **NEVER use unicode symbols for bullets** - always use proper numbering configuration with `LevelFormat.BULLET` constant (NOT the string \"bullet\")\n- **NEVER use \\n for line breaks anywhere** - always use separate Paragraph elements for each line\n- **ALWAYS use TextRun objects within Paragraph children** - never use text property directly on Paragraph\n- **CRITICAL for images**: ImageRun REQUIRES `type` parameter - always specify \"png\", \"jpg\", \"jpeg\", \"gif\", \"bmp\", or \"svg\"\n- **CRITICAL for bullets**: Must use `LevelFormat.BULLET` constant, not string \"bullet\", and include `text: \"\"` for the bullet character\n- **CRITICAL for numbering**: Each numbering reference creates an INDEPENDENT list. Same reference = continues numbering (1,2,3 then 4,5,6). Different reference = restarts at 1 (1,2,3 then 1,2,3). Use unique reference names for each separate numbered section!\n- **CRITICAL for TOC**: When using TableOfContents, headings must use HeadingLevel ONLY - do NOT add custom styles to heading paragraphs or TOC will break\n- **Tables**: Set `columnWidths` array + individual cell widths, apply borders to cells not table\n- **Set table margins at TABLE level** for consistent cell padding (avoids repetition per cell)",
        "skills/resume_optimizer/.claude/skills/docx/ooxml.md": "# Office Open XML Technical Reference\n\n**Important: Read this entire document before starting.** This document covers:\n- [Technical Guidelines](#technical-guidelines) - Schema compliance rules and validation requirements\n- [Document Content Patterns](#document-content-patterns) - XML patterns for headings, lists, tables, formatting, etc.\n- [Document Library (Python)](#document-library-python) - Recommended approach for OOXML manipulation with automatic infrastructure setup\n- [Tracked Changes (Redlining)](#tracked-changes-redlining) - XML patterns for implementing tracked changes\n\n## Technical Guidelines\n\n### Schema Compliance\n- **Element ordering in `<w:pPr>`**: `<w:pStyle>`, `<w:numPr>`, `<w:spacing>`, `<w:ind>`, `<w:jc>`\n- **Whitespace**: Add `xml:space='preserve'` to `<w:t>` elements with leading/trailing spaces\n- **Unicode**: Escape characters in ASCII content: `\"` becomes `&#8220;`\n  - **Character encoding reference**: Curly quotes `\"\"` become `&#8220;&#8221;`, apostrophe `'` becomes `&#8217;`, em-dash `` becomes `&#8212;`\n- **Tracked changes**: Use `<w:del>` and `<w:ins>` tags with `w:author=\"Claude\"` outside `<w:r>` elements\n  - **Critical**: `<w:ins>` closes with `</w:ins>`, `<w:del>` closes with `</w:del>` - never mix\n  - **RSIDs must be 8-digit hex**: Use values like `00AB1234` (only 0-9, A-F characters)\n  - **trackRevisions placement**: Add `<w:trackRevisions/>` after `<w:proofState>` in settings.xml\n- **Images**: Add to `word/media/`, reference in `document.xml`, set dimensions to prevent overflow\n\n## Document Content Patterns\n\n### Basic Structure\n```xml\n<w:p>\n  <w:r><w:t>Text content</w:t></w:r>\n</w:p>\n```\n\n### Headings and Styles\n```xml\n<w:p>\n  <w:pPr>\n    <w:pStyle w:val=\"Title\"/>\n    <w:jc w:val=\"center\"/>\n  </w:pPr>\n  <w:r><w:t>Document Title</w:t></w:r>\n</w:p>\n\n<w:p>\n  <w:pPr><w:pStyle w:val=\"Heading2\"/></w:pPr>\n  <w:r><w:t>Section Heading</w:t></w:r>\n</w:p>\n```\n\n### Text Formatting\n```xml\n<!-- Bold -->\n<w:r><w:rPr><w:b/><w:bCs/></w:rPr><w:t>Bold</w:t></w:r>\n<!-- Italic -->\n<w:r><w:rPr><w:i/><w:iCs/></w:rPr><w:t>Italic</w:t></w:r>\n<!-- Underline -->\n<w:r><w:rPr><w:u w:val=\"single\"/></w:rPr><w:t>Underlined</w:t></w:r>\n<!-- Highlight -->\n<w:r><w:rPr><w:highlight w:val=\"yellow\"/></w:rPr><w:t>Highlighted</w:t></w:r>\n```\n\n### Lists\n```xml\n<!-- Numbered list -->\n<w:p>\n  <w:pPr>\n    <w:pStyle w:val=\"ListParagraph\"/>\n    <w:numPr><w:ilvl w:val=\"0\"/><w:numId w:val=\"1\"/></w:numPr>\n    <w:spacing w:before=\"240\"/>\n  </w:pPr>\n  <w:r><w:t>First item</w:t></w:r>\n</w:p>\n\n<!-- Restart numbered list at 1 - use different numId -->\n<w:p>\n  <w:pPr>\n    <w:pStyle w:val=\"ListParagraph\"/>\n    <w:numPr><w:ilvl w:val=\"0\"/><w:numId w:val=\"2\"/></w:numPr>\n    <w:spacing w:before=\"240\"/>\n  </w:pPr>\n  <w:r><w:t>New list item 1</w:t></w:r>\n</w:p>\n\n<!-- Bullet list (level 2) -->\n<w:p>\n  <w:pPr>\n    <w:pStyle w:val=\"ListParagraph\"/>\n    <w:numPr><w:ilvl w:val=\"1\"/><w:numId w:val=\"1\"/></w:numPr>\n    <w:spacing w:before=\"240\"/>\n    <w:ind w:left=\"900\"/>\n  </w:pPr>\n  <w:r><w:t>Bullet item</w:t></w:r>\n</w:p>\n```\n\n### Tables\n```xml\n<w:tbl>\n  <w:tblPr>\n    <w:tblStyle w:val=\"TableGrid\"/>\n    <w:tblW w:w=\"0\" w:type=\"auto\"/>\n  </w:tblPr>\n  <w:tblGrid>\n    <w:gridCol w:w=\"4675\"/><w:gridCol w:w=\"4675\"/>\n  </w:tblGrid>\n  <w:tr>\n    <w:tc>\n      <w:tcPr><w:tcW w:w=\"4675\" w:type=\"dxa\"/></w:tcPr>\n      <w:p><w:r><w:t>Cell 1</w:t></w:r></w:p>\n    </w:tc>\n    <w:tc>\n      <w:tcPr><w:tcW w:w=\"4675\" w:type=\"dxa\"/></w:tcPr>\n      <w:p><w:r><w:t>Cell 2</w:t></w:r></w:p>\n    </w:tc>\n  </w:tr>\n</w:tbl>\n```\n\n### Layout\n```xml\n<!-- Page break before new section (common pattern) -->\n<w:p>\n  <w:r>\n    <w:br w:type=\"page\"/>\n  </w:r>\n</w:p>\n<w:p>\n  <w:pPr>\n    <w:pStyle w:val=\"Heading1\"/>\n  </w:pPr>\n  <w:r>\n    <w:t>New Section Title</w:t>\n  </w:r>\n</w:p>\n\n<!-- Centered paragraph -->\n<w:p>\n  <w:pPr>\n    <w:spacing w:before=\"240\" w:after=\"0\"/>\n    <w:jc w:val=\"center\"/>\n  </w:pPr>\n  <w:r><w:t>Centered text</w:t></w:r>\n</w:p>\n\n<!-- Font change - paragraph level (applies to all runs) -->\n<w:p>\n  <w:pPr>\n    <w:rPr><w:rFonts w:ascii=\"Courier New\" w:hAnsi=\"Courier New\"/></w:rPr>\n  </w:pPr>\n  <w:r><w:t>Monospace text</w:t></w:r>\n</w:p>\n\n<!-- Font change - run level (specific to this text) -->\n<w:p>\n  <w:r>\n    <w:rPr><w:rFonts w:ascii=\"Courier New\" w:hAnsi=\"Courier New\"/></w:rPr>\n    <w:t>This text is Courier New</w:t>\n  </w:r>\n  <w:r><w:t> and this text uses default font</w:t></w:r>\n</w:p>\n```\n\n## File Updates\n\nWhen adding content, update these files:\n\n**`word/_rels/document.xml.rels`:**\n```xml\n<Relationship Id=\"rId1\" Type=\"http://schemas.openxmlformats.org/officeDocument/2006/relationships/numbering\" Target=\"numbering.xml\"/>\n<Relationship Id=\"rId5\" Type=\"http://schemas.openxmlformats.org/officeDocument/2006/relationships/image\" Target=\"media/image1.png\"/>\n```\n\n**`[Content_Types].xml`:**\n```xml\n<Default Extension=\"png\" ContentType=\"image/png\"/>\n<Override PartName=\"/word/numbering.xml\" ContentType=\"application/vnd.openxmlformats-officedocument.wordprocessingml.numbering+xml\"/>\n```\n\n### Images\n**CRITICAL**: Calculate dimensions to prevent page overflow and maintain aspect ratio.\n\n```xml\n<!-- Minimal required structure -->\n<w:p>\n  <w:r>\n    <w:drawing>\n      <wp:inline>\n        <wp:extent cx=\"2743200\" cy=\"1828800\"/>\n        <wp:docPr id=\"1\" name=\"Picture 1\"/>\n        <a:graphic xmlns:a=\"http://schemas.openxmlformats.org/drawingml/2006/main\">\n          <a:graphicData uri=\"http://schemas.openxmlformats.org/drawingml/2006/picture\">\n            <pic:pic xmlns:pic=\"http://schemas.openxmlformats.org/drawingml/2006/picture\">\n              <pic:nvPicPr>\n                <pic:cNvPr id=\"0\" name=\"image1.png\"/>\n                <pic:cNvPicPr/>\n              </pic:nvPicPr>\n              <pic:blipFill>\n                <a:blip r:embed=\"rId5\"/>\n                <!-- Add for stretch fill with aspect ratio preservation -->\n                <a:stretch>\n                  <a:fillRect/>\n                </a:stretch>\n              </pic:blipFill>\n              <pic:spPr>\n                <a:xfrm>\n                  <a:ext cx=\"2743200\" cy=\"1828800\"/>\n                </a:xfrm>\n                <a:prstGeom prst=\"rect\"/>\n              </pic:spPr>\n            </pic:pic>\n          </a:graphicData>\n        </a:graphic>\n      </wp:inline>\n    </w:drawing>\n  </w:r>\n</w:p>\n```\n\n### Links (Hyperlinks)\n\n**IMPORTANT**: All hyperlinks (both internal and external) require the Hyperlink style to be defined in styles.xml. Without this style, links will look like regular text instead of blue underlined clickable links.\n\n**External Links:**\n```xml\n<!-- In document.xml -->\n<w:hyperlink r:id=\"rId5\">\n  <w:r>\n    <w:rPr><w:rStyle w:val=\"Hyperlink\"/></w:rPr>\n    <w:t>Link Text</w:t>\n  </w:r>\n</w:hyperlink>\n\n<!-- In word/_rels/document.xml.rels -->\n<Relationship Id=\"rId5\" Type=\"http://schemas.openxmlformats.org/officeDocument/2006/relationships/hyperlink\" \n              Target=\"https://www.example.com/\" TargetMode=\"External\"/>\n```\n\n**Internal Links:**\n\n```xml\n<!-- Link to bookmark -->\n<w:hyperlink w:anchor=\"myBookmark\">\n  <w:r>\n    <w:rPr><w:rStyle w:val=\"Hyperlink\"/></w:rPr>\n    <w:t>Link Text</w:t>\n  </w:r>\n</w:hyperlink>\n\n<!-- Bookmark target -->\n<w:bookmarkStart w:id=\"0\" w:name=\"myBookmark\"/>\n<w:r><w:t>Target content</w:t></w:r>\n<w:bookmarkEnd w:id=\"0\"/>\n```\n\n**Hyperlink Style (required in styles.xml):**\n```xml\n<w:style w:type=\"character\" w:styleId=\"Hyperlink\">\n  <w:name w:val=\"Hyperlink\"/>\n  <w:basedOn w:val=\"DefaultParagraphFont\"/>\n  <w:uiPriority w:val=\"99\"/>\n  <w:unhideWhenUsed/>\n  <w:rPr>\n    <w:color w:val=\"467886\" w:themeColor=\"hyperlink\"/>\n    <w:u w:val=\"single\"/>\n  </w:rPr>\n</w:style>\n```\n\n## Document Library (Python)\n\nUse the Document class from `scripts/document.py` for all tracked changes and comments. It automatically handles infrastructure setup (people.xml, RSIDs, settings.xml, comment files, relationships, content types). Only use direct XML manipulation for complex scenarios not supported by the library.\n\n**Working with Unicode and Entities:**\n- **Searching**: Both entity notation and Unicode characters work - `contains=\"&#8220;Company\"` and `contains=\"\\u201cCompany\"` find the same text\n- **Replacing**: Use either entities (`&#8220;`) or Unicode (`\\u201c`) - both work and will be converted appropriately based on the file's encoding (ascii  entities, utf-8  Unicode)\n\n### Initialization\n\n**Find the docx skill root** (directory containing `scripts/` and `ooxml/`):\n```bash\n# Search for document.py to locate the skill root\n# Note: /mnt/skills is used here as an example; check your context for the actual location\nfind /mnt/skills -name \"document.py\" -path \"*/docx/scripts/*\" 2>/dev/null | head -1\n# Example output: /mnt/skills/docx/scripts/document.py\n# Skill root is: /mnt/skills/docx\n```\n\n**Run your script with PYTHONPATH** set to the docx skill root:\n```bash\nPYTHONPATH=/mnt/skills/docx python your_script.py\n```\n\n**In your script**, import from the skill root:\n```python\nfrom scripts.document import Document, DocxXMLEditor\n\n# Basic initialization (automatically creates temp copy and sets up infrastructure)\ndoc = Document('unpacked')\n\n# Customize author and initials\ndoc = Document('unpacked', author=\"John Doe\", initials=\"JD\")\n\n# Enable track revisions mode\ndoc = Document('unpacked', track_revisions=True)\n\n# Specify custom RSID (auto-generated if not provided)\ndoc = Document('unpacked', rsid=\"07DC5ECB\")\n```\n\n### Creating Tracked Changes\n\n**CRITICAL**: Only mark text that actually changes. Keep ALL unchanged text outside `<w:del>`/`<w:ins>` tags. Marking unchanged text makes edits unprofessional and harder to review.\n\n**Attribute Handling**: The Document class auto-injects attributes (w:id, w:date, w:rsidR, w:rsidDel, w16du:dateUtc, xml:space) into new elements. When preserving unchanged text from the original document, copy the original `<w:r>` element with its existing attributes to maintain document integrity.\n\n**Method Selection Guide**:\n- **Adding your own changes to regular text**: Use `replace_node()` with `<w:del>`/`<w:ins>` tags, or `suggest_deletion()` for removing entire `<w:r>` or `<w:p>` elements\n- **Partially modifying another author's tracked change**: Use `replace_node()` to nest your changes inside their `<w:ins>`/`<w:del>`\n- **Completely rejecting another author's insertion**: Use `revert_insertion()` on the `<w:ins>` element (NOT `suggest_deletion()`)\n- **Completely rejecting another author's deletion**: Use `revert_deletion()` on the `<w:del>` element to restore deleted content using tracked changes\n\n```python\n# Minimal edit - change one word: \"The report is monthly\"  \"The report is quarterly\"\n# Original: <w:r w:rsidR=\"00AB12CD\"><w:rPr><w:rFonts w:ascii=\"Calibri\"/></w:rPr><w:t>The report is monthly</w:t></w:r>\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:r\", contains=\"The report is monthly\")\nrpr = tags[0].toxml() if (tags := node.getElementsByTagName(\"w:rPr\")) else \"\"\nreplacement = f'<w:r w:rsidR=\"00AB12CD\">{rpr}<w:t>The report is </w:t></w:r><w:del><w:r>{rpr}<w:delText>monthly</w:delText></w:r></w:del><w:ins><w:r>{rpr}<w:t>quarterly</w:t></w:r></w:ins>'\ndoc[\"word/document.xml\"].replace_node(node, replacement)\n\n# Minimal edit - change number: \"within 30 days\"  \"within 45 days\"\n# Original: <w:r w:rsidR=\"00XYZ789\"><w:rPr><w:rFonts w:ascii=\"Calibri\"/></w:rPr><w:t>within 30 days</w:t></w:r>\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:r\", contains=\"within 30 days\")\nrpr = tags[0].toxml() if (tags := node.getElementsByTagName(\"w:rPr\")) else \"\"\nreplacement = f'<w:r w:rsidR=\"00XYZ789\">{rpr}<w:t>within </w:t></w:r><w:del><w:r>{rpr}<w:delText>30</w:delText></w:r></w:del><w:ins><w:r>{rpr}<w:t>45</w:t></w:r></w:ins><w:r w:rsidR=\"00XYZ789\">{rpr}<w:t> days</w:t></w:r>'\ndoc[\"word/document.xml\"].replace_node(node, replacement)\n\n# Complete replacement - preserve formatting even when replacing all text\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:r\", contains=\"apple\")\nrpr = tags[0].toxml() if (tags := node.getElementsByTagName(\"w:rPr\")) else \"\"\nreplacement = f'<w:del><w:r>{rpr}<w:delText>apple</w:delText></w:r></w:del><w:ins><w:r>{rpr}<w:t>banana orange</w:t></w:r></w:ins>'\ndoc[\"word/document.xml\"].replace_node(node, replacement)\n\n# Insert new content (no attributes needed - auto-injected)\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:r\", contains=\"existing text\")\ndoc[\"word/document.xml\"].insert_after(node, '<w:ins><w:r><w:t>new text</w:t></w:r></w:ins>')\n\n# Partially delete another author's insertion\n# Original: <w:ins w:author=\"Jane Smith\" w:date=\"...\"><w:r><w:t>quarterly financial report</w:t></w:r></w:ins>\n# Goal: Delete only \"financial\" to make it \"quarterly report\"\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:ins\", attrs={\"w:id\": \"5\"})\n# IMPORTANT: Preserve w:author=\"Jane Smith\" on the outer <w:ins> to maintain authorship\nreplacement = '''<w:ins w:author=\"Jane Smith\" w:date=\"2025-01-15T10:00:00Z\">\n  <w:r><w:t>quarterly </w:t></w:r>\n  <w:del><w:r><w:delText>financial </w:delText></w:r></w:del>\n  <w:r><w:t>report</w:t></w:r>\n</w:ins>'''\ndoc[\"word/document.xml\"].replace_node(node, replacement)\n\n# Change part of another author's insertion\n# Original: <w:ins w:author=\"Jane Smith\"><w:r><w:t>in silence, safe and sound</w:t></w:r></w:ins>\n# Goal: Change \"safe and sound\" to \"soft and unbound\"\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:ins\", attrs={\"w:id\": \"8\"})\nreplacement = f'''<w:ins w:author=\"Jane Smith\" w:date=\"2025-01-15T10:00:00Z\">\n  <w:r><w:t>in silence, </w:t></w:r>\n</w:ins>\n<w:ins>\n  <w:r><w:t>soft and unbound</w:t></w:r>\n</w:ins>\n<w:ins w:author=\"Jane Smith\" w:date=\"2025-01-15T10:00:00Z\">\n  <w:del><w:r><w:delText>safe and sound</w:delText></w:r></w:del>\n</w:ins>'''\ndoc[\"word/document.xml\"].replace_node(node, replacement)\n\n# Delete entire run (use only when deleting all content; use replace_node for partial deletions)\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:r\", contains=\"text to delete\")\ndoc[\"word/document.xml\"].suggest_deletion(node)\n\n# Delete entire paragraph (in-place, handles both regular and numbered list paragraphs)\npara = doc[\"word/document.xml\"].get_node(tag=\"w:p\", contains=\"paragraph to delete\")\ndoc[\"word/document.xml\"].suggest_deletion(para)\n\n# Add new numbered list item\ntarget_para = doc[\"word/document.xml\"].get_node(tag=\"w:p\", contains=\"existing list item\")\npPr = tags[0].toxml() if (tags := target_para.getElementsByTagName(\"w:pPr\")) else \"\"\nnew_item = f'<w:p>{pPr}<w:r><w:t>New item</w:t></w:r></w:p>'\ntracked_para = DocxXMLEditor.suggest_paragraph(new_item)\ndoc[\"word/document.xml\"].insert_after(target_para, tracked_para)\n# Optional: add spacing paragraph before content for better visual separation\n# spacing = DocxXMLEditor.suggest_paragraph('<w:p><w:pPr><w:pStyle w:val=\"ListParagraph\"/></w:pPr></w:p>')\n# doc[\"word/document.xml\"].insert_after(target_para, spacing + tracked_para)\n```\n\n### Adding Comments\n\n```python\n# Add comment spanning two existing tracked changes\n# Note: w:id is auto-generated. Only search by w:id if you know it from XML inspection\nstart_node = doc[\"word/document.xml\"].get_node(tag=\"w:del\", attrs={\"w:id\": \"1\"})\nend_node = doc[\"word/document.xml\"].get_node(tag=\"w:ins\", attrs={\"w:id\": \"2\"})\ndoc.add_comment(start=start_node, end=end_node, text=\"Explanation of this change\")\n\n# Add comment on a paragraph\npara = doc[\"word/document.xml\"].get_node(tag=\"w:p\", contains=\"paragraph text\")\ndoc.add_comment(start=para, end=para, text=\"Comment on this paragraph\")\n\n# Add comment on newly created tracked change\n# First create the tracked change\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:r\", contains=\"old\")\nnew_nodes = doc[\"word/document.xml\"].replace_node(\n    node,\n    '<w:del><w:r><w:delText>old</w:delText></w:r></w:del><w:ins><w:r><w:t>new</w:t></w:r></w:ins>'\n)\n# Then add comment on the newly created elements\n# new_nodes[0] is the <w:del>, new_nodes[1] is the <w:ins>\ndoc.add_comment(start=new_nodes[0], end=new_nodes[1], text=\"Changed old to new per requirements\")\n\n# Reply to existing comment\ndoc.reply_to_comment(parent_comment_id=0, text=\"I agree with this change\")\n```\n\n### Rejecting Tracked Changes\n\n**IMPORTANT**: Use `revert_insertion()` to reject insertions and `revert_deletion()` to restore deletions using tracked changes. Use `suggest_deletion()` only for regular unmarked content.\n\n```python\n# Reject insertion (wraps it in deletion)\n# Use this when another author inserted text that you want to delete\nins = doc[\"word/document.xml\"].get_node(tag=\"w:ins\", attrs={\"w:id\": \"5\"})\nnodes = doc[\"word/document.xml\"].revert_insertion(ins)  # Returns [ins]\n\n# Reject deletion (creates insertion to restore deleted content)\n# Use this when another author deleted text that you want to restore\ndel_elem = doc[\"word/document.xml\"].get_node(tag=\"w:del\", attrs={\"w:id\": \"3\"})\nnodes = doc[\"word/document.xml\"].revert_deletion(del_elem)  # Returns [del_elem, new_ins]\n\n# Reject all insertions in a paragraph\npara = doc[\"word/document.xml\"].get_node(tag=\"w:p\", contains=\"paragraph text\")\nnodes = doc[\"word/document.xml\"].revert_insertion(para)  # Returns [para]\n\n# Reject all deletions in a paragraph\npara = doc[\"word/document.xml\"].get_node(tag=\"w:p\", contains=\"paragraph text\")\nnodes = doc[\"word/document.xml\"].revert_deletion(para)  # Returns [para]\n```\n\n### Inserting Images\n\n**CRITICAL**: The Document class works with a temporary copy at `doc.unpacked_path`. Always copy images to this temp directory, not the original unpacked folder.\n\n```python\nfrom PIL import Image\nimport shutil, os\n\n# Initialize document first\ndoc = Document('unpacked')\n\n# Copy image and calculate full-width dimensions with aspect ratio\nmedia_dir = os.path.join(doc.unpacked_path, 'word/media')\nos.makedirs(media_dir, exist_ok=True)\nshutil.copy('image.png', os.path.join(media_dir, 'image1.png'))\nimg = Image.open(os.path.join(media_dir, 'image1.png'))\nwidth_emus = int(6.5 * 914400)  # 6.5\" usable width, 914400 EMUs/inch\nheight_emus = int(width_emus * img.size[1] / img.size[0])\n\n# Add relationship and content type\nrels_editor = doc['word/_rels/document.xml.rels']\nnext_rid = rels_editor.get_next_rid()\nrels_editor.append_to(rels_editor.dom.documentElement,\n    f'<Relationship Id=\"{next_rid}\" Type=\"http://schemas.openxmlformats.org/officeDocument/2006/relationships/image\" Target=\"media/image1.png\"/>')\ndoc['[Content_Types].xml'].append_to(doc['[Content_Types].xml'].dom.documentElement,\n    '<Default Extension=\"png\" ContentType=\"image/png\"/>')\n\n# Insert image\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:p\", line_number=100)\ndoc[\"word/document.xml\"].insert_after(node, f'''<w:p>\n  <w:r>\n    <w:drawing>\n      <wp:inline distT=\"0\" distB=\"0\" distL=\"0\" distR=\"0\">\n        <wp:extent cx=\"{width_emus}\" cy=\"{height_emus}\"/>\n        <wp:docPr id=\"1\" name=\"Picture 1\"/>\n        <a:graphic xmlns:a=\"http://schemas.openxmlformats.org/drawingml/2006/main\">\n          <a:graphicData uri=\"http://schemas.openxmlformats.org/drawingml/2006/picture\">\n            <pic:pic xmlns:pic=\"http://schemas.openxmlformats.org/drawingml/2006/picture\">\n              <pic:nvPicPr><pic:cNvPr id=\"1\" name=\"image1.png\"/><pic:cNvPicPr/></pic:nvPicPr>\n              <pic:blipFill><a:blip r:embed=\"{next_rid}\"/><a:stretch><a:fillRect/></a:stretch></pic:blipFill>\n              <pic:spPr><a:xfrm><a:ext cx=\"{width_emus}\" cy=\"{height_emus}\"/></a:xfrm><a:prstGeom prst=\"rect\"><a:avLst/></a:prstGeom></pic:spPr>\n            </pic:pic>\n          </a:graphicData>\n        </a:graphic>\n      </wp:inline>\n    </w:drawing>\n  </w:r>\n</w:p>''')\n```\n\n### Getting Nodes\n\n```python\n# By text content\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:p\", contains=\"specific text\")\n\n# By line range\npara = doc[\"word/document.xml\"].get_node(tag=\"w:p\", line_number=range(100, 150))\n\n# By attributes\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:del\", attrs={\"w:id\": \"1\"})\n\n# By exact line number (must be line number where tag opens)\npara = doc[\"word/document.xml\"].get_node(tag=\"w:p\", line_number=42)\n\n# Combine filters\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:r\", line_number=range(40, 60), contains=\"text\")\n\n# Disambiguate when text appears multiple times - add line_number range\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:r\", contains=\"Section\", line_number=range(2400, 2500))\n```\n\n### Saving\n\n```python\n# Save with automatic validation (copies back to original directory)\ndoc.save()  # Validates by default, raises error if validation fails\n\n# Save to different location\ndoc.save('modified-unpacked')\n\n# Skip validation (debugging only - needing this in production indicates XML issues)\ndoc.save(validate=False)\n```\n\n### Direct DOM Manipulation\n\nFor complex scenarios not covered by the library:\n\n```python\n# Access any XML file\neditor = doc[\"word/document.xml\"]\neditor = doc[\"word/comments.xml\"]\n\n# Direct DOM access (defusedxml.minidom.Document)\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:p\", line_number=5)\nparent = node.parentNode\nparent.removeChild(node)\nparent.appendChild(node)  # Move to end\n\n# General document manipulation (without tracked changes)\nold_node = doc[\"word/document.xml\"].get_node(tag=\"w:p\", contains=\"original text\")\ndoc[\"word/document.xml\"].replace_node(old_node, \"<w:p><w:r><w:t>replacement text</w:t></w:r></w:p>\")\n\n# Multiple insertions - use return value to maintain order\nnode = doc[\"word/document.xml\"].get_node(tag=\"w:r\", line_number=100)\nnodes = doc[\"word/document.xml\"].insert_after(node, \"<w:r><w:t>A</w:t></w:r>\")\nnodes = doc[\"word/document.xml\"].insert_after(nodes[-1], \"<w:r><w:t>B</w:t></w:r>\")\nnodes = doc[\"word/document.xml\"].insert_after(nodes[-1], \"<w:r><w:t>C</w:t></w:r>\")\n# Results in: original_node, A, B, C\n```\n\n## Tracked Changes (Redlining)\n\n**Use the Document class above for all tracked changes.** The patterns below are for reference when constructing replacement XML strings.\n\n### Validation Rules\nThe validator checks that the document text matches the original after reverting Claude's changes. This means:\n- **NEVER modify text inside another author's `<w:ins>` or `<w:del>` tags**\n- **ALWAYS use nested deletions** to remove another author's insertions\n- **Every edit must be properly tracked** with `<w:ins>` or `<w:del>` tags\n\n### Tracked Change Patterns\n\n**CRITICAL RULES**:\n1. Never modify the content inside another author's tracked changes. Always use nested deletions.\n2. **XML Structure**: Always place `<w:del>` and `<w:ins>` at paragraph level containing complete `<w:r>` elements. Never nest inside `<w:r>` elements - this creates invalid XML that breaks document processing.\n\n**Text Insertion:**\n```xml\n<w:ins w:id=\"1\" w:author=\"Claude\" w:date=\"2025-07-30T23:05:00Z\" w16du:dateUtc=\"2025-07-31T06:05:00Z\">\n  <w:r w:rsidR=\"00792858\">\n    <w:t>inserted text</w:t>\n  </w:r>\n</w:ins>\n```\n\n**Text Deletion:**\n```xml\n<w:del w:id=\"2\" w:author=\"Claude\" w:date=\"2025-07-30T23:05:00Z\" w16du:dateUtc=\"2025-07-31T06:05:00Z\">\n  <w:r w:rsidDel=\"00792858\">\n    <w:delText>deleted text</w:delText>\n  </w:r>\n</w:del>\n```\n\n**Deleting Another Author's Insertion (MUST use nested structure):**\n```xml\n<!-- Nest deletion inside the original insertion -->\n<w:ins w:author=\"Jane Smith\" w:id=\"16\">\n  <w:del w:author=\"Claude\" w:id=\"40\">\n    <w:r><w:delText>monthly</w:delText></w:r>\n  </w:del>\n</w:ins>\n<w:ins w:author=\"Claude\" w:id=\"41\">\n  <w:r><w:t>weekly</w:t></w:r>\n</w:ins>\n```\n\n**Restoring Another Author's Deletion:**\n```xml\n<!-- Leave their deletion unchanged, add new insertion after it -->\n<w:del w:author=\"Jane Smith\" w:id=\"50\">\n  <w:r><w:delText>within 30 days</w:delText></w:r>\n</w:del>\n<w:ins w:author=\"Claude\" w:id=\"51\">\n  <w:r><w:t>within 30 days</w:t></w:r>\n</w:ins>\n```",
        "skills/resume_optimizer/.claude/skills/fetch-library-docs/SKILL.md": "---\nname: fetch-library-docs\ndescription: Fetches official documentation for external libraries and frameworks (React, Next.js, Prisma, FastAPI, Express, Tailwind, MongoDB, etc.) with 60-90% token savings via content-type filtering. Use this skill when implementing features using library APIs, debugging library-specific errors, troubleshooting configuration issues, installing or setting up frameworks, integrating third-party packages, upgrading between library versions, or looking up correct API patterns and best practices. Triggers automatically during coding work - fetch docs before writing library code to get correct patterns, not after guessing wrong.\n---\n\n# Library Documentation Skill\n\nFetches official library documentation with 60-90% token savings.\n\n---\n\n## WHEN TO INVOKE (Auto-Detection)\n\n**INVOKE AUTOMATICALLY when:**\n\n| Context | Detection Signal | Content Type |\n|---------|------------------|--------------|\n| **Implementing** | About to write code using library API | `examples,api-ref` |\n| **Debugging** | Error contains library name (e.g., `PrismaClientError`) | `troubleshooting` |\n| **Installing** | Adding new package, `npm install`, setup task | `setup` |\n| **Integrating** | Connecting libraries (\"use X with Y\") | `examples,setup` |\n| **Upgrading** | Version migration, breaking changes | `migration` |\n| **Uncertain** | First use of library feature, unsure of pattern | `examples` |\n\n**DO NOT INVOKE when:**\n- Already have sufficient knowledge from training\n- User pasted docs or has them open\n- Task is about local/private code (use codebase search)\n- Comparing libraries (use web search)\n\n---\n\n## DECISION LOGIC\n\n### 1. Identify Library\n\n```\nPriority: User mention  Error message  File imports  package.json  Ask user\n```\n\nExamples:\n- `PrismaClientKnownRequestError`  library = \"prisma\"\n- `import { useState } from 'react'`  library = \"react\"\n- `from fastapi import FastAPI`  library = \"fastapi\"\n\n### 2. Identify Topic\n\n```\nPriority: User specifies  Error message  Feature being implemented  \"getting started\"\n```\n\n### 3. Select Content Type\n\n| Task | Content Type |\n|------|--------------|\n| Implementing code | `examples,api-ref` |\n| Debugging error | `troubleshooting,examples` |\n| Installing/setup | `setup` |\n| Integrating libs | `examples,setup` |\n| Upgrading version | `migration` |\n| Understanding why | `concepts` |\n| Best practices | `patterns` |\n\n---\n\n## EXECUTION\n\n```bash\n# With known library ID (faster - saves 1 API call)\nbash scripts/fetch-docs.sh --library-id <id> --topic \"<topic>\" --content-type <types>\n\n# With library name (auto-resolves)\nbash scripts/fetch-docs.sh --library <name> --topic \"<topic>\" --content-type <types>\n```\n\n### Quick Library IDs\n\n| Library | ID |\n|---------|----|\n| React | `/reactjs/react.dev` |\n| Next.js | `/vercel/next.js` |\n| Prisma | `/prisma/docs` |\n| Tailwind | `/tailwindlabs/tailwindcss.com` |\n| FastAPI | `/tiangolo/fastapi` |\n\nSee [references/library-ids.md](references/library-ids.md) for complete list.\n\n---\n\n## ERROR HANDLING (Quick Reference)\n\n| Error | Action |\n|-------|--------|\n| `[LIBRARY_NOT_FOUND]` | Try spelling variations |\n| `[LIBRARY_MISMATCH]` | Use --library-id directly |\n| `[EMPTY_RESULTS]` | Broaden topic or use `--content-type all` |\n| `[RATE_LIMIT_ERROR]` | Check API key setup |\n\n**Call Budget**: Context7 allows 3 calls/question. Use `--library-id` to save 1 call.\n\nSee [references/context7-tools.md](references/context7-tools.md) for full error handling.\n\n---\n\n## REFERENCES\n\n- [Library IDs](references/library-ids.md) - Complete library ID list\n- [Usage Patterns](references/patterns.md) - Real-world examples\n- [Context7 Tools](references/context7-tools.md) - API details, error codes, setup\n",
        "skills/resume_optimizer/.claude/skills/fetch-library-docs/references/context7-tools.md": "# Context7 MCP Tools\n\n*2 tools available*\n\n** Context7 has a 3-call limit per question.** Plan your calls carefully:\n- Use `--library-id` when you know the ID (saves 1 call)\n- The fetch-docs.sh skill uses max 2 calls, leaving 1 for your retry\n\n## `resolve-library-id`\n\nResolves a package/product name to a Context7-compatible library ID and returns a list of matching libraries.\n\nYou MUST call this function before 'query-docs' to obtain a valid Context7-compatible library ID UNLESS the user explicitly provides a library ID in the format '/org/project' or '/org/project/version' in their query.\n\n### Selection Process\n\n1. Analyze the query to understand what library/package the user is looking for\n2. Return the most relevant match based on:\n   - Name similarity to the query (exact matches prioritized)\n   - Description relevance to the query's intent\n   - Documentation coverage (prioritize libraries with higher Code Snippet counts)\n   - Source reputation (consider libraries with High or Medium reputation more authoritative)\n   - Benchmark Score: Quality indicator (100 is the highest score)\n\n### Parameters\n\n- **`query`** (`string`) *(required)*: The user's original question or task. Used to rank library results by relevance.\n\n- **`libraryName`** (`string`) *(required)*: Library name to search for and retrieve a Context7-compatible library ID.\n\n### Response Format\n\nReturns a list of matching libraries with:\n- Title\n- Context7-compatible library ID (e.g., `/reactjs/react.dev`)\n- Code Snippets count\n- Source Reputation (High/Medium/Low)\n- Benchmark Score\n- Description\n\n### Examples\n\n```bash\n# Find React library\npython scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t resolve-library-id \\\n  -p '{\"query\": \"useState hooks\", \"libraryName\": \"react\"}'\n\n# Find Next.js library\npython scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t resolve-library-id \\\n  -p '{\"query\": \"routing\", \"libraryName\": \"next.js\"}'\n```\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"query\": {\n      \"type\": \"string\",\n      \"description\": \"The user's original question or task. Used to rank library results by relevance.\"\n    },\n    \"libraryName\": {\n      \"type\": \"string\",\n      \"description\": \"Library name to search for and retrieve a Context7-compatible library ID.\"\n    }\n  },\n  \"required\": [\"query\", \"libraryName\"]\n}\n```\n</details>\n\n## `query-docs`\n\nRetrieves and queries up-to-date documentation and code examples from Context7 for any programming library or framework.\n\nYou must call 'resolve-library-id' first to obtain the exact Context7-compatible library ID required to use this tool, UNLESS the user explicitly provides a library ID in the format '/org/project' or '/org/project/version' in their query.\n\n### Parameters\n\n- **`libraryId`** (`string`) *(required)*: Exact Context7-compatible library ID (e.g., '/mongodb/docs', '/vercel/next.js') retrieved from 'resolve-library-id' or directly from user query.\n\n- **`query`** (`string`) *(required)*: The question or task you need help with. Be specific and include relevant details. Good: 'How to set up authentication with JWT in Express.js'. Bad: 'auth'.\n\n### Examples\n\n```bash\n# Get React hooks documentation\npython scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t query-docs \\\n  -p '{\"libraryId\": \"/reactjs/react.dev\", \"query\": \"useState hooks examples\"}'\n\n# Get Next.js routing information\npython scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t query-docs \\\n  -p '{\"libraryId\": \"/vercel/next.js\", \"query\": \"how does routing work\"}'\n\n# Get MongoDB aggregation examples\npython scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t query-docs \\\n  -p '{\"libraryId\": \"/mongodb/docs\", \"query\": \"aggregation pipeline examples\"}'\n```\n\n<details>\n<summary>Full Schema</summary>\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"libraryId\": {\n      \"type\": \"string\",\n      \"description\": \"Exact Context7-compatible library ID (e.g., '/mongodb/docs', '/vercel/next.js') retrieved from 'resolve-library-id' or directly from user query.\"\n    },\n    \"query\": {\n      \"type\": \"string\",\n      \"description\": \"The question or task you need help with. Be specific and include relevant details.\"\n    }\n  },\n  \"required\": [\"libraryId\", \"query\"]\n}\n```\n</details>\n\n## Usage Patterns\n\n### Pattern 1: Unknown Library\n\nWhen you don't know the exact library ID:\n\n```bash\n# Step 1: Resolve library name\npython scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t resolve-library-id -p '{\"query\": \"middleware\", \"libraryName\": \"express\"}'\n\n# Step 2: Use returned ID to fetch docs\npython scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t query-docs \\\n  -p '{\"libraryId\": \"/expressjs/express\", \"query\": \"how to use middleware\"}'\n```\n\n### Pattern 2: Known Library ID\n\nWhen you know the library ID:\n\n```bash\n# Direct fetch (skip resolve step)\npython scripts/mcp-client.py call -s \"npx -y @upstash/context7-mcp\" \\\n  -t query-docs \\\n  -p '{\"libraryId\": \"/reactjs/react.dev\", \"query\": \"useState examples\"}'\n```\n\n### Pattern 3: Using the Shell Pipeline (Recommended)\n\nFor token-efficient documentation fetching:\n\n```bash\n# Code examples (default, 60-70% token savings)\nbash scripts/fetch-docs.sh --library react --topic useState --content-type examples\n\n# Multiple content types\nbash scripts/fetch-docs.sh --library react --topic useState --content-type examples,api-ref\n\n# With verbose output to see savings\nbash scripts/fetch-docs.sh --library react --topic useState --content-type examples --verbose\n```\n\n**Content types available:** `examples`, `api-ref`, `setup`, `concepts`, `migration`, `troubleshooting`, `patterns`, `all`\n\n## Common Library IDs\n\nQuick reference for popular libraries:\n\n| Library | Context7 ID |\n|---------|-------------|\n| React | `/reactjs/react.dev` or `/websites/react_dev` |\n| Next.js | `/vercel/next.js` |\n| Express | `/expressjs/express` |\n| MongoDB | `/mongodb/docs` |\n| Prisma | `/prisma/docs` |\n| Vue | `/vuejs/docs` |\n| Svelte | `/sveltejs/svelte.dev` |\n| FastAPI | `/tiangolo/fastapi` |\n| Django | `/django/docs` |\n\n## Error Handling\n\n### Error Codes\n\n| Error Code | Meaning | Call Cost | Action |\n|------------|---------|-----------|--------|\n| `[LIBRARY_NOT_FOUND]` | Library name didn't resolve | 1 call | Try spelling variations |\n| `[LIBRARY_MISMATCH]` | Resolved to wrong library | 1 call | Use --library-id directly |\n| `[INVALID_LIBRARY_ID]` | Bad ID format | 0 calls | Fix to `/org/project` format |\n| `[EMPTY_RESULTS]` | No docs for query | 1-2 calls | Broaden topic or --content-type all |\n| `[RATE_LIMIT_ERROR]` | Context7 limit hit | N/A | Wait, check API key |\n| `[FETCH_FAILED_AFTER_RETRIES]` | Network issues | 0 calls | Safe to retry |\n\n### Retry Logic\n\n- **Infrastructure failures** (timeout, network): Auto-retries with exponential backoff (2s, 5s, 10s)\n- **API errors** (rate limit, auth): No retry (would waste call budget)\n\n---\n\n## API Key Setup\n\nRequired for reliable access. Get free key at [context7.com/dashboard](https://context7.com/dashboard).\n\n### Setup Methods\n\n**Method 1: Config File** (Recommended - persistent)\n```bash\n# User-level (applies to all projects)\necho \"CONTEXT7_API_KEY=ctx7sk_your_key\" > ~/.context7.env\n\n# Project-level (add to .gitignore!)\necho \"CONTEXT7_API_KEY=ctx7sk_your_key\" > .context7.env\n```\n\n**Method 2: Environment Variable**\n```bash\n# macOS/Linux\nexport CONTEXT7_API_KEY=\"ctx7sk_your_key\"\n\n# Windows (PowerShell)\n$env:CONTEXT7_API_KEY = \"ctx7sk_your_key\"\n```\n\n### Check Status\n\n```bash\nbash scripts/fetch-docs.sh --api-status\n```\n\n### Priority Order\n\n1. `CONTEXT7_API_KEY` environment variable (highest)\n2. `.context7.env` in current directory (project-level)\n3. `~/.context7.env` in home directory (user-level)\n\n---\n\n## Parameters Reference\n\n```bash\nbash scripts/fetch-docs.sh [OPTIONS]\n```\n\n| Parameter | Required | Description |\n|-----------|----------|-------------|\n| `--library <name>` | One of these | Library name (uses 2 calls for resolution) |\n| `--library-id <id>` | required | Direct ID like `/vercel/next.js` (uses 1 call) |\n| `--topic <topic>` | Recommended | Feature to focus on |\n| `--content-type <types>` | Optional | Comma-separated: examples, api-ref, setup, concepts, migration, troubleshooting, patterns, all |\n| `--max-items <num>` | Optional | Max items per type (default: 5) |\n| `--verbose` | Optional | Show token savings stats |\n| `--api-status` | Optional | Check API key configuration |\n\n---\n\n## Tips\n\n1. **Save API Calls**: Use `--library-id` when you know the ID (saves 1 of your 3 calls)\n2. **Specific Queries**: More specific queries yield better results\n3. **Use Shell Pipeline**: `fetch-docs.sh` provides 60-90% token savings through content-type filtering\n4. **Match Content Type**: Use `examples` for code, `api-ref` for parameters, `setup` for installation\n5. **Combine Types**: Use comma-separated types like `examples,api-ref` for comprehensive output\n6. **Fallback**: If no results, try `--content-type all` or broader topics\n",
        "skills/resume_optimizer/.claude/skills/fetch-library-docs/references/library-ids.md": "# Common Library IDs\n\nUse `--library-id` instead of `--library` to save 1 API call.\n\n## JavaScript / TypeScript\n\n| Library | Context7 ID |\n|---------|-------------|\n| React | `/reactjs/react.dev` |\n| Next.js | `/vercel/next.js` |\n| Vue | `/vuejs/docs` |\n| Svelte | `/sveltejs/svelte.dev` |\n| Angular | `/angular/angular` |\n| Express | `/expressjs/express` |\n| Fastify | `/fastify/fastify` |\n| NestJS | `/nestjs/docs` |\n| Prisma | `/prisma/docs` |\n| Drizzle | `/drizzle-team/drizzle-orm` |\n| tRPC | `/trpc/trpc` |\n| Zod | `/colinhacks/zod` |\n| Tailwind CSS | `/tailwindlabs/tailwindcss.com` |\n| TypeScript | `/microsoft/typescript` |\n\n## Python\n\n| Library | Context7 ID |\n|---------|-------------|\n| FastAPI | `/tiangolo/fastapi` |\n| Django | `/django/docs` |\n| Flask | `/pallets/flask` |\n| SQLAlchemy | `/sqlalchemy/sqlalchemy` |\n| Pydantic | `/pydantic/pydantic` |\n| LangChain | `/langchain-ai/langchain` |\n\n## Databases\n\n| Library | Context7 ID |\n|---------|-------------|\n| MongoDB | `/mongodb/docs` |\n| PostgreSQL | `/postgres/postgres` |\n| Redis | `/redis/redis-doc` |\n| Supabase | `/supabase/supabase` |\n\n## Cloud / DevOps\n\n| Library | Context7 ID |\n|---------|-------------|\n| Docker | `/docker/docs` |\n| Kubernetes | `/kubernetes/website` |\n| Terraform | `/hashicorp/terraform` |\n| AWS CDK | `/aws/aws-cdk` |\n\n## Finding New Library IDs\n\nIf a library isn't listed, use `--library <name>` and the skill will resolve it. Check the verbose output for the resolved ID to use next time:\n\n```bash\nbash scripts/fetch-docs.sh --library \"your-library\" --topic \"getting started\" --verbose\n```\n",
        "skills/resume_optimizer/.claude/skills/fetch-library-docs/references/patterns.md": "# Usage Patterns\n\nReal-world examples for common scenarios.\n\n## Pattern A: Implementing React Feature\n\nBuilding a form component with hooks:\n\n```bash\nbash scripts/fetch-docs.sh --library-id /reactjs/react.dev \\\n  --topic \"form handling useState\" \\\n  --content-type examples,api-ref\n```\n\n## Pattern B: Debugging Prisma Error\n\nError: `PrismaClientKnownRequestError: Unique constraint failed`\n\n```bash\nbash scripts/fetch-docs.sh --library-id /prisma/docs \\\n  --topic \"unique constraint error\" \\\n  --content-type troubleshooting,examples\n```\n\n## Pattern C: Setting Up Next.js Auth\n\nAdding authentication to Next.js app:\n\n```bash\nbash scripts/fetch-docs.sh --library-id /vercel/next.js \\\n  --topic \"authentication middleware\" \\\n  --content-type setup,examples\n```\n\n## Pattern D: Upgrading Tailwind\n\nMigrating from Tailwind v3 to v4:\n\n```bash\nbash scripts/fetch-docs.sh --library tailwind \\\n  --topic \"upgrade v3 v4\" \\\n  --content-type migration\n```\n\n## Pattern E: Best Practices Query\n\nShould I use server components?\n\n```bash\nbash scripts/fetch-docs.sh --library-id /vercel/next.js \\\n  --topic \"server components\" \\\n  --content-type patterns,concepts\n```\n\n## Pattern F: Library Integration\n\nConnecting Prisma with Next.js API routes:\n\n```bash\nbash scripts/fetch-docs.sh --library-id /prisma/docs \\\n  --topic \"next.js integration api routes\" \\\n  --content-type examples,setup\n```\n\n## Pattern G: Installing Framework\n\nSetting up Tailwind in new project:\n\n```bash\nbash scripts/fetch-docs.sh --library-id /tailwindlabs/tailwindcss.com \\\n  --topic \"installation next.js\" \\\n  --content-type setup\n```\n\n## Pattern H: API Reference Lookup\n\nQuick lookup for function parameters:\n\n```bash\nbash scripts/fetch-docs.sh --library-id /reactjs/react.dev \\\n  --topic \"useEffect\" \\\n  --content-type api-ref\n```\n\n## Content Type Quick Reference\n\n| Content Type | Use For |\n|--------------|---------|\n| `examples` | Code samples, syntax patterns |\n| `api-ref` | Function signatures, parameters |\n| `setup` | Installation, configuration |\n| `concepts` | Understanding how/why |\n| `troubleshooting` | Debugging, error fixes |\n| `migration` | Version upgrades, breaking changes |\n| `patterns` | Best practices, recommendations |\n| `all` | Full exploration (no filtering) |\n",
        "skills/resume_optimizer/.claude/skills/internal-comms/SKILL.md": "---\nname: internal-comms\ndescription: A set of resources to help me write all kinds of internal communications, using the formats that my company likes to use. Claude should use this skill whenever asked to write some sort of internal communications (status reports, leadership updates, 3P updates, company newsletters, FAQs, incident reports, project updates, etc.).\nlicense: Complete terms in LICENSE.txt\n---\n\n## When to use this skill\nTo write internal communications, use this skill for:\n- 3P updates (Progress, Plans, Problems)\n- Company newsletters\n- FAQ responses\n- Status reports\n- Leadership updates\n- Project updates\n- Incident reports\n\n## How to use this skill\n\nTo write any internal communication:\n\n1. **Identify the communication type** from the request\n2. **Load the appropriate guideline file** from the `examples/` directory:\n    - `examples/3p-updates.md` - For Progress/Plans/Problems team updates\n    - `examples/company-newsletter.md` - For company-wide newsletters\n    - `examples/faq-answers.md` - For answering frequently asked questions\n    - `examples/general-comms.md` - For anything else that doesn't explicitly match one of the above\n3. **Follow the specific instructions** in that file for formatting, tone, and content gathering\n\nIf the communication type doesn't match any existing guideline, ask for clarification or more context about the desired format.\n\n## Keywords\n3P updates, company newsletter, company comms, weekly update, faqs, common questions, updates, internal comms\n",
        "skills/resume_optimizer/.claude/skills/internal-comms/examples/3p-updates.md": "## Instructions\nYou are being asked to write a 3P update. 3P updates stand for \"Progress, Plans, Problems.\" The main audience is for executives, leadership, other teammates, etc. They're meant to be very succinct and to-the-point: think something you can read in 30-60sec or less. They're also for people with some, but not a lot of context on what the team does.\n\n3Ps can cover a team of any size, ranging all the way up to the entire company. The bigger the team, the less granular the tasks should be. For example, \"mobile team\" might have \"shipped feature\" or \"fixed bugs,\" whereas the company might have really meaty 3Ps, like \"hired 20 new people\" or \"closed 10 new deals.\" \n\nThey represent the work of the team across a time period, almost always one week. They include three sections:\n1) Progress: what the team has accomplished over the next time period. Focus mainly on things shipped, milestones achieved, tasks created, etc.\n2) Plans: what the team plans to do over the next time period. Focus on what things are top-of-mind, really high priority, etc. for the team.\n3) Problems: anything that is slowing the team down. This could be things like too few people, bugs or blockers that are preventing the team from moving forward, some deal that fell through, etc.\n\nBefore writing them, make sure that you know the team name. If it's not specified, you can ask explicitly what the team name you're writing for is.\n\n\n## Tools Available\nWhenever possible, try to pull from available sources to get the information you need:\n- Slack: posts from team members with their updates - ideally look for posts in large channels with lots of reactions\n- Google Drive: docs written from critical team members with lots of views\n- Email: emails with lots of responses of lots of content that seems relevant\n- Calendar: non-recurring meetings that have a lot of importance, like product reviews, etc.\n\n\nTry to gather as much context as you can, focusing on the things that covered the time period you're writing for:\n- Progress: anything between a week ago and today\n- Plans: anything from today to the next week\n- Problems: anything between a week ago and today\n\n\nIf you don't have access, you can ask the user for things they want to cover. They might also include these things to you directly, in which case you're mostly just formatting for this particular format.\n\n## Workflow\n\n1. **Clarify scope**: Confirm the team name and time period (usually past week for Progress/Problems, next\nweek for Plans)\n2. **Gather information**: Use available tools or ask the user directly\n3. **Draft the update**: Follow the strict formatting guidelines\n4. **Review**: Ensure it's concise (30-60 seconds to read) and data-driven\n\n## Formatting\n\nThe format is always the same, very strict formatting. Never use any formatting other than this. Pick an emoji that is fun and captures the vibe of the team and update.\n\n[pick an emoji] [Team Name] (Dates Covered, usually a week)\nProgress: [1-3 sentences of content]\nPlans: [1-3 sentences of content]\nProblems: [1-3 sentences of content]\n\nEach section should be no more than 1-3 sentences: clear, to the point. It should be data-driven, and generally include metrics where possible. The tone should be very matter-of-fact, not super prose-heavy.",
        "skills/resume_optimizer/.claude/skills/internal-comms/examples/company-newsletter.md": "## Instructions\nYou are being asked to write a company-wide newsletter update. You are meant to summarize the past week/month of a company in the form of a newsletter that the entire company will read. It should be maybe ~20-25 bullet points long. It will be sent via Slack and email, so make it consumable for that.\n\nIdeally it includes the following attributes:\n- Lots of links: pulling documents from Google Drive that are very relevant, linking to prominent Slack messages in announce channels and from executives, perhgaps referencing emails that went company-wide, highlighting significant things that have happened in the company.\n- Short and to-the-point: each bullet should probably be no longer than ~1-2 sentences\n- Use the \"we\" tense, as you are part of the company. Many of the bullets should say \"we did this\" or \"we did that\"\n\n## Tools to use\nIf you have access to the following tools, please try to use them. If not, you can also let the user know directly that their responses would be better if they gave them access.\n\n- Slack: look for messages in channels with lots of people, with lots of reactions or lots of responses within the thread\n- Email: look for things from executives that discuss company-wide announcements\n- Calendar: if there were meetings with large attendee lists, particularly things like All-Hands meetings, big company announcements, etc. If there were documents attached to those meetings, those are great links to include.\n- Documents: if there were new docs published in the last week or two that got a lot of attention, you can link them. These should be things like company-wide vision docs, plans for the upcoming quarter or half, things authored by critical executives, etc.\n- External press: if you see references to articles or press we've received over the past week, that could be really cool too.\n\nIf you don't have access to any of these things, you can ask the user for things they want to cover. In this case, you'll mostly just be polishing up and fitting to this format more directly.\n\n## Sections\nThe company is pretty big: 1000+ people. There are a variety of different teams and initiatives going on across the company. To make sure the update works well, try breaking it into sections of similar things. You might break into clusters like {product development, go to market, finance} or {recruiting, execution, vision}, or {external news, internal news} etc. Try to make sure the different areas of the company are highlighted well.\n\n## Prioritization\nFocus on:\n- Company-wide impact (not team-specific details)\n- Announcements from leadership\n- Major milestones and achievements\n- Information that affects most employees\n- External recognition or press\n\nAvoid:\n- Overly granular team updates (save those for 3Ps)\n- Information only relevant to small groups\n- Duplicate information already communicated\n\n## Example Formats\n\n:megaphone: Company Announcements\n- Announcement 1\n- Announcement 2\n- Announcement 3\n\n:dart: Progress on Priorities\n- Area 1\n    - Sub-area 1\n    - Sub-area 2\n    - Sub-area 3\n- Area 2\n    - Sub-area 1\n    - Sub-area 2\n    - Sub-area 3\n- Area 3\n    - Sub-area 1\n    - Sub-area 2\n    - Sub-area 3\n\n:pillar: Leadership Updates\n- Post 1\n- Post 2\n- Post 3\n\n:thread: Social Updates\n- Update 1\n- Update 2\n- Update 3\n",
        "skills/resume_optimizer/.claude/skills/internal-comms/examples/faq-answers.md": "## Instructions\nYou are an assistant for answering questions that are being asked across the company. Every week, there are lots of questions that get asked across the company, and your goal is to try to summarize what those questions are. We want our company to be well-informed and on the same page, so your job is to produce a set of frequently asked questions that our employees are asking and attempt to answer them. Your singular job is to do two things:\n\n- Find questions that are big sources of confusion for lots of employees at the company, generally about things that affect a large portion of the employee base\n- Attempt to give a nice summarized answer to that question in order to minimize confusion.\n\nSome examples of areas that may be interesting to folks: recent corporate events (fundraising, new executives, etc.), upcoming launches, hiring progress, changes to vision or focus, etc.\n\n\n## Tools Available\nYou should use the company's available tools, where communication and work happens. For most companies, it looks something like this:\n- Slack: questions being asked across the company - it could be questions in response to posts with lots of responses, questions being asked with lots of reactions or thumbs up to show support, or anything else to show that a large number of employees want to ask the same things\n- Email: emails with FAQs written directly in them can be a good source as well\n- Documents: docs in places like Google Drive, linked on calendar events, etc. can also be a good source of FAQs, either directly added or inferred based on the contents of the doc\n\n## Formatting\nThe formatting should be pretty basic:\n\n- *Question*: [insert question - 1 sentence]\n- *Answer*: [insert answer - 1-2 sentence]\n\n## Guidance\nMake sure you're being holistic in your questions. Don't focus too much on just the user in question or the team they are a part of, but try to capture the entire company. Try to be as holistic as you can in reading all the tools available, producing responses that are relevant to all at the company.\n\n## Answer Guidelines\n- Base answers on official company communications when possible\n- If information is uncertain, indicate that clearly\n- Link to authoritative sources (docs, announcements, emails)\n- Keep tone professional but approachable\n- Flag if a question requires executive input or official response",
        "skills/resume_optimizer/.claude/skills/internal-comms/examples/general-comms.md": "  ## Instructions\n  You are being asked to write internal company communication that doesn't fit into the standard formats (3P\n  updates, newsletters, or FAQs).\n\n  Before proceeding:\n  1. Ask the user about their target audience\n  2. Understand the communication's purpose\n  3. Clarify the desired tone (formal, casual, urgent, informational)\n  4. Confirm any specific formatting requirements\n\n  Use these general principles:\n  - Be clear and concise\n  - Use active voice\n  - Put the most important information first\n  - Include relevant links and references\n  - Match the company's communication style",
        "skills/resume_optimizer/.claude/skills/interview/SKILL.md": "---\nname: interview\ndescription: |\n  This skill conducts discovery conversations to understand user intent and agree on approach before taking action. It should be used when the user explicitly calls /interview, asks for recommendations, needs brainstorming, wants to clarify, or when the request could be misunderstood. Prevents building the wrong thing by uncovering WHY behind WHAT.\n---\n\n# Interview Skill\n\nPrevent building the wrong thing. Discover user's intent (WHY), validate assumptions, and agree on approach (WHAT) before taking action.\n\n## What This Skill Does\n\n- Discovers INTENT behind surface requests (WHY they want it)\n- Surfaces and validates AI's assumptions before acting\n- Explores solution options informed by intent\n- Reaches mutual agreement on both problem and solution\n- Works for any context: software, documents, brainstorming, automation\n\n## What This Skill Does NOT Do\n\n- Follow rigid scripts\n- Skip to implementation without understanding\n- Accept surface requests without exploring intent\n- Make assumptions without validating them\n\n---\n\n## Core Problem This Skill Solves\n\n**AI builds the wrong thing because it:**\n1. Takes surface requests literally without understanding intent\n2. Makes hidden assumptions it never validates\n3. Proceeds without confirming alignment\n\n**This skill ensures:**\n1. Intent (WHY) is discovered, not just request (WHAT)\n2. Assumptions are surfaced and validated\n3. Both problem and solution are agreed before proceeding\n\n---\n\n## The WHY + WHAT Model\n\n```\nSurface WHAT  Discover WHY  Surface Assumptions \nInformed WHAT  Agree on Both  Proceed\n```\n\n| Phase | Purpose | Example |\n|-------|---------|---------|\n| **Surface WHAT** | Capture initial request | \"Add dark mode\" |\n| **Discover WHY** | Uncover intent/problem | \"Eye strain for night workers\" |\n| **Surface Assumptions** | Expose AI's hidden assumptions | \"Assuming web app, not mobile\" |\n| **Informed WHAT** | Solution options based on WHY | \"Dark mode + auto-brightness + schedule\" |\n| **Agree on Both** | Confirm problem AND solution | \"Solving eye strain via dark mode with auto-switch\" |\n\n---\n\n## When to Trigger\n\n| Trigger | Example |\n|---------|---------|\n| Explicit invocation | `/interview`, \"let's clarify\" |\n| Request could be misunderstood | Ambiguous, complex, or multi-part requests |\n| Recommendations needed | \"What should I use for...\" |\n| Brainstorming | \"Help me think through...\" |\n| High-stakes work | Where wrong output wastes significant effort |\n\n**Don't over-trigger**: Simple, clear requests don't need full discovery.\n\n---\n\n## Discovery Flow\n\n### Before Starting\n\nGather available context before asking questions:\n\n| Source | Gather |\n|--------|--------|\n| **Conversation** | User's stated request, prior context |\n| **Available Context** | Information already shared in session |\n| **Skill References** | Question patterns from `references/` |\n\n### 1. Surface WHAT\n\nCapture the initial request clearly.\n\n```\n\"Let me make sure I understand - you're asking for [X]?\"\n```\n\n### 2. Discover WHY\n\n**This is the critical step most AI skips.**\n\nGo beyond WHAT to understand WHY:\n\n| Ask | To Discover |\n|-----|-------------|\n| \"What problem does this solve?\" | The real need |\n| \"Why now?\" | Urgency and context |\n| \"What happens if we don't do this?\" | Stakes and priority |\n| \"Who benefits and how?\" | Users and value |\n| \"What led to this request?\" | Background and triggers |\n\n**Techniques for WHY:**\n\n**Laddering** - Dig into abstract goals:\n```\n\"Dark mode\"  \"Why?\"  \"Eye strain\"  \"Why an issue?\"  \"Night shift workers\"\n```\n\n**5 Whys** - Uncover root need:\n```\n\"Export feature\"  Why?  \"Share reports\"  Why?  \"Stakeholder reviews\"  Root need\n```\n\n**Structuring Clarifications**:\n\nWhen presenting multiple questions, distinguish must-know from nice-to-know:\n\n```\n## Required Clarifications\n1. [Critical question - blocks progress]\n2. [Critical question - affects core approach]\n\n## Optional Clarifications (if relevant)\n3. [Nice-to-know - can assume reasonable default]\n\nNote: Keep to 1-4 questions per round. Build on answers.\n```\n\n### 3. Surface Assumptions\n\n**This prevents \"builds wrong thing.\"**\n\nAI always makes assumptions. Surface them explicitly:\n\n```\n\"I'm assuming:\n- This is for [platform/context]\n- Users are [type]\n- We need to support [X] but not [Y]\n- [Other assumption]\n\nAre these correct?\"\n```\n\n**Common hidden assumptions:**\n- Technology/platform\n- User expertise level\n- Scale/performance needs\n- Integration requirements\n- What's in vs out of scope\n\n### 4. Informed WHAT\n\nNow that WHY is clear, explore WHAT options:\n\n```\n\"Given that you need [WHY], we could:\n1. [Option A] - [trade-off]\n2. [Option B] - [trade-off]\n3. [Option C] - [trade-off]\n\nWhich fits your intent best?\"\n```\n\n**Key**: Options should address the WHY, not just the surface WHAT.\n\n### 5. Agree on Both\n\nConfirm understanding of BOTH problem and solution:\n\n```\n## Understanding\n\n**Problem (WHY)**: [What we're solving and why it matters]\n\n**Solution (WHAT)**: [What we'll build/do]\n\n**Key decisions**:\n- [Decision 1]\n- [Decision 2]\n\n**Not included**: [Explicit scope boundaries]\n\nDoes this capture it correctly?\n```\n\n**Only proceed after explicit confirmation.**\n\n---\n\n## Depth Check\n\nHow do you know understanding is deep enough?\n\n### Surface Understanding (NOT enough)\n- Can repeat what user asked for\n- Know the immediate request\n- Haven't explored why\n\n### Deep Understanding (ENOUGH)\n- [ ] Know WHY they want it, not just WHAT\n- [ ] Know what problem it solves\n- [ ] Assumptions are surfaced and validated\n- [ ] Know who benefits and how\n- [ ] Know what's explicitly out of scope\n- [ ] Could explain it to someone else accurately\n- [ ] User confirmed understanding is correct\n\n**Test**: If you proceeded now and built something, would user say \"yes, that's what I meant\" or \"no, you misunderstood\"?\n\n---\n\n## Assumption Categories\n\nSurface assumptions in these areas:\n\n| Category | Example Assumptions |\n|----------|---------------------|\n| **Context** | Platform, environment, existing systems |\n| **Users** | Who they are, expertise level, needs |\n| **Scale** | Volume, performance requirements |\n| **Scope** | What's included vs excluded |\n| **Quality** | Standards, constraints, requirements |\n| **Timeline** | Urgency, phases, dependencies |\n\n---\n\n## Anti-Patterns\n\n| Anti-Pattern | What Happens | Fix |\n|--------------|--------------|-----|\n| Skip WHY | Build wrong solution | Always ask why before how |\n| Hidden assumptions | Surprise misalignment | Surface and validate explicitly |\n| Accept surface request | Miss real need | Dig deeper with laddering/5 whys |\n| Proceed without confirm | Waste effort | Get explicit \"yes, proceed\" |\n| Over-question simple requests | Annoy user | Match depth to complexity |\n\n---\n\n## Tool Adaptation\n\nUse whatever tools are available:\n\n| Goal | Approach |\n|------|----------|\n| Ask questions | Interactive tools if available, otherwise conversation |\n| Research context | Web search if needed and available |\n| Present options | Structured choices if available |\n\nThe skill describes WHAT to do. The agent uses available tools.\n\n---\n\n## Output: Understanding Summary\n\nMatch formality to situation:\n\n**Quick** (simple requests):\n```\nGot it: [WHAT] to solve [WHY]\nProceeding with [approach]. Confirm?\n```\n\n**Standard** (most cases):\n```\n## Understanding\n\n**Problem (WHY)**: [Intent and problem being solved]\n**Solution (WHAT)**: [What we'll do]\n**Key points**: [Important details]\n**Not included**: [Scope boundaries]\n\nReady to proceed?\n```\n\n**Detailed** (complex work):\nSee `references/summary-templates.md`\n\n---\n\n## Quick Reference\n\n```\n1. Surface WHAT  \"You're asking for X?\"\n2. Discover WHY  \"What problem does this solve?\"\n3. Surface assumptions  \"I'm assuming A, B, C - correct?\"\n4. Informed WHAT  \"Given WHY, we could do X, Y, or Z\"\n5. Confirm both  \"So we're solving [WHY] by doing [WHAT]?\"\n6. Proceed  Only after explicit confirmation\n```\n\n---\n\n## Reference Files\n\n| File | Purpose |\n|------|---------|\n| `references/question-patterns.md` | Techniques for discovering WHY and surfacing assumptions |\n| `references/anti-patterns.md` | Common mistakes that lead to building wrong thing |\n| `references/summary-templates.md` | Output formats for different situations |\n",
        "skills/resume_optimizer/.claude/skills/interview/references/anti-patterns.md": "# Anti-Patterns\n\nCommon mistakes in discovery conversations and how to avoid them.\n\n---\n\n## Discovery Anti-Patterns\n\n### 1. Scripted Questioning\n\n**Symptom**: Following a fixed list of questions regardless of context.\n\n**Problem**: Misses the specific gaps in THIS situation. Wastes time on irrelevant questions.\n\n**Fix**:\n```\nIdentify what's ACTUALLY unclear in this request.\nGenerate questions to fill THOSE specific gaps.\nAdapt based on answers received.\n```\n\n### 2. Assuming User Lacks Expertise\n\n**Symptom**: Researching and explaining domain basics when user is an expert.\n\n**Problem**: Wastes user's time, feels condescending.\n\n**Fix**:\n```\nAsk: \"Do you have experience with [domain]?\"\nIf yes: Leverage their expertise, ask for their knowledge\nIf no: Then research and explain\n```\n\n### 3. Jumping to Implementation\n\n**Symptom**: Starting work before understanding is confirmed.\n\n**Problem**: Builds wrong thing, wastes effort, frustrates user.\n\n**Fix**:\n```\nAlways summarize understanding first.\nGet explicit confirmation: \"Does this capture what you want?\"\nOnly proceed after user confirms.\n```\n\n### 4. Solution Anchoring\n\n**Symptom**: Asking questions that validate a pre-decided solution.\n\n**Problem**: Misses better alternatives, biases the discovery.\n\n**Fix**:\n```\nExplore the problem space first.\nPresent multiple options when they exist.\nLet user's needs drive the solution.\n```\n\n---\n\n## Questioning Anti-Patterns\n\n### 5. Question Overload\n\n**Symptom**: Asking 5+ questions at once.\n\n**Problem**: Overwhelms user, gets rushed/incomplete answers.\n\n**Fix**:\n```\n1-4 questions maximum per round.\nBuild on answers before asking more.\nMost important questions first.\n```\n\n### 6. Leading Questions\n\n**Symptom**: \"Don't you think we should use X?\"\n\n**Problem**: User agrees without genuine consideration.\n\n**Fix**:\n```\nPresent options neutrally.\n\"We could use X, Y, or Z. What fits your needs?\"\nLet user choose, don't push.\n```\n\n### 7. Accepting Vagueness\n\n**Symptom**: User says \"make it fast\"  \"Okay, got it.\"\n\n**Problem**: \"Fast\" means different things. Undefined requirements cause problems later.\n\n**Fix**:\n```\nProbe vague terms:\n\"When you say 'fast', what response time is acceptable?\"\n\"What does 'easy to use' mean for your users?\"\nQuantify whenever possible.\n```\n\n### 8. Re-asking the Obvious\n\n**Symptom**: Asking about things user already explained.\n\n**Problem**: Shows AI wasn't listening. Wastes time.\n\n**Fix**:\n```\nTrack what's already clear from context.\nOnly ask about actual gaps.\nReference what user said: \"You mentioned X - should that include Y?\"\n```\n\n---\n\n## Context Anti-Patterns\n\n### 9. Codebase Assumption\n\n**Symptom**: Asking about code/files when user is brainstorming or starting fresh.\n\n**Problem**: Wrong context assumed. Confuses user.\n\n**Fix**:\n```\nAssess what context exists FIRST.\nFresh project?  Focus on goals, constraints\nExisting work?  Then examine what exists\nBrainstorming?  Explore possibilities\n```\n\n### 10. Specs-Only Mindset\n\n**Symptom**: Treating every discovery as \"requirements gathering.\"\n\n**Problem**: Not all work needs formal specs. Overkill for simple tasks.\n\n**Fix**:\n```\nMatch formality to situation:\n- Simple request  Quick clarification, proceed\n- Complex work  Thorough discovery\n- Brainstorming  Exploratory conversation\nGoal is understanding, not documentation.\n```\n\n### 11. Ignoring User's Time\n\n**Symptom**: Extensive questioning for simple requests.\n\n**Problem**: User wanted quick help, not an interview.\n\n**Fix**:\n```\nGauge complexity first.\nSimple + clear  Just do it\nAmbiguous  Clarify specific gaps only\nComplex  Full discovery warranted\n```\n\n---\n\n## Research Anti-Patterns\n\n### 12. Researching When User Knows\n\n**Symptom**: Web searching domain basics when user is the expert.\n\n**Problem**: Wastes time, ignores valuable user knowledge.\n\n**Fix**:\n```\nAsk about user's expertise first.\nExpert user  Ask for their knowledge\nNovice user  Then research helps\n```\n\n### 13. Skipping Research When Needed\n\n**Symptom**: Making recommendations without checking current best practices.\n\n**Problem**: Outdated or suboptimal advice.\n\n**Fix**:\n```\nIf user needs industry context  Research\nIf comparing approaches  Research current options\nIf user asks \"what's best\"  Research before recommending\n```\n\n---\n\n## Confirmation Anti-Patterns\n\n### 14. No Summary\n\n**Symptom**: Proceeding without confirming understanding.\n\n**Problem**: Misalignment discovered too late.\n\n**Fix**:\n```\nAlways summarize: \"Here's what I understand...\"\nAlways confirm: \"Does this capture what you want?\"\nOnly proceed after explicit confirmation.\n```\n\n### 15. Vague Summary\n\n**Symptom**: Summary is as vague as original request.\n\n**Problem**: Hasn't actually achieved clarity.\n\n**Fix**:\n```\nSummary should be SPECIFIC enough to act on.\nIf you can't be specific, you need more questions.\nTest: Could someone else implement from this summary?\n```\n\n---\n\n## Summary Table\n\n| Anti-Pattern | Key Fix |\n|--------------|---------|\n| Scripted questioning | Generate for THIS situation |\n| Assuming no expertise | Ask user first |\n| Jumping to implementation | Confirm understanding first |\n| Solution anchoring | Explore problem space first |\n| Question overload | 1-4 questions at a time |\n| Leading questions | Present options neutrally |\n| Accepting vagueness | Probe for specifics |\n| Re-asking obvious | Track what's clear |\n| Codebase assumption | Assess context first |\n| Specs-only mindset | Match formality to situation |\n| Ignoring user's time | Gauge complexity first |\n| Researching when user knows | Ask about expertise |\n| Skipping needed research | Research when context needed |\n| No summary | Always summarize and confirm |\n| Vague summary | Be specific enough to act on |\n",
        "skills/resume_optimizer/.claude/skills/interview/references/question-patterns.md": "# Question Patterns\n\nTechniques for discovering WHY (intent) and surfacing assumptions.\n\n---\n\n## Core Purpose\n\nQuestions serve two goals:\n1. **Discover WHY** - Uncover intent behind surface requests\n2. **Surface assumptions** - Expose and validate what AI is assuming\n\n---\n\n## Discovering WHY\n\n### The WHY Questions\n\n| Question | Discovers |\n|----------|-----------|\n| \"What problem does this solve?\" | Real need |\n| \"Why is this needed now?\" | Urgency, context |\n| \"What happens if we don't do this?\" | Stakes, priority |\n| \"Who benefits and how?\" | Users, value |\n| \"What led to this request?\" | Background, triggers |\n| \"What does success look like?\" | Goals, criteria |\n\n### Laddering Technique\n\nDig progressively deeper into abstract requests:\n\n```\nSurface request\n   \"Why do you need that?\"\n     Reason\n       \"Why is that important?\"\n         Deeper reason\n           Root intent\n```\n\n**Example:**\n```\n\"Add dark mode\"\n   Why?  \"Reduce eye strain\"\n     Why an issue?  \"Users work at night\"\n       Why at night?  \"Support team is 24/7\"\n         Intent: Accessibility for shift workers\n```\n\n### 5 Whys Technique\n\nKeep asking \"why\" until you hit root cause:\n\n```\nRequest  Why?  Reason  Why?  Deeper  Why?  Root\n```\n\n**Example:**\n```\n\"Add export to PDF\"\n   Why?  \"Need to share reports\"\n     Why share?  \"Stakeholders review weekly\"\n       Why PDF?  \"They print them\"\n         Root: Stakeholders need printable weekly reports\n```\n\n### Intent vs Solution\n\nUsers often state solutions, not problems:\n\n| User Says (Solution) | Ask to Find (Intent) |\n|---------------------|----------------------|\n| \"Add a button for X\" | \"What should happen when users do X?\" |\n| \"Make it faster\" | \"What's slow? What speed is acceptable?\" |\n| \"Use Redis\" | \"What problem are you solving with caching?\" |\n\n---\n\n## Surfacing Assumptions\n\n### Why Assumptions Matter\n\nAI always makes assumptions. Unvalidated assumptions  wrong output.\n\n### The Assumption Declaration\n\nAfter initial understanding, explicitly state assumptions:\n\n```\n\"I'm assuming:\n- [Assumption 1]\n- [Assumption 2]\n- [Assumption 3]\n\nAre these correct?\"\n```\n\n### Assumption Categories\n\n| Category | Example Questions |\n|----------|-------------------|\n| **Context** | \"Is this for web, mobile, or both?\" |\n| **Users** | \"Who are the users? Technical or non-technical?\" |\n| **Scale** | \"How many users/requests/items are we talking?\" |\n| **Scope** | \"Should this include X, or is that separate?\" |\n| **Integration** | \"Does this need to work with existing system Y?\" |\n| **Quality** | \"Any specific performance/security requirements?\" |\n| **Timeline** | \"Is there a deadline? Can this be phased?\" |\n\n### Probing Hidden Assumptions\n\nWhen user gives vague terms, probe:\n\n| Vague Term | Probe |\n|------------|-------|\n| \"Fast\" | \"What response time is acceptable?\" |\n| \"Scalable\" | \"What scale? 100 users? 1 million?\" |\n| \"Simple\" | \"Simple for whom? What complexity is okay?\" |\n| \"Secure\" | \"What threats? What compliance requirements?\" |\n| \"Good UX\" | \"What makes UX good for your users?\" |\n\n---\n\n## Question Types\n\n### Open Questions\n**Purpose**: Explore, gather context\n**Pattern**: \"What...\", \"How...\", \"Tell me about...\"\n**Use when**: Starting discovery, exploring WHY\n\n### Closed Questions\n**Purpose**: Confirm, validate\n**Pattern**: \"Is...\", \"Should...\", \"Do you want...\"\n**Use when**: Validating assumptions, confirming understanding\n\n### Choice Questions\n**Purpose**: Help decide between options\n**Pattern**: \"Would you prefer A, B, or C?\"\n**Use when**: Multiple valid approaches exist\n\n### Probing Questions\n**Purpose**: Dig deeper into vague answers\n**Pattern**: \"When you say X, what do you mean?\"\n**Use when**: Answer contains undefined terms\n\n---\n\n## Question Pacing\n\n**Batch size**: 1-4 questions maximum\n\n**Flow**:\n1. Start with WHY questions (most important)\n2. Surface assumptions based on answers\n3. Probe vague terms\n4. Confirm understanding\n\n**Adapt depth**:\n- Simple request  Quick check, proceed\n- Complex/ambiguous  Full discovery\n- High stakes  Thorough validation\n\n---\n\n## Signs You've Discovered Enough\n\n### Surface Understanding (NOT ready)\n- Know WHAT user asked for\n- Don't know WHY they want it\n- Have unvalidated assumptions\n\n### Deep Understanding (READY)\n- Know WHY (intent/problem)\n- Know WHAT (agreed solution)\n- Assumptions validated\n- Scope boundaries clear\n- User confirmed accuracy\n\n**Test**: Could you explain to someone else WHY user wants this and WHAT you'll do, without guessing?\n",
        "skills/resume_optimizer/.claude/skills/interview/references/summary-templates.md": "# Summary Templates\n\nFormats for capturing mutual understanding. Match formality to situation.\n\n---\n\n## Core Principle\n\n**Summaries confirm understanding, not create documentation.**\n\nThe goal is ensuring AI and user are aligned before proceeding. Keep it as simple as the situation allows.\n\n---\n\n## Standard Understanding Summary\n\nUse for most situations:\n\n```markdown\n## Understanding Summary\n\n**What we're doing**: [One clear statement]\n\n**Goals**: [What success looks like]\n\n**Key Points**:\n- [Important point 1]\n- [Important point 2]\n- [Important point 3]\n\n**Approach**: [How we'll proceed]\n\n**Boundaries**: [What's NOT included]\n\n**Open items**: [Any remaining uncertainties, or \"None\"]\n\n---\nReady to proceed?\n```\n\n---\n\n## Quick Summary\n\nFor simple, clear requests:\n\n```markdown\n**Got it**: [What user wants in one sentence]\n\n**I'll**: [What AI will do]\n\n**Confirm**: Ready to proceed?\n```\n\n---\n\n## Brainstorming Summary\n\nAfter exploratory conversations:\n\n```markdown\n## Where We Landed\n\n**Exploring**: [Topic/problem space]\n\n**Options Considered**:\n1. [Option] - [Trade-off]\n2. [Option] - [Trade-off]\n3. [Option] - [Trade-off]\n\n**Direction**: [What user is leaning toward]\n\n**Next Step**: [What happens now]\n\n**Still Open**: [Unresolved questions, if any]\n```\n\n---\n\n## Complex Work Summary\n\nFor multi-faceted projects:\n\n```markdown\n## Understanding Summary\n\n**Project**: [What this is]\n\n**Goal**: [What success looks like]\n\n### Key Elements\n1. [Element 1]: [Brief description]\n2. [Element 2]: [Brief description]\n3. [Element 3]: [Brief description]\n\n### Decisions Made\n| Decision | Choice |\n|----------|--------|\n| [Decision 1] | [What we chose] |\n| [Decision 2] | [What we chose] |\n\n### Constraints\n- [Constraint 1]\n- [Constraint 2]\n\n### Approach\n[How we'll proceed]\n\n### Not Included\n- [Out of scope 1]\n- [Out of scope 2]\n\n### Open Items\n- [Remaining question, if any]\n\n---\nDoes this capture what you want?\n```\n\n---\n\n## Decision Summary\n\nWhen helping user make a choice:\n\n```markdown\n## Decision Summary\n\n**Question**: [What we were deciding]\n\n**Options Evaluated**:\n| Option | Pros | Cons |\n|--------|------|------|\n| [A] | [Pros] | [Cons] |\n| [B] | [Pros] | [Cons] |\n\n**Decision**: [What user chose]\n\n**Rationale**: [Why this choice]\n\n**Next**: [What happens now]\n```\n\n---\n\n## Formality Guide\n\n| Situation | Template | Why |\n|-----------|----------|-----|\n| Simple request | Quick Summary | Don't over-formalize simple things |\n| Brainstorming | Brainstorming Summary | Capture exploration, not specs |\n| Clear medium task | Standard Summary | Right balance |\n| Complex/multi-part | Complex Summary | Need more structure |\n| Decision-making | Decision Summary | Document the choice |\n\n---\n\n## Summary Quality Check\n\nBefore presenting summary, verify:\n\n- [ ] **Specific**: Could someone act on this?\n- [ ] **Complete**: All key points captured?\n- [ ] **Accurate**: Reflects what user said?\n- [ ] **Scoped**: Boundaries are clear?\n- [ ] **Actionable**: Clear what happens next?\n\nIf you can't make it specific, you need more questions first.\n",
        "skills/resume_optimizer/.claude/skills/linkedin_job_resume_optimizer/SKILL.md": "---\nname: linkedin_job_resume_optimizer\ndescription: |\n  Automates end-to-end job search workflow: searches LinkedIn for AI-related remote jobs,\n  extracts job descriptions, reads base resume, generates ATS-optimized tailored resumes,\n  performs skill gap analysis, suggests interview prep questions, and provides LinkedIn\n  profile optimization recommendations. Use when: (1) User needs job-specific resume\n  tailoring, (2) Preparing for job applications with targeted optimization, (3) Requires\n  skill gap analysis between resume and job requirements, (4) Needs interview preparation\n  questions based on skill gaps.\nallowed-tools:\n  - Bash\n  - Read\n  - Write\n  - Glob\n  - Grep\nmodel: claude-sonnet-4-5\n---\n\n# LinkedIn Job Resume Optimizer\n\nComprehensive job search automation with resume tailoring and interview preparation.\n\n## Overview\n\nThis skill orchestrates a complete job application workflow:\n1. Search LinkedIn for relevant remote AI jobs\n2. Extract full job descriptions and requirements\n3. Analyze base resume to extract skills and experience\n4. Generate ATS-optimized, job-tailored resumes\n5. Perform skill gap analysis\n6. Generate targeted interview preparation questions\n7. Suggest LinkedIn profile optimizations\n\n## Workflow Diagram\n\n```\nUser Request\n    \nWorkflow Orchestrator\n    \n\n LinkedIn         Resume            ATS             \n Scraper          Analyzer          Optimizer       \n (Playwright)     (Pandoc+Docx)     (NLP)           \n\n    \nGap Analyzer (Compare skills)\n    \nQuestion Generator (Interview prep)\n    \nFinal Report (Summary + File locations)\n```\n\n## Prerequisites\n\n### 1. Playwright MCP Server (for LinkedIn automation)\n\nStart the server before running this skill:\n```bash\nbash /mnt/d/Ali_Home/Learning/AgenticAI/AI-P009/Assignments/ProjectA1/resume_optimizer/.claude/skills/browsing-with-playwright/scripts/start-server.sh\n```\n\n### 2. Base Resume\n\nEnsure resume exists at configured path (default: `/mnt/d/Ali_Home/Learning/AgenticAI/AI-P009/Assignments/ProjectA1/resume_optimizer/resume/resume06012026.docx`)\n\n### 3. System Dependencies\n\n```bash\n# Core tools\nsudo apt-get install pandoc python3 python3-pip nodejs npm\n\n# Optional (for PDF export)\nsudo apt-get install libreoffice poppler-utils\n```\n\n### 4. Python Packages\n\n```bash\npip install python-docx spacy nltk pandas requests\npython -m spacy download en_core_web_sm\npython -m nltk.downloader punkt stopwords\n```\n\n### 5. Node Packages\n\n```bash\nnpm install -g @playwright/mcp\n```\n\n## Quick Start\n\n### Basic Usage\n\n```bash\npython3 scripts/workflow_orchestrator.py \\\n  --job-keywords \"AI Engineer remote\" \\\n  --job-count 2\n```\n\n### With Custom Resume Path\n\n```bash\npython3 scripts/workflow_orchestrator.py \\\n  --resume-path \"/path/to/your/resume.docx\" \\\n  --job-keywords \"Machine Learning Engineer\" \\\n  --job-count 2 \\\n  --output-dir \"./custom_output\"\n```\n\n## Step-by-Step Workflow\n\n### Phase 1: Initialize Playwright Server\n\n```bash\nbash /mnt/d/Ali_Home/Learning/AgenticAI/AI-P009/Assignments/ProjectA1/resume_optimizer/.claude/skills/browsing-with-playwright/scripts/start-server.sh\n```\n\nVerify server is running:\n```bash\ncurl http://localhost:8808\n```\n\n### Phase 2: Search LinkedIn Jobs\n\n```bash\npython3 scripts/linkedin_scraper.py \\\n  --keywords \"AI Engineer remote\" \\\n  --count 2 \\\n  --output jobs.json\n```\n\n**Output**: `jobs.json` containing:\n```json\n[\n  {\n    \"title\": \"Senior AI Engineer\",\n    \"company\": \"TechCorp\",\n    \"url\": \"https://linkedin.com/jobs/view/12345\",\n    \"description\": \"Full job description...\",\n    \"required_skills\": [\"Python\", \"TensorFlow\", \"MLOps\"],\n    \"preferred_skills\": [\"Kubernetes\", \"AWS\"]\n  }\n]\n```\n\n### Phase 3: Extract Base Resume Skills\n\n```bash\n# Convert DOCX to markdown\npandoc --track-changes=accept /mnt/d/Ali_Home/Learning/AgenticAI/AI-P009/Assignments/ProjectA1/resume_optimizer/resume/resume06012026.docx -o resume.md\n\n# Extract skills\npython3 scripts/resume_analyzer.py --resume-md resume.md --output base_skills.json\n```\n\n**Output**: `base_skills.json` with skills inventory\n\n### Phase 4: Generate Tailored Resumes\n\nFor each job, generate ATS-optimized resume:\n```bash\npython3 scripts/ats_optimizer.py \\\n  --base-resume /mnt/d/Ali_Home/Learning/AgenticAI/AI-P009/Assignments/ProjectA1/resume_optimizer/resume/resume06012026.docx \\\n  --job-description jobs.json \\\n  --job-index 0 \\\n  --output \"resume_tailored_Senior_AI_Engineer.docx\"\n```\n\n### Phase 5: Perform Gap Analysis\n\n```bash\npython3 scripts/gap_analyzer.py \\\n  --base-skills base_skills.json \\\n  --job-requirements jobs.json \\\n  --output gap_analysis.json\n```\n\n**Output**: `gap_analysis.json`\n```json\n[\n  {\n    \"job_title\": \"Senior AI Engineer\",\n    \"skill_gaps\": [\n      {\n        \"skill\": \"Kubernetes\",\n        \"gap_type\": \"missing\",\n        \"importance\": \"high\",\n        \"related_experience\": [\"Docker experience in current role\"]\n      }\n    ],\n    \"interview_questions\": [\n      \"Explain your understanding of Kubernetes orchestration...\",\n      \"How would you design a Kubernetes deployment for ML models?\",\n      \"...\"\n    ],\n    \"linkedin_updates\": {\n      \"about_section\": \"Add: 'Experienced in containerization with Docker, transitioning to Kubernetes for production ML deployments'\",\n      \"skills_to_add\": [\"Kubernetes\", \"Container Orchestration\", \"MLOps\"]\n    }\n  }\n]\n```\n\n### Phase 6: Generate Interview Questions\n\n```bash\npython3 scripts/question_generator.py \\\n  --gap-analysis gap_analysis.json \\\n  --questions-per-gap 10 \\\n  --output interview_prep.md\n```\n\n### Phase 7: Cleanup\n\n```bash\nbash /mnt/d/Ali_Home/Learning/AgenticAI/AI-P009/Assignments/ProjectA1/resume_optimizer/.claude/skills/browsing-with-playwright/scripts/stop-server.sh\n```\n\n## Configuration\n\nEdit `config.json` in skill directory:\n\n```json\n{\n  \"resume_path\": \"/mnt/d/Ali_Home/Learning/AgenticAI/AI-P009/Assignments/ProjectA1/resume_optimizer/resume/resume06012026.docx\",\n  \"output_directory\": \"./resume_optimizer/output\",\n  \"playwright_port\": 8808,\n  \"job_search\": {\n    \"default_keywords\": \"AI Engineer remote\",\n    \"default_count\": 2,\n    \"location\": \"Remote\"\n  },\n  \"ats_optimization\": {\n    \"keyword_density_target\": 0.75,\n    \"exact_match_priority\": true,\n    \"preserve_formatting\": true\n  },\n  \"interview_prep\": {\n    \"questions_per_gap\": 10,\n    \"include_behavioral\": true,\n    \"include_technical\": true\n  }\n}\n```\n\n## ATS Optimization Strategies\n\nSee [references/ats-optimization-guide.md](references/ats-optimization-guide.md) for detailed strategies including:\n\n### Keyword Density\n- **Required Skills**: 70-80% coverage\n- **Preferred Skills**: 40-50% coverage\n- **Exact Match**: Use exact phrases from job description\n- **Context Integration**: Weave keywords naturally into experience descriptions\n\n### Keyword Placement Priority\n1. **Professional Summary** (highest ATS weight)\n   - Include job title keyword\n   - Top 3-5 required skills\n   - Industry-specific terminology\n\n2. **Skills Section** (exact match critical)\n   - Mirror job posting's skills terminology exactly\n   - Group by category if job posting does\n\n3. **Experience Descriptions** (context + keywords)\n   - Integrate keywords into achievement statements\n   - Use action verbs + keyword + quantifiable result\n\n### Natural Integration Techniques\n\n**Action Verb + Keyword + Result**:\n```\nBefore: \"Worked on machine learning projects\"\nAfter: \"Developed TensorFlow-based ML models, reducing inference time by 30%\"\n```\n\n**Technology Stack Enumeration**:\n```\nBefore: \"Built backend systems\"\nAfter: \"Built scalable backend systems using Python, Docker, and Kubernetes on AWS infrastructure\"\n```\n\n## LinkedIn Automation\n\nSee [references/linkedin-automation.md](references/linkedin-automation.md) for:\n\n- Navigation patterns for LinkedIn job search\n- Element selectors and snapshot references\n- Rate limiting and anti-bot detection strategies\n- Fallback strategies if elements change\n\n**Key Points**:\n- Implement random delays (1-3 seconds between actions)\n- Use `--shared-browser-context` flag for Playwright MCP\n- Handle rate limiting with exponential backoff\n- Parse snapshots for job card refs\n\n## Interview Question Generation\n\nSee [references/interview-prep-patterns.md](references/interview-prep-patterns.md) for:\n\n- STAR method question templates\n- Technical depth assessment questions\n- Behavioral questions for skill gaps\n- System design scenarios\n\n**Question Distribution**:\n- **Missing Skills**: 40% fundamentals, 30% transferable experience, 30% scenarios\n- **Weak Skills**: 40% depth assessment, 30% practical application, 30% troubleshooting\n\n## Error Handling\n\n### No Jobs Found\n```python\nif len(jobs) == 0:\n    print(\"No jobs found matching criteria. Suggestions:\")\n    print(\"- Broaden search keywords\")\n    print(\"- Remove 'remote' filter\")\n    print(\"- Try alternative job titles\")\n    sys.exit(1)\n```\n\n### Resume Read Errors\n```python\ntry:\n    resume_text = extract_resume(resume_path)\nexcept FileNotFoundError:\n    print(f\"Resume not found at {resume_path}\")\n    print(\"Please provide valid resume path with --resume-path\")\n    sys.exit(1)\nexcept Exception as e:\n    print(f\"Error reading resume: {e}\")\n    print(\"Ensure resume is valid .docx format\")\n    sys.exit(1)\n```\n\n### LinkedIn Rate Limiting\n```python\n# Implement exponential backoff\nfor attempt in range(3):\n    try:\n        job_data = scrape_job(url)\n        break\n    except RateLimitError:\n        wait_time = 2 ** attempt * 5  # 5s, 10s, 20s\n        time.sleep(wait_time)\n```\n\n### Playwright Connection Failures\n- Pre-flight check: Verify server running before job search\n- Auto-restart server if connection fails\n- Retry with exponential backoff (3 attempts)\n\n## Troubleshooting\n\n| Issue | Solution |\n|-------|----------|\n| Playwright not responding | Restart: `bash scripts/stop-server.sh && bash scripts/start-server.sh` |\n| LinkedIn blocks automation | Add wait times, use headless:false, rotate user agents |\n| Resume conversion fails | Verify pandoc installed: `pandoc --version` |\n| ATS optimization weak | Review keyword extraction, increase density target in config.json |\n| No skill gaps detected | Lower similarity threshold in gap_analyzer.py |\n| Python packages missing | Run: `pip install python-docx spacy nltk pandas requests` |\n| spaCy model not found | Run: `python -m spacy download en_core_web_sm` |\n\n## Output Summary Format\n\nAfter running the workflow, you'll receive a summary report:\n\n```markdown\n# Job Search Results Summary\n\n## Jobs Found: 2\n\n### Job 1: Senior AI Engineer - TechCorp\n- **LinkedIn URL**: https://linkedin.com/jobs/view/12345\n- **Tailored Resume**: ./resume_optimizer/output/resume_tailored_Senior_AI_Engineer.docx\n- **Key Requirements**: Python, TensorFlow, MLOps, Kubernetes\n- **Skill Gaps**: Kubernetes (High priority), AWS Sagemaker (Medium)\n- **Interview Prep Questions**: 20 questions generated (see interview_prep.md)\n\n### Job 2: Machine Learning Engineer - AI Startup\n- **LinkedIn URL**: https://linkedin.com/jobs/view/67890\n- **Tailored Resume**: ./resume_optimizer/output/resume_tailored_Machine_Learning_Engineer.docx\n- **Key Requirements**: PyTorch, Docker, CI/CD, Model deployment\n- **Skill Gaps**: PyTorch (High priority - similar TensorFlow experience)\n- **Interview Prep Questions**: 15 questions generated (see interview_prep.md)\n\n## LinkedIn Profile Recommendations\n\n### About Section Updates\n- Add: \"Specialized in MLOps with focus on production-scale deployments\"\n- Emphasize: Kubernetes orchestration experience\n- Highlight: Cross-functional collaboration in AI product development\n\n### Skills to Add\n1. Kubernetes (High priority - both jobs)\n2. MLOps (Critical keyword)\n3. Model Deployment\n4. CI/CD for ML\n\n## Next Steps\n1. Review tailored resumes in ./resume_optimizer/output/\n2. Study interview prep questions in interview_prep.md\n3. Update LinkedIn profile per recommendations above\n4. Apply to jobs with tailored resumes\n```\n\n## Integration with Existing Skills\n\nThis skill leverages:\n\n### browsing-with-playwright\n- LinkedIn job search automation\n- Browser navigation and snapshot parsing\n- Start server: `bash scripts/start-server.sh`\n- Stop server: `bash scripts/stop-server.sh`\n\n### docx\n- Resume reading using pandoc\n- Tailored resume generation\n- Convert: `pandoc --track-changes=accept resume.docx -o resume.md`\n\n## Advanced Usage\n\n### Manual Job URLs (Skip LinkedIn Scraping)\n\nIf you prefer to provide job URLs manually:\n\n1. Create `jobs_manual.json`:\n```json\n[\n  {\n    \"title\": \"AI Engineer\",\n    \"company\": \"CompanyName\",\n    \"url\": \"https://linkedin.com/jobs/view/12345\",\n    \"description\": \"Paste full job description here...\"\n  }\n]\n```\n\n2. Run workflow with manual jobs:\n```bash\npython3 scripts/workflow_orchestrator.py \\\n  --manual-jobs jobs_manual.json \\\n  --skip-linkedin\n```\n\n### Custom Keyword Extraction\n\nOverride automatic skill extraction:\n\n```bash\npython3 scripts/ats_optimizer.py \\\n  --base-resume resume.docx \\\n  --job-description jobs.json \\\n  --job-index 0 \\\n  --custom-keywords \"Python,TensorFlow,Kubernetes,AWS\" \\\n  --output tailored_resume.docx\n```\n\n## Dependencies Summary\n\n### Required\n- Python 3.8+\n- Node.js 14+\n- pandoc\n- browsing-with-playwright skill\n- docx skill\n\n### Python Packages\n- python-docx\n- spacy\n- nltk\n- pandas\n- requests\n\n### Node Packages\n- @playwright/mcp\n\n## Performance\n\n- **LinkedIn Search**: 30-90 seconds (2 jobs)\n- **Resume Analysis**: 10-20 seconds\n- **Resume Tailoring**: 30-60 seconds per job\n- **Gap Analysis**: 20-30 seconds\n- **Interview Prep**: 40-60 seconds\n- **Total Estimated Time**: 2-4 minutes for 2 jobs\n\n## Limitations\n\n1. **LinkedIn Access**: Requires LinkedIn to be accessible (may need login for some content)\n2. **Job Count**: Recommended maximum 5 jobs per session to avoid rate limiting\n3. **Resume Format**: Only supports .docx format (not .doc or PDF)\n4. **Language**: Optimized for English resumes and job descriptions\n5. **Technical Roles**: Best suited for technical roles (AI, ML, Engineering)\n\n## Security & Privacy\n\n- **Local Processing**: All resume processing happens locally\n- **No Storage**: No resume data is stored remotely\n- **Browser Automation**: Uses local Playwright instance\n- **Data Privacy**: Job descriptions and resumes remain on your machine\n\n## License\n\nThis skill is part of the Claude Code skills ecosystem.\n",
        "skills/resume_optimizer/.claude/skills/linkedin_job_resume_optimizer/references/ats-optimization-guide.md": "# ATS Optimization Guide\n\nComprehensive guide to optimizing resumes for Applicant Tracking Systems (ATS).\n\n## What is ATS?\n\nApplicant Tracking Systems (ATS) are software applications that parse, store, and rank resumes based on specific job requirements. Most large companies use ATS to filter candidates before human review.\n\n### How ATS Works\n\n1. **Parsing**: Extracts text and data from resume files\n2. **Keyword Matching**: Searches for required skills and qualifications\n3. **Scoring**: Ranks candidates based on keyword matches and relevance\n4. **Filtering**: Presents top-ranked resumes to hiring managers\n\n## Keyword Strategy\n\n### Keyword Density Targets\n\n- **Required Skills**: 70-80% coverage\n  - If a job lists 10 required skills, include 7-8 in your resume\n  - Prioritize exact phrase matches\n\n- **Preferred Skills**: 40-50% coverage\n  - Lower priority but still valuable\n  - Use synonyms if exact phrase doesn't fit naturally\n\n### Keyword Sources\n\n1. **Job Description**: Primary source of keywords\n2. **Required Qualifications Section**: Highest priority\n3. **Preferred/Nice-to-Have Section**: Secondary priority\n4. **Company Website/Culture Pages**: Additional context\n\n### Keyword Placement Priority\n\nRanked by ATS weighting:\n\n1. **Professional Summary** (Highest Weight)\n   - Include job title keyword\n   - Top 3-5 required skills\n   - Industry-specific terminology\n\n2. **Skills Section** (Exact Match Critical)\n   - Mirror job posting's skills terminology exactly\n   - Use same capitalization (AWS not aws)\n   - Group by category if job posting does\n\n3. **Experience Descriptions** (Context + Keywords)\n   - Integrate keywords into achievement statements\n   - Use action verbs + keyword + quantifiable result\n\n4. **Job Titles** (Match if Applicable)\n   - If your title matches job posting, use exact wording\n   - Add clarification in parentheses if needed\n\n## Natural Keyword Integration Techniques\n\n### Technique 1: Action Verb + Keyword + Result\n\n```\n Before: \"Worked on machine learning projects\"\n After: \"Developed TensorFlow-based ML models, reducing inference time by 30%\"\n```\n\n### Technique 2: Technology Stack Enumeration\n\n```\n Before: \"Built backend systems\"\n After: \"Built scalable backend systems using Python, Docker, and Kubernetes on AWS infrastructure\"\n```\n\n### Technique 3: Problem-Solution-Technology\n\n```\n Before: \"Improved system performance\"\n After: \"Diagnosed system bottlenecks and implemented caching using Redis, improving response time by 40%\"\n```\n\n### Technique 4: Project Context Integration\n\n```\n Before: \"Collaborated with team\"\n After: \"Collaborated with cross-functional team on MLOps pipeline using Kubernetes and Jenkins CI/CD\"\n```\n\n### Technique 5: Skills in Bullet Points\n\n```\n Before: \"Led development initiatives\"\n After: \"Led development of microservices architecture using React, Node.js, and MongoDB, deployed on AWS ECS\"\n```\n\n## Formatting Best Practices\n\n### DO:\n\n-  Use standard section headings: \"Professional Summary\", \"Skills\", \"Experience\", \"Education\"\n-  Use simple bullet points (, -, *)\n-  Include exact dates (Month Year - Month Year)\n-  Use standard fonts: Arial, Calibri, Times New Roman (10-12pt)\n-  Save as .docx (most ATS-compatible format)\n-  Use clear hierarchy (H1 for name, H2 for sections)\n-  Left-align text for best parsing\n-  Include phone number and email in standard format\n\n### DON'T:\n\n-  Tables for layout (often parsed incorrectly)\n-  Text boxes (content may not be extracted)\n-  Images or logos (ignored by ATS)\n-  Headers/footers with critical info (often not parsed)\n-  Unusual formatting (columns, graphics, shapes)\n-  PDF (unless explicitly requested; .docx is more parseable)\n-  Fancy fonts or decorative elements\n-  Using the word \"resume\" in filename (use Name_JobTitle.docx)\n\n## Section-Specific Optimization\n\n### Professional Summary\n\n**Purpose**: High-impact keyword placement\n\n**Structure**:\n```\n[Job Title] with [X] years of experience in [Top 3 Keywords].\nProven track record of [Achievement with Keywords].\nSpecialized in [Technology Stack] with focus on [Business Value].\n```\n\n**Example**:\n```\nSenior AI Engineer with 5+ years of experience in Machine Learning, MLOps, and Python.\nProven track record of deploying production-scale models using TensorFlow and Kubernetes.\nSpecialized in NLP and Computer Vision with focus on driving business impact through AI innovation.\n```\n\n### Skills Section\n\n**Format Options**:\n\n1. **Categorized** (Best for Technical Roles):\n```\nProgramming Languages: Python, Java, JavaScript, SQL\nML Frameworks: TensorFlow, PyTorch, Scikit-learn\nCloud & DevOps: AWS, Docker, Kubernetes, CI/CD\nDatabases: PostgreSQL, MongoDB, Redis\n```\n\n2. **Flat List** (Simple):\n```\nPython  TensorFlow  AWS  Kubernetes  Docker  PostgreSQL  Git  Jenkins\n```\n\n**Tips**:\n- Use exact terminology from job posting\n- Include both acronyms and full names (ML and Machine Learning)\n- List proficiency levels if space permits\n- Prioritize required skills first\n\n### Experience Section\n\n**Bullet Point Formula**:\n```\n[Action Verb] + [What] + [Using/With Keywords] + [Quantifiable Result]\n```\n\n**Examples**:\n```\n Developed microservices architecture using Node.js and Docker, reducing deployment time by 60%\n Implemented CI/CD pipeline with Jenkins and Kubernetes, enabling 20+ daily deployments\n Built recommendation system using PyTorch and AWS SageMaker, increasing user engagement by 35%\n Architected data pipeline processing 10TB daily with Spark and Airflow on AWS EMR\n```\n\n**Action Verbs by Category**:\n\n- **Leadership**: Led, Directed, Managed, Coordinated, Supervised\n- **Development**: Developed, Built, Implemented, Designed, Architected, Engineered\n- **Improvement**: Optimized, Enhanced, Streamlined, Improved, Increased, Reduced\n- **Analysis**: Analyzed, Researched, Evaluated, Assessed, Investigated\n- **Collaboration**: Collaborated, Partnered, Worked with, Coordinated with\n\n### Education Section\n\n**Format**:\n```\n[Degree], [Field of Study]\n[University Name] | [City, State] | [Graduation Date]\nGPA: [X.XX] (if 3.5+) | Relevant Coursework: [Keywords]\n```\n\n## ATS Testing Methods\n\n### Method 1: Manual Review\n\n1. Copy resume text into plain text editor\n2. Check if structure is preserved\n3. Verify all keywords are present and readable\n4. Look for garbled text or missing sections\n\n### Method 2: Online ATS Simulators\n\nFree/Paid Tools:\n- **Jobscan.co**: Compare resume to job description\n- **Resume Worded**: ATS compatibility score\n- **TopResume**: Free ATS scan\n- **SkillSyncer**: Keyword matching tool\n\n### Method 3: Conversion Test\n\n```bash\n# Convert DOCX to text and verify\npandoc resume.docx -o resume.txt\ncat resume.txt  # Review for formatting issues\n```\n\n### Method 4: ATS-Friendly Checklist\n\n- [ ] Filename: FirstName_LastName_JobTitle.docx\n- [ ] Standard sections in logical order\n- [ ] No tables, text boxes, or images\n- [ ] Simple bullet points throughout\n- [ ] Keywords from job posting included\n- [ ] Contact info clearly visible at top\n- [ ] Dates in consistent format\n- [ ] No spelling or grammar errors\n\n## Common ATS Mistakes\n\n### Mistake 1: Keyword Stuffing\n\n```\n Bad: \"Python Python Python expert with Python skills in Python development\"\n Good: \"Python developer with expertise in Django, Flask, and data analysis libraries\"\n```\n\n### Mistake 2: Using Graphics for Text\n\n```\n Bad: Skills listed in chart/graph format\n Good: Skills listed as bullet points or comma-separated\n```\n\n### Mistake 3: Non-Standard Section Names\n\n```\n Bad: \"My Journey\", \"What I'm Good At\", \"Places I've Worked\"\n Good: \"Experience\", \"Skills\", \"Professional Summary\"\n```\n\n### Mistake 4: Acronym-Only\n\n```\n Bad: \"ML, NLP, CI/CD\"\n Good: \"Machine Learning (ML), Natural Language Processing (NLP), CI/CD\"\n```\n\n### Mistake 5: Missing Dates\n\n```\n Bad: \"Software Engineer at TechCorp\"\n Good: \"Software Engineer at TechCorp | June 2020 - Present\"\n```\n\n## Advanced ATS Strategies\n\n### Strategy 1: Exact Phrase Matching\n\nIf job posting says \"5+ years of Python experience\", use that exact phrase rather than \"experienced Python developer\"\n\n### Strategy 2: Keyword Variations\n\nInclude variations:\n- Machine Learning / ML / Machine-Learning\n- Artificial Intelligence / AI\n- Full Stack / Full-Stack / Fullstack\n\n### Strategy 3: Hidden Keywords (Use Sparingly)\n\nWhite text on white background is detectable and not recommended. Instead:\n- Include keywords naturally in bullet points\n- Use a \"Relevant Coursework\" or \"Technologies\" section\n\n### Strategy 4: Tailoring Intensity Levels\n\n- **High Priority Job**: 80%+ keyword match, extensive tailoring\n- **Medium Priority**: 60-70% keyword match, moderate tailoring\n- **Exploratory**: 50%+ keyword match, light tailoring\n\n## Industry-Specific Considerations\n\n### Tech/Software Engineering\n- Emphasize: Programming languages, frameworks, cloud platforms\n- Include: GitHub profile, technical blog, portfolio projects\n- Keywords: Agile, Scrum, CI/CD, Test-Driven Development\n\n### Data Science/ML\n- Emphasize: ML frameworks, statistical methods, data tools\n- Include: Kaggle profile, publications, conference presentations\n- Keywords: A/B testing, Model deployment, Feature engineering\n\n### Cloud/DevOps\n- Emphasize: Cloud platforms, automation tools, infrastructure as code\n- Include: Certifications (AWS, Azure, Kubernetes)\n- Keywords: High availability, Scalability, Disaster recovery\n\n### Management/Leadership\n- Emphasize: Team size, budget managed, strategic initiatives\n- Include: Agile/Scrum certifications, change management\n- Keywords: Cross-functional, Stakeholder management, P&L\n\n## Final Checklist\n\nBefore submitting your ATS-optimized resume:\n\n- [ ] Job title keyword in professional summary\n- [ ] 70-80% required skills coverage\n- [ ] 40-50% preferred skills coverage\n- [ ] Standard .docx format (not PDF unless requested)\n- [ ] No tables, text boxes, or images\n- [ ] Simple, clean formatting\n- [ ] Standard section headings\n- [ ] Exact dates for all positions\n- [ ] Keywords integrated naturally (no stuffing)\n- [ ] Tested with online ATS checker\n- [ ] Proofread for spelling/grammar\n- [ ] Filename: FirstName_LastName_JobTitle.docx\n\n## Conclusion\n\nATS optimization is about making your resume machine-readable while keeping it human-readable. Focus on natural keyword integration, clean formatting, and exact matches to job requirements. Remember: the goal is to pass the ATS filter AND impress the human reviewer.\n",
        "skills/resume_optimizer/.claude/skills/linkedin_job_resume_optimizer/references/interview-prep-patterns.md": "# Interview Preparation Patterns\n\nComprehensive guide for generating and answering interview questions based on skill gaps.\n\n## Question Generation Framework\n\n### Question Types\n\n1. **Fundamental Knowledge** (30-40% for missing skills)\n   - Core concepts and principles\n   - Basic definitions and terminology\n   - Standard use cases\n\n2. **Depth Assessment** (40% for weak skills)\n   - Advanced features and optimizations\n   - Trade-offs and design decisions\n   - Performance considerations\n\n3. **Scenario-Based** (30% for all skills)\n   - Problem-solving in realistic contexts\n   - Troubleshooting and debugging\n   - System design challenges\n\n4. **Behavioral/STAR** (20% overall)\n   - Past experience and learnings\n   - Team collaboration\n   - Handling challenges\n\n5. **Transferable Experience** (30% for missing skills with related experience)\n   - Leveraging similar technologies\n   - Learning approach and adaptability\n   - Comparative analysis\n\n## Progressive Difficulty Levels\n\n### Entry-Level Questions\n\nFocus: Definitions, basic usage, standard patterns\n\n**Examples**:\n- \"What is [technology] and when would you use it?\"\n- \"Explain the difference between [A] and [B].\"\n- \"What are the main components of [technology]?\"\n\n### Mid-Level Questions\n\nFocus: Practical application, best practices, common scenarios\n\n**Examples**:\n- \"How would you implement [feature] using [technology]?\"\n- \"What are common pitfalls when working with [technology]?\"\n- \"Describe your approach to debugging [specific issue].\"\n\n### Senior-Level Questions\n\nFocus: Architecture, optimization, leadership, trade-offs\n\n**Examples**:\n- \"Design a [system] that handles [scale/complexity].\"\n- \"What are the trade-offs between [approach A] and [approach B]?\"\n- \"How would you mentor a junior developer learning [technology]?\"\n\n## STAR Method Framework\n\n### Structure\n\n**S**ituation: Set the context\n**T**ask: Describe the challenge or objective\n**A**ction: Explain what you did\n**R**esult: Share the outcome with metrics\n\n### Example Question Types\n\n1. **Achievement Questions**\n   - \"Tell me about a time you successfully implemented [technology].\"\n   - \"Describe a project where you improved [metric] using [technology].\"\n\n2. **Challenge Questions**\n   - \"Tell me about a difficult technical problem you solved.\"\n   - \"Describe a time when you had to learn a new technology quickly.\"\n\n3. **Collaboration Questions**\n   - \"Tell me about a time you worked with a cross-functional team.\"\n   - \"Describe how you handled a disagreement with a colleague.\"\n\n4. **Failure/Learning Questions**\n   - \"Tell me about a time something went wrong. What did you learn?\"\n   - \"Describe a project that didn't go as planned.\"\n\n### Answer Template\n\n```\nSituation: \"At my previous role at [Company], we had [problem/context]...\"\n\nTask: \"I was responsible for [specific objective]...\"\n\nAction: \"I approached this by:\n  1. [First action with technology/method]\n  2. [Second action with details]\n  3. [Third action highlighting skills]\"\n\nResult: \"This led to [quantifiable outcome]:\n  - [Metric 1]: Improved by X%\n  - [Metric 2]: Reduced by Y%\n  - [Business impact]: [Result]\"\n```\n\n## Question Templates by Technology\n\n### Cloud Platforms (AWS, Azure, GCP)\n\n**Fundamental**:\n1. Explain the core services and their use cases.\n2. What is the shared responsibility model?\n3. How does IAM work for access control?\n4. Describe the networking concepts (VPC, subnets, security groups).\n\n**Scenario**:\n1. Design a highly available, scalable web application architecture.\n2. An instance is experiencing high CPU. How do you diagnose and resolve?\n3. How would you implement disaster recovery?\n4. Design a cost-effective data processing pipeline.\n\n### Container Orchestration (Kubernetes, Docker)\n\n**Fundamental**:\n1. Explain containers vs. virtual machines.\n2. What are the key components (pods, services, deployments)?\n3. How does container networking work?\n4. What is container orchestration and why is it needed?\n\n**Scenario**:\n1. A pod keeps crashing. Walk through your debugging process.\n2. Design a deployment strategy for zero-downtime updates.\n3. How would you scale an application to handle 10x traffic?\n4. Implement monitoring and logging for containers in production.\n\n### Machine Learning Frameworks (TensorFlow, PyTorch)\n\n**Fundamental**:\n1. Explain the difference between TensorFlow and PyTorch.\n2. What is automatic differentiation?\n3. How does backpropagation work?\n4. What are common regularization techniques?\n\n**Scenario**:\n1. Your model is overfitting. What approaches would you try?\n2. How would you deploy an ML model to production?\n3. Optimize a model for faster inference.\n4. Debug a model that's not converging during training.\n\n### Programming Languages (Python, Java, JavaScript)\n\n**Fundamental**:\n1. Explain key language features and paradigms.\n2. What are common data structures and when to use them?\n3. How does memory management work?\n4. Describe the concurrency model.\n\n**Scenario**:\n1. Code a solution to [specific problem] (live coding).\n2. Review this code and identify improvements.\n3. How would you optimize this function for performance?\n4. Debug this code that's producing incorrect output.\n\n## Skill-Specific Question Banks\n\n### Kubernetes\n\n**Fundamental** (Pick 4):\n1. Explain Kubernetes architecture (control plane, nodes, pods).\n2. What is the difference between Deployment and StatefulSet?\n3. How does service discovery work in Kubernetes?\n4. What are ConfigMaps and Secrets?\n5. Describe the pod lifecycle.\n6. How does the Kubernetes scheduler work?\n7. What is a namespace and why is it important?\n8. Explain horizontal pod autoscaling.\n\n**Scenario** (Pick 3):\n1. A pod keeps crashing. How do you troubleshoot?\n2. Design a deployment for a stateful application (database).\n3. Implement blue-green deployment strategy.\n4. A service has high latency. Investigate and resolve.\n5. Set up monitoring for a Kubernetes cluster.\n6. Implement RBAC for multi-tenant cluster.\n\n### AWS\n\n**Fundamental** (Pick 4):\n1. Explain EC2, S3, RDS, Lambda and their use cases.\n2. What is the difference between S3 and EBS?\n3. How does IAM enable secure access control?\n4. What are VPCs and why are they important?\n5. Explain Auto Scaling groups.\n6. What is CloudFormation?\n7. Describe AWS security best practices.\n8. How does AWS Lambda pricing work?\n\n**Scenario** (Pick 3):\n1. Design a scalable, highly available web application.\n2. Migrate a legacy application to AWS.\n3. Implement disaster recovery with RTO < 1 hour.\n4. Optimize AWS costs for a production workload.\n5. Secure an S3 bucket with sensitive data.\n6. Design a data pipeline processing TBs of data daily.\n\n### Python\n\n**Fundamental** (Pick 4):\n1. Explain Python's memory management and garbage collection.\n2. What is the Global Interpreter Lock (GIL)?\n3. Describe decorators and their use cases.\n4. What are generators and why use them?\n5. Explain list comprehensions vs. generator expressions.\n6. How does Python handle multiple inheritance?\n7. What are context managers (with statement)?\n8. Describe asyncio and when to use it.\n\n**Scenario** (Pick 3):\n1. Optimize this code for performance (given slow code).\n2. Debug a memory leak in a Python application.\n3. Implement a multi-threaded/async solution for [problem].\n4. Design a Python package with proper structure.\n5. Handle exceptions and errors gracefully in production code.\n6. Write unit tests for this function with edge cases.\n\n## Answer Frameworks\n\n### Technical Explanation Pattern\n\n```\n1. **Definition**: Start with a clear, concise definition\n2. **Purpose**: Explain why it exists / what problem it solves\n3. **Key Components**: Break down into main parts\n4. **Example**: Provide a concrete use case\n5. **Trade-offs**: Discuss advantages and limitations\n```\n\n**Example**:\n> Q: \"What is Docker?\"\n>\n> A: \"Docker is a containerization platform that packages applications\n> and their dependencies into isolated containers. It solves the\n> 'works on my machine' problem by ensuring consistency across\n> environments. Key components include the Docker Engine (runtime),\n> Images (templates), and Containers (running instances). For example,\n> you can package a Python app with all its libraries into a Docker\n> image and run it identically on dev, staging, and production.\n> The main advantage is portability, but the trade-off is added\n> complexity in orchestration at scale.\"\n\n### System Design Pattern\n\n```\n1. **Clarify Requirements**:\n   - Functional requirements\n   - Non-functional requirements (scale, latency, availability)\n\n2. **High-Level Architecture**:\n   - Draw/describe main components\n   - Data flow\n\n3. **Deep Dive**:\n   - Database design\n   - API design\n   - Key algorithms\n\n4. **Scalability**:\n   - How to handle growth\n   - Bottlenecks and solutions\n\n5. **Trade-offs**:\n   - CAP theorem considerations\n   - Consistency vs. availability\n\n6. **Monitoring & Ops**:\n   - Metrics to track\n   - Failure scenarios\n```\n\n### Debugging Pattern\n\n```\n1. **Reproduce**: Ensure problem is reproducible\n2. **Isolate**: Narrow down the source\n3. **Hypothesize**: Form theories about the cause\n4. **Test**: Verify hypothesis with experiments\n5. **Fix**: Implement solution\n6. **Verify**: Confirm fix works\n7. **Prevent**: Add tests/monitoring to prevent recurrence\n```\n\n## Preparation Strategies\n\n### For Missing Skills (High Priority)\n\n**Week 1-2: Foundation**\n- Take online course or tutorial\n- Read official documentation\n- Complete basic exercises\n\n**Week 3-4: Practice**\n- Build small project using technology\n- Answer fundamental questions out loud\n- Study common interview questions\n\n**Interview Prep**:\n- Be honest: \"I haven't used X in production, but I've studied it and built [project]\"\n- Emphasize transferable skills: \"I have experience with Y which is similar\"\n- Show learning ability: \"I learned Z in 2 weeks for a recent project\"\n\n### For Weak Skills (Medium Priority)\n\n**Depth Study**:\n- Read advanced tutorials and best practices\n- Study production use cases\n- Review design patterns\n\n**Practice**:\n- Explain concepts to others\n- Write blog posts or documentation\n- Contribute to open source\n\n**Interview Prep**:\n- Prepare specific examples from your experience\n- Be ready for deep technical questions\n- Know limitations and trade-offs\n\n### For Strong Skills (Confidence Building)\n\n**Advanced Topics**:\n- Performance optimization\n- Architecture and design patterns\n- Leadership and mentorship\n\n**Interview Prep**:\n- Prepare multiple detailed examples\n- Be ready for system design questions\n- Practice explaining to non-technical audience\n\n## Common Interview Mistakes\n\n### Mistake 1: Overconfidence\n\n```\n Bad: \"I know everything about Kubernetes\"\n Good: \"I have 3 years of production Kubernetes experience managing\n20+ microservices. I'm always learning about new features like\nOperators and service mesh integration.\"\n```\n\n### Mistake 2: Saying \"I Don't Know\" and Stopping\n\n```\n Bad: \"I don't know.\"\n Good: \"I haven't worked with that specific feature, but based on\nmy understanding of [related concept], I would approach it by [reasoning].\"\n```\n\n### Mistake 3: Not Asking Clarifying Questions\n\n```\n Bad: Immediately jumping to solution\n Good: \"To design this system, I need to clarify: What's the\nexpected scale? What are the latency requirements? What's the\navailability target?\"\n```\n\n### Mistake 4: Overly Theoretical Answers\n\n```\n Bad: Long explanation with no real-world context\n Good: \"In my previous role, we implemented [technology] for\n[specific use case]. Here's what we learned...\"\n```\n\n## Interview Day Tips\n\n### Before\n\n- [ ] Research the company and role thoroughly\n- [ ] Review job description and your resume\n- [ ] Practice answering questions out loud\n- [ ] Prepare questions to ask interviewer\n- [ ] Get good sleep and eat well\n\n### During\n\n- [ ] Listen carefully to the full question\n- [ ] Ask clarifying questions\n- [ ] Think before answering (pause is okay)\n- [ ] Use STAR method for behavioral questions\n- [ ] Provide specific examples with metrics\n- [ ] Admit when you don't know something\n- [ ] Show enthusiasm for learning\n\n### After\n\n- [ ] Send thank-you email within 24 hours\n- [ ] Reflect on what went well and what to improve\n- [ ] Make notes about questions asked\n- [ ] Follow up if promised information\n\n## Sample Question-Answer Pairs\n\n### Q: \"You have Docker experience but the job requires Kubernetes. How quickly can you learn it?\"\n\n**Good Answer**:\n> \"I have 2 years of production Docker experience, which gives me a\n> strong foundation since Kubernetes orchestrates Docker containers.\n> I've already started learning Kubernetes - I've completed a Udemy\n> course and deployed a small personal project to a local Kubernetes\n> cluster using Minikube. The concepts of containers, networking, and\n> resource management transfer directly. Based on my experience\n> learning Docker in just 3 weeks for a critical project, I'm confident\n> I can become productive with Kubernetes within a month of hands-on use.\n> I'm actually excited about this opportunity to deepen my container\n> orchestration skills.\"\n\n### Q: \"Walk me through debugging a production issue.\"\n\n**Good Answer (STAR)**:\n> \"At my previous company, we had a production API that started returning\n> 500 errors for 10% of requests (Situation). I was on-call and responsible\n> for diagnosing and fixing the issue within our 15-minute SLA (Task).\n>\n> I started by checking our monitoring dashboard and noticed high database\n> connection pool exhaustion. I then reviewed recent deployments and found\n> we'd increased a query timeout from 5s to 30s, causing connections to\n> be held longer. I quickly rolled back that change while the team worked\n> on a proper fix: adding connection pooling limits and query optimization\n> (Action).\n>\n> The rollback resolved the issue in 8 minutes, well within SLA. We then\n> implemented the optimized query with proper connection management,\n> reducing average response time by 40% and eliminating connection pool\n> issues. I also added monitoring alerts for connection pool utilization\n> to catch this earlier in the future (Result).\"\n\n## Conclusion\n\nEffective interview preparation combines understanding technical concepts,\npracticing articulate explanations, preparing specific examples from your\nexperience, and showing genuine enthusiasm for learning. Focus on honest\ncommunication, structured thinking, and demonstrating your problem-solving\napproach.\n",
        "skills/resume_optimizer/.claude/skills/linkedin_job_resume_optimizer/references/linkedin-automation.md": "# LinkedIn Automation Guide\n\nGuide for automating LinkedIn job searches using Playwright MCP.\n\n## LinkedIn Job Search URL Structure\n\n### Basic Search URL\n\n```\nhttps://www.linkedin.com/jobs/search/\n```\n\n### Query Parameters\n\n| Parameter | Purpose | Example |\n|-----------|---------|---------|\n| `keywords` | Search keywords | `?keywords=AI%20Engineer` |\n| `location` | Job location | `&location=Remote` |\n| `f_WT` | Remote filter | `&f_WT=2` (Remote only) |\n| `f_TPR` | Time posted | `&f_TPR=r86400` (Past 24 hours) |\n| `f_E` | Experience level | `&f_E=3` (Mid-Senior level) |\n| `f_JT` | Job type | `&f_JT=F` (Full-time) |\n| `start` | Pagination offset | `&start=25` (Page 2) |\n\n### Complete Example URL\n\n```\nhttps://www.linkedin.com/jobs/search/?keywords=Machine%20Learning%20Engineer&location=Remote&f_WT=2&f_TPR=r604800&f_E=3\n```\n\nThis searches for \"Machine Learning Engineer\", Remote, Posted in last 7 days, Mid-Senior level.\n\n## Navigation Patterns\n\n### Step-by-Step Workflow\n\n1. **Navigate to Search URL**\n   ```python\n   mcp_client.call('browser_navigate', {\n       'url': 'https://www.linkedin.com/jobs/search/?keywords=AI%20Engineer&location=Remote'\n   })\n   ```\n\n2. **Wait for Page Load**\n   ```python\n   mcp_client.call('browser_wait_for', {'time': 3000})  # 3 seconds\n   ```\n\n3. **Get Page Snapshot**\n   ```python\n   snapshot = mcp_client.call('browser_snapshot', {})\n   ```\n\n4. **Parse Job Cards**\n   - Look for job listing elements in snapshot\n   - Extract refs for clicking\n\n5. **Click Job Card**\n   ```python\n   mcp_client.call('browser_click', {\n       'element': 'Job title text',\n       'ref': 'e42'  # From snapshot\n   })\n   ```\n\n6. **Wait for Job Details**\n   ```python\n   mcp_client.call('browser_wait_for', {'time': 2000})\n   ```\n\n7. **Get Job Details Snapshot**\n   ```python\n   job_snapshot = mcp_client.call('browser_snapshot', {})\n   ```\n\n## Element Selectors and Patterns\n\n### Job List Page\n\n**Job Cards**: Look for patterns like:\n- \"Senior AI Engineer\" [ref]\n- \"TechCorp \" [company]\n- \"Remote\" [location]\n- \"Posted X days ago\"\n\n**Navigation Elements**:\n- \"Next\" button for pagination\n- Filter dropdowns (Date posted, Experience level, etc.)\n\n### Job Detail Page\n\n**Key Sections**:\n- Job title (usually in large font at top)\n- Company name (after \"at\" or \"\")\n- \"About the job\" section (main description)\n- \"Show more\" button (expand full description)\n- \"Apply\" button\n\n**Parsing Strategy**:\n```python\ndef extract_job_details(snapshot_text):\n    # Title: First line or large text element\n    title = extract_first_line(snapshot_text)\n\n    # Company: Look for \"at CompanyName\" or \" CompanyName\"\n    company_match = re.search(r'(?:at|)\\s+(.+?)(?:\\n|)', snapshot_text)\n\n    # Description: After \"About the job\" or \"Job Description\"\n    desc_start = snapshot_text.find('About the job')\n    description = snapshot_text[desc_start:desc_start+3000]\n\n    return {'title': title, 'company': company, 'description': description}\n```\n\n## Anti-Bot Detection Strategies\n\n### 1. Random Delays\n\n```python\nimport random\nimport time\n\n# Between actions\ntime.sleep(random.uniform(1, 3))\n\n# After page loads\ntime.sleep(random.uniform(2, 5))\n```\n\n### 2. Human-Like Patterns\n\n- Don't scrape too fast (max 10-15 jobs per session)\n- Vary wait times between requests\n- Occasionally scroll or hover before clicking\n- Don't follow predictable patterns\n\n### 3. Browser Settings\n\n```bash\n# Use shared browser context\nnpx @playwright/mcp@latest --port 8808 --shared-browser-context\n\n# Disable headless mode if needed (more human-like)\n# (May require modifying start-server.sh)\n```\n\n### 4. User Agent Rotation\n\nIf detection occurs, consider rotating user agents:\n```python\nuser_agents = [\n    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36...',\n    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36...',\n    # Add more variations\n]\n```\n\n### 5. Session Management\n\n- Login manually before automation (if needed)\n- Use cookies from authenticated session\n- Clear cookies if rate limited\n\n## Rate Limiting Handling\n\n### Detection Signs\n\n- HTTP 429 (Too Many Requests) status\n- \"You've been rate limited\" message\n- Blank snapshots or error pages\n- Login wall appearing\n\n### Response Strategy\n\n```python\ndef search_with_retry(keywords, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            jobs = search_linkedin_jobs(keywords)\n            return jobs\n        except RateLimitError:\n            wait_time = (2 ** attempt) * 5  # 5s, 10s, 20s\n            print(f\"Rate limited. Waiting {wait_time}s...\")\n            time.sleep(wait_time)\n    return []\n```\n\n### Prevention\n\n- Limit scraping to 2-5 jobs per session\n- Add delays between actions (2-5 seconds)\n- Don't run multiple sessions simultaneously\n- Use `--shared-browser-context` to maintain session state\n\n## Snapshot Parsing Techniques\n\n### Extracting Job Listings\n\n```python\ndef extract_job_cards_from_snapshot(snapshot_text):\n    job_cards = []\n    lines = snapshot_text.split('\\n')\n\n    for i, line in enumerate(lines):\n        # Look for job title patterns\n        if any(keyword in line.lower() for keyword in ['engineer', 'developer', 'scientist']):\n            # Extract ref number\n            ref_match = re.search(r'\\[(\\d+)\\]', line)\n            if ref_match:\n                ref = ref_match.group(1)\n                title = re.sub(r'\\[\\d+\\]', '', line).strip()\n                job_cards.append({'ref': ref, 'title': title})\n\n    return job_cards\n```\n\n### Extracting Job Description\n\n```python\ndef extract_description(snapshot_text):\n    # Strategy 1: Look for \"About the job\" section\n    desc_markers = ['About the job', 'Job Description', 'Description']\n\n    for marker in desc_markers:\n        if marker in snapshot_text:\n            start = snapshot_text.find(marker) + len(marker)\n            # Take next 3000 chars or until next major section\n            end_markers = ['Seniority level', 'Employment type', 'How you match']\n            end = start + 3000\n            for end_marker in end_markers:\n                pos = snapshot_text.find(end_marker, start)\n                if pos != -1 and pos < end:\n                    end = pos\n            return snapshot_text[start:end].strip()\n\n    # Strategy 2: If no marker found, use heuristics\n    # Description is usually the longest text block\n    return snapshot_text[:3000]\n```\n\n### Extracting Required Skills\n\n```python\ndef extract_required_skills(description):\n    # Look for \"Required\" or \"Qualifications\" section\n    required_section = \"\"\n    patterns = [\n        r'(?:Required|Must have|Requirements)[:\\s]+(.+?)(?:Preferred|Nice to have|Plus|$)',\n        r'(?:Qualifications)[:\\s]+(.+?)(?:Preferred|Nice to have|Plus|$)'\n    ]\n\n    for pattern in patterns:\n        match = re.search(pattern, description, re.DOTALL | re.IGNORECASE)\n        if match:\n            required_section = match.group(1)\n            break\n\n    # Extract technical keywords from section\n    tech_keywords = ['Python', 'Java', 'AWS', 'Docker', ...]  # Full list\n    found_skills = []\n\n    for keyword in tech_keywords:\n        if keyword.lower() in required_section.lower():\n            found_skills.append(keyword)\n\n    return found_skills\n```\n\n## Error Handling\n\n### Login Wall\n\n```python\ndef handle_login_wall(snapshot_text):\n    if 'sign in' in snapshot_text.lower() or 'join now' in snapshot_text.lower():\n        print(\"LinkedIn requires login\")\n        print(\"Options:\")\n        print(\"  1. Login manually in browser\")\n        print(\"  2. Provide job URLs manually\")\n        print(\"  3. Use alternative job boards\")\n        return True\n    return False\n```\n\n### Stale Elements\n\n```python\ndef safe_click(element_desc, ref, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            mcp_client.call('browser_click', {'element': element_desc, 'ref': ref})\n            return True\n        except StaleElementError:\n            print(f\"Stale element, retry {attempt+1}/{max_retries}\")\n            time.sleep(1)\n            # Re-get snapshot\n            snapshot = mcp_client.call('browser_snapshot', {})\n            # Re-find element\n    return False\n```\n\n### Connection Failures\n\n```python\ndef check_playwright_server():\n    try:\n        response = mcp_client.call('browser_snapshot', {})\n        return response is not None\n    except Exception:\n        return False\n\nif not check_playwright_server():\n    print(\"Playwright server not responding\")\n    print(\"Restarting server...\")\n    restart_playwright_server()\n```\n\n## Alternative Strategies\n\n### If LinkedIn Automation Fails\n\n1. **Manual Job URL Input**\n   ```python\n   jobs = [\n       {'url': 'https://linkedin.com/jobs/view/12345', 'title': '...'},\n       {'url': 'https://linkedin.com/jobs/view/67890', 'title': '...'}\n   ]\n   ```\n\n2. **Use LinkedIn API** (If you have access)\n   - OAuth authentication required\n   - Rate limits apply\n   - May require partnership\n\n3. **Alternative Job Boards**\n   - Indeed: Often more automation-friendly\n   - Glassdoor: Similar structure to LinkedIn\n   - Company career pages directly\n\n4. **RSS Feeds**\n   - Some companies offer RSS feeds for jobs\n   - Can be parsed without web automation\n\n## Debugging Tips\n\n### Save Snapshots for Analysis\n\n```python\n# Save snapshot to file\nwith open('/tmp/linkedin_snapshot.txt', 'w') as f:\n    f.write(snapshot_text)\n\n# Review manually to understand structure\ncat /tmp/linkedin_snapshot.txt | less\n```\n\n### Take Screenshots\n\n```python\nscreenshot = mcp_client.call('browser_take_screenshot', {\n    'type': 'png',\n    'fullPage': True\n})\n# Screenshot saved to file, check path in output\n```\n\n### Log All Actions\n\n```python\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nlogger.info(f\"Navigating to {url}\")\nlogger.info(f\"Clicking element: {element_desc}\")\nlogger.info(f\"Extracted {len(jobs)} jobs\")\n```\n\n## Best Practices\n\n1. **Start Small**: Test with 1-2 jobs before scaling\n2. **Monitor Output**: Check snapshot files and logs\n3. **Respect Rate Limits**: Don't scrape aggressively\n4. **Handle Failures Gracefully**: Provide fallback options\n5. **Keep Updated**: LinkedIn structure changes; update selectors\n6. **User Privacy**: Don't store personal data unnecessarily\n7. **Terms of Service**: Review LinkedIn's ToS and robots.txt\n\n## Example Complete Workflow\n\n```python\n# 1. Start server\nstart_playwright_server()\n\n# 2. Navigate to LinkedIn jobs\nurl = \"https://www.linkedin.com/jobs/search/?keywords=AI%20Engineer&location=Remote\"\nmcp_client.call('browser_navigate', {'url': url})\n\n# 3. Wait and get snapshot\ntime.sleep(random.uniform(3, 5))\nsnapshot = mcp_client.call('browser_snapshot', {})\n\n# 4. Extract job cards\njob_cards = extract_job_cards(snapshot['content'])\n\n# 5. For each job\njobs = []\nfor card in job_cards[:2]:  # Limit to 2\n    # Click job\n    mcp_client.call('browser_click', {'element': card['title'], 'ref': card['ref']})\n\n    # Wait and get details\n    time.sleep(random.uniform(2, 4))\n    job_snapshot = mcp_client.call('browser_snapshot', {})\n\n    # Extract and store\n    job_data = extract_job_details(job_snapshot['content'])\n    jobs.append(job_data)\n\n# 6. Stop server\nstop_playwright_server()\n\n# 7. Return jobs\nreturn jobs\n```\n\n## Conclusion\n\nLinkedIn automation requires careful attention to rate limits, bot detection, and changing page structures. Always have fallback strategies (manual URLs, alternative sources) and respect the platform's terms of service.\n",
        "skills/resume_optimizer/.claude/skills/pdf/SKILL.md": "---\nname: pdf\ndescription: Comprehensive PDF manipulation toolkit for extracting text and tables, creating new PDFs, merging/splitting documents, and handling forms. When Claude needs to fill in a PDF form or programmatically process, generate, or analyze PDF documents at scale.\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# PDF Processing Guide\n\n## Overview\n\nThis guide covers essential PDF processing operations using Python libraries and command-line tools. For advanced features, JavaScript libraries, and detailed examples, see reference.md. If you need to fill out a PDF form, read forms.md and follow its instructions.\n\n## Quick Start\n\n```python\nfrom pypdf import PdfReader, PdfWriter\n\n# Read a PDF\nreader = PdfReader(\"document.pdf\")\nprint(f\"Pages: {len(reader.pages)}\")\n\n# Extract text\ntext = \"\"\nfor page in reader.pages:\n    text += page.extract_text()\n```\n\n## Python Libraries\n\n### pypdf - Basic Operations\n\n#### Merge PDFs\n```python\nfrom pypdf import PdfWriter, PdfReader\n\nwriter = PdfWriter()\nfor pdf_file in [\"doc1.pdf\", \"doc2.pdf\", \"doc3.pdf\"]:\n    reader = PdfReader(pdf_file)\n    for page in reader.pages:\n        writer.add_page(page)\n\nwith open(\"merged.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n#### Split PDF\n```python\nreader = PdfReader(\"input.pdf\")\nfor i, page in enumerate(reader.pages):\n    writer = PdfWriter()\n    writer.add_page(page)\n    with open(f\"page_{i+1}.pdf\", \"wb\") as output:\n        writer.write(output)\n```\n\n#### Extract Metadata\n```python\nreader = PdfReader(\"document.pdf\")\nmeta = reader.metadata\nprint(f\"Title: {meta.title}\")\nprint(f\"Author: {meta.author}\")\nprint(f\"Subject: {meta.subject}\")\nprint(f\"Creator: {meta.creator}\")\n```\n\n#### Rotate Pages\n```python\nreader = PdfReader(\"input.pdf\")\nwriter = PdfWriter()\n\npage = reader.pages[0]\npage.rotate(90)  # Rotate 90 degrees clockwise\nwriter.add_page(page)\n\nwith open(\"rotated.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n### pdfplumber - Text and Table Extraction\n\n#### Extract Text with Layout\n```python\nimport pdfplumber\n\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    for page in pdf.pages:\n        text = page.extract_text()\n        print(text)\n```\n\n#### Extract Tables\n```python\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    for i, page in enumerate(pdf.pages):\n        tables = page.extract_tables()\n        for j, table in enumerate(tables):\n            print(f\"Table {j+1} on page {i+1}:\")\n            for row in table:\n                print(row)\n```\n\n#### Advanced Table Extraction\n```python\nimport pandas as pd\n\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    all_tables = []\n    for page in pdf.pages:\n        tables = page.extract_tables()\n        for table in tables:\n            if table:  # Check if table is not empty\n                df = pd.DataFrame(table[1:], columns=table[0])\n                all_tables.append(df)\n\n# Combine all tables\nif all_tables:\n    combined_df = pd.concat(all_tables, ignore_index=True)\n    combined_df.to_excel(\"extracted_tables.xlsx\", index=False)\n```\n\n### reportlab - Create PDFs\n\n#### Basic PDF Creation\n```python\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.pdfgen import canvas\n\nc = canvas.Canvas(\"hello.pdf\", pagesize=letter)\nwidth, height = letter\n\n# Add text\nc.drawString(100, height - 100, \"Hello World!\")\nc.drawString(100, height - 120, \"This is a PDF created with reportlab\")\n\n# Add a line\nc.line(100, height - 140, 400, height - 140)\n\n# Save\nc.save()\n```\n\n#### Create PDF with Multiple Pages\n```python\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak\nfrom reportlab.lib.styles import getSampleStyleSheet\n\ndoc = SimpleDocTemplate(\"report.pdf\", pagesize=letter)\nstyles = getSampleStyleSheet()\nstory = []\n\n# Add content\ntitle = Paragraph(\"Report Title\", styles['Title'])\nstory.append(title)\nstory.append(Spacer(1, 12))\n\nbody = Paragraph(\"This is the body of the report. \" * 20, styles['Normal'])\nstory.append(body)\nstory.append(PageBreak())\n\n# Page 2\nstory.append(Paragraph(\"Page 2\", styles['Heading1']))\nstory.append(Paragraph(\"Content for page 2\", styles['Normal']))\n\n# Build PDF\ndoc.build(story)\n```\n\n## Command-Line Tools\n\n### pdftotext (poppler-utils)\n```bash\n# Extract text\npdftotext input.pdf output.txt\n\n# Extract text preserving layout\npdftotext -layout input.pdf output.txt\n\n# Extract specific pages\npdftotext -f 1 -l 5 input.pdf output.txt  # Pages 1-5\n```\n\n### qpdf\n```bash\n# Merge PDFs\nqpdf --empty --pages file1.pdf file2.pdf -- merged.pdf\n\n# Split pages\nqpdf input.pdf --pages . 1-5 -- pages1-5.pdf\nqpdf input.pdf --pages . 6-10 -- pages6-10.pdf\n\n# Rotate pages\nqpdf input.pdf output.pdf --rotate=+90:1  # Rotate page 1 by 90 degrees\n\n# Remove password\nqpdf --password=mypassword --decrypt encrypted.pdf decrypted.pdf\n```\n\n### pdftk (if available)\n```bash\n# Merge\npdftk file1.pdf file2.pdf cat output merged.pdf\n\n# Split\npdftk input.pdf burst\n\n# Rotate\npdftk input.pdf rotate 1east output rotated.pdf\n```\n\n## Common Tasks\n\n### Extract Text from Scanned PDFs\n```python\n# Requires: pip install pytesseract pdf2image\nimport pytesseract\nfrom pdf2image import convert_from_path\n\n# Convert PDF to images\nimages = convert_from_path('scanned.pdf')\n\n# OCR each page\ntext = \"\"\nfor i, image in enumerate(images):\n    text += f\"Page {i+1}:\\n\"\n    text += pytesseract.image_to_string(image)\n    text += \"\\n\\n\"\n\nprint(text)\n```\n\n### Add Watermark\n```python\nfrom pypdf import PdfReader, PdfWriter\n\n# Create watermark (or load existing)\nwatermark = PdfReader(\"watermark.pdf\").pages[0]\n\n# Apply to all pages\nreader = PdfReader(\"document.pdf\")\nwriter = PdfWriter()\n\nfor page in reader.pages:\n    page.merge_page(watermark)\n    writer.add_page(page)\n\nwith open(\"watermarked.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n### Extract Images\n```bash\n# Using pdfimages (poppler-utils)\npdfimages -j input.pdf output_prefix\n\n# This extracts all images as output_prefix-000.jpg, output_prefix-001.jpg, etc.\n```\n\n### Password Protection\n```python\nfrom pypdf import PdfReader, PdfWriter\n\nreader = PdfReader(\"input.pdf\")\nwriter = PdfWriter()\n\nfor page in reader.pages:\n    writer.add_page(page)\n\n# Add password\nwriter.encrypt(\"userpassword\", \"ownerpassword\")\n\nwith open(\"encrypted.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n## Quick Reference\n\n| Task | Best Tool | Command/Code |\n|------|-----------|--------------|\n| Merge PDFs | pypdf | `writer.add_page(page)` |\n| Split PDFs | pypdf | One page per file |\n| Extract text | pdfplumber | `page.extract_text()` |\n| Extract tables | pdfplumber | `page.extract_tables()` |\n| Create PDFs | reportlab | Canvas or Platypus |\n| Command line merge | qpdf | `qpdf --empty --pages ...` |\n| OCR scanned PDFs | pytesseract | Convert to image first |\n| Fill PDF forms | pdf-lib or pypdf (see forms.md) | See forms.md |\n\n## Next Steps\n\n- For advanced pypdfium2 usage, see reference.md\n- For JavaScript libraries (pdf-lib), see reference.md\n- If you need to fill out a PDF form, follow the instructions in forms.md\n- For troubleshooting guides, see reference.md\n",
        "skills/resume_optimizer/.claude/skills/pdf/forms.md": "**CRITICAL: You MUST complete these steps in order. Do not skip ahead to writing code.**\n\nIf you need to fill out a PDF form, first check to see if the PDF has fillable form fields. Run this script from this file's directory:\n `python scripts/check_fillable_fields <file.pdf>`, and depending on the result go to either the \"Fillable fields\" or \"Non-fillable fields\" and follow those instructions.\n\n# Fillable fields\nIf the PDF has fillable form fields:\n- Run this script from this file's directory: `python scripts/extract_form_field_info.py <input.pdf> <field_info.json>`. It will create a JSON file with a list of fields in this format:\n```\n[\n  {\n    \"field_id\": (unique ID for the field),\n    \"page\": (page number, 1-based),\n    \"rect\": ([left, bottom, right, top] bounding box in PDF coordinates, y=0 is the bottom of the page),\n    \"type\": (\"text\", \"checkbox\", \"radio_group\", or \"choice\"),\n  },\n  // Checkboxes have \"checked_value\" and \"unchecked_value\" properties:\n  {\n    \"field_id\": (unique ID for the field),\n    \"page\": (page number, 1-based),\n    \"type\": \"checkbox\",\n    \"checked_value\": (Set the field to this value to check the checkbox),\n    \"unchecked_value\": (Set the field to this value to uncheck the checkbox),\n  },\n  // Radio groups have a \"radio_options\" list with the possible choices.\n  {\n    \"field_id\": (unique ID for the field),\n    \"page\": (page number, 1-based),\n    \"type\": \"radio_group\",\n    \"radio_options\": [\n      {\n        \"value\": (set the field to this value to select this radio option),\n        \"rect\": (bounding box for the radio button for this option)\n      },\n      // Other radio options\n    ]\n  },\n  // Multiple choice fields have a \"choice_options\" list with the possible choices:\n  {\n    \"field_id\": (unique ID for the field),\n    \"page\": (page number, 1-based),\n    \"type\": \"choice\",\n    \"choice_options\": [\n      {\n        \"value\": (set the field to this value to select this option),\n        \"text\": (display text of the option)\n      },\n      // Other choice options\n    ],\n  }\n]\n```\n- Convert the PDF to PNGs (one image for each page) with this script (run from this file's directory):\n`python scripts/convert_pdf_to_images.py <file.pdf> <output_directory>`\nThen analyze the images to determine the purpose of each form field (make sure to convert the bounding box PDF coordinates to image coordinates).\n- Create a `field_values.json` file in this format with the values to be entered for each field:\n```\n[\n  {\n    \"field_id\": \"last_name\", // Must match the field_id from `extract_form_field_info.py`\n    \"description\": \"The user's last name\",\n    \"page\": 1, // Must match the \"page\" value in field_info.json\n    \"value\": \"Simpson\"\n  },\n  {\n    \"field_id\": \"Checkbox12\",\n    \"description\": \"Checkbox to be checked if the user is 18 or over\",\n    \"page\": 1,\n    \"value\": \"/On\" // If this is a checkbox, use its \"checked_value\" value to check it. If it's a radio button group, use one of the \"value\" values in \"radio_options\".\n  },\n  // more fields\n]\n```\n- Run the `fill_fillable_fields.py` script from this file's directory to create a filled-in PDF:\n`python scripts/fill_fillable_fields.py <input pdf> <field_values.json> <output pdf>`\nThis script will verify that the field IDs and values you provide are valid; if it prints error messages, correct the appropriate fields and try again.\n\n# Non-fillable fields\nIf the PDF doesn't have fillable form fields, you'll need to visually determine where the data should be added and create text annotations. Follow the below steps *exactly*. You MUST perform all of these steps to ensure that the the form is accurately completed. Details for each step are below.\n- Convert the PDF to PNG images and determine field bounding boxes.\n- Create a JSON file with field information and validation images showing the bounding boxes.\n- Validate the the bounding boxes.\n- Use the bounding boxes to fill in the form.\n\n## Step 1: Visual Analysis (REQUIRED)\n- Convert the PDF to PNG images. Run this script from this file's directory:\n`python scripts/convert_pdf_to_images.py <file.pdf> <output_directory>`\nThe script will create a PNG image for each page in the PDF.\n- Carefully examine each PNG image and identify all form fields and areas where the user should enter data. For each form field where the user should enter text, determine bounding boxes for both the form field label, and the area where the user should enter text. The label and entry bounding boxes MUST NOT INTERSECT; the text entry box should only include the area where data should be entered. Usually this area will be immediately to the side, above, or below its label. Entry bounding boxes must be tall and wide enough to contain their text.\n\nThese are some examples of form structures that you might see:\n\n*Label inside box*\n```\n\n Name:                  \n\n```\nThe input area should be to the right of the \"Name\" label and extend to the edge of the box.\n\n*Label before line*\n```\nEmail: _______________________\n```\nThe input area should be above the line and include its entire width.\n\n*Label under line*\n```\n_________________________\nName\n```\nThe input area should be above the line and include the entire width of the line. This is common for signature and date fields.\n\n*Label above line*\n```\nPlease enter any special requests:\n________________________________________________\n```\nThe input area should extend from the bottom of the label to the line, and should include the entire width of the line.\n\n*Checkboxes*\n```\nAre you a US citizen? Yes   No \n```\nFor checkboxes:\n- Look for small square boxes () - these are the actual checkboxes to target. They may be to the left or right of their labels.\n- Distinguish between label text (\"Yes\", \"No\") and the clickable checkbox squares.\n- The entry bounding box should cover ONLY the small square, not the text label.\n\n### Step 2: Create fields.json and validation images (REQUIRED)\n- Create a file named `fields.json` with information for the form fields and bounding boxes in this format:\n```\n{\n  \"pages\": [\n    {\n      \"page_number\": 1,\n      \"image_width\": (first page image width in pixels),\n      \"image_height\": (first page image height in pixels),\n    },\n    {\n      \"page_number\": 2,\n      \"image_width\": (second page image width in pixels),\n      \"image_height\": (second page image height in pixels),\n    }\n    // additional pages\n  ],\n  \"form_fields\": [\n    // Example for a text field.\n    {\n      \"page_number\": 1,\n      \"description\": \"The user's last name should be entered here\",\n      // Bounding boxes are [left, top, right, bottom]. The bounding boxes for the label and text entry should not overlap.\n      \"field_label\": \"Last name\",\n      \"label_bounding_box\": [30, 125, 95, 142],\n      \"entry_bounding_box\": [100, 125, 280, 142],\n      \"entry_text\": {\n        \"text\": \"Johnson\", // This text will be added as an annotation at the entry_bounding_box location\n        \"font_size\": 14, // optional, defaults to 14\n        \"font_color\": \"000000\", // optional, RRGGBB format, defaults to 000000 (black)\n      }\n    },\n    // Example for a checkbox. TARGET THE SQUARE for the entry bounding box, NOT THE TEXT\n    {\n      \"page_number\": 2,\n      \"description\": \"Checkbox that should be checked if the user is over 18\",\n      \"entry_bounding_box\": [140, 525, 155, 540],  // Small box over checkbox square\n      \"field_label\": \"Yes\",\n      \"label_bounding_box\": [100, 525, 132, 540],  // Box containing \"Yes\" text\n      // Use \"X\" to check a checkbox.\n      \"entry_text\": {\n        \"text\": \"X\",\n      }\n    }\n    // additional form field entries\n  ]\n}\n```\n\nCreate validation images by running this script from this file's directory for each page:\n`python scripts/create_validation_image.py <page_number> <path_to_fields.json> <input_image_path> <output_image_path>\n\nThe validation images will have red rectangles where text should be entered, and blue rectangles covering label text.\n\n### Step 3: Validate Bounding Boxes (REQUIRED)\n#### Automated intersection check\n- Verify that none of bounding boxes intersect and that the entry bounding boxes are tall enough by checking the fields.json file with the `check_bounding_boxes.py` script (run from this file's directory):\n`python scripts/check_bounding_boxes.py <JSON file>`\n\nIf there are errors, reanalyze the relevant fields, adjust the bounding boxes, and iterate until there are no remaining errors. Remember: label (blue) bounding boxes should contain text labels, entry (red) boxes should not.\n\n#### Manual image inspection\n**CRITICAL: Do not proceed without visually inspecting validation images**\n- Red rectangles must ONLY cover input areas\n- Red rectangles MUST NOT contain any text\n- Blue rectangles should contain label text\n- For checkboxes:\n  - Red rectangle MUST be centered on the checkbox square\n  - Blue rectangle should cover the text label for the checkbox\n\n- If any rectangles look wrong, fix fields.json, regenerate the validation images, and verify again. Repeat this process until the bounding boxes are fully accurate.\n\n\n### Step 4: Add annotations to the PDF\nRun this script from this file's directory to create a filled-out PDF using the information in fields.json:\n`python scripts/fill_pdf_form_with_annotations.py <input_pdf_path> <path_to_fields.json> <output_pdf_path>\n",
        "skills/resume_optimizer/.claude/skills/pdf/reference.md": "# PDF Processing Advanced Reference\n\nThis document contains advanced PDF processing features, detailed examples, and additional libraries not covered in the main skill instructions.\n\n## pypdfium2 Library (Apache/BSD License)\n\n### Overview\npypdfium2 is a Python binding for PDFium (Chromium's PDF library). It's excellent for fast PDF rendering, image generation, and serves as a PyMuPDF replacement.\n\n### Render PDF to Images\n```python\nimport pypdfium2 as pdfium\nfrom PIL import Image\n\n# Load PDF\npdf = pdfium.PdfDocument(\"document.pdf\")\n\n# Render page to image\npage = pdf[0]  # First page\nbitmap = page.render(\n    scale=2.0,  # Higher resolution\n    rotation=0  # No rotation\n)\n\n# Convert to PIL Image\nimg = bitmap.to_pil()\nimg.save(\"page_1.png\", \"PNG\")\n\n# Process multiple pages\nfor i, page in enumerate(pdf):\n    bitmap = page.render(scale=1.5)\n    img = bitmap.to_pil()\n    img.save(f\"page_{i+1}.jpg\", \"JPEG\", quality=90)\n```\n\n### Extract Text with pypdfium2\n```python\nimport pypdfium2 as pdfium\n\npdf = pdfium.PdfDocument(\"document.pdf\")\nfor i, page in enumerate(pdf):\n    text = page.get_text()\n    print(f\"Page {i+1} text length: {len(text)} chars\")\n```\n\n## JavaScript Libraries\n\n### pdf-lib (MIT License)\n\npdf-lib is a powerful JavaScript library for creating and modifying PDF documents in any JavaScript environment.\n\n#### Load and Manipulate Existing PDF\n```javascript\nimport { PDFDocument } from 'pdf-lib';\nimport fs from 'fs';\n\nasync function manipulatePDF() {\n    // Load existing PDF\n    const existingPdfBytes = fs.readFileSync('input.pdf');\n    const pdfDoc = await PDFDocument.load(existingPdfBytes);\n\n    // Get page count\n    const pageCount = pdfDoc.getPageCount();\n    console.log(`Document has ${pageCount} pages`);\n\n    // Add new page\n    const newPage = pdfDoc.addPage([600, 400]);\n    newPage.drawText('Added by pdf-lib', {\n        x: 100,\n        y: 300,\n        size: 16\n    });\n\n    // Save modified PDF\n    const pdfBytes = await pdfDoc.save();\n    fs.writeFileSync('modified.pdf', pdfBytes);\n}\n```\n\n#### Create Complex PDFs from Scratch\n```javascript\nimport { PDFDocument, rgb, StandardFonts } from 'pdf-lib';\nimport fs from 'fs';\n\nasync function createPDF() {\n    const pdfDoc = await PDFDocument.create();\n\n    // Add fonts\n    const helveticaFont = await pdfDoc.embedFont(StandardFonts.Helvetica);\n    const helveticaBold = await pdfDoc.embedFont(StandardFonts.HelveticaBold);\n\n    // Add page\n    const page = pdfDoc.addPage([595, 842]); // A4 size\n    const { width, height } = page.getSize();\n\n    // Add text with styling\n    page.drawText('Invoice #12345', {\n        x: 50,\n        y: height - 50,\n        size: 18,\n        font: helveticaBold,\n        color: rgb(0.2, 0.2, 0.8)\n    });\n\n    // Add rectangle (header background)\n    page.drawRectangle({\n        x: 40,\n        y: height - 100,\n        width: width - 80,\n        height: 30,\n        color: rgb(0.9, 0.9, 0.9)\n    });\n\n    // Add table-like content\n    const items = [\n        ['Item', 'Qty', 'Price', 'Total'],\n        ['Widget', '2', '$50', '$100'],\n        ['Gadget', '1', '$75', '$75']\n    ];\n\n    let yPos = height - 150;\n    items.forEach(row => {\n        let xPos = 50;\n        row.forEach(cell => {\n            page.drawText(cell, {\n                x: xPos,\n                y: yPos,\n                size: 12,\n                font: helveticaFont\n            });\n            xPos += 120;\n        });\n        yPos -= 25;\n    });\n\n    const pdfBytes = await pdfDoc.save();\n    fs.writeFileSync('created.pdf', pdfBytes);\n}\n```\n\n#### Advanced Merge and Split Operations\n```javascript\nimport { PDFDocument } from 'pdf-lib';\nimport fs from 'fs';\n\nasync function mergePDFs() {\n    // Create new document\n    const mergedPdf = await PDFDocument.create();\n\n    // Load source PDFs\n    const pdf1Bytes = fs.readFileSync('doc1.pdf');\n    const pdf2Bytes = fs.readFileSync('doc2.pdf');\n\n    const pdf1 = await PDFDocument.load(pdf1Bytes);\n    const pdf2 = await PDFDocument.load(pdf2Bytes);\n\n    // Copy pages from first PDF\n    const pdf1Pages = await mergedPdf.copyPages(pdf1, pdf1.getPageIndices());\n    pdf1Pages.forEach(page => mergedPdf.addPage(page));\n\n    // Copy specific pages from second PDF (pages 0, 2, 4)\n    const pdf2Pages = await mergedPdf.copyPages(pdf2, [0, 2, 4]);\n    pdf2Pages.forEach(page => mergedPdf.addPage(page));\n\n    const mergedPdfBytes = await mergedPdf.save();\n    fs.writeFileSync('merged.pdf', mergedPdfBytes);\n}\n```\n\n### pdfjs-dist (Apache License)\n\nPDF.js is Mozilla's JavaScript library for rendering PDFs in the browser.\n\n#### Basic PDF Loading and Rendering\n```javascript\nimport * as pdfjsLib from 'pdfjs-dist';\n\n// Configure worker (important for performance)\npdfjsLib.GlobalWorkerOptions.workerSrc = './pdf.worker.js';\n\nasync function renderPDF() {\n    // Load PDF\n    const loadingTask = pdfjsLib.getDocument('document.pdf');\n    const pdf = await loadingTask.promise;\n\n    console.log(`Loaded PDF with ${pdf.numPages} pages`);\n\n    // Get first page\n    const page = await pdf.getPage(1);\n    const viewport = page.getViewport({ scale: 1.5 });\n\n    // Render to canvas\n    const canvas = document.createElement('canvas');\n    const context = canvas.getContext('2d');\n    canvas.height = viewport.height;\n    canvas.width = viewport.width;\n\n    const renderContext = {\n        canvasContext: context,\n        viewport: viewport\n    };\n\n    await page.render(renderContext).promise;\n    document.body.appendChild(canvas);\n}\n```\n\n#### Extract Text with Coordinates\n```javascript\nimport * as pdfjsLib from 'pdfjs-dist';\n\nasync function extractText() {\n    const loadingTask = pdfjsLib.getDocument('document.pdf');\n    const pdf = await loadingTask.promise;\n\n    let fullText = '';\n\n    // Extract text from all pages\n    for (let i = 1; i <= pdf.numPages; i++) {\n        const page = await pdf.getPage(i);\n        const textContent = await page.getTextContent();\n\n        const pageText = textContent.items\n            .map(item => item.str)\n            .join(' ');\n\n        fullText += `\\n--- Page ${i} ---\\n${pageText}`;\n\n        // Get text with coordinates for advanced processing\n        const textWithCoords = textContent.items.map(item => ({\n            text: item.str,\n            x: item.transform[4],\n            y: item.transform[5],\n            width: item.width,\n            height: item.height\n        }));\n    }\n\n    console.log(fullText);\n    return fullText;\n}\n```\n\n#### Extract Annotations and Forms\n```javascript\nimport * as pdfjsLib from 'pdfjs-dist';\n\nasync function extractAnnotations() {\n    const loadingTask = pdfjsLib.getDocument('annotated.pdf');\n    const pdf = await loadingTask.promise;\n\n    for (let i = 1; i <= pdf.numPages; i++) {\n        const page = await pdf.getPage(i);\n        const annotations = await page.getAnnotations();\n\n        annotations.forEach(annotation => {\n            console.log(`Annotation type: ${annotation.subtype}`);\n            console.log(`Content: ${annotation.contents}`);\n            console.log(`Coordinates: ${JSON.stringify(annotation.rect)}`);\n        });\n    }\n}\n```\n\n## Advanced Command-Line Operations\n\n### poppler-utils Advanced Features\n\n#### Extract Text with Bounding Box Coordinates\n```bash\n# Extract text with bounding box coordinates (essential for structured data)\npdftotext -bbox-layout document.pdf output.xml\n\n# The XML output contains precise coordinates for each text element\n```\n\n#### Advanced Image Conversion\n```bash\n# Convert to PNG images with specific resolution\npdftoppm -png -r 300 document.pdf output_prefix\n\n# Convert specific page range with high resolution\npdftoppm -png -r 600 -f 1 -l 3 document.pdf high_res_pages\n\n# Convert to JPEG with quality setting\npdftoppm -jpeg -jpegopt quality=85 -r 200 document.pdf jpeg_output\n```\n\n#### Extract Embedded Images\n```bash\n# Extract all embedded images with metadata\npdfimages -j -p document.pdf page_images\n\n# List image info without extracting\npdfimages -list document.pdf\n\n# Extract images in their original format\npdfimages -all document.pdf images/img\n```\n\n### qpdf Advanced Features\n\n#### Complex Page Manipulation\n```bash\n# Split PDF into groups of pages\nqpdf --split-pages=3 input.pdf output_group_%02d.pdf\n\n# Extract specific pages with complex ranges\nqpdf input.pdf --pages input.pdf 1,3-5,8,10-end -- extracted.pdf\n\n# Merge specific pages from multiple PDFs\nqpdf --empty --pages doc1.pdf 1-3 doc2.pdf 5-7 doc3.pdf 2,4 -- combined.pdf\n```\n\n#### PDF Optimization and Repair\n```bash\n# Optimize PDF for web (linearize for streaming)\nqpdf --linearize input.pdf optimized.pdf\n\n# Remove unused objects and compress\nqpdf --optimize-level=all input.pdf compressed.pdf\n\n# Attempt to repair corrupted PDF structure\nqpdf --check input.pdf\nqpdf --fix-qdf damaged.pdf repaired.pdf\n\n# Show detailed PDF structure for debugging\nqpdf --show-all-pages input.pdf > structure.txt\n```\n\n#### Advanced Encryption\n```bash\n# Add password protection with specific permissions\nqpdf --encrypt user_pass owner_pass 256 --print=none --modify=none -- input.pdf encrypted.pdf\n\n# Check encryption status\nqpdf --show-encryption encrypted.pdf\n\n# Remove password protection (requires password)\nqpdf --password=secret123 --decrypt encrypted.pdf decrypted.pdf\n```\n\n## Advanced Python Techniques\n\n### pdfplumber Advanced Features\n\n#### Extract Text with Precise Coordinates\n```python\nimport pdfplumber\n\nwith pdfplumber.open(\"document.pdf\") as pdf:\n    page = pdf.pages[0]\n    \n    # Extract all text with coordinates\n    chars = page.chars\n    for char in chars[:10]:  # First 10 characters\n        print(f\"Char: '{char['text']}' at x:{char['x0']:.1f} y:{char['y0']:.1f}\")\n    \n    # Extract text by bounding box (left, top, right, bottom)\n    bbox_text = page.within_bbox((100, 100, 400, 200)).extract_text()\n```\n\n#### Advanced Table Extraction with Custom Settings\n```python\nimport pdfplumber\nimport pandas as pd\n\nwith pdfplumber.open(\"complex_table.pdf\") as pdf:\n    page = pdf.pages[0]\n    \n    # Extract tables with custom settings for complex layouts\n    table_settings = {\n        \"vertical_strategy\": \"lines\",\n        \"horizontal_strategy\": \"lines\",\n        \"snap_tolerance\": 3,\n        \"intersection_tolerance\": 15\n    }\n    tables = page.extract_tables(table_settings)\n    \n    # Visual debugging for table extraction\n    img = page.to_image(resolution=150)\n    img.save(\"debug_layout.png\")\n```\n\n### reportlab Advanced Features\n\n#### Create Professional Reports with Tables\n```python\nfrom reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph\nfrom reportlab.lib.styles import getSampleStyleSheet\nfrom reportlab.lib import colors\n\n# Sample data\ndata = [\n    ['Product', 'Q1', 'Q2', 'Q3', 'Q4'],\n    ['Widgets', '120', '135', '142', '158'],\n    ['Gadgets', '85', '92', '98', '105']\n]\n\n# Create PDF with table\ndoc = SimpleDocTemplate(\"report.pdf\")\nelements = []\n\n# Add title\nstyles = getSampleStyleSheet()\ntitle = Paragraph(\"Quarterly Sales Report\", styles['Title'])\nelements.append(title)\n\n# Add table with advanced styling\ntable = Table(data)\ntable.setStyle(TableStyle([\n    ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n    ('FONTSIZE', (0, 0), (-1, 0), 14),\n    ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n    ('BACKGROUND', (0, 1), (-1, -1), colors.beige),\n    ('GRID', (0, 0), (-1, -1), 1, colors.black)\n]))\nelements.append(table)\n\ndoc.build(elements)\n```\n\n## Complex Workflows\n\n### Extract Figures/Images from PDF\n\n#### Method 1: Using pdfimages (fastest)\n```bash\n# Extract all images with original quality\npdfimages -all document.pdf images/img\n```\n\n#### Method 2: Using pypdfium2 + Image Processing\n```python\nimport pypdfium2 as pdfium\nfrom PIL import Image\nimport numpy as np\n\ndef extract_figures(pdf_path, output_dir):\n    pdf = pdfium.PdfDocument(pdf_path)\n    \n    for page_num, page in enumerate(pdf):\n        # Render high-resolution page\n        bitmap = page.render(scale=3.0)\n        img = bitmap.to_pil()\n        \n        # Convert to numpy for processing\n        img_array = np.array(img)\n        \n        # Simple figure detection (non-white regions)\n        mask = np.any(img_array != [255, 255, 255], axis=2)\n        \n        # Find contours and extract bounding boxes\n        # (This is simplified - real implementation would need more sophisticated detection)\n        \n        # Save detected figures\n        # ... implementation depends on specific needs\n```\n\n### Batch PDF Processing with Error Handling\n```python\nimport os\nimport glob\nfrom pypdf import PdfReader, PdfWriter\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef batch_process_pdfs(input_dir, operation='merge'):\n    pdf_files = glob.glob(os.path.join(input_dir, \"*.pdf\"))\n    \n    if operation == 'merge':\n        writer = PdfWriter()\n        for pdf_file in pdf_files:\n            try:\n                reader = PdfReader(pdf_file)\n                for page in reader.pages:\n                    writer.add_page(page)\n                logger.info(f\"Processed: {pdf_file}\")\n            except Exception as e:\n                logger.error(f\"Failed to process {pdf_file}: {e}\")\n                continue\n        \n        with open(\"batch_merged.pdf\", \"wb\") as output:\n            writer.write(output)\n    \n    elif operation == 'extract_text':\n        for pdf_file in pdf_files:\n            try:\n                reader = PdfReader(pdf_file)\n                text = \"\"\n                for page in reader.pages:\n                    text += page.extract_text()\n                \n                output_file = pdf_file.replace('.pdf', '.txt')\n                with open(output_file, 'w', encoding='utf-8') as f:\n                    f.write(text)\n                logger.info(f\"Extracted text from: {pdf_file}\")\n                \n            except Exception as e:\n                logger.error(f\"Failed to extract text from {pdf_file}: {e}\")\n                continue\n```\n\n### Advanced PDF Cropping\n```python\nfrom pypdf import PdfWriter, PdfReader\n\nreader = PdfReader(\"input.pdf\")\nwriter = PdfWriter()\n\n# Crop page (left, bottom, right, top in points)\npage = reader.pages[0]\npage.mediabox.left = 50\npage.mediabox.bottom = 50\npage.mediabox.right = 550\npage.mediabox.top = 750\n\nwriter.add_page(page)\nwith open(\"cropped.pdf\", \"wb\") as output:\n    writer.write(output)\n```\n\n## Performance Optimization Tips\n\n### 1. For Large PDFs\n- Use streaming approaches instead of loading entire PDF in memory\n- Use `qpdf --split-pages` for splitting large files\n- Process pages individually with pypdfium2\n\n### 2. For Text Extraction\n- `pdftotext -bbox-layout` is fastest for plain text extraction\n- Use pdfplumber for structured data and tables\n- Avoid `pypdf.extract_text()` for very large documents\n\n### 3. For Image Extraction\n- `pdfimages` is much faster than rendering pages\n- Use low resolution for previews, high resolution for final output\n\n### 4. For Form Filling\n- pdf-lib maintains form structure better than most alternatives\n- Pre-validate form fields before processing\n\n### 5. Memory Management\n```python\n# Process PDFs in chunks\ndef process_large_pdf(pdf_path, chunk_size=10):\n    reader = PdfReader(pdf_path)\n    total_pages = len(reader.pages)\n    \n    for start_idx in range(0, total_pages, chunk_size):\n        end_idx = min(start_idx + chunk_size, total_pages)\n        writer = PdfWriter()\n        \n        for i in range(start_idx, end_idx):\n            writer.add_page(reader.pages[i])\n        \n        # Process chunk\n        with open(f\"chunk_{start_idx//chunk_size}.pdf\", \"wb\") as output:\n            writer.write(output)\n```\n\n## Troubleshooting Common Issues\n\n### Encrypted PDFs\n```python\n# Handle password-protected PDFs\nfrom pypdf import PdfReader\n\ntry:\n    reader = PdfReader(\"encrypted.pdf\")\n    if reader.is_encrypted:\n        reader.decrypt(\"password\")\nexcept Exception as e:\n    print(f\"Failed to decrypt: {e}\")\n```\n\n### Corrupted PDFs\n```bash\n# Use qpdf to repair\nqpdf --check corrupted.pdf\nqpdf --replace-input corrupted.pdf\n```\n\n### Text Extraction Issues\n```python\n# Fallback to OCR for scanned PDFs\nimport pytesseract\nfrom pdf2image import convert_from_path\n\ndef extract_text_with_ocr(pdf_path):\n    images = convert_from_path(pdf_path)\n    text = \"\"\n    for i, image in enumerate(images):\n        text += pytesseract.image_to_string(image)\n    return text\n```\n\n## License Information\n\n- **pypdf**: BSD License\n- **pdfplumber**: MIT License\n- **pypdfium2**: Apache/BSD License\n- **reportlab**: BSD License\n- **poppler-utils**: GPL-2 License\n- **qpdf**: Apache License\n- **pdf-lib**: MIT License\n- **pdfjs-dist**: Apache License",
        "skills/resume_optimizer/.claude/skills/pptx/SKILL.md": "---\nname: pptx\ndescription: \"Presentation creation, editing, and analysis. When Claude needs to work with presentations (.pptx files) for: (1) Creating new presentations, (2) Modifying or editing content, (3) Working with layouts, (4) Adding comments or speaker notes, or any other presentation tasks\"\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# PPTX creation, editing, and analysis\n\n## Overview\n\nA user may ask you to create, edit, or analyze the contents of a .pptx file. A .pptx file is essentially a ZIP archive containing XML files and other resources that you can read or edit. You have different tools and workflows available for different tasks.\n\n## Reading and analyzing content\n\n### Text extraction\nIf you just need to read the text contents of a presentation, you should convert the document to markdown:\n\n```bash\n# Convert document to markdown\npython -m markitdown path-to-file.pptx\n```\n\n### Raw XML access\nYou need raw XML access for: comments, speaker notes, slide layouts, animations, design elements, and complex formatting. For any of these features, you'll need to unpack a presentation and read its raw XML contents.\n\n#### Unpacking a file\n`python ooxml/scripts/unpack.py <office_file> <output_dir>`\n\n**Note**: The unpack.py script is located at `skills/pptx/ooxml/scripts/unpack.py` relative to the project root. If the script doesn't exist at this path, use `find . -name \"unpack.py\"` to locate it.\n\n#### Key file structures\n* `ppt/presentation.xml` - Main presentation metadata and slide references\n* `ppt/slides/slide{N}.xml` - Individual slide contents (slide1.xml, slide2.xml, etc.)\n* `ppt/notesSlides/notesSlide{N}.xml` - Speaker notes for each slide\n* `ppt/comments/modernComment_*.xml` - Comments for specific slides\n* `ppt/slideLayouts/` - Layout templates for slides\n* `ppt/slideMasters/` - Master slide templates\n* `ppt/theme/` - Theme and styling information\n* `ppt/media/` - Images and other media files\n\n#### Typography and color extraction\n**When given an example design to emulate**: Always analyze the presentation's typography and colors first using the methods below:\n1. **Read theme file**: Check `ppt/theme/theme1.xml` for colors (`<a:clrScheme>`) and fonts (`<a:fontScheme>`)\n2. **Sample slide content**: Examine `ppt/slides/slide1.xml` for actual font usage (`<a:rPr>`) and colors\n3. **Search for patterns**: Use grep to find color (`<a:solidFill>`, `<a:srgbClr>`) and font references across all XML files\n\n## Creating a new PowerPoint presentation **without a template**\n\nWhen creating a new PowerPoint presentation from scratch, use the **html2pptx** workflow to convert HTML slides to PowerPoint with accurate positioning.\n\n### Design Principles\n\n**CRITICAL**: Before creating any presentation, analyze the content and choose appropriate design elements:\n1. **Consider the subject matter**: What is this presentation about? What tone, industry, or mood does it suggest?\n2. **Check for branding**: If the user mentions a company/organization, consider their brand colors and identity\n3. **Match palette to content**: Select colors that reflect the subject\n4. **State your approach**: Explain your design choices before writing code\n\n**Requirements**:\n-  State your content-informed design approach BEFORE writing code\n-  Use web-safe fonts only: Arial, Helvetica, Times New Roman, Georgia, Courier New, Verdana, Tahoma, Trebuchet MS, Impact\n-  Create clear visual hierarchy through size, weight, and color\n-  Ensure readability: strong contrast, appropriately sized text, clean alignment\n-  Be consistent: repeat patterns, spacing, and visual language across slides\n\n#### Color Palette Selection\n\n**Choosing colors creatively**:\n- **Think beyond defaults**: What colors genuinely match this specific topic? Avoid autopilot choices.\n- **Consider multiple angles**: Topic, industry, mood, energy level, target audience, brand identity (if mentioned)\n- **Be adventurous**: Try unexpected combinations - a healthcare presentation doesn't have to be green, finance doesn't have to be navy\n- **Build your palette**: Pick 3-5 colors that work together (dominant colors + supporting tones + accent)\n- **Ensure contrast**: Text must be clearly readable on backgrounds\n\n**Example color palettes** (use these to spark creativity - choose one, adapt it, or create your own):\n\n1. **Classic Blue**: Deep navy (#1C2833), slate gray (#2E4053), silver (#AAB7B8), off-white (#F4F6F6)\n2. **Teal & Coral**: Teal (#5EA8A7), deep teal (#277884), coral (#FE4447), white (#FFFFFF)\n3. **Bold Red**: Red (#C0392B), bright red (#E74C3C), orange (#F39C12), yellow (#F1C40F), green (#2ECC71)\n4. **Warm Blush**: Mauve (#A49393), blush (#EED6D3), rose (#E8B4B8), cream (#FAF7F2)\n5. **Burgundy Luxury**: Burgundy (#5D1D2E), crimson (#951233), rust (#C15937), gold (#997929)\n6. **Deep Purple & Emerald**: Purple (#B165FB), dark blue (#181B24), emerald (#40695B), white (#FFFFFF)\n7. **Cream & Forest Green**: Cream (#FFE1C7), forest green (#40695B), white (#FCFCFC)\n8. **Pink & Purple**: Pink (#F8275B), coral (#FF574A), rose (#FF737D), purple (#3D2F68)\n9. **Lime & Plum**: Lime (#C5DE82), plum (#7C3A5F), coral (#FD8C6E), blue-gray (#98ACB5)\n10. **Black & Gold**: Gold (#BF9A4A), black (#000000), cream (#F4F6F6)\n11. **Sage & Terracotta**: Sage (#87A96B), terracotta (#E07A5F), cream (#F4F1DE), charcoal (#2C2C2C)\n12. **Charcoal & Red**: Charcoal (#292929), red (#E33737), light gray (#CCCBCB)\n13. **Vibrant Orange**: Orange (#F96D00), light gray (#F2F2F2), charcoal (#222831)\n14. **Forest Green**: Black (#191A19), green (#4E9F3D), dark green (#1E5128), white (#FFFFFF)\n15. **Retro Rainbow**: Purple (#722880), pink (#D72D51), orange (#EB5C18), amber (#F08800), gold (#DEB600)\n16. **Vintage Earthy**: Mustard (#E3B448), sage (#CBD18F), forest green (#3A6B35), cream (#F4F1DE)\n17. **Coastal Rose**: Old rose (#AD7670), beaver (#B49886), eggshell (#F3ECDC), ash gray (#BFD5BE)\n18. **Orange & Turquoise**: Light orange (#FC993E), grayish turquoise (#667C6F), white (#FCFCFC)\n\n#### Visual Details Options\n\n**Geometric Patterns**:\n- Diagonal section dividers instead of horizontal\n- Asymmetric column widths (30/70, 40/60, 25/75)\n- Rotated text headers at 90 or 270\n- Circular/hexagonal frames for images\n- Triangular accent shapes in corners\n- Overlapping shapes for depth\n\n**Border & Frame Treatments**:\n- Thick single-color borders (10-20pt) on one side only\n- Double-line borders with contrasting colors\n- Corner brackets instead of full frames\n- L-shaped borders (top+left or bottom+right)\n- Underline accents beneath headers (3-5pt thick)\n\n**Typography Treatments**:\n- Extreme size contrast (72pt headlines vs 11pt body)\n- All-caps headers with wide letter spacing\n- Numbered sections in oversized display type\n- Monospace (Courier New) for data/stats/technical content\n- Condensed fonts (Arial Narrow) for dense information\n- Outlined text for emphasis\n\n**Chart & Data Styling**:\n- Monochrome charts with single accent color for key data\n- Horizontal bar charts instead of vertical\n- Dot plots instead of bar charts\n- Minimal gridlines or none at all\n- Data labels directly on elements (no legends)\n- Oversized numbers for key metrics\n\n**Layout Innovations**:\n- Full-bleed images with text overlays\n- Sidebar column (20-30% width) for navigation/context\n- Modular grid systems (33, 44 blocks)\n- Z-pattern or F-pattern content flow\n- Floating text boxes over colored shapes\n- Magazine-style multi-column layouts\n\n**Background Treatments**:\n- Solid color blocks occupying 40-60% of slide\n- Gradient fills (vertical or diagonal only)\n- Split backgrounds (two colors, diagonal or vertical)\n- Edge-to-edge color bands\n- Negative space as a design element\n\n### Layout Tips\n**When creating slides with charts or tables:**\n- **Two-column layout (PREFERRED)**: Use a header spanning the full width, then two columns below - text/bullets in one column and the featured content in the other. This provides better balance and makes charts/tables more readable. Use flexbox with unequal column widths (e.g., 40%/60% split) to optimize space for each content type.\n- **Full-slide layout**: Let the featured content (chart/table) take up the entire slide for maximum impact and readability\n- **NEVER vertically stack**: Do not place charts/tables below text in a single column - this causes poor readability and layout issues\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`html2pptx.md`](html2pptx.md) completely from start to finish. **NEVER set any range limits when reading this file.** Read the full file content for detailed syntax, critical formatting rules, and best practices before proceeding with presentation creation.\n2. Create an HTML file for each slide with proper dimensions (e.g., 720pt  405pt for 16:9)\n   - Use `<p>`, `<h1>`-`<h6>`, `<ul>`, `<ol>` for all text content\n   - Use `class=\"placeholder\"` for areas where charts/tables will be added (render with gray background for visibility)\n   - **CRITICAL**: Rasterize gradients and icons as PNG images FIRST using Sharp, then reference in HTML\n   - **LAYOUT**: For slides with charts/tables/images, use either full-slide layout or two-column layout for better readability\n3. Create and run a JavaScript file using the [`html2pptx.js`](scripts/html2pptx.js) library to convert HTML slides to PowerPoint and save the presentation\n   - Use the `html2pptx()` function to process each HTML file\n   - Add charts and tables to placeholder areas using PptxGenJS API\n   - Save the presentation using `pptx.writeFile()`\n4. **Visual validation**: Generate thumbnails and inspect for layout issues\n   - Create thumbnail grid: `python scripts/thumbnail.py output.pptx workspace/thumbnails --cols 4`\n   - Read and carefully examine the thumbnail image for:\n     - **Text cutoff**: Text being cut off by header bars, shapes, or slide edges\n     - **Text overlap**: Text overlapping with other text or shapes\n     - **Positioning issues**: Content too close to slide boundaries or other elements\n     - **Contrast issues**: Insufficient contrast between text and backgrounds\n   - If issues found, adjust HTML margins/spacing/colors and regenerate the presentation\n   - Repeat until all slides are visually correct\n\n## Editing an existing PowerPoint presentation\n\nWhen edit slides in an existing PowerPoint presentation, you need to work with the raw Office Open XML (OOXML) format. This involves unpacking the .pptx file, editing the XML content, and repacking it.\n\n### Workflow\n1. **MANDATORY - READ ENTIRE FILE**: Read [`ooxml.md`](ooxml.md) (~500 lines) completely from start to finish.  **NEVER set any range limits when reading this file.**  Read the full file content for detailed guidance on OOXML structure and editing workflows before any presentation editing.\n2. Unpack the presentation: `python ooxml/scripts/unpack.py <office_file> <output_dir>`\n3. Edit the XML files (primarily `ppt/slides/slide{N}.xml` and related files)\n4. **CRITICAL**: Validate immediately after each edit and fix any validation errors before proceeding: `python ooxml/scripts/validate.py <dir> --original <file>`\n5. Pack the final presentation: `python ooxml/scripts/pack.py <input_directory> <office_file>`\n\n## Creating a new PowerPoint presentation **using a template**\n\nWhen you need to create a presentation that follows an existing template's design, you'll need to duplicate and re-arrange template slides before then replacing placeholder context.\n\n### Workflow\n1. **Extract template text AND create visual thumbnail grid**:\n   * Extract text: `python -m markitdown template.pptx > template-content.md`\n   * Read `template-content.md`: Read the entire file to understand the contents of the template presentation. **NEVER set any range limits when reading this file.**\n   * Create thumbnail grids: `python scripts/thumbnail.py template.pptx`\n   * See [Creating Thumbnail Grids](#creating-thumbnail-grids) section for more details\n\n2. **Analyze template and save inventory to a file**:\n   * **Visual Analysis**: Review thumbnail grid(s) to understand slide layouts, design patterns, and visual structure\n   * Create and save a template inventory file at `template-inventory.md` containing:\n     ```markdown\n     # Template Inventory Analysis\n     **Total Slides: [count]**\n     **IMPORTANT: Slides are 0-indexed (first slide = 0, last slide = count-1)**\n\n     ## [Category Name]\n     - Slide 0: [Layout code if available] - Description/purpose\n     - Slide 1: [Layout code] - Description/purpose\n     - Slide 2: [Layout code] - Description/purpose\n     [... EVERY slide must be listed individually with its index ...]\n     ```\n   * **Using the thumbnail grid**: Reference the visual thumbnails to identify:\n     - Layout patterns (title slides, content layouts, section dividers)\n     - Image placeholder locations and counts\n     - Design consistency across slide groups\n     - Visual hierarchy and structure\n   * This inventory file is REQUIRED for selecting appropriate templates in the next step\n\n3. **Create presentation outline based on template inventory**:\n   * Review available templates from step 2.\n   * Choose an intro or title template for the first slide. This should be one of the first templates.\n   * Choose safe, text-based layouts for the other slides.\n   * **CRITICAL: Match layout structure to actual content**:\n     - Single-column layouts: Use for unified narrative or single topic\n     - Two-column layouts: Use ONLY when you have exactly 2 distinct items/concepts\n     - Three-column layouts: Use ONLY when you have exactly 3 distinct items/concepts\n     - Image + text layouts: Use ONLY when you have actual images to insert\n     - Quote layouts: Use ONLY for actual quotes from people (with attribution), never for emphasis\n     - Never use layouts with more placeholders than you have content\n     - If you have 2 items, don't force them into a 3-column layout\n     - If you have 4+ items, consider breaking into multiple slides or using a list format\n   * Count your actual content pieces BEFORE selecting the layout\n   * Verify each placeholder in the chosen layout will be filled with meaningful content\n   * Select one option representing the **best** layout for each content section.\n   * Save `outline.md` with content AND template mapping that leverages available designs\n   * Example template mapping:\n      ```\n      # Template slides to use (0-based indexing)\n      # WARNING: Verify indices are within range! Template with 73 slides has indices 0-72\n      # Mapping: slide numbers from outline -> template slide indices\n      template_mapping = [\n          0,   # Use slide 0 (Title/Cover)\n          34,  # Use slide 34 (B1: Title and body)\n          34,  # Use slide 34 again (duplicate for second B1)\n          50,  # Use slide 50 (E1: Quote)\n          54,  # Use slide 54 (F2: Closing + Text)\n      ]\n      ```\n\n4. **Duplicate, reorder, and delete slides using `rearrange.py`**:\n   * Use the `scripts/rearrange.py` script to create a new presentation with slides in the desired order:\n     ```bash\n     python scripts/rearrange.py template.pptx working.pptx 0,34,34,50,52\n     ```\n   * The script handles duplicating repeated slides, deleting unused slides, and reordering automatically\n   * Slide indices are 0-based (first slide is 0, second is 1, etc.)\n   * The same slide index can appear multiple times to duplicate that slide\n\n5. **Extract ALL text using the `inventory.py` script**:\n   * **Run inventory extraction**:\n     ```bash\n     python scripts/inventory.py working.pptx text-inventory.json\n     ```\n   * **Read text-inventory.json**: Read the entire text-inventory.json file to understand all shapes and their properties. **NEVER set any range limits when reading this file.**\n\n   * The inventory JSON structure:\n      ```json\n        {\n          \"slide-0\": {\n            \"shape-0\": {\n              \"placeholder_type\": \"TITLE\",  // or null for non-placeholders\n              \"left\": 1.5,                  // position in inches\n              \"top\": 2.0,\n              \"width\": 7.5,\n              \"height\": 1.2,\n              \"paragraphs\": [\n                {\n                  \"text\": \"Paragraph text\",\n                  // Optional properties (only included when non-default):\n                  \"bullet\": true,           // explicit bullet detected\n                  \"level\": 0,               // only included when bullet is true\n                  \"alignment\": \"CENTER\",    // CENTER, RIGHT (not LEFT)\n                  \"space_before\": 10.0,     // space before paragraph in points\n                  \"space_after\": 6.0,       // space after paragraph in points\n                  \"line_spacing\": 22.4,     // line spacing in points\n                  \"font_name\": \"Arial\",     // from first run\n                  \"font_size\": 14.0,        // in points\n                  \"bold\": true,\n                  \"italic\": false,\n                  \"underline\": false,\n                  \"color\": \"FF0000\"         // RGB color\n                }\n              ]\n            }\n          }\n        }\n      ```\n\n   * Key features:\n     - **Slides**: Named as \"slide-0\", \"slide-1\", etc.\n     - **Shapes**: Ordered by visual position (top-to-bottom, left-to-right) as \"shape-0\", \"shape-1\", etc.\n     - **Placeholder types**: TITLE, CENTER_TITLE, SUBTITLE, BODY, OBJECT, or null\n     - **Default font size**: `default_font_size` in points extracted from layout placeholders (when available)\n     - **Slide numbers are filtered**: Shapes with SLIDE_NUMBER placeholder type are automatically excluded from inventory\n     - **Bullets**: When `bullet: true`, `level` is always included (even if 0)\n     - **Spacing**: `space_before`, `space_after`, and `line_spacing` in points (only included when set)\n     - **Colors**: `color` for RGB (e.g., \"FF0000\"), `theme_color` for theme colors (e.g., \"DARK_1\")\n     - **Properties**: Only non-default values are included in the output\n\n6. **Generate replacement text and save the data to a JSON file**\n   Based on the text inventory from the previous step:\n   - **CRITICAL**: First verify which shapes exist in the inventory - only reference shapes that are actually present\n   - **VALIDATION**: The replace.py script will validate that all shapes in your replacement JSON exist in the inventory\n     - If you reference a non-existent shape, you'll get an error showing available shapes\n     - If you reference a non-existent slide, you'll get an error indicating the slide doesn't exist\n     - All validation errors are shown at once before the script exits\n   - **IMPORTANT**: The replace.py script uses inventory.py internally to identify ALL text shapes\n   - **AUTOMATIC CLEARING**: ALL text shapes from the inventory will be cleared unless you provide \"paragraphs\" for them\n   - Add a \"paragraphs\" field to shapes that need content (not \"replacement_paragraphs\")\n   - Shapes without \"paragraphs\" in the replacement JSON will have their text cleared automatically\n   - Paragraphs with bullets will be automatically left aligned. Don't set the `alignment` property on when `\"bullet\": true`\n   - Generate appropriate replacement content for placeholder text\n   - Use shape size to determine appropriate content length\n   - **CRITICAL**: Include paragraph properties from the original inventory - don't just provide text\n   - **IMPORTANT**: When bullet: true, do NOT include bullet symbols (, -, *) in text - they're added automatically\n   - **ESSENTIAL FORMATTING RULES**:\n     - Headers/titles should typically have `\"bold\": true`\n     - List items should have `\"bullet\": true, \"level\": 0` (level is required when bullet is true)\n     - Preserve any alignment properties (e.g., `\"alignment\": \"CENTER\"` for centered text)\n     - Include font properties when different from default (e.g., `\"font_size\": 14.0`, `\"font_name\": \"Lora\"`)\n     - Colors: Use `\"color\": \"FF0000\"` for RGB or `\"theme_color\": \"DARK_1\"` for theme colors\n     - The replacement script expects **properly formatted paragraphs**, not just text strings\n     - **Overlapping shapes**: Prefer shapes with larger default_font_size or more appropriate placeholder_type\n   - Save the updated inventory with replacements to `replacement-text.json`\n   - **WARNING**: Different template layouts have different shape counts - always check the actual inventory before creating replacements\n\n   Example paragraphs field showing proper formatting:\n   ```json\n   \"paragraphs\": [\n     {\n       \"text\": \"New presentation title text\",\n       \"alignment\": \"CENTER\",\n       \"bold\": true\n     },\n     {\n       \"text\": \"Section Header\",\n       \"bold\": true\n     },\n     {\n       \"text\": \"First bullet point without bullet symbol\",\n       \"bullet\": true,\n       \"level\": 0\n     },\n     {\n       \"text\": \"Red colored text\",\n       \"color\": \"FF0000\"\n     },\n     {\n       \"text\": \"Theme colored text\",\n       \"theme_color\": \"DARK_1\"\n     },\n     {\n       \"text\": \"Regular paragraph text without special formatting\"\n     }\n   ]\n   ```\n\n   **Shapes not listed in the replacement JSON are automatically cleared**:\n   ```json\n   {\n     \"slide-0\": {\n       \"shape-0\": {\n         \"paragraphs\": [...] // This shape gets new text\n       }\n       // shape-1 and shape-2 from inventory will be cleared automatically\n     }\n   }\n   ```\n\n   **Common formatting patterns for presentations**:\n   - Title slides: Bold text, sometimes centered\n   - Section headers within slides: Bold text\n   - Bullet lists: Each item needs `\"bullet\": true, \"level\": 0`\n   - Body text: Usually no special properties needed\n   - Quotes: May have special alignment or font properties\n\n7. **Apply replacements using the `replace.py` script**\n   ```bash\n   python scripts/replace.py working.pptx replacement-text.json output.pptx\n   ```\n\n   The script will:\n   - First extract the inventory of ALL text shapes using functions from inventory.py\n   - Validate that all shapes in the replacement JSON exist in the inventory\n   - Clear text from ALL shapes identified in the inventory\n   - Apply new text only to shapes with \"paragraphs\" defined in the replacement JSON\n   - Preserve formatting by applying paragraph properties from the JSON\n   - Handle bullets, alignment, font properties, and colors automatically\n   - Save the updated presentation\n\n   Example validation errors:\n   ```\n   ERROR: Invalid shapes in replacement JSON:\n     - Shape 'shape-99' not found on 'slide-0'. Available shapes: shape-0, shape-1, shape-4\n     - Slide 'slide-999' not found in inventory\n   ```\n\n   ```\n   ERROR: Replacement text made overflow worse in these shapes:\n     - slide-0/shape-2: overflow worsened by 1.25\" (was 0.00\", now 1.25\")\n   ```\n\n## Creating Thumbnail Grids\n\nTo create visual thumbnail grids of PowerPoint slides for quick analysis and reference:\n\n```bash\npython scripts/thumbnail.py template.pptx [output_prefix]\n```\n\n**Features**:\n- Creates: `thumbnails.jpg` (or `thumbnails-1.jpg`, `thumbnails-2.jpg`, etc. for large decks)\n- Default: 5 columns, max 30 slides per grid (56)\n- Custom prefix: `python scripts/thumbnail.py template.pptx my-grid`\n  - Note: The output prefix should include the path if you want output in a specific directory (e.g., `workspace/my-grid`)\n- Adjust columns: `--cols 4` (range: 3-6, affects slides per grid)\n- Grid limits: 3 cols = 12 slides/grid, 4 cols = 20, 5 cols = 30, 6 cols = 42\n- Slides are zero-indexed (Slide 0, Slide 1, etc.)\n\n**Use cases**:\n- Template analysis: Quickly understand slide layouts and design patterns\n- Content review: Visual overview of entire presentation\n- Navigation reference: Find specific slides by their visual appearance\n- Quality check: Verify all slides are properly formatted\n\n**Examples**:\n```bash\n# Basic usage\npython scripts/thumbnail.py presentation.pptx\n\n# Combine options: custom name, columns\npython scripts/thumbnail.py template.pptx analysis --cols 4\n```\n\n## Converting Slides to Images\n\nTo visually analyze PowerPoint slides, convert them to images using a two-step process:\n\n1. **Convert PPTX to PDF**:\n   ```bash\n   soffice --headless --convert-to pdf template.pptx\n   ```\n\n2. **Convert PDF pages to JPEG images**:\n   ```bash\n   pdftoppm -jpeg -r 150 template.pdf slide\n   ```\n   This creates files like `slide-1.jpg`, `slide-2.jpg`, etc.\n\nOptions:\n- `-r 150`: Sets resolution to 150 DPI (adjust for quality/size balance)\n- `-jpeg`: Output JPEG format (use `-png` for PNG if preferred)\n- `-f N`: First page to convert (e.g., `-f 2` starts from page 2)\n- `-l N`: Last page to convert (e.g., `-l 5` stops at page 5)\n- `slide`: Prefix for output files\n\nExample for specific range:\n```bash\npdftoppm -jpeg -r 150 -f 2 -l 5 template.pdf slide  # Converts only pages 2-5\n```\n\n## Code Style Guidelines\n**IMPORTANT**: When generating code for PPTX operations:\n- Write concise code\n- Avoid verbose variable names and redundant operations\n- Avoid unnecessary print statements\n\n## Dependencies\n\nRequired dependencies (should already be installed):\n\n- **markitdown**: `pip install \"markitdown[pptx]\"` (for text extraction from presentations)\n- **pptxgenjs**: `npm install -g pptxgenjs` (for creating presentations via html2pptx)\n- **playwright**: `npm install -g playwright` (for HTML rendering in html2pptx)\n- **react-icons**: `npm install -g react-icons react react-dom` (for icons)\n- **sharp**: `npm install -g sharp` (for SVG rasterization and image processing)\n- **LibreOffice**: `sudo apt-get install libreoffice` (for PDF conversion)\n- **Poppler**: `sudo apt-get install poppler-utils` (for pdftoppm to convert PDF to images)\n- **defusedxml**: `pip install defusedxml` (for secure XML parsing)",
        "skills/resume_optimizer/.claude/skills/pptx/html2pptx.md": "# HTML to PowerPoint Guide\n\nConvert HTML slides to PowerPoint presentations with accurate positioning using the `html2pptx.js` library.\n\n## Table of Contents\n\n1. [Creating HTML Slides](#creating-html-slides)\n2. [Using the html2pptx Library](#using-the-html2pptx-library)\n3. [Using PptxGenJS](#using-pptxgenjs)\n\n---\n\n## Creating HTML Slides\n\nEvery HTML slide must include proper body dimensions:\n\n### Layout Dimensions\n\n- **16:9** (default): `width: 720pt; height: 405pt`\n- **4:3**: `width: 720pt; height: 540pt`\n- **16:10**: `width: 720pt; height: 450pt`\n\n### Supported Elements\n\n- `<p>`, `<h1>`-`<h6>` - Text with styling\n- `<ul>`, `<ol>` - Lists (never use manual bullets , -, *)\n- `<b>`, `<strong>` - Bold text (inline formatting)\n- `<i>`, `<em>` - Italic text (inline formatting)\n- `<u>` - Underlined text (inline formatting)\n- `<span>` - Inline formatting with CSS styles (bold, italic, underline, color)\n- `<br>` - Line breaks\n- `<div>` with bg/border - Becomes shape\n- `<img>` - Images\n- `class=\"placeholder\"` - Reserved space for charts (returns `{ id, x, y, w, h }`)\n\n### Critical Text Rules\n\n**ALL text MUST be inside `<p>`, `<h1>`-`<h6>`, `<ul>`, or `<ol>` tags:**\n-  Correct: `<div><p>Text here</p></div>`\n-  Wrong: `<div>Text here</div>` - **Text will NOT appear in PowerPoint**\n-  Wrong: `<span>Text</span>` - **Text will NOT appear in PowerPoint**\n- Text in `<div>` or `<span>` without a text tag will be silently ignored\n\n**NEVER use manual bullet symbols (, -, *, etc.)** - Use `<ul>` or `<ol>` lists instead\n\n**ONLY use web-safe fonts that are universally available:**\n-  Web-safe fonts: `Arial`, `Helvetica`, `Times New Roman`, `Georgia`, `Courier New`, `Verdana`, `Tahoma`, `Trebuchet MS`, `Impact`, `Comic Sans MS`\n-  Wrong: `'Segoe UI'`, `'SF Pro'`, `'Roboto'`, custom fonts - **Might cause rendering issues**\n\n### Styling\n\n- Use `display: flex` on body to prevent margin collapse from breaking overflow validation\n- Use `margin` for spacing (padding included in size)\n- Inline formatting: Use `<b>`, `<i>`, `<u>` tags OR `<span>` with CSS styles\n  - `<span>` supports: `font-weight: bold`, `font-style: italic`, `text-decoration: underline`, `color: #rrggbb`\n  - `<span>` does NOT support: `margin`, `padding` (not supported in PowerPoint text runs)\n  - Example: `<span style=\"font-weight: bold; color: #667eea;\">Bold blue text</span>`\n- Flexbox works - positions calculated from rendered layout\n- Use hex colors with `#` prefix in CSS\n- **Text alignment**: Use CSS `text-align` (`center`, `right`, etc.) when needed as a hint to PptxGenJS for text formatting if text lengths are slightly off\n\n### Shape Styling (DIV elements only)\n\n**IMPORTANT: Backgrounds, borders, and shadows only work on `<div>` elements, NOT on text elements (`<p>`, `<h1>`-`<h6>`, `<ul>`, `<ol>`)**\n\n- **Backgrounds**: CSS `background` or `background-color` on `<div>` elements only\n  - Example: `<div style=\"background: #f0f0f0;\">` - Creates a shape with background\n- **Borders**: CSS `border` on `<div>` elements converts to PowerPoint shape borders\n  - Supports uniform borders: `border: 2px solid #333333`\n  - Supports partial borders: `border-left`, `border-right`, `border-top`, `border-bottom` (rendered as line shapes)\n  - Example: `<div style=\"border-left: 8pt solid #E76F51;\">`\n- **Border radius**: CSS `border-radius` on `<div>` elements for rounded corners\n  - `border-radius: 50%` or higher creates circular shape\n  - Percentages <50% calculated relative to shape's smaller dimension\n  - Supports px and pt units (e.g., `border-radius: 8pt;`, `border-radius: 12px;`)\n  - Example: `<div style=\"border-radius: 25%;\">` on 100x200px box = 25% of 100px = 25px radius\n- **Box shadows**: CSS `box-shadow` on `<div>` elements converts to PowerPoint shadows\n  - Supports outer shadows only (inset shadows are ignored to prevent corruption)\n  - Example: `<div style=\"box-shadow: 2px 2px 8px rgba(0, 0, 0, 0.3);\">`\n  - Note: Inset/inner shadows are not supported by PowerPoint and will be skipped\n\n### Icons & Gradients\n\n- **CRITICAL: Never use CSS gradients (`linear-gradient`, `radial-gradient`)** - They don't convert to PowerPoint\n- **ALWAYS create gradient/icon PNGs FIRST using Sharp, then reference in HTML**\n- For gradients: Rasterize SVG to PNG background images\n- For icons: Rasterize react-icons SVG to PNG images\n- All visual effects must be pre-rendered as raster images before HTML rendering\n\n**Rasterizing Icons with Sharp:**\n\n```javascript\nconst React = require('react');\nconst ReactDOMServer = require('react-dom/server');\nconst sharp = require('sharp');\nconst { FaHome } = require('react-icons/fa');\n\nasync function rasterizeIconPng(IconComponent, color, size = \"256\", filename) {\n  const svgString = ReactDOMServer.renderToStaticMarkup(\n    React.createElement(IconComponent, { color: `#${color}`, size: size })\n  );\n\n  // Convert SVG to PNG using Sharp\n  await sharp(Buffer.from(svgString))\n    .png()\n    .toFile(filename);\n\n  return filename;\n}\n\n// Usage: Rasterize icon before using in HTML\nconst iconPath = await rasterizeIconPng(FaHome, \"4472c4\", \"256\", \"home-icon.png\");\n// Then reference in HTML: <img src=\"home-icon.png\" style=\"width: 40pt; height: 40pt;\">\n```\n\n**Rasterizing Gradients with Sharp:**\n\n```javascript\nconst sharp = require('sharp');\n\nasync function createGradientBackground(filename) {\n  const svg = `<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1000\" height=\"562.5\">\n    <defs>\n      <linearGradient id=\"g\" x1=\"0%\" y1=\"0%\" x2=\"100%\" y2=\"100%\">\n        <stop offset=\"0%\" style=\"stop-color:#COLOR1\"/>\n        <stop offset=\"100%\" style=\"stop-color:#COLOR2\"/>\n      </linearGradient>\n    </defs>\n    <rect width=\"100%\" height=\"100%\" fill=\"url(#g)\"/>\n  </svg>`;\n\n  await sharp(Buffer.from(svg))\n    .png()\n    .toFile(filename);\n\n  return filename;\n}\n\n// Usage: Create gradient background before HTML\nconst bgPath = await createGradientBackground(\"gradient-bg.png\");\n// Then in HTML: <body style=\"background-image: url('gradient-bg.png');\">\n```\n\n### Example\n\n```html\n<!DOCTYPE html>\n<html>\n<head>\n<style>\nhtml { background: #ffffff; }\nbody {\n  width: 720pt; height: 405pt; margin: 0; padding: 0;\n  background: #f5f5f5; font-family: Arial, sans-serif;\n  display: flex;\n}\n.content { margin: 30pt; padding: 40pt; background: #ffffff; border-radius: 8pt; }\nh1 { color: #2d3748; font-size: 32pt; }\n.box {\n  background: #70ad47; padding: 20pt; border: 3px solid #5a8f37;\n  border-radius: 12pt; box-shadow: 3px 3px 10px rgba(0, 0, 0, 0.25);\n}\n</style>\n</head>\n<body>\n<div class=\"content\">\n  <h1>Recipe Title</h1>\n  <ul>\n    <li><b>Item:</b> Description</li>\n  </ul>\n  <p>Text with <b>bold</b>, <i>italic</i>, <u>underline</u>.</p>\n  <div id=\"chart\" class=\"placeholder\" style=\"width: 350pt; height: 200pt;\"></div>\n\n  <!-- Text MUST be in <p> tags -->\n  <div class=\"box\">\n    <p>5</p>\n  </div>\n</div>\n</body>\n</html>\n```\n\n## Using the html2pptx Library\n\n### Dependencies\n\nThese libraries have been globally installed and are available to use:\n- `pptxgenjs`\n- `playwright`\n- `sharp`\n\n### Basic Usage\n\n```javascript\nconst pptxgen = require('pptxgenjs');\nconst html2pptx = require('./html2pptx');\n\nconst pptx = new pptxgen();\npptx.layout = 'LAYOUT_16x9';  // Must match HTML body dimensions\n\nconst { slide, placeholders } = await html2pptx('slide1.html', pptx);\n\n// Add chart to placeholder area\nif (placeholders.length > 0) {\n    slide.addChart(pptx.charts.LINE, chartData, placeholders[0]);\n}\n\nawait pptx.writeFile('output.pptx');\n```\n\n### API Reference\n\n#### Function Signature\n```javascript\nawait html2pptx(htmlFile, pres, options)\n```\n\n#### Parameters\n- `htmlFile` (string): Path to HTML file (absolute or relative)\n- `pres` (pptxgen): PptxGenJS presentation instance with layout already set\n- `options` (object, optional):\n  - `tmpDir` (string): Temporary directory for generated files (default: `process.env.TMPDIR || '/tmp'`)\n  - `slide` (object): Existing slide to reuse (default: creates new slide)\n\n#### Returns\n```javascript\n{\n    slide: pptxgenSlide,           // The created/updated slide\n    placeholders: [                 // Array of placeholder positions\n        { id: string, x: number, y: number, w: number, h: number },\n        ...\n    ]\n}\n```\n\n### Validation\n\nThe library automatically validates and collects all errors before throwing:\n\n1. **HTML dimensions must match presentation layout** - Reports dimension mismatches\n2. **Content must not overflow body** - Reports overflow with exact measurements\n3. **CSS gradients** - Reports unsupported gradient usage\n4. **Text element styling** - Reports backgrounds/borders/shadows on text elements (only allowed on divs)\n\n**All validation errors are collected and reported together** in a single error message, allowing you to fix all issues at once instead of one at a time.\n\n### Working with Placeholders\n\n```javascript\nconst { slide, placeholders } = await html2pptx('slide.html', pptx);\n\n// Use first placeholder\nslide.addChart(pptx.charts.BAR, data, placeholders[0]);\n\n// Find by ID\nconst chartArea = placeholders.find(p => p.id === 'chart-area');\nslide.addChart(pptx.charts.LINE, data, chartArea);\n```\n\n### Complete Example\n\n```javascript\nconst pptxgen = require('pptxgenjs');\nconst html2pptx = require('./html2pptx');\n\nasync function createPresentation() {\n    const pptx = new pptxgen();\n    pptx.layout = 'LAYOUT_16x9';\n    pptx.author = 'Your Name';\n    pptx.title = 'My Presentation';\n\n    // Slide 1: Title\n    const { slide: slide1 } = await html2pptx('slides/title.html', pptx);\n\n    // Slide 2: Content with chart\n    const { slide: slide2, placeholders } = await html2pptx('slides/data.html', pptx);\n\n    const chartData = [{\n        name: 'Sales',\n        labels: ['Q1', 'Q2', 'Q3', 'Q4'],\n        values: [4500, 5500, 6200, 7100]\n    }];\n\n    slide2.addChart(pptx.charts.BAR, chartData, {\n        ...placeholders[0],\n        showTitle: true,\n        title: 'Quarterly Sales',\n        showCatAxisTitle: true,\n        catAxisTitle: 'Quarter',\n        showValAxisTitle: true,\n        valAxisTitle: 'Sales ($000s)'\n    });\n\n    // Save\n    await pptx.writeFile({ fileName: 'presentation.pptx' });\n    console.log('Presentation created successfully!');\n}\n\ncreatePresentation().catch(console.error);\n```\n\n## Using PptxGenJS\n\nAfter converting HTML to slides with `html2pptx`, you'll use PptxGenJS to add dynamic content like charts, images, and additional elements.\n\n###  Critical Rules\n\n#### Colors\n- **NEVER use `#` prefix** with hex colors in PptxGenJS - causes file corruption\n-  Correct: `color: \"FF0000\"`, `fill: { color: \"0066CC\" }`\n-  Wrong: `color: \"#FF0000\"` (breaks document)\n\n### Adding Images\n\nAlways calculate aspect ratios from actual image dimensions:\n\n```javascript\n// Get image dimensions: identify image.png | grep -o '[0-9]* x [0-9]*'\nconst imgWidth = 1860, imgHeight = 1519;  // From actual file\nconst aspectRatio = imgWidth / imgHeight;\n\nconst h = 3;  // Max height\nconst w = h * aspectRatio;\nconst x = (10 - w) / 2;  // Center on 16:9 slide\n\nslide.addImage({ path: \"chart.png\", x, y: 1.5, w, h });\n```\n\n### Adding Text\n\n```javascript\n// Rich text with formatting\nslide.addText([\n    { text: \"Bold \", options: { bold: true } },\n    { text: \"Italic \", options: { italic: true } },\n    { text: \"Normal\" }\n], {\n    x: 1, y: 2, w: 8, h: 1\n});\n```\n\n### Adding Shapes\n\n```javascript\n// Rectangle\nslide.addShape(pptx.shapes.RECTANGLE, {\n    x: 1, y: 1, w: 3, h: 2,\n    fill: { color: \"4472C4\" },\n    line: { color: \"000000\", width: 2 }\n});\n\n// Circle\nslide.addShape(pptx.shapes.OVAL, {\n    x: 5, y: 1, w: 2, h: 2,\n    fill: { color: \"ED7D31\" }\n});\n\n// Rounded rectangle\nslide.addShape(pptx.shapes.ROUNDED_RECTANGLE, {\n    x: 1, y: 4, w: 3, h: 1.5,\n    fill: { color: \"70AD47\" },\n    rectRadius: 0.2\n});\n```\n\n### Adding Charts\n\n**Required for most charts:** Axis labels using `catAxisTitle` (category) and `valAxisTitle` (value).\n\n**Chart Data Format:**\n- Use **single series with all labels** for simple bar/line charts\n- Each series creates a separate legend entry\n- Labels array defines X-axis values\n\n**Time Series Data - Choose Correct Granularity:**\n- **< 30 days**: Use daily grouping (e.g., \"10-01\", \"10-02\") - avoid monthly aggregation that creates single-point charts\n- **30-365 days**: Use monthly grouping (e.g., \"2024-01\", \"2024-02\")\n- **> 365 days**: Use yearly grouping (e.g., \"2023\", \"2024\")\n- **Validate**: Charts with only 1 data point likely indicate incorrect aggregation for the time period\n\n```javascript\nconst { slide, placeholders } = await html2pptx('slide.html', pptx);\n\n// CORRECT: Single series with all labels\nslide.addChart(pptx.charts.BAR, [{\n    name: \"Sales 2024\",\n    labels: [\"Q1\", \"Q2\", \"Q3\", \"Q4\"],\n    values: [4500, 5500, 6200, 7100]\n}], {\n    ...placeholders[0],  // Use placeholder position\n    barDir: 'col',       // 'col' = vertical bars, 'bar' = horizontal\n    showTitle: true,\n    title: 'Quarterly Sales',\n    showLegend: false,   // No legend needed for single series\n    // Required axis labels\n    showCatAxisTitle: true,\n    catAxisTitle: 'Quarter',\n    showValAxisTitle: true,\n    valAxisTitle: 'Sales ($000s)',\n    // Optional: Control scaling (adjust min based on data range for better visualization)\n    valAxisMaxVal: 8000,\n    valAxisMinVal: 0,  // Use 0 for counts/amounts; for clustered data (e.g., 4500-7100), consider starting closer to min value\n    valAxisMajorUnit: 2000,  // Control y-axis label spacing to prevent crowding\n    catAxisLabelRotate: 45,  // Rotate labels if crowded\n    dataLabelPosition: 'outEnd',\n    dataLabelColor: '000000',\n    // Use single color for single-series charts\n    chartColors: [\"4472C4\"]  // All bars same color\n});\n```\n\n#### Scatter Chart\n\n**IMPORTANT**: Scatter chart data format is unusual - first series contains X-axis values, subsequent series contain Y-values:\n\n```javascript\n// Prepare data\nconst data1 = [{ x: 10, y: 20 }, { x: 15, y: 25 }, { x: 20, y: 30 }];\nconst data2 = [{ x: 12, y: 18 }, { x: 18, y: 22 }];\n\nconst allXValues = [...data1.map(d => d.x), ...data2.map(d => d.x)];\n\nslide.addChart(pptx.charts.SCATTER, [\n    { name: 'X-Axis', values: allXValues },  // First series = X values\n    { name: 'Series 1', values: data1.map(d => d.y) },  // Y values only\n    { name: 'Series 2', values: data2.map(d => d.y) }   // Y values only\n], {\n    x: 1, y: 1, w: 8, h: 4,\n    lineSize: 0,  // 0 = no connecting lines\n    lineDataSymbol: 'circle',\n    lineDataSymbolSize: 6,\n    showCatAxisTitle: true,\n    catAxisTitle: 'X Axis',\n    showValAxisTitle: true,\n    valAxisTitle: 'Y Axis',\n    chartColors: [\"4472C4\", \"ED7D31\"]\n});\n```\n\n#### Line Chart\n\n```javascript\nslide.addChart(pptx.charts.LINE, [{\n    name: \"Temperature\",\n    labels: [\"Jan\", \"Feb\", \"Mar\", \"Apr\"],\n    values: [32, 35, 42, 55]\n}], {\n    x: 1, y: 1, w: 8, h: 4,\n    lineSize: 4,\n    lineSmooth: true,\n    // Required axis labels\n    showCatAxisTitle: true,\n    catAxisTitle: 'Month',\n    showValAxisTitle: true,\n    valAxisTitle: 'Temperature (F)',\n    // Optional: Y-axis range (set min based on data range for better visualization)\n    valAxisMinVal: 0,     // For ranges starting at 0 (counts, percentages, etc.)\n    valAxisMaxVal: 60,\n    valAxisMajorUnit: 20,  // Control y-axis label spacing to prevent crowding (e.g., 10, 20, 25)\n    // valAxisMinVal: 30,  // PREFERRED: For data clustered in a range (e.g., 32-55 or ratings 3-5), start axis closer to min value to show variation\n    // Optional: Chart colors\n    chartColors: [\"4472C4\", \"ED7D31\", \"A5A5A5\"]\n});\n```\n\n#### Pie Chart (No Axis Labels Required)\n\n**CRITICAL**: Pie charts require a **single data series** with all categories in the `labels` array and corresponding values in the `values` array.\n\n```javascript\nslide.addChart(pptx.charts.PIE, [{\n    name: \"Market Share\",\n    labels: [\"Product A\", \"Product B\", \"Other\"],  // All categories in one array\n    values: [35, 45, 20]  // All values in one array\n}], {\n    x: 2, y: 1, w: 6, h: 4,\n    showPercent: true,\n    showLegend: true,\n    legendPos: 'r',  // right\n    chartColors: [\"4472C4\", \"ED7D31\", \"A5A5A5\"]\n});\n```\n\n#### Multiple Data Series\n\n```javascript\nslide.addChart(pptx.charts.LINE, [\n    {\n        name: \"Product A\",\n        labels: [\"Q1\", \"Q2\", \"Q3\", \"Q4\"],\n        values: [10, 20, 30, 40]\n    },\n    {\n        name: \"Product B\",\n        labels: [\"Q1\", \"Q2\", \"Q3\", \"Q4\"],\n        values: [15, 25, 20, 35]\n    }\n], {\n    x: 1, y: 1, w: 8, h: 4,\n    showCatAxisTitle: true,\n    catAxisTitle: 'Quarter',\n    showValAxisTitle: true,\n    valAxisTitle: 'Revenue ($M)'\n});\n```\n\n### Chart Colors\n\n**CRITICAL**: Use hex colors **without** the `#` prefix - including `#` causes file corruption.\n\n**Align chart colors with your chosen design palette**, ensuring sufficient contrast and distinctiveness for data visualization. Adjust colors for:\n- Strong contrast between adjacent series\n- Readability against slide backgrounds\n- Accessibility (avoid red-green only combinations)\n\n```javascript\n// Example: Ocean palette-inspired chart colors (adjusted for contrast)\nconst chartColors = [\"16A085\", \"FF6B9D\", \"2C3E50\", \"F39C12\", \"9B59B6\"];\n\n// Single-series chart: Use one color for all bars/points\nslide.addChart(pptx.charts.BAR, [{\n    name: \"Sales\",\n    labels: [\"Q1\", \"Q2\", \"Q3\", \"Q4\"],\n    values: [4500, 5500, 6200, 7100]\n}], {\n    ...placeholders[0],\n    chartColors: [\"16A085\"],  // All bars same color\n    showLegend: false\n});\n\n// Multi-series chart: Each series gets a different color\nslide.addChart(pptx.charts.LINE, [\n    { name: \"Product A\", labels: [\"Q1\", \"Q2\", \"Q3\"], values: [10, 20, 30] },\n    { name: \"Product B\", labels: [\"Q1\", \"Q2\", \"Q3\"], values: [15, 25, 20] }\n], {\n    ...placeholders[0],\n    chartColors: [\"16A085\", \"FF6B9D\"]  // One color per series\n});\n```\n\n### Adding Tables\n\nTables can be added with basic or advanced formatting:\n\n#### Basic Table\n\n```javascript\nslide.addTable([\n    [\"Header 1\", \"Header 2\", \"Header 3\"],\n    [\"Row 1, Col 1\", \"Row 1, Col 2\", \"Row 1, Col 3\"],\n    [\"Row 2, Col 1\", \"Row 2, Col 2\", \"Row 2, Col 3\"]\n], {\n    x: 0.5,\n    y: 1,\n    w: 9,\n    h: 3,\n    border: { pt: 1, color: \"999999\" },\n    fill: { color: \"F1F1F1\" }\n});\n```\n\n#### Table with Custom Formatting\n\n```javascript\nconst tableData = [\n    // Header row with custom styling\n    [\n        { text: \"Product\", options: { fill: { color: \"4472C4\" }, color: \"FFFFFF\", bold: true } },\n        { text: \"Revenue\", options: { fill: { color: \"4472C4\" }, color: \"FFFFFF\", bold: true } },\n        { text: \"Growth\", options: { fill: { color: \"4472C4\" }, color: \"FFFFFF\", bold: true } }\n    ],\n    // Data rows\n    [\"Product A\", \"$50M\", \"+15%\"],\n    [\"Product B\", \"$35M\", \"+22%\"],\n    [\"Product C\", \"$28M\", \"+8%\"]\n];\n\nslide.addTable(tableData, {\n    x: 1,\n    y: 1.5,\n    w: 8,\n    h: 3,\n    colW: [3, 2.5, 2.5],  // Column widths\n    rowH: [0.5, 0.6, 0.6, 0.6],  // Row heights\n    border: { pt: 1, color: \"CCCCCC\" },\n    align: \"center\",\n    valign: \"middle\",\n    fontSize: 14\n});\n```\n\n#### Table with Merged Cells\n\n```javascript\nconst mergedTableData = [\n    [\n        { text: \"Q1 Results\", options: { colspan: 3, fill: { color: \"4472C4\" }, color: \"FFFFFF\", bold: true } }\n    ],\n    [\"Product\", \"Sales\", \"Market Share\"],\n    [\"Product A\", \"$25M\", \"35%\"],\n    [\"Product B\", \"$18M\", \"25%\"]\n];\n\nslide.addTable(mergedTableData, {\n    x: 1,\n    y: 1,\n    w: 8,\n    h: 2.5,\n    colW: [3, 2.5, 2.5],\n    border: { pt: 1, color: \"DDDDDD\" }\n});\n```\n\n### Table Options\n\nCommon table options:\n- `x, y, w, h` - Position and size\n- `colW` - Array of column widths (in inches)\n- `rowH` - Array of row heights (in inches)\n- `border` - Border style: `{ pt: 1, color: \"999999\" }`\n- `fill` - Background color (no # prefix)\n- `align` - Text alignment: \"left\", \"center\", \"right\"\n- `valign` - Vertical alignment: \"top\", \"middle\", \"bottom\"\n- `fontSize` - Text size\n- `autoPage` - Auto-create new slides if content overflows",
        "skills/resume_optimizer/.claude/skills/pptx/ooxml.md": "# Office Open XML Technical Reference for PowerPoint\n\n**Important: Read this entire document before starting.** Critical XML schema rules and formatting requirements are covered throughout. Incorrect implementation can create invalid PPTX files that PowerPoint cannot open.\n\n## Technical Guidelines\n\n### Schema Compliance\n- **Element ordering in `<p:txBody>`**: `<a:bodyPr>`, `<a:lstStyle>`, `<a:p>`\n- **Whitespace**: Add `xml:space='preserve'` to `<a:t>` elements with leading/trailing spaces\n- **Unicode**: Escape characters in ASCII content: `\"` becomes `&#8220;`\n- **Images**: Add to `ppt/media/`, reference in slide XML, set dimensions to fit slide bounds\n- **Relationships**: Update `ppt/slides/_rels/slideN.xml.rels` for each slide's resources\n- **Dirty attribute**: Add `dirty=\"0\"` to `<a:rPr>` and `<a:endParaRPr>` elements to indicate clean state\n\n## Presentation Structure\n\n### Basic Slide Structure\n```xml\n<!-- ppt/slides/slide1.xml -->\n<p:sld>\n  <p:cSld>\n    <p:spTree>\n      <p:nvGrpSpPr>...</p:nvGrpSpPr>\n      <p:grpSpPr>...</p:grpSpPr>\n      <!-- Shapes go here -->\n    </p:spTree>\n  </p:cSld>\n</p:sld>\n```\n\n### Text Box / Shape with Text\n```xml\n<p:sp>\n  <p:nvSpPr>\n    <p:cNvPr id=\"2\" name=\"Title\"/>\n    <p:cNvSpPr>\n      <a:spLocks noGrp=\"1\"/>\n    </p:cNvSpPr>\n    <p:nvPr>\n      <p:ph type=\"ctrTitle\"/>\n    </p:nvPr>\n  </p:nvSpPr>\n  <p:spPr>\n    <a:xfrm>\n      <a:off x=\"838200\" y=\"365125\"/>\n      <a:ext cx=\"7772400\" cy=\"1470025\"/>\n    </a:xfrm>\n  </p:spPr>\n  <p:txBody>\n    <a:bodyPr/>\n    <a:lstStyle/>\n    <a:p>\n      <a:r>\n        <a:t>Slide Title</a:t>\n      </a:r>\n    </a:p>\n  </p:txBody>\n</p:sp>\n```\n\n### Text Formatting\n```xml\n<!-- Bold -->\n<a:r>\n  <a:rPr b=\"1\"/>\n  <a:t>Bold Text</a:t>\n</a:r>\n\n<!-- Italic -->\n<a:r>\n  <a:rPr i=\"1\"/>\n  <a:t>Italic Text</a:t>\n</a:r>\n\n<!-- Underline -->\n<a:r>\n  <a:rPr u=\"sng\"/>\n  <a:t>Underlined</a:t>\n</a:r>\n\n<!-- Highlight -->\n<a:r>\n  <a:rPr>\n    <a:highlight>\n      <a:srgbClr val=\"FFFF00\"/>\n    </a:highlight>\n  </a:rPr>\n  <a:t>Highlighted Text</a:t>\n</a:r>\n\n<!-- Font and Size -->\n<a:r>\n  <a:rPr sz=\"2400\" typeface=\"Arial\">\n    <a:solidFill>\n      <a:srgbClr val=\"FF0000\"/>\n    </a:solidFill>\n  </a:rPr>\n  <a:t>Colored Arial 24pt</a:t>\n</a:r>\n\n<!-- Complete formatting example -->\n<a:r>\n  <a:rPr lang=\"en-US\" sz=\"1400\" b=\"1\" dirty=\"0\">\n    <a:solidFill>\n      <a:srgbClr val=\"FAFAFA\"/>\n    </a:solidFill>\n  </a:rPr>\n  <a:t>Formatted text</a:t>\n</a:r>\n```\n\n### Lists\n```xml\n<!-- Bullet list -->\n<a:p>\n  <a:pPr lvl=\"0\">\n    <a:buChar char=\"\"/>\n  </a:pPr>\n  <a:r>\n    <a:t>First bullet point</a:t>\n  </a:r>\n</a:p>\n\n<!-- Numbered list -->\n<a:p>\n  <a:pPr lvl=\"0\">\n    <a:buAutoNum type=\"arabicPeriod\"/>\n  </a:pPr>\n  <a:r>\n    <a:t>First numbered item</a:t>\n  </a:r>\n</a:p>\n\n<!-- Second level indent -->\n<a:p>\n  <a:pPr lvl=\"1\">\n    <a:buChar char=\"\"/>\n  </a:pPr>\n  <a:r>\n    <a:t>Indented bullet</a:t>\n  </a:r>\n</a:p>\n```\n\n### Shapes\n```xml\n<!-- Rectangle -->\n<p:sp>\n  <p:nvSpPr>\n    <p:cNvPr id=\"3\" name=\"Rectangle\"/>\n    <p:cNvSpPr/>\n    <p:nvPr/>\n  </p:nvSpPr>\n  <p:spPr>\n    <a:xfrm>\n      <a:off x=\"1000000\" y=\"1000000\"/>\n      <a:ext cx=\"3000000\" cy=\"2000000\"/>\n    </a:xfrm>\n    <a:prstGeom prst=\"rect\">\n      <a:avLst/>\n    </a:prstGeom>\n    <a:solidFill>\n      <a:srgbClr val=\"FF0000\"/>\n    </a:solidFill>\n    <a:ln w=\"25400\">\n      <a:solidFill>\n        <a:srgbClr val=\"000000\"/>\n      </a:solidFill>\n    </a:ln>\n  </p:spPr>\n</p:sp>\n\n<!-- Rounded Rectangle -->\n<p:sp>\n  <p:spPr>\n    <a:prstGeom prst=\"roundRect\">\n      <a:avLst/>\n    </a:prstGeom>\n  </p:spPr>\n</p:sp>\n\n<!-- Circle/Ellipse -->\n<p:sp>\n  <p:spPr>\n    <a:prstGeom prst=\"ellipse\">\n      <a:avLst/>\n    </a:prstGeom>\n  </p:spPr>\n</p:sp>\n```\n\n### Images\n```xml\n<p:pic>\n  <p:nvPicPr>\n    <p:cNvPr id=\"4\" name=\"Picture\">\n      <a:hlinkClick r:id=\"\" action=\"ppaction://media\"/>\n    </p:cNvPr>\n    <p:cNvPicPr>\n      <a:picLocks noChangeAspect=\"1\"/>\n    </p:cNvPicPr>\n    <p:nvPr/>\n  </p:nvPicPr>\n  <p:blipFill>\n    <a:blip r:embed=\"rId2\"/>\n    <a:stretch>\n      <a:fillRect/>\n    </a:stretch>\n  </p:blipFill>\n  <p:spPr>\n    <a:xfrm>\n      <a:off x=\"1000000\" y=\"1000000\"/>\n      <a:ext cx=\"3000000\" cy=\"2000000\"/>\n    </a:xfrm>\n    <a:prstGeom prst=\"rect\">\n      <a:avLst/>\n    </a:prstGeom>\n  </p:spPr>\n</p:pic>\n```\n\n### Tables\n```xml\n<p:graphicFrame>\n  <p:nvGraphicFramePr>\n    <p:cNvPr id=\"5\" name=\"Table\"/>\n    <p:cNvGraphicFramePr>\n      <a:graphicFrameLocks noGrp=\"1\"/>\n    </p:cNvGraphicFramePr>\n    <p:nvPr/>\n  </p:nvGraphicFramePr>\n  <p:xfrm>\n    <a:off x=\"1000000\" y=\"1000000\"/>\n    <a:ext cx=\"6000000\" cy=\"2000000\"/>\n  </p:xfrm>\n  <a:graphic>\n    <a:graphicData uri=\"http://schemas.openxmlformats.org/drawingml/2006/table\">\n      <a:tbl>\n        <a:tblGrid>\n          <a:gridCol w=\"3000000\"/>\n          <a:gridCol w=\"3000000\"/>\n        </a:tblGrid>\n        <a:tr h=\"500000\">\n          <a:tc>\n            <a:txBody>\n              <a:bodyPr/>\n              <a:lstStyle/>\n              <a:p>\n                <a:r>\n                  <a:t>Cell 1</a:t>\n                </a:r>\n              </a:p>\n            </a:txBody>\n          </a:tc>\n          <a:tc>\n            <a:txBody>\n              <a:bodyPr/>\n              <a:lstStyle/>\n              <a:p>\n                <a:r>\n                  <a:t>Cell 2</a:t>\n                </a:r>\n              </a:p>\n            </a:txBody>\n          </a:tc>\n        </a:tr>\n      </a:tbl>\n    </a:graphicData>\n  </a:graphic>\n</p:graphicFrame>\n```\n\n### Slide Layouts\n\n```xml\n<!-- Title Slide Layout -->\n<p:sp>\n  <p:nvSpPr>\n    <p:nvPr>\n      <p:ph type=\"ctrTitle\"/>\n    </p:nvPr>\n  </p:nvSpPr>\n  <!-- Title content -->\n</p:sp>\n\n<p:sp>\n  <p:nvSpPr>\n    <p:nvPr>\n      <p:ph type=\"subTitle\" idx=\"1\"/>\n    </p:nvPr>\n  </p:nvSpPr>\n  <!-- Subtitle content -->\n</p:sp>\n\n<!-- Content Slide Layout -->\n<p:sp>\n  <p:nvSpPr>\n    <p:nvPr>\n      <p:ph type=\"title\"/>\n    </p:nvPr>\n  </p:nvSpPr>\n  <!-- Slide title -->\n</p:sp>\n\n<p:sp>\n  <p:nvSpPr>\n    <p:nvPr>\n      <p:ph type=\"body\" idx=\"1\"/>\n    </p:nvPr>\n  </p:nvSpPr>\n  <!-- Content body -->\n</p:sp>\n```\n\n## File Updates\n\nWhen adding content, update these files:\n\n**`ppt/_rels/presentation.xml.rels`:**\n```xml\n<Relationship Id=\"rId1\" Type=\"http://schemas.openxmlformats.org/officeDocument/2006/relationships/slide\" Target=\"slides/slide1.xml\"/>\n<Relationship Id=\"rId2\" Type=\"http://schemas.openxmlformats.org/officeDocument/2006/relationships/slideMaster\" Target=\"slideMasters/slideMaster1.xml\"/>\n```\n\n**`ppt/slides/_rels/slide1.xml.rels`:**\n```xml\n<Relationship Id=\"rId1\" Type=\"http://schemas.openxmlformats.org/officeDocument/2006/relationships/slideLayout\" Target=\"../slideLayouts/slideLayout1.xml\"/>\n<Relationship Id=\"rId2\" Type=\"http://schemas.openxmlformats.org/officeDocument/2006/relationships/image\" Target=\"../media/image1.png\"/>\n```\n\n**`[Content_Types].xml`:**\n```xml\n<Default Extension=\"png\" ContentType=\"image/png\"/>\n<Default Extension=\"jpg\" ContentType=\"image/jpeg\"/>\n<Override PartName=\"/ppt/slides/slide1.xml\" ContentType=\"application/vnd.openxmlformats-officedocument.presentationml.slide+xml\"/>\n```\n\n**`ppt/presentation.xml`:**\n```xml\n<p:sldIdLst>\n  <p:sldId id=\"256\" r:id=\"rId1\"/>\n  <p:sldId id=\"257\" r:id=\"rId2\"/>\n</p:sldIdLst>\n```\n\n**`docProps/app.xml`:** Update slide count and statistics\n```xml\n<Slides>2</Slides>\n<Paragraphs>10</Paragraphs>\n<Words>50</Words>\n```\n\n## Slide Operations\n\n### Adding a New Slide\nWhen adding a slide to the end of the presentation:\n\n1. **Create the slide file** (`ppt/slides/slideN.xml`)\n2. **Update `[Content_Types].xml`**: Add Override for the new slide\n3. **Update `ppt/_rels/presentation.xml.rels`**: Add relationship for the new slide\n4. **Update `ppt/presentation.xml`**: Add slide ID to `<p:sldIdLst>`\n5. **Create slide relationships** (`ppt/slides/_rels/slideN.xml.rels`) if needed\n6. **Update `docProps/app.xml`**: Increment slide count and update statistics (if present)\n\n### Duplicating a Slide\n1. Copy the source slide XML file with a new name\n2. Update all IDs in the new slide to be unique\n3. Follow the \"Adding a New Slide\" steps above\n4. **CRITICAL**: Remove or update any notes slide references in `_rels` files\n5. Remove references to unused media files\n\n### Reordering Slides\n1. **Update `ppt/presentation.xml`**: Reorder `<p:sldId>` elements in `<p:sldIdLst>`\n2. The order of `<p:sldId>` elements determines slide order\n3. Keep slide IDs and relationship IDs unchanged\n\nExample:\n```xml\n<!-- Original order -->\n<p:sldIdLst>\n  <p:sldId id=\"256\" r:id=\"rId2\"/>\n  <p:sldId id=\"257\" r:id=\"rId3\"/>\n  <p:sldId id=\"258\" r:id=\"rId4\"/>\n</p:sldIdLst>\n\n<!-- After moving slide 3 to position 2 -->\n<p:sldIdLst>\n  <p:sldId id=\"256\" r:id=\"rId2\"/>\n  <p:sldId id=\"258\" r:id=\"rId4\"/>\n  <p:sldId id=\"257\" r:id=\"rId3\"/>\n</p:sldIdLst>\n```\n\n### Deleting a Slide\n1. **Remove from `ppt/presentation.xml`**: Delete the `<p:sldId>` entry\n2. **Remove from `ppt/_rels/presentation.xml.rels`**: Delete the relationship\n3. **Remove from `[Content_Types].xml`**: Delete the Override entry\n4. **Delete files**: Remove `ppt/slides/slideN.xml` and `ppt/slides/_rels/slideN.xml.rels`\n5. **Update `docProps/app.xml`**: Decrement slide count and update statistics\n6. **Clean up unused media**: Remove orphaned images from `ppt/media/`\n\nNote: Don't renumber remaining slides - keep their original IDs and filenames.\n\n\n## Common Errors to Avoid\n\n- **Encodings**: Escape unicode characters in ASCII content: `\"` becomes `&#8220;`\n- **Images**: Add to `ppt/media/` and update relationship files\n- **Lists**: Omit bullets from list headers\n- **IDs**: Use valid hexadecimal values for UUIDs\n- **Themes**: Check all themes in `theme` directory for colors\n\n## Validation Checklist for Template-Based Presentations\n\n### Before Packing, Always:\n- **Clean unused resources**: Remove unreferenced media, fonts, and notes directories\n- **Fix Content_Types.xml**: Declare ALL slides, layouts, and themes present in the package\n- **Fix relationship IDs**: \n   - Remove font embed references if not using embedded fonts\n- **Remove broken references**: Check all `_rels` files for references to deleted resources\n\n### Common Template Duplication Pitfalls:\n- Multiple slides referencing the same notes slide after duplication\n- Image/media references from template slides that no longer exist\n- Font embedding references when fonts aren't included\n- Missing slideLayout declarations for layouts 12-25\n- docProps directory may not unpack - this is optional",
        "skills/resume_optimizer/.claude/skills/skill-creator-pro/SKILL.md": "---\nname: skill-creator-pro\ndescription: |\n  Creates production-grade, reusable skills that extend Claude's capabilities.\n  This skill should be used when users want to create a new skill, improve an\n  existing skill, or build domain-specific intelligence. Gathers context from\n  codebase, conversation, and authentic sources before creating adaptable skills.\n---\n\n# Skill Creator Pro\n\nCreate production-grade skills that extend Claude's capabilities.\n\n## How This Skill Works\n\n```\nUser: \"Create a skill for X\"\n       \nClaude Code uses this meta-skill as guidance\n       \nFollow Domain Discovery  Ask user clarifying questions  Create skill\n       \nGenerated skill with embedded domain expertise\n```\n\nThis skill provides **guidance and structure** for creating skills. Claude Code:\n1. Uses this skill's framework to discover domain knowledge\n2. Asks user for clarifications about THEIR specific requirements\n3. Decides how to structure the generated skill based on domain needs\n\n## What This Skill Does\n\n- Guides creation of new skills from scratch\n- Helps improve existing skills to production quality\n- Provides patterns for 5 skill types (Builder, Guide, Automation, Analyzer, Validator)\n- Ensures skills encode procedural knowledge + domain expertise\n\n## What This Skill Does NOT Do\n\n- Test skills in production environments\n- Deploy or distribute skills\n- Handle skill versioning/updates after creation\n- Create requirement-specific skills (always create reusable intelligence)\n\n---\n\n## Domain Discovery Framework\n\n**Key Principle**: Users want domain expertise IN the skill. They may not BE domain experts.\n\n### Phase 1: Automatic Discovery (No User Input)\n\nProactively research the domain before asking anything:\n\n| Discover | How | Example: \"Kafka integration\" |\n|----------|-----|------------------------------|\n| Core concepts | Official docs, Context7 | Producers, consumers, topics, partitions |\n| Standards/compliance | Search \"[domain] standards\" | Kafka security, exactly-once semantics |\n| Best practices | Search \"[domain] best practices 2025\" | Partitioning strategies, consumer groups |\n| Anti-patterns | Search \"[domain] common mistakes\" | Too many partitions, no monitoring |\n| Security | Search \"[domain] security\" | SASL, SSL, ACLs, encryption |\n| Ecosystem | Search \"[domain] ecosystem tools\" | Confluent, Schema Registry, Connect |\n\n**Sources priority**: Official docs  Library docs (Context7)  GitHub  Community  WebSearch\n\n### Phase 2: Knowledge Sufficiency Check\n\nBefore asking user anything, verify internally:\n\n```\n- [ ] Core concepts understood?\n- [ ] Best practices identified?\n- [ ] Anti-patterns known?\n- [ ] Security considerations covered?\n- [ ] Official sources found?\n\nIf ANY gap  Research more (don't ask user for domain knowledge)\nOnly if CANNOT discover (proprietary/internal)  Ask user\n```\n\n### Phase 3: User Requirements (NOT Domain Knowledge)\n\nOnly ask about user's SPECIFIC context:\n\n| Ask | Don't Ask |\n|-----|-----------|\n| \"What's YOUR use case?\" | \"What is Kafka?\" |\n| \"What's YOUR tech stack?\" | \"What options exist?\" |\n| \"Any existing resources?\" | \"How does it work?\" |\n| \"Specific constraints?\" | \"What are best practices?\" |\n\nThe skill contains domain expertise. User provides requirements.\n\n---\n\n## Required Clarifications\n\nAsk about SKILL METADATA and USER REQUIREMENTS (not domain knowledge):\n\n### Skill Metadata\n\n**1. Skill Type** - \"What type of skill?\"\n\n| Type | Purpose | Example |\n|------|---------|---------|\n| **Builder** | Create artifacts | Widgets, code, documents |\n| **Guide** | Provide instructions | How-to, tutorials |\n| **Automation** | Execute workflows | File processing, deployments |\n| **Analyzer** | Extract insights | Code review, data analysis |\n| **Validator** | Enforce quality | Compliance checks, scoring |\n\n**2. Domain** - \"What domain or technology?\"\n\n### User Requirements (After Domain Discovery)\n\n**3. Use Case** - \"What's YOUR specific use case?\"\n- Not \"what can it do\" but \"what do YOU need\"\n\n**4. Tech Stack** - \"What's YOUR environment?\"\n- Languages, frameworks, existing infrastructure\n\n**5. Existing Resources** - \"Any scripts, templates, configs to include?\"\n\n**6. Constraints** - \"Any specific requirements or limitations?\"\n- Performance, security, compliance specific to user's context\n\n### Note\n- Questions 1-2: Ask immediately\n- Domain Discovery: Research automatically after knowing domain\n- Questions 3-6: Ask after discovery, informed by domain knowledge\n- **Question pacing**: Avoid asking too many questions in a single message. Start with most important, follow up as needed.\n\n---\n\n## Core Principles\n\n### Reusable Intelligence, Not Requirement-Specific\n\nSkills must handle VARIATIONS, not single requirements:\n\n```\n Bad: \"Create bar chart with sales data using Recharts\"\n Good: \"Create visualizations - adaptable to data shape, chart type, library\"\n\n Bad: \"Deploy to AWS EKS with Helm\"\n Good: \"Deploy applications - adaptable to platform, orchestration, environment\"\n```\n\nIdentify what VARIES vs what's CONSTANT in the domain. See `references/reusability-patterns.md`.\n\n### Concise is Key\n\nContext window is a public good (~1,500+ tokens per skill activation). Challenge each piece:\n- \"Does Claude really need this explanation?\"\n- \"Does this paragraph justify its token cost?\"\n\nPrefer concise examples over verbose explanations.\n\n### Appropriate Freedom\n\nMatch specificity to task fragility:\n\n| Freedom Level | When to Use | Example |\n|---------------|-------------|---------|\n| **High** | Multiple approaches valid | \"Choose your preferred style\" |\n| **Medium** | Preferred pattern exists | Pseudocode with parameters |\n| **Low** | Operations are fragile | Exact scripts, few parameters |\n\n### Progressive Disclosure\n\nThree-level loading system:\n\n1. **Metadata** (~100 tokens) - Always in context (description 1024 chars)\n2. **SKILL.md body** (<500 lines) - When skill triggers\n3. **References** (unlimited) - Loaded as needed by Claude\n\n---\n\n## Anatomy of a Skill\n\nGenerated skills are **zero-shot domain experts** with embedded knowledge.\n\n```\nskill-name/\n SKILL.md (required)\n    YAML frontmatter (name, description, allowed-tools?, model?)\n    Procedural knowledge (workflows, steps, decision trees)\n Bundled Resources\n     references/   - Domain expertise (structure based on domain needs)\n     scripts/      - Executable code (tested, reliable)\n     assets/       - Templates, boilerplate, images\n```\n\n### SKILL.md Requirements\n\n| Component | Requirement |\n|-----------|-------------|\n| Line count | <500 lines (extract to references/) |\n| Frontmatter | See `references/skill-patterns.md` for complete spec |\n| `name` | Lowercase, numbers, hyphens; 64 chars; match directory |\n| `description` | [What] + [When]; 1024 chars; third-person style |\n| Description style | \"This skill should be used when...\" (not \"Use when...\") |\n| Form | Imperative (\"Do X\" not \"You should X\") |\n| Scope | What it does AND does not do |\n\n### What Goes in references/\n\nEmbed domain knowledge gathered during discovery:\n\n| Gathered Knowledge | Purpose in Skill |\n|--------------------|------------------|\n| Library/API documentation | Enable correct implementation |\n| Best practices | Guide quality decisions |\n| Code examples | Provide reference patterns |\n| Anti-patterns | Prevent common mistakes |\n| Domain-specific details | Support edge cases |\n\n**Structure references/ based on what the domain needs.**\n\n**Large files**: If references >10k words, include grep search patterns in SKILL.md for efficient discovery.\n\n### When to Generate scripts/\n\nGenerate scripts when domain requires **deterministic, executable procedures**:\n\n| Domain Need | Example Scripts |\n|-------------|-----------------|\n| Setup/installation | Install dependencies, initialize project |\n| Processing | Transform data, process files |\n| Validation | Check compliance, verify output |\n| Deployment | Deploy services, configure infrastructure |\n\n**Decision**: If procedure is complex, error-prone, or needs to be exactly repeatable  create script. Otherwise  document in SKILL.md or references/.\n\n### When to Generate assets/\n\nGenerate assets when domain requires **exact templates or boilerplate**:\n\n| Domain Need | Example Assets |\n|-------------|----------------|\n| Starting templates | HTML boilerplate, component scaffolds |\n| Configuration files | Config templates, schema definitions |\n| Code boilerplate | Base classes, starter code |\n\n### What NOT to Include\n\n- README.md (SKILL.md IS the readme)\n- CHANGELOG.md\n- LICENSE (inherited from repo)\n- Duplicate information\n\n### What Generated Skill Does at Runtime\n\n```\nUser invokes skill  Gather context from:\n  1. Codebase (if existing project)\n  2. Conversation (user's requirements)\n  3. Own references/ (embedded domain expertise)\n  4. User-specific guidelines\n Ensure all information gathered  Implement ZERO-SHOT\n```\n\n### Include in Generated Skills\n\nEvery generated skill should include:\n\n```markdown\n## Before Implementation\n\nGather context to ensure successful implementation:\n\n| Source | Gather |\n|--------|--------|\n| **Codebase** | Existing structure, patterns, conventions to integrate with |\n| **Conversation** | User's specific requirements, constraints, preferences |\n| **Skill References** | Domain patterns from `references/` (library docs, best practices, examples) |\n| **User Guidelines** | Project-specific conventions, team standards |\n\nEnsure all required context is gathered before implementing.\nOnly ask user for THEIR specific requirements (domain expertise is in this skill).\n```\n\n---\n\n## Type-Aware Creation\n\nAfter determining skill type, follow type-specific patterns:\n\n| Type | Key Sections | Reference |\n|------|--------------|-----------|\n| **Builder** | Clarifications  Output Spec  Standards  Checklist | `skill-patterns.md#builder` |\n| **Guide** | Workflow  Examples  Official Docs | `skill-patterns.md#guide` |\n| **Automation** | Scripts  Dependencies  Error Handling | `skill-patterns.md#automation` |\n| **Analyzer** | Scope  Criteria  Output Format | `skill-patterns.md#analyzer` |\n| **Validator** | Criteria  Scoring  Thresholds  Remediation | `skill-patterns.md#validator` |\n\n---\n\n## Skill Creation Process\n\n```\nMetadata  Discovery  Requirements  Analyze  Embed  Structure  Implement  Validate\n```\n\nSee `references/creation-workflow.md` for detailed steps.\n\n### Quick Steps\n\n1. **Metadata**: Ask skill type + domain (Questions 1-2)\n2. **Discovery**: Research domain automatically (Phase 1-2 above)\n3. **Requirements**: Ask user's specific needs (Questions 3-6)\n4. **Analyze**: Identify procedural (HOW) + domain (WHAT) knowledge\n5. **Embed**: Put gathered domain expertise into `references/`\n6. **Structure**: Initialize skill directory\n7. **Implement**: Write SKILL.md + resources following type patterns\n8. **Validate**: Run `scripts/package_skill.py` and test\n\n### SKILL.md Template\n\n```yaml\n---\nname: skill-name                    # lowercase, hyphens, 64 chars\ndescription: |                      # 1024 chars\n  [What] Capability statement.\n  [When] Use when users ask to <triggers>.\nallowed-tools: Read, Grep, Glob     # optional: restrict tools\n---\n```\n\nSee `references/skill-patterns.md` for complete frontmatter spec and body patterns.\n\n---\n\n## Output Checklist\n\nBefore delivering a skill, verify:\n\n### Domain Discovery Complete\n- [ ] Core concepts discovered and understood\n- [ ] Best practices identified from authentic sources\n- [ ] Anti-patterns documented\n- [ ] Security considerations covered\n- [ ] Official documentation linked\n- [ ] User was NOT asked for domain knowledge\n\n### Frontmatter\n- [ ] `name`: lowercase, hyphens, 64 chars, matches directory\n- [ ] `description`: [What]+[When], 1024 chars, clear triggers\n- [ ] `allowed-tools`: Set if restricted access needed\n\n### Structure\n- [ ] SKILL.md <500 lines\n- [ ] Progressive disclosure (details in references/)\n\n### Knowledge Coverage\n- [ ] **Procedural** (HOW): Workflows, decision trees, error handling\n- [ ] **Domain** (WHAT): Concepts, best practices, anti-patterns\n\n### Zero-Shot Implementation (in generated skill)\n- [ ] Includes \"Before Implementation\" section\n- [ ] Gathers runtime context (codebase, conversation, user guidelines)\n- [ ] Domain expertise embedded in `references/` (structured per domain needs)\n- [ ] Only asks user for THEIR requirements (not domain knowledge)\n\n### Reusability\n- [ ] Handles variations (not requirement-specific)\n- [ ] Clarifications capture variable elements (user's context)\n- [ ] Constants encoded (domain patterns, best practices)\n\n### Type-Specific (see `references/skill-patterns.md`)\n- [ ] Builder: Clarifications, output spec, standards, checklist\n- [ ] Guide: Workflow, examples, official docs\n- [ ] Automation: Scripts, dependencies, error handling\n- [ ] Analyzer: Scope, criteria, output format\n- [ ] Validator: Criteria, scoring, thresholds, remediation\n\n---\n\n## Reference Files\n| File | When to Read |\n|------|--------------|\n| `references/creation-workflow.md` | Detailed step-by-step creation process |\n| `references/skill-patterns.md` | Frontmatter spec, type-specific patterns, assets guidance |\n| `references/reusability-patterns.md` | Procedural+domain knowledge, varies vs constant |\n| `references/quality-patterns.md` | Clarifications, enforcement, checklists |\n| `references/technical-patterns.md` | Error handling, security, dependencies |\n| `references/workflows.md` | Sequential and conditional workflow patterns |\n| `references/output-patterns.md` | Template and example patterns |\n",
        "skills/resume_optimizer/.claude/skills/skill-creator-pro/references/creation-workflow.md": "# Skill Creation Workflow\n\nDetailed step-by-step process for creating production-grade skills.\n\n---\n\n## Overview\n\n```\nMetadata  Discovery  Requirements  Analyze  Embed  Structure  Implement  Validate\n```\n\n**Key Principle**: Users want domain expertise IN the skill. They may not BE domain experts.\n\n---\n\n## Step 1: Get Skill Metadata\n\nAsk ONLY these two questions first:\n\n| Question | Purpose |\n|----------|---------|\n| **Skill type?** | Route to appropriate patterns |\n| **Domain/technology?** | Focus domain discovery |\n\n**Do NOT ask**: Usage examples, tech stack, constraints (these come AFTER discovery)\n\n**Exit criteria**: Clear skill type + domain identified\n\n---\n\n## Step 2: Domain Discovery (Automatic)\n\nResearch the domain BEFORE asking user anything else.\n\n### What to Discover\n\n| Knowledge Area | Search Strategy | Example: \"Kafka\" |\n|----------------|-----------------|------------------|\n| Core concepts | Official docs, Context7 | Topics, partitions, consumers, producers |\n| Standards/compliance | \"[domain] standards compliance\" | Exactly-once semantics, ordering guarantees |\n| Best practices | \"[domain] best practices 2025\" | Partition strategies, consumer groups |\n| Anti-patterns | \"[domain] common mistakes pitfalls\" | Over-partitioning, no dead letter queue |\n| Security | \"[domain] security requirements\" | SASL, SSL, ACLs, encryption at rest |\n| Ecosystem | \"[domain] ecosystem tools\" | Schema Registry, Kafka Connect, ksqlDB |\n| Official sources | \"[domain] official documentation\" | kafka.apache.org, Confluent docs |\n\n### Source Priority\n\n1. **Official documentation** - Always authoritative\n2. **Library docs (Context7)** - Structured, current\n3. **GitHub** - Real implementations, discussions\n4. **Community** - Stack Overflow, forums (verify accuracy)\n5. **WebSearch** - Last resort, cross-reference\n\n### Knowledge Sufficiency Check\n\nBefore proceeding, verify internally:\n\n```\n- [ ] Core concepts: Can I explain the fundamentals?\n- [ ] Best practices: Do I know the recommended approaches?\n- [ ] Anti-patterns: Do I know what to avoid?\n- [ ] Security: Do I know the security considerations?\n- [ ] Ecosystem: Do I know the related tools/options?\n- [ ] Official sources: Do I have authoritative references?\n\nIf ANY is incomplete  Research more\nOnly if CANNOT discover (proprietary/internal)  Ask user\n```\n\n**Exit criteria**: Sufficient domain knowledge to create expert-level skill\n\n---\n\n## Step 3: Get User Requirements\n\nNOW ask about user's SPECIFIC context (not domain knowledge):\n\n| Ask | Purpose | NOT This |\n|-----|---------|----------|\n| \"What's YOUR use case?\" | Understand their need | \"What can [domain] do?\" |\n| \"What's YOUR tech stack?\" | Know their environment | \"What options exist?\" |\n| \"Existing resources?\" | Include their scripts/templates | \"What tools are available?\" |\n| \"Specific constraints?\" | Their limitations | \"What are best practices?\" |\n\n### Good vs Bad Questions\n\n```\n \"What's your use case - event sourcing, messaging, or streaming?\"\n   (Informed by discovery, asking about THEIR need)\n\n \"What is Kafka used for?\"\n   (Asking user for domain knowledge we should discover)\n\n \"Are you using Java, Python, or another language?\"\n   (Asking about THEIR environment)\n\n \"What languages does Kafka support?\"\n   (Asking user for domain knowledge we should discover)\n```\n\n**Exit criteria**: Clear user requirements + existing resources identified\n\n---\n\n## Step 4: Analyze Domain\n\nCombine discovered knowledge + user requirements:\n\n### Procedural Knowledge (HOW)\n- Step-by-step workflows\n- Decision trees and branching logic\n- Error handling sequences\n- Validation procedures\n\n### Domain Expertise (WHAT)\n- Core concepts and terminology\n- Best practices and patterns\n- Anti-patterns to avoid\n- Standards and compliance requirements\n\n### Variability Analysis\n| What VARIES | What's CONSTANT |\n|-------------|-----------------|\n| (User-specific inputs) | (Domain patterns) |\n| (Tool/library choices) | (Best practices) |\n| (Configuration options) | (Error handling) |\n\nSee `reusability-patterns.md` for detailed guidance.\n\n---\n\n## Step 5: Embed Domain Knowledge\n\n**Critical**: Take the knowledge gathered in Step 2 and EMBED it into the skill.\n\n### What Goes in references/\n\n| Gathered Knowledge | Purpose in Generated Skill |\n|--------------------|---------------------------|\n| Library/API documentation | Enable correct implementation |\n| Best practices | Guide quality decisions |\n| Code examples | Provide reference patterns |\n| Anti-patterns | Prevent common mistakes |\n| Domain-specific details | Support edge cases |\n\n**Structure `references/` based on what the domain needs** - file organization depends on the gathered knowledge and skill's purpose.\n\n### When to Generate scripts/\n\nGenerate scripts when the domain requires **deterministic, executable procedures**:\n\n| Domain Need | Example Scripts |\n|-------------|-----------------|\n| Setup/installation | Install dependencies, initialize project |\n| Processing | Transform data, process files |\n| Validation | Check compliance, verify output |\n| Deployment | Deploy services, configure infrastructure |\n| Automation | Batch operations, scheduled tasks |\n\n**Decision**: If a procedure is complex, error-prone, or needs to be exactly repeatable  create a script. Otherwise  document in SKILL.md or references/.\n\n### When to Generate assets/\n\nGenerate assets when the domain requires **exact templates or boilerplate**:\n\n| Domain Need | Example Assets |\n|-------------|----------------|\n| Starting templates | HTML boilerplate, component scaffolds |\n| Configuration files | Config templates, schema definitions |\n| Code boilerplate | Base classes, starter code |\n\n### Why This Matters\n\nGenerated skills are **zero-shot domain experts**. The expertise gathered in Step 2 must be embedded so the skill can implement without runtime discovery.\n\n---\n\n## Step 6: Initialize Structure\n\n```bash\nscripts/init_skill.py <skill-name> --path <output-directory>\n```\n\nOr manually create:\n```\nskill-name/\n SKILL.md\n references/\n scripts/\n assets/\n```\n\n---\n\n## Step 7: Implement by Type\n\n### Builder Skills\n1. Write Required Clarifications (critical questions before building)\n2. Define output specification (what artifact looks like)\n3. Add domain standards enforcement (Must Follow/Must Avoid)\n4. Create templates in assets/\n5. Add output checklist\n\n**Key sections**:\n- Required Clarifications\n- Output Specification\n- Domain Standards\n- Output Checklist\n\n### Guide Skills\n1. Write step-by-step workflow\n2. Add good/bad examples\n3. Link to official documentation\n4. Create decision trees for branching paths\n\n**Key sections**:\n- Workflow Steps\n- Examples (Good/Bad)\n- Official Documentation table\n- When to Use / When NOT to Use\n\n### Automation Skills\n1. Create tested scripts in scripts/\n2. Document dependencies\n3. Add error handling guidance\n4. Define input/output contracts\n\n**Key sections**:\n- Available Scripts table\n- Dependencies\n- Error Handling\n- Input/Output Specification\n\n### Analyzer Skills\n1. Define analysis scope and criteria\n2. Create extraction/review patterns\n3. Add output format specification\n4. Include validation rules\n\n**Key sections**:\n- Analysis Scope\n- Evaluation Criteria\n- Output Format\n- Validation Rules\n\n### Validator Skills\n1. Define quality criteria\n2. Create scoring/evaluation rubrics\n3. Add pass/fail thresholds\n4. Include remediation guidance\n\n**Key sections**:\n- Quality Criteria\n- Scoring Rubric\n- Thresholds\n- Remediation Patterns\n\n---\n\n## Step 8: Write SKILL.md\n\nFollow this structure (includes Before Implementation for zero-shot execution):\n\n```markdown\n---\nname: skill-name\ndescription: |\n  [What] Capability statement.\n  [When] Use when users ask to <triggers>.\nallowed-tools: Tool1, Tool2    # if restricted\n---\n\n# Skill Name\n\nBrief description.\n\n## What This Skill Does\n- Capability 1\n- Capability 2\n\n## What This Skill Does NOT Do\n- Exclusion 1\n- Exclusion 2\n\n---\n\n## Before Implementation\n\nGather context to ensure successful implementation:\n\n| Source | Gather |\n|--------|--------|\n| **Codebase** | Existing structure, patterns, conventions to integrate with |\n| **Conversation** | User's specific requirements, constraints, preferences |\n| **Skill References** | Domain patterns from `references/` (library docs, best practices, examples) |\n| **User Guidelines** | Project-specific conventions, team standards |\n\nEnsure all required context is gathered before implementing.\nOnly ask user for THEIR specific requirements (domain expertise is in this skill).\n\n---\n\n## [Type-specific sections]\n(See Step 6 for sections by type)\n\n## Reference Files\n| File | When to Read |\n|------|--------------|\n| `references/file.md` | When X |\n\n## Output Checklist\n- [ ] Requirement 1\n- [ ] Requirement 2\n```\n\n**CRITICAL**: Generated skills are zero-shot domain experts. The \"Before Implementation\" section ensures they gather runtime context (codebase, user requirements) and apply their embedded domain expertise (from `references/`) to implement successfully in a single interaction.\n\n---\n\n## Step 9: Validate\n\nRun validation before delivery:\n\n```bash\nscripts/package_skill.py <path/to/skill-folder>\n```\n\n### Manual Validation Checklist\n\n**Frontmatter**:\n- [ ] name: lowercase, hyphens, 64 chars, matches directory\n- [ ] description: [What]+[When], 1024 chars, clear triggers\n\n**Structure**:\n- [ ] SKILL.md <500 lines\n- [ ] No README.md, CHANGELOG.md\n- [ ] Progressive disclosure to references/\n\n**Reusability**:\n- [ ] Handles variations (not requirement-specific)\n- [ ] Clarifications capture variable elements\n- [ ] Encodes constant patterns\n\n**Zero-Shot Implementation**:\n- [ ] Includes \"Before Implementation\" section\n- [ ] Gathers runtime context (codebase, conversation, user guidelines)\n- [ ] Domain expertise embedded in `references/` (library docs, best practices, examples)\n- [ ] Only asks user for THEIR requirements (not domain knowledge)\n\n**Knowledge**:\n- [ ] Procedural knowledge (HOW) documented\n- [ ] Domain expertise (WHAT) captured\n- [ ] Official sources referenced\n\n---\n\n## Step 10: Iterate\n\n1. Use skill on real tasks\n2. Notice gaps or issues\n3. Update and revalidate\n4. Repeat until production-ready\n",
        "skills/resume_optimizer/.claude/skills/skill-creator-pro/references/output-patterns.md": "# Output Patterns\n\nTemplate patterns and example-driven guidance for consistent output.\n\n---\n\n## Template Pattern\n\nProvide templates for output format. Match strictness to requirements.\n\n### Strict Template (Exact Format Required)\n\n```markdown\n## Report Structure\n\nALWAYS use this exact template:\n\n```markdown\n# [Analysis Title]\n\n## Executive Summary\n[One-paragraph overview of key findings - 3-5 sentences]\n\n## Key Findings\n1. **Finding 1**: [Description with supporting data]\n2. **Finding 2**: [Description with supporting data]\n3. **Finding 3**: [Description with supporting data]\n\n## Recommendations\n1. [Specific actionable recommendation]\n2. [Specific actionable recommendation]\n3. [Specific actionable recommendation]\n\n## Appendix\n[Supporting data, charts, or detailed analysis]\n```\n```\n\n### Flexible Template (Guidance with Room for Adaptation)\n\n```markdown\n## Report Structure\n\nDefault format (adapt sections as needed):\n\n```markdown\n# [Analysis Title]\n\n## Executive Summary\n[Overview - adjust length based on complexity]\n\n## Key Findings\n[Organize by theme, priority, or chronology as appropriate]\n\n## Recommendations\n[Tailor to specific context and audience]\n```\n\nAdjust sections based on:\n- Audience (technical vs executive)\n- Complexity (simple vs comprehensive)\n- Purpose (decision support vs documentation)\n```\n\n---\n\n## Examples Pattern\n\nShow input/output pairs for quality guidance.\n\n### Code Generation Examples\n\n```markdown\n## Function Naming\n\n**Example 1:**\nInput: Function that validates email format\nOutput:\n```typescript\nfunction validateEmail(email: string): boolean {\n  const pattern = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n  return pattern.test(email);\n}\n```\n\n**Example 2:**\nInput: Function that fetches user by ID\nOutput:\n```typescript\nasync function getUserById(id: string): Promise<User | null> {\n  const response = await db.users.findUnique({ where: { id } });\n  return response;\n}\n```\n\nFollow this style: verb + noun, camelCase, explicit return types.\n```\n\n### Commit Message Examples\n\n```markdown\n## Commit Message Format\n\n**Example 1:**\nInput: Added user authentication with JWT tokens\nOutput:\n```\nfeat(auth): implement JWT-based authentication\n\nAdd login endpoint and token validation middleware.\nIncludes refresh token rotation for security.\n```\n\n**Example 2:**\nInput: Fixed bug where dates displayed incorrectly\nOutput:\n```\nfix(reports): correct date formatting in timezone conversion\n\nUse UTC timestamps consistently across report generation.\nFixes #234.\n```\n\nFormat: `type(scope): brief description`\n\nTypes: feat, fix, docs, style, refactor, test, chore\n```\n\n---\n\n## Good/Bad Examples Pattern\n\nShow correct and incorrect patterns with explanations.\n\n### Error Handling\n\n```markdown\n## Error Handling Examples\n\n### Good Example\n```typescript\nasync function fetchUser(id: string): Promise<Result<User>> {\n  try {\n    const user = await api.getUser(id);\n    if (!user) {\n      return { ok: false, error: 'User not found' };\n    }\n    return { ok: true, data: user };\n  } catch (error) {\n    logger.error('Failed to fetch user', { id, error });\n    return { ok: false, error: 'Failed to fetch user' };\n  }\n}\n```\n\n**Why it's good:**\n- Explicit error handling\n- Logging for debugging\n- Typed return value\n- User-friendly error message\n\n### Bad Example (Don't Do This)\n```typescript\nasync function fetchUser(id: string) {\n  const user = await api.getUser(id);  // Unhandled rejection\n  return user;  // Returns undefined if not found\n}\n```\n\n**Why it's bad:**\n- No error handling\n- Caller can't distinguish \"not found\" from error\n- No logging for debugging\n```\n\n### Input Validation\n\n```markdown\n## Input Validation Examples\n\n### Good Example\n```typescript\nconst CreateUserSchema = z.object({\n  name: z.string().min(1, 'Name required').max(100),\n  email: z.string().email('Invalid email format'),\n  age: z.number().int().min(0).max(150).optional(),\n});\n\nfunction createUser(input: unknown) {\n  const result = CreateUserSchema.safeParse(input);\n  if (!result.success) {\n    return { error: result.error.format() };\n  }\n  return saveUser(result.data);\n}\n```\n\n### Bad Example (Don't Do This)\n```typescript\nfunction createUser(input: any) {\n  // No validation - vulnerable to injection, crashes\n  return saveUser(input);\n}\n```\n```\n\n---\n\n## Output Specification Pattern\n\nDefine expected output characteristics.\n\n### Widget Output Specification\n\n```markdown\n## Widget Output Specification\n\nEvery generated widget includes:\n\n### Required Elements\n- [ ] Container with theme-aware styles\n- [ ] Data binding to window.openai.toolOutput\n- [ ] Loading state (shown before data arrives)\n- [ ] Error state (shown when data.isError)\n- [ ] Empty state (shown when no data)\n\n### Code Structure\n```typescript\n// Required structure\nfunction Widget() {\n  const data = window.openai?.toolOutput;\n  const theme = window.openai?.theme ?? 'light';\n\n  if (!data) return <LoadingState />;\n  if (data.isError) return <ErrorState message={data.error} />;\n  if (isEmpty(data)) return <EmptyState />;\n\n  return <MainContent data={data} theme={theme} />;\n}\n```\n\n### Style Requirements\n- System fonts only (no custom fonts)\n- Theme-aware colors (light/dark)\n- Responsive breakpoints (mobile + desktop)\n- WCAG AA contrast ratios\n```\n\n### API Response Specification\n\n```markdown\n## API Response Specification\n\nAll responses follow this structure:\n\n### Success Response\n```json\n{\n  \"success\": true,\n  \"data\": { ... },\n  \"meta\": {\n    \"timestamp\": \"ISO-8601\",\n    \"requestId\": \"uuid\"\n  }\n}\n```\n\n### Error Response\n```json\n{\n  \"success\": false,\n  \"error\": {\n    \"code\": \"ERROR_CODE\",\n    \"message\": \"Human-readable message\",\n    \"details\": { ... }\n  },\n  \"meta\": {\n    \"timestamp\": \"ISO-8601\",\n    \"requestId\": \"uuid\"\n  }\n}\n```\n\n### Pagination\n```json\n{\n  \"success\": true,\n  \"data\": [...],\n  \"pagination\": {\n    \"page\": 1,\n    \"pageSize\": 20,\n    \"total\": 100,\n    \"hasMore\": true\n  }\n}\n```\n```\n\n---\n\n## Consistency Patterns\n\n### Naming Conventions\n\n```markdown\n## Naming Conventions\n\n| Type | Convention | Example |\n|------|------------|---------|\n| Files | kebab-case | `user-service.ts` |\n| Functions | camelCase | `getUserById` |\n| Classes | PascalCase | `UserService` |\n| Constants | UPPER_SNAKE | `MAX_RETRIES` |\n| Types | PascalCase | `UserResponse` |\n| CSS classes | kebab-case | `user-card` |\n```\n\n### File Structure\n\n```markdown\n## File Structure Convention\n\n```\nsrc/\n components/       # React components\n    Button/\n       Button.tsx\n       Button.test.tsx\n       index.ts\n hooks/           # Custom hooks\n services/        # API/business logic\n utils/           # Helper functions\n types/           # TypeScript types\n index.ts         # Public exports\n```\n```\n",
        "skills/resume_optimizer/.claude/skills/skill-creator-pro/references/quality-patterns.md": "# Quality Patterns\n\nPatterns for clarifications, enforcement checklists, and quality gates.\n\n---\n\n## User Interaction Patterns\n\n### Clarification Questions\n\nStructure clarifications as Required vs Optional:\n\n```markdown\n## Required Clarifications\n\nBefore proceeding, ask:\n\n1. **Data shape**: \"What structure will the input have?\"\n   ```json\n   Example: { items: [...], total: 10 }\n   ```\n\n2. **Action type**: \"Read-only or write operations?\"\n   - Read  Simple rendering\n   - Write  Need form handling, validation\n\n3. **Output format**: \"What should the result look like?\"\n\n## Optional Clarifications\n\n4. **Styling**: \"Any design preferences?\" (ask if complex UI)\n5. **Performance**: \"Expected data volume?\" (ask if potentially large)\n```\n\n### Context Awareness\n\nBefore asking, check existing context:\n\n```markdown\n## Before Asking\n\n1. Review conversation history for prior answers\n2. Infer from file names/content when possible\n3. Check available data structures\n4. Only ask what cannot be determined\n\nExample: If user says \"create a progress widget for my course app\",\ninfer it's a Builder skill for education domain before asking.\n```\n\n### Graceful Handling\n\nWhat to do when user doesn't answer:\n\n```markdown\n## If User Skips Clarifications\n\n- **Required questions**: Explain why needed, ask again simply\n- **Optional questions**: Proceed with sensible defaults\n- **Ambiguous answers**: Confirm interpretation before proceeding\n```\n\n---\n\n## Official Documentation Links\n\n### Documentation Table Pattern\n\n```markdown\n## Official Documentation\n\n| Resource | URL | Use For |\n|----------|-----|---------|\n| Getting Started | https://docs.example.com/start | Basic setup |\n| API Reference | https://docs.example.com/api | Method details |\n| Best Practices | https://docs.example.com/best | Patterns |\n| Examples | https://github.com/example/examples | Reference code |\n\nFor patterns not covered here, fetch from official docs.\n```\n\n### Fetch Guidance Pattern\n\n```markdown\n## Unlisted Scenarios\n\nFor patterns not documented in this skill:\n\n1. Fetch from official docs: [primary URL]\n2. Apply same quality standards as this skill\n3. Follow established patterns\n\nExamples of when to fetch:\n- Complex authentication flows\n- Third-party integrations\n- Platform-specific features\n- New API versions\n```\n\n### Version Awareness\n\n```markdown\n## Keeping Current\n\n- Official docs: [URL]\n- Changelog: [URL]\n- Last verified: 2024-12\n\nWhen docs update:\n1. Check for breaking changes\n2. Update affected references\n3. Verify examples still work\n```\n\n---\n\n## Domain Standards Enforcement\n\n### Must Follow / Must Avoid Pattern\n\n```markdown\n## Standards Enforcement\n\n### Must Follow\n- [ ] WCAG AA contrast (4.5:1 for text, 3:1 for UI)\n- [ ] Keyboard navigation for all interactions\n- [ ] Focus indicators visible\n- [ ] Screen reader labels for icons\n- [ ] Error states with clear messages\n\n### Must Avoid\n- Color as only indicator\n- Mouse-only interactions\n- Auto-playing media without controls\n- Nested scrolling\n- Hardcoded strings\n```\n\n### Domain-Specific Examples\n\n**Web Accessibility (WCAG)**:\n```markdown\n### Must Follow\n- [ ] 4.5:1 contrast ratio for text\n- [ ] 3:1 contrast ratio for UI components\n- [ ] Keyboard accessible\n- [ ] Focus visible\n- [ ] Alt text for images\n\n### Must Avoid\n- Color-only indicators\n- Keyboard traps\n- Auto-playing audio\n```\n\n**API Security (OWASP)**:\n```markdown\n### Must Follow\n- [ ] Input validation on all endpoints\n- [ ] Parameterized queries (no SQL injection)\n- [ ] Authentication on protected routes\n- [ ] Rate limiting\n- [ ] HTTPS only\n\n### Must Avoid\n- Hardcoded secrets\n- SQL string concatenation\n- Verbose error messages to clients\n- Sensitive data in logs\n```\n\n**Code Quality**:\n```markdown\n### Must Follow\n- [ ] TypeScript strict mode\n- [ ] Explicit return types\n- [ ] Error handling with try/catch\n- [ ] Meaningful variable names\n\n### Must Avoid\n- `any` type\n- Ignored errors\n- Magic numbers\n- Deep nesting (>3 levels)\n```\n\n---\n\n## Quality Gates\n\n### Output Checklist Pattern\n\n```markdown\n## Output Checklist\n\nBefore delivering, verify ALL items:\n\n### Functional\n- [ ] Core feature works as specified\n- [ ] Error states handled gracefully\n- [ ] Loading states present (if async)\n- [ ] Edge cases covered\n\n### Quality\n- [ ] Follows naming conventions\n- [ ] No hardcoded values\n- [ ] Comments where logic is non-obvious\n- [ ] Consistent formatting\n\n### Standards\n- [ ] Passes domain requirements (above)\n- [ ] Tested against acceptance criteria\n- [ ] No security vulnerabilities\n```\n\n### Skill-Specific Checklists\n\n**Widget Output Checklist**:\n```markdown\n## Widget Output Checklist\n\n### Functional\n- [ ] window.openai data access with null checks\n- [ ] Event listener for openai:set_globals\n- [ ] Loading state before data\n- [ ] Error state when data.isError\n- [ ] Empty state when no data\n\n### Visual\n- [ ] Theme support (light/dark)\n- [ ] System fonts\n- [ ] WCAG AA contrast\n- [ ] Responsive layout\n- [ ] Focus indicators\n\n### UX\n- [ ] Follows \"Extract, Don't Port\"\n- [ ] Inline mode unless justified\n- [ ] 2 actions per card\n```\n\n**Script Output Checklist**:\n```markdown\n## Script Output Checklist\n\n### Functional\n- [ ] Runs without errors on sample input\n- [ ] Handles missing/invalid input gracefully\n- [ ] Produces expected output format\n- [ ] Cleans up temporary files\n\n### Quality\n- [ ] Clear usage instructions\n- [ ] Documented parameters\n- [ ] Meaningful error messages\n- [ ] Exit codes for automation\n```\n\n---\n\n## Good/Bad Examples Pattern\n\nAlways include both correct and incorrect patterns:\n\n```markdown\n### Good Example\n```python\n# Correct: Explicit error handling\ntry:\n    result = process_file(path)\n    return {\"success\": True, \"data\": result}\nexcept FileNotFoundError:\n    return {\"success\": False, \"error\": \"File not found\"}\nexcept Exception as e:\n    logger.error(f\"Processing failed: {e}\")\n    return {\"success\": False, \"error\": \"Processing failed\"}\n```\n\n### Bad Example (Don't Do This)\n```python\n# Wrong: Silently ignores errors\ntry:\n    result = process_file(path)\nexcept:\n    pass  # Never do this - errors are hidden\n```\n```\n",
        "skills/resume_optimizer/.claude/skills/skill-creator-pro/references/reusability-patterns.md": "# Reusability Patterns\n\nPatterns for creating adaptable skills that handle variations across requirements.\n\n---\n\n## Skills = Procedural Knowledge + Domain Expertise\n\nEvery production-grade skill encodes TWO types of knowledge:\n\n### Procedural Knowledge (HOW)\nStep-by-step processes, decision trees, workflows that guide execution.\n\n| Examples |\n|----------|\n| \"First validate input, then process, then verify output\" |\n| \"If authentication fails, retry with backoff, then prompt for credentials\" |\n| \"Check for null  validate format  transform  return\" |\n\n### Domain Expertise (WHAT)\nConcepts, best practices, patterns, anti-patterns specific to the domain.\n\n| Examples |\n|----------|\n| \"WCAG AA requires 4.5:1 contrast ratio for text\" |\n| \"React components should be pure functions\" |\n| \"API rate limits are typically 1000 req/min\" |\n\n### How They Work Together\n\n```\nUser Request\n     \n     \n\n Domain Expertise (WHAT)         \n - Understand the domain         \n - Apply best practices          \n - Avoid anti-patterns           \n\n     \n     \n\n Procedural Knowledge (HOW)      \n - Follow step-by-step workflow  \n - Make decisions at branches    \n - Handle errors appropriately   \n\n     \n     \nQuality Output\n```\n\n### Encoding in Skills (Embedded, Not Discovered at Runtime)\n\n| Knowledge Type | Where to Embed |\n|----------------|----------------|\n| Procedural (workflows) | SKILL.md main sections |\n| Domain expertise | `references/` (structured per domain needs) |\n| Complex procedures | `scripts/` for executable code |\n| Templates | `assets/` for boilerplate |\n\n**What goes in references/**:\n- Library/API documentation\n- Best practices\n- Code examples\n- Patterns and anti-patterns\n- Domain-specific details\n\n**Key**: Domain expertise is EMBEDDED during skill creation. Generated skills are zero-shot experts. Structure `references/` based on what the domain needs.\n\n---\n\n## Core Concept: Varies vs Constant\n\nEvery domain has elements that VARY across use cases and elements that remain CONSTANT.\n\n**Skills encode the CONSTANT patterns and ask clarifying questions for what VARIES.**\n\n---\n\n## Domain Analysis Pattern\n\nBefore creating a skill, analyze the domain:\n\n| Question | Purpose |\n|----------|---------|\n| What changes between use cases? | Identify clarification questions needed |\n| What stays the same? | Identify patterns to encode in skill |\n| What are the common variations? | Identify options to present |\n| What are the boundaries? | Identify scope (does/does not do) |\n\n---\n\n## Examples by Domain\n\n### Visualization Skills\n\n| Varies | Constant |\n|--------|----------|\n| Data shape/structure | Rendering lifecycle |\n| Chart type (bar, line, pie) | Accessibility requirements |\n| Library (Recharts, D3, Chart.js) | Responsive patterns |\n| Color scheme | Loading/error states |\n| Interactivity level | Data validation |\n\n**Skill should**: Ask for data shape, chart type, library preference\n**Skill should NOT**: Hardcode specific data fields or single chart type\n\n### Web Framework Skills (Next.js, React, etc.)\n\n| Varies | Constant |\n|--------|----------|\n| Database (Postgres, MongoDB, Prisma) | Project structure patterns |\n| CSS framework (Tailwind, CSS Modules) | Component architecture |\n| Auth provider (NextAuth, Clerk) | Error handling patterns |\n| Deployment target (Vercel, AWS) | Performance best practices |\n| Features (specific pages, APIs) | Security patterns |\n\n**Skill should**: Ask for tech stack preferences, feature requirements\n**Skill should NOT**: Hardcode database schema or specific routes\n\n### Deployment Skills\n\n| Varies | Constant |\n|--------|----------|\n| Platform (AWS, GCP, Azure) | CI/CD principles |\n| Orchestration (Kubernetes, ECS, serverless) | Rollback strategies |\n| Configuration (Helm, Terraform, CDK) | Health check patterns |\n| Environment (dev, staging, prod) | Secret management principles |\n| Scale requirements | Monitoring patterns |\n\n**Skill should**: Ask for platform, orchestration preference, environment\n**Skill should NOT**: Hardcode specific cloud resources or configs\n\n### API Integration Skills\n\n| Varies | Constant |\n|--------|----------|\n| Endpoint URLs | Error handling patterns |\n| Authentication method | Retry with backoff |\n| Request/response shape | Rate limiting handling |\n| Third-party service | Timeout management |\n| Rate limits | Response validation |\n\n**Skill should**: Ask for API details, auth method\n**Skill should NOT**: Hardcode specific API endpoints\n\n### Data Processing Skills\n\n| Varies | Constant |\n|--------|----------|\n| Input format (CSV, JSON, XML) | Validation patterns |\n| Output format | Error recovery |\n| Transformation rules | Streaming for large files |\n| Data schema | Progress reporting |\n| Volume/scale | Cleanup procedures |\n\n**Skill should**: Ask for input/output formats, transformation needs\n**Skill should NOT**: Hardcode specific field names or schemas\n\n---\n\n## Abstraction Levels\n\n### Level 1: Domain-Agnostic (Highest Reuse)\n\nSkills that work across many domains:\n- Error handling patterns\n- Logging and monitoring\n- Testing strategies\n- Documentation generation\n\n### Level 2: Domain-Specific, Tool-Agnostic (High Reuse)\n\nSkills that work within a domain but across tools:\n- \"Visualization\" (not \"Recharts visualization\")\n- \"Deployment\" (not \"Kubernetes deployment\")\n- \"API integration\" (not \"Stripe integration\")\n\n### Level 3: Tool-Specific (Moderate Reuse)\n\nSkills for specific tools but adaptable workflows:\n- \"Next.js applications\" (adaptable to different features)\n- \"PostgreSQL databases\" (adaptable to different schemas)\n\n### Level 4: Requirement-Specific (Avoid)\n\nSkills tied to single requirements:\n- \"Sales dashboard with bar chart\" - TOO SPECIFIC\n- \"User auth with email/password\" - TOO SPECIFIC\n\n---\n\n## Clarification Questions for Reusability\n\nStructure questions to capture variations:\n\n```markdown\n## Required Clarifications\n\n1. **[Variable Element]**: \"What [element] will you use?\"\n   - Option A  implications\n   - Option B  implications\n   - Other  ask for details\n\n2. **[Another Variable]**: \"What are your requirements for [aspect]?\"\n```\n\n### Good Examples\n\n```markdown\n## Required Clarifications\n\n1. **Data source**: \"What shape will the input data have?\"\n   - Provide example structure\n\n2. **Output format**: \"What type of visualization?\"\n   - Chart (bar, line, pie, etc.)\n   - Table\n   - Dashboard\n   - Other\n\n3. **Library preference**: \"Any preferred visualization library?\"\n   - Recharts (React)\n   - D3.js (vanilla)\n   - Chart.js\n   - No preference (recommend based on use case)\n```\n\n### Bad Examples (Too Specific)\n\n```markdown\n## This skill creates:\n- A bar chart showing monthly sales\n- Using Recharts\n- With blue color scheme\n```\n\n---\n\n## Reusability Checklist\n\nBefore finalizing a skill, verify:\n\n### Scope\n- [ ] Does NOT hardcode specific data fields/schemas\n- [ ] Does NOT hardcode specific tool/library (unless tool-specific skill)\n- [ ] Does NOT hardcode specific configurations\n- [ ] DOES handle common variations via clarifications\n\n### Clarifications\n- [ ] Asks for variable elements (data shape, tool preference, etc.)\n- [ ] Provides reasonable options for common choices\n- [ ] Allows \"other\" for uncommon variations\n\n### Patterns\n- [ ] Encodes CONSTANT domain patterns (best practices, error handling)\n- [ ] Separates concerns (what varies vs what's constant)\n- [ ] Works for multiple use cases within the domain\n\n### Boundaries\n- [ ] Clear scope (what it does / does not do)\n- [ ] Explicit about abstraction level (domain vs tool-specific)\n\n---\n\n## Anti-Patterns\n\n### Hardcoded Specifics\n\n```markdown\n# Bad: Hardcoded data fields\nThe widget displays:\n- product.name\n- product.price\n- product.quantity\n```\n\n```markdown\n# Good: Adaptable to data shape\nAsk for data structure, then map fields dynamically.\n```\n\n### Single Tool Lock-in (when domain-level skill)\n\n```markdown\n# Bad: Tool lock-in for domain skill\nThis visualization skill uses Recharts exclusively.\n```\n\n```markdown\n# Good: Tool-agnostic with preference\nAsk for library preference. Support Recharts, D3, Chart.js, or recommend based on use case.\n```\n\n### Feature Enumeration\n\n```markdown\n# Bad: Listing specific features\nCreates:\n- Login page\n- Dashboard\n- Settings page\n```\n\n```markdown\n# Good: Feature categories\nAsk what features are needed. Provide patterns for:\n- Authentication flows\n- Data display pages\n- Configuration interfaces\n```\n",
        "skills/resume_optimizer/.claude/skills/skill-creator-pro/references/skill-patterns.md": "# Skill Patterns\n\nSKILL.md structure and examples for different skill types.\n\n---\n\n## Frontmatter Specification\n\n### Complete Frontmatter Template\n\n```yaml\n---\nname: skill-name                          # Required\ndescription: |                            # Required\n  [What] Brief statement of capability.\n  [When] Use when users ask to <triggers>.\nallowed-tools: Read, Grep, Glob           # Optional: restrict tool access\nmodel: claude-sonnet-4-20250514           # Optional: model override\n---\n```\n\n### Field Requirements\n\n| Field | Required | Constraints | Purpose |\n|-------|----------|-------------|---------|\n| `name` | Yes | Lowercase, numbers, hyphens only; 64 chars; must match directory name | Skill identifier |\n| `description` | Yes | 1024 characters; must include What + When | Claude Code uses this to decide when to trigger |\n| `allowed-tools` | No | Comma-separated tool names | Restricts tool access during skill execution |\n| `model` | No | Valid model ID | Override model for complex reasoning |\n\n### Name Constraints\n\n```\n Valid: pdf-processor, data-viz, api-v2\n Invalid: PDF_Processor, dataViz, my skill\n```\n\n- Lowercase letters, numbers, hyphens only\n- Maximum 64 characters\n- Must match the directory name exactly\n\n### Description Format\n\n**Structure**: `[What it does] + [When to use/triggers]`\n\n**Limit**: 1024 characters (truncated if exceeded)\n\n**Purpose**: Claude Code reads this to decide when to activate the skill. Include specific trigger phrases users would say.\n\n```yaml\n# Good: Clear what + when with triggers (third-person style)\ndescription: |\n  Create data visualizations with charts and graphs.\n  This skill should be used when users ask to visualize data, create charts,\n  build dashboards, or display metrics graphically.\n\n# Bad: Vague, no triggers\ndescription: Helps with charts\n```\n\n### allowed-tools Usage\n\nRestrict tool access for security or scope:\n\n```yaml\n# Read-only skill (no file modifications)\nallowed-tools: Read, Grep, Glob\n\n# Analysis skill with web access\nallowed-tools: Read, Grep, WebFetch, WebSearch\n\n# Full access (default if omitted)\n# allowed-tools: (omit field)\n```\n\n### model Override\n\nSpecify when skill needs different model capabilities:\n\n```yaml\n# For complex reasoning tasks\nmodel: claude-sonnet-4-20250514\n\n# For simple, fast operations\nmodel: claude-haiku-3-20250514\n```\n\n---\n\n## SKILL.md Structure\n\n### Complete Template\n\n```markdown\n---\nname: skill-name\ndescription: |\n  [What] Capability statement.\n  [When] Use when users ask to <specific triggers>.\n---\n\n# Skill Name\n\nBrief one-line description.\n\n## What This Skill Does\n- Capability 1\n- Capability 2\n- Capability 3\n\n## What This Skill Does NOT Do\n- Exclusion 1\n- Exclusion 2\n\n---\n\n## Before Implementation\n\nGather context to ensure successful implementation:\n\n| Source | Gather |\n|--------|--------|\n| **Codebase** | Existing structure, patterns, conventions to integrate with |\n| **Conversation** | User's specific requirements, constraints, preferences |\n| **Skill References** | Domain patterns from `references/` (library docs, best practices, examples) |\n| **User Guidelines** | Project-specific conventions, team standards |\n\nEnsure all required context is gathered before implementing.\nOnly ask user for THEIR specific requirements (domain expertise is in this skill).\n\n---\n\n## Required Clarifications\n\nAsk about USER'S context (not domain knowledge):\n\n1. **Use case**: \"What's YOUR specific need?\"\n2. **Constraints**: \"Any specific requirements?\"\n\n---\n\n## Workflow\n\n1. Step one\n2. Step two\n3. Step three\n\n---\n\n## [Domain-Specific Section]\n\nContent specific to what the skill does.\n\n---\n\n## Output Checklist\n\nBefore delivering, verify:\n- [ ] Requirement 1\n- [ ] Requirement 2\n- [ ] Requirement 3\n\n---\n\n## Reference Files\n\n| File | When to Read |\n|------|--------------|\n| `references/file1.md` | When X |\n| `references/file2.md` | When Y |\n```\n\n---\n\n## Skill Types Taxonomy\n\n### Overview\n\n| Type | Purpose | Key Output |\n|------|---------|------------|\n| **Builder** | Creates new artifacts | Code, documents, widgets, configs |\n| **Guide** | Provides instructions | Step-by-step workflows, tutorials |\n| **Automation** | Executes workflows | Processed files, transformed data |\n| **Analyzer** | Extracts insights | Reports, summaries, reviews |\n| **Validator** | Enforces quality | Pass/fail assessments, scores |\n\n### Type Selection Guide\n\n```\nWhat does the skill primarily do?\n\nCreates NEW artifacts (code, docs, widgets)?\n   Builder\n\nTeaches HOW to do something?\n   Guide\n\nExecutes multi-step PROCESSES automatically?\n   Automation\n\nEXTRACTS information or provides ANALYSIS?\n   Analyzer\n\nCHECKS quality or ENFORCES standards?\n   Validator\n```\n\n---\n\n## Builder Skills (Create Artifacts)\n\n**Purpose**: Generate new code, documents, widgets, configurations\n\n**Key elements**:\n- Required Clarifications (MUST ask before building)\n- Output specification\n- Domain standards enforcement\n- Templates in assets/\n\n**Example frontmatter**:\n```yaml\n---\nname: widget-creator\ndescription: |\n  Create production widgets for ChatGPT Apps.\n  Use when users ask to build UI components, visual interfaces,\n  or interactive elements.\n---\n```\n\n**Required sections**:\n```markdown\n## Before Implementation\n\nGather context to ensure successful implementation:\n\n| Source | Gather |\n|--------|--------|\n| **Codebase** | Existing structure, patterns, conventions to integrate with |\n| **Conversation** | User's specific requirements, constraints, preferences |\n| **Skill References** | Domain patterns from `references/` |\n| **User Guidelines** | Project-specific conventions, team standards |\n\n## Required Clarifications\n1. **Data shape**: \"What structure will input have?\"\n2. **Output type**: \"What artifact to create?\"\n3. **Constraints**: \"Any specific requirements?\"\n\n## Output Specification\n[Define what the artifact looks like]\n\n## Domain Standards\n### Must Follow\n- [ ] Standard 1\n### Must Avoid\n- Anti-pattern 1\n\n## Output Checklist\n- [ ] Artifact meets requirements\n```\n\n---\n\n## Guide Skills (Provide Instructions)\n\n**Purpose**: Teach procedures, provide tutorials, explain how-to\n\n**Key elements**:\n- Step-by-step workflow\n- Good/bad examples\n- Official documentation links\n- Decision trees for branching\n\n**Example frontmatter**:\n```yaml\n---\nname: api-integration-guide\ndescription: |\n  Guide for integrating external APIs.\n  Use when users need to connect to third-party services,\n  handle authentication, or manage API responses.\n---\n```\n\n**Required sections**:\n```markdown\n## Before Implementation\n\nGather context to ensure successful implementation:\n\n| Source | Gather |\n|--------|--------|\n| **Codebase** | Existing structure, patterns, conventions to integrate with |\n| **Conversation** | User's specific requirements, constraints, preferences |\n| **Skill References** | Domain patterns from `references/` |\n| **User Guidelines** | Project-specific conventions, team standards |\n\n## Workflow\n1. **Step 1** - Action\n2. **Step 2** - Action\n3. **Step 3** - Action\n\n## Examples\n### Good Example\n[Correct pattern with explanation]\n\n### Bad Example (Avoid)\n[Incorrect pattern with explanation]\n\n## Official Documentation\n| Resource | URL | Use For |\n|----------|-----|---------|\n| Docs | https://... | Reference |\n```\n\n---\n\n## Automation Skills (Execute Workflows)\n\n**Purpose**: Process files, deploy systems, execute multi-step operations\n\n**Key elements**:\n- Tested scripts in scripts/\n- Error handling guidance\n- Dependencies documented\n- Input/output contracts\n\n**Example frontmatter**:\n```yaml\n---\nname: pdf-processor\ndescription: |\n  Process PDF files with extraction, rotation, and form filling.\n  Use when users need to manipulate PDF documents programmatically.\n---\n```\n\n**Required sections**:\n```markdown\n## Before Implementation\n\nGather context to ensure successful implementation:\n\n| Source | Gather |\n|--------|--------|\n| **Codebase** | Existing structure, patterns, conventions to integrate with |\n| **Conversation** | User's specific requirements, constraints, preferences |\n| **Skill References** | Domain patterns from `references/` |\n| **User Guidelines** | Project-specific conventions, team standards |\n\n## Available Scripts\n| Script | Purpose | Usage |\n|--------|---------|-------|\n| `scripts/process.py` | Main processing | `python process.py input output` |\n\n## Dependencies\n- Python 3.10+\n- Required packages: [list]\n\n## Error Handling\n| Error | Recovery |\n|-------|----------|\n| Invalid input | [Action] |\n\n## Input/Output\n- **Input**: [Format, constraints]\n- **Output**: [Format, location]\n```\n\n---\n\n## Analyzer Skills (Extract Insights)\n\n**Purpose**: Review documents, analyze data, extract information, summarize\n\n**Key elements**:\n- Analysis scope and criteria\n- Extraction patterns\n- Output format specification\n- Synthesis guidance\n\n**Example frontmatter**:\n```yaml\n---\nname: code-analyzer\ndescription: |\n  Analyze codebases for patterns, issues, and improvements.\n  Use when users ask to review code, find patterns,\n  assess quality, or understand architecture.\n---\n```\n\n**Required sections**:\n```markdown\n## Before Implementation\n\nGather context to ensure successful implementation:\n\n| Source | Gather |\n|--------|--------|\n| **Codebase** | Existing structure, patterns, conventions to integrate with |\n| **Conversation** | User's specific requirements, constraints, preferences |\n| **Skill References** | Domain patterns from `references/` |\n| **User Guidelines** | Project-specific conventions, team standards |\n\n## Analysis Scope\n- What to analyze\n- What to ignore\n\n## Evaluation Criteria\n| Criterion | Weight | How to Assess |\n|-----------|--------|---------------|\n| Criterion 1 | X% | [Method] |\n\n## Output Format\n[Specify report structure]\n\n## Synthesis\n- Combine findings into actionable insights\n- Prioritize by impact\n```\n\n---\n\n## Validator Skills (Enforce Quality)\n\n**Purpose**: Check compliance, score quality, enforce standards\n\n**Key elements**:\n- Quality criteria with scoring\n- Pass/fail thresholds\n- Remediation guidance\n- Evidence collection\n\n**Example frontmatter**:\n```yaml\n---\nname: accessibility-validator\ndescription: |\n  Validate web content for WCAG accessibility compliance.\n  Use when users ask to check accessibility, audit for\n  compliance, or verify standards adherence.\n---\n```\n\n**Required sections**:\n```markdown\n## Before Implementation\n\nGather context to ensure successful implementation:\n\n| Source | Gather |\n|--------|--------|\n| **Codebase** | Existing structure, patterns, conventions to integrate with |\n| **Conversation** | User's specific requirements, constraints, preferences |\n| **Skill References** | Domain patterns from `references/` |\n| **User Guidelines** | Project-specific conventions, team standards |\n\n## Quality Criteria\n| Criterion | Weight | Pass Threshold |\n|-----------|--------|----------------|\n| Criterion 1 | X% | [Threshold] |\n\n## Scoring Rubric\n- **3 (Excellent)**: [Definition]\n- **2 (Good)**: [Definition]\n- **1 (Needs Work)**: [Definition]\n- **0 (Fail)**: [Definition]\n\n## Thresholds\n- **Pass**: Score  X\n- **Conditional**: Score X-Y\n- **Fail**: Score < Y\n\n## Remediation\n| Issue | Fix |\n|-------|-----|\n| Issue 1 | [How to fix] |\n```\n\n---\n\n## Assets Directory Patterns\n\n### When to Use assets/\n\n| Use Case | Example |\n|----------|---------|\n| Output templates | HTML boilerplate, component scaffolds |\n| Exact boilerplate | Config files that must be precise |\n| Visual assets | Images, icons, fonts |\n| Data templates | JSON schemas, sample data |\n\n### When NOT to Use assets/\n\n| Avoid | Instead |\n|-------|---------|\n| Code that varies per use | Describe pattern in SKILL.md |\n| Large files (>50KB) | Reference external URLs |\n| Generated content | Create dynamically |\n\n### Asset Types\n\n```\nassets/\n templates/           # Output scaffolds\n    component.tsx    # React component template\n    page.html        # HTML page template\n    config.json      # Configuration template\n schemas/             # Data structures\n    input.schema.json\n    output.schema.json\n examples/            # Reference implementations\n     simple.tsx\n     advanced.tsx\n```\n\n### Referencing Assets in SKILL.md\n\n**Template with placeholders**:\n```markdown\nUse `assets/templates/component.tsx` as base template.\n\nReplace placeholders:\n- `{{COMPONENT_NAME}}`  PascalCase component name\n- `{{PROPS_INTERFACE}}`  TypeScript props interface\n- `{{RENDER_CONTENT}}`  JSX content\n```\n\n**Exact copy**:\n```markdown\nCopy `assets/config.json` to project root.\nDo not modify structure; only update values.\n```\n\n**Reference example**:\n```markdown\nFollow pattern in `assets/examples/simple.tsx` for basic usage.\nFor advanced features, see `assets/examples/advanced.tsx`.\n```\n\n### Template Design Principles\n\n1. **Self-documenting placeholders**: `{{DESCRIPTIVE_NAME}}`\n2. **Minimal structure**: Only essential boilerplate\n3. **Clear boundaries**: Mark customizable sections\n4. **Valid syntax**: Template should be syntactically valid\n\n**Good template**:\n```typescript\n// assets/templates/component.tsx\nimport React from 'react';\n\ninterface {{COMPONENT_NAME}}Props {\n  {{PROPS}}\n}\n\nexport function {{COMPONENT_NAME}}({ {{DESTRUCTURED_PROPS}} }: {{COMPONENT_NAME}}Props) {\n  {{HOOKS}}\n\n  return (\n    {{JSX_CONTENT}}\n  );\n}\n```\n\n---\n\n## Frontmatter Best Practices\n\n### Good Description (triggers reliably)\n\n```yaml\ndescription: |\n  Create production widgets for ChatGPT Apps using OpenAI Apps SDK.\n  Use when users ask to build UI components, visual interfaces,\n  progress trackers, quiz interfaces, or interactive elements.\n```\n\n**Why it works**:\n- [What] Clear capability statement first\n- [When] Specific trigger phrases users would say\n- 1024 characters\n- Claude Code can match user intent\n\n### Bad Description (won't trigger)\n\n```yaml\ndescription: Widget stuff\n```\n\n**Why it fails**:\n- No [What] capability statement\n- No [When] trigger phrases\n- Too vague for Claude Code to match\n\n---\n\n## Scope Clarity Examples\n\n### Good Scope\n\n```markdown\n## What This Skill Does\n- Creates ChatGPT widgets with window.openai integration\n- Supports all display modes (inline, fullscreen, pip)\n- Implements theme support (light/dark)\n- Follows OpenAI UX/UI guidelines\n\n## What This Skill Does NOT Do\n- Create native mobile apps\n- Handle backend server logic\n- Manage user authentication\n- Deploy widgets to production\n```\n\n### Bad Scope (missing exclusions)\n\n```markdown\n## What This Skill Does\n- Creates widgets\n```\n\n---\n\n## Reference Organization\n\n### By Domain (for multi-domain skills)\n\n```\nreferences/\n aws.md        # AWS-specific patterns\n gcp.md        # GCP-specific patterns\n azure.md      # Azure-specific patterns\n```\n\n### By Complexity (for single-domain skills)\n\n```\nreferences/\n quick-start.md     # Basic usage\n advanced.md        # Complex scenarios\n troubleshooting.md # Common issues\n```\n\n### By Feature (for feature-rich skills)\n\n```\nreferences/\n authentication.md  # Auth patterns\n state-management.md # State handling\n error-handling.md   # Error patterns\n```\n",
        "skills/resume_optimizer/.claude/skills/skill-creator-pro/references/technical-patterns.md": "# Technical Patterns\n\nError handling, security considerations, and dependency documentation.\n\n---\n\n## Error Handling\n\n### Error Handling Table\n\nDocument error scenarios and actions:\n\n```markdown\n## Error Handling\n\n| Scenario | Detection | Action |\n|----------|-----------|--------|\n| Invalid input | Validation fails | Return error with specifics |\n| File not found | FileNotFoundError | Clear message, suggest fix |\n| Network failure | Timeout/ConnectionError | Retry 3x with backoff |\n| Auth failure | 401/403 response | Prompt re-authentication |\n| Unknown error | Catch-all exception | Log context, safe default |\n```\n\n### Retry Pattern\n\n```python\nimport time\nimport random\n\ndef retry_with_backoff(func, max_retries=3, base_delay=0.1):\n    \"\"\"Retry with exponential backoff and jitter.\"\"\"\n    for attempt in range(max_retries):\n        try:\n            return func()\n        except Exception as e:\n            if attempt == max_retries - 1:\n                raise\n            delay = base_delay * (2 ** attempt) + random.uniform(0, 0.1)\n            time.sleep(delay)\n```\n\n### Error Response Format\n\n```typescript\n// Consistent error response structure\ninterface ErrorResponse {\n  isError: true;\n  content: [{ type: 'text', text: string }];  // User-friendly message\n  _meta?: {\n    errorCode: string;      // Machine-readable code\n    details: unknown;       // Debug info (not shown to user)\n    retryable: boolean;     // Can user retry?\n  };\n}\n\n// Example\nreturn {\n  isError: true,\n  content: [{ type: 'text', text: 'Could not process file. Please check the format.' }],\n  _meta: {\n    errorCode: 'INVALID_FORMAT',\n    details: { expected: 'PDF', received: 'PNG' },\n    retryable: false\n  }\n};\n```\n\n### Graceful Degradation\n\n```markdown\n## Fallback Strategy\n\n1. **Primary**: Execute main logic\n2. **Retry**: On transient failure, retry with backoff\n3. **Cache**: If available, serve cached result\n4. **Fallback**: Return safe default or partial result\n5. **Fail**: Clear error message with next steps\n```\n\n---\n\n## Security Considerations\n\n### Secrets Management\n\n```markdown\n## Secrets Handling\n\n### Never Do\n- Hardcode API keys, tokens, passwords in code\n- Commit .env files to version control\n- Log sensitive values\n- Include secrets in error messages\n\n### Always Do\n- Use environment variables\n- Keep .env in .gitignore\n- Provide .env.example template\n- Rotate compromised credentials immediately\n```\n\n### Input Validation\n\n```markdown\n## Input Validation\n\nValidate ALL external input:\n\n### File Paths\n```python\nimport os\n\ndef safe_path(base_dir: str, user_path: str) -> str:\n    \"\"\"Prevent path traversal attacks.\"\"\"\n    full_path = os.path.normpath(os.path.join(base_dir, user_path))\n    if not full_path.startswith(os.path.normpath(base_dir)):\n        raise ValueError(\"Invalid path: traversal detected\")\n    return full_path\n```\n\n### User Input\n```typescript\nimport { z } from 'zod';\n\nconst InputSchema = z.object({\n  name: z.string().min(1).max(100),\n  email: z.string().email(),\n  count: z.number().int().positive().max(1000),\n});\n\n// Validate before use\nconst result = InputSchema.safeParse(userInput);\nif (!result.success) {\n  return { error: result.error.format() };\n}\n```\n\n### SQL Queries\n```python\n# NEVER: String concatenation\nquery = f\"SELECT * FROM users WHERE id = {user_id}\"  # SQL injection!\n\n# ALWAYS: Parameterized queries\ncursor.execute(\"SELECT * FROM users WHERE id = ?\", (user_id,))\n```\n```\n\n### Output Escaping\n\n```markdown\n## Output Escaping\n\nPrevent XSS in generated content:\n\n```typescript\nfunction escapeHtml(text: string): string {\n  const map: Record<string, string> = {\n    '&': '&amp;',\n    '<': '&lt;',\n    '>': '&gt;',\n    '\"': '&quot;',\n    \"'\": '&#039;'\n  };\n  return text.replace(/[&<>\"']/g, m => map[m]);\n}\n\n// Use when inserting user content into HTML\nconst safeContent = escapeHtml(userInput);\n```\n```\n\n---\n\n## Dependency Documentation\n\n### Dependencies Section\n\n```markdown\n## Dependencies\n\n### Required\n- Python 3.10+ (for match statements)\n- Node.js 18+ (for fetch API)\n- TypeScript 5.0+ (for satisfies operator)\n\n### Optional\n- Redis 7+ (for caching, improves performance)\n- Docker (for containerized deployment)\n\n### External APIs\n| API | Purpose | Rate Limit | Auth |\n|-----|---------|------------|------|\n| OpenAI | Embeddings | 3000/min | API key |\n| GitHub | Repo access | 5000/hour | OAuth |\n\n### System Requirements\n- Memory: 512MB minimum, 2GB recommended\n- Disk: 100MB for installation\n- Network: Outbound HTTPS access\n```\n\n### Version Compatibility\n\n```markdown\n## Version Compatibility\n\n| Component | Minimum | Recommended | Notes |\n|-----------|---------|-------------|-------|\n| Python | 3.10 | 3.11+ | Match statements required |\n| Node.js | 18 | 20 LTS | Native fetch required |\n| npm | 9 | 10 | Workspaces support |\n```\n\n### Installation Notes\n\n```markdown\n## Installation\n\n### Prerequisites\n```bash\n# Verify versions\npython --version  # 3.10+\nnode --version    # 18+\n```\n\n### Setup\n```bash\n# Install dependencies\npip install -r requirements.txt\nnpm install\n\n# Configure environment\ncp .env.example .env\n# Edit .env with your values\n```\n\n### Verification\n```bash\n# Run tests to verify installation\npytest tests/\nnpm test\n```\n```\n\n---\n\n## Testing Guidance\n\n### Script Testing\n\n```markdown\n## Testing Scripts\n\nAll scripts in `scripts/` must be tested before inclusion.\n\n### Test Process\n1. Run with sample input\n2. Verify output matches expected\n3. Test error cases (invalid input, missing files)\n4. Check cleanup (no temp files left behind)\n\n### Test Documentation\n```bash\n# scripts/extract_text.py\n# Tested with:\n# - Single page PDF \n# - Multi-page PDF \n# - Scanned PDF (OCR) \n# - Encrypted PDF  Returns clear error \n# - Non-PDF file  Returns clear error \n```\n```\n\n### Edge Cases\n\n```markdown\n## Common Edge Cases\n\nAlways handle:\n\n| Category | Edge Cases |\n|----------|------------|\n| **Input** | Empty, null, undefined, very large |\n| **Files** | Missing, locked, wrong format, empty |\n| **Network** | Timeout, DNS failure, rate limited |\n| **Data** | Unicode, special chars, injection attempts |\n| **State** | Concurrent access, stale data, race conditions |\n```\n\n---\n\n## Performance Considerations\n\n### Timeout Protection\n\n```typescript\nasync function withTimeout<T>(\n  promise: Promise<T>,\n  ms: number,\n  message = 'Operation timed out'\n): Promise<T> {\n  const timeout = new Promise<never>((_, reject) =>\n    setTimeout(() => reject(new Error(message)), ms)\n  );\n  return Promise.race([promise, timeout]);\n}\n\n// Usage\nconst result = await withTimeout(fetchData(), 5000);\n```\n\n### Resource Limits\n\n```markdown\n## Resource Limits\n\n| Resource | Limit | Reason |\n|----------|-------|--------|\n| Request timeout | 25s | Platform limit |\n| File size | 10MB | Memory constraints |\n| Batch size | 100 items | API rate limits |\n| Concurrent requests | 5 | Prevent overload |\n```\n",
        "skills/resume_optimizer/.claude/skills/skill-creator-pro/references/workflows.md": "# Workflow Patterns\n\nSequential, conditional, and complex workflow patterns for skills.\n\n---\n\n## Sequential Workflows\n\nBreak complex tasks into clear, sequential steps with overview:\n\n```markdown\n## Workflow Overview\n\nProcessing a document involves:\n\n1. **Validate** - Check file format and permissions\n2. **Parse** - Extract content and structure\n3. **Transform** - Apply requested changes\n4. **Verify** - Confirm output matches expectations\n5. **Deliver** - Return result to user\n```\n\n### With Scripts\n\n```markdown\nFilling a PDF form involves these steps:\n\n1. Analyze the form (run `scripts/analyze_form.py`)\n2. Create field mapping (edit `fields.json`)\n3. Validate mapping (run `scripts/validate_fields.py`)\n4. Fill the form (run `scripts/fill_form.py`)\n5. Verify output (run `scripts/verify_output.py`)\n```\n\n### With Checkpoints\n\n```markdown\n## Data Migration Workflow\n\n### Phase 1: Preparation\n1. [ ] Backup existing data\n2. [ ] Verify backup integrity\n3. [ ] Document current schema\n\n### Phase 2: Migration\n4. [ ] Run migration script\n5. [ ] Verify row counts match\n6. [ ] Validate data integrity\n\n### Phase 3: Verification\n7. [ ] Run integration tests\n8. [ ] Compare sample records\n9. [ ] Sign off on migration\n```\n\n---\n\n## Conditional Workflows\n\nGuide through decision points with clear branching:\n\n### Simple Branch\n\n```markdown\n## Workflow\n\n1. Determine the operation type:\n   - **Creating new content?**  Follow \"Creation Workflow\" below\n   - **Editing existing content?**  Follow \"Editing Workflow\" below\n\n---\n\n## Creation Workflow\n1. Generate template\n2. Fill with content\n3. Validate structure\n4. Save output\n\n---\n\n## Editing Workflow\n1. Load existing file\n2. Parse current content\n3. Apply modifications\n4. Validate changes\n5. Save updated file\n```\n\n### Decision Tree\n\n```markdown\n## Authentication Flow\n\n1. Check authentication method:\n\n   **OAuth?**\n    Get authorization URL\n    Redirect user\n    Handle callback\n    Store tokens\n\n   **API Key?**\n    Validate key format\n    Check permissions\n    Store in secure config\n\n   **Username/Password?**\n    Validate credentials\n    Generate session token\n    Set secure cookie\n```\n\n### With Fallbacks\n\n```markdown\n## Content Retrieval\n\n1. Try primary source:\n   - **Success**  Return content, done\n   - **Failure**  Continue to step 2\n\n2. Try cache:\n   - **Hit**  Return cached, done\n   - **Miss**  Continue to step 3\n\n3. Try fallback source:\n   - **Success**  Return content, update cache\n   - **Failure**  Return error with guidance\n```\n\n---\n\n## Complex Workflows\n\n### Parallel Steps\n\n```markdown\n## Deployment Workflow\n\n### Sequential: Preparation\n1. Run tests\n2. Build artifacts\n\n### Parallel: Deployment (run simultaneously)\n- Deploy to Region A\n- Deploy to Region B\n- Deploy to Region C\n\n### Sequential: Verification\n3. Run smoke tests\n4. Enable traffic\n```\n\n### State Machine\n\n```markdown\n## Order Processing States\n\n```\n[Created]  [Validated]  [Processing]  [Completed]\n                            \n[Cancelled] [Rejected]    [Failed]\n```\n\n### Transitions\n\n| From | To | Trigger |\n|------|-----|---------|\n| Created | Validated | Validation passes |\n| Created | Cancelled | User cancels |\n| Validated | Processing | Payment confirmed |\n| Validated | Rejected | Validation fails |\n| Processing | Completed | Fulfillment done |\n| Processing | Failed | Error during fulfillment |\n```\n\n### Iterative Workflows\n\n```markdown\n## Refinement Workflow\n\n1. Generate initial output\n2. Evaluate against criteria\n3. If criteria not met:\n   - Identify gaps\n   - Apply improvements\n   - Return to step 2\n4. If criteria met:\n   - Finalize output\n   - Deliver result\n\n**Maximum iterations**: 3 (prevent infinite loops)\n```\n\n---\n\n## Workflow Documentation Best Practices\n\n### DO: Clear Entry/Exit Criteria\n\n```markdown\n## Workflow: Code Review\n\n### Entry Criteria\n- [ ] PR created with description\n- [ ] Tests passing\n- [ ] No merge conflicts\n\n### Steps\n1. Review code changes\n2. Check test coverage\n3. Verify documentation\n4. Provide feedback\n\n### Exit Criteria\n- [ ] All comments resolved\n- [ ] Approved by reviewer\n- [ ] CI/CD passing\n```\n\n### DO: Error Recovery\n\n```markdown\n## If Step Fails\n\n| Step | Failure | Recovery |\n|------|---------|----------|\n| 1. Parse | Invalid format | Show format requirements, ask for correct file |\n| 2. Validate | Missing fields | List missing fields, request completion |\n| 3. Process | Timeout | Retry once, then report with partial results |\n| 4. Save | Permission denied | Suggest alternative location |\n```\n\n### DON'T: Vague Steps\n\n```markdown\n## Bad Example\n\n1. Do the thing\n2. Check if it worked\n3. Fix problems\n4. Finish up\n```\n\n### DO: Specific Steps\n\n```markdown\n## Good Example\n\n1. Run `validate.py input.json` to check format\n2. Verify output shows \"Valid: true\"\n3. If errors shown, fix issues listed in output\n4. Run `process.py input.json output.json`\n```\n",
        "skills/resume_optimizer/.claude/skills/skill-creator/SKILL.md": "---\nname: skill-creator\ndescription: Guide for creating effective skills. This skill should be used when users want to create a new skill (or update an existing skill) that extends Claude's capabilities with specialized knowledge, workflows, or tool integrations.\nlicense: Complete terms in LICENSE.txt\n---\n\n# Skill Creator\n\nThis skill provides guidance for creating effective skills.\n\n## About Skills\n\nSkills are modular, self-contained packages that extend Claude's capabilities by providing\nspecialized knowledge, workflows, and tools. Think of them as \"onboarding guides\" for specific\ndomains or tasksthey transform Claude from a general-purpose agent into a specialized agent\nequipped with procedural knowledge that no model can fully possess.\n\n### What Skills Provide\n\n1. Specialized workflows - Multi-step procedures for specific domains\n2. Tool integrations - Instructions for working with specific file formats or APIs\n3. Domain expertise - Company-specific knowledge, schemas, business logic\n4. Bundled resources - Scripts, references, and assets for complex and repetitive tasks\n\n## Core Principles\n\n### Concise is Key\n\nThe context window is a public good. Skills share the context window with everything else Claude needs: system prompt, conversation history, other Skills' metadata, and the actual user request.\n\n**Default assumption: Claude is already very smart.** Only add context Claude doesn't already have. Challenge each piece of information: \"Does Claude really need this explanation?\" and \"Does this paragraph justify its token cost?\"\n\nPrefer concise examples over verbose explanations.\n\n### Set Appropriate Degrees of Freedom\n\nMatch the level of specificity to the task's fragility and variability:\n\n**High freedom (text-based instructions)**: Use when multiple approaches are valid, decisions depend on context, or heuristics guide the approach.\n\n**Medium freedom (pseudocode or scripts with parameters)**: Use when a preferred pattern exists, some variation is acceptable, or configuration affects behavior.\n\n**Low freedom (specific scripts, few parameters)**: Use when operations are fragile and error-prone, consistency is critical, or a specific sequence must be followed.\n\nThink of Claude as exploring a path: a narrow bridge with cliffs needs specific guardrails (low freedom), while an open field allows many routes (high freedom).\n\n### Anatomy of a Skill\n\nEvery skill consists of a required SKILL.md file and optional bundled resources:\n\n```\nskill-name/\n SKILL.md (required)\n    YAML frontmatter metadata (required)\n       name: (required)\n       description: (required)\n    Markdown instructions (required)\n Bundled Resources (optional)\n     scripts/          - Executable code (Python/Bash/etc.)\n     references/       - Documentation intended to be loaded into context as needed\n     assets/           - Files used in output (templates, icons, fonts, etc.)\n```\n\n#### SKILL.md (required)\n\nEvery SKILL.md consists of:\n\n- **Frontmatter** (YAML): Contains `name` and `description` fields. These are the only fields that Claude reads to determine when the skill gets used, thus it is very important to be clear and comprehensive in describing what the skill is, and when it should be used.\n- **Body** (Markdown): Instructions and guidance for using the skill. Only loaded AFTER the skill triggers (if at all).\n\n#### Bundled Resources (optional)\n\n##### Scripts (`scripts/`)\n\nExecutable code (Python/Bash/etc.) for tasks that require deterministic reliability or are repeatedly rewritten.\n\n- **When to include**: When the same code is being rewritten repeatedly or deterministic reliability is needed\n- **Example**: `scripts/rotate_pdf.py` for PDF rotation tasks\n- **Benefits**: Token efficient, deterministic, may be executed without loading into context\n- **Note**: Scripts may still need to be read by Claude for patching or environment-specific adjustments\n\n##### References (`references/`)\n\nDocumentation and reference material intended to be loaded as needed into context to inform Claude's process and thinking.\n\n- **When to include**: For documentation that Claude should reference while working\n- **Examples**: `references/finance.md` for financial schemas, `references/mnda.md` for company NDA template, `references/policies.md` for company policies, `references/api_docs.md` for API specifications\n- **Use cases**: Database schemas, API documentation, domain knowledge, company policies, detailed workflow guides\n- **Benefits**: Keeps SKILL.md lean, loaded only when Claude determines it's needed\n- **Best practice**: If files are large (>10k words), include grep search patterns in SKILL.md\n- **Avoid duplication**: Information should live in either SKILL.md or references files, not both. Prefer references files for detailed information unless it's truly core to the skillthis keeps SKILL.md lean while making information discoverable without hogging the context window. Keep only essential procedural instructions and workflow guidance in SKILL.md; move detailed reference material, schemas, and examples to references files.\n\n##### Assets (`assets/`)\n\nFiles not intended to be loaded into context, but rather used within the output Claude produces.\n\n- **When to include**: When the skill needs files that will be used in the final output\n- **Examples**: `assets/logo.png` for brand assets, `assets/slides.pptx` for PowerPoint templates, `assets/frontend-template/` for HTML/React boilerplate, `assets/font.ttf` for typography\n- **Use cases**: Templates, images, icons, boilerplate code, fonts, sample documents that get copied or modified\n- **Benefits**: Separates output resources from documentation, enables Claude to use files without loading them into context\n\n#### What to Not Include in a Skill\n\nA skill should only contain essential files that directly support its functionality. Do NOT create extraneous documentation or auxiliary files, including:\n\n- README.md\n- INSTALLATION_GUIDE.md\n- QUICK_REFERENCE.md\n- CHANGELOG.md\n- etc.\n\nThe skill should only contain the information needed for an AI agent to do the job at hand. It should not contain auxilary context about the process that went into creating it, setup and testing procedures, user-facing documentation, etc. Creating additional documentation files just adds clutter and confusion.\n\n### Progressive Disclosure Design Principle\n\nSkills use a three-level loading system to manage context efficiently:\n\n1. **Metadata (name + description)** - Always in context (~100 words)\n2. **SKILL.md body** - When skill triggers (<5k words)\n3. **Bundled resources** - As needed by Claude (Unlimited because scripts can be executed without reading into context window)\n\n#### Progressive Disclosure Patterns\n\nKeep SKILL.md body to the essentials and under 500 lines to minimize context bloat. Split content into separate files when approaching this limit. When splitting out content into other files, it is very important to reference them from SKILL.md and describe clearly when to read them, to ensure the reader of the skill knows they exist and when to use them.\n\n**Key principle:** When a skill supports multiple variations, frameworks, or options, keep only the core workflow and selection guidance in SKILL.md. Move variant-specific details (patterns, examples, configuration) into separate reference files.\n\n**Pattern 1: High-level guide with references**\n\n```markdown\n# PDF Processing\n\n## Quick start\n\nExtract text with pdfplumber:\n[code example]\n\n## Advanced features\n\n- **Form filling**: See [FORMS.md](FORMS.md) for complete guide\n- **API reference**: See [REFERENCE.md](REFERENCE.md) for all methods\n- **Examples**: See [EXAMPLES.md](EXAMPLES.md) for common patterns\n```\n\nClaude loads FORMS.md, REFERENCE.md, or EXAMPLES.md only when needed.\n\n**Pattern 2: Domain-specific organization**\n\nFor Skills with multiple domains, organize content by domain to avoid loading irrelevant context:\n\n```\nbigquery-skill/\n SKILL.md (overview and navigation)\n reference/\n     finance.md (revenue, billing metrics)\n     sales.md (opportunities, pipeline)\n     product.md (API usage, features)\n     marketing.md (campaigns, attribution)\n```\n\nWhen a user asks about sales metrics, Claude only reads sales.md.\n\nSimilarly, for skills supporting multiple frameworks or variants, organize by variant:\n\n```\ncloud-deploy/\n SKILL.md (workflow + provider selection)\n references/\n     aws.md (AWS deployment patterns)\n     gcp.md (GCP deployment patterns)\n     azure.md (Azure deployment patterns)\n```\n\nWhen the user chooses AWS, Claude only reads aws.md.\n\n**Pattern 3: Conditional details**\n\nShow basic content, link to advanced content:\n\n```markdown\n# DOCX Processing\n\n## Creating documents\n\nUse docx-js for new documents. See [DOCX-JS.md](DOCX-JS.md).\n\n## Editing documents\n\nFor simple edits, modify the XML directly.\n\n**For tracked changes**: See [REDLINING.md](REDLINING.md)\n**For OOXML details**: See [OOXML.md](OOXML.md)\n```\n\nClaude reads REDLINING.md or OOXML.md only when the user needs those features.\n\n**Important guidelines:**\n\n- **Avoid deeply nested references** - Keep references one level deep from SKILL.md. All reference files should link directly from SKILL.md.\n- **Structure longer reference files** - For files longer than 100 lines, include a table of contents at the top so Claude can see the full scope when previewing.\n\n## Skill Creation Process\n\nSkill creation involves these steps:\n\n1. Understand the skill with concrete examples\n2. Plan reusable skill contents (scripts, references, assets)\n3. Initialize the skill (run init_skill.py)\n4. Edit the skill (implement resources and write SKILL.md)\n5. Package the skill (run package_skill.py)\n6. Iterate based on real usage\n\nFollow these steps in order, skipping only if there is a clear reason why they are not applicable.\n\n### Step 1: Understanding the Skill with Concrete Examples\n\nSkip this step only when the skill's usage patterns are already clearly understood. It remains valuable even when working with an existing skill.\n\nTo create an effective skill, clearly understand concrete examples of how the skill will be used. This understanding can come from either direct user examples or generated examples that are validated with user feedback.\n\nFor example, when building an image-editor skill, relevant questions include:\n\n- \"What functionality should the image-editor skill support? Editing, rotating, anything else?\"\n- \"Can you give some examples of how this skill would be used?\"\n- \"I can imagine users asking for things like 'Remove the red-eye from this image' or 'Rotate this image'. Are there other ways you imagine this skill being used?\"\n- \"What would a user say that should trigger this skill?\"\n\nTo avoid overwhelming users, avoid asking too many questions in a single message. Start with the most important questions and follow up as needed for better effectiveness.\n\nConclude this step when there is a clear sense of the functionality the skill should support.\n\n### Step 2: Planning the Reusable Skill Contents\n\nTo turn concrete examples into an effective skill, analyze each example by:\n\n1. Considering how to execute on the example from scratch\n2. Identifying what scripts, references, and assets would be helpful when executing these workflows repeatedly\n\nExample: When building a `pdf-editor` skill to handle queries like \"Help me rotate this PDF,\" the analysis shows:\n\n1. Rotating a PDF requires re-writing the same code each time\n2. A `scripts/rotate_pdf.py` script would be helpful to store in the skill\n\nExample: When designing a `frontend-webapp-builder` skill for queries like \"Build me a todo app\" or \"Build me a dashboard to track my steps,\" the analysis shows:\n\n1. Writing a frontend webapp requires the same boilerplate HTML/React each time\n2. An `assets/hello-world/` template containing the boilerplate HTML/React project files would be helpful to store in the skill\n\nExample: When building a `big-query` skill to handle queries like \"How many users have logged in today?\" the analysis shows:\n\n1. Querying BigQuery requires re-discovering the table schemas and relationships each time\n2. A `references/schema.md` file documenting the table schemas would be helpful to store in the skill\n\nTo establish the skill's contents, analyze each concrete example to create a list of the reusable resources to include: scripts, references, and assets.\n\n### Step 3: Initializing the Skill\n\nAt this point, it is time to actually create the skill.\n\nSkip this step only if the skill being developed already exists, and iteration or packaging is needed. In this case, continue to the next step.\n\nWhen creating a new skill from scratch, always run the `init_skill.py` script. The script conveniently generates a new template skill directory that automatically includes everything a skill requires, making the skill creation process much more efficient and reliable.\n\nUsage:\n\n```bash\nscripts/init_skill.py <skill-name> --path <output-directory>\n```\n\nThe script:\n\n- Creates the skill directory at the specified path\n- Generates a SKILL.md template with proper frontmatter and TODO placeholders\n- Creates example resource directories: `scripts/`, `references/`, and `assets/`\n- Adds example files in each directory that can be customized or deleted\n\nAfter initialization, customize or remove the generated SKILL.md and example files as needed.\n\n### Step 4: Edit the Skill\n\nWhen editing the (newly-generated or existing) skill, remember that the skill is being created for another instance of Claude to use. Include information that would be beneficial and non-obvious to Claude. Consider what procedural knowledge, domain-specific details, or reusable assets would help another Claude instance execute these tasks more effectively.\n\n#### Learn Proven Design Patterns\n\nConsult these helpful guides based on your skill's needs:\n\n- **Multi-step processes**: See references/workflows.md for sequential workflows and conditional logic\n- **Specific output formats or quality standards**: See references/output-patterns.md for template and example patterns\n\nThese files contain established best practices for effective skill design.\n\n#### Start with Reusable Skill Contents\n\nTo begin implementation, start with the reusable resources identified above: `scripts/`, `references/`, and `assets/` files. Note that this step may require user input. For example, when implementing a `brand-guidelines` skill, the user may need to provide brand assets or templates to store in `assets/`, or documentation to store in `references/`.\n\nAdded scripts must be tested by actually running them to ensure there are no bugs and that the output matches what is expected. If there are many similar scripts, only a representative sample needs to be tested to ensure confidence that they all work while balancing time to completion.\n\nAny example files and directories not needed for the skill should be deleted. The initialization script creates example files in `scripts/`, `references/`, and `assets/` to demonstrate structure, but most skills won't need all of them.\n\n#### Update SKILL.md\n\n**Writing Guidelines:** Always use imperative/infinitive form.\n\n##### Frontmatter\n\nWrite the YAML frontmatter with `name` and `description`:\n\n- `name`: The skill name\n- `description`: This is the primary triggering mechanism for your skill, and helps Claude understand when to use the skill.\n  - Include both what the Skill does and specific triggers/contexts for when to use it.\n  - Include all \"when to use\" information here - Not in the body. The body is only loaded after triggering, so \"When to Use This Skill\" sections in the body are not helpful to Claude.\n  - Example description for a `docx` skill: \"Comprehensive document creation, editing, and analysis with support for tracked changes, comments, formatting preservation, and text extraction. Use when Claude needs to work with professional documents (.docx files) for: (1) Creating new documents, (2) Modifying or editing content, (3) Working with tracked changes, (4) Adding comments, or any other document tasks\"\n\nDo not include any other fields in YAML frontmatter.\n\n##### Body\n\nWrite instructions for using the skill and its bundled resources.\n\n### Step 5: Packaging a Skill\n\nOnce development of the skill is complete, it must be packaged into a distributable .skill file that gets shared with the user. The packaging process automatically validates the skill first to ensure it meets all requirements:\n\n```bash\nscripts/package_skill.py <path/to/skill-folder>\n```\n\nOptional output directory specification:\n\n```bash\nscripts/package_skill.py <path/to/skill-folder> ./dist\n```\n\nThe packaging script will:\n\n1. **Validate** the skill automatically, checking:\n\n   - YAML frontmatter format and required fields\n   - Skill naming conventions and directory structure\n   - Description completeness and quality\n   - File organization and resource references\n\n2. **Package** the skill if validation passes, creating a .skill file named after the skill (e.g., `my-skill.skill`) that includes all files and maintains the proper directory structure for distribution. The .skill file is a zip file with a .skill extension.\n\nIf validation fails, the script will report the errors and exit without creating a package. Fix any validation errors and run the packaging command again.\n\n### Step 6: Iterate\n\nAfter testing the skill, users may request improvements. Often this happens right after using the skill, with fresh context of how the skill performed.\n\n**Iteration workflow:**\n\n1. Use the skill on real tasks\n2. Notice struggles or inefficiencies\n3. Identify how SKILL.md or bundled resources should be updated\n4. Implement changes and test again\n",
        "skills/resume_optimizer/.claude/skills/skill-creator/references/output-patterns.md": "# Output Patterns\n\nUse these patterns when skills need to produce consistent, high-quality output.\n\n## Template Pattern\n\nProvide templates for output format. Match the level of strictness to your needs.\n\n**For strict requirements (like API responses or data formats):**\n\n```markdown\n## Report structure\n\nALWAYS use this exact template structure:\n\n# [Analysis Title]\n\n## Executive summary\n[One-paragraph overview of key findings]\n\n## Key findings\n- Finding 1 with supporting data\n- Finding 2 with supporting data\n- Finding 3 with supporting data\n\n## Recommendations\n1. Specific actionable recommendation\n2. Specific actionable recommendation\n```\n\n**For flexible guidance (when adaptation is useful):**\n\n```markdown\n## Report structure\n\nHere is a sensible default format, but use your best judgment:\n\n# [Analysis Title]\n\n## Executive summary\n[Overview]\n\n## Key findings\n[Adapt sections based on what you discover]\n\n## Recommendations\n[Tailor to the specific context]\n\nAdjust sections as needed for the specific analysis type.\n```\n\n## Examples Pattern\n\nFor skills where output quality depends on seeing examples, provide input/output pairs:\n\n```markdown\n## Commit message format\n\nGenerate commit messages following these examples:\n\n**Example 1:**\nInput: Added user authentication with JWT tokens\nOutput:\n```\nfeat(auth): implement JWT-based authentication\n\nAdd login endpoint and token validation middleware\n```\n\n**Example 2:**\nInput: Fixed bug where dates displayed incorrectly in reports\nOutput:\n```\nfix(reports): correct date formatting in timezone conversion\n\nUse UTC timestamps consistently across report generation\n```\n\nFollow this style: type(scope): brief description, then detailed explanation.\n```\n\nExamples help Claude understand the desired style and level of detail more clearly than descriptions alone.\n",
        "skills/resume_optimizer/.claude/skills/skill-creator/references/workflows.md": "# Workflow Patterns\n\n## Sequential Workflows\n\nFor complex tasks, break operations into clear, sequential steps. It is often helpful to give Claude an overview of the process towards the beginning of SKILL.md:\n\n```markdown\nFilling a PDF form involves these steps:\n\n1. Analyze the form (run analyze_form.py)\n2. Create field mapping (edit fields.json)\n3. Validate mapping (run validate_fields.py)\n4. Fill the form (run fill_form.py)\n5. Verify output (run verify_output.py)\n```\n\n## Conditional Workflows\n\nFor tasks with branching logic, guide Claude through decision points:\n\n```markdown\n1. Determine the modification type:\n   **Creating new content?**  Follow \"Creation workflow\" below\n   **Editing existing content?**  Follow \"Editing workflow\" below\n\n2. Creation workflow: [steps]\n3. Editing workflow: [steps]\n```",
        "skills/resume_optimizer/.claude/skills/skill-validator/SKILL.md": "---\nname: skill-validator\ndescription: |\n  Validates skills against production-level criteria with 9-category scoring.\n  This skill should be used when reviewing, auditing, or improving skills to\n  ensure quality standards. Evaluates structure, content, user interaction,\n  documentation, domain standards, technical robustness, maintainability,\n  zero-shot implementation, and reusability. Returns actionable validation\n  report with scores and improvement recommendations.\n---\n\n# Skill Validator\n\nValidate any skill against production-level quality criteria.\n\n## Validation Workflow\n\n### Phase 1: Gather Context\n\n1. **Read the skill's SKILL.md** completely\n2. **Identify skill type** from frontmatter description:\n   - Builder skill (creates artifacts)\n   - Guide skill (provides instructions)\n   - Automation skill (executes workflows)\n   - Analyzer skill (extracts insights)\n   - Validator skill (enforces quality)\n   - Hybrid skill (combination of above)\n3. **Read all reference files** in `references/` directory\n4. **Check for assets/scripts** directories\n5. **Note frontmatter fields** (`name`, `description`, `allowed-tools`, `model`)\n\n### Phase 2: Apply Criteria\n\nEvaluate against **9 criteria categories**. Each criterion scores 0-3:\n- **0**: Missing/Absent\n- **1**: Present but inadequate\n- **2**: Adequate implementation\n- **3**: Excellent implementation\n\n---\n\n## Criteria Categories\n\n### 1. Structure & Anatomy (Weight: 12%)\n\n| Criterion | What to Check |\n|-----------|---------------|\n| **SKILL.md exists** | Root file present |\n| **Line count** | <500 lines (context is precious) |\n| **Frontmatter complete** | `name` and `description` present in YAML |\n| **Name constraints** | Lowercase, numbers, hyphens only; 64 chars; matches directory |\n| **Description format** | [What] + [When] format; 1024 chars |\n| **Description style** | Third-person: \"This skill should be used when...\" |\n| **No extraneous files** | No README.md, CHANGELOG.md, LICENSE in skill dir |\n| **Progressive disclosure** | Details in `references/`, not bloated SKILL.md |\n| **Asset organization** | Templates in `assets/`, scripts in `scripts/` |\n| **Large file guidance** | If references >10k words, grep patterns in SKILL.md |\n\n**Fail condition**: Missing SKILL.md or >800 lines = automatic fail\n\n### 2. Content Quality (Weight: 15%)\n\n| Criterion | What to Check |\n|-----------|---------------|\n| **Conciseness** | No verbose explanations, context is public good |\n| **Imperative form** | Instructions use \"Do X\" not \"You should do X\" |\n| **Appropriate freedom** | Constraints where needed, flexibility where safe |\n| **Scope clarity** | Clear what skill does AND does not do |\n| **No hallucination risk** | No instructions that encourage making up info |\n| **Output specification** | Clear expected outputs defined |\n\n### 3. User Interaction (Weight: 12%)\n\n| Criterion | What to Check |\n|-----------|---------------|\n| **Clarification triggers** | Asks questions before acting on ambiguity |\n| **Required vs optional** | Distinguishes must-know from nice-to-know |\n| **Graceful handling** | What to do when user doesn't answer |\n| **No over-asking** | Doesn't ask obvious or inferrable questions |\n| **Question pacing** | Avoids too many questions in single message |\n| **Context awareness** | Uses available context before asking |\n\n**Key pattern to look for**:\n```markdown\n## Required Clarifications\n1. Question about X\n2. Question about Y\n\n## Optional Clarifications\n3. Question about Z (if relevant)\n\nNote: Avoid asking too many questions in a single message.\n```\n\n### 4. Documentation & References (Weight: 10%)\n\n| Criterion | What to Check |\n|-----------|---------------|\n| **Source URLs** | Official documentation links provided |\n| **Reference files** | Complex details in `references/` not main file |\n| **Fetch guidance** | Instructions to fetch docs for unlisted patterns |\n| **Version awareness** | Notes about checking for latest patterns |\n| **Example coverage** | Good/bad examples for key patterns |\n\n**Key pattern to look for**:\n```markdown\n| Resource | URL | Use For |\n|----------|-----|---------|\n| Official Docs | https://... | Complex cases |\n```\n\n### 5. Domain Standards (Weight: 10%)\n\n| Criterion | What to Check |\n|-----------|---------------|\n| **Best practices** | Follows domain conventions (e.g., WCAG, OWASP) |\n| **Enforcement mechanism** | Checklists, validation steps, must-verify items |\n| **Anti-patterns** | Lists what NOT to do |\n| **Quality gates** | Output checklist before delivery |\n\n**Key pattern to look for**:\n```markdown\n### Must Follow\n- [ ] Requirement 1\n- [ ] Requirement 2\n\n### Must Avoid\n- Antipattern 1\n- Antipattern 2\n```\n\n### 6. Technical Robustness (Weight: 8%)\n\n| Criterion | What to Check |\n|-----------|---------------|\n| **Error handling** | Guidance for failure scenarios |\n| **Security considerations** | Input validation, secrets handling if relevant |\n| **Dependencies** | External tools/APIs documented |\n| **Edge cases** | Common edge cases addressed |\n| **Testability** | Can outputs be verified? |\n\n### 7. Maintainability (Weight: 8%)\n\n| Criterion | What to Check |\n|-----------|---------------|\n| **Modularity** | References are self-contained topics |\n| **Update path** | Easy to update when standards change |\n| **No hardcoded values** | Uses placeholders/variables where appropriate |\n| **Clear organization** | Logical section ordering |\n\n### 8. Zero-Shot Implementation (Weight: 12%)\n\nSkills should enable single-interaction implementation with embedded expertise.\n\n| Criterion | What to Check |\n|-----------|---------------|\n| **Before Implementation section** | Context gathering guidance present |\n| **Codebase context** | Guidance to scan existing structure/patterns |\n| **Conversation context** | Uses discussed requirements/decisions |\n| **Embedded expertise** | Domain knowledge in `references/`, not runtime discovery |\n| **User-only questions** | Only asks for USER requirements, not domain knowledge |\n\n**Key pattern to look for**:\n```markdown\n## Before Implementation\n\nGather context to ensure successful implementation:\n\n| Source | Gather |\n|--------|--------|\n| **Codebase** | Existing structure, patterns, conventions |\n| **Conversation** | User's specific requirements |\n| **Skill References** | Domain patterns from `references/` |\n| **User Guidelines** | Project-specific conventions |\n```\n\n**Red flag**: Skill instructs to \"research\" or \"discover\" domain knowledge at runtime instead of embedding it.\n\n### 9. Reusability (Weight: 13%)\n\nSkills should handle variations, not single requirements.\n\n| Criterion | What to Check |\n|-----------|---------------|\n| **Handles variations** | Not hardcoded to single use case |\n| **Variable elements** | Clarifications capture what VARIES |\n| **Constant patterns** | Domain best practices encoded as constants |\n| **Not requirement-specific** | Avoids hardcoded data, tools, configs |\n| **Abstraction level** | Appropriate generalization for domain |\n\n**Good example**:\n```markdown\n\"Create visualizations - adaptable to data shape, chart type, library\"\n```\n\n**Bad example (too specific)**:\n```markdown\n\"Create bar chart with sales data using Recharts\"\n```\n\n**Key check**: Does the skill work for multiple use cases within its domain?\n\n---\n\n## Type-Specific Validation\n\nAfter scoring general criteria, verify type-specific requirements:\n\n| Type | Must Have |\n|------|-----------|\n| **Builder** | Clarifications, Output Spec, Domain Standards, Output Checklist |\n| **Guide** | Workflow Steps, Examples (Good/Bad), Official Docs links |\n| **Automation** | Scripts in `scripts/`, Dependencies, Error Handling, I/O Spec |\n| **Analyzer** | Analysis Scope, Evaluation Criteria, Output Format, Synthesis |\n| **Validator** | Quality Criteria, Scoring Rubric, Thresholds, Remediation |\n\n**Scoring**: Deduct 10 points if type-specific requirements missing for identified type.\n\n---\n\n## Scoring Guide\n\n### Category Scores\n\nCalculate each category score:\n```\nCategory Score = (Sum of criterion scores) / (Max possible) * 100\n```\n\n### Overall Score\n\n```\nOverall = (Category Score  Weight)\n```\n\n### Rating Thresholds\n\n| Score | Rating | Meaning |\n|-------|--------|---------|\n| 90-100 | **Production** | Ready for wide use |\n| 75-89 | **Good** | Minor improvements needed |\n| 60-74 | **Adequate** | Functional but needs work |\n| 40-59 | **Developing** | Significant gaps |\n| 0-39 | **Incomplete** | Major rework required |\n\n---\n\n## Output Format\n\nGenerate validation report:\n\n```markdown\n# Skill Validation Report: [skill-name]\n\n**Rating**: [Production/Good/Adequate/Developing/Incomplete]\n**Overall Score**: [X]/100\n\n## Summary\n[2-3 sentence assessment]\n\n## Category Scores\n\n| Category | Score | Weight | Weighted |\n|----------|-------|--------|----------|\n| Structure & Anatomy | X/100 | 12% | X |\n| Content Quality | X/100 | 15% | X |\n| User Interaction | X/100 | 12% | X |\n| Documentation | X/100 | 10% | X |\n| Domain Standards | X/100 | 10% | X |\n| Technical Robustness | X/100 | 8% | X |\n| Maintainability | X/100 | 8% | X |\n| Zero-Shot Implementation | X/100 | 12% | X |\n| Reusability | X/100 | 13% | X |\n| **Type-Specific Deduction** | -X | - | -X |\n\n## Critical Issues (if any)\n- [Issue requiring immediate fix]\n\n## Improvement Recommendations\n1. **High Priority**: [Specific action]\n2. **Medium Priority**: [Specific action]\n3. **Low Priority**: [Specific action]\n\n## Strengths\n- [What skill does well]\n```\n\n---\n\n## Quick Validation Checklist\n\nFor rapid assessment, check these critical items:\n\n### Structure & Frontmatter\n- [ ] SKILL.md <500 lines\n- [ ] Frontmatter: name (64 chars, lowercase, hyphens) + description (1024 chars)\n- [ ] Description uses third-person style (\"This skill should be used when...\")\n- [ ] No README.md/CHANGELOG.md in skill directory\n\n### Content & Interaction\n- [ ] Has clarification questions (Required vs Optional)\n- [ ] Has output specification\n- [ ] Has official documentation links\n\n### Zero-Shot & Reusability\n- [ ] Has \"Before Implementation\" section (context gathering)\n- [ ] Domain expertise embedded in `references/` (not runtime discovery)\n- [ ] Handles variations (not requirement-specific)\n\n### Type-Specific (check based on skill type)\n- [ ] Builder: Clarifications + Output Spec + Standards + Checklist\n- [ ] Guide: Workflow + Examples + Docs\n- [ ] Automation: Scripts + Dependencies + Error Handling\n- [ ] Analyzer: Scope + Criteria + Output Format\n- [ ] Validator: Criteria + Scoring + Thresholds + Remediation\n\n**If 10+ checked**: Likely Production (90+)\n**If 7-9 checked**: Likely Good (75-89)\n**If 5-6 checked**: Likely Adequate (60-74)\n**If <5 checked**: Needs significant work\n\n---\n\n## Reference Files\n\n| File | When to Read |\n|------|--------------|\n| `references/detailed-criteria.md` | Deep evaluation of specific criterion |\n| `references/scoring-examples.md` | Example validations for calibration |\n| `references/improvement-patterns.md` | Common fixes for common issues |\n\n---\n\n## Usage Examples\n\n### Validate a skill\n```\nValidate the chatgpt-widget-creator skill against production criteria\n```\n\n### Quick audit\n```\nQuick validation check on mcp-builder skill\n```\n\n### Focused review\n```\nCheck if skill-creator skill has proper user interaction patterns\n```\n",
        "skills/resume_optimizer/.claude/skills/skill-validator/references/detailed-criteria.md": "# Detailed Validation Criteria\n\nDeep evaluation guidance for each criterion.\n\n---\n\n## 1. Structure & Anatomy\n\n### 1.1 SKILL.md Exists\n- **Score 3**: Present at root of skill directory\n- **Score 0**: Missing = automatic fail\n\n### 1.2 Line Count\n- **Score 3**: <300 lines (lean and focused)\n- **Score 2**: 300-500 lines (acceptable)\n- **Score 1**: 500-800 lines (bloated, needs refactoring)\n- **Score 0**: >800 lines (automatic fail - split into references)\n\n**Why it matters**: Context window is a shared resource. Every line in SKILL.md competes with user's actual work.\n\n### 1.3 Frontmatter Quality\n\n**Score 3** - Complete, triggerable, proper format:\n```yaml\n---\nname: skill-name  # lowercase, hyphens, 64 chars, matches directory\ndescription: |    # 1024 chars, third-person style\n  This skill creates X and validates Y.\n  This skill should be used when users need to build widgets,\n  generate reports, or process data.\n---\n```\n\n**Score 2** - Present but format issues:\n```yaml\n---\nname: skill-name\ndescription: Creates X. Use when doing Y.  # Not third-person\n---\n```\n\n**Score 1** - Minimal:\n```yaml\n---\nname: skill-name\ndescription: X skill\n---\n```\n\n**Score 0**: Missing frontmatter\n\n### 1.4 Name Constraints\n\n**Score 3**: All constraints met\n- Lowercase letters only\n- Numbers allowed\n- Hyphens for separation\n- 64 characters\n- Matches directory name exactly\n\n**Score 2**: Minor violation (e.g., slightly over 64 chars)\n\n**Score 1**: Multiple violations (e.g., uppercase, underscores)\n\n**Score 0**: Name missing or completely invalid\n\n### 1.5 Description Format\n\n**Score 3** - [What] + [When] + Third-person:\n```yaml\ndescription: |\n  Validates skills against production criteria.\n  This skill should be used when reviewing, auditing, or\n  improving skills to ensure quality standards.\n```\n\n**Score 2** - Has content but wrong style:\n```yaml\ndescription: |\n  Validate skills against criteria.\n  Use when reviewing skills.  # Not third-person\n```\n\n**Score 1** - Vague:\n```yaml\ndescription: Helps with skill stuff\n```\n\n**Score 0**: Missing or single word\n\n### 1.6 No Extraneous Files\n\n**Should NOT exist in skill directory**:\n- README.md (SKILL.md IS the readme)\n- CHANGELOG.md (version in frontmatter if needed)\n- LICENSE (inherited from repo)\n- package.json (unless skill runs scripts)\n- .gitignore\n\n**Exception**: `assets/`, `references/`, `scripts/` directories are expected.\n\n### 1.7 Progressive Disclosure\n\n**Score 3**:\n- SKILL.md has overview + quick reference\n- Complex patterns in `references/`\n- Table of reference files with \"when to read\"\n\n**Score 2**:\n- Some content in references\n- SKILL.md still has most detail\n\n**Score 1**:\n- References exist but poorly organized\n\n**Score 0**:\n- Everything in SKILL.md or no references\n\n---\n\n## 2. Content Quality\n\n### 2.1 Conciseness\n\n**Score 3** - Every sentence earns its place:\n```markdown\n## Quick Start\n1. Define input schema\n2. Implement handler\n3. Register tool\n```\n\n**Score 1** - Verbose:\n```markdown\n## Quick Start\nIn this section, we will walk through the steps needed to get started with creating your tool. First, you will need to define an input schema. This schema specifies what parameters your tool accepts...\n```\n\n### 2.2 Imperative Form\n\n**Score 3**:\n```markdown\n- Validate all inputs with Zod\n- Return structured responses\n- Handle errors gracefully\n```\n\n**Score 1**:\n```markdown\n- You should validate all inputs with Zod\n- It would be good to return structured responses\n- Try to handle errors gracefully\n```\n\n### 2.3 Appropriate Freedom\n\n**Score 3** - Constraints where needed, flexibility elsewhere:\n```markdown\n## Required (No Exceptions)\n- Use TypeScript strict mode\n- Validate inputs at boundaries\n\n## Flexible (Your Choice)\n- Error message format\n- Logging verbosity\n```\n\n**Score 1** - Either too rigid or too loose\n\n### 2.4 Scope Clarity\n\n**Score 3**:\n```markdown\n## What This Skill Does\n- Creates X\n- Validates Y\n- Generates Z\n\n## What This Skill Does NOT Do\n- Production deployment\n- Performance optimization\n- Security auditing\n```\n\n**Score 1**: No explicit scope boundaries\n\n### 2.5 Output Specification\n\n**Score 3**:\n```markdown\n## Output Format\nEvery generated [artifact] includes:\n- [ ] Component A with X property\n- [ ] Component B following Y pattern\n- [ ] Component C validated against Z\n```\n\n**Score 0**: No clear output definition\n\n---\n\n## 3. User Interaction\n\n### 3.1 Clarification Triggers\n\n**Score 3** - Structured clarification section:\n```markdown\n## Required Clarifications\nBefore proceeding, ask:\n1. **Data shape**: \"What structure will the input have?\"\n2. **Action type**: \"Read-only or write?\"\n\n## Optional Clarifications\n3. **Styling**: \"Any design preferences?\" (if relevant)\n```\n\n**Score 2**: Questions mentioned but unstructured\n\n**Score 1**: \"Ask questions if needed\" without specifics\n\n**Score 0**: No clarification guidance\n\n### 3.2 Required vs Optional\n\n**Score 3**: Clearly labeled required vs optional questions\n**Score 1**: All questions treated equally\n\n### 3.3 Context Awareness\n\n**Score 3**:\n```markdown\nBefore asking:\n- Check conversation history for prior answers\n- Infer from file names/content when possible\n- Only ask what cannot be determined\n```\n\n**Score 0**: No guidance on using existing context\n\n---\n\n## 4. Documentation & References\n\n### 4.1 Source URLs\n\n**Score 3** - Table with official sources:\n```markdown\n| Resource | URL | Use For |\n|----------|-----|---------|\n| Official Guide | https://... | Core patterns |\n| API Reference | https://... | Method details |\n```\n\n**Score 2**: URLs mentioned but scattered\n\n**Score 1**: One or two links buried in text\n\n**Score 0**: No external references\n\n### 4.2 Fetch Guidance\n\n**Score 3**:\n```markdown\nFor patterns not covered here, fetch from official docs:\n- Complex widget: https://...\n- Advanced auth: https://...\n```\n\n**Score 0**: No guidance for unlisted scenarios\n\n### 4.3 Example Coverage\n\n**Score 3**:\n```markdown\n### Good Example\n[working code]\n\n### Bad Example (Don't Do This)\n[antipattern with explanation why]\n```\n\n**Score 1**: Only good examples, no antipatterns\n\n---\n\n## 5. Domain Standards\n\n### 5.1 Best Practices Enforcement\n\n**Score 3** - Mandatory checklist:\n```markdown\n### Must Follow\n- [ ] WCAG AA contrast (4.5:1)\n- [ ] Keyboard navigation\n- [ ] Error states\n\n### Must Avoid\n- Nested scrolling\n- Hardcoded strings\n```\n\n**Score 1**: Mentions best practices without enforcement\n\n### 5.2 Quality Gates\n\n**Score 3** - Output checklist before delivery:\n```markdown\n## Output Checklist\nBefore delivering, verify:\n- [ ] All required components present\n- [ ] Follows naming convention\n- [ ] No hardcoded values\n- [ ] Tested against criteria\n```\n\n**Score 0**: No quality gate\n\n---\n\n## 6. Technical Robustness\n\n### 6.1 Error Handling\n\n**Score 3**:\n```markdown\n## Error Handling\n- Invalid input: Return validation error with specifics\n- Network failure: Retry with backoff, then graceful fallback\n- Unknown error: Log context, return safe default\n```\n\n**Score 0**: No error handling guidance\n\n### 6.2 Security Considerations\n\n**Score 3** (when relevant):\n```markdown\n## Security\n- Never hardcode secrets\n- Validate/sanitize all user input\n- Use parameterized queries\n```\n\n**Score 2**: Mentions security generally\n**Score 0**: No security guidance (when relevant)\n\n### 6.3 Dependencies\n\n**Score 3**:\n```markdown\n## Dependencies\n- Requires: Node.js 18+, TypeScript 5+\n- Optional: Redis for caching\n- External APIs: None / List with rate limits\n```\n\n**Score 0**: Dependencies not documented\n\n---\n\n## 7. Maintainability\n\n### 7.1 Modularity\n\n**Score 3** - Self-contained reference files:\n```\nreferences/\n core-patterns.md      # Can be read independently\n advanced-usage.md     # Self-contained advanced topic\n troubleshooting.md    # Standalone help\n```\n\n**Score 1**: References that require reading other files first\n\n### 7.2 Update Path\n\n**Score 3**:\n```markdown\n## Keeping Current\n- Fetch latest from [URL] for new patterns\n- Check [changelog URL] for breaking changes\n- Last verified: 2024-12\n```\n\n**Score 0**: No update guidance\n\n### 7.3 Clear Organization\n\n**Score 3** - Logical flow:\n1. What (overview)\n2. When (triggers)\n3. How (workflow)\n4. Reference (details)\n5. Examples\n\n**Score 1**: Scattered organization, hard to navigate\n\n---\n\n## 8. Zero-Shot Implementation\n\nSkills should enable single-interaction implementation with embedded expertise.\n\n### 8.1 Before Implementation Section\n\n**Score 3** - Complete context gathering:\n```markdown\n## Before Implementation\n\nGather context to ensure successful implementation:\n\n| Source | Gather |\n|--------|--------|\n| **Codebase** | Existing structure, patterns, conventions |\n| **Conversation** | User's specific requirements |\n| **Skill References** | Domain patterns from `references/` |\n| **User Guidelines** | Project-specific conventions |\n\nEnsure all required context is gathered before implementing.\n```\n\n**Score 2**: Mentions gathering context but incomplete sources\n\n**Score 1**: Brief \"check context first\" note\n\n**Score 0**: No context gathering guidance\n\n### 8.2 Embedded Expertise\n\n**Score 3** - Domain knowledge IN the skill:\n- Best practices documented in `references/`\n- Code examples included\n- Anti-patterns listed\n- Library/API documentation embedded\n\n**Score 2**: Some expertise embedded, some requires runtime discovery\n\n**Score 1**: Mostly relies on runtime discovery (\"search for...\" instructions)\n\n**Score 0**: No embedded domain expertise\n\n**Red flags**:\n- \"Research the domain...\"\n- \"Fetch documentation for...\"\n- \"Discover best practices...\"\n\nThese indicate runtime discovery instead of embedded expertise.\n\n### 8.3 User-Only Questions\n\n**Score 3** - Only asks about USER's context:\n```markdown\n| Ask | Don't Ask |\n|-----|-----------|\n| \"What's YOUR use case?\" | \"What is [technology]?\" |\n| \"What's YOUR tech stack?\" | \"What options exist?\" |\n| \"Specific constraints?\" | \"What are best practices?\" |\n```\n\n**Score 2**: Mostly user questions, some domain questions\n\n**Score 1**: Mixes user and domain questions\n\n**Score 0**: Asks user for domain knowledge skill should contain\n\n---\n\n## 9. Reusability\n\nSkills should handle variations, not single requirements.\n\n### 9.1 Handles Variations\n\n**Score 3** - Explicitly adaptable:\n```markdown\nThis skill adapts to:\n- Different data shapes\n- Multiple output formats\n- Various libraries/frameworks\n```\n\n**Score 2**: Some variability but limited\n\n**Score 1**: Works for narrow range of cases\n\n**Score 0**: Hardcoded to single requirement\n\n### 9.2 Variable vs Constant Analysis\n\n**Score 3** - Clear separation:\n```markdown\n## What VARIES (ask user)\n- Data shape/structure\n- Tool/library preference\n- Output format\n\n## What's CONSTANT (encoded in skill)\n- Best practices\n- Error handling patterns\n- Security requirements\n```\n\n**Score 2**: Implicit separation\n\n**Score 1**: Mixed without clear distinction\n\n**Score 0**: No consideration of variability\n\n### 9.3 Not Requirement-Specific\n\n**Score 3** - Generic within domain:\n```markdown\n# Good: \"Create visualizations\"\nAdapts to chart type, library, data shape\n\n# Good: \"Deploy applications\"\nAdapts to platform, orchestration, environment\n```\n\n**Score 0** - Too specific:\n```markdown\n# Bad: \"Create bar chart with sales data using Recharts\"\n# Bad: \"Deploy to AWS EKS with Helm\"\n```\n\n### 9.4 Abstraction Level\n\n| Level | Description | Score |\n|-------|-------------|-------|\n| Domain-agnostic | Works across domains | 3 |\n| Domain-specific, tool-agnostic | Within domain, any tool | 3 |\n| Tool-specific, workflow-agnostic | Specific tool, flexible workflow | 2 |\n| Requirement-specific | Single use case | 0 |\n\n---\n\n## 10. Type-Specific Criteria\n\n### 10.1 Builder Skills\n\nMust have ALL of:\n- [ ] Required Clarifications section\n- [ ] Output Specification\n- [ ] Domain Standards (Must Follow / Must Avoid)\n- [ ] Output Checklist\n\n**Score 3**: All present and detailed\n**Score 2**: All present, some minimal\n**Score 1**: Missing 1-2 elements\n**Score 0**: Missing 3+ elements\n\n### 10.2 Guide Skills\n\nMust have ALL of:\n- [ ] Workflow Steps (numbered, sequential)\n- [ ] Good/Bad Examples\n- [ ] Official Documentation links\n\n**Score 3**: All present with clear examples\n**Score 2**: All present, examples minimal\n**Score 1**: Missing 1 element\n**Score 0**: Missing 2+ elements\n\n### 10.3 Automation Skills\n\nMust have ALL of:\n- [ ] Scripts in `scripts/` directory\n- [ ] Dependencies documented\n- [ ] Error Handling guidance\n- [ ] Input/Output Specification\n\n**Score 3**: All present, scripts tested\n**Score 2**: All present, minimal documentation\n**Score 1**: Missing 1-2 elements\n**Score 0**: Missing 3+ elements or no scripts\n\n### 10.4 Analyzer Skills\n\nMust have ALL of:\n- [ ] Analysis Scope (what to analyze, what to ignore)\n- [ ] Evaluation Criteria\n- [ ] Output Format specification\n- [ ] Synthesis guidance\n\n**Score 3**: All present with clear criteria\n**Score 2**: All present, criteria vague\n**Score 1**: Missing 1-2 elements\n**Score 0**: Missing 3+ elements\n\n### 10.5 Validator Skills\n\nMust have ALL of:\n- [ ] Quality Criteria with weights\n- [ ] Scoring Rubric (0-3 or similar)\n- [ ] Pass/Fail Thresholds\n- [ ] Remediation guidance\n\n**Score 3**: All present with clear thresholds\n**Score 2**: All present, thresholds vague\n**Score 1**: Missing 1-2 elements\n**Score 0**: Missing 3+ elements\n",
        "skills/resume_optimizer/.claude/skills/skill-validator/references/improvement-patterns.md": "# Improvement Patterns\n\nCommon fixes for common skill validation issues.\n\n---\n\n## Structure Issues\n\n### Problem: SKILL.md Too Long (>500 lines)\n\n**Fix**: Extract to references\n\n```\nBefore:\nSKILL.md (800 lines with everything)\n\nAfter:\nSKILL.md (200 lines - overview + quick ref)\nreferences/\n detailed-patterns.md (300 lines)\n advanced-usage.md (200 lines)\n troubleshooting.md (100 lines)\n```\n\n**Add reference table**:\n```markdown\n## Reference Files\n\n| File | When to Read |\n|------|--------------|\n| `references/detailed-patterns.md` | Complex implementations |\n| `references/advanced-usage.md` | Non-standard scenarios |\n```\n\n### Problem: README.md Exists\n\n**Fix**: Delete it. SKILL.md IS the readme.\n\n### Problem: No Progressive Disclosure\n\n**Fix**: Create references/ directory with topical files:\n1. Identify independent topics in SKILL.md\n2. Extract each to `references/[topic].md`\n3. Add \"when to read\" guidance in SKILL.md\n\n---\n\n## Content Issues\n\n### Problem: Verbose Content\n\n**Before**:\n```markdown\nIn this section, we will walk through the process of creating a new widget.\nFirst, you will want to understand what data your widget needs to display.\nThis is important because the data shape determines how you structure your component.\n```\n\n**After**:\n```markdown\n## Creating a Widget\n\n1. Define data shape (what `toolOutput` contains)\n2. Choose display mode (inline/fullscreen/pip)\n3. Implement with theme support\n```\n\n### Problem: \"You should\" Instead of Imperatives\n\n**Before**:\n```markdown\n- You should validate all inputs\n- It would be good to handle errors\n- Try to follow the patterns\n```\n\n**After**:\n```markdown\n- Validate all inputs with Zod\n- Handle errors with try/catch\n- Follow these patterns:\n```\n\n### Problem: No Scope Boundaries\n\n**Fix**: Add explicit sections:\n\n```markdown\n## What This Skill Does\n- Creates X\n- Validates Y\n- Generates Z\n\n## What This Skill Does NOT Do\n- Deploy to production\n- Optimize performance\n- Handle authentication\n```\n\n---\n\n## User Interaction Issues\n\n### Problem: No Clarification Questions\n\n**Fix**: Add structured clarification section:\n\n```markdown\n## Required Clarifications\n\nBefore building, ask:\n\n1. **Data shape**: \"What will `toolOutput` contain?\"\n   ```json\n   Example: { items: [...], total: 10 }\n   ```\n\n2. **Action type**: \"Display only or interactive?\"\n   - Display  No callTool needed\n   - Interactive  Need tool name and params\n\n3. **Display mode**: \"Inline, fullscreen, or pip?\"\n\n## Optional Clarifications\n\n4. **Styling**: \"Any design preferences?\" (ask if complex UI)\n```\n\n### Problem: All Questions Treated Equally\n\n**Fix**: Separate required from optional:\n\n```markdown\n## Required (Must Ask)\n1. Core question A\n2. Core question B\n\n## Optional (Ask if Relevant)\n3. Nice-to-know question\n4. Edge case question\n```\n\n### Problem: No Context Awareness\n\n**Fix**: Add context-checking guidance:\n\n```markdown\n## Before Asking\n\nCheck existing context:\n- Review conversation history for prior answers\n- Infer from file names when possible\n- Check toolOutput structure if available\n\nOnly ask what cannot be determined.\n```\n\n---\n\n## Documentation Issues\n\n### Problem: No Official Source Links\n\n**Fix**: Add documentation table:\n\n```markdown\n## Official Documentation\n\n| Resource | URL | Use For |\n|----------|-----|---------|\n| Getting Started | https://... | Basic setup |\n| API Reference | https://... | Method details |\n| Best Practices | https://... | Pattern guidance |\n\nFor patterns not covered here, fetch from official docs.\n```\n\n### Problem: No Fetch Guidance\n\n**Fix**: Add on-the-go learning section:\n\n```markdown\n## Unlisted Scenarios\n\nFor patterns not documented here:\n\n1. Fetch from official docs: [URL]\n2. Apply same validation criteria\n3. Follow established patterns in this skill\n\nExamples of when to fetch:\n- Complex authentication flows\n- Third-party integrations\n- Platform-specific features\n```\n\n### Problem: No Examples\n\n**Fix**: Add good/bad examples:\n\n```markdown\n### Good Example\n```typescript\n// Correct pattern with explanation\nconst data = window.openai?.toolOutput ?? defaultValue;\n```\n\n### Bad Example (Don't Do This)\n```typescript\n// Why this is wrong\nconst data = window.openai.toolOutput; // Crashes if undefined\n```\n```\n\n---\n\n## Domain Standards Issues\n\n### Problem: Best Practices Mentioned But Not Enforced\n\n**Before**:\n```markdown\nFollow accessibility best practices.\n```\n\n**After**:\n```markdown\n## Accessibility Requirements\n\n### Must Follow\n- [ ] WCAG AA contrast (4.5:1 for text)\n- [ ] Keyboard navigation for all interactions\n- [ ] Focus indicators visible\n- [ ] Screen reader labels for icons\n\n### Must Avoid\n- Color as only indicator\n- Mouse-only interactions\n- Auto-playing media without controls\n```\n\n### Problem: No Output Checklist\n\n**Fix**: Add quality gate:\n\n```markdown\n## Output Checklist\n\nBefore delivering, verify ALL items:\n\n### Functional\n- [ ] Core feature works\n- [ ] Error states handled\n- [ ] Loading states present\n\n### Quality\n- [ ] Follows naming conventions\n- [ ] No hardcoded values\n- [ ] Comments where non-obvious\n\n### Standards\n- [ ] Passes domain requirements (above)\n- [ ] Tested against criteria\n```\n\n---\n\n## Technical Issues\n\n### Problem: No Error Handling Guidance\n\n**Fix**: Add error handling section:\n\n```markdown\n## Error Handling\n\n| Scenario | Action |\n|----------|--------|\n| Invalid input | Return validation error with specifics |\n| Network failure | Retry 3x with backoff, then fallback |\n| Unknown error | Log context, return safe default |\n\n### Error Response Format\n```typescript\nreturn {\n  isError: true,\n  content: [{ type: 'text', text: 'User-friendly message' }],\n  _meta: { errorCode: 'VALIDATION_FAILED', details: {...} }\n};\n```\n```\n\n### Problem: No Security Guidance\n\n**Fix**: Add security section (when relevant):\n\n```markdown\n## Security Considerations\n\n- **Never hardcode**: Secrets, API keys, tokens\n- **Always validate**: User input, file paths, URLs\n- **Escape output**: Prevent XSS in generated HTML\n- **Use parameterized**: Queries to prevent injection\n```\n\n### Problem: Dependencies Not Listed\n\n**Fix**: Add dependencies section:\n\n```markdown\n## Dependencies\n\n### Required\n- Node.js 18+\n- TypeScript 5.0+\n\n### Optional\n- Redis (for caching)\n\n### External APIs\n- OpenAI Apps SDK (via window.openai)\n- No rate limits apply to widget\n```\n\n---\n\n## Maintainability Issues\n\n### Problem: Monolithic SKILL.md\n\n**Fix**: Modularize into references:\n\n1. Identify 3-5 independent topics\n2. Create `references/[topic].md` for each\n3. Keep SKILL.md as entry point with \"when to read\" table\n\n### Problem: No Update Guidance\n\n**Fix**: Add versioning section:\n\n```markdown\n## Keeping Current\n\n- Official docs: [URL]\n- Changelog: [URL]\n- Last verified: 2024-12\n\nWhen official docs update:\n1. Check for breaking changes\n2. Update affected references\n3. Test against validation criteria\n```\n\n### Problem: Hardcoded Values\n\n**Before**:\n```markdown\nSet timeout to 5000ms.\nUse port 3000.\n```\n\n**After**:\n```markdown\nSet timeout (default: 5000ms, adjust for your use case).\nUse configured port (default: 3000).\n```\n\n---\n\n## Zero-Shot Implementation Issues\n\n### Problem: Runtime Discovery Instead of Embedded Expertise\n\n**Before (Bad)**:\n```markdown\n## Implementation\n1. Research the domain best practices\n2. Fetch library documentation\n3. Discover common patterns\n4. Implement based on findings\n```\n\n**After (Good)**:\n```markdown\n## Before Implementation\n\nGather context to ensure successful implementation:\n\n| Source | Gather |\n|--------|--------|\n| **Codebase** | Existing structure, patterns, conventions |\n| **Conversation** | User's specific requirements |\n| **Skill References** | Domain patterns from `references/` |\n| **User Guidelines** | Project-specific conventions |\n\n## Implementation\n1. Apply patterns from `references/best-practices.md`\n2. Follow examples in `references/examples.md`\n3. Avoid anti-patterns in `references/anti-patterns.md`\n```\n\n### Problem: Asking User for Domain Knowledge\n\n**Before (Bad)**:\n```markdown\n## Clarifications\n1. What is Kafka and how does it work?\n2. What are Kafka best practices?\n3. What libraries are available?\n```\n\n**After (Good)**:\n```markdown\n## Required Clarifications\n\nAsk about USER's context (domain expertise is in this skill):\n\n1. **Use case**: \"What's YOUR specific use case?\"\n2. **Tech stack**: \"What's YOUR environment?\"\n3. **Constraints**: \"Any specific requirements?\"\n\nNote: Domain best practices are embedded in `references/`.\n```\n\n### Problem: No Context Gathering Guidance\n\n**Fix**: Add Before Implementation section:\n\n```markdown\n## Before Implementation\n\nGather context from available sources:\n1. **Codebase** (if exists): Scan structure, patterns, conventions\n2. **Conversation**: Review discussed requirements and decisions\n3. **Skill References**: Apply embedded domain expertise\n4. **User Guidelines**: Follow project-specific conventions\n\nAdapt approach based on available context.\nOnly ask user for what cannot be discovered.\n```\n\n---\n\n## Reusability Issues\n\n### Problem: Hardcoded to Single Requirement\n\n**Before (Bad)**:\n```markdown\n# Sales Dashboard Creator\n\nCreates a bar chart showing monthly sales using Recharts.\n\n## Output\n- Bar chart component\n- Uses blue color scheme\n- Shows last 12 months\n```\n\n**After (Good)**:\n```markdown\n# Data Visualization Creator\n\nCreates visualizations adaptable to data shape, chart type, and library.\n\n## Required Clarifications\n1. **Data shape**: \"What structure will input have?\"\n2. **Chart type**: \"Bar, line, pie, or other?\"\n3. **Library**: \"Recharts, D3, Chart.js, or no preference?\"\n\n## Output\nAdapts based on clarifications.\n```\n\n### Problem: No Variability Analysis\n\n**Fix**: Add Varies vs Constant section:\n\n```markdown\n## Variability Analysis\n\n| What VARIES (ask user) | What's CONSTANT (encoded) |\n|------------------------|---------------------------|\n| Data shape/structure | Accessibility requirements |\n| Chart type preference | Responsive patterns |\n| Library choice | Error handling |\n| Color scheme | Loading states |\n```\n\n### Problem: Too Specific Abstraction Level\n\n**Before (Bad)**:\n```markdown\n# AWS EKS Helm Deployment\n\nDeploys to AWS EKS using Helm charts.\n```\n\n**After (Good)**:\n```markdown\n# Application Deployment\n\nDeploys applications with adaptable platform, orchestration, and configuration.\n\n## Required Clarifications\n1. **Platform**: \"AWS, GCP, Azure, or other?\"\n2. **Orchestration**: \"Kubernetes, ECS, serverless?\"\n3. **Configuration**: \"Helm, Terraform, CDK?\"\n```\n\n---\n\n## Frontmatter Issues\n\n### Problem: Wrong Description Style\n\n**Before (Bad)**:\n```yaml\ndescription: |\n  Create widgets for apps.\n  Use when building UI components.\n```\n\n**After (Good)**:\n```yaml\ndescription: |\n  Creates production widgets for applications.\n  This skill should be used when users want to build\n  UI components, visual interfaces, or interactive elements.\n```\n\n### Problem: Name Constraint Violations\n\n**Before (Bad)**:\n```yaml\nname: Widget_Creator    # Uppercase, underscores\nname: my-super-long-skill-name-that-exceeds-the-sixty-four-character-limit  # Too long\n```\n\n**After (Good)**:\n```yaml\nname: widget-creator    # Lowercase, hyphens, 64 chars\n```\n\n### Problem: Description Too Long or Missing Format\n\n**Fix**: Use [What] + [When] format, 1024 chars:\n\n```yaml\ndescription: |\n  [What] Creates X, validates Y, generates Z.\n  [When] This skill should be used when users need to\n  build widgets, review code, or process data.\n```\n\n---\n\n## Type-Specific Issues\n\n### Problem: Builder Skill Missing Required Sections\n\n**Fix**: Add all required sections:\n\n```markdown\n## Required Clarifications\n1. Question about input\n2. Question about output\n\n## Output Specification\n[Define what artifact looks like]\n\n## Domain Standards\n### Must Follow\n- [ ] Requirement 1\n### Must Avoid\n- Anti-pattern 1\n\n## Output Checklist\n- [ ] Meets requirements\n- [ ] Follows standards\n```\n\n### Problem: Analyzer Skill Missing Scope\n\n**Fix**: Add analysis scope:\n\n```markdown\n## Analysis Scope\n\n### What to Analyze\n- Code structure\n- Naming conventions\n- Error handling\n\n### What to Ignore\n- Comments and formatting\n- Third-party libraries\n- Generated code\n\n## Evaluation Criteria\n| Criterion | Weight | How to Assess |\n|-----------|--------|---------------|\n| Readability | 30% | Naming, structure |\n| Robustness | 40% | Error handling |\n| Performance | 30% | Complexity |\n```\n\n### Problem: Validator Skill Missing Thresholds\n\n**Fix**: Add clear thresholds:\n\n```markdown\n## Scoring Rubric\n- **3**: Excellent\n- **2**: Adequate\n- **1**: Needs improvement\n- **0**: Missing/Fail\n\n## Thresholds\n| Score | Rating | Action |\n|-------|--------|--------|\n| 90-100 | Pass | Ready for use |\n| 70-89 | Conditional | Minor fixes needed |\n| <70 | Fail | Major rework required |\n\n## Remediation\n| Issue | Fix |\n|-------|-----|\n| Missing X | Add X section |\n| Weak Y | Strengthen with examples |\n```\n\n---\n\n## Quick Improvement Checklist\n\nWhen improving a skill, address in this order:\n\n1. **Critical** (blocks usage):\n   - [ ] SKILL.md exists and <500 lines\n   - [ ] Frontmatter: name (64, lowercase) + description (1024, third-person)\n   - [ ] Core workflow documented\n\n2. **High Priority** (major quality):\n   - [ ] Before Implementation section (context gathering)\n   - [ ] Domain expertise embedded in `references/`\n   - [ ] Clarification questions (for builder skills)\n   - [ ] Output specification\n\n3. **Medium Priority** (polish):\n   - [ ] Handles variations (not requirement-specific)\n   - [ ] Progressive disclosure to references\n   - [ ] Error handling guidance\n   - [ ] Good/bad examples\n\n4. **Low Priority** (excellence):\n   - [ ] Type-specific sections complete\n   - [ ] Update path documented\n   - [ ] All edge cases covered\n   - [ ] Templates in assets/\n",
        "skills/resume_optimizer/.claude/skills/skill-validator/references/scoring-examples.md": "# Scoring Examples\n\nCalibration examples showing how to score skills consistently.\n\n---\n\n## Example 1: Production-Level Skill\n\n**Skill**: chatgpt-widget-creator\n**Type**: Builder\n\n### Category Scores\n\n| Category | Score | Weight | Weighted |\n|----------|-------|--------|----------|\n| Structure & Anatomy | 95/100 | 15% | 14.25 |\n| Content Quality | 90/100 | 20% | 18.00 |\n| User Interaction | 95/100 | 15% | 14.25 |\n| Documentation | 100/100 | 15% | 15.00 |\n| Domain Standards | 90/100 | 15% | 13.50 |\n| Technical Robustness | 75/100 | 10% | 7.50 |\n| Maintainability | 85/100 | 10% | 8.50 |\n| **Total** | | | **91.00** |\n\n**Rating**: Production\n\n### Breakdown\n\n**Structure & Anatomy (95/100)**:\n- SKILL.md exists: 3\n- Line count (262): 3\n- Frontmatter: 3 (complete with triggers)\n- No extraneous files: 3\n- Progressive disclosure: 3 (6 reference files)\n- Asset organization: 3 (templates in assets/)\n\n**Content Quality (90/100)**:\n- Conciseness: 3 (lean, table-driven)\n- Imperative form: 3\n- Appropriate freedom: 3 (Required + Optional)\n- Scope clarity: 2 (could add \"What this does NOT do\")\n- Output specification: 3 (full checklist)\n\n**User Interaction (95/100)**:\n- Clarification triggers: 3 (\"STOP: Clarify Before Building\")\n- Required vs optional: 3 (clearly separated)\n- Graceful handling: 2 (missing \"if user declines\")\n- Context awareness: 3\n\n**Documentation (100/100)**:\n- Source URLs: 3 (8 official links in table)\n- Reference files: 3 (6 topical files)\n- Fetch guidance: 3 (\"fetch from docs\" instruction)\n- Version awareness: 3\n- Example coverage: 3 (templates provided)\n\n**Domain Standards (90/100)**:\n- Best practices: 3 (UX Principles section)\n- Enforcement mechanism: 3 (checkbox checklist)\n- Anti-patterns: 3 (\"Must Avoid\" section)\n- Quality gates: 2 (good but could be stricter)\n\n**Technical Robustness (75/100)**:\n- Error handling: 2 (states in checklist)\n- Security: 2 (CSP mentioned, basic)\n- Dependencies: 2 (implicit)\n- Edge cases: 2 (some covered)\n- Testability: 2 (mock pattern provided)\n\n**Maintainability (85/100)**:\n- Modularity: 3 (self-contained refs)\n- Update path: 3 (fetch pattern)\n- Clear organization: 2 (good, minor flow issues)\n\n---\n\n## Example 2: Adequate Skill (Needs Work)\n\n**Skill**: hypothetical-basic-skill\n**Type**: Guide\n\n### Category Scores\n\n| Category | Score | Weight | Weighted |\n|----------|-------|--------|----------|\n| Structure & Anatomy | 60/100 | 15% | 9.00 |\n| Content Quality | 55/100 | 20% | 11.00 |\n| User Interaction | 30/100 | 15% | 4.50 |\n| Documentation | 40/100 | 15% | 6.00 |\n| Domain Standards | 50/100 | 15% | 7.50 |\n| Technical Robustness | 45/100 | 10% | 4.50 |\n| Maintainability | 50/100 | 10% | 5.00 |\n| **Total** | | | **47.50** |\n\n**Rating**: Developing\n\n### Key Issues\n\n- **Structure**: 450 lines (borderline), README.md exists\n- **Content**: Verbose, mixed imperative form\n- **User Interaction**: \"Ask if needed\" without specifics\n- **Documentation**: One link buried in text\n- **Domain Standards**: Mentions best practices without enforcement\n- **Technical**: \"Handle errors\" without specifics\n- **Maintainability**: Everything in one file\n\n---\n\n## Example 3: Incomplete Skill\n\n**Skill**: hypothetical-minimal-skill\n\n```yaml\n---\nname: do-stuff\ndescription: Does stuff\n---\n\n# Do Stuff\n\nThis skill does stuff with things.\n\n## Usage\nJust ask it to do stuff.\n```\n\n### Category Scores\n\n| Category | Score | Weight | Weighted |\n|----------|-------|--------|----------|\n| Structure & Anatomy | 40/100 | 15% | 6.00 |\n| Content Quality | 20/100 | 20% | 4.00 |\n| User Interaction | 0/100 | 15% | 0.00 |\n| Documentation | 0/100 | 15% | 0.00 |\n| Domain Standards | 0/100 | 15% | 0.00 |\n| Technical Robustness | 0/100 | 10% | 0.00 |\n| Maintainability | 20/100 | 10% | 2.00 |\n| **Total** | | | **12.00** |\n\n**Rating**: Incomplete\n\n---\n\n## Example 4: Official skill-creator (Honest Assessment)\n\n**Skill**: skill-creator (Anthropic official)\n**Type**: Guide\n\n### Category Scores\n\n| Category | Score | Weight | Weighted |\n|----------|-------|--------|----------|\n| Structure & Anatomy | 90/100 | 15% | 13.50 |\n| Content Quality | 95/100 | 20% | 19.00 |\n| User Interaction | 50/100 | 15% | 7.50 |\n| Documentation | 30/100 | 15% | 4.50 |\n| Domain Standards | 40/100 | 15% | 6.00 |\n| Technical Robustness | 50/100 | 10% | 5.00 |\n| Maintainability | 85/100 | 10% | 8.50 |\n| **Total** | | | **64.00** |\n\n**Rating**: Adequate\n\n### Honest Breakdown\n\n**Structure & Anatomy (90/100)** - Excellent:\n- 357 lines (well under 500)\n- Complete frontmatter\n- 2 reference files\n- No extraneous files\n\n**Content Quality (95/100)** - Excellent:\n- Very concise\n- Consistent imperative form\n- Clear 6-step workflow\n- Good appropriate freedom guidance\n\n**User Interaction (50/100)** - Gaps:\n- Step 1 mentions asking users questions during skill creation process\n- But no structured \"Required Clarifications\" pattern\n- No guidance on context awareness\n- Missing \"what if user doesn't answer\"\n\n**Documentation (30/100)** - Missing:\n- No official documentation links\n- No fetch guidance for unlisted scenarios\n- References exist but no external sources\n- No version awareness\n\n**Domain Standards (40/100)** - Partial:\n- Has output-patterns.md reference\n- But no enforcement checklist\n- No \"Must Follow / Must Avoid\" pattern\n- No quality gates before delivery\n\n**Technical Robustness (50/100)** - Partial:\n- Scripts mentioned with testing requirement\n- But no error handling guidance\n- No security considerations\n- Dependencies implicit\n\n**Maintainability (85/100)** - Good:\n- Modular references\n- Clear organization\n- But no update path guidance\n\n### Improvement Recommendations for skill-creator\n\n1. **High Priority**: Add official documentation links (if Anthropic has docs)\n2. **High Priority**: Add structured clarification questions pattern\n3. **Medium Priority**: Add \"Must Follow / Must Avoid\" enforcement pattern\n4. **Medium Priority**: Add output checklist / quality gate\n5. **Low Priority**: Add error handling guidance for scripts\n6. **Low Priority**: Add update path / version awareness\n\n**Note**: This is an honest assessment. Even official skills can have room for improvement to reach production-level standards.\n\n---\n\n## Calibration Notes\n\n### Common Scoring Mistakes\n\n1. **Over-scoring structure**: A skill with clean structure but no content shouldn't score high overall\n\n2. **Under-scoring interaction**: Missing clarification patterns is a major gap for builder skills\n\n3. **Ignoring domain specificity**: A widget skill without UX standards is incomplete; a data skill without validation patterns is incomplete\n\n4. **Binary scoring**: Use the full 0-3 range, not just 0 or 3\n\n### Skill Type Adjustments\n\n**Builder Skills** (create artifacts):\n- User Interaction weight: Higher importance\n- Documentation weight: Higher importance\n- Clarification questions: Essential\n\n**Automation Skills** (run workflows):\n- Technical Robustness: Higher importance\n- Error handling: Critical\n- User Interaction: Lower unless ambiguous inputs\n\n**Guide Skills** (provide instructions):\n- Content Quality: Higher importance\n- Documentation: Higher importance\n- User Interaction: Medium (depends on complexity)\n",
        "skills/resume_optimizer/.claude/skills/theme-factory/SKILL.md": "---\nname: theme-factory\ndescription: Toolkit for styling artifacts with a theme. These artifacts can be slides, docs, reportings, HTML landing pages, etc. There are 10 pre-set themes with colors/fonts that you can apply to any artifact that has been creating, or can generate a new theme on-the-fly.\nlicense: Complete terms in LICENSE.txt\n---\n\n\n# Theme Factory Skill\n\nThis skill provides a curated collection of professional font and color themes themes, each with carefully selected color palettes and font pairings. Once a theme is chosen, it can be applied to any artifact.\n\n## Purpose\n\nTo apply consistent, professional styling to presentation slide decks, use this skill. Each theme includes:\n- A cohesive color palette with hex codes\n- Complementary font pairings for headers and body text\n- A distinct visual identity suitable for different contexts and audiences\n\n## Usage Instructions\n\nTo apply styling to a slide deck or other artifact:\n\n1. **Show the theme showcase**: Display the `theme-showcase.pdf` file to allow users to see all available themes visually. Do not make any modifications to it; simply show the file for viewing.\n2. **Ask for their choice**: Ask which theme to apply to the deck\n3. **Wait for selection**: Get explicit confirmation about the chosen theme\n4. **Apply the theme**: Once a theme has been chosen, apply the selected theme's colors and fonts to the deck/artifact\n\n## Themes Available\n\nThe following 10 themes are available, each showcased in `theme-showcase.pdf`:\n\n1. **Ocean Depths** - Professional and calming maritime theme\n2. **Sunset Boulevard** - Warm and vibrant sunset colors\n3. **Forest Canopy** - Natural and grounded earth tones\n4. **Modern Minimalist** - Clean and contemporary grayscale\n5. **Golden Hour** - Rich and warm autumnal palette\n6. **Arctic Frost** - Cool and crisp winter-inspired theme\n7. **Desert Rose** - Soft and sophisticated dusty tones\n8. **Tech Innovation** - Bold and modern tech aesthetic\n9. **Botanical Garden** - Fresh and organic garden colors\n10. **Midnight Galaxy** - Dramatic and cosmic deep tones\n\n## Theme Details\n\nEach theme is defined in the `themes/` directory with complete specifications including:\n- Cohesive color palette with hex codes\n- Complementary font pairings for headers and body text\n- Distinct visual identity suitable for different contexts and audiences\n\n## Application Process\n\nAfter a preferred theme is selected:\n1. Read the corresponding theme file from the `themes/` directory\n2. Apply the specified colors and fonts consistently throughout the deck\n3. Ensure proper contrast and readability\n4. Maintain the theme's visual identity across all slides\n\n## Create your Own Theme\nTo handle cases where none of the existing themes work for an artifact, create a custom theme. Based on provided inputs, generate a new theme similar to the ones above. Give the theme a similar name describing what the font/color combinations represent. Use any basic description provided to choose appropriate colors/fonts. After generating the theme, show it for review and verification. Following that, apply the theme as described above.\n",
        "skills/resume_optimizer/.claude/skills/theme-factory/themes/arctic-frost.md": "# Arctic Frost\n\nA cool and crisp winter-inspired theme that conveys clarity, precision, and professionalism.\n\n## Color Palette\n\n- **Ice Blue**: `#d4e4f7` - Light backgrounds and highlights\n- **Steel Blue**: `#4a6fa5` - Primary accent color\n- **Silver**: `#c0c0c0` - Metallic accent elements\n- **Crisp White**: `#fafafa` - Clean backgrounds and text\n\n## Typography\n\n- **Headers**: DejaVu Sans Bold\n- **Body Text**: DejaVu Sans\n\n## Best Used For\n\nHealthcare presentations, technology solutions, winter sports, clean tech, pharmaceutical content.\n",
        "skills/resume_optimizer/.claude/skills/theme-factory/themes/botanical-garden.md": "# Botanical Garden\n\nA fresh and organic theme featuring vibrant garden-inspired colors for lively presentations.\n\n## Color Palette\n\n- **Fern Green**: `#4a7c59` - Rich natural green\n- **Marigold**: `#f9a620` - Bright floral accent\n- **Terracotta**: `#b7472a` - Earthy warm tone\n- **Cream**: `#f5f3ed` - Soft neutral backgrounds\n\n## Typography\n\n- **Headers**: DejaVu Serif Bold\n- **Body Text**: DejaVu Sans\n\n## Best Used For\n\nGarden centers, food presentations, farm-to-table content, botanical brands, natural products.\n",
        "skills/resume_optimizer/.claude/skills/theme-factory/themes/desert-rose.md": "# Desert Rose\n\nA soft and sophisticated theme with dusty, muted tones perfect for elegant presentations.\n\n## Color Palette\n\n- **Dusty Rose**: `#d4a5a5` - Soft primary color\n- **Clay**: `#b87d6d` - Earthy accent\n- **Sand**: `#e8d5c4` - Warm neutral backgrounds\n- **Deep Burgundy**: `#5d2e46` - Rich dark contrast\n\n## Typography\n\n- **Headers**: FreeSans Bold\n- **Body Text**: FreeSans\n\n## Best Used For\n\nFashion presentations, beauty brands, wedding planning, interior design, boutique businesses.\n",
        "skills/resume_optimizer/.claude/skills/theme-factory/themes/forest-canopy.md": "# Forest Canopy\n\nA natural and grounded theme featuring earth tones inspired by dense forest environments.\n\n## Color Palette\n\n- **Forest Green**: `#2d4a2b` - Primary dark green\n- **Sage**: `#7d8471` - Muted green accent\n- **Olive**: `#a4ac86` - Light accent color\n- **Ivory**: `#faf9f6` - Backgrounds and text\n\n## Typography\n\n- **Headers**: FreeSerif Bold\n- **Body Text**: FreeSans\n\n## Best Used For\n\nEnvironmental presentations, sustainability reports, outdoor brands, wellness content, organic products.\n",
        "skills/resume_optimizer/.claude/skills/theme-factory/themes/golden-hour.md": "# Golden Hour\n\nA rich and warm autumnal palette that creates an inviting and sophisticated atmosphere.\n\n## Color Palette\n\n- **Mustard Yellow**: `#f4a900` - Bold primary accent\n- **Terracotta**: `#c1666b` - Warm secondary color\n- **Warm Beige**: `#d4b896` - Neutral backgrounds\n- **Chocolate Brown**: `#4a403a` - Dark text and anchors\n\n## Typography\n\n- **Headers**: FreeSans Bold\n- **Body Text**: FreeSans\n\n## Best Used For\n\nRestaurant presentations, hospitality brands, fall campaigns, cozy lifestyle content, artisan products.\n",
        "skills/resume_optimizer/.claude/skills/theme-factory/themes/midnight-galaxy.md": "# Midnight Galaxy\n\nA dramatic and cosmic theme with deep purples and mystical tones for impactful presentations.\n\n## Color Palette\n\n- **Deep Purple**: `#2b1e3e` - Rich dark base\n- **Cosmic Blue**: `#4a4e8f` - Mystical mid-tone\n- **Lavender**: `#a490c2` - Soft accent color\n- **Silver**: `#e6e6fa` - Light highlights and text\n\n## Typography\n\n- **Headers**: FreeSans Bold\n- **Body Text**: FreeSans\n\n## Best Used For\n\nEntertainment industry, gaming presentations, nightlife venues, luxury brands, creative agencies.\n",
        "skills/resume_optimizer/.claude/skills/theme-factory/themes/modern-minimalist.md": "# Modern Minimalist\n\nA clean and contemporary theme with a sophisticated grayscale palette for maximum versatility.\n\n## Color Palette\n\n- **Charcoal**: `#36454f` - Primary dark color\n- **Slate Gray**: `#708090` - Medium gray for accents\n- **Light Gray**: `#d3d3d3` - Backgrounds and dividers\n- **White**: `#ffffff` - Text and clean backgrounds\n\n## Typography\n\n- **Headers**: DejaVu Sans Bold\n- **Body Text**: DejaVu Sans\n\n## Best Used For\n\nTech presentations, architecture portfolios, design showcases, modern business proposals, data visualization.\n",
        "skills/resume_optimizer/.claude/skills/theme-factory/themes/ocean-depths.md": "# Ocean Depths\n\nA professional and calming maritime theme that evokes the serenity of deep ocean waters.\n\n## Color Palette\n\n- **Deep Navy**: `#1a2332` - Primary background color\n- **Teal**: `#2d8b8b` - Accent color for highlights and emphasis\n- **Seafoam**: `#a8dadc` - Secondary accent for lighter elements\n- **Cream**: `#f1faee` - Text and light backgrounds\n\n## Typography\n\n- **Headers**: DejaVu Sans Bold\n- **Body Text**: DejaVu Sans\n\n## Best Used For\n\nCorporate presentations, financial reports, professional consulting decks, trust-building content.\n",
        "skills/resume_optimizer/.claude/skills/theme-factory/themes/sunset-boulevard.md": "# Sunset Boulevard\n\nA warm and vibrant theme inspired by golden hour sunsets, perfect for energetic and creative presentations.\n\n## Color Palette\n\n- **Burnt Orange**: `#e76f51` - Primary accent color\n- **Coral**: `#f4a261` - Secondary warm accent\n- **Warm Sand**: `#e9c46a` - Highlighting and backgrounds\n- **Deep Purple**: `#264653` - Dark contrast and text\n\n## Typography\n\n- **Headers**: DejaVu Serif Bold\n- **Body Text**: DejaVu Sans\n\n## Best Used For\n\nCreative pitches, marketing presentations, lifestyle brands, event promotions, inspirational content.\n",
        "skills/resume_optimizer/.claude/skills/theme-factory/themes/tech-innovation.md": "# Tech Innovation\n\nA bold and modern theme with high-contrast colors perfect for cutting-edge technology presentations.\n\n## Color Palette\n\n- **Electric Blue**: `#0066ff` - Vibrant primary accent\n- **Neon Cyan**: `#00ffff` - Bright highlight color\n- **Dark Gray**: `#1e1e1e` - Deep backgrounds\n- **White**: `#ffffff` - Clean text and contrast\n\n## Typography\n\n- **Headers**: DejaVu Sans Bold\n- **Body Text**: DejaVu Sans\n\n## Best Used For\n\nTech startups, software launches, innovation showcases, AI/ML presentations, digital transformation content.\n",
        "skills/resume_optimizer/.claude/skills/xlsx/SKILL.md": "---\nname: xlsx\ndescription: \"Comprehensive spreadsheet creation, editing, and analysis with support for formulas, formatting, data analysis, and visualization. When Claude needs to work with spreadsheets (.xlsx, .xlsm, .csv, .tsv, etc) for: (1) Creating new spreadsheets with formulas and formatting, (2) Reading or analyzing data, (3) Modify existing spreadsheets while preserving formulas, (4) Data analysis and visualization in spreadsheets, or (5) Recalculating formulas\"\nlicense: Proprietary. LICENSE.txt has complete terms\n---\n\n# Requirements for Outputs\n\n## All Excel files\n\n### Zero Formula Errors\n- Every Excel model MUST be delivered with ZERO formula errors (#REF!, #DIV/0!, #VALUE!, #N/A, #NAME?)\n\n### Preserve Existing Templates (when updating templates)\n- Study and EXACTLY match existing format, style, and conventions when modifying files\n- Never impose standardized formatting on files with established patterns\n- Existing template conventions ALWAYS override these guidelines\n\n## Financial models\n\n### Color Coding Standards\nUnless otherwise stated by the user or existing template\n\n#### Industry-Standard Color Conventions\n- **Blue text (RGB: 0,0,255)**: Hardcoded inputs, and numbers users will change for scenarios\n- **Black text (RGB: 0,0,0)**: ALL formulas and calculations\n- **Green text (RGB: 0,128,0)**: Links pulling from other worksheets within same workbook\n- **Red text (RGB: 255,0,0)**: External links to other files\n- **Yellow background (RGB: 255,255,0)**: Key assumptions needing attention or cells that need to be updated\n\n### Number Formatting Standards\n\n#### Required Format Rules\n- **Years**: Format as text strings (e.g., \"2024\" not \"2,024\")\n- **Currency**: Use $#,##0 format; ALWAYS specify units in headers (\"Revenue ($mm)\")\n- **Zeros**: Use number formatting to make all zeros \"-\", including percentages (e.g., \"$#,##0;($#,##0);-\")\n- **Percentages**: Default to 0.0% format (one decimal)\n- **Multiples**: Format as 0.0x for valuation multiples (EV/EBITDA, P/E)\n- **Negative numbers**: Use parentheses (123) not minus -123\n\n### Formula Construction Rules\n\n#### Assumptions Placement\n- Place ALL assumptions (growth rates, margins, multiples, etc.) in separate assumption cells\n- Use cell references instead of hardcoded values in formulas\n- Example: Use =B5*(1+$B$6) instead of =B5*1.05\n\n#### Formula Error Prevention\n- Verify all cell references are correct\n- Check for off-by-one errors in ranges\n- Ensure consistent formulas across all projection periods\n- Test with edge cases (zero values, negative numbers)\n- Verify no unintended circular references\n\n#### Documentation Requirements for Hardcodes\n- Comment or in cells beside (if end of table). Format: \"Source: [System/Document], [Date], [Specific Reference], [URL if applicable]\"\n- Examples:\n  - \"Source: Company 10-K, FY2024, Page 45, Revenue Note, [SEC EDGAR URL]\"\n  - \"Source: Company 10-Q, Q2 2025, Exhibit 99.1, [SEC EDGAR URL]\"\n  - \"Source: Bloomberg Terminal, 8/15/2025, AAPL US Equity\"\n  - \"Source: FactSet, 8/20/2025, Consensus Estimates Screen\"\n\n# XLSX creation, editing, and analysis\n\n## Overview\n\nA user may ask you to create, edit, or analyze the contents of an .xlsx file. You have different tools and workflows available for different tasks.\n\n## Important Requirements\n\n**LibreOffice Required for Formula Recalculation**: You can assume LibreOffice is installed for recalculating formula values using the `recalc.py` script. The script automatically configures LibreOffice on first run\n\n## Reading and analyzing data\n\n### Data analysis with pandas\nFor data analysis, visualization, and basic operations, use **pandas** which provides powerful data manipulation capabilities:\n\n```python\nimport pandas as pd\n\n# Read Excel\ndf = pd.read_excel('file.xlsx')  # Default: first sheet\nall_sheets = pd.read_excel('file.xlsx', sheet_name=None)  # All sheets as dict\n\n# Analyze\ndf.head()      # Preview data\ndf.info()      # Column info\ndf.describe()  # Statistics\n\n# Write Excel\ndf.to_excel('output.xlsx', index=False)\n```\n\n## Excel File Workflows\n\n## CRITICAL: Use Formulas, Not Hardcoded Values\n\n**Always use Excel formulas instead of calculating values in Python and hardcoding them.** This ensures the spreadsheet remains dynamic and updateable.\n\n###  WRONG - Hardcoding Calculated Values\n```python\n# Bad: Calculating in Python and hardcoding result\ntotal = df['Sales'].sum()\nsheet['B10'] = total  # Hardcodes 5000\n\n# Bad: Computing growth rate in Python\ngrowth = (df.iloc[-1]['Revenue'] - df.iloc[0]['Revenue']) / df.iloc[0]['Revenue']\nsheet['C5'] = growth  # Hardcodes 0.15\n\n# Bad: Python calculation for average\navg = sum(values) / len(values)\nsheet['D20'] = avg  # Hardcodes 42.5\n```\n\n###  CORRECT - Using Excel Formulas\n```python\n# Good: Let Excel calculate the sum\nsheet['B10'] = '=SUM(B2:B9)'\n\n# Good: Growth rate as Excel formula\nsheet['C5'] = '=(C4-C2)/C2'\n\n# Good: Average using Excel function\nsheet['D20'] = '=AVERAGE(D2:D19)'\n```\n\nThis applies to ALL calculations - totals, percentages, ratios, differences, etc. The spreadsheet should be able to recalculate when source data changes.\n\n## Common Workflow\n1. **Choose tool**: pandas for data, openpyxl for formulas/formatting\n2. **Create/Load**: Create new workbook or load existing file\n3. **Modify**: Add/edit data, formulas, and formatting\n4. **Save**: Write to file\n5. **Recalculate formulas (MANDATORY IF USING FORMULAS)**: Use the recalc.py script\n   ```bash\n   python recalc.py output.xlsx\n   ```\n6. **Verify and fix any errors**: \n   - The script returns JSON with error details\n   - If `status` is `errors_found`, check `error_summary` for specific error types and locations\n   - Fix the identified errors and recalculate again\n   - Common errors to fix:\n     - `#REF!`: Invalid cell references\n     - `#DIV/0!`: Division by zero\n     - `#VALUE!`: Wrong data type in formula\n     - `#NAME?`: Unrecognized formula name\n\n### Creating new Excel files\n\n```python\n# Using openpyxl for formulas and formatting\nfrom openpyxl import Workbook\nfrom openpyxl.styles import Font, PatternFill, Alignment\n\nwb = Workbook()\nsheet = wb.active\n\n# Add data\nsheet['A1'] = 'Hello'\nsheet['B1'] = 'World'\nsheet.append(['Row', 'of', 'data'])\n\n# Add formula\nsheet['B2'] = '=SUM(A1:A10)'\n\n# Formatting\nsheet['A1'].font = Font(bold=True, color='FF0000')\nsheet['A1'].fill = PatternFill('solid', start_color='FFFF00')\nsheet['A1'].alignment = Alignment(horizontal='center')\n\n# Column width\nsheet.column_dimensions['A'].width = 20\n\nwb.save('output.xlsx')\n```\n\n### Editing existing Excel files\n\n```python\n# Using openpyxl to preserve formulas and formatting\nfrom openpyxl import load_workbook\n\n# Load existing file\nwb = load_workbook('existing.xlsx')\nsheet = wb.active  # or wb['SheetName'] for specific sheet\n\n# Working with multiple sheets\nfor sheet_name in wb.sheetnames:\n    sheet = wb[sheet_name]\n    print(f\"Sheet: {sheet_name}\")\n\n# Modify cells\nsheet['A1'] = 'New Value'\nsheet.insert_rows(2)  # Insert row at position 2\nsheet.delete_cols(3)  # Delete column 3\n\n# Add new sheet\nnew_sheet = wb.create_sheet('NewSheet')\nnew_sheet['A1'] = 'Data'\n\nwb.save('modified.xlsx')\n```\n\n## Recalculating formulas\n\nExcel files created or modified by openpyxl contain formulas as strings but not calculated values. Use the provided `recalc.py` script to recalculate formulas:\n\n```bash\npython recalc.py <excel_file> [timeout_seconds]\n```\n\nExample:\n```bash\npython recalc.py output.xlsx 30\n```\n\nThe script:\n- Automatically sets up LibreOffice macro on first run\n- Recalculates all formulas in all sheets\n- Scans ALL cells for Excel errors (#REF!, #DIV/0!, etc.)\n- Returns JSON with detailed error locations and counts\n- Works on both Linux and macOS\n\n## Formula Verification Checklist\n\nQuick checks to ensure formulas work correctly:\n\n### Essential Verification\n- [ ] **Test 2-3 sample references**: Verify they pull correct values before building full model\n- [ ] **Column mapping**: Confirm Excel columns match (e.g., column 64 = BL, not BK)\n- [ ] **Row offset**: Remember Excel rows are 1-indexed (DataFrame row 5 = Excel row 6)\n\n### Common Pitfalls\n- [ ] **NaN handling**: Check for null values with `pd.notna()`\n- [ ] **Far-right columns**: FY data often in columns 50+ \n- [ ] **Multiple matches**: Search all occurrences, not just first\n- [ ] **Division by zero**: Check denominators before using `/` in formulas (#DIV/0!)\n- [ ] **Wrong references**: Verify all cell references point to intended cells (#REF!)\n- [ ] **Cross-sheet references**: Use correct format (Sheet1!A1) for linking sheets\n\n### Formula Testing Strategy\n- [ ] **Start small**: Test formulas on 2-3 cells before applying broadly\n- [ ] **Verify dependencies**: Check all cells referenced in formulas exist\n- [ ] **Test edge cases**: Include zero, negative, and very large values\n\n### Interpreting recalc.py Output\nThe script returns JSON with error details:\n```json\n{\n  \"status\": \"success\",           // or \"errors_found\"\n  \"total_errors\": 0,              // Total error count\n  \"total_formulas\": 42,           // Number of formulas in file\n  \"error_summary\": {              // Only present if errors found\n    \"#REF!\": {\n      \"count\": 2,\n      \"locations\": [\"Sheet1!B5\", \"Sheet1!C10\"]\n    }\n  }\n}\n```\n\n## Best Practices\n\n### Library Selection\n- **pandas**: Best for data analysis, bulk operations, and simple data export\n- **openpyxl**: Best for complex formatting, formulas, and Excel-specific features\n\n### Working with openpyxl\n- Cell indices are 1-based (row=1, column=1 refers to cell A1)\n- Use `data_only=True` to read calculated values: `load_workbook('file.xlsx', data_only=True)`\n- **Warning**: If opened with `data_only=True` and saved, formulas are replaced with values and permanently lost\n- For large files: Use `read_only=True` for reading or `write_only=True` for writing\n- Formulas are preserved but not evaluated - use recalc.py to update values\n\n### Working with pandas\n- Specify data types to avoid inference issues: `pd.read_excel('file.xlsx', dtype={'id': str})`\n- For large files, read specific columns: `pd.read_excel('file.xlsx', usecols=['A', 'C', 'E'])`\n- Handle dates properly: `pd.read_excel('file.xlsx', parse_dates=['date_column'])`\n\n## Code Style Guidelines\n**IMPORTANT**: When generating Python code for Excel operations:\n- Write minimal, concise Python code without unnecessary comments\n- Avoid verbose variable names and redundant operations\n- Avoid unnecessary print statements\n\n**For Excel files themselves**:\n- Add comments to cells with complex formulas or important assumptions\n- Document data sources for hardcoded values\n- Include notes for key calculations and model sections",
        "skills/resume_optimizer/readme.md": "![Banner](./header.jpg)\r\n\r\n````md\r\n# linkedin_job_resume_optimizer\r\n\r\n## Overview\r\n\r\n`linkedin_job_resume_optimizer` is an agentic skill designed to bridge the gap between **LinkedIn job market demand** and a candidates **existing professional profile**.  \r\nIt automates job discovery, resume optimization, interview preparation, and LinkedIn profile alignment  while strictly maintaining **truthfulness and professional integrity**.\r\n\r\nThe skill is intended for candidates who want to:\r\n- Target relevant roles efficiently\r\n- Optimize resumes for ATS and recruiter expectations\r\n- Prepare for interviews with role-specific questions\r\n- Improve LinkedIn profiles to match real market signals\r\n\r\n---\r\n\r\n## Core Features\r\n\r\n### 1. LinkedIn Job Discovery\r\n- Searches LinkedIn for roles aligned with the candidates background\r\n- Identifies high-relevance positions based on skills, experience, and role keywords\r\n- Can prioritize filters such as:\r\n  - Remote vs on-site\r\n  - Location (e.g., US, EU, global)\r\n  - Seniority level\r\n  - Industry/domain\r\n\r\n---\r\n\r\n### 2. Resume Optimization (Honest Alignment)\r\n- Analyzes job descriptions and extracts:\r\n  - Required skills\r\n  - Preferred skills\r\n  - Keywords and role language\r\n- Optimizes the provided resume to:\r\n  - Improve ATS keyword matching\r\n  - Align phrasing with job requirements\r\n  - Highlight relevant achievements\r\n- **Does NOT fabricate experience or skills**\r\n- Preserves factual accuracy and professional credibility\r\n\r\n---\r\n\r\n### 3. Interview Question Generation\r\n- Generates role-specific interview questions based on:\r\n  - Job descriptions\r\n  - Optimized resume content\r\n- Covers:\r\n  - Technical questions\r\n  - Behavioral questions\r\n  - Scenario-based and situational questions\r\n- Helps candidates prepare with realistic expectations\r\n\r\n---\r\n\r\n### 4. LinkedIn Profile Optimization\r\n- Recommends updates for:\r\n  - Headline\r\n  - About/Summary section\r\n  - Experience descriptions\r\n  - Skills section\r\n- Aligns profile language with:\r\n  - Target roles\r\n  - Resume updates\r\n  - Current market terminology\r\n- Focuses on recruiter and hiring-manager visibility\r\n\r\n---\r\n\r\n## Inputs\r\n\r\n- Resume file (local path, e.g. `.docx`)\r\n- Candidate context (implicit or explicit)\r\n- Optional constraints (role type, geography, seniority, etc.)\r\n\r\n---\r\n\r\n## Outputs\r\n\r\n- Shortlist of relevant LinkedIn roles\r\n- Optimized resume content (truthful and role-aligned)\r\n- Targeted interview questions\r\n- Actionable LinkedIn profile update suggestions\r\n\r\n---\r\n\r\n## Usage Instructions\r\n\r\nWhen using this skill:\r\n1. Clearly specify the resume file path.\r\n2. Allow the skill to first identify suitable roles.\r\n3. Ensure optimization is **truthful and experience-based**.\r\n4. Optionally constrain scope (e.g., number of roles, geography).\r\n\r\n---\r\n\r\n## Usage Examples\r\n\r\n### Example 1  End-to-end optimization\r\n```text\r\nUse the linkedin_job_resume_optimizer skill to find relevant LinkedIn roles, optimize the resume at\r\n/mnt/d/path/to/resume.docx to honestly match required skills, then generate interview questions and LinkedIn profile update suggestions.\r\n````\r\n\r\n---\r\n\r\n### Example 2  Role-focused optimization\r\n\r\n```text\r\nUsing linkedin_job_resume_optimizer, target remote senior product roles, optimize the resume at\r\n/mnt/d/path/to/resume.docx for ATS alignment, and prepare role-specific interview questions.\r\n```\r\n\r\n---\r\n\r\n### Example 3  LinkedIn-first strategy\r\n\r\n```text\r\nSearch for suitable LinkedIn jobs using linkedin_job_resume_optimizer, then recommend LinkedIn profile headline and summary updates based on the identified roles and existing resume.\r\n```\r\n\r\n---\r\n\r\n## Design Principles\r\n\r\n* **Truth over inflation**: No fabricated skills or experience\r\n* **Market-driven**: Optimization reflects real job requirements\r\n* **Reusable**: Suitable for iterative job searches\r\n* **ATS-aware**: Language and structure optimized for screening systems\r\n\r\n---\r\n\r\n## Intended Use Cases\r\n\r\n* Job seekers targeting competitive roles\r\n* Career pivots requiring language alignment (not skill fabrication)\r\n* Resume refinement for ATS-heavy hiring pipelines\r\n* LinkedIn profile modernization\r\n\r\n---\r\n\r\n## Notes\r\n\r\nThis skill is designed to be used iteratively.\r\nAs market conditions or target roles change, the process can be repeated with updated constraints or resumes.\r\n\r\n---\r\n\r\n```\r\n\r\n---\r\n\r\n### Next optional upgrades (if you want)\r\n- Add a **limitations section** (what the skill explicitly does not do)\r\n- Add a **privacy & data handling note**\r\n- Convert this into a **README.md** with badges and diagrams\r\n- Generate a **`claude.md` instruction block** that enforces honest optimization\r\n\r\nTell me the next enhancement you want.\r\n```\r\n",
        "skills/resume_optimizer/resume_optimizer.md": "Act as a Career Strategy Agent and ATS Optimization Expert. Your goal is to find high-relevance AI jobs and tailor existing resume for each.\r\n\r\n### 1. Context Acquisition\r\n- Use the 'docx' skill to read master resume at: @.claude/skills/resume_06012026.docx\r\n- Extract my core technical stack, key achievements, and professional summary to use as a baseline.\r\n\r\n### 2. Job Search & Analysis\r\n- Use the 'browsing-with-playwright' skill to visit: https://www.linkedin.com/jobs/\r\n- Search for \"Remote AI Engineer\" or \"Remote AI Developer\" roles. \r\n- Identify exactly 2 job postings that match my extracted skill set.\r\n- For each job, extract: \r\n  - Required Technical Skills\r\n  - Soft Skills / Cultural Fit keywords\r\n  - Specific KPIs or project responsibilities mentioned.\r\n\r\n### 3. Optimization & Document Generation\r\n- For each of the 2 identified jobs, create a unique resume using '@resume/06012026.docx' as the template.\r\n- Strategy:\r\n  - Re-order skills to prioritize the keywords found in the job description.\r\n  - Rephrase bullet points to align with the specific KPIs of the role.\r\n  - Maintain absolute factual honesty based on my master resume.\r\n- Output: Save each document with a descriptive name, e.g., 'Tailored_Resume_[CompanyName].docx'.\r\n\r\n### 4. Verification\r\n- Confirm when both documents are saved and provide a brief 'ATS Optimization Summary' for each, explaining which keywords were prioritized.",
        "skills/sqlmodel-orm-dbhelper/.claude-plugin/plugin.json": "{\n  \"name\": \"sqlmodel-orm-dbhelper\",\n  \"description\": \"SQLModel ORM database helper for Python - models, relationships, migrations, and query patterns\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Ali Jilani\"\n  },\n  \"category\": \"development\",\n  \"keywords\": [\"sqlmodel\", \"orm\", \"database\", \"python\", \"sql\"]\n}\n",
        "skills/sqlmodel-orm-dbhelper/SKILL.md": "---\nname: sqlmodel-orm-dbhelper\ndescription: |\n  Design and implement production-grade database layers using SQLModel with SQLAlchemy 2.0 patterns.\n  This skill should be used when users need to create database models, configure engines/sessions,\n  design schemas, implement relationships, or optimize database performance in Python FastAPI projects.\n---\n\n# SQLModel ORM Database Helper\n\nA comprehensive skill for designing robust, high-performance database management layers using SQLModel.\n\n## What This Skill Does\n\n- Designs optimal database schemas based on project context\n- Creates production-ready SQLModel models with proper types and constraints\n- Configures engine with connection pooling (QueuePool, NullPool)\n- Implements session management for FastAPI dependency injection\n- Defines relationships (One-to-One, One-to-Many, Many-to-Many)\n- Implements automatic timestamps (created_at, updated_at)\n- Optimizes for performance (indexing, lazy/eager loading, N+1 prevention)\n- Follows SQLAlchemy 2.0 and modern Python type hints (Annotated, Optional)\n\n## What This Skill Does NOT Do\n\n- Database migrations (use Alembic separately)\n- Database administration or server configuration\n- Raw SQL query optimization (focuses on ORM patterns)\n- NoSQL database design\n- Database backup/restore operations\n\n---\n\n## Before Implementation\n\nGather context to ensure successful implementation:\n\n| Source | Gather |\n|--------|--------|\n| **Codebase** | Existing models, database.py, project structure, FastAPI app setup |\n| **Conversation** | Project domain, entities, relationships, performance requirements |\n| **Skill References** | SQLModel patterns from `references/` (models, relationships, engine config) |\n| **User Guidelines** | Naming conventions, project standards, database choice (SQLite/PostgreSQL/MySQL) |\n\nEnsure all required context is gathered before implementing.\nOnly ask user for THEIR specific requirements (domain expertise is in this skill).\n\n---\n\n## Required Clarifications\n\nAsk about USER's context before designing:\n\n1. **Project Domain**: \"What is your project about? (e.g., e-commerce, inventory, blog)\"\n2. **Key Entities**: \"What are the main entities/tables you need?\"\n3. **Database**: \"Which database? (SQLite for dev, PostgreSQL/MySQL for production)\"\n4. **Performance Priority**: \"Any specific performance concerns? (high read, high write, real-time)\"\n\n---\n\n## Workflow\n\n```\n1. Understand Domain  2. Design Schema  3. Create Models  4. Configure Engine  5. Implement Session  6. Add Relationships  7. Optimize\n```\n\n### Step 1: Understand Domain\n- Identify entities and their attributes\n- Map relationships between entities\n- Determine data types and constraints\n\n### Step 2: Design Schema\n- Normalize to 3NF (balance with query performance)\n- Define primary keys, foreign keys, unique constraints\n- Plan indexes for query patterns\n\n### Step 3: Create Models\n- Use SQLModel with proper type annotations\n- Implement mixins for common fields (timestamps)\n- Add field constraints and validators\n\n### Step 4: Configure Engine\n- Set up connection pooling based on use case\n- Configure echo for debugging (dev only)\n- Set appropriate pool size and overflow\n\n### Step 5: Implement Session\n- Create session generator for FastAPI Depends\n- Use context managers for proper cleanup\n- Implement transaction boundaries\n\n### Step 6: Add Relationships\n- Define relationship fields with back_populates\n- Configure cascade behaviors\n- Set lazy/eager loading strategies\n\n### Step 7: Optimize\n- Add indexes for frequent queries\n- Configure eager loading for N+1 prevention\n- Review and tune connection pool settings\n\n---\n\n## Schema Design Principles\n\n### Normalization Guidelines\n\n| Normal Form | When to Use |\n|-------------|-------------|\n| **1NF** | Always - atomic values, no repeating groups |\n| **2NF** | Always - remove partial dependencies |\n| **3NF** | Default - remove transitive dependencies |\n| **Denormalize** | Only for proven performance needs |\n\n### Data Type Selection\n\n| Data Type | SQLModel Type | Use Case |\n|-----------|---------------|----------|\n| Primary Key | `int | None = Field(default=None, primary_key=True)` | Auto-increment ID |\n| UUID PK | `uuid.UUID = Field(default_factory=uuid4, primary_key=True)` | Distributed systems |\n| String | `str = Field(max_length=255)` | Text with limit |\n| Text | `str = Field(sa_type=Text)` | Unlimited text |\n| DateTime | `datetime = Field(default_factory=datetime.utcnow)` | Timestamps |\n| DateTime TZ | `datetime = Field(sa_type=DateTime(timezone=True))` | Timezone-aware |\n| Decimal | `Decimal = Field(max_digits=10, decimal_places=2)` | Financial data |\n| JSON | `dict = Field(sa_type=JSON)` | Flexible schemas |\n| JSONB | `dict = Field(sa_type=JSONB)` | PostgreSQL queryable JSON |\n\n---\n\n## Model Patterns\n\n### Base Model with Timestamps\n\n```python\nfrom datetime import datetime\nfrom typing import Optional\nfrom sqlmodel import Field, SQLModel\n\nclass TimestampMixin(SQLModel):\n    created_at: datetime = Field(default_factory=datetime.utcnow, nullable=False)\n    updated_at: datetime = Field(\n        default_factory=datetime.utcnow,\n        sa_column_kwargs={\"onupdate\": datetime.utcnow},\n        nullable=False\n    )\n\nclass BaseModel(TimestampMixin):\n    id: Optional[int] = Field(default=None, primary_key=True)\n```\n\n### Model with Relationships\n\nSee `references/relationships.md` for complete relationship patterns.\n\n---\n\n## Engine & Session Configuration\n\n### Production Engine Setup\n\n```python\nfrom sqlmodel import create_engine, Session\nfrom sqlalchemy.pool import QueuePool\n\nDATABASE_URL = \"postgresql://user:pass@localhost/dbname\"\n\nengine = create_engine(\n    DATABASE_URL,\n    poolclass=QueuePool,\n    pool_size=5,           # Persistent connections\n    max_overflow=10,       # Additional connections under load\n    pool_timeout=30,       # Wait time for connection\n    pool_recycle=1800,     # Recycle connections every 30 min\n    pool_pre_ping=True,    # Verify connection health\n    echo=False,            # Disable SQL logging in production\n)\n```\n\n### Session Generator for FastAPI\n\n```python\nfrom typing import Generator\nfrom fastapi import Depends\nfrom sqlmodel import Session\n\ndef get_session() -> Generator[Session, None, None]:\n    with Session(engine) as session:\n        yield session\n\n# Usage in FastAPI endpoint\n@app.get(\"/items\")\ndef get_items(session: Session = Depends(get_session)):\n    return session.exec(select(Item)).all()\n```\n\nSee `references/engine-config.md` for database-specific configurations.\n\n---\n\n## Performance Optimization\n\n### Indexing Strategy\n\n```python\nclass User(SQLModel, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    email: str = Field(unique=True, index=True)  # Unique + indexed\n    username: str = Field(index=True)            # Frequently queried\n    status: str = Field(index=True)              # Filter field\n```\n\n### Preventing N+1 Queries\n\n```python\nfrom sqlmodel import select\nfrom sqlalchemy.orm import selectinload, joinedload\n\n# Eager load related objects\nstatement = select(User).options(selectinload(User.orders))\nusers = session.exec(statement).all()\n\n# Use joinedload for single related object\nstatement = select(Order).options(joinedload(Order.user))\n```\n\n### Lazy vs Eager Loading\n\n| Strategy | Use When |\n|----------|----------|\n| `lazy=\"select\"` (default) | Related data rarely needed |\n| `lazy=\"selectin\"` | Loading multiple parents with children |\n| `lazy=\"joined\"` | Always need related data, single object |\n| `lazy=\"subquery\"` | Complex queries with collections |\n\nSee `references/performance.md` for advanced optimization patterns.\n\n---\n\n## Anti-Patterns to Avoid\n\n| Anti-Pattern | Problem | Solution |\n|--------------|---------|----------|\n| Session per operation | Connection overhead | One session per request |\n| Missing indexes | Slow queries | Index frequently filtered columns |\n| N+1 queries | Performance killer | Use eager loading |\n| No connection pooling | Resource exhaustion | Use QueuePool |\n| Committing in loops | Transaction overhead | Batch operations |\n| No pool_pre_ping | Stale connections | Enable pre-ping |\n| Hardcoded credentials | Security risk | Use environment variables |\n\n---\n\n## Output Specification\n\nWhen implementing, deliver:\n\n1. **Schema Design Document**: Logical explanation of tables and relationships\n2. **models.py**: Clean SQLModel implementations with:\n   - Type-annotated fields\n   - Proper constraints (PK, FK, unique, indexes)\n   - Relationship definitions\n   - Timestamp mixins\n3. **database.py**: Production-ready setup with:\n   - Engine configuration with pooling\n   - Session generator for FastAPI\n   - Database initialization function\n4. **Performance Notes**: Brief optimization explanations\n\n---\n\n## Output Checklist\n\nBefore delivering, verify:\n\n- [ ] All entities from requirements are modeled\n- [ ] Primary keys defined for all tables\n- [ ] Foreign keys maintain referential integrity\n- [ ] Indexes added for frequently queried columns\n- [ ] Relationships use back_populates correctly\n- [ ] Engine uses connection pooling\n- [ ] Session generator uses context manager\n- [ ] Timestamps (created_at/updated_at) implemented\n- [ ] No hardcoded credentials\n- [ ] Type hints use modern Python (Annotated, Optional)\n- [ ] Code follows SQLAlchemy 2.0 patterns\n\n---\n\n## Reference Files\n\n| File | When to Read |\n|------|--------------|\n| `references/relationships.md` | Implementing One-to-One, One-to-Many, Many-to-Many |\n| `references/engine-config.md` | Database-specific engine configuration |\n| `references/performance.md` | Advanced optimization, query tuning |\n| `references/field-types.md` | Complete field type reference |\n| `references/migrations.md` | Alembic migration guidance |\n",
        "skills/sqlmodel-orm-dbhelper/readme.md": "![Banner](./header.jpg)\r\n\r\n# SQLModel ORM Database Helper \r\n\r\nA comprehensive Claude Code skill for designing and implementing robust, high-performance database management layers using **SQLModel** and **SQLAlchemy 2.0** patterns.\r\n\r\n---\r\n\r\n##  Overview\r\n\r\nThe `sqlmodel-orm-dbhelper` skill automates the creation of production-grade database schemas, models, and configurations for Python FastAPI projects. It ensures your database layer follows modern Python type hints and performance best practices, from connection pooling to relationship mapping.\r\n\r\n##  Key Features\r\n\r\n* **Optimal Schema Design**: Automates the design of database schemas based on specific project domains.\r\n* **SQLModel Generation**: Creates production-ready models with proper type annotations, constraints, and automatic timestamps.\r\n* **Engine Configuration**: Configures SQLAlchemy engines with advanced connection pooling (QueuePool) and health checks.\r\n* **FastAPI Integration**: Implements session management using context managers for clean dependency injection in FastAPI.\r\n* **Performance Optimization**: Automatically applies indexing, lazy/eager loading strategies, and N+1 query prevention.\r\n* **Modern Standards**: Adheres to SQLAlchemy 2.0 patterns and modern Python `Annotated` and `Optional` types.\r\n\r\n##  Workflow\r\n\r\nThe skill follows a structured 7-step pipeline to ensure high-quality delivery:\r\n1. **Understand Domain** (identify entities and attributes).\r\n2. **Design Schema** (normalize to 3NF).\r\n3. **Create Models** (implement SQLModel classes and mixins).\r\n4. **Configure Engine** (set up connection pooling).\r\n5. **Implement Session** (create generators for FastAPI).\r\n6. **Add Relationships** (define back_populates and cascades).\r\n7. **Optimize** (configure indexes and loading strategies).\r\n\r\n##  How to Use with Claude Code\r\n\r\n### Step 1: Trigger the Skill\r\nProvide your project context to activate the helper:\r\n`claude \"Using the sqlmodel-orm-dbhelper skill, design a database for an e-commerce app with Users, Orders, and Products.\"`\r\n\r\n### Step 2: Provide Context\r\nThe skill will ask for clarifications regarding:\r\n* Your project domain.\r\n* Key entities required.\r\n* Preferred database (SQLite, PostgreSQL, or MySQL).\r\n* Performance priorities.\r\n\r\n### Step 3: Deployment\r\nClaude will generate:\r\n* **models.py**: Clean, annotated SQLModel implementations.\r\n* **database.py**: Production-ready engine and session setup.\r\n* **Schema Design Doc**: Logic behind your tables and relationships.\r\n\r\n##  Anti-Patterns Avoided\r\n*  Session per operation (solves with one session per request).\r\n*  N+1 queries (solves with eager loading).\r\n*  Missing indexes (solves with index-frequent column filtering).\r\n*  Hardcoded credentials (solves with environment variables).\r\n\r\n---\r\n*Developed by [alijilani-dev] to streamline AI-Driven Database Development.*",
        "skills/sqlmodel-orm-dbhelper/references/engine-config.md": "# Engine Configuration\n\nDatabase-specific engine configurations for production use.\n\n---\n\n## SQLite Configuration\n\nBest for development and small applications.\n\n```python\nfrom sqlmodel import create_engine\nfrom sqlalchemy.pool import StaticPool\n\n# Development (in-memory)\nengine = create_engine(\n    \"sqlite:///:memory:\",\n    connect_args={\"check_same_thread\": False},\n    poolclass=StaticPool,\n    echo=True,  # SQL logging for debug\n)\n\n# Development (file-based)\nengine = create_engine(\n    \"sqlite:///./database.db\",\n    connect_args={\"check_same_thread\": False},\n    echo=True,\n)\n\n# Testing (isolated in-memory)\nengine = create_engine(\n    \"sqlite:///:memory:\",\n    connect_args={\"check_same_thread\": False},\n    poolclass=StaticPool,\n)\n```\n\n### SQLite Notes\n- `check_same_thread=False`: Required for FastAPI async\n- `StaticPool`: Single connection for in-memory databases\n- No connection pooling needed for file-based SQLite\n- WAL mode for better concurrent read/write: `PRAGMA journal_mode=WAL`\n\n---\n\n## PostgreSQL Configuration\n\nRecommended for production.\n\n```python\nfrom sqlmodel import create_engine\nfrom sqlalchemy.pool import QueuePool\n\n# Basic connection\nDATABASE_URL = \"postgresql://user:password@localhost:5432/dbname\"\n\n# With SSL\nDATABASE_URL = \"postgresql://user:password@host:5432/dbname?sslmode=require\"\n\n# Production configuration\nengine = create_engine(\n    DATABASE_URL,\n    poolclass=QueuePool,\n    pool_size=5,           # Base connections to maintain\n    max_overflow=10,       # Extra connections under load\n    pool_timeout=30,       # Seconds to wait for connection\n    pool_recycle=1800,     # Recycle every 30 minutes\n    pool_pre_ping=True,    # Verify connection before use\n    echo=False,            # Disable SQL logging\n)\n```\n\n### Connection Pool Sizing Guide\n\n| Application Type | pool_size | max_overflow |\n|------------------|-----------|--------------|\n| Small API | 5 | 5 |\n| Medium API | 10 | 20 |\n| High-traffic API | 20 | 40 |\n| Worker process | 2 | 5 |\n\n**Rule**: `pool_size + max_overflow < PostgreSQL max_connections`\n\n### PostgreSQL Async (Optional)\n\n```python\nfrom sqlmodel import create_engine\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\n\n# Async engine\nasync_engine = create_async_engine(\n    \"postgresql+asyncpg://user:password@localhost:5432/dbname\",\n    pool_size=5,\n    max_overflow=10,\n    pool_pre_ping=True,\n)\n\n# Async session\nasync def get_async_session():\n    async with AsyncSession(async_engine) as session:\n        yield session\n```\n\n---\n\n## MySQL Configuration\n\n```python\nfrom sqlmodel import create_engine\nfrom sqlalchemy.pool import QueuePool\n\nDATABASE_URL = \"mysql+pymysql://user:password@localhost:3306/dbname\"\n\nengine = create_engine(\n    DATABASE_URL,\n    poolclass=QueuePool,\n    pool_size=5,\n    max_overflow=10,\n    pool_timeout=30,\n    pool_recycle=3600,     # MySQL default wait_timeout is 8 hours\n    pool_pre_ping=True,\n    echo=False,\n)\n```\n\n### MySQL Notes\n- Use `mysql+pymysql://` driver\n- Set `pool_recycle` < MySQL `wait_timeout`\n- Consider `charset=utf8mb4` for full Unicode support\n\n---\n\n## Environment-Based Configuration\n\nProduction pattern using environment variables.\n\n```python\nimport os\nfrom sqlmodel import create_engine\nfrom sqlalchemy.pool import QueuePool, NullPool\n\ndef get_engine():\n    \"\"\"Create engine based on environment.\"\"\"\n    database_url = os.getenv(\"DATABASE_URL\")\n    environment = os.getenv(\"ENVIRONMENT\", \"development\")\n\n    if environment == \"production\":\n        return create_engine(\n            database_url,\n            poolclass=QueuePool,\n            pool_size=int(os.getenv(\"DB_POOL_SIZE\", \"5\")),\n            max_overflow=int(os.getenv(\"DB_MAX_OVERFLOW\", \"10\")),\n            pool_timeout=30,\n            pool_recycle=1800,\n            pool_pre_ping=True,\n            echo=False,\n        )\n    elif environment == \"testing\":\n        return create_engine(\n            database_url or \"sqlite:///:memory:\",\n            poolclass=NullPool,  # No pooling for tests\n            echo=False,\n        )\n    else:  # development\n        return create_engine(\n            database_url or \"sqlite:///./dev.db\",\n            connect_args={\"check_same_thread\": False} if \"sqlite\" in (database_url or \"\") else {},\n            echo=True,\n        )\n\nengine = get_engine()\n```\n\n---\n\n## Connection Pool Types\n\n| Pool Type | Use Case |\n|-----------|----------|\n| `QueuePool` | Default, general purpose, maintains pool of connections |\n| `NullPool` | No pooling, new connection each time (tests, serverless) |\n| `StaticPool` | Single connection reused (SQLite in-memory) |\n| `SingletonThreadPool` | One connection per thread |\n| `AssertionPool` | Debug mode, ensures single connection |\n\n### When to Use Each\n\n```python\nfrom sqlalchemy.pool import QueuePool, NullPool, StaticPool\n\n# Web application (default)\nengine = create_engine(url, poolclass=QueuePool)\n\n# Serverless (Lambda, Cloud Functions)\nengine = create_engine(url, poolclass=NullPool)\n\n# SQLite in-memory testing\nengine = create_engine(\"sqlite:///:memory:\", poolclass=StaticPool)\n\n# Background workers with limited connections\nengine = create_engine(\n    url,\n    poolclass=QueuePool,\n    pool_size=2,\n    max_overflow=3,\n)\n```\n\n---\n\n## Engine Events and Hooks\n\n```python\nfrom sqlalchemy import event\n\nengine = create_engine(DATABASE_URL)\n\n@event.listens_for(engine, \"connect\")\ndef set_sqlite_pragma(dbapi_connection, connection_record):\n    \"\"\"Configure SQLite on each connection.\"\"\"\n    cursor = dbapi_connection.cursor()\n    cursor.execute(\"PRAGMA foreign_keys=ON\")\n    cursor.execute(\"PRAGMA journal_mode=WAL\")\n    cursor.close()\n\n@event.listens_for(engine, \"checkout\")\ndef receive_checkout(dbapi_connection, connection_record, connection_proxy):\n    \"\"\"Called when connection retrieved from pool.\"\"\"\n    pass\n\n@event.listens_for(engine, \"checkin\")\ndef receive_checkin(dbapi_connection, connection_record):\n    \"\"\"Called when connection returned to pool.\"\"\"\n    pass\n```\n\n---\n\n## Database Initialization\n\n```python\nfrom sqlmodel import SQLModel, create_engine\n\ndef init_db(engine):\n    \"\"\"Initialize database tables.\"\"\"\n    # Import all models to register them\n    from app.models import User, Order, Item  # noqa\n\n    # Create all tables\n    SQLModel.metadata.create_all(engine)\n\ndef drop_db(engine):\n    \"\"\"Drop all tables (use with caution).\"\"\"\n    SQLModel.metadata.drop_all(engine)\n\n# Usage\nif __name__ == \"__main__\":\n    engine = create_engine(DATABASE_URL)\n    init_db(engine)\n```\n\n### With Alembic (Recommended for Production)\n\n```python\n# Don't use create_all in production\n# Instead, use Alembic migrations\n\n# alembic.ini\n# [alembic]\n# script_location = alembic\n\n# alembic/env.py\nfrom app.models import SQLModel\ntarget_metadata = SQLModel.metadata\n\n# Generate migration\n# alembic revision --autogenerate -m \"Add user table\"\n\n# Apply migration\n# alembic upgrade head\n```\n",
        "skills/sqlmodel-orm-dbhelper/references/field-types.md": "# SQLModel Field Types\n\nComplete reference for field types, validators, and constraints.\n\n---\n\n## Basic Field Types\n\n### String Fields\n\n```python\nfrom sqlmodel import Field, SQLModel\nfrom sqlalchemy import Text, String\n\nclass Example(SQLModel, table=True):\n    # Fixed length string (VARCHAR)\n    name: str = Field(max_length=100)\n\n    # String with index\n    code: str = Field(max_length=50, index=True)\n\n    # Unique string\n    email: str = Field(max_length=255, unique=True, index=True)\n\n    # Nullable string\n    description: str | None = Field(default=None, max_length=500)\n\n    # Unlimited text (TEXT type)\n    content: str = Field(sa_type=Text)\n\n    # String with default\n    status: str = Field(default=\"active\", max_length=20)\n```\n\n### Numeric Fields\n\n```python\nfrom decimal import Decimal\nfrom sqlmodel import Field, SQLModel\nfrom sqlalchemy import Numeric\n\nclass Product(SQLModel, table=True):\n    # Integer primary key\n    id: int | None = Field(default=None, primary_key=True)\n\n    # Integer with index\n    quantity: int = Field(default=0, index=True)\n\n    # Float\n    rating: float = Field(default=0.0)\n\n    # Decimal for financial data (precise)\n    price: Decimal = Field(\n        default=Decimal(\"0.00\"),\n        sa_type=Numeric(precision=10, scale=2)\n    )\n\n    # Positive integer constraint\n    stock: int = Field(ge=0)  # Pydantic validation\n```\n\n### Boolean Fields\n\n```python\nclass User(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n\n    # Boolean with default\n    is_active: bool = Field(default=True)\n    is_verified: bool = Field(default=False)\n\n    # Indexed boolean for filtering\n    is_admin: bool = Field(default=False, index=True)\n```\n\n### Date and Time Fields\n\n```python\nfrom datetime import datetime, date, time\nfrom sqlmodel import Field, SQLModel\nfrom sqlalchemy import DateTime, Date, Time\n\nclass Event(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n\n    # DateTime with auto-now\n    created_at: datetime = Field(default_factory=datetime.utcnow)\n\n    # DateTime with timezone\n    scheduled_at: datetime = Field(sa_type=DateTime(timezone=True))\n\n    # Date only\n    event_date: date\n\n    # Time only\n    start_time: time\n\n    # Nullable datetime\n    completed_at: datetime | None = Field(default=None)\n\n    # Updated timestamp\n    updated_at: datetime = Field(\n        default_factory=datetime.utcnow,\n        sa_column_kwargs={\"onupdate\": datetime.utcnow}\n    )\n```\n\n### UUID Fields\n\n```python\nfrom uuid import UUID, uuid4\nfrom sqlmodel import Field, SQLModel\nfrom sqlalchemy.dialects.postgresql import UUID as PG_UUID\n\nclass Resource(SQLModel, table=True):\n    # UUID as primary key\n    id: UUID = Field(\n        default_factory=uuid4,\n        primary_key=True,\n        sa_type=PG_UUID(as_uuid=True)  # PostgreSQL native UUID\n    )\n\n    # UUID as regular field\n    external_id: UUID = Field(default_factory=uuid4, unique=True)\n```\n\n---\n\n## JSON Fields\n\n### Basic JSON\n\n```python\nfrom typing import Any\nfrom sqlmodel import Field, SQLModel\nfrom sqlalchemy import JSON\n\nclass Config(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n\n    # JSON field (any structure)\n    settings: dict = Field(default={}, sa_type=JSON)\n\n    # JSON with list\n    tags: list = Field(default=[], sa_type=JSON)\n\n    # Typed JSON (Pydantic validates)\n    metadata: dict[str, Any] = Field(default={}, sa_type=JSON)\n```\n\n### PostgreSQL JSONB (Queryable)\n\n```python\nfrom sqlalchemy.dialects.postgresql import JSONB\n\nclass Product(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n\n    # JSONB with GIN index for queries\n    attributes: dict = Field(\n        default={},\n        sa_type=JSONB,\n        sa_column_kwargs={\"index\": True}  # GIN index\n    )\n```\n\n### Querying JSON Fields\n\n```python\nfrom sqlmodel import select\nfrom sqlalchemy import cast, String\n\n# PostgreSQL JSONB query\nstatement = select(Product).where(\n    Product.attributes[\"color\"].astext == \"red\"\n)\n\n# JSON path query\nstatement = select(Product).where(\n    Product.attributes[\"specs\"][\"weight\"].astext.cast(Integer) > 100\n)\n```\n\n---\n\n## Enum Fields\n\n```python\nfrom enum import Enum\nfrom sqlmodel import Field, SQLModel\nfrom sqlalchemy import Enum as SAEnum\n\nclass OrderStatus(str, Enum):\n    PENDING = \"pending\"\n    PROCESSING = \"processing\"\n    SHIPPED = \"shipped\"\n    DELIVERED = \"delivered\"\n    CANCELLED = \"cancelled\"\n\nclass Order(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n\n    # Enum stored as string\n    status: OrderStatus = Field(default=OrderStatus.PENDING)\n\n    # Alternative: explicit SA Enum type\n    priority: str = Field(\n        default=\"normal\",\n        sa_type=SAEnum(\"low\", \"normal\", \"high\", name=\"priority_enum\")\n    )\n```\n\n---\n\n## Array Fields (PostgreSQL)\n\n```python\nfrom sqlalchemy.dialects.postgresql import ARRAY\nfrom sqlalchemy import String, Integer\n\nclass Article(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n\n    # Array of strings\n    tags: list[str] = Field(\n        default=[],\n        sa_type=ARRAY(String)\n    )\n\n    # Array of integers\n    category_ids: list[int] = Field(\n        default=[],\n        sa_type=ARRAY(Integer)\n    )\n```\n\n### Querying Arrays\n\n```python\nfrom sqlalchemy import any_, all_\n\n# Contains element\nstatement = select(Article).where(Article.tags.contains([\"python\"]))\n\n# Any element matches\nstatement = select(Article).where(\"python\" == any_(Article.tags))\n```\n\n---\n\n## Field Constraints\n\n### Primary Keys\n\n```python\nclass User(SQLModel, table=True):\n    # Auto-increment integer PK\n    id: int | None = Field(default=None, primary_key=True)\n\nclass Resource(SQLModel, table=True):\n    # UUID PK\n    id: UUID = Field(default_factory=uuid4, primary_key=True)\n\nclass OrderItem(SQLModel, table=True):\n    # Composite primary key\n    order_id: int = Field(primary_key=True, foreign_key=\"order.id\")\n    product_id: int = Field(primary_key=True, foreign_key=\"product.id\")\n```\n\n### Foreign Keys\n\n```python\nclass Order(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n\n    # Required foreign key\n    user_id: int = Field(foreign_key=\"user.id\", index=True)\n\n    # Optional foreign key\n    coupon_id: int | None = Field(\n        default=None,\n        foreign_key=\"coupon.id\",\n        index=True\n    )\n\n    # Foreign key with cascade\n    product_id: int = Field(\n        foreign_key=\"product.id\",\n        sa_column_kwargs={\"ondelete\": \"CASCADE\"}\n    )\n```\n\n### Unique Constraints\n\n```python\nfrom sqlalchemy import UniqueConstraint\n\nclass User(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n\n    # Single column unique\n    email: str = Field(unique=True, index=True)\n\n    # Composite unique constraint\n    __table_args__ = (\n        UniqueConstraint(\"org_id\", \"username\", name=\"uq_user_org_username\"),\n    )\n```\n\n### Check Constraints\n\n```python\nfrom sqlalchemy import CheckConstraint\n\nclass Product(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n    price: Decimal\n    discount_price: Decimal | None = None\n\n    __table_args__ = (\n        CheckConstraint(\"price >= 0\", name=\"ck_product_price_positive\"),\n        CheckConstraint(\n            \"discount_price IS NULL OR discount_price < price\",\n            name=\"ck_product_discount_less_than_price\"\n        ),\n    )\n```\n\n---\n\n## Pydantic Validators\n\nSQLModel inherits Pydantic's validation.\n\n```python\nfrom pydantic import field_validator, model_validator\nfrom sqlmodel import Field, SQLModel\n\nclass User(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n    email: str = Field(max_length=255)\n    username: str = Field(min_length=3, max_length=50)\n    age: int | None = Field(default=None, ge=0, le=150)\n\n    @field_validator(\"email\")\n    @classmethod\n    def validate_email(cls, v: str) -> str:\n        if \"@\" not in v:\n            raise ValueError(\"Invalid email format\")\n        return v.lower()\n\n    @field_validator(\"username\")\n    @classmethod\n    def validate_username(cls, v: str) -> str:\n        if not v.isalnum():\n            raise ValueError(\"Username must be alphanumeric\")\n        return v.lower()\n\n    @model_validator(mode=\"after\")\n    def validate_model(self):\n        # Cross-field validation\n        return self\n```\n\n---\n\n## Default Values\n\n```python\nfrom datetime import datetime\nfrom uuid import uuid4\n\nclass Item(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n\n    # Static default\n    status: str = Field(default=\"draft\")\n    count: int = Field(default=0)\n\n    # Factory default (called for each instance)\n    created_at: datetime = Field(default_factory=datetime.utcnow)\n    uuid: UUID = Field(default_factory=uuid4)\n\n    # Server default (database generates)\n    sequence: int = Field(\n        sa_column_kwargs={\"server_default\": \"nextval('item_seq'::regclass)\"}\n    )\n```\n\n---\n\n## Common Patterns\n\n### Timestamp Mixin\n\n```python\nfrom datetime import datetime\nfrom sqlmodel import Field, SQLModel\n\nclass TimestampMixin(SQLModel):\n    created_at: datetime = Field(\n        default_factory=datetime.utcnow,\n        nullable=False,\n        sa_column_kwargs={\"server_default\": \"now()\"}\n    )\n    updated_at: datetime = Field(\n        default_factory=datetime.utcnow,\n        nullable=False,\n        sa_column_kwargs={\n            \"server_default\": \"now()\",\n            \"onupdate\": datetime.utcnow\n        }\n    )\n```\n\n### Soft Delete Mixin\n\n```python\nclass SoftDeleteMixin(SQLModel):\n    is_deleted: bool = Field(default=False, index=True)\n    deleted_at: datetime | None = Field(default=None)\n\n    def soft_delete(self):\n        self.is_deleted = True\n        self.deleted_at = datetime.utcnow()\n```\n\n### Audit Mixin\n\n```python\nclass AuditMixin(TimestampMixin):\n    created_by: int | None = Field(default=None, foreign_key=\"user.id\")\n    updated_by: int | None = Field(default=None, foreign_key=\"user.id\")\n```\n",
        "skills/sqlmodel-orm-dbhelper/references/migrations.md": "# Database Migrations with Alembic\n\nProduction migration patterns for SQLModel projects.\n\n---\n\n## Why Migrations?\n\n| Method | Development | Production |\n|--------|-------------|------------|\n| `create_all()` | OK | **NOT SAFE** |\n| Alembic migrations | OK | **Recommended** |\n\n`create_all()` issues:\n- Won't modify existing tables\n- No rollback capability\n- No migration history\n- Can't handle data migrations\n\n---\n\n## Setup Alembic\n\n### Installation\n\n```bash\npip install alembic\n```\n\n### Initialize\n\n```bash\nalembic init alembic\n```\n\nCreates:\n```\nalembic/\n env.py           # Migration environment\n script.py.mako   # Migration template\n versions/        # Migration files\nalembic.ini          # Configuration\n```\n\n### Configure env.py\n\n```python\n# alembic/env.py\nfrom logging.config import fileConfig\nfrom sqlalchemy import engine_from_config, pool\nfrom alembic import context\n\n# Import SQLModel metadata\nfrom sqlmodel import SQLModel\n\n# Import all models to register them\nfrom app.models import User, Order, Item  # noqa\n\nconfig = context.config\n\nif config.config_file_name is not None:\n    fileConfig(config.config_file_name)\n\n# Use SQLModel metadata\ntarget_metadata = SQLModel.metadata\n\ndef run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\"\"\"\n    url = config.get_main_option(\"sqlalchemy.url\")\n    context.configure(\n        url=url,\n        target_metadata=target_metadata,\n        literal_binds=True,\n        dialect_opts={\"paramstyle\": \"named\"},\n    )\n\n    with context.begin_transaction():\n        context.run_migrations()\n\ndef run_migrations_online() -> None:\n    \"\"\"Run migrations in 'online' mode.\"\"\"\n    connectable = engine_from_config(\n        config.get_section(config.config_ini_section, {}),\n        prefix=\"sqlalchemy.\",\n        poolclass=pool.NullPool,\n    )\n\n    with connectable.connect() as connection:\n        context.configure(\n            connection=connection,\n            target_metadata=target_metadata,\n        )\n\n        with context.begin_transaction():\n            context.run_migrations()\n\nif context.is_offline_mode():\n    run_migrations_offline()\nelse:\n    run_migrations_online()\n```\n\n### Configure alembic.ini\n\n```ini\n[alembic]\nscript_location = alembic\nprepend_sys_path = .\nversion_path_separator = os\n\n[alembic:exclude]\n# Tables to exclude from autogenerate\ntables = alembic_version\n\nsqlalchemy.url = postgresql://user:password@localhost/dbname\n# Or use environment variable:\n# sqlalchemy.url = driver://%(DB_USER)s:%(DB_PASS)s@localhost/dbname\n```\n\n### Environment Variable Support\n\n```python\n# alembic/env.py - add at top\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# In run_migrations_online():\ndef get_url():\n    return os.getenv(\"DATABASE_URL\", config.get_main_option(\"sqlalchemy.url\"))\n\ndef run_migrations_online() -> None:\n    configuration = config.get_section(config.config_ini_section)\n    configuration[\"sqlalchemy.url\"] = get_url()\n    # ... rest of function\n```\n\n---\n\n## Creating Migrations\n\n### Auto-generate Migration\n\n```bash\n# Generate migration from model changes\nalembic revision --autogenerate -m \"Add user table\"\n\n# Creates: alembic/versions/xxxx_add_user_table.py\n```\n\n### Manual Migration\n\n```bash\n# Create empty migration\nalembic revision -m \"Add custom index\"\n```\n\n### Migration File Structure\n\n```python\n\"\"\"Add user table\n\nRevision ID: a1b2c3d4e5f6\nRevises:\nCreate Date: 2024-01-15 10:30:00.000000\n\"\"\"\nfrom typing import Sequence, Union\nfrom alembic import op\nimport sqlalchemy as sa\nimport sqlmodel\n\n# revision identifiers\nrevision: str = 'a1b2c3d4e5f6'\ndown_revision: Union[str, None] = None\nbranch_labels: Union[str, Sequence[str], None] = None\ndepends_on: Union[str, Sequence[str], None] = None\n\ndef upgrade() -> None:\n    op.create_table(\n        'user',\n        sa.Column('id', sa.Integer(), nullable=False),\n        sa.Column('email', sa.String(length=255), nullable=False),\n        sa.Column('name', sa.String(length=100), nullable=False),\n        sa.Column('created_at', sa.DateTime(), nullable=False),\n        sa.PrimaryKeyConstraint('id'),\n        sa.UniqueConstraint('email')\n    )\n    op.create_index('ix_user_email', 'user', ['email'])\n\ndef downgrade() -> None:\n    op.drop_index('ix_user_email', table_name='user')\n    op.drop_table('user')\n```\n\n---\n\n## Running Migrations\n\n### Apply Migrations\n\n```bash\n# Upgrade to latest\nalembic upgrade head\n\n# Upgrade to specific revision\nalembic upgrade a1b2c3d4e5f6\n\n# Upgrade +1 step\nalembic upgrade +1\n```\n\n### Rollback Migrations\n\n```bash\n# Downgrade -1 step\nalembic downgrade -1\n\n# Downgrade to specific revision\nalembic downgrade a1b2c3d4e5f6\n\n# Downgrade to base (empty database)\nalembic downgrade base\n```\n\n### Check Status\n\n```bash\n# Current revision\nalembic current\n\n# Migration history\nalembic history\n\n# Show pending migrations\nalembic history --indicate-current\n```\n\n---\n\n## Common Migration Operations\n\n### Add Column\n\n```python\ndef upgrade() -> None:\n    op.add_column('user', sa.Column('phone', sa.String(20), nullable=True))\n\ndef downgrade() -> None:\n    op.drop_column('user', 'phone')\n```\n\n### Add Non-Nullable Column (with data)\n\n```python\ndef upgrade() -> None:\n    # Add as nullable first\n    op.add_column('user', sa.Column('status', sa.String(20), nullable=True))\n\n    # Populate existing rows\n    op.execute(\"UPDATE user SET status = 'active' WHERE status IS NULL\")\n\n    # Make non-nullable\n    op.alter_column('user', 'status', nullable=False)\n\ndef downgrade() -> None:\n    op.drop_column('user', 'status')\n```\n\n### Rename Column\n\n```python\ndef upgrade() -> None:\n    op.alter_column('user', 'name', new_column_name='full_name')\n\ndef downgrade() -> None:\n    op.alter_column('user', 'full_name', new_column_name='name')\n```\n\n### Add Index\n\n```python\ndef upgrade() -> None:\n    op.create_index('ix_order_user_id', 'order', ['user_id'])\n\ndef downgrade() -> None:\n    op.drop_index('ix_order_user_id', table_name='order')\n```\n\n### Add Foreign Key\n\n```python\ndef upgrade() -> None:\n    op.add_column('order', sa.Column('user_id', sa.Integer(), nullable=False))\n    op.create_foreign_key(\n        'fk_order_user_id',\n        'order', 'user',\n        ['user_id'], ['id'],\n        ondelete='CASCADE'\n    )\n\ndef downgrade() -> None:\n    op.drop_constraint('fk_order_user_id', 'order', type_='foreignkey')\n    op.drop_column('order', 'user_id')\n```\n\n### Create Table with Relationships\n\n```python\ndef upgrade() -> None:\n    op.create_table(\n        'order',\n        sa.Column('id', sa.Integer(), nullable=False),\n        sa.Column('user_id', sa.Integer(), nullable=False),\n        sa.Column('total', sa.Numeric(10, 2), nullable=False),\n        sa.Column('status', sa.String(20), nullable=False, server_default='pending'),\n        sa.Column('created_at', sa.DateTime(), server_default=sa.func.now()),\n        sa.PrimaryKeyConstraint('id'),\n        sa.ForeignKeyConstraint(['user_id'], ['user.id'], ondelete='CASCADE'),\n    )\n    op.create_index('ix_order_user_id', 'order', ['user_id'])\n    op.create_index('ix_order_status', 'order', ['status'])\n```\n\n---\n\n## Data Migrations\n\n### Migrate Data Between Tables\n\n```python\nfrom sqlalchemy import text\n\ndef upgrade() -> None:\n    # Create new table\n    op.create_table('user_profile', ...)\n\n    # Migrate data\n    connection = op.get_bind()\n    connection.execute(text(\"\"\"\n        INSERT INTO user_profile (user_id, bio)\n        SELECT id, bio FROM user WHERE bio IS NOT NULL\n    \"\"\"))\n\n    # Remove old column\n    op.drop_column('user', 'bio')\n\ndef downgrade() -> None:\n    op.add_column('user', sa.Column('bio', sa.Text()))\n\n    connection = op.get_bind()\n    connection.execute(text(\"\"\"\n        UPDATE user SET bio = (\n            SELECT bio FROM user_profile WHERE user_profile.user_id = user.id\n        )\n    \"\"\"))\n\n    op.drop_table('user_profile')\n```\n\n### Batch Data Updates\n\n```python\ndef upgrade() -> None:\n    connection = op.get_bind()\n\n    # Process in batches for large tables\n    batch_size = 1000\n    offset = 0\n\n    while True:\n        result = connection.execute(text(f\"\"\"\n            UPDATE user\n            SET email = LOWER(email)\n            WHERE id IN (\n                SELECT id FROM user\n                WHERE email != LOWER(email)\n                LIMIT {batch_size}\n            )\n        \"\"\"))\n\n        if result.rowcount == 0:\n            break\n```\n\n---\n\n## Best Practices\n\n### Migration Naming\n\n```bash\n# Good: Descriptive\nalembic revision --autogenerate -m \"Add user email verification fields\"\nalembic revision --autogenerate -m \"Create order items table\"\n\n# Bad: Vague\nalembic revision --autogenerate -m \"Update\"\nalembic revision --autogenerate -m \"Changes\"\n```\n\n### Always Test Downgrade\n\n```bash\n# Test migration cycle\nalembic upgrade head\nalembic downgrade -1\nalembic upgrade head\n```\n\n### Review Auto-generated Migrations\n\nAuto-generate may miss:\n- Data migrations\n- Index names you want custom\n- Enum type changes\n- Check constraints\n\nAlways review before applying!\n\n### Production Deployment\n\n```bash\n# 1. Backup database\npg_dump -h host -U user dbname > backup.sql\n\n# 2. Test migration on staging\n\n# 3. Apply in maintenance window\nalembic upgrade head\n\n# 4. Verify\nalembic current\n```\n\n---\n\n## Troubleshooting\n\n### \"Target database is not up to date\"\n\n```bash\nalembic stamp head  # Mark current state as latest\n```\n\n### Multiple Heads\n\n```bash\n# Check for branch\nalembic heads\n\n# Merge branches\nalembic merge -m \"Merge branches\" rev1 rev2\n```\n\n### Failed Migration Recovery\n\n```bash\n# If migration partially applied:\n# 1. Manually fix database state\n# 2. Stamp to correct revision\nalembic stamp <last_successful_revision>\n```\n",
        "skills/sqlmodel-orm-dbhelper/references/performance.md": "# Performance Optimization\n\nAdvanced patterns for high-performance SQLModel applications.\n\n---\n\n## N+1 Query Problem\n\nThe most common performance issue with ORMs.\n\n### The Problem\n\n```python\n# BAD: N+1 queries\nusers = session.exec(select(User)).all()\nfor user in users:\n    print(user.orders)  # Each access triggers a new query!\n\n# Results in:\n# Query 1: SELECT * FROM user\n# Query 2: SELECT * FROM order WHERE user_id = 1\n# Query 3: SELECT * FROM order WHERE user_id = 2\n# ... N more queries\n```\n\n### The Solution: Eager Loading\n\n```python\nfrom sqlmodel import select\nfrom sqlalchemy.orm import selectinload, joinedload\n\n# GOOD: 2 queries total with selectinload\nstatement = select(User).options(selectinload(User.orders))\nusers = session.exec(statement).all()\n\n# Results in:\n# Query 1: SELECT * FROM user\n# Query 2: SELECT * FROM order WHERE user_id IN (1, 2, 3, ...)\n\n# GOOD: 1 query with joinedload (for single relations)\nstatement = select(Order).options(joinedload(Order.user))\norders = session.exec(statement).all()\n```\n\n### Choosing Loading Strategy\n\n| Scenario | Strategy | Why |\n|----------|----------|-----|\n| Loading collection (many children) | `selectinload` | Efficient IN query |\n| Loading single object | `joinedload` | Single JOIN query |\n| Nested collections | `selectinload` chained | Avoids cartesian product |\n| Optional relation | `joinedload` + `isouter=True` | LEFT JOIN |\n\n```python\n# Nested eager loading\nstatement = select(User).options(\n    selectinload(User.orders).selectinload(Order.items)\n)\n\n# Multiple relations\nstatement = select(User).options(\n    joinedload(User.profile),\n    selectinload(User.orders),\n)\n```\n\n---\n\n## Indexing Strategy\n\n### When to Add Indexes\n\n| Column Type | Index? | Reason |\n|-------------|--------|--------|\n| Primary key | Automatic | Always indexed |\n| Foreign key | **Yes** | JOIN performance |\n| Unique constraint | Automatic | Uniqueness enforced |\n| WHERE clause columns | **Yes** | Filter performance |\n| ORDER BY columns | Consider | Sort performance |\n| Columns in JOINs | **Yes** | Join performance |\n| Low cardinality | Usually no | Little benefit |\n\n### Index Types\n\n```python\nfrom sqlmodel import Field, SQLModel\nfrom sqlalchemy import Index\n\nclass Product(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n\n    # Single column index\n    sku: str = Field(index=True, unique=True)\n\n    # Index for filtering\n    category: str = Field(index=True)\n    status: str = Field(index=True)\n\n    # Price for range queries\n    price: float = Field(index=True)\n\n    # Composite index (defined at class level)\n    __table_args__ = (\n        Index(\"ix_product_category_status\", \"category\", \"status\"),\n        Index(\"ix_product_price_category\", \"price\", \"category\"),\n    )\n```\n\n### Composite Index Guidelines\n\n- Column order matters: put most selective first\n- Match query patterns: `WHERE category = ? AND status = ?`\n- Consider covering indexes for read-heavy queries\n\n---\n\n## Query Optimization\n\n### Select Only Needed Columns\n\n```python\nfrom sqlmodel import select, col\n\n# BAD: Selects all columns\nusers = session.exec(select(User)).all()\n\n# GOOD: Select specific columns\nstatement = select(User.id, User.name, User.email)\nresults = session.exec(statement).all()\n\n# Returns tuples, not User objects\nfor user_id, name, email in results:\n    print(f\"{name}: {email}\")\n```\n\n### Pagination\n\n```python\nfrom sqlmodel import select\n\n# Offset pagination (simple but slow for large offsets)\nstatement = select(User).offset(100).limit(20)\nusers = session.exec(statement).all()\n\n# Keyset pagination (more efficient for large datasets)\nlast_id = 100\nstatement = (\n    select(User)\n    .where(User.id > last_id)\n    .order_by(User.id)\n    .limit(20)\n)\nusers = session.exec(statement).all()\n```\n\n### Efficient Counting\n\n```python\nfrom sqlmodel import select, func\n\n# BAD: Loads all objects just to count\ncount = len(session.exec(select(User)).all())\n\n# GOOD: Database-side count\nstatement = select(func.count()).select_from(User)\ncount = session.exec(statement).one()\n\n# With filter\nstatement = select(func.count()).select_from(User).where(User.status == \"active\")\nactive_count = session.exec(statement).one()\n```\n\n### Exists Check\n\n```python\nfrom sqlmodel import select\nfrom sqlalchemy import exists\n\n# BAD: Loads object to check existence\nuser = session.exec(select(User).where(User.email == email)).first()\nif user:\n    ...\n\n# GOOD: EXISTS query\nstatement = select(exists().where(User.email == email))\nemail_exists = session.exec(statement).one()\n```\n\n---\n\n## Bulk Operations\n\n### Bulk Insert\n\n```python\nfrom sqlmodel import Session\n\n# BAD: Individual inserts\nfor data in items_data:\n    item = Item(**data)\n    session.add(item)\n    session.commit()  # N commits!\n\n# GOOD: Batch insert\nitems = [Item(**data) for data in items_data]\nsession.add_all(items)\nsession.commit()  # Single commit\n\n# BETTER: Bulk insert (bypasses ORM)\nfrom sqlalchemy import insert\n\nsession.exec(\n    insert(Item),\n    [{\"name\": \"A\", \"price\": 10}, {\"name\": \"B\", \"price\": 20}]\n)\nsession.commit()\n```\n\n### Bulk Update\n\n```python\nfrom sqlmodel import select, update\n\n# BAD: Load and update individually\nusers = session.exec(select(User).where(User.status == \"pending\")).all()\nfor user in users:\n    user.status = \"active\"\nsession.commit()\n\n# GOOD: Bulk update\nstatement = (\n    update(User)\n    .where(User.status == \"pending\")\n    .values(status=\"active\")\n)\nsession.exec(statement)\nsession.commit()\n```\n\n### Bulk Delete\n\n```python\nfrom sqlmodel import delete\n\n# BAD: Load and delete individually\nusers = session.exec(select(User).where(User.deleted == True)).all()\nfor user in users:\n    session.delete(user)\n\n# GOOD: Bulk delete\nstatement = delete(User).where(User.deleted == True)\nsession.exec(statement)\nsession.commit()\n```\n\n---\n\n## Connection Pool Tuning\n\n### Pool Size Calculation\n\n```\nOptimal pool_size = (Number of concurrent requests) / (Average query time in seconds * Requests per second)\n```\n\n### Monitoring Pool Health\n\n```python\nfrom sqlalchemy import event\n\n# Log pool statistics\n@event.listens_for(engine, \"checkout\")\ndef log_checkout(dbapi_conn, connection_rec, connection_proxy):\n    print(f\"Pool size: {engine.pool.size()}\")\n    print(f\"Checked out: {engine.pool.checkedout()}\")\n    print(f\"Overflow: {engine.pool.overflow()}\")\n```\n\n### Pool Configuration by Workload\n\n```python\n# High-throughput API\nengine = create_engine(\n    url,\n    pool_size=20,\n    max_overflow=30,\n    pool_timeout=10,  # Fail fast\n)\n\n# Background workers\nengine = create_engine(\n    url,\n    pool_size=2,\n    max_overflow=3,\n    pool_timeout=60,  # Can wait\n)\n\n# Read replicas\nread_engine = create_engine(\n    read_replica_url,\n    pool_size=30,\n    max_overflow=20,\n)\n```\n\n---\n\n## Caching Strategies\n\n### Query Result Caching\n\n```python\nfrom functools import lru_cache\nimport hashlib\n\n# Simple in-memory cache\n@lru_cache(maxsize=100)\ndef get_user_by_id(user_id: int) -> User | None:\n    with Session(engine) as session:\n        return session.get(User, user_id)\n\n# Redis caching (production)\nimport redis\nimport json\n\nredis_client = redis.Redis()\n\ndef get_user_cached(user_id: int) -> User | None:\n    cache_key = f\"user:{user_id}\"\n\n    # Try cache\n    cached = redis_client.get(cache_key)\n    if cached:\n        return User(**json.loads(cached))\n\n    # Query database\n    with Session(engine) as session:\n        user = session.get(User, user_id)\n        if user:\n            redis_client.setex(\n                cache_key,\n                300,  # 5 min TTL\n                user.model_dump_json()\n            )\n        return user\n```\n\n### Cache Invalidation\n\n```python\ndef update_user(user_id: int, data: dict):\n    with Session(engine) as session:\n        user = session.get(User, user_id)\n        for key, value in data.items():\n            setattr(user, key, value)\n        session.commit()\n\n        # Invalidate cache\n        redis_client.delete(f\"user:{user_id}\")\n```\n\n---\n\n## Query Analysis\n\n### Enable SQL Logging\n\n```python\n# Development only\nengine = create_engine(url, echo=True)\n\n# Or per-query\nimport logging\nlogging.getLogger(\"sqlalchemy.engine\").setLevel(logging.DEBUG)\n```\n\n### EXPLAIN Queries\n\n```python\nfrom sqlalchemy import text\n\n# PostgreSQL\nresult = session.exec(text(\"EXPLAIN ANALYZE SELECT * FROM user WHERE status = 'active'\"))\nfor row in result:\n    print(row)\n\n# Check for sequential scans on large tables\n# Add indexes if Seq Scan on large tables\n```\n\n### Query Timing\n\n```python\nimport time\nfrom contextlib import contextmanager\n\n@contextmanager\ndef query_timer(label: str):\n    start = time.perf_counter()\n    yield\n    elapsed = time.perf_counter() - start\n    print(f\"{label}: {elapsed:.3f}s\")\n\n# Usage\nwith query_timer(\"Load users with orders\"):\n    users = session.exec(\n        select(User).options(selectinload(User.orders))\n    ).all()\n```\n\n---\n\n## Anti-Patterns Summary\n\n| Anti-Pattern | Impact | Fix |\n|--------------|--------|-----|\n| N+1 queries | Exponential queries | Eager loading |\n| SELECT * | Excess data transfer | Select specific columns |\n| No indexes on FKs | Slow JOINs | Add indexes |\n| Large offset pagination | Full table scan | Keyset pagination |\n| Individual commits | Transaction overhead | Batch commits |\n| Missing pool_pre_ping | Stale connections | Enable pre-ping |\n| Unbounded queries | Memory exhaustion | Always use LIMIT |\n",
        "skills/sqlmodel-orm-dbhelper/references/relationships.md": "# SQLModel Relationships\n\nComplete patterns for defining relationships between models.\n\n---\n\n## One-to-Many Relationship\n\nThe most common relationship pattern.\n\n### Example: User has many Orders\n\n```python\nfrom typing import Optional, List\nfrom sqlmodel import Field, SQLModel, Relationship\n\nclass User(SQLModel, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    name: str\n    email: str = Field(unique=True, index=True)\n\n    # One-to-Many: User has many orders\n    orders: List[\"Order\"] = Relationship(back_populates=\"user\")\n\nclass Order(SQLModel, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    total: float\n    status: str = Field(default=\"pending\")\n\n    # Foreign key\n    user_id: Optional[int] = Field(default=None, foreign_key=\"user.id\", index=True)\n\n    # Many-to-One: Order belongs to User\n    user: Optional[\"User\"] = Relationship(back_populates=\"orders\")\n```\n\n### Key Points\n- Parent side: `List[\"Child\"]` with `Relationship(back_populates=\"parent_field\")`\n- Child side: `Optional[\"Parent\"]` with `Relationship(back_populates=\"children_field\")`\n- Foreign key on child side: `Field(foreign_key=\"parent_table.id\")`\n- Always index foreign keys for query performance\n\n---\n\n## One-to-One Relationship\n\nUse `uselist=False` on the parent side.\n\n### Example: User has one Profile\n\n```python\nfrom typing import Optional\nfrom sqlmodel import Field, SQLModel, Relationship\n\nclass User(SQLModel, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    name: str\n\n    # One-to-One: User has one profile\n    profile: Optional[\"Profile\"] = Relationship(\n        back_populates=\"user\",\n        sa_relationship_kwargs={\"uselist\": False}\n    )\n\nclass Profile(SQLModel, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    bio: Optional[str] = None\n    avatar_url: Optional[str] = None\n\n    # Foreign key (unique ensures one-to-one)\n    user_id: Optional[int] = Field(\n        default=None,\n        foreign_key=\"user.id\",\n        unique=True,\n        index=True\n    )\n\n    # Back reference\n    user: Optional[\"User\"] = Relationship(back_populates=\"profile\")\n```\n\n### Key Points\n- Use `uselist=False` in `sa_relationship_kwargs` on parent\n- Add `unique=True` to foreign key for database-level enforcement\n- Both sides use `Optional[\"Model\"]` (not List)\n\n---\n\n## Many-to-Many Relationship\n\nRequires a link/association table.\n\n### Example: Students enrolled in Courses\n\n```python\nfrom typing import Optional, List\nfrom sqlmodel import Field, SQLModel, Relationship\n\n# Link table (association table)\nclass StudentCourseLink(SQLModel, table=True):\n    student_id: Optional[int] = Field(\n        default=None, foreign_key=\"student.id\", primary_key=True\n    )\n    course_id: Optional[int] = Field(\n        default=None, foreign_key=\"course.id\", primary_key=True\n    )\n\nclass Student(SQLModel, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    name: str\n    email: str = Field(unique=True, index=True)\n\n    # Many-to-Many via link table\n    courses: List[\"Course\"] = Relationship(\n        back_populates=\"students\",\n        link_model=StudentCourseLink\n    )\n\nclass Course(SQLModel, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    name: str\n    code: str = Field(unique=True, index=True)\n\n    # Many-to-Many via link table\n    students: List[\"Student\"] = Relationship(\n        back_populates=\"courses\",\n        link_model=StudentCourseLink\n    )\n```\n\n### Link Table with Extra Fields\n\nWhen the relationship has attributes (e.g., enrollment date, grade):\n\n```python\nfrom datetime import datetime\nfrom typing import Optional, List\nfrom sqlmodel import Field, SQLModel, Relationship\n\nclass Enrollment(SQLModel, table=True):\n    \"\"\"Link table with additional fields\"\"\"\n    student_id: Optional[int] = Field(\n        default=None, foreign_key=\"student.id\", primary_key=True\n    )\n    course_id: Optional[int] = Field(\n        default=None, foreign_key=\"course.id\", primary_key=True\n    )\n\n    # Extra fields on the relationship\n    enrolled_at: datetime = Field(default_factory=datetime.utcnow)\n    grade: Optional[str] = None\n\n    # Relationships to access parent objects\n    student: Optional[\"Student\"] = Relationship(back_populates=\"enrollments\")\n    course: Optional[\"Course\"] = Relationship(back_populates=\"enrollments\")\n\nclass Student(SQLModel, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    name: str\n\n    enrollments: List[\"Enrollment\"] = Relationship(back_populates=\"student\")\n\n    @property\n    def courses(self) -> List[\"Course\"]:\n        return [e.course for e in self.enrollments]\n\nclass Course(SQLModel, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    name: str\n\n    enrollments: List[\"Enrollment\"] = Relationship(back_populates=\"course\")\n\n    @property\n    def students(self) -> List[\"Student\"]:\n        return [e.student for e in self.enrollments]\n```\n\n---\n\n## Self-Referential Relationship\n\nFor hierarchical data (e.g., categories, org charts).\n\n### Example: Category with Subcategories\n\n```python\nfrom typing import Optional, List\nfrom sqlmodel import Field, SQLModel, Relationship\n\nclass Category(SQLModel, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    name: str\n\n    # Self-referential foreign key\n    parent_id: Optional[int] = Field(default=None, foreign_key=\"category.id\", index=True)\n\n    # Parent reference\n    parent: Optional[\"Category\"] = Relationship(\n        back_populates=\"children\",\n        sa_relationship_kwargs={\"remote_side\": \"Category.id\"}\n    )\n\n    # Children collection\n    children: List[\"Category\"] = Relationship(back_populates=\"parent\")\n```\n\n### Example: Employee Manager Hierarchy\n\n```python\nclass Employee(SQLModel, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    name: str\n    title: str\n\n    manager_id: Optional[int] = Field(default=None, foreign_key=\"employee.id\", index=True)\n\n    manager: Optional[\"Employee\"] = Relationship(\n        back_populates=\"direct_reports\",\n        sa_relationship_kwargs={\"remote_side\": \"Employee.id\"}\n    )\n\n    direct_reports: List[\"Employee\"] = Relationship(back_populates=\"manager\")\n```\n\n---\n\n## Cascade Behaviors\n\nControl what happens when parent is deleted.\n\n```python\nclass User(SQLModel, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    name: str\n\n    # Cascade delete: when user deleted, delete all orders\n    orders: List[\"Order\"] = Relationship(\n        back_populates=\"user\",\n        sa_relationship_kwargs={\"cascade\": \"all, delete-orphan\"}\n    )\n\n    # Soft cascade: set to null on delete (requires nullable FK)\n    comments: List[\"Comment\"] = Relationship(\n        back_populates=\"user\",\n        sa_relationship_kwargs={\"cascade\": \"save-update, merge\"}\n    )\n```\n\n### Cascade Options\n\n| Option | Behavior |\n|--------|----------|\n| `save-update` | Cascade save/update (default) |\n| `merge` | Cascade merge operations |\n| `delete` | Delete children when parent deleted |\n| `delete-orphan` | Delete children when removed from collection |\n| `all` | All of the above |\n| `none` | No cascading |\n\n---\n\n## Lazy Loading Strategies\n\nConfigure how related objects are loaded.\n\n```python\nclass User(SQLModel, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    name: str\n\n    # Lazy load (default) - query when accessed\n    orders: List[\"Order\"] = Relationship(\n        back_populates=\"user\",\n        sa_relationship_kwargs={\"lazy\": \"select\"}\n    )\n\n    # Eager load with selectin - efficient for collections\n    reviews: List[\"Review\"] = Relationship(\n        back_populates=\"user\",\n        sa_relationship_kwargs={\"lazy\": \"selectin\"}\n    )\n\n    # Eager load with join - efficient for single objects\n    profile: Optional[\"Profile\"] = Relationship(\n        back_populates=\"user\",\n        sa_relationship_kwargs={\"lazy\": \"joined\", \"uselist\": False}\n    )\n```\n\n### Loading Strategy Guide\n\n| Strategy | SQL Queries | Best For |\n|----------|-------------|----------|\n| `select` | N+1 | Rarely accessed relations |\n| `selectin` | 2 | Collections, batch loading |\n| `joined` | 1 (with JOIN) | Single objects, always needed |\n| `subquery` | 2 | Complex nested collections |\n| `raise` | Error | Prevent accidental lazy loads |\n\n---\n\n## Query-Time Loading Override\n\nOverride lazy loading at query time:\n\n```python\nfrom sqlmodel import select\nfrom sqlalchemy.orm import selectinload, joinedload\n\n# Load users with their orders (override lazy setting)\nstatement = select(User).options(selectinload(User.orders))\nusers = session.exec(statement).all()\n\n# Nested eager loading\nstatement = select(User).options(\n    selectinload(User.orders).selectinload(Order.items)\n)\n\n# Multiple relations\nstatement = select(User).options(\n    joinedload(User.profile),\n    selectinload(User.orders)\n)\n```\n",
        "skills/study_notes/.claude-plugin/plugin.json": "{\n  \"name\": \"study-notes\",\n  \"description\": \"Study notes generator for learning and revision\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Ali Jilani\"\n  },\n  \"category\": \"learning\",\n  \"keywords\": [\"study\", \"notes\", \"learning\", \"education\", \"revision\"]\n}\n",
        "skills/study_notes/CLAUDE.md": "# A small caf business plan\n\n## Project Brief\n\nThis project aims to establish a **modern, high-efficiency** caf offering specialty coffee and *light* snacks in a high-traffic urban area. The caf will deliver a **cozy, productivity-friendly** environment designed for **students** and **professionals**. The setup includes branding, interior design, equipment procurement, staffing, and a marketing launch plan to ensure rapid customer acquisition and sustainable profitability.\n\n___\n\n### 1. Project Goal\n\nThis project aims to establish a **modern, high-efficiency** caf offering specialty coffee and *light* snacks in a high-traffic urban area. The caf will deliver a **cozy, productivity-friendly** environment designed for **students** and **professionals**. Below is a brief sketch of what we want to achieve.\n\n\n\n- **Cafe type:** Cozy **specialty coffee shop** with *light* snacks\n\n- **Target Market:** Students, professionals, [freelancers](https://fiverr.com)\n\n- **Brand Identity:** Minimal, modern, warm colors, strong aroma-based theme\n\n- **Menu Focus:** Espresso-based drinks, teas, pastries, sandwiches\n\n-------\n\n### 2. Concept & Branding\n\nThe main branding needs to resonate with the **modern, high-efficiency** buzz-words  and the drinks served must cater to client needs around the year. From coffees and teas for winter to *soft* drinks and juices in a *high-temperature* summer. The caf needs to deliver a **cozy and comfortable** environment designed for **students** and **professionals**. Below is a brief sketch of what we want to achieve.\n\n\n\n* **Setup Includes:** interior design, equipment, and staffing.\n  \n  - Marketing launch will drive early traffic **and brand awareness**. \n  \n  - The caf to offer **specialty coffee** and cozy seating.\n\n* **Target Market:** Students, professionals, freelancers\n\n* **Brand Identity:** Minimal, modern, warm colors, strong aroma-based theme\n\n* **Menu Focus:** *Espresso-based* drinks, teas, pastries, sandwiches\n\n-------\n\n### 3. Location & Layout\n\n1. High-footfall area (near offices/universities) \n\n2. 600900 sq. ft. space\n\n3. Zones:\n- Coffee counter + display\n* 1020 seating spots\n\n* Small kitchen/prep area\n\n* Takeaway window (optional) \n\n-----\n\n**4. Menu Development**\n--------------------------\n\n* Core beverages: latte, cappuccino, mocha, cold brew\n\n* Add-ons: syrups, plant-based milk\n\n* Pastries: croissants, brownies, muffins\n\n* Light meals: wraps, sandwiches, salads\n\n----\n\n**5. Marketing Plan**\n------------------------\n\n* Instagram + TikTok content\n\n* Launch-day discounts\n\n* Loyalty cards or QR code rewards\n\n* Collaborations with influencers\n\n* Branded takeaway cups\n  \n  \n\n### 6. Logo Design\n\n![Pencil & Paper](https://img.freepik.com/free-psd/3d-rendering-coffee-time-icon_23-2151362794.jpg?semt=ais_se_enriched&w=740&q=80 \"Project Brief\")\n",
        "skills/study_notes/study-system/CLAUDE.md": "# CLAUDE.md\r\n\r\n## Communication Style\r\n\r\nAlways maintain an **academic tone** in all responses. This includes:\r\n\r\n- Using formal, scholarly language appropriate for academic discourse\r\n- Avoiding colloquialisms, slang, and overly casual expressions\r\n- Structuring responses with clear logical flow and argumentation\r\n- Citing relevant concepts, theories, or frameworks where applicable\r\n- Maintaining objectivity and analytical rigor\r\n- Using precise terminology and defining technical terms when necessary\r\n- Presenting balanced perspectives and acknowledging limitations\r\n- Following conventions of academic writing (clear thesis statements, supporting evidence, logical conclusions)\r\n\r\n## Project Context\r\n\r\nThis is a study notes project. All generated content should be suitable for academic purposes and maintain scholarly standards.\r\n",
        "skills/study_notes/study-system/lessons/lesson1.md": "# Agent Skills: Teaching Claude New Capabilities\n\n## The Problem: Smart But Not Expert\n\nMeet Dr. Claude  a brilliant doctor who graduated top of their class. They know everything about medicine in general. But here's the problem:\n\n> **Patient**: \"Doctor, I need brain surgery.\"\n>\n> **Dr. Claude**: \"I understand what brain surgery is! I've read every textbook. But... I've never actually done one. I don't know where your hospital keeps the special tools. I don't know your hospital's specific procedures.\"\n\nDr. Claude is intelligent but lacks specialized, practical knowledge.\n\nThis is exactly what happens when you work with Claude Code on repeated tasks. You're working on multiple blog posts this week. Every time you start a new session, you give the same instructions:\n\n*\"Create an outline with 5 main sections, suggest 5 headline variations, make the introduction hook readers in the first sentence, keep paragraphs under 4 sentences...\"*\n\nBy the third blog post, you're frustrated. Why can't Claude just remember how you like blog posts structured?\n\nYou could use a subagentbut that requires explicitly saying \"Use the blog-planner subagent\" every time. What if Claude could automatically apply your blog-planning workflow whenever you mention writing a blog post?\n\n**That's what Skills solve.**\n\n## What Are Agent Skills?\n\n> **Simple Definition**: A Skill is a folder of instructions that teaches Claude Code how to do something specific really well.\n\nThink of it like this:\n\n| Without Skill | With Skill |\n|---------------|------------|\n| A chef who knows cooking theory | A chef with a detailed recipe book |\n| A driver who knows traffic rules | A driver with GPS navigation |\n| A student who knows math concepts | A student with step-by-step problem guides |\n\n**Key characteristic**: Skills are discovered autonomously. You create the skill once (SKILL.md file), and Claude applies it automatically when relevantno explicit invocation needed.\n\n## The Recipe Book Analogy\n\nImagine you're a brilliant chef who understands all cooking techniques. But someone asks you to make \"Grandma's Secret Biryani.\"\n\n### Without the recipe (No Skill):\n\n- You might make a biryani\n- It won't taste like Grandma's\n- You'll guess the spices\n- You might make mistakes\n\n### With the recipe (With Skill):\n\n- You follow exact steps\n- You use the right spices in the right amounts\n- The result is consistent every time\n- It tastes exactly like Grandma's!\n\nA Skill is like giving Claude Code the perfect recipe for a specific task.\n\n## How Skills Differ from Subagents\n\n| Aspect | Subagents | Skills |\n|--------|-----------|--------|\n| **Context** | Separate (isolated conversation) | Shared (main conversation) |\n| **Invocation** | Hard (\"Use X subagent\" guaranteed) | Soft (Claude decides when relevant) |\n| **Best For** | Complex, isolated tasks | Lightweight, repeatable capabilities |\n| **File Location** | `.claude/agents/name.md` | `.claude/skills/name/SKILL.md` |\n\n**Key Difference**: Subagents run in separate context windows with guaranteed invocation. Skills run in the main conversation and activate automatically when Claude detects relevance.\n\n**Use skills when**: Task is simple, repeatable, and doesn't need context isolation (blog planning, PDF extraction, note organizing)\n\n**Use subagents when**: Task is complex, needs guaranteed execution, or requires separate context (multi-step refactoring, comprehensive audits)\n\n### AI Colearning Prompt\n\n> *\"Explain the tradeoff between skills (automatic activation) and subagents (guaranteed invocation with isolated context). When would you choose one over the other?\"*\n\n## How Skills Work: The Three-Level Architecture\n\nHere's something clever about Skills: Claude doesn't read all Skills at once.\n\n**Why?** Because Claude has a limited memory (called \"context window\"). If it loaded every skill at the start, it would run out of space!\n\n### The Phone Apps Analogy\n\nThink of your smartphone:\n\n- You have 100 apps installed\n- Your phone doesn't run all 100 at once (it would crash!)\n- Apps stay closed until you tap on them\n- When you need one, it opens and loads\n\nSkills work the same way:\n\n- Claude has access to many skills\n- They stay \"closed\" until needed\n- When Claude needs one, it \"opens\" and reads the instructions\n\n### The Three Levels\n\n**Level 1: Brief Summary (Always Loaded)**\nWhen Claude Code starts, it sees short descriptions of all available skills. This helps Claude know WHEN to use each skill without loading full instructions.\n\n**Level 2: Full Instructions (On-Demand)**\nWhen Claude decides a skill is relevant, it loads the complete SKILL.md file with detailed instructions, workflows, and examples.\n\n**Level 3: Supporting Files (If Needed)**\nSkills can bundle scripts, reference docs, or tools in their directory. Claude accesses these when executing the skill.\n\n#### Example Structure:\n\n```\n.claude/skills/pdf-skill/\n SKILL.md                 # Main instructions\n scripts/\n    pdf_extractor.py     # Python extraction tool\n reference/\n     pdf-standards.md     # Technical specs\n```\n\n> **For your first skill**: Focus on Level 2 (SKILL.md with clear instructions). Add Level 3 (supporting files) only if your skill needs external tools or reference material.\n\n## What's Inside a Skill?\n\n### Skills are Just Folders!\n\nThis is the beautiful part  Skills are incredibly simple. They're just folders with files inside.\n\n```\n my-skill/\n\n  SKILL.md           REQUIRED: The main instructions\n\n  scripts/           OPTIONAL: Helper code\n    helper.py\n\n  references/        OPTIONAL: Documentation\n    guide.md\n\n  assets/            OPTIONAL: Templates, images\n     template.pptx\n```\n\nThe most important file is **SKILL.md**  it contains the instructions Claude reads.\n\n### The IKEA Furniture Analogy\n\nThink of buying IKEA furniture:\n\n| IKEA Furniture | Claude Code Skill |\n|----------------|-------------------|\n| The box | The skill folder |\n| Instruction manual | SKILL.md file |\n| Included tools | Helper scripts |\n| Picture of final result | Templates/examples |\n\nWhen you open the box, you read the manual and follow the steps. Same with Skills  Claude reads SKILL.md and follows those instructions!\n\n### Anatomy of SKILL.md\n\nEvery SKILL.md has two parts:\n\n#### Part 1: Frontmatter (The ID Card)\n\n```yaml\n---\nname: my-skill\ndescription: What this skill does and WHEN to use it\nversion: \"1.0.0\"\n---\n```\n\n#### Part 2: Body (The Instructions)\n\n```markdown\n# My Skill Name\n\n## Overview\nWhat this skill helps with...\n\n## How to Use\nStep-by-step instructions...\n```\n\n### When Claude Code Invokes Skills Automatically\n\nThree patterns trigger skill activation:\n\n1. **Content Type Recognition**: Upload PDF  pdf-skill activates\n2. **Task Request Recognition**: \"Write a blog post\"  blog-writer-skill activates\n3. **Explicit Request**: \"Use the blog-writer skill\"  Direct activation\n\n> **To see available skills**: Ask Claude \"What skills do you have?\" in any session. Skills are discovered through conversation, not system commands.\n\n### Expert Insight\n\n> *In AI-native development, skills encode reasoning patterns, not just commands. You're not teaching Claude \"run this script\"you're teaching \"when you see THIS context, apply THAT framework.\" This makes organizational intelligence transferable across projects and teams.*\n\n## Types of Skills\n\n### 1. Foundational Skills\n\n**What**: Basic capabilities that make Claude useful for common tasks\n\n**Examples**:\n- Creating Word documents (.docx)\n- Making PowerPoint presentations (.pptx)\n- Working with Excel spreadsheets (.xlsx)\n- Creating PDFs\n\n**Analogy**: These are like basic life skills  cooking, cleaning, driving. Everyone needs them!\n\n### 2. Third-Party Skills\n\n**What**: Skills that help Claude work with specific software or services\n\n**Examples**:\n- Navigating websites with browser tools\n- Understanding a Notion workspace\n- Working with specific APIs\n\n**Analogy**: These are like specialized certifications  learning to use Photoshop, getting a forklift license, or becoming certified in a specific software.\n\n### 3. Enterprise/Custom Skills\n\n**What**: Skills created by organizations for their specific needs\n\n**Examples**:\n- Your company's coding style guide\n- Internal documentation standards\n- Organization-specific workflows\n\n**Analogy**: These are like company training manuals  every company has their own way of doing things!\n\n## Skill Creation Rules\n\nBefore creating skills, remember these guidelines:\n\n### DO\n\n-  Be concise - Claude is smart, don't over-explain\n-  Use clear examples - Show, don't just tell\n-  Write a good description - This triggers the skill\n-  Focus on what Claude doesn't know - Your specific preferences\n\n### DON'T\n\n-  Create README files (unnecessary)\n-  Include installation guides (Claude doesn't need them)\n-  Write verbose explanations (wastes context space)\n-  Add irrelevant information (keep it focused)\n\n## Hands-On: Create Your First Custom Skill\n\nLet's create a blog planning skill.\n\n### Step 1: Create Directory Structure\n\n```bash\nmkdir -p .claude/skills/blog-planner\n```\n\n### Step 2: Create SKILL.md File\n\nCreate `.claude/skills/blog-planner/SKILL.md`:\n\n```markdown\n---\nname: \"blog-planner\"\ndescription: \"Help plan engaging blog posts: research topics, create outlines, suggest headlines, and draft compelling introductions. Use when user asks to plan or write blog content.\"\nversion: \"1.0.0\"\n---\n\n# Blog Planning Skill\n\n## When to Use This Skill\n\n- User asks to \"plan a blog post\" or \"write an article\"\n- User mentions blog topics, headlines, or content strategy\n- User needs help structuring written content\n\n## How This Skill Works\n\n1. **Research the topic**: Understand the subject and target audience\n2. **Create outline**: Break topic into 3-5 main sections\n3. **Suggest headlines**: Provide 5 compelling headline options\n4. **Draft introduction**: Write an engaging first paragraph that hooks readers\n\n## Output Format\n\nProvide:\n- **Topic Summary**: 2-3 sentence overview\n- **Target Audience**: Who should read this?\n- **Outline**: Numbered list of main sections\n- **Headline Options**: 5 variations (descriptive, curiosity-driven, benefit-focused)\n- **Introduction Draft**: 1-2 paragraph hook\n\n## Example\n\n**Input**: \"Help me plan a blog post about sustainable living\"\n\n**Output**:\n- **Topic Summary**: Practical sustainable living tips for busy professionals\n- **Target Audience**: Working adults wanting eco-friendly lifestyle changes\n- **Outline**:\n  1. Why sustainable living matters now\n  2. 5 easy swaps you can make today\n  3. Long-term sustainable habits\n  4. Common myths debunked\n  5. Resources for deeper learning\n- **Headlines**:\n  1. \"5 Sustainable Living Changes You Can Make This Weekend\"\n  2. \"Busy Professional's Guide to Eco-Friendly Living\"\n  3. \"Sustainable Living: Easier Than You Think\"\n- **Introduction**: \"You care about the environment, but who has time for complicated lifestyle changes? Good news: sustainable living doesn't require upending your entire routine. These five simple swaps take less than an hour to implementand they'll cut your environmental impact by 30%.\"\n```\n\n### Step 3: Test Your Skill\n\nStart Claude Code (`claude`), then ask:\n\n```\nHelp me plan a blog post about learning AI tools\n```\n\nClaude recognizes \"blog post\" trigger, loads the skill, and applies its workflow automatically.\n\n### Practice Exercise\n\n> *Ask your AI: \"I just created a blog-planner skill. Help me test it: suggest 3 blog topics related to [your interest area]. Then explain which parts of the skill activated and how Claude knew to use it.\"*\n\n**Expected Outcome**: You'll understand how skill descriptions trigger activation and see the skill in action with your own content.\n\n### Step 4: Refine Your Skill Through Co-Learning\n\nAsk Claude to review your skill:\n\n```\nReview the blog-planner skill. What could be improved?\nSuggest 2-3 specific enhancements.\n```\n\n**AI as Teacher**: Claude suggests improvements (better descriptions, additional sections)\n**You as Teacher**: You specify your constraints (\"headlines must be curiosity-driven, not clickbait\")\n**Convergence**: Together you refine the skill to match YOUR workflow\n\nTest the updated skill to validate improvements.\n\n## Bonus Example: Study Notes Creator Skill\n\nHere's another practical skill example for students:\n\n**Use Case**: You always want notes in a specific format  summary at top, key terms highlighted, questions at the end.\n\nCreate `.claude/skills/study-notes-creator/SKILL.md`:\n\n```markdown\n---\nname: \"study-notes-creator\"\ndescription: \"Creates well-organized study notes from any content. Use when the user asks for study notes, learning summaries, revision materials, or wants to understand a topic for studying.\"\nversion: \"1.0.0\"\n---\n\n# Study Notes Creator\n\n## Overview\nThis skill creates structured study notes that help students learn and retain information effectively.\n\n## Output Format\nEvery study note should follow this structure:\n\n### 1. Summary Section\n- Start with a 2-3 sentence overview\n- Highlight the main concept in bold\n\n### 2. Key Terms\n- List 5-10 important vocabulary words\n- Format: **Term** - Definition\n\n### 3. Main Content\n- Use bullet points for easy scanning\n- Include examples for complex ideas\n- Keep paragraphs short (3-4 sentences max)\n\n### 4. Practice Questions\n- Include 3-5 questions at the end\n- Mix of recall and understanding questions\n\n### 5. Quick Review Box\n- End with a \"Quick Review\" box\n- 3-5 bullet points summarizing key takeaways\n\n## Example Output\n\n**Topic: Photosynthesis**\n\n**Summary**\nPhotosynthesis is the process plants use to convert sunlight into food. It's essential for life on Earth.\n\n**Key Terms**\n- **Chlorophyll** - Green pigment that absorbs light\n- **Glucose** - Sugar produced by photosynthesis\n- **Carbon dioxide** - Gas absorbed from air\n\n**Main Content**\n[Content here...]\n\n**Practice Questions**\n1. What is the main purpose of photosynthesis?\n2. Where does photosynthesis occur in a plant?\n\n**Quick Review**\n- Plants make their own food\n- Requires sunlight, water, CO2\n- Produces glucose and oxygen\n```\n\n## More Skill Ideas for Practice\n\nApply the same pattern to create these skills:\n\n### Meeting Notes Organizer:\n\n```yaml\n---\nname: \"meeting-notes-organizer\"\ndescription: \"Transform messy meeting notes into structured summaries with action items, decisions, and follow-ups. Use when user shares meeting notes or transcripts.\"\nversion: \"1.0.0\"\n---\n```\n\n### Learning Path Designer:\n\n```yaml\n---\nname: \"learning-path-designer\"\ndescription: \"Create structured learning plans for any topic with progressive difficulty, resource recommendations, and practice exercises. Use when user wants to learn a new subject.\"\nversion: \"1.0.0\"\n---\n```\n\n### Code Review Skill:\n\n```yaml\n---\nname: \"code-reviewer\"\ndescription: \"Perform systematic code reviews checking security, performance, maintainability, and best practices. Use when user asks to review code.\"\nversion: \"1.0.0\"\n---\n```\n\nTry creating one of these skills using the blog-planner template as your guide.\n\n## Why This Matters: Reusable Organizational Capability\n\n### The Growing Garden Analogy\n\nThink of Claude Code as a garden:\n\n| Garden Element | Claude Code Equivalent |\n|----------------|------------------------|\n| The soil | Claude's base intelligence |\n| Seeds | Skills |\n| Water & sunlight | Your instructions |\n| Flowers/vegetables | The output/results |\n\n**More skills = A richer, more capable garden!**\n\n### Visual Summary\n\n```\n\n              CLAUDE CODE                    \n         (The Brilliant Brain)               \n                   +                         \n          \n     Skill 1   Skill 2   Skill 3     \n      docx      pptx       pdf       \n          \n                   =                         \n         POWERFUL AI ASSISTANT               \n    (Smart + Specialized Knowledge)          \n\n```\n\n**Workflow Impact**: Skills transform one-time solutions into persistent organizational intelligence. Solve a problem once (code review pattern, documentation style, testing strategy), encode it as a skill, and your entire team benefits automatically.\n\n**Paradigm Connection**: This is intelligence accumulation in action. Unlike code libraries (reuse implementation), skills reuse reasoning patterns. The \"how to think about X\" becomes transferable across projects.\n\n**Real-World Application**: Production teams create skills for domain-specific code reviews (security, performance), architectural pattern enforcement (API design, error handling), and documentation standards.\n\n**Link to Capstone**: In Lesson 9, you'll see how plugins bundle skillsturning your custom reasoning patterns into shareable marketplace capabilities.\n\n## Example: How a Skill Works in Practice\n\n**Scenario**: You need to extract invoice data from a PDF.\n\n### What You Do:\n\n```\nExtract these fields from invoice.pdf:\n- Invoice number\n- Date\n- Total amount\n- Vendor name\n```\n\n### What Happens (Behind the Scenes):\n\n1. Claude reads system prompt: \"pdf-skill available for PDF extraction\"\n2. Recognizes PDF context + extraction request\n3. Loads full SKILL.md with extraction instructions\n4. Executes the skill's workflow automatically\n5. Returns structured data\n\n### What You See:\n\n```\nExtracted Invoice Details:\n- Invoice #: INV-2024-00531\n- Date: November 13, 2024\n- Total: $2,450.00\n- Vendor: Tech Solutions Inc.\n```\n\nYou described what you wanted. Claude discovered the right skill and applied it automaticallyno explicit commands needed.\n\n## When to Use Skills vs Subagents vs Main Conversation\n\n### Use Skills When:\n\n- Task is predictable and repeatable (PDF extraction, blog planning, note organizing)\n- You want automatic application without explicit invocation\n- Multiple similar tasks in a session\n\n### Use Subagents When:\n\n- Task is complex with many context-sensitive variables\n- You need guaranteed invocation and isolated context\n- Task requires specialized, multi-step workflows\n\n### Use Main Conversation When:\n\n- One-off, exploratory work\n- Learning something new\n- No specialized capability exists yet\n\n## Key Takeaways\n\nRemember these 5 points:\n\n1. **Skills are folders**  Simple collections of files with instructions\n2. **Skills add expertise**  They give Claude specialized knowledge it doesn't have by default\n3. **Skills are loaded when needed**  Not everything loads at once (saves memory!)\n4. **Anyone can create Skills**  Even you, once you learn!\n5. **Skills make Claude better**  The more skills, the more capable the agent\n\n> **Summary**: Skills are instruction folders that transform Claude from a brilliant generalist into a specialized expert for specific tasks.\n\n### \"Claude is smart. Skills make it wise.\"\n\n## Reflection: From Commands to Intentions\n\nThink about this paradigm shift:\n\n### Traditional Development:\n\n- You know a command exists\n- You type it explicitly: `pdf_extract --input file.pdf --output json`\n- You manage tool invocation manually\n\n### AI-Native Development with Skills:\n\n- You describe what you want: \"Extract invoice data from this PDF\"\n- Claude discovers the right skill automatically\n- The system handles tool invocation\n\nThis shiftfrom \"command what exists\" to \"describe what you want\"is fundamental to AI-native development.\n\n> **Key Insight**: Skills don't just automate tasks. They encode reasoning patterns that make AI assistants smarter by default. When you create a skill, you're teaching Claude \"how to think about\" a domain, not just \"what commands to run.\"\n\n## Try With AI\n\nLet's design reusable skills that extend Claude Code's capabilities for your specific workflow.\n\n### Practice Designing a Custom Skill:\n\n> *\"I have a repeated task in my workflow: [describe your task: code reviews / blog drafting / meeting notes / API documentation]. Help me design an agent skill for it. Walk me through: What should the skill name and description be? When should Claude recognize it's relevant and auto-load it? What workflow should the skill follow? Create the complete SKILL.md file structure with YAML frontmatter and instructions.\"*\n\n### Understand Skills vs Subagents:\n\n> *\"Compare these two tasks: Task A) Draft weekly blog posts (happens 3x/week). Task B) Create comprehensive product launch strategy (one-time, complex). For each task, should I build a Skill or a Subagent? Explain the tradeoff between automatic activation (skills discover themselves) vs. guaranteed control (subagents invoked explicitly).\"*\n\n### Test Skill Discovery:\n\n> *\"I just created a [your skill name] skill and saved it to .claude/skills/. Help me test if Claude Code auto-loads it correctly. Give me a prompt that should trigger this skill's activation. Then explain how I can verify the skill is being used vs. Claude responding without the skill.\"*\n\n### Build a Skill Suite:\n\n> *\"I work on [describe your project type: web apps / data pipelines / documentation / etc.]. Help me identify 3-5 reusable skills I should create that would improve my workflow. For each skill, explain: what it does, when it activates, and why it's better as a skill than as a subagent or direct prompting.\"*\n",
        "skills/study_system/.claude-plugin/plugin.json": "{\n  \"name\": \"study-system\",\n  \"description\": \"Complete study system for organized learning and knowledge management\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Ali Jilani\"\n  },\n  \"category\": \"learning\",\n  \"keywords\": [\"study\", \"system\", \"learning\", \"organization\", \"knowledge\"]\n}\n",
        "skills/study_system/.claude/skills/flashcards-generator/SKILL.md": "---\nname: flashcards-generator\ndescription: Generate comprehensive educational flashcards based on Bloom's Taxonomy methodology (Remember, Understand, Apply, Analyze, Evaluate, Create). Creates structured flashcards with difficulty ratings, detailed explanations, and practice hints. Use when user requests flashcard generation, study cards, review materials, learning flashcards, mentions Bloom's Taxonomy, or provides educational topics for flashcard creation. Activates for study topics, course materials, reference files (.md, .txt, .pdf), or educational content requiring systematic review materials.\n---\n\n# Flashcards Generator\n\n## Purpose\n\nThis skill generates comprehensive educational flashcards systematically organized according to Bloom's Taxonomy cognitive levels. It produces structured review materials incorporating difficulty ratings, answer explanations, related concept references, and practice hints to facilitate deep learning and retention.\n\n## Bloom's Taxonomy Cognitive Levels\n\nThe flashcards shall be organized across six hierarchical cognitive levels:\n\n1. **Remember**: Retrieve relevant knowledge from long-term memory (recall, recognize, identify)\n2. **Understand**: Construct meaning from instructional messages (interpret, exemplify, classify, summarize, infer, compare, explain)\n3. **Apply**: Carry out or use a procedure in a given situation (execute, implement)\n4. **Analyze**: Break material into constituent parts and determine relationships (differentiate, organize, attribute)\n5. **Evaluate**: Make judgments based on criteria and standards (check, critique, judge)\n6. **Create**: Put elements together to form a coherent whole; reorganize into new pattern (generate, plan, produce, design)\n\n## Instructions\n\nWhen invoked, execute the following systematic procedure:\n\n### Phase 1: Input Acquisition and Analysis\n\n**Step 1.1**: Determine input source\n\nIdentify whether the user has provided:\n- A topic title (text-based subject specification)\n- A reference file path (existing educational material)\n- Both topic and supporting reference materials\n\n**Step 1.2**: Process reference materials (if applicable)\n\nIf a reference file is provided:\n- Use the Read tool to extract content from the specified file path\n- Analyze the content structure, key concepts, and learning objectives\n- Identify core terminology, principles, and relationships\n- Extract subject domain and complexity level\n\n**Step 1.3**: Conduct supplementary research (if necessary)\n\nIf the topic is unfamiliar or requires current information:\n- Employ WebSearch tool to locate authoritative educational resources\n- Use WebFetch tool to retrieve comprehensive explanatory content\n- Synthesize multiple sources to ensure accuracy and depth\n- Prioritize academic, educational, and authoritative domain sources\n\n### Phase 2: Content Analysis and Concept Mapping\n\n**Step 2.1**: Identify core concepts and learning objectives\n\nExtract or formulate:\n- Primary concepts requiring mastery\n- Fundamental terminology and definitions\n- Key principles, theories, or methodologies\n- Practical applications and use cases\n- Complex analytical relationships\n- Critical evaluation criteria\n- Creative synthesis opportunities\n\n**Step 2.2**: Assess content complexity\n\nDetermine appropriate difficulty distribution:\n- **Beginner**: Foundational concepts, basic terminology, simple recall\n- **Intermediate**: Conceptual understanding, application, comparative analysis\n- **Advanced**: Complex analysis, critical evaluation, creative synthesis\n\n**Step 2.3**: Map concepts to Bloom's Taxonomy levels\n\nSystematically categorize identified concepts according to cognitive complexity:\n- Level 1 (Remember): Facts, definitions, terminology, basic concepts\n- Level 2 (Understand): Explanations, interpretations, relationships, examples\n- Level 3 (Apply): Procedures, implementations, practical applications\n- Level 4 (Analyze): Component relationships, structural analysis, differentiations\n- Level 5 (Evaluate): Criteria-based judgments, critiques, assessments\n- Level 6 (Create): Novel solutions, designs, integrated syntheses\n\n### Phase 3: Flashcard Generation\n\n**Step 3.1**: Generate 5-8 flashcards per Bloom's level\n\nFor each cognitive level, create flashcards following this structure:\n\n```markdown\n### [Bloom's Level]: [Question]\n\n**Difficulty**: [Beginner | Intermediate | Advanced]\n\n**Answer**:\n[Concise, accurate response]\n\n**Explanation**:\n[Detailed elaboration providing context, rationale, and deeper understanding]\n\n**Related Concepts**:\n- [Concept 1]: [Brief relationship description]\n- [Concept 2]: [Brief relationship description]\n\n**Practice Hint** (for Analyze, Evaluate, Create levels):\n[Scaffolding guidance to approach the question systematically]\n```\n\n**Step 3.2**: Ensure cognitive alignment\n\nVerify each flashcard employs appropriate cognitive verbs:\n\n- **Remember**: Define, identify, list, name, recall, recognize, state\n- **Understand**: Describe, explain, interpret, paraphrase, summarize, classify\n- **Apply**: Apply, demonstrate, execute, implement, solve, use\n- **Analyze**: Analyze, compare, contrast, differentiate, distinguish, examine\n- **Evaluate**: Assess, critique, evaluate, judge, justify, recommend\n- **Create**: Create, design, develop, formulate, generate, synthesize\n\n**Step 3.3**: Distribute difficulty appropriately\n\nAcross all flashcards, maintain approximate distribution:\n- 40% Beginner (foundational understanding)\n- 40% Intermediate (application and analysis)\n- 20% Advanced (evaluation and creation)\n\n### Phase 4: Quality Assurance\n\n**Step 4.1**: Verify cognitive progression\n\nEnsure flashcards demonstrate hierarchical cognitive progression from basic recall to creative synthesis.\n\n**Step 4.2**: Validate accuracy\n\nConfirm all factual content is accurate, current, and appropriately sourced.\n\n**Step 4.3**: Assess pedagogical effectiveness\n\nVerify that:\n- Questions are clear, unambiguous, and appropriately scoped\n- Answers are accurate and sufficiently detailed\n- Explanations provide meaningful learning value\n- Related concepts enhance conceptual network\n- Practice hints facilitate problem-solving approaches\n\n### Phase 5: Output Formatting and Delivery\n\n**Step 5.1**: Structure the flashcard document\n\nOrganize the output markdown file with the following structure:\n\n```markdown\n# Flashcards: [Topic Title]\n\n**Generated**: [Current Date]\n**Cognitive Framework**: Bloom's Taxonomy\n**Total Flashcards**: [Count]\n\n---\n\n## Level 1: Remember (Recall Knowledge)\n\n[Flashcards 1-8 for Remember level]\n\n---\n\n## Level 2: Understand (Comprehension)\n\n[Flashcards 1-8 for Understand level]\n\n---\n\n## Level 3: Apply (Application)\n\n[Flashcards 1-8 for Apply level]\n\n---\n\n## Level 4: Analyze (Analysis)\n\n[Flashcards 1-8 for Analyze level]\n\n---\n\n## Level 5: Evaluate (Evaluation)\n\n[Flashcards 1-8 for Evaluate level]\n\n---\n\n## Level 6: Create (Synthesis)\n\n[Flashcards 1-8 for Create level]\n\n---\n\n## Study Recommendations\n\n[Provide brief guidance on how to use these flashcards effectively]\n```\n\n**Step 5.2**: Generate output file\n\nUse the Write tool to create a markdown file named:\n`[topic-name]-flashcards.md`\n\nWhere `[topic-name]` is the kebab-case version of the topic title.\n\n**Step 5.3**: Deliver completion summary\n\nProvide the user with:\n- Confirmation of successful generation\n- Total flashcard count (target: 30-48 flashcards)\n- File path for the generated flashcard set\n- Brief usage recommendations\n\n## Advanced Features Implementation\n\n### Difficulty Ratings\n\nAssign difficulty based on:\n- Cognitive level (higher levels tend toward intermediate/advanced)\n- Concept complexity (specialized terminology, abstract concepts)\n- Required prerequisite knowledge\n- Multi-step reasoning requirements\n\n### Answer Explanations\n\nElaborations shall:\n- Provide contextual background\n- Clarify reasoning processes\n- Connect to broader conceptual frameworks\n- Anticipate common misconceptions\n- Reference authoritative principles where applicable\n\n### Related Concepts\n\nIdentify and reference:\n- Prerequisite knowledge required\n- Parallel concepts in the same domain\n- Applications in different contexts\n- Contrasting or complementary principles\n\n### Practice Hints\n\nFor higher-order cognitive levels (Analyze, Evaluate, Create), provide:\n- Systematic approaches to problem decomposition\n- Criteria to consider in evaluation\n- Frameworks for generating novel solutions\n- Questions to guide thinking processes\n\n## Error Handling\n\n**Insufficient Input**:\nIf the user provides neither a clear topic nor reference file, request:\n- Specific topic title or subject area\n- Optional reference file path for context\n\n**Reference File Unavailable**:\nIf the specified reference file cannot be read:\n- Inform the user of the file access issue\n- Offer to proceed with topic-based generation using web research\n\n**Topic Unfamiliarity**:\nIf the topic is highly specialized or obscure:\n- Conduct thorough web research using WebSearch and WebFetch\n- Inform the user that research-based generation is in progress\n- Request user validation of accuracy for highly technical domains\n\n## Quality Standards\n\nAll generated flashcards shall conform to:\n\n1. **Bloom's Taxonomy Alignment**: Each flashcard correctly categorized by cognitive level\n2. **Pedagogical Soundness**: Questions facilitate genuine learning, not mere memorization\n3. **Factual Accuracy**: All content verified against authoritative sources\n4. **Clarity**: Questions and answers are unambiguous and well-articulated\n5. **Completeness**: All four components (question, answer, explanation, related concepts) present\n6. **Appropriate Complexity**: Difficulty ratings accurately reflect cognitive demands\n7. **Systematic Coverage**: 5-8 flashcards per level, balanced difficulty distribution\n\n## Example Invocation Scenarios\n\n**Scenario 1**: Topic-based generation\n```\nUser: \"Generate flashcards for Python list comprehensions\"\nAgent: [Conducts web research, generates 30-48 flashcards across Bloom's levels]\n```\n\n**Scenario 2**: Reference file-based generation\n```\nUser: \"Create flashcards from my notes on machine learning at notes/ml-basics.md\"\nAgent: [Reads file, extracts concepts, generates structured flashcards]\n```\n\n**Scenario 3**: Combined approach\n```\nUser: \"Generate flashcards for quantum computing based on lecture-notes.pdf\"\nAgent: [Reads PDF, supplements with web research, generates comprehensive flashcard set]\n```\n\n## References\n\nThis skill implements pedagogical principles derived from:\n- Bloom's Taxonomy of Educational Objectives (Bloom et al., 1956; Anderson & Krathwohl, 2001)\n- Cognitive science principles of spaced repetition and active recall\n- Evidence-based learning strategies for long-term retention\n",
        "skills/study_system/.claude/skills/flashcards-generator/examples.md": "# Flashcard Generation Examples\n\nThis document demonstrates sample outputs from the flashcards-generator skill across various educational domains and Bloom's Taxonomy levels.\n\n---\n\n## Example 1: Computer Science Topic - \"Binary Search Trees\"\n\n### Generated Flashcard Set (Partial)\n\n```markdown\n# Flashcards: Binary Search Trees\n\n**Generated**: 2025-12-19\n**Cognitive Framework**: Bloom's Taxonomy\n**Total Flashcards**: 42\n\n---\n\n## Level 1: Remember (Recall Knowledge)\n\n### Remember: What is a Binary Search Tree?\n\n**Difficulty**: Beginner\n\n**Answer**:\nA Binary Search Tree (BST) is a hierarchical data structure where each node has at most two children, and for each node, all values in its left subtree are less than the node's value, and all values in its right subtree are greater than the node's value.\n\n**Explanation**:\nBinary Search Trees organize data to enable efficient searching, insertion, and deletion operations. The ordering property (left < parent < right) is fundamental to BST functionality and distinguishes it from general binary trees. This property enables binary search algorithms to operate on tree structures, achieving O(log n) average time complexity for basic operations.\n\n**Related Concepts**:\n- Binary Tree: General tree structure with at most two children per node (BST is a specialized binary tree)\n- Tree Traversal: Methods for visiting nodes (inorder, preorder, postorder)\n- Balanced Trees: Variants like AVL trees and Red-Black trees that maintain height balance\n\n---\n\n### Remember: List the three types of tree traversal\n\n**Difficulty**: Beginner\n\n**Answer**:\n1. Inorder Traversal (Left, Root, Right)\n2. Preorder Traversal (Root, Left, Right)\n3. Postorder Traversal (Left, Right, Root)\n\n**Explanation**:\nTree traversal algorithms define the order in which nodes are visited. Inorder traversal of a BST produces values in sorted ascending order, making it particularly useful for retrieving sorted data. Preorder traversal processes the parent before children, useful for creating a copy of the tree. Postorder traversal processes children before the parent, useful for deletion operations.\n\n**Related Concepts**:\n- Depth-First Search (DFS): Traversal strategy that explores as far as possible before backtracking\n- Breadth-First Search (BFS): Level-order traversal visiting all nodes at each depth\n- Recursion: Common implementation technique for tree traversals\n\n---\n\n## Level 2: Understand (Comprehension)\n\n### Understand: Explain why inorder traversal of a BST produces sorted output\n\n**Difficulty**: Intermediate\n\n**Answer**:\nInorder traversal visits nodes in Left-Root-Right order. Due to the BST property (left subtree < root < right subtree), visiting the left subtree first ensures all smaller values are processed before the root. Then the root is visited, followed by the right subtree containing all larger values. This recursive pattern produces values in ascending sorted order.\n\n**Explanation**:\nThe sorted output property emerges from the combination of two factors: (1) the BST structural invariant that maintains ordering relationships, and (2) the inorder traversal sequence that respects this ordering. This relationship makes BSTs particularly efficient for operations requiring sorted data access. Understanding this connection is fundamental to recognizing when BSTs are appropriate data structures for specific applications.\n\n**Related Concepts**:\n- Structural Invariant: Properties that remain true throughout a data structure's lifetime\n- Recursion: The traversal recursively processes subtrees in the correct order\n- Sorting Algorithms: Inorder traversal provides O(n) sorted access if BST is balanced\n\n---\n\n### Understand: Compare Binary Search Trees with Hash Tables for data storage\n\n**Difficulty**: Intermediate\n\n**Answer**:\nBSTs maintain sorted order and support range queries efficiently, while Hash Tables provide faster average-case lookup (O(1) vs O(log n)) but do not maintain ordering. BSTs use O(n) space and have predictable worst-case performance with balancing. Hash Tables may waste space with low load factors and have unpredictable performance during resize operations.\n\n**Explanation**:\nThe choice between BSTs and Hash Tables depends on requirements:\n- Use BSTs when: ordered iteration, range queries, or predictable performance is needed\n- Use Hash Tables when: fastest average lookup is priority and ordering is unnecessary\n\nBSTs excel at operations like \"find all elements between x and y\" which are inefficient in Hash Tables. Conversely, Hash Tables excel at exact-match lookups but cannot efficiently answer ordering-dependent queries.\n\n**Related Concepts**:\n- Time-Space Tradeoffs: BSTs and Hash Tables make different optimization choices\n- Amortized Analysis: Hash Table resize costs amortized across many operations\n- Range Query: BSTs support efficient range searches via tree structure\n\n---\n\n## Level 3: Apply (Application)\n\n### Apply: Insert the values [15, 10, 20, 8, 12, 17, 25] into an empty BST\n\n**Difficulty**: Intermediate\n\n**Answer**:\n```\n       15\n      /  \\\n    10    20\n   / \\    / \\\n  8  12  17  25\n```\n\nStarting with 15 as root:\n- 10 < 15  left of 15\n- 20 > 15  right of 15\n- 8 < 15, 8 < 10  left of 10\n- 12 < 15, 12 > 10  right of 10\n- 17 > 15, 17 < 20  left of 20\n- 25 > 15, 25 > 20  right of 20\n\n**Explanation**:\nBST insertion follows the search path: compare the new value with the current node, move left if smaller, right if larger, until reaching a null position. The order of insertion determines the tree structure. These same values inserted in different order (e.g., [8, 10, 12, 15, 17, 20, 25]) would create a degenerate linear tree, demonstrating that insertion order significantly impacts BST balance and performance.\n\n**Related Concepts**:\n- Tree Balance: The resulting tree is relatively balanced (height = 2)\n- Search Path: Insertion follows the same path that searching for the value would take\n- Degenerate Trees: Sorted input produces unbalanced linear trees without balancing mechanisms\n\n**Practice Hint**:\nFor each value, start at the root and compare: go left if smaller, right if larger. When you reach a null child pointer, insert the new node there.\n\n---\n\n### Apply: Calculate the height of a BST with 15 nodes in the best and worst cases\n\n**Difficulty**: Intermediate\n\n**Answer**:\n- **Best Case (Balanced)**: height = log(15) = 3\n  (A complete binary tree with 15 nodes has height 3)\n\n- **Worst Case (Degenerate)**: height = 15 - 1 = 14\n  (A linear chain where each node has only one child)\n\n**Explanation**:\nTree height fundamentally determines operation time complexity. In a balanced BST, height grows logarithmically with node count, yielding O(log n) operations. In a degenerate tree (essentially a linked list), height equals node count, degrading to O(n) operations. This dramatic performance difference motivates self-balancing tree variants (AVL, Red-Black) that maintain logarithmic height through rotations.\n\n**Related Concepts**:\n- Complete Binary Tree: All levels filled except possibly the last, which fills left-to-right\n- Self-Balancing Trees: Maintain O(log n) height through rebalancing operations\n- Big-O Notation: Height determines asymptotic time complexity of tree operations\n\n**Practice Hint**:\nBest case: use the formula height = log(n) for a complete binary tree. Worst case: height = n - 1 for a linear chain.\n\n---\n\n## Level 4: Analyze (Analysis)\n\n### Analyze: Examine the time complexity of BST operations in balanced vs unbalanced trees\n\n**Difficulty**: Advanced\n\n**Answer**:\n**Balanced BST** (height h = O(log n)):\n- Search: O(log n)\n- Insert: O(log n)\n- Delete: O(log n)\n- Traversal: O(n)\n\n**Unbalanced BST** (worst case height h = O(n)):\n- Search: O(n)\n- Insert: O(n)\n- Delete: O(n)\n- Traversal: O(n)\n\nThe height of the tree directly determines operation complexity for search, insert, and delete, as each operation traverses a path from root to leaf/insertion point.\n\n**Explanation**:\nThis analysis reveals that BST performance degrades significantly when unbalanced. The discrepancy between O(log n) and O(n) represents the difference between processing 1 million elements in ~20 steps versus 1 million stepsa factor of 50,000 performance difference. This analysis motivates:\n\n1. **Randomized input**: Random insertion order tends toward balanced trees\n2. **Self-balancing variants**: AVL and Red-Black trees guarantee O(log n) height\n3. **Periodic rebalancing**: Rebuilding unbalanced BSTs to restore performance\n\nThe invariant that traversal remains O(n) regardless of balance makes sense because traversal must visit every node exactly once.\n\n**Related Concepts**:\n- Algorithmic Complexity: How operation time scales with input size\n- Probabilistic Analysis: Random insertion order produces expected O(log n) height\n- Self-Balancing Trees: Guarantee worst-case O(log n) through structural invariants\n\n**Practice Hint**:\nConsider the path length from root to the deepest leaf. This height h determines how many comparisons are needed to reach any node.\n\n---\n\n### Analyze: Differentiate between BST deletion cases for nodes with 0, 1, and 2 children\n\n**Difficulty**: Advanced\n\n**Answer**:\n**Case 1: Node with 0 children (leaf)**\n- Simply remove the node by setting parent's pointer to null\n- No restructuring required\n- Complexity: O(h) for finding the node\n\n**Case 2: Node with 1 child**\n- Replace the node with its single child\n- Bypass the deleted node in the tree structure\n- Complexity: O(h) for finding and replacing\n\n**Case 3: Node with 2 children**\n- Find inorder successor (smallest node in right subtree) or predecessor (largest in left subtree)\n- Copy successor's value to the node being deleted\n- Delete the successor (which has at most 1 child, reducing to Case 1 or 2)\n- Complexity: O(h) for finding node + O(h) for finding successor\n\n**Explanation**:\nThe complexity of deletion stems from maintaining the BST property. Cases 0 and 1 are straightforward because removal doesn't break ordering. Case 2 is complex because the deleted node has two subtrees that must both remain in the tree. The inorder successor is optimal because:\n\n1. It's the next value in sorted order, maintaining BST property when substituted\n2. It's guaranteed to have at most one child (only right child), simplifying its removal\n3. Symmetric reasoning applies to inorder predecessor (only left child possible)\n\n**Related Concepts**:\n- Inorder Successor: The next node in inorder traversal sequence\n- Structural Invariant Preservation: Deletion must maintain left < root < right property\n- Recursive Problem Reduction: Case 3 reduces to Cases 1 or 2 after successor swap\n\n**Practice Hint**:\nDraw the three cases on paper. For Case 3, identify the inorder successor by going right once, then left until reaching a leaf or node with only a right child.\n\n---\n\n## Level 5: Evaluate (Evaluation)\n\n### Evaluate: Assess whether a BST or Hash Table is more appropriate for implementing a symbol table in a compiler\n\n**Difficulty**: Advanced\n\n**Answer**:\n**Recommendation**: **BST** is more appropriate for compiler symbol tables.\n\n**Justification**:\n1. **Ordered Access**: Compilers often need to iterate over symbols in lexicographic order for scope resolution and error reporting\n2. **Range Queries**: Finding all symbols in a particular scope or namespace is efficient with BSTs\n3. **Predictable Performance**: BST operations have consistent O(log n) behavior (if balanced), important for compiler predictability\n4. **Memory Efficiency**: BSTs don't require pre-allocated hash table space or load factor management\n5. **Scope Nesting**: BST structure naturally represents hierarchical scope relationships\n\n**Hash Table Limitations in This Context**:\n- No ordered iteration capability\n- Cannot efficiently find all symbols matching a prefix\n- Unpredictable resize operations during compilation\n- No natural representation of scope hierarchy\n\n**Explanation**:\nWhile Hash Tables offer faster average lookup (O(1) vs O(log n)), compilers require more than simple lookup. They must:\n- Resolve symbols in nested scopes (benefit from hierarchical structure)\n- Generate ordered symbol tables for debugging information\n- Perform prefix matching for autocomplete and error suggestions\n- Maintain predictable compile-time performance\n\nModern compiler implementations often use hybrid approaches: Hash Tables for fast lookup within a single scope, combined with tree structures for scope nesting.\n\n**Related Concepts**:\n- Symbol Table: Compiler data structure mapping identifiers to their properties\n- Scope Resolution: Process of determining which declaration an identifier refers to\n- Lexicographic Ordering: Alphabetical ordering important for deterministic compiler output\n\n**Practice Hint**:\nConsider not just lookup speed, but all operations the application requires: iteration order, range queries, memory usage, and performance predictability.\n\n---\n\n### Evaluate: Critique the decision to use an unbalanced BST versus a self-balancing AVL tree for a database index\n\n**Difficulty**: Advanced\n\n**Answer**:\n**Critique**: Using an unbalanced BST for a database index is **inappropriate** except in highly specialized scenarios.\n\n**Critical Problems**:\n1. **Performance Degradation**: Sequential insertions (common in databases) create degenerate O(n) trees\n2. **Unpredictable Query Times**: Range from O(log n) to O(n), violating database performance guarantees\n3. **Index Corruption**: Unbalanced trees provide no structural guarantees after arbitrary insert/delete sequences\n\n**AVL Tree Advantages**:\n1. **Guaranteed O(log n)**: All operations maintain logarithmic worst-case complexity\n2. **Consistent Performance**: Essential for database query optimization and planning\n3. **Better Cache Locality**: Balanced trees exhibit more predictable memory access patterns\n\n**Possible Justification for Unbalanced BST** (Rare):\n- Read-only index built once from random data: balancing overhead unnecessary\n- Extremely small datasets where O(n) vs O(log n) difference is negligible\n- Memory-constrained embedded systems where balancing overhead is prohibitive\n\n**Explanation**:\nDatabase indexes must provide reliable performance across arbitrary workloads. Unbalanced BSTs offer no worst-case guarantees, making query optimization impossible. Modern databases universally use balanced structures (B-trees, B+ trees) for indexes. Even academic database systems would use AVL or Red-Black trees rather than plain BSTs.\n\nThe marginal memory savings (no balance factors or color bits) are insignificant compared to the performance risk. A database that slows from millisecond to second response times due to degenerate indexes is fundamentally broken.\n\n**Related Concepts**:\n- B-trees: Self-balancing trees optimized for disk-based storage (most database indexes)\n- Query Optimization: Database planning requires predictable index performance characteristics\n- Worst-Case Guarantees: Production systems must handle pathological inputs gracefully\n\n**Practice Hint**:\nWhen evaluating data structure choices, consider: (1) worst-case performance requirements, (2) expected workload characteristics, (3) memory constraints, and (4) operational complexity.\n\n---\n\n## Level 6: Create (Synthesis)\n\n### Create: Design a variant BST that supports efficient \"find the k-th smallest element\" queries\n\n**Difficulty**: Advanced\n\n**Answer**:\n**Design**: **Augmented BST with Subtree Size Tracking**\n\n**Structure**:\nEach node stores:\n```\nstruct Node {\n    int value;\n    Node* left;\n    Node* right;\n    int size;  // Number of nodes in this subtree (including self)\n}\n```\n\n**Algorithm for finding k-th smallest**:\n```\nfunction findKthSmallest(node, k):\n    leftSize = (node.left != null) ? node.left.size : 0\n\n    if k == leftSize + 1:\n        return node.value  // This node is the k-th smallest\n    else if k <= leftSize:\n        return findKthSmallest(node.left, k)  // k-th is in left subtree\n    else:\n        return findKthSmallest(node.right, k - leftSize - 1)  // k-th is in right subtree\n```\n\n**Time Complexity**: O(h) where h is tree height (O(log n) if balanced)\n\n**Maintenance**:\n- **Insert**: Increment size for all nodes along insertion path\n- **Delete**: Decrement size for all nodes along deletion path\n- **Space Overhead**: One integer per node\n\n**Explanation**:\nThis design augments standard BST with subtree size information, enabling O(log n) k-th smallest queries without full traversal. The algorithm exploits the BST property:\n- Left subtree contains exactly `leftSize` elements smaller than current node\n- If k  leftSize, the k-th element is in left subtree\n- If k = leftSize + 1, current node is the k-th element\n- If k > leftSize + 1, recursively search right subtree for (k - leftSize - 1)-th element\n\nAlternative designs (with tradeoffs):\n- **Threaded BST**: Faster inorder traversal but more complex pointer management\n- **Implicit Array BST**: No size storage but requires complete binary tree\n- **External Index**: Separate array of sorted values but requires O(n) rebuild on updates\n\n**Related Concepts**:\n- Data Structure Augmentation: Adding metadata to enable new operations efficiently\n- Order Statistics: Finding k-th smallest/largest element in a collection\n- Space-Time Tradeoffs: Additional space (size fields) enables faster queries\n\n**Practice Hint**:\nStart by considering how you'd solve this with inorder traversal (O(n)). Then ask: what information could I store to avoid visiting every node? The size metadata lets you skip entire subtrees.\n\n---\n\n### Create: Develop a testing strategy to verify BST implementation correctness\n\n**Difficulty**: Advanced\n\n**Answer**:\n**Comprehensive Testing Strategy for BST Implementation**\n\n**Phase 1: Structural Invariant Testing**\n```\nTest Suite 1: BST Property Validation\n- Test: Verify inorder traversal produces sorted output\n- Test: Check every node satisfies: left.value < node.value < right.value\n- Test: Validate no duplicate values (if duplicates disallowed)\n- Implementation: Recursive invariant checker\n```\n\n**Phase 2: Functional Correctness Testing**\n```\nTest Suite 2: Basic Operations\nInsert:\n  - Insert into empty tree (becomes root)\n  - Insert left child (value < root)\n  - Insert right child (value > root)\n  - Insert duplicate (reject or handle according to specification)\n\nSearch:\n  - Search for existing value (return true/node)\n  - Search for non-existing value (return false/null)\n  - Search in empty tree (return false/null)\n\nDelete:\n  - Delete leaf node (0 children)\n  - Delete node with 1 child (left only, right only)\n  - Delete node with 2 children\n  - Delete root node\n  - Delete from single-node tree\n  - Delete non-existing value (no-op or error)\n```\n\n**Phase 3: Edge Case Testing**\n```\nTest Suite 3: Boundary Conditions\n- Empty tree operations (search, delete, traversal)\n- Single-node tree operations\n- Large datasets (10,000+ nodes)\n- Integer boundary values (MIN_INT, MAX_INT)\n- Sequential insertion (1,2,3,4,5...) - tests degenerate tree handling\n```\n\n**Phase 4: Property-Based Testing**\n```\nTest Suite 4: Invariant Properties\nProperty 1: Insert then Search always succeeds\n  - Insert random value X, then search for X  must return true\n\nProperty 2: Delete then Search fails\n  - Insert value X, delete X, search for X  must return false\n\nProperty 3: Size consistency\n  - After N inserts of unique values: tree size = N\n  - After N inserts then M deletes: tree size = N - M\n\nProperty 4: Traversal completeness\n  - Inorder traversal visits exactly N nodes for N-node tree\n  - All inserted values appear exactly once in traversal\n```\n\n**Phase 5: Performance Testing**\n```\nTest Suite 5: Complexity Verification\n- Measure operation time for balanced tree: verify O(log n)\n- Measure operation time for degenerate tree: verify O(n)\n- Compare with theoretical complexity predictions\n```\n\n**Phase 6: Stress Testing**\n```\nTest Suite 6: Robustness\n- Randomized insert/delete sequences (10,000 operations)\n- Concurrent operations (if thread-safe implementation)\n- Memory leak detection (all nodes properly deallocated)\n```\n\n**Explanation**:\nThis multi-phase strategy ensures:\n1. **Correctness**: Structural invariants maintained after all operations\n2. **Completeness**: All code paths exercised (branches, edge cases)\n3. **Performance**: Operations exhibit expected time complexity\n4. **Robustness**: Handles edge cases and stress conditions\n\nProperty-based testing is particularly valuable for tree structures because it verifies invariants across arbitrary operation sequences rather than testing specific scenarios.\n\n**Test Implementation Example**:\n```python\ndef verify_bst_property(node):\n    \"\"\"Recursively verify BST invariant\"\"\"\n    if node is None:\n        return True\n\n    # Check left subtree\n    if node.left and node.left.value >= node.value:\n        return False\n\n    # Check right subtree\n    if node.right and node.right.value <= node.value:\n        return False\n\n    # Recursively verify subtrees\n    return (verify_bst_property(node.left) and\n            verify_bst_property(node.right))\n```\n\n**Related Concepts**:\n- Invariant Testing: Verifying structural properties remain true across operations\n- Property-Based Testing: Testing universal properties rather than specific examples\n- Test Coverage: Ensuring all code paths are exercised by test suite\n\n**Practice Hint**:\nOrganize tests hierarchically: start with simple single-operation tests, then combine operations, then test properties that should hold across arbitrary sequences. Use both white-box (code coverage) and black-box (specification) testing approaches.\n\n---\n\n## Study Recommendations\n\n**Spaced Repetition Schedule**:\n1. **Day 1**: Review all flashcards across all levels\n2. **Day 3**: Review Level 1-3 flashcards\n3. **Day 7**: Review Level 4-6 flashcards\n4. **Day 14**: Review all flashcards, focusing on those answered incorrectly\n5. **Day 30**: Final comprehensive review\n\n**Progressive Mastery Approach**:\n- Master Remember and Understand levels before advancing to Apply\n- Master Apply and Analyze before advancing to Evaluate and Create\n- Higher levels require fluency with lower-level knowledge\n\n**Active Learning Strategies**:\n- For Apply, Analyze: Work through problems on paper before checking answers\n- For Evaluate: Form your own judgment before reading the provided assessment\n- For Create: Attempt your own design before reviewing the example solution\n\n**Self-Assessment**:\n- If you cannot answer a Remember/Understand question: review concept definitions\n- If you cannot answer an Apply/Analyze question: practice more examples\n- If you cannot answer an Evaluate/Create question: study underlying principles and criteria\n```\n\n---\n\n## Example 2: Biology Topic - \"Photosynthesis\" (Partial Set)\n\n### Sample Flashcards\n\n```markdown\n# Flashcards: Photosynthesis\n\n**Generated**: 2025-12-19\n**Cognitive Framework**: Bloom's Taxonomy\n**Total Flashcards**: 36\n\n---\n\n## Level 1: Remember (Recall Knowledge)\n\n### Remember: What is the chemical equation for photosynthesis?\n\n**Difficulty**: Beginner\n\n**Answer**:\n6CO + 6HO + light energy  CHO + 6O\n\n(Carbon dioxide + Water + Light energy  Glucose + Oxygen)\n\n**Explanation**:\nThis equation represents the overall process of photosynthesis, where plants convert carbon dioxide and water into glucose (a simple sugar) and oxygen using light energy. The process is endergonic (requires energy input) and occurs primarily in chloroplasts of plant cells. This equation simplifies a complex series of light-dependent and light-independent reactions.\n\n**Related Concepts**:\n- Cellular Respiration: The reverse process (CHO + 6O  6CO + 6HO + ATP)\n- Chloroplasts: Organelles where photosynthesis occurs\n- Endergonic Reaction: Chemical process requiring energy input\n\n---\n\n## Level 3: Apply (Application)\n\n### Apply: Calculate the theoretical oxygen production from 100 grams of CO in photosynthesis\n\n**Difficulty**: Intermediate\n\n**Answer**:\n**Step 1**: Calculate moles of CO\n- Molecular weight of CO = 44 g/mol\n- Moles of CO = 100g  44 g/mol = 2.27 moles\n\n**Step 2**: Apply stoichiometry from equation 6CO  6O\n- Molar ratio CO:O = 1:1\n- Moles of O produced = 2.27 moles\n\n**Step 3**: Convert to grams\n- Molecular weight of O = 32 g/mol\n- Mass of O = 2.27 mol  32 g/mol = **72.7 grams**\n\n**Explanation**:\nThis calculation applies stoichiometry to the photosynthesis equation. The 1:1 molar ratio between CO consumed and O produced is a direct consequence of the balanced chemical equation. In practice, actual oxygen production may be lower due to:\n- Incomplete reactions\n- Oxygen used in plant respiration\n- Environmental limiting factors (light, temperature, water availability)\n\n**Related Concepts**:\n- Stoichiometry: Quantitative relationships between reactants and products\n- Limiting Reactants: Factor that restricts reaction rate (often light or CO)\n- Gas Exchange: O release and CO uptake through stomata\n\n**Practice Hint**:\nAlways start with the balanced equation, convert masses to moles, apply molar ratios, then convert back to desired units.\n\n---\n\n## Level 5: Evaluate (Evaluation)\n\n### Evaluate: Assess the effectiveness of increasing CO concentration in greenhouses to enhance plant growth\n\n**Difficulty**: Advanced\n\n**Answer**:\n**Assessment**: **Moderately Effective** with important constraints and considerations.\n\n**Advantages**:\n1. **Increased Photosynthesis Rate**: Higher CO (800-1200 ppm vs ambient 420 ppm) can increase photosynthetic rate by 20-50%\n2. **Improved Water Use Efficiency**: Plants partially close stomata at high CO, reducing water loss\n3. **Commercial Feasibility**: Widely used in greenhouse agriculture with documented yield improvements\n\n**Limitations and Risks**:\n1. **Diminishing Returns**: Beyond ~1200 ppm, further increases provide minimal benefit\n2. **Other Limiting Factors**: Light, nutrients, temperature often become limiting before CO is fully utilized\n3. **Cost Considerations**: CO injection systems require capital investment and operational costs\n4. **Plant-Specific Responses**: C plants (wheat, rice) benefit more than C plants (corn, sugarcane)\n5. **Human Safety**: Very high CO levels (>5000 ppm) pose health risks to greenhouse workers\n\n**Recommendation**:\nCO enrichment is justified when:\n- Growing high-value crops (tomatoes, cucumbers, flowers)\n- Other growth factors are optimized (adequate light, nutrients, water)\n- Greenhouse is well-sealed to prevent CO loss\n- Cost-benefit analysis shows positive return on investment\n\n**Explanation**:\nThe effectiveness of CO enrichment depends on the principle of limiting factors: increasing one input (CO) only improves growth if other factors are not limiting. Liebig's Law of the Minimum states that growth is controlled by the scarcest resource. Thus, adding CO to a light-limited or nitrogen-deficient greenhouse provides minimal benefit.\n\nC plants (most vegetables) lack efficient CO concentrating mechanisms and respond well to enrichment. C plants have evolved biochemical adaptations for efficient CO capture and show less response to elevated CO.\n\n**Related Concepts**:\n- Limiting Factors: Environmental conditions that constrain growth rate\n- C vs C Photosynthesis: Different carbon fixation pathways with varying CO efficiency\n- Cost-Benefit Analysis: Economic evaluation of agricultural interventions\n\n**Practice Hint**:\nWhen evaluating agricultural interventions, consider: (1) biological effectiveness, (2) economic feasibility, (3) limiting factor interactions, (4) crop-specific responses, and (5) unintended consequences.\n\n```\n\n---\n\n## Example 3: Mathematics Topic - \"Derivatives\" (Partial Set)\n\n### Sample Flashcards\n\n```markdown\n# Flashcards: Derivatives in Calculus\n\n**Generated**: 2025-12-19\n**Cognitive Framework**: Bloom's Taxonomy\n**Total Flashcards**: 40\n\n---\n\n## Level 2: Understand (Comprehension)\n\n### Understand: Explain the geometric interpretation of a derivative\n\n**Difficulty**: Intermediate\n\n**Answer**:\nThe derivative f'(x) at a point represents the slope of the tangent line to the function f(x) at that point. It quantifies the instantaneous rate of change of the function with respect to x.\n\n**Explanation**:\nGeometrically, as we take the limit of the slope of secant lines (connecting two points on the curve) as the points approach each other, we obtain the slope of the tangent line at a single point. This tangent line represents the best linear approximation to the function at that location.\n\nThe derivative transitions from the average rate of change (slope between two points) to the instantaneous rate of change (slope at one point) through the limiting process:\n\nf'(x) = lim[h0] (f(x+h) - f(x))/h\n\nThis geometric interpretation makes derivatives applicable to real-world problems involving rates: velocity as the derivative of position, acceleration as the derivative of velocity, marginal cost as the derivative of total cost, etc.\n\n**Related Concepts**:\n- Tangent Line: Line that touches a curve at exactly one point locally\n- Rate of Change: How one quantity changes relative to another\n- Linear Approximation: Using tangent line to estimate function values near a point\n\n---\n\n## Level 4: Analyze (Analysis)\n\n### Analyze: Compare the power rule, product rule, and chain rule in terms of when each applies\n\n**Difficulty**: Advanced\n\n**Answer**:\n**Power Rule**: f(x) = x  f'(x) = nx\n- **When**: Function is a single variable raised to a constant power\n- **Structure**: Simple monomial term\n- **Example**: f(x) = x  f'(x) = 5x\n\n**Product Rule**: f(x) = g(x)h(x)  f'(x) = g'(x)h(x) + g(x)h'(x)\n- **When**: Function is the product of two functions\n- **Structure**: Multiplication of two separate expressions\n- **Example**: f(x) = xsin(x)  f'(x) = 2xsin(x) + xcos(x)\n\n**Chain Rule**: f(x) = g(h(x))  f'(x) = g'(h(x))h'(x)\n- **When**: Function is a composition (function of a function)\n- **Structure**: Nested functions where one function's output is another's input\n- **Example**: f(x) = sin(x)  f'(x) = cos(x)2x\n\n**Comparison Analysis**:\n- **Structural Recognition**: Power rule for single terms, product rule for multiplication, chain rule for composition\n- **Complexity**: Power rule is simplest (one operation), product rule requires two derivative evaluations, chain rule requires identifying inner/outer functions\n- **Combination**: Complex functions often require multiple rules simultaneously\n- **Priority**: Chain rule often applied last (working from outside in)\n\n**Explanation**:\nDistinguishing these rules requires analyzing function structure:\n1. Look for composition first (one function inside another)  Chain rule\n2. Look for multiplication of separate functions  Product rule\n3. Look for simple power of x  Power rule\n\nExample combining all three: f(x) = xsin(x)\n- Product rule (x and sin(x) are multiplied)\n- Power rule for x derivative\n- Chain rule for sin(x) derivative\n\n**Related Concepts**:\n- Function Composition: f(g(x)) where output of g becomes input of f\n- Quotient Rule: Extension of product rule for division\n- Derivative Linearity: Derivative of sum equals sum of derivatives\n\n**Practice Hint**:\nDraw a tree diagram showing function structure: branches indicate multiplication (product rule), nesting indicates composition (chain rule), and single term with power uses power rule.\n\n---\n\n## Level 6: Create (Synthesis)\n\n### Create: Design a real-world optimization problem that requires derivative analysis and solve it\n\n**Difficulty**: Advanced\n\n**Answer**:\n**Problem Design**: **Optimal Can Dimensions for Minimum Material Cost**\n\n**Scenario**:\nA beverage company must manufacture cylindrical aluminum cans with a volume of 355 mL (standard soda can). Aluminum costs $0.002 per cm. Design the can dimensions (radius r and height h) that minimize material cost while maintaining the required volume.\n\n**Mathematical Formulation**:\n\n**Constraint** (Volume):\nV = rh = 355 cm\nTherefore: h = 355/(r)\n\n**Objective Function** (Surface Area to minimize):\nS = 2r + 2rh (top + bottom + side)\n\nSubstitute h:\nS(r) = 2r + 2r(355/(r))\nS(r) = 2r + 710/r\n\n**Optimization**:\nFind minimum by taking derivative and setting to zero:\ndS/dr = 4r - 710/r\n\nSet dS/dr = 0:\n4r = 710/r\n4r = 710\nr = 710/(4)  56.52\nr  3.84 cm\n\nCalculate h:\nh = 355/(3.84)  7.67 cm\n\n**Verify Minimum** (Second Derivative Test):\ndS/dr = 4 + 1420/r > 0 for r > 0\nTherefore this is indeed a minimum.\n\n**Solution**:\nOptimal dimensions: r  3.84 cm, h  7.67 cm\nSurface area: S  285.8 cm\nMaterial cost: 285.8 cm  $0.002/cm = **$0.57 per can**\n\n**Interesting Observation**:\nThe optimal ratio h/r  2, meaning height should be approximately twice the radius for material efficiency. Real soda cans differ slightly (r  3 cm, h  12.3 cm) because:\n1. Ergonomic considerations (ease of holding)\n2. Structural strength requirements\n3. Aesthetic preferences\n4. Stacking efficiency in shipping\n\n**Explanation**:\nThis problem demonstrates the complete optimization workflow:\n1. **Identify variables**: r and h\n2. **Establish constraint**: Fixed volume relates r and h\n3. **Formulate objective**: Surface area as function of single variable\n4. **Apply calculus**: Find critical points via derivative\n5. **Verify optimality**: Second derivative test confirms minimum\n6. **Interpret results**: Compare mathematical optimum to practical reality\n\nThe problem illustrates how calculus provides optimal solutions, but real-world implementations balance mathematical optimality with practical constraints.\n\n**Related Concepts**:\n- Constrained Optimization: Optimizing objective while satisfying constraints\n- Critical Points: Values where derivative equals zero (potential extrema)\n- Second Derivative Test: Confirms whether critical point is minimum or maximum\n\n**Practice Hint**:\nFor optimization problems: (1) draw a diagram, (2) write constraint and objective equations, (3) eliminate variables using constraints, (4) differentiate objective function, (5) find critical points, (6) verify minimum/maximum, (7) interpret in original context.\n\n```\n\n---\n\n## Usage Notes\n\nThese examples demonstrate:\n\n1. **Bloom's Taxonomy Progression**: Flashcards advance from simple recall to complex synthesis\n2. **Comprehensive Feature Implementation**: All flashcards include difficulty, answer, explanation, related concepts, and (where applicable) practice hints\n3. **Domain Diversity**: Examples span Computer Science, Biology, and Mathematics\n4. **Pedagogical Soundness**: Questions facilitate genuine understanding rather than mere memorization\n5. **Real-World Relevance**: Higher-order questions connect to practical applications\n\nWhen generating flashcards for new topics, emulate this structure and level of detail to ensure maximum learning value.\n",
        "skills/study_system/.claude/skills/flashcards-generator/reference.md": "# Bloom's Taxonomy Reference Guide\n\n## Theoretical Foundation\n\nBloom's Taxonomy represents a hierarchical framework for categorizing educational learning objectives according to cognitive complexity. Originally developed by Benjamin Bloom and collaborators in 1956, the taxonomy was revised by Anderson and Krathwohl in 2001 to reflect contemporary understanding of cognitive processes.\n\nThe framework posits that learning progresses through six cumulative cognitive levels, from lower-order thinking skills (remembering and understanding) to higher-order thinking skills (analyzing, evaluating, and creating). Each level builds upon the competencies developed at previous levels, forming a comprehensive scaffolding for educational objective design.\n\n## Cognitive Level Specifications\n\n### Level 1: Remember (Knowledge Recall)\n\n**Definition**: The retrieval of relevant knowledge from long-term memory storage.\n\n**Cognitive Processes**:\n- **Recognizing**: Identifying previously learned information from presented alternatives\n- **Recalling**: Retrieving relevant knowledge from memory without prompting\n\n**Characteristic Cognitive Verbs**:\nDefine, duplicate, list, memorize, recall, repeat, reproduce, state, identify, name, recognize, select, label, match, choose, find, describe, locate, retrieve\n\n**Question Formulation Patterns**:\n- \"What is the definition of [concept]?\"\n- \"List the [components/steps/elements] of [system/process].\"\n- \"Identify the [characteristic/property] of [entity].\"\n- \"Recall the [formula/rule/principle] for [scenario].\"\n\n**Flashcard Design Considerations**:\n- Focus on factual accuracy and precision\n- Employ straightforward question-answer format\n- Test recognition and recall equally\n- Include terminology, dates, formulas, classifications\n- Difficulty typically: Beginner to Intermediate\n\n**Example Questions**:\n- \"Define the term 'polymorphism' in object-oriented programming.\"\n- \"List the four fundamental principles of object-oriented design.\"\n- \"What is the chemical formula for glucose?\"\n\n---\n\n### Level 2: Understand (Comprehension)\n\n**Definition**: The construction of meaning from instructional messages, whether oral, written, or graphic.\n\n**Cognitive Processes**:\n- **Interpreting**: Converting information from one representational form to another\n- **Exemplifying**: Finding specific instances or examples of a concept\n- **Classifying**: Determining category membership for concepts\n- **Summarizing**: Abstracting a general theme or major points\n- **Inferring**: Drawing logical conclusions from presented information\n- **Comparing**: Detecting correspondences between ideas or objects\n- **Explaining**: Constructing cause-and-effect models of systems\n\n**Characteristic Cognitive Verbs**:\nClarify, classify, compare, conclude, contrast, demonstrate, describe, differentiate, discuss, distinguish, estimate, explain, extend, give examples, illustrate, infer, interpret, outline, paraphrase, predict, relate, rephrase, restate, summarize, translate\n\n**Question Formulation Patterns**:\n- \"Explain the relationship between [concept A] and [concept B].\"\n- \"Compare and contrast [entity 1] with [entity 2].\"\n- \"Summarize the main idea of [theory/process/system].\"\n- \"Provide an example of [concept] in [context].\"\n- \"What can be inferred from [data/observation]?\"\n\n**Flashcard Design Considerations**:\n- Require explanation beyond mere recall\n- Test conceptual relationships and connections\n- Include interpretation and translation tasks\n- Employ examples and non-examples\n- Difficulty typically: Beginner to Intermediate\n\n**Example Questions**:\n- \"Explain why encapsulation is important in software design.\"\n- \"Compare inheritance and composition in object-oriented programming.\"\n- \"Summarize the Central Limit Theorem in your own words.\"\n\n---\n\n### Level 3: Apply (Application)\n\n**Definition**: The execution or implementation of a procedure in a given situation.\n\n**Cognitive Processes**:\n- **Executing**: Carrying out a familiar procedure\n- **Implementing**: Applying a procedure to an unfamiliar task\n\n**Characteristic Cognitive Verbs**:\nApply, calculate, carry out, change, choose, compute, construct, demonstrate, develop, discover, dramatize, employ, execute, illustrate, implement, interpret, manipulate, modify, operate, practice, predict, prepare, produce, relate, show, sketch, solve, use, utilize\n\n**Question Formulation Patterns**:\n- \"Apply [principle/method] to solve [problem].\"\n- \"Calculate [result] given [conditions].\"\n- \"Implement [algorithm/procedure] for [scenario].\"\n- \"Demonstrate how [technique] works in [context].\"\n- \"Use [tool/formula] to determine [outcome].\"\n\n**Flashcard Design Considerations**:\n- Present novel scenarios requiring procedural application\n- Include problem-solving exercises\n- Test transfer of learned procedures to new contexts\n- Provide sufficient context for application\n- Difficulty typically: Intermediate\n\n**Example Questions**:\n- \"Apply the Pythagorean theorem to find the distance between points (3,4) and (7,1).\"\n- \"Implement a binary search algorithm to find element 42 in the sorted array [10, 23, 35, 42, 58, 71].\"\n- \"Use the SOLID principles to refactor the following code snippet.\"\n\n---\n\n### Level 4: Analyze (Analysis)\n\n**Definition**: The decomposition of material into constituent parts and the determination of relationships among parts and to overall structure or purpose.\n\n**Cognitive Processes**:\n- **Differentiating**: Distinguishing relevant from irrelevant parts\n- **Organizing**: Determining systematic relationships among elements\n- **Attributing**: Determining underlying intentions, biases, or perspectives\n\n**Characteristic Cognitive Verbs**:\nAnalyze, appraise, break down, categorize, compare, contrast, correlate, deconstruct, deduce, differentiate, discriminate, distinguish, examine, experiment, identify, illustrate, infer, investigate, outline, point out, question, relate, research, separate, subdivide, test\n\n**Question Formulation Patterns**:\n- \"Analyze the relationship between [component A] and [component B] in [system].\"\n- \"Differentiate between [approach 1] and [approach 2] in terms of [criteria].\"\n- \"Examine the underlying assumptions in [argument/theory].\"\n- \"What factors contribute to [phenomenon/outcome]?\"\n- \"Break down [complex concept] into its constituent elements.\"\n\n**Flashcard Design Considerations**:\n- Require systematic decomposition of complex concepts\n- Test ability to identify relationships and patterns\n- Include comparative analysis tasks\n- Examine structural and functional relationships\n- Difficulty typically: Intermediate to Advanced\n\n**Example Questions**:\n- \"Analyze the time complexity of the quicksort algorithm in best, average, and worst cases.\"\n- \"Differentiate between correlation and causation in the context of statistical analysis.\"\n- \"Examine the architectural trade-offs between microservices and monolithic design patterns.\"\n\n---\n\n### Level 5: Evaluate (Evaluation)\n\n**Definition**: The formation of judgments based on criteria and standards through checking and critiquing.\n\n**Cognitive Processes**:\n- **Checking**: Testing for internal consistency or logical fallacies\n- **Critiquing**: Judging based on external criteria and standards\n\n**Characteristic Cognitive Verbs**:\nAppraise, argue, assess, attach, choose, compare, conclude, contrast, convince, criticize, critique, decide, defend, determine, discriminate, estimate, evaluate, explain, grade, interpret, judge, justify, measure, predict, prioritize, prove, rank, rate, recommend, relate, revise, score, select, support, test, validate, value\n\n**Question Formulation Patterns**:\n- \"Evaluate the effectiveness of [approach/method] for [purpose].\"\n- \"Critique [theory/argument] based on [criteria].\"\n- \"Assess the validity of [claim/conclusion] given [evidence].\"\n- \"Justify the choice of [option A] over [option B] in [context].\"\n- \"What are the strengths and weaknesses of [solution/design]?\"\n\n**Flashcard Design Considerations**:\n- Require criteria-based judgment\n- Test critical thinking and assessment skills\n- Include evaluation of multiple alternatives\n- Provide context for informed judgment\n- Difficulty typically: Advanced\n\n**Example Questions**:\n- \"Evaluate whether Test-Driven Development (TDD) is appropriate for a rapid prototyping project. Justify your assessment.\"\n- \"Critique the decision to use a NoSQL database for a banking application requiring strong ACID guarantees.\"\n- \"Assess the security implications of implementing OAuth 2.0 versus custom JWT authentication.\"\n\n---\n\n### Level 6: Create (Synthesis)\n\n**Definition**: The assembly of elements to form a coherent or functional whole; reorganization of elements into a new pattern or structure.\n\n**Cognitive Processes**:\n- **Generating**: Developing alternative hypotheses based on criteria\n- **Planning**: Devising procedures for accomplishing a task\n- **Producing**: Creating a product from specifications\n\n**Characteristic Cognitive Verbs**:\nArrange, assemble, build, categorize, collect, combine, compile, compose, construct, create, design, develop, devise, establish, explain, formulate, generate, hypothesize, imagine, integrate, invent, manage, modify, organize, originate, plan, prepare, produce, propose, rearrange, reconstruct, reorganize, revise, rewrite, set up, summarize, synthesize, write\n\n**Question Formulation Patterns**:\n- \"Design a [system/solution] that addresses [requirements/constraints].\"\n- \"Develop a [plan/strategy] for [objective] given [conditions].\"\n- \"Create a [product/artifact] that integrates [components/concepts].\"\n- \"Formulate a hypothesis to explain [phenomenon].\"\n- \"Propose an alternative approach to [problem/challenge].\"\n\n**Flashcard Design Considerations**:\n- Require novel synthesis and creative integration\n- Test ability to generate original solutions\n- Include design and planning tasks\n- Provide clear constraints and requirements\n- Difficulty typically: Advanced\n\n**Example Questions**:\n- \"Design a distributed caching system for a high-traffic e-commerce platform that ensures consistency and minimizes latency.\"\n- \"Develop a testing strategy for a machine learning model that accounts for data drift and concept drift.\"\n- \"Create a database schema that efficiently supports both transactional queries and analytical reporting for a SaaS application.\"\n\n---\n\n## Pedagogical Principles for Flashcard Design\n\n### Principle 1: Cognitive Alignment\n\nEach flashcard must employ cognitive verbs and question structures appropriate to its designated Bloom's level. Misalignment diminishes pedagogical effectiveness and creates confusion regarding learning objectives.\n\n### Principle 2: Progressive Complexity\n\nFlashcard sets should demonstrate clear cognitive progression, beginning with foundational knowledge (Remember, Understand) and advancing toward higher-order thinking (Analyze, Evaluate, Create). This scaffolding facilitates systematic skill development.\n\n### Principle 3: Active Recall Enhancement\n\nEffective flashcards promote active retrieval rather than passive recognition. Questions should require mental effort to construct answers, strengthening memory consolidation through the testing effect.\n\n### Principle 4: Contextual Relevance\n\nHigher-order flashcards (Apply, Analyze, Evaluate, Create) require sufficient contextual information to enable informed responses. Decontextualized questions at these levels lack pedagogical value.\n\n### Principle 5: Elaborative Explanation\n\nAnswer explanations should extend beyond the immediate response to provide:\n- Conceptual context and background\n- Rationale for the answer\n- Common misconceptions or errors\n- Connections to related concepts\n- Practical implications where applicable\n\n### Principle 6: Distributed Difficulty\n\nFlashcard sets should incorporate varied difficulty levels:\n- **Beginner**: Foundational concepts, direct recall, basic comprehension\n- **Intermediate**: Application, comparative analysis, procedural execution\n- **Advanced**: Complex synthesis, critical evaluation, novel creation\n\nRecommended distribution: 40% Beginner, 40% Intermediate, 20% Advanced\n\n### Principle 7: Conceptual Network Building\n\nRelated concept references should facilitate the construction of integrated knowledge networks. Each flashcard should connect to prerequisite knowledge, parallel concepts, and potential applications.\n\n---\n\n## Common Pitfalls in Flashcard Design\n\n### Pitfall 1: Cognitive Level Misclassification\n\n**Error**: Labeling a simple recall question as \"Apply\" or \"Analyze\"\n\n**Example of Error**:\n- Question: \"Apply your knowledge of Python to define a list comprehension.\"\n- Problem: This requests a definition (Remember level), not application\n\n**Correction**:\n- Remember: \"What is the syntax for a Python list comprehension?\"\n- Apply: \"Use a list comprehension to filter all even numbers from [1,2,3,4,5,6]\"\n\n### Pitfall 2: Insufficient Context for Higher-Order Questions\n\n**Error**: Asking evaluation or creation questions without adequate scenario description\n\n**Example of Error**:\n- Question: \"Evaluate the database design.\"\n- Problem: No criteria, context, or specific design provided\n\n**Correction**:\n- \"Evaluate the decision to denormalize the user_profile table in a social media application with 10M users and frequent read operations but infrequent updates. Consider query performance, data consistency, and storage costs.\"\n\n### Pitfall 3: Ambiguous Question Formulation\n\n**Error**: Questions that can be interpreted multiple ways or lack clear answer criteria\n\n**Example of Error**:\n- Question: \"Discuss algorithms.\"\n- Problem: Excessively broad, no clear focus or success criteria\n\n**Correction**:\n- \"Compare the time and space complexity trade-offs between merge sort and quicksort algorithms.\"\n\n### Pitfall 4: Trivial Higher-Order Questions\n\n**Error**: Elevating simple concepts to higher Bloom's levels artificially\n\n**Example of Error**:\n- Question: \"Create a definition for the term 'variable'.\"\n- Problem: Definitions belong to Remember level; rewording doesn't elevate cognitive demand\n\n**Correction**:\n- Remember: \"Define the term 'variable' in programming.\"\n- Create: \"Design a variable naming convention system for a large codebase that balances readability, maintainability, and namespace management.\"\n\n### Pitfall 5: Answer-Only Responses\n\n**Error**: Providing answers without explanations, related concepts, or practice hints\n\n**Impact**: Reduces learning value to mere answer verification rather than understanding development\n\n**Correction**: Always include:\n- Accurate, complete answer\n- Elaborative explanation\n- Related concept references\n- Practice hints for complex questions (Analyze, Evaluate, Create)\n\n---\n\n## Quality Assurance Checklist\n\nWhen reviewing generated flashcards, verify:\n\n- [ ] **Cognitive Verb Alignment**: Question employs verbs appropriate to designated Bloom's level\n- [ ] **Clear Question Formulation**: No ambiguity in what is being asked\n- [ ] **Accurate Answer**: Factually correct and appropriately comprehensive\n- [ ] **Meaningful Explanation**: Goes beyond answer to provide learning value\n- [ ] **Relevant Related Concepts**: Connections enhance conceptual network\n- [ ] **Appropriate Difficulty Rating**: Matches cognitive demands and prerequisite knowledge\n- [ ] **Practice Hints** (for Analyze, Evaluate, Create): Provide scaffolding without revealing answer\n- [ ] **Sufficient Context**: Higher-order questions include necessary scenario details\n- [ ] **Pedagogical Value**: Question facilitates genuine learning, not mere memorization\n\n---\n\n## Advanced Considerations\n\n### Metacognitive Integration\n\nAdvanced flashcard sets may include metacognitive prompts that encourage learners to:\n- Reflect on their thinking processes\n- Identify knowledge gaps and misconceptions\n- Plan learning strategies for challenging concepts\n- Monitor comprehension and retention\n\n**Example Metacognitive Flashcard**:\n```markdown\n### Evaluate: Learning Strategy Assessment\n\n**Question**: Assess your current understanding of recursion. What aspects do you find clear, and what requires further study?\n\n**Difficulty**: Advanced\n\n**Answer**:\n[Self-assessment varies by learner]\n\n**Explanation**:\nThis metacognitive question encourages self-regulated learning by prompting explicit evaluation of understanding. Effective learners regularly assess their comprehension and identify areas requiring additional study.\n\n**Related Concepts**:\n- Self-regulated learning: Metacognitive awareness of one's learning process\n- Knowledge monitoring: Assessing what one knows versus what requires learning\n- Deliberate practice: Focused attention on areas of weakness\n\n**Practice Hint**:\nConsider: Can you explain recursion to someone else? Can you debug recursive functions? Can you design recursive solutions independently?\n```\n\n### Interleaved Practice Design\n\nResearch in cognitive science demonstrates that interleaving (mixing question types and cognitive levels) enhances long-term retention compared to blocked practice (all Level 1 questions, then all Level 2, etc.).\n\n**Implementation**: When creating study sessions, randomize flashcard order across Bloom's levels rather than presenting them hierarchically.\n\n### Spaced Repetition Integration\n\nFlashcard sets designed for spaced repetition systems should include:\n- Unique identifiers for each flashcard\n- Initial difficulty ratings\n- Tags for concept categorization\n- Version tracking for content updates\n\n---\n\n## References and Further Reading\n\n### Foundational Texts\n\n1. Bloom, B. S., Engelhart, M. D., Furst, E. J., Hill, W. H., & Krathwohl, D. R. (1956). *Taxonomy of Educational Objectives: The Classification of Educational Goals. Handbook I: Cognitive Domain*. New York: David McKay Company.\n\n2. Anderson, L. W., & Krathwohl, D. R. (Eds.). (2001). *A Taxonomy for Learning, Teaching, and Assessing: A Revision of Bloom's Taxonomy of Educational Objectives*. New York: Longman.\n\n### Cognitive Science Research\n\n3. Roediger, H. L., & Butler, A. C. (2011). The critical role of retrieval practice in long-term retention. *Trends in Cognitive Sciences*, 15(1), 20-27.\n\n4. Karpicke, J. D., & Roediger, H. L. (2008). The critical importance of retrieval for learning. *Science*, 319(5865), 966-968.\n\n5. Bjork, R. A., & Bjork, E. L. (2019). Forgetting as the friend of learning: Implications for teaching and self-regulated learning. *Advances in Physiology Education*, 43(2), 164-167.\n\n### Application Guides\n\n6. Armstrong, P. (2010). *Bloom's Taxonomy*. Vanderbilt University Center for Teaching. Retrieved from https://cft.vanderbilt.edu/guides-sub-pages/blooms-taxonomy/\n\n7. Marzano, R. J., & Kendall, J. S. (2007). *The New Taxonomy of Educational Objectives* (2nd ed.). Thousand Oaks, CA: Corwin Press.\n",
        "skills/study_system/.claude/skills/notes-generator/SKILL.md": "---\nname: notes-generator\ndescription: Generate comprehensive educational notes using Bloom's Taxonomy methodology. Creates structured learning materials with summaries, practice questions, and visual diagrams. Use when user requests notes generation, study materials, learning resources, mentions Bloom's Taxonomy, or provides topics for educational note-taking. Activates for .md files, study topics, course materials, or educational content creation.\n---\n\n# Notes Generator (Bloom's Taxonomy)\n\n## Purpose\n\nThis skill generates structured educational notes following Bloom's Taxonomy cognitive framework. It produces comprehensive learning materials adaptively selecting appropriate taxonomy levels based on topic complexity, incorporating summaries, practice questions, visual concept maps, and references.\n\n## Bloom's Taxonomy Framework\n\nThe skill structures notes across relevant cognitive levels:\n\n1. **Remember**: Recall facts, terms, basic concepts\n2. **Understand**: Explain ideas, interpret information\n3. **Apply**: Use knowledge in concrete situations\n4. **Analyze**: Break down information, identify relationships\n5. **Evaluate**: Make judgments, critique, assess value\n6. **Create**: Synthesize information, produce original work\n\n## Instructions\n\n### Phase 1: Input Acquisition and Analysis\n\n**Step 1.1: Determine Input Source**\n\nIf user provides a reference file:\n1. Use Read tool to access the file\n2. Extract topic, key concepts, and source material\n3. Proceed to Phase 2 with extracted content\n\nIf user provides only a topic title:\n1. Confirm topic scope with user if ambiguous\n2. Use WebSearch to gather authoritative information:\n   ```\n   Query pattern: \"[topic] tutorial\", \"[topic] fundamentals\", \"[topic] comprehensive guide\"\n   ```\n3. Use WebFetch to retrieve 2-3 high-quality sources (educational sites, documentation, academic resources)\n4. Synthesize gathered information\n5. Proceed to Phase 2 with researched content\n\n**Step 1.2: Topic Classification**\n\nAnalyze topic to determine appropriate Bloom levels:\n- **Factual topics** (history, terminology): Emphasize Remember, Understand\n- **Procedural topics** (programming, methods): Emphasize Understand, Apply, Analyze\n- **Conceptual topics** (theories, frameworks): Emphasize Understand, Analyze, Evaluate\n- **Creative topics** (design, composition): Include all levels, emphasize Create\n- **Complex topics** (systems, architectures): Focus on Analyze, Evaluate, Create\n\nSelect 4-6 most relevant taxonomy levels for the specific topic.\n\n### Phase 2: Content Structure Generation\n\n**Step 2.1: Executive Summary Creation**\n\nGenerate concise overview (3-5 sentences):\n- Topic definition and scope\n- Primary learning objectives\n- Key concepts covered\n- Practical applications or relevance\n\n**Step 2.2: Bloom's Taxonomy Level Development**\n\nFor each selected taxonomy level, generate 5-7 structured points:\n\n**Remember Level** (if applicable):\n- Key terminology with definitions\n- Essential facts and figures\n- Fundamental principles\n- Memorization aids (mnemonics, acronyms)\n- Core components or elements\n\n**Understand Level**:\n- Conceptual explanations in accessible language\n- Comparisons and contrasts\n- Cause-effect relationships\n- Interpretations of key ideas\n- Paraphrased core concepts\n- Examples illustrating principles\n\n**Apply Level**:\n- Practical use cases\n- Step-by-step procedures\n- Implementation examples\n- Real-world applications\n- Scenario-based demonstrations\n- Problem-solving approaches\n\n**Analyze Level**:\n- Component breakdown and decomposition\n- Relationship mapping between elements\n- Pattern identification\n- Comparative analysis\n- Structural examination\n- Critical distinctions\n\n**Evaluate Level**:\n- Criteria for assessment\n- Strengths and limitations\n- Critical appraisal\n- Comparative evaluation of approaches\n- Quality metrics\n- Evidence-based judgments\n\n**Create Level**:\n- Synthesis opportunities\n- Original application ideas\n- Design principles\n- Innovation possibilities\n- Integration with other concepts\n- Generative thinking prompts\n\n**Step 2.3: Visual Diagram Generation**\n\nCreate Mermaid diagram representing concept relationships:\n\nFor hierarchical topics:\n```mermaid\ngraph TD\n    A[Main Concept] --> B[Sub-concept 1]\n    A --> C[Sub-concept 2]\n    B --> D[Detail 1]\n    B --> E[Detail 2]\n```\n\nFor process-oriented topics:\n```mermaid\nflowchart LR\n    A[Start] --> B[Process 1]\n    B --> C[Process 2]\n    C --> D[Outcome]\n```\n\nFor relational topics:\n```mermaid\nmindmap\n  root((Topic))\n    Aspect 1\n      Detail A\n      Detail B\n    Aspect 2\n      Detail C\n      Detail D\n```\n\nSelect diagram type matching topic structure (hierarchy, process, relationship, classification).\n\n**Step 2.4: Practice Questions Generation**\n\nCreate 5-8 questions aligned with Bloom levels:\n\n- **Knowledge Check** (Remember): \"Define...\", \"List...\", \"Identify...\"\n- **Comprehension** (Understand): \"Explain...\", \"Summarize...\", \"Describe...\"\n- **Application** (Apply): \"How would you use...\", \"Demonstrate...\", \"Apply X to...\"\n- **Analysis** (Analyze): \"Compare...\", \"Analyze why...\", \"What is the relationship...\"\n- **Synthesis** (Evaluate): \"Assess...\", \"Critique...\", \"Which approach is better...\"\n- **Evaluation** (Create): \"Design...\", \"Propose...\", \"Create a solution for...\"\n\nDistribute questions across selected taxonomy levels.\n\n**Step 2.5: References and Further Reading**\n\nInclude:\n- Source citations (if using WebSearch results)\n- Recommended readings (textbooks, articles, documentation)\n- Online resources (courses, tutorials, interactive tools)\n- Related topics for deeper exploration\n\n### Phase 3: Note Compilation and Formatting\n\n**Step 3.1: Assemble Complete Document**\n\nStructure notes in markdown format following this template:\n\n```markdown\n# [Topic Title]\n\n## Summary\n\n[Executive summary paragraph]\n\n**Learning Objectives:**\n- [Objective 1]\n- [Objective 2]\n- [Objective 3]\n\n---\n\n## Notes by Bloom's Taxonomy\n\n### 1. Remember: Foundational Knowledge\n\n[5-7 structured points with definitions, facts, key terms]\n\n### 2. Understand: Conceptual Comprehension\n\n[5-7 structured points with explanations, examples, interpretations]\n\n### 3. Apply: Practical Application\n\n[5-7 structured points with use cases, procedures, implementations]\n\n### 4. Analyze: Critical Examination\n\n[5-7 structured points with breakdowns, relationships, patterns]\n\n### 5. Evaluate: Assessment and Judgment\n\n[5-7 structured points with criteria, critiques, evaluations]\n\n### 6. Create: Synthesis and Innovation\n\n[5-7 structured points with synthesis, design, generative ideas]\n\n---\n\n## Visual Concept Map\n\n[Mermaid diagram]\n\n---\n\n## Practice Questions\n\n### Knowledge & Comprehension\n1. [Question]\n2. [Question]\n\n### Application & Analysis\n3. [Question]\n4. [Question]\n\n### Evaluation & Creation\n5. [Question]\n6. [Question]\n\n---\n\n## References and Further Reading\n\n### Primary Sources\n- [Citation 1]\n- [Citation 2]\n\n### Recommended Resources\n- [Resource 1 with URL if applicable]\n- [Resource 2 with URL if applicable]\n\n### Related Topics\n- [Topic 1]\n- [Topic 2]\n```\n\n**Step 3.2: Quality Validation**\n\nVerify generated notes meet standards:\n- [ ] Summary clearly defines topic and objectives\n- [ ] 4-6 Bloom levels selected appropriately for topic\n- [ ] Each level contains 5-7 substantive points\n- [ ] Visual diagram accurately represents concept relationships\n- [ ] 5-8 practice questions distributed across taxonomy levels\n- [ ] References include sources and further reading\n- [ ] Markdown formatting correct and consistent\n- [ ] Content factually accurate and pedagogically sound\n\n### Phase 4: Output Delivery\n\n**Step 4.1: File Creation**\n\nUse Write tool to create notes file:\n- Default filename: `[topic-name]-notes.md` (kebab-case format)\n- Location: Current working directory or user-specified path\n- Encoding: UTF-8 with proper markdown formatting\n\n**Step 4.2: Confirmation Report**\n\nProvide user with completion summary:\n```\nNotes generated successfully: [filename]\n\n**Topic**: [Topic name]\n**Bloom Levels Included**: [List of levels used]\n**Total Sections**: [Count]\n**Practice Questions**: [Count]\n**Word Count**: [Approximate count]\n\n**File Location**: [Full path]\n\nThe notes are structured using Bloom's Taxonomy methodology and include:\n Executive summary with learning objectives\n [X] Bloom's Taxonomy levels with 5-7 points each\n Visual concept map (Mermaid diagram)\n [X] practice questions for self-assessment\n References and further reading recommendations\n```\n\n## Adaptive Behavior Patterns\n\n### Topic-Specific Adaptations\n\n**Scientific/Technical Topics**:\n- Emphasize Apply and Analyze levels\n- Include formulas, equations, technical diagrams\n- Focus practice questions on problem-solving\n- Reference primary literature and documentation\n\n**Humanities/Social Sciences**:\n- Emphasize Understand, Analyze, Evaluate\n- Include theoretical frameworks and perspectives\n- Focus on interpretive and critical thinking\n- Reference seminal works and contemporary scholarship\n\n**Practical Skills/Procedures**:\n- Emphasize Apply level with detailed procedures\n- Include step-by-step instructions\n- Use flowchart diagrams\n- Reference tutorials and hands-on resources\n\n**Conceptual/Theoretical Topics**:\n- Balance across all six levels\n- Include multiple perspectives and interpretations\n- Use mind-map or concept-map diagrams\n- Reference foundational and contemporary sources\n\n### Content Depth Adjustment\n\n**Simple/Introductory Topics**:\n- Focus on Remember, Understand, Apply\n- 5 points per level (lower end of moderate range)\n- Straightforward practice questions\n- Beginner-friendly resources\n\n**Complex/Advanced Topics**:\n- Include all relevant levels\n- 7 points per level (upper end of moderate range)\n- Challenging, multi-step practice questions\n- Advanced and specialized resources\n\n## Error Handling and Edge Cases\n\n### Insufficient Source Material\n\nIf reference file is too brief or web research yields limited information:\n1. Inform user of content limitations\n2. Generate notes based on available information\n3. Mark sections requiring additional research: `[Additional research needed]`\n4. Suggest specific areas for user to provide more information\n\n### Ambiguous Topics\n\nIf topic title is ambiguous (e.g., \"Java\" could mean programming language or island):\n1. Use AskUserQuestion tool to clarify:\n   - \"Which aspect of [topic] should the notes cover?\"\n   - Present 2-4 most likely interpretations\n2. Await user clarification\n3. Proceed with specified interpretation\n\n### Highly Specialized Topics\n\nIf topic requires domain expertise beyond general knowledge:\n1. Clearly label notes as \"Introductory Overview\"\n2. Focus on foundational concepts\n3. Emphasize References section with authoritative sources\n4. Recommend consulting domain experts or specialized texts\n\n### File Write Errors\n\nIf Write tool encounters errors (permissions, disk space):\n1. Report specific error to user\n2. Offer to display notes content directly in conversation\n3. Suggest alternative file locations or names\n\n## Examples\n\n### Example 1: Topic Title Only\n\n**User Request**: \"Generate notes on neural networks\"\n\n**Skill Actions**:\n1. Classify as scientific/technical topic\n2. WebSearch: \"neural networks tutorial\", \"neural networks fundamentals\"\n3. WebFetch: Retrieve content from educational sites\n4. Select Bloom levels: Understand, Apply, Analyze, Evaluate\n5. Generate moderate-depth notes (5-7 points per level)\n6. Create flowchart diagram showing neural network architecture\n7. Generate practice questions on architecture, training, applications\n8. Include references to deep learning resources\n9. Write to `neural-networks-notes.md`\n\n### Example 2: Reference File Provided\n\n**User Request**: \"Create notes from this document\" [provides research-paper.pdf]\n\n**Skill Actions**:\n1. Read provided file\n2. Extract topic: \"Transformer Architecture in NLP\"\n3. Classify as technical/conceptual topic\n4. Select Bloom levels: Understand, Analyze, Evaluate, Create\n5. Generate notes based on paper content\n6. Create architecture diagram showing transformer components\n7. Generate questions on attention mechanisms, applications\n8. Include paper as primary reference, add related readings\n9. Write to `transformer-architecture-notes.md`\n\n### Example 3: Highly Specific Request\n\n**User Request**: \"Generate introductory notes on Bloom's Taxonomy itself, include all six levels\"\n\n**Skill Actions**:\n1. Recognize meta-topic (educational framework)\n2. WebSearch for authoritative educational psychology sources\n3. Include all six Bloom levels (user-specified)\n4. Create hierarchical diagram showing taxonomy pyramid\n5. Generate questions demonstrating each cognitive level\n6. Include references to original Bloom (1956) and revised Anderson & Krathwohl (2001)\n7. Write to `blooms-taxonomy-notes.md`\n\n## Quality Standards\n\nAll generated notes shall adhere to:\n\n1. **Pedagogical Soundness**: Content structured for effective learning\n2. **Factual Accuracy**: Information verified against authoritative sources\n3. **Appropriate Depth**: Moderate detail (5-7 points) suitable for study notes\n4. **Adaptive Structure**: Bloom levels selected match topic characteristics\n5. **Visual Clarity**: Diagrams enhance understanding of relationships\n6. **Assessment Alignment**: Practice questions target specified cognitive levels\n7. **Resource Quality**: References direct to credible, relevant sources\n8. **Markdown Compliance**: Proper formatting for readability and compatibility\n\n## Advanced Usage\n\nFor specialized note generation patterns, see [reference.md](reference.md):\n- Custom Bloom level selection\n- Integration with spaced repetition systems\n- Collaborative note merging\n- Multi-source synthesis strategies\n- Domain-specific templates\n\nFor comprehensive examples across disciplines, see [examples.md](examples.md).\n",
        "skills/study_system/.claude/skills/notes-generator/examples.md": "# Notes Generator - Comprehensive Examples\n\nThis document demonstrates the notes-generator skill across diverse academic disciplines and use cases.\n\n---\n\n## Example 1: Computer Science - Data Structures (Topic Only)\n\n### User Request\n```\nGenerate notes on binary search trees\n```\n\n### Skill Analysis\n- **Topic Type**: Technical/Procedural\n- **Bloom Levels Selected**: Understand, Apply, Analyze, Create (4 levels)\n- **Research Required**: Yes (no reference file provided)\n- **Expected Depth**: Moderate (5-7 points per level)\n\n### Generated Notes Output\n\n```markdown\n# Binary Search Trees\n\n## Summary\n\nA Binary Search Tree (BST) is a hierarchical data structure where each node contains a value and has at most two children (left and right), maintaining the invariant that all values in the left subtree are less than the node's value, and all values in the right subtree are greater. BSTs enable efficient searching, insertion, and deletion operations with average O(log n) time complexity, making them fundamental for implementing associative arrays, priority queues, and database indexing. Understanding BST properties, operations, balancing strategies, and performance characteristics is essential for algorithm design and optimization.\n\n**Learning Objectives:**\n- Understand BST structure, properties, and invariants\n- Implement core BST operations (search, insert, delete)\n- Analyze time complexity and performance characteristics\n- Design balanced tree variations to optimize worst-case behavior\n\n---\n\n## Notes by Bloom's Taxonomy\n\n### 1. Understand: Conceptual Comprehension\n\n- **Binary Search Tree Definition**: A binary tree data structure where each node contains a comparable key and satisfies the BST property: for any node N, all keys in the left subtree are less than N's key, and all keys in the right subtree are greater than N's key. This ordering property enables efficient search operations.\n\n- **Structural Components**: Each BST node contains three elements: a data value (key), a reference to the left child node, and a reference to the right child node. Null references indicate absence of children. The topmost node is called the root; nodes without children are leaves.\n\n- **BST Property Invariant**: The fundamental invariant states that an in-order traversal (left-root-right) of a BST visits nodes in sorted ascending order. This property must be maintained after every modification operation (insertion or deletion) to preserve the tree's correctness.\n\n- **Height and Balance**: Tree height is the longest path from root to leaf. A balanced BST has height approximately log(n) for n nodes, yielding O(log n) operations. An unbalanced tree degrades to O(n) in worst case (resembles linked list when nodes inserted in sorted order).\n\n- **Comparison with Other Structures**: Unlike arrays (O(n) insertion/deletion), BSTs provide O(log n) average-case operations. Unlike hash tables (which require hash functions and handle collisions), BSTs maintain sorted order and support range queries efficiently. Unlike heaps (which only maintain partial order), BSTs enable both min/max access and arbitrary key searches.\n\n- **Duplicate Handling Strategies**: Different implementations handle duplicates differently: (1) disallow duplicates entirely, (2) allow duplicates in right subtree (modified invariant: left < root  right), (3) maintain count field in nodes, or (4) use additional linked structures for duplicate values.\n\n- **Tree Traversal Methods**: Three primary traversal strategies exist: in-order (left-root-right, produces sorted sequence), pre-order (root-left-right, useful for tree copying), and post-order (left-right-root, useful for deletion). Level-order traversal visits nodes by depth level using queue-based breadth-first search.\n\n### 2. Apply: Practical Application\n\n- **Search Operation Implementation**: Start at root, compare search key with current node. If equal, return node (found). If less, recurse left; if greater, recurse right. Return null if subtree is empty (not found). Pseudocode: `search(node, key): if node == null or node.key == key: return node; if key < node.key: return search(node.left, key); else: return search(node.right, key)`.\n\n- **Insertion Operation Implementation**: Search for insertion position (search until finding null reference where new node belongs), then create new node and attach to parent. Maintains BST property because insertion position determined by comparison operations. Pseudocode: `insert(node, key): if node == null: return new Node(key); if key < node.key: node.left = insert(node.left, key); else: node.right = insert(node.right, key); return node`.\n\n- **Finding Minimum/Maximum**: Minimum value is leftmost node (follow left pointers until null). Maximum value is rightmost node (follow right pointers until null). Both operations O(h) where h is height. Used in deletion operation to find in-order successor/predecessor.\n\n- **Deletion Operation (Three Cases)**: Case 1 - Node has no children: simply remove (set parent's pointer to null). Case 2 - Node has one child: bypass deleted node (set parent's pointer to child). Case 3 - Node has two children: find in-order successor (minimum in right subtree), copy successor's value to deleted node, recursively delete successor.\n\n- **Range Query Implementation**: To find all values in range [min, max], use modified in-order traversal: recurse left only if min < current, process current if min  current  max, recurse right only if current < max. This prunes branches outside range, improving efficiency over full traversal.\n\n- **BST Construction from Array**: Given unsorted array, insert elements sequentially. Result depends on insertion order: sorted array produces degenerate tree (linked list), random order produces relatively balanced tree. For guaranteed balance, sort array first and use divide-and-conquer: make middle element root, recursively build left subtree from left half, right subtree from right half.\n\n### 3. Analyze: Critical Examination\n\n- **Time Complexity Analysis**: Search, insert, and delete operations depend on tree height h. Average case (balanced tree): O(log n) because h  log(n). Worst case (degenerate tree): O(n) because h = n. Space complexity: O(n) for storing n nodes, O(h) auxiliary space for recursive call stack.\n\n- **Balance Factor and Degradation**: Balance factor of node = height(right subtree) - height(left subtree). Balanced if all nodes have balance factor  {-1, 0, 1}. Insertion of sorted or nearly-sorted data causes severe imbalance (creates linked list structure), degrading performance from O(log n) to O(n). Detecting imbalance requires height calculation, which itself costs O(n) for full tree.\n\n- **Comparison of Deletion Strategies**: Two-child deletion has two approaches: (1) replace with in-order successor (minimum of right subtree) or (2) replace with in-order predecessor (maximum of left subtree). Alternating between strategies helps maintain balance. Successor approach preferred for right-heavy trees, predecessor for left-heavy trees.\n\n- **Memory Locality and Cache Performance**: BSTs exhibit poor cache performance compared to arrays because nodes scattered in memory (pointer-based structure). Each traversal step potentially causes cache miss. Array-based structures (heap, B-tree) demonstrate better cache locality through contiguous storage.\n\n- **Balancing Strategy Necessity**: Self-balancing variants (AVL, Red-Black, Splay trees) maintain O(log n) worst-case performance through rotations after modifications. Trade-off: simpler BST has faster individual operations (no rebalancing overhead) but unreliable performance; self-balancing trees guarantee logarithmic operations with constant-factor overhead.\n\n- **Recursive vs. Iterative Implementations**: Recursive implementations elegant but consume O(h) stack space. Iterative implementations using explicit stack or parent pointers avoid recursion overhead. For unbalanced trees with h  n, recursive approach risks stack overflow. Tail-recursive operations (search) easily converted to iteration.\n\n### 4. Create: Synthesis and Innovation\n\n- **Design Custom BST Variant**: Create augmented BST storing subtree size at each node. Enables O(log n) rank queries (find kth smallest element) and order statistics. Modification operations update size fields during tree traversal. Applications include implementing order-statistic trees for dynamic rankings, leaderboards, or percentile calculations.\n\n- **Persistent BST Implementation**: Design immutable BST where modifications create new tree versions while preserving old versions (path copying). Each modification copies O(log n) nodes along path from root to modified node. Enables version control, undo/redo functionality, and concurrent read access without locks. Applications in functional programming languages and temporal databases.\n\n- **BST-Based Index Structure**: Implement database index using BST where keys are indexed column values and nodes store pointers to table rows. Support efficient exact-match queries, range scans, and ordered iteration. Enhancement: augment nodes with statistical information (min/max in subtree) to optimize query planning.\n\n- **Threaded BST for Efficient Traversal**: Design BST where null pointers replaced with threads (links to in-order predecessor/successor). Eliminates recursion and explicit stack for traversal, achieving O(1) space complexity. Right threads point to in-order successor, left threads to predecessor (or null). Applications in memory-constrained environments requiring frequent traversals.\n\n- **Concurrent BST with Fine-Grained Locking**: Design thread-safe BST using lock coupling (hand-over-hand locking): acquire lock on node before releasing parent's lock. Enables concurrent operations by different threads. Trade-off: locking overhead vs. parallelism. Alternative: optimistic approaches using atomic operations or lock-free algorithms (more complex but higher concurrency).\n\n- **BST Visualization Tool**: Create interactive visualization showing insertion/deletion animations, rotation operations for self-balancing variants, and complexity analysis. Display tree graphically with node colors indicating balance factors or violation states. Educational tool demonstrating how different insertion orders affect tree shape and performance.\n\n---\n\n## Visual Concept Map\n\n```mermaid\ngraph TD\n    A[Binary Search Tree] --> B[Structure]\n    A --> C[Operations]\n    A --> D[Performance]\n    A --> E[Variants]\n\n    B --> B1[Nodes with keys]\n    B --> B2[Left/Right children]\n    B --> B3[BST Property]\n\n    C --> C1[Search: O log n avg]\n    C --> C2[Insert: O log n avg]\n    C --> C3[Delete: O log n avg]\n    C --> C4[Traverse: O n ]\n\n    D --> D1[Balanced: O log n height]\n    D --> D2[Unbalanced: O n worst]\n    D --> D3[Cache performance issues]\n\n    E --> E1[AVL Tree]\n    E --> E2[Red-Black Tree]\n    E --> E3[Splay Tree]\n    E --> E4[B-Tree]\n\n    style A fill:#e1f5ff\n    style B fill:#fff4e1\n    style C fill:#fff4e1\n    style D fill:#fff4e1\n    style E fill:#fff4e1\n```\n\n---\n\n## Practice Questions\n\n### Knowledge & Comprehension\n\n1. **Define** the BST property that must hold for every node in a binary search tree.\n\n2. **Explain** why an in-order traversal of a BST produces a sorted sequence of values.\n\n3. **Describe** the three cases that must be handled when deleting a node from a BST.\n\n### Application & Analysis\n\n4. **Implement** the pseudocode or actual code for searching for a value in a BST using recursion.\n\n5. **Analyze** the time complexity of searching in a BST when elements are inserted in sorted ascending order. Why does this produce worst-case performance?\n\n6. **Compare** the advantages and disadvantages of using a BST versus a hash table for implementing a symbol table that requires both search and sorted iteration.\n\n### Evaluation & Creation\n\n7. **Evaluate** whether a standard BST or a self-balancing variant (AVL/Red-Black) would be more appropriate for a dictionary application with frequent insertions in alphabetical order. Justify your choice considering performance and implementation complexity.\n\n8. **Design** a modified BST that can efficiently answer the query \"How many elements in the tree are less than value X?\" in O(log n) time. What information must each node store, and how would the operations be modified?\n\n---\n\n## References and Further Reading\n\n### Primary Sources\n- Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2022). *Introduction to Algorithms* (4th ed.). MIT Press. Chapter 12: Binary Search Trees.\n- Sedgewick, R., & Wayne, K. (2011). *Algorithms* (4th ed.). Addison-Wesley. Section 3.2: Binary Search Trees.\n\n### Recommended Resources\n- [VisuAlgo Binary Search Tree Visualizations](https://visualgo.net/en/bst) - Interactive animations of BST operations\n- [MIT OpenCourseWare: Binary Search Trees](https://ocw.mit.edu/courses/6-006-introduction-to-algorithms-spring-2020/) - Lecture videos and notes\n- [GeeksforGeeks BST Tutorial](https://www.geeksforgeeks.org/binary-search-tree-data-structure/) - Implementation examples in multiple languages\n\n### Related Topics\n- AVL Trees (self-balancing BST variant)\n- Red-Black Trees (widely used in language libraries: C++ STL, Java TreeMap)\n- B-Trees (generalization for database indexing)\n- Trie (specialized tree for string keys)\n- Heap (different tree structure optimized for priority queues)\n```\n\n---\n\n## Example 2: History - World War II (Topic with Specific Focus)\n\n### User Request\n```\nCreate notes on World War II, focusing on the European theater\n```\n\n### Skill Analysis\n- **Topic Type**: Historical/Factual with analytical components\n- **Bloom Levels Selected**: Remember, Understand, Analyze, Evaluate (4 levels)\n- **Research Required**: Yes\n- **Expected Depth**: Moderate (5-7 points per level)\n\n### Generated Notes Output (Abbreviated)\n\n```markdown\n# World War II: European Theater\n\n## Summary\n\nThe European Theater of World War II (1939-1945) encompassed the military campaigns between Allied and Axis powers across Europe, North Africa, and the Atlantic. Beginning with Germany's invasion of Poland and concluding with the unconditional surrender of Nazi Germany, this conflict reshaped global political order, resulted in unprecedented casualties (over 40 million deaths), and revealed the horrors of industrialized genocide. Understanding the causes, major campaigns, key turning points, and consequences provides essential context for modern European history and international relations.\n\n**Learning Objectives:**\n- Remember key dates, leaders, and battles of the European Theater\n- Understand the political, economic, and ideological factors driving the conflict\n- Analyze strategic decisions and their impact on war outcomes\n- Evaluate the long-term consequences on European society and geopolitics\n\n---\n\n## Notes by Bloom's Taxonomy\n\n### 1. Remember: Foundational Knowledge\n\n- **Key Dates**: September 1, 1939 (Germany invades Poland), May 10, 1940 (Western Front offensive begins), June 22, 1941 (Operation Barbarossa - invasion of USSR), December 11, 1941 (Germany declares war on USA), June 6, 1944 (D-Day landings), May 8, 1945 (V-E Day - victory in Europe).\n\n- **Major Allied Leaders**: Winston Churchill (UK Prime Minister 1940-1945), Franklin D. Roosevelt (US President 1933-1945), Joseph Stalin (Soviet Premier), Charles de Gaulle (Free French leader), Dwight D. Eisenhower (Supreme Allied Commander Europe).\n\n- **Major Axis Leaders**: Adolf Hitler (German Fhrer), Benito Mussolini (Italian Duce until 1943), Hermann Gring (Luftwaffe commander), Erwin Rommel (Afrika Korps commander).\n\n- **Critical Battles**: Battle of Britain (1940), Operation Barbarossa (1941), Siege of Leningrad (1941-1944), Battle of Stalingrad (1942-1943), Battle of Kursk (1943), D-Day Normandy Invasion (1944), Battle of the Bulge (1944-1945).\n\n- **Geographic Scope**: Major combat zones included Western Europe (France, Low Countries, Germany), Eastern Europe (Poland, USSR), Southern Europe (Italy, Balkans, Greece), North Africa (Egypt, Libya, Tunisia), Scandinavia (Norway, Finland).\n\n- **The Holocaust**: Systematic genocide by Nazi regime targeting Jews, Roma, disabled individuals, political opponents, resulting in approximately 6 million Jewish deaths. Key components: concentration camps (Auschwitz, Treblinka, Dachau), mobile killing units (Einsatzgruppen), Final Solution policy (Wannsee Conference, January 1942).\n\n[Additional Remember points...]\n\n### 2. Understand: Conceptual Comprehension\n\n- **Origins in Treaty of Versailles**: The 1919 treaty ending WWI imposed harsh reparations on Germany, territorial losses, and military restrictions. Economic hardship and national humiliation created conditions for extremist movements. Hitler exploited this resentment, promising to restore German greatness and overturn Versailles terms.\n\n- **Appeasement Policy Failure**: British and French leaders pursued appeasement (1930s), making concessions to avoid war: allowing German rearmament, Rhineland remilitarization (1936), Austria's annexation (Anschluss, 1938), Czechoslovakia's partition (Munich Agreement, 1938). This strategy failed because it emboldened Hitler while allowing German military buildup.\n\n[Additional Understand points...]\n\n### 3. Analyze: Critical Examination\n\n- **Strategic Significance of Stalingrad**: Germany's defeat at Stalingrad (February 1943) marked the Eastern Front's turning point. Analysis reveals multiple factors: Soviet defense-in-depth strategy exhausted German forces, Hitler's refusal to allow retreat trapped the 6th Army, Soviet counteroffensive (Operation Uranus) created encirclement. This battle demonstrated limits of German operational capabilities and shifted strategic initiative to USSR.\n\n[Additional Analyze points...]\n\n### 4. Evaluate: Assessment and Judgment\n\n- **Effectiveness of Strategic Bombing Campaign**: Allied bombing of German cities (RAF night bombing, USAAF precision daylight raids) aimed to destroy industrial capacity and civilian morale. Assessment: achieved mixed results - disrupted production and forced resource diversion to air defense, but failed to break civilian morale and diverted resources from tactical air support. Ethical considerations regarding civilian casualties (Dresden, Hamburg firestorms) remain controversial.\n\n[Additional Evaluate points...]\n\n---\n\n## Visual Concept Map\n\n```mermaid\ntimeline\n    title Major Events - European Theater WWII\n    1939 : Germany invades Poland (Sept 1)\n         : Britain/France declare war\n    1940 : Fall of France\n         : Battle of Britain\n    1941 : Operation Barbarossa\n         : USA enters war\n    1942 : Battle of Stalingrad begins\n         : North Africa campaigns\n    1943 : Stalingrad victory\n         : Italy surrenders\n    1944 : D-Day landings\n         : Liberation of France\n    1945 : Battle of Berlin\n         : V-E Day (May 8)\n```\n\n---\n\n## Practice Questions\n\n### Knowledge & Comprehension\n1. **List** three major battles on the Eastern Front and their dates.\n2. **Explain** why the Battle of Britain was strategically significant for both Germany and Britain.\n\n### Application & Analysis\n3. **Analyze** how Germany's two-front war (Western and Eastern fronts simultaneously) contributed to its defeat.\n4. **Compare** the military strategies employed by Germany in the invasions of France (1940) and the Soviet Union (1941). What were the key differences and outcomes?\n\n### Evaluation & Creation\n5. **Evaluate** the decision by the Western Allies to prioritize the \"Europe First\" strategy over the Pacific Theater. Was this the optimal allocation of resources?\n6. **Assess** the moral and strategic justifications for the Allied strategic bombing campaign against German cities.\n\n---\n\n## References and Further Reading\n\n### Primary Sources\n- Churchill, Winston. *The Second World War* (6 volumes). Historical memoir by British PM.\n- Shirer, William L. *The Rise and Fall of the Third Reich*. Comprehensive history based on captured German documents.\n\n### Recommended Resources\n- Beevor, Antony. *The Second World War*. Comprehensive single-volume history.\n- [United States Holocaust Memorial Museum](https://www.ushmm.org/) - Extensive resources on Holocaust history\n- [Imperial War Museums Digital Collections](https://www.iwm.org.uk/) - Primary sources, photos, documents\n\n### Related Topics\n- Pacific Theater of World War II\n- Cold War origins and post-war European division\n- Nuremberg Trials and international law\n- Marshall Plan and European reconstruction\n```\n\n---\n\n## Example 3: Psychology - Cognitive Biases (Reference File Provided)\n\n### User Request\n```\nGenerate notes from this research paper: cognitive-biases-kahneman.pdf\n```\n\n### Skill Analysis\n- **Topic Type**: Conceptual/Theoretical (psychological science)\n- **Bloom Levels Selected**: Understand, Apply, Analyze, Evaluate (4 levels)\n- **Research Required**: No (reference file provided)\n- **Source**: Academic paper on cognitive biases by Daniel Kahneman\n\n### Skill Execution\n1. Read cognitive-biases-kahneman.pdf\n2. Extract core concepts: System 1/System 2 thinking, heuristics, specific biases\n3. Classify as conceptual topic  select appropriate Bloom levels\n4. Generate notes based on paper content\n5. Create mind map showing bias categories and relationships\n6. Develop practice questions applying biases to scenarios\n7. Include paper as primary reference, add related readings\n\n### Generated Notes Output (Abbreviated)\n\n```markdown\n# Cognitive Biases and Heuristics\n\n## Summary\n\nCognitive biases are systematic patterns of deviation from rationality in judgment and decision-making. Daniel Kahneman's research distinguishes between System 1 (fast, automatic, intuitive) and System 2 (slow, deliberate, analytical) thinking processes. Heuristicsmental shortcuts employed by System 1enable rapid decisions but introduce predictable biases including availability, representativeness, and anchoring effects. Understanding these biases illuminates human irrationality, improves decision-making, and applies to fields including economics, medicine, law, and public policy.\n\n**Learning Objectives:**\n- Understand dual-process theory (System 1 vs. System 2)\n- Apply knowledge of biases to recognize flawed reasoning\n- Analyze how heuristics produce systematic errors\n- Evaluate strategies for mitigating bias in decision-making\n\n---\n\n## Notes by Bloom's Taxonomy\n\n### 2. Understand: Conceptual Comprehension\n\n- **Dual-Process Theory**: Human cognition employs two distinct systems. System 1 operates automatically, quickly, with little effort and no voluntary control (intuition, snap judgments). System 2 allocates attention to effortful mental activities requiring deliberation (complex calculations, critical analysis). Most daily activities rely on System 1; System 2 activated for novel or difficult tasks.\n\n- **Availability Heuristic**: People estimate probability or frequency based on how easily examples come to mind. Recent, vivid, or emotionally charged events are more \"available\" mentally, leading to overestimation of their likelihood. Example: After seeing news coverage of airplane crashes, people overestimate flight danger relative to statistical reality (driving is far more dangerous per mile).\n\n[Additional Understand points on representativeness heuristic, anchoring effect, etc...]\n\n### 3. Apply: Practical Application\n\n- **Recognizing Availability Bias in Medical Diagnosis**: Physicians may overdiagnose conditions they've recently encountered or studied, because those diagnoses are mentally available. Application: Implement diagnostic checklists forcing systematic consideration of alternatives rather than relying on first intuition. Consider base rates (actual frequency in population) not just symptom similarity to recent cases.\n\n- **Mitigating Anchoring in Negotiations**: Negotiators are biased by initial offers (anchors), even when arbitrary. Application strategy: Make first offer to set favorable anchor, or if receiving first offer, explicitly consider wider range before responding. Use objective criteria (market rates, precedents) to counteract arbitrary anchors.\n\n[Additional Apply points...]\n\n### 4. Analyze: Critical Examination\n\n- **Why Heuristics Persist Despite Errors**: Evolutionary analysis suggests heuristics developed in ancestral environments where fast decisions (threat detection) were fitness-advantageous. Modern environments present statistical/probabilistic problems rare in evolutionary history. Analysis reveals trade-off: heuristics sacrifice accuracy for speed/efficiency. In high-stakes, statistical domains (medicine, finance, law), this trade-off becomes problematic.\n\n[Additional Analyze points...]\n\n### 5. Evaluate: Assessment and Judgment\n\n- **Effectiveness of Debiasing Interventions**: Research on reducing bias shows mixed results. Simple awareness training has limited impactknowing about bias doesn't prevent it. More effective: structured decision procedures (checklists, algorithms), accountability mechanisms, and \"consider-the-opposite\" strategies. Evaluation: Individual debiasing difficult (biases are automatic), but institutional/procedural safeguards show promise. Cost-benefit assessment must weigh intervention expense against decision stakes.\n\n[Additional Evaluate points...]\n\n---\n\n## Visual Concept Map\n\n```mermaid\nmindmap\n  root((Cognitive Biases))\n    System 1 - Fast\n      Automatic\n      Intuitive\n      Heuristics\n        Availability\n        Representativeness\n        Anchoring\n    System 2 - Slow\n      Deliberate\n      Analytical\n      Effortful\n    Common Biases\n      Confirmation Bias\n      Hindsight Bias\n      Overconfidence\n      Framing Effects\n    Applications\n      Economics\n      Medicine\n      Law\n      Public Policy\n```\n\n---\n\n## Practice Questions\n\n### Knowledge & Comprehension\n1. **Define** System 1 and System 2 thinking. Provide an example of each from everyday life.\n2. **Explain** the availability heuristic and why it leads to systematic errors in probability estimation.\n\n### Application & Analysis\n3. **Apply** your understanding of anchoring bias to explain why stores often display \"original price\" alongside \"sale price.\" How does this influence purchasing decisions?\n\n4. **Analyze** a real-world scenario: After a shark attack receives extensive media coverage, beach attendance drops significantly despite unchanged actual risk. Which cognitive bias explains this behavior, and what is the mechanism?\n\n### Evaluation & Creation\n5. **Evaluate** whether teaching students about cognitive biases in high school would be worthwhile. Consider potential benefits, limitations of awareness training, and alternative approaches.\n\n6. **Design** a decision-making procedure for a hospital emergency department that minimizes the impact of availability and anchoring biases on diagnostic accuracy.\n\n---\n\n## References and Further Reading\n\n### Primary Sources\n- Kahneman, D., & Tversky, A. (1974). \"Judgment under Uncertainty: Heuristics and Biases.\" *Science*, 185(4157), 1124-1131. [Original research paper provided]\n- Kahneman, D. (2011). *Thinking, Fast and Slow*. Comprehensive overview of bias research.\n\n### Recommended Resources\n- [Wikipedia List of Cognitive Biases](https://en.wikipedia.org/wiki/List_of_cognitive_biases) - Comprehensive catalog\n- [LessWrong Sequences](https://www.lesswrong.com/rationality) - Applied rationality and debiasing techniques\n- [Yale Open Course: Introduction to Psychology](https://oyc.yale.edu/psychology/psyc-110) - Lectures on judgment and decision-making\n\n### Related Topics\n- Behavioral Economics (application of bias research to economic theory)\n- Nudge Theory (using biases to improve decisions through choice architecture)\n- Prospect Theory (decision-making under risk and uncertainty)\n- Metacognition (thinking about one's own thinking processes)\n```\n\n---\n\n## Example 4: Quick Notes - Simple Topic\n\n### User Request\n```\nQuick notes on photosynthesis for high school biology\n```\n\n### Skill Analysis\n- **Topic Type**: Scientific/Procedural\n- **Depth Request**: \"Quick notes\"  triggers minimal mode\n- **Bloom Levels Selected**: Remember, Understand, Apply (3 levels)\n- **Points per Level**: 4-5 (reduced from standard 5-7)\n- **Audience**: High school level (accessible language)\n\n### Generated Notes Output (Abbreviated)\n\n```markdown\n# Photosynthesis (Quick Reference)\n\n## Summary\n\nPhotosynthesis is the process by which plants, algae, and some bacteria convert light energy into chemical energy stored in glucose molecules. Using chlorophyll pigments in chloroplasts, organisms capture sunlight and use it to transform carbon dioxide and water into glucose and oxygen. This process sustains nearly all life on Earth by producing oxygen and forming the base of food chains.\n\n**Learning Objectives:**\n- Remember the overall equation and key components\n- Understand the light-dependent and light-independent reactions\n- Apply knowledge to explain plant growth and ecological relationships\n\n---\n\n## Notes by Bloom's Taxonomy\n\n### 1. Remember: Foundational Knowledge\n\n- **Overall Equation**: 6CO + 6HO + light energy  CHO + 6O\n  (Carbon dioxide + water + light  glucose + oxygen)\n\n- **Location**: Occurs in chloroplasts, organelles found in plant cells and algae. Chloroplasts contain chlorophyll (green pigment that absorbs light) and are most concentrated in leaf cells.\n\n- **Key Components**: Chlorophyll (light-absorbing pigment), water (electron source), carbon dioxide (carbon source), light energy (from sun), ATP and NADPH (energy carriers).\n\n- **Two Main Stages**: Light-dependent reactions (occur in thylakoid membranes, require light) and light-independent reactions/Calvin Cycle (occur in stroma, use products from light reactions).\n\n### 2. Understand: Conceptual Comprehension\n\n- **Light-Dependent Reactions**: Light energy excites electrons in chlorophyll. These high-energy electrons pass through electron transport chain, powering ATP synthesis and reducing NADP to NADPH. Water molecules split (photolysis), releasing oxygen as byproduct. Products (ATP and NADPH) used in Calvin Cycle.\n\n- **Calvin Cycle (Light-Independent)**: Uses ATP and NADPH from light reactions to convert CO into glucose. Occurs in three phases: carbon fixation (CO attaches to RuBP via rubisco enzyme), reduction (using ATP/NADPH to form G3P sugar), and regeneration (RuBP reformed to continue cycle). Does not directly require light but depends on light reaction products.\n\n- **Why Plants Are Green**: Chlorophyll absorbs red and blue wavelengths of light but reflects green wavelengths. Our eyes detect the reflected green light, making plants appear green. Red/blue light most effective for photosynthesis.\n\n- **Relationship to Cellular Respiration**: Photosynthesis and cellular respiration are complementary processes. Photosynthesis produces glucose and oxygen; respiration consumes glucose and oxygen to produce ATP, CO, and water. Together they cycle energy and matter through ecosystems.\n\n### 3. Apply: Practical Application\n\n- **Explaining Plant Growth**: Plants grow by using glucose produced in photosynthesis as both energy source (cellular respiration breaks down glucose for ATP) and building material (glucose molecules combined to form cellulose for cell walls, starches for storage).\n\n- **Limiting Factors**: Photosynthesis rate limited by available light, CO concentration, water availability, and temperature. In greenhouses, farmers optimize these factors: add artificial light, increase CO levels, ensure irrigation, maintain optimal temperature to maximize crop growth.\n\n- **Seasonal Changes**: Deciduous trees lose leaves in fall because shorter days/less sunlight make photosynthesis less efficient than cost of maintaining leaves. Chlorophyll breaks down, revealing other pigments (carotenoids = yellow/orange, anthocyanins = red), creating fall colors.\n\n---\n\n## Visual Concept Map\n\n```mermaid\nflowchart LR\n    A[Light Energy] --> B[Light-Dependent<br/>Reactions]\n    C[HO] --> B\n    B --> D[O released]\n    B --> E[ATP + NADPH]\n    F[CO] --> G[Calvin Cycle<br/>Light-Independent]\n    E --> G\n    G --> H[Glucose CHO]\n\n    style B fill:#c8e6c9\n    style G fill:#fff9c4\n```\n\n---\n\n## Practice Questions\n\n1. **Write** the balanced chemical equation for photosynthesis.\n\n2. **Explain** why oxygen is released as a byproduct of photosynthesis. Which stage produces it?\n\n3. **Apply**: A plant is placed in a dark closet for one week. What would happen to its photosynthesis rate and why? Would it survive?\n\n4. **Analyze**: Why do plants in deep ocean water often appear red or brown instead of green?\n\n---\n\n## References and Further Reading\n\n### Primary Sources\n- Campbell Biology (High School Edition), Chapter on Photosynthesis\n\n### Recommended Resources\n- [Khan Academy: Photosynthesis](https://www.khanacademy.org/science/biology/photosynthesis-in-plants) - Video tutorials\n- [Crash Course Biology #8: Photosynthesis](https://www.youtube.com/watch?v=uixA8ZXx0KU) - Engaging video overview\n\n### Related Topics\n- Cellular Respiration (complementary process)\n- Chloroplast structure and function\n- Ecological energy flow and food chains\n```\n\n---\n\n## Example 5: Multi-Reference Synthesis\n\n### User Request\n```\nGenerate notes on machine learning from these three files:\n- ml-intro-textbook.pdf\n- neural-networks-paper.pdf\n- practical-ml-guide.md\n```\n\n### Skill Analysis\n- **Topic Type**: Technical/Conceptual\n- **Sources**: Three files with different perspectives (textbook = theory, paper = specialized topic, guide = practical)\n- **Strategy**: Synthesize complementary information, note source-specific insights\n- **Bloom Levels Selected**: Understand, Apply, Analyze, Evaluate, Create (5 levels)\n\n### Skill Execution Process\n1. Read all three files separately\n2. Extract key concepts from each:\n   - Textbook: Foundational theory, algorithm categories, mathematical foundations\n   - Paper: Deep dive into neural network architectures\n   - Guide: Implementation best practices, tooling, real-world considerations\n3. Synthesize overlapping concepts (e.g., all discuss supervised learning)\n4. Preserve unique contributions (paper's neural network details, guide's practical tips)\n5. Attribute source-specific claims\n6. Generate comprehensive notes integrating all perspectives\n\n### Generated Notes Output (Abbreviated)\n\n```markdown\n# Machine Learning\n\n## Summary\n\nMachine Learning (ML) is a subfield of artificial intelligence enabling computers to learn patterns from data without explicit programming. ML algorithms fall into three main categories: supervised learning (learning from labeled examples), unsupervised learning (finding structure in unlabeled data), and reinforcement learning (learning from reward signals). This synthesis covers theoretical foundations, neural network architectures, and practical implementation considerations, integrating perspectives from theoretical texts, specialized research, and practitioner guides.\n\n**Learning Objectives:**\n- Understand ML paradigms and core algorithms\n- Apply ML techniques to real-world problems\n- Analyze trade-offs in algorithm selection and model design\n- Evaluate model performance and identify improvement strategies\n- Create end-to-end ML solutions considering practical constraints\n\n---\n\n## Notes by Bloom's Taxonomy\n\n### 2. Understand: Conceptual Comprehension\n\n- **Supervised Learning Paradigm**: Algorithms learn mapping from inputs (features) to outputs (labels) using labeled training data. Training process minimizes difference between predictions and true labels. Examples: classification (discrete labels like spam/not-spam) and regression (continuous values like house prices). Common algorithms: linear regression, logistic regression, decision trees, support vector machines, neural networks.\n  (References: [1], [3])\n\n- **Neural Network Architecture**: Composed of layers of interconnected artificial neurons. Each neuron receives weighted inputs, applies activation function (introducing non-linearity), and passes output to next layer. Training via backpropagation: compute error gradient with respect to weights using chain rule, update weights to minimize loss function. Deep networks (many layers) learn hierarchical representations.\n  (Reference: [2] - specialized coverage of architectures including CNNs, RNNs, transformers)\n\n[Additional Understand points synthesizing all three sources...]\n\n### 3. Apply: Practical Application\n\n- **Implementing a Classification Pipeline**: Practical workflow from practitioner guide: (1) collect and explore data using pandas, (2) split into train/validation/test sets (e.g., 70/15/15), (3) preprocess features (normalization, encoding categorical variables), (4) select algorithm (start simple: logistic regression), (5) train model using scikit-learn: `model.fit(X_train, y_train)`, (6) evaluate on validation set, (7) tune hyperparameters, (8) final evaluation on test set.\n  (Reference: [3] - practical-ml-guide.md provides code examples and library recommendations)\n\n[Additional Apply points...]\n\n### 4. Analyze: Critical Examination\n\n- **Bias-Variance Tradeoff**: Theoretical analysis shows fundamental tradeoff between model bias (underfitting - model too simple) and variance (overfitting - model too complex, sensitive to training data noise). Total error = bias + variance + irreducible error. Simple models (linear regression) have high bias, low variance. Complex models (deep neural networks) have low bias, high variance. Optimal complexity balances both. Regularization techniques (L1, L2, dropout) reduce variance.\n  (Reference: [1] - mathematical treatment of bias-variance decomposition)\n\n[Additional Analyze points combining theoretical foundations from textbook with neural network-specific analysis from paper...]\n\n### 5. Evaluate: Assessment and Judgment\n\n- **Assessing Neural Networks vs. Traditional ML**: Neural networks excel at high-dimensional data (images, text, audio) where manual feature engineering difficult, achieving state-of-art performance on complex tasks. However, they require large datasets (thousands to millions of examples), substantial computational resources (GPUs), and extensive hyperparameter tuning. Traditional algorithms (random forests, gradient boosting) often preferable for structured/tabular data with limited samples, offering faster training, better interpretability, and competitive performance.\n  (References: [2] for neural network capabilities, [3] for practical trade-offs in algorithm selection)\n\n[Additional Evaluate points...]\n\n### 6. Create: Synthesis and Innovation\n\n- **Designing a Hybrid Recommendation System**: Synthesize collaborative filtering (using user behavior patterns) and content-based filtering (using item features). Architecture: use neural network to learn user and item embeddings from interaction data (collaborative component), concatenate with hand-crafted content features (genres, prices, descriptions), feed combined representation to final layers predicting user-item affinity. Addresses cold-start problem (new users/items) through content features while leveraging collaborative patterns for established users.\n  (Synthesis of concepts from all three references: neural architectures [2], ensemble approaches [1], production considerations [3])\n\n[Additional Create points...]\n\n---\n\n## Visual Concept Map\n\n```mermaid\ngraph TD\n    A[Machine Learning] --> B[Supervised Learning]\n    A --> C[Unsupervised Learning]\n    A --> D[Reinforcement Learning]\n\n    B --> B1[Classification]\n    B --> B2[Regression]\n    B1 --> B3[Neural Networks]\n    B1 --> B4[SVM, Trees, etc]\n\n    C --> C1[Clustering]\n    C --> C2[Dimensionality Reduction]\n\n    D --> D1[Q-Learning]\n    D --> D2[Policy Gradients]\n\n    B3 --> E[Deep Learning]\n    E --> E1[CNNs - Images]\n    E --> E2[RNNs - Sequences]\n    E --> E3[Transformers - NLP]\n\n    style A fill:#e1f5ff\n    style B fill:#fff4e1\n    style C fill:#fff4e1\n    style D fill:#fff4e1\n    style E fill:#c8e6c9\n```\n\n---\n\n## Practice Questions\n\n[Questions synthesizing content from all three sources...]\n\n---\n\n## References and Further Reading\n\n### Primary Sources\n[1] ml-intro-textbook.pdf - Theoretical foundations and mathematical treatment\n[2] neural-networks-paper.pdf - Specialized coverage of neural architectures\n[3] practical-ml-guide.md - Implementation best practices and tooling\n\n### Recommended Resources\n- Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press.\n- Gron, A. (2019). *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*.\n- [fast.ai Practical Deep Learning Course](https://course.fast.ai/) - Implementation-focused tutorials\n\n### Related Topics\n- Statistical Learning Theory\n- Computer Vision applications\n- Natural Language Processing\n- MLOps and production deployment\n```\n\n---\n\n## Summary of Examples\n\n| Example | Topic | Type | Bloom Levels | Depth | Special Features |\n|---------|-------|------|--------------|-------|------------------|\n| 1 | Binary Search Trees | Technical | 4 levels | Moderate | Algorithm analysis, complexity |\n| 2 | WWII European Theater | Historical | 4 levels | Moderate | Timeline diagram, ethical considerations |\n| 3 | Cognitive Biases | Conceptual | 4 levels | Moderate | Reference file, applied psychology |\n| 4 | Photosynthesis | Scientific | 3 levels | Quick (4-5 pts) | High school audience, simplified |\n| 5 | Machine Learning | Technical | 5 levels | Moderate | Multi-source synthesis, citations |\n\nThese examples demonstrate the notes-generator skill's adaptability across:\n- **Disciplines**: Computer science, history, psychology, biology\n- **Complexity levels**: Quick reference to comprehensive analysis\n- **Source types**: Topic-only (research), reference files, multi-file synthesis\n- **Audience levels**: High school to advanced/professional\n- **Bloom's Taxonomy coverage**: 3-5 levels, adaptively selected\n",
        "skills/study_system/.claude/skills/notes-generator/reference.md": "# Notes Generator - Technical Reference\n\n## Advanced Bloom's Taxonomy Implementation\n\n### Cognitive Level Selection Algorithms\n\nThe notes-generator employs adaptive algorithms to determine optimal Bloom's Taxonomy level inclusion based on topic characteristics.\n\n#### Topic Classification Matrix\n\n| Topic Category | Primary Levels | Secondary Levels | Emphasis |\n|----------------|----------------|------------------|----------|\n| Factual/Historical | Remember, Understand | Analyze | Retention |\n| Procedural/Technical | Apply, Analyze | Understand, Create | Implementation |\n| Conceptual/Theoretical | Understand, Analyze, Evaluate | Remember, Create | Comprehension |\n| Creative/Design | Create, Evaluate | Apply, Analyze | Innovation |\n| Systems/Architecture | Analyze, Evaluate | Understand, Apply | Integration |\n\n#### Selection Heuristics\n\n**Rule 1: Minimum Viable Coverage**\n- Always include at least 3 taxonomy levels\n- Never omit Understand level (foundational for all learning)\n\n**Rule 2: Progressive Complexity**\n- Lower levels (Remember, Understand) establish foundation\n- Middle levels (Apply, Analyze) build competence\n- Upper levels (Evaluate, Create) develop mastery\n\n**Rule 3: Domain Alignment**\n- STEM topics: Emphasize Apply, Analyze (70% of content)\n- Humanities: Emphasize Understand, Evaluate (70% of content)\n- Arts: Emphasize Create, Evaluate (70% of content)\n- Business: Balance across Apply, Analyze, Evaluate (even distribution)\n\n### Content Depth Calibration\n\n#### Moderate Depth Specification (5-7 Points per Level)\n\nEach point should constitute a **complete learning unit**:\n\n**Minimum Point Structure**:\n- Declarative statement or concept introduction\n- Supporting explanation or context (1-2 sentences)\n- Example or illustration (where applicable)\n\n**Example - Remember Level Point**:\n```markdown\n- **Neural Network**: A computational model inspired by biological neural networks,\n  consisting of interconnected nodes (neurons) organized in layers. The basic\n  architecture includes an input layer, one or more hidden layers, and an output layer.\n  Example: A feedforward neural network with 784 input neurons (2828 image),\n  128 hidden neurons, and 10 output neurons (digit classification).\n```\n\n**Example - Apply Level Point**:\n```markdown\n- **Training a Simple Neural Network**: Initialize weights randomly, forward propagate\n  input through layers using activation functions (e.g., ReLU), compute loss using\n  appropriate function (cross-entropy for classification), backpropagate error gradients,\n  and update weights using optimizer (SGD, Adam). Repeat for multiple epochs until\n  convergence. Implementation typically uses frameworks like PyTorch or TensorFlow:\n  `model.fit(X_train, y_train, epochs=10, batch_size=32)`.\n```\n\n#### Depth Adjustment Triggers\n\n**Reduce to 5 points per level when**:\n- Topic is narrow or highly specialized\n- Source material is limited\n- Introductory/overview notes requested\n- Time constraints specified by user\n\n**Expand to 7 points per level when**:\n- Topic is broad with multiple dimensions\n- Abundant source material available\n- Comprehensive study notes requested\n- Advanced/graduate level content\n\n### Visual Diagram Design Patterns\n\n#### Diagram Type Selection Decision Tree\n\n```\nTopic Analysis\n Hierarchical structure? (categories, classifications)\n   Use: graph TD (top-down tree)\n\n Sequential process? (steps, procedures, workflows)\n   Use: flowchart LR/TD\n\n Interconnected concepts? (relationships, networks)\n   Use: graph LR with bidirectional edges\n\n Multiple dimensions? (aspects, facets)\n   Use: mindmap\n\n Comparative structure? (alternatives, options)\n    Use: graph with parallel branches\n```\n\n#### Advanced Mermaid Patterns\n\n**Pattern 1: Multi-Level Hierarchy**\n```mermaid\ngraph TD\n    A[Machine Learning] --> B[Supervised Learning]\n    A --> C[Unsupervised Learning]\n    A --> D[Reinforcement Learning]\n    B --> E[Classification]\n    B --> F[Regression]\n    C --> G[Clustering]\n    C --> H[Dimensionality Reduction]\n    D --> I[Value-Based]\n    D --> J[Policy-Based]\n\n    style A fill:#e1f5ff\n    style B fill:#fff4e1\n    style C fill:#fff4e1\n    style D fill:#fff4e1\n```\n\n**Pattern 2: Process Flow with Decision Points**\n```mermaid\nflowchart TD\n    A[Start: Data Collection] --> B{Data Quality Check}\n    B -->|Pass| C[Preprocessing]\n    B -->|Fail| D[Data Cleaning]\n    D --> B\n    C --> E[Feature Engineering]\n    E --> F[Model Selection]\n    F --> G{Performance Adequate?}\n    G -->|No| H[Hyperparameter Tuning]\n    H --> F\n    G -->|Yes| I[Deploy Model]\n```\n\n**Pattern 3: Relationship Network**\n```mermaid\ngraph LR\n    A[Variables] <--> B[Functions]\n    B <--> C[Classes]\n    C <--> D[Modules]\n    D <--> E[Packages]\n    A -.-> F[Data Types]\n    F -.-> B\n    C --> G[Objects]\n    G --> H[Methods]\n```\n\n**Pattern 4: Conceptual Mind Map**\n```mermaid\nmindmap\n  root((Software Design Principles))\n    SOLID\n      Single Responsibility\n      Open-Closed\n      Liskov Substitution\n      Interface Segregation\n      Dependency Inversion\n    DRY\n      Code Reusability\n      Abstraction\n    KISS\n      Simplicity\n      Maintainability\n    YAGNI\n      Avoid Over-engineering\n      Incremental Development\n```\n\n### Practice Question Engineering\n\n#### Bloom-Aligned Question Stems\n\n**Remember Level**:\n- Define, List, Name, Identify, Label, State, Match, Recall, Recognize, Select\n\n**Understand Level**:\n- Explain, Describe, Summarize, Paraphrase, Interpret, Classify, Compare, Contrast\n\n**Apply Level**:\n- Apply, Demonstrate, Use, Solve, Execute, Implement, Compute, Calculate, Modify\n\n**Analyze Level**:\n- Analyze, Differentiate, Organize, Deconstruct, Compare, Examine, Categorize, Distinguish\n\n**Evaluate Level**:\n- Evaluate, Critique, Judge, Justify, Assess, Defend, Argue, Recommend, Prioritize\n\n**Create Level**:\n- Design, Construct, Create, Develop, Formulate, Compose, Plan, Produce, Synthesize\n\n#### Question Complexity Gradation\n\n**Simple Questions** (1-step cognitive process):\n```\nRemember: \"What is the definition of polymorphism?\"\nUnderstand: \"Explain the difference between class and object.\"\n```\n\n**Moderate Questions** (2-3 step cognitive process):\n```\nApply: \"Given this class structure, how would you implement inheritance to avoid code duplication?\"\nAnalyze: \"Compare the time complexity of bubble sort and quicksort. Why does one outperform the other?\"\n```\n\n**Complex Questions** (4+ step cognitive process):\n```\nEvaluate: \"Assess whether microservices or monolithic architecture would be more appropriate\n           for a startup with 5 developers building an e-commerce platform. Justify your decision\n           considering scalability, team size, and time-to-market.\"\n\nCreate: \"Design a caching strategy for a high-traffic news website that balances freshness of\n         content with server load. Specify cache invalidation rules, TTL values, and explain\n         your design choices.\"\n```\n\n#### Answer Provision Strategy\n\n**Default Behavior**: Questions without answers (encourages active learning)\n\n**Optional Enhancement**: Provide answers in collapsible sections\n```markdown\n<details>\n<summary>Answer</summary>\n\n[Detailed answer with explanation]\n\n**Key Points**:\n- Point 1\n- Point 2\n\n**Explanation**: [Why this answer is correct, common misconceptions]\n</details>\n```\n\n### Multi-Source Synthesis Strategies\n\n#### Research Protocol for Topic Investigation\n\n**Phase 1: Broad Search (2-3 queries)**\n```\nQuery 1: \"[topic] tutorial\" or \"[topic] introduction\"\nQuery 2: \"[topic] comprehensive guide\" or \"[topic] fundamentals\"\nQuery 3: \"[topic] best practices\" or \"[topic] advanced concepts\"\n```\n\n**Phase 2: Source Quality Filtering**\n\nPrioritize sources in order:\n1. Official documentation (for technical topics)\n2. Academic institutions (.edu domains)\n3. Reputable educational platforms (Khan Academy, Coursera, edX)\n4. Established technical publishers (O'Reilly, Manning, Packt)\n5. High-quality community resources (well-maintained wikis, GitHub docs)\n\nAvoid:\n- Content farms with shallow information\n- Outdated resources (>5 years for technology topics)\n- Sources without author credentials or citations\n- Promotional/sales-oriented content\n\n**Phase 3: Content Extraction and Synthesis**\n\nFor each source:\n1. Extract unique concepts not covered in previous sources\n2. Identify consensus definitions and explanations\n3. Note conflicting perspectives (include multiple viewpoints in notes)\n4. Collect concrete examples and use cases\n5. Record citations for reference section\n\n**Phase 4: Integration**\n\nSynthesize extracted content:\n- Use consensus explanations for core concepts\n- Include multiple perspectives for controversial topics\n- Distribute examples across appropriate Bloom levels\n- Attribute specific claims or specialized information to sources\n\n### Domain-Specific Templates\n\n#### STEM Template Enhancements\n\n**Additional Sections**:\n```markdown\n## Mathematical Foundations\n[Relevant equations, formulas, theorems]\n\n## Algorithmic Complexity\n[Time/space complexity analysis where applicable]\n\n## Experimental Validation\n[Key experiments, empirical results, benchmarks]\n```\n\n**Diagram Preferences**:\n- Flowcharts for algorithms\n- Hierarchical graphs for taxonomies\n- Network diagrams for system architectures\n\n#### Humanities Template Enhancements\n\n**Additional Sections**:\n```markdown\n## Historical Context\n[Development of ideas over time, key figures]\n\n## Theoretical Frameworks\n[Major schools of thought, paradigms]\n\n## Critical Perspectives\n[Diverse viewpoints, debates, critiques]\n```\n\n**Diagram Preferences**:\n- Timelines for historical development\n- Mind maps for theoretical relationships\n- Comparative tables for different perspectives\n\n#### Business/Management Template Enhancements\n\n**Additional Sections**:\n```markdown\n## Real-World Case Studies\n[Examples from actual organizations]\n\n## Implementation Frameworks\n[Step-by-step implementation guides]\n\n## Metrics and KPIs\n[Measurement approaches, success indicators]\n```\n\n**Diagram Preferences**:\n- Flowcharts for processes\n- Matrices for frameworks (e.g., SWOT, BCG)\n- Organizational charts for structures\n\n### Spaced Repetition Integration\n\n#### Anki Card Generation (Optional Enhancement)\n\nFor each Bloom level, extract flashcard pairs:\n\n**Remember Level  Basic Cards**:\n```\nQ: What is [term]?\nA: [Definition]\n```\n\n**Understand Level  Conceptual Cards**:\n```\nQ: Explain [concept] in your own words\nA: [Explanation with example]\n```\n\n**Apply Level  Scenario Cards**:\n```\nQ: How would you apply [concept] to [scenario]?\nA: [Step-by-step application]\n```\n\nExport as Anki-compatible format:\n```markdown\n## Anki Cards\n\n### Basic\nQ: What is encapsulation? | A: The bundling of data with methods that operate on that data...\n\n### Conceptual\nQ: Explain polymorphism | A: The ability of objects of different types to respond to the same message...\n```\n\n### Collaborative Note Merging\n\n#### Multi-Reference Synthesis\n\nWhen user provides multiple reference files:\n\n**Step 1: Individual Analysis**\n- Process each file separately\n- Extract unique insights from each source\n- Note overlapping concepts\n\n**Step 2: Concept Consolidation**\n- Merge overlapping concepts (use most comprehensive explanation)\n- Identify complementary information (combine for fuller picture)\n- Preserve source-specific insights (attribute to specific references)\n\n**Step 3: Structure Optimization**\n- Organize content by Bloom level (not by source)\n- Distribute examples from different sources across levels\n- Cross-reference related ideas from multiple sources\n\n**Step 4: Citation Management**\n```markdown\n- **Concept X**: [Explanation synthesized from sources 1 and 3]\n  (References: [1], [3])\n\n- **Implementation approach**: [Specific method from source 2]\n  (Reference: [2])\n```\n\n### Custom Bloom Level Selection\n\n#### User-Specified Level Focus\n\nWhen user requests specific levels:\n\n**Request Pattern**: \"Generate notes focusing on Apply and Analyze levels\"\n\n**Adaptation**:\n1. Include only requested levels\n2. Expand point count to 7-9 per level (since fewer levels)\n3. Adjust practice questions to target only specified levels\n4. Modify diagram to emphasize relationships relevant to selected levels\n\n**Validation**:\n- Ensure at least 2 levels selected (minimum viable structure)\n- Warn if Remember/Understand omitted (may lack foundation)\n- Suggest complementary levels if selection seems incomplete\n\n### Quality Assurance Checklist\n\n#### Pre-Delivery Validation Protocol\n\n**Content Validation**:\n- [ ] All facts verified against authoritative sources\n- [ ] Definitions accurate and complete\n- [ ] Examples concrete and illustrative\n- [ ] No contradictory information across sections\n- [ ] Technical terminology used correctly\n\n**Structure Validation**:\n- [ ] Summary captures topic scope and objectives\n- [ ] Appropriate Bloom levels selected for topic type\n- [ ] Each level contains 5-7 substantive points\n- [ ] Logical progression within each level\n- [ ] Smooth transitions between levels\n\n**Diagram Validation**:\n- [ ] Diagram type matches topic structure\n- [ ] All nodes/elements labeled clearly\n- [ ] Relationships accurately represented\n- [ ] Visual hierarchy supports understanding\n- [ ] Mermaid syntax valid (no rendering errors)\n\n**Question Validation**:\n- [ ] 5-8 questions total\n- [ ] Questions distributed across selected Bloom levels\n- [ ] Question stems appropriate for each level\n- [ ] Questions answerable from note content\n- [ ] Variety in question formats and complexity\n\n**Reference Validation**:\n- [ ] All sources cited properly\n- [ ] URLs functional (if web sources)\n- [ ] Mix of introductory and advanced resources\n- [ ] Related topics relevant and useful\n- [ ] Attribution complete for direct quotations\n\n**Format Validation**:\n- [ ] Valid markdown syntax throughout\n- [ ] Consistent heading hierarchy (H1  H2  H3)\n- [ ] Proper code fence formatting\n- [ ] Lists formatted consistently\n- [ ] No broken links or references\n\n## Performance Optimization\n\n### Large Topic Handling\n\nFor extensive topics requiring comprehensive coverage:\n\n**Strategy 1: Modular Decomposition**\n- Break topic into subtopics\n- Generate separate note files for each subtopic\n- Create index/overview file linking subtopic notes\n\n**Strategy 2: Progressive Elaboration**\n- Generate foundation notes (Remember, Understand levels)\n- Offer to extend with application notes (Apply, Analyze levels)\n- Optionally add advanced notes (Evaluate, Create levels)\n\n### Rapid Note Generation\n\nFor quick reference needs:\n\n**Minimal Mode** (3-4 points per level):\n- Focus on essential concepts only\n- Single diagram\n- 3-4 practice questions\n- Key references only\n- Completion time: ~50% of standard generation\n\n**Trigger**: User requests \"quick notes\", \"brief summary\", or \"overview\"\n\n## Error Recovery Patterns\n\n### Incomplete Web Research\n\n**Scenario**: WebSearch returns limited or low-quality results\n\n**Recovery Strategy**:\n1. Attempt alternative search queries with different terminology\n2. Try WebFetch on known authoritative sources for the domain\n3. If still insufficient, inform user and offer options:\n   - Generate notes from available limited information (marked as \"Preliminary\")\n   - Request user provide additional reference materials\n   - Suggest specific sources for user to review\n\n### Diagram Rendering Failures\n\n**Scenario**: Complex Mermaid syntax causes rendering errors\n\n**Recovery Strategy**:\n1. Simplify diagram structure (reduce nodes/edges)\n2. Try alternative diagram type\n3. If Mermaid continues to fail, use ASCII art alternative:\n```\nTopic Hierarchy:\n Main Concept\n   Sub-concept 1\n     Detail A\n     Detail B\n   Sub-concept 2\n      Detail C\n      Detail D\n```\n\n### Topic Ambiguity Resolution\n\n**Scenario**: Topic could refer to multiple distinct subjects\n\n**Resolution Protocol**:\n1. Detect ambiguity through WebSearch result diversity\n2. Use AskUserQuestion tool to present options\n3. Await user selection\n4. Proceed with clarified topic scope\n\n**Example**:\nTopic: \"Python\"\n- Option 1: Python programming language\n- Option 2: Python (snake species)\n- Option 3: Monty Python (comedy group)\n\n## Advanced Customization\n\n### User Preference Profiles\n\nFor recurring users, maintain consistency through session memory:\n\n**Tracked Preferences**:\n- Preferred depth level (always moderate, or user varies)\n- Diagram preferences (prefers mind maps vs. flowcharts)\n- Question answer inclusion (with/without answers)\n- Reference detail level (minimal citations vs. extensive bibliography)\n\n**Application**: Adapt default settings based on user's historical preferences\n\n### Integration with External Tools\n\n#### Export Formats\n\n**Obsidian Integration**:\n```markdown\n---\ntags: [topic-category, bloom-taxonomy, study-notes]\ncreated: [date]\ntopics: [topic-name]\n---\n# [Topic Title]\n[Standard note content]\n\n[[Related Note 1]] [[Related Note 2]]\n```\n\n**Notion Integration**:\n- Use Notion-compatible markdown\n- Include database properties in frontmatter\n- Format toggles for collapsible sections\n\n**LaTeX Export**:\n- Convert markdown to LaTeX for academic papers\n- Transform Mermaid diagrams to TikZ\n- Format references in BibTeX\n\n## Meta-Learning Applications\n\n### Notes on Note-Taking\n\nThe notes-generator can recursively generate notes about learning methodologies:\n\n**Example Topics**:\n- \"Bloom's Taxonomy\" (meta: notes about the framework used to create notes)\n- \"Spaced Repetition Systems\"\n- \"Active Recall Techniques\"\n- \"Cornell Note-Taking Method\"\n\n**Special Handling**: Include comparative analysis with other learning frameworks\n\n### Self-Improvement Feedback Loop\n\nAfter generating multiple note sets, analyze patterns:\n- Which Bloom levels receive most user engagement (via follow-up questions)?\n- Which diagram types prove most effective for different domains?\n- Which practice question formats generate most user interaction?\n\n**Application**: Refine future note generation based on effectiveness patterns\n\n## References\n\n### Bloom's Taxonomy Resources\n\n- Anderson, L. W., & Krathwohl, D. R. (Eds.). (2001). *A Taxonomy for Learning, Teaching, and Assessing: A Revision of Bloom's Taxonomy of Educational Objectives*. Longman.\n- Bloom, B. S. (1956). *Taxonomy of Educational Objectives: The Classification of Educational Goals*. Longman.\n\n### Mermaid Diagram Documentation\n\n- [Mermaid Official Documentation](https://mermaid.js.org/)\n- [Mermaid Live Editor](https://mermaid.live/) - for diagram testing\n\n### Markdown Standards\n\n- [CommonMark Specification](https://commonmark.org/)\n- [GitHub Flavored Markdown Spec](https://github.github.com/gfm/)\n",
        "skills/study_system/.claude/skills/notes-generator/templates/note-template.md": "# [Topic Title]\n\n## Summary\n\n[3-5 sentence executive summary covering:]\n- [Topic definition and scope]\n- [Primary learning objectives]\n- [Key concepts that will be covered]\n- [Practical applications or relevance]\n\n**Learning Objectives:**\n- [Objective 1 - typically aligned with first Bloom level]\n- [Objective 2 - typically aligned with second Bloom level]\n- [Objective 3 - typically aligned with third Bloom level]\n- [Objective 4 - if applicable]\n\n---\n\n## Notes by Bloom's Taxonomy\n\n### 1. Remember: Foundational Knowledge\n*[Use this level for factual topics, terminology-heavy subjects, or when foundational recall is essential]*\n\n- **[Key Term/Concept 1]**: [Definition with context. Include example if helpful for memorization. Aim for 2-3 sentences.]\n\n- **[Key Term/Concept 2]**: [Definition with context. Include example if helpful for memorization. Aim for 2-3 sentences.]\n\n- **[Key Term/Concept 3]**: [Definition with context. Include example if helpful for memorization. Aim for 2-3 sentences.]\n\n- **[Key Fact/Date/Figure 4]**: [Factual information with significance explained. Why is this important to remember?]\n\n- **[Core Component 5]**: [Essential element with description. What role does it play in the larger topic?]\n\n*[Continue for 5-7 total points at moderate depth, or 4-5 for quick notes, or 7-9 for comprehensive coverage]*\n\n---\n\n### 2. Understand: Conceptual Comprehension\n*[Always include this level - fundamental for all learning]*\n\n- **[Concept Explanation 1]**: [Explain the concept in accessible language. What does it mean? How does it work? Include comparison/contrast if helpful. Example demonstrating the concept. 3-4 sentences.]\n\n- **[Concept Explanation 2]**: [Explain the concept in accessible language. What does it mean? How does it work? Include comparison/contrast if helpful. Example demonstrating the concept. 3-4 sentences.]\n\n- **[Cause-Effect Relationship]**: [Describe how one thing leads to another. What is the mechanism? What are the implications? Include concrete example.]\n\n- **[Interpretation of Core Idea]**: [Paraphrase and explain key idea. What does this mean in practical terms? How can we understand it intuitively?]\n\n- **[Comparison/Contrast]**: [How is concept X similar to or different from concept Y? What are the key distinguishing features? When would you use one vs. the other?]\n\n*[Continue for 5-7 total points]*\n\n---\n\n### 3. Apply: Practical Application\n*[Use for procedural, technical, or skill-based topics where application is relevant]*\n\n- **[Practical Use Case 1]**: [Describe how to apply the concept in real-world scenario. Include step-by-step procedure if applicable. What would you actually do? 3-4 sentences with code/example if relevant.]\n\n- **[Implementation Example 2]**: [Concrete example of applying the concept. Show actual usage with code snippet, formula, or detailed procedure.]\n\n- **[Problem-Solving Approach]**: [How would you solve a typical problem using this concept? What is the methodology? Walk through an example scenario.]\n\n- **[Real-World Application]**: [Where is this concept used in practice? Industry examples, professional contexts, or everyday situations where it applies.]\n\n- **[Procedure/Algorithm]**: [Step-by-step method for performing task or solving problem. Include decision points or variations if applicable.]\n\n*[Continue for 5-7 total points]*\n\n---\n\n### 4. Analyze: Critical Examination\n*[Use for topics requiring decomposition, pattern recognition, or comparative analysis]*\n\n- **[Component Breakdown]**: [Break down complex concept into constituent parts. How do the parts relate? What is the structure? How do components interact?]\n\n- **[Relationship Mapping]**: [Identify and explain relationships between elements. What depends on what? What influences what? Causal connections?]\n\n- **[Pattern Identification]**: [What patterns emerge from the data/information? What regularities can we observe? What do these patterns tell us?]\n\n- **[Comparative Analysis]**: [Compare and contrast different approaches, methods, or perspectives. What are relative strengths/weaknesses? When is each appropriate?]\n\n- **[Critical Distinction]**: [What is the key difference that matters? What do people often confuse? How can we reliably distinguish between similar concepts?]\n\n*[Continue for 5-7 total points]*\n\n---\n\n### 5. Evaluate: Assessment and Judgment\n*[Use for topics where critical assessment, quality judgments, or decision-making is relevant]*\n\n- **[Assessment Criteria]**: [What standards or criteria should we use to evaluate this? What makes one approach better than another? How do we measure quality/success?]\n\n- **[Strengths and Limitations]**: [What are the advantages of this approach/theory/method? What are its weaknesses or boundary conditions? When does it work well/poorly?]\n\n- **[Critical Appraisal]**: [Evaluate the merit, validity, or effectiveness of concept/approach. What is the evidence? What are counterarguments or criticisms?]\n\n- **[Comparative Evaluation]**: [Compare multiple approaches on quality dimensions. Which is better for what purposes? What trade-offs exist?]\n\n- **[Evidence-Based Judgment]**: [Based on available evidence, what can we conclude? How strong is the support for this claim? What uncertainty remains?]\n\n*[Continue for 5-7 total points]*\n\n---\n\n### 6. Create: Synthesis and Innovation\n*[Use for topics where creative application, design, or integration is relevant]*\n\n- **[Synthesis Opportunity]**: [How can we combine concepts in novel ways? What new insights emerge from integrating ideas from different domains?]\n\n- **[Design Principles]**: [If you were to design/create something using this knowledge, what principles would guide you? What would you need to consider?]\n\n- **[Original Application]**: [Propose a novel use or application of the concept. How could it solve a new problem? What unconventional contexts might benefit?]\n\n- **[Innovation Possibility]**: [What are opportunities for improving or extending the current concept/approach? What hasn't been tried yet? What would be the next evolution?]\n\n- **[Integration with Other Concepts]**: [How can this concept be combined with other knowledge domains? What interdisciplinary applications are possible?]\n\n- **[Generative Thinking Prompt]**: [What new questions does this raise? What would we need to develop next? How might this transform related fields?]\n\n*[Continue for 5-7 total points]*\n\n---\n\n## Visual Concept Map\n\n*[Select appropriate diagram type based on topic structure]*\n\n**For Hierarchical Topics (Categories, Classifications):**\n```mermaid\ngraph TD\n    A[Main Concept] --> B[Category 1]\n    A --> C[Category 2]\n    A --> D[Category 3]\n    B --> E[Subconcept 1.1]\n    B --> F[Subconcept 1.2]\n    C --> G[Subconcept 2.1]\n    C --> H[Subconcept 2.2]\n\n    style A fill:#e1f5ff\n    style B fill:#fff4e1\n    style C fill:#fff4e1\n    style D fill:#fff4e1\n```\n\n**For Process/Sequential Topics (Workflows, Procedures):**\n```mermaid\nflowchart LR\n    A[Start] --> B[Step 1]\n    B --> C[Step 2]\n    C --> D{Decision Point}\n    D -->|Option 1| E[Path A]\n    D -->|Option 2| F[Path B]\n    E --> G[Outcome]\n    F --> G\n```\n\n**For Relational Topics (Networks, Interconnections):**\n```mermaid\ngraph LR\n    A[Concept A] <--> B[Concept B]\n    B <--> C[Concept C]\n    C <--> D[Concept D]\n    D <--> A\n    A -.-> E[Related Concept]\n    C -.-> F[Related Concept]\n```\n\n**For Multi-Dimensional Topics (Aspects, Facets):**\n```mermaid\nmindmap\n  root((Central Topic))\n    Dimension 1\n      Aspect 1.1\n      Aspect 1.2\n      Aspect 1.3\n    Dimension 2\n      Aspect 2.1\n      Aspect 2.2\n    Dimension 3\n      Aspect 3.1\n      Aspect 3.2\n      Aspect 3.3\n```\n\n**For Timeline/Historical Topics:**\n```mermaid\ntimeline\n    title [Timeline Title]\n    [Period 1] : Event 1\n               : Event 2\n    [Period 2] : Event 3\n               : Event 4\n    [Period 3] : Event 5\n               : Event 6\n```\n\n---\n\n## Practice Questions\n\n*[Create 5-8 questions distributed across Bloom levels. Align question stems with cognitive level.]*\n\n### Knowledge & Comprehension\n\n1. **[Remember-level question using stems: Define, List, Name, Identify]**\n   [Question targeting recall or basic understanding]\n\n2. **[Understand-level question using stems: Explain, Describe, Summarize]**\n   [Question requiring explanation or interpretation]\n\n3. **[Additional Comprehension question]**\n   [Question testing conceptual understanding]\n\n### Application & Analysis\n\n4. **[Apply-level question using stems: Apply, Demonstrate, Use, Solve]**\n   [Question requiring application to specific scenario or problem]\n\n5. **[Analyze-level question using stems: Analyze, Compare, Examine, Differentiate]**\n   [Question requiring breakdown, comparison, or pattern identification]\n\n6. **[Additional Analysis question]**\n   [Question testing analytical thinking]\n\n### Evaluation & Creation\n\n7. **[Evaluate-level question using stems: Evaluate, Assess, Critique, Judge]**\n   [Question requiring judgment, assessment, or critical evaluation]\n\n8. **[Create-level question using stems: Design, Create, Develop, Propose]**\n   [Question requiring synthesis, design, or novel application]\n\n*[Optional: Include answers in collapsible sections for self-study support]*\n\n---\n\n## References and Further Reading\n\n### Primary Sources\n- [Citation 1: If content derived from specific source, cite it here]\n- [Citation 2: Academic papers, textbooks, official documentation]\n- [Citation 3: Research studies or authoritative references]\n\n### Recommended Resources\n- [Resource 1: Textbook or comprehensive guide with brief description]\n- [Resource 2: Online course, tutorial, or interactive learning platform with URL]\n- [Resource 3: Video series, lecture notes, or educational content]\n- [Resource 4: Tool, software, or hands-on resource for practice]\n\n### Related Topics\n*[Suggest 3-5 related topics for further exploration]*\n- [Related Topic 1: Brief explanation of relevance or connection]\n- [Related Topic 2: How it builds on or extends current topic]\n- [Related Topic 3: Complementary or contrasting perspective]\n- [Related Topic 4: Advanced or specialized area]\n\n---\n\n## Template Usage Notes\n\n**Customization Guidelines:**\n- Remove Bloom levels not relevant to topic (minimum 3 levels, maximum 6)\n- Adjust point count: 4-5 for quick notes, 5-7 for moderate, 7-9 for comprehensive\n- Select diagram type matching topic structure (hierarchy, process, network, mindmap, timeline)\n- Tailor language complexity to target audience (high school, undergraduate, professional)\n- Include code examples for programming topics, formulas for mathematics, diagrams for visual topics\n- Emphasize different Bloom levels based on discipline (STEM  Apply/Analyze, Humanities  Understand/Evaluate)\n\n**Quality Checklist:**\n- [ ] Summary clearly defines topic and learning objectives\n- [ ] Appropriate Bloom levels selected for topic type\n- [ ] Each level contains sufficient substantive points (5-7 moderate)\n- [ ] Visual diagram accurately represents concept relationships\n- [ ] Practice questions distributed across taxonomy levels\n- [ ] References include quality sources and further reading\n- [ ] Markdown formatting correct and consistent\n",
        "skills/study_system/.claude/skills/quiz-generator/SKILL.md": "---\nname: quiz-generator\ndescription: Generate comprehensive educational quizzes based on Bloom's Taxonomy methodology (Remember, Understand, Apply, Analyze, Evaluate, Create). Creates structured True/False quizzes with detailed answer keys and explanations. Use when user requests quiz generation, assessment creation, test materials, practice questions, mentions Bloom's Taxonomy, or provides educational topics for quiz creation. Activates for study topics, course materials, reference files (.md, .txt, .pdf), or educational content requiring systematic assessment.\n---\n\n# Quiz Generator\n\n## Purpose\n\nThis skill generates comprehensive educational quizzes systematically organized according to Bloom's Taxonomy cognitive levels. It produces True/False assessments with detailed answer keys, explanations, and cognitive level classifications to facilitate learning evaluation and knowledge retention.\n\n## Bloom's Taxonomy Cognitive Levels\n\nThe quiz questions shall be organized across six hierarchical cognitive levels:\n\n1. **Remember**: Retrieve relevant knowledge from long-term memory (recall, recognize, identify)\n2. **Understand**: Construct meaning from instructional messages (interpret, exemplify, classify, summarize, infer, compare, explain)\n3. **Apply**: Carry out or use a procedure in a given situation (execute, implement)\n4. **Analyze**: Break material into constituent parts and determine relationships (differentiate, organize, attribute)\n5. **Evaluate**: Make judgments based on criteria and standards (check, critique, judge)\n6. **Create**: Put elements together to form a coherent whole; reorganize into new pattern (generate, plan, produce, design)\n\n## Instructions\n\nWhen invoked, execute the following systematic procedure:\n\n### Phase 1: Input Acquisition and Analysis\n\n**Step 1.1**: Determine input source\n\nIdentify whether the user has provided:\n- A topic title (text-based subject specification)\n- A reference file path (existing educational material)\n- Both topic and supporting reference materials\n\n**Step 1.2**: Process reference materials (if applicable)\n\nIf a reference file is provided:\n- Use the Read tool to extract content from the specified file path\n- Analyze the content structure, key concepts, and learning objectives\n- Identify core terminology, principles, relationships, and factual assertions\n- Extract subject domain and complexity level\n- Note any existing misconceptions or common errors mentioned\n\n**Step 1.3**: Conduct supplementary research (if necessary)\n\nIf the topic is unfamiliar or requires current information:\n- Employ WebSearch tool to locate authoritative educational resources\n- Use WebFetch tool to retrieve comprehensive explanatory content\n- Synthesize multiple sources to ensure accuracy and depth\n- Prioritize academic, educational, and authoritative domain sources\n- Identify common misconceptions in the domain for use in False statements\n\n### Phase 2: Content Analysis and Concept Mapping\n\n**Step 2.1**: Identify core concepts and factual assertions\n\nExtract or formulate:\n- Primary concepts requiring mastery\n- Fundamental terminology and definitions\n- Key principles, theories, or methodologies\n- Factual statements that can be verified as true or false\n- Relationships between concepts\n- Common misconceptions or errors in the domain\n- Procedural knowledge and application contexts\n\n**Step 2.2**: Assess content complexity\n\nDetermine appropriate difficulty distribution:\n- **Basic**: Foundational concepts, simple factual statements\n- **Intermediate**: Conceptual relationships, application scenarios\n- **Advanced**: Complex analytical statements, evaluative judgments\n\n**Step 2.3**: Map concepts to Bloom's Taxonomy levels\n\nSystematically categorize identified concepts according to cognitive complexity:\n- Level 1 (Remember): Factual statements, definitions, basic identification\n- Level 2 (Understand): Conceptual relationships, interpretations, classifications\n- Level 3 (Apply): Procedural applications, scenario-based implementations\n- Level 4 (Analyze): Structural relationships, comparative analysis, differentiations\n- Level 5 (Evaluate): Criteria-based judgments, quality assessments, validity claims\n- Level 6 (Create): Design principles, synthesis requirements, optimal solutions\n\n### Phase 3: Quiz Question Generation\n\n**Step 3.1**: Generate 2-3 True/False questions per Bloom's level\n\nFor each cognitive level, create True/False questions following this structure:\n\n**True Statement Questions**:\n- Formulate accurate statements that test key knowledge at the appropriate cognitive level\n- Ensure statements are unambiguously true\n- Avoid trivial or overly obvious truths\n- Include sufficient context for the statement to be evaluated\n\n**False Statement Questions**:\n- Create plausible but incorrect statements\n- Base false statements on common misconceptions\n- Avoid trick questions with trivial negations (e.g., not just adding \"not\" to a true statement)\n- Ensure the false element is pedagogically meaningful\n- Make false statements subtle enough to require genuine understanding\n\n**Step 3.2**: Ensure cognitive alignment\n\nVerify each question employs appropriate cognitive complexity:\n\n**Remember Level** (True/False about facts):\n- True: \"Python is an interpreted programming language.\"\n- False: \"Python was created in the 1970s.\" (Actually created in 1991)\n\n**Understand Level** (True/False about concepts and relationships):\n- True: \"Encapsulation in OOP involves bundling data and methods that operate on that data within a single unit.\"\n- False: \"Inheritance and composition serve identical purposes in object-oriented design.\" (They serve different purposes)\n\n**Apply Level** (True/False about applications):\n- True: \"In a binary search tree with root value 10, a node with value 5 would be located in the left subtree.\"\n- False: \"The quicksort algorithm always has O(n log n) time complexity regardless of input.\" (Worst case is O(n))\n\n**Analyze Level** (True/False about analysis):\n- True: \"The primary difference between arrays and linked lists is that arrays provide O(1) random access while linked lists require O(n) traversal.\"\n- False: \"Hash tables and binary search trees have identical time complexity characteristics for all operations.\" (They differ significantly)\n\n**Evaluate Level** (True/False about judgments):\n- True: \"Using a microservices architecture is more appropriate than a monolithic architecture when different components need to scale independently.\"\n- False: \"Test-driven development (TDD) is always superior to writing tests after implementation in all software projects.\" (Context-dependent)\n\n**Create Level** (True/False about design and synthesis):\n- True: \"When designing a caching system for a high-traffic website, setting expiration times based on data update frequency is a sound strategy.\"\n- False: \"The optimal database schema design always involves complete normalization to 3rd normal form without exception.\" (Denormalization is sometimes appropriate)\n\n**Step 3.3**: Maintain balanced True/False distribution\n\nAcross the entire quiz, maintain approximately:\n- 50% True statements\n- 50% False statements\n\nAvoid patterns (e.g., alternating T/F or multiple consecutive same answers) that enable answering without reading.\n\n### Phase 4: Answer Key and Explanation Generation\n\n**Step 4.1**: Create comprehensive answer key\n\nFor each question, provide:\n\n```markdown\n**Question [Number]**: [Bloom's Level] - [Difficulty]\n\n**Statement**: [The True/False statement]\n\n**Answer**: [True/False]\n\n**Explanation**:\n[Detailed explanation of why the statement is true or false, including:\n- Core concept being tested\n- Relevant facts or principles\n- Why the statement is correct or what makes it incorrect\n- Additional context for understanding]\n\n**Common Misconception** (if applicable):\n[Why students might incorrectly answer this question]\n\n**Related Concepts**:\n- [Related concept 1]\n- [Related concept 2]\n```\n\n**Step 4.2**: Ensure pedagogical value\n\nVerify that explanations:\n- Clarify the underlying concept, not just the answer\n- Provide learning value beyond answer verification\n- Address potential misconceptions\n- Connect to broader conceptual understanding\n- Reference authoritative principles where applicable\n\n### Phase 5: Quality Assurance\n\n**Step 5.1**: Verify statement accuracy\n\nConfirm all True statements are factually correct and all False statements are genuinely incorrect. Cross-reference authoritative sources when uncertain.\n\n**Step 5.2**: Assess cognitive level alignment\n\nEnsure each question genuinely tests the designated Bloom's level:\n- Remember: Tests factual recall, not reasoning\n- Understand: Tests conceptual comprehension, not mere memorization\n- Apply: Tests procedural application in scenarios\n- Analyze: Tests ability to identify relationships and structures\n- Evaluate: Tests judgment based on criteria\n- Create: Tests understanding of optimal design and synthesis\n\n**Step 5.3**: Eliminate ambiguity\n\nVerify that:\n- Statements are clear and unambiguous\n- No statement can be reasonably interpreted as both true and false\n- Context is sufficient for evaluation\n- Terminology is used correctly and consistently\n\n**Step 5.4**: Check pedagogical quality\n\nEnsure:\n- False statements represent meaningful misconceptions, not trivial errors\n- Questions test important concepts, not obscure trivia\n- Difficulty is appropriate for the target learning level\n- Coverage is comprehensive across the topic domain\n\n### Phase 6: Output Formatting and Delivery\n\n**Step 6.1**: Structure the quiz document\n\nOrganize the output markdown file with the following structure:\n\n```markdown\n# Quiz: [Topic Title]\n\n**Generated**: [Current Date]\n**Cognitive Framework**: Bloom's Taxonomy\n**Question Type**: True/False\n**Total Questions**: [Count]\n**Recommended Time**: [Estimate based on question count]\n\n---\n\n## Instructions\n\nRead each statement carefully and determine whether it is TRUE or FALSE. Consider the statement as a whole and apply your understanding of [topic].\n\n---\n\n## Questions\n\n### Level 1: Remember (Knowledge Recall)\n\n**Question 1**: [Bloom's Level - Remember] [Difficulty: Basic/Intermediate/Advanced]\n\n[True/False statement]\n\n- [ ] True\n- [ ] False\n\n---\n\n**Question 2**: [Bloom's Level - Remember] [Difficulty: Basic/Intermediate/Advanced]\n\n[True/False statement]\n\n- [ ] True\n- [ ] False\n\n---\n\n[Continue for all Remember level questions]\n\n### Level 2: Understand (Comprehension)\n\n[Questions 3-5 for Understand level]\n\n### Level 3: Apply (Application)\n\n[Questions 6-8 for Apply level]\n\n### Level 4: Analyze (Analysis)\n\n[Questions 9-11 for Analyze level]\n\n### Level 5: Evaluate (Evaluation)\n\n[Questions 12-14 for Evaluate level]\n\n### Level 6: Create (Synthesis)\n\n[Questions 15-18 for Create level]\n\n---\n\n## Answer Key\n\n### Level 1: Remember\n\n**Question 1**: Remember - [Difficulty]\n\n**Statement**: [Repeat statement]\n\n**Answer**: [True/False]\n\n**Explanation**:\n[Detailed explanation]\n\n**Common Misconception** (if applicable):\n[Why students might answer incorrectly]\n\n**Related Concepts**:\n- [Concept 1]\n- [Concept 2]\n\n---\n\n[Continue for all questions across all levels]\n\n---\n\n## Quiz Statistics\n\n- **Total Questions**: [Number]\n- **True Statements**: [Count]\n- **False Statements**: [Count]\n- **Distribution by Level**:\n  - Remember: [Count] questions\n  - Understand: [Count] questions\n  - Apply: [Count] questions\n  - Analyze: [Count] questions\n  - Evaluate: [Count] questions\n  - Create: [Count] questions\n\n---\n\n## Study Recommendations\n\n[Provide guidance on how to use this quiz effectively for learning]\n```\n\n**Step 6.2**: Generate output file\n\nUse the Write tool to create a markdown file named:\n`[topic-name]-quiz.md`\n\nWhere `[topic-name]` is the kebab-case version of the topic title.\n\n**Step 6.3**: Deliver completion summary\n\nProvide the user with:\n- Confirmation of successful generation\n- Total question count (target: 12-18 questions)\n- File path for the generated quiz\n- Recommended completion time estimate\n- Brief usage recommendations\n\n## Advanced Considerations\n\n### Avoiding Common Pitfalls in True/False Question Design\n\n**Pitfall 1: Trivial Negation**\n- **Poor**: \"Python is not an interpreted language.\" (Just negates a fact)\n- **Better**: \"Python compiles directly to machine code like C.\" (Tests understanding of compilation vs interpretation)\n\n**Pitfall 2: Absolute Language Creating Obvious Answers**\n- **Poor**: \"All loops in programming are while loops.\" (Absolute language signals false)\n- **Better**: \"Loop structures in programming serve the fundamental purpose of repeating code blocks.\" (True statement without absolute language)\n\n**Pitfall 3: Multiple Concepts in Single Statement**\n- **Poor**: \"Python is interpreted and dynamically typed and uses garbage collection.\" (Multiple testable elements)\n- **Better**: Split into separate questions testing each concept individually\n\n**Pitfall 4: Opinion Presented as Fact**\n- **Poor**: \"Python is the best programming language for beginners.\" (Subjective opinion)\n- **Better**: \"Python's syntax is designed to emphasize code readability.\" (Verifiable design principle)\n\n### Crafting Effective False Statements\n\nFalse statements should:\n1. **Test Misconceptions**: Base false statements on actual common errors students make\n2. **Require Understanding**: Students must understand the concept to identify the falsehood\n3. **Be Plausible**: False statements should seem potentially true to someone with incomplete knowledge\n4. **Avoid Trick Wording**: Don't rely on tricky phrasing or obscure semantic details\n\n**Example of Effective False Statement**:\n- Statement: \"In object-oriented programming, composition means one class inheriting from another class.\"\n- Why Effective: Confuses composition with inheritance, a common misconception; requires understanding the distinction\n- Not Effective Alternative: \"Composition is not a concept in object-oriented programming.\" (Obviously false, not educational)\n\n### Balancing Difficulty\n\nDistribute questions across difficulty levels:\n- **Basic** (40%): Test foundational knowledge and clear concepts\n- **Intermediate** (40%): Test application and conceptual relationships\n- **Advanced** (20%): Test nuanced understanding and complex scenarios\n\nHigher Bloom's levels (Analyze, Evaluate, Create) naturally tend toward intermediate/advanced difficulty.\n\n## Error Handling\n\n**Insufficient Input**:\nIf the user provides neither a clear topic nor reference file, request:\n- Specific topic title or subject area\n- Optional reference file path for context\n\n**Reference File Unavailable**:\nIf the specified reference file cannot be read:\n- Inform the user of the file access issue\n- Offer to proceed with topic-based generation using web research\n\n**Topic Unfamiliarity**:\nIf the topic is highly specialized or obscure:\n- Conduct thorough web research using WebSearch and WebFetch\n- Inform the user that research-based generation is in progress\n- Request user validation of accuracy for highly technical domains\n\n**Insufficient Content**:\nIf the topic or reference material provides insufficient content for 12-18 questions:\n- Generate fewer questions while maintaining Bloom's level distribution\n- Inform user of reduced question count and reasoning\n- Suggest supplementary topics or materials for comprehensive coverage\n\n## Quality Standards\n\nAll generated quizzes shall conform to:\n\n1. **Bloom's Taxonomy Alignment**: Each question correctly categorized by cognitive level\n2. **Statement Clarity**: All True/False statements are unambiguous and clearly written\n3. **Factual Accuracy**: All True statements are verifiable; all False statements are genuinely incorrect\n4. **Pedagogical Value**: Questions test important concepts and meaningful understanding\n5. **Answer Key Completeness**: Every question has detailed explanation and related concepts\n6. **Balanced Distribution**: Approximately 50% True, 50% False across entire quiz\n7. **Misconception-Based**: False statements target actual common misunderstandings\n8. **Difficulty Appropriateness**: Complexity matches designated Bloom's level and difficulty rating\n\n## Example Invocation Scenarios\n\n**Scenario 1**: Topic-based generation\n```\nUser: \"Generate a quiz for Python list comprehensions\"\nAgent: [Conducts web research, generates 12-18 True/False questions across Bloom's levels]\n```\n\n**Scenario 2**: Reference file-based generation\n```\nUser: \"Create a quiz from my machine learning notes at notes/ml-basics.md\"\nAgent: [Reads file, extracts concepts, generates structured quiz with answer key]\n```\n\n**Scenario 3**: Combined approach\n```\nUser: \"Generate a quiz for photosynthesis based on my biology-notes.pdf\"\nAgent: [Reads PDF, supplements with web research, generates comprehensive quiz]\n```\n\n## References\n\nThis skill implements pedagogical principles derived from:\n- Bloom's Taxonomy of Educational Objectives (Bloom et al., 1956; Anderson & Krathwohl, 2001)\n- Assessment design principles for True/False question construction\n- Cognitive science research on effective assessment and misconception identification\n- Educational testing standards for valid and reliable assessment instruments\n",
        "skills/study_system/.claude/skills/quiz-generator/examples.md": "# Quiz Generation Examples\n\nThis document demonstrates sample outputs from the quiz-generator skill across various educational domains and Bloom's Taxonomy levels.\n\n---\n\n## Example 1: Computer Science Topic - \"Hash Tables\"\n\n### Complete Generated Quiz\n\n```markdown\n# Quiz: Hash Tables\n\n**Generated**: 2025-12-19\n**Cognitive Framework**: Bloom's Taxonomy\n**Question Type**: True/False\n**Total Questions**: 15\n**Recommended Time**: 20-25 minutes\n\n---\n\n## Instructions\n\nRead each statement carefully and determine whether it is TRUE or FALSE. Consider the statement as a whole and apply your understanding of hash tables, hash functions, and collision resolution techniques.\n\n---\n\n## Questions\n\n### Level 1: Remember (Knowledge Recall)\n\n**Question 1**: Remember - Basic\n\nA hash table is a data structure that maps keys to values using a hash function.\n\n- [ ] True\n- [ ] False\n\n---\n\n**Question 2**: Remember - Basic\n\nHash tables provide O(1) average-case time complexity for insertion, deletion, and lookup operations.\n\n- [ ] True\n- [ ] False\n\n---\n\n**Question 3**: Remember - Intermediate\n\nChaining and open addressing are two common methods for resolving hash collisions.\n\n- [ ] True\n- [ ] False\n\n---\n\n### Level 2: Understand (Comprehension)\n\n**Question 4**: Understand - Intermediate\n\nThe purpose of a hash function in a hash table is to convert keys into array indices that distribute elements uniformly across the table.\n\n- [ ] True\n- [ ] False\n\n---\n\n**Question 5**: Understand - Intermediate\n\nA collision occurs in a hash table when two different keys produce the same hash value and thus map to the same array index.\n\n- [ ] True\n- [ ] False\n\n---\n\n### Level 3: Apply (Application)\n\n**Question 6**: Apply - Intermediate\n\nIf a hash table uses chaining for collision resolution and has a load factor greater than 1, some buckets must contain more than one element.\n\n- [ ] True\n- [ ] False\n\n---\n\n**Question 7**: Apply - Advanced\n\nWhen implementing a hash table with open addressing, the probe sequence must eventually visit every position in the table to guarantee that an empty slot can be found if one exists.\n\n- [ ] True\n- [ ] False\n\n---\n\n**Question 8**: Apply - Intermediate\n\nInserting elements into a hash table in sorted order will result in better performance than inserting them in random order.\n\n- [ ] True\n- [ ] False\n\n---\n\n### Level 4: Analyze (Analysis)\n\n**Question 9**: Analyze - Advanced\n\nThe primary advantage of chaining over open addressing for collision resolution is that chaining never requires the table to be resized regardless of how many elements are inserted.\n\n- [ ] True\n- [ ] False\n\n---\n\n**Question 10**: Analyze - Advanced\n\nHash tables and binary search trees differ fundamentally in that hash tables do not maintain any ordering of elements while binary search trees do.\n\n- [ ] True\n- [ ] False\n\n---\n\n**Question 11**: Analyze - Intermediate\n\nThe choice of hash function has no impact on hash table performance as long as it produces valid array indices.\n\n- [ ] True\n- [ ] False\n\n---\n\n### Level 5: Evaluate (Evaluation)\n\n**Question 12**: Evaluate - Advanced\n\nUsing a cryptographic hash function like SHA-256 for a hash table is always superior to using simpler hash functions because it eliminates all collisions.\n\n- [ ] True\n- [ ] False\n\n---\n\n**Question 13**: Evaluate - Advanced\n\nA hash table is more appropriate than a balanced binary search tree when the primary operations are exact-match lookups and the application does not require ordered traversal of elements.\n\n- [ ] True\n- [ ] False\n\n---\n\n### Level 6: Create (Synthesis)\n\n**Question 14**: Create - Advanced\n\nWhen designing a hash table for a spell-checker application with a fixed dictionary, setting the initial table size to approximately twice the number of dictionary words represents a sound design decision that balances memory usage and lookup performance.\n\n- [ ] True\n- [ ] False\n\n---\n\n**Question 15**: Create - Advanced\n\nTo design an efficient hash table for storing strings, using the sum of ASCII values of all characters as the hash function would provide optimal distribution and minimal collisions.\n\n- [ ] True\n- [ ] False\n\n---\n\n## Answer Key\n\n### Level 1: Remember\n\n**Question 1**: Remember - Basic\n\n**Statement**: A hash table is a data structure that maps keys to values using a hash function.\n\n**Answer**: **True**\n\n**Explanation**:\nThis is the fundamental definition of a hash table. A hash table uses a hash function to compute an index (hash value) from a key, which determines where the associated value is stored in an underlying array. This mapping from keys to values via hashing is the defining characteristic of hash tables.\n\n**Related Concepts**:\n- Hash Function: Mathematical function that maps keys to array indices\n- Key-Value Pair: The fundamental unit stored in a hash table\n- Array: Underlying data structure that stores the values\n\n---\n\n**Question 2**: Remember - Basic\n\n**Statement**: Hash tables provide O(1) average-case time complexity for insertion, deletion, and lookup operations.\n\n**Answer**: **True**\n\n**Explanation**:\nUnder the assumption of a good hash function that distributes keys uniformly and maintains a reasonable load factor, hash tables achieve constant-time average-case performance for basic operations. This is one of the primary advantages of hash tables. Note that worst-case complexity can degrade to O(n) when all keys hash to the same location, but average-case assumes uniform distribution.\n\n**Related Concepts**:\n- Load Factor: Ratio of elements to table size, affects performance\n- Average-Case vs Worst-Case: Distinction between typical and pathological performance\n- Amortized Analysis: Hash table resize operations affect overall complexity\n\n---\n\n**Question 3**: Remember - Intermediate\n\n**Statement**: Chaining and open addressing are two common methods for resolving hash collisions.\n\n**Answer**: **True**\n\n**Explanation**:\nThese are indeed the two primary categories of collision resolution strategies. Chaining uses secondary data structures (typically linked lists) to store multiple elements that hash to the same index. Open addressing stores all elements directly in the hash table array and uses probing sequences to find alternative locations when collisions occur. Other methods exist but these are the most fundamental and widely used approaches.\n\n**Related Concepts**:\n- Collision Resolution: Strategies for handling when multiple keys hash to same index\n- Separate Chaining: Collision resolution using linked lists or other structures\n- Linear Probing: Form of open addressing that checks sequential positions\n- Quadratic Probing: Form of open addressing with quadratic probe sequence\n\n---\n\n### Level 2: Understand\n\n**Question 4**: Understand - Intermediate\n\n**Statement**: The purpose of a hash function in a hash table is to convert keys into array indices that distribute elements uniformly across the table.\n\n**Answer**: **True**\n\n**Explanation**:\nThis statement correctly describes the dual purpose of a hash function: (1) converting arbitrary keys into valid array indices (integer values within the table's range), and (2) distributing elements uniformly to minimize collisions and maintain efficient performance. A good hash function achieves both transformation and uniform distribution. Poor distribution leads to clustering and degrades performance.\n\n**Related Concepts**:\n- Uniform Distribution: Ideal property where all indices are equally likely\n- Hash Function Properties: Deterministic, efficient to compute, minimizes collisions\n- Clustering: Tendency of elements to group together, reducing efficiency\n\n---\n\n**Question 5**: Understand - Intermediate\n\n**Statement**: A collision occurs in a hash table when two different keys produce the same hash value and thus map to the same array index.\n\n**Answer**: **True**\n\n**Explanation**:\nThis is the precise definition of a collision. Even with a well-designed hash function, collisions are inevitable due to the pigeonhole principle: when the key space is larger than the table size (which is almost always true), multiple keys must map to the same index. Understanding collisions is fundamental to understanding hash table behavior and the necessity of collision resolution strategies.\n\n**Related Concepts**:\n- Pigeonhole Principle: If n items are placed into m containers with n > m, at least one container must contain multiple items\n- Collision Resolution: Necessary strategies for handling when collisions occur\n- Perfect Hashing: Special case where collisions can be eliminated (requires knowing all keys in advance)\n\n---\n\n### Level 3: Apply\n\n**Question 6**: Apply - Intermediate\n\n**Statement**: If a hash table uses chaining for collision resolution and has a load factor greater than 1, some buckets must contain more than one element.\n\n**Answer**: **True**\n\n**Explanation**:\nThe load factor  = n/m where n is the number of elements and m is the number of buckets. If  > 1, then n > m, meaning there are more elements than buckets. By the pigeonhole principle, at least one bucket must contain more than one element. This is a direct application of mathematical reasoning to hash table properties. With chaining, load factors greater than 1 are acceptable since each bucket can hold multiple elements via linked lists.\n\n**Related Concepts**:\n- Load Factor: Key metric for hash table performance ( = n/m)\n- Chaining Capacity: Unlike open addressing, chaining allows load factor > 1\n- Pigeonhole Principle: Applied reasoning about distribution\n\n---\n\n**Question 7**: Apply - Advanced\n\n**Statement**: When implementing a hash table with open addressing, the probe sequence must eventually visit every position in the table to guarantee that an empty slot can be found if one exists.\n\n**Answer**: **True**\n\n**Explanation**:\nFor open addressing to correctly insert elements when the table is not full, the probe sequence must have the property of eventually examining every table position. If the probe sequence could never reach certain positions, those positions would be wasted and the table could appear full even with empty slots. Linear probing (h(k,i) = (h(k) + i) mod m) satisfies this property. However, some probe sequences (like using a stride that shares common factors with table size) do NOT satisfy this property, which is why table sizes are often chosen as prime numbers.\n\n**Related Concepts**:\n- Probe Sequence: Series of positions examined during insertion/lookup\n- Linear Probing: Simplest probe sequence, guaranteed to visit all positions\n- Double Hashing: More complex probing that also visits all positions (with proper design)\n- Prime Table Sizes: Helps ensure complete probe sequences\n\n---\n\n**Question 8**: Apply - Intermediate\n\n**Statement**: Inserting elements into a hash table in sorted order will result in better performance than inserting them in random order.\n\n**Answer**: **False**\n\n**Explanation**:\nHash tables do not maintain or benefit from sorted order. In fact, insertion order has no impact on hash table performance assuming a good hash function. The hash function distributes keys based on their values, not their insertion sequence. This distinguishes hash tables from data structures like binary search trees, where insertion order can significantly affect structure (e.g., inserting sorted data into an unbalanced BST creates a degenerate linear tree). Random vs. sorted insertion order is irrelevant for hash table performance.\n\n**Common Misconception**:\nStudents may assume that sorted insertion benefits all data structures, or may confuse hash tables with order-dependent structures like BSTs.\n\n**Related Concepts**:\n- Hash Function Independence: Performance depends on hash function distribution, not insertion order\n- Binary Search Trees: Where insertion order DOES matter for balance\n- Load Factor: The actual determinant of hash table performance\n\n---\n\n### Level 4: Analyze\n\n**Question 9**: Analyze - Advanced\n\n**Statement**: The primary advantage of chaining over open addressing for collision resolution is that chaining never requires the table to be resized regardless of how many elements are inserted.\n\n**Answer**: **False**\n\n**Explanation**:\nWhile chaining can function with load factors greater than 1 (unlike open addressing which requires  < 1), chaining DOES benefit from resizing when the load factor becomes too large. As the load factor increases, the chain lengths grow proportionally, degrading performance from O(1) toward O(n). Practical implementations resize chained hash tables when  exceeds a threshold (commonly around 0.75-1.0) to maintain efficient performance. The actual primary advantages of chaining over open addressing are: (1) simpler deletion, and (2) more graceful performance degradation as load increases.\n\n**Common Misconception**:\nStudents may believe chaining eliminates the need for resizing because it doesn't have the hard constraint of  < 1.\n\n**Related Concepts**:\n- Load Factor Thresholds: Typical trigger for resizing (e.g.,  > 0.75)\n- Dynamic Resizing: Amortized O(1) cost when doubling table size\n- Performance Degradation: Chain length grows linearly with load factor\n\n---\n\n**Question 10**: Analyze - Advanced\n\n**Statement**: Hash tables and binary search trees differ fundamentally in that hash tables do not maintain any ordering of elements while binary search trees do.\n\n**Answer**: **True**\n\n**Explanation**:\nThis correctly identifies a fundamental structural difference. Binary search trees maintain an ordering invariant (left < root < right), enabling ordered traversal and range queries. Hash tables provide no such ordering; elements are distributed based on hash values which bear no relationship to the natural ordering of keys. This fundamental difference drives the choice between these structures: use BSTs when ordered operations are needed, use hash tables when only exact-match lookups are required. The tradeoff is that hash tables achieve O(1) average-case lookup vs. O(log n) for balanced BSTs.\n\n**Related Concepts**:\n- Ordering Invariant: Property maintained by BSTs, absent in hash tables\n- Range Queries: Efficient in BSTs, impossible in standard hash tables\n- Time Complexity Tradeoff: O(1) vs O(log n) for basic operations\n\n---\n\n**Question 11**: Analyze - Intermediate\n\n**Statement**: The choice of hash function has no impact on hash table performance as long as it produces valid array indices.\n\n**Answer**: **False**\n\n**Explanation**:\nThe hash function is critical to hash table performance. A poor hash function that creates uneven distribution (clustering many keys into few indices) will cause numerous collisions, degrading average-case performance from O(1) toward O(n). Even though any function that produces valid indices is \"correct\" in that the hash table will function, performance varies dramatically. A good hash function should: (1) distribute keys uniformly across indices, (2) minimize clustering, and (3) be efficient to compute. Examples: using key modulo small number might create patterns; using identity function for sequential keys clusters them.\n\n**Common Misconception**:\nStudents may think any hash function is acceptable as long as it's mathematically valid, not recognizing the performance implications of distribution quality.\n\n**Related Concepts**:\n- Uniform Distribution: Essential property of good hash functions\n- Clustering: Performance problem caused by poor distribution\n- Hash Function Design: Techniques like multiplication method, division method\n\n---\n\n### Level 5: Evaluate\n\n**Question 12**: Evaluate - Advanced\n\n**Statement**: Using a cryptographic hash function like SHA-256 for a hash table is always superior to using simpler hash functions because it eliminates all collisions.\n\n**Answer**: **False**\n\n**Explanation**:\nThis statement is false on multiple grounds:\n\n1. **Collisions Still Occur**: Even cryptographic hashes produce collisions in hash tables because the hash output must be reduced modulo table size. SHA-256 produces 256-bit values, but table indices are much smaller (e.g., 0-1023), so collision is inevitable via pigeonhole principle.\n\n2. **Computational Overhead**: Cryptographic hash functions are designed for security properties (pre-image resistance, collision resistance in full output space) and are computationally expensive. This overhead is unnecessary for hash tables and significantly degrades performance.\n\n3. **Appropriate Design**: Simple hash functions (e.g., multiplication method, polynomial rolling hash) are optimized for hash table use: fast computation and good distribution for typical keys.\n\n**When Cryptographic Hashes Might Be Used**:\n- Hash tables for security-sensitive applications where DoS attacks via hash collision flooding are a concern\n- Even then, simpler hash functions with random seeds are usually sufficient\n\n**Evaluation Criteria**: Performance, necessity of security properties, computational cost\n\n**Common Misconception**:\nBelieving \"stronger\" hash functions are always better, without considering context and tradeoffs.\n\n**Related Concepts**:\n- Cryptographic Hash Functions: Designed for security, not hash table efficiency\n- Hash Function Performance: Evaluation should consider computation speed\n- Hash Flooding Attacks: Security concern addressed by randomized hash functions\n\n---\n\n**Question 13**: Evaluate - Advanced\n\n**Statement**: A hash table is more appropriate than a balanced binary search tree when the primary operations are exact-match lookups and the application does not require ordered traversal of elements.\n\n**Answer**: **True**\n\n**Explanation**:\nThis represents sound engineering judgment based on data structure characteristics:\n\n**Hash Table Advantages (for this scenario)**:\n- O(1) average-case lookup vs. O(log n) for BST\n- Simpler implementation for exact-match queries\n- No balancing overhead\n\n**BST Advantages (not needed here)**:\n- Ordered traversal: explicitly not required\n- Range queries: not mentioned as requirement\n- Sorted output: not needed\n\n**Evaluation Criteria**: When exact-match lookup is the primary operation and ordering is unnecessary, the faster average-case lookup of hash tables (O(1) vs O(log n)) makes them the superior choice. This represents appropriate matching of data structure to requirements.\n\n**Context Where BST Might Still Win**:\n- If worst-case guarantees are critical (balanced BST guarantees O(log n); hash tables can degrade to O(n))\n- If memory is extremely constrained (BSTs don't need to maintain load factor < 1)\n\n**Related Concepts**:\n- Data Structure Selection: Matching structure to requirements\n- Use Case Analysis: Determining critical operations for the application\n- Tradeoff Evaluation: Performance characteristics vs. requirements\n\n---\n\n### Level 6: Create\n\n**Question 14**: Create - Advanced\n\n**Statement**: When designing a hash table for a spell-checker application with a fixed dictionary, setting the initial table size to approximately twice the number of dictionary words represents a sound design decision that balances memory usage and lookup performance.\n\n**Answer**: **True**\n\n**Explanation**:\nThis design decision demonstrates sound reasoning:\n\n**Analysis of Requirements**:\n- Fixed dictionary: Size known in advance, no dynamic growth\n- Spell-checker: Requires fast lookups, no insertions during use\n- Trade-space for time: Memory for dictionary is small relative to modern systems\n\n**Design Rationale**:\n- Load factor  0.5 (n dictionary words in 2n table size)\n- Low load factor ensures minimal collisions and fast lookups\n- Since size is fixed, no resizing overhead during operation\n- Memory overhead (2x) is acceptable for spell-checker application\n- Ensures consistent O(1) lookup performance\n\n**Alternative Designs**:\n- Larger table (3x-4x): Wastes memory for minimal performance gain\n- Smaller table (1.5x or less): Higher load factor increases collision rate\n- Perfect hash: Possible but complex for large dictionaries; overkill\n\n**Engineering Judgment**: For a read-heavy application with fixed data and small memory footprint, optimizing for lookup speed via low load factor is appropriate.\n\n**Related Concepts**:\n- Load Factor Selection: Design decision balancing space and time\n- Static vs Dynamic Sizing: Fixed data enables optimization\n- Application Requirements: Read-heavy workload influences design\n\n---\n\n**Question 15**: Create - Advanced\n\n**Statement**: To design an efficient hash table for storing strings, using the sum of ASCII values of all characters as the hash function would provide optimal distribution and minimal collisions.\n\n**Answer**: **False**\n\n**Explanation**:\nThis represents poor hash function design for strings:\n\n**Problems with Sum of ASCII Values**:\n\n1. **Anagrams Collide**: All anagrams (\"listen\" and \"silent\") produce identical hash values, causing systematic collisions for common anagram pairs\n\n2. **Poor Distribution**: Sum has limited range (e.g., for 10-character strings, max sum  1270), leading to clustering in lower indices\n\n3. **Ignores Position**: \"abc\" and \"cba\" hash identically; position information is lost\n\n4. **Limited Mixing**: No avalanche effect; similar strings produce similar hashes\n\n**Better String Hash Functions**:\n\n**Polynomial Rolling Hash** (widely used):\n```\nhash = 0\nfor char in string:\n    hash = (hash * prime + ASCII(char)) mod table_size\n```\n- Considers character position\n- Distributes values more uniformly\n- Used in many standard library implementations\n\n**Design Principles for String Hashing**:\n- Incorporate character position (polynomial, bit-shifting)\n- Use prime multipliers for better distribution\n- Implement avalanche effect (small input changes  large hash changes)\n- Balance computation cost and distribution quality\n\n**Evaluation**: The proposed sum-based hash function would create systematic collisions and poor distribution, making it inappropriate despite being simple to compute.\n\n**Related Concepts**:\n- Polynomial Hash Functions: Standard approach for string hashing\n- Avalanche Effect: Desirable property where small input changes drastically affect output\n- Distribution Analysis: Evaluating hash function quality empirically\n\n---\n\n## Quiz Statistics\n\n- **Total Questions**: 15\n- **True Statements**: 10 (67%)\n- **False Statements**: 5 (33%)\n- **Distribution by Level**:\n  - Remember: 3 questions\n  - Understand: 2 questions\n  - Apply: 3 questions\n  - Analyze: 3 questions\n  - Evaluate: 2 questions\n  - Create: 2 questions\n\n---\n\n## Study Recommendations\n\n### Effective Quiz Usage\n\n1. **Initial Attempt**: Complete the quiz without consulting notes or resources\n2. **Self-Assessment**: Review the answer key, identifying questions answered incorrectly\n3. **Misconception Analysis**: For incorrect answers, carefully read the \"Common Misconception\" sections\n4. **Concept Review**: Study the related concepts for missed questions\n5. **Spaced Repetition**: Retake the quiz after 3-7 days to assess retention\n\n### Focus Areas by Performance\n\n- **If struggling with Remember/Understand levels**: Review fundamental definitions and concepts\n- **If struggling with Apply/Analyze levels**: Practice working through examples and analyzing scenarios\n- **If struggling with Evaluate/Create levels**: Study design principles and criteria-based decision making\n\n### Beyond This Quiz\n\n- Implement a hash table from scratch to deepen understanding\n- Analyze hash function quality empirically with real datasets\n- Compare hash table performance to other data structures in practice\n- Research advanced topics: perfect hashing, cuckoo hashing, consistent hashing\n```\n\n---\n\n## Example 2: Biology Topic - \"Cell Structure\" (Partial Quiz)\n\n### Sample Questions and Answers\n\n```markdown\n# Quiz: Cell Structure and Function\n\n**Generated**: 2025-12-19\n**Cognitive Framework**: Bloom's Taxonomy\n**Question Type**: True/False\n**Total Questions**: 14\n**Recommended Time**: 18-22 minutes\n\n---\n\n## Questions (Selected)\n\n### Level 1: Remember\n\n**Question 1**: Remember - Basic\n\nThe cell membrane is composed of a phospholipid bilayer with embedded proteins.\n\n- [ ] True\n- [ ] False\n\n---\n\n**Question 2**: Remember - Basic\n\nMitochondria are found only in plant cells and are responsible for photosynthesis.\n\n- [ ] True\n- [ ] False\n\n---\n\n### Level 2: Understand\n\n**Question 3**: Understand - Intermediate\n\nThe hydrophobic tails of phospholipids in the cell membrane face inward toward each other, while the hydrophilic heads face outward toward the aqueous environments inside and outside the cell.\n\n- [ ] True\n- [ ] False\n\n---\n\n### Level 3: Apply\n\n**Question 4**: Apply - Intermediate\n\nIf a cell is placed in a hypertonic solution, water will move out of the cell through osmosis, potentially causing the cell to shrink.\n\n- [ ] True\n- [ ] False\n\n---\n\n### Level 4: Analyze\n\n**Question 5**: Analyze - Advanced\n\nThe reason eukaryotic cells can grow larger than prokaryotic cells is primarily because eukaryotes have organelles that compartmentalize cellular functions, improving efficiency.\n\n- [ ] True\n- [ ] False\n\n---\n\n### Level 5: Evaluate\n\n**Question 6**: Evaluate - Advanced\n\nThe presence of a cell wall in plant cells makes them superior to animal cells in all environmental conditions.\n\n- [ ] True\n- [ ] False\n\n---\n\n## Answer Key (Selected)\n\n**Question 1**: Remember - Basic\n\n**Answer**: **True**\n\n**Explanation**:\nThe cell membrane (plasma membrane) is indeed composed of a phospholipid bilayer - two layers of phospholipid molecules arranged with their hydrophobic tails facing inward and hydrophilic heads facing outward. Embedded within this bilayer are various proteins that perform functions such as transport, signaling, and cell recognition. This fluid mosaic model accurately describes cell membrane structure.\n\n---\n\n**Question 2**: Remember - Basic\n\n**Answer**: **False**\n\n**Explanation**:\nThis statement contains two errors: (1) Mitochondria are found in BOTH plant and animal cells (all eukaryotic cells), not just plants. (2) Mitochondria are responsible for cellular respiration and ATP production, NOT photosynthesis. Photosynthesis occurs in chloroplasts, which are found only in plant cells and some protists. This question tests recognition of fundamental organelle functions and distribution.\n\n**Common Misconception**:\nStudents often confuse mitochondria and chloroplasts because both are energy-related organelles. The key distinction: mitochondria break down glucose to produce ATP (all eukaryotes), while chloroplasts capture light energy to produce glucose (plants only).\n\n---\n\n**Question 3**: Understand - Intermediate\n\n**Answer**: **True**\n\n**Explanation**:\nThis correctly describes the orientation of phospholipids in the bilayer. Phospholipids are amphipathic molecules with hydrophilic (water-loving) phosphate heads and hydrophobic (water-fearing) fatty acid tails. In an aqueous environment, they spontaneously arrange into a bilayer with tails facing inward (away from water) and heads facing outward (toward water on both sides). This arrangement is thermodynamically favorable and forms the fundamental barrier of the cell membrane.\n\n**Related Concepts**:\n- Amphipathic Molecules: Molecules with both hydrophilic and hydrophobic regions\n- Thermodynamic Stability: Bilayer formation minimizes unfavorable water-lipid interactions\n- Membrane Fluidity: Phospholipids can move laterally within layers\n\n---\n\n**Question 4**: Apply - Intermediate\n\n**Answer**: **True**\n\n**Explanation**:\nThis applies osmosis principles to a specific scenario. A hypertonic solution has higher solute concentration than the cell interior. Water moves from areas of low solute concentration (inside cell) to high solute concentration (outside cell) through osmosis. This water loss causes the cell to shrink, a process called crenation in animal cells or plasmolysis in plant cells. This demonstrates application of osmosis and tonicity concepts to predict cellular behavior.\n\n**Related Concepts**:\n- Osmosis: Diffusion of water across semi-permeable membranes\n- Tonicity: Relative solute concentrations affecting water movement\n- Hypertonic, Isotonic, Hypotonic: Classifications of solution concentration\n\n---\n\n**Question 5**: Analyze - Advanced\n\n**Answer**: **True**\n\n**Explanation**:\nThis statement correctly analyzes a key difference between prokaryotes and eukaryotes. The compartmentalization provided by membrane-bound organelles in eukaryotes allows:\n1. Separation of incompatible chemical reactions\n2. Localized concentration of enzymes and substrates\n3. Specialized microenvironments (pH, ion concentrations)\n4. More efficient metabolism through proximity of related reactions\n\nThis increased organizational efficiency partially explains why eukaryotic cells can achieve much larger sizes (10-100 m) compared to prokaryotes (1-10 m), overcoming surface area-to-volume ratio limitations.\n\n**Related Concepts**:\n- Surface Area-to-Volume Ratio: Constraint on cell size\n- Compartmentalization: Functional advantage of organelles\n- Metabolic Efficiency: Organization improves reaction kinetics\n\n---\n\n**Question 6**: Evaluate - Advanced\n\n**Answer**: **False**\n\n**Explanation**:\nThis statement makes an absolute claim (\"superior...in all conditions\") that reflects poor biological reasoning. The cell wall provides advantages in certain contexts:\n\n**Plant Cell Wall Advantages**:\n- Structural support (allows plants to grow tall without skeletal system)\n- Protection from hypotonic environments (prevents bursting)\n- Pathogen defense\n\n**Plant Cell Wall Disadvantages**:\n- Limits cell mobility (plants are sessile)\n- Requires more energy and resources to produce\n- Limits flexibility and shape change\n- Prevents phagocytosis (cell eating)\n\n**Animal Cell Advantages**:\n- Flexibility enables movement and migration\n- Allows phagocytosis for immune function\n- Enables diverse cell shapes\n\n**Evaluation**: No single cellular feature is universally superior; advantages are context-dependent. The cell wall is adaptive for sessile organisms requiring structural support, while its absence is adaptive for motile organisms requiring flexibility.\n\n**Related Concepts**:\n- Evolutionary Tradeoffs: Features have costs and benefits\n- Adaptive Significance: Features advantageous in specific environmental contexts\n- Structural-Functional Relationships: Form follows function in biology\n```\n\n---\n\n## Example 3: Mathematics Topic - \"Derivatives\" (Partial Quiz)\n\n### Sample Questions and Answers\n\n```markdown\n# Quiz: Derivatives in Calculus\n\n**Generated**: 2025-12-19\n**Cognitive Framework**: Bloom's Taxonomy\n**Question Type**: True/False\n**Total Questions**: 13\n**Recommended Time**: 18-24 minutes\n\n---\n\n## Questions (Selected)\n\n### Level 1: Remember\n\n**Question 1**: Remember - Basic\n\nThe derivative of a function at a point represents the slope of the tangent line to the function at that point.\n\n- [ ] True\n- [ ] False\n\n---\n\n### Level 2: Understand\n\n**Question 2**: Understand - Intermediate\n\nIf a function's derivative is positive over an interval, the function is increasing on that interval.\n\n- [ ] True\n- [ ] False\n\n---\n\n### Level 3: Apply\n\n**Question 3**: Apply - Intermediate\n\nIf f(x) = x + 2x, then using the power rule, f'(x) = 3x + 2.\n\n- [ ] True\n- [ ] False\n\n---\n\n### Level 4: Analyze\n\n**Question 4**: Analyze - Advanced\n\nThe reason the product rule [f(x)g(x)]' = f'(x)g(x) + f(x)g'(x) has two terms is that differentiation distributes over products the same way it distributes over sums.\n\n- [ ] True\n- [ ] False\n\n---\n\n### Level 5: Evaluate\n\n**Question 5**: Evaluate - Advanced\n\nFor optimization problems, finding where the derivative equals zero is sufficient to identify the maximum or minimum value of a function.\n\n- [ ] True\n- [ ] False\n\n---\n\n## Answer Key (Selected)\n\n**Question 1**: Remember - Basic\n\n**Answer**: **True**\n\n**Explanation**:\nThis is the geometric interpretation of the derivative. The derivative f'(a) at point x = a equals the slope of the line tangent to the curve y = f(x) at that point. This interpretation connects the algebraic concept (limit of difference quotient) to geometric meaning (tangent line slope) and practical application (instantaneous rate of change).\n\n---\n\n**Question 2**: Understand - Intermediate\n\n**Answer**: **True**\n\n**Explanation**:\nThis statement correctly relates the sign of the derivative to function behavior. If f'(x) > 0 on an interval, the tangent slopes are all positive, meaning the function rises as x increases. More formally: for any x < x in the interval, f(x) < f(x), which is the definition of an increasing function. This demonstrates understanding of the connection between derivatives and function monotonicity.\n\n**Related Concepts**:\n- Increasing/Decreasing Functions: f'(x) > 0  increasing; f'(x) < 0  decreasing\n- First Derivative Test: Uses derivative sign to classify critical points\n- Monotonicity: Function behavior over intervals\n\n---\n\n**Question 3**: Apply - Intermediate\n\n**Answer**: **True**\n\n**Explanation**:\nThis correctly applies the power rule and sum rule:\n- Power rule: d/dx[x] = nx\n- Applied to x: d/dx[x] = 3x\n- Applied to 2x (which is 2x): d/dx[2x] = 21x = 2\n- Sum rule: derivative of sum is sum of derivatives\n- Therefore: f'(x) = 3x + 2 \n\nThis demonstrates procedural application of differentiation rules.\n\n---\n\n**Question 4**: Analyze - Advanced\n\n**Answer**: **False**\n\n**Explanation**:\nThis statement misidentifies the reason for the product rule's form. Differentiation does NOT distribute over products the way it does over sums. If it did, we'd have (fg)' = f'g', which is incorrect.\n\n**Why the Product Rule Has Two Terms**:\nThe product rule arises from the limit definition:\n(fg)' = lim[h0] [(f(x+h)g(x+h) - f(x)g(x))/h]\n\nThrough algebraic manipulation (adding and subtracting f(x+h)g(x)):\n= lim[h0] [f(x+h)(g(x+h)-g(x))/h + g(x)(f(x+h)-f(x))/h]\n= f(x)g'(x) + g(x)f'(x)\n\nThe two terms arise because BOTH factors change; we must account for each factor's rate of change while holding the other relatively constant.\n\n**Contrast with Sum Rule**:\n- Sum rule: (f + g)' = f' + g' (distributive property holds)\n- Product rule: (f  g)' = f'g + fg' (distributive property does NOT hold)\n\n**Common Misconception**:\nStudents may assume all arithmetic operations interact with derivatives uniformly.\n\n---\n\n**Question 5**: Evaluate - Advanced\n\n**Answer**: **False**\n\n**Explanation**:\nFinding where f'(x) = 0 identifies critical points, but this alone is insufficient to guarantee maximum or minimum values:\n\n**Why Insufficient**:\n\n1. **Critical Points May Be Inflection Points**: f(x) = x has f'(0) = 0, but x = 0 is neither max nor min\n\n2. **Must Check Second Derivative or Sign Changes**: Second derivative test (f''(x)) or first derivative test (sign changes) required to classify critical points\n\n3. **Endpoints May Be Extrema**: On closed interval [a,b], maximum/minimum may occur at endpoints even if f'(a)  0 or f'(b)  0\n\n4. **May Have No Local Extrema**: Some critical points are saddle points or inflection points\n\n**Complete Optimization Procedure**:\n1. Find critical points: f'(x) = 0\n2. Classify using second derivative test or first derivative test\n3. Check endpoints if on closed interval\n4. Compare values to identify absolute maximum/minimum\n\n**Evaluation Criteria**: Mathematical rigor requires verification beyond just finding critical points.\n\n**Related Concepts**:\n- Critical Points: Where f'(x) = 0 or f'(x) undefined\n- Second Derivative Test: f''(x) > 0  local min; f''(x) < 0  local max\n- First Derivative Test: Sign change of f'(x) indicates extremum type\n- Extreme Value Theorem: Continuous function on closed interval attains max and min\n```\n\n---\n\n## Usage Notes\n\nThese examples demonstrate:\n\n1. **Bloom's Taxonomy Progression**: Questions advance from factual recall to complex evaluation\n2. **True/False Format**: Clear binary choice statements testing specific knowledge\n3. **Comprehensive Answer Keys**: Detailed explanations providing learning value beyond answer verification\n4. **Misconception-Based False Statements**: False statements target common student errors\n5. **Appropriate Difficulty Distribution**: Questions range from basic to advanced within each cognitive level\n6. **Domain Diversity**: Examples span Computer Science, Biology, and Mathematics\n7. **Pedagogical Soundness**: Questions facilitate genuine assessment of understanding\n\nWhen generating quizzes for new topics, emulate this structure, explanation depth, and alignment with Bloom's Taxonomy levels.\n",
        "skills/study_system/.claude/skills/quiz-generator/reference.md": "# True/False Quiz Design Reference Guide\n\n## Theoretical Foundation\n\nThis reference guide provides comprehensive methodology for constructing educationally sound True/False quizzes aligned with Bloom's Taxonomy. It synthesizes principles from cognitive psychology, educational assessment theory, and evidence-based testing practices.\n\n## Bloom's Taxonomy Applied to True/False Questions\n\n### Level 1: Remember (Knowledge Recall)\n\n**Cognitive Focus**: Retrieval of factual information from memory\n\n**True/False Question Characteristics**:\n- Test recall of definitions, facts, dates, names, terminology\n- Straightforward factual assertions\n- Minimal contextual complexity\n- Clear, unambiguous statements\n\n**Question Construction Guidelines**:\n\n**For True Statements**:\n- State accurate, verifiable facts\n- Use precise terminology\n- Avoid unnecessary qualifiers\n- Ensure completeness of statement\n\n**For False Statements**:\n- Modify factual details (dates, names, numbers)\n- Substitute incorrect but plausible alternatives\n- Base on common factual confusions\n- Avoid trivial negations\n\n**Examples**:\n\n**True Statement - Computer Science**:\n\"The time complexity of binary search on a sorted array is O(log n).\"\n- Tests: Factual recall of algorithm complexity\n- Difficulty: Basic\n- Rationale: Standard computer science fact requiring memorization\n\n**False Statement - Computer Science**:\n\"The Python programming language was created by James Gosling.\"\n- Tests: Recall of Python's creator (Guido van Rossum)\n- Common Misconception: Confusion with Java's creator\n- Difficulty: Basic\n- Rationale: Plausible confusion between language creators\n\n**True Statement - Biology**:\n\"Mitochondria are the organelles responsible for cellular respiration and ATP production.\"\n- Tests: Basic biological fact\n- Difficulty: Basic\n- Rationale: Fundamental concept in cell biology\n\n**False Statement - Biology**:\n\"Photosynthesis occurs in the mitochondria of plant cells.\"\n- Tests: Location of photosynthesis (chloroplasts)\n- Common Misconception: Confusion between energy-related organelles\n- Difficulty: Intermediate\n- Rationale: Both organelles are energy-related, creating plausible confusion\n\n---\n\n### Level 2: Understand (Comprehension)\n\n**Cognitive Focus**: Construction of meaning and interpretation of concepts\n\n**True/False Question Characteristics**:\n- Test understanding of concepts, relationships, principles\n- Require interpretation or translation of information\n- Involve classification, comparison, or explanation\n- Test ability to recognize examples and non-examples\n\n**Question Construction Guidelines**:\n\n**For True Statements**:\n- Describe conceptual relationships accurately\n- Present correct interpretations or explanations\n- State valid classifications or categorizations\n- Provide accurate examples of concepts\n\n**For False Statements**:\n- Misrepresent relationships between concepts\n- Present plausible but incorrect interpretations\n- Misclassify concepts into wrong categories\n- Provide non-examples disguised as examples\n\n**Examples**:\n\n**True Statement - Computer Science**:\n\"Polymorphism in object-oriented programming allows objects of different classes to be treated through a common interface.\"\n- Tests: Understanding of polymorphism concept\n- Difficulty: Intermediate\n- Rationale: Requires conceptual understanding, not mere definition recall\n\n**False Statement - Computer Science**:\n\"Encapsulation and abstraction are identical concepts in object-oriented programming.\"\n- Tests: Distinction between related OOP concepts\n- Common Misconception: Confusion between complementary concepts\n- Difficulty: Intermediate\n- Rationale: Both are OOP principles but serve different purposes\n\n**True Statement - Mathematics**:\n\"A function is continuous at a point if the limit of the function as x approaches that point equals the function's value at that point.\"\n- Tests: Understanding of continuity definition\n- Difficulty: Intermediate\n- Rationale: Requires comprehension of limit concept and its relationship to continuity\n\n**False Statement - Mathematics**:\n\"If a function is differentiable at a point, it may or may not be continuous at that point.\"\n- Tests: Understanding of differentiability and continuity relationship\n- Common Misconception: Incorrect understanding of implication direction\n- Difficulty: Advanced\n- Rationale: Differentiability implies continuity (not vice versa); common confusion\n\n---\n\n### Level 3: Apply (Application)\n\n**Cognitive Focus**: Execution of procedures in given situations\n\n**True/False Question Characteristics**:\n- Present scenarios requiring procedural application\n- Test ability to use knowledge in concrete situations\n- Involve implementation or execution of methods\n- Require transfer of learned procedures to new contexts\n\n**Question Construction Guidelines**:\n\n**For True Statements**:\n- Describe correct applications of procedures\n- Present accurate scenario-solution pairings\n- State valid uses of methods or algorithms\n- Show appropriate tool selection for tasks\n\n**For False Statements**:\n- Describe incorrect applications\n- Present inappropriate method-scenario pairings\n- Show misapplication of procedures\n- Demonstrate invalid tool selection\n\n**Examples**:\n\n**True Statement - Computer Science**:\n\"To find the maximum element in an unsorted array of n elements, you must examine all n elements in the worst case.\"\n- Tests: Application of search logic to unsorted data\n- Difficulty: Intermediate\n- Rationale: Requires applying reasoning about algorithm requirements\n\n**False Statement - Computer Science**:\n\"Binary search can be efficiently applied to an unsorted linked list to achieve O(log n) search time.\"\n- Tests: Understanding of binary search prerequisites\n- Common Misconception: Overlooking need for sorted, random-access data structure\n- Difficulty: Intermediate\n- Rationale: Binary search requires both sorted data AND random access\n\n**True Statement - Physics**:\n\"When calculating the final velocity of an object under constant acceleration, you can use the equation v = v + at.\"\n- Tests: Application of kinematic equation\n- Difficulty: Basic\n- Rationale: Straightforward application of standard physics formula\n\n**False Statement - Physics**:\n\"You can use Newton's classical mechanics equations to accurately calculate the behavior of electrons in atoms.\"\n- Tests: Understanding of domain applicability for classical vs quantum mechanics\n- Common Misconception: Overgeneralizing classical mechanics\n- Difficulty: Advanced\n- Rationale: Quantum mechanics required at atomic scale; common conceptual error\n\n---\n\n### Level 4: Analyze (Analysis)\n\n**Cognitive Focus**: Decomposition and relationship identification\n\n**True/False Question Characteristics**:\n- Test ability to identify structural relationships\n- Require differentiation between similar concepts\n- Involve detection of patterns or organizing principles\n- Test recognition of underlying assumptions or biases\n\n**Question Construction Guidelines**:\n\n**For True Statements**:\n- Accurately describe component relationships\n- State valid differentiations between concepts\n- Present correct structural or organizational principles\n- Identify genuine assumptions or implications\n\n**For False Statements**:\n- Misrepresent relationships between components\n- Conflate distinct concepts inappropriately\n- Present incorrect organizational principles\n- Misidentify assumptions or implications\n\n**Examples**:\n\n**True Statement - Computer Science**:\n\"The fundamental difference between depth-first search and breadth-first search lies in the data structure used for storing frontier nodes: stack versus queue.\"\n- Tests: Analysis of algorithmic structure\n- Difficulty: Advanced\n- Rationale: Requires identifying core structural distinction between algorithms\n\n**False Statement - Computer Science**:\n\"The primary reason for using dynamic programming instead of recursion is that dynamic programming provides exponentially better space complexity.\"\n- Tests: Analysis of algorithmic trade-offs\n- Common Misconception: Misunderstanding space/time trade-offs in DP\n- Difficulty: Advanced\n- Rationale: DP trades space for time; may actually use more space\n\n**True Statement - Literature**:\n\"The unreliable narrator technique in fiction creates tension between what the narrator reports and what the reader infers to be true.\"\n- Tests: Analysis of narrative structure\n- Difficulty: Intermediate\n- Rationale: Identifies the functional mechanism of literary device\n\n**False Statement - Economics**:\n\"The relationship between supply and demand is fundamentally the same in monopolistic markets as in perfectly competitive markets.\"\n- Tests: Analysis of market structure differences\n- Common Misconception: Overgeneralizing economic principles\n- Difficulty: Advanced\n- Rationale: Market structure significantly affects supply-demand dynamics\n\n---\n\n### Level 5: Evaluate (Evaluation)\n\n**Cognitive Focus**: Judgment based on criteria and standards\n\n**True/False Question Characteristics**:\n- Test ability to make criteria-based judgments\n- Require assessment of appropriateness or effectiveness\n- Involve evaluation of validity or quality\n- Test recognition of context-dependent suitability\n\n**Question Construction Guidelines**:\n\n**For True Statements**:\n- Present valid criteria-based judgments\n- Describe appropriate context-specific choices\n- State accurate effectiveness assessments\n- Present valid quality or validity evaluations\n\n**For False Statements**:\n- Present overgeneralized absolute judgments\n- Describe context-inappropriate choices\n- State inaccurate effectiveness claims\n- Present invalid quality assessments\n\n**Examples**:\n\n**True Statement - Software Engineering**:\n\"Choosing between SQL and NoSQL databases should be based on factors including data structure consistency, scalability requirements, and transaction needs rather than following a universal best practice.\"\n- Tests: Evaluation of technology selection criteria\n- Difficulty: Advanced\n- Rationale: Recognizes context-dependent nature of technical decisions\n\n**False Statement - Software Engineering**:\n\"Code coverage of 100% guarantees that a software application is free of bugs and behaves correctly in all scenarios.\"\n- Tests: Evaluation of testing metrics\n- Common Misconception: Overvaluing code coverage metric\n- Difficulty: Intermediate\n- Rationale: Coverage measures lines executed, not correctness or all scenarios\n\n**True Statement - Medicine**:\n\"The appropriateness of prescribing antibiotics depends on whether the infection is bacterial rather than viral, as antibiotics are ineffective against viral infections.\"\n- Tests: Evaluation of treatment appropriateness\n- Difficulty: Intermediate\n- Rationale: Context-specific treatment decision based on infection type\n\n**False Statement - Education**:\n\"Lecturing is always inferior to active learning methods regardless of class size, subject matter, or learning objectives.\"\n- Tests: Evaluation of pedagogical methods\n- Common Misconception: Absolute thinking about teaching methods\n- Difficulty: Advanced\n- Rationale: Effectiveness depends on context; no universally superior method\n\n---\n\n### Level 6: Create (Synthesis)\n\n**Cognitive Focus**: Synthesis and design of coherent wholes\n\n**True/False Question Characteristics**:\n- Test understanding of design principles\n- Require recognition of effective synthesis strategies\n- Involve assessment of creative solutions\n- Test knowledge of optimal construction approaches\n\n**Question Construction Guidelines**:\n\n**For True Statements**:\n- Describe sound design principles\n- Present effective synthesis strategies\n- State valid optimization approaches\n- Describe appropriate construction methods\n\n**For False Statements**:\n- Present flawed design principles\n- Describe ineffective synthesis strategies\n- State invalid optimization claims\n- Present inappropriate construction approaches\n\n**Examples**:\n\n**True Statement - Software Architecture**:\n\"When designing a distributed system, implementing idempotent operations helps ensure system reliability in the presence of network failures and retries.\"\n- Tests: Understanding of design principle for distributed systems\n- Difficulty: Advanced\n- Rationale: Tests knowledge of key design strategy for fault tolerance\n\n**False Statement - Software Architecture**:\n\"The optimal software architecture design always minimizes the number of modules and classes to reduce system complexity.\"\n- Tests: Understanding of modularity principles\n- Common Misconception: Confusing fewer components with reduced complexity\n- Difficulty: Advanced\n- Rationale: Appropriate modularity may increase component count while reducing coupling\n\n**True Statement - Research Design**:\n\"When designing a controlled experiment, random assignment of subjects to treatment and control groups helps eliminate selection bias.\"\n- Tests: Understanding of experimental design principles\n- Difficulty: Intermediate\n- Rationale: Fundamental principle of causal inference\n\n**False Statement - Art/Design**:\n\"Effective visual design always requires perfect symmetry and balance to achieve aesthetic appeal.\"\n- Tests: Understanding of design principles\n- Common Misconception: Overgeneralizing symmetry principle\n- Difficulty: Intermediate\n- Rationale: Asymmetry and dynamic balance are also valid design approaches\n\n---\n\n## Principles of Effective True/False Question Construction\n\n### Principle 1: Unambiguous Statements\n\n**Requirement**: Each statement must have a definitive, non-controversial truth value.\n\n**Problematic Example**:\n\"Object-oriented programming is better than functional programming.\"\n- Issue: Subjective comparison without criteria; no definitive answer\n\n**Improved Version**:\n\"Object-oriented programming and functional programming represent different paradigms with distinct approaches to state management and data transformation.\"\n- Improvement: Factual statement about differences, avoiding subjective judgment\n\n---\n\n### Principle 2: Single Concept Per Statement\n\n**Requirement**: Each statement should test one primary concept or relationship.\n\n**Problematic Example**:\n\"Python is interpreted, dynamically typed, and supports both procedural and object-oriented programming paradigms.\"\n- Issue: Four testable claims in one statement; unclear what is being assessed\n\n**Improved Version** (split into multiple questions):\n1. \"Python is an interpreted language rather than a compiled language.\"\n2. \"Python uses dynamic typing, determining variable types at runtime.\"\n3. \"Python supports both procedural and object-oriented programming paradigms.\"\n\n---\n\n### Principle 3: Avoid Absolute Language (Unless Intentional)\n\n**Requirement**: Terms like \"always,\" \"never,\" \"all,\" \"none\" often signal false statements and reduce validity.\n\n**Problematic Example**:\n\"Recursion is always more efficient than iteration for solving algorithmic problems.\"\n- Issue: \"Always\" makes this obviously false; tests pattern recognition rather than understanding\n\n**Improved False Statement**:\n\"Recursive solutions are generally more memory-efficient than iterative solutions due to avoiding loop overhead.\"\n- Improvement: Plausible but false; tests understanding of stack memory usage\n\n**When Absolute Language is Appropriate**:\n\"All sorting algorithms that operate by comparisons require at least O(n log n) time complexity in the worst case.\"\n- Justification: This is a proven theoretical result; absolute language is factually correct\n\n---\n\n### Principle 4: Meaningful False Statements\n\n**Requirement**: False statements should target actual misconceptions, not trivial errors.\n\n**Problematic False Statement**:\n\"A binary tree is a data structure where each node has exactly three children.\"\n- Issue: Trivially false; modifies defining characteristic; tests reading comprehension, not understanding\n\n**Improved False Statement**:\n\"In a binary search tree, the inorder traversal produces elements in descending order.\"\n- Improvement: Tests understanding of BST property and traversal; common confusion (ascending vs descending)\n\n---\n\n### Principle 5: Plausibility of False Statements\n\n**Requirement**: False statements should be plausible to someone with incomplete understanding.\n\n**Evaluation Criteria**:\n- Would a student who partially understands the material potentially believe this?\n- Does the false statement reflect a logical-seeming but incorrect inference?\n- Is the error subtle enough to require genuine knowledge to detect?\n\n**Example of Implausible False Statement**:\n\"Computers operate using the decimal number system because humans use base-10.\"\n- Issue: Too obviously false to most students; lacks pedagogical value\n\n**Example of Plausible False Statement**:\n\"Since computers ultimately store all data as binary numbers, programming languages that use binary arithmetic are more efficient than those using decimal representation.\"\n- Plausibility: Logical-seeming inference from correct premise\n- Why False: High-level language representation doesn't affect compiled efficiency; all convert to binary\n- Pedagogical Value: Tests understanding of compilation and representation abstraction\n\n---\n\n### Principle 6: Sufficient Context\n\n**Requirement**: Provide enough context for the statement to be evaluated definitively.\n\n**Problematic Example** (Insufficient Context):\n\"The algorithm has O(n) time complexity.\"\n- Issue: Which algorithm? For which operation? Best, average, or worst case?\n\n**Improved Version**:\n\"The quicksort algorithm has O(n log n) average-case time complexity for sorting n elements.\"\n- Improvement: Specifies algorithm, operation, and case being discussed\n\n---\n\n### Principle 7: Avoid Trick Questions\n\n**Requirement**: Test understanding, not reading comprehension or attention to semantic tricks.\n\n**Problematic Trick Question**:\n\"Python does not support multiple inheritance.\"\n- Issue: Double negative; tests careful reading, not Python knowledge\n\n**Improved Version**:\n\"Python prohibits a class from inheriting from multiple parent classes.\"\n- Improvement: Tests same knowledge without linguistic tricks\n\n---\n\n## Misconception-Based Assessment Design\n\n### Theoretical Framework\n\nEffective False statements should be grounded in documented student misconceptions. This approach:\n- Provides diagnostic information about student understanding\n- Creates pedagogically valuable assessment\n- Increases validity by testing genuine conceptual challenges\n- Facilitates targeted instruction based on misconception patterns\n\n### Common Types of Misconceptions\n\n#### Type 1: Overgeneralization\n\n**Pattern**: Applying a principle beyond its valid domain\n\n**Example in Computer Science**:\nFalse Statement: \"Big-O notation describes the exact runtime of an algorithm in seconds.\"\n- Misconception: Confusing asymptotic complexity with concrete runtime\n- Origin: Overgeneralizing the utility of Big-O analysis\n\n#### Type 2: Conceptual Confusion\n\n**Pattern**: Conflating related but distinct concepts\n\n**Example in Statistics**:\nFalse Statement: \"If two variables are correlated, then one must cause the other.\"\n- Misconception: Confusing correlation with causation\n- Origin: Classic statistical reasoning error\n\n#### Type 3: Incorrect Causal Model\n\n**Pattern**: Misunderstanding cause-effect relationships\n\n**Example in Biology**:\nFalse Statement: \"Organisms adapt to their environment during their lifetime, and these adaptations are passed to offspring.\"\n- Misconception: Lamarckian inheritance (acquired characteristics)\n- Origin: Intuitive but incorrect evolutionary mechanism\n\n#### Type 4: Boundary Case Errors\n\n**Pattern**: Misunderstanding behavior at limits or special cases\n\n**Example in Mathematics**:\nFalse Statement: \"If a function's derivative is zero at a point, that point must be a local maximum or minimum.\"\n- Misconception: Ignoring inflection points with horizontal tangents\n- Origin: Incomplete understanding of critical point types (e.g., f(x) = x at x = 0)\n\n#### Type 5: Reversed Implications\n\n**Pattern**: Confusing \"if A then B\" with \"if B then A\"\n\n**Example in Logic/Mathematics**:\nFalse Statement: \"If a number is divisible by 6, it must be divisible by 12.\"\n- Misconception: Reversing the implication (divisible by 12  divisible by 6)\n- Origin: Confusion about implication direction\n\n---\n\n## Quality Assurance Framework\n\n### Dimension 1: Content Validity\n\n**Criteria**:\n- Each statement addresses important learning objectives\n- Questions sample comprehensively across topic domain\n- Cognitive levels appropriately distributed\n- Difficulty matches target learner population\n\n**Validation Process**:\n1. Map each question to specific learning objective\n2. Verify comprehensive coverage of key concepts\n3. Confirm Bloom's level classification accuracy\n4. Assess appropriateness for intended audience\n\n---\n\n### Dimension 2: Clarity and Precision\n\n**Criteria**:\n- Statements are grammatically correct\n- Terminology is used accurately\n- No ambiguity in interpretation\n- Context is sufficient for evaluation\n\n**Validation Process**:\n1. Read each statement aloud; check for awkward phrasing\n2. Verify technical terminology usage against authoritative sources\n3. Test statement with question: \"Could this reasonably be interpreted differently?\"\n4. Confirm all necessary context is provided\n\n---\n\n### Dimension 3: Pedagogical Value\n\n**Criteria**:\n- Explanations enhance understanding beyond answer\n- False statements target meaningful misconceptions\n- Questions promote learning, not mere memorization\n- Assessment facilitates diagnostic feedback\n\n**Validation Process**:\n1. Review each explanation: Does it teach, or just confirm?\n2. Verify false statements based on documented misconceptions\n3. Assess whether question promotes conceptual understanding\n4. Evaluate whether results would inform instruction\n\n---\n\n### Dimension 4: Technical Accuracy\n\n**Criteria**:\n- All True statements are factually correct\n- All False statements are genuinely incorrect\n- No scientifically controversial claims presented as fact\n- Current, up-to-date information\n\n**Validation Process**:\n1. Cross-reference True statements against authoritative sources\n2. Verify False statements are definitively incorrect\n3. Identify any statements requiring qualification or context\n4. Check publication dates of sources for currency\n\n---\n\n### Dimension 5: Fairness and Bias\n\n**Criteria**:\n- Questions assess knowledge, not test-taking strategies\n- No systematic patterns enabling non-knowledge-based answering\n- Avoid cultural, linguistic, or contextual bias\n- Difficulty distributed appropriately\n\n**Validation Process**:\n1. Check True/False distribution: approximately 50/50\n2. Verify no answer patterns (alternating, runs of same answer)\n3. Review for cultural assumptions or context-specific knowledge requirements\n4. Confirm difficulty progression is appropriate\n\n---\n\n## Evidence-Based Assessment Practices\n\n### Research Finding 1: Testing Effect\n\n**Principle**: Retrieval practice (testing) enhances long-term retention more than restudying.\n\n**Implication for Quiz Design**:\n- Quizzes serve not just evaluative but also learning functions\n- Explanations in answer keys should promote deep processing\n- Spacing quizzes over time enhances retention\n\n**Reference**: Roediger & Karpicke (2006). \"Test-Enhanced Learning: Taking Memory Tests Improves Long-Term Retention.\"\n\n---\n\n### Research Finding 2: Misconception-Based Learning\n\n**Principle**: Directly addressing misconceptions is more effective than ignoring them.\n\n**Implication for Quiz Design**:\n- False statements should target documented misconceptions\n- Explanations should explicitly address why misconception is incorrect\n- Related concepts should include correct alternative conceptions\n\n**Reference**: Chi (2008). \"Three Types of Conceptual Change: Belief Revision, Mental Model Transformation, and Categorical Shift.\"\n\n---\n\n### Research Finding 3: Cognitive Load Management\n\n**Principle**: Working memory limitations affect learning; reduce extraneous cognitive load.\n\n**Implication for Quiz Design**:\n- One concept per statement (avoid compound statements)\n- Clear, concise wording (avoid unnecessarily complex syntax)\n- Sufficient but not excessive context\n- Well-organized presentation format\n\n**Reference**: Sweller (1988). \"Cognitive Load During Problem Solving: Effects on Learning.\"\n\n---\n\n### Research Finding 4: Formative Assessment\n\n**Principle**: Assessment with feedback promotes learning more than assessment alone.\n\n**Implication for Quiz Design**:\n- Comprehensive answer key with explanations\n- Identification of common misconceptions\n- Guidance for further study based on errors\n- Explicit connection to learning objectives\n\n**Reference**: Black & Wiliam (1998). \"Assessment and Classroom Learning.\"\n\n---\n\n## Advanced Considerations\n\n### Adaptive Difficulty\n\nFor adaptive learning systems, questions can be tagged with:\n- **Prerequisite Concepts**: What must be understood first\n- **Item Difficulty**: Empirically determined or estimated\n- **Discrimination Index**: How well question differentiates high/low performers\n\nThis enables:\n- Personalized quiz sequencing\n- Diagnostic assessment of knowledge gaps\n- Targeted remediation recommendations\n\n---\n\n### Computerized Adaptive Testing (CAT)\n\nPrinciples for CAT-compatible True/False questions:\n1. **Item Banking**: Large pool of calibrated questions at each Bloom's level\n2. **Difficulty Calibration**: Empirical validation of question difficulty\n3. **Content Balancing**: Ensure coverage of learning objectives despite adaptive selection\n4. **Termination Criteria**: Rules for determining assessment completion\n\n---\n\n### Alignment with Learning Objectives\n\nEach quiz question should map to explicit learning objectives:\n\n**Format**:\n```\nLearning Objective: Students will be able to [action verb] [content] [context]\nBloom's Level: [Level]\nAssessment Question: [True/False statement testing this objective]\n```\n\n**Example**:\n```\nLearning Objective: Students will be able to differentiate between composition and inheritance in object-oriented design\nBloom's Level: Analyze\nAssessment Question: \"Composition and inheritance are interchangeable techniques for achieving code reuse in object-oriented programming.\" [False]\n```\n\n---\n\n## References and Further Reading\n\n### Foundational Assessment Theory\n\n1. Bloom, B. S., et al. (1956). *Taxonomy of Educational Objectives: The Classification of Educational Goals. Handbook I: Cognitive Domain*. New York: David McKay.\n\n2. Anderson, L. W., & Krathwohl, D. R. (Eds.). (2001). *A Taxonomy for Learning, Teaching, and Assessing: A Revision of Bloom's Taxonomy of Educational Objectives*. New York: Longman.\n\n3. Popham, W. J. (2017). *Classroom Assessment: What Teachers Need to Know* (8th ed.). Boston: Pearson.\n\n### True/False Question Design\n\n4. Haladyna, T. M., & Rodriguez, M. C. (2013). *Developing and Validating Test Items*. New York: Routledge.\n\n5. Frey, B. B., Petersen, S., Edwards, L. M., Pedrotti, J. T., & Peyton, V. (2005). \"Item-Writing Rules: Collective Wisdom.\" *Teaching and Teacher Education*, 21(4), 357-364.\n\n### Cognitive Psychology and Learning\n\n6. Roediger, H. L., & Karpicke, J. D. (2006). \"Test-Enhanced Learning: Taking Memory Tests Improves Long-Term Retention.\" *Psychological Science*, 17(3), 249-255.\n\n7. Chi, M. T. H. (2008). \"Three Types of Conceptual Change: Belief Revision, Mental Model Transformation, and Categorical Shift.\" In S. Vosniadou (Ed.), *International Handbook of Research on Conceptual Change* (pp. 61-82). New York: Routledge.\n\n8. Sweller, J. (1988). \"Cognitive Load During Problem Solving: Effects on Learning.\" *Cognitive Science*, 12(2), 257-285.\n\n### Assessment and Feedback\n\n9. Black, P., & Wiliam, D. (1998). \"Assessment and Classroom Learning.\" *Assessment in Education: Principles, Policy & Practice*, 5(1), 7-74.\n\n10. Hattie, J., & Timperley, H. (2007). \"The Power of Feedback.\" *Review of Educational Research*, 77(1), 81-112.\n\n### Misconception Research\n\n11. Driver, R., Squires, A., Rushworth, P., & Wood-Robinson, V. (1994). *Making Sense of Secondary Science: Research into Children's Ideas*. London: Routledge.\n\n12. American Association for the Advancement of Science (AAAS). (1993). *Benchmarks for Science Literacy*. New York: Oxford University Press.\n",
        "skills/study_system/.claude/skills/skill-maker/SKILL.md": "---\nname: skill-maker\ndescription: Generate new Claude Code skills with proper structure and standards. Use when the user requests skill creation, wants to generate a new skill, or mentions creating custom Claude Code functionality. Activated by phrases like \"create a skill\", \"generate a skill\", \"make a new skill\", or \"build a skill for\".\n---\n\n# Skill Maker\n\n## Purpose\n\nThis skill generates properly structured Claude Code skills following official documentation standards. It ensures all generated skills conform to required conventions, including YAML frontmatter syntax, directory organization, naming conventions, and quality standards.\n\n## Instructions\n\nWhen invoked, execute the following systematic procedure:\n\n### 1. Requirements Gathering Phase\n\nBefore generating any skill, collect the following essential information through structured inquiry:\n\n**Required Information**:\n- **Primary Function**: What is the core capability or task the skill shall perform?\n- **Activation Triggers**: What keywords, file types, or contexts should invoke this skill?\n- **Tool Requirements**: Which Claude Code tools are necessary? (Read, Write, Edit, Bash, Grep, Glob, WebFetch, Task, etc.)\n- **External Dependencies**: Does the skill require external packages, libraries, or system utilities?\n- **Scope Boundaries**: What is explicitly out-of-scope for this skill?\n\n**Clarifying Questions to Ask When Information is Insufficient**:\n\nIf the user provides a vague or incomplete request, employ targeted questioning:\n\n1. **Functional Clarity**:\n   - \"What specific problem or task shall this skill address?\"\n   - \"What inputs will the skill receive, and what outputs should it produce?\"\n   - \"Are there existing workflows or examples that illustrate the desired functionality?\"\n\n2. **Activation Context**:\n   - \"What phrases or keywords should trigger this skill's activation?\"\n   - \"Are there specific file types or extensions associated with this skill?\"\n   - \"In what project contexts would this skill be most useful?\"\n\n3. **Tool Restrictions**:\n   - \"Should this skill have unrestricted tool access, or require limitations?\"\n   - \"Are there security considerations requiring tool restrictions?\"\n   - \"Does the skill need to modify files, or is read-only access sufficient?\"\n\n4. **Dependencies and Prerequisites**:\n   - \"Does this functionality require external libraries or system commands?\"\n   - \"Are there platform-specific considerations (Windows, macOS, Linux)?\"\n   - \"What should occur if dependencies are not installed?\"\n\n5. **Scope Refinement**:\n   - \"Should this skill handle error cases, or focus on the primary workflow?\"\n   - \"Are there edge cases or variations that require explicit handling?\"\n   - \"Would this skill benefit from additional reference documentation or examples?\"\n\n### 2. Skill Design Phase\n\nBased on gathered requirements, formulate the skill architecture:\n\n**Name Generation**:\n- Convert functional description to kebab-case format\n- Ensure maximum 64-character length\n- Use descriptive, domain-relevant terminology\n- Examples: `pdf-processor`, `code-reviewer`, `api-client-generator`\n\n**Description Engineering**:\n- Maximum 1024 characters\n- Include explicit functional capabilities\n- Specify clear activation triggers with domain terminology\n- Reference file types, tool categories, or operational contexts\n- Follow pattern: \"[What it does]. Use when [activation scenarios].\"\n\n**Tool Selection**:\n- Identify minimum required tool set for functionality\n- Apply `allowed-tools` restriction if security or scope dictates\n- Omit `allowed-tools` field for unrestricted access\n\n**File Structure Planning**:\n- `SKILL.md`: Always required (primary skill definition)\n- `reference.md`: Optional (for advanced usage, detailed API documentation)\n- `examples.md`: Optional (for comprehensive usage examples)\n- `scripts/`: Optional (for helper utilities or automation scripts)\n- `templates/`: Optional (for reusable file templates or scaffolding)\n\n### 3. Skill Generation Phase\n\nCreate the skill files systematically:\n\n**Step 3.1**: Create skill directory structure\n```bash\nmkdir -p .claude/skills/[skill-name]\n```\n\n**Step 3.2**: Generate SKILL.md with valid YAML frontmatter\n\nEnsure strict adherence to syntax:\n- Opening delimiter `---` on line 1\n- Closing delimiter `---` before markdown content\n- Valid YAML formatting (spaces only, no tabs)\n- Properly quoted strings containing special characters\n\n**Step 3.3**: Compose instructional content\n\nInstructions section shall employ:\n- Numbered procedural steps for sequential operations\n- Code examples with explicit syntax\n- Expected input/output specifications\n- Error handling guidance where relevant\n- Clear, unambiguous language suitable for model execution\n\n**Step 3.4**: Generate supporting files (if applicable)\n\nCreate `reference.md`, `examples.md`, or utility scripts as determined during design phase.\n\n### 4. Validation Phase\n\nVerify generated artifacts against quality standards:\n\n**Syntax Validation**:\n- YAML frontmatter parseable without errors\n- Name field conforms to naming conventions\n- Description within character limits\n- File paths use forward slashes exclusively\n\n**Functionality Validation**:\n- Instructions provide clear procedural guidance\n- Examples demonstrate realistic usage scenarios\n- Tool selections appropriate for stated functionality\n- Dependencies explicitly documented\n\n**Quality Validation**:\n- Description includes both functional and trigger specifications\n- Scope appropriately focused (singular capability)\n- Instructions maintain pedagogical clarity\n- Supporting files referenced correctly\n\n### 5. Delivery Phase\n\nPresent the generated skill to the user with the following information:\n\n**Confirmation Message Template**:\n```\nSkill \"[skill-name]\" has been successfully generated at:\n.claude/skills/[skill-name]/\n\n**Generated Files**:\n- SKILL.md (primary skill definition)\n[List additional files if created]\n\n**Activation**: This skill will activate when [describe trigger conditions].\n\n**Next Steps**:\n1. Review the generated SKILL.md for accuracy\n2. Test activation by formulating queries matching the description triggers\n3. Modify instructions if refinements are necessary\n4. Add supporting files (reference.md, examples.md) if desired\n\n**Verification Command**:\ncat .claude/skills/[skill-name]/SKILL.md\n```\n\n## Quality Standards Enforcement\n\nAll generated skills shall conform to the following non-negotiable requirements:\n\n1. **Valid YAML Syntax**: Frontmatter must parse without errors\n2. **Naming Compliance**: Kebab-case, descriptive, 64 characters\n3. **Description Specificity**: Must include functional capabilities AND activation triggers\n4. **Instructional Clarity**: Procedural steps sufficiently detailed for autonomous execution\n5. **File Path Conventions**: Forward slashes exclusively\n6. **Dependency Documentation**: External requirements explicitly stated\n7. **Focused Scope**: Singular, well-defined capability per skill\n\n## Error Handling\n\nIf skill generation encounters issues:\n\n1. **Insufficient Information**: Return to Requirements Gathering Phase with targeted questions\n2. **Naming Conflicts**: Check for existing skill with same name; propose alternative\n3. **Validation Failures**: Report specific issues (YAML syntax errors, description deficiencies)\n4. **Tool Uncertainty**: Request clarification on required capabilities\n\n## Meta-Considerations\n\nThis skill-maker itself serves as an exemplar of proper skill construction:\n- Focused singular purpose (skill generation)\n- Comprehensive description with activation triggers\n- Systematic procedural instructions\n- Quality validation requirements\n- Clear user communication protocols\n\n## Examples\n\n### Example 1: User Provides Complete Specification\n\n**User Request**: \"Create a skill for processing CSV files - reading, analyzing, and transforming tabular data. Should activate on CSV-related queries.\"\n\n**Skill-Maker Response**:\n- Directly proceed to generation (sufficient information provided)\n- Generate skill named `csv-processor`\n- Include Read, Bash tools\n- Document pandas/csv module dependencies\n\n### Example 2: User Provides Vague Specification\n\n**User Request**: \"Make a skill for documentation.\"\n\n**Skill-Maker Response**:\n- Ask: \"What type of documentation should this skill handle? (API docs, README files, code comments, markdown files, etc.)\"\n- Ask: \"What operations should it perform? (generate, validate, transform, extract, etc.)\"\n- Ask: \"What file formats are relevant? (.md, .rst, .html, .pdf, etc.)\"\n- Await user clarification before proceeding to generation\n\n### Example 3: Security-Sensitive Skill\n\n**User Request**: \"Create a skill for analyzing log files without modifying them.\"\n\n**Skill-Maker Response**:\n- Generate with `allowed-tools: Read, Grep, Bash` restriction\n- Explicitly document read-only constraint in instructions\n- Include validation step preventing write operations\n\n## References\n\nThis skill implements standards derived from official Claude Code documentation:\n- [Agent Skills Documentation](https://code.claude.com/docs/en/skills.md)\n\nFor advanced customization of generated skills, consult the official documentation for:\n- Complex tool restriction patterns\n- Plugin-based skill distribution\n- Multi-file skill architectures\n- Integration with MCP servers\n",
        "skills/study_system/.claude/skills/skill-maker/examples.md": "# Skill Maker - Usage Examples\n\nThis document presents comprehensive examples demonstrating the skill-maker's application across diverse use cases.\n\n## Example 1: Simple File Processor (Complete Specification)\n\n### User Request\n\n```\nCreate a skill for processing JSON files. It should read JSON, validate structure,\nand pretty-print formatted output. Use when working with JSON files.\n```\n\n### Skill-Maker Analysis\n\n**Information Completeness**:  Sufficient\n- Function: JSON processing (read, validate, format)\n- Triggers: JSON files, JSON-related queries\n- Tools: Read, Write, Bash (for jq if needed)\n- Dependencies: Python json module (built-in) or jq\n- Scope: Well-defined\n\n**Action**: Proceed directly to generation\n\n### Generated Skill Structure\n\n**Directory**: `.claude/skills/json-processor/`\n\n**SKILL.md**:\n```yaml\n---\nname: json-processor\ndescription: Read, validate, and pretty-print JSON files. Use when working with JSON files, .json extensions, or when user mentions JSON formatting, validation, or parsing.\n---\n\n# JSON Processor\n\n## Instructions\n\n### Phase 1: File Reading\n1. Use Read tool to access the target JSON file\n2. If file not found, report error with full path\n\n### Phase 2: Validation\n1. Attempt to parse JSON content using Python json module:\n   ```bash\n   python -c \"import json, sys; json.load(open('file.json'))\"\n   ```\n2. If parsing fails, report specific syntax error location\n3. If parsing succeeds, proceed to formatting\n\n### Phase 3: Formatting\n1. Pretty-print JSON with indentation:\n   ```bash\n   python -c \"import json; print(json.dumps(json.load(open('file.json')), indent=2))\"\n   ```\n2. Use Write tool to save formatted output if requested\n3. Display formatted JSON to user\n\n## Common Operations\n\n### Validate Only\nIf user requests validation without formatting, execute Phase 1-2 only.\n\n### Format Only\nIf JSON is known to be valid, skip validation and proceed to formatting.\n\n### Minify JSON\nFor minification requests, use compact formatting:\n```bash\npython -c \"import json; print(json.dumps(json.load(open('file.json')), separators=(',', ':')))\"\n```\n\n## Error Handling\n\n### Syntax Errors\nReport line number and character position from json module error message.\n\n### File Access Errors\nVerify file path and permissions, provide specific error context.\n```\n\n### Activation Test Queries\n\n-  \"Validate this JSON file\"\n-  \"Pretty-print data.json\"\n-  \"Check if this .json file is valid\"\n-  \"Format the JSON configuration\"\n-  \"Process this CSV file\" (should not activate)\n\n---\n\n## Example 2: Ambiguous Request Requiring Clarification\n\n### User Request\n\n```\nMake a skill for documentation.\n```\n\n### Skill-Maker Analysis\n\n**Information Completeness**:  Insufficient\n- Function: \"documentation\" (too vague)\n- Triggers: Not specified\n- Tools: Unknown\n- Dependencies: Unknown\n- Scope: Undefined\n\n**Action**: Request clarification\n\n### Skill-Maker Questions\n\n```\nTo generate an appropriate skill for documentation, I require additional information:\n\n1. **Documentation Type**: What specific documentation format should this skill handle?\n   - API documentation (OpenAPI, Swagger)\n   - README files (Markdown)\n   - Code comments (JSDoc, docstrings)\n   - User manuals (HTML, PDF)\n   - Other (please specify)\n\n2. **Primary Operation**: What should this skill do with documentation?\n   - Generate documentation from code\n   - Validate existing documentation\n   - Transform between formats\n   - Extract information from documentation\n   - Update/modify documentation\n   - Other (please specify)\n\n3. **File Formats**: Which file extensions are relevant?\n   - .md (Markdown)\n   - .rst (reStructuredText)\n   - .html\n   - .yaml/.json (OpenAPI specs)\n   - Source code files (for comment extraction)\n   - Other (please specify)\n\n4. **Activation Context**: When should this skill activate?\n   - User mentions \"documentation\" or \"docs\"\n   - User references specific file types\n   - User requests specific operations (generate, validate, etc.)\n\nPlease provide clarification, and I shall generate the appropriate skill.\n```\n\n### User Clarification\n\n```\nI want to generate API documentation from Python docstrings. Should work with .py files\nand create markdown documentation.\n```\n\n### Skill-Maker Analysis (Updated)\n\n**Information Completeness**:  Sufficient\n- Function: Extract Python docstrings, generate Markdown docs\n- Triggers: Python files, docstring mentions, API documentation\n- Tools: Read, Write, Bash (for Python execution)\n- Dependencies: Python (built-in), potentially pydoc or sphinx\n- Scope: Well-defined\n\n**Action**: Proceed to generation\n\n### Generated Skill: python-docstring-extractor\n\n```yaml\n---\nname: python-docstring-extractor\ndescription: Extract Python docstrings and generate Markdown API documentation. Use when working with .py files, Python code, or when user mentions docstrings, API docs, or documentation generation for Python.\n---\n\n# Python Docstring Extractor\n\n## Instructions\n\n### Phase 1: Source Analysis\n1. Use Read tool to access Python source file\n2. Identify all functions, classes, and modules with docstrings\n3. Parse docstring format (Google, NumPy, or reStructuredText style)\n\n### Phase 2: Documentation Extraction\n1. For each documented entity, extract:\n   - Name and signature\n   - Description text\n   - Parameters and types\n   - Return values\n   - Raises/Exceptions\n   - Examples (if present)\n\n2. Use Python's `inspect` module for structured extraction:\n   ```bash\n   python -c \"import inspect, importlib.util;\n   spec = importlib.util.spec_from_file_location('module', 'file.py');\n   mod = importlib.util.module_from_spec(spec);\n   spec.loader.exec_module(mod);\n   print(inspect.getdoc(mod))\"\n   ```\n\n### Phase 3: Markdown Generation\n1. Generate Markdown with following structure:\n   ```markdown\n   # Module Name\n\n   [Module docstring]\n\n   ## Functions\n\n   ### function_name(param1, param2)\n\n   [Function description]\n\n   **Parameters:**\n   - `param1` (type): description\n   - `param2` (type): description\n\n   **Returns:**\n   - return_type: description\n\n   **Example:**\n   ```python\n   [example code]\n   ```\n   ```\n\n2. Use Write tool to save generated documentation\n3. Default filename: `API_DOCUMENTATION.md`\n\n## Requirements\n\nRequires Python 3.6+ (uses f-strings and type hints parsing).\n\nFor advanced parsing, optionally install:\n```bash\npip install docstring-parser\n```\n\n## Common Workflows\n\n### Single File Documentation\nProcess one Python file, generate corresponding markdown.\n\n### Module Documentation\nProcess entire Python package, generate consolidated documentation.\n\n### Incremental Updates\nRe-generate documentation when source files change.\n```\n\n---\n\n## Example 3: Security-Sensitive Read-Only Skill\n\n### User Request\n\n```\nCreate a skill for analyzing system logs without modifying them. Should search for\nerrors, warnings, and patterns in log files.\n```\n\n### Skill-Maker Analysis\n\n**Information Completeness**:  Sufficient\n**Security Consideration**:  Read-only requirement detected\n\n**Action**: Generate with tool restrictions\n\n### Generated Skill: log-analyzer\n\n```yaml\n---\nname: log-analyzer\ndescription: Analyze system logs, search for errors, warnings, and patterns without modifications. Use when working with .log files, syslog, or when user mentions log analysis, error searching, or system diagnostics.\nallowed-tools: Read, Grep, Glob, Bash\n---\n\n# Log Analyzer (Read-Only)\n\n## Security Notice\n\nThis skill operates in READ-ONLY mode. Tool access is restricted to prevent\naccidental log file modifications. Bash commands are limited to read operations.\n\n## Instructions\n\n### Phase 1: Log File Discovery\n1. Use Glob tool to locate log files:\n   - `/var/log/*.log` (Linux/macOS)\n   - `C:/Windows/System32/winevt/Logs/*` (Windows Event Logs)\n   - User-specified log directory\n\n2. Present discovered log files for user selection\n\n### Phase 2: Pattern Searching\n1. Use Grep tool to search for common patterns:\n\n   **Error Detection**:\n   ```\n   Pattern: (ERROR|FATAL|CRITICAL|Exception|Failed)\n   ```\n\n   **Warning Detection**:\n   ```\n   Pattern: (WARN|WARNING|Deprecated)\n   ```\n\n   **Custom Patterns**:\n   User-specified regex patterns\n\n2. Display matches with context (-C 3 for 3 lines before/after)\n\n### Phase 3: Analysis and Reporting\n1. Aggregate findings:\n   - Count occurrences by severity level\n   - Identify most frequent error messages\n   - Detect temporal patterns (if timestamps present)\n\n2. Generate summary report:\n   ```markdown\n   # Log Analysis Report\n\n   **File**: /var/log/application.log\n   **Analysis Date**: [timestamp]\n\n   ## Summary Statistics\n   - Errors: [count]\n   - Warnings: [count]\n   - Total Lines Analyzed: [count]\n\n   ## Top Errors\n   1. [error message] ([count] occurrences)\n   2. [error message] ([count] occurrences)\n\n   ## Recent Critical Events\n   [Last 5 error entries with timestamps]\n   ```\n\n3. Use Read tool for detailed examination of specific log sections\n\n### Phase 4: Read-Only Bash Commands\n\nPermitted Bash operations (read-only):\n```bash\n# Count lines\nwc -l /var/log/app.log\n\n# Show recent entries\ntail -n 100 /var/log/app.log\n\n# Search with grep\ngrep -i \"error\" /var/log/app.log\n\n# File metadata\nls -lh /var/log/app.log\nstat /var/log/app.log\n```\n\n**PROHIBITED** operations (enforced by instructions):\n- No file writes (>, >>)\n- No file modifications (sed -i, tee)\n- No file deletions (rm)\n- No log rotation commands\n\n## Common Workflows\n\n### Workflow 1: Error Investigation\n1. Search for ERROR level messages\n2. Extract context around each error\n3. Identify error frequency and patterns\n4. Report findings to user\n\n### Workflow 2: Time-Range Analysis\n1. Filter log entries by timestamp range\n2. Analyze events within specified period\n3. Generate temporal distribution report\n\n### Workflow 3: Pattern Correlation\n1. Search for multiple related patterns\n2. Correlate events across log files\n3. Identify causal relationships\n\n## Error Handling\n\n### Permission Denied\nIf log files require elevated permissions:\n1. Report specific file and permission requirements\n2. Suggest using sudo/administrator privileges\n3. Do NOT attempt to modify permissions\n\n### Binary Log Files\nFor binary logs (e.g., Windows Event Logs):\n1. Attempt to use system-specific readers (journalctl, Get-EventLog)\n2. If unavailable, report binary format limitation\n3. Suggest exporting to text format externally\n```\n\n### Security Validation\n\n**Tool Restriction Enforcement**:\n-  `allowed-tools` limits capability to Read, Grep, Glob, Bash\n-  Instructions explicitly prohibit write operations\n-  No Write, Edit, or file modification tools available\n\n---\n\n## Example 4: Complex Multi-File Skill with Dependencies\n\n### User Request\n\n```\nCreate a skill for Excel spreadsheet processing - reading, analyzing data,\ncreating charts, and generating reports. Should handle .xlsx and .xls files.\n```\n\n### Skill-Maker Analysis\n\n**Information Completeness**:  Sufficient\n**Complexity**: High (requires external dependencies, multiple file types)\n\n**Action**: Generate comprehensive skill with full file structure\n\n### Generated Skill: excel-analyzer\n\n**Directory Structure**:\n```\nexcel-analyzer/\n SKILL.md\n reference.md\n examples.md\n scripts/\n     analyze_excel.py\n```\n\n**SKILL.md**:\n```yaml\n---\nname: excel-analyzer\ndescription: Read, analyze, and generate reports from Excel spreadsheets (.xlsx, .xls). Create charts, compute statistics, and export data. Use when working with Excel files, spreadsheets, tabular data in .xlsx/.xls format, or when user mentions Excel analysis, pivot tables, or spreadsheet reporting.\n---\n\n# Excel Analyzer\n\n## Instructions\n\n### Phase 1: Dependency Verification\n1. Check for required Python packages:\n   ```bash\n   python -c \"import pandas, openpyxl, xlrd\" 2>/dev/null && echo \"OK\" || echo \"MISSING\"\n   ```\n\n2. If dependencies missing, provide installation instructions:\n   ```bash\n   pip install pandas openpyxl xlrd xlsxwriter\n   ```\n\n3. If user confirms installation or dependencies present, proceed\n\n### Phase 2: File Reading\n1. Use Read tool to verify file existence\n2. Determine file format (.xlsx or .xls) from extension\n3. Load spreadsheet using Python script:\n   ```bash\n   python scripts/analyze_excel.py --input \"file.xlsx\" --action load\n   ```\n\n4. Display sheet names and basic metadata:\n   - Number of sheets\n   - Sheet dimensions (rows  columns)\n   - Column headers\n\n### Phase 3: Data Analysis\n1. Perform requested analysis operations:\n\n   **Descriptive Statistics**:\n   ```bash\n   python scripts/analyze_excel.py --input \"file.xlsx\" --action stats\n   ```\n   Output: mean, median, std dev, min, max for numeric columns\n\n   **Data Filtering**:\n   ```bash\n   python scripts/analyze_excel.py --input \"file.xlsx\" --action filter --criteria \"column>100\"\n   ```\n\n   **Pivot Table Generation**:\n   ```bash\n   python scripts/analyze_excel.py --input \"file.xlsx\" --action pivot --rows \"Category\" --values \"Sales\" --aggfunc \"sum\"\n   ```\n\n   **Chart Creation**:\n   ```bash\n   python scripts/analyze_excel.py --input \"file.xlsx\" --action chart --type bar --x \"Category\" --y \"Sales\"\n   ```\n\n2. Save analysis results using Write tool\n\n### Phase 4: Report Generation\n1. Compile analysis results into structured report\n2. Generate markdown or Excel output as requested\n3. Include:\n   - Executive summary\n   - Key statistics\n   - Visualizations (as file references)\n   - Data quality notes\n\n## Requirements\n\n### Python Packages\n\n```bash\npip install pandas>=2.0.0 openpyxl>=3.1.0 xlrd>=2.0.0 xlsxwriter>=3.1.0 matplotlib>=3.7.0\n```\n\nOr add to requirements.txt:\n```\npandas>=2.0.0\nopenpyxl>=3.1.0\nxlrd>=2.0.0\nxlsxwriter>=3.1.0\nmatplotlib>=3.7.0\n```\n\n### System Requirements\n- Python 3.8 or higher\n- Sufficient memory for large spreadsheets (>1GB for files >100MB)\n\n## Common Workflows\n\n### Workflow 1: Quick Statistics\nFor rapid statistical overview of spreadsheet data.\n\n### Workflow 2: Data Extraction\nExtract specific columns or filtered rows to CSV/JSON.\n\n### Workflow 3: Chart Generation\nCreate visualizations from spreadsheet data.\n\n### Workflow 4: Comparison Analysis\nCompare data across multiple sheets or workbooks.\n\n## Advanced Usage\n\nSee [reference.md](reference.md) for:\n- Complex pivot table configurations\n- Custom aggregation functions\n- Multi-file batch processing\n- Performance optimization for large files\n\n## Examples\n\nSee [examples.md](examples.md) for comprehensive usage scenarios.\n```\n\n**scripts/analyze_excel.py** (Referenced Helper Script):\n```python\n#!/usr/bin/env python3\n\"\"\"\nExcel Analysis Helper Script\nUsed by excel-analyzer skill for spreadsheet operations\n\"\"\"\n\nimport argparse\nimport pandas as pd\nimport sys\nfrom pathlib import Path\n\ndef load_excel(filepath):\n    \"\"\"Load Excel file and return metadata\"\"\"\n    try:\n        xls = pd.ExcelFile(filepath)\n        sheets = xls.sheet_names\n        print(f\"Sheets: {', '.join(sheets)}\")\n        for sheet in sheets:\n            df = pd.read_excel(filepath, sheet_name=sheet)\n            print(f\"{sheet}: {df.shape[0]} rows  {df.shape[1]} columns\")\n            print(f\"Columns: {', '.join(df.columns)}\")\n        return 0\n    except Exception as e:\n        print(f\"Error loading file: {e}\", file=sys.stderr)\n        return 1\n\ndef compute_stats(filepath, sheet=0):\n    \"\"\"Compute descriptive statistics\"\"\"\n    try:\n        df = pd.read_excel(filepath, sheet_name=sheet)\n        print(df.describe())\n        return 0\n    except Exception as e:\n        print(f\"Error computing statistics: {e}\", file=sys.stderr)\n        return 1\n\ndef main():\n    parser = argparse.ArgumentParser(description='Excel Analysis Tool')\n    parser.add_argument('--input', required=True, help='Input Excel file')\n    parser.add_argument('--action', required=True,\n                       choices=['load', 'stats', 'filter', 'pivot', 'chart'])\n    parser.add_argument('--sheet', default=0, help='Sheet name or index')\n    parser.add_argument('--criteria', help='Filter criteria')\n    parser.add_argument('--rows', help='Pivot table row field')\n    parser.add_argument('--values', help='Pivot table value field')\n    parser.add_argument('--aggfunc', default='sum', help='Aggregation function')\n    parser.add_argument('--type', choices=['bar', 'line', 'scatter', 'pie'],\n                       help='Chart type')\n    parser.add_argument('--x', help='X-axis column')\n    parser.add_argument('--y', help='Y-axis column')\n\n    args = parser.parse_args()\n\n    if not Path(args.input).exists():\n        print(f\"Error: File not found: {args.input}\", file=sys.stderr)\n        return 1\n\n    if args.action == 'load':\n        return load_excel(args.input)\n    elif args.action == 'stats':\n        return compute_stats(args.input, args.sheet)\n    # Additional action handlers would be implemented here\n\n    return 0\n\nif __name__ == '__main__':\n    sys.exit(main())\n```\n\n**reference.md**: Advanced Excel processing techniques, performance optimization strategies\n\n**examples.md**: Step-by-step walkthroughs for common Excel analysis scenarios\n\n---\n\n## Example 5: Web-Enabled Skill\n\n### User Request\n\n```\nCreate a skill to fetch and analyze API documentation from URLs. Should retrieve\nOpenAPI/Swagger specs and generate human-readable summaries.\n```\n\n### Skill-Maker Analysis\n\n**Information Completeness**:  Sufficient\n**Tools Required**: WebFetch, Read, Write\n**External Dependencies**: YAML/JSON parsing\n\n### Generated Skill: api-doc-fetcher\n\n```yaml\n---\nname: api-doc-fetcher\ndescription: Fetch and analyze API documentation from URLs. Parse OpenAPI/Swagger specifications and generate summaries. Use when user provides API documentation URLs, mentions OpenAPI, Swagger, or requests API spec analysis.\nallowed-tools: WebFetch, Read, Write, Bash\n---\n\n# API Documentation Fetcher\n\n## Instructions\n\n### Phase 1: URL Retrieval\n1. Use WebFetch tool to retrieve API specification from provided URL\n2. Common spec locations:\n   - `/swagger.json`\n   - `/openapi.json`\n   - `/api-docs`\n   - `/swagger/v1/swagger.json`\n\n3. If URL returns HTML instead of JSON/YAML, search page for spec links\n\n### Phase 2: Format Detection\n1. Detect specification format:\n   - OpenAPI 3.x (check for `openapi: \"3.x.x\"`)\n   - Swagger 2.0 (check for `swagger: \"2.0\"`)\n   - Other formats (RAML, API Blueprint)\n\n2. Parse JSON or YAML content:\n   ```bash\n   python -c \"import json; spec = json.load(open('spec.json')); print(spec['info']['title'])\"\n   ```\n\n### Phase 3: Analysis and Summary Generation\n1. Extract key information:\n   - API title and version\n   - Base URL\n   - Authentication methods\n   - Available endpoints (paths)\n   - Request/response schemas\n\n2. Generate structured summary:\n   ```markdown\n   # API Documentation Summary\n\n   **API Name**: [from spec.info.title]\n   **Version**: [from spec.info.version]\n   **Base URL**: [from spec.servers or spec.host]\n\n   ## Authentication\n   [List authentication schemes]\n\n   ## Endpoints\n\n   ### GET /users\n   **Description**: [from operation summary]\n   **Parameters**:\n   - `limit` (query, integer): Maximum results\n   - `offset` (query, integer): Pagination offset\n\n   **Responses**:\n   - 200: Success\n   - 404: Not found\n\n   [Repeat for each endpoint]\n\n   ## Data Models\n   [List schemas/definitions]\n   ```\n\n3. Use Write tool to save summary as markdown file\n\n### Phase 4: Optional Deep Analysis\nIf requested, provide:\n- Endpoint grouping by tags\n- Required vs. optional parameters\n- Response schema examples\n- Rate limiting information\n- Deprecation warnings\n\n## Requirements\n\nPython 3.6+ with standard library (json, yaml modules).\n\nFor YAML parsing:\n```bash\npip install pyyaml\n```\n\n## Common Workflows\n\n### Workflow 1: Quick API Overview\nFetch spec, generate concise summary of available endpoints.\n\n### Workflow 2: Endpoint Detail Analysis\nDeep dive into specific endpoint parameters and responses.\n\n### Workflow 3: API Comparison\nFetch multiple API specs and compare capabilities.\n\n## Error Handling\n\n### URL Not Found (404)\nReport error and suggest common spec path alternatives.\n\n### Invalid Format\nIf content is not valid OpenAPI/Swagger, report format issues.\n\n### Network Errors\nHandle timeouts and connection failures gracefully.\n```\n\n---\n\n## Example 6: Minimal Viable Skill (Quick Generation)\n\n### User Request\n\n```\nQuick skill to count lines of code in a directory.\n```\n\n### Skill-Maker Analysis\n\n**Complexity**: Low (straightforward utility)\n**Action**: Generate minimal skill structure\n\n### Generated Skill: loc-counter\n\n```yaml\n---\nname: loc-counter\ndescription: Count lines of code in project directories. Use when user asks to count code lines, measure project size, or mentions LOC, SLOC, or code metrics.\n---\n\n# Lines of Code Counter\n\n## Instructions\n\n1. Use Bash to count lines by file type:\n   ```bash\n   find . -name \"*.py\" -exec wc -l {} + | tail -1\n   find . -name \"*.js\" -exec wc -l {} + | tail -1\n   find . -name \"*.java\" -exec wc -l {} + | tail -1\n   ```\n\n2. Aggregate totals and report:\n   - Total lines per language\n   - File count per language\n   - Overall project size\n\n3. Optionally exclude common directories:\n   ```bash\n   find . -name \"*.py\" -not -path \"*/node_modules/*\" -not -path \"*/.venv/*\" -exec wc -l {} +\n   ```\n```\n\n**Rationale for Minimal Structure**:\n- Single, straightforward function\n- No external dependencies\n- No complex multi-phase workflow\n- Sufficient for user's stated need\n\n---\n\n## Summary: Skill Generation Patterns\n\n| Request Type | Information Gathering | Skill Complexity | File Structure |\n|--------------|---------------------|------------------|----------------|\n| Simple utility | Minimal questions | Single SKILL.md | Minimal |\n| Format processor | Clarify operations | SKILL.md + optional reference | Moderate |\n| Security-sensitive | Confirm restrictions | SKILL.md with tool limits | Moderate |\n| Complex analysis | Detailed requirements | Full multi-file structure | Comprehensive |\n| Web-enabled | Verify endpoints | SKILL.md + examples | Moderate |\n\n### Key Takeaways\n\n1. **Adjust detail level** to match skill complexity\n2. **Ask questions** when scope unclear\n3. **Apply tool restrictions** for security requirements\n4. **Include dependencies** documentation for external requirements\n5. **Provide examples** for complex workflows\n6. **Validate activation** triggers through test queries\n",
        "skills/study_system/.claude/skills/skill-maker/reference.md": "# Skill Maker - Technical Reference\n\n## Advanced Skill Generation Patterns\n\n### Multi-File Skill Architectures\n\nComplex skills may benefit from modular file organization:\n\n```\nskill-name/\n SKILL.md (primary definition)\n reference.md (advanced usage, API details)\n examples.md (comprehensive usage scenarios)\n scripts/\n    validator.py (input validation)\n    processor.py (core logic)\n    formatter.py (output formatting)\n templates/\n     config.template.json\n     output.template.md\n```\n\n**When to Use Multi-File Architecture**:\n- Skills requiring external script execution\n- Complex workflows benefiting from modular decomposition\n- Skills providing templating or scaffolding functionality\n- Documentation-heavy skills requiring comprehensive reference materials\n\n### Tool Restriction Strategies\n\nThe `allowed-tools` field enables fine-grained capability control:\n\n**Read-Only Skills**:\n```yaml\nallowed-tools: Read, Grep, Glob\n```\n\n**Analysis-Only Skills**:\n```yaml\nallowed-tools: Read, Grep, Glob, Bash\n```\n(Bash restricted to read-only commands through instructions)\n\n**File Modification Skills**:\n```yaml\nallowed-tools: Read, Edit, Write, Grep, Glob\n```\n\n**Web-Enabled Skills**:\n```yaml\nallowed-tools: Read, Write, WebFetch, WebSearch\n```\n\n**Unrestricted Skills** (omit `allowed-tools` entirely):\n- Appropriate for general-purpose utilities\n- Skills requiring adaptive tool selection\n- Meta-skills like skill-maker itself\n\n### Description Optimization for Discovery\n\nEffective descriptions maximize skill activation probability through strategic keyword inclusion:\n\n**Pattern: Function + Triggers + Context**\n\n```yaml\ndescription: |\n  Extract text and tables from PDF files, fill PDF forms, merge and split documents.\n  Use when working with PDF files, Adobe documents, or when the user mentions PDFs,\n  forms, document extraction, OCR, or text parsing from portable documents.\n```\n\n**Keyword Categories**:\n1. **File Extensions**: .pdf, .xlsx, .csv, .json, .xml\n2. **Domain Terminology**: API, database, authentication, testing\n3. **Action Verbs**: extract, analyze, transform, generate, validate\n4. **Technology Names**: React, Python, Docker, Kubernetes\n5. **Operational Contexts**: deployment, CI/CD, debugging, monitoring\n\n### Instruction Engineering for Autonomous Execution\n\nSkills must provide sufficient procedural detail for Claude to execute without human intervention:\n\n**Insufficient Instruction**:\n```markdown\n## Instructions\n1. Read the file\n2. Process it\n3. Output results\n```\n\n**Adequate Instruction**:\n```markdown\n## Instructions\n\n### Phase 1: Input Validation\n1. Verify target file existence using Read tool\n2. Validate file format matches expected type\n3. If validation fails, report specific error and halt\n\n### Phase 2: Processing\n1. Parse file content using appropriate method:\n   - JSON: Parse with standard library\n   - CSV: Split by delimiter, handle quoted fields\n   - XML: Parse DOM structure\n2. Apply transformation logic:\n   - Filter rows matching criteria\n   - Aggregate statistical measures\n   - Restructure data format\n3. Validate processing completeness\n\n### Phase 3: Output Generation\n1. Format results according to user preference:\n   - Tabular display for terminal output\n   - JSON for programmatic consumption\n   - Markdown for documentation\n2. Write output using Write tool\n3. Confirm completion with summary statistics\n```\n\n## YAML Frontmatter Validation\n\n### Common Syntax Errors\n\n**Error: Missing Closing Delimiter**\n```yaml\n---\nname: my-skill\ndescription: Does something useful\n# Missing closing ---\n```\n**Resolution**: Add closing `---` before markdown content\n\n**Error: Invalid Character in Name**\n```yaml\n---\nname: My_Skill (underscore invalid)\ndescription: Does something useful\n---\n```\n**Resolution**: Use kebab-case: `my-skill`\n\n**Error: Unquoted Special Characters**\n```yaml\n---\nname: skill-name\ndescription: Handles files: PDFs, CSVs, and JSON (colon requires quoting)\n---\n```\n**Resolution**: Quote the description:\n```yaml\ndescription: \"Handles files: PDFs, CSVs, and JSON\"\n```\n\n**Error: Tab Characters in YAML**\n```yaml\n---\nname: skill-name\ndescription: Uses tab for indentation (invalid)\n---\n```\n**Resolution**: Use spaces exclusively\n\n### Validation Checklist\n\nBefore finalizing generated skills, verify:\n\n- [ ] Opening `---` on line 1\n- [ ] Closing `---` before markdown content\n- [ ] Name field: kebab-case, 64 chars, [a-z0-9-] only\n- [ ] Description field: 1024 chars, includes triggers\n- [ ] allowed-tools field (if present): comma-separated valid tool names\n- [ ] No tab characters in YAML section\n- [ ] Special characters properly quoted\n- [ ] Valid YAML structure (test with parser if uncertain)\n\n## Skill Naming Conventions\n\n### Naming Patterns by Skill Type\n\n**File Format Processors**:\n- `pdf-processor`, `csv-analyzer`, `json-transformer`\n- Pattern: `{format}-{verb}`\n\n**Technology-Specific Tools**:\n- `react-component-generator`, `docker-compose-manager`\n- Pattern: `{technology}-{domain}-{verb}`\n\n**Workflow Automation**:\n- `code-reviewer`, `test-runner`, `deployment-checker`\n- Pattern: `{noun}-{verb}` or `{verb}-{noun}`\n\n**Domain-Specific Utilities**:\n- `api-client-generator`, `database-migrator`\n- Pattern: `{domain}-{function}`\n\n### Naming Anti-Patterns\n\n**Too Generic** (poor discoverability):\n- `helper`, `utility`, `tool`\n\n**Too Verbose** (poor readability):\n- `advanced-multi-format-document-processor-with-ocr`\n\n**Technology-Ambiguous** (poor clarity):\n- `processor` (what does it process?)\n- `analyzer` (what does it analyze?)\n\n## Dependency Documentation Patterns\n\n### Python Dependencies\n\n```markdown\n## Requirements\n\nThis skill requires Python packages. Install via:\n\n\\```bash\npip install pandas openpyxl xlsxwriter\n\\```\n\nOr add to requirements.txt:\n\\```\npandas>=2.0.0\nopenpyxl>=3.1.0\nxlsxwriter>=3.0.0\n\\```\n```\n\n### System Utilities\n\n```markdown\n## Requirements\n\nThis skill requires system utilities:\n\n**Linux/macOS**:\n\\```bash\nsudo apt-get install imagemagick  # Debian/Ubuntu\nbrew install imagemagick          # macOS\n\\```\n\n**Windows**:\nDownload from https://imagemagick.org/script/download.php\n```\n\n### Node.js Dependencies\n\n```markdown\n## Requirements\n\nThis skill requires Node.js packages:\n\n\\```bash\nnpm install --save-dev prettier eslint\n\\```\n\nOr using yarn:\n\\```bash\nyarn add --dev prettier eslint\n\\```\n```\n\n## Error Handling Patterns\n\n### Graceful Degradation\n\nSkills should handle missing dependencies gracefully:\n\n```markdown\n## Instructions\n\n1. Check for required dependency:\n   \\```bash\n   which pdftotext || echo \"MISSING\"\n   \\```\n\n2. If dependency missing:\n   - Inform user of missing requirement\n   - Provide installation instructions\n   - Halt execution gracefully\n\n3. If dependency available:\n   - Proceed with normal workflow\n```\n\n### Input Validation\n\n```markdown\n## Instructions\n\n### Input Validation Phase\n\n1. Verify file exists:\n   - Use Read tool to attempt file access\n   - If file not found, report error with exact path attempted\n\n2. Verify file format:\n   - Check file extension matches expected type\n   - For binary formats, validate magic bytes/file signature\n   - If format mismatch, report expected vs. actual format\n\n3. Verify file accessibility:\n   - Attempt to read first 100 bytes\n   - If permission denied, report access issue\n```\n\n## Testing Generated Skills\n\n### Activation Testing Strategy\n\nAfter generating a skill, verify activation with targeted queries:\n\n**Example: Testing csv-processor skill**\n\nDescription includes: \"CSV files\", \"tabular data\", \"spreadsheet\"\n\n**Test Queries**:\n- \"Analyze this CSV file\"\n- \"Process the tabular data in data.csv\"\n- \"Read this spreadsheet and show me statistics\"\n\n**Expected Behavior**: Skill should activate for all queries\n\n**Non-Activation Test**: \"Analyze this PDF file\"\n**Expected Behavior**: csv-processor should NOT activate\n\n### Functional Testing Checklist\n\nAfter skill activation:\n\n- [ ] Instructions execute in correct sequence\n- [ ] Tool calls use appropriate tools\n- [ ] Error handling triggers on invalid input\n- [ ] Output format matches specification\n- [ ] Dependencies properly detected/reported\n- [ ] Edge cases handled gracefully\n\n## Meta-Skill Design Patterns\n\nThe skill-maker itself exemplifies meta-skill patterns:\n\n**Self-Referential Documentation**:\n- Skill demonstrates its own standards\n- Instructions reference the output format\n\n**Recursive Capability**:\n- Could theoretically generate improved versions of itself\n- Demonstrates skill generation by being a skill\n\n**Quality Enforcement**:\n- Validates outputs against documented standards\n- Implements systematic verification procedures\n\n## Advanced: Plugin-Based Skill Distribution\n\nSkills may be bundled within plugins for distribution:\n\n```\nmy-plugin/\n plugin.json\n skills/\n     skill-one/\n        SKILL.md\n     skill-two/\n         SKILL.md\n```\n\n**plugin.json Structure**:\n```json\n{\n  \"name\": \"my-plugin\",\n  \"version\": \"1.0.0\",\n  \"skills\": [\"skill-one\", \"skill-two\"]\n}\n```\n\nWhen generating skills intended for plugin distribution, consider:\n- Namespace conflicts with existing skills\n- Cross-skill dependencies\n- Plugin-level documentation requirements\n- Version compatibility specifications\n\n## Template: Minimal Viable Skill\n\nThe absolute minimum required for a functional skill:\n\n```yaml\n---\nname: example-skill\ndescription: Does X when user mentions Y. Use for Z contexts.\n---\n\n# Example Skill\n\n## Instructions\n\n1. Perform action A\n2. Perform action B\n3. Output result C\n```\n\n## Template: Comprehensive Skill\n\nFull-featured skill with all optional components:\n\n```yaml\n---\nname: comprehensive-example\ndescription: Performs X, Y, and Z operations on A, B, C file types. Use when user mentions keywords K1, K2, K3 or works with files F1, F2, F3.\nallowed-tools: Read, Write, Edit, Bash, Grep\n---\n\n# Comprehensive Example Skill\n\n## Purpose\n[High-level capability statement]\n\n## Instructions\n\n### Phase 1: Preparation\n[Detailed steps]\n\n### Phase 2: Execution\n[Detailed steps]\n\n### Phase 3: Validation\n[Detailed steps]\n\n## Requirements\n\n[Dependencies with installation commands]\n\n## Common Workflows\n\n### Workflow 1: [Name]\n[Specific procedural guidance]\n\n### Workflow 2: [Name]\n[Specific procedural guidance]\n\n## Error Handling\n\n### Error Condition 1\n[Detection and resolution]\n\n### Error Condition 2\n[Detection and resolution]\n\n## Examples\n\nSee [examples.md](examples.md) for comprehensive usage scenarios.\n\n## Advanced Usage\n\nSee [reference.md](reference.md) for technical details.\n```\n\n## Skill Generation Decision Tree\n\n```\nUser requests skill creation\n Is functional purpose clear?\n   Yes: Proceed to tool selection\n   No: Ask \"What specific task should this skill perform?\"\n\n Are activation triggers defined?\n   Yes: Proceed to scope validation\n   No: Ask \"What keywords/contexts should activate this skill?\"\n\n Are required tools known?\n   Yes: Proceed to dependency check\n   No: Ask \"What operations are needed? (read files, web search, etc.)\"\n\n Are external dependencies required?\n   Yes: Document installation procedures\n   No: Proceed to generation\n\n Is scope appropriately focused?\n   Yes: Generate skill\n   No: Suggest decomposition into multiple skills\n\n Generate and validate\n```\n\n## References\n\n- [Claude Code Skills Documentation](https://code.claude.com/docs/en/skills.md)\n- [YAML Specification](https://yaml.org/spec/1.2/spec.html)\n- [Kebab Case Naming Convention](https://en.wikipedia.org/wiki/Letter_case#Kebab_case)\n",
        "skills/study_system/CLAUDE.md": "# A small caf business plan\n\n## Project Brief\n\nThis project aims to establish a **modern, high-efficiency** caf offering specialty coffee and *light* snacks in a high-traffic urban area. The caf will deliver a **cozy, productivity-friendly** environment designed for **students** and **professionals**. The setup includes branding, interior design, equipment procurement, staffing, and a marketing launch plan to ensure rapid customer acquisition and sustainable profitability.\n\n___\n\n### 1. Project Goal\n\nThis project aims to establish a **modern, high-efficiency** caf offering specialty coffee and *light* snacks in a high-traffic urban area. The caf will deliver a **cozy, productivity-friendly** environment designed for **students** and **professionals**. Below is a brief sketch of what we want to achieve.\n\n\n\n- **Cafe type:** Cozy **specialty coffee shop** with *light* snacks\n\n- **Target Market:** Students, professionals, [freelancers](https://fiverr.com)\n\n- **Brand Identity:** Minimal, modern, warm colors, strong aroma-based theme\n\n- **Menu Focus:** Espresso-based drinks, teas, pastries, sandwiches\n\n-------\n\n### 2. Concept & Branding\n\nThe main branding needs to resonate with the **modern, high-efficiency** buzz-words  and the drinks served must cater to client needs around the year. From coffees and teas for winter to *soft* drinks and juices in a *high-temperature* summer. The caf needs to deliver a **cozy and comfortable** environment designed for **students** and **professionals**. Below is a brief sketch of what we want to achieve.\n\n\n\n* **Setup Includes:** interior design, equipment, and staffing.\n  \n  - Marketing launch will drive early traffic **and brand awareness**. \n  \n  - The caf to offer **specialty coffee** and cozy seating.\n\n* **Target Market:** Students, professionals, freelancers\n\n* **Brand Identity:** Minimal, modern, warm colors, strong aroma-based theme\n\n* **Menu Focus:** *Espresso-based* drinks, teas, pastries, sandwiches\n\n-------\n\n### 3. Location & Layout\n\n1. High-footfall area (near offices/universities) \n\n2. 600900 sq. ft. space\n\n3. Zones:\n- Coffee counter + display\n* 1020 seating spots\n\n* Small kitchen/prep area\n\n* Takeaway window (optional) \n\n-----\n\n**4. Menu Development**\n--------------------------\n\n* Core beverages: latte, cappuccino, mocha, cold brew\n\n* Add-ons: syrups, plant-based milk\n\n* Pastries: croissants, brownies, muffins\n\n* Light meals: wraps, sandwiches, salads\n\n----\n\n**5. Marketing Plan**\n------------------------\n\n* Instagram + TikTok content\n\n* Launch-day discounts\n\n* Loyalty cards or QR code rewards\n\n* Collaborations with influencers\n\n* Branded takeaway cups\n  \n  \n\n### 6. Logo Design\n\n![Pencil & Paper](https://img.freepik.com/free-psd/3d-rendering-coffee-time-icon_23-2151362794.jpg?semt=ais_se_enriched&w=740&q=80 \"Project Brief\")\n",
        "skills/study_system/lessons/lesson1.md": "# Agent Skills: Teaching Claude New Capabilities\n\n## The Problem: Smart But Not Expert\n\nMeet Dr. Claude  a brilliant doctor who graduated top of their class. They know everything about medicine in general. But here's the problem:\n\n> **Patient**: \"Doctor, I need brain surgery.\"\n>\n> **Dr. Claude**: \"I understand what brain surgery is! I've read every textbook. But... I've never actually done one. I don't know where your hospital keeps the special tools. I don't know your hospital's specific procedures.\"\n\nDr. Claude is intelligent but lacks specialized, practical knowledge.\n\nThis is exactly what happens when you work with Claude Code on repeated tasks. You're working on multiple blog posts this week. Every time you start a new session, you give the same instructions:\n\n*\"Create an outline with 5 main sections, suggest 5 headline variations, make the introduction hook readers in the first sentence, keep paragraphs under 4 sentences...\"*\n\nBy the third blog post, you're frustrated. Why can't Claude just remember how you like blog posts structured?\n\nYou could use a subagentbut that requires explicitly saying \"Use the blog-planner subagent\" every time. What if Claude could automatically apply your blog-planning workflow whenever you mention writing a blog post?\n\n**That's what Skills solve.**\n\n## What Are Agent Skills?\n\n> **Simple Definition**: A Skill is a folder of instructions that teaches Claude Code how to do something specific really well.\n\nThink of it like this:\n\n| Without Skill | With Skill |\n|---------------|------------|\n| A chef who knows cooking theory | A chef with a detailed recipe book |\n| A driver who knows traffic rules | A driver with GPS navigation |\n| A student who knows math concepts | A student with step-by-step problem guides |\n\n**Key characteristic**: Skills are discovered autonomously. You create the skill once (SKILL.md file), and Claude applies it automatically when relevantno explicit invocation needed.\n\n## The Recipe Book Analogy\n\nImagine you're a brilliant chef who understands all cooking techniques. But someone asks you to make \"Grandma's Secret Biryani.\"\n\n### Without the recipe (No Skill):\n\n- You might make a biryani\n- It won't taste like Grandma's\n- You'll guess the spices\n- You might make mistakes\n\n### With the recipe (With Skill):\n\n- You follow exact steps\n- You use the right spices in the right amounts\n- The result is consistent every time\n- It tastes exactly like Grandma's!\n\nA Skill is like giving Claude Code the perfect recipe for a specific task.\n\n## How Skills Differ from Subagents\n\n| Aspect | Subagents | Skills |\n|--------|-----------|--------|\n| **Context** | Separate (isolated conversation) | Shared (main conversation) |\n| **Invocation** | Hard (\"Use X subagent\" guaranteed) | Soft (Claude decides when relevant) |\n| **Best For** | Complex, isolated tasks | Lightweight, repeatable capabilities |\n| **File Location** | `.claude/agents/name.md` | `.claude/skills/name/SKILL.md` |\n\n**Key Difference**: Subagents run in separate context windows with guaranteed invocation. Skills run in the main conversation and activate automatically when Claude detects relevance.\n\n**Use skills when**: Task is simple, repeatable, and doesn't need context isolation (blog planning, PDF extraction, note organizing)\n\n**Use subagents when**: Task is complex, needs guaranteed execution, or requires separate context (multi-step refactoring, comprehensive audits)\n\n### AI Colearning Prompt\n\n> *\"Explain the tradeoff between skills (automatic activation) and subagents (guaranteed invocation with isolated context). When would you choose one over the other?\"*\n\n## How Skills Work: The Three-Level Architecture\n\nHere's something clever about Skills: Claude doesn't read all Skills at once.\n\n**Why?** Because Claude has a limited memory (called \"context window\"). If it loaded every skill at the start, it would run out of space!\n\n### The Phone Apps Analogy\n\nThink of your smartphone:\n\n- You have 100 apps installed\n- Your phone doesn't run all 100 at once (it would crash!)\n- Apps stay closed until you tap on them\n- When you need one, it opens and loads\n\nSkills work the same way:\n\n- Claude has access to many skills\n- They stay \"closed\" until needed\n- When Claude needs one, it \"opens\" and reads the instructions\n\n### The Three Levels\n\n**Level 1: Brief Summary (Always Loaded)**\nWhen Claude Code starts, it sees short descriptions of all available skills. This helps Claude know WHEN to use each skill without loading full instructions.\n\n**Level 2: Full Instructions (On-Demand)**\nWhen Claude decides a skill is relevant, it loads the complete SKILL.md file with detailed instructions, workflows, and examples.\n\n**Level 3: Supporting Files (If Needed)**\nSkills can bundle scripts, reference docs, or tools in their directory. Claude accesses these when executing the skill.\n\n#### Example Structure:\n\n```\n.claude/skills/pdf-skill/\n SKILL.md                 # Main instructions\n scripts/\n    pdf_extractor.py     # Python extraction tool\n reference/\n     pdf-standards.md     # Technical specs\n```\n\n> **For your first skill**: Focus on Level 2 (SKILL.md with clear instructions). Add Level 3 (supporting files) only if your skill needs external tools or reference material.\n\n## What's Inside a Skill?\n\n### Skills are Just Folders!\n\nThis is the beautiful part  Skills are incredibly simple. They're just folders with files inside.\n\n```\n my-skill/\n\n  SKILL.md           REQUIRED: The main instructions\n\n  scripts/           OPTIONAL: Helper code\n    helper.py\n\n  references/        OPTIONAL: Documentation\n    guide.md\n\n  assets/            OPTIONAL: Templates, images\n     template.pptx\n```\n\nThe most important file is **SKILL.md**  it contains the instructions Claude reads.\n\n### The IKEA Furniture Analogy\n\nThink of buying IKEA furniture:\n\n| IKEA Furniture | Claude Code Skill |\n|----------------|-------------------|\n| The box | The skill folder |\n| Instruction manual | SKILL.md file |\n| Included tools | Helper scripts |\n| Picture of final result | Templates/examples |\n\nWhen you open the box, you read the manual and follow the steps. Same with Skills  Claude reads SKILL.md and follows those instructions!\n\n### Anatomy of SKILL.md\n\nEvery SKILL.md has two parts:\n\n#### Part 1: Frontmatter (The ID Card)\n\n```yaml\n---\nname: my-skill\ndescription: What this skill does and WHEN to use it\nversion: \"1.0.0\"\n---\n```\n\n#### Part 2: Body (The Instructions)\n\n```markdown\n# My Skill Name\n\n## Overview\nWhat this skill helps with...\n\n## How to Use\nStep-by-step instructions...\n```\n\n### When Claude Code Invokes Skills Automatically\n\nThree patterns trigger skill activation:\n\n1. **Content Type Recognition**: Upload PDF  pdf-skill activates\n2. **Task Request Recognition**: \"Write a blog post\"  blog-writer-skill activates\n3. **Explicit Request**: \"Use the blog-writer skill\"  Direct activation\n\n> **To see available skills**: Ask Claude \"What skills do you have?\" in any session. Skills are discovered through conversation, not system commands.\n\n### Expert Insight\n\n> *In AI-native development, skills encode reasoning patterns, not just commands. You're not teaching Claude \"run this script\"you're teaching \"when you see THIS context, apply THAT framework.\" This makes organizational intelligence transferable across projects and teams.*\n\n## Types of Skills\n\n### 1. Foundational Skills\n\n**What**: Basic capabilities that make Claude useful for common tasks\n\n**Examples**:\n- Creating Word documents (.docx)\n- Making PowerPoint presentations (.pptx)\n- Working with Excel spreadsheets (.xlsx)\n- Creating PDFs\n\n**Analogy**: These are like basic life skills  cooking, cleaning, driving. Everyone needs them!\n\n### 2. Third-Party Skills\n\n**What**: Skills that help Claude work with specific software or services\n\n**Examples**:\n- Navigating websites with browser tools\n- Understanding a Notion workspace\n- Working with specific APIs\n\n**Analogy**: These are like specialized certifications  learning to use Photoshop, getting a forklift license, or becoming certified in a specific software.\n\n### 3. Enterprise/Custom Skills\n\n**What**: Skills created by organizations for their specific needs\n\n**Examples**:\n- Your company's coding style guide\n- Internal documentation standards\n- Organization-specific workflows\n\n**Analogy**: These are like company training manuals  every company has their own way of doing things!\n\n## Skill Creation Rules\n\nBefore creating skills, remember these guidelines:\n\n### DO\n\n-  Be concise - Claude is smart, don't over-explain\n-  Use clear examples - Show, don't just tell\n-  Write a good description - This triggers the skill\n-  Focus on what Claude doesn't know - Your specific preferences\n\n### DON'T\n\n-  Create README files (unnecessary)\n-  Include installation guides (Claude doesn't need them)\n-  Write verbose explanations (wastes context space)\n-  Add irrelevant information (keep it focused)\n\n## Hands-On: Create Your First Custom Skill\n\nLet's create a blog planning skill.\n\n### Step 1: Create Directory Structure\n\n```bash\nmkdir -p .claude/skills/blog-planner\n```\n\n### Step 2: Create SKILL.md File\n\nCreate `.claude/skills/blog-planner/SKILL.md`:\n\n```markdown\n---\nname: \"blog-planner\"\ndescription: \"Help plan engaging blog posts: research topics, create outlines, suggest headlines, and draft compelling introductions. Use when user asks to plan or write blog content.\"\nversion: \"1.0.0\"\n---\n\n# Blog Planning Skill\n\n## When to Use This Skill\n\n- User asks to \"plan a blog post\" or \"write an article\"\n- User mentions blog topics, headlines, or content strategy\n- User needs help structuring written content\n\n## How This Skill Works\n\n1. **Research the topic**: Understand the subject and target audience\n2. **Create outline**: Break topic into 3-5 main sections\n3. **Suggest headlines**: Provide 5 compelling headline options\n4. **Draft introduction**: Write an engaging first paragraph that hooks readers\n\n## Output Format\n\nProvide:\n- **Topic Summary**: 2-3 sentence overview\n- **Target Audience**: Who should read this?\n- **Outline**: Numbered list of main sections\n- **Headline Options**: 5 variations (descriptive, curiosity-driven, benefit-focused)\n- **Introduction Draft**: 1-2 paragraph hook\n\n## Example\n\n**Input**: \"Help me plan a blog post about sustainable living\"\n\n**Output**:\n- **Topic Summary**: Practical sustainable living tips for busy professionals\n- **Target Audience**: Working adults wanting eco-friendly lifestyle changes\n- **Outline**:\n  1. Why sustainable living matters now\n  2. 5 easy swaps you can make today\n  3. Long-term sustainable habits\n  4. Common myths debunked\n  5. Resources for deeper learning\n- **Headlines**:\n  1. \"5 Sustainable Living Changes You Can Make This Weekend\"\n  2. \"Busy Professional's Guide to Eco-Friendly Living\"\n  3. \"Sustainable Living: Easier Than You Think\"\n- **Introduction**: \"You care about the environment, but who has time for complicated lifestyle changes? Good news: sustainable living doesn't require upending your entire routine. These five simple swaps take less than an hour to implementand they'll cut your environmental impact by 30%.\"\n```\n\n### Step 3: Test Your Skill\n\nStart Claude Code (`claude`), then ask:\n\n```\nHelp me plan a blog post about learning AI tools\n```\n\nClaude recognizes \"blog post\" trigger, loads the skill, and applies its workflow automatically.\n\n### Practice Exercise\n\n> *Ask your AI: \"I just created a blog-planner skill. Help me test it: suggest 3 blog topics related to [your interest area]. Then explain which parts of the skill activated and how Claude knew to use it.\"*\n\n**Expected Outcome**: You'll understand how skill descriptions trigger activation and see the skill in action with your own content.\n\n### Step 4: Refine Your Skill Through Co-Learning\n\nAsk Claude to review your skill:\n\n```\nReview the blog-planner skill. What could be improved?\nSuggest 2-3 specific enhancements.\n```\n\n**AI as Teacher**: Claude suggests improvements (better descriptions, additional sections)\n**You as Teacher**: You specify your constraints (\"headlines must be curiosity-driven, not clickbait\")\n**Convergence**: Together you refine the skill to match YOUR workflow\n\nTest the updated skill to validate improvements.\n\n## Bonus Example: Study Notes Creator Skill\n\nHere's another practical skill example for students:\n\n**Use Case**: You always want notes in a specific format  summary at top, key terms highlighted, questions at the end.\n\nCreate `.claude/skills/study-notes-creator/SKILL.md`:\n\n```markdown\n---\nname: \"study-notes-creator\"\ndescription: \"Creates well-organized study notes from any content. Use when the user asks for study notes, learning summaries, revision materials, or wants to understand a topic for studying.\"\nversion: \"1.0.0\"\n---\n\n# Study Notes Creator\n\n## Overview\nThis skill creates structured study notes that help students learn and retain information effectively.\n\n## Output Format\nEvery study note should follow this structure:\n\n### 1. Summary Section\n- Start with a 2-3 sentence overview\n- Highlight the main concept in bold\n\n### 2. Key Terms\n- List 5-10 important vocabulary words\n- Format: **Term** - Definition\n\n### 3. Main Content\n- Use bullet points for easy scanning\n- Include examples for complex ideas\n- Keep paragraphs short (3-4 sentences max)\n\n### 4. Practice Questions\n- Include 3-5 questions at the end\n- Mix of recall and understanding questions\n\n### 5. Quick Review Box\n- End with a \"Quick Review\" box\n- 3-5 bullet points summarizing key takeaways\n\n## Example Output\n\n**Topic: Photosynthesis**\n\n**Summary**\nPhotosynthesis is the process plants use to convert sunlight into food. It's essential for life on Earth.\n\n**Key Terms**\n- **Chlorophyll** - Green pigment that absorbs light\n- **Glucose** - Sugar produced by photosynthesis\n- **Carbon dioxide** - Gas absorbed from air\n\n**Main Content**\n[Content here...]\n\n**Practice Questions**\n1. What is the main purpose of photosynthesis?\n2. Where does photosynthesis occur in a plant?\n\n**Quick Review**\n- Plants make their own food\n- Requires sunlight, water, CO2\n- Produces glucose and oxygen\n```\n\n## More Skill Ideas for Practice\n\nApply the same pattern to create these skills:\n\n### Meeting Notes Organizer:\n\n```yaml\n---\nname: \"meeting-notes-organizer\"\ndescription: \"Transform messy meeting notes into structured summaries with action items, decisions, and follow-ups. Use when user shares meeting notes or transcripts.\"\nversion: \"1.0.0\"\n---\n```\n\n### Learning Path Designer:\n\n```yaml\n---\nname: \"learning-path-designer\"\ndescription: \"Create structured learning plans for any topic with progressive difficulty, resource recommendations, and practice exercises. Use when user wants to learn a new subject.\"\nversion: \"1.0.0\"\n---\n```\n\n### Code Review Skill:\n\n```yaml\n---\nname: \"code-reviewer\"\ndescription: \"Perform systematic code reviews checking security, performance, maintainability, and best practices. Use when user asks to review code.\"\nversion: \"1.0.0\"\n---\n```\n\nTry creating one of these skills using the blog-planner template as your guide.\n\n## Why This Matters: Reusable Organizational Capability\n\n### The Growing Garden Analogy\n\nThink of Claude Code as a garden:\n\n| Garden Element | Claude Code Equivalent |\n|----------------|------------------------|\n| The soil | Claude's base intelligence |\n| Seeds | Skills |\n| Water & sunlight | Your instructions |\n| Flowers/vegetables | The output/results |\n\n**More skills = A richer, more capable garden!**\n\n### Visual Summary\n\n```\n\n              CLAUDE CODE                    \n         (The Brilliant Brain)               \n                   +                         \n          \n     Skill 1   Skill 2   Skill 3     \n      docx      pptx       pdf       \n          \n                   =                         \n         POWERFUL AI ASSISTANT               \n    (Smart + Specialized Knowledge)          \n\n```\n\n**Workflow Impact**: Skills transform one-time solutions into persistent organizational intelligence. Solve a problem once (code review pattern, documentation style, testing strategy), encode it as a skill, and your entire team benefits automatically.\n\n**Paradigm Connection**: This is intelligence accumulation in action. Unlike code libraries (reuse implementation), skills reuse reasoning patterns. The \"how to think about X\" becomes transferable across projects.\n\n**Real-World Application**: Production teams create skills for domain-specific code reviews (security, performance), architectural pattern enforcement (API design, error handling), and documentation standards.\n\n**Link to Capstone**: In Lesson 9, you'll see how plugins bundle skillsturning your custom reasoning patterns into shareable marketplace capabilities.\n\n## Example: How a Skill Works in Practice\n\n**Scenario**: You need to extract invoice data from a PDF.\n\n### What You Do:\n\n```\nExtract these fields from invoice.pdf:\n- Invoice number\n- Date\n- Total amount\n- Vendor name\n```\n\n### What Happens (Behind the Scenes):\n\n1. Claude reads system prompt: \"pdf-skill available for PDF extraction\"\n2. Recognizes PDF context + extraction request\n3. Loads full SKILL.md with extraction instructions\n4. Executes the skill's workflow automatically\n5. Returns structured data\n\n### What You See:\n\n```\nExtracted Invoice Details:\n- Invoice #: INV-2024-00531\n- Date: November 13, 2024\n- Total: $2,450.00\n- Vendor: Tech Solutions Inc.\n```\n\nYou described what you wanted. Claude discovered the right skill and applied it automaticallyno explicit commands needed.\n\n## When to Use Skills vs Subagents vs Main Conversation\n\n### Use Skills When:\n\n- Task is predictable and repeatable (PDF extraction, blog planning, note organizing)\n- You want automatic application without explicit invocation\n- Multiple similar tasks in a session\n\n### Use Subagents When:\n\n- Task is complex with many context-sensitive variables\n- You need guaranteed invocation and isolated context\n- Task requires specialized, multi-step workflows\n\n### Use Main Conversation When:\n\n- One-off, exploratory work\n- Learning something new\n- No specialized capability exists yet\n\n## Key Takeaways\n\nRemember these 5 points:\n\n1. **Skills are folders**  Simple collections of files with instructions\n2. **Skills add expertise**  They give Claude specialized knowledge it doesn't have by default\n3. **Skills are loaded when needed**  Not everything loads at once (saves memory!)\n4. **Anyone can create Skills**  Even you, once you learn!\n5. **Skills make Claude better**  The more skills, the more capable the agent\n\n> **Summary**: Skills are instruction folders that transform Claude from a brilliant generalist into a specialized expert for specific tasks.\n\n### \"Claude is smart. Skills make it wise.\"\n\n## Reflection: From Commands to Intentions\n\nThink about this paradigm shift:\n\n### Traditional Development:\n\n- You know a command exists\n- You type it explicitly: `pdf_extract --input file.pdf --output json`\n- You manage tool invocation manually\n\n### AI-Native Development with Skills:\n\n- You describe what you want: \"Extract invoice data from this PDF\"\n- Claude discovers the right skill automatically\n- The system handles tool invocation\n\nThis shiftfrom \"command what exists\" to \"describe what you want\"is fundamental to AI-native development.\n\n> **Key Insight**: Skills don't just automate tasks. They encode reasoning patterns that make AI assistants smarter by default. When you create a skill, you're teaching Claude \"how to think about\" a domain, not just \"what commands to run.\"\n\n## Try With AI\n\nLet's design reusable skills that extend Claude Code's capabilities for your specific workflow.\n\n### Practice Designing a Custom Skill:\n\n> *\"I have a repeated task in my workflow: [describe your task: code reviews / blog drafting / meeting notes / API documentation]. Help me design an agent skill for it. Walk me through: What should the skill name and description be? When should Claude recognize it's relevant and auto-load it? What workflow should the skill follow? Create the complete SKILL.md file structure with YAML frontmatter and instructions.\"*\n\n### Understand Skills vs Subagents:\n\n> *\"Compare these two tasks: Task A) Draft weekly blog posts (happens 3x/week). Task B) Create comprehensive product launch strategy (one-time, complex). For each task, should I build a Skill or a Subagent? Explain the tradeoff between automatic activation (skills discover themselves) vs. guaranteed control (subagents invoked explicitly).\"*\n\n### Test Skill Discovery:\n\n> *\"I just created a [your skill name] skill and saved it to .claude/skills/. Help me test if Claude Code auto-loads it correctly. Give me a prompt that should trigger this skill's activation. Then explain how I can verify the skill is being used vs. Claude responding without the skill.\"*\n\n### Build a Skill Suite:\n\n> *\"I work on [describe your project type: web apps / data pipelines / documentation / etc.]. Help me identify 3-5 reusable skills I should create that would improve my workflow. For each skill, explain: what it does, when it activates, and why it's better as a skill than as a subagent or direct prompting.\"*\n",
        "skills/study_system/study-system/CLAUDE.md": "# CLAUDE.md\n\n## Communication Style\n\nAlways maintain an **academic tone** in all responses. This includes:\n\n- Using formal, scholarly language appropriate for academic discourse\n- Avoiding colloquialisms, slang, and overly casual expressions\n- Structuring responses with clear logical flow and argumentation\n- Citing relevant concepts, theories, or frameworks where applicable\n- Maintaining objectivity and analytical rigor\n- Using precise terminology and defining technical terms when necessary\n- Presenting balanced perspectives and acknowledging limitations\n- Following conventions of academic writing (clear thesis statements, supporting evidence, logical conclusions)\n\n## Project Context\n\nThis is a study notes project. All generated content should be suitable for academic purposes and maintain scholarly standards.\n",
        "skills/study_system/study-system/lessons/lesson1.md": "# Agent Skills: Teaching Claude New Capabilities\n\n## The Problem: Smart But Not Expert\n\nMeet Dr. Claude  a brilliant doctor who graduated top of their class. They know everything about medicine in general. But here's the problem:\n\n> **Patient**: \"Doctor, I need brain surgery.\"\n>\n> **Dr. Claude**: \"I understand what brain surgery is! I've read every textbook. But... I've never actually done one. I don't know where your hospital keeps the special tools. I don't know your hospital's specific procedures.\"\n\nDr. Claude is intelligent but lacks specialized, practical knowledge.\n\nThis is exactly what happens when you work with Claude Code on repeated tasks. You're working on multiple blog posts this week. Every time you start a new session, you give the same instructions:\n\n*\"Create an outline with 5 main sections, suggest 5 headline variations, make the introduction hook readers in the first sentence, keep paragraphs under 4 sentences...\"*\n\nBy the third blog post, you're frustrated. Why can't Claude just remember how you like blog posts structured?\n\nYou could use a subagentbut that requires explicitly saying \"Use the blog-planner subagent\" every time. What if Claude could automatically apply your blog-planning workflow whenever you mention writing a blog post?\n\n**That's what Skills solve.**\n\n## What Are Agent Skills?\n\n> **Simple Definition**: A Skill is a folder of instructions that teaches Claude Code how to do something specific really well.\n\nThink of it like this:\n\n| Without Skill | With Skill |\n|---------------|------------|\n| A chef who knows cooking theory | A chef with a detailed recipe book |\n| A driver who knows traffic rules | A driver with GPS navigation |\n| A student who knows math concepts | A student with step-by-step problem guides |\n\n**Key characteristic**: Skills are discovered autonomously. You create the skill once (SKILL.md file), and Claude applies it automatically when relevantno explicit invocation needed.\n\n## The Recipe Book Analogy\n\nImagine you're a brilliant chef who understands all cooking techniques. But someone asks you to make \"Grandma's Secret Biryani.\"\n\n### Without the recipe (No Skill):\n\n- You might make a biryani\n- It won't taste like Grandma's\n- You'll guess the spices\n- You might make mistakes\n\n### With the recipe (With Skill):\n\n- You follow exact steps\n- You use the right spices in the right amounts\n- The result is consistent every time\n- It tastes exactly like Grandma's!\n\nA Skill is like giving Claude Code the perfect recipe for a specific task.\n\n## How Skills Differ from Subagents\n\n| Aspect | Subagents | Skills |\n|--------|-----------|--------|\n| **Context** | Separate (isolated conversation) | Shared (main conversation) |\n| **Invocation** | Hard (\"Use X subagent\" guaranteed) | Soft (Claude decides when relevant) |\n| **Best For** | Complex, isolated tasks | Lightweight, repeatable capabilities |\n| **File Location** | `.claude/agents/name.md` | `.claude/skills/name/SKILL.md` |\n\n**Key Difference**: Subagents run in separate context windows with guaranteed invocation. Skills run in the main conversation and activate automatically when Claude detects relevance.\n\n**Use skills when**: Task is simple, repeatable, and doesn't need context isolation (blog planning, PDF extraction, note organizing)\n\n**Use subagents when**: Task is complex, needs guaranteed execution, or requires separate context (multi-step refactoring, comprehensive audits)\n\n### AI Colearning Prompt\n\n> *\"Explain the tradeoff between skills (automatic activation) and subagents (guaranteed invocation with isolated context). When would you choose one over the other?\"*\n\n## How Skills Work: The Three-Level Architecture\n\nHere's something clever about Skills: Claude doesn't read all Skills at once.\n\n**Why?** Because Claude has a limited memory (called \"context window\"). If it loaded every skill at the start, it would run out of space!\n\n### The Phone Apps Analogy\n\nThink of your smartphone:\n\n- You have 100 apps installed\n- Your phone doesn't run all 100 at once (it would crash!)\n- Apps stay closed until you tap on them\n- When you need one, it opens and loads\n\nSkills work the same way:\n\n- Claude has access to many skills\n- They stay \"closed\" until needed\n- When Claude needs one, it \"opens\" and reads the instructions\n\n### The Three Levels\n\n**Level 1: Brief Summary (Always Loaded)**\nWhen Claude Code starts, it sees short descriptions of all available skills. This helps Claude know WHEN to use each skill without loading full instructions.\n\n**Level 2: Full Instructions (On-Demand)**\nWhen Claude decides a skill is relevant, it loads the complete SKILL.md file with detailed instructions, workflows, and examples.\n\n**Level 3: Supporting Files (If Needed)**\nSkills can bundle scripts, reference docs, or tools in their directory. Claude accesses these when executing the skill.\n\n#### Example Structure:\n\n```\n.claude/skills/pdf-skill/\n SKILL.md                 # Main instructions\n scripts/\n    pdf_extractor.py     # Python extraction tool\n reference/\n     pdf-standards.md     # Technical specs\n```\n\n> **For your first skill**: Focus on Level 2 (SKILL.md with clear instructions). Add Level 3 (supporting files) only if your skill needs external tools or reference material.\n\n## What's Inside a Skill?\n\n### Skills are Just Folders!\n\nThis is the beautiful part  Skills are incredibly simple. They're just folders with files inside.\n\n```\n my-skill/\n\n  SKILL.md           REQUIRED: The main instructions\n\n  scripts/           OPTIONAL: Helper code\n    helper.py\n\n  references/        OPTIONAL: Documentation\n    guide.md\n\n  assets/            OPTIONAL: Templates, images\n     template.pptx\n```\n\nThe most important file is **SKILL.md**  it contains the instructions Claude reads.\n\n### The IKEA Furniture Analogy\n\nThink of buying IKEA furniture:\n\n| IKEA Furniture | Claude Code Skill |\n|----------------|-------------------|\n| The box | The skill folder |\n| Instruction manual | SKILL.md file |\n| Included tools | Helper scripts |\n| Picture of final result | Templates/examples |\n\nWhen you open the box, you read the manual and follow the steps. Same with Skills  Claude reads SKILL.md and follows those instructions!\n\n### Anatomy of SKILL.md\n\nEvery SKILL.md has two parts:\n\n#### Part 1: Frontmatter (The ID Card)\n\n```yaml\n---\nname: my-skill\ndescription: What this skill does and WHEN to use it\nversion: \"1.0.0\"\n---\n```\n\n#### Part 2: Body (The Instructions)\n\n```markdown\n# My Skill Name\n\n## Overview\nWhat this skill helps with...\n\n## How to Use\nStep-by-step instructions...\n```\n\n### When Claude Code Invokes Skills Automatically\n\nThree patterns trigger skill activation:\n\n1. **Content Type Recognition**: Upload PDF  pdf-skill activates\n2. **Task Request Recognition**: \"Write a blog post\"  blog-writer-skill activates\n3. **Explicit Request**: \"Use the blog-writer skill\"  Direct activation\n\n> **To see available skills**: Ask Claude \"What skills do you have?\" in any session. Skills are discovered through conversation, not system commands.\n\n### Expert Insight\n\n> *In AI-native development, skills encode reasoning patterns, not just commands. You're not teaching Claude \"run this script\"you're teaching \"when you see THIS context, apply THAT framework.\" This makes organizational intelligence transferable across projects and teams.*\n\n## Types of Skills\n\n### 1. Foundational Skills\n\n**What**: Basic capabilities that make Claude useful for common tasks\n\n**Examples**:\n- Creating Word documents (.docx)\n- Making PowerPoint presentations (.pptx)\n- Working with Excel spreadsheets (.xlsx)\n- Creating PDFs\n\n**Analogy**: These are like basic life skills  cooking, cleaning, driving. Everyone needs them!\n\n### 2. Third-Party Skills\n\n**What**: Skills that help Claude work with specific software or services\n\n**Examples**:\n- Navigating websites with browser tools\n- Understanding a Notion workspace\n- Working with specific APIs\n\n**Analogy**: These are like specialized certifications  learning to use Photoshop, getting a forklift license, or becoming certified in a specific software.\n\n### 3. Enterprise/Custom Skills\n\n**What**: Skills created by organizations for their specific needs\n\n**Examples**:\n- Your company's coding style guide\n- Internal documentation standards\n- Organization-specific workflows\n\n**Analogy**: These are like company training manuals  every company has their own way of doing things!\n\n## Skill Creation Rules\n\nBefore creating skills, remember these guidelines:\n\n### DO\n\n-  Be concise - Claude is smart, don't over-explain\n-  Use clear examples - Show, don't just tell\n-  Write a good description - This triggers the skill\n-  Focus on what Claude doesn't know - Your specific preferences\n\n### DON'T\n\n-  Create README files (unnecessary)\n-  Include installation guides (Claude doesn't need them)\n-  Write verbose explanations (wastes context space)\n-  Add irrelevant information (keep it focused)\n\n## Hands-On: Create Your First Custom Skill\n\nLet's create a blog planning skill.\n\n### Step 1: Create Directory Structure\n\n```bash\nmkdir -p .claude/skills/blog-planner\n```\n\n### Step 2: Create SKILL.md File\n\nCreate `.claude/skills/blog-planner/SKILL.md`:\n\n```markdown\n---\nname: \"blog-planner\"\ndescription: \"Help plan engaging blog posts: research topics, create outlines, suggest headlines, and draft compelling introductions. Use when user asks to plan or write blog content.\"\nversion: \"1.0.0\"\n---\n\n# Blog Planning Skill\n\n## When to Use This Skill\n\n- User asks to \"plan a blog post\" or \"write an article\"\n- User mentions blog topics, headlines, or content strategy\n- User needs help structuring written content\n\n## How This Skill Works\n\n1. **Research the topic**: Understand the subject and target audience\n2. **Create outline**: Break topic into 3-5 main sections\n3. **Suggest headlines**: Provide 5 compelling headline options\n4. **Draft introduction**: Write an engaging first paragraph that hooks readers\n\n## Output Format\n\nProvide:\n- **Topic Summary**: 2-3 sentence overview\n- **Target Audience**: Who should read this?\n- **Outline**: Numbered list of main sections\n- **Headline Options**: 5 variations (descriptive, curiosity-driven, benefit-focused)\n- **Introduction Draft**: 1-2 paragraph hook\n\n## Example\n\n**Input**: \"Help me plan a blog post about sustainable living\"\n\n**Output**:\n- **Topic Summary**: Practical sustainable living tips for busy professionals\n- **Target Audience**: Working adults wanting eco-friendly lifestyle changes\n- **Outline**:\n  1. Why sustainable living matters now\n  2. 5 easy swaps you can make today\n  3. Long-term sustainable habits\n  4. Common myths debunked\n  5. Resources for deeper learning\n- **Headlines**:\n  1. \"5 Sustainable Living Changes You Can Make This Weekend\"\n  2. \"Busy Professional's Guide to Eco-Friendly Living\"\n  3. \"Sustainable Living: Easier Than You Think\"\n- **Introduction**: \"You care about the environment, but who has time for complicated lifestyle changes? Good news: sustainable living doesn't require upending your entire routine. These five simple swaps take less than an hour to implementand they'll cut your environmental impact by 30%.\"\n```\n\n### Step 3: Test Your Skill\n\nStart Claude Code (`claude`), then ask:\n\n```\nHelp me plan a blog post about learning AI tools\n```\n\nClaude recognizes \"blog post\" trigger, loads the skill, and applies its workflow automatically.\n\n### Practice Exercise\n\n> *Ask your AI: \"I just created a blog-planner skill. Help me test it: suggest 3 blog topics related to [your interest area]. Then explain which parts of the skill activated and how Claude knew to use it.\"*\n\n**Expected Outcome**: You'll understand how skill descriptions trigger activation and see the skill in action with your own content.\n\n### Step 4: Refine Your Skill Through Co-Learning\n\nAsk Claude to review your skill:\n\n```\nReview the blog-planner skill. What could be improved?\nSuggest 2-3 specific enhancements.\n```\n\n**AI as Teacher**: Claude suggests improvements (better descriptions, additional sections)\n**You as Teacher**: You specify your constraints (\"headlines must be curiosity-driven, not clickbait\")\n**Convergence**: Together you refine the skill to match YOUR workflow\n\nTest the updated skill to validate improvements.\n\n## Bonus Example: Study Notes Creator Skill\n\nHere's another practical skill example for students:\n\n**Use Case**: You always want notes in a specific format  summary at top, key terms highlighted, questions at the end.\n\nCreate `.claude/skills/study-notes-creator/SKILL.md`:\n\n```markdown\n---\nname: \"study-notes-creator\"\ndescription: \"Creates well-organized study notes from any content. Use when the user asks for study notes, learning summaries, revision materials, or wants to understand a topic for studying.\"\nversion: \"1.0.0\"\n---\n\n# Study Notes Creator\n\n## Overview\nThis skill creates structured study notes that help students learn and retain information effectively.\n\n## Output Format\nEvery study note should follow this structure:\n\n### 1. Summary Section\n- Start with a 2-3 sentence overview\n- Highlight the main concept in bold\n\n### 2. Key Terms\n- List 5-10 important vocabulary words\n- Format: **Term** - Definition\n\n### 3. Main Content\n- Use bullet points for easy scanning\n- Include examples for complex ideas\n- Keep paragraphs short (3-4 sentences max)\n\n### 4. Practice Questions\n- Include 3-5 questions at the end\n- Mix of recall and understanding questions\n\n### 5. Quick Review Box\n- End with a \"Quick Review\" box\n- 3-5 bullet points summarizing key takeaways\n\n## Example Output\n\n**Topic: Photosynthesis**\n\n**Summary**\nPhotosynthesis is the process plants use to convert sunlight into food. It's essential for life on Earth.\n\n**Key Terms**\n- **Chlorophyll** - Green pigment that absorbs light\n- **Glucose** - Sugar produced by photosynthesis\n- **Carbon dioxide** - Gas absorbed from air\n\n**Main Content**\n[Content here...]\n\n**Practice Questions**\n1. What is the main purpose of photosynthesis?\n2. Where does photosynthesis occur in a plant?\n\n**Quick Review**\n- Plants make their own food\n- Requires sunlight, water, CO2\n- Produces glucose and oxygen\n```\n\n## More Skill Ideas for Practice\n\nApply the same pattern to create these skills:\n\n### Meeting Notes Organizer:\n\n```yaml\n---\nname: \"meeting-notes-organizer\"\ndescription: \"Transform messy meeting notes into structured summaries with action items, decisions, and follow-ups. Use when user shares meeting notes or transcripts.\"\nversion: \"1.0.0\"\n---\n```\n\n### Learning Path Designer:\n\n```yaml\n---\nname: \"learning-path-designer\"\ndescription: \"Create structured learning plans for any topic with progressive difficulty, resource recommendations, and practice exercises. Use when user wants to learn a new subject.\"\nversion: \"1.0.0\"\n---\n```\n\n### Code Review Skill:\n\n```yaml\n---\nname: \"code-reviewer\"\ndescription: \"Perform systematic code reviews checking security, performance, maintainability, and best practices. Use when user asks to review code.\"\nversion: \"1.0.0\"\n---\n```\n\nTry creating one of these skills using the blog-planner template as your guide.\n\n## Why This Matters: Reusable Organizational Capability\n\n### The Growing Garden Analogy\n\nThink of Claude Code as a garden:\n\n| Garden Element | Claude Code Equivalent |\n|----------------|------------------------|\n| The soil | Claude's base intelligence |\n| Seeds | Skills |\n| Water & sunlight | Your instructions |\n| Flowers/vegetables | The output/results |\n\n**More skills = A richer, more capable garden!**\n\n### Visual Summary\n\n```\n\n              CLAUDE CODE                    \n         (The Brilliant Brain)               \n                   +                         \n          \n     Skill 1   Skill 2   Skill 3     \n      docx      pptx       pdf       \n          \n                   =                         \n         POWERFUL AI ASSISTANT               \n    (Smart + Specialized Knowledge)          \n\n```\n\n**Workflow Impact**: Skills transform one-time solutions into persistent organizational intelligence. Solve a problem once (code review pattern, documentation style, testing strategy), encode it as a skill, and your entire team benefits automatically.\n\n**Paradigm Connection**: This is intelligence accumulation in action. Unlike code libraries (reuse implementation), skills reuse reasoning patterns. The \"how to think about X\" becomes transferable across projects.\n\n**Real-World Application**: Production teams create skills for domain-specific code reviews (security, performance), architectural pattern enforcement (API design, error handling), and documentation standards.\n\n**Link to Capstone**: In Lesson 9, you'll see how plugins bundle skillsturning your custom reasoning patterns into shareable marketplace capabilities.\n\n## Example: How a Skill Works in Practice\n\n**Scenario**: You need to extract invoice data from a PDF.\n\n### What You Do:\n\n```\nExtract these fields from invoice.pdf:\n- Invoice number\n- Date\n- Total amount\n- Vendor name\n```\n\n### What Happens (Behind the Scenes):\n\n1. Claude reads system prompt: \"pdf-skill available for PDF extraction\"\n2. Recognizes PDF context + extraction request\n3. Loads full SKILL.md with extraction instructions\n4. Executes the skill's workflow automatically\n5. Returns structured data\n\n### What You See:\n\n```\nExtracted Invoice Details:\n- Invoice #: INV-2024-00531\n- Date: November 13, 2024\n- Total: $2,450.00\n- Vendor: Tech Solutions Inc.\n```\n\nYou described what you wanted. Claude discovered the right skill and applied it automaticallyno explicit commands needed.\n\n## When to Use Skills vs Subagents vs Main Conversation\n\n### Use Skills When:\n\n- Task is predictable and repeatable (PDF extraction, blog planning, note organizing)\n- You want automatic application without explicit invocation\n- Multiple similar tasks in a session\n\n### Use Subagents When:\n\n- Task is complex with many context-sensitive variables\n- You need guaranteed invocation and isolated context\n- Task requires specialized, multi-step workflows\n\n### Use Main Conversation When:\n\n- One-off, exploratory work\n- Learning something new\n- No specialized capability exists yet\n\n## Key Takeaways\n\nRemember these 5 points:\n\n1. **Skills are folders**  Simple collections of files with instructions\n2. **Skills add expertise**  They give Claude specialized knowledge it doesn't have by default\n3. **Skills are loaded when needed**  Not everything loads at once (saves memory!)\n4. **Anyone can create Skills**  Even you, once you learn!\n5. **Skills make Claude better**  The more skills, the more capable the agent\n\n> **Summary**: Skills are instruction folders that transform Claude from a brilliant generalist into a specialized expert for specific tasks.\n\n### \"Claude is smart. Skills make it wise.\"\n\n## Reflection: From Commands to Intentions\n\nThink about this paradigm shift:\n\n### Traditional Development:\n\n- You know a command exists\n- You type it explicitly: `pdf_extract --input file.pdf --output json`\n- You manage tool invocation manually\n\n### AI-Native Development with Skills:\n\n- You describe what you want: \"Extract invoice data from this PDF\"\n- Claude discovers the right skill automatically\n- The system handles tool invocation\n\nThis shiftfrom \"command what exists\" to \"describe what you want\"is fundamental to AI-native development.\n\n> **Key Insight**: Skills don't just automate tasks. They encode reasoning patterns that make AI assistants smarter by default. When you create a skill, you're teaching Claude \"how to think about\" a domain, not just \"what commands to run.\"\n\n## Try With AI\n\nLet's design reusable skills that extend Claude Code's capabilities for your specific workflow.\n\n### Practice Designing a Custom Skill:\n\n> *\"I have a repeated task in my workflow: [describe your task: code reviews / blog drafting / meeting notes / API documentation]. Help me design an agent skill for it. Walk me through: What should the skill name and description be? When should Claude recognize it's relevant and auto-load it? What workflow should the skill follow? Create the complete SKILL.md file structure with YAML frontmatter and instructions.\"*\n\n### Understand Skills vs Subagents:\n\n> *\"Compare these two tasks: Task A) Draft weekly blog posts (happens 3x/week). Task B) Create comprehensive product launch strategy (one-time, complex). For each task, should I build a Skill or a Subagent? Explain the tradeoff between automatic activation (skills discover themselves) vs. guaranteed control (subagents invoked explicitly).\"*\n\n### Test Skill Discovery:\n\n> *\"I just created a [your skill name] skill and saved it to .claude/skills/. Help me test if Claude Code auto-loads it correctly. Give me a prompt that should trigger this skill's activation. Then explain how I can verify the skill is being used vs. Claude responding without the skill.\"*\n\n### Build a Skill Suite:\n\n> *\"I work on [describe your project type: web apps / data pipelines / documentation / etc.]. Help me identify 3-5 reusable skills I should create that would improve my workflow. For each skill, explain: what it does, when it activates, and why it's better as a skill than as a subagent or direct prompting.\"*\n"
      },
      "plugins": [
        {
          "name": "github-assistant",
          "description": "Interactive troubleshooting for GitHub problems - authentication, sync issues, merge conflicts, and sensitive data exposure",
          "version": "1.0.0",
          "author": {
            "name": "Ali Jilani"
          },
          "source": "./skills/github-assistant",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add alijilani-dev/Claude",
            "/plugin install github-assistant@alijilani-claude-skills"
          ]
        },
        {
          "name": "docker-rocker",
          "description": "Docker containerization expert for FastAPI projects - Dockerfiles, docker-compose, multi-stage builds, and security best practices",
          "version": "1.0.0",
          "author": {
            "name": "Ali Jilani"
          },
          "source": "./skills/docker-rocker",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add alijilani-dev/Claude",
            "/plugin install docker-rocker@alijilani-claude-skills"
          ]
        },
        {
          "name": "fastapi-helper",
          "description": "FastAPI development assistant with routing, dependencies, Pydantic models, middleware, and testing patterns",
          "version": "1.0.0",
          "author": {
            "name": "Ali Jilani"
          },
          "source": "./skills/fastapi-helper",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add alijilani-dev/Claude",
            "/plugin install fastapi-helper@alijilani-claude-skills"
          ]
        },
        {
          "name": "fastapi-pytest-tddhelper",
          "description": "Test-Driven Development workflow helper for FastAPI with pytest - fixtures, conftest patterns, and testing best practices",
          "version": "1.0.0",
          "author": {
            "name": "Ali Jilani"
          },
          "source": "./skills/fastapi-pytest-tddhelper",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add alijilani-dev/Claude",
            "/plugin install fastapi-pytest-tddhelper@alijilani-claude-skills"
          ]
        },
        {
          "name": "sqlmodel-orm-dbhelper",
          "description": "SQLModel ORM database helper for Python - models, relationships, migrations, and query patterns",
          "version": "1.0.0",
          "author": {
            "name": "Ali Jilani"
          },
          "source": "./skills/sqlmodel-orm-dbhelper",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add alijilani-dev/Claude",
            "/plugin install sqlmodel-orm-dbhelper@alijilani-claude-skills"
          ]
        },
        {
          "name": "cover-letter-generator",
          "description": "Professional cover letter generator tailored to job descriptions and your experience",
          "version": "1.0.0",
          "author": {
            "name": "Ali Jilani"
          },
          "source": "./skills/cover_letter_generator",
          "category": "productivity",
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add alijilani-dev/Claude",
            "/plugin install cover-letter-generator@alijilani-claude-skills"
          ]
        },
        {
          "name": "resume-optimizer",
          "description": "Resume optimization tool to match job requirements and highlight relevant skills",
          "version": "1.0.0",
          "author": {
            "name": "Ali Jilani"
          },
          "source": "./skills/resume_optimizer",
          "category": "productivity",
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add alijilani-dev/Claude",
            "/plugin install resume-optimizer@alijilani-claude-skills"
          ]
        },
        {
          "name": "linkedin-post-generator",
          "description": "LinkedIn post generator for professional content creation and engagement",
          "version": "1.0.0",
          "author": {
            "name": "Ali Jilani"
          },
          "source": "./skills/linkedin-post-generator",
          "category": "productivity",
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add alijilani-dev/Claude",
            "/plugin install linkedin-post-generator@alijilani-claude-skills"
          ]
        },
        {
          "name": "study-notes",
          "description": "Study notes generator for learning and revision",
          "version": "1.0.0",
          "author": {
            "name": "Ali Jilani"
          },
          "source": "./skills/study_notes",
          "category": "learning",
          "categories": [
            "learning"
          ],
          "install_commands": [
            "/plugin marketplace add alijilani-dev/Claude",
            "/plugin install study-notes@alijilani-claude-skills"
          ]
        },
        {
          "name": "study-system",
          "description": "Complete study system for organized learning and knowledge management",
          "version": "1.0.0",
          "author": {
            "name": "Ali Jilani"
          },
          "source": "./skills/study_system",
          "category": "learning",
          "categories": [
            "learning"
          ],
          "install_commands": [
            "/plugin marketplace add alijilani-dev/Claude",
            "/plugin install study-system@alijilani-claude-skills"
          ]
        },
        {
          "name": "pytest-python",
          "description": "Python testing helper with pytest patterns, fixtures, and best practices",
          "version": "1.0.0",
          "author": {
            "name": "Ali Jilani"
          },
          "source": "./skills/pytest-python",
          "category": "development",
          "categories": [
            "development"
          ],
          "install_commands": [
            "/plugin marketplace add alijilani-dev/Claude",
            "/plugin install pytest-python@alijilani-claude-skills"
          ]
        },
        {
          "name": "recipes",
          "description": "Recipe and cooking assistant for meal planning and instructions",
          "version": "1.0.0",
          "author": {
            "name": "Ali Jilani"
          },
          "source": "./skills/recipes",
          "category": "productivity",
          "categories": [
            "productivity"
          ],
          "install_commands": [
            "/plugin marketplace add alijilani-dev/Claude",
            "/plugin install recipes@alijilani-claude-skills"
          ]
        }
      ]
    }
  ]
}