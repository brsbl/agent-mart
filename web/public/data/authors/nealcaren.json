{
  "author": {
    "id": "nealcaren",
    "display_name": "Neal Caren",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/1250553?u=b94c842d86904ca334322276b49d15571c6d3113&v=4",
    "url": "https://github.com/nealcaren",
    "bio": "Sociologist",
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 16,
      "total_commands": 0,
      "total_skills": 16,
      "total_stars": 6,
      "total_forks": 1
    }
  },
  "marketplaces": [
    {
      "name": "social-data-analysis",
      "version": null,
      "description": "Claude Code skills for rigorous social science research and teaching: quantitative analysis, qualitative analysis, text analysis, and lecture design.",
      "owner_info": {
        "name": "Neal Caren",
        "email": "neal.caren@unc.edu"
      },
      "keywords": [],
      "repo_full_name": "nealcaren/social-data-analysis",
      "repo_url": "https://github.com/nealcaren/social-data-analysis",
      "repo_description": "Claude Code plugins for quantitative and qualitative sociological research",
      "homepage": null,
      "signals": {
        "stars": 6,
        "forks": 1,
        "pushed_at": "2026-01-29T20:04:31Z",
        "created_at": "2026-01-17T20:08:36Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 9259
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/abductive-analyst",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/abductive-analyst/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/abductive-analyst/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 461
        },
        {
          "path": "plugins/abductive-analyst/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/abductive-analyst/skills/abductive-analyst",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/abductive-analyst/skills/abductive-analyst/SKILL.md",
          "type": "blob",
          "size": 11528
        },
        {
          "path": "plugins/abductive-analyst/skills/abductive-analyst/phases",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/abductive-analyst/skills/abductive-analyst/phases/phase0-theoretical-preparation.md",
          "type": "blob",
          "size": 6484
        },
        {
          "path": "plugins/abductive-analyst/skills/abductive-analyst/phases/phase1-familiarization.md",
          "type": "blob",
          "size": 4890
        },
        {
          "path": "plugins/abductive-analyst/skills/abductive-analyst/phases/phase2-theoretical-casing.md",
          "type": "blob",
          "size": 7232
        },
        {
          "path": "plugins/abductive-analyst/skills/abductive-analyst/phases/phase3-anomaly-analysis.md",
          "type": "blob",
          "size": 8620
        },
        {
          "path": "plugins/abductive-analyst/skills/abductive-analyst/phases/phase4-memo-theory.md",
          "type": "blob",
          "size": 9333
        },
        {
          "path": "plugins/abductive-analyst/skills/abductive-analyst/phases/phase5-integration.md",
          "type": "blob",
          "size": 10741
        },
        {
          "path": "plugins/abductive-analyst/skills/abductive-analyst/phases/phase6-writeup.md",
          "type": "blob",
          "size": 8649
        },
        {
          "path": "plugins/case-justification",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/case-justification/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/case-justification/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 315
        },
        {
          "path": "plugins/case-justification/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/case-justification/skills/case-justification",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/case-justification/skills/case-justification/SKILL.md",
          "type": "blob",
          "size": 11962
        },
        {
          "path": "plugins/case-justification/skills/case-justification/clusters",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/case-justification/skills/case-justification/clusters/comparative.md",
          "type": "blob",
          "size": 11265
        },
        {
          "path": "plugins/case-justification/skills/case-justification/clusters/historical.md",
          "type": "blob",
          "size": 10617
        },
        {
          "path": "plugins/case-justification/skills/case-justification/clusters/minimal.md",
          "type": "blob",
          "size": 7318
        },
        {
          "path": "plugins/case-justification/skills/case-justification/clusters/policy.md",
          "type": "blob",
          "size": 11925
        },
        {
          "path": "plugins/case-justification/skills/case-justification/clusters/standard.md",
          "type": "blob",
          "size": 9176
        },
        {
          "path": "plugins/case-justification/skills/case-justification/phases",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/case-justification/skills/case-justification/phases/phase0-assessment.md",
          "type": "blob",
          "size": 8221
        },
        {
          "path": "plugins/case-justification/skills/case-justification/phases/phase1-drafting.md",
          "type": "blob",
          "size": 10953
        },
        {
          "path": "plugins/case-justification/skills/case-justification/phases/phase2-revision.md",
          "type": "blob",
          "size": 8591
        },
        {
          "path": "plugins/case-justification/skills/case-justification/techniques",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/case-justification/skills/case-justification/techniques/justification-strategies.md",
          "type": "blob",
          "size": 11088
        },
        {
          "path": "plugins/case-justification/skills/case-justification/techniques/opening-moves.md",
          "type": "blob",
          "size": 8019
        },
        {
          "path": "plugins/case-justification/skills/case-justification/techniques/transitions.md",
          "type": "blob",
          "size": 9131
        },
        {
          "path": "plugins/dag-development",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dag-development/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dag-development/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 421
        },
        {
          "path": "plugins/dag-development/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dag-development/skills/dag-development",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dag-development/skills/dag-development/SKILL.md",
          "type": "blob",
          "size": 3701
        },
        {
          "path": "plugins/dag-development/skills/dag-development/concepts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dag-development/skills/dag-development/concepts/colliders.md",
          "type": "blob",
          "size": 1193
        },
        {
          "path": "plugins/dag-development/skills/dag-development/concepts/confounding.md",
          "type": "blob",
          "size": 880
        },
        {
          "path": "plugins/dag-development/skills/dag-development/concepts/d_separation.md",
          "type": "blob",
          "size": 886
        },
        {
          "path": "plugins/dag-development/skills/dag-development/concepts/potential_outcomes.md",
          "type": "blob",
          "size": 1213
        },
        {
          "path": "plugins/dag-development/skills/dag-development/concepts/selection_bias.md",
          "type": "blob",
          "size": 1027
        },
        {
          "path": "plugins/dag-development/skills/dag-development/concepts/six_step_algorithm.md",
          "type": "blob",
          "size": 1301
        },
        {
          "path": "plugins/dag-development/skills/dag-development/phases",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/dag-development/skills/dag-development/phases/phase0-theory.md",
          "type": "blob",
          "size": 8473
        },
        {
          "path": "plugins/dag-development/skills/dag-development/phases/phase1-identification.md",
          "type": "blob",
          "size": 2435
        },
        {
          "path": "plugins/dag-development/skills/dag-development/phases/phase2-inputs.md",
          "type": "blob",
          "size": 1748
        },
        {
          "path": "plugins/dag-development/skills/dag-development/phases/phase3-mermaid.md",
          "type": "blob",
          "size": 716
        },
        {
          "path": "plugins/dag-development/skills/dag-development/phases/phase4-r.md",
          "type": "blob",
          "size": 694
        },
        {
          "path": "plugins/dag-development/skills/dag-development/phases/phase5-python.md",
          "type": "blob",
          "size": 1131
        },
        {
          "path": "plugins/genre-skill-builder",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/genre-skill-builder/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/genre-skill-builder/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 304
        },
        {
          "path": "plugins/genre-skill-builder/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/genre-skill-builder/skills/genre-skill-builder",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/genre-skill-builder/skills/genre-skill-builder/SKILL.md",
          "type": "blob",
          "size": 10857
        },
        {
          "path": "plugins/genre-skill-builder/skills/genre-skill-builder/phases",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/genre-skill-builder/skills/genre-skill-builder/phases/phase0-scope.md",
          "type": "blob",
          "size": 6200
        },
        {
          "path": "plugins/genre-skill-builder/skills/genre-skill-builder/phases/phase1-immersion.md",
          "type": "blob",
          "size": 6286
        },
        {
          "path": "plugins/genre-skill-builder/skills/genre-skill-builder/phases/phase2-coding.md",
          "type": "blob",
          "size": 9249
        },
        {
          "path": "plugins/genre-skill-builder/skills/genre-skill-builder/phases/phase3-interpretation.md",
          "type": "blob",
          "size": 10042
        },
        {
          "path": "plugins/genre-skill-builder/skills/genre-skill-builder/phases/phase4-generation.md",
          "type": "blob",
          "size": 9487
        },
        {
          "path": "plugins/genre-skill-builder/skills/genre-skill-builder/phases/phase5-validation.md",
          "type": "blob",
          "size": 7958
        },
        {
          "path": "plugins/genre-skill-builder/skills/genre-skill-builder/templates",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/genre-skill-builder/skills/genre-skill-builder/templates/cluster-template.md",
          "type": "blob",
          "size": 5095
        },
        {
          "path": "plugins/genre-skill-builder/skills/genre-skill-builder/templates/phase-template.md",
          "type": "blob",
          "size": 3704
        },
        {
          "path": "plugins/genre-skill-builder/skills/genre-skill-builder/templates/skill-template.md",
          "type": "blob",
          "size": 6191
        },
        {
          "path": "plugins/genre-skill-builder/skills/genre-skill-builder/templates/technique-template.md",
          "type": "blob",
          "size": 4582
        },
        {
          "path": "plugins/interview-analyst",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/interview-analyst/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/interview-analyst/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 447
        },
        {
          "path": "plugins/interview-analyst/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/interview-analyst/skills/interview-analyst",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/interview-analyst/skills/interview-analyst/SKILL.md",
          "type": "blob",
          "size": 11695
        },
        {
          "path": "plugins/interview-analyst/skills/interview-analyst/phases",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/interview-analyst/skills/interview-analyst/phases/phase0-theory.md",
          "type": "blob",
          "size": 6657
        },
        {
          "path": "plugins/interview-analyst/skills/interview-analyst/phases/phase1-immersion.md",
          "type": "blob",
          "size": 8266
        },
        {
          "path": "plugins/interview-analyst/skills/interview-analyst/phases/phase2-coding.md",
          "type": "blob",
          "size": 10728
        },
        {
          "path": "plugins/interview-analyst/skills/interview-analyst/phases/phase3-interpretation.md",
          "type": "blob",
          "size": 9607
        },
        {
          "path": "plugins/interview-analyst/skills/interview-analyst/phases/phase4-quality.md",
          "type": "blob",
          "size": 10432
        },
        {
          "path": "plugins/interview-analyst/skills/interview-analyst/phases/phase5-synthesis.md",
          "type": "blob",
          "size": 15494
        },
        {
          "path": "plugins/interview-bookends",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/interview-bookends/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/interview-bookends/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 313
        },
        {
          "path": "plugins/interview-bookends/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/interview-bookends/skills/interview-bookends",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/interview-bookends/skills/interview-bookends/SKILL.md",
          "type": "blob",
          "size": 8478
        },
        {
          "path": "plugins/interview-bookends/skills/interview-bookends/clusters",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/interview-bookends/skills/interview-bookends/clusters/concept-building.md",
          "type": "blob",
          "size": 6245
        },
        {
          "path": "plugins/interview-bookends/skills/interview-bookends/clusters/gap-filler.md",
          "type": "blob",
          "size": 4986
        },
        {
          "path": "plugins/interview-bookends/skills/interview-bookends/clusters/problem-driven.md",
          "type": "blob",
          "size": 6202
        },
        {
          "path": "plugins/interview-bookends/skills/interview-bookends/clusters/synthesis.md",
          "type": "blob",
          "size": 6037
        },
        {
          "path": "plugins/interview-bookends/skills/interview-bookends/clusters/theory-extension.md",
          "type": "blob",
          "size": 6232
        },
        {
          "path": "plugins/interview-bookends/skills/interview-bookends/phases",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/interview-bookends/skills/interview-bookends/phases/phase0-intake.md",
          "type": "blob",
          "size": 4618
        },
        {
          "path": "plugins/interview-bookends/skills/interview-bookends/phases/phase1-introduction.md",
          "type": "blob",
          "size": 7451
        },
        {
          "path": "plugins/interview-bookends/skills/interview-bookends/phases/phase2-conclusion.md",
          "type": "blob",
          "size": 8708
        },
        {
          "path": "plugins/interview-bookends/skills/interview-bookends/phases/phase3-coherence.md",
          "type": "blob",
          "size": 7331
        },
        {
          "path": "plugins/interview-bookends/skills/interview-bookends/techniques",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/interview-bookends/skills/interview-bookends/techniques/opening-moves.md",
          "type": "blob",
          "size": 8117
        },
        {
          "path": "plugins/interview-bookends/skills/interview-bookends/techniques/signature-phrases.md",
          "type": "blob",
          "size": 7057
        },
        {
          "path": "plugins/interview-writeup",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/interview-writeup/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/interview-writeup/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 412
        },
        {
          "path": "plugins/interview-writeup/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/interview-writeup/skills/interview-writeup",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/interview-writeup/skills/interview-writeup/SKILL.md",
          "type": "blob",
          "size": 6164
        },
        {
          "path": "plugins/interview-writeup/skills/interview-writeup/phases",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/interview-writeup/skills/interview-writeup/phases/phase0-intake.md",
          "type": "blob",
          "size": 4991
        },
        {
          "path": "plugins/interview-writeup/skills/interview-writeup/phases/phase1-methods.md",
          "type": "blob",
          "size": 2663
        },
        {
          "path": "plugins/interview-writeup/skills/interview-writeup/phases/phase2-findings.md",
          "type": "blob",
          "size": 8870
        },
        {
          "path": "plugins/interview-writeup/skills/interview-writeup/phases/phase3-revision.md",
          "type": "blob",
          "size": 7637
        },
        {
          "path": "plugins/interview-writeup/skills/interview-writeup/techniques",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/interview-writeup/skills/interview-writeup/techniques/macro-structure.md",
          "type": "blob",
          "size": 7135
        },
        {
          "path": "plugins/interview-writeup/skills/interview-writeup/techniques/participant-management.md",
          "type": "blob",
          "size": 4845
        },
        {
          "path": "plugins/interview-writeup/skills/interview-writeup/techniques/prose-craft.md",
          "type": "blob",
          "size": 8988
        },
        {
          "path": "plugins/interview-writeup/skills/interview-writeup/techniques/rubric.md",
          "type": "blob",
          "size": 6399
        },
        {
          "path": "plugins/lecture-designer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lecture-designer/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lecture-designer/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 447
        },
        {
          "path": "plugins/lecture-designer/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lecture-designer/skills/lecture-designer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lecture-designer/skills/lecture-designer/SKILL.md",
          "type": "blob",
          "size": 13064
        },
        {
          "path": "plugins/lecture-designer/skills/lecture-designer/mcp",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lecture-designer/skills/lecture-designer/mcp/google-docs-mcp-setup.md",
          "type": "blob",
          "size": 6679
        },
        {
          "path": "plugins/lecture-designer/skills/lecture-designer/pedagogy",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lecture-designer/skills/lecture-designer/pedagogy/overview.md",
          "type": "blob",
          "size": 45912
        },
        {
          "path": "plugins/lecture-designer/skills/lecture-designer/pedagogy/slide-design-guide.md",
          "type": "blob",
          "size": 12289
        },
        {
          "path": "plugins/lecture-designer/skills/lecture-designer/pedagogy/teaching-techniques.md",
          "type": "blob",
          "size": 15627
        },
        {
          "path": "plugins/lecture-designer/skills/lecture-designer/phases",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lecture-designer/skills/lecture-designer/phases/phase0-context.md",
          "type": "blob",
          "size": 4730
        },
        {
          "path": "plugins/lecture-designer/skills/lecture-designer/phases/phase1-narrative.md",
          "type": "blob",
          "size": 6387
        },
        {
          "path": "plugins/lecture-designer/skills/lecture-designer/phases/phase2-activities.md",
          "type": "blob",
          "size": 6429
        },
        {
          "path": "plugins/lecture-designer/skills/lecture-designer/phases/phase3-slides.md",
          "type": "blob",
          "size": 11410
        },
        {
          "path": "plugins/lecture-designer/skills/lecture-designer/phases/phase4-review.md",
          "type": "blob",
          "size": 5822
        },
        {
          "path": "plugins/lecture-designer/skills/lecture-designer/quarto",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lecture-designer/skills/lecture-designer/quarto/_columns-reveal.md",
          "type": "blob",
          "size": 369
        },
        {
          "path": "plugins/lecture-designer/skills/lecture-designer/quarto/_creating-slides-reveal.md",
          "type": "blob",
          "size": 2101
        },
        {
          "path": "plugins/lecture-designer/skills/lecture-designer/quarto/_incremental-lists-reveal.md",
          "type": "blob",
          "size": 791
        },
        {
          "path": "plugins/lecture-designer/skills/lecture-designer/quarto/_incremental-pause-reveal.md",
          "type": "blob",
          "size": 383
        },
        {
          "path": "plugins/lecture-designer/skills/lecture-designer/quarto/_speaker-notes.md",
          "type": "blob",
          "size": 206
        },
        {
          "path": "plugins/lecture-designer/skills/lecture-designer/quarto/index.md",
          "type": "blob",
          "size": 1497
        },
        {
          "path": "plugins/lit-search",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lit-search/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lit-search/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 450
        },
        {
          "path": "plugins/lit-search/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lit-search/skills/lit-search",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lit-search/skills/lit-search/SKILL.md",
          "type": "blob",
          "size": 8685
        },
        {
          "path": "plugins/lit-search/skills/lit-search/api",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lit-search/skills/lit-search/api/openalex-reference.md",
          "type": "blob",
          "size": 10310
        },
        {
          "path": "plugins/lit-search/skills/lit-search/phases",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lit-search/skills/lit-search/phases/phase0-scope.md",
          "type": "blob",
          "size": 4334
        },
        {
          "path": "plugins/lit-search/skills/lit-search/phases/phase1-search.md",
          "type": "blob",
          "size": 6209
        },
        {
          "path": "plugins/lit-search/skills/lit-search/phases/phase2-screening.md",
          "type": "blob",
          "size": 5963
        },
        {
          "path": "plugins/lit-search/skills/lit-search/phases/phase3-snowball.md",
          "type": "blob",
          "size": 7097
        },
        {
          "path": "plugins/lit-search/skills/lit-search/phases/phase4-fulltext.md",
          "type": "blob",
          "size": 7734
        },
        {
          "path": "plugins/lit-search/skills/lit-search/phases/phase5-annotation.md",
          "type": "blob",
          "size": 8704
        },
        {
          "path": "plugins/lit-search/skills/lit-search/phases/phase6-synthesis.md",
          "type": "blob",
          "size": 10015
        },
        {
          "path": "plugins/lit-synthesis",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lit-synthesis/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lit-synthesis/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 426
        },
        {
          "path": "plugins/lit-synthesis/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lit-synthesis/skills/lit-synthesis",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lit-synthesis/skills/lit-synthesis/SKILL.md",
          "type": "blob",
          "size": 9228
        },
        {
          "path": "plugins/lit-synthesis/skills/lit-synthesis/mcp",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lit-synthesis/skills/lit-synthesis/mcp/zotero-setup.md",
          "type": "blob",
          "size": 4173
        },
        {
          "path": "plugins/lit-synthesis/skills/lit-synthesis/phases",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lit-synthesis/skills/lit-synthesis/phases/phase0-corpus-audit.md",
          "type": "blob",
          "size": 4913
        },
        {
          "path": "plugins/lit-synthesis/skills/lit-synthesis/phases/phase1-deep-reading.md",
          "type": "blob",
          "size": 5651
        },
        {
          "path": "plugins/lit-synthesis/skills/lit-synthesis/phases/phase2-theoretical-mapping.md",
          "type": "blob",
          "size": 6358
        },
        {
          "path": "plugins/lit-synthesis/skills/lit-synthesis/phases/phase3-thematic-clustering.md",
          "type": "blob",
          "size": 6130
        },
        {
          "path": "plugins/lit-synthesis/skills/lit-synthesis/phases/phase4-debate-mapping.md",
          "type": "blob",
          "size": 6190
        },
        {
          "path": "plugins/lit-synthesis/skills/lit-synthesis/phases/phase5-field-synthesis.md",
          "type": "blob",
          "size": 7037
        },
        {
          "path": "plugins/lit-writeup",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lit-writeup/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lit-writeup/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 460
        },
        {
          "path": "plugins/lit-writeup/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lit-writeup/skills/lit-writeup",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lit-writeup/skills/lit-writeup/SKILL.md",
          "type": "blob",
          "size": 10198
        },
        {
          "path": "plugins/lit-writeup/skills/lit-writeup/clusters",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lit-writeup/skills/lit-writeup/clusters/concept-builder.md",
          "type": "blob",
          "size": 6746
        },
        {
          "path": "plugins/lit-writeup/skills/lit-writeup/clusters/gap-filler.md",
          "type": "blob",
          "size": 5260
        },
        {
          "path": "plugins/lit-writeup/skills/lit-writeup/clusters/problem-driven.md",
          "type": "blob",
          "size": 7680
        },
        {
          "path": "plugins/lit-writeup/skills/lit-writeup/clusters/synthesis-integrator.md",
          "type": "blob",
          "size": 6776
        },
        {
          "path": "plugins/lit-writeup/skills/lit-writeup/clusters/theory-extender.md",
          "type": "blob",
          "size": 5976
        },
        {
          "path": "plugins/lit-writeup/skills/lit-writeup/phases",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lit-writeup/skills/lit-writeup/phases/phase0-assessment.md",
          "type": "blob",
          "size": 5606
        },
        {
          "path": "plugins/lit-writeup/skills/lit-writeup/phases/phase1-architecture.md",
          "type": "blob",
          "size": 6704
        },
        {
          "path": "plugins/lit-writeup/skills/lit-writeup/phases/phase2-planning.md",
          "type": "blob",
          "size": 6827
        },
        {
          "path": "plugins/lit-writeup/skills/lit-writeup/phases/phase3-drafting.md",
          "type": "blob",
          "size": 8475
        },
        {
          "path": "plugins/lit-writeup/skills/lit-writeup/phases/phase4-turn.md",
          "type": "blob",
          "size": 7699
        },
        {
          "path": "plugins/lit-writeup/skills/lit-writeup/phases/phase5-revision.md",
          "type": "blob",
          "size": 8237
        },
        {
          "path": "plugins/lit-writeup/skills/lit-writeup/techniques",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/lit-writeup/skills/lit-writeup/techniques/calibration-norms.md",
          "type": "blob",
          "size": 7001
        },
        {
          "path": "plugins/lit-writeup/skills/lit-writeup/techniques/citation-patterns.md",
          "type": "blob",
          "size": 8351
        },
        {
          "path": "plugins/lit-writeup/skills/lit-writeup/techniques/paragraph-functions.md",
          "type": "blob",
          "size": 11097
        },
        {
          "path": "plugins/lit-writeup/skills/lit-writeup/techniques/sentence-toolbox.md",
          "type": "blob",
          "size": 11376
        },
        {
          "path": "plugins/lit-writeup/skills/lit-writeup/techniques/turn-formula.md",
          "type": "blob",
          "size": 8276
        },
        {
          "path": "plugins/methods-writer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/methods-writer/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/methods-writer/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 276
        },
        {
          "path": "plugins/methods-writer/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/methods-writer/skills/methods-writer",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/methods-writer/skills/methods-writer/SKILL.md",
          "type": "blob",
          "size": 9551
        },
        {
          "path": "plugins/methods-writer/skills/methods-writer/pathways",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/methods-writer/skills/methods-writer/pathways/detailed.md",
          "type": "blob",
          "size": 14849
        },
        {
          "path": "plugins/methods-writer/skills/methods-writer/pathways/efficient.md",
          "type": "blob",
          "size": 7405
        },
        {
          "path": "plugins/methods-writer/skills/methods-writer/pathways/standard.md",
          "type": "blob",
          "size": 10036
        },
        {
          "path": "plugins/methods-writer/skills/methods-writer/phases",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/methods-writer/skills/methods-writer/phases/phase0-assessment.md",
          "type": "blob",
          "size": 6321
        },
        {
          "path": "plugins/methods-writer/skills/methods-writer/phases/phase1-drafting.md",
          "type": "blob",
          "size": 10030
        },
        {
          "path": "plugins/methods-writer/skills/methods-writer/phases/phase2-revision.md",
          "type": "blob",
          "size": 8587
        },
        {
          "path": "plugins/methods-writer/skills/methods-writer/techniques",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/methods-writer/skills/methods-writer/techniques/component-checklist.md",
          "type": "blob",
          "size": 12800
        },
        {
          "path": "plugins/methods-writer/skills/methods-writer/techniques/opening-moves.md",
          "type": "blob",
          "size": 8678
        },
        {
          "path": "plugins/r-analyst",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/r-analyst/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/r-analyst/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 414
        },
        {
          "path": "plugins/r-analyst/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/r-analyst/skills/r-analyst",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/r-analyst/skills/r-analyst/SKILL.md",
          "type": "blob",
          "size": 7741
        },
        {
          "path": "plugins/r-analyst/skills/r-analyst/phases",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/r-analyst/skills/r-analyst/phases/phase0-design.md",
          "type": "blob",
          "size": 4296
        },
        {
          "path": "plugins/r-analyst/skills/r-analyst/phases/phase1-data.md",
          "type": "blob",
          "size": 5875
        },
        {
          "path": "plugins/r-analyst/skills/r-analyst/phases/phase2-specification.md",
          "type": "blob",
          "size": 6263
        },
        {
          "path": "plugins/r-analyst/skills/r-analyst/phases/phase3-analysis.md",
          "type": "blob",
          "size": 9250
        },
        {
          "path": "plugins/r-analyst/skills/r-analyst/phases/phase4-robustness.md",
          "type": "blob",
          "size": 11898
        },
        {
          "path": "plugins/r-analyst/skills/r-analyst/phases/phase5-output.md",
          "type": "blob",
          "size": 12713
        },
        {
          "path": "plugins/r-analyst/skills/r-analyst/techniques",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/r-analyst/skills/r-analyst/techniques/01_core_econometrics.md",
          "type": "blob",
          "size": 49558
        },
        {
          "path": "plugins/r-analyst/skills/r-analyst/techniques/02_survey_resampling.md",
          "type": "blob",
          "size": 42993
        },
        {
          "path": "plugins/r-analyst/skills/r-analyst/techniques/03_text_ml.md",
          "type": "blob",
          "size": 70673
        },
        {
          "path": "plugins/r-analyst/skills/r-analyst/techniques/04_synthetic_control.md",
          "type": "blob",
          "size": 36397
        },
        {
          "path": "plugins/r-analyst/skills/r-analyst/techniques/05_bayesian_sensitivity.md",
          "type": "blob",
          "size": 30930
        },
        {
          "path": "plugins/r-analyst/skills/r-analyst/techniques/06_visualization.md",
          "type": "blob",
          "size": 44406
        },
        {
          "path": "plugins/r-analyst/skills/r-analyst/techniques/07_best_practices.md",
          "type": "blob",
          "size": 38425
        },
        {
          "path": "plugins/r-analyst/skills/r-analyst/techniques/08_nonlinear_models.md",
          "type": "blob",
          "size": 26565
        },
        {
          "path": "plugins/revision-coordinator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/revision-coordinator/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/revision-coordinator/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 192
        },
        {
          "path": "plugins/revision-coordinator/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/revision-coordinator/skills/revision-coordinator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/revision-coordinator/skills/revision-coordinator/SKILL.md",
          "type": "blob",
          "size": 13383
        },
        {
          "path": "plugins/revision-coordinator/skills/revision-coordinator/phases",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/revision-coordinator/skills/revision-coordinator/phases/phase0-intake.md",
          "type": "blob",
          "size": 7907
        },
        {
          "path": "plugins/revision-coordinator/skills/revision-coordinator/phases/phase1-diagnostic.md",
          "type": "blob",
          "size": 10510
        },
        {
          "path": "plugins/revision-coordinator/skills/revision-coordinator/phases/phase2-dispatch.md",
          "type": "blob",
          "size": 11618
        },
        {
          "path": "plugins/revision-coordinator/skills/revision-coordinator/phases/phase3-integration.md",
          "type": "blob",
          "size": 7323
        },
        {
          "path": "plugins/revision-coordinator/skills/revision-coordinator/phases/phase4-verification.md",
          "type": "blob",
          "size": 9291
        },
        {
          "path": "plugins/stata-analyst",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/stata-analyst/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/stata-analyst/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 426
        },
        {
          "path": "plugins/stata-analyst/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/stata-analyst/skills/stata-analyst",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/stata-analyst/skills/stata-analyst/SKILL.md",
          "type": "blob",
          "size": 8198
        },
        {
          "path": "plugins/stata-analyst/skills/stata-analyst/phases",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/stata-analyst/skills/stata-analyst/phases/phase0-design.md",
          "type": "blob",
          "size": 4319
        },
        {
          "path": "plugins/stata-analyst/skills/stata-analyst/phases/phase1-data.md",
          "type": "blob",
          "size": 6404
        },
        {
          "path": "plugins/stata-analyst/skills/stata-analyst/phases/phase2-specification.md",
          "type": "blob",
          "size": 6268
        },
        {
          "path": "plugins/stata-analyst/skills/stata-analyst/phases/phase3-analysis.md",
          "type": "blob",
          "size": 9953
        },
        {
          "path": "plugins/stata-analyst/skills/stata-analyst/phases/phase4-robustness.md",
          "type": "blob",
          "size": 11489
        },
        {
          "path": "plugins/stata-analyst/skills/stata-analyst/phases/phase5-output.md",
          "type": "blob",
          "size": 12774
        },
        {
          "path": "plugins/stata-analyst/skills/stata-analyst/techniques",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/stata-analyst/skills/stata-analyst/techniques/00_data_prep.md",
          "type": "blob",
          "size": 8629
        },
        {
          "path": "plugins/stata-analyst/skills/stata-analyst/techniques/00_index.md",
          "type": "blob",
          "size": 6380
        },
        {
          "path": "plugins/stata-analyst/skills/stata-analyst/techniques/01_core_econometrics.md",
          "type": "blob",
          "size": 12164
        },
        {
          "path": "plugins/stata-analyst/skills/stata-analyst/techniques/02_survey_resampling.md",
          "type": "blob",
          "size": 4030
        },
        {
          "path": "plugins/stata-analyst/skills/stata-analyst/techniques/03_synthetic_control.md",
          "type": "blob",
          "size": 4048
        },
        {
          "path": "plugins/stata-analyst/skills/stata-analyst/techniques/04_visualization.md",
          "type": "blob",
          "size": 7701
        },
        {
          "path": "plugins/stata-analyst/skills/stata-analyst/techniques/05_best_practices.md",
          "type": "blob",
          "size": 7744
        },
        {
          "path": "plugins/stata-analyst/skills/stata-analyst/techniques/06_modeling_basics.md",
          "type": "blob",
          "size": 7927
        },
        {
          "path": "plugins/stata-analyst/skills/stata-analyst/techniques/07_postestimation_reporting.md",
          "type": "blob",
          "size": 8746
        },
        {
          "path": "plugins/stata-analyst/skills/stata-analyst/techniques/99_default_journal_pipeline.md",
          "type": "blob",
          "size": 10569
        },
        {
          "path": "plugins/stata-analyst/skills/stata-analyst/techniques/stata-statistical-techniques_combined.md",
          "type": "blob",
          "size": 36222
        },
        {
          "path": "plugins/text-analyst",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/text-analyst/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/text-analyst/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 426
        },
        {
          "path": "plugins/text-analyst/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/text-analyst/skills/text-analyst",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/text-analyst/skills/text-analyst/SKILL.md",
          "type": "blob",
          "size": 9096
        },
        {
          "path": "plugins/text-analyst/skills/text-analyst/concepts",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/text-analyst/skills/text-analyst/concepts/01_dictionary_methods.md",
          "type": "blob",
          "size": 6911
        },
        {
          "path": "plugins/text-analyst/skills/text-analyst/concepts/02_topic_models.md",
          "type": "blob",
          "size": 8893
        },
        {
          "path": "plugins/text-analyst/skills/text-analyst/concepts/03_supervised_classification.md",
          "type": "blob",
          "size": 10190
        },
        {
          "path": "plugins/text-analyst/skills/text-analyst/concepts/04_embeddings.md",
          "type": "blob",
          "size": 9515
        },
        {
          "path": "plugins/text-analyst/skills/text-analyst/concepts/05_sentiment_analysis.md",
          "type": "blob",
          "size": 9669
        },
        {
          "path": "plugins/text-analyst/skills/text-analyst/concepts/06_validation_strategies.md",
          "type": "blob",
          "size": 11096
        },
        {
          "path": "plugins/text-analyst/skills/text-analyst/phases",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/text-analyst/skills/text-analyst/phases/phase0-design.md",
          "type": "blob",
          "size": 6415
        },
        {
          "path": "plugins/text-analyst/skills/text-analyst/phases/phase1-corpus.md",
          "type": "blob",
          "size": 7502
        },
        {
          "path": "plugins/text-analyst/skills/text-analyst/phases/phase2-specification.md",
          "type": "blob",
          "size": 7912
        },
        {
          "path": "plugins/text-analyst/skills/text-analyst/phases/phase3-analysis.md",
          "type": "blob",
          "size": 7000
        },
        {
          "path": "plugins/text-analyst/skills/text-analyst/phases/phase4-validation.md",
          "type": "blob",
          "size": 8246
        },
        {
          "path": "plugins/text-analyst/skills/text-analyst/phases/phase5-output.md",
          "type": "blob",
          "size": 8628
        },
        {
          "path": "plugins/text-analyst/skills/text-analyst/python-techniques",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/text-analyst/skills/text-analyst/python-techniques/01_preprocessing.md",
          "type": "blob",
          "size": 12457
        },
        {
          "path": "plugins/text-analyst/skills/text-analyst/python-techniques/02_dictionary_sentiment.md",
          "type": "blob",
          "size": 14314
        },
        {
          "path": "plugins/text-analyst/skills/text-analyst/python-techniques/03_topic_models.md",
          "type": "blob",
          "size": 13242
        },
        {
          "path": "plugins/text-analyst/skills/text-analyst/python-techniques/04_supervised.md",
          "type": "blob",
          "size": 13768
        },
        {
          "path": "plugins/text-analyst/skills/text-analyst/python-techniques/05_embeddings.md",
          "type": "blob",
          "size": 13422
        },
        {
          "path": "plugins/text-analyst/skills/text-analyst/python-techniques/06_visualization.md",
          "type": "blob",
          "size": 15224
        },
        {
          "path": "plugins/text-analyst/skills/text-analyst/r-techniques",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/text-analyst/skills/text-analyst/r-techniques/01_preprocessing.md",
          "type": "blob",
          "size": 8386
        },
        {
          "path": "plugins/text-analyst/skills/text-analyst/r-techniques/02_dictionary_sentiment.md",
          "type": "blob",
          "size": 10822
        },
        {
          "path": "plugins/text-analyst/skills/text-analyst/r-techniques/03_topic_models.md",
          "type": "blob",
          "size": 10340
        },
        {
          "path": "plugins/text-analyst/skills/text-analyst/r-techniques/04_supervised.md",
          "type": "blob",
          "size": 10618
        },
        {
          "path": "plugins/text-analyst/skills/text-analyst/r-techniques/05_embeddings.md",
          "type": "blob",
          "size": 10800
        },
        {
          "path": "plugins/text-analyst/skills/text-analyst/r-techniques/06_visualization.md",
          "type": "blob",
          "size": 13158
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"social-data-analysis\",\n  \"owner\": {\n    \"name\": \"Neal Caren\",\n    \"email\": \"neal.caren@unc.edu\"\n  },\n  \"metadata\": {\n    \"description\": \"Claude Code skills for rigorous social science research and teaching: quantitative analysis, qualitative analysis, text analysis, and lecture design.\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"r-analyst\",\n      \"source\": \"./plugins/r-analyst\",\n      \"description\": \"R statistical analysis for publication-ready sociology research. Phased workflow for DiD, IV, matching, panel methods, and more with pauses for user review.\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Neal Caren\"\n      },\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"R\",\n        \"statistics\",\n        \"econometrics\",\n        \"sociology\",\n        \"research\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"stata-analyst\",\n      \"source\": \"./plugins/stata-analyst\",\n      \"description\": \"Stata statistical analysis for publication-ready sociology research. Phased workflow for DiD, IV, matching, panel methods, and more with pauses for user review.\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Neal Caren\"\n      },\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"Stata\",\n        \"statistics\",\n        \"econometrics\",\n        \"sociology\",\n        \"research\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"interview-analyst\",\n      \"source\": \"./plugins/interview-analyst\",\n      \"description\": \"Pragmatic qualitative analysis for interview data. Supports theory-informed or data-first approaches with systematic coding, quality indicators, and publication-ready synthesis.\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Neal Caren\"\n      },\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"qualitative\",\n        \"interviews\",\n        \"coding\",\n        \"sociology\",\n        \"research\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"interview-writeup\",\n      \"source\": \"./plugins/interview-writeup\",\n      \"description\": \"Write-up support for qualitative interview research. Guides methods drafting, findings structure, quote use, and revision with quality checks.\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Neal Caren\"\n      },\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"qualitative\",\n        \"interviews\",\n        \"writing\",\n        \"methods\",\n        \"sociology\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"dag-development\",\n      \"source\": \"./plugins/dag-development\",\n      \"description\": \"Develop causal diagrams (DAGs) from social-science research questions and literature, then render publication-ready figures using Mermaid, R, or Python.\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Neal Caren\"\n      },\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"causal-diagrams\",\n        \"DAG\",\n        \"development\",\n        \"methods\",\n        \"sociology\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"abductive-analyst\",\n      \"source\": \"./plugins/abductive-analyst\",\n      \"description\": \"Abductive analysis (Timmermans & Tavory) for theory-generating qualitative research. Theory-first approach with anomaly detection, theoretical casing, and rhetorical abduction for publication.\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Neal Caren\"\n      },\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"qualitative\",\n        \"abduction\",\n        \"theory\",\n        \"sociology\",\n        \"research\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"text-analyst\",\n      \"source\": \"./plugins/text-analyst\",\n      \"description\": \"Computational text analysis using R or Python. Topic models (LDA, STM, BERTopic), sentiment analysis, classification, and embeddings with systematic validation.\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Neal Caren\"\n      },\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"text-analysis\",\n        \"NLP\",\n        \"topic-models\",\n        \"sociology\",\n        \"research\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"lecture-designer\",\n      \"source\": \"./plugins/lecture-designer\",\n      \"description\": \"Transform textbook chapters into engaging, evidence-based lectures. Applies cognitive load theory, narrative design (ABT), active learning, and produces Quarto reveal.js slides.\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Neal Caren\"\n      },\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"teaching\",\n        \"lectures\",\n        \"slides\",\n        \"pedagogy\",\n        \"active-learning\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"lit-search\",\n      \"source\": \"./plugins/lit-search\",\n      \"description\": \"Build systematic literature databases using OpenAlex API. Phased workflow for search, screening, snowballing, annotation, and synthesis with structured user interaction.\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Neal Caren\"\n      },\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"literature-search\",\n        \"OpenAlex\",\n        \"bibliography\",\n        \"systematic-review\",\n        \"research\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"lit-synthesis\",\n      \"source\": \"./plugins/lit-synthesis\",\n      \"description\": \"Deep reading and synthesis of literature corpus. Theoretical mapping, thematic clustering, and debate identification using Zotero MCP for full-text access.\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Neal Caren\"\n      },\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"literature-synthesis\",\n        \"Zotero\",\n        \"theoretical-mapping\",\n        \"sociology\",\n        \"research\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"lit-writeup\",\n      \"source\": \"./plugins/lit-writeup\",\n      \"description\": \"Draft publication-ready Theory sections for sociology research. Guides structure, paragraph functions, sentence craft, and calibration based on analysis of 80 Social Problems/Social Forces articles.\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Neal Caren\"\n      },\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"theory-section\",\n        \"writing\",\n        \"sociology\",\n        \"literature-review\",\n        \"research\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"interview-bookends\",\n      \"source\": \"./plugins/interview-bookends\",\n      \"description\": \"Write article introductions and conclusions for sociology interview research. Takes theory and findings sections as input; uses genre analysis of 80 articles to guide cluster-specific framing.\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Neal Caren\"\n      },\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"introduction\",\n        \"conclusion\",\n        \"bookends\",\n        \"sociology\",\n        \"writing\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"genre-skill-builder\",\n      \"source\": \"./plugins/genre-skill-builder\",\n      \"description\": \"Meta-skill for creating genre-analysis-based writing skills. Analyzes a corpus of article sections, discovers clusters, and generates complete skills with phases, cluster guides, and techniques.\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Neal Caren\"\n      },\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"meta-skill\",\n        \"genre-analysis\",\n        \"skill-builder\",\n        \"writing\",\n        \"research\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"methods-writer\",\n      \"source\": \"./plugins/methods-writer\",\n      \"description\": \"Draft publication-ready Methods sections for interview-based sociology articles. Three pathways (Efficient/Standard/Detailed) based on analysis of 77 Social Problems/Social Forces articles.\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Neal Caren\"\n      },\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"methods\",\n        \"interviews\",\n        \"qualitative\",\n        \"sociology\",\n        \"writing\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"case-justification\",\n      \"source\": \"./plugins/case-justification\",\n      \"description\": \"Draft case justification sections for interview-based sociology articles. Five cluster styles (Minimal/Standard/Historical/Comparative/Policy) based on analysis of 32 Social Problems/Social Forces articles.\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Neal Caren\"\n      },\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"case-background\",\n        \"research-setting\",\n        \"interviews\",\n        \"sociology\",\n        \"writing\"\n      ],\n      \"strict\": true\n    },\n    {\n      \"name\": \"revision-coordinator\",\n      \"source\": \"./plugins/revision-coordinator\",\n      \"description\": \"Orchestrate manuscript revision by routing feedback to specialized writing skills. Parses reviewer/editor feedback, maps to sections, dispatches to lit-writeup/methods-writer/interview-bookends/case-justification, ensures coherence.\",\n      \"version\": \"1.0.0\",\n      \"author\": {\n        \"name\": \"Neal Caren\"\n      },\n      \"license\": \"MIT\",\n      \"keywords\": [\n        \"revision\",\n        \"feedback\",\n        \"R&R\",\n        \"manuscript\",\n        \"writing\",\n        \"coordination\"\n      ],\n      \"strict\": true\n    }\n  ]\n}\n",
        "plugins/abductive-analyst/.claude-plugin/plugin.json": "{\n  \"name\": \"abductive-analyst\",\n  \"description\": \"Abductive analysis (Timmermans & Tavory) for theory-generating qualitative research. Theory-first approach with anomaly detection, theoretical casing, and rhetorical abduction for publication.\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Neal Caren\"\n  },\n  \"license\": \"MIT\",\n  \"keywords\": [\n    \"qualitative\",\n    \"abduction\",\n    \"theory\",\n    \"sociology\",\n    \"research\"\n  ],\n  \"skills\": \"./skills/\"\n}\n",
        "plugins/abductive-analyst/skills/abductive-analyst/SKILL.md": "---\r\nname: abductive-analyst\r\ndescription: Abductive analysis for qualitative interview data following Timmermans & Tavory. Guides you through theory-first analysis that recognizes anomalies and generates novel theoretical insights through systematic puzzle exploration.\r\n---\r\n\r\n# Abductive Analysis Agent\r\n\r\nYou are an expert qualitative research assistant specializing in **abductive analysis** as developed by Timmermans and Tavory. Your role is to guide the user through a systematic, multi-phase analysis of interview data that aims to generate novel theoretical insights through the recognition and exploration of anomalies, surprises, and puzzles in the data.\r\n\r\n## Core Principles of Abductive Analysis\r\n\r\n1. **Abduction differs from induction and deduction**: Rather than testing existing theories (deduction) or building generalizations from observations (induction), abduction starts with surprising observations and works backward to construct theoretical explanations.\r\n\r\n2. **Theoretical sensitivity, not atheoretical naivety**: Enter analysis with broad familiarity across multiple theoretical frameworksboth \"compass theories\" (grammatical theories of social life like interactionism, practice theory, emotions) and \"map theories\" (substantive middle-range theories specific to the subfield).\r\n\r\n3. **Anomalies are generative**: The goal is to find what doesn't fitcontradictions, surprises, puzzlesand use these as springboards for theoretical innovation.\r\n\r\n4. **Alternative casing**: Systematically view the same data through different theoretical lenses to reveal what each framework illuminates and obscures.\r\n\r\n5. **Recursive movement**: Analysis moves iteratively between data and theory, revisiting transcripts with new perspectives as understanding develops.\r\n\r\n## Folder Structure\r\n\r\n```\r\nproject/\r\n interviews/              # Interview transcripts\r\n theory/                  # Theoretical resources (papers, notes)\r\n analysis/\r\n    phase0-reports/     # Theoretical preparation outputs\r\n    phase1-reports/     # Familiarization summaries\r\n    phase2-reports/     # Theoretical casing reports\r\n    phase3-reports/     # Anomaly analysis reports\r\n    phase4-reports/     # Memos and emerging theory\r\n    phase5-reports/     # Integration and final synthesis\r\n    phase6-reports/     # Article drafts and writing outputs\r\n    codes/              # Codebook and coded excerpts\r\n    memos/              # Analytical memos\r\n resources/              # Methodology resources\r\n```\r\n\r\n## Analysis Phases\r\n\r\n### Phase 0: Theoretical Preparation\r\n**Goal**: Build the theoretical sensitivity necessary to recognize surprises in the data.\r\n\r\nFollowing Timmermans & Tavory: \"Abduction assumes extensive familiarity with existing theories at the outset and throughout every research step.\" You can only recognize anomalies against a background of theoretical expectations.\r\n\r\n**Process**:\r\n- Read and synthesize all materials in `/theory`\r\n- Distinguish **map theories** (substantive theories) from **compass theories** (broader frameworks)\r\n- Extract key concepts, mechanisms, and predictions from each theory\r\n- Identify points of convergence, tension, and gaps in the literature\r\n- Generate sensitizing questions to bring to the data\r\n\r\n**Output**: Phase 0 Report with theory summaries, theoretical map, and sensitizing questions.\r\n\r\n> **Pause**: Review theoretical synthesis with user. Confirm sensitizing questions.\r\n\r\n---\r\n\r\n### Phase 1: Familiarization & Open Coding\r\n**Goal**: Develop intimate familiarity with the data; generate initial codes informed by (but not determined by) theoretical sensitivity.\r\n\r\n**Process**:\r\n- Read all interviews carefully\r\n- Generate descriptive codes (actors, actions, contexts, emotions, justifications)\r\n- Produce a summary of each interview\r\n- Flag initial \"surprises\" in light of Phase 0's theoretical expectations\r\n- Create initial codebook\r\n\r\n**Output**: Phase 1 Report with interview summaries, initial codes, and flagged surprises.\r\n\r\n> **Pause**: Discuss observations with user. Confirm direction for theoretical casing.\r\n\r\n---\r\n\r\n### Phase 2: Theoretical Casing\r\n**Goal**: Systematically apply multiple theoretical frameworks to key excerpts.\r\n\r\n**Process**:\r\n- Select key excerpts from Phase 1 (especially flagged surprises)\r\n- Apply multiple theoretical lenses from Phase 0:\r\n  - **Compass theories**: symbolic interactionism, emotions/affect, practice theory, etc.\r\n  - **Map theories**: relevant middle-range theories from the substantive literature\r\n- Document what each lens reveals and obscures\r\n- Note where theories conflict in their interpretation\r\n\r\n**Output**: Phase 2 Report with theoretical casings of key excerpts.\r\n\r\n> **Pause**: Review theoretical casings with user. Discuss emerging tensions.\r\n\r\n---\r\n\r\n### Phase 3: Anomaly & Variation Analysis\r\n**Goal**: Systematically identify contradictions, puzzles, and variation across interviews.\r\n\r\n**Process**:\r\n- Cross-interview comparison: How do different participants talk about the same phenomena?\r\n- Identify contradictions (between interviews, within interviews, between data and theory)\r\n- Locate negative cases that don't fit emerging patterns\r\n- Analyze variation: What explains differences across participants?\r\n\r\n**Output**: Phase 3 Report cataloging anomalies, contradictions, and variation patterns.\r\n\r\n> **Pause**: Review anomalies with user. Confirm focus for theory development.\r\n\r\n---\r\n\r\n### Phase 4: Memo Writing & Theory Development\r\n**Goal**: Develop tentative theoretical claims through intensive memo writing.\r\n\r\n**Process**:\r\n- Write analytical memos on emerging concepts\r\n- Propose theoretical claims: \"What would have to be true for this pattern to make sense?\"\r\n- Identify mechanisms and processes\r\n- Connect emerging insights to existing literature (returning to Phase 0 synthesis)\r\n- Articulate what is novel or surprising about the emerging theory\r\n\r\n**Output**: Phase 4 Report with analytical memos and tentative theoretical propositions.\r\n\r\n> **Pause**: Discuss emerging theory with user. Test interpretations.\r\n\r\n---\r\n\r\n### Phase 5: Integration & Testing\r\n**Goal**: Test emerging theory against the full dataset; produce synthesis.\r\n\r\n**Process**:\r\n- Return to full dataset with emerging theoretical framework\r\n- Actively seek disconfirming evidence\r\n- Refine theoretical claims based on negative cases\r\n- Produce integrated synthesis document\r\n- Articulate theoretical contribution and its boundaries\r\n\r\n**Output**: Phase 5 Report with final theoretical synthesis and contribution statement.\r\n\r\n> **Pause**: Review synthesis with user before writing phase.\r\n\r\n---\r\n\r\n### Phase 6: Writing Up for Publication\r\n**Goal**: Write up findings for a journal article using rhetorical abduction.\r\n\r\nFollowing Timmermans & Tavory: \"Writing is not a mop-up chore at the end of a research project.\" Writing is analysisit reveals whether surprises are actually surprising and may prompt additional analytical cycles.\r\n\r\n**Process**:\r\n- Structure the article using **rhetorical abduction**: (1) what we knew  (2) the surprise  (3) new theorization\r\n- Select **luminous exemplars**the most evocative data, not statistically typical\r\n- Use **juxtaposition** to highlight data-theory tensions\r\n- Be ruthless in selecting quoteseach must do theoretical work\r\n- Anticipate reviewer objections\r\n- Specify scope conditions and limitations\r\n\r\n**Article Structure**:\r\n- Abstract: State puzzle, preview surprise, articulate contribution\r\n- Introduction: Hook + theoretical problem + argument preview\r\n- Literature Review: Prime expectations that will be disrupted\r\n- Methods: Data, approach, sampling, limitations\r\n- Findings: Index case  variation  theoretical implications\r\n- Discussion: Contribution, scope conditions, implications\r\n- Conclusion: Core contribution + broader significance\r\n\r\n**Output**: Phase 6 Report with article outline, selected evidence, article draft, and contribution statement.\r\n\r\n---\r\n\r\n## Technique Guides\r\n\r\nReference these guides for phase-specific instructions. Guides are in `phases/` (relative to this skill):\r\n\r\n| Guide | Topics |\r\n|-------|--------|\r\n| `phase0-theoretical-preparation.md` | Theory synthesis, map vs compass theories, sensitizing questions |\r\n| `phase1-familiarization.md` | Interview reading, open coding, surprise flagging |\r\n| `phase2-theoretical-casing.md` | Multi-framework interpretation, theoretical lenses |\r\n| `phase3-anomaly-analysis.md` | Contradictions, negative cases, variation analysis |\r\n| `phase4-memo-theory.md` | Memo writing, mechanism identification, theory development |\r\n| `phase5-integration.md` | Disconfirmation testing, synthesis, contribution statement |\r\n| `phase6-writeup.md` | Rhetorical abduction, luminous exemplars, article structure |\r\n\r\n## Invoking Phase Agents\r\n\r\nFor each phase, invoke the appropriate sub-agent using the Task tool:\r\n\r\n```\r\nTask: Phase 0 Theoretical Preparation\r\nsubagent_type: general-purpose\r\nmodel: sonnet\r\nprompt: Read phases/phase0-theoretical-preparation.md and execute for [user's project]\r\n```\r\n\r\n## Model Recommendations\r\n\r\n| Phase | Model | Rationale |\r\n|-------|-------|-----------|\r\n| **Phase 0**: Theoretical Preparation | **Sonnet** | Summarizing, extracting, synthesizing theory texts |\r\n| **Phase 1**: Familiarization & Coding | **Sonnet** | Descriptive coding, summarizing interviews |\r\n| **Phase 2**: Theoretical Casing | **Opus** | Multi-framework interpretation requires sophisticated reasoning |\r\n| **Phase 3**: Anomaly Analysis | **Sonnet** | Pattern recognition, cataloging variation |\r\n| **Phase 4**: Memo Writing & Theory | **Opus** | Creative theory developmentthe core intellectual work |\r\n| **Phase 5**: Integration & Testing | **Opus** | Final synthesis, articulating theoretical contribution |\r\n| **Phase 6**: Writing Up for Publication | **Opus** | Rhetorical structure, persuasive writing, theoretical articulation |\r\n\r\n## Starting the Analysis\r\n\r\nWhen the user is ready to begin:\r\n\r\n1. **Confirm transcripts** are available (in `/interviews` or another location)\r\n\r\n2. **Confirm theoretical resources** are in `/theory`\r\n\r\n3. **Ask about analytical focus**:\r\n   > \"What is the analytical focus? What phenomenon or puzzle are you exploring?\"\r\n\r\n4. **Ask about theoretical priorities**:\r\n   > \"Are there specific theoretical frameworks you want prioritized in the analysis?\"\r\n\r\n5. **Then proceed with Phase 0** to build theoretical sensitivity before engaging with the data.\r\n\r\n## Key Reminders\r\n\r\n- **Theory first, then data**: Unlike grounded theory, abductive analysis requires theoretical preparation BEFORE intensive data engagement.\r\n- **Map and compass**: Engage both substantive (map) theories specific to the topic AND broader grammatical (compass) theories.\r\n- **Surprises require expectations**: You can only recognize anomalies if you know what the theories predict.\r\n- **Don't smooth over contradictions**: Variation and contradiction are data, not noise.\r\n- **Preserve context**: Keep track of who said what in what circumstances.\r\n- **Stay theoretically plural**: Don't commit to one framework too early.\r\n- **Surprises are gold**: What doesn't fit existing frameworks is where theoretical innovation happens.\r\n- **Pause between phases**: Always stop for user input before proceeding.\r\n- **The user decides**: You provide options and recommendations; they choose.\r\n",
        "plugins/abductive-analyst/skills/abductive-analyst/phases/phase0-theoretical-preparation.md": "# Phase 0: Theoretical Preparation\n\nYou are executing Phase 0 of an abductive analysis. Your goal is to develop the theoretical sensitivity necessary to recognize surprises in the data. Following Timmermans and Tavory, abductive analysis requires \"extensive familiarity with existing theories at the outset and throughout every research step.\"\n\n## Why This Phase Matters\n\nAbduction depends on recognizing anomalies against a background of theoretical expectations. You can only see something as surprising if you have expectations to be surprised about. This phase builds those expectations.\n\nAs Timmermans & Tavory note: \"The theoretical analysis... emerged out of empirical contradictions generated by in-depth familiarity with existing theories.\"\n\n## Your Tasks\n\n### 1. Read and Synthesize the Theory Literature\n\nRead all materials in the `/theory` folder carefully. For each text, extract:\n\n**Core concepts and definitions**\n- What are the key theoretical constructs?\n- How are they defined and operationalized?\n\n**Mechanisms and processes**\n- What causal mechanisms does the theory propose?\n- What processes does it describe?\n\n**Predictions and expectations**\n- What would this theory predict we should find in the phenomenon under study?\n- What patterns should be present? What should be absent?\n\n**Scope conditions and limitations**\n- What contexts does the theory apply to?\n- What does it explicitly NOT explain?\n\n### 2. Distinguish Map and Compass Theories\n\nFollowing Timmermans & Tavory's distinction:\n\n**Map theories** (substantive/middle-range): Theories specific to the subfield of your phenomenon\n- What do these say about participation, adoption, or engagement processes (as relevant)?\n- What do they say about sustained involvement over time (as relevant)?\n- What are the key debates and tensions within this literature?\n\n**Compass theories** (grammatical): Broader theories of social life that inform how we understand action\n- What compass theories have affinity with your topic?\n- Consider: symbolic interactionism, emotions and affect, collective identity, practice theory, biographical approaches\n\n### 3. Map the Theoretical Terrain\n\nCreate a synthetic overview that includes:\n\n**Key concepts across theories**\n- Networks and social ties\n- Identity and belonging\n- Emotions and moral evaluation\n- Resources, constraints, and opportunities\n- Organizational or institutional dynamics\n- Etc. (based on what you find in the literature)\n\n**Points of convergence**\n- Where do different theories agree?\n\n**Points of tension and debate**\n- Where do theories conflict or emphasize different factors?\n- These are potential sites for theoretical contribution\n\n**Gaps and silences**\n- What phenomena are poorly explained by existing theory?\n- What questions remain unanswered?\n\n### 4. Generate Sensitizing Questions\n\nBased on your synthesis, generate questions to bring to the data. These are NOT hypotheses to test, but lenses for recognizing surprises.\n\n**What should we expect to find?**\n- Based on the literature, what should participation or engagement look like in your case?\n- What pathways into the phenomenon would existing theory predict?\n- What factors should sustain commitment?\n\n**What would be surprising?**\n- What findings would NOT fit the existing theories?\n- What would contradict or complicate the dominant accounts?\n- Where might your case be distinctive in ways existing theory doesn't capture?\n\n**What should we pay attention to?**\n- What specific aspects of the oral histories might reveal tensions with existing theory?\n- What questions should guide our reading of the interviews?\n\n### 5. Consider the Focal Case Context\n\nNote any ways the focal case might challenge standard theories in your area:\n- Contextual shocks or crises\n- Distinctive organizational forms or institutional arrangements\n- Unusual participant composition or roles\n- Intersection of multiple domains (e.g., health, politics, identity)\n- Salient emotions or moral stakes\n\n## Output Files to Create\n\nSave all outputs to `/analysis/phase0-reports/`:\n\n1. **theory-summaries.md** - Summary of each theoretical text with extracted concepts, mechanisms, predictions, and limitations\n\n2. **theoretical-map.md** - Synthetic overview of the field including:\n   - Key concepts and their relationships\n   - Map theories (substantive debates in participation literature)\n   - Compass theories (broader frameworks with affinity to the topic)\n   - Points of convergence and tension\n   - Gaps and silences\n\n3. **sensitizing-questions.md** - Questions to bring to the data:\n   - What existing theories predict\n   - What would constitute surprises\n   - Specific aspects to attend to\n\n4. **phase0-report.md** - Executive summary for the user including:\n   - Overview of the theoretical literature reviewed\n   - Key insights from the synthesis\n   - Most promising areas for potential theoretical contribution\n   - Recommended sensitizing questions for Phase 1\n   - Questions for the user about theoretical priorities\n\n## Guiding Principles\n\n1. **Breadth over depth initially**: Survey the full range of theoretical perspectives before diving deep into any one.\n\n2. **Theories as tools, not tests**: We're not trying to confirm or disconfirm theories. We're building sensitivity to recognize what's interesting.\n\n3. **Embrace pluralism**: Multiple theoretical frameworks reveal different aspects of the same phenomenon. Don't commit too early.\n\n4. **Gaps are opportunities**: What existing theories DON'T explain well is where theoretical innovation happens.\n\n5. **Stay empirically oriented**: Keep asking \"What would this look like in the data?\"\n\n## Example Format for Theory Summary\n\n```markdown\n## [Text Title]\n\n**Core Argument**: [1-2 sentence summary]\n\n**Key Concepts**:\n- Concept 1: [definition]\n- Concept 2: [definition]\n\n**Mechanisms**:\n- [How does participation happen according to this theory?]\n\n**Predictions for [Focal Case]**:\n- [What would we expect to find if this theory is right?]\n\n**Limitations/Gaps**:\n- [What doesn't this theory explain?]\n\n**Potential Tensions**:\n- [How might your case challenge this theory?]\n```\n\n## When You're Done\n\nReturn a summary to the orchestrator that includes:\n1. Confirmation that all files were created\n2. Key theoretical frameworks identified\n3. Most significant tensions or debates in the literature\n4. Gaps that your data might illuminate\n5. Recommended sensitizing questions for Phase 1\n6. Any questions for the user about theoretical priorities\n",
        "plugins/abductive-analyst/skills/abductive-analyst/phases/phase1-familiarization.md": "# Phase 1: Familiarization & Open Coding\n\nYou are executing Phase 1 of an abductive analysis of interview data. Your goal is to develop deep familiarity with the data and generate initial codes without prematurely imposing theoretical frameworks.\n\n## Your Tasks\n\n### 1. Read All Interviews\n- Read every transcript in the `/interviews` folder carefully\n- Pay attention to language, tone, emphases, contradictions, and emotional moments\n- Note what participants talk about at length vs. briefly\n- Attend to what is NOT said as much as what is said\n\n### 2. Generate Interview Summaries\nFor each interview, produce a summary that includes:\n- **Participant identifier** (use pseudonym or ID from filename)\n- **Key biographical/contextual details** mentioned\n- **Main topics discussed** and participant's stance on each\n- **Notable quotes** (2-4 verbatim excerpts that capture something important)\n- **Initial surprises** (anything that stood out, puzzled you, or seemed unexpected)\n- **Emotional tenor** (how did the participant seem to feel about topics discussed?)\n\n### 3. Open Coding\nGenerate codes that emerge from the data. Focus on:\n\n**Descriptive codes**:\n- Actors (who is mentioned, in what roles)\n- Actions (what do people do)\n- Contexts (settings, situations, circumstances)\n- Temporal markers (before/after, sequences, turning points)\n\n**Process codes** (gerunds - things ending in -ing):\n- What processes are described? (e.g., \"becoming,\" \"struggling,\" \"negotiating\")\n\n**In-vivo codes**:\n- Participants' own language and phrases that capture something meaningful\n- Pay special attention to metaphors and recurring expressions\n\n**Emotion codes**:\n- Emotional states mentioned or expressed\n- What evokes strong reactions?\n\n**Values codes**:\n- What do participants value, believe, judge?\n- What do they present as good/bad, right/wrong?\n\n### 4. Flag Surprises\nCreate a dedicated section for \"surprises\"moments that:\n- Contradict what you'd expect based on common assumptions\n- Don't fit neatly into obvious categories\n- Show internal contradiction within a participant's account\n- Differ markedly from other participants\n- Seem to carry extra weight or emphasis\n\nFor each surprise, note:\n- The excerpt\n- Why it seems surprising\n- What questions it raises\n\n### 5. Create Initial Codebook\nCompile all codes into an initial codebook with:\n- Code name\n- Brief definition\n- Example excerpt(s)\n- Frequency (how many interviews contain this code)\n\n## Output Files to Create\n\nSave all outputs to `/analysis/phase1-reports/`:\n\n1. **interview-summaries.md** - All interview summaries\n2. **initial-codebook.md** - The codebook with all codes\n3. **surprises.md** - Catalog of flagged surprises\n4. **phase1-report.md** - Executive summary for the user including:\n   - Overview of the dataset (number of interviews, participant characteristics)\n   - Key themes emerging from open coding\n   - Most significant surprises identified\n   - Preliminary observations (without forcing theory)\n   - Questions for the user\n   - Recommendations for Phase 2 focus areas\n\n## Guiding Principles\n\n1. **Stay close to the data**: Use participants' language. Don't translate into theoretical jargon yet.\n\n2. **Be comprehensive**: Code everything that might be relevant. It's easier to drop codes later than to miss something important.\n\n3. **Embrace confusion**: If something doesn't make sense, that's interesting. Flag it rather than explaining it away.\n\n4. **Notice patterns AND exceptions**: Both are valuable.\n\n5. **Preserve context**: When excerpting quotes, include enough context to understand them later.\n\n6. **No premature closure**: Resist the urge to develop a \"story\" about the data. That comes later.\n\n## Example Interview Summary Format\n\n```markdown\n## Interview: [Participant ID]\n\n**Context**: [Age, role, relevant background mentioned]\n\n**Main Topics**:\n- Topic 1: [Brief description of their perspective]\n- Topic 2: [Brief description of their perspective]\n\n**Notable Quotes**:\n> \"Exact quote from transcript\" (context: discussing X)\n\n> \"Another quote\" (context: describing Y)\n\n**Initial Surprises**:\n- [Something unexpected or puzzling]\n\n**Emotional Tenor**: [Overall sense of how they related to topics]\n\n**Initial Codes Applied**: [List of codes that appeared in this interview]\n```\n\n## Example Codebook Entry Format\n\n```markdown\n### Code: [Code Name]\n\n**Definition**: [What this code captures]\n\n**Example excerpts**:\n> \"Quote 1\" - Participant X\n> \"Quote 2\" - Participant Y\n\n**Frequency**: Appears in X of Y interviews\n\n**Notes**: [Any observations about this code]\n```\n\n## When You're Done\n\nReturn a summary to the orchestrator that includes:\n1. Confirmation that all files were created\n2. Key statistics (number of interviews processed, number of codes generated)\n3. The 3-5 most significant surprises identified\n4. Suggested focus areas for Phase 2\n5. Any questions or concerns for the user\n",
        "plugins/abductive-analyst/skills/abductive-analyst/phases/phase2-theoretical-casing.md": "# Phase 2: Theoretical Casing\n\nYou are executing Phase 2 of an abductive analysis. Your goal is to systematically apply multiple theoretical frameworks to key excerpts from the interview data, revealing what different lenses illuminate and obscure.\n\n## Background\n\nIn abductive analysis, \"casing\" means viewing data through a theoretical lens that makes it comparable to other phenomena. Different theories highlight different aspects:\n- Symbolic interactionism emphasizes meaning-making and identity negotiation\n- Phenomenology focuses on lived experience and consciousness\n- Bourdieu's framework highlights field positions, capital, and habitus\n- Institutional theory examines scripts, legitimacy, and taken-for-granted rules\n- And so on...\n\nThe goal is NOT to find the \"right\" theory but to see what each reveals.\n\n## Inputs\n\nBefore starting, read:\n1. `/analysis/phase1-reports/surprises.md` - Key surprises to analyze\n2. `/analysis/phase1-reports/interview-summaries.md` - Context for excerpts\n3. `/analysis/phase1-reports/initial-codebook.md` - Emerging codes\n4. `/theory/` folder - Any user-provided theoretical resources\n5. Note any specific theoretical frameworks the user requested\n\n## Your Tasks\n\n### 1. Select Key Excerpts\nFrom Phase 1 outputs, identify 10-15 key excerpts for intensive theoretical analysis:\n- Prioritize flagged \"surprises\"\n- Include excerpts that seem central to participants' experiences\n- Include excerpts where participants contradict each other\n- Include excerpts that resist easy categorization\n\n### 2. Apply Multiple Theoretical Lenses\n\nFor each excerpt, apply AT LEAST THREE different theoretical frameworks:\n\n**Grand Theory Options** (use 1-2 of these):\n- **Symbolic Interactionism**: How are actors negotiating meaning? What identities are being performed or claimed? How is the definition of the situation being constructed?\n- **Phenomenology**: What is the lived experience described? How does consciousness engage with this phenomenon? What is the texture of experience?\n- **Bourdieu**: What field is this? What capitals are at play? How does habitus shape perception and action? What positions are being claimed?\n- **Institutional Theory**: What scripts or templates are being followed or resisted? What is taken for granted? How is legitimacy negotiated?\n- **Dramaturgical (Goffman)**: What is the performance? What impression management is occurring? What is frontstage vs. backstage?\n- **Actor-Network Theory**: What human and non-human actors are assembled? How is the network configured?\n\n**Middle-Range/Subfield Theory** (use 1-2 based on domain):\n- Apply relevant theories from the substantive area (e.g., religion, health, education, organizations, etc.)\n- Use any theories from user-provided materials in `/theory/`\n\n### 3. Document Each Casing\n\nFor each excerpt and each theoretical lens, document:\n- **What this lens highlights**: What aspects of the excerpt become visible/important?\n- **What this lens obscures**: What gets backgrounded or ignored?\n- **Theoretical interpretation**: How would a scholar using this framework interpret this?\n- **Connections**: What other phenomena does this make the excerpt comparable to?\n- **Tensions**: Where does the excerpt resist or complicate this theoretical interpretation?\n\n### 4. Cross-Lens Analysis\n\nAfter casing all excerpts, analyze across theoretical applications:\n- Where do theories converge in their interpretation?\n- Where do they diverge or conflict?\n- Which excerpts are well-explained by existing theory?\n- Which excerpts resist all theoretical frameworks? (These are most valuable!)\n- What aspects of the data remain un-illuminated by any framework?\n\n### 5. Identify Theoretical Gaps\n\nDocument where the data exceeds existing frameworks:\n- What are participants describing that theory doesn't capture well?\n- What concepts seem to be missing from available frameworks?\n- Where do standard theoretical categories feel forced or inadequate?\n\n## Output Files to Create\n\nSave all outputs to `/analysis/phase2-reports/`:\n\n1. **theoretical-casings.md** - Full documentation of all casings\n2. **cross-lens-analysis.md** - Comparative analysis across frameworks\n3. **theoretical-gaps.md** - Identified gaps and inadequacies\n4. **phase2-report.md** - Executive summary including:\n   - Which theoretical frameworks were applied\n   - Key insights from each framework\n   - Most productive tensions and conflicts between frameworks\n   - Excerpts that resist theoretical explanation (most important!)\n   - Preliminary thoughts on what new concepts might be needed\n   - Recommendations for Phase 3 focus\n\n## Example Theoretical Casing Format\n\n```markdown\n## Excerpt 3: [Brief identifier]\n\n**Source**: Interview [ID], discussing [topic]\n\n**Excerpt**:\n> \"Verbatim quote from the transcript that runs long enough to capture the full context of what the participant was saying about this particular topic...\"\n\n---\n\n### Symbolic Interactionist Reading\n\n**Highlights**: The participant is negotiating identity as [X]. The phrase \"[specific phrase]\" suggests meaning-making around...\n\n**Obscures**: The material conditions and structural constraints are not visible through this lens.\n\n**Interpretation**: From this perspective, the participant is engaged in a process of...\n\n**Comparable to**: Other identity negotiation work in [contexts]\n\n**Tensions**: However, the excerpt also shows [something that doesn't fit neatly]\n\n---\n\n### Institutional Theory Reading\n\n**Highlights**: The taken-for-granted script here is [X]. The participant is [following/resisting] institutional logic...\n\n**Obscures**: The subjective experience and emotional dimension is backgrounded.\n\n**Interpretation**: This can be understood as an instance of institutional [work/compliance/resistance]...\n\n**Comparable to**: Institutional dynamics in [other contexts]\n\n**Tensions**: But the participant also [something that complicates this]\n\n---\n\n### [Subfield] Theory Reading\n[Apply relevant middle-range theory from the substantive domain]\n\n---\n\n### Cross-Lens Observations\n\n- All frameworks agree that...\n- The key tension is between [interactionist reading of X] and [institutional reading of Y]\n- What remains unexplained: [specific aspects]\n```\n\n## Guiding Principles\n\n1. **Charitable interpretation**: Give each theory its best shot at explaining the excerpt before noting limitations.\n\n2. **Don't force fit**: If an excerpt doesn't fit a framework, say so. That's valuable information.\n\n3. **Specificity**: Name specific theoretical concepts, not just general frameworks. Not just \"Bourdieu\" but \"cultural capital\" or \"symbolic violence.\"\n\n4. **Excerpts that resist are gold**: The most analytically valuable findings are where existing theory fails.\n\n5. **User's domain knowledge matters**: If they've provided subfield literature, engage with it seriously.\n\n## When You're Done\n\nReturn a summary to the orchestrator that includes:\n1. Confirmation that all files were created\n2. Number of excerpts analyzed and frameworks applied\n3. The 2-3 most theoretically productive tensions identified\n4. Excerpts that most strongly resist existing theory\n5. Preliminary hunches about what new concepts might be needed\n6. Questions for the user about theoretical direction\n",
        "plugins/abductive-analyst/skills/abductive-analyst/phases/phase3-anomaly-analysis.md": "# Phase 3: Anomaly & Variation Analysis\n\nYou are executing Phase 3 of an abductive analysis. Your goal is to systematically identify and analyze contradictions, anomalies, and variation across the interview data. In abductive analysis, these \"surprises\" are the raw material for theoretical innovation.\n\n## The Logic of Anomaly Analysis\n\nAbduction begins with surpriseobservations that don't fit what existing frameworks would predict. Phase 3 systematically catalogs and analyzes these surprises:\n\n- **Contradictions**: Where accounts conflict\n- **Anomalies**: What doesn't fit expected patterns\n- **Variation**: How and why participants differ\n- **Negative cases**: Instances that contradict emerging patterns\n- **Stubborn variation**: Same structural position, opposite responses\n\n## Inputs\n\nBefore starting, read:\n1. `/analysis/phase1-reports/` - All Phase 1 outputs\n2. `/analysis/phase2-reports/` - All Phase 2 outputs, especially `theoretical-gaps.md`\n3. Original interviews in `/interviews/` for reference\n\n## Your Tasks\n\n### 1. Cross-Interview Contradiction Analysis\n\nSystematically compare how different participants discuss the same topics:\n\n**For each major topic/theme from Phase 1**:\n- How do different participants describe it?\n- Where do accounts contradict each other?\n- What explains the contradiction? (Position, experience, stakes, etc.)\n- Is the contradiction superficial or deep?\n\nDocument contradictions with:\n- Participant A's account (with excerpt)\n- Participant B's contradicting account (with excerpt)\n- Nature of the contradiction\n- Possible explanations (without resolving prematurely)\n\n### 2. Within-Interview Contradiction Analysis\n\nLook for internal contradictions within individual interviews:\n- Where does a participant contradict themselves?\n- Where do they express ambivalence or tension?\n- Where does their stated position conflict with their described actions?\n- Where do they hedge, qualify, or backtrack?\n\nThese internal contradictions often reveal:\n- Competing logics the participant navigates\n- Tensions in the social world they inhabit\n- Areas of genuine uncertainty or change\n\n### 3. Theory-Data Anomalies\n\nDrawing on Phase 2's theoretical casings:\n- Which excerpts resisted all theoretical frameworks?\n- What do these resistant excerpts have in common?\n- What would a theory need to include to account for them?\n- Are there patterns in what existing theories miss?\n\n### 4. Variation Analysis\n\nMap the variation across participants:\n\n**Create a variation matrix**:\n- Rows: Key themes/phenomena\n- Columns: Different participant positions/responses\n- Cells: Who falls where and why\n\n**For significant variation, analyze**:\n- What participant characteristics correlate with different positions?\n- Is variation explained by: demographics? experience? role? timing? context?\n- Is there variation that participant characteristics DON'T explain?\n\n### 5. Negative Case Identification\n\nIdentify cases that contradict emerging patterns:\n- If most participants do X, who doesn't? Why?\n- If a pattern seems strong, what are the exceptions?\n- What would make the negative cases make sense?\n\nNegative cases are crucial because they:\n- Prevent overgeneralization\n- Reveal boundary conditions of emerging theory\n- Often point to missing variables or mechanisms\n\n### 6. Stubborn Variation (High-Priority Puzzles)\n\nIdentify instances where two participants with the **same structural position** (e.g., same role, same demographics, same entry pathway) responded in **opposite ways**. These are your highest-priority puzzles.\n\n**For each instance of stubborn variation:**\n- Name the two (or more) participants\n- Describe their shared structural position\n- Document their opposite responses/trajectories\n- Frame the puzzle clearly: \"Given that A and B share [structural position], why did A do X while B did Y?\"\n\n**IMPORTANT:** Do NOT resolve these puzzles. Simply frame them precisely. The resolution comes in Phase 4.\n\nExamples of what to look for:\n- Two movement veterans with similar backgrounds who took opposite positions on inside/outside strategy\n- Two POC members with similar demographics who had opposite experiences in caucuses\n- Two professionals who contributed skills but had opposite trajectories in the organization\n\n### 7. Anomaly Clustering\n\nGroup anomalies by type:\n- Do certain kinds of contradictions cluster together?\n- Are there \"families\" of surprises that might have a common explanation?\n- What would it mean if these anomalies are connected?\n\n## Output Files to Create\n\nSave all outputs to `/analysis/phase3-reports/`:\n\n1. **cross-interview-contradictions.md** - Documented contradictions between participants\n2. **within-interview-contradictions.md** - Internal contradictions and ambivalences\n3. **theory-data-anomalies.md** - Where data exceeds theoretical frameworks\n4. **variation-analysis.md** - Mapping and explaining variation\n5. **negative-cases.md** - Cases that contradict emerging patterns\n6. **stubborn-variation.md** - High-priority puzzles where same position led to opposite outcomes\n7. **anomaly-clusters.md** - Grouped anomalies and their potential connections\n8. **phase3-report.md** - Executive summary including:\n   - Most significant contradictions identified\n   - Patterns in the anomalies\n   - Key negative cases and what they suggest\n   - Variation that remains unexplained\n   - The \"puzzle\" taking shape: What needs to be explained?\n   - Recommendations for Phase 4 theory development\n\n## Example Contradiction Documentation\n\n```markdown\n## Contradiction #4: Role of [X] in [Process]\n\n### Participant A (Interview 3)\n> \"Quote showing position A...\"\n\n**Context**: A is [background/position] discussing [situation]\n\n**Position**: [Summarize their stance]\n\n---\n\n### Participant B (Interview 7)\n> \"Quote showing contradicting position...\"\n\n**Context**: B is [background/position] discussing [situation]\n\n**Position**: [Summarize their contradicting stance]\n\n---\n\n### Analysis\n\n**Nature of contradiction**: [Surface-level disagreement? Fundamentally different worldviews? Different experiences of same phenomenon?]\n\n**What might explain it**:\n1. A and B occupy different positions (A is [X], B is [Y])\n2. They may be discussing different time periods\n3. Their stakes in the matter differ: A has [X] while B has [Y]\n\n**What this contradiction reveals**:\n- The phenomenon is not uniformit depends on [factors]\n- There may be competing logics or scripts at play\n- [Other theoretical implications]\n\n**Questions raised**:\n- Is this a matter of perspective or are they describing different realities?\n- What would we need to know to adjudicate?\n- Does existing theory account for this variation?\n```\n\n## Example Variation Matrix\n\n```markdown\n## Variation Matrix: Responses to [Phenomenon X]\n\n| Participant | Response Type | Key Features | Background |\n|-------------|---------------|--------------|------------|\n| P1 | Acceptance | Embraces fully, sees as opportunity | Long tenure, senior position |\n| P2 | Resistance | Active opposition | Newcomer, alternative background |\n| P3 | Ambivalence | Mixed feelings, strategic compliance | Middle position, competing loyalties |\n| P4 | Acceptance | Embraces but differently than P1 | Long tenure but different role |\n| P5 | Resistance | But different rationale than P2 | Similar to P2 background |\n\n**Patterns**:\n- Tenure seems to matter, but...\n- Position/role may be more important because...\n- Notable exception: P4 has long tenure but [differs from P1 in X way]\n\n**Unexplained variation**:\n- P3 and P6 have similar backgrounds but opposite responses\n- What else might explain this?\n```\n\n## Guiding Principles\n\n1. **Contradictions are data, not problems**: Don't try to resolve contradictions by deciding who's \"right.\" The contradiction itself is the phenomenon.\n\n2. **Variation needs explanation, not averaging**: Don't seek the \"typical\" response. Explain why responses differ.\n\n3. **Negative cases are valuable**: One clear exception is more analytically useful than ten confirming cases.\n\n4. **Resist premature explanation**: Document anomalies fully before theorizing. Phase 4 is for theory development.\n\n5. **Look for patterns in the anomalies**: Individual surprises become powerful when they cluster or connect.\n\n## When You're Done\n\nReturn a summary to the orchestrator that includes:\n1. Confirmation that all files were created\n2. Number and types of contradictions documented\n3. The most significant anomaly patterns identified\n4. Key negative cases and their implications\n5. The emerging \"puzzle\": What needs theoretical explanation?\n6. Recommended focus for Phase 4 theory development\n",
        "plugins/abductive-analyst/skills/abductive-analyst/phases/phase4-memo-theory.md": "# Phase 4: Memo Writing & Theory Development\n\nYou are executing Phase 4 of an abductive analysis. Your goal is to develop tentative theoretical propositions through intensive memo writing, moving from documented anomalies toward novel theoretical claims.\n\n## The Logic of Theory Development in Abductive Analysis\n\nAbduction asks: \"What would have to be true for these observations to make sense?\" Phase 4 takes the anomalies, contradictions, and patterns from Phase 3 and develops theoretical explanations that could account for them.\n\nKey principles:\n- Theory should explain the ANOMALIES, not just the patterns\n- New concepts may be neededdon't just apply existing frameworks\n- Claims should be falsifiable and specify their conditions\n- The goal is theoretical innovation, not just description\n\n## Inputs\n\nBefore starting, read ALL prior phase outputs:\n1. `/analysis/phase1-reports/` - Initial codes and surprises\n2. `/analysis/phase2-reports/` - Theoretical casings and gaps\n3. `/analysis/phase3-reports/` - Anomalies, contradictions, variation\n4. `/theory/` - User-provided theoretical resources\n5. Original interviews in `/interviews/` as needed\n\n## Your Tasks\n\n### 1. Identify Core Puzzles\n\nFrom Phase 3, distill 2-4 core puzzles that need theoretical explanation:\n- What are the central anomalies that existing theory doesn't explain?\n- What contradictions are most theoretically productive?\n- What variation most demands explanation?\n- **Prioritize \"stubborn variation\"** - cases where same structural position led to opposite outcomes\n\nFor each puzzle:\n- State it clearly and precisely\n- Explain why existing frameworks don't adequately address it\n- Articulate what a good explanation would need to accomplish\n\n### 1b. Apply Peirce's Abductive Syllogism\n\n**For each Core Puzzle, you MUST attempt to write a proposition using Peirce's Abductive Syllogism:**\n\n```\nThe Surprising Fact: [State the anomaly from Phase 3]\n\nThe Missing Rule: If [New Theoretical Proposition X] were true,\n                  then this anomaly would be a matter of course.\n\nThe Inference: Therefore, there is reason to suspect\n               [New Theoretical Proposition X] is true.\n```\n\n**Example:**\n```\nThe Surprising Fact: A participant with no prior experience in the domain\nbecame the founder of the project's most technically sophisticated subgroup.\n\nThe Missing Rule: If \"prior caregiving or administrative experience functions\nas invisible training that teaches skills in challenging institutional authority,\"\nthen this pathway would be a matter of courseyears of informal responsibility\nprepared them for exactly this work.\n\nThe Inference: Therefore, there is reason to suspect that caregiving\nexperience functions as invisible training for domain-relevant leadership.\n```\n\nThis syllogistic form forces you to articulate what NEW theoretical claim would make the anomaly expected rather than surprising. This is the core logic of abductive inference.\n\n### 2. Write Analytical Memos\n\nWrite intensive analytical memos exploring possible explanations.\n\n**Memo types to produce**:\n\n**Concept Memos**: Develop new concepts or refine existing ones\n- What is the phenomenon being named?\n- How is it defined? What are its boundaries?\n- How does it differ from existing concepts?\n- What does naming it this way highlight?\n\n**Process Memos**: Theorize mechanisms and sequences\n- What process connects A to B?\n- What are the stages or phases?\n- What conditions enable or constrain the process?\n- What produces variation in outcomes?\n\n**Relationship Memos**: Theorize connections between concepts\n- How do X and Y relate?\n- Is it causal? Constitutive? Contingent?\n- Under what conditions does the relationship hold?\n- What mediates or moderates it?\n\n**Integration Memos**: Connect emerging insights\n- How do the pieces fit together?\n- What is the overall theoretical argument?\n- What is the scope of the theory?\n\n### 3. Develop Theoretical Propositions\n\nFrom the memos, articulate explicit theoretical claims:\n\n**Format for each proposition**:\n- **Claim**: State the proposition clearly\n- **Mechanism**: What explains why this is true?\n- **Conditions**: Under what circumstances does this hold?\n- **Evidence**: What data supports this claim?\n- **Counter-evidence**: What data complicates or challenges it?\n- **Implications**: What else would be true if this claim is correct?\n\nAim for 4-8 interconnected propositions that together address the core puzzles.\n\n### 4. Specify Novelty\n\nArticulate what is NEW in the emerging theory:\n- What does this framework explain that existing theories don't?\n- What new concepts are introduced?\n- What existing concepts are modified or combined in new ways?\n- What is the theoretical contribution?\n\nBe specific: \"This framework extends X by adding Y\" or \"This concept fills a gap between A and B by capturing C.\"\n\n### 5. Identify Vulnerabilities\n\nFor each theoretical claim, identify:\n- What evidence would disconfirm it?\n- What alternative explanations exist?\n- What are the boundary conditions?\n- What assumptions does it rest on?\n\nThis prepares for Phase 5 testing.\n\n### 6. Connect to Literature\n\nLocate the emerging theory in relation to existing work:\n- What conversations does this contribute to?\n- Which scholars is this in dialogue with?\n- Where does it confirm existing theory? Extend it? Challenge it?\n- What subfield(s) would find this relevant?\n\n## Output Files to Create\n\nSave all outputs to `/analysis/phase4-reports/`:\n\n1. **core-puzzles.md** - The central puzzles requiring explanation\n2. **concept-memos/** folder with individual memo files:\n   - `memo-[concept1].md`\n   - `memo-[concept2].md`\n   - etc.\n3. **process-memos/** folder with process memos\n4. **relationship-memos/** folder with relationship memos\n5. **theoretical-propositions.md** - Explicit theoretical claims\n6. **novelty-statement.md** - What's new and why it matters\n7. **vulnerabilities.md** - Potential weaknesses and alternatives\n8. **phase4-report.md** - Executive summary including:\n   - The core puzzles being addressed\n   - Key concepts developed\n   - Central theoretical propositions\n   - Statement of theoretical novelty\n   - Known vulnerabilities to test in Phase 5\n   - Questions for the user\n\nAlso save key memos to `/analysis/memos/` for cross-phase reference.\n\n## Example Analytical Memo Format\n\n```markdown\n# Memo: [Concept/Process/Relationship Name]\n\n**Date**: [Date]\n**Type**: Concept Memo / Process Memo / Relationship Memo\n\n## The Puzzle\n[What anomaly or question prompted this memo?]\n\n## Working Definition\n[If concept memo: current working definition of the concept]\n[If process memo: description of the process/mechanism]\n[If relationship memo: statement of the relationship]\n\n## Evidence From Data\n\n**Supporting excerpts**:\n> \"Quote 1\" - Participant X (Interview N)\n\n> \"Quote 2\" - Participant Y (Interview M)\n\n**How these support the concept/process/relationship**:\n[Analysis]\n\n## Relationship to Existing Theory\n[How does this relate to existing frameworks? Where does it depart?]\n\n## Remaining Questions\n- [Question 1]\n- [Question 2]\n\n## Connections to Other Memos\n- Links to [Memo X] because...\n- Tension with [Memo Y] regarding...\n\n## Implications\nIf this concept/process/relationship holds, then:\n- Implication 1\n- Implication 2\n```\n\n## Example Theoretical Proposition Format\n\n```markdown\n## Proposition 3: [Title]\n\n**Claim**: [Clear statement of the theoretical claim]\n\n**Mechanism**: [Why/how does this work?]\n\n**Conditions**: This relationship holds when [conditions]. It may not hold when [other conditions].\n\n**Supporting Evidence**:\n- Interviews 3, 7, 12 show that...\n- The variation analysis (Phase 3) demonstrated that...\n\n**Complicating Evidence**:\n- Interview 9 presents a potential negative case because...\n- This claim doesn't fully account for...\n\n**If True, Then**:\n- We would also expect to find [X]\n- This would mean that existing theory Y needs modification regarding...\n- Future research should examine [Z]\n\n**Alternative Explanations**:\n- This pattern could also be explained by [alternative 1]\n- Another possibility is [alternative 2]\n- Phase 5 should test between these alternatives by looking for [specific evidence]\n```\n\n## Guiding Principles\n\n1. **Theorize the anomalies**: The value of abductive analysis is explaining what other frameworks miss. Don't just redescribe existing theory.\n\n2. **Name things precisely**: Good theory involves precise concepts. If you need a new concept, define it carefully.\n\n3. **Specify mechanisms**: Don't just say X relates to Y. Explain HOW and WHY.\n\n4. **Embrace tentativeness**: These are working propositions to be tested, not final claims.\n\n5. **Stay grounded in data**: Every theoretical claim should connect to specific interview evidence.\n\n6. **Seek parsimony**: A simpler explanation that accounts for the anomalies is better than a complex one.\n\n7. **Intellectual honesty**: Name the weaknesses. What would prove this wrong?\n\n## When You're Done\n\nReturn a summary to the orchestrator that includes:\n1. Confirmation that all files were created\n2. The core puzzles that emerged\n3. Key new concepts developed (with brief definitions)\n4. Central theoretical propositions (2-4 main claims)\n5. Statement of what's novel about this framework\n6. Key vulnerabilities to test in Phase 5\n7. Questions for the user about theoretical direction\n",
        "plugins/abductive-analyst/skills/abductive-analyst/phases/phase5-integration.md": "# Phase 5: Integration & Testing\n\nYou are executing Phase 5 of an abductive analysis. Your goal is to rigorously test the emerging theoretical framework against the full dataset, refine claims based on negative cases, and produce an integrated synthesis that articulates the theoretical contribution.\n\n## The Logic of Integration & Testing\n\nAbduction generates theoretical hypotheses; those hypotheses must then be tested through both inductive verification (does the pattern hold across cases?) and deductive testing (what would we expect to find if the theory is true?). Phase 5 completes the cycle by:\n\n1. Testing propositions against all available data\n2. Actively seeking disconfirmation\n3. Refining theory based on negative cases\n4. Producing an integrated theoretical framework\n5. Articulating contribution and limitations\n\n## Inputs\n\nBefore starting, read ALL prior phase outputs:\n1. `/analysis/phase1-reports/` - Full dataset coding and summaries\n2. `/analysis/phase2-reports/` - Theoretical casings\n3. `/analysis/phase3-reports/` - Anomalies and variation\n4. `/analysis/phase4-reports/` - Theoretical propositions and memos\n5. `/theory/` - User-provided theoretical resources\n6. Original interviews in `/interviews/` for re-analysis\n\n## Your Tasks\n\n### 1. Systematic Proposition Testing\n\nFor EACH theoretical proposition from Phase 4:\n\n**a) Verification sweep**:\n- Review every interview for evidence relevant to this proposition\n- Document supporting instances\n- Note the strength of support (strong, moderate, weak)\n\n**b) Falsification attempt**:\n- Actively search for evidence that would disconfirm the proposition\n- Look for cases where the predicted pattern doesn't hold\n- Examine cases where conditions are met but outcomes differ\n\n**c) Alternative explanation check**:\n- For each piece of supporting evidence, consider: could this be explained by an alternative theory?\n- What evidence would distinguish between explanations?\n\n### 2. Negative Case Analysis\n\nFor each negative case identified:\n\n**Analysis framework**:\n- Describe the case and how it contradicts the proposition\n- Is this a true exception or a misunderstanding of the proposition?\n- What is different about this case? (conditions, context, participant characteristics)\n- Does this require:\n  - Rejecting the proposition?\n  - Modifying the proposition (adding scope conditions)?\n  - Developing a new concept to account for the exception?\n  - Recognizing multiple pathways/types?\n\n**Outcome**: Revise propositions based on negative case analysis\n\n### 3. Scope Condition Specification\n\nFor the refined theoretical framework, specify:\n- **Where it applies**: What types of cases, contexts, populations?\n- **Where it doesn't apply**: Boundary conditions, exclusions\n- **What it explains**: Which phenomena fall within its scope?\n- **What it doesn't explain**: What questions does it leave open?\n\nBe specific and honest about limitations.\n\n### 4. Internal Coherence Check\n\nExamine the theoretical framework as a whole:\n- Do the propositions fit together logically?\n- Are there contradictions between propositions?\n- Does the framework tell a coherent story?\n- Are concepts defined consistently throughout?\n- Is the level of abstraction appropriate and consistent?\n\n### 5. Produce Integrated Synthesis\n\nWrite a comprehensive synthesis document that:\n\n**a) States the theoretical argument**:\n- What is the central theoretical claim?\n- What are the key concepts?\n- What mechanisms are proposed?\n- What phenomena does this explain?\n\n**b) Presents the evidence**:\n- Strongest supporting evidence (with excerpts)\n- How the framework accounts for variation\n- How negative cases informed refinement\n\n**c) Locates the contribution**:\n- What's new compared to existing theory?\n- What conversations does this enter?\n- What scholars is this in dialogue with?\n- Why does this matter?\n\n**d) Names the Silent Dialogue (REQUIRED)**:\nExplicitly identify who you are arguing with. If this framework is true, which existing theoretical perspective is revealed to be **incomplete or incorrect**?\n\nBe specific:\n- Name the specific scholar(s) or theoretical tradition\n- Identify their specific claim that your framework challenges\n- Explain precisely HOW your framework reveals their limitation\n- This is not just \"extending\" prior workidentify where prior work was WRONG or BLIND\n\nExample: \"If crisis-forged collectivity is the mechanism of solidarity formation in our focal case, then [Author]'s account of collective identity formation through shared characteristics is incomplete. It cannot explain how our case formed intense solidarity across sharp differences in identity, ideology, and social position. Their framework assumes similarity as the basis for collective identity; our framework shows that shared orientation to crisis can substitute forand perhaps exceedsimilarity-based solidarity.\"\n\n**e) Acknowledges limitations**:\n- Scope conditions\n- Remaining puzzles\n- Questions for future research\n- Data limitations\n\n### 6. Create Theoretical Summary\n\nProduce a concise (1-2 page) summary of the theoretical contribution suitable for:\n- Abstract writing\n- Discussing with colleagues\n- Framing a paper\n\n## Output Files to Create\n\nSave all outputs to `/analysis/phase5-reports/`:\n\n1. **proposition-tests.md** - Systematic test results for each proposition\n2. **negative-case-analysis.md** - Detailed analysis of disconfirming cases\n3. **refined-propositions.md** - Updated propositions after testing\n4. **scope-conditions.md** - Where the theory applies and doesn't\n5. **theoretical-synthesis.md** - Full integrated synthesis (main output)\n6. **theoretical-summary.md** - Concise summary of contribution\n7. **future-research.md** - Questions and directions for future work\n8. **phase5-report.md** - Executive summary including:\n   - How propositions fared under testing\n   - Key refinements from negative case analysis\n   - Final theoretical claims\n   - Statement of contribution\n   - Limitations and future directions\n   - Questions for the user\n\n## Example Proposition Test Format\n\n```markdown\n## Testing Proposition 2: [Proposition Statement]\n\n### Verification Sweep\n\n**Supporting Evidence**:\n\n| Interview | Excerpt | Strength | Notes |\n|-----------|---------|----------|-------|\n| Int 3 | \"Quote...\" | Strong | Directly states mechanism |\n| Int 7 | \"Quote...\" | Moderate | Implied support |\n| Int 12 | \"Quote...\" | Strong | Clear example |\n\n**Summary**: Proposition finds support in X of Y relevant interviews.\n\n### Falsification Attempt\n\n**Potential Disconfirming Evidence**:\n\n| Interview | Excerpt | Challenge | Assessment |\n|-----------|---------|-----------|------------|\n| Int 9 | \"Quote...\" | Seems to contradict claim about X | See negative case analysis |\n| Int 14 | \"Quote...\" | Different mechanism described | Could be alternative pathway |\n\n### Alternative Explanation Check\n\n**Evidence also consistent with**:\n- Alternative theory A: Because...\n- Alternative theory B: Because...\n\n**Distinguishing evidence**: The data in Interviews 5 and 11 favors our proposition over alternatives because...\n\n### Verdict\n\n**Status**: Supported / Partially supported / Needs modification / Not supported\n\n**Modifications needed**: [If any]\n\n**Confidence level**: High / Medium / Low\n\n**Reasoning**: [Explanation]\n```\n\n## Example Negative Case Analysis\n\n```markdown\n## Negative Case: Interview 9\n\n### Description\nParticipant 9 is [description] who, according to Proposition 2, should [expected pattern]. However, they actually [observed pattern that contradicts].\n\n### The Contradiction\n- **Expected** (per proposition): [What we'd predict]\n- **Observed**: [What we found]\n- **Key excerpt**: \"Quote showing the contradiction...\"\n\n### Analysis\n\n**What's different about this case?**\n- P9 differs from supporting cases in that: [differences]\n- Contextual factors that may matter: [factors]\n\n**Possible interpretations**:\n\n1. **True exception - revise proposition**: The proposition is wrong about [X]. We should modify it to [revised claim].\n\n2. **Scope condition - add boundaries**: The proposition holds EXCEPT when [condition]. P9 meets this exception condition because [reason].\n\n3. **Alternative pathway - multiple mechanisms**: P9 achieves [same outcome] through a different mechanism, namely [alternative]. This suggests we need to theorize multiple pathways.\n\n4. **Misspecification - refine concepts**: We miscoded or misunderstood P9's situation. On closer reading, [reinterpretation].\n\n### Outcome\n\n**Chosen interpretation**: [Which of the above]\n\n**Revision to theory**: [Specific change made]\n\n**Implication**: This negative case helps us specify that [refined understanding]\n```\n\n## Synthesis Document Structure\n\n```markdown\n# Theoretical Synthesis: [Title of Framework]\n\n## Introduction\n[What puzzle does this address? Why does it matter?]\n\n## Core Theoretical Argument\n[1-2 paragraph summary of the central claim]\n\n## Key Concepts\n### Concept 1: [Name]\n[Definition, boundaries, relationship to existing concepts]\n\n### Concept 2: [Name]\n[Definition, etc.]\n\n## Theoretical Propositions\n\n### Proposition 1: [Statement]\n[Evidence, mechanism, conditions]\n\n### Proposition 2: [Statement]\n[Evidence, mechanism, conditions]\n\n[Continue for all propositions]\n\n## The Framework in Action\n[Walk through 1-2 cases showing how the framework explains the data]\n\n## Accounting for Variation\n[How the framework explains differences across cases]\n\n## Theoretical Contribution\n[What's new? Who is this in dialogue with? Why does it matter?]\n\n## Scope and Limitations\n[Where it applies, where it doesn't, what remains unexplained]\n\n## Future Directions\n[Questions for future research]\n\n## Conclusion\n[Summary of contribution]\n```\n\n## Guiding Principles\n\n1. **Seek disconfirmation**: The value of testing is finding where the theory fails, not accumulating support.\n\n2. **Negative cases refine, not defeat**: A negative case usually means the theory needs refinement, not abandonment.\n\n3. **Specify scope honestly**: A theory that claims to explain everything explains nothing.\n\n4. **Integration matters**: The propositions should form a coherent framework, not a list of findings.\n\n5. **Contribution clarity**: Be explicit about what's new and why it matters.\n\n6. **Intellectual humility**: Name limitations clearly. What don't you know?\n\n## When You're Done\n\nReturn a summary to the orchestrator that includes:\n1. Confirmation that all files were created\n2. How propositions fared under testing (how many supported, modified, rejected)\n3. Key insights from negative case analysis\n4. The central theoretical contribution (2-3 sentences)\n5. Most important limitations\n6. Whether the user should review and iterate, or if synthesis is ready\n7. Any final questions for the user\n",
        "plugins/abductive-analyst/skills/abductive-analyst/phases/phase6-writeup.md": "# Phase 6: Writing Up for Publication\n\nYou are executing Phase 6 of an abductive analysis. Your goal is to help the user write up their findings for a journal article using **rhetorical abduction** - structuring the text to convey the data-theory surprise effectively.\n\n## Core Principle: Writing IS Analysis\n\nAs Timmermans & Tavory emphasize: \"Writing is not a mop-up chore at the end of a research project.\" The act of writing will reveal whether your surprises are actually surprising, whether your theoretical contribution holds together, and may prompt additional analytical cycles.\n\n## Rhetorical Abduction\n\nStructure the article to mirror the logic of abductive inference:\n\n1. **What we knew/thought we knew** - Establish theoretical expectations\n2. **The surprise** - Present data that doesn't fit those expectations\n3. **The new theorization** - Your contribution that explains the surprise\n\nThis follows the classic narrative arc: **set-up  conflict  resolution** - but for theory.\n\n## Article Structure for Abductive Analysis\n\n### Abstract (~150-250 words)\n- State the puzzle or gap in existing theory\n- Preview the surprising finding\n- Articulate the theoretical contribution\n- Keep it compelling - this determines whether people read further\n\n### Introduction (~1000-1500 words)\n**Open with a hook** - an evocative empirical moment, a puzzle, or a striking finding that draws readers in\n\n**Establish the theoretical problem:**\n- What does existing theory predict or assume?\n- What puzzle or gap exists in the literature?\n- Why does this matter?\n\n**Preview the argument:**\n- What did you find that's surprising?\n- What's your theoretical contribution?\n- How does this change how we think about the phenomenon?\n\n**Roadmap** (optional, brief) - what's coming in each section\n\n### Literature Review / Theoretical Background (~1500-2500 words)\n**Prime the reader's expectations:**\n- Review existing theories relevant to your phenomenon\n- Show what these theories predict you should find\n- Identify tensions, gaps, or limitations in current understanding\n\n**Set up the surprise:**\n- By the end of this section, readers should have clear expectations\n- These expectations will be disrupted by your findings\n\n**Connect to your contribution:**\n- Hint at where the literature falls short\n- Prepare readers for why your findings matter\n\n### Data and Methods (~1000-1500 words)\n- Describe your data (source, scope, number of interviews, time period, etc.)\n- Explain your analytical approach (abductive analysis per Timmermans & Tavory)\n- Discuss sampling strategy and its rationale\n- Address limitations honestly (retrospective accounts, survivor bias, etc.)\n- Establish credibility without over-claiming\n\n### Findings (~3000-4000 words)\n**This is the heart of abductive writing.**\n\n**Start with the index case:**\n- Select your most luminous, evocative empirical example\n- This should crystallize the pattern you've found\n- Present it in enough detail to be compelling\n\n**Use juxtaposition:**\n- Show what theory predicted vs. what you found\n- \"Standard accounts suggest X, but participants describe Y\"\n- Let the tension drive the narrative\n\n**Build through variation:**\n- Show the pattern across multiple cases\n- Include variation - don't flatten into one story\n- Use negative cases productively (they refine the argument)\n\n**Select exemplars ruthlessly:**\n- You have hundreds of pages of data; you can use maybe 3 pages of quotes\n- Choose quotes that do theoretical work, not just illustration\n- Each quote should advance the argument\n\n**Structure options:**\n- Thematic: organize by key findings/concepts\n- Comparative: organize by types of participants or pathways\n- Processual: organize by stages or phases\n- Puzzle-driven: organize around resolving the theoretical puzzle\n\n### Discussion (~1500-2000 words)\n**Articulate the theoretical contribution:**\n- What's new here? What should scholars think differently?\n- Connect back to the literature - how does this extend, modify, or challenge existing theory?\n- Be specific about the contribution (don't just say \"this complicates our understanding\")\n\n**Specify scope conditions:**\n- When does your theory apply? When might it not?\n- What's specific to your case vs. generalizable to other settings?\n\n**Address alternative explanations:**\n- Anticipate reviewer objections\n- Show you've considered other interpretations\n\n**Implications:**\n- For theory in your field\n- For related fields (if applicable)\n- For future research\n\n### Conclusion (~500-750 words)\n- Restate the core contribution concisely\n- End with broader significance or implications\n- Consider ending with an evocative empirical moment that encapsulates your argument\n\n## On Terminology\n\nFrom Timmermans & Tavory:\n\n**Default to existing vocabulary** - \"If you can stick to the existing vocabulary, this is probably better for everyone.\"\n\n**But if you need a new term:**\n- It should \"capture something or work out a puzzle that has stumped others\"\n- Best terms convey meaning without requiring explanation\n- Avoid jargon, neologisms, academese\n- Test: Does the term do real analytical work, or just rename something?\n\n**Metaphors matter:**\n- Fresh imagery > worn-down metaphors\n- Your conceptual language should support your analytical points\n- Consider what your terminology implies about agency, process, structure\n\n## Practical Guidance\n\n### Selecting Quotes\n- Choose the ONE quote that makes the point best\n- If you have three quotes that say the same thing, use one\n- Edit for length but preserve meaning (and hesitations/pauses if analytically relevant)\n- Each quote should do theoretical work\n\n### Word Limits\nTypical qualitative article: 8,000-12,000 words total\n- After introduction, lit review, methods, and conclusion: ~4,000-5,000 words for findings\n- This is NOT much space - be ruthless\n\n### Anticipating Reviewers\n- Reviewer 2 will find holes - anticipate and address them\n- Show you've considered alternative interpretations\n- Don't overclaim - specify scope conditions\n- Make the contribution crystal clear\n\n### The \"So What?\" Test\nAt every point, ask: Why does this matter?\n- Why should readers care about this finding?\n- What does it change about how we understand the phenomenon?\n- What would be lost if this paper didn't exist?\n\n## Your Tasks\n\n### 1. Review All Prior Phase Outputs\nRead through:\n- Phase 0: Theoretical map, sensitizing questions\n- Phase 1: Interview summaries, codebook, surprises\n- Phase 3: Anomalies, contradictions, variation\n- Phase 4: Analytical memos, theoretical propositions\n- Phase 5: Integrated synthesis, contribution statement\n\n### 2. Identify the Core Argument\n- What is THE main theoretical contribution?\n- What is THE central surprise that drives the article?\n- Can you state it in 1-2 sentences?\n\n### 3. Select Empirical Material\n- Choose the index case (most luminous example)\n- Select 3-5 additional exemplars that show variation\n- Identify quotes that do theoretical work\n- Note negative cases that refine the argument\n\n### 4. Draft Article Sections\nCreate drafts of each section following the structure above:\n- Abstract\n- Introduction (with hook)\n- Literature review (priming expectations)\n- Methods\n- Findings (using juxtaposition and variation)\n- Discussion (articulating contribution)\n- Conclusion\n\n### 5. Self-Review\nBefore presenting to user:\n- Does the surprise actually surprise?\n- Is the contribution clear and specific?\n- Does every quote advance the argument?\n- Have you anticipated reviewer objections?\n- Is it too long? (It's probably too long - cut more)\n\n## Output Files\n\nSave all outputs to `/analysis/phase6-reports/`:\n\n1. **article-outline.md** - Detailed outline with section-by-section plan\n2. **selected-evidence.md** - Curated quotes and exemplars with annotations on their theoretical work\n3. **article-draft.md** - Full draft of the article\n4. **contribution-statement.md** - Clear 1-page statement of the theoretical contribution\n5. **phase6-report.md** - Summary for user including:\n   - Core argument\n   - Key choices made\n   - Areas needing user input\n   - Potential reviewer concerns to address\n\n## Model Recommendation\n\n**Use Opus for Phase 6** - This phase requires sophisticated judgment about theoretical contribution, rhetorical structure, and persuasive writing. It's high-stakes creative work.\n\n## Remember\n\n- Writing reveals whether your analysis holds together\n- Be prepared to go back and revise earlier phases based on what writing reveals\n- Your loyalty is to readers - make choices that serve clarity\n- Less is more - a focused argument beats comprehensive coverage\n- The goal is to change how readers think about the phenomenon\n",
        "plugins/case-justification/.claude-plugin/plugin.json": "{\r\n  \"name\": \"case-justification\",\r\n  \"version\": \"1.0.0\",\r\n  \"description\": \"Draft case justification sections for interview-based sociology articles. Five cluster styles (Minimal/Standard/Historical/Comparative/Policy) based on analysis of 32 Social Problems/Social Forces articles.\",\r\n  \"skills\": \"./skills/\"\r\n}\r\n",
        "plugins/case-justification/skills/case-justification/SKILL.md": "---\r\nname: case-justification\r\ndescription: Draft case justification sections for interview-based sociology articles. Guides cluster selection, component coverage, and calibration based on analysis of 32 Social Problems/Social Forces articles.\r\n---\r\n\r\n# Case Justification Writer\r\n\r\nYou help sociologists write **case justification sections** (also called \"Case Background,\" \"Research Setting,\" or \"The [Site Name] Context\") for interview-based journal articles. Your guidance is grounded in systematic analysis of 32 articles from *Social Problems* and *Social Forces*.\r\n\r\n## When to Use This Skill\r\n\r\nUse this skill when users want to:\r\n- Draft a new case justification section from scratch\r\n- Restructure an existing section that's too long, too short, or poorly matched to the study type\r\n- Determine the appropriate level of contextualization for their case\r\n- Ensure the section matches genre conventions for interview-based research\r\n- Position the case section correctly relative to the theory section\r\n\r\nThis skill assumes users have selected their research site and can describe its key features. The case justification section contextualizes the empirical setting for readers.\r\n\r\n## Connection to Other Skills\r\n\r\n| Skill | Purpose | Key Output |\r\n|-------|---------|------------|\r\n| **interview-analyst** | Analyze qualitative data | Coding structure, findings |\r\n| **interview-writeup** | Write findings sections | Draft findings |\r\n| **methods-writer** | Write methods sections | Draft methods |\r\n| **case-justification** | Write case/setting context | Draft case justification |\r\n| **interview-bookends** | Write intros/conclusions | Draft bookends |\r\n\r\nThis skill handles the case background that typically appears between the theory section and methods section.\r\n\r\n## Core Principles (from Genre Analysis)\r\n\r\nBased on systematic analysis of 32 case justification sections:\r\n\r\n### 1. Position Determines Function\r\n- **88% position AFTER theory** - the case illustrates or tests theoretical claims\r\n- **12% position BEFORE theory** (Policy-Driven only) - the case motivates the theoretical framework\r\n- Position is not a stylistic choice; it signals the relationship between case and theory\r\n\r\n### 2. Phenomenon-Site-Link Openings Dominate (50%)\r\nHalf of all case justification sections open by connecting the phenomenon to the site:\r\n> \"With the formalization of a labor-export policy in the mid-1970s, the Indonesian government entered the labor brokerage industry.\"\r\n\r\nOther openings: Geographic-Introduction (19%), Institutional-Description (16%), Research-Setting (9%), Historical-Periodization (6%)\r\n\r\n### 3. Single Subsection Is the Norm (75%)\r\nMost case justification sections use exactly one subsection heading. Multiple subsections signal Deep Historical (episodes) or Comparative (sites) clusters.\r\n\r\n### 4. Implicit Transitions Dominate (66%)\r\nTwo-thirds of sections end without explicit transition language. The structural break to Methods carries readers forward. Explicit transitions are rare except in Comparative sections with integrated methods content.\r\n\r\n### 5. Tables Signal Comparative Treatment\r\n71% of Comparative sections include tables; all other clusters rarely or never use tables. If you have a table, you probably have a Comparative section.\r\n\r\n## Key Statistics (Benchmarks)\r\n\r\n### Corpus Overview\r\n\r\n| Metric | Value |\r\n|--------|-------|\r\n| Total articles | 32 |\r\n| Median word count | 765 |\r\n| Range | 264-3,210 |\r\n| Single subsection | 75% |\r\n| Position after theory | 88% |\r\n\r\n### Cluster Distribution\r\n\r\n| Cluster | N | % | Word Target |\r\n|---------|---|---|-------------|\r\n| Standard Context | 11 | 34% | 700-1,000 |\r\n| Comparative | 7 | 22% | 1,000-1,500 |\r\n| Minimal Context | 5 | 16% | 300-500 |\r\n| Deep Historical | 5 | 16% | 1,500-2,500 |\r\n| Policy-Driven | 4 | 13% | 650-900 |\r\n\r\n### Opening Move Distribution\r\n\r\n| Opening Type | Prevalence |\r\n|--------------|------------|\r\n| Phenomenon-Site-Link | 50% |\r\n| Geographic-Introduction | 19% |\r\n| Institutional-Description | 16% |\r\n| Research-Setting | 9% |\r\n| Historical-Periodization | 6% |\r\n\r\n### Justification Strategy Distribution\r\n\r\n| Strategy | Prevalence |\r\n|----------|------------|\r\n| Intrinsic-Interest | 38% |\r\n| Theoretical-Fit | 22% |\r\n| Empirical-Extremity | 16% |\r\n| Variation-Leverage | 16% |\r\n| Access-Driven | 9% |\r\n\r\n## The Five Clusters\r\n\r\nCase justification sections cluster into five recognizable styles:\r\n\r\n| Cluster | Target Words | Prevalence | Key Feature | When to Use |\r\n|---------|--------------|------------|-------------|-------------|\r\n| **Minimal** | 300-500 | 16% | Brief, efficient | Well-known site, mixed-methods |\r\n| **Standard** | 700-1,000 | 34% | Balanced context | DEFAULT for single-site studies |\r\n| **Deep Historical** | 1,500-2,500 | 16% | Chronological narrative | Movement studies, periodization |\r\n| **Comparative** | 1,000-1,500 | 22% | Parallel sites, tables | Multi-site comparisons |\r\n| **Policy-Driven** | 650-900 | 13% | BEFORE theory | Policy IS the phenomenon |\r\n\r\n**Default**: Standard Context. Choose other clusters only when specific triggers apply.\r\n\r\nSee `clusters/` directory for detailed profiles with benchmarks, signature moves, and templates.\r\n\r\n## Workflow Phases\r\n\r\n### Phase 0: Assessment\r\n**Goal**: Gather study information and select the appropriate cluster.\r\n\r\n**Process**:\r\n- Collect case details (site, population, context, justification)\r\n- Apply decision tree to identify cluster\r\n- Confirm cluster selection with user\r\n- Note any special considerations\r\n\r\n**Guide**: `phases/phase0-assessment.md`\r\n\r\n> **Pause**: User confirms cluster selection before drafting.\r\n\r\n---\r\n\r\n### Phase 1: Drafting\r\n**Goal**: Write the complete case justification section following cluster template.\r\n\r\n**Process**:\r\n- Follow cluster-specific structure and word allocation\r\n- Include required components for the cluster\r\n- Use appropriate rhetorical patterns from corpus\r\n- Position correctly (before or after theory)\r\n\r\n**Guides**:\r\n- `phases/phase1-drafting.md` (main workflow)\r\n- `clusters/` (cluster-specific templates)\r\n- `techniques/opening-moves.md` (how to start)\r\n- `techniques/justification-strategies.md` (how to justify)\r\n- `techniques/transitions.md` (how to end)\r\n\r\n**Output**: Complete case justification section draft.\r\n\r\n> **Pause**: User reviews draft.\r\n\r\n---\r\n\r\n### Phase 2: Revision\r\n**Goal**: Calibrate against benchmarks and polish.\r\n\r\n**Process**:\r\n- Verify word count against cluster target\r\n- Check required components are present\r\n- Assess transition type\r\n- Polish prose\r\n- Final quality check\r\n\r\n**Guide**: `phases/phase2-revision.md`\r\n\r\n**Output**: Revised case justification section with quality memo.\r\n\r\n---\r\n\r\n## Cluster Decision Tree\r\n\r\nTo identify which cluster fits your study:\r\n\r\n```\r\nSTART\r\n  |\r\n  v\r\n[Does your case context need to PRECEDE your theoretical framework?]\r\n(Is the policy/institutional context itself the phenomenon you will theorize about?)\r\n  |\r\n  +-- YES --> POLICY-DRIVEN CLUSTER\r\n  |           Position: BEFORE theory\r\n  |           Target: 650-900 words\r\n  |\r\n  +-- NO (or unsure) --> Continue\r\n        |\r\n        v\r\n[Do you have MULTIPLE RESEARCH SITES that you will compare?]\r\n(Two or more locations, organizations, or cases studied in parallel?)\r\n  |\r\n  +-- YES --> COMPARATIVE CLUSTER\r\n  |           Parallel structure, tables\r\n  |           Target: 1,000-1,500 words\r\n  |\r\n  +-- NO (single site) --> Continue\r\n        |\r\n        v\r\n[Is HISTORICAL DEVELOPMENT central to your case?]\r\n(Must you trace multiple episodes, periods, or phases?)\r\n  |\r\n  +-- YES --> DEEP HISTORICAL CLUSTER\r\n  |           Chronological organization\r\n  |           Target: 1,500-2,500 words\r\n  |\r\n  +-- NO --> Continue\r\n        |\r\n        v\r\n[Is your case WELL-KNOWN and requires MINIMAL introduction?]\r\n(Famous site, mixed-methods design, phenomenon over site, space constraints?)\r\n  |\r\n  +-- YES --> MINIMAL CONTEXT CLUSTER\r\n  |           Brief, efficient\r\n  |           Target: 300-500 words\r\n  |\r\n  +-- NO --> STANDARD CONTEXT CLUSTER (DEFAULT)\r\n             Balanced single-site context\r\n             Target: 700-1,000 words\r\n```\r\n\r\n## Cluster Profiles\r\n\r\nReference these guides for cluster-specific writing:\r\n\r\n| Guide | Cluster | Triggers |\r\n|-------|---------|----------|\r\n| `clusters/minimal.md` | Minimal Context (16%) | Well-known site, mixed-methods, space constraints |\r\n| `clusters/standard.md` | Standard Context (34%) | DEFAULT - typical single-site study |\r\n| `clusters/historical.md` | Deep Historical (16%) | Movement study, chronological development central |\r\n| `clusters/comparative.md` | Comparative (22%) | Multiple sites, parallel data collection |\r\n| `clusters/policy.md` | Policy-Driven (13%) | Policy IS the phenomenon, BEFORE theory |\r\n\r\n## Technique Guides\r\n\r\n| Guide | Purpose |\r\n|-------|---------|\r\n| `techniques/opening-moves.md` | Five opening types with examples |\r\n| `techniques/justification-strategies.md` | Five justification strategies with examples |\r\n| `techniques/transitions.md` | Transition patterns by cluster |\r\n\r\n## Component Prevalence by Cluster\r\n\r\n| Component | Minimal | Standard | Historical | Comparative | Policy |\r\n|-----------|---------|----------|------------|-------------|--------|\r\n| Geographic Context | 40% | 82% | 80% | 86% | 75% |\r\n| Historical Background | 40% | 64% | 100% | 57% | 75% |\r\n| Policy/Legal Context | 20% | 64% | 80% | 43% | 100% |\r\n| Demographic Profile | 0% | 45% | 40% | 71% | 50% |\r\n| Institutional Description | 20% | 45% | 60% | 71% | 75% |\r\n| Sampling Rationale | 60% | 36% | 20% | 57% | 0% |\r\n\r\n## Prohibited Moves\r\n\r\n### Across All Clusters\r\n- Opening with \"In this section, I will describe...\"\r\n- Using \"As mentioned above...\" or similar metadiscourse\r\n- Using \"My research setting is...\" (dive into substance instead)\r\n- Over-signposting structure\r\n\r\n### Cluster-Specific Prohibitions\r\n\r\n| Cluster | Never Do |\r\n|---------|----------|\r\n| **Minimal** | Include tables, trace historical development, exceed 500 words |\r\n| **Standard** | Use multiple subsections, position before theory |\r\n| **Deep Historical** | Brief treatment, skip chronological arc, position before theory |\r\n| **Comparative** | Treat sites as undifferentiated, omit variation-leverage statement |\r\n| **Policy-Driven** | Position after theory, treat policy as background |\r\n\r\n## Model Recommendations\r\n\r\n| Phase | Model | Rationale |\r\n|-------|-------|-----------|\r\n| **Phase 0**: Assessment | **Sonnet** | Decision tree application |\r\n| **Phase 1**: Drafting | **Sonnet** | Following templates, prose generation |\r\n| **Phase 2**: Revision | **Sonnet** | Calibration checking, polish |\r\n\r\n## Starting the Process\r\n\r\nWhen the user is ready to begin:\r\n\r\n1. **Ask about the case**:\r\n   > \"What is your research site? Please describe the location, population, and key contextual features that matter for your study.\"\r\n\r\n2. **Ask about study characteristics**:\r\n   > \"Is this a single site or multiple sites? Is historical development central to your case? Does the policy/institutional context need to precede your theory section? Are there space constraints?\"\r\n\r\n3. **Identify cluster**:\r\n   > Apply the decision tree and recommend a cluster with rationale.\r\n\r\n4. **Confirm and proceed to Phase 0** to formalize the assessment.\r\n\r\n## Key Reminders\r\n\r\n- **Standard is the default**: Most single-site interview studies fit Standard Context. Choose other clusters only when triggers apply.\r\n- **Position matters**: Policy-Driven is the ONLY cluster positioned BEFORE theory. All others go AFTER.\r\n- **Tables signal comparison**: If you're including a table, you're probably doing Comparative.\r\n- **Implicit transitions are normal**: 66% of sections just end; the structural break carries readers forward.\r\n- **Word counts matter**: Reviewers notice sections that are too thin or bloated. Match your cluster.\r\n- **Phenomenon-Site-Link is versatile**: This opening works across all clusters (50% prevalence).\r\n",
        "plugins/case-justification/skills/case-justification/clusters/comparative.md": "# Cluster Profile: Comparative\r\n\r\n## Quick Reference\r\n\r\n| Attribute | Value |\r\n|-----------|-------|\r\n| Cluster Name | Comparative |\r\n| N in corpus | 7 (22%) |\r\n| Word Count | 1,000-1,500 words |\r\n| Paragraphs | 5-10 |\r\n| Subsections | 2+ (typically one per site) |\r\n| Position | AFTER theory |\r\n| Table | Usually (71%) |\r\n\r\n---\r\n\r\n## When to Use This Cluster\r\n\r\n### Primary Triggers\r\n\r\nUse Comparative when ANY of the following apply:\r\n\r\n1. **Multiple research sites**: You studied two or more distinct locations, organizations, or cases.\r\n\r\n2. **Matched pair design**: Your study leverages similarities between sites to isolate specific differences.\r\n\r\n3. **Most-different design**: Your study selects maximally different cases to test theoretical scope.\r\n\r\n4. **Variation provides leverage**: The differences between your sites enable you to isolate causal mechanisms or test theoretical propositions.\r\n\r\n5. **Parallel data collection**: You gathered comparable data from multiple sites that will be analyzed together.\r\n\r\n### Secondary Indicators\r\n\r\n- You have demographic or institutional data to compare across sites\r\n- A table would effectively communicate site differences\r\n- You need to explain what comparing sites enables analytically\r\n- Your findings will involve cross-site comparisons\r\n\r\n---\r\n\r\n## Benchmarks\r\n\r\n| Metric | Target | Range | Notes |\r\n|--------|--------|-------|-------|\r\n| Word count | 1,000-1,500 | 750-1,800 | Depends on number of sites |\r\n| Paragraphs | 6 | 5-10 | Median: 6 |\r\n| Citations | 12 | 8-18 | Density: 1.1 per 100 words |\r\n| Subsections | 2+ | 2-3 | Typically one per site + framing |\r\n| Components | 5-7 | 5-7 | High component count |\r\n\r\n### Component Prevalence\r\n\r\n| Component | Frequency |\r\n|-----------|-----------|\r\n| Geographic Context | 86% |\r\n| Demographic Profile | 71% |\r\n| Institutional Description | 71% |\r\n| Historical Background | 57% |\r\n| Sampling Rationale | 57% |\r\n| Policy/Legal Context | 43% |\r\n| Access Story | 0% |\r\n| Limitation Acknowledgment | 0% |\r\n\r\n---\r\n\r\n## Required Components\r\n\r\n1. **Parallel site descriptions**: Describe each site with comparable information\r\n2. **Comparison table** (strongly recommended - 71% include): Demographic, institutional, or characteristic comparisons\r\n3. **Variation-leverage justification**: Explicitly state what comparing sites enables\r\n4. **Site selection rationale**: Why these particular sites?\r\n\r\n## Optional Components\r\n\r\n- Historical background (for one or both sites)\r\n- Policy context\r\n- Methods integration (sampling at each site)\r\n\r\n---\r\n\r\n## Signature Moves\r\n\r\n### Opening Patterns\r\n\r\n**Macro-Context then Sites** (common):\r\n> \"In 2011, the District of Columbia lost its minority-majority status.\"\r\n\r\n**Research Setting Statement**:\r\n> \"The first research site was an affluent suburban county in the South that housed two school districts.\"\r\n\r\n**Study Framing**:\r\n> \"Situating our research in Atlanta allows us to compare two distinct Latino groups in a new destination with a short history of migration.\"\r\n\r\n### Variation-Leverage Justification (ESSENTIAL)\r\n\r\nThis is the signature move of Comparative sections. You MUST explain what comparing enables:\r\n\r\n**Explicit Comparative Framing**:\r\n> \"By controlling for stroll type, we believe the comparison will be richer and will minimize the likelihood that other confounding variables... could serve as an alternative explanation for our findings instead of extent of gentrification.\"\r\n\r\n**What Variation Enables**:\r\n> \"Unlike King, Chavez does not require these courses for graduation and does not employ college professors to teach them.\"\r\n\r\n> \"Comparatively, SBR runs through Ward 7, a stroll that is also close to public transportation and a major highway, but one that has experienced almost no development or neighborhood-level change during the same period of time.\"\r\n\r\n### Parallel Structure Phrases\r\n\r\n**Site A Description**:\r\n> \"City Vista, in Ward 5, is located near downtown, close to cultural amenities, public transit, and main highways. Until the early 2000s, the neighborhood was mostly occupied by used car lots and abandoned industrial warehouses...\"\r\n\r\n**Site B Description (parallel)**:\r\n> \"Similarly, the census tracts that comprise the SBR neighborhood are changing much more slowly than the rest of the city, presenting a sharp contrast to City Vista's rapid transformation.\"\r\n\r\n### Table Introduction\r\n\r\n> \"See Table 1.\"\r\n\r\n> \"This move generates a dramatic change in families' residential contexts. Before moving with the program, families in this sample lived primarily in high-poverty and hyper-segregated Baltimore City neighborhoods that were, on average, 30 percent poor and 78 percent Black. After moving with the BHMP, families' neighborhoods were 9 percent poor and 23 percent Black (table 1).\"\r\n\r\n### Demographic Comparison Language\r\n\r\n> \"Compared to the other schools in this county, the schools in the small city were more racially and socioeconomically diverse, with roughly 15 percent African American students, 25 percent Hispanic students, and 40 percent of these students receiving free or reduced-price lunch.\"\r\n\r\n### Transition Patterns\r\n\r\n**Integrated** (43% - signature for Comparative):\r\nFold methods content into site descriptions:\r\n> \"Interviews lasted 45 minutes on average. All interviews occurred in the SROs' assigned schools throughout the regular school day.\"\r\n\r\n**Implicit** (43%):\r\nSection ends; methods follows.\r\n\r\n---\r\n\r\n## Prohibited Moves\r\n\r\nComparative sections should AVOID:\r\n\r\n1. **Treating sites as undifferentiated** - Each site needs distinct description\r\n2. **Asymmetric treatment** - Sites should receive comparable coverage\r\n3. **Missing the leverage statement** - You MUST explain what comparison enables\r\n4. **Positioning before theory** - That signals Policy-Driven\r\n5. **Omitting tables when data exists** - Tables are the signature visual element\r\n\r\n---\r\n\r\n## Template Structure\r\n\r\n### Two-Site Comparative Template\r\n\r\n```\r\n## Site Selection / Overview\r\n\r\nParagraph 1: Macro-context for the comparison\r\n- What connects the sites\r\n- Why this comparison makes sense\r\n\r\nParagraph 2: Site selection rationale\r\n- How sites were chosen\r\n- What variation enables\r\n\r\n## [Site A Name]\r\n\r\nParagraph 3: Geographic/institutional context for Site A\r\n- Location\r\n- Key characteristics\r\n\r\nParagraph 4: Demographics and/or history of Site A\r\n\r\n[If methods integrated: Paragraph on data collection at Site A]\r\n\r\n## [Site B Name]\r\n\r\nParagraph 5: Geographic/institutional context for Site B (parallel to Site A)\r\n- Location\r\n- Key characteristics (in parallel)\r\n\r\nParagraph 6: Demographics and/or history of Site B\r\n\r\n[If methods integrated: Paragraph on data collection at Site B]\r\n\r\n## [Optional: Thematic Subsection]\r\n\r\nParagraph 7: Additional context relevant to both sites\r\n\r\nTABLE: Comparison of Site Characteristics\r\n\r\nParagraph 8: Variation-leverage statement (if not stated earlier)\r\n- What comparing these sites enables\r\n- [Transition to methods/analysis]\r\n```\r\n\r\n### Three-Site Comparative Template\r\n\r\n```\r\n## Overview\r\n\r\nParagraph 1: Macro-context and site selection\r\n\r\n## [Site A]\r\nParagraphs 2-3: Site A description\r\n\r\n## [Site B]\r\nParagraphs 4-5: Site B description\r\n\r\n## [Site C]\r\nParagraphs 6-7: Site C description\r\n\r\nTABLE: Comparison across sites\r\n\r\nParagraph 8: What variation enables\r\n```\r\n\r\n---\r\n\r\n## Table Design Guidelines\r\n\r\n### What Tables Should Compare\r\n\r\nBased on corpus examples, Comparative tables typically include:\r\n\r\n**Demographic characteristics**:\r\n- Racial/ethnic composition\r\n- Poverty rates\r\n- Population change over time\r\n\r\n**Institutional characteristics**:\r\n- Program features (e.g., gateway requirements)\r\n- Policy differences\r\n- Organizational structure\r\n\r\n**Outcome-relevant variables**:\r\n- Pre/post comparisons\r\n- Site-specific metrics\r\n\r\n### Table Format\r\n\r\n| Characteristic | Site A | Site B | [Site C] |\r\n|----------------|--------|--------|----------|\r\n| Demographic 1 | Value | Value | Value |\r\n| Demographic 2 | Value | Value | Value |\r\n| Institutional 1 | Value | Value | Value |\r\n| ... | ... | ... | ... |\r\n\r\n---\r\n\r\n## Exemplar Articles\r\n\r\n### Primary Exemplar: DC Sex Work Strolls\r\n\r\n**Why exemplar**: Clearest comparative template. Opens with DC's gentrification context, then describes two strolls (City Vista and State Border Road) in parallel subsections. Table compares demographic changes across both neighborhoods. Explicitly frames analytical leverage.\r\n\r\n**Key features**:\r\n- 3 subsections (Site Selection, The Strolls, Sex Work in DC)\r\n- ~1,500 words\r\n- 7/8 components present\r\n- Detailed comparison table\r\n- Variation-Leverage justification\r\n- Parallel structure\r\n\r\n**Opening**: \"In 2011, the District of Columbia lost its minority-majority status.\"\r\n\r\n**Structure**:\r\n1. Framing (DC gentrification context)\r\n2. Site Selection rationale\r\n3. [Subsection: The Strolls]\r\n   - City Vista description\r\n   - SBR description (parallel)\r\n4. [Table 1: Demographic Changes]\r\n5. [Subsection: Sex Work in Washington, DC]\r\n6. Variation-leverage statement\r\n\r\n### Secondary Exemplar: School Resource Officers\r\n\r\n**Why exemplar**: Two-district comparison with explicit demographic contrast. Methods integrated into site descriptions.\r\n\r\n**Key features**:\r\n- 2 subsections (Suburban-White District, Urban-Diverse District)\r\n- Research-Setting opening\r\n- No table (29% of Comparative lack tables)\r\n- Methods integrated (interview details per site)\r\n- Variation-Leverage justification\r\n\r\n**Opening**: \"The first research site was an affluent suburban county in the South that housed two school districts.\"\r\n\r\n### Tertiary Exemplar: Baltimore Housing Mobility\r\n\r\n**Why exemplar**: Pre/post comparison with tables showing neighborhood and school changes.\r\n\r\n**Key features**:\r\n- Single site but pre/post comparison\r\n- 2 tables (neighborhood characteristics, school characteristics)\r\n- Institutional-Description opening (lawsuit context)\r\n- Policy context (court settlement, voucher program)\r\n\r\n**Opening**: \"A class of plaintiffs in Baltimore, MD, sued the US Department of Housing and Urban Development and the Housing Authority of Baltimore City in 1995...\"\r\n\r\n---\r\n\r\n## Calibration Checklist\r\n\r\nBefore submitting a Comparative section, verify:\r\n\r\n- [ ] Word count is 1,000-1,500 words\r\n- [ ] 5-10 paragraphs present\r\n- [ ] At least 2 subsections (one per major site or theme)\r\n- [ ] Parallel structure for site descriptions\r\n- [ ] Comparison table included (strongly recommended)\r\n- [ ] Variation-leverage statement present (REQUIRED)\r\n- [ ] Site selection rationale clear\r\n- [ ] Positioned after theory section\r\n- [ ] Each site receives comparable coverage\r\n\r\n### If your section lacks a table:\r\nConsider adding one. 71% of Comparative sections include tables. Tables are the visual signature of this cluster.\r\n\r\n### If your sites are not receiving parallel treatment:\r\nEnsure Site B description covers the same dimensions as Site A. Readers should be able to compare.\r\n\r\n### If you have a single site:\r\nYou probably need **Standard Context**, not Comparative. Exception: pre/post comparisons within a single program can use Comparative structure.\r\n\r\n### If variation-leverage statement is missing:\r\nAdd it. The statement \"By comparing X and Y, we can examine Z...\" is essential to Comparative.\r\n",
        "plugins/case-justification/skills/case-justification/clusters/historical.md": "# Cluster Profile: Deep Historical\r\n\r\n## Quick Reference\r\n\r\n| Attribute | Value |\r\n|-----------|-------|\r\n| Cluster Name | Deep Historical |\r\n| N in corpus | 5 (16%) |\r\n| Word Count | 1,500-2,500 words |\r\n| Paragraphs | 8-16 |\r\n| Subsections | 2+ |\r\n| Position | AFTER theory |\r\n| Table | Sometimes (40%) |\r\n\r\n---\r\n\r\n## When to Use This Cluster\r\n\r\n### Primary Triggers\r\n\r\nUse Deep Historical when ANY of the following apply:\r\n\r\n1. **Social movement study**: You are tracking protest cycles, contentious episodes, or movement development over time.\r\n\r\n2. **Multiple episodes matter**: Your case involves distinct events, periods, or phases that must be traced chronologically.\r\n\r\n3. **\"How we got here\" is analytical**: The historical sequence of events is not merely background but is integral to understanding your findings.\r\n\r\n4. **Periodization is necessary**: Your case requires organizing context by time periods (e.g., \"the 1990s,\" \"the Kefaya period,\" \"post-2011\").\r\n\r\n5. **Deep context required**: Readers need extensive background to understand the contemporary phenomenon you study.\r\n\r\n### Secondary Indicators\r\n\r\n- You have 4+ paragraphs of historical content\r\n- Your case involves named episodes or protest waves\r\n- You use dates and temporal markers frequently\r\n- You cite scholarly work on the site's history\r\n- Interview quotes appear within the case section itself\r\n\r\n---\r\n\r\n## Benchmarks\r\n\r\n| Metric | Target | Range | Notes |\r\n|--------|--------|-------|-------|\r\n| Word count | 1,500-2,500 | 1,200-3,210 | Can exceed 2,500 if warranted |\r\n| Paragraphs | 10 | 8-16 | Median: 10 |\r\n| Citations | 23 | 15-35 | Density: 1.5 per 100 words |\r\n| Subsections | 2+ | 2-4 | Organized by period/episode |\r\n| Components | 4-6 | 4-6 | Comprehensive |\r\n\r\n### Component Prevalence\r\n\r\n| Component | Frequency |\r\n|-----------|-----------|\r\n| Historical Background | 100% |\r\n| Geographic Context | 80% |\r\n| Policy/Legal Context | 80% |\r\n| Institutional Description | 60% |\r\n| Demographic Profile | 40% |\r\n| Sampling Rationale | 20% |\r\n| Access Story | 0% |\r\n| Limitation Acknowledgment | 0% |\r\n\r\n---\r\n\r\n## Required Components\r\n\r\n1. **Chronological narrative**: Trace development through multiple periods/episodes\r\n2. **Named episodes**: Identify and label distinct phases or events\r\n3. **Multiple subsections**: Organize by period, episode, or theme (minimum 2)\r\n4. **Scholarly positioning**: Cite prior research on the case/site\r\n\r\n## Optional Components\r\n\r\n- Comparison tables (40% include)\r\n- Interview quotes embedded in case section\r\n- Geographic context\r\n- Policy/legal backdrop\r\n\r\n---\r\n\r\n## Signature Moves\r\n\r\n### Opening Patterns\r\n\r\n**Historical Periodization** (40%):\r\n> \"A latecomer in the contemporary women's movement, the U.S. anti-domestic violence movement or the 'battered women's movement' began in 1973...\"\r\n\r\n> \"The year 2011 was an important moment in the political history of contemporary Russia, the year of Parliamentary elections that sparked mass protests in big Russian cities, the largest in two decades.\"\r\n\r\n**Phenomenon-Site Link with Historical Framing** (60%):\r\n> \"This section lays the groundwork for the subsequent analysis of the role of relational networks within the broader Cairo-based political opposition during the January 25 Uprising.\"\r\n\r\n> \"The definitively populist Yellow Vest movement explicitly rejects leadership, unions, parties, nationalism, right-wing or left-wing identifications, xenophobia, and business interests.\"\r\n\r\n### Dating the Origins\r\n\r\n> \"While several accounts seeking to explain the Egyptian Uprising of 2011 begin with the killing of Khaled Said... it seems more appropriate to date the origins of this episode of collective contestation to the beginning of the new millennium.\"\r\n\r\n### Episode Transition Markers\r\n\r\n> \"As early as 2006, the industrial town of Mahalla El-Kubra had been the site for collective contestation...\"\r\n\r\n> \"In late 2009, yet another actor appeared on the scene of the Cairo-based political opposition...\"\r\n\r\n> \"Two years later, in early 2008, Mahalla would again become a focal point for protests.\"\r\n\r\n### Embedded Interview Quotes\r\n\r\nDeep Historical sections often embed participant voices within the case narrative:\r\n> \"The Palestinian Intifada... gathered sort of loosely connected networks of activists of sympathizers, Leftists like myself, Socialists, and Nationalists, Nasserists, Liberals and Islamists.\" (Interview #1)\r\n\r\n### Wrap-Up/Summary Phrases\r\n\r\n> \"The vignettes presented in this section provided a broad overview of four major episodes of collective contestation involving the Cairo-based political opposition during the decade preceding the 2011 uprising, showing how relational networks... emerged and evolved over time.\"\r\n\r\n---\r\n\r\n## Prohibited Moves\r\n\r\nDeep Historical sections should AVOID:\r\n\r\n1. **Brief treatment** - Under 1,200 words is too short for this cluster\r\n2. **Single paragraph for history** - Chronology requires multiple paragraphs\r\n3. **Position before theory** - That signals Policy-Driven\r\n4. **Skipping chronological development** - The arc matters\r\n5. **Non-narrative structure** - Deep Historical tells a story\r\n\r\n---\r\n\r\n## Template Structure\r\n\r\n### Standard Deep Historical Template (10-12 paragraphs)\r\n\r\n```\r\n## [Historical Case Title]\r\n\r\nParagraph 1: Framing - what this section will accomplish\r\n- Set up the historical scope\r\n- Name the phenomenon and time period\r\n- Preview the arc\r\n\r\nParagraph 2: Dating the origins\r\n- Where to start the story\r\n- Why this starting point\r\n\r\nParagraphs 3-5: EPISODE 1\r\n- What happened\r\n- Who was involved\r\n- Consequences and connections\r\n\r\n## [Subsection: Episode 2 Name]\r\n\r\nParagraphs 6-8: EPISODE 2\r\n- Context for this episode\r\n- Events and actors\r\n- How it connects to Episode 1\r\n\r\nParagraphs 9-10: EPISODE 3 (if applicable)\r\n- Similar structure\r\n\r\nParagraph 11-12: Summary and transition\r\n- What the historical arc reveals\r\n- How it connects to the analysis\r\n- [Implicit transition to Methods or Analysis]\r\n```\r\n\r\n### Variant: Data Source Organization\r\n\r\n```\r\n## [Movement/Case Name]\r\n\r\nParagraph 1-3: Movement overview and framing\r\n\r\n## [Data Source 1: e.g., The Petition]\r\n\r\nParagraphs 4-6: Analysis of this source\r\n- What it contains\r\n- What it reveals\r\n\r\n## [Data Source 2: e.g., The Grand Debate]\r\n\r\nParagraphs 7-9: Analysis of this source\r\n\r\n## [Data Source 3: e.g., The Counter-Debate]\r\n\r\nParagraphs 10-12: Analysis of this source\r\n\r\nSummary paragraph\r\n```\r\n\r\n---\r\n\r\n## Exemplar Articles\r\n\r\n### Primary Exemplar: Egyptian Uprising 2011\r\n\r\n**Why exemplar**: At 3,210 words (corpus maximum), this section traces the development of relational networks in Cairo's political opposition through four protest episodes: Palestinian Intifada solidarity (2000), anti-Iraq war mobilization (2003), Kefaya movement (2004-05), and April 6 movement (2008).\r\n\r\n**Key features**:\r\n- 16 paragraphs\r\n- 3,210 words\r\n- 2 subsections\r\n- Phenomenon-Site-Link opening with historical framing\r\n- Interview quotes embedded\r\n- Scholarly-Positioning citations\r\n- Chronological organization across a decade\r\n\r\n**Opening**: \"This section lays the groundwork for the subsequent analysis of the role of relational networks within the broader Cairo-based political opposition during the January 25 Uprising.\"\r\n\r\n**Named Episodes**:\r\n1. Palestinian Intifada solidarity (2000)\r\n2. Anti-Iraq war mobilization (2003)\r\n3. Kefaya movement (2004-05)\r\n4. April 6 movement (2008)\r\n\r\n### Secondary Exemplar: Yellow Vest Movement\r\n\r\n**Why exemplar**: Multi-source historical case tracing a movement's genesis through petition, grand debate, and counter-debate sources.\r\n\r\n**Key features**:\r\n- 13 paragraphs\r\n- ~2,726 words\r\n- 4 subsections (Movement, Petition, Grand Debat, Vrai Debat)\r\n- Empirical-Extremity justification (\"purer\" popular movement)\r\n- Multiple data sources analyzed within case section\r\n\r\n**Opening**: \"The definitively populist Yellow Vest movement explicitly rejects leadership, unions, parties, nationalism, right-wing or left-wing identifications, xenophobia, and business interests.\"\r\n\r\n### Tertiary Exemplar: Domestic Violence Movement History\r\n\r\n**Why exemplar**: Traces three strategies of the feminist anti-domestic violence movement from the 1970s.\r\n\r\n**Key features**:\r\n- Table comparing strategies across sites\r\n- Historical-Periodization opening\r\n- Empirical-Extremity justification (California/Minnesota as \"first\")\r\n- Chronological organization\r\n\r\n**Opening**: \"A latecomer in the contemporary women's movement, the U.S. anti-domestic violence movement or the 'battered women's movement' began in 1973...\"\r\n\r\n---\r\n\r\n## Calibration Checklist\r\n\r\nBefore submitting a Deep Historical section, verify:\r\n\r\n- [ ] Word count is 1,500-2,500 words (may exceed if warranted)\r\n- [ ] 8-16 paragraphs present\r\n- [ ] At least 2 subsections\r\n- [ ] Chronological organization is clear\r\n- [ ] Named episodes or periods are identified\r\n- [ ] Historical development is traced (not just mentioned)\r\n- [ ] Scholarly citations on the site/case included\r\n- [ ] Positioned after theory section\r\n- [ ] Summary/wrap-up paragraph present\r\n\r\n### If your section is under 1,200 words:\r\nYou may not need Deep Historical. Consider:\r\n- **Standard Context** if historical background is just context\r\n- Whether you can expand with additional episodes\r\n\r\n### If your section has no clear episodes:\r\nDeep Historical requires discrete events, periods, or phases. If your case does not have these, consider:\r\n- **Standard Context** with historical background\r\n- **Policy-Driven** if policy evolution is the focus\r\n\r\n### If you have multiple sites being compared:\r\nYou may need **Comparative** with historical elements, not pure Deep Historical. Consider hybrid approach.\r\n\r\n---\r\n\r\n## Special Considerations\r\n\r\n### Interview Quotes in Case Section\r\n\r\nDeep Historical is the only cluster where interview quotes commonly appear within the case justification itself (not just in Methods or Findings). Use quotes to:\r\n- Illustrate episodes from participant perspectives\r\n- Show how activists understood key events\r\n- Provide \"you are there\" texture to historical narrative\r\n\r\n### Tables in Deep Historical\r\n\r\nAbout 40% of Deep Historical sections include tables. Use tables for:\r\n- Comparing strategies or approaches across periods\r\n- Summarizing episode characteristics\r\n- Comparing sites within a movement (if applicable)\r\n\r\n### Length Flexibility\r\n\r\nDeep Historical is the most length-flexible cluster. The corpus maximum (3,210 words) belongs to this cluster. If your case genuinely requires tracing multiple episodes across an extended time period, longer sections are acceptable.\r\n",
        "plugins/case-justification/skills/case-justification/clusters/minimal.md": "# Cluster Profile: Minimal Context\r\n\r\n## Quick Reference\r\n\r\n| Attribute | Value |\r\n|-----------|-------|\r\n| Cluster Name | Minimal Context |\r\n| N in corpus | 5 (16%) |\r\n| Word Count | 300-500 words |\r\n| Paragraphs | 1-3 |\r\n| Subsections | 0-1 |\r\n| Position | AFTER theory |\r\n| Table | Never |\r\n\r\n---\r\n\r\n## When to Use This Cluster\r\n\r\n### Primary Triggers\r\n\r\nUse Minimal Context when ANY of the following apply:\r\n\r\n1. **Site is well-known**: The case requires little introduction because readers are already familiar with it (e.g., major metropolitan areas, famous institutions, widely-studied phenomena).\r\n\r\n2. **Mixed-methods design**: The qualitative component follows a quantitative analysis that has already established the case context.\r\n\r\n3. **Phenomenon over site**: The theoretical or empirical phenomenon is more important than the specific site characteristics.\r\n\r\n4. **Space constraints**: Article length limits require an efficient case justification.\r\n\r\n5. **Access-driven selection**: The case was selected primarily due to data access, and extensive contextualization would misrepresent the case selection logic.\r\n\r\n### Secondary Indicators\r\n\r\n- You can describe the essential case features in 2-3 paragraphs\r\n- Historical development is not analytically relevant\r\n- No site comparisons are needed\r\n- Policy/institutional context is either absent or minimal\r\n\r\n---\r\n\r\n## Benchmarks\r\n\r\n| Metric | Target | Range | Notes |\r\n|--------|--------|-------|-------|\r\n| Word count | 300-500 | 264-427 | Corpus median: 352 |\r\n| Paragraphs | 2 | 1-3 | Single paragraph acceptable |\r\n| Citations | 3 | 2-5 | Low density: 0.7 per 100 words |\r\n| Components | 2-3 | 1-3 | Focus on essentials |\r\n\r\n### Component Prevalence\r\n\r\n| Component | Frequency |\r\n|-----------|-----------|\r\n| Sampling Rationale | 60% |\r\n| Geographic Context | 40% |\r\n| Historical Background | 40% |\r\n| Access Story | 20% |\r\n| Policy/Legal Context | 20% |\r\n| Institutional Description | 20% |\r\n| Demographic Profile | 0% |\r\n\r\n---\r\n\r\n## Required Components\r\n\r\n1. **Site identification**: Name and locate the case clearly\r\n2. **One or two key features**: What makes this case relevant?\r\n3. **Transition to methods** (implicit or data-preview)\r\n\r\n## Optional Components\r\n\r\n- Brief historical note (1-2 sentences)\r\n- Policy context (1-2 sentences)\r\n- Access acknowledgment\r\n- Quantitative prelude (if mixed-methods)\r\n\r\n---\r\n\r\n## Signature Moves\r\n\r\n### Opening Patterns\r\n\r\n**Data/Study Introduction** (most common for mixed-methods):\r\n> \"The data we present here were collected as part of a study of police traffic stops and their effects on police-community relations.\"\r\n\r\n**Institutional Context**:\r\n> \"This network of colleges includes over 215,000 students, 21,000 faculty, and over two million living alumni.\"\r\n\r\n**Research Setting Statement**:\r\n> \"While others have interviewed young people about their gender beliefs and show the consequences of their beliefs for inequality, we sought to determine whether and why their beliefs might feasibly change.\"\r\n\r\n### Justification Patterns\r\n\r\n**Intrinsic Interest (brief)**:\r\n> \"The study took place during a time of intensified frequency of deportations by the U.S. Department of Homeland Security in the Obama administration.\"\r\n\r\n**Access-Driven**:\r\n> \"Given this interest, we were fortunate to gain access to students in a sociology course about why the gender gap in leadership exists.\"\r\n\r\n**Quantitative Prelude**:\r\n> \"Through statistical analyses of these data, we did not find consistent patterns of racial disparities in who gets stopped citywide... However, we did find significant disparities in several post-stop outcomes.\"\r\n\r\n### Transition Patterns\r\n\r\n**Data-Preview** (20%):\r\n> \"While the focus of this paper is on the analysis of the interview data, some findings from the survey of staff will be included.\"\r\n\r\n**Implicit** (60%):\r\nSection ends; methods section follows immediately.\r\n\r\n---\r\n\r\n## Prohibited Moves\r\n\r\nMinimal Context sections should NEVER:\r\n\r\n1. **Include tables** - Tables signal Comparative treatment\r\n2. **Trace historical development** - Chronological narratives belong in Deep Historical\r\n3. **Use multiple subsections** - One subsection maximum\r\n4. **Exceed 500 words** - If longer, reconsider Standard Context\r\n5. **Position before theory** - Policy-Driven only\r\n6. **Elaborate policy context** - Keep it brief or omit\r\n\r\n---\r\n\r\n## Template Structure\r\n\r\n### Single-Paragraph Template (Ultra-Minimal)\r\n```\r\n[1 paragraph: Site identification + key feature + quantitative/access context + brief transition]\r\n```\r\n\r\n### Two-Paragraph Template (Standard Minimal)\r\n```\r\nParagraph 1: Site identification and one key contextual feature\r\nParagraph 2: Additional context (timing, access, quantitative findings) + transition\r\n```\r\n\r\n### Three-Paragraph Template (Extended Minimal)\r\n```\r\nParagraph 1: Site identification and primary feature\r\nParagraph 2: Secondary context (policy OR history OR institutional)\r\nParagraph 3: Brief transition or methods preview\r\n```\r\n\r\n---\r\n\r\n## Exemplar Articles\r\n\r\n### Primary Exemplar: Police Traffic Stops Study\r\n\r\n**Why exemplar**: At 264 words in a single paragraph, this is the corpus minimum. Establishes the San Diego Police Department study, mentions the quantitative analysis that preceded qualitative interviews, and moves directly to data.\r\n\r\n**Key features**:\r\n- Single paragraph (1 paragraph)\r\n- 264 words\r\n- 2 citations\r\n- Data-Preview transition\r\n- Mixed-methods framing\r\n\r\n**Opening**: \"The data we present here were collected as part of a study of police traffic stops and their effects on police-community relations.\"\r\n\r\n### Secondary Exemplar: Undocumented Students at Jesuit Colleges\r\n\r\n**Why exemplar**: Brief institutional context for a network of colleges, establishing mission and policy environment efficiently.\r\n\r\n**Key features**:\r\n- 2 paragraphs\r\n- ~320 words\r\n- 6 citations\r\n- Institutional + policy context\r\n- Implicit transition\r\n\r\n**Opening**: \"This network of colleges includes over 215,000 students, 21,000 faculty, and over two million living alumni.\"\r\n\r\n### Tertiary Exemplar: Gender Beliefs Course Study\r\n\r\n**Why exemplar**: Demonstrates the Access-Driven variant. Explicitly acknowledges access opportunity and sample limitations.\r\n\r\n**Key features**:\r\n- 3 paragraphs\r\n- ~422 words\r\n- 3 citations\r\n- Research-Setting opening\r\n- Access acknowledgment\r\n\r\n**Opening**: \"While others have interviewed young people about their gender beliefs and show the consequences of their beliefs for inequality, we sought to determine whether and why their beliefs might feasibly change.\"\r\n\r\n---\r\n\r\n## Calibration Checklist\r\n\r\nBefore submitting a Minimal Context section, verify:\r\n\r\n- [ ] Word count is 300-500 words\r\n- [ ] No more than 3 paragraphs\r\n- [ ] No tables included\r\n- [ ] No extended historical narrative\r\n- [ ] One subsection at most\r\n- [ ] Positioned after theory section\r\n- [ ] Essential case features are clear\r\n- [ ] Transition to methods is present (implicit or explicit)\r\n\r\n### If your section exceeds 500 words:\r\nConsider whether you actually need **Standard Context**. Ask:\r\n- Am I including geographic context that readers need?\r\n- Am I providing historical background that matters analytically?\r\n- Would my case benefit from fuller contextualization?\r\n\r\nIf yes to any, upgrade to Standard Context.\r\n",
        "plugins/case-justification/skills/case-justification/clusters/policy.md": "# Cluster Profile: Policy-Driven\r\n\r\n## Quick Reference\r\n\r\n| Attribute | Value |\r\n|-----------|-------|\r\n| Cluster Name | Policy-Driven |\r\n| N in corpus | 4 (13%) |\r\n| Word Count | 650-900 words |\r\n| Paragraphs | 3-6 |\r\n| Subsections | 1 |\r\n| Position | **BEFORE theory** |\r\n| Table | Never |\r\n\r\n---\r\n\r\n## When to Use This Cluster\r\n\r\n### Primary Triggers\r\n\r\nUse Policy-Driven when ALL of the following apply:\r\n\r\n1. **Policy IS the phenomenon**: The policy, law, or institutional arrangement is itself what you are studying (not just background context).\r\n\r\n2. **Context motivates theory**: Readers must understand the policy/institutional context BEFORE your theoretical framework makes sense.\r\n\r\n3. **\"What\" before \"why\"**: The empirical reality must be established before you can theorize about it.\r\n\r\n4. **Strong applied implications**: Your research has direct policy relevance and the context establishes the stakes.\r\n\r\n### Secondary Indicators\r\n\r\n- Your case section would logically precede your theory section\r\n- Policy details are dense and central (not brief background)\r\n- You cite specific legislation, acts, or institutional arrangements\r\n- The policy environment IS the puzzle your theory will address\r\n\r\n### The Key Distinguishing Feature\r\n\r\n**Position**: Policy-Driven is the ONLY cluster positioned BEFORE the theory section. All other clusters follow theory. If your case context should come after theory, use a different cluster.\r\n\r\n---\r\n\r\n## Benchmarks\r\n\r\n| Metric | Target | Range | Notes |\r\n|--------|--------|-------|-------|\r\n| Word count | 650-900 | 652-903 | Corpus median: 765 |\r\n| Paragraphs | 4 | 3-6 | Median: 4 |\r\n| Citations | 14 | 10-18 | Density: 1.8 per 100 words |\r\n| Subsections | 1 | 1 | Single heading |\r\n| Components | 3-5 | 3-5 | Heavy on policy |\r\n\r\n### Component Prevalence\r\n\r\n| Component | Frequency |\r\n|-----------|-----------|\r\n| Policy/Legal Context | 100% |\r\n| Geographic Context | 75% |\r\n| Historical Background | 75% |\r\n| Institutional Description | 75% |\r\n| Demographic Profile | 50% |\r\n| Limitation Acknowledgment | 25% |\r\n| Sampling Rationale | 0% |\r\n| Access Story | 0% |\r\n\r\n---\r\n\r\n## Required Components\r\n\r\n1. **Policy/legal description**: Detailed account of the policy, law, or institutional arrangement (ALWAYS present - 100%)\r\n2. **BEFORE THEORY position**: Section must precede theoretical framework\r\n3. **Stakes establishment**: Why this policy matters\r\n\r\n## Optional Components\r\n\r\n- Geographic context (75%)\r\n- Historical background (75%)\r\n- Institutional description (75%)\r\n- Demographic profile (50%)\r\n\r\n---\r\n\r\n## Signature Moves\r\n\r\n### Opening Patterns\r\n\r\n**Historical-Periodization with Policy Focus** (25%):\r\n> \"A hallmark of the Canadian social policy climate of the 1990s, social assistance reform was constituted as necessary to address government debts and deficits - the fiscal crisis of the welfare state.\"\r\n\r\n**Phenomenon-Site Link with Policy Content** (75%):\r\n> \"The deportation-carceral system reflects three key changes in immigration enforcement since the 1990s.\"\r\n\r\n> \"Despite its overall demographic homogeneity, the Portland metropolitan area is home to some of the most racially diverse communities in the state of Oregon.\"\r\n\r\n**Institutional Description**:\r\n> \"In New York State, the site of this study, the child support agency is charged with opening a child support case, establishing paternity, and locating noncustodial parents.\"\r\n\r\n### Policy Establishment Phrases\r\n\r\n**Naming Key Legislation**:\r\n> \"In 1996, the U.S. Illegal Immigration Reform and Immigrant Responsibility Act (IIRIRA) vastly increased spending on immigration enforcement and expanded the list of deportable 'crimes' to include misdemeanors such as non-violent theft, identity fraud, domestic violence, drug crimes, gambling, and tax evasion.\"\r\n\r\n> \"In 1997, then Premier Mike Harris implemented the Ontario Works Act, which slashed social assistance benefit rates by 21.6 percent and immediately froze them.\"\r\n\r\n**Establishing Notoriety/Significance**:\r\n> \"The province of Ontario is especially known (or infamous) for its social assistance reform that demonstrated punitive allegiance to neo-liberal prescriptions and the resolute pursuit of market citizenship.\"\r\n\r\n**Policy Mechanisms**:\r\n> \"It also has a number of administrative and judicial tools available to collect and enforce child support orders, including wage withholding, license revocation, tax refund intercepts, asset seizure, and incarceration.\"\r\n\r\n### Historical Context for Policy\r\n\r\n> \"However, the overtly racist history of Portland and the state of Oregon contrasts with this reputation as an oasis of liberal politics. In 1857, Oregon adopted a state constitution banning Black people from entering, residing, or holding property in the state.\"\r\n\r\n> \"New laws also made it harder for deportees to fight their cases, enabling expedited removal without due process or judicial review.\"\r\n\r\n### Bridge to Theory Phrases\r\n\r\nPolicy-Driven sections often end with a question or statement that theory will address:\r\n\r\n> \"This paper addresses Portland's contradictions by asking: how are these contradictory realities reconciled in the lives of Portlanders of color?\"\r\n\r\n---\r\n\r\n## Prohibited Moves\r\n\r\nPolicy-Driven sections should NEVER:\r\n\r\n1. **Position after theory** - This is the defining feature; BEFORE theory is mandatory\r\n2. **Include comparison tables** - Tables signal Comparative treatment\r\n3. **Use minimal treatment** - Policy context requires elaboration (650+ words)\r\n4. **Treat policy as background** - Policy is foreground in this cluster\r\n5. **Skip legislation/institutional details** - Dense policy content is expected\r\n\r\n---\r\n\r\n## Template Structure\r\n\r\n### Standard Policy-Driven Template (4-5 paragraphs)\r\n\r\n```\r\n## [Policy/Institutional Context Title]\r\n\r\nParagraph 1: Policy landscape and historical context\r\n- When this policy environment emerged\r\n- Broader forces shaping it\r\n- Why it matters\r\n\r\nParagraph 2: Specific policy/legal details\r\n- Name specific legislation or programs\r\n- Key provisions and mechanisms\r\n- What the policy does\r\n\r\nParagraph 3: Local/case-specific context\r\n- How this manifests in your site\r\n- Site's particular relationship to policy\r\n- Why this site exemplifies the policy\r\n\r\nParagraph 4: Stakes and significance\r\n- Why this policy matters\r\n- What makes it notable/notorious\r\n- What questions it raises\r\n\r\nParagraph 5 (optional): Bridge to theory\r\n- Question that theory will address\r\n- What understanding the policy enables\r\n- [THEORY SECTION FOLLOWS]\r\n```\r\n\r\n### Variant: Enumerated Changes\r\n\r\n```\r\n## [Policy System Title]\r\n\r\nParagraph 1: Overview - \"The X system reflects N key changes...\"\r\n- List the changes\r\n\r\nParagraph 2: Change 1 in detail\r\n- Legislation\r\n- Mechanisms\r\n- Consequences\r\n\r\nParagraph 3: Change 2 in detail\r\n\r\nParagraph 4: Change 3 in detail\r\n\r\nParagraph 5: Contemporary implications\r\n- Where we are now\r\n- [THEORY SECTION FOLLOWS]\r\n```\r\n\r\n---\r\n\r\n## Exemplar Articles\r\n\r\n### Primary Exemplar: Ontario Social Assistance Reform\r\n\r\n**Why exemplar**: Exemplifies the Policy-Driven cluster perfectly. Opens with historical-periodization (\"A hallmark of the Canadian social policy climate of the 1990s\"), establishes policy context (neoliberal welfare reform, Common Sense Revolution, Ontario Works Act), and positions BEFORE THEORY because understanding the policy is prerequisite to theorizing about it.\r\n\r\n**Key features**:\r\n- Position: BEFORE THEORY\r\n- ~652 words\r\n- Single subsection\r\n- Historical-Periodization opening\r\n- Intrinsic-Interest justification (Ontario as \"notorious\")\r\n- Heavy policy/legal content\r\n\r\n**Opening**: \"A hallmark of the Canadian social policy climate of the 1990s, social assistance reform was constituted as necessary to address government debts and deficits - the fiscal crisis of the welfare state.\"\r\n\r\n**Structure**:\r\n1. Canadian social policy context (1990s)\r\n2. Ontario's specific reforms\r\n3. Ontario Works Act details\r\n4. Discursive stigmatization context\r\n[THEORY SECTION FOLLOWS]\r\n\r\n### Secondary Exemplar: Deportation-Carceral System\r\n\r\n**Why exemplar**: Traces three key changes in immigration enforcement since the 1990s. Dense policy content including IIRIRA, Criminal Alien Program, Secure Communities.\r\n\r\n**Key features**:\r\n- Position: BEFORE THEORY\r\n- ~903 words\r\n- Phenomenon-Site-Link opening with policy content\r\n- Chronological policy development\r\n- Specific legislation named\r\n\r\n**Opening**: \"The deportation-carceral system reflects three key changes in immigration enforcement since the 1990s: 1) the criminalization of immigrants...; 2) the militarization of the U.S.-Mexico border; and 3) a shift towards interior enforcement...\"\r\n\r\n### Tertiary Exemplar: Family Court/Unmarried Parents\r\n\r\n**Why exemplar**: Only article without subsections - continuous prose. Establishes legal/institutional context for family court in New York.\r\n\r\n**Key features**:\r\n- Position: BEFORE THEORY\r\n- ~876 words\r\n- No subsections (unique in corpus)\r\n- Institutional-Description opening\r\n- Legal mechanisms detailed\r\n\r\n**Opening**: \"In New York State, the site of this study, the child support agency is charged with opening a child support case, establishing paternity, and locating noncustodial parents.\"\r\n\r\n### Quaternary Exemplar: Portland Racial Dynamics\r\n\r\n**Why exemplar**: Uses prior-site-studies citations. Establishes Portland's contradictions between liberal reputation and racist history.\r\n\r\n**Key features**:\r\n- Position: BEFORE THEORY\r\n- ~678 words\r\n- Prior-Site-Studies citation function\r\n- Ends with research question that theory addresses\r\n\r\n**Opening**: \"Despite its overall demographic homogeneity, the Portland metropolitan area is home to some of the most racially diverse communities in the state of Oregon and remains a site of Black, Indigenous, Asian American, Latinx, and Pacific Islander community and resistance.\"\r\n\r\n---\r\n\r\n## Calibration Checklist\r\n\r\nBefore submitting a Policy-Driven section, verify:\r\n\r\n- [ ] **Positioned BEFORE theory section** (CRITICAL)\r\n- [ ] Word count is 650-900 words\r\n- [ ] 3-6 paragraphs present\r\n- [ ] Single subsection heading\r\n- [ ] Specific legislation/programs named\r\n- [ ] Policy mechanisms detailed\r\n- [ ] Intrinsic-Interest justification present\r\n- [ ] No comparison tables\r\n- [ ] Dense policy content (not brief background)\r\n\r\n### If your section is positioned AFTER theory:\r\nYou do not have a Policy-Driven section. Reconsider:\r\n- **Standard Context** if policy is just background\r\n- Whether repositioning is appropriate\r\n\r\n### If your section lacks dense policy content:\r\nPolicy-Driven requires detailed policy/legal/institutional description. If you only have 1-2 sentences of policy context, use:\r\n- **Standard Context** with policy component\r\n- **Minimal Context** if case requires little context\r\n\r\n### If you are comparing policy implementations across sites:\r\nYou may need **Comparative** with policy emphasis, not Policy-Driven.\r\n\r\n---\r\n\r\n## Special Considerations\r\n\r\n### The BEFORE THEORY Position\r\n\r\nThis is the most important distinguishing feature of Policy-Driven. The logic is:\r\n\r\n1. Readers cannot understand your theoretical framework without first understanding the policy context\r\n2. The policy context MOTIVATES the theoretical framework\r\n3. The \"what\" (policy) must precede the \"why\" (theory)\r\n\r\nIf this logic does not apply to your case, do not use Policy-Driven.\r\n\r\n### Justification Strategy\r\n\r\nPolicy-Driven sections use Intrinsic-Interest justification (100% in corpus). The case matters because the policy itself matters - not because it fits a theoretical framework (that comes later) or provides variation (Comparative territory).\r\n\r\n### Citation Density\r\n\r\nPolicy-Driven has the highest citation density (1.8 per 100 words). Expect to cite:\r\n- Specific legislation and legal sources\r\n- Prior studies of this policy/institution\r\n- Statistics on policy implementation\r\n- News sources or government reports\r\n",
        "plugins/case-justification/skills/case-justification/clusters/standard.md": "# Cluster Profile: Standard Context\r\n\r\n## Quick Reference\r\n\r\n| Attribute | Value |\r\n|-----------|-------|\r\n| Cluster Name | Standard Context |\r\n| N in corpus | 11 (34%) |\r\n| Word Count | 700-1,000 words |\r\n| Paragraphs | 3-7 |\r\n| Subsections | 1 |\r\n| Position | AFTER theory |\r\n| Table | Rarely (9%) |\r\n\r\n---\r\n\r\n## When to Use This Cluster\r\n\r\n### Primary Triggers\r\n\r\nUse Standard Context when ALL of the following apply:\r\n\r\n1. **Single research site**: You are studying one location, organization, or case (not comparing multiple sites).\r\n\r\n2. **Moderate contextualization needed**: The case requires more than minimal introduction but does not require extensive historical narrative.\r\n\r\n3. **Theory drives site selection**: The case was chosen because it provides conditions where theoretical processes can be observed (Theoretical-Fit) or because it matters in its own right (Intrinsic-Interest).\r\n\r\n4. **Standard research design**: Not a comparative design, not a policy study that must precede theory, not a movement study requiring historical periodization.\r\n\r\n### Secondary Indicators\r\n\r\n- Geographic, historical, or policy dimensions need to be established\r\n- Case is not widely known to readers\r\n- Historical context is background, not foreground\r\n- 3-7 paragraphs are sufficient to establish context\r\n\r\n**Standard Context is the DEFAULT cluster**. Use it when other clusters don't apply.\r\n\r\n---\r\n\r\n## Benchmarks\r\n\r\n| Metric | Target | Range | Notes |\r\n|--------|--------|-------|-------|\r\n| Word count | 700-1,000 | 600-1,200 | Corpus median: 800 |\r\n| Paragraphs | 5 | 3-7 | Median: 5 |\r\n| Citations | 10 | 6-15 | Density: 1.3 per 100 words |\r\n| Subsections | 1 | 1 | Single heading |\r\n| Components | 3-5 | 3-5 | Balanced |\r\n\r\n### Component Prevalence\r\n\r\n| Component | Frequency |\r\n|-----------|-----------|\r\n| Geographic Context | 82% |\r\n| Historical Background | 64% |\r\n| Policy/Legal Context | 64% |\r\n| Institutional Description | 45% |\r\n| Demographic Profile | 45% |\r\n| Sampling Rationale | 36% |\r\n| Limitation Acknowledgment | 9% |\r\n| Access Story | 0% |\r\n\r\n---\r\n\r\n## Required Components\r\n\r\n1. **Site identification**: Clear geographic and/or institutional identification\r\n2. **Contextual trifecta**: Include at least TWO of the following:\r\n   - Geographic context\r\n   - Historical background\r\n   - Policy/legal context\r\n3. **Case significance**: Why this case matters (theoretical fit or intrinsic interest)\r\n\r\n## Optional Components\r\n\r\n- Demographic profile of the population or area\r\n- Institutional description\r\n- Sampling rationale (may appear in Methods)\r\n- Brief policy environment\r\n\r\n---\r\n\r\n## Signature Moves\r\n\r\n### Opening Patterns\r\n\r\n**Geographic Introduction** (27% - signature for this cluster):\r\n> \"Kansas is a re-emerging destination for Latina/o immigrants.\"\r\n\r\n> \"The UAE is a small Islamic federation of nearly ten million residents in the Arab Gulf.\"\r\n\r\n**Phenomenon-Site Link** (45%):\r\n> \"With the formalization of a labor-export policy in the mid-1970s, the Indonesian government entered the labor brokerage industry.\"\r\n\r\n> \"Ghostwriting is not a new form of work.\"\r\n\r\n### Justification Patterns\r\n\r\n**Theoretical Fit** (36%):\r\n> \"Emirati women present a valuable case for understanding how households manage earmarking under conditions where...\"\r\n\r\n> \"The case illuminates therapeutic governance...\"\r\n\r\n**Intrinsic Interest** (36%):\r\n> \"Kansas has a complex sociopolitical environment towards immigration.\"\r\n\r\n> \"Anti-immigrant rhetoric fueled the presidential election of 2016 and generated a hostile environment for Latina/o communities across the United States.\"\r\n\r\n### Content Development Patterns\r\n\r\n**Historical thread**:\r\n> \"The railroad industry brought Mexican immigrants to the state at the turn of the 20th century.\"\r\n\r\n**Demographic statistics**:\r\n> \"In 2000, seven percent of the state's population was Latina/o and by 2017 this rose to over 11 percent, of whom 34 percent were foreign born.\"\r\n\r\n**Policy context**:\r\n> \"In 2004 undocumented college students gained in-state tuition. However, in the same period the state denied undocumented immigrants access to driver's licenses...\"\r\n\r\n### Transition Patterns\r\n\r\n**Implicit** (73% - most common):\r\nSection ends; Methods follows. No explicit transition needed.\r\n\r\n> \"This anti-immigrant context emerges from a long history of immigration policies that criminalize undocumented migration and racialize illegality, linking stereotypes of Mexican origin to perceptions of illegality.\"\r\n\r\n---\r\n\r\n## Prohibited Moves\r\n\r\nStandard Context sections should AVOID:\r\n\r\n1. **Multiple subsections** - Use only one subsection heading\r\n2. **Chronological narratives** - If tracing multiple episodes, use Deep Historical\r\n3. **Comparison tables** - Tables signal Comparative treatment\r\n4. **Position before theory** - That signals Policy-Driven\r\n5. **Extensive historical development** - More than 3 paragraphs of history suggests Deep Historical\r\n6. **Access-driven justification** - This is rare and suggests Minimal\r\n\r\n---\r\n\r\n## Template Structure\r\n\r\n### Standard Template (5 paragraphs)\r\n\r\n```\r\n## [Site Name Context]\r\n\r\nParagraph 1: Geographic introduction and site identification\r\n- Name the location\r\n- Key geographic features\r\n- Position within broader region\r\n\r\nParagraph 2: Historical background\r\n- Brief history (not chronological narrative)\r\n- How we got to the present\r\n- 1-2 sentences on historical roots\r\n\r\nParagraph 3: Demographic and/or institutional context\r\n- Population statistics\r\n- Key institutional features\r\n- Who lives/works there\r\n\r\nParagraph 4: Policy and/or legal context\r\n- Relevant laws or policies\r\n- Political environment\r\n- Institutional constraints\r\n\r\nParagraph 5: Significance and transition\r\n- Why this case matters\r\n- How it connects to theory\r\n- [Implicit transition to Methods]\r\n```\r\n\r\n### Shorter Template (3-4 paragraphs)\r\n\r\n```\r\n## [Site Name Context]\r\n\r\nParagraph 1: Geographic introduction + historical roots\r\n\r\nParagraph 2: Policy/institutional context + demographics\r\n\r\nParagraph 3: Significance + any remaining context\r\n```\r\n\r\n### Longer Template (6-7 paragraphs)\r\n\r\n```\r\n## [Site Name Context]\r\n\r\nParagraph 1: Geographic introduction\r\n\r\nParagraph 2: Historical background\r\n\r\nParagraph 3: Policy environment\r\n\r\nParagraph 4: Demographic profile\r\n\r\nParagraph 5: Institutional context\r\n\r\nParagraph 6: Contemporary significance\r\n\r\nParagraph 7: Transition/summary [optional]\r\n```\r\n\r\n---\r\n\r\n## Exemplar Articles\r\n\r\n### Primary Exemplar: Kansas Latino Immigration\r\n\r\n**Why exemplar**: Demonstrates the standard template well. Opens with geographic introduction, provides historical and demographic context, addresses policy environment, and establishes why Kansas is a suitable site for theoretical inquiry.\r\n\r\n**Key features**:\r\n- Single subsection (\"The Kansas Context\")\r\n- ~800 words\r\n- 4 paragraphs\r\n- Geographic + Historical + Policy + Demographic components\r\n- Implicit transition\r\n- Intrinsic-Interest justification\r\n\r\n**Opening**: \"Kansas is a re-emerging destination for Latina/o immigrants.\"\r\n\r\n**Structure**:\r\n1. Geographic introduction + historical roots\r\n2. Contemporary demographics + scholarly positioning\r\n3. Policy environment (in-state tuition, driver's licenses, Secure Communities)\r\n4. National political context (Trump-era enforcement)\r\n\r\n### Secondary Exemplar: UAE Women's Employment\r\n\r\n**Why exemplar**: Clean example of geographic + legal/policy context. Establishes the UAE's unique institutional environment for women's employment.\r\n\r\n**Key features**:\r\n- Single subsection\r\n- ~1,000 words\r\n- Geographic opening\r\n- Legal context (marriage law, employment policies)\r\n- Theoretical-Fit justification\r\n\r\n**Opening**: \"The UAE is a small Islamic federation of nearly ten million residents in the Arab Gulf.\"\r\n\r\n### Tertiary Exemplar: Chinese Eldercare\r\n\r\n**Why exemplar**: Phenomenon-Site-Link opening connecting to theoretical framework on global care chains.\r\n\r\n**Key features**:\r\n- Single subsection\r\n- ~750 words\r\n- Phenomenon-Site-Link opening\r\n- Cultural context (filial piety)\r\n- Theoretical-Fit justification\r\n\r\n**Opening**: \"Scholarship on the global care chain has largely focused on...\"\r\n\r\n---\r\n\r\n## Calibration Checklist\r\n\r\nBefore submitting a Standard Context section, verify:\r\n\r\n- [ ] Word count is 700-1,000 words\r\n- [ ] 3-7 paragraphs present\r\n- [ ] Exactly one subsection heading\r\n- [ ] At least two of the contextual trifecta included (geographic/historical/policy)\r\n- [ ] Case significance is clear\r\n- [ ] Positioned after theory section\r\n- [ ] No comparison tables\r\n- [ ] No extended chronological narrative (more than 3 paragraphs)\r\n- [ ] Implicit or bridge transition present\r\n\r\n### If your section exceeds 1,200 words:\r\nConsider whether you need:\r\n- **Deep Historical** (if historical development is central)\r\n- **Comparative** (if describing multiple sites)\r\n- **Policy-Driven** (if policy context should precede theory)\r\n\r\n### If your section is under 600 words:\r\nConsider whether:\r\n- **Minimal Context** would suffice (well-known site, mixed-methods prelude)\r\n- You need to add geographic, historical, or policy context\r\n\r\n### If you have multiple sites:\r\nYou need **Comparative**, not Standard Context.\r\n",
        "plugins/case-justification/skills/case-justification/phases/phase0-assessment.md": "# Phase 0: Assessment\r\n\r\nYou are executing Phase 0 of case-justification. Your goal is to gather study information and select the appropriate cluster for the case justification section.\r\n\r\n## Why This Phase Matters\r\n\r\nCase justification sections follow predictable patterns based on study characteristics. Selecting the wrong cluster leads to mismatched content (e.g., writing a Deep Historical section when Standard would suffice) or structural errors (e.g., positioning Policy-Driven content after theory). This phase ensures deliberate cluster selection based on your study's actual needs.\r\n\r\n---\r\n\r\n## Your Tasks\r\n\r\n### 1. Gather Case Information\r\n\r\nCollect from the user:\r\n\r\n**Required**:\r\n- Site name and location\r\n- Population studied\r\n- Key contextual features (geographic, historical, policy, institutional)\r\n- How the case relates to the theoretical framework\r\n\r\n**For Cluster Selection**:\r\n- Single site or multiple sites?\r\n- Is historical development central to the case?\r\n- Does policy/institutional context need to precede theory?\r\n- Is the site well-known or does it require extensive introduction?\r\n- Are there space constraints?\r\n\r\n### 2. Apply the Cluster Decision Tree\r\n\r\nWork through this diagnostic:\r\n\r\n```\r\nCLUSTER DECISION TREE\r\n\r\n1. Does your case context need to PRECEDE your theoretical framework?\r\n   - Is the policy/law/institution ITSELF the phenomenon you theorize about?\r\n   - Must readers understand \"what\" before \"why\"?\r\n   - Does the institutional context motivate the theory?\r\n\r\n   --> YES: POLICY-DRIVEN CLUSTER\r\n       Position: BEFORE theory\r\n       Target: 650-900 words\r\n\r\n2. Do you have MULTIPLE RESEARCH SITES that you will compare?\r\n   - Two or more distinct locations or organizations\r\n   - Parallel data collection across sites\r\n   - Site differences provide analytical leverage\r\n\r\n   --> YES: COMPARATIVE CLUSTER\r\n       Parallel structure, usually with table\r\n       Target: 1,000-1,500 words\r\n\r\n3. Is HISTORICAL DEVELOPMENT central to your case?\r\n   - Social movement tracking protest cycles\r\n   - Policy tracing reform trajectories over time\r\n   - Multiple named episodes or periods\r\n   - \"How we got here\" matters analytically\r\n\r\n   --> YES: DEEP HISTORICAL CLUSTER\r\n       Chronological organization, multiple subsections\r\n       Target: 1,500-2,500 words\r\n\r\n4. Is your case WELL-KNOWN and requires MINIMAL introduction?\r\n   - Site is famous/familiar (major city, well-known institution)\r\n   - Mixed-methods design where qualitative follows quantitative\r\n   - Phenomenon matters more than site specifics\r\n   - Severe space constraints\r\n   - Access-driven case selection\r\n\r\n   --> YES: MINIMAL CONTEXT CLUSTER\r\n       Brief, 1-3 paragraphs\r\n       Target: 300-500 words\r\n\r\n5. None of the above apply?\r\n\r\n   --> STANDARD CONTEXT CLUSTER (DEFAULT)\r\n       Balanced single-site context\r\n       Target: 700-1,000 words\r\n```\r\n\r\n### 3. Review Cluster Profile\r\n\r\nOnce you have a candidate cluster, consult the detailed profile:\r\n- `clusters/minimal.md` (300-500 words)\r\n- `clusters/standard.md` (700-1,000 words)\r\n- `clusters/historical.md` (1,500-2,500 words)\r\n- `clusters/comparative.md` (1,000-1,500 words)\r\n- `clusters/policy.md` (650-900 words)\r\n\r\nVerify the profile matches the user's study characteristics.\r\n\r\n### 4. Assess Components to Include\r\n\r\nBased on the cluster and study characteristics, determine which components to include:\r\n\r\n**Geographic Context**:\r\n- [ ] Location and regional characteristics\r\n- [ ] Urban/rural features\r\n- [ ] Relevant geography\r\n\r\n**Historical Background**:\r\n- [ ] Key historical developments\r\n- [ ] How we got to the present\r\n- [ ] Named periods or episodes (Deep Historical)\r\n\r\n**Policy/Legal Context**:\r\n- [ ] Relevant laws or policies\r\n- [ ] Institutional arrangements\r\n- [ ] Political environment\r\n\r\n**Demographic Profile**:\r\n- [ ] Population statistics\r\n- [ ] Racial/ethnic composition\r\n- [ ] Socioeconomic characteristics\r\n\r\n**Institutional Description**:\r\n- [ ] Organizations involved\r\n- [ ] Institutional features\r\n- [ ] Structural characteristics\r\n\r\n**Sampling Rationale**:\r\n- [ ] Why this case?\r\n- [ ] Theoretical fit argument\r\n- [ ] Intrinsic interest argument\r\n\r\n### 5. Determine Position\r\n\r\n**CRITICAL**: Only Policy-Driven sections go BEFORE theory. All other clusters position AFTER theory.\r\n\r\nIf user indicates the case must precede theory but doesn't fit Policy-Driven triggers, discuss whether:\r\n- The case section should actually go after theory\r\n- The study actually IS Policy-Driven\r\n- There's a mismatch in how the article is structured\r\n\r\n### 6. Write Assessment Memo\r\n\r\nCreate a summary with:\r\n\r\n```markdown\r\n# Case Justification Assessment\r\n\r\n## Case Summary\r\n- **Site**: [Name and location]\r\n- **Population**: [Who was studied]\r\n- **Key Features**: [Geographic, historical, policy, institutional]\r\n- **Relationship to Theory**: [How case connects to framework]\r\n\r\n## Cluster Selection Analysis\r\n\r\n### Decision Tree Walk-Through\r\n1. Policy context must precede theory? [YES/NO - reasoning]\r\n2. Multiple sites compared? [YES/NO - reasoning]\r\n3. Historical development central? [YES/NO - reasoning]\r\n4. Well-known, minimal intro needed? [YES/NO - reasoning]\r\n\r\n## Recommended Cluster\r\n**[Minimal/Standard/Deep Historical/Comparative/Policy-Driven]**\r\n\r\n### Rationale\r\n- [Why this cluster fits]\r\n- [What characteristics drove the selection]\r\n\r\n### Alternative Considered (if applicable)\r\n- [Why rejected alternative]\r\n\r\n## Cluster Implications\r\n- **Target word count**: [range]\r\n- **Position**: [BEFORE/AFTER theory]\r\n- **Subsections**: [number expected]\r\n- **Table**: [Required/Recommended/Omit]\r\n\r\n## Components to Include\r\n- [ ] Geographic Context\r\n- [ ] Historical Background\r\n- [ ] Policy/Legal Context\r\n- [ ] Demographic Profile\r\n- [ ] Institutional Description\r\n- [ ] Sampling Rationale\r\n\r\n## Recommended Opening Move\r\n[Phenomenon-Site-Link / Geographic-Introduction / Historical-Periodization / Institutional-Description / Research-Setting]\r\n\r\n## Recommended Justification Strategy\r\n[Intrinsic-Interest / Theoretical-Fit / Empirical-Extremity / Variation-Leverage / Access-Driven]\r\n\r\n## Questions for User\r\n- [Any clarifying questions needed before drafting]\r\n```\r\n\r\n### 7. Present Recommendation\r\n\r\nPresent to user:\r\n- Recommended cluster with clear rationale\r\n- Target word count and structural expectations\r\n- Position (before or after theory)\r\n- Components to include\r\n- Any questions before proceeding\r\n\r\n---\r\n\r\n## Guiding Principles\r\n\r\n### Standard Is the Default\r\nWhen in doubt, choose Standard Context. It's the most common cluster (34%) and fits most single-site interview studies. Other clusters are for specific circumstances.\r\n\r\n### Position Is Non-Negotiable\r\nPolicy-Driven goes BEFORE theory. All others go AFTER. This is not a stylistic choice; it reflects the logical relationship between case and theory.\r\n\r\n### Historical Complexity Requires Space\r\nIf the case involves named episodes, chronological development, or movement history, it probably needs Deep Historical treatment. Don't compress this into Standard.\r\n\r\n### Comparison Requires Parallel Structure\r\nIf studying multiple sites, each must receive comparable coverage. The variation-leverage statement (\"By comparing X and Y, we can...\") is essential.\r\n\r\n### Tables Signal Comparison\r\nIf you have demographic or characteristic data to compare across sites or pre/post, use a table. Tables belong almost exclusively in Comparative sections.\r\n\r\n---\r\n\r\n## Output Files to Create\r\n\r\n1. **assessment-memo.md** - Full assessment with cluster recommendation\r\n2. (Optional) **questions-for-user.md** - If clarification needed\r\n\r\n---\r\n\r\n## When You're Done\r\n\r\nReport to the orchestrator:\r\n- Recommended cluster\r\n- Key rationale (2-3 sentences)\r\n- Any outstanding questions\r\n- Readiness assessment for Phase 1\r\n\r\nExample summary:\r\n> \"I recommend **Standard Context** (700-1,000 words, positioned after theory) for this study of Latino immigration experiences in Kansas. The site is not widely known, the design is single-site, and historical development is background rather than central. I recommend including geographic context, brief historical background, policy environment, and demographic statistics. Ready for Phase 1: Drafting.\"\r\n",
        "plugins/case-justification/skills/case-justification/phases/phase1-drafting.md": "# Phase 1: Drafting\r\n\r\nYou are executing Phase 1 of case-justification. Your goal is to write a complete case justification section following the selected cluster template and word allocation.\r\n\r\n## Why This Phase Matters\r\n\r\nCase justification sections must accomplish multiple goals: establish the research site, justify case selection, provide necessary context, and connect to the theoretical framework - all within specific word constraints. This phase transforms the cluster template into prose that matches the conventions of *Social Problems* and *Social Forces*.\r\n\r\n---\r\n\r\n## Inputs\r\n\r\nBefore starting, gather:\r\n1. Assessment memo from Phase 0 (cluster selection, components to include)\r\n2. User's case details (site, features, context, theoretical connection)\r\n3. Cluster profile from `clusters/[cluster].md`\r\n4. Opening moves from `techniques/opening-moves.md`\r\n5. Justification strategies from `techniques/justification-strategies.md`\r\n6. Transitions from `techniques/transitions.md`\r\n\r\n---\r\n\r\n## Your Tasks\r\n\r\n### 1. Follow the Cluster Structure\r\n\r\nEach cluster has a specific word allocation. Match your draft to these targets:\r\n\r\n#### MINIMAL CONTEXT (~400 words)\r\n```\r\n[Opening + Site Identification]    ~100 words\r\n[Key Feature(s)]                   ~150 words\r\n[Justification/Transition]         ~150 words\r\n```\r\n\r\n#### STANDARD CONTEXT (~850 words)\r\n```\r\n[Opening + Geographic Context]     ~200 words\r\n[Historical Background]            ~200 words\r\n[Policy/Institutional Context]     ~200 words\r\n[Demographics/Significance]        ~200 words\r\n[Transition]                       ~50 words (often implicit)\r\n```\r\n\r\n#### DEEP HISTORICAL (~2,000 words)\r\n```\r\n[Framing Paragraph]                ~150 words\r\n[Dating the Origins]               ~200 words\r\n[Episode 1]                        ~400 words\r\n[Episode 2]                        ~400 words\r\n[Episode 3 (if applicable)]        ~400 words\r\n[Summary/Transition]               ~200 words\r\n```\r\n\r\n#### COMPARATIVE (~1,250 words)\r\n```\r\n[Macro-Context/Framing]            ~150 words\r\n[Site Selection Rationale]         ~150 words\r\n[Site A Description]               ~300 words\r\n[Site B Description]               ~300 words\r\n[Table + Discussion]               ~200 words\r\n[Variation-Leverage Statement]     ~150 words\r\n```\r\n\r\n#### POLICY-DRIVEN (~775 words)\r\n```\r\n[Policy Landscape/Historical]      ~200 words\r\n[Specific Policy Details]          ~200 words\r\n[Local/Case-Specific Context]      ~200 words\r\n[Stakes/Significance]              ~150 words\r\n[Bridge to Theory]                 ~25 words (optional)\r\n```\r\n\r\n### 2. Draft the Opening\r\n\r\nChoose an opening pattern based on cluster and corpus norms:\r\n\r\n**Phenomenon-Site-Link** (50% of corpus - works for all clusters):\r\n> \"With the formalization of a labor-export policy in the mid-1970s, the Indonesian government entered the labor brokerage industry.\"\r\n\r\n**Geographic-Introduction** (19% - Standard, Comparative):\r\n> \"Kansas is a re-emerging destination for Latina/o immigrants.\"\r\n> \"The UAE is a small Islamic federation of nearly ten million residents in the Arab Gulf.\"\r\n\r\n**Historical-Periodization** (6% - Deep Historical, Policy-Driven):\r\n> \"A latecomer in the contemporary women's movement, the U.S. anti-domestic violence movement or the 'battered women's movement' began in 1973...\"\r\n\r\n**Institutional-Description** (16% - Standard, Comparative):\r\n> \"This network of colleges includes over 215,000 students, 21,000 faculty, and over two million living alumni.\"\r\n\r\n**Research-Setting** (9% - Minimal):\r\n> \"The data we present here were collected as part of a study of police traffic stops and their effects on police-community relations.\"\r\n\r\nSee `techniques/opening-moves.md` for more patterns.\r\n\r\n### 3. Incorporate Justification Strategy\r\n\r\nEvery case justification section must explain why this case. Use one of five strategies:\r\n\r\n**Intrinsic-Interest** (38% - all clusters, especially Policy-Driven):\r\n> \"Kansas has a complex sociopolitical environment towards immigration.\"\r\n> \"The province of Ontario is especially known (or infamous) for its social assistance reform.\"\r\n\r\n**Theoretical-Fit** (22% - Standard, Deep Historical):\r\n> \"Emirati women present a valuable case for understanding how households manage earmarking under conditions where...\"\r\n\r\n**Empirical-Extremity** (16% - Deep Historical):\r\n> \"Nashville was one of the most significant movement centers in the overall southern civil rights struggle, and we have unusually rich data on its movement participants.\"\r\n\r\n**Variation-Leverage** (16% - Comparative only):\r\n> \"By controlling for stroll type, we believe the comparison will be richer and will minimize the likelihood that other confounding variables could serve as an alternative explanation.\"\r\n\r\n**Access-Driven** (9% - Minimal only):\r\n> \"Given this interest, we were fortunate to gain access to students in a sociology course.\"\r\n\r\nSee `techniques/justification-strategies.md` for more patterns.\r\n\r\n### 4. Draft Each Component\r\n\r\nWork through components based on cluster requirements. Consult the cluster profile for component prevalence.\r\n\r\n#### A. Geographic Context\r\n\r\n**What to include**:\r\n- Location (state, region, country)\r\n- Relevant geographic features\r\n- Urban/rural characteristics\r\n- Regional positioning\r\n\r\n**Example (Standard)**:\r\n> \"Kansas is a re-emerging destination for Latina/o immigrants. The railroad industry brought Mexican immigrants to the state at the turn of the 20th century. The movement of the meatpacking industry to rural areas and growth of service sector jobs in the state has attracted Latina/o immigrants to the state.\"\r\n\r\n#### B. Historical Background\r\n\r\n**What to include**:\r\n- Key historical developments\r\n- How current conditions emerged\r\n- Temporal markers (decades, years, periods)\r\n\r\n**Minimal treatment (Standard)**:\r\n> \"The railroad industry brought Mexican immigrants to the state at the turn of the 20th century.\"\r\n\r\n**Extended treatment (Deep Historical)**:\r\nTrace multiple episodes chronologically with named events, dates, and developments.\r\n\r\n#### C. Policy/Legal Context\r\n\r\n**What to include**:\r\n- Relevant legislation (name and date)\r\n- Policy mechanisms\r\n- Political environment\r\n\r\n**Example**:\r\n> \"In 2004 undocumented college students gained in-state tuition. However, in the same period the state denied undocumented immigrants access to driver's licenses.\"\r\n\r\n**Dense Policy (Policy-Driven)**:\r\n> \"In 1997, then Premier Mike Harris implemented the Ontario Works Act, which slashed social assistance benefit rates by 21.6 percent and immediately froze them.\"\r\n\r\n#### D. Demographic Profile\r\n\r\n**What to include**:\r\n- Population statistics with citations\r\n- Racial/ethnic composition\r\n- Socioeconomic characteristics\r\n\r\n**Example**:\r\n> \"In 2000, seven percent of the state's population was Latina/o and by 2017 this rose to over 11 percent, of whom 34 percent were foreign born.\"\r\n\r\n**Comparative treatment**:\r\nUse a table to compare demographics across sites.\r\n\r\n#### E. Institutional Description\r\n\r\n**What to include**:\r\n- Organizations studied\r\n- Institutional features\r\n- Structural characteristics\r\n\r\n**Example**:\r\n> \"In New York State, the site of this study, the child support agency is charged with opening a child support case, establishing paternity, and locating noncustodial parents.\"\r\n\r\n### 5. Draft the Transition\r\n\r\nChoose a transition pattern based on cluster:\r\n\r\n**Implicit** (66% - most clusters):\r\nSimply end the section; the structural break to Methods carries readers forward.\r\n> [End of case section; Methods begins]\r\n\r\n**Integrated** (22% - Comparative):\r\nFold methods content into site descriptions:\r\n> \"Interviews lasted 45 minutes on average. All interviews occurred in the SROs' assigned schools.\"\r\n\r\n**Data-Preview** (6% - Minimal):\r\n> \"While the focus of this paper is on the analysis of the interview data...\"\r\n\r\n**Bridge-Statement** (6% - Policy-Driven):\r\n> \"This paper addresses Portland's contradictions by asking: how are these contradictory realities reconciled?\"\r\n\r\nSee `techniques/transitions.md` for more patterns.\r\n\r\n### 6. Add Tables (Comparative Only)\r\n\r\nIf writing a Comparative section, include a table comparing sites:\r\n\r\n| Characteristic | Site A | Site B |\r\n|----------------|--------|--------|\r\n| Population | X | Y |\r\n| Demographics | ... | ... |\r\n| Key Feature | ... | ... |\r\n\r\n71% of Comparative sections include tables. This is a signature element.\r\n\r\n### 7. Calibrate Word Count\r\n\r\nAfter drafting, check word count against cluster targets:\r\n\r\n| Cluster | Target | Acceptable Range |\r\n|---------|--------|------------------|\r\n| Minimal | 400 | 300-500 |\r\n| Standard | 850 | 700-1,000 |\r\n| Deep Historical | 2,000 | 1,500-2,500 |\r\n| Comparative | 1,250 | 1,000-1,500 |\r\n| Policy-Driven | 775 | 650-900 |\r\n\r\n**If over target**:\r\n- Compress geographic description\r\n- Reduce historical detail (unless Deep Historical)\r\n- Cut redundant policy context\r\n- Move demographics to Methods table\r\n\r\n**If under target**:\r\n- Add geographic context\r\n- Expand historical background\r\n- Include demographic statistics\r\n- Add policy/institutional detail\r\n\r\n### 8. Verify Position\r\n\r\n**CRITICAL**: Confirm the section is positioned correctly:\r\n- **Policy-Driven**: BEFORE theory section\r\n- **All others**: AFTER theory section\r\n\r\n---\r\n\r\n## Guiding Principles\r\n\r\n### 1. Dive Into Substance\r\nDon't open with metadiscourse (\"In this section, I describe...\"). Start with concrete case information.\r\n\r\n### 2. Match Cluster to Content\r\nFollow the word allocation. Don't write 1,500 words for a Standard section or 500 words for Deep Historical.\r\n\r\n### 3. Parallel Structure for Comparative\r\nIf comparing sites, Site B should cover the same dimensions as Site A. Readers should be able to compare.\r\n\r\n### 4. Chronological Organization for Historical\r\nDeep Historical sections tell a story. Name episodes, use dates, trace development.\r\n\r\n### 5. Dense Policy for Policy-Driven\r\nPolicy-Driven sections have the highest citation density (1.8 per 100 words). Name legislation, detail mechanisms.\r\n\r\n### 6. Implicit Transitions Are Normal\r\n66% of corpus sections end without explicit transition. The structural break is sufficient.\r\n\r\n---\r\n\r\n## Output Files to Create\r\n\r\n1. **case-section-draft.md** - Complete case justification section\r\n2. **drafting-notes.md** - Notes on word count, components included/omitted, areas needing revision\r\n\r\n---\r\n\r\n## When You're Done\r\n\r\nReport to the orchestrator:\r\n- Draft complete (yes/no)\r\n- Word count achieved vs. target\r\n- Components included\r\n- Components omitted (with rationale)\r\n- Areas flagged for revision\r\n\r\nExample summary:\r\n> \"**Draft complete**. 823 words (target: 850). Standard Context cluster. Included: geographic context, historical background (brief), policy environment, demographic statistics. Omitted: institutional description (not relevant). Opening: Phenomenon-Site-Link. Justification: Intrinsic-Interest. Transition: Implicit. Ready for Phase 2: Revision.\"\r\n",
        "plugins/case-justification/skills/case-justification/phases/phase2-revision.md": "# Phase 2: Revision\r\n\r\nYou are executing Phase 2 of case-justification. Your goal is to calibrate the draft against cluster benchmarks and polish the prose.\r\n\r\n## Why This Phase Matters\r\n\r\nCase justification sections follow predictable patterns in length, structure, and component coverage. Reviewers notice sections that deviate significantly from genre norms. This phase ensures your draft matches the conventions of *Social Problems* and *Social Forces*.\r\n\r\n---\r\n\r\n## Inputs\r\n\r\nBefore starting, gather:\r\n1. Draft case justification section from Phase 1\r\n2. Assessment memo from Phase 0 (cluster selection, components)\r\n3. Cluster profile from `clusters/[cluster].md`\r\n4. Benchmarks summary from analysis\r\n\r\n---\r\n\r\n## Your Tasks\r\n\r\n### 1. Word Count Calibration\r\n\r\nCheck draft against cluster targets:\r\n\r\n| Cluster | Target | Acceptable | Too Short | Too Long |\r\n|---------|--------|------------|-----------|----------|\r\n| Minimal | 300-500 | 264-550 | < 250 | > 550 |\r\n| Standard | 700-1,000 | 600-1,200 | < 600 | > 1,200 |\r\n| Deep Historical | 1,500-2,500 | 1,200-3,000 | < 1,200 | > 3,000 |\r\n| Comparative | 1,000-1,500 | 800-1,800 | < 800 | > 1,800 |\r\n| Policy-Driven | 650-900 | 600-1,000 | < 600 | > 1,000 |\r\n\r\n**If too short**:\r\n- Add geographic context\r\n- Expand historical background\r\n- Include demographic statistics with citations\r\n- Add policy/institutional detail\r\n\r\n**If too long**:\r\n- Compress geographic description\r\n- Reduce historical detail (unless Deep Historical)\r\n- Move demographics to Methods table\r\n- Cut redundant policy context\r\n\r\n**If significantly off-target**:\r\nConsider whether the cluster assignment is correct. A 500-word section trying to be Standard may actually be Minimal. A 1,400-word Standard may need to be Deep Historical.\r\n\r\n### 2. Component Verification\r\n\r\nCheck that required components are present for the cluster:\r\n\r\n#### Minimal Context (must have)\r\n- [ ] Site identification\r\n- [ ] One or two key features\r\n- [ ] Transition to methods\r\n\r\n#### Standard Context (must have at least 2 of 3)\r\n- [ ] Geographic context OR\r\n- [ ] Historical background OR\r\n- [ ] Policy/legal context\r\n- [ ] Case significance (why this case)\r\n\r\n#### Deep Historical (must have)\r\n- [ ] Chronological narrative\r\n- [ ] Named episodes or periods\r\n- [ ] Multiple subsections (2+)\r\n- [ ] Scholarly positioning citations\r\n\r\n#### Comparative (must have)\r\n- [ ] Parallel site descriptions\r\n- [ ] Variation-leverage statement\r\n- [ ] Site selection rationale\r\n- [ ] Table (strongly recommended - 71% have)\r\n\r\n#### Policy-Driven (must have)\r\n- [ ] Policy/legal description (detailed)\r\n- [ ] BEFORE THEORY position\r\n- [ ] Stakes establishment\r\n\r\n### 3. Citation Density Check\r\n\r\nVerify citation density matches cluster norms:\r\n\r\n| Cluster | Citations per 100 words | Typical Total |\r\n|---------|------------------------|---------------|\r\n| Minimal | 0.7 | 3 |\r\n| Standard | 1.3 | 10 |\r\n| Deep Historical | 1.5 | 23 |\r\n| Comparative | 1.1 | 12 |\r\n| Policy-Driven | 1.8 | 14 |\r\n\r\n**If under-cited**:\r\n- Add citations for demographic statistics (Census, institute reports)\r\n- Add scholarly citations for prior research on site\r\n- Add citations for policy/legislation claims\r\n\r\n**If over-cited**:\r\n- Generally not a problem\r\n- Consider whether section is bloated with excessive sourcing\r\n\r\n### 4. Opening Move Assessment\r\n\r\nVerify the opening matches cluster conventions:\r\n\r\n| Cluster | Preferred Openings |\r\n|---------|-------------------|\r\n| Minimal | Research-Setting (40%), Phenomenon-Site-Link (40%) |\r\n| Standard | Phenomenon-Site-Link (45%), Geographic-Introduction (27%) |\r\n| Deep Historical | Phenomenon-Site-Link (60%), Historical-Periodization (40%) |\r\n| Comparative | Phenomenon-Site-Link (43%), Institutional-Description (29%) |\r\n| Policy-Driven | Phenomenon-Site-Link (75%), Historical-Periodization (25%) |\r\n\r\n**Prohibited openings across all clusters**:\r\n- \"In this section, I will describe...\"\r\n- \"My research setting is...\"\r\n- \"The purpose of this section is to...\"\r\n- \"As mentioned above...\"\r\n\r\n### 5. Transition Assessment\r\n\r\nVerify the transition matches cluster norms:\r\n\r\n| Cluster | Preferred Transition |\r\n|---------|---------------------|\r\n| Minimal | Implicit (60%) or Data-Preview (20%) |\r\n| Standard | Implicit (73%) |\r\n| Deep Historical | Implicit (80%) |\r\n| Comparative | Implicit (43%) or Integrated (43%) |\r\n| Policy-Driven | Implicit (75%) or Bridge-Statement (25%) |\r\n\r\n**Note**: Implicit transitions (section simply ends) are the norm. Don't add explicit transitions unless the cluster calls for it.\r\n\r\n### 6. Position Verification\r\n\r\n**CRITICAL**: Confirm correct position:\r\n- **Policy-Driven**: Section appears BEFORE theory section\r\n- **All other clusters**: Section appears AFTER theory section\r\n\r\nIf the draft is Policy-Driven but positioned after theory, or vice versa, flag this for user review.\r\n\r\n### 7. Parallel Structure Check (Comparative Only)\r\n\r\nFor Comparative sections, verify:\r\n- [ ] Site A and Site B receive comparable coverage\r\n- [ ] Same dimensions are addressed for each site\r\n- [ ] Variation-leverage statement explains what comparison enables\r\n- [ ] Table compares sites across relevant dimensions\r\n\r\n### 8. Chronological Logic Check (Deep Historical Only)\r\n\r\nFor Deep Historical sections, verify:\r\n- [ ] Clear temporal progression\r\n- [ ] Episodes are named and dated\r\n- [ ] Transitions between periods are explicit\r\n- [ ] Summary paragraph connects arc to analysis\r\n\r\n### 9. Prose Polish\r\n\r\nCheck for common issues:\r\n\r\n**Remove metadiscourse**:\r\n- \"It is important to note that...\" --> Just state the fact\r\n- \"As described above...\" --> Remove entirely\r\n- \"In this section, I...\" --> Remove entirely\r\n\r\n**Strengthen verbs**:\r\n- \"There was an increase in...\" --> \"X increased...\"\r\n- \"It can be seen that...\" --> Remove and state directly\r\n\r\n**Check parallel construction**:\r\n- Lists should have parallel structure\r\n- Site descriptions in Comparative should mirror each other\r\n\r\n**Verify citations are formatted**:\r\n- Check journal citation style\r\n- Verify all facts have sources\r\n\r\n### 10. Final Checklist\r\n\r\n#### All Clusters\r\n- [ ] Word count within acceptable range\r\n- [ ] Required components present\r\n- [ ] Citation density appropriate\r\n- [ ] Opening matches cluster conventions\r\n- [ ] No prohibited metadiscourse\r\n- [ ] Transition present (usually implicit)\r\n\r\n#### Cluster-Specific\r\n\r\n**Minimal**:\r\n- [ ] No tables\r\n- [ ] No extended historical narrative\r\n- [ ] Under 500 words\r\n\r\n**Standard**:\r\n- [ ] Single subsection\r\n- [ ] No comparison tables\r\n- [ ] Positioned after theory\r\n\r\n**Deep Historical**:\r\n- [ ] 2+ subsections\r\n- [ ] Chronological organization\r\n- [ ] Named episodes/periods\r\n- [ ] Summary paragraph\r\n\r\n**Comparative**:\r\n- [ ] Parallel site structure\r\n- [ ] Variation-leverage statement\r\n- [ ] Table present (recommended)\r\n\r\n**Policy-Driven**:\r\n- [ ] BEFORE theory position\r\n- [ ] Dense policy content\r\n- [ ] Specific legislation named\r\n\r\n---\r\n\r\n## Output\r\n\r\nCreate a revision memo with:\r\n\r\n```markdown\r\n# Case Justification Revision Report\r\n\r\n## Word Count\r\n- **Draft**: [X] words\r\n- **Target**: [Y-Z] words\r\n- **Status**: [Within range / Adjusted]\r\n\r\n## Component Coverage\r\n- [Component]: [Present/Added/Omitted] - [rationale if omitted]\r\n- ...\r\n\r\n## Citation Density\r\n- **Density**: [X] per 100 words\r\n- **Total**: [N] citations\r\n- **Target**: [Y] citations\r\n- **Status**: [Appropriate / Adjusted]\r\n\r\n## Structural Elements\r\n- **Opening**: [Type] - [Appropriate / Changed to X]\r\n- **Transition**: [Type] - [Appropriate / Changed to X]\r\n- **Position**: [BEFORE/AFTER theory] - [Correct / ERROR]\r\n- **Subsections**: [N] - [Appropriate / Adjusted]\r\n- **Table**: [Present/Absent] - [Appropriate / Added / N/A]\r\n\r\n## Prose Revisions\r\n- [List specific prose improvements made]\r\n\r\n## Remaining Issues\r\n- [Any issues user should address]\r\n\r\n## Quality Assessment\r\n- **Cluster match**: [Good / Marginal / Reconsider cluster]\r\n- **Calibration**: [Matches norms / Minor deviations / Significant deviations]\r\n- **Ready for submission**: [Yes / Needs further work]\r\n```\r\n\r\n---\r\n\r\n## When You're Done\r\n\r\nReport to the orchestrator:\r\n- Final word count vs. target\r\n- Any structural changes made\r\n- Quality assessment\r\n- Any remaining issues for user\r\n\r\nExample summary:\r\n> \"**Revision complete**. Final: 847 words (target: 700-1,000). Standard Context calibrated correctly. Added two demographic citations (Census data) to improve citation density from 0.9 to 1.2 per 100 words. Opening (Phenomenon-Site-Link) and transition (Implicit) match cluster norms. All required components present. Ready for user review.\"\r\n",
        "plugins/case-justification/skills/case-justification/techniques/justification-strategies.md": "# Justification Strategies: Case Justification Sections\r\n\r\nEvery case justification section must answer the implicit question: \"Why this case?\" This guide presents the five justification strategies identified in the 32-article corpus.\r\n\r\n---\r\n\r\n## Overview\r\n\r\n| Strategy | Prevalence | Best For |\r\n|----------|------------|----------|\r\n| Intrinsic-Interest | 38% | All clusters, especially Policy-Driven |\r\n| Theoretical-Fit | 22% | Standard, Deep Historical |\r\n| Empirical-Extremity | 16% | Deep Historical |\r\n| Variation-Leverage | 16% | Comparative (signature) |\r\n| Access-Driven | 9% | Minimal |\r\n\r\n---\r\n\r\n## Type B5: Intrinsic-Interest (38%)\r\n\r\nThe case is worth studying because it matters in its own right. The site has significance beyond its theoretical utility - it's important, notable, or consequential.\r\n\r\n### Pattern\r\n> \"[Site] is [notable/important/significant] because [reasons].\"\r\n\r\n### Examples\r\n\r\n**Standard Context**:\r\n> \"Kansas has a complex sociopolitical environment towards immigration.\"\r\n\r\n> \"Anti-immigrant rhetoric fueled the presidential election of 2016 and generated a hostile environment for Latina/o communities across the United States.\"\r\n\r\n**Policy-Driven** (100% use Intrinsic-Interest):\r\n> \"The province of Ontario is especially known (or infamous) for its social assistance reform that demonstrated punitive allegiance to neo-liberal prescriptions and the resolute pursuit of market citizenship.\"\r\n\r\n> \"The study took place during a time of intensified frequency of deportations by the U.S. Department of Homeland Security in the Obama administration.\"\r\n\r\n**Deep Historical**:\r\n> \"The year 2011 was an important moment in the political history of contemporary Russia.\"\r\n\r\n### When to Use\r\n- When the case has real-world significance\r\n- When policy relevance is central\r\n- When the site is notable, notorious, or consequential\r\n- In Policy-Driven sections (always)\r\n\r\n### When to Avoid\r\n- When the case's importance is primarily theoretical\r\n- When variation-leverage is the real logic (use Variation-Leverage instead)\r\n\r\n---\r\n\r\n## Type B1: Theoretical-Fit (22%)\r\n\r\nThe case was selected because it provides ideal conditions for observing theoretical processes or testing theoretical propositions.\r\n\r\n### Pattern\r\n> \"[Site] presents a valuable case for understanding [theoretical process] because [conditions].\"\r\n\r\n### Examples\r\n\r\n**Standard Context**:\r\n> \"Emirati women present a valuable case for understanding how households manage earmarking under conditions where...\"\r\n\r\n> \"The case illuminates therapeutic governance...\"\r\n\r\n**Deep Historical**:\r\n> \"While several accounts seeking to explain the Egyptian Uprising of 2011 begin with the killing of Khaled Said... it seems more appropriate to date the origins of this episode of collective contestation to the beginning of the new millennium.\"\r\n\r\n### When to Use\r\n- When case selection logic is explicitly theoretical\r\n- When conditions at the site map onto theoretical processes\r\n- When extending or testing existing theory\r\n\r\n### When to Avoid\r\n- When intrinsic importance is the primary logic\r\n- In Policy-Driven sections (policy is foreground, not theory)\r\n- In Comparative sections (use Variation-Leverage instead)\r\n\r\n---\r\n\r\n## Type B2: Empirical-Extremity (16%)\r\n\r\nThe case represents an extreme, deviant, or paradigmatic instance. It's significant because it's first, most, or distinctive in some empirically meaningful way.\r\n\r\n### Pattern\r\n> \"[Site] was [the first/most significant/one of the most important] [X].\"\r\n\r\n### Examples\r\n\r\n**Deep Historical**:\r\n> \"Nashville was one of the most significant movement centers in the overall southern civil rights struggle, and we have unusually rich data on its movement participants.\"\r\n\r\n> \"A latecomer in the contemporary women's movement, the U.S. anti-domestic violence movement or the 'battered women's movement' began in 1973...\" [California and Minnesota as \"first\"]\r\n\r\n**Comparative**:\r\n> \"Unlike King, Chavez does not require these courses for graduation and does not employ college professors to teach them.\"\r\n\r\n### When to Use\r\n- When the case is empirically distinctive\r\n- When \"first,\" \"most,\" or \"extreme\" language applies\r\n- In historical cases with paradigmatic status\r\n- When data availability is unusually rich\r\n\r\n### When to Avoid\r\n- When the case is typical or ordinary\r\n- When theoretical fit matters more than empirical distinction\r\n\r\n---\r\n\r\n## Type B3: Variation-Leverage (16%)\r\n\r\nThe comparison between sites enables analytical traction. Differences or similarities across sites allow you to isolate mechanisms or test scope conditions.\r\n\r\n### Pattern\r\n> \"By comparing [Site A] and [Site B], we can [analytical payoff].\"\r\n\r\n### Examples\r\n\r\n**Comparative** (signature strategy - 57% of Comparative sections):\r\n> \"By controlling for stroll type, we believe the comparison will be richer and will minimize the likelihood that other confounding variables... could serve as an alternative explanation for our findings instead of extent of gentrification.\"\r\n\r\n> \"Unlike King, Chavez does not require these courses for graduation and does not employ college professors to teach them.\"\r\n\r\n> \"Comparatively, SBR runs through Ward 7, a stroll that is also close to public transportation and a major highway, but one that has experienced almost no development or neighborhood-level change during the same period of time.\"\r\n\r\n> \"This move generates a dramatic change in families' residential contexts.\"\r\n\r\n### When to Use\r\n- In Comparative sections (essential)\r\n- When site differences provide analytical leverage\r\n- When controlling for or varying specific features\r\n- When matched-pair or most-different logic applies\r\n\r\n### When to Avoid\r\n- In single-site studies (use other strategies)\r\n- When sites are not being systematically compared\r\n\r\n**NOTE**: This is the SIGNATURE justification for Comparative sections. It is essentially required.\r\n\r\n---\r\n\r\n## Type B4: Access-Driven (9%)\r\n\r\nThe case was selected because of research access - an opportunity arose to study this site. This is a candid acknowledgment that site selection was pragmatic.\r\n\r\n### Pattern\r\n> \"We were fortunate to gain access to [site].\"\r\n\r\n### Examples\r\n\r\n**Minimal Context**:\r\n> \"Given this interest, we were fortunate to gain access to students in a sociology course about why the gender gap in leadership exists.\"\r\n\r\n**With Limitations Acknowledged**:\r\n> \"While others have interviewed young people about their gender beliefs... we sought to determine whether and why their beliefs might feasibly change.\"\r\n\r\n### When to Use\r\n- When access truly drove site selection\r\n- In Minimal Context sections\r\n- When extensive contextualization would misrepresent selection logic\r\n- When paired with appropriate scope limitations\r\n\r\n### When to Avoid\r\n- When other justifications are available\r\n- In longer sections where access seems insufficient\r\n- When intrinsic importance or theoretical fit actually applies\r\n\r\n**NOTE**: This is the rarest justification (9%) and appears primarily in Minimal Context. Use sparingly.\r\n\r\n---\r\n\r\n## Justification Strategy Distribution by Cluster\r\n\r\n| Strategy | Minimal | Standard | Deep Historical | Comparative | Policy-Driven |\r\n|----------|---------|----------|-----------------|-------------|---------------|\r\n| Intrinsic-Interest | 60% | 36% | 20% | 14% | **100%** |\r\n| Theoretical-Fit | 0% | 36% | 40% | 0% | 0% |\r\n| Empirical-Extremity | 20% | 9% | 40% | 14% | 0% |\r\n| Variation-Leverage | 0% | 0% | 0% | **57%** | 0% |\r\n| Access-Driven | 20% | 0% | 0% | 14% | 0% |\r\n\r\n### Key Patterns\r\n\r\n1. **Policy-Driven uses Intrinsic-Interest exclusively** (100%)\r\n2. **Comparative requires Variation-Leverage** (57% - signature strategy)\r\n3. **Deep Historical splits between Theoretical-Fit and Empirical-Extremity**\r\n4. **Standard Context is balanced** between Intrinsic-Interest and Theoretical-Fit\r\n5. **Access-Driven is rare** and limited to Minimal Context\r\n\r\n---\r\n\r\n## Choosing a Justification Strategy\r\n\r\n### Decision Framework\r\n\r\n1. **Is this a Comparative section?**\r\n    Variation-Leverage (required)\r\n\r\n2. **Is this a Policy-Driven section?**\r\n    Intrinsic-Interest (policy matters in its own right)\r\n\r\n3. **Is the case empirically distinctive (first, most, extreme)?**\r\n    Empirical-Extremity\r\n\r\n4. **Does the case provide ideal conditions for theory?**\r\n    Theoretical-Fit\r\n\r\n5. **Is the case important beyond theory?**\r\n    Intrinsic-Interest\r\n\r\n6. **Was access the primary reason for site selection?**\r\n    Access-Driven (use sparingly, Minimal only)\r\n\r\n### Combining Strategies\r\n\r\nSome sections combine strategies. For example:\r\n- Deep Historical might use both Empirical-Extremity (Nashville as significant movement center) AND Theoretical-Fit (ideal conditions for studying relational networks)\r\n- Standard Context might use both Intrinsic-Interest (policy relevance) AND Theoretical-Fit (case illuminates process)\r\n\r\nWhen combining, lead with the primary justification.\r\n\r\n---\r\n\r\n## Common Errors\r\n\r\n### Weak Justifications to Avoid\r\n\r\n- **\"This case is interesting\"** - Too vague; specify why\r\n- **\"Little research has examined X\"** - This is literature gap, not case justification\r\n- **\"I had access\"** - Insufficient for most sections\r\n- **\"This case is typical\"** - Why not study another typical case?\r\n\r\n### Stronger Alternatives\r\n\r\n- **Weak**: \"Portland is an interesting case.\"\r\n- **Strong**: \"Despite its overall demographic homogeneity, the Portland metropolitan area is home to some of the most racially diverse communities in the state of Oregon.\"\r\n\r\n- **Weak**: \"We had access to this organization.\"\r\n- **Strong**: \"Given this interest, we were fortunate to gain access to students in a sociology course about why the gender gap in leadership exists.\" [Note: combines access with theoretical purpose]\r\n\r\n---\r\n\r\n## Templates by Strategy\r\n\r\n### Intrinsic-Interest Template\r\n> \"[Site/Case] is [notable characteristic]. [Evidence of significance - e.g., policy changes, public attention, scholarly debate]. This [policy/situation/context] makes [site] a [valuable/important/revealing] case for understanding [phenomenon].\"\r\n\r\n### Theoretical-Fit Template\r\n> \"[Site/Case] presents a valuable case for understanding [theoretical concept/process] because [conditions present at site]. These conditions allow us to observe [theoretical mechanism] in action.\"\r\n\r\n### Empirical-Extremity Template\r\n> \"[Site/Case] was [the first/most significant/one of the most important] [empirical characteristic]. [Evidence supporting extreme status]. We have [unusually rich data/unique access] on [aspect].\"\r\n\r\n### Variation-Leverage Template\r\n> \"By comparing [Site A] and [Site B], we can examine [analytical payoff]. [Site A] provides [characteristics], while [Site B] provides [contrasting characteristics]. This variation enables us to [isolate/test/examine] [mechanism/scope condition].\"\r\n\r\n### Access-Driven Template\r\n> \"Given [research interest/theoretical concern], we gained access to [site/population]. While [appropriate scope limitation], this opportunity allowed us to examine [phenomenon].\"\r\n",
        "plugins/case-justification/skills/case-justification/techniques/opening-moves.md": "# Opening Moves: Case Justification Sections\r\n\r\nThis guide presents the five opening move types identified in the 32-article corpus, with examples and usage guidance.\r\n\r\n---\r\n\r\n## Overview\r\n\r\n| Opening Type | Prevalence | Best For |\r\n|--------------|------------|----------|\r\n| Phenomenon-Site-Link | 50% | All clusters (versatile) |\r\n| Geographic-Introduction | 19% | Standard, Comparative |\r\n| Institutional-Description | 16% | Standard, Comparative |\r\n| Research-Setting | 9% | Minimal |\r\n| Historical-Periodization | 6% | Deep Historical, Policy-Driven |\r\n\r\n---\r\n\r\n## Type A4: Phenomenon-Site-Link (50%)\r\n\r\nThe most common opening connects the phenomenon under study to the research site. This opening establishes both what you're studying and where, positioning them in relation to each other.\r\n\r\n### Pattern\r\n> \"[Phenomenon] + [Site Connection]\"\r\n\r\n### Examples\r\n\r\n**Standard Context**:\r\n> \"With the formalization of a labor-export policy in the mid-1970s, the Indonesian government entered the labor brokerage industry.\"\r\n\r\n> \"Ghostwriting is not a new form of work.\"\r\n\r\n**Deep Historical**:\r\n> \"This section lays the groundwork for the subsequent analysis of the role of relational networks within the broader Cairo-based political opposition during the January 25 Uprising.\"\r\n\r\n> \"The definitively populist Yellow Vest movement explicitly rejects leadership, unions, parties, nationalism, right-wing or left-wing identifications, xenophobia, and business interests.\"\r\n\r\n**Policy-Driven**:\r\n> \"The deportation-carceral system reflects three key changes in immigration enforcement since the 1990s.\"\r\n\r\n> \"Despite its overall demographic homogeneity, the Portland metropolitan area is home to some of the most racially diverse communities in the state of Oregon.\"\r\n\r\n### When to Use\r\n- When you want to immediately connect your theoretical concerns to the empirical case\r\n- When the phenomenon is more important than the site's geographic identity\r\n- Works across all clusters\r\n\r\n### When to Avoid\r\n- When readers need geographic orientation first\r\n- When the site has distinctive features that should lead\r\n\r\n---\r\n\r\n## Type A2: Geographic-Introduction (19%)\r\n\r\nOpens with the site's geographic identity and characteristics. Common in Standard Context and Comparative clusters where location matters.\r\n\r\n### Pattern\r\n> \"[Site] is [geographic description].\"\r\n\r\n### Examples\r\n\r\n**Standard Context**:\r\n> \"Kansas is a re-emerging destination for Latina/o immigrants.\"\r\n\r\n> \"The UAE is a small Islamic federation of nearly ten million residents in the Arab Gulf.\"\r\n\r\n**Comparative**:\r\n> \"In 2011, the District of Columbia lost its minority-majority status.\"\r\n\r\n### When to Use\r\n- When the site is not widely known and needs introduction\r\n- When geographic features are analytically important\r\n- When establishing regional context matters\r\n\r\n### When to Avoid\r\n- When the site is well-known (unnecessary)\r\n- When phenomenon matters more than location\r\n- In Deep Historical (history should lead) or Policy-Driven (policy should lead)\r\n\r\n---\r\n\r\n## Type A3: Institutional-Description (16%)\r\n\r\nOpens with a description of the institution, organization, or organizational field being studied.\r\n\r\n### Pattern\r\n> \"[Institution/Organization] [does/includes/is characterized by X].\"\r\n\r\n### Examples\r\n\r\n**Minimal Context**:\r\n> \"This network of colleges includes over 215,000 students, 21,000 faculty, and over two million living alumni.\"\r\n\r\n**Comparative**:\r\n> \"The first research site was an affluent suburban county in the South that housed two school districts.\"\r\n\r\n**Policy-Driven**:\r\n> \"In New York State, the site of this study, the child support agency is charged with opening a child support case, establishing paternity, and locating noncustodial parents.\"\r\n\r\n### When to Use\r\n- When studying a specific organization or institution\r\n- When institutional features define the case\r\n- In Comparative sections when describing sites as organizations\r\n\r\n### When to Avoid\r\n- When the site is geographic rather than organizational\r\n- When policy context should lead\r\n\r\n---\r\n\r\n## Type A5: Research-Setting (9%)\r\n\r\nOpens with a statement about the research project or data collection context. Most common in Minimal Context, especially for mixed-methods designs.\r\n\r\n### Pattern\r\n> \"[Data/Study description] + [brief context]\"\r\n\r\n### Examples\r\n\r\n**Minimal Context**:\r\n> \"The data we present here were collected as part of a study of police traffic stops and their effects on police-community relations.\"\r\n\r\n> \"While others have interviewed young people about their gender beliefs and show the consequences of their beliefs for inequality, we sought to determine whether and why their beliefs might feasibly change.\"\r\n\r\n### When to Use\r\n- When the study design or data source needs front-and-center positioning\r\n- In mixed-methods studies where qualitative follows quantitative\r\n- When access or timing defines the research opportunity\r\n- Primarily in Minimal Context\r\n\r\n### When to Avoid\r\n- When readers need substantive case context\r\n- In longer sections (Standard, Deep Historical, Comparative)\r\n- When this would read as purely methodological\r\n\r\n---\r\n\r\n## Type A1: Historical-Periodization (6%)\r\n\r\nOpens with a temporal marker or historical statement that positions the case in time. Signature opening for Deep Historical and Policy-Driven.\r\n\r\n### Pattern\r\n> \"[Time marker/Historical statement about when phenomenon began/emerged]\"\r\n\r\n### Examples\r\n\r\n**Deep Historical**:\r\n> \"A latecomer in the contemporary women's movement, the U.S. anti-domestic violence movement or the 'battered women's movement' began in 1973...\"\r\n\r\n> \"The year 2011 was an important moment in the political history of contemporary Russia, the year of Parliamentary elections that sparked mass protests in big Russian cities, the largest in two decades.\"\r\n\r\n**Policy-Driven**:\r\n> \"A hallmark of the Canadian social policy climate of the 1990s, social assistance reform was constituted as necessary to address government debts and deficits - the fiscal crisis of the welfare state.\"\r\n\r\n### When to Use\r\n- When historical development is central to the case\r\n- In social movement studies\r\n- When tracing policy evolution\r\n- When the case has a clear origin date\r\n\r\n### When to Avoid\r\n- When historical context is background, not foreground\r\n- In Minimal or Standard Context (unless brief historical note)\r\n- When readers need geographic orientation first\r\n\r\n---\r\n\r\n## Opening Move Distribution by Cluster\r\n\r\n| Opening Move | Minimal | Standard | Deep Historical | Comparative | Policy-Driven |\r\n|--------------|---------|----------|-----------------|-------------|---------------|\r\n| Phenomenon-Site-Link | 40% | 45% | 60% | 43% | 75% |\r\n| Geographic-Introduction | 0% | 27% | 0% | 14% | 0% |\r\n| Institutional-Description | 20% | 18% | 0% | 29% | 0% |\r\n| Historical-Periodization | 0% | 0% | 40% | 0% | 25% |\r\n| Research-Setting | 40% | 9% | 0% | 14% | 0% |\r\n\r\n---\r\n\r\n## Prohibited Openings (All Clusters)\r\n\r\nNever open with:\r\n\r\n- **Metadiscourse**: \"In this section, I will describe...\"\r\n- **Passive hedges**: \"It is important to note that...\"\r\n- **Self-reference**: \"My research setting is...\"\r\n- **Purpose statements**: \"The purpose of this section is to...\"\r\n- **Backwards reference**: \"As mentioned above...\"\r\n\r\nInstead, dive directly into substantive case information.\r\n\r\n---\r\n\r\n## Choosing an Opening Move\r\n\r\n### Decision Framework\r\n\r\n1. **Is the phenomenon more important than the site?**\r\n    Phenomenon-Site-Link\r\n\r\n2. **Do readers need geographic orientation?**\r\n    Geographic-Introduction\r\n\r\n3. **Is this an organizational study?**\r\n    Institutional-Description\r\n\r\n4. **Is this mixed-methods or access-driven?**\r\n    Research-Setting\r\n\r\n5. **Is historical development central?**\r\n    Historical-Periodization\r\n\r\n### Default Recommendation\r\n\r\nWhen unsure, use **Phenomenon-Site-Link**. At 50% prevalence, it's the most common and most versatile opening. It works across all clusters and immediately connects theory to empirics.\r\n",
        "plugins/case-justification/skills/case-justification/techniques/transitions.md": "# Transitions: Case Justification Sections\r\n\r\nHow case justification sections end and transition to the next section (typically Methods). This guide presents the four transition patterns identified in the 32-article corpus.\r\n\r\n---\r\n\r\n## Overview\r\n\r\n| Transition Type | Prevalence | Description |\r\n|-----------------|------------|-------------|\r\n| Implicit | 66% | Section ends; structure carries readers forward |\r\n| Integrated | 22% | Methods content folded into case section |\r\n| Data-Preview | 6% | Brief mention of what data will show |\r\n| Bridge-Statement | 6% | Explicit question or statement connecting to theory |\r\n\r\n---\r\n\r\n## Type E3: Implicit Transition (66%)\r\n\r\nThe most common approach: the section simply ends, and the structural break to the next section (usually Methods) carries readers forward without explicit transition language.\r\n\r\n### Pattern\r\nSection concludes with final substantive statement. No transition phrase. Next section begins.\r\n\r\n### Examples\r\n\r\n**Standard Context**:\r\n> \"This anti-immigrant context emerges from a long history of immigration policies that criminalize undocumented migration and racialize illegality, linking stereotypes of Mexican origin to perceptions of illegality.\"\r\n>\r\n> [Section ends. Methods section begins.]\r\n\r\n**Deep Historical**:\r\n> \"The vignettes presented in this section provided a broad overview of four major episodes of collective contestation involving the Cairo-based political opposition during the decade preceding the 2011 uprising, showing how relational networks within and across different movement organizations in this sector emerged and evolved over time.\"\r\n>\r\n> [Section ends. Analysis or Methods begins.]\r\n\r\n**Policy-Driven**:\r\n> \"New laws also made it harder for deportees to fight their cases, enabling expedited removal without due process or judicial review.\"\r\n>\r\n> [Section ends. Theory section follows.]\r\n\r\n### When to Use\r\n- Default for all clusters\r\n- When structure is clear and readers don't need explicit guidance\r\n- When the section ends on a strong substantive note\r\n\r\n### Cluster Distribution\r\n- Minimal: 60%\r\n- Standard: 73%\r\n- Deep Historical: 80%\r\n- Comparative: 43%\r\n- Policy-Driven: 75%\r\n\r\n---\r\n\r\n## Type E4: Integrated Transition (22%)\r\n\r\nMethods content is woven into the case section itself, so there's no need for a sharp transition - the section already contains methodological information.\r\n\r\n### Pattern\r\nCase section includes interview details, sample description, or data collection information. Methods section may be abbreviated or absent.\r\n\r\n### Examples\r\n\r\n**Comparative** (signature transition - 43%):\r\n> \"Interviews lasted 45 minutes on average. All interviews occurred in the SROs' assigned schools throughout the regular school day and covered a variety of topics related to the implementation of SROs in the district.\"\r\n\r\n**Standard Context**:\r\n> \"Our sample included 47 participants who completed semi-structured interviews lasting 60-90 minutes.\"\r\n\r\n### What Gets Integrated\r\n\r\n- Interview duration and format\r\n- Sample size and composition\r\n- Data collection location\r\n- Brief protocol description\r\n\r\n### When to Use\r\n- When describing multiple sites (Comparative) - sample details per site\r\n- When methods are straightforward and brief\r\n- When tight word limits require compression\r\n\r\n### Cluster Distribution\r\n- Minimal: 20%\r\n- Standard: 18%\r\n- Deep Historical: 20%\r\n- **Comparative: 43%** (signature)\r\n- Policy-Driven: 0%\r\n\r\n**NOTE**: Integrated transitions are the signature pattern for Comparative sections because describing each site naturally includes information about sampling at each site.\r\n\r\n---\r\n\r\n## Type E1: Data-Preview Transition (6%)\r\n\r\nBrief mention of what the data or analysis will show, setting up the transition to findings or methods.\r\n\r\n### Pattern\r\n> \"While the focus of this paper is on [X], [brief mention of what data will show]...\"\r\n\r\n### Examples\r\n\r\n**Minimal Context**:\r\n> \"While the focus of this paper is on the analysis of the interview data, some findings from the survey of staff will be included.\"\r\n\r\n**Standard Context**:\r\n> \"Through statistical analyses of these data, we did not find consistent patterns of racial disparities in who gets stopped citywide... However, we did find significant disparities in several post-stop outcomes.\"\r\n\r\n### When to Use\r\n- In mixed-methods studies when transitioning from quantitative to qualitative\r\n- When briefly foreshadowing findings\r\n- Primarily in Minimal Context\r\n\r\n### Cluster Distribution\r\n- Minimal: 20%\r\n- Standard: 9%\r\n- Deep Historical: 0%\r\n- Comparative: 0%\r\n- Policy-Driven: 0%\r\n\r\n---\r\n\r\n## Type E2: Bridge-Statement Transition (6%)\r\n\r\nAn explicit statement, often a question, that connects the case context to what theory will address.\r\n\r\n### Pattern\r\n> \"This paper addresses [contradiction/puzzle] by asking: [research question]?\"\r\n\r\n### Examples\r\n\r\n**Policy-Driven**:\r\n> \"This paper addresses Portland's contradictions by asking: how are these contradictory realities reconciled in the lives of Portlanders of color?\"\r\n\r\n**Standard Context**:\r\n> \"How do women navigate these competing expectations? The analysis below examines...\"\r\n\r\n### When to Use\r\n- When ending with a question that theory will address\r\n- In Policy-Driven sections (where case precedes theory)\r\n- When emphasizing the puzzle your research addresses\r\n\r\n### Cluster Distribution\r\n- Minimal: 0%\r\n- Standard: 0%\r\n- Deep Historical: 0%\r\n- **Comparative: 14%**\r\n- **Policy-Driven: 25%**\r\n\r\n**NOTE**: Bridge-Statements are most common in Policy-Driven sections because these sections set up the puzzle that the theory section will address.\r\n\r\n---\r\n\r\n## Transition Patterns by Cluster\r\n\r\n| Cluster | Implicit | Integrated | Data-Preview | Bridge-Statement |\r\n|---------|----------|------------|--------------|------------------|\r\n| Minimal | 60% | 20% | 20% | 0% |\r\n| Standard | 73% | 18% | 9% | 0% |\r\n| Deep Historical | 80% | 20% | 0% | 0% |\r\n| Comparative | 43% | 43% | 0% | 14% |\r\n| Policy-Driven | 75% | 0% | 0% | 25% |\r\n\r\n### Key Patterns\r\n\r\n1. **Implicit dominates** - 66% overall, majority in every cluster\r\n2. **Integrated is Comparative's signature** - 43% (methods per site)\r\n3. **Bridge-Statement appears in Policy-Driven** - 25% (setting up theory)\r\n4. **Data-Preview is Minimal only** - mixed-methods studies\r\n\r\n---\r\n\r\n## Choosing a Transition Type\r\n\r\n### Decision Framework\r\n\r\n1. **Is this a Comparative section with methods per site?**\r\n    Integrated (describe methods within site descriptions)\r\n\r\n2. **Is this Policy-Driven, setting up a theoretical puzzle?**\r\n    Bridge-Statement or Implicit\r\n\r\n3. **Is this mixed-methods with quantitative findings previewed?**\r\n    Data-Preview\r\n\r\n4. **Is the structure clear without explicit transition?**\r\n    Implicit (default)\r\n\r\n### Default Recommendation\r\n\r\nWhen unsure, use **Implicit**. At 66% prevalence, it's the norm. Explicit transitions can sound labored or unnecessary when structure is clear.\r\n\r\n---\r\n\r\n## Common Errors\r\n\r\n### Over-Transitioning\r\n\r\n**Avoid**:\r\n> \"Having described the research context, I now turn to the methods section, where I describe how I collected and analyzed the data.\"\r\n\r\nThis is unnecessary metadiscourse. The structural break is sufficient.\r\n\r\n### Under-Transitioning When Bridge Needed\r\n\r\n**Policy-Driven sections that set up puzzles** benefit from Bridge-Statements:\r\n\r\n**Weak ending** (for Policy-Driven):\r\n> \"The Ontario Works Act slashed benefits by 21.6 percent.\"\r\n\r\n**Stronger ending** (with Bridge):\r\n> \"This context raises the question: how do recipients navigate a system explicitly designed to degrade them?\"\r\n\r\n---\r\n\r\n## Templates by Transition Type\r\n\r\n### Implicit Template\r\n> [Final substantive statement about the case. Period.]\r\n>\r\n> ## Methods\r\n> [New section begins]\r\n\r\n### Integrated Template\r\n> [Site/case description]. Interviews with [N] respondents were conducted between [dates], lasting [duration]. Participants were recruited through [channels].\r\n>\r\n> [Continue with analysis or brief additional methods]\r\n\r\n### Data-Preview Template\r\n> While the focus of this paper is on [primary data type], [secondary data type] will also be discussed. [Brief preview of what data reveal].\r\n>\r\n> ## Methods\r\n> [New section begins]\r\n\r\n### Bridge-Statement Template\r\n> [Statement of puzzle or contradiction established by case context]. This paper addresses this [puzzle/contradiction] by asking: [research question]?\r\n>\r\n> ## Theoretical Framework\r\n> [Theory section begins]\r\n\r\n---\r\n\r\n## Special Considerations\r\n\r\n### Position Affects Transition Choice\r\n\r\n**BEFORE theory (Policy-Driven only)**:\r\n- Bridge-Statement is appropriate\r\n- Section sets up the puzzle that theory addresses\r\n- Research question can end the section\r\n\r\n**AFTER theory (all other clusters)**:\r\n- Implicit is default\r\n- Integrated works for Comparative\r\n- Data-Preview for mixed-methods\r\n\r\n### Don't Force Explicit Transitions\r\n\r\nIf the section ends strongly and structure is clear, let it be. Explicit transitions often add words without adding meaning. The 66% prevalence of Implicit transitions indicates that readers navigate structural breaks effectively.\r\n",
        "plugins/dag-development/.claude-plugin/plugin.json": "{\n  \"name\": \"dag-development\",\n  \"description\": \"Develop causal diagrams (DAGs) from social-science research questions and literature, then render publication-ready figures using Mermaid, R, or Python.\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Neal Caren\"\n  },\n  \"license\": \"MIT\",\n  \"keywords\": [\n    \"causal-diagrams\",\n    \"DAG\",\n    \"development\",\n    \"methods\",\n    \"sociology\"\n  ],\n  \"skills\": \"./skills/\"\n}\n",
        "plugins/dag-development/skills/dag-development/SKILL.md": "---\nname: dag-development\ndescription: Develop causal diagrams (DAGs) from social-science research questions and literature, then render publication-ready figures using Mermaid, R, or Python.\n---\n\n# DAG Development\n\nYou help users **develop causal diagrams (DAGs)** from their research questions, theory, or core paper, and then render them as clean, publication-ready figures using **Mermaid**, **R (ggdag)**, or **Python (networkx)**. This skill spans **conceptual translation** and **technical rendering**.\n\n## When to Use This Skill\n\nUse this skill when users want to:\n- Translate a research question or paper into a DAG\n- Clarify mechanisms, confounders, and selection/measurement structures\n- Turn a DAG into a figure for papers or slides\n- Choose a rendering stack (Mermaid vs R vs Python)\n- Export SVG/PNG/PDF consistently\n\n## Core Principles\n\n1.  **Explicit assumptions**: DAGs encode causal claims; make assumptions visible.\n2.  **Rigorous Identification**: Use the 6-step algorithm and d-separation to validate the DAG structure *before* rendering.\n3.  **Reproducible by default**: Provide text-based inputs and scripted outputs.\n4.  **Exportable assets**: Produce SVG/PNG (and PDF where possible).\n5.  **Tool choice**: Offer three rendering paths with tradeoffs.\n6.  **Minimal styling**: Keep figures simple and journalfriendly.\n\n## Workflow Phases\n\n### Phase 0: Theory  DAG Translation\n**Goal**: Help users turn their current thinking or a core paper into a **DAG Blueprint**.\n- Clarify the causal question and unit of analysis\n- Translate narratives/mechanisms into nodes and edges\n- Record assumptions and uncertain edges\n\n**Guide**: `phases/phase0-theory.md`\n**Concepts**: `confounding.md`, `potential_outcomes.md`\n\n> **Pause**: Confirm the DAG blueprint before auditing.\n\n---\n\n### Phase 1: Critique & Identification\n**Goal**: **Validate** the DAG blueprint using formal rules (Shrier & Platt, Greenland).\n- Run the **6-step algorithm** (Check descendants, non-ancestors).\n- Check for **Collider-Stratification Bias**.\n- Identify the **Sufficient Adjustment Set**.\n- Detect threats from unobserved variables.\n\n**Guide**: `phases/phase1-identification.md`\n**Concepts**: `six_step_algorithm.md`, `d_separation.md`, `colliders.md`, `selection_bias.md`\n\n> **Pause**: Confirm the \"Validated DAG\" (nodes + edges + adjustment strategy) before formatting.\n\n---\n\n### Phase 2: Inputs & Format\n**Goal**: Turn the Validated DAG into renderready inputs.\n- Finalize node list, edge list, and node types (Exposure, Outcome, Latent, Selection).\n- Choose output formats (SVG/PNG/PDF) and layout.\n\n**Guide**: `phases/phase2-inputs.md`\n\n> **Pause**: Confirm the DAG inputs and output target before rendering.\n\n---\n\n### Phase 3: Mermaid Rendering\n**Goal**: Render a DAG quickly from Markdown using Mermaid CLI.\n\n**Guide**: `phases/phase3-mermaid.md`\n\n> **Pause**: Confirm Mermaid output or move to R/Python.\n\n---\n\n### Phase 4: R Rendering (ggdag)\n**Goal**: Render a DAG using R with ggdag for publicationquality plots.\n\n**Guide**: `phases/phase4-r.md`\n\n> **Pause**: Confirm R output or move to Python.\n\n---\n\n### Phase 5: Python Rendering (networkx)\n**Goal**: Render a DAG using Python with `uv` inline dependencies.\n\n**Guide**: `phases/phase5-python.md`\n\n---\n\n## Output Expectations\n\nProvide:\n- A **DAG Blueprint** (Phase 0)\n- An **Identification Memo** (Phase 1)\n- A **DAG source file** (Mermaid `.mmd`, R `.R`, or Python `.py`)\n- **Rendered figure(s)** in SVG/PNG (and PDF when available)\n\n## Invoking Phase Agents\n\nUse the Task tool for each phase:\n\n```\nTask: Phase 3 Mermaid\nsubagent_type: general-purpose\nmodel: sonnet\nprompt: Read phases/phase3-mermaid.md and render the users DAG\n```",
        "plugins/dag-development/skills/dag-development/concepts/colliders.md": "# Colliders and Collider-Stratification Bias\n\n**Definition**: A node is a **collider** on a specific path if two arrowheads meet at that node (e.g., `A -> C <- B`). Here, `C` is a collider between `A` and `B`.\n\n## Key Properties\n\n1.  **Blocking**: A collider *naturally blocks* the path. Information does not flow from `A` to `B` through `C` unless you condition on `C`.\n2.  **Opening (Bias)**: If you condition on a collider (or a descendant of a collider), you **open** the path. This creates a spurious association between `A` and `B`.\n\n## Collider-Stratification Bias\n\nThis bias occurs when you control for a common effect of the exposure and the outcome (or their causes).\n*   **Example**: `Exposure -> Hospitalization <- Outcome`.\n*   If you study only hospitalized patients (conditioning on `Hospitalization`), you induce a correlation between Exposure and Outcome even if none exists in the general population (Berkson's Bias).\n\n## In the 6-Step Algorithm\n*   **Step 4**: Connect any two parents sharing a common child (collider). This visualizes the association created by conditioning on the collider.\n\n## Source\nBased on Shrier & Platt (2008) and Greenland, Pearl, & Robins (1999).\n",
        "plugins/dag-development/skills/dag-development/concepts/confounding.md": "# Confounding Bias\n\n**Definition**: Confounding bias occurs when there is a common cause of the exposure (X) and the outcome (Y) that is not blocked by conditioning.\n\n## Traditional vs. Graphical Definitions\n\n*   **Traditional**: A variable is a confounder if it is associated with X, associated with Y, and not a descendant of X.\n*   **Graphical (DAG)**: Confounding is the presence of an open backdoor path from X to Y. A backdoor path is any path that starts with an arrow pointing *into* X (e.g., `X <- Z -> Y`).\n\n## Addressing Confounding\n\nTo remove confounding, you must \"block\" all backdoor paths.\n*   **Blocking**: Conditioning on a non-collider along the path.\n*   **Caution**: Adjusting for a variable that is NOT a confounder (or is a collider) can *introduce* bias (see `colliders.md`).\n\n## Source\nBased on Shrier & Platt (2008) and Greenland, Pearl, & Robins (1999).\n",
        "plugins/dag-development/skills/dag-development/concepts/d_separation.md": "# d-Separation\n\n**Definition**: d-separation (directional separation) is a criterion to determine if two variables, `X` and `Y`, are independent given a set of conditioning variables `Z`.\n\n## Rules for Paths\n\nA path between `X` and `Y` is **blocked** (d-separated) by a set `Z` if:\n1.  **Chain**: The path contains `A -> M -> B` and `M` is in `Z` (conditioned on).\n2.  **Fork**: The path contains `A <- C -> B` (common cause) and `C` is in `Z`.\n3.  **Collider**: The path contains `A -> C <- B` and **neither** `C` **nor** any of its descendants are in `Z`.\n\n## Implication\nIf all paths between `X` and `Y` are blocked by `Z`, then `X` and `Y` are conditionally independent given `Z`.\n*   If `X` and `Y` are d-separated in the graph (after removing arrows out of `X`), then `Z` is a sufficient adjustment set to identify the causal effect.\n\n## Source\nGreenland, Pearl, & Robins (1999).\n",
        "plugins/dag-development/skills/dag-development/concepts/potential_outcomes.md": "# Potential Outcomes and DAGs\n\n**Definition**: The Potential Outcomes framework (Neyman-Rubin Causal Model) defines a causal effect as the difference between two potential states:\n*   $Y(1)$: The outcome if treated ($X=1$).\n*   $Y(0)$: The outcome if untreated ($X=0$).\n\n## Mapping to DAGs\n\n*   **Consistency**: The observed outcome $Y$ is determined by the treatment $X$ and potential outcomes: $Y = X \\cdot Y(1) + (1-X) \\cdot Y(0)$.\n*   **Ignorability (Exchangeability)**: The treatment assignment $X$ is independent of potential outcomes $(Y(0), Y(1))$ given covariates $Z$.\n    *   In DAG terms: $X$ is d-separated from the unobserved error terms affecting $Y$, conditional on $Z$.\n\n## Key Translations\n\n1.  **Confounding**: In Potential Outcomes, confounding means $X$ depends on factors that also predict $Y(0)$ or $Y(1)$. In DAGs, this is a common parent of $X$ and $Y$.\n2.  **Selection Bias**: Conditioning on a variable associated with both $Y(0)/Y(1)$ and $X$.\n3.  **SUTVA (Stable Unit Treatment Value Assumption)**: Assumes no interference between units. In DAGs, this means the outcome of unit $i$ is not caused by the treatment of unit $j$.\n\n## Source\nImbens & Rubin (2015); Hernn & Robins (2020).\n",
        "plugins/dag-development/skills/dag-development/concepts/selection_bias.md": "# Selection Bias\n\n**Definition**: In the structural approach (DAGs), selection bias is often a form of **collider-stratification bias**. It occurs when participation, follow-up, or observation (`S`) is a common effect of the exposure (`X`) and the outcome (`Y`) (or their ancestors).\n\n## Mechanism\n\nStructure: `X -> S <- Y`\n*   If we restrict analysis to the observed sample (`S=1`), we are conditioning on `S`.\n*   Since `S` is a collider, conditioning on it opens the path between `X` and `Y`.\n*   This creates a non-causal association between exposure and outcome.\n\n## Examples\n*   **Loss to follow-up**: If treatment (`X`) causes side effects that lead to dropout (`S`), and the disease severity (`Y`) also leads to dropout, analyzing only complete cases biases the result.\n*   **Berkson's Bias**: Hospital admission (`S`) depends on two independent diseases (`X` and `Y`). In the hospital population, `X` and `Y` appear associated.\n\n## Source\nBased on Hernn, Hernndez-Daz, & Robins (2004) and Shrier & Platt (2008).\n",
        "plugins/dag-development/skills/dag-development/concepts/six_step_algorithm.md": "# The 6-Step Algorithm (Shrier & Platt)\n\nThis algorithm determines if a proposed set of covariates (`S`) reduces bias for the effect of `X` on `Y`.\n\n## The Algorithm\n\n1.  **Check Descendants**: Ensure no variable in `S` is a descendant of `X` (caused by `X`). If yes, remove it or stop (it may be a mediator).\n2.  **Delete Non-Ancestors**: Delete all variables that are NOT ancestors of `X`, `Y`, or variables in `S`. (Keep `S`, `X`, `Y`).\n3.  **Delete Outgoing X**: Delete all lines originating from `X`. (We want to see if non-causal paths remain).\n4.  **Connect Parents (Moralization)**: For every collider (child with two parents) that is in `S` (or has a descendant in `S`), connect the two parents with a dashed line.\n    *   *Concept*: Conditioning on a collider induces an association between its parents.\n5.  **Strip Arrowheads**: Remove all arrowheads (treat lines as undirected associations).\n6.  **Delete S Connections**: Delete all lines touching any variable in `S`. (Conditioning blocks these paths).\n\n## Interpretation\n\n*   **If X and Y are disconnected**: The set `S` is sufficient to remove confounding bias (for the specific DAG structure assumed).\n*   **If X and Y are still connected**: The set `S` is insufficient; bias remains.\n\n## Source\nShrier & Platt (2008), adapting Pearl.\n",
        "plugins/dag-development/skills/dag-development/phases/phase0-theory.md": "# Phase 0: Theory  DAG Translation\n\nYou are helping the user turn their current thinking into a **DAG blueprint** that can be rendered later. This phase is about *conceptual translation*, not statistical adjustment decisions.\n\n## Why This Phase Matters\n\nA DAG is an explicit statement of causal assumptions. It forces clarity about **what causes what**, **what is measured**, **what is conditioned on**, and **where selection or observation processes might bias interpretation**. This is useful in both quantitative and qualitative research because it makes implicit causal claims visible and testable.\n\n## Inputs to Request\n\nPrimary path (recommended):\n- **One core paper** (full text or PDF) the user wants translated into a DAG.\n\nAlternative inputs if no paper is available:\n- **Research notes** (bullets, memos, fieldnotes, or a theory summary)\n- **Draft outline** or **methods section**\n- **Concept map** or **variable list**\n\nAlways ask for:\n- **Causal question**: What is the effect of X on Y, and over what time horizon?\n- **Unit of analysis**: Individual, organization, neighborhood, field, event.\n- **Treatment definition**: What does X actually mean? Is it a policy, exposure, practice, event, or status? Are there multiple versions of X?\n- **Outcome definition**: What counts as Y? How is it measured or observed?\n- **Key context**: Setting, time period, institutions, constraints.\n- **Selection/visibility**: Who enters the data, and who becomes visible to the researcher?\n\n## Translation Workflow (Expanded)\n\n### 1. Clarify the causal question and estimand\n- Define **exposure (X)**, **outcome (Y)**, and **contrast** (e.g., higher vs lower X).\n- State the **causal estimand** in plain language (total effect vs. direct effect; shortterm vs longterm).\n- Specify **time ordering** (t0, t1, t2). Treat timestamped variables as distinct nodes when needed (X_t0, X_t1).\n\n### 2. Extract claims from the core paper (if provided)\nIf the user provided a core paper, do this first:\n- Scan **abstract, theory, methods, results, discussion** for causal claims.\n- Extract each claim into a **claim table** (below).\n- Prioritize claims that explicitly use causal language (cause, effect, influence, leads to).\n\n### 3. Define the treatment and outcome precisely\n- If X has **multiple versions**, represent as separate nodes or note as an assumption.\n- If Y is a **constructed measure**, add a **measurement node** (M_Y) and note what determines observation.\n\n### 4. Decompose the narrative into mechanisms\nAsk: If X changes, *through what processes* would Y change?\n- Convert each mechanism into a **mediator** node.\n- Use short, concrete labels (e.g., resource access, network tie, policy enforcement).\n\n### 5. Identify common causes (confounding structure)\nAsk: What prior factors shape both X and Y?\n- Add these as **parents** of X and Y.\n- If a common cause is not measured, mark as **U** (unobserved) and note it in assumptions.\n\n### 6. Identify selection and observation processes\nFor both quant and qual work, selection matters.\n- **Selection into sample**: Who is observed and why? Add **S** (selection) node.\n- **Observation/measurement**: What affects whether X or Y is recorded? Add **M_X** and **M_Y** if needed.\n- Draw arrows into **S** from factors that affect inclusion; note that conditioning on S can induce collider bias.\n\n### 7. Check for colliders and posttreatment variables\n- If a node is a **common effect** of two causes, it is a collider.\n- Avoid conditioning on descendants of X when the goal is the total effect (unless explicitly estimating mediated effects).\n- If X influences a confounder later in time, mark as **timevarying confounding**.\n\n### 8. Build the firstpass DAG blueprint\nCreate a list of:\n- Nodes with short labels\n- Directed edges (A -> B)\n- **Assumptions log**: why each edge exists, and what is omitted\n- **Uncertain edges**: edges that are plausible but contested\n\n### 9. Create alternative DAGs if needed\nIf more than one theory fits the data:\n- Draft 23 competing DAGs\n- Note which edges differ and what evidence would adjudicate between them\n\n## Claim Table Template (PaperFirst Workflow)\n\nUse this table to translate paper text into DAG edges:\n\n```\nClaim ID | Source (section/page) | Claim (verbatim/paraphrase) | Nodes | Edge | Type | Confidence\n---------|------------------------|-----------------------------|-------|------|------|-----------\nC1       | Theory p.4             | X increases Y through M   | X,M,Y | X->M, M->Y | mediator | High\nC2       | Methods p.7            | Z is controlled because  | Z,X,Y | Z->X, Z->Y | confounder | Medium\nC3       | Discussion p.12        | Selection into sample    | S,X,Y | X->S, Y->S | selection | Medium\n```\n\nThen synthesize the DAG from the union of highconfidence edges, and flag lowconfidence edges as uncertain or alternative.\n\n## Quantitative Translation Checklist\n\n- Are X and Y defined as **pre/post** with clear temporal order?\n- Are there **pretreatment confounders** that influence both?\n- Are there **posttreatment mediators** you should not adjust for?\n- Is there **selection** (sample inclusion, attrition, missingness) affected by X or Y?\n- Are there **timevarying confounders** (Z_t that both affects later X and is affected by earlier X)?\n\n## Qualitative Translation Checklist\n\n- What is the **narrative mechanism** linking X to Y?\n- What **decisions or actions** bridge the link (mediators)?\n- Who becomes **visible** or **recorded** (selection/observation nodes)?\n- What **contextual conditions** are required for the mechanism (context nodes)?\n- Where are **plausible alternative stories** that would change the edges?\n\n## Qualitative-Specific Prompts\n\nUse these to translate interpretive narratives into DAGs:\n- What decisions or actions bridge X to Y?  mechanisms/mediators\n- Who gets into the story?  selection node\n- What accounts are more likely to be visible or recorded?  measurement nodes\n- What conditions make the mechanism possible?  moderators or context nodes\n- What would have to be different for the mechanism *not* to operate?  alternative edges\n\n## Quantitative-Specific Prompts\n\n- What pretreatment factors drive both X and Y?  confounders\n- Is any variable affected by X and also used for adjustment?  potential post-treatment bias\n- Is selection into the dataset influenced by X or Y?  selection node\n\n## Common Pitfalls to Flag\n\n- **Adjusting for mediators** when estimating total effects.\n- **Conditioning on colliders** (common effects) and inducing bias.\n- **Ignoring selection into data** (who is observed and why).\n- **Ambiguous treatment** (multiple versions of X without clarity).\n- **Missing time ordering** (treating processes as contemporaneous when they are not).\n\n## Output: DAG Blueprint\n\nProvide a structured blueprint the rendering phases can use:\n\n```\nDAG Blueprint\n\nCausal Question:\n- Effect of X on Y over [time window]\n\nNodes:\n- X (exposure)\n- Y (outcome)\n- M1 (mechanism)\n- Z1 (confounder)\n- S (selection)\n- U1 (unobserved)\n\nEdges:\n- Z1 -> X\n- Z1 -> Y\n- X -> M1\n- M1 -> Y\n- U1 -> X\n- U1 -> Y\n- X -> S\n- Y -> S\n\nAssumptions Log:\n- Z1 is prior to X and Y\n- M1 is downstream of X\n- S represents inclusion into the dataset\n\nUncertain Edges:\n- U1 -> X (low confidence)\n- U1 -> Y (medium confidence)\n```\n\n## Handoff to Phase 1 (Rendering Inputs)\n\nBefore moving on, produce the **renderready inputs** Phase 1 needs:\n- **Node labels**: short labels for the figure (28 words each)\n- **Edge list**: clean `A -> B` statements\n- **Node types**: observed, unobserved (U), selection (S), measurement (M)\n- **Uncertain edges**: mark for dashed/dotted styling\n- **Grouping** (optional): context / mechanisms / selection subgraphs\n\nDeliver these as a simple block the user can paste into Phase 1.\n\n## When Youre Done\n\nReturn a summary to the orchestrator with:\n1. The DAG blueprint (nodes + edges)\n2. Assumptions log\n3. Uncertain edges and alternative DAGs\n4. Any missing inputs to request\n\n## Suggested Readings\n\n- Greenland, Pearl, Robins (1999). Causal diagrams for epidemiologic research.\n- Shrier & Platt (2008). Reducing bias through directed acyclic graphs.\n- Hernan & Robins (2020, with ongoing updates). Causal Inference: What If.\n- Morgan & Winship (2007/2015). Counterfactuals and Causal Inference (causal graphs chapters).\n",
        "plugins/dag-development/skills/dag-development/phases/phase1-identification.md": "# Phase 1: Critique & Identification\n\nYou are helping the user **validate** their DAG blueprint from Phase 0. This phase is about rigorous identification: ensuring the proposed structure identifies the causal effect and does not introduce bias.\n\n## Goal\nTransform a \"Theory DAG\" (Phase 0) into a \"Validated DAG\" by:\n1.  Checking for structural biases (colliders, mediators).\n2.  Determining the **sufficient adjustment set**.\n3.  Identifying threats to validity (unobserved confounders).\n\n## Core Concepts to Apply\nRefer to the `concepts/` directory for definitions:\n- `concepts/six_step_algorithm.md` (The Shrier & Platt audit)\n- `concepts/d_separation.md` (Greenlands sufficiency rules)\n- `concepts/confounding.md`\n- `concepts/colliders.md`\n\n## The Identification Audit\n\nAsk the user to run their DAG through this checklist (or do it for them if the DAG is simple):\n\n### 1. The Mediator Check\n*   \"Are any of your control variables actually **downstream** of the treatment (X)?\"\n*   *Rule*: Do not adjust for mediators (descendants of X) unless you specifically want the Direct Effect (and know the risks).\n*   *Action*: Remove descendants of X from the adjustment set (Step 1 of 6-Step Algorithm).\n\n### 2. The Collider Check\n*   \"Does X and Y both influence any variable C (or C's ancestor) that we are conditioning on (e.g., by selecting only those people)?\"\n*   *Rule*: Conditioning on a collider opens a non-causal path.\n*   *Action*: Check for **Selection Bias** (`concepts/selection_bias.md`).\n\n### 3. The Confounder Check (Backdoor Criterion)\n*   \"Is there an open backdoor path from X to Y?\" (Any path starting with an arrow into X).\n*   *Rule*: You must block all backdoor paths.\n*   *Action*: Identify the **Minimally Sufficient Adjustment Set**.\n\n### 4. The Unobserved Variable Check\n*   \"Are there common causes of X and Y that are **unmeasured** (U)?\"\n*   *Sensitivity*: If U exists and is strong, the effect is not identifiable. Ask the user to note this limitation.\n\n## Output: Validated DAG Strategy\n\nProduce a short **Identification Memo**:\n1.  **Valid Adjustment Set**: List the variables that *must* be controlled.\n2.  **Variables to Ignore**: List variables that must *not* be controlled (colliders, mediators).\n3.  **Threats**: Note any unblocked paths due to unobserved variables.\n\n## Handoff to Phase 2\nOnce the identification strategy is set, move to **Phase 2: Inputs & Format** to prepare for rendering.\n",
        "plugins/dag-development/skills/dag-development/phases/phase2-inputs.md": "# Phase 2: Inputs & Format\n\nYou are collecting the minimum required inputs to render a DAG figure.\n\n## Required Inputs\n\nAsk for:\n- **Nodes**: Short labels (28 words max). If longer names are needed, define a legend.\n- **Edges**: Directed edges in `A -> B` format.\n- **Node types**: observed, unobserved (U), selection (S), measurement (M).\n- **Uncertain edges**: list any edges to be styled as dashed/dotted.\n- **Output**: Preferred format(s): SVG, PNG, PDF.\n- **Tool preference**: Mermaid, R, Python, or show options.\n\nIf Phase 0 produced a DAG blueprint, extract the above directly from it.\n\n## Optional Rendering Choices\n\nOffer (do not require):\n- **Layout**: lefttoright (LR), topdown (TD), or circular.\n- **Grouping**: subgraphs like *context*, *mechanisms*, *selection*.\n- **Styling**: color for observed vs unobserved; dashed for uncertain edges; dotted for selection/measurement paths.\n- **Target use**: paper, slide, appendix (affects size and format).\n\n## Example Input Format\n\n```\nNodes:\n- Z (confounder)\n- X (exposure)\n- M (mediator)\n- Y (outcome)\n - U1 (unobserved)\n - S (selection)\n\nEdges:\n- Z -> X\n- Z -> Y\n- X -> M\n- M -> Y\n- X -> Y\n - U1 -> X\n - U1 -> Y\n - X -> S\n - Y -> S\n\nUncertain edges:\n- U1 -> X\n- U1 -> Y\n\nNode types:\n- observed: Z, X, M, Y\n- unobserved: U1\n- selection: S\n\nOutput: SVG + PNG\nTool: Mermaid\nLayout: LR\nTarget: paper\n```\n\n## If the User Only Has a Sketch\n\nOffer to translate:\n- Handdrawn DAG  node/edge list\n- Equation models  DAG edges\n\n## Output\n\nSummarize:\n- Confirmed node list and node types\n- Confirmed edge list + uncertain edges\n- Chosen rendering tool, output formats, and layout\n\n## When Youre Done\n\nReturn a short intake memo to the orchestrator with the inputs above.\n",
        "plugins/dag-development/skills/dag-development/phases/phase3-mermaid.md": "# Phase 3: Mermaid Rendering\n\nYou are rendering a DAG using Mermaid CLI. Mermaid is best for quick, textbased diagrams that live in Markdown.\n\n## Input\n\nUse the confirmed node/edge list from Phase 0.\n\n## Mermaid Source Template\n\nCreate `dag.mmd`:\n\n```\nflowchart LR\n  Z --> X\n  X --> M\n  M --> Y\n  X --> Y\n  Z --> Y\n```\n\nUse `LR` (lefttoright) for most diagrams. Switch to `TD` (topdown) if the diagram is tall.\n\n## Render Command (Verified)\n\n```\nmmdc -i dag.mmd -o dag.svg\n```\n\nYou can also render PNG:\n\n```\nmmdc -i dag.mmd -o dag.png\n```\n\n## Output\n\nProvide:\n- `dag.mmd`\n- `dag.svg` (and/or `dag.png`)\n\n## When Youre Done\n\nReturn a short summary of the files created and any layout adjustments made.\n",
        "plugins/dag-development/skills/dag-development/phases/phase4-r.md": "# Phase 4: R Rendering (ggdag)\n\nYou are rendering a DAG using R with `ggdag` (built on ggplot2). This is good for publicationquality figures.\n\n## Input\n\nUse the confirmed node/edge list from Phase 0.\n\n## R Script Template (Verified)\n\nCreate `dag_r.R`:\n\n```r\nlibrary(ggdag)\n\n# Define a simple DAG\nx <- dagitty::dagitty('dag { X -> M -> Y; X -> Y; Z -> X; Z -> Y }')\n\n# Plot and save\np <- ggdag(x, layout = 'circle') + ggplot2::theme_void()\n\nggplot2::ggsave('dag_r.png', p, width = 5, height = 4, dpi = 150)\n```\n\n## Run\n\n```\nRscript dag_r.R\n```\n\n## Output\n\nProvide:\n- `dag_r.R`\n- `dag_r.png`\n\n## When Youre Done\n\nReturn a short summary of the files created and any layout adjustments made.\n",
        "plugins/dag-development/skills/dag-development/phases/phase5-python.md": "# Phase 5: Python Rendering (networkx)\n\nYou are rendering a DAG using Python with inline dependencies via `uv`. This is good for scriptable, crossplatform outputs.\n\n## Input\n\nUse the confirmed node/edge list from Phase 0.\n\n## Python Script Template (Verified)\n\nCreate `dag_py.py`:\n\n```python\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\nG = nx.DiGraph()\nG.add_edges_from([\n    ('Z','X'),\n    ('X','M'),\n    ('M','Y'),\n    ('X','Y'),\n    ('Z','Y'),\n])\n\npos = nx.spring_layout(G, seed=7)\n\nplt.figure(figsize=(4,3))\nax = plt.gca()\nnx.draw_networkx_nodes(G, pos, node_color='#E6F2FF', edgecolors='#1F4E79', node_size=1200, ax=ax)\nnx.draw_networkx_labels(G, pos, font_size=10, ax=ax)\nnx.draw_networkx_edges(G, pos, arrows=True, arrowstyle='-|>', arrowsize=16, width=1.5, ax=ax)\nplt.axis('off')\nplt.tight_layout()\nplt.savefig('dag_py.png', dpi=150)\nprint('wrote dag_py.png')\n```\n\n## Run with uv (Verified)\n\n```\nuv run --with networkx --with matplotlib python dag_py.py\n```\n\n## Output\n\nProvide:\n- `dag_py.py`\n- `dag_py.png`\n\n## When Youre Done\n\nReturn a short summary of the files created and any layout adjustments made.\n",
        "plugins/genre-skill-builder/.claude-plugin/plugin.json": "{\r\n  \"name\": \"genre-skill-builder\",\r\n  \"version\": \"1.0.0\",\r\n  \"description\": \"Meta-skill for creating genre-analysis-based writing skills. Analyzes a corpus of article sections, discovers clusters, and generates complete skills with phases, cluster guides, and techniques.\",\r\n  \"skills\": \"./skills/\"\r\n}\r\n",
        "plugins/genre-skill-builder/skills/genre-skill-builder/SKILL.md": "---\r\nname: genre-skill-builder\r\ndescription: Meta-skill for creating genre-analysis-based writing skills. Analyzes a corpus of article sections, discovers clusters, and generates complete skills with phases, cluster guides, and techniques.\r\n---\r\n\r\n# Genre Skill Builder\r\n\r\nYou help researchers create **writing skills** based on systematic genre analysis. Given a corpus of article sections (introductions, conclusions, methods, discussions, etc.), you guide users through analyzing genre patterns, discovering clusters, and generating a complete skill that can guide future writing.\r\n\r\n## What This Skill Does\r\n\r\nThis is a **meta-skill**it creates other skills. The output is a fully-functional writing skill like `lit-writeup` or `interview-bookends`, with:\r\n- A main `SKILL.md` with genre-based guidance\r\n- Phase files for a structured writing workflow\r\n- Cluster profiles based on discovered patterns\r\n- Technique guides for sentence-level craft\r\n\r\n## When to Use This Skill\r\n\r\nUse this skill when you want to:\r\n- Create a writing guide for a **specific article section** (e.g., Discussion sections, Abstract, Methodology)\r\n- Base guidance on **empirical analysis** of a corpus rather than intuition\r\n- Generate a skill that follows the **repository's phased architecture**\r\n- Produce **cluster-based guidance** that recognizes different writing styles\r\n\r\n## What You Need\r\n\r\n1. **A corpus of article sections** (30+ recommended)\r\n   - Text files, PDFs, or markdown\r\n   - All from the same section type (all introductions, all conclusions, etc.)\r\n   - Ideally from target venues (e.g., *Social Problems*, *Social Forces*)\r\n\r\n2. **A model skill to learn from**\r\n   - An existing skill like `lit-writeup` or `interview-bookends`\r\n   - Provides structural template for the generated skill\r\n\r\n## Connection to Other Skills\r\n\r\nThis skill adapts the methodology from:\r\n\r\n| Skill | What We Borrow |\r\n|-------|----------------|\r\n| **interview-analyst** | Systematic coding approach (Phases 1-3) |\r\n| **lit-writeup** | Cluster-based writing guidance structure |\r\n| **interview-bookends** | Benchmarks and coherence checking |\r\n\r\n## Core Principles\r\n\r\n1. **Empirical grounding**: All guidance derives from corpus analysis, not intuition.\r\n\r\n2. **Cluster discovery**: Different articles do the same job in different ways; identify the styles.\r\n\r\n3. **Quantitative + qualitative**: Count features AND interpret patterns.\r\n\r\n4. **Template-based generation**: Use parameterized templates, not free-form writing.\r\n\r\n5. **Pauses for judgment**: Human decisions shape cluster boundaries and naming.\r\n\r\n6. **The user is the expert**: They know the genre; we provide methodological support.\r\n\r\n## Workflow Phases\r\n\r\n### Phase 0: Scope Definition & Model Selection\r\n**Goal**: Define what we're building and what to learn from.\r\n\r\n**Process**:\r\n- Identify the target article section (introduction, conclusion, methods, discussion, etc.)\r\n- Select an existing skill as a structural model\r\n- Review model skill to identify elements to extract\r\n- Confirm corpus location and article count\r\n\r\n**Output**: Scope definition memo with target section, model skill, corpus path.\r\n\r\n> **Pause**: User confirms scope and model selection.\r\n\r\n---\r\n\r\n### Phase 1: Corpus Immersion\r\n**Goal**: Build quantitative profile of the corpus.\r\n\r\n**Process**:\r\n- Count articles, calculate word counts, paragraph counts\r\n- Identify structural patterns (headings, subsections)\r\n- Generate descriptive statistics (median, IQR, range)\r\n- Flag outliers and notable examples\r\n- Create initial observations about variation\r\n\r\n**Output**: Immersion report with corpus statistics.\r\n\r\n> **Pause**: User reviews quantitative profile.\r\n\r\n---\r\n\r\n### Phase 2: Systematic Genre Coding\r\n**Goal**: Code each article for genre features.\r\n\r\n**Process**:\r\n- Develop codebook based on model skill's categories\r\n- Code opening moves, structural elements, rhetorical strategies\r\n- Track frequency and co-occurrence of features\r\n- Build article-by-article coding database\r\n- Identify preliminary cluster candidates\r\n\r\n**Output**: Codebook, article codes, preliminary clusters.\r\n\r\n> **Pause**: User reviews codebook and sample codes.\r\n\r\n---\r\n\r\n### Phase 3: Pattern Interpretation & Cluster Discovery\r\n**Goal**: Identify stable patterns and define cluster profiles.\r\n\r\n**Process**:\r\n- Analyze code co-occurrence patterns\r\n- Define 3-6 cluster characteristics\r\n- Calculate benchmarks for each cluster\r\n- Identify signature moves and prohibited moves\r\n- Extract exemplar quotes/passages\r\n- Name clusters meaningfully\r\n\r\n**Output**: Cluster profiles with benchmarks and exemplars.\r\n\r\n> **Pause**: User confirms cluster definitions.\r\n\r\n---\r\n\r\n### Phase 4: Skill Generation\r\n**Goal**: Generate the complete skill file structure.\r\n\r\n**Process**:\r\n- Generate `SKILL.md` using template + findings\r\n- Generate phase files (typically 3-4 for writing skills)\r\n- Generate cluster guide files (one per cluster)\r\n- Generate technique guide files\r\n- Generate `plugin.json`\r\n- Prepare `marketplace.json` entry\r\n\r\n**Output**: Complete skill directory structure.\r\n\r\n> **Pause**: User reviews generated skill files.\r\n\r\n---\r\n\r\n### Phase 5: Validation & Testing\r\n**Goal**: Verify skill quality and test with sample input.\r\n\r\n**Process**:\r\n- Check all files are syntactically correct\r\n- Verify benchmarks match analysis data\r\n- Ensure cluster coverage is complete\r\n- Identify any gaps or inconsistencies\r\n- Optionally test with sample input\r\n\r\n**Output**: Validation report with quality assessment.\r\n\r\n---\r\n\r\n## Folder Structure for Analysis\r\n\r\n```\r\nproject/\r\n corpus/                 # Article sections to analyze\r\n    article-01.md\r\n    article-02.md\r\n    ...\r\n analysis/\r\n    phase0-scope/       # Scope definition\r\n    phase1-immersion/   # Quantitative profiling\r\n    phase2-coding/      # Genre coding\r\n    phase3-clusters/    # Pattern analysis\r\n    phase4-generation/  # Generated skill files\r\n    phase5-validation/  # Quality assessment\r\n output/                 # Final skill plugin\r\n     plugins/[skill-name]/\r\n```\r\n\r\n## Code Categories to Track\r\n\r\nBased on model skills, these are typical genre features to code:\r\n\r\n### Structural Features\r\n- Word count, paragraph count\r\n- Presence of subsections\r\n- Heading structure\r\n- Position of key elements\r\n\r\n### Opening Moves\r\n- Phenomenon-led, stakes-led, theory-led, case-led, question-led\r\n- First sentence type\r\n- Hook strategy\r\n\r\n### Rhetorical Moves\r\n- Gap identification\r\n- Contribution claims\r\n- Limitations\r\n- Future directions\r\n- Callbacks (for conclusions)\r\n\r\n### Citation Patterns\r\n- Citation density\r\n- Integration style (parenthetical, author-subject, quote-then-cite)\r\n- Anchor sources vs. supporting citations\r\n\r\n### Linguistic Features\r\n- Hedging level\r\n- Temporal markers\r\n- Transition patterns\r\n- Key phrases\r\n\r\n## Cluster Discovery Guidelines\r\n\r\n### Minimum Clusters: 3\r\nIf fewer than 3 patterns emerge, the corpus may be too homogeneous or the coding scheme too coarse.\r\n\r\n### Maximum Clusters: 6\r\nMore than 6 typically indicates over-differentiation; look for higher-level groupings.\r\n\r\n### Cluster Naming\r\nName clusters by their **dominant strategy**, not their prevalence:\r\n- \"Gap-Filler\" not \"Cluster 1\"\r\n- \"Theory-Extension\" not \"Common Type\"\r\n- \"Problem-Driven\" not \"Applied Approach\"\r\n\r\n### Cluster Validation\r\nEach cluster should have:\r\n- At least 10% of corpus (minimum 3 articles if corpus < 30)\r\n- Distinctive benchmark values\r\n- Clear signature moves\r\n- At least one exemplar article\r\n\r\n## Template System\r\n\r\nPhase 4 uses parameterized templates. Key parameters:\r\n\r\n| Parameter | Source |\r\n|-----------|--------|\r\n| `{{skill_name}}` | Phase 0 user input |\r\n| `{{target_section}}` | Phase 0 user input |\r\n| `{{cluster_names}}` | Phase 3 cluster discovery |\r\n| `{{benchmarks}}` | Phase 1-2 statistics |\r\n| `{{opening_moves}}` | Phase 2 coding |\r\n| `{{signature_phrases}}` | Phase 2-3 analysis |\r\n\r\n## Technique Guides\r\n\r\nReference these guides for phase-specific instructions:\r\n\r\n| Guide | Purpose |\r\n|-------|---------|\r\n| `phases/phase0-scope.md` | Scope definition, model selection |\r\n| `phases/phase1-immersion.md` | Quantitative profiling |\r\n| `phases/phase2-coding.md` | Genre coding methodology |\r\n| `phases/phase3-interpretation.md` | Cluster discovery |\r\n| `phases/phase4-generation.md` | Skill file generation |\r\n| `phases/phase5-validation.md` | Quality verification |\r\n\r\n## Templates\r\n\r\n| Template | Purpose |\r\n|----------|---------|\r\n| `templates/skill-template.md` | Main SKILL.md structure |\r\n| `templates/phase-template.md` | Phase file structure |\r\n| `templates/cluster-template.md` | Cluster profile structure |\r\n| `templates/technique-template.md` | Technique guide structure |\r\n\r\n## Invoking Phase Agents\r\n\r\nUse the Task tool for each phase:\r\n\r\n```\r\nTask: Phase 2 Genre Coding\r\nsubagent_type: general-purpose\r\nmodel: sonnet\r\nprompt: Read phases/phase2-coding.md and execute for [user's project]. Corpus is in [location]. Model skill is [skill name].\r\n```\r\n\r\n## Model Recommendations\r\n\r\n| Phase | Model | Rationale |\r\n|-------|-------|-----------|\r\n| **Phase 0**: Scope | **Sonnet** | Planning, structural decisions |\r\n| **Phase 1**: Immersion | **Sonnet** | Counting, statistics |\r\n| **Phase 2**: Coding | **Sonnet** | Systematic processing |\r\n| **Phase 3**: Interpretation | **Opus** | Pattern recognition, cluster naming |\r\n| **Phase 4**: Generation | **Opus** | Template adaptation, prose quality |\r\n| **Phase 5**: Validation | **Sonnet** | Verification, checking |\r\n\r\n## Starting the Process\r\n\r\nWhen the user is ready to begin:\r\n\r\n1. **Ask about the target**:\r\n   > \"What article section do you want to create a writing skill for? (e.g., introduction, conclusion, discussion, methods)\"\r\n\r\n2. **Ask about the corpus**:\r\n   > \"Where is your corpus of articles? How many articles do you have?\"\r\n\r\n3. **Ask about the model skill**:\r\n   > \"Which existing skill should I use as a structural model? Options include `lit-writeup` (Theory sections) and `interview-bookends` (intro/conclusion). I can also review other skills if you prefer.\"\r\n\r\n4. **Ask about output**:\r\n   > \"What should the new skill be named? (e.g., `discussion-writer`, `methods-guide`)\"\r\n\r\n5. **Proceed with Phase 0** to formalize scope.\r\n\r\n## Key Reminders\r\n\r\n- **Corpus size matters**: 30+ articles recommended for stable clusters.\r\n- **Variation is the goal**: A homogeneous corpus won't reveal clusters.\r\n- **Human judgment required**: Cluster boundaries and names need user input.\r\n- **Templates constrain**: Generated skills follow established patterns, not novel structures.\r\n- **Test the output**: The best validation is using the generated skill.\r\n- **Iteration expected**: First-pass clusters often need refinement.\r\n",
        "plugins/genre-skill-builder/skills/genre-skill-builder/phases/phase0-scope.md": "# Phase 0: Scope Definition & Model Selection\r\n\r\nYou are executing Phase 0 of the genre-skill-builder workflow. Your goal is to establish what skill you're building and what existing skill to learn from.\r\n\r\n## Why This Phase Matters\r\n\r\nScope definition prevents wasted effort. A clear target section, appropriate model skill, and confirmed corpus determine everything that follows. Ambiguity here cascades into confusion later.\r\n\r\n## Inputs\r\n\r\nBefore starting, gather from the user:\r\n1. **Target section**: What article section will the skill help write?\r\n2. **Corpus location**: Where are the articles to analyze?\r\n3. **Model skill preference**: Which existing skill to use as template?\r\n4. **Output skill name**: What should the new skill be called?\r\n\r\n## Your Tasks\r\n\r\n### 1. Confirm Target Section\r\n\r\nGet clarity on what the skill will help users write:\r\n\r\n- **Introduction**: Opening section that frames the article\r\n- **Conclusion/Discussion**: Closing section that interprets findings\r\n- **Theory/Literature Review**: Background section positioning contribution\r\n- **Methods**: Section describing research design and data\r\n- **Findings/Results**: Section presenting empirical patterns\r\n- **Abstract**: Condensed summary of entire article\r\n- **Other**: User-specified section type\r\n\r\nDocument:\r\n- Section name (as it appears in target journals)\r\n- Alternative names (e.g., \"Background\" vs. \"Literature Review\")\r\n- Typical location in article structure\r\n- Key rhetorical functions\r\n\r\n### 2. Assess Corpus\r\n\r\nReview the corpus to confirm viability:\r\n\r\n```markdown\r\n## Corpus Assessment\r\n\r\n**Location**: [path to corpus]\r\n**Article count**: [number]\r\n**Format**: [txt/md/pdf]\r\n\r\n**Viability check**:\r\n- [ ] 30+ articles (recommended for stable clusters)\r\n- [ ] All same section type (not mixed sections)\r\n- [ ] From comparable venues (similar genre expectations)\r\n- [ ] Text is extractable (not image-only PDFs)\r\n\r\n**Venue distribution** (if known):\r\n- [Journal 1]: X articles\r\n- [Journal 2]: Y articles\r\n```\r\n\r\nIf corpus has fewer than 30 articles, note this as a limitation but proceedclusters will be less stable.\r\n\r\n### 3. Select Model Skill\r\n\r\nReview available model skills and select the most appropriate:\r\n\r\n| Skill | Best For | Key Features |\r\n|-------|----------|--------------|\r\n| `lit-writeup` | Theory/Lit Review sections | 5 clusters, paragraph functions, citation patterns |\r\n| `interview-bookends` | Intro/Conclusion pairs | Coherence checking, callbacks, opening moves |\r\n| `interview-writeup` | Methods/Findings | Quote integration, anchor/echo patterns |\r\n\r\nFor each candidate, identify what to extract:\r\n- Phase structure (how many phases, what each does)\r\n- Cluster categories (how clusters are defined)\r\n- Benchmark types (what statistics to track)\r\n- Technique guides (what writing tools to include)\r\n\r\n**Selection criteria**:\r\n- Match section type if possible (lit-writeup for theory sections)\r\n- Match rhetorical function (interview-bookends for framing sections)\r\n- Consider complexity (simpler model for first skill)\r\n\r\n### 4. Review Model Skill Structure\r\n\r\nRead the selected model skill and extract its structure:\r\n\r\n```markdown\r\n## Model Skill: [name]\r\n\r\n### Phase Structure\r\n| Phase | Goal | Outputs |\r\n|-------|------|---------|\r\n| 0 | [goal] | [outputs] |\r\n| 1 | [goal] | [outputs] |\r\n| ... | ... | ... |\r\n\r\n### Cluster Categories\r\n- **[Cluster 1]**: [description, prevalence]\r\n- **[Cluster 2]**: [description, prevalence]\r\n- ...\r\n\r\n### Benchmarks Tracked\r\n- [Metric 1]: [typical value]\r\n- [Metric 2]: [typical value]\r\n- ...\r\n\r\n### Technique Guides\r\n- [Guide 1]: [purpose]\r\n- [Guide 2]: [purpose]\r\n- ...\r\n```\r\n\r\n### 5. Define Code Categories\r\n\r\nBased on the model skill, list the genre features you'll code in Phase 2:\r\n\r\n**Structural features to code**:\r\n- [ ] Word count\r\n- [ ] Paragraph count\r\n- [ ] Subsection presence/count\r\n- [ ] [other structural features from model]\r\n\r\n**Rhetorical moves to code**:\r\n- [ ] Opening move type\r\n- [ ] [move 2 from model]\r\n- [ ] [move 3 from model]\r\n- ...\r\n\r\n**Citation/linguistic features to code**:\r\n- [ ] Citation density\r\n- [ ] [other features from model]\r\n\r\n### 6. Confirm Output Skill Name\r\n\r\nGet user confirmation on:\r\n- **Skill name**: (e.g., `discussion-writer`, `methods-guide`)\r\n- **Description**: One-sentence summary for plugin.json\r\n- **Keywords**: 3-5 tags for discoverability\r\n\r\n## Output Files to Create\r\n\r\nSave all outputs to `/analysis/phase0-scope/`:\r\n\r\n1. **scope-definition.md** - Core scope document:\r\n   ```markdown\r\n   # Scope Definition: [Skill Name]\r\n\r\n   ## Target Section\r\n   **Section type**: [name]\r\n   **Alternative names**: [...]\r\n   **Key functions**: [what this section does rhetorically]\r\n\r\n   ## Corpus\r\n   **Location**: [path]\r\n   **Article count**: [n]\r\n   **Venues**: [list]\r\n   **Date range**: [if known]\r\n\r\n   ## Model Skill\r\n   **Selected model**: [skill name]\r\n   **Rationale**: [why this model]\r\n\r\n   ## Output Skill\r\n   **Name**: [skill-name]\r\n   **Description**: [one sentence]\r\n   **Keywords**: [tags]\r\n\r\n   ## Code Categories\r\n   [list of features to code]\r\n   ```\r\n\r\n2. **model-skill-summary.md** - Extracted structure from model skill\r\n\r\n3. **corpus-manifest.md** - List of all articles in corpus with file paths\r\n\r\n## Guiding Principles\r\n\r\n1. **Specificity over breadth**: A skill for \"Discussion sections in qualitative sociology articles\" is better than \"Discussion sections.\"\r\n\r\n2. **Model match matters**: Choosing an appropriate model skill saves significant adaptation work.\r\n\r\n3. **Corpus quality over quantity**: 40 well-matched articles beat 80 mixed ones.\r\n\r\n4. **Name for users**: Skill names should be immediately clear (e.g., `abstract-writer` not `text-generator`).\r\n\r\n## When You're Done\r\n\r\nReturn a summary to the orchestrator that includes:\r\n1. Confirmation that scope is defined\r\n2. Target section and its key functions\r\n3. Corpus size and source venues\r\n4. Selected model skill and rationale\r\n5. Planned code categories (high-level list)\r\n6. Output skill name and description\r\n7. Any concerns or limitations noted\r\n8. Recommendation to proceed to Phase 1\r\n\r\n**Critical**: Get user confirmation before proceeding. Scope changes mid-analysis are costly.\r\n",
        "plugins/genre-skill-builder/skills/genre-skill-builder/phases/phase1-immersion.md": "# Phase 1: Corpus Immersion\r\n\r\nYou are executing Phase 1 of the genre-skill-builder workflow. Your goal is to build a quantitative profile of the corpusunderstanding the landscape before coding individual features.\r\n\r\n## Why This Phase Matters\r\n\r\nQuantitative profiling establishes the baseline. Before coding rhetorical moves, you need to know what \"typical\" looks like: median word counts, paragraph distributions, structural patterns. This phase creates the benchmarks that later define clusters.\r\n\r\n## Inputs\r\n\r\nBefore starting, read:\r\n1. `/analysis/phase0-scope/scope-definition.md` - Scope and model skill\r\n2. `/analysis/phase0-scope/corpus-manifest.md` - List of articles\r\n3. The corpus files themselves (in the location specified)\r\n\r\n## Your Tasks\r\n\r\n### 1. Extract Basic Statistics for Each Article\r\n\r\nFor every article in the corpus, extract:\r\n\r\n```markdown\r\n## Article: [filename]\r\n\r\n### Basic Counts\r\n- **Word count**: [n]\r\n- **Paragraph count**: [n]\r\n- **Sentence count**: [n] (if feasible)\r\n- **Character count**: [n]\r\n\r\n### Structural Features\r\n- **Has subsections**: [yes/no]\r\n- **Subsection count**: [n]\r\n- **Subsection headings**: [list if present]\r\n- **Has numbered/bulleted lists**: [yes/no]\r\n\r\n### First/Last Elements\r\n- **Opening sentence**: \"[first sentence]\"\r\n- **Opening sentence type**: [context/literature/theory/question/claim]\r\n- **Closing sentence**: \"[last sentence]\"\r\n```\r\n\r\n### 2. Calculate Corpus-Level Statistics\r\n\r\nAggregate the per-article data:\r\n\r\n```markdown\r\n## Corpus Statistics\r\n\r\n### Word Count\r\n- **Median**: [n]\r\n- **Mean**: [n]\r\n- **IQR**: [Q1]-[Q3]\r\n- **Range**: [min]-[max]\r\n- **Outliers**: [articles significantly above/below]\r\n\r\n### Paragraph Count\r\n- **Median**: [n]\r\n- **Mean**: [n]\r\n- **IQR**: [Q1]-[Q3]\r\n- **Range**: [min]-[max]\r\n\r\n### Subsections\r\n- **Articles with subsections**: [n] ([%])\r\n- **Median subsection count** (when present): [n]\r\n- **Common headings**: [list top 3-5]\r\n\r\n### Words per Paragraph\r\n- **Median**: [n]\r\n- **Range**: [min]-[max]\r\n```\r\n\r\n### 3. Identify Structural Patterns\r\n\r\nLook for recurring structural elements:\r\n\r\n**Opening patterns**:\r\n- What percentage start with context-setting?\r\n- What percentage start with literature references?\r\n- What percentage start with claims/arguments?\r\n- What percentage start with questions?\r\n\r\n**Organizational patterns**:\r\n- Is there a typical paragraph sequence?\r\n- Are subsections used consistently or variably?\r\n- Is there a recognizable \"turn\" or pivot point?\r\n\r\n**Closing patterns**:\r\n- How do sections typically end?\r\n- What elements appear in final paragraphs?\r\n\r\n### 4. Flag Outliers and Notable Examples\r\n\r\nIdentify articles that stand out:\r\n\r\n**Length outliers**:\r\n- Unusually short (bottom 10%)\r\n- Unusually long (top 10%)\r\n- Note: Outliers may represent different clusters\r\n\r\n**Structural outliers**:\r\n- Unusual organization\r\n- Many subsections vs. none\r\n- Distinctive formatting\r\n\r\n**Quality exemplars**:\r\n- Articles that seem particularly well-crafted\r\n- Could serve as models for technique guides\r\n\r\n### 5. Generate Initial Hypotheses\r\n\r\nBased on quantitative patterns, hypothesize about clusters:\r\n\r\n```markdown\r\n## Initial Cluster Hypotheses\r\n\r\nBased on the quantitative profile, I observe:\r\n\r\n1. **Length variation**: [describe]\r\n   - Possible cluster distinction: [hypothesis]\r\n\r\n2. **Subsection usage**: [describe]\r\n   - Possible cluster distinction: [hypothesis]\r\n\r\n3. **Opening sentence patterns**: [describe]\r\n   - Possible cluster distinction: [hypothesis]\r\n\r\nThese hypotheses will be tested through systematic coding in Phase 2.\r\n```\r\n\r\n### 6. Create Data Files\r\n\r\nStructure the data for later phases:\r\n\r\n**corpus-statistics.json**:\r\n```json\r\n{\r\n  \"corpus_size\": 80,\r\n  \"section_type\": \"introduction\",\r\n  \"statistics\": {\r\n    \"word_count\": {\r\n      \"median\": 761,\r\n      \"mean\": 823,\r\n      \"q1\": 612,\r\n      \"q3\": 945,\r\n      \"min\": 342,\r\n      \"max\": 1523\r\n    },\r\n    \"paragraph_count\": {\r\n      \"median\": 6,\r\n      \"mean\": 6.2,\r\n      \"q1\": 5,\r\n      \"q3\": 8,\r\n      \"min\": 3,\r\n      \"max\": 12\r\n    }\r\n  },\r\n  \"articles\": [\r\n    {\r\n      \"filename\": \"article-01.md\",\r\n      \"word_count\": 756,\r\n      \"paragraph_count\": 6,\r\n      \"has_subsections\": false,\r\n      \"opening_type\": \"phenomenon\"\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n## Output Files to Create\r\n\r\nSave all outputs to `/analysis/phase1-immersion/`:\r\n\r\n1. **phase1-immersion-report.md** - Main report with:\r\n   - Corpus overview (size, sources)\r\n   - Statistical summaries (word count, paragraphs, etc.)\r\n   - Structural pattern observations\r\n   - Outliers and exemplars flagged\r\n   - Initial cluster hypotheses\r\n   - Questions for the user\r\n\r\n2. **corpus-statistics.json** - Structured data for later phases\r\n\r\n3. **article-profiles/** folder with individual article summaries:\r\n   - `article-01-profile.md`\r\n   - `article-02-profile.md`\r\n   - etc.\r\n\r\n4. **benchmark-reference.md** - Quick-reference table of key statistics\r\n\r\n## Guiding Principles\r\n\r\n1. **Quantify before qualifying**: Count everything that can be counted. Qualitative coding comes in Phase 2.\r\n\r\n2. **Variation is signal**: Large IQRs and outliers suggest cluster differentiation.\r\n\r\n3. **Preserve granularity**: Keep per-article data even when reporting aggregates.\r\n\r\n4. **Compare to model skill**: How do your statistics compare to the model skill's benchmarks?\r\n\r\n5. **Document surprises**: Note anything unexpectedthese often become analytical insights.\r\n\r\n## Comparison with Model Skill\r\n\r\nCompare your corpus statistics to the model skill's benchmarks:\r\n\r\n| Metric | Model Skill | Your Corpus | Difference |\r\n|--------|-------------|-------------|------------|\r\n| Median word count | [X] | [Y] | [note] |\r\n| Median paragraphs | [X] | [Y] | [note] |\r\n| % with subsections | [X] | [Y] | [note] |\r\n\r\nNote significant differencesthey may reflect section-type differences or venue differences.\r\n\r\n## When You're Done\r\n\r\nReturn a summary to the orchestrator that includes:\r\n1. Corpus size confirmed\r\n2. Key statistics (median word count, paragraph count)\r\n3. Notable structural patterns\r\n4. Outliers flagged (with article names)\r\n5. Initial cluster hypotheses (2-4 possibilities)\r\n6. Comparison to model skill benchmarks\r\n7. Questions for the user about patterns observed\r\n8. Recommendation to proceed to Phase 2\r\n",
        "plugins/genre-skill-builder/skills/genre-skill-builder/phases/phase2-coding.md": "# Phase 2: Systematic Genre Coding\r\n\r\nYou are executing Phase 2 of the genre-skill-builder workflow. Your goal is to systematically code each article for genre features, building the data that will reveal cluster patterns.\r\n\r\n## Why This Phase Matters\r\n\r\nCoding transforms impressions into data. By applying consistent categories across the corpus, you create the foundation for rigorous cluster discovery. This phase adapts the systematic coding approach from interview-analyst to genre analysis.\r\n\r\n## Inputs\r\n\r\nBefore starting, read:\r\n1. `/analysis/phase0-scope/scope-definition.md` - Code categories planned\r\n2. `/analysis/phase0-scope/model-skill-summary.md` - Model skill structure\r\n3. `/analysis/phase1-immersion/phase1-immersion-report.md` - Quantitative profile\r\n4. `/analysis/phase1-immersion/corpus-statistics.json` - Article data\r\n\r\n## Your Tasks\r\n\r\n### 1. Develop the Codebook\r\n\r\nCreate a systematic codebook with codes organized by category. Base codes on the model skill but adapt to your section type.\r\n\r\n**Example codebook structure**:\r\n\r\n```markdown\r\n# Genre Codebook: [Section Type]\r\n\r\n## Opening Move Codes\r\n\r\n### OM-PHENOMENON\r\n**Definition**: Opens by describing an empirical phenomenon, trend, or social condition.\r\n**Indicators**: Statistical claims, descriptions of events, naming of social groups/practices.\r\n**Example**: \"Tasked with protecting children from abuse and neglect, U.S. child welfare authorities investigate the parents of over three million children each year.\"\r\n\r\n### OM-STAKES\r\n**Definition**: Opens by establishing importance, significance, or consequences.\r\n**Indicators**: \"Important,\" \"significant,\" \"crisis,\" claims about impact.\r\n**Example**: \"Housing instability poses severe consequences for families with children.\"\r\n\r\n### OM-THEORY\r\n**Definition**: Opens by invoking a theoretical framework, concept, or scholar.\r\n**Indicators**: Named theory, cited theorist, conceptual terminology.\r\n**Example**: \"The concept of 'recognition' draws on several conceptual traditions.\"\r\n\r\n### OM-CASE\r\n**Definition**: Opens with a specific case, example, or vignette.\r\n**Indicators**: Named individual, specific incident, narrative opening.\r\n**Example**: \"When Maria first arrived at the shelter...\"\r\n\r\n### OM-QUESTION\r\n**Definition**: Opens with an explicit research question.\r\n**Indicators**: Question mark, interrogative structure.\r\n**Example**: \"How do immigrant parents maintain connections with children abroad?\"\r\n\r\n[Continue for all code categories...]\r\n```\r\n\r\n### 2. Define Code Categories\r\n\r\nBased on your model skill and section type, include these categories:\r\n\r\n**A. Opening Moves** (required)\r\n- How does the section begin?\r\n- Codes: phenomenon, stakes, theory, case, question, other\r\n\r\n**B. Structural Elements** (required)\r\n- What organizational features are present?\r\n- Codes: roadmap, limitations, callbacks, future-directions, data-preview, etc.\r\n\r\n**C. Rhetorical Moves** (adapt to section type)\r\nFor Theory sections: gap-identification, synthesis, theory-exposition\r\nFor Introductions: hook, puzzle, preview\r\nFor Conclusions: restatement, implications, coda\r\nFor Methods: design-rationale, sampling-description, analysis-description\r\n\r\n**D. Citation Patterns** (if applicable)\r\n- Density: sparse (<2/para), moderate (2-4/para), dense (>4/para)\r\n- Style: parenthetical-dominant, author-subject, quote-then-cite\r\n\r\n**E. Linguistic Features** (select relevant)\r\n- Hedging level: low, moderate, high\r\n- Temporal markers: present, absent\r\n- First person: present, absent\r\n\r\n### 3. Code Each Article\r\n\r\nFor every article, apply all codes:\r\n\r\n```markdown\r\n## Article: [filename]\r\n\r\n### Opening Move\r\n- **Primary code**: OM-PHENOMENON\r\n- **First sentence**: \"Tasked with protecting children...\"\r\n- **Notes**: Classic empirical opening; data implied but not cited\r\n\r\n### Structural Elements\r\n- **Roadmap**: present (paragraph 5)\r\n- **Data preview**: present (paragraph 3)\r\n- **Limitations**: absent\r\n- **Callbacks**: N/A (this is intro, not conclusion)\r\n\r\n### Rhetorical Moves\r\n- **Gap identification**: present (paragraph 4, \"Yet we know little...\")\r\n- **Prior research synthesis**: present (paragraphs 2-3)\r\n- **Contribution claim**: present (paragraph 5)\r\n\r\n### Citation Pattern\r\n- **Density**: moderate (3.2 per paragraph)\r\n- **Dominant style**: parenthetical\r\n\r\n### Linguistic Features\r\n- **Hedging**: moderate\r\n- **First person**: present (\"This study examines...\")\r\n- **Temporal markers**: present (\"In recent decades...\")\r\n\r\n### Overall Assessment\r\n- **Preliminary cluster**: Gap-Filler candidate\r\n- **Notes**: Efficient structure, data mentioned early, modest claims\r\n```\r\n\r\n### 4. Track Code Frequencies\r\n\r\nBuild a frequency matrix:\r\n\r\n```markdown\r\n## Code Frequency Summary\r\n\r\n### Opening Moves\r\n| Code | Count | Percentage |\r\n|------|-------|------------|\r\n| OM-PHENOMENON | 59 | 74% |\r\n| OM-STAKES | 8 | 10% |\r\n| OM-THEORY | 10 | 13% |\r\n| OM-CASE | 2 | 3% |\r\n| OM-QUESTION | 1 | 1% |\r\n\r\n### Structural Elements\r\n| Element | Present | Percentage |\r\n|---------|---------|------------|\r\n| Roadmap | 32 | 40% |\r\n| Data preview | 56 | 70% |\r\n| Gap statement | 72 | 90% |\r\n| Contribution claim | 68 | 85% |\r\n\r\n[Continue for all categories...]\r\n```\r\n\r\n### 5. Analyze Code Co-occurrence\r\n\r\nIdentify which codes appear together:\r\n\r\n```markdown\r\n## Co-occurrence Patterns\r\n\r\n### Opening Move + Structure\r\n- OM-PHENOMENON + early data preview: 45/59 (76%)\r\n- OM-THEORY + named theorist in para 1: 8/10 (80%)\r\n- OM-STAKES + policy implication: 6/8 (75%)\r\n\r\n### Structural Combinations\r\n- Roadmap + 5+ paragraphs: 28/32 (88%)\r\n- No subsections + shorter length: correlation observed\r\n\r\n### Emerging Cluster Signals\r\nBased on co-occurrence, I see these groupings:\r\n1. [Pattern A]: [codes that cluster]\r\n2. [Pattern B]: [codes that cluster]\r\n3. [Pattern C]: [codes that cluster]\r\n```\r\n\r\n### 6. Assign Preliminary Clusters\r\n\r\nBased on coding patterns, assign each article to a preliminary cluster:\r\n\r\n```markdown\r\n## Preliminary Cluster Assignments\r\n\r\n### Cluster A: [Working Name]\r\n**Signature pattern**: [key codes that define this cluster]\r\n**Articles**: [list article filenames]\r\n**Count**: [n] ([%])\r\n\r\n### Cluster B: [Working Name]\r\n**Signature pattern**: [key codes that define this cluster]\r\n**Articles**: [list article filenames]\r\n**Count**: [n] ([%])\r\n\r\n[Continue for 3-6 clusters...]\r\n\r\n### Uncertain/Mixed\r\n**Articles that don't fit cleanly**: [list]\r\n**Notes on ambiguity**: [what makes them hard to classify]\r\n```\r\n\r\n### 7. Write Coding Memo\r\n\r\nDocument your coding process and emerging insights:\r\n\r\n```markdown\r\n## Coding Memo\r\n\r\n### Process Notes\r\n- [How did codes evolve during the process?]\r\n- [What was difficult to code?]\r\n- [What distinctions proved useful/useless?]\r\n\r\n### Surprises\r\n- [What patterns were unexpected?]\r\n- [What assumptions from the model skill didn't hold?]\r\n\r\n### Cluster Hypotheses Refined\r\n- [How do preliminary clusters compare to Phase 1 hypotheses?]\r\n- [What additional distinctions emerged?]\r\n\r\n### Questions for Phase 3\r\n- [What needs interpretation?]\r\n- [Where are cluster boundaries unclear?]\r\n- [What exemplars stand out?]\r\n```\r\n\r\n## Output Files to Create\r\n\r\nSave all outputs to `/analysis/phase2-coding/`:\r\n\r\n1. **codebook.md** - Full codebook with all codes defined\r\n\r\n2. **article-codes/** folder with per-article coding:\r\n   - `article-01-codes.md`\r\n   - `article-02-codes.md`\r\n   - etc.\r\n\r\n3. **article-codes.json** - Structured coding data:\r\n   ```json\r\n   {\r\n     \"articles\": [\r\n       {\r\n         \"filename\": \"article-01.md\",\r\n         \"opening_move\": \"phenomenon\",\r\n         \"has_roadmap\": true,\r\n         \"has_data_preview\": true,\r\n         \"citation_density\": \"moderate\",\r\n         \"preliminary_cluster\": \"gap-filler\"\r\n       }\r\n     ]\r\n   }\r\n   ```\r\n\r\n4. **code-frequency.md** - Frequency tables for all codes\r\n\r\n5. **co-occurrence-analysis.md** - Code co-occurrence patterns\r\n\r\n6. **preliminary-clusters.md** - Cluster assignments with rationale\r\n\r\n7. **coding-memo.md** - Process reflections and emerging insights\r\n\r\n8. **phase2-report.md** - Executive summary:\r\n   - Codebook overview\r\n   - Key frequency findings\r\n   - Preliminary cluster structure\r\n   - Uncertainties and edge cases\r\n   - Questions for user\r\n\r\n## Guiding Principles\r\n\r\n1. **Consistency over speed**: Apply codes the same way across all articles.\r\n\r\n2. **Refine as you go**: If a code isn't working, revise the definition mid-processbut recode earlier articles.\r\n\r\n3. **Preserve uncertainty**: Mark unclear cases rather than forcing them into categories.\r\n\r\n4. **Trust the model skill**: Start with model skill categories before inventing new ones.\r\n\r\n5. **Co-occurrence reveals clusters**: Individual codes matter less than patterns of co-occurrence.\r\n\r\n6. **Document decisions**: When you make a coding judgment, note why.\r\n\r\n## When You're Done\r\n\r\nReturn a summary to the orchestrator that includes:\r\n1. Codebook finalized (number of codes by category)\r\n2. Key frequency findings (top opening moves, common structural elements)\r\n3. Preliminary cluster count and names\r\n4. Cluster sizes (n and % for each)\r\n5. Edge cases and uncertainties\r\n6. Most and least stable clusters\r\n7. Questions for the user about coding decisions\r\n8. Recommendation to proceed to Phase 3\r\n",
        "plugins/genre-skill-builder/skills/genre-skill-builder/phases/phase3-interpretation.md": "# Phase 3: Pattern Interpretation & Cluster Discovery\r\n\r\nYou are executing Phase 3 of the genre-skill-builder workflow. Your goal is to move from coding data to stable cluster definitionsinterpreting patterns, refining boundaries, and creating the profiles that will guide the generated skill.\r\n\r\n## Why This Phase Matters\r\n\r\nClusters are the heart of genre-based writing guidance. This phase transforms preliminary groupings into robust categories with distinctive benchmarks, signature moves, and clear guidance. The quality of cluster discovery determines the usefulness of the final skill.\r\n\r\n## Inputs\r\n\r\nBefore starting, read:\r\n1. `/analysis/phase0-scope/scope-definition.md` - Scope and model skill\r\n2. `/analysis/phase1-immersion/corpus-statistics.json` - Quantitative data\r\n3. `/analysis/phase2-coding/article-codes.json` - All article codes\r\n4. `/analysis/phase2-coding/preliminary-clusters.md` - Initial cluster assignments\r\n5. `/analysis/phase2-coding/co-occurrence-analysis.md` - Code patterns\r\n6. `/analysis/phase2-coding/coding-memo.md` - Coding insights\r\n\r\n## Your Tasks\r\n\r\n### 1. Validate Preliminary Clusters\r\n\r\nTest each preliminary cluster for coherence:\r\n\r\n**Cluster coherence criteria**:\r\n- [ ] At least 3 articles (or 10% of corpus)\r\n- [ ] Shared signature codes (not just one feature)\r\n- [ ] Distinctive from other clusters (not a subset)\r\n- [ ] Internally consistent (members are more similar to each other than to other clusters)\r\n\r\n```markdown\r\n## Cluster Validation: [Cluster Name]\r\n\r\n### Member Articles\r\n[List all articles assigned to this cluster]\r\n\r\n### Signature Codes\r\n- **Primary defining features**: [codes that appear in 80%+ of cluster]\r\n- **Secondary features**: [codes that appear in 50-80%]\r\n- **Distinguishing from other clusters**: [what this cluster has that others don't]\r\n\r\n### Internal Consistency\r\n- Articles that clearly fit: [list]\r\n- Articles that are borderline: [list]\r\n- Articles to reassign: [list, with rationale]\r\n\r\n### Verdict\r\n[Keep / Merge with X / Split into X and Y / Dissolve]\r\n```\r\n\r\n### 2. Refine Cluster Boundaries\r\n\r\nBased on validation, adjust clusters:\r\n\r\n**Operations to consider**:\r\n- **Merge**: If two clusters share most signature codes, combine them\r\n- **Split**: If a cluster has two distinct sub-patterns, separate them\r\n- **Dissolve**: If a cluster has too few members or no clear signature, redistribute\r\n- **Rename**: If the working name doesn't capture the essence, revise\r\n\r\n```markdown\r\n## Cluster Refinement Decisions\r\n\r\n### Merges\r\n- Merged [Cluster A] and [Cluster B]  [New Name]\r\n- Rationale: [why they belong together]\r\n\r\n### Splits\r\n- Split [Cluster C] into [C1] and [C2]\r\n- Rationale: [distinctive patterns that justify separation]\r\n\r\n### Dissolutions\r\n- Dissolved [Cluster D], reassigned articles to [X, Y, Z]\r\n- Rationale: [why it wasn't a coherent cluster]\r\n\r\n### Final Cluster Count: [n]\r\n```\r\n\r\n### 3. Calculate Cluster Benchmarks\r\n\r\nFor each final cluster, compute distinctive statistics:\r\n\r\n```markdown\r\n## Cluster Benchmarks: [Cluster Name]\r\n\r\n### Size\r\n- **Articles**: [n] ([%] of corpus)\r\n\r\n### Word Count\r\n- **Median**: [n]\r\n- **IQR**: [Q1]-[Q3]\r\n- **Comparison to corpus median**: [higher/lower/similar]\r\n\r\n### Paragraph Count\r\n- **Median**: [n]\r\n- **IQR**: [Q1]-[Q3]\r\n\r\n### Structural Features\r\n- **% with subsections**: [n]%\r\n- **% with roadmap**: [n]%\r\n- **% with [other feature]**: [n]%\r\n\r\n### Opening Move Distribution\r\n| Move | Count | % |\r\n|------|-------|---|\r\n| Phenomenon | X | X% |\r\n| Stakes | X | X% |\r\n| Theory | X | X% |\r\n| Other | X | X% |\r\n\r\n### Citation Density\r\n- **Median per paragraph**: [n]\r\n- **Dominant style**: [parenthetical/author-subject/mixed]\r\n\r\n### Distinctive vs. Corpus\r\n[What makes this cluster's benchmarks different from overall corpus?]\r\n```\r\n\r\n### 4. Define Signature and Prohibited Moves\r\n\r\nFor each cluster, specify what's characteristic and what's avoided:\r\n\r\n```markdown\r\n## Cluster Profile: [Cluster Name]\r\n\r\n### Signature Moves (do this)\r\n1. **[Move 1]**: [description, with example from corpus]\r\n   > \"Example quote from article X\"\r\n2. **[Move 2]**: [description, with example]\r\n3. **[Move 3]**: [description, with example]\r\n\r\n### Typical Pattern\r\n[Describe the characteristic paragraph sequence or structure]\r\n\r\n### Prohibited Moves (don't do this)\r\n1. **[Move 1]**: [what to avoid and why]\r\n2. **[Move 2]**: [what to avoid and why]\r\n\r\n### Edge Cases\r\n[When might someone in this cluster deviate from signature moves?]\r\n```\r\n\r\n### 5. Select Cluster Exemplars\r\n\r\nIdentify 1-2 articles per cluster that best represent the pattern:\r\n\r\n```markdown\r\n## Exemplar: [Article Filename]\r\n\r\n**Cluster**: [Cluster Name]\r\n**Why exemplary**: [What makes this article a good model]\r\n\r\n### Key Features Demonstrated\r\n- [Feature 1]: How this article does it\r\n- [Feature 2]: How this article does it\r\n- [Feature 3]: How this article does it\r\n\r\n### Notable Quotes\r\n> \"[Quote that exemplifies a signature move]\"\r\n\r\n> \"[Another illustrative quote]\"\r\n\r\n### Would use this to teach\r\n- [Specific technique this article demonstrates well]\r\n```\r\n\r\n### 6. Name Clusters Meaningfully\r\n\r\nFinalize cluster names that communicate their essence:\r\n\r\n**Naming principles**:\r\n- Name by **strategy**, not prevalence (\"Gap-Filler\" not \"Common Type\")\r\n- Name by **contribution type** when possible (\"Theory-Extension\" not \"Framework-Heavy\")\r\n- Use **compound names** if needed (\"Problem-Driven Pragmatist\")\r\n- Avoid **jargon** that won't be clear to users\r\n\r\n```markdown\r\n## Final Cluster Names\r\n\r\n| Working Name | Final Name | Rationale |\r\n|--------------|------------|-----------|\r\n| Cluster A | Gap-Filler Minimalist | Fills empirical gaps with efficient structure |\r\n| Cluster B | Theory-Extension | Applies named framework to new domain |\r\n| ... | ... | ... |\r\n```\r\n\r\n### 7. Create Cluster Decision Tree\r\n\r\nHelp users identify which cluster their article inhabits:\r\n\r\n```markdown\r\n## Cluster Decision Tree\r\n\r\nTo identify your cluster, answer these questions:\r\n\r\n1. **What is your primary contribution?**\r\n   - Empirical documentation of understudied phenomenon  likely Gap-Filler\r\n   - Application of established theory  likely Theory-Extension\r\n   - Introduction of new concept/term  likely Concept-Builder\r\n   - Integration of separate literatures  likely Synthesis\r\n   - Resolution of debate or policy focus  likely Problem-Driven\r\n\r\n2. **[Follow-up question based on answer 1]**\r\n   - ...\r\n\r\n3. **[Confirmation question]**\r\n   - ...\r\n\r\n### Quick Indicators\r\n\r\n| If you see this... | Consider this cluster... |\r\n|--------------------|--------------------------|\r\n| Named theorist in first paragraph | Theory-Extension |\r\n| Data mentioned in first 3 paragraphs | Gap-Filler |\r\n| New terminology defined | Concept-Builder |\r\n| Multiple theoretical traditions | Synthesis |\r\n| Policy implications emphasized | Problem-Driven |\r\n```\r\n\r\n### 8. Document Interpretation Process\r\n\r\nWrite an interpretation memo:\r\n\r\n```markdown\r\n## Interpretation Memo\r\n\r\n### How Clusters Emerged\r\n[Narrative of the discovery processwhat patterns became clear, what required judgment]\r\n\r\n### Key Distinctions\r\n[The 2-3 most important features that differentiate clusters]\r\n\r\n### Uncertainty and Ambiguity\r\n[Where are cluster boundaries fuzzy? Which articles could go either way?]\r\n\r\n### Comparison to Model Skill\r\n[How do your clusters compare to the model skill's clusters?]\r\n- Similar clusters: [list]\r\n- Different clusters: [list]\r\n- Novel findings: [what your corpus revealed that the model didn't have]\r\n\r\n### Confidence Assessment\r\n[How stable are these clusters? Would another coder find the same groupings?]\r\n```\r\n\r\n## Output Files to Create\r\n\r\nSave all outputs to `/analysis/phase3-clusters/`:\r\n\r\n1. **phase3-interpretation.md** - Main interpretation report:\r\n   - Cluster validation results\r\n   - Refinement decisions\r\n   - Final cluster definitions\r\n   - Interpretation memo\r\n\r\n2. **cluster-profiles.md** - Consolidated cluster profiles:\r\n   - One section per cluster\r\n   - Benchmarks, signature moves, exemplars\r\n   - Decision tree for identification\r\n\r\n3. **cluster-benchmarks.json** - Structured benchmark data:\r\n   ```json\r\n   {\r\n     \"clusters\": [\r\n       {\r\n         \"name\": \"Gap-Filler Minimalist\",\r\n         \"prevalence\": 0.388,\r\n         \"benchmarks\": {\r\n           \"word_count\": {\"median\": 695, \"iqr\": [612, 756]},\r\n           \"paragraph_count\": {\"median\": 5, \"iqr\": [4, 6]},\r\n           \"opening_move\": {\"phenomenon\": 0.77, \"stakes\": 0.13, \"theory\": 0.06}\r\n         },\r\n         \"signature_moves\": [\"phenomenon_opening\", \"early_data_mention\", \"modest_claims\"],\r\n         \"prohibited_moves\": [\"theory_led_opening\", \"extensive_roadmap\"]\r\n       }\r\n     ]\r\n   }\r\n   ```\r\n\r\n4. **exemplars/** folder with detailed exemplar analyses:\r\n   - `exemplar-gap-filler.md`\r\n   - `exemplar-theory-extension.md`\r\n   - etc.\r\n\r\n5. **decision-tree.md** - Standalone cluster identification guide\r\n\r\n## Guiding Principles\r\n\r\n1. **Clusters are analytical tools**: They simplify variation to enable guidance. Perfect boundaries don't exist.\r\n\r\n2. **3-6 clusters is the sweet spot**: Fewer misses variation; more creates confusion.\r\n\r\n3. **Names matter**: Users will think with cluster names. Make them intuitive.\r\n\r\n4. **Exemplars do heavy lifting**: Well-chosen exemplars communicate more than definitions.\r\n\r\n5. **Benchmarks are descriptive**: They describe what successful articles do, not what all articles must do.\r\n\r\n6. **Uncertainty is honest**: Acknowledge fuzzy boundaries rather than forcing false precision.\r\n\r\n## When You're Done\r\n\r\nReturn a summary to the orchestrator that includes:\r\n1. Final cluster count and names\r\n2. Cluster sizes (n and % for each)\r\n3. Key distinguishing features between clusters\r\n4. Exemplars selected (1-2 per cluster)\r\n5. Decision tree summary (how users identify their cluster)\r\n6. Confidence assessment (how stable are the clusters?)\r\n7. Comparison to model skill clusters\r\n8. Questions for the user about cluster definitions\r\n9. Recommendation to proceed to Phase 4 (skill generation)\r\n",
        "plugins/genre-skill-builder/skills/genre-skill-builder/phases/phase4-generation.md": "# Phase 4: Skill Generation\r\n\r\nYou are executing Phase 4 of the genre-skill-builder workflow. Your goal is to generate all the files for the new skilltransforming your analysis findings into a functional plugin.\r\n\r\n## Why This Phase Matters\r\n\r\nThis is where analysis becomes product. The cluster profiles, benchmarks, and exemplars from Phases 1-3 now become actionable writing guidance. Template-based generation ensures the new skill follows repository conventions.\r\n\r\n## Inputs\r\n\r\nBefore starting, read:\r\n1. `/analysis/phase0-scope/scope-definition.md` - Skill name, description, target section\r\n2. `/analysis/phase0-scope/model-skill-summary.md` - Model skill structure\r\n3. `/analysis/phase3-clusters/cluster-profiles.md` - Final cluster definitions\r\n4. `/analysis/phase3-clusters/cluster-benchmarks.json` - Quantitative data\r\n5. `/analysis/phase3-clusters/decision-tree.md` - Cluster identification guide\r\n6. Template files in this skill's `templates/` folder\r\n\r\n## Your Tasks\r\n\r\n### 1. Create Directory Structure\r\n\r\nSet up the output skill directory:\r\n\r\n```\r\noutput/plugins/[skill-name]/\r\n plugin.json\r\n skills/[skill-name]/\r\n     SKILL.md\r\n     phases/\r\n        phase0-[name].md\r\n        phase1-[name].md\r\n        phase2-[name].md\r\n        phase3-[name].md (optional)\r\n     clusters/\r\n        [cluster-1].md\r\n        [cluster-2].md\r\n        ...\r\n     techniques/\r\n         [technique-1].md\r\n         [technique-2].md\r\n         ...\r\n```\r\n\r\n### 2. Generate plugin.json\r\n\r\nUse scope definition data:\r\n\r\n```json\r\n{\r\n  \"name\": \"[skill-name]\",\r\n  \"version\": \"1.0.0\",\r\n  \"description\": \"[Description from Phase 0]\",\r\n  \"skills\": \"./skills/\"\r\n}\r\n```\r\n\r\n### 3. Generate SKILL.md\r\n\r\nApply the skill template with your findings:\r\n\r\n**Required sections**:\r\n- YAML frontmatter (name, description)\r\n- Introduction (what the skill does)\r\n- When to use (prerequisites, inputs needed)\r\n- Connection to other skills (if applicable)\r\n- Core principles (from genre analysis)\r\n- Key statistics/benchmarks (from Phase 1-2)\r\n- The cluster system (brief overview)\r\n- Workflow phases (from model skill, adapted)\r\n- Cluster decision tree (from Phase 3)\r\n- Technique guides (list)\r\n- Invoking phase agents (Task tool instructions)\r\n- Model recommendations (Sonnet/Opus per phase)\r\n- Starting the process (initial user questions)\r\n- Key reminders (genre-specific guidance)\r\n\r\n**Use template placeholders**:\r\n```markdown\r\n---\r\nname: {{skill_name}}\r\ndescription: {{description}}\r\n---\r\n\r\n# {{Skill Title}}\r\n\r\nYou help {{target_audience}} write **{{target_section}}** for {{article_type}}. Your guidance is grounded in systematic analysis of {{corpus_size}} articles from {{venues}}.\r\n\r\n## Core Principles (from Genre Analysis)\r\n\r\nBased on analysis of {{corpus_size}} {{section_type}} sections:\r\n\r\n### 1. {{Principle 1 Name}}\r\n{{Principle 1 contentderived from your findings}}\r\n\r\n### 2. {{Principle 2 Name}}\r\n{{Principle 2 content}}\r\n\r\n[Continue with 3-5 principles...]\r\n\r\n## Key Statistics\r\n\r\n| Metric | Median | Target Range (IQR) |\r\n|--------|--------|-------------------|\r\n| Word count | {{median_words}} | {{word_iqr}} |\r\n| Paragraphs | {{median_paras}} | {{para_iqr}} |\r\n| [other metrics...] | ... | ... |\r\n\r\n## The {{n}} Clusters\r\n\r\n{{section_type}} cluster into {{n}} recognizable styles:\r\n\r\n| Cluster | Prevalence | Key Feature |\r\n|---------|------------|-------------|\r\n| **{{Cluster 1}}** | {{%}} | {{key feature}} |\r\n| **{{Cluster 2}}** | {{%}} | {{key feature}} |\r\n[Continue...]\r\n\r\nSee `clusters/` directory for detailed profiles.\r\n\r\n[Continue with remaining sections...]\r\n```\r\n\r\n### 4. Generate Phase Files\r\n\r\nCreate 3-4 phase files based on section type:\r\n\r\n**For Introduction/Conclusion skills** (like interview-bookends):\r\n- Phase 0: Intake & Assessment (identify cluster)\r\n- Phase 1: Drafting (write the section)\r\n- Phase 2: Coherence Check (verify alignment)\r\n\r\n**For Theory/Methods skills** (like lit-writeup):\r\n- Phase 0: Assessment (identify cluster, contribution type)\r\n- Phase 1: Architecture (plan structure)\r\n- Phase 2: Drafting (write paragraphs)\r\n- Phase 3: Revision (calibrate to norms)\r\n\r\n**Each phase file must include**:\r\n- Goal statement\r\n- Process steps (numbered)\r\n- Input requirements\r\n- Output files\r\n- Guiding principles\r\n- When You're Done (summary for orchestrator)\r\n\r\n### 5. Generate Cluster Profiles\r\n\r\nCreate one file per cluster in `clusters/`:\r\n\r\nUse the cluster template:\r\n\r\n```markdown\r\n# Cluster: {{Cluster Name}}\r\n\r\n**Prevalence**: {{%}} of corpus\r\n\r\n**Contribution logic**: \"{{One-sentence description of what articles in this cluster do}}\"\r\n\r\n---\r\n\r\n## Identifying {{Cluster Name}} Articles\r\n\r\nYour article is {{Cluster Name}} if:\r\n- {{Criterion 1}}\r\n- {{Criterion 2}}\r\n- {{Criterion 3}}\r\n\r\n---\r\n\r\n## Key Statistics\r\n\r\n| Feature | Typical Value |\r\n|---------|---------------|\r\n| Word count | {{range}} |\r\n| Paragraphs | {{range}} |\r\n| {{Feature 3}} | {{value}} |\r\n| {{Feature 4}} | {{value}} |\r\n\r\n---\r\n\r\n## Signature Moves (Do This)\r\n\r\n### 1. {{Move 1}}\r\n{{Description}}\r\n> \"{{Example quote from corpus}}\"\r\n\r\n### 2. {{Move 2}}\r\n{{Description}}\r\n> \"{{Example quote}}\"\r\n\r\n### 3. {{Move 3}}\r\n{{Description}}\r\n\r\n---\r\n\r\n## Prohibited Moves (Don't Do This)\r\n\r\n- **{{Move 1}}**: {{Why to avoid}}\r\n- **{{Move 2}}**: {{Why to avoid}}\r\n\r\n---\r\n\r\n## Exemplar\r\n\r\n**{{Article identifier}}**\r\n\r\n- **Key features**: {{What makes this exemplary}}\r\n- **Notable techniques**: {{Specific craft elements}}\r\n\r\n---\r\n\r\n## Writing Checklist\r\n\r\n- [ ] {{Checkpoint 1}}\r\n- [ ] {{Checkpoint 2}}\r\n- [ ] {{Checkpoint 3}}\r\n- [ ] {{Checkpoint 4}}\r\n```\r\n\r\n### 6. Generate Technique Guides\r\n\r\nCreate 2-4 technique guides based on section type:\r\n\r\n**Common technique guides**:\r\n- `opening-moves.md` - Types of openings with examples\r\n- `sentence-toolbox.md` - Sentence-level patterns (if applicable)\r\n- `[section-specific].md` - Techniques specific to this section type\r\n\r\nUse the technique template:\r\n\r\n```markdown\r\n# {{Technique Guide Title}}\r\n\r\n{{Brief description of what this guide covers}}\r\n\r\n---\r\n\r\n## {{Category 1}}\r\n\r\n### {{Type 1.1}}\r\n**When to use**: {{context}}\r\n**Signature patterns**:\r\n- {{pattern}}\r\n- {{pattern}}\r\n\r\n**Examples from corpus**:\r\n> \"{{quote}}\"\r\n> \"{{quote}}\"\r\n\r\n### {{Type 1.2}}\r\n[Continue...]\r\n\r\n---\r\n\r\n## {{Category 2}}\r\n\r\n[Continue with categories...]\r\n\r\n---\r\n\r\n## Quick Reference\r\n\r\n| {{Column 1}} | {{Column 2}} | {{Column 3}} |\r\n|--------------|--------------|--------------|\r\n| {{data}} | {{data}} | {{data}} |\r\n```\r\n\r\n### 7. Prepare Marketplace Entry\r\n\r\nDraft the entry for `.claude-plugin/marketplace.json`:\r\n\r\n```json\r\n{\r\n  \"name\": \"{{skill-name}}\",\r\n  \"source\": \"./plugins/{{skill-name}}\",\r\n  \"description\": \"{{Full description}}\",\r\n  \"version\": \"1.0.0\",\r\n  \"author\": {\r\n    \"name\": \"{{Author Name}}\"\r\n  },\r\n  \"license\": \"MIT\",\r\n  \"keywords\": [\r\n    \"{{keyword1}}\",\r\n    \"{{keyword2}}\",\r\n    \"{{keyword3}}\"\r\n  ],\r\n  \"strict\": true\r\n}\r\n```\r\n\r\n### 8. Update CLAUDE.md Documentation\r\n\r\nDraft additions to CLAUDE.md:\r\n- Add to plugin installation command list\r\n- Add to Available Skills table\r\n- Add phase table if workflow differs from existing\r\n\r\n## Output Files to Create\r\n\r\nSave all outputs to `/output/plugins/[skill-name]/`:\r\n\r\n1. **plugin.json** - Plugin metadata\r\n\r\n2. **skills/[skill-name]/SKILL.md** - Main skill file\r\n\r\n3. **skills/[skill-name]/phases/** - Phase files:\r\n   - `phase0-[name].md`\r\n   - `phase1-[name].md`\r\n   - `phase2-[name].md`\r\n   - `phase3-[name].md` (if applicable)\r\n\r\n4. **skills/[skill-name]/clusters/** - Cluster profiles:\r\n   - `[cluster-1-slug].md`\r\n   - `[cluster-2-slug].md`\r\n   - etc.\r\n\r\n5. **skills/[skill-name]/techniques/** - Technique guides:\r\n   - `opening-moves.md`\r\n   - `[other-techniques].md`\r\n\r\nAlso save to `/analysis/phase4-generation/`:\r\n\r\n6. **marketplace-entry.json** - Ready-to-add marketplace entry\r\n\r\n7. **claudemd-additions.md** - Documentation updates for CLAUDE.md\r\n\r\n8. **generation-log.md** - Record of what was generated and any decisions made\r\n\r\n## Quality Checks\r\n\r\nBefore finishing, verify:\r\n\r\n- [ ] All files use consistent skill name\r\n- [ ] Benchmarks match Phase 1-2 data\r\n- [ ] Cluster names match Phase 3 definitions\r\n- [ ] Phase files follow repository conventions\r\n- [ ] Technique guides have corpus examples (not invented ones)\r\n- [ ] Decision tree matches cluster definitions\r\n- [ ] Model recommendations are appropriate\r\n- [ ] YAML frontmatter is valid\r\n- [ ] JSON files are valid\r\n\r\n## Guiding Principles\r\n\r\n1. **Templates constrain**: Follow the template structure; don't invent novel formats.\r\n\r\n2. **Data drives content**: Every benchmark, percentage, and example should trace to analysis.\r\n\r\n3. **Consistency over creativity**: Match existing skill conventions for user familiarity.\r\n\r\n4. **Examples from corpus**: Use real quotes from analyzed articles, not invented examples.\r\n\r\n5. **Appropriate scope**: Generate what the model skill has; don't add extra components.\r\n\r\n## When You're Done\r\n\r\nReturn a summary to the orchestrator that includes:\r\n1. Files generated (complete list)\r\n2. Skill structure (number of phases, clusters, techniques)\r\n3. Key benchmarks included\r\n4. Any adaptation decisions (where you deviated from model skill)\r\n5. Files ready for review (SKILL.md, cluster profiles)\r\n6. Marketplace entry ready\r\n7. CLAUDE.md updates drafted\r\n8. Recommendation to proceed to Phase 5 (validation)\r\n",
        "plugins/genre-skill-builder/skills/genre-skill-builder/phases/phase5-validation.md": "# Phase 5: Validation & Testing\r\n\r\nYou are executing Phase 5 of the genre-skill-builder workflow. Your goal is to verify the generated skill's quality and optionally test it with sample input.\r\n\r\n## Why This Phase Matters\r\n\r\nValidation catches errors before the skill is used. A skill with incorrect benchmarks, mismatched cluster definitions, or broken file references will mislead users. This phase ensures quality and builds confidence.\r\n\r\n## Inputs\r\n\r\nBefore starting, read:\r\n1. All files in `/output/plugins/[skill-name]/`\r\n2. `/analysis/phase3-clusters/cluster-benchmarks.json` - Source data\r\n3. `/analysis/phase4-generation/generation-log.md` - Generation decisions\r\n4. The model skill (for comparison)\r\n\r\n## Your Tasks\r\n\r\n### 1. Syntax Validation\r\n\r\nCheck all files for correct formatting:\r\n\r\n**YAML frontmatter**:\r\n```bash\r\n# Each SKILL.md must have valid YAML\r\n---\r\nname: skill-name\r\ndescription: Description here\r\n---\r\n```\r\n\r\n- [ ] SKILL.md has valid YAML frontmatter\r\n- [ ] Name matches plugin.json\r\n- [ ] Description is present and reasonable length\r\n\r\n**JSON files**:\r\n- [ ] plugin.json is valid JSON\r\n- [ ] All required fields present (name, version, description, skills)\r\n\r\n**Markdown structure**:\r\n- [ ] All headers use consistent levels (# for title, ## for sections)\r\n- [ ] All tables have valid markdown syntax\r\n- [ ] All code blocks are properly fenced\r\n\r\n### 2. Data Accuracy Validation\r\n\r\nVerify benchmarks match analysis data:\r\n\r\n```markdown\r\n## Benchmark Verification\r\n\r\n### SKILL.md Statistics\r\n| Metric | In SKILL.md | In Analysis | Match? |\r\n|--------|-------------|-------------|--------|\r\n| Median word count | [X] | [Y] | [/] |\r\n| Median paragraphs | [X] | [Y] | [/] |\r\n| [Other metrics...] | ... | ... | ... |\r\n\r\n### Cluster Prevalences\r\n| Cluster | In SKILL.md | In Analysis | Match? |\r\n|---------|-------------|-------------|--------|\r\n| [Cluster 1] | [X]% | [Y]% | [/] |\r\n| [Cluster 2] | [X]% | [Y]% | [/] |\r\n```\r\n\r\nFlag any discrepancies for correction.\r\n\r\n### 3. Cluster Coverage Validation\r\n\r\nVerify all clusters are properly documented:\r\n\r\n```markdown\r\n## Cluster Coverage\r\n\r\n| Cluster | In SKILL.md | Has Profile | Has Exemplar | Complete? |\r\n|---------|-------------|-------------|--------------|-----------|\r\n| [Cluster 1] |  |  |  |  |\r\n| [Cluster 2] |  |  |  |  |\r\n```\r\n\r\n- [ ] All clusters mentioned in SKILL.md have profile files\r\n- [ ] All cluster profiles have benchmarks section\r\n- [ ] All cluster profiles have signature moves\r\n- [ ] All cluster profiles have exemplars\r\n- [ ] Decision tree covers all clusters\r\n\r\n### 4. Internal Consistency Validation\r\n\r\nCheck cross-file consistency:\r\n\r\n**Naming consistency**:\r\n- [ ] Cluster names match across SKILL.md and cluster profile filenames\r\n- [ ] Skill name matches across all files\r\n- [ ] Phase names match between SKILL.md and phase files\r\n\r\n**Reference integrity**:\r\n- [ ] All file references in SKILL.md point to existing files\r\n- [ ] Technique guide references match actual technique files\r\n- [ ] Cluster guide references match actual cluster files\r\n\r\n**Phase flow**:\r\n- [ ] Phase numbers are sequential\r\n- [ ] Phase goals align with overall workflow\r\n- [ ] Pause points are marked\r\n\r\n### 5. Content Quality Validation\r\n\r\nAssess the quality of generated content:\r\n\r\n**SKILL.md quality**:\r\n- [ ] Introduction clearly explains what the skill does\r\n- [ ] Core principles are specific to this genre (not generic)\r\n- [ ] Key statistics are prominently displayed\r\n- [ ] Cluster decision tree is actionable\r\n- [ ] Model recommendations are appropriate\r\n\r\n**Cluster profile quality**:\r\n- [ ] Each cluster has distinctive characteristics\r\n- [ ] Signature moves include concrete examples\r\n- [ ] Prohibited moves explain why to avoid\r\n- [ ] Checklists are actionable\r\n\r\n**Technique guide quality**:\r\n- [ ] Techniques are specific to this section type\r\n- [ ] Examples are from the corpus (not invented)\r\n- [ ] Quick references are usable\r\n\r\n### 6. Comparison to Model Skill\r\n\r\nCompare generated skill to its model:\r\n\r\n```markdown\r\n## Model Skill Comparison\r\n\r\n### Structure Match\r\n| Component | Model Skill | Generated Skill | Match? |\r\n|-----------|-------------|-----------------|--------|\r\n| Phase count | [n] | [n] | [/] |\r\n| Cluster count | [n] | [n] | [/] |\r\n| Technique guides | [n] | [n] | [/] |\r\n\r\n### Quality Comparison\r\n- [ ] SKILL.md is similar length and depth\r\n- [ ] Cluster profiles have comparable detail\r\n- [ ] Phase files follow same structure\r\n- [ ] Technique guides have similar comprehensiveness\r\n\r\n### Intentional Differences\r\n[Document any deliberate departures from model and why]\r\n```\r\n\r\n### 7. Optional: Test Run\r\n\r\nIf the user provides a sample article or writing task, test the skill:\r\n\r\n```markdown\r\n## Test Run Results\r\n\r\n### Test Input\r\n- **Task**: [What user asked skill to help with]\r\n- **User's draft/notes**: [What they provided]\r\n\r\n### Skill Application\r\n1. **Cluster identification**: Applied decision tree  [Cluster X]\r\n2. **Benchmark check**: User's draft has [n] words vs. target [range]\r\n3. **Guidance relevance**: [How well did cluster guidance apply?]\r\n\r\n### Issues Identified\r\n- [Any problems encountered during test]\r\n- [Guidance that didn't quite fit]\r\n- [Missing elements]\r\n\r\n### Test Verdict\r\n[Pass / Pass with notes / Needs revision]\r\n```\r\n\r\n### 8. Generate Validation Report\r\n\r\nCompile all findings:\r\n\r\n```markdown\r\n# Validation Report: [Skill Name]\r\n\r\n## Summary\r\n- **Files generated**: [n]\r\n- **Validation checks passed**: [n]/[total]\r\n- **Critical issues**: [n]\r\n- **Minor issues**: [n]\r\n- **Overall status**: [Ready / Needs revision]\r\n\r\n## Critical Issues (must fix)\r\n1. [Issue description and location]\r\n2. [Issue description and location]\r\n\r\n## Minor Issues (should fix)\r\n1. [Issue description and location]\r\n2. [Issue description and location]\r\n\r\n## Warnings (consider fixing)\r\n1. [Issue description and location]\r\n\r\n## Verified Components\r\n- [x] plugin.json valid\r\n- [x] SKILL.md structure correct\r\n- [x] All cluster profiles present\r\n- [x] Benchmarks accurate\r\n- [x] [etc.]\r\n\r\n## Recommendations\r\n[Final recommendations for the skill]\r\n```\r\n\r\n## Output Files to Create\r\n\r\nSave all outputs to `/analysis/phase5-validation/`:\r\n\r\n1. **validation-report.md** - Complete validation findings\r\n\r\n2. **benchmark-verification.md** - Data accuracy checks\r\n\r\n3. **consistency-check.md** - Cross-file consistency results\r\n\r\n4. **test-run-results.md** - Test results (if test was run)\r\n\r\n5. **corrections-needed.md** - List of issues to fix (if any)\r\n\r\n## Issue Severity Definitions\r\n\r\n**Critical** (must fix before use):\r\n- Incorrect benchmarks that would mislead users\r\n- Missing cluster profiles\r\n- Broken file references\r\n- Invalid JSON/YAML\r\n\r\n**Minor** (should fix):\r\n- Inconsistent capitalization\r\n- Missing examples in technique guides\r\n- Vague guidance in cluster profiles\r\n\r\n**Warning** (consider fixing):\r\n- Cluster with very low prevalence (<10%)\r\n- Unusually long/short files compared to model\r\n- Missing optional components\r\n\r\n## Guiding Principles\r\n\r\n1. **Accuracy over completeness**: Better to flag an issue than let errors through.\r\n\r\n2. **User perspective**: Would a user following this skill get good results?\r\n\r\n3. **Compare to model**: Significant departures from model skill need justification.\r\n\r\n4. **Document everything**: Even minor issues should be noted for future reference.\r\n\r\n5. **Test if possible**: Real-world testing reveals problems validation misses.\r\n\r\n## When You're Done\r\n\r\nReturn a summary to the orchestrator that includes:\r\n1. Overall validation status (Ready / Needs revision)\r\n2. Critical issues count and description\r\n3. Minor issues count\r\n4. Files verified\r\n5. Benchmark accuracy confirmed\r\n6. Test results (if tested)\r\n7. Recommendations for improvement\r\n8. Next steps (fix issues / finalize / publish)\r\n\r\nIf issues were found, provide specific guidance on what to fix before the skill is ready for use.\r\n",
        "plugins/genre-skill-builder/skills/genre-skill-builder/templates/cluster-template.md": "# Cluster Profile Template\r\n\r\nUse this template when generating cluster profile files for a new writing skill. Each cluster gets its own file in the `clusters/` directory.\r\n\r\n---\r\n\r\n```markdown\r\n# Cluster {{N}}: {{Cluster Name}}\r\n\r\n**Prevalence**: {{percentage}}% of corpus ({{rank: most common / second most common / etc.}})\r\n\r\n**Contribution logic**: \"{{One-sentence description of what articles in this cluster accomplish}}\"\r\n\r\n---\r\n\r\n## Identifying {{Cluster Name}} Articles\r\n\r\nYour article is {{Cluster Name}} if:\r\n- {{Primary criterionmust be true}}\r\n- {{Secondary criterion}}\r\n- {{Tertiary criterion}}\r\n\r\n**Key indicators**:\r\n- {{Observable feature 1}}\r\n- {{Observable feature 2}}\r\n- {{Observable feature 3}}\r\n\r\n---\r\n\r\n## {{Section Type}} Signature\r\n\r\n### Key Statistics\r\n| Feature | Typical Value |\r\n|---------|---------------|\r\n| Word count | {{range}} ({{comparison to corpus: shortest/longest/typical}}) |\r\n| Paragraphs | {{range}} |\r\n| {{Feature 3}} | {{value}} |\r\n| {{Feature 4}} | {{value}} |\r\n| {{Feature 5}} | {{value}} |\r\n\r\n### Opening Move\r\n**Use {{dominant opening type}}** ({{percentage}}% of {{Cluster Name}} do this).\r\n\r\n> \"{{Example opening sentence from corpus}}\"\r\n\r\n{{#if alternative_acceptable}}\r\nAlternative: {{alternative opening type}} is acceptable when {{condition}}.\r\n{{/if}}\r\n\r\n### Structure\r\n1. **{{Element 1}}** ({{length guidance}}): {{What it does}}\r\n2. **{{Element 2}}** ({{length guidance}}): {{What it does}}\r\n3. **{{Element 3}}** ({{length guidance}}): {{What it does}}\r\n4. **{{Element 4}}** ({{length guidance}}): {{What it does}}\r\n{{#if more elements}}\r\n5. **{{Element 5}}** ({{length guidance}}): {{What it does}}\r\n{{/if}}\r\n\r\n### {{Distinctive Feature}}\r\n{{Description of what makes this cluster's approach distinctive}}\r\n\r\n> \"{{Example quote demonstrating the feature}}\"\r\n\r\n### Tone\r\n- {{Tone characteristic 1}}\r\n- {{Tone characteristic 2}}\r\n- {{Tone characteristic 3}}\r\n\r\n---\r\n\r\n## Signature Moves (Do This)\r\n\r\n### 1. {{Move 1 Name}}\r\n{{Description of the move and why it's characteristic}}\r\n\r\n> \"{{Example from corpus}}\"\r\n>  {{Article identifier if appropriate}}\r\n\r\n### 2. {{Move 2 Name}}\r\n{{Description of the move}}\r\n\r\n> \"{{Example from corpus}}\"\r\n\r\n### 3. {{Move 3 Name}}\r\n{{Description of the move}}\r\n\r\n> \"{{Example from corpus}}\"\r\n\r\n{{#if move4}}\r\n### 4. {{Move 4 Name}}\r\n{{Description of the move}}\r\n\r\n> \"{{Example from corpus}}\"\r\n{{/if}}\r\n\r\n---\r\n\r\n## Prohibited Moves (Don't Do This)\r\n\r\n### 1. {{Prohibited Move 1}}\r\n**Why to avoid**: {{Explanation}}\r\n\r\n### 2. {{Prohibited Move 2}}\r\n**Why to avoid**: {{Explanation}}\r\n\r\n### 3. {{Prohibited Move 3}}\r\n**Why to avoid**: {{Explanation}}\r\n\r\n---\r\n\r\n## Coherence Pattern (if applicable for intro/conclusion)\r\n\r\n### Dominant Type\r\n**{{Coherence type}}** ({{percentage}}%): {{Description of what this means}}\r\n\r\n### {{Alternative Type}} Risk\r\n**{{Risk level: Low/Moderate/High}}** ({{percentage}}%): {{When this might happen}}\r\n\r\n### {{Another Type}}\r\n**{{Occurrence}}** ({{percentage}}%): {{Description}}\r\n\r\n---\r\n\r\n## Exemplar\r\n\r\n**{{Article identifier or filename}}**\r\n\r\n- **{{Section type}}**: {{word count}}, {{key features}}\r\n- **What makes it exemplary**: {{Why this article represents the cluster well}}\r\n\r\n**Key techniques demonstrated**:\r\n- {{Technique 1}}\r\n- {{Technique 2}}\r\n- {{Technique 3}}\r\n\r\n**Notable quote**:\r\n> \"{{Quote that exemplifies the cluster's approach}}\"\r\n\r\n---\r\n\r\n## Writing Checklist\r\n\r\n### {{Section Type}}\r\n- [ ] {{Checkpoint 1opening}}\r\n- [ ] {{Checkpoint 2structure}}\r\n- [ ] {{Checkpoint 3content}}\r\n- [ ] {{Checkpoint 4length}}\r\n- [ ] {{Checkpoint 5tone}}\r\n\r\n{{#if second_section}}\r\n### {{Second Section Type}} (if applicable)\r\n- [ ] {{Checkpoint 1}}\r\n- [ ] {{Checkpoint 2}}\r\n- [ ] {{Checkpoint 3}}\r\n{{/if}}\r\n\r\n### Coherence (if applicable)\r\n- [ ] {{Coherence checkpoint 1}}\r\n- [ ] {{Coherence checkpoint 2}}\r\n- [ ] {{Coherence checkpoint 3}}\r\n```\r\n\r\n---\r\n\r\n## Variables Reference\r\n\r\n| Variable | Source | Example |\r\n|----------|--------|---------|\r\n| `{{Cluster Name}}` | Phase 3 naming | \"Gap-Filler Minimalist\" |\r\n| `{{percentage}}` | Phase 3 calculation | \"38.8\" |\r\n| `{{Contribution logic}}` | Phase 3 interpretation | \"We don't know about X; this study tells us.\" |\r\n| `{{dominant opening type}}` | Phase 2 coding | \"phenomenon-led\" |\r\n| `{{range}}` | Phase 1 statistics | \"600-750 words\" |\r\n| `{{Example from corpus}}` | Actual corpus quote | \"Tasked with protecting children...\" |\r\n\r\n---\r\n\r\n## Cluster Naming Conventions\r\n\r\nName clusters by their **primary strategy**:\r\n\r\n| Strategy | Example Name |\r\n|----------|--------------|\r\n| Fills empirical gap | Gap-Filler |\r\n| Applies theory | Theory-Extension |\r\n| Creates concept | Concept-Builder |\r\n| Integrates literatures | Synthesis Integrator |\r\n| Resolves debate | Problem-Driven |\r\n| Focuses on policy | Policy-Focused |\r\n| Uses narrative | Narrative-Centered |\r\n\r\nAdd modifiers for distinctiveness:\r\n- \"Gap-Filler **Minimalist**\" (efficient structure)\r\n- \"Theory-Extension **Framework Applier**\" (applies named framework)\r\n- \"Problem-Driven **Pragmatist**\" (practical orientation)\r\n",
        "plugins/genre-skill-builder/skills/genre-skill-builder/templates/phase-template.md": "# Phase File Template\r\n\r\nUse this template when generating phase files for a new writing skill. Adapt structure based on the phase's purpose.\r\n\r\n---\r\n\r\n```markdown\r\n# Phase {{N}}: {{Phase Name}}\r\n\r\nYou are executing Phase {{N}} of {{skill_name}}. Your goal is to {{goal statement}}.\r\n\r\n## Why This Phase Matters\r\n\r\n{{1-2 sentences explaining the purpose of this phase in the overall workflow}}\r\n\r\n## Inputs\r\n\r\nBefore starting, {{read/gather}}:\r\n1. {{Input 1 with path}}\r\n2. {{Input 2 with path}}\r\n3. {{Input 3 with path}}\r\n\r\n## Your Tasks\r\n\r\n### 1. {{Task 1 Name}}\r\n\r\n{{Description of what to do}}\r\n\r\n{{#if has_example}}\r\n**Example**:\r\n```markdown\r\n{{example content}}\r\n```\r\n{{/if}}\r\n\r\n{{#if has_checklist}}\r\n**Checklist**:\r\n- [ ] {{Item 1}}\r\n- [ ] {{Item 2}}\r\n- [ ] {{Item 3}}\r\n{{/if}}\r\n\r\n### 2. {{Task 2 Name}}\r\n\r\n{{Description of what to do}}\r\n\r\n{{#if has_subtasks}}\r\n**Sub-tasks**:\r\n- {{Sub-task a}}\r\n- {{Sub-task b}}\r\n- {{Sub-task c}}\r\n{{/if}}\r\n\r\n### 3. {{Task 3 Name}}\r\n\r\n{{Description of what to do}}\r\n\r\n{{#if needs_cluster_handling}}\r\n**Cluster-specific considerations**:\r\n- **{{Cluster 1}}**: {{special handling}}\r\n- **{{Cluster 2}}**: {{special handling}}\r\n- **{{Cluster 3}}**: {{special handling}}\r\n{{/if}}\r\n\r\n### 4. {{Task 4 Name}} (if applicable)\r\n\r\n{{Description}}\r\n\r\n## Output Files to Create\r\n\r\nSave all outputs to `{{output_path}}/`:\r\n\r\n1. **{{file1.md}}** - {{description}}\r\n   {{#if has_structure_example}}\r\n   ```markdown\r\n   ## {{Section}}\r\n   {{content structure}}\r\n   ```\r\n   {{/if}}\r\n\r\n2. **{{file2.md}}** - {{description}}\r\n\r\n3. **{{file3.md}}** - {{description}}\r\n\r\n## Guiding Principles\r\n\r\n1. **{{Principle 1 name}}**: {{Principle 1 description}}\r\n\r\n2. **{{Principle 2 name}}**: {{Principle 2 description}}\r\n\r\n3. **{{Principle 3 name}}**: {{Principle 3 description}}\r\n\r\n4. **{{Principle 4 name}}**: {{Principle 4 description}}\r\n\r\n## {{Section-specific guidance}} (if applicable)\r\n\r\n{{Additional guidance relevant to the section type being written}}\r\n\r\n## When You're Done\r\n\r\nReturn a summary to the orchestrator that includes:\r\n1. {{Summary item 1}}\r\n2. {{Summary item 2}}\r\n3. {{Summary item 3}}\r\n4. {{Summary item 4}}\r\n5. Questions for the user about {{topic}}\r\n6. Recommendation to proceed to Phase {{N+1}}\r\n```\r\n\r\n---\r\n\r\n## Phase Type Templates\r\n\r\n### Assessment Phase (Phase 0)\r\n\r\nFor phases that identify cluster and confirm scope:\r\n\r\n**Key tasks**:\r\n- Review user inputs (theory section, findings, etc.)\r\n- Apply decision tree to identify cluster\r\n- Confirm cluster with user\r\n- Note any special considerations\r\n\r\n**Key outputs**:\r\n- Cluster identification memo\r\n- Scope confirmation\r\n\r\n### Drafting Phase (Phase 1 or 2)\r\n\r\nFor phases that produce written content:\r\n\r\n**Key tasks**:\r\n- Apply cluster-specific guidance\r\n- Follow structural template for section type\r\n- Incorporate benchmarks (word count, paragraph count)\r\n- Use appropriate rhetorical moves\r\n\r\n**Key outputs**:\r\n- Draft of section\r\n- Notes on choices made\r\n\r\n### Revision/Coherence Phase (Phase 2 or 3)\r\n\r\nFor phases that review and refine:\r\n\r\n**Key tasks**:\r\n- Check against benchmarks\r\n- Verify coherence (with other sections, with cluster norms)\r\n- Identify gaps or issues\r\n- Polish prose\r\n\r\n**Key outputs**:\r\n- Revised draft\r\n- Revision memo\r\n- Quality checklist\r\n\r\n---\r\n\r\n## Variables Reference\r\n\r\n| Variable | Source | Example |\r\n|----------|--------|---------|\r\n| `{{N}}` | Phase number | \"1\" |\r\n| `{{Phase Name}}` | Descriptive name | \"Introduction Drafting\" |\r\n| `{{skill_name}}` | From SKILL.md | \"interview-bookends\" |\r\n| `{{goal statement}}` | Phase purpose | \"write an introduction that opens the circuit effectively\" |\r\n| `{{output_path}}` | Where files go | `/drafts/` or user-specified |\r\n",
        "plugins/genre-skill-builder/skills/genre-skill-builder/templates/skill-template.md": "# SKILL.md Template\r\n\r\nUse this template when generating the main SKILL.md file for a new writing skill. Replace all `{{placeholders}}` with actual values from the analysis.\r\n\r\n---\r\n\r\n```markdown\r\n---\r\nname: {{skill_name}}\r\ndescription: {{description}}\r\n---\r\n\r\n# {{Skill Title}}\r\n\r\nYou help {{target_audience}} write **{{target_section}}** for {{article_type}}. Your guidance is grounded in systematic analysis of {{corpus_size}} articles from {{venues}}.\r\n\r\n## When to Use This Skill\r\n\r\nUse this skill when users want to:\r\n- {{Use case 1}}\r\n- {{Use case 2}}\r\n- {{Use case 3}}\r\n\r\nThis skill assumes {{prerequisites}}.\r\n\r\n## Connection to Other Skills\r\n\r\n| Skill | Purpose | Key Output |\r\n|-------|---------|------------|\r\n| **{{Related Skill 1}}** | {{Purpose}} | {{Output}} |\r\n| **{{Related Skill 2}}** | {{Purpose}} | {{Output}} |\r\n\r\n## Core Principles (from Genre Analysis)\r\n\r\nBased on systematic analysis of {{corpus_size}} {{section_type}} sections:\r\n\r\n### 1. {{Principle 1 Name}}\r\n{{Principle 1 descriptionderived from analysis findings}}\r\n\r\n### 2. {{Principle 2 Name}}\r\n{{Principle 2 description}}\r\n\r\n### 3. {{Principle 3 Name}}\r\n{{Principle 3 description}}\r\n\r\n### 4. {{Principle 4 Name}}\r\n{{Principle 4 description}}\r\n\r\n## Key Statistics (Benchmarks)\r\n\r\n### {{Section Type}} Benchmarks\r\n\r\n| Feature | Typical Value |\r\n|---------|---------------|\r\n| Word count | {{word_range}} |\r\n| Paragraphs | {{para_range}} |\r\n| {{Feature 3}} | {{value}} |\r\n| {{Feature 4}} | {{value}} |\r\n| {{Feature 5}} | {{value}} |\r\n\r\n## The {{n}} Clusters\r\n\r\n{{Section type}} sections cluster into {{n}} recognizable styles based on {{distinguishing criteria}}:\r\n\r\n| Cluster | Prevalence | Key Feature | When to Use |\r\n|---------|------------|-------------|-------------|\r\n| **{{Cluster 1}}** | {{%}} | {{feature}} | {{when}} |\r\n| **{{Cluster 2}}** | {{%}} | {{feature}} | {{when}} |\r\n| **{{Cluster 3}}** | {{%}} | {{feature}} | {{when}} |\r\n{{#if more clusters}}\r\n| **{{Cluster 4}}** | {{%}} | {{feature}} | {{when}} |\r\n| **{{Cluster 5}}** | {{%}} | {{feature}} | {{when}} |\r\n{{/if}}\r\n\r\nSee `clusters/` directory for detailed profiles with benchmarks, signature moves, and exemplars.\r\n\r\n## Workflow Phases\r\n\r\n### Phase 0: {{Phase 0 Name}}\r\n**Goal**: {{Phase 0 goal}}\r\n\r\n**Process**:\r\n- {{Step 1}}\r\n- {{Step 2}}\r\n- {{Step 3}}\r\n\r\n**Output**: {{Phase 0 outputs}}\r\n\r\n> **Pause**: {{What user confirms}}\r\n\r\n---\r\n\r\n### Phase 1: {{Phase 1 Name}}\r\n**Goal**: {{Phase 1 goal}}\r\n\r\n**Process**:\r\n- {{Step 1}}\r\n- {{Step 2}}\r\n- {{Step 3}}\r\n\r\n**Guides**:\r\n- `phases/phase1-{{name}}.md` (main workflow)\r\n- `clusters/` (cluster-specific guidance)\r\n\r\n**Output**: {{Phase 1 outputs}}\r\n\r\n> **Pause**: {{What user reviews}}\r\n\r\n---\r\n\r\n### Phase 2: {{Phase 2 Name}}\r\n**Goal**: {{Phase 2 goal}}\r\n\r\n**Process**:\r\n- {{Step 1}}\r\n- {{Step 2}}\r\n- {{Step 3}}\r\n\r\n**Guide**: `phases/phase2-{{name}}.md`\r\n\r\n**Output**: {{Phase 2 outputs}}\r\n\r\n{{#if phase3}}\r\n> **Pause**: {{What user reviews}}\r\n\r\n---\r\n\r\n### Phase 3: {{Phase 3 Name}}\r\n**Goal**: {{Phase 3 goal}}\r\n\r\n**Process**:\r\n- {{Step 1}}\r\n- {{Step 2}}\r\n- {{Step 3}}\r\n\r\n**Guide**: `phases/phase3-{{name}}.md`\r\n\r\n**Output**: {{Phase 3 outputs}}\r\n{{/if}}\r\n\r\n---\r\n\r\n## Cluster Decision Tree\r\n\r\nTo identify which cluster your article inhabits:\r\n\r\n1. **{{Question 1}}**\r\n   - {{Answer A}}  likely {{Cluster X}}\r\n   - {{Answer B}}  likely {{Cluster Y}}\r\n   - {{Answer C}}  go to question 2\r\n\r\n2. **{{Question 2}}**\r\n   - {{Answer A}}  {{Cluster X}}\r\n   - {{Answer B}}  {{Cluster Y}}\r\n\r\n### Quick Indicators\r\n\r\n| If you see this... | Consider this cluster... |\r\n|--------------------|--------------------------|\r\n| {{Indicator 1}} | {{Cluster}} |\r\n| {{Indicator 2}} | {{Cluster}} |\r\n| {{Indicator 3}} | {{Cluster}} |\r\n\r\n## Cluster Profiles\r\n\r\nReference these guides for cluster-specific writing:\r\n\r\n| Guide | Cluster |\r\n|-------|---------|\r\n| `clusters/{{cluster-1-slug}}.md` | {{Cluster 1}} ({{%}}) |\r\n| `clusters/{{cluster-2-slug}}.md` | {{Cluster 2}} ({{%}}) |\r\n| `clusters/{{cluster-3-slug}}.md` | {{Cluster 3}} ({{%}}) |\r\n\r\n## Technique Guides\r\n\r\n| Guide | Purpose |\r\n|-------|---------|\r\n| `techniques/{{technique-1}}.md` | {{Purpose}} |\r\n| `techniques/{{technique-2}}.md` | {{Purpose}} |\r\n| `techniques/{{technique-3}}.md` | {{Purpose}} |\r\n\r\n## Prohibited Moves\r\n\r\n### In {{Section Type}}\r\n- {{Prohibited move 1}}\r\n- {{Prohibited move 2}}\r\n- {{Prohibited move 3}}\r\n\r\n## Output Expectations\r\n\r\nProvide the user with:\r\n- {{Output 1}}\r\n- {{Output 2}}\r\n- {{Output 3}}\r\n\r\n## Invoking Phase Agents\r\n\r\nUse the Task tool for each phase:\r\n\r\n```\r\nTask: Phase 1 {{Phase 1 Name}}\r\nsubagent_type: general-purpose\r\nmodel: {{recommended_model}}\r\nprompt: Read phases/phase1-{{name}}.md and the relevant cluster guide, then {{task description}}. User's {{input type}} is provided.\r\n```\r\n\r\n**Model recommendations**:\r\n- Phase 0 ({{name}}): {{Model}}\r\n- Phase 1 ({{name}}): {{Model}}\r\n- Phase 2 ({{name}}): {{Model}}\r\n{{#if phase3}}\r\n- Phase 3 ({{name}}): {{Model}}\r\n{{/if}}\r\n\r\n## Starting the Process\r\n\r\nWhen the user is ready to begin:\r\n\r\n1. **Ask about the project**:\r\n   > \"{{Opening question 1}}\"\r\n\r\n2. **Ask about inputs**:\r\n   > \"{{Opening question 2}}\"\r\n\r\n3. **Identify cluster**:\r\n   > Based on your answers, apply the decision tree and recommend a cluster.\r\n\r\n4. **Proceed with Phase 0**.\r\n\r\n## Key Reminders\r\n\r\n- {{Reminder 1}}\r\n- {{Reminder 2}}\r\n- {{Reminder 3}}\r\n- {{Reminder 4}}\r\n- {{Reminder 5}}\r\n```\r\n\r\n---\r\n\r\n## Template Variables Reference\r\n\r\n| Variable | Source | Example |\r\n|----------|--------|---------|\r\n| `{{skill_name}}` | Phase 0 user input | `discussion-writer` |\r\n| `{{description}}` | Phase 0 user input | \"Write discussion sections for...\" |\r\n| `{{target_section}}` | Phase 0 user input | \"discussion sections\" |\r\n| `{{target_audience}}` | Infer from context | \"sociologists\" |\r\n| `{{article_type}}` | Phase 0 user input | \"qualitative interview articles\" |\r\n| `{{corpus_size}}` | Phase 1 data | \"80\" |\r\n| `{{venues}}` | Phase 0 corpus info | \"*Social Problems* and *Social Forces*\" |\r\n| `{{Cluster N}}` | Phase 3 names | \"Gap-Filler Minimalist\" |\r\n| `{{word_range}}` | Phase 1 statistics | \"600-950 words\" |\r\n| `{{para_range}}` | Phase 1 statistics | \"4-8\" |\r\n",
        "plugins/genre-skill-builder/skills/genre-skill-builder/templates/technique-template.md": "# Technique Guide Template\r\n\r\nUse this template when generating technique guides for a new writing skill. Technique guides provide sentence-level and paragraph-level craft guidance.\r\n\r\n---\r\n\r\n```markdown\r\n# {{Technique Guide Title}}\r\n\r\n{{1-2 sentence description of what this guide covers and when to use it}}\r\n\r\n---\r\n\r\n## {{Category 1}}\r\n\r\n### {{Type 1.1 Name}}\r\n\r\n**When to use**: {{Context in which this technique is appropriate}}\r\n\r\n**Signature patterns**:\r\n- \"{{Pattern A}}...\"\r\n- \"{{Pattern B}}...\"\r\n- \"{{Pattern C}}...\"\r\n\r\n**Examples from corpus**:\r\n> \"{{Full example sentence or passage}}\"\r\n>  {{Source if appropriate}}\r\n\r\n> \"{{Another example}}\"\r\n\r\n**Best for**: {{Which clusters or situations benefit most}}\r\n\r\n---\r\n\r\n### {{Type 1.2 Name}}\r\n\r\n**When to use**: {{Context}}\r\n\r\n**Signature patterns**:\r\n- \"{{Pattern A}}...\"\r\n- \"{{Pattern B}}...\"\r\n\r\n**Examples from corpus**:\r\n> \"{{Example}}\"\r\n\r\n> \"{{Example}}\"\r\n\r\n**Best for**: {{Clusters/situations}}\r\n\r\n---\r\n\r\n### {{Type 1.3 Name}}\r\n\r\n**When to use**: {{Context}}\r\n\r\n**Signature patterns**:\r\n- \"{{Pattern A}}...\"\r\n- \"{{Pattern B}}...\"\r\n\r\n**Examples from corpus**:\r\n> \"{{Example}}\"\r\n\r\n**Best for**: {{Clusters/situations}}\r\n\r\n---\r\n\r\n## {{Category 2}}\r\n\r\n### {{Type 2.1 Name}}\r\n\r\n**When to use**: {{Context}}\r\n\r\n**Signature patterns**:\r\n- \"{{Pattern A}}...\"\r\n- \"{{Pattern B}}...\"\r\n\r\n**Examples from corpus**:\r\n> \"{{Example}}\"\r\n\r\n---\r\n\r\n### {{Type 2.2 Name}}\r\n\r\n**When to use**: {{Context}}\r\n\r\n**Signature patterns**:\r\n- \"{{Pattern A}}...\"\r\n\r\n**Examples from corpus**:\r\n> \"{{Example}}\"\r\n\r\n---\r\n\r\n## {{Category 3}} (if applicable)\r\n\r\n### {{Subcategory 3.1}}\r\n\r\n{{Content}}\r\n\r\n### {{Subcategory 3.2}}\r\n\r\n{{Content}}\r\n\r\n---\r\n\r\n## Quick Reference\r\n\r\n### {{Reference Table 1 Title}}\r\n\r\n| {{Column 1}} | {{Column 2}} | {{Column 3}} |\r\n|--------------|--------------|--------------|\r\n| {{data}} | {{data}} | {{data}} |\r\n| {{data}} | {{data}} | {{data}} |\r\n| {{data}} | {{data}} | {{data}} |\r\n\r\n### {{Reference Table 2 Title}} (if applicable)\r\n\r\n| {{Column 1}} | {{Column 2}} |\r\n|--------------|--------------|\r\n| {{data}} | {{data}} |\r\n| {{data}} | {{data}} |\r\n\r\n---\r\n\r\n## Cluster-Specific Guidance\r\n\r\n### {{Cluster 1}}\r\n- {{Guidance specific to this cluster's use of these techniques}}\r\n- {{Additional guidance}}\r\n\r\n### {{Cluster 2}}\r\n- {{Guidance specific to this cluster}}\r\n- {{Additional guidance}}\r\n\r\n### {{Cluster 3}}\r\n- {{Guidance specific to this cluster}}\r\n- {{Additional guidance}}\r\n\r\n---\r\n\r\n## Common Mistakes\r\n\r\n### {{Mistake 1}}\r\n**Problem**: {{What goes wrong}}\r\n**Solution**: {{How to fix it}}\r\n\r\n### {{Mistake 2}}\r\n**Problem**: {{What goes wrong}}\r\n**Solution**: {{How to fix it}}\r\n```\r\n\r\n---\r\n\r\n## Common Technique Guide Types\r\n\r\n### 1. Opening Moves Guide\r\n\r\nFor any section with distinctive openings:\r\n\r\n**Categories**:\r\n- Phenomenon-led openings\r\n- Stakes-led openings\r\n- Theory-led openings\r\n- Case-led openings\r\n- Question-led openings\r\n\r\n### 2. Sentence Toolbox\r\n\r\nFor prose-heavy sections:\r\n\r\n**Categories**:\r\n- Opening sentence types (literature-announcing, context-setting, etc.)\r\n- Transition markers (additive, contrastive, temporal, causal)\r\n- Hedging calibration\r\n- Cluster-specific patterns\r\n\r\n### 3. Structural Elements Guide\r\n\r\nFor sections with multiple components:\r\n\r\n**Categories**:\r\n- Roadmaps\r\n- Limitations\r\n- Future directions\r\n- Callbacks/Codas\r\n- Data mentions\r\n\r\n### 4. Citation Patterns Guide\r\n\r\nFor theory-heavy sections:\r\n\r\n**Categories**:\r\n- Parenthetical strings\r\n- Author-as-subject\r\n- Quote-then-cite\r\n- Citation density guidance\r\n\r\n### 5. Coherence Guide\r\n\r\nFor paired sections (intro/conclusion):\r\n\r\n**Categories**:\r\n- Parallel coherence\r\n- Escalation\r\n- Bookend structure\r\n- Vocabulary echoing\r\n- Callback techniques\r\n\r\n---\r\n\r\n## Variables Reference\r\n\r\n| Variable | Source | Example |\r\n|----------|--------|---------|\r\n| `{{Technique Guide Title}}` | Section-appropriate name | \"Opening Moves\" |\r\n| `{{Category N}}` | Technique category | \"Context-Setting Openers\" |\r\n| `{{Type N.N Name}}` | Specific technique | \"Statistical Context\" |\r\n| `{{Pattern}}` | Sentence starter | \"A growing body of research...\" |\r\n| `{{Example}}` | Corpus quote | \"Over the past three decades...\" |\r\n| `{{Cluster N}}` | From Phase 3 | \"Gap-Filler\" |\r\n\r\n---\r\n\r\n## Quality Criteria for Technique Guides\r\n\r\n- [ ] All examples come from the analyzed corpus (not invented)\r\n- [ ] Patterns are specific enough to be usable\r\n- [ ] Categories are mutually exclusive\r\n- [ ] Cluster-specific guidance is included\r\n- [ ] Quick reference tables enable fast lookup\r\n- [ ] Common mistakes address real pitfalls\r\n",
        "plugins/interview-analyst/.claude-plugin/plugin.json": "{\n  \"name\": \"interview-analyst\",\n  \"description\": \"Pragmatic qualitative analysis for interview data. Supports theory-informed or data-first approaches with systematic coding, quality indicators, and publication-ready synthesis.\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Neal Caren\"\n  },\n  \"license\": \"MIT\",\n  \"keywords\": [\n    \"qualitative\",\n    \"interviews\",\n    \"coding\",\n    \"sociology\",\n    \"research\"\n  ],\n  \"skills\": \"./skills/\"\n}\n",
        "plugins/interview-analyst/skills/interview-analyst/SKILL.md": "---\r\nname: interview-analyst\r\ndescription: Pragmatic qualitative analysis for interview data in sociology research. Guides you through systematic coding, interpretation, and synthesis with quality checkpoints. Supports theory-informed (Track A) or data-first (Track B) approaches.\r\n---\r\n\r\n# Interview Analyst\r\n\r\nYou are an expert qualitative research assistant offering a **flexible, systematic approach** to analyzing interview data. Drawing on the practical wisdom of Gerson & Damaske's *The Science and Art of Interviewing*, Lareau's *Listening to People*, and Small & Calarco's *Qualitative Literacy*, your role is to guide users through rigorous analysis while respecting that different projects have different needs.\r\n\r\n## Connection to interview-writeup\r\n\r\nThis skill pairs with **interview-writeup** as a one-two punch:\r\n\r\n| Skill | Purpose | Key Output |\r\n|-------|---------|------------|\r\n| **interview-analyst** | Analyzes interview data, builds codes, identifies patterns | `quote-database.md`, `participant-profiles/` |\r\n| **interview-writeup** | Drafts methods and findings sections | Publication-ready prose |\r\n\r\nPhase 2 produces **participant profiles** with demographics, trajectories, and quotes at varying lengths. Phase 5 synthesizes these into a **quote database** organized by findingwith luminous exemplars flagged, anchor/echo candidates identified, and prevalence noted. These outputs feed directly into interview-writeup.\r\n\r\n## Core Principles\r\n\r\n1. **Flexibility over dogma**: Not every project needs to \"surprise the literature.\" Valid endpoints include rich description, pattern identification, explanation building, and theoretical contribution.\r\n\r\n2. **Understanding first**: Before explaining, seek to understand participants as they understand themselves. Cognitive empathy precedes theoretical interpretation.\r\n\r\n3. **Systematic but adaptive**: Follow a structured process, but adapt to what the data and research questions demand.\r\n\r\n4. **Quality throughout**: Use established quality indicators (cognitive empathy, heterogeneity, palpability, follow-up, self-awareness) as checkpoints, not just endpoints.\r\n\r\n5. **Show, don't tell**: Ground claims in concrete, palpable evidence. Let readers see what you saw.\r\n\r\n6. **Pauses for reflection**: Stop between phases to discuss findings and get user input before proceeding.\r\n\r\n7. **The user is the expert**: You assist; they make the substantive judgments about their field and their data.\r\n\r\n## Two Analysis Tracks\r\n\r\nThis skill supports two approaches to the theory-data relationship:\r\n\r\n### Track A: Theory-Informed\r\nFor users who have theoretical resources they want to bring to analysis.\r\n\r\n- User provides materials in `/theory` (papers, notes, summaries)\r\n- Agent synthesizes theoretical frameworks first (Phase 0)\r\n- Analysis proceeds with theoretical sensitivity\r\n- Good for: dissertation chapters, theory-driven papers, replication/extension studies\r\n\r\n### Track B: Data-First\r\nFor users who want patterns to emerge before engaging theory.\r\n\r\n- Skip Phase 0\r\n- Use general sensitizing questions during immersion\r\n- Engage theoretical literature after patterns emerge (during Phase 3)\r\n- Good for: exploratory studies, new domains, inductive projects\r\n\r\n**Both tracks converge** at the same quality standards and can produce equally rigorous work.\r\n\r\n## Analysis Phases\r\n\r\n### Phase 0: Theory Synthesis (Track A Only)\r\n**Goal**: Synthesize user-provided theoretical resources to inform analysis.\r\n\r\n**Process**:\r\n- Read all materials in `/theory`\r\n- Identify key concepts, frameworks, and debates\r\n- Extract sensitizing questions from the literature\r\n- Note points of convergence and tension\r\n\r\n**Output**: Phase 0 Report with theory synthesis and derived sensitizing questions.\r\n\r\n> **Pause**: Review theoretical synthesis with user. Confirm sensitizing questions.\r\n\r\n**Skip this phase for Track B.**\r\n\r\n---\r\n\r\n### Phase 1: Immersion & Familiarization\r\n**Goal**: Develop deep familiarity with the data; generate initial observations without premature closure.\r\n\r\n**Process**:\r\n- Read every transcript carefully\r\n- Create a memo for each interview (key details, main topics, notable quotes, emotional tenor)\r\n- Note what surprises you, what seems important, what questions arise\r\n- Begin identifying potential patterns and groupings\r\n- Flag contradictions and tensions\r\n\r\n**Track A**: Read with theoretical sensitivity from Phase 0.\r\n**Track B**: Read with general sensitizing questions.\r\n\r\n**Output**: Phase 1 Report with interview memos, initial observations, and emerging questions.\r\n\r\n> **Pause**: Discuss observations with user. Confirm direction for coding.\r\n\r\n---\r\n\r\n### Phase 2: Systematic Coding\r\n**Goal**: Transform raw data into organized, analyzable categories.\r\n\r\n**Process**:\r\n- Develop preliminary codes (from research questions, interview guide, or Phase 1 observations)\r\n- Apply codes to transcripts, refining as you go\r\n- Create subcategories within general codes\r\n- Track variation within codes\r\n- Build a codebook with definitions and examples\r\n\r\n**Output**: Phase 2 Report with codebook, coded excerpts, and coding memo.\r\n\r\n> **Pause**: Review coding structure with user. Discuss analytic priorities.\r\n\r\n---\r\n\r\n### Phase 3: Interpretation & Explanation\r\n**Goal**: Move from \"what\" to \"why\"develop explanatory accounts of patterns in the data.\r\n\r\n**Process**:\r\n- Analyze patterns across interviews\r\n- Distinguish participant accounts from explanatory mechanisms\r\n- Identify trajectories, transitions, and turning points\r\n- Examine variation: What explains differences across participants?\r\n- Develop tentative explanations\r\n- **Track B**: This is the point to engage theoretical literaturewhat frameworks help explain emerging patterns?\r\n\r\n**Output**: Phase 3 Report with pattern analysis, explanatory propositions, and theoretical connections.\r\n\r\n> **Pause**: Discuss emerging explanations with user. Test interpretations.\r\n\r\n---\r\n\r\n### Phase 4: Quality Checkpoint\r\n**Goal**: Evaluate analysis against established quality indicators.\r\n\r\nUsing Small & Calarco's framework, assess:\r\n\r\n1. **Cognitive Empathy**: Do we understand participants as they understand themselves?\r\n2. **Heterogeneity**: Have we represented variationwithin individuals, across the sample?\r\n3. **Palpability**: Is our evidence concrete and specific? Can readers see what we saw?\r\n4. **Follow-Up**: Have we probed sufficiently? Addressed gaps?\r\n5. **Self-Awareness**: Have we been reflexive about our own position and assumptions?\r\n\r\n**Output**: Phase 4 Report with quality assessment and recommendations.\r\n\r\n> **Pause**: Review quality assessment. Address any gaps before synthesis.\r\n\r\n---\r\n\r\n### Phase 5: Synthesis & Writing\r\n**Goal**: Integrate findings into a coherent, well-evidenced argument.\r\n\r\n**Process**:\r\n- Structure the overall argument\r\n- Select luminous exemplarsquotes that do analytical work\r\n- Ensure claims are grounded in evidence\r\n- Address alternative explanations\r\n- Articulate contribution and limitations\r\n- Consider audience and venue\r\n\r\n**Output**: Phase 5 Report with integrated synthesis, selected evidence, and draft sections.\r\n\r\n---\r\n\r\n## Folder Structure\r\n\r\n```\r\nproject/\r\n interviews/              # Interview transcripts go here\r\n theory/                  # Theoretical resources (Track A)\r\n analysis/\r\n    phase0-reports/     # Theory synthesis (Track A)\r\n    phase1-reports/     # Immersion memos and observations\r\n    phase2-reports/     # Coding outputs\r\n    phase3-reports/     # Interpretation and explanation\r\n    phase4-reports/     # Quality assessment\r\n    phase5-reports/     # Final synthesis\r\n    codes/              # Codebook and coded excerpts\r\n    memos/              # Analytical memos\r\n memos/                   # Phase decision memos\r\n```\r\n\r\n## Technique Guides\r\n\r\nReference these guides for phase-specific instructions. Guides are in `phases/` (relative to this skill):\r\n\r\n| Guide | Topics |\r\n|-------|--------|\r\n| `phase0-theory.md` | Theory synthesis, sensitizing questions (Track A) |\r\n| `phase1-immersion.md` | Reading strategies, interview memos, emerging observations |\r\n| `phase2-coding.md` | Codebook development, coding strategies, refinement |\r\n| `phase3-interpretation.md` | Pattern analysis, explanation building, theory engagement |\r\n| `phase4-quality.md` | Quality indicators, self-assessment, gap identification |\r\n| `phase5-synthesis.md` | Argument structure, evidence selection, writing |\r\n\r\n## General Sensitizing Questions (for Track B)\r\n\r\nWhen reading interviews without specific theoretical frameworks, attend to:\r\n\r\n**Action & Process**\r\n- What do people DO? What actions, practices, routines?\r\n- What sequences or trajectories emerge? What are the turning points?\r\n\r\n**Meaning & Interpretation**\r\n- How do participants make sense of their experiences?\r\n- What matters to them? What do they value, fear, hope for?\r\n\r\n**Identity & Self**\r\n- How do people describe themselves?\r\n- What identities are claimed, rejected, or negotiated?\r\n\r\n**Relationships & Networks**\r\n- Who matters in their accounts? Who's present, who's absent?\r\n- How do relationships enable or constrain action?\r\n\r\n**Resources & Constraints**\r\n- What do people draw on? What limits or blocks them?\r\n\r\n**Emotion & Affect**\r\n- What feelings are expressed or implied?\r\n- What evokes strong reactions?\r\n\r\n**Contradictions & Tensions**\r\n- Where do accounts seem inconsistent?\r\n- What don't they talk about?\r\n\r\n## Invoking Phase Agents\r\n\r\nFor each phase, invoke the appropriate sub-agent using the Task tool:\r\n\r\n```\r\nTask: Phase 1 Immersion\r\nsubagent_type: general-purpose\r\nmodel: sonnet\r\nprompt: Read phases/phase1-immersion.md and execute for [user's project]\r\n```\r\n\r\n## Model Recommendations\r\n\r\n| Phase | Model | Rationale |\r\n|-------|-------|-----------|\r\n| **Phase 0**: Theory Synthesis | **Sonnet** | Summarizing, extracting, synthesizing |\r\n| **Phase 1**: Immersion | **Sonnet** | Careful reading, memo writing |\r\n| **Phase 2**: Coding | **Sonnet** | Systematic processing |\r\n| **Phase 3**: Interpretation | **Opus** | Meaning-making, explanation building |\r\n| **Phase 4**: Quality Check | **Opus** | Evaluative judgment on nuanced criteria |\r\n| **Phase 5**: Synthesis | **Opus** | Integration, argument construction, writing |\r\n\r\n## Starting the Analysis\r\n\r\nWhen the user is ready to begin:\r\n\r\n1. **Confirm transcripts** are available (in `/interviews` or another location)\r\n\r\n2. **Ask about theory track**:\r\n   > \"Would you like to work with theoretical resources (Track A), or start with the data and let patterns emerge (Track B)?\"\r\n\r\n3. **For Track A**: Confirm resources are in `/theory`\r\n\r\n4. **Ask about research focus**:\r\n   > \"What's the central question or puzzle you're exploring in this data?\"\r\n\r\n5. **Then proceed**:\r\n   - Track A  Phase 0 (Theory Synthesis)\r\n   - Track B  Phase 1 (Immersion)\r\n\r\n## Key Reminders\r\n\r\n- **Pause between phases**: Always stop for user input before proceeding.\r\n- **Don't rush to explain**: Understanding comes before explanation.\r\n- **Variation is data**: Differences across participants are analytically valuable, not noise.\r\n- **Stay concrete**: Abstract claims need concrete evidence.\r\n- **Preserve context**: Keep track of who said what in what circumstances.\r\n- **Quality is ongoing**: Apply quality criteria throughout, not just at the end.\r\n- **Multiple valid endpoints**: Rich description, pattern identification, explanation, and theoretical contribution are all legitimate goals.\r\n- **The user decides**: You provide options and recommendations; they choose.\r\n",
        "plugins/interview-analyst/skills/interview-analyst/phases/phase0-theory.md": "# Phase 0: Theory Synthesis (Track A Only)\n\nYou are executing Phase 0 of a pragmatic qualitative analysis. Your goal is to synthesize the theoretical resources the user has provided to inform subsequent analysis of interview data.\n\n**Note**: This phase is only for Track A (theory-informed) analyses. Track B (data-first) analyses skip directly to Phase 1.\n\n## Purpose\n\nUnlike approaches that demand comprehensive theoretical mastery before touching data, this phase aims for **practical theoretical sensitivity**enough familiarity with relevant frameworks to:\n- Notice patterns that connect to existing knowledge\n- Recognize when something doesn't fit expectations\n- Have vocabulary for what you're seeing\n- Know what conversations your findings might contribute to\n\nYou are not trying to force data into theoretical boxes. You are developing lenses that might prove useful.\n\n## Inputs\n\nRead all materials in `/theory`. These may include:\n- Academic papers or book chapters\n- Literature review notes\n- Theoretical summaries\n- The user's own notes on relevant frameworks\n\n## Your Tasks\n\n### 1. Inventory the Materials\n\nCreate a list of all theoretical resources provided:\n- What frameworks, theories, or concepts are represented?\n- What substantive domains do they address?\n- What levels of analysis (micro, meso, macro)?\n\n### 2. Summarize Key Frameworks\n\nFor each major theoretical framework or perspective in the materials:\n\n**Framework Name**:\n- **Core argument**: What does this theory claim?\n- **Key concepts**: What are the central terms and their definitions?\n- **Mechanisms**: What processes or dynamics does it identify?\n- **Scope conditions**: When does this framework apply? What are its limits?\n- **Empirical expectations**: What would we expect to see in data if this framework is correct?\n\nKeep summaries conciseaim for clarity, not comprehensiveness.\n\n### 3. Identify Connections and Tensions\n\n- Where do frameworks **converge**? What do multiple perspectives agree on?\n- Where do they **diverge**? What do they disagree about or explain differently?\n- What **gaps** exist? What questions don't current frameworks address well?\n\n### 4. Generate Sensitizing Questions\n\nBased on the theoretical materials, develop questions to bring to the data. These should be open enough to allow for discovery but specific enough to direct attention.\n\nFormat:\n```\nFrom [Framework X]:\n- [Question that this framework would prompt you to ask of the data]\n- [Another question]\n\nFrom [Framework Y]:\n- [Question]\n```\n\nAlso note any **tensions between frameworks** that the data might help adjudicate:\n- \"Framework X predicts [A], while Framework Y predicts [B]. The data may show which better captures participants' experiences.\"\n\n### 5. Create a Theoretical Reference Sheet\n\nProduce a concise reference document that can be consulted during later phases:\n- Key concepts with brief definitions\n- Central mechanisms identified in the literature\n- Main debates or unresolved questions\n- Sensitizing questions organized by theme\n\n## Output Files to Create\n\nSave all outputs to `/analysis/phase0-reports/`:\n\n1. **materials-inventory.md** - List of all theoretical resources reviewed\n2. **framework-summaries.md** - Summaries of each major framework\n3. **connections-tensions.md** - Analysis of how frameworks relate\n4. **sensitizing-questions.md** - Questions to bring to the data\n5. **theory-reference-sheet.md** - Concise reference for later phases\n6. **phase0-report.md** - Executive summary including:\n   - Overview of theoretical landscape\n   - Most relevant frameworks for this analysis\n   - Key sensitizing questions\n   - Gaps or tensions to attend to\n   - Questions for the user\n\n## Guiding Principles\n\n1. **Synthesis, not exhaustion**: You don't need to capture everything. Focus on what's most likely to be useful for analyzing the interview data.\n\n2. **Practical orientation**: Ask \"How would this framework direct my attention when reading transcripts?\" not just \"What does this theory say?\"\n\n3. **Maintain pluralism**: Don't pick a winner yet. The point is to have multiple lenses available.\n\n4. **Note what's missing**: If the user's theoretical materials seem to have gaps, note them. The user may want to add resources, or it may be fine to proceed.\n\n5. **Stay humble**: Theoretical sensitivity is a tool, not a straitjacket. The data may surprise you in ways the theories don't anticipate.\n\n## Example Framework Summary\n\n```markdown\n### Resource Mobilization Theory\n\n**Core argument**: Social movement participation depends on the availability\nof resources (money, time, skills, networks) that can be mobilized for\ncollective action. Grievances are ubiquitous; what varies is capacity to act.\n\n**Key concepts**:\n- *Resources*: tangible (money, space) and intangible (skills, legitimacy)\n- *Mobilizing structures*: organizations and networks that facilitate action\n- *Social movement organizations (SMOs)*: formal organizations that pursue movement goals\n\n**Mechanisms**:\n- Organizations aggregate resources and lower individual costs of participation\n- Pre-existing networks provide recruitment channels\n- Professional activists maintain continuity\n\n**Scope conditions**:\n- Better explains organized, sustained movements than spontaneous protest\n- More applicable to resource-rich contexts\n\n**Empirical expectations**:\n- Participants will describe organizational involvement\n- Network ties will figure prominently in recruitment stories\n- Resource constraints will appear as barriers to participation\n```\n\n## Example Sensitizing Questions\n\n```markdown\n### From Resource Mobilization Theory:\n- What resources do participants describe drawing on?\n- How did organizations facilitate (or fail to facilitate) their participation?\n- What role did pre-existing networks play in recruitment?\n\n### From Identity-Based Approaches:\n- How do participants connect their involvement to who they are?\n- What identity claims or transformations appear in their accounts?\n- How does participation relate to other identities they hold?\n\n### Tensions to Explore:\n- Resource mobilization emphasizes organizational capacity; identity\n  approaches emphasize meaning and selfhood. Do participants' accounts\n  foreground one over the other? Do they integrate both?\n```\n\n## When You're Done\n\nReturn a summary to the orchestrator that includes:\n1. Confirmation that all files were created\n2. Number and type of theoretical resources synthesized\n3. The 3-5 most relevant frameworks for this analysis\n4. Key sensitizing questions to bring to Phase 1\n5. Any gaps in theoretical coverage the user might want to address\n6. Questions for the user about theoretical priorities\n",
        "plugins/interview-analyst/skills/interview-analyst/phases/phase1-immersion.md": "# Phase 1: Immersion & Familiarization\n\nYou are executing Phase 1 of a pragmatic qualitative analysis. Your goal is to develop deep familiarity with the interview datato know these transcripts intimately before attempting systematic analysis.\n\n## The Logic of Immersion\n\nAs Gerson & Damaske emphasize, \"the immersion process is not just an extension of conducting interviews.\" Even if someone has already read these transcripts, the analytic task requires reading them differentlywith an eye toward patterns, connections, and puzzles that will only become visible when considering the full dataset.\n\nThis phase is about **acquaintance before analysis**. You are getting to know the data, not yet trying to explain it.\n\n## Inputs\n\n**For Track A (Theory-Informed)**:\n- Read `/analysis/phase0-reports/sensitizing-questions.md` and `/analysis/phase0-reports/theory-reference-sheet.md`\n- Keep theoretical frameworks in mind as lenses, not as conclusions\n\n**For Track B (Data-First)**:\n- Use the General Sensitizing Questions below\n- Stay open to what emerges\n\n**Both tracks**: Read all interview transcripts in `/interviews`\n\n## General Sensitizing Questions (Track B, or supplementary for Track A)\n\nWhen reading interviews, attend to:\n\n**Action & Process**\n- What do people DO? What actions, practices, routines?\n- What sequences or trajectories emerge? What are the turning points?\n- How do things unfold over time?\n\n**Meaning & Interpretation**\n- How do participants make sense of their experiences?\n- What matters to them? What do they value, fear, hope for?\n- What language and metaphors do they use?\n\n**Identity & Self**\n- How do people describe themselves?\n- How does the phenomenon relate to who they are?\n- What identities are claimed, rejected, or negotiated?\n\n**Relationships & Networks**\n- Who matters in their accounts? Who's present, who's absent?\n- How do relationships enable or constrain action?\n- What social contexts shape their experiences?\n\n**Resources & Constraints**\n- What do people draw on? (time, money, skills, knowledge, support)\n- What limits or blocks them?\n- What's possible vs. impossible in their world?\n\n**Emotion & Affect**\n- What feelings are expressed or implied?\n- What evokes strong reactions?\n- What's the emotional texture of their experience?\n\n**Contradictions & Tensions**\n- Where do accounts seem inconsistent?\n- What dilemmas or trade-offs do people navigate?\n- What don't they talk about?\n\n## Your Tasks\n\n### 1. Read Every Interview\n\nRead each transcript carefully and completely. Pay attention to:\n- What participants say at length vs. briefly\n- Tone, emphasis, hedging, certainty\n- Emotional moments\n- What seems to matter most to them\n- What is NOT saidsilences, deflections, avoided topics\n\n### 2. Create Interview Memos\n\nFor each interview, produce a memo that includes:\n\n```markdown\n## Interview Memo: [Participant ID]\n\n**Basic Information**:\n- [Any demographic or contextual details mentioned]\n\n**Overview**:\n- [2-3 sentence summary of this person's account]\n\n**Main Topics & Perspectives**:\n- [Topic 1]: [Their stance/experience]\n- [Topic 2]: [Their stance/experience]\n\n**Notable Quotes** (2-4 excerpts that capture something important):\n> \"Quote\" (context: discussing X)\n\n**What Stood Out**:\n- [Anything surprising, puzzling, or particularly striking]\n\n**Emotional Tenor**:\n- [How did they seem to feel about what they discussed?]\n\n**Initial Questions**:\n- [Questions this interview raises for you]\n\n**Potential Connections**:\n- [How might this interview relate to others?]\n```\n\n### 3. Track Emerging Observations\n\nAs you read across interviews, maintain running notes on:\n\n**Patterns beginning to emerge**:\n- What do multiple participants talk about similarly?\n- What common experiences, concerns, or framings appear?\n\n**Variation you're noticing**:\n- Where do participants differ?\n- What seems to explain those differences?\n\n**Surprises**:\n- What didn't you expect?\n- What challenges assumptions (yours or conventional)?\n\n**Puzzles**:\n- What doesn't make sense yet?\n- What would you need to understand better?\n\n**Potential groupings**:\n- Are there types or categories emerging?\n- Which participants seem similar to each other?\n\n### 4. Note Questions for Later Phases\n\nWhat questions should inform coding (Phase 2)?\nWhat will need explanation (Phase 3)?\nWhat might the user need to weigh in on?\n\n## Output Files to Create\n\nSave all outputs to `/analysis/phase1-reports/`:\n\n1. **interview-memos.md** - All individual interview memos\n2. **emerging-observations.md** - Patterns, variation, surprises, puzzles\n3. **initial-questions.md** - Questions for later phases\n4. **phase1-report.md** - Executive summary including:\n   - Overview of dataset (number of interviews, participant characteristics)\n   - Key observations from immersion\n   - Most striking patterns or variations\n   - Surprises and puzzles\n   - Preliminary thoughts on potential groupings\n   - Questions for the user\n   - Recommendations for Phase 2 focus\n\n## Guiding Principles\n\n1. **Read generously**: Try to understand each participant on their own terms before comparing or judging.\n\n2. **Stay descriptive**: Resist the urge to explain. \"I notice X\" is better than \"X means Y\" at this stage.\n\n3. **Embrace uncertainty**: If something is confusing, that's data. Flag it rather than resolving it prematurely.\n\n4. **Notice both patterns and exceptions**: The person who doesn't fit the pattern is as important as those who do.\n\n5. **Preserve context**: When excerpting quotes, include enough context to understand them later.\n\n6. **Trust the process**: You don't need to have it figured out. Immersion builds the foundation for everything that follows.\n\n## Reading Stance\n\nApproach each transcript asking:\n- What is this person trying to tell me?\n- What matters to them about this?\n- What would I need to know to understand their experience from the inside?\n- What surprises me? What confirms my expectations?\n- How does this compare to other interviews I've read?\n\n## Example Interview Memo\n\n```markdown\n## Interview Memo: Participant 07\n\n**Basic Information**:\n- Female, mid-40s, joined organization in 2018, currently in leadership role\n\n**Overview**:\nDescribes a gradual deepening of involvement that she frames as\n\"accidental\"she came for one event and kept getting pulled in. Strong\nemphasis on relationships with other members as what keeps her engaged.\n\n**Main Topics & Perspectives**:\n- Initial involvement: Came to support a friend, not personally committed to cause\n- Escalation: Took on small tasks, then larger roles \"before I knew it\"\n- Current role: Reluctant leaderfeels obligated but also energized\n- Costs: Significant time strain on family; some tension with spouse\n\n**Notable Quotes**:\n> \"I always say I backed into this. I never meant to be the person running\n> things. But someone had to, and I was there.\" (discussing how she became\n> co-chair)\n\n> \"The people keep me coming back. The cause matters, don't get me wrong,\n> but if it wasn't for [names several members], I don't know if I'd still\n> be doing this.\" (discussing motivation)\n\n**What Stood Out**:\n- Tension between framing involvement as \"accidental\" while also describing\n  clear choice points where she could have stepped back\n- Relationship-centered account contrasts with some other participants who\n  lead with ideology\n\n**Emotional Tenor**:\n- Warm when discussing relationships; somewhat stressed when discussing\n  time costs; proud but self-deprecating about leadership role\n\n**Initial Questions**:\n- Is the \"accidental\" framing a way of managing the identity implications\n  of deep commitment?\n- How common is this relationship-centered pathway?\n\n**Potential Connections**:\n- Similar to P03 and P11 in emphasizing relationships\n- Contrasts with P02 and P09 who lead with ideological commitment\n```\n\n## When You're Done\n\nReturn a summary to the orchestrator that includes:\n1. Confirmation that all files were created\n2. Number of interviews processed\n3. Key observations: 3-5 most notable patterns or findings\n4. Key surprises: 2-3 things that stood out as unexpected\n5. Key puzzles: 2-3 things that need further investigation\n6. Preliminary sense of variation/groupings in the data\n7. Recommended focus areas for Phase 2\n8. Questions for the user\n",
        "plugins/interview-analyst/skills/interview-analyst/phases/phase2-coding.md": "# Phase 2: Systematic Coding\n\nYou are executing Phase 2 of a pragmatic qualitative analysis. Your goal is to transform the rich, complex interview material into organized, analyzable categories through systematic coding.\n\n## The Logic of Coding\n\nCoding is not just labelingit's **conceptual work**. As Gerson & Damaske explain, \"Developing conceptual categories is an interactive and iterative process that involves moving back and forth between the interview transcripts and an evolving list of substantive and theoretical categories.\"\n\nThe goal is **analytic traction**: to organize the data in ways that let you see patterns, make comparisons, and develop explanations.\n\n## Inputs\n\nBefore starting, read:\n1. `/analysis/phase1-reports/` - All Phase 1 outputs, especially interview memos and emerging observations\n2. Original interview transcripts in `/interviews/`\n3. **Track A**: Also review `/analysis/phase0-reports/theory-reference-sheet.md` for theoretically-derived categories\n\n## Your Tasks\n\n### 1. Develop Preliminary Codes\n\nStart with a list of codes that reflect:\n- The research questions guiding this analysis\n- Sections of the interview guide (if available)\n- Key observations from Phase 1\n- **Track A**: Concepts from theoretical frameworks\n\nThese preliminary codes can be quite general. You'll refine them as you work.\n\n**Types of codes to consider**:\n\n**Descriptive codes** (what's happening):\n- Actors and roles\n- Actions and practices\n- Settings and contexts\n- Events and sequences\n- Temporal markers (before/after, turning points)\n\n**Process codes** (dynamics, using gerunds):\n- Becoming, changing, transitioning\n- Deciding, choosing, weighing\n- Managing, coping, navigating\n- Building, maintaining, losing\n\n**Meaning codes** (interpretation and significance):\n- How participants understand events\n- What things mean to them\n- Values and priorities\n- Justifications and accounts\n\n**Emotion codes**:\n- Feelings expressed or implied\n- Affective responses to events\n- Emotional dimensions of experiences\n\n**Relationship codes**:\n- Key relationships mentioned\n- Relationship dynamics\n- Social support and conflict\n\n**In-vivo codes** (participants' own language):\n- Distinctive phrases or terms\n- Metaphors and imagery\n- Recurring expressions\n\n### 2. Apply Codes to Transcripts\n\nWork through each transcript, applying codes to relevant segments. As you code:\n\n**Refine as you go**:\n- Split overly broad codes into subcategories\n- Merge codes that overlap significantly\n- Add new codes when you encounter something the current list doesn't capture\n- Drop codes that aren't proving useful\n\n**Track variation within codes**:\n- Note the different forms a coded phenomenon takes\n- Pay attention to who exhibits what variation\n- Look for dimensions within categories\n\n**Example of code refinement**:\n```\nInitial code: \"Parental influence\"\n\nRefined into:\n- Parental influence: explicit encouragement\n- Parental influence: modeling behavior\n- Parental influence: providing resources\n- Parental influence: discouragement/barriers\n- Parental influence: absence/non-involvement\n\nEach subcode captures a different way parental influence operates.\n```\n\n### 3. Build the Codebook\n\nCreate a systematic codebook documenting each code:\n\n```markdown\n### Code: [Code Name]\n\n**Definition**: [Clear statement of what this code captures]\n\n**Inclusion criteria**: [When to apply this code]\n\n**Exclusion criteria**: [When NOT to apply this code; distinctions from similar codes]\n\n**Example excerpts**:\n> \"Quote 1\" - Participant X\n> \"Quote 2\" - Participant Y\n\n**Subcodes** (if applicable):\n- [Subcode A]: [definition]\n- [Subcode B]: [definition]\n\n**Frequency**: Appears in X of Y interviews\n\n**Notes**: [Any observations about this codevariation, questions, connections]\n```\n\n### 4. Create Coded Excerpt Files\n\nFor each major code, compile the relevant excerpts:\n\n```markdown\n# Code: [Code Name]\n\n## Participant 01\n> \"Excerpt 1\" (context: discussing X)\n> \"Excerpt 2\" (context: discussing Y)\n\n## Participant 02\n> \"Excerpt\" (context: discussing Z)\n\n[etc.]\n```\n\nThis makes it easy to see all instances of a code together.\n\n### 5. Write a Coding Memo\n\nDocument your coding process and emerging insights:\n- How did codes evolve during the process?\n- What patterns are becoming visible?\n- What's the relationship between codes?\n- What surprised you during coding?\n- What remains unclear or puzzling?\n- What should Phase 3 focus on?\n\n### 6. Build Participant Profiles\n\nFor each participant, create a profile that captures everything needed for later writeup. This is where you preserve the rich, person-level detail that gets lost if you only organize by code.\n\n```markdown\n## Participant: [Name or Pseudonym]\n\n**Interview**: #[number]\n**Demographics**: [Race/ethnicity], [gender], [age or age range], [other relevant identifiers]\n**Role/Position**: [Role in organization, occupation, committee membership, etc.]\n**Key trajectory**: [1-sentence summary of their arc relevant to your study]\n\n### Extended Quotes (potential anchors)\nQuotes of 3+ sentences that show this person's voice, reasoning, or experience unfolding. These are candidates for deep vignettes in the writeup.\n\n> \"[Full quote, preserving the participant's voice and logic]\"\n> *Context*: [What prompted thisinterview question, topic being discussed]\n> *Codes*: [Which codes apply]\n\n> \"[Another extended quote if available]\"\n> *Context*: [...]\n\n### Punchy Quotes (potential echoes)\nShorter quotes (1-2 sentences or memorable phrases) that capture something vividly. These work as prevalence indicators or embedded voice in the writeup.\n\n> \"[Short quote]\"\n> \"[Another short quote]\"\n> \"[Memorable phrase]\"\n\n### In-Vivo Terms\nDistinctive language this participant usesmetaphors, recurring phrases, terms of art.\n- \"[term 1]\"\n- \"[term 2]\"\n\n### Emotional Tenor\n[How does this participant come across? Angry, resigned, reflective, defiant? What feelings are expressed or implied?]\n\n### What Makes This Participant Distinctive\n[Why might this person serve as an anchor case for a particular finding? What do they exemplify clearly? Are they typical or a useful outlier?]\n\n### Codes Heavily Present\n- [Code 1]: [brief note on how it manifests]\n- [Code 2]: [brief note]\n```\n\n**Why this matters for writeup**: The interview-writeup skill needs:\n- Full demographics for attribution (\"Maria, a 34-year-old Puerto Rican lesbian who co-founded the Latino Caucus...\")\n- Extended quotes for anchor vignettes (3-6 sentences showing voice and reasoning)\n- Short quotes for echo patterns (1-2 sentences showing prevalence)\n- Sense of who would exemplify which finding\n\nBuilding this nowwhile you're deep in each interviewis far easier than reconstructing it later.\n\n## Output Files to Create\n\nSave all outputs to `/analysis/phase2-reports/`:\n\n1. **codebook.md** - Full codebook with all codes documented\n2. **coded-excerpts/** folder with files for each major code:\n   - `code-[name].md`\n3. **participant-profiles/** folder with a file for each participant:\n   - `participant-[name].md` - Demographics, trajectory, extended quotes, punchy quotes, in-vivo terms, what makes them distinctive\n4. **coding-memo.md** - Reflections on the coding process\n5. **code-frequency-matrix.md** - Which codes appear in which interviews\n6. **phase2-report.md** - Executive summary including:\n   - Overview of coding structure (number of codes, organization)\n   - Most prevalent codes\n   - Most analytically interesting codes\n   - Key patterns visible through coding\n   - Remaining puzzles\n   - Questions for the user\n   - Recommendations for Phase 3\n\nAlso save the codebook to `/analysis/codes/codebook.md` for cross-phase reference.\n\n## Guiding Principles\n\n1. **Coding is thinking**: The point isn't to label everythingit's to develop analytic categories that help you see.\n\n2. **Iterate freely**: Return to earlier transcripts when you develop new codes. Recoding is normal.\n\n3. **Balance breadth and depth**: Code comprehensively, but invest most energy in codes that matter for your research questions.\n\n4. **Embrace complexity**: If something fits multiple codes, code it multiple times. Overlap is informative.\n\n5. **Stay close to the data**: Codes should emerge from what participants actually say, not just from theoretical categories.\n\n6. **Track your moves**: Document code changes so you (and others) can follow your reasoning.\n\n## Deductive vs. Inductive Coding\n\n**Deductive** (theory  data):\n- Start with theoretically-derived codes\n- Apply them to see if/how they fit\n- Useful when you have clear theoretical expectations\n\n**Inductive** (data  theory):\n- Let codes emerge from the material\n- Build categories from the ground up\n- Useful when exploring new terrain\n\n**In practice, use both**:\n- Start with some deductive codes from research questions or theory\n- Remain open to inductive codes that emerge\n- Let the two inform each other\n\n## Example Codebook Entry\n\n```markdown\n### Code: Turning Point - Recruitment\n\n**Definition**: A specific moment or event that participants identify as\nwhen/how they first became involved or committed.\n\n**Inclusion criteria**: Apply when participant describes a discrete event,\nencounter, or realization that marked the beginning of their involvement\nor a significant deepening of commitment.\n\n**Exclusion criteria**: Do NOT apply to gradual processes without a\nmarked moment. Use \"Gradual Involvement\" for those cases.\n\n**Example excerpts**:\n> \"I remember exactly when it clicked for me. I was at this rally, and\n> someone handed me a flyer, and I just thoughtI have to do something.\"\n> - Participant 03\n\n> \"My turning point was when my neighbor's kid got sick. That made it\n> real for me. Before that it was abstract.\" - Participant 11\n\n**Subcodes**:\n- Turning point - personal experience: triggered by direct personal encounter\n- Turning point - vicarious: triggered by witnessing others' experiences\n- Turning point - political event: triggered by external political events\n- Turning point - invitation: triggered by being asked by someone\n\n**Frequency**: Appears in 14 of 20 interviews\n\n**Notes**: Strong pattern of discrete turning points, but 6 participants\ndescribe more gradual paths. The turning point narratives may be\nretrospective constructionsworth exploring in Phase 3.\n```\n\n## When You're Done\n\nReturn a summary to the orchestrator that includes:\n1. Confirmation that all files were created\n2. Total number of codes developed (and how organized)\n3. Most frequent codes (top 5-7)\n4. Most analytically interesting codes (and why)\n5. Key patterns visible through coding\n6. Notable variation within important codes\n7. Puzzles or questions for Phase 3\n8. Questions for the user about coding priorities or interpretation\n",
        "plugins/interview-analyst/skills/interview-analyst/phases/phase3-interpretation.md": "# Phase 3: Interpretation & Explanation\n\nYou are executing Phase 3 of a pragmatic qualitative analysis. Your goal is to move from **description to explanation**to develop accounts of why patterns take the form they do.\n\n## The Logic of Interpretation\n\nPhases 1 and 2 established what's in the data. Phase 3 asks: **Why?**\n\nAs Gerson & Damaske emphasize, the goal is not to take participant accounts at face value, nor to dismiss them as \"false consciousness.\" Instead: \"understand how and why people develop their particular views and take the actions they do.\" This requires distinguishing between:\n\n- **Accounts**: What participants say and how they make sense of their experiences\n- **Explanations**: The factors, mechanisms, and processes that explain why things happen as they do\n\nParticipants are experts on their own experience but may not see the larger patterns, structural forces, or hidden dynamics that shape their lives. Your job is to identify these while respecting participants' perspectives.\n\n## Inputs\n\nBefore starting, read:\n1. `/analysis/phase1-reports/` - Interview memos and observations\n2. `/analysis/phase2-reports/` - Codebook, coded excerpts, coding memo\n3. Original transcripts in `/interviews/` as needed\n\n**Track A (Theory-Informed)**: Also have `/analysis/phase0-reports/theory-reference-sheet.md` available for connecting patterns to theoretical frameworks.\n\n**Track B (Data-First)**: This is the phase where you begin engaging theoretical literature. Based on emerging patterns, identify relevant frameworks that might help explain what you're seeing. (You may use web search or ask the user for relevant literature.)\n\n## Your Tasks\n\n### 1. Analyze Patterns Across Interviews\n\nWorking from coded data, identify and analyze key patterns:\n\n**For each significant pattern**:\n- What is the pattern? (Describe it precisely)\n- How prevalent is it? (How many/which participants?)\n- What variation exists within it?\n- What conditions seem associated with it?\n\n**Cross-cutting analysis**:\n- How do patterns relate to each other?\n- What combinations occur together?\n- What sequences or trajectories appear?\n\n### 2. Distinguish Accounts from Explanations\n\nParticipants offer **accounts**their interpretations of their experiences. These are data, not conclusions.\n\nFor key phenomena, document:\n- **What participants say**: How do they explain/justify/make sense of X?\n- **Patterns in accounts**: Do participants tell similar stories? Different ones?\n- **What accounts might obscure**: What might participants not see or acknowledge?\n- **Possible explanations**: What factors might explain the pattern beyond what participants identify?\n\n**Example**:\n```markdown\n### Phenomenon: High commitment despite significant personal costs\n\n**Participant accounts**:\nMost participants frame continued involvement despite costs as \"just who I am\"\nor \"I couldn't live with myself otherwise\"identity-based explanations.\n\n**What accounts might obscure**:\n- The social relationships that would be lost by leaving\n- Sunk costs and escalating commitment dynamics\n- Lack of perceived alternatives\n\n**Possible explanations beyond participant accounts**:\n- Identity commitment mechanisms (once publicly identified, hard to exit)\n- Network embeddedness (leaving means losing valued relationships)\n- Meaning-making (involvement provides purpose that alternatives don't)\n```\n\n### 3. Identify Trajectories, Transitions, and Turning Points\n\nMany interview studies examine processes over time. Analyze:\n\n**Trajectories**: What paths do participants follow? Are there distinct types?\n\n**Transitions**: What moves people from one state to another? What enables or triggers change?\n\n**Turning points**: What moments do participants (or you) identify as pivotal? What makes them pivotal?\n\n**Sequences**: Does order matter? Do certain things need to happen before others?\n\n### 4. Examine Variation\n\nVariation is analytically powerful. Ask:\n\n- **What varies?** Outcomes, pathways, interpretations, experiences?\n- **Among whom?** Are there subgroups with different patterns?\n- **Why?** What conditions, characteristics, or contexts explain differences?\n\n**Create comparison matrices**:\n```markdown\n| Participant | Outcome | Key Factor A | Key Factor B | Notes |\n|-------------|---------|--------------|--------------|-------|\n| P01         | High    | Present      | Absent       | ...   |\n| P02         | Low     | Absent       | Present      | ...   |\n```\n\nLook for combinations that predict outcomes.\n\n### 5. Develop Explanatory Propositions\n\nBased on your analysis, formulate tentative explanations:\n\n```markdown\n### Proposition: [Clear statement of the claim]\n\n**What it explains**: [What pattern or puzzle does this address?]\n\n**The mechanism**: [How/why does this work?]\n\n**Supporting evidence**:\n- [Evidence from coded data]\n- [Quotes that illustrate]\n\n**Complicating evidence**:\n- [Cases that don't fit or complicate the picture]\n\n**Conditions/scope**: [When does this hold? When might it not?]\n```\n\nAim for propositions that are:\n- **Grounded**: Clearly connected to evidence\n- **Specific**: Not so general as to be unfalsifiable\n- **Mechanistic**: Explaining how, not just that\n\n### 6. Connect to Theoretical Literature\n\n**Track A**: Return to Phase 0 frameworks. How do your findings:\n- Confirm theoretical expectations?\n- Extend or modify existing frameworks?\n- Challenge or contradict theory?\n- Fill gaps between frameworks?\n\n**Track B**: Based on emerging patterns, identify relevant theoretical conversations:\n- What literatures address similar phenomena?\n- What concepts from existing work help name what you're seeing?\n- Where do your findings fit in ongoing debates?\n\n(For Track B, you may search for relevant literature or ask the user for resources.)\n\n### 7. Identify What Remains Unexplained\n\nBe honest about gaps:\n- What patterns can't you explain yet?\n- What negative cases challenge your propositions?\n- What would you need to know to resolve puzzles?\n\n## Output Files to Create\n\nSave all outputs to `/analysis/phase3-reports/`:\n\n1. **pattern-analysis.md** - Systematic analysis of key patterns\n2. **accounts-vs-explanations.md** - Distinguishing participant accounts from analytic explanations\n3. **trajectories-and-turning-points.md** - Analysis of temporal dynamics\n4. **variation-analysis.md** - Examination of differences and what explains them\n5. **explanatory-propositions.md** - Tentative explanatory claims\n6. **theoretical-connections.md** - Links to relevant literature\n7. **remaining-puzzles.md** - What remains unexplained\n8. **phase3-report.md** - Executive summary including:\n   - Key patterns identified\n   - Central explanatory propositions\n   - Most important theoretical connections\n   - Negative cases and complications\n   - Remaining puzzles\n   - Questions for the user\n   - Recommendations for Phase 4\n\nAlso save key memos to `/analysis/memos/` for reference.\n\n## Guiding Principles\n\n1. **Explanation requires distance**: You must step back from participant accounts to see patterns they can't see.\n\n2. **But respect participant knowledge**: They know their experience. You're adding a different kind of knowledge, not replacing theirs.\n\n3. **Mechanisms matter**: \"X is associated with Y\" is weaker than \"X produces Y through Z mechanism.\"\n\n4. **Negative cases are valuable**: Cases that don't fit force you to refine explanations.\n\n5. **Stay grounded**: Every claim should connect to specific evidence.\n\n6. **Maintain humility**: These are tentative explanations, not final truths.\n\n## Example Explanatory Proposition\n\n```markdown\n### Proposition: Relational embedding predicts sustained involvement\nmore than ideological commitment\n\n**What it explains**: Why some highly committed participants eventually\nleave while others with weaker stated commitments persist.\n\n**The mechanism**: Involvement creates valued relationships. Over time,\nthe relational stakes of participation (friendships, community belonging,\nmutual support) come to outweigh the original ideological motivation.\nLeaving means losing these relationships, which creates independent\nmotivation to stay regardless of belief intensity.\n\n**Supporting evidence**:\n- 11 of 14 long-term participants emphasized relationships when\n  discussing what keeps them involved (vs. 3 of 8 short-term)\n- P07: \"The people keep me coming back. The cause matters, but if it\n  wasn't for [names members], I don't know if I'd still be doing this.\"\n- P15: \"I've tried to leave twice. Both times I came back because I\n  missed the people.\"\n- Coding shows \"relationship-centered accounts\" strongly associated\n  with duration of involvement\n\n**Complicating evidence**:\n- P02 and P09 show high commitment with low relational emphasis\n  (both are relatively isolated members who work independently)\n- Some relationship-centered participants DID leave (P04, P18)\n  suggests relationships are important but not sufficient\n\n**Conditions/scope**:\n- Strongest where organization provides opportunities for relationship\n  formation (regular meetings, social events, collaborative work)\n- May not hold for online/distributed participation\n- Likely interacts with alternativesrelationship retention matters\n  more when participants lack other sources of community\n```\n\n## When You're Done\n\nReturn a summary to the orchestrator that includes:\n1. Confirmation that all files were created\n2. Number of patterns analyzed\n3. Key explanatory propositions (3-5 main claims)\n4. Most important theoretical connections\n5. Most challenging negative cases\n6. Remaining puzzles\n7. Questions for the user about interpretation or direction\n",
        "plugins/interview-analyst/skills/interview-analyst/phases/phase4-quality.md": "# Phase 4: Quality Checkpoint\n\nYou are executing Phase 4 of a pragmatic qualitative analysis. Your goal is to **evaluate the analysis so far** against established quality indicators and identify what needs strengthening before synthesis.\n\n## The Logic of Quality Assessment\n\nQuality in qualitative research isn't about replicating quantitative standardsit requires its own criteria. Drawing on Small & Calarco's *Qualitative Literacy*, this phase assesses the analysis against five indicators that mark high-quality interview research.\n\nThis is a **checkpoint, not a gate**. The goal is to identify strengths and weaknesses so they can be addressed, not to pass/fail the analysis.\n\n## Inputs\n\nBefore starting, read ALL prior phase outputs:\n1. `/analysis/phase1-reports/` - Immersion and familiarization\n2. `/analysis/phase2-reports/` - Coding\n3. `/analysis/phase3-reports/` - Interpretation and explanation\n4. Original transcripts in `/interviews/` as needed\n\n## The Five Quality Indicators\n\n### 1. Cognitive Empathy\n\n**The standard**: Has the analysis come to understand participants as they understand themselves?\n\nCognitive empathy has three dimensions:\n- **Perception**: How do participants perceive their situation, their world?\n- **Meaning**: What do things mean to them? How do they interpret events?\n- **Motivation**: What drives their actions? What do they want, fear, hope?\n\n**Assessment questions**:\n- Do the interview memos and analysis capture how participants see things?\n- Is there evidence of understanding participants' perspectives from the inside?\n- Can a reader grasp what the experience is like for participants?\n- Have we avoided reducing participants' views to \"false consciousness\" or dismissing them?\n- Have we also avoided simply taking accounts at face value without critical analysis?\n\n**Red flags**:\n- Analysis that speaks primarily in the analyst's voice without participant perspective\n- Claims about what participants think/feel without supporting evidence\n- Dismissive treatment of participant accounts\n- Failure to consider how participants understand their own situations\n\n### 2. Heterogeneity\n\n**The standard**: Has the analysis captured variationwithin individuals, across the sample, and across contexts?\n\n**Dimensions to assess**:\n- **Within individuals**: Do we see participants as complex, potentially contradictory, changing over time?\n- **Across the sample**: Have we represented the diversity of experiences, perspectives, and trajectories?\n- **Across contexts**: Where relevant, have we captured variation across settings or circumstances?\n\n**Assessment questions**:\n- Does the analysis acknowledge differences across participants, or does it homogenize?\n- Are negative cases and outliers addressed, not just ignored?\n- Do we see individuals as complex people, not one-dimensional types?\n- Is variation treated as analytically interesting, not just noise?\n\n**Red flags**:\n- Treating the sample as uniform (\"Participants said...\")\n- Ignoring or minimizing cases that don't fit the main pattern\n- Presenting participants as simple embodiments of a type\n- Failing to examine what explains differences\n\n### 3. Palpability\n\n**The standard**: Is the evidence concrete and specific, not abstract and general?\n\nPalpable evidence lets readers see what actually happenedspecific events, actual words, real situations. Abstract evidence tells readers what to conclude without showing them the basis.\n\n**Assessment questions**:\n- Do quotes capture specific moments, events, situationsnot just general attitudes?\n- Is there enough context to understand what quotes mean?\n- Can readers \"see\" what we're describing?\n- Do claims rest on concrete evidence or general impressions?\n\n**Red flags**:\n- Over-reliance on general summary statements\n- Quotes that express attitudes rather than describe experiences\n- Claims made without specific evidentiary support\n- Evidence that feels thin or cherry-picked\n\n### 4. Follow-Up\n\n**The standard**: Has the analysis pursued emergent leads and probed beneath surface responses?\n\nGood interviewing and analysis involves following upnot accepting initial responses as complete, but probing for specifics, examples, elaboration.\n\n**Assessment questions**:\n- Is there evidence of probing beneath surface accounts?\n- Have emergent themes been pursued across interviews?\n- Are there gaps in the evidence that could have been filled with follow-up?\n- Does the analysis go beyond what any single interview reveals?\n\n**Red flags**:\n- Taking initial responses at face value without probing\n- Ignoring interesting leads that emerge during analysis\n- Gaps in evidence that sharper interviewing might have filled\n- Relying too heavily on general statements rather than specific examples\n\n### 5. Self-Awareness\n\n**The standard**: Has there been reflexivity about the analyst's position, assumptions, and potential biases?\n\n**Domains to assess**:\n- **Positionality**: How might the analyst's social position affect what they see/miss?\n- **Assumptions**: What assumptions have guided the analysis? Have they been examined?\n- **Limitations**: What are the boundaries of what this analysis can claim?\n\n**Assessment questions**:\n- Is there acknowledgment of the analyst's perspective?\n- Have assumptions been made explicit and examined?\n- Are limitations honestly addressed?\n- Is there awareness of what this data/analysis cannot show?\n\n**Red flags**:\n- Invisible analyst (claims presented as objective truth)\n- Unexamined assumptions driving interpretation\n- Overclaiming (asserting more than evidence supports)\n- Failure to acknowledge limitations\n\n## Your Tasks\n\n### 1. Assess Each Indicator\n\nFor each of the five indicators, evaluate the analysis:\n\n```markdown\n## [Indicator Name]\n\n**Strengths**:\n- [What the analysis does well on this dimension]\n\n**Weaknesses**:\n- [Where the analysis falls short]\n\n**Specific examples**:\n- [Evidence from the analysis illustrating strengths or weaknesses]\n\n**Recommendations**:\n- [What could be done to strengthen this dimension]\n\n**Rating**: Strong / Adequate / Needs Improvement\n```\n\n### 2. Identify Critical Gaps\n\nBased on your assessment:\n- What are the most significant quality concerns?\n- Which gaps would most undermine the analysis if unaddressed?\n- What must be done before proceeding to synthesis?\n\n### 3. Recommend Remediation\n\nFor each significant weakness, recommend specific actions:\n- Return to transcripts for additional coding?\n- Reread with attention to specific dimensions?\n- Revise interpretations?\n- Acknowledge limitations?\n- Strengthen evidentiary basis?\n\n### 4. Note Strengths\n\nAlso document what the analysis does wellthis helps maintain what's working and provides language for methods sections.\n\n## Output Files to Create\n\nSave all outputs to `/analysis/phase4-reports/`:\n\n1. **cognitive-empathy-assessment.md** - Evaluation of indicator 1\n2. **heterogeneity-assessment.md** - Evaluation of indicator 2\n3. **palpability-assessment.md** - Evaluation of indicator 3\n4. **follow-up-assessment.md** - Evaluation of indicator 4\n5. **self-awareness-assessment.md** - Evaluation of indicator 5\n6. **critical-gaps.md** - Most significant concerns and required remediation\n7. **strengths-summary.md** - What the analysis does well\n8. **phase4-report.md** - Executive summary including:\n   - Overview assessment (overall quality rating)\n   - Summary of each indicator\n   - Critical gaps requiring attention\n   - Recommended remediation actions\n   - Strengths to preserve\n   - Questions for the user\n   - Recommendation: proceed to Phase 5 or return to earlier phase?\n\n## Assessment Scale\n\nFor each indicator:\n\n**Strong**: Clearly meets the standard. Evidence of careful attention to this dimension throughout.\n\n**Adequate**: Generally meets the standard with minor weaknesses. Some room for improvement but not critically flawed.\n\n**Needs Improvement**: Significant gaps. Should be addressed before finalizing analysis.\n\n## Guiding Principles\n\n1. **Be honest, not harsh**: The goal is improvement, not judgment. Identify issues clearly but constructively.\n\n2. **Be specific**: \"Needs more palpability\" is less useful than \"Claims about recruitment patterns lack specific quotes showing how invitation conversations actually happened.\"\n\n3. **Distinguish critical from minor**: Some gaps undermine the analysis; others are minor imperfections. Focus energy on what matters most.\n\n4. **Consider the research questions**: Quality assessment should be calibrated to what the study aims to accomplish.\n\n5. **Improvement is always possible**: Even strong work can be strengthened. Even weak work can be salvaged.\n\n## Example Assessment\n\n```markdown\n## Palpability Assessment\n\n**Strengths**:\n- Turning point narratives are richly detailed with specific quotes\n- P07 and P11's accounts of recruitment are particularly vivid\n- The coding memo provides concrete examples for most codes\n\n**Weaknesses**:\n- Claims about \"relationship maintenance\" lack specific examples of\n  how participants actually maintain relationships (general statements\n  about \"staying in touch\" but no descriptions of specific interactions)\n- Evidence for \"cost-benefit calculations\" is thinmostly analyst\n  inference rather than participant accounts\n- Section on organizational dynamics relies on summary rather than\n  showing actual meeting interactions\n\n**Specific examples**:\n- Strong: \"The quote from P03 about 'the moment everything changed'\n  lets readers see exactly what happened at that rally.\"\n- Weak: \"The claim that 'participants weighed costs and benefits'\n  (Phase 3 report, p.7) has no supporting quotes showing this process.\"\n\n**Recommendations**:\n- Return to transcripts to find specific examples of relationship\n  maintenance practices\n- Either find better evidence for cost-benefit framing or revise\n  interpretation\n- Add concrete quotes to organizational dynamics section\n\n**Rating**: Adequate (strong in some areas, but gaps in evidence\nfor key claims need addressing)\n```\n\n## When You're Done\n\nReturn a summary to the orchestrator that includes:\n1. Confirmation that all files were created\n2. Overall quality assessment\n3. Rating for each of the five indicators\n4. Most critical gaps (1-3 that must be addressed)\n5. Key strengths to preserve\n6. Recommended next steps:\n   - Proceed to Phase 5?\n   - Return to earlier phase for additional work?\n   - Specific remediation tasks?\n7. Questions for the user about priorities\n",
        "plugins/interview-analyst/skills/interview-analyst/phases/phase5-synthesis.md": "# Phase 5: Synthesis & Writing\n\nYou are executing Phase 5 of a pragmatic qualitative analysis. Your goal is to **integrate everything into a coherent, well-evidenced argument** ready for presentation or publication.\n\n## The Logic of Synthesis\n\nSynthesis is where analysis becomes communication. You're transforming a rich, complex process into a compelling narrative that:\n- Makes a clear argument\n- Supports claims with evidence\n- Anticipates objections\n- Articulates contribution and limitations\n\nAs Lareau emphasizes: **show, don't tell**. Readers should see the evidence, not just trust your conclusions.\n\n## Inputs\n\nBefore starting, read ALL prior outputs:\n1. `/analysis/phase1-reports/` - Interview memos, initial observations\n2. `/analysis/phase2-reports/` - Codebook, coded excerpts\n3. `/analysis/phase3-reports/` - Patterns, explanations, theoretical connections\n4. `/analysis/phase4-reports/` - Quality assessment, strengths, gaps addressed\n5. Original transcripts in `/interviews/` for selecting quotes\n\n## Your Tasks\n\n### 1. Clarify the Core Argument\n\nBefore writing, articulate:\n\n**The research question**: What did this study set out to explore?\n\n**The central argument**: In 2-3 sentences, what is the main claim?\n\n**The contribution**: What does this add to existing knowledge?\n- Does it describe something previously undocumented?\n- Does it explain patterns in a new way?\n- Does it challenge existing assumptions?\n- Does it extend or modify existing theory?\n\n### 2. Structure the Argument\n\nDevelop an outline that moves readers through your analysis logically.\n\n**Common structures for interview studies**:\n\n**Structure A: Phenomenon  Patterns  Explanation**\n1. Introduction to the phenomenon\n2. Data and methods\n3. Key patterns in the data\n4. Explaining the patterns\n5. Discussion and implications\n\n**Structure B: Question  Cases  Comparative Analysis**\n1. Research question and theoretical context\n2. Data and methods\n3. Case Type A (with analysis)\n4. Case Type B (with analysis)\n5. Comparison and theoretical implications\n\n**Structure C: Process/Trajectory**\n1. Introduction\n2. Data and methods\n3. Stage 1 of the process\n4. Stage 2 of the process\n5. Variation and contingencies\n6. Theoretical implications\n\nChoose a structure that fits your findings. Modify as needed.\n\n### 3. Select Evidence\n\n**Selecting quotes**:\n\nQuotes should be **luminous exemplars**not statistically typical, but analytically illuminating. A good quote:\n- Illustrates a claim vividly\n- Shows rather than tells\n- Gives readers direct access to participant voice\n- Does theoretical work (not just decoration)\n\n**For each major claim, select**:\n- 1-2 strong quotes that illustrate the point clearly\n- Supporting quotes that show variation or additional dimensions\n- Counter-examples or complicating cases (address, don't hide, complexity)\n\n**Quote selection criteria**:\n- Does this quote show what I'm claiming, or do I have to explain it heavily?\n- Is there enough context for readers to understand?\n- Does it feel representative, or is it cherry-picked?\n- Does it let readers hear the participant's voice?\n\n### 4. Address Alternative Explanations\n\nStrong analysis anticipates objections:\n\n- What other explanations could account for these patterns?\n- Why is your interpretation more compelling than alternatives?\n- What would someone skeptical of your claims say?\n- How do you respond to potential criticisms?\n\nDocument this explicitlyit strengthens rather than weakens your argument.\n\n### 5. Articulate Limitations\n\nBe honest and specific:\n\n**Sample limitations**:\n- Who is included/excluded?\n- How might the sample shape findings?\n- What generalization is/isn't warranted?\n\n**Method limitations**:\n- What can interview data show vs. not show?\n- What alternative methods might have revealed?\n- What remains unobserved?\n\n**Analytic limitations**:\n- What couldn't you explain?\n- What questions remain open?\n- What would future research need to address?\n\n### 6. Write Draft Sections\n\nProduce drafts of key sections:\n\n**Introduction** (1-2 pages):\n- Hook: Why should readers care?\n- Research question or puzzle\n- Preview of argument\n- Overview of data and approach\n- Roadmap of paper\n\n**Methods section** (1-2 pages):\n- Data: What interviews, how many, with whom, when\n- Sampling strategy and rationale\n- Interview approach\n- Analysis process\n- Limitations acknowledgment\n- Ethical considerations\n\n**Methods Section: Detailed Templates**\n\nThe methods section must address five editorial expectations:\n\n**1. Case Selection Logic:**\n```markdown\nWe conducted interviews with [N] [PARTICIPANT TYPE]. Our target population\nwas [DEFINITION]. We used [SAMPLING STRATEGY: purposive, snowball, theoretical]\nsampling to recruit participants who [SELECTION CRITERIA].\n\n[If exclusions]: We excluded individuals who [CRITERIA] because [RATIONALE].\nThis may limit generalizability to [GROUPS/CONTEXTS].\n```\n\n**2. Recruitment Transparency:**\n```markdown\nRecruitment occurred through [CHANNELS: organizations, social media, referrals,\npublic records, etc.] between [DATES]. [If organizational access]: We gained\naccess through [GATEKEEPER/RELATIONSHIP], who [ROLE IN RECRUITMENT].\n\n[If response rate calculable]: Of [N] individuals contacted, [X] agreed to\nparticipate ([Y]% response rate). [If not calculable]: Due to [snowball/referral\nsampling], we cannot calculate a traditional response rate.\n\n[If refusals]: Those who declined cited [REASONS]. We [addressed/could not address]\npotential selection by [METHOD].\n```\n\n**3. Sample Size Justification:**\n\nChoose the approach that fits your study:\n\n*Comparative Structure Approach:*\n```markdown\nSample size was determined by our comparative design. We interviewed [N] participants\nin [GROUP A] and [N] in [GROUP B] to enable systematic comparison across [DIMENSION].\nThis structure allows us to examine [RESEARCH QUESTION] while acknowledging that\nfindings may not capture [ADDITIONAL VARIATION].\n```\n\n*Population Coverage Approach:*\n```markdown\nOur sample includes [N] of the approximately [TOTAL] [POPULATION]. This [X]%\ncoverage allows us to [CLAIM TYPE] while acknowledging that [UNCOVERED PORTIONS]\nmay differ on [DIMENSIONS].\n```\n\n*Saturation Evidence Approach:*\n```markdown\nWe continued interviewing until reaching thematic saturation, operationalized as\n[DEFINITION: no new codes / no new themes / diminishing returns]. Saturation on\ncore themes emerged after approximately [N] interviews; we conducted [ADDITIONAL]\ninterviews to confirm the pattern and explore negative cases.\n```\n\n*Pragmatic Constraints Approach:*\n```markdown\nOur sample size was constrained by [FACTOR: access, population size, resources].\nWe mitigated this limitation by [METHOD: extended interviews, multiple interviews\nper participant, intensive follow-up, triangulation with other data sources].\n```\n\n**4. Interview Protocol:**\n```markdown\nInterviews were [semi-structured / in-depth / life history], lasting [DURATION RANGE]\n(average: [MEAN]). Topics included [MAJOR DOMAINS]. Interviews were conducted\n[in person / by phone / by video] in [LANGUAGE], recorded with consent, and\ntranscribed [verbatim / with conventions].\n\n[If repeat interviews]: We conducted [N] interviews with [X] participants to\ncapture [CHANGE/DEPTH].\n\n[If supplementary data]: We also collected [DOCUMENTS, OBSERVATIONS, ETC.] to\n[PURPOSE].\n```\n\n**5. Positionality (when appropriate):**\n```markdown\n[RELEVANT IDENTITIES] may have shaped data collection and interpretation.\n[SPECIFIC INFLUENCE]: For example, [CONCRETE EXAMPLE of how positionality\naffected access, rapport, interpretation, or blind spots]. We addressed this\nby [MITIGATION STRATEGIES: member checks, peer debriefing, reflexive memos,\ndiverse research team].\n```\n\n**Findings sections** (bulk of paper):\n- Organized by your chosen structure\n- Each section makes a clear point\n- Claims supported by evidence (quotes + analysis)\n- Variation and negative cases addressed\n- Clear transitions between sections\n\n**Discussion** (1-2 pages):\n- Summary of key findings\n- Theoretical implications\n- Connections to existing literature\n- Scope conditions and limitations\n- Future research directions\n\n**Conclusion** (0.5-1 page):\n- Core contribution restated\n- Broader significance\n- Closing thought\n\n### 7. Build the Quote Database for Writeup\n\nThis is the key handoff to the interview-writeup skill. Reorganize the person-level material from Phase 2 participant profiles into a finding-level database ready for drafting.\n\n**For each major finding/mechanism:**\n\n```markdown\n# Finding [N]: [Mechanism Name]\n## [Sub-finding if applicable]\n\n### Luminous Exemplar\nThe single best quote for opening this sectionvivid, captures the mechanism perfectly, memorable.\n\n> \"[The quote]\"\n> **[Name]**, [demographics], [role/position]\n> *Context*: [What prompted this quote]\n> *Why luminous*: [What makes this quote exceptional for this finding]\n\n### Anchor Candidates\nParticipants who could serve as the deep vignette for this section. For each:\n- **[Name]**: [1-sentence reason they exemplify this finding]\n  - Extended quote available: Yes/No\n  - Trajectory shows mechanism clearly: Yes/No\n  - Social location adds analytical value: [How]\n\n### Echo Candidates\nShorter quotes showing this pattern across multiple participants:\n\n> \"[Short quote]\"\n> **[Name]**, [brief attribution]\n\n> \"[Short quote]\"\n> **[Name]**, [brief attribution]\n\n> \"[Short quote]\"\n> **[Name]**, [brief attribution]\n\n### Prevalence\n- Pattern appeared in [X] of [Y] participants ([Z]%)\n- Variation by [demographic/position]: [notes]\n\n### Negative Cases / Variation\n- **[Name]**: [How they diverge from the pattern]\n- Explanation for variation: [If known]\n```\n\n**Selection criteria for luminous exemplars:**\n- Does it capture the mechanism in the participant's own voice?\n- Is it vivid and memorable?\n- Does it \"show\" rather than require extensive explanation?\n- Would a reader remember this quote?\n\n**Anchor vs. Echo distinction:**\n- **Anchor**: Extended quote (3-6 sentences) from one person, developed into a vignette with context, interpretation, and follow-up. Go deep.\n- **Echo**: Short quote (1-2 sentences) showing the same pattern appears in other participants. Show breadth.\n\n**The writeup skill expects:**\n- 1 anchor + 2-3 echoes per subsection\n- Full demographics for attribution\n- Context for extended quotes\n- Prevalence indicators\n- Variation/negative cases flagged\n\n### 8. Create an Evidence Appendix\n\nCompile:\n- Key quotes organized by theme/claim\n- Context for each quote (participant, circumstances)\n- Additional quotes that didn't make the main text but support the analysis\n\nThis is useful for revision and for responding to reviewers.\n\n## Output Files to Create\n\nSave all outputs to `/analysis/phase5-reports/`:\n\n1. **core-argument.md** - Statement of research question, central argument, contribution\n2. **paper-outline.md** - Detailed structural outline\n3. **quote-database.md** - Quotes organized by finding/mechanism with:\n   - Luminous exemplars flagged\n   - Anchor candidates identified\n   - Echo quotes compiled\n   - Prevalence noted\n   - Attribution complete\n   *(This is the primary handoff to interview-writeup)*\n4. **evidence-selection.md** - Selected quotes organized by claim (legacy format)\n5. **alternative-explanations.md** - Addressed objections and alternatives\n6. **limitations.md** - Honest assessment of limitations\n7. **draft-introduction.md** - Draft introduction section\n8. **draft-methods.md** - Draft methods section\n9. **draft-findings.md** - Draft findings/analysis sections\n10. **draft-discussion.md** - Draft discussion section\n11. **evidence-appendix.md** - Extended evidence compilation\n12. **phase5-report.md** - Executive summary including:\n    - Core argument in brief\n    - Structure overview\n    - Key evidence highlights\n    - Main limitations\n    - Questions for the user\n    - Next steps for revision\n\n## Writing Principles\n\n### Show, Don't Tell\n```markdown\nWEAK (telling):\n\"Participants experienced significant emotional challenges during their involvement.\"\n\nSTRONG (showing):\n\"The emotional toll was palpable. As Maria described, 'There were nights\nI couldn't sleep, just replaying everything. The anger, the fearit\ndoesn't just turn off when you go home.' This affective residue appeared\nacross interviews...\"\n```\n\n### Ground Claims in Evidence\n```markdown\nWEAK:\n\"Relationships were important for sustained involvement.\"\n\nSTRONG:\n\"Fourteen of twenty long-term participants explicitly cited relationships\nas central to their continued involvement. As P07 put it, 'The people keep\nme coming back.' This relational embedding appeared to operate through...\"\n```\n\n### Acknowledge Complexity\n```markdown\nWEAK:\n\"Participants joined because of ideological commitment.\"\n\nSTRONG:\n\"While most participants cited ideological reasons when asked why they joined,\ncloser examination revealed more complexity. Only four described ideology as\nsufficient motivation; the majority paired ideological commitment with\nrelational or circumstantial factors...\"\n```\n\n### Make Transitions Work\n```markdown\nWEAK:\n\"Another theme was organizational structure.\" [abrupt shift]\n\nSTRONG:\n\"If relational ties drew participants in, organizational structure shaped\nwhat happened next. How the organization channeled new members' energy\nproved consequential for their trajectories...\" [connected transition]\n```\n\n## Guiding Principles\n\n1. **Argument-driven**: Every section should advance your argument. Cut interesting material that doesn't serve the whole.\n\n2. **Evidence-rich**: Claims need support. When in doubt, add another quote.\n\n3. **Honest about limits**: Acknowledging what you can't claim strengthens what you can.\n\n4. **Reader-oriented**: Write for someone unfamiliar with your data. What do they need to understand?\n\n5. **Revise ruthlessly**: First drafts are for getting ideas down. Revision is where writing becomes good.\n\n## Example Evidence Selection\n\n```markdown\n## Claim: Turning points were typically relational, not ideological\n\n### Primary quote:\n> \"I wasn't political at all before this. Zero. What happened was my\n> neighborshe'd been going to meetingsshe just asked me to come once.\n> I went because she asked, not because I cared about the issue. But\n> once I was there, meeting these people, seeing what they were doing...\n> that's when it became real for me.\"  P12\n\n**Why this quote**: Directly illustrates the relational nature of the turning\npoint. Also shows the sequence: relationship first, then ideological engagement.\n\n### Supporting quotes:\n> \"My sister dragged me to that first meeting. Literally dragged me.\"  P03\n> \"It was [my friend's] passion that got me. I wanted to understand what\n> made her care so much.\"  P08\n\n### Counter-example to address:\n> \"I'd been reading about this for years. When I found out there was a local\n> chapter, I showed up on my own. Didn't know anyone.\"  P02\n\n**How to handle**: Acknowledge this as genuine ideological pathway, but note\nits rarity (2 of 20 participants) and that even P02 describes subsequent\nrelational embedding as crucial for continued involvement.\n```\n\n## When You're Done\n\nReturn a summary to the orchestrator that includes:\n1. Confirmation that all files were created\n2. Core argument statement (2-3 sentences)\n3. Key contribution claim\n4. Paper structure overview\n5. Strongest evidence highlights\n6. Main limitations\n7. Questions for the user about direction, emphasis, or audience\n8. Recommendations for next steps in revision\n",
        "plugins/interview-bookends/.claude-plugin/plugin.json": "{\r\n  \"name\": \"interview-bookends\",\r\n  \"version\": \"1.0.0\",\r\n  \"description\": \"Write article introductions and conclusions for sociology interview research. Takes theory and findings sections as input and produces publication-ready framing prose based on genre analysis of 80 articles.\",\r\n  \"skills\": \"./skills/\"\n}\n",
        "plugins/interview-bookends/skills/interview-bookends/SKILL.md": "---\r\nname: interview-bookends\r\ndescription: Write article introductions and conclusions for sociology interview research. Takes theory and findings sections as input and produces publication-ready framing prose.\r\n---\r\n\r\n# Interview Bookends\r\n\r\nYou help sociologists write **introductions and conclusions** for interview-based research articles. Given the Theory section and Findings section, you guide users through drafting the framing prose that opens and closes the article.\r\n\r\n## When to Use This Skill\r\n\r\nUse this skill when users have:\r\n- A drafted **Theory/Literature Review section**\r\n- A drafted **Findings section**\r\n- Need help writing the **Introduction** and/or **Conclusion**\r\n\r\nThis skill assumes the intellectual work is donethe contribution is clear, the findings are established. The task is crafting the framing prose that positions the contribution and delivers on promises.\r\n\r\n## Connection to Other Skills\r\n\r\n| Skill | Purpose | Key Output |\r\n|-------|---------|------------|\r\n| **interview-analyst** | Analyzes interview data | Codes, patterns, quote database |\r\n| **interview-writeup** | Drafts methods and findings | Methods & Findings sections |\r\n| **interview-bookends** | Drafts introduction and conclusion | Complete framing prose |\r\n\r\nThis skill completes the article writing workflow.\r\n\r\n## Core Principles (from Genre Analysis)\r\n\r\nBased on systematic analysis of 80 sociology interview articles from *Social Problems* and *Social Forces*:\r\n\r\n### 1. Introductions Are Efficient; Conclusions Do Heavy Work\r\n- **Median introduction**: 761 words, 6 paragraphs\r\n- **Median conclusion**: 1,173 words, 8 paragraphs\r\n- **Ratio**: Conclusions are 67% longer than introductions\r\n- Introductions *subtract* (narrow to the gap); conclusions *expand* (project to significance)\r\n\r\n### 2. Phenomenon-Led Openings Dominate (74%)\r\n- Most introductions open with empirical phenomena, not questions\r\n- Question-led openings are rare (1%)they feel performative\r\n- Theory-led openings cluster in theory-extension articles (30%)\r\n- Show the puzzle; don't just assert it exists\r\n\r\n### 3. Parallel Coherence Is Normative (66%)\r\n- Introductions make promises; conclusions must keep them\r\n- Escalation (20%) is acceptableexceeding promises reads as discovery\r\n- Deflation (6%) is penalizedoverpromising damages credibility\r\n- **Callbacks to introduction are universal (100%)**\r\n\r\n### 4. Match Framing to Contribution Type\r\nFive cluster styles require different approaches:\r\n\r\n| Cluster | Intro Signature | Conclusion Signature |\r\n|---------|-----------------|---------------------|\r\n| **Gap-Filler** | Short, phenomenon-led, data early | Long (2x), summary + implications |\r\n| **Theory-Extension** | Theory-led (30%), framework early | Framework affirmation |\r\n| **Concept-Building** | Long, motivate conceptual need | Balanced length, concept consolidation |\r\n| **Synthesis** | Multiple traditions named | Integration claims, no deflation |\r\n| **Problem-Driven** | Stakes-led (25%), policy focus | Escalation to implications |\r\n\r\n## Workflow Phases\r\n\r\n### Phase 0: Intake & Assessment\r\n**Goal**: Review inputs, identify cluster, confirm scope.\r\n\r\n- Read the Theory section to understand positioning and contribution type\r\n- Read the Findings section to understand what was discovered\r\n- Identify which cluster the article inhabits\r\n- Confirm whether user needs introduction, conclusion, or both\r\n\r\n**Guide**: `phases/phase0-intake.md`\r\n\r\n> **Pause**: Confirm cluster identification and scope before drafting.\r\n\r\n---\r\n\r\n### Phase 1: Introduction Drafting\r\n**Goal**: Write an introduction that opens the circuit effectively.\r\n\r\n- Choose opening move type (phenomenon, stakes, case, theory, question)\r\n- Establish stakes and context\r\n- Identify the gap/puzzle\r\n- Preview data and argument\r\n- Include roadmap (optional but recommended for complex articles)\r\n\r\n**Guides**:\r\n- `phases/phase1-introduction.md` (main workflow)\r\n- `techniques/opening-moves.md` (opening strategies)\r\n- `clusters/` (cluster-specific guidance)\r\n\r\n> **Pause**: Review introduction draft for coherence with theory section.\r\n\r\n---\r\n\r\n### Phase 2: Conclusion Drafting\r\n**Goal**: Write a conclusion that closes the circuit and projects significance.\r\n\r\n- Open with restatement or summary (not the same words as intro)\r\n- Recap key findings efficiently\r\n- State contribution claims\r\n- Integrate with prior literature\r\n- Acknowledge limitations\r\n- Project implications and future directions\r\n- Craft callback to introduction\r\n- End with resonant closing\r\n\r\n**Guides**:\r\n- `phases/phase2-conclusion.md` (main workflow)\r\n- `techniques/conclusion-moves.md` (structural elements)\r\n- `techniques/callbacks.md` (closing the circuit)\r\n\r\n> **Pause**: Review conclusion for coherence with introduction.\r\n\r\n---\r\n\r\n### Phase 3: Coherence Check\r\n**Goal**: Ensure introduction and conclusion work together.\r\n\r\n- Verify vocabulary echoes (key terms appear in both)\r\n- Check promise-delivery alignment\r\n- Assess coherence type (Parallel, Escalators, Bookends)\r\n- Confirm callback is present and effective\r\n- Calibrate ambition across sections\r\n\r\n**Guide**: `phases/phase3-coherence.md`\r\n\r\n---\r\n\r\n## Cluster Profiles\r\n\r\nReference these guides for cluster-specific writing:\r\n\r\n| Guide | Cluster |\r\n|-------|---------|\r\n| `clusters/gap-filler.md` | Gap-Filler Minimalist (38.8%) |\r\n| `clusters/theory-extension.md` | Theory-Extension Framework Applier (22.5%) |\r\n| `clusters/concept-building.md` | Concept-Building Architect (15.0%) |\r\n| `clusters/synthesis.md` | Synthesis Integrator (17.5%) |\r\n| `clusters/problem-driven.md` | Problem-Driven Pragmatist (15.0%) |\r\n\r\n## Technique Guides\r\n\r\n| Guide | Purpose |\r\n|-------|---------|\r\n| `techniques/opening-moves.md` | Five opening move types with examples |\r\n| `techniques/conclusion-moves.md` | Structural elements of conclusions |\r\n| `techniques/callbacks.md` | Closing the circuit effectively |\r\n| `techniques/coherence-types.md` | Parallel, Escalators, Bookends, Deflators |\r\n| `techniques/signature-phrases.md` | Common phrases for intros and conclusions |\r\n\r\n## Key Statistics (Benchmarks)\r\n\r\n### Introduction Benchmarks\r\n\r\n| Feature | Typical Value |\r\n|---------|---------------|\r\n| Word count | 600-950 words |\r\n| Paragraphs | 4-8 |\r\n| Opening move | Phenomenon-led (74%) |\r\n| Data mention | Middle of section |\r\n| Roadmap | Present in 40% |\r\n\r\n### Conclusion Benchmarks\r\n\r\n| Feature | Typical Value |\r\n|---------|---------------|\r\n| Word count | 900-1,450 words |\r\n| Paragraphs | 6-10 |\r\n| Opening move | Restatement (71%) |\r\n| Limitations | Present in 69% |\r\n| Future directions | Present in 76% |\r\n| Callback | **Required (100%)** |\r\n\r\n### Coherence Benchmarks\r\n\r\n| Type | Frequency | Meaning |\r\n|------|-----------|---------|\r\n| Parallel | 66% | Deliver what you promised |\r\n| Escalators | 20% | Exceed your promises |\r\n| Bookends | 8% | Strong mirror structure |\r\n| Deflators | 6% | Fall short (avoid) |\r\n\r\n## Prohibited Moves\r\n\r\n### In Introductions\r\n- Opening with a direct question (unless theory-extension)\r\n- Claiming the literature \"has overlooked\" without justification\r\n- Promising more than the findings deliver\r\n- Lengthy method description (save for Methods section)\r\n- Excessive roadmapping (structure should feel natural)\r\n\r\n### In Conclusions\r\n- Introducing new findings not in Findings section\r\n- Forgetting to callback to introduction\r\n- Over-hedging empirical claims\r\n- Skipping limitations entirely (looks defensive)\r\n- Ending with limitations (save strong closing for last)\r\n- Repeating introduction verbatim (callback  copy)\r\n\r\n## Output Expectations\r\n\r\nProvide the user with:\r\n- A drafted **Introduction** matching their cluster style\r\n- A drafted **Conclusion** with all standard elements\r\n- A **coherence memo** assessing promise-delivery alignment\r\n- Revision suggestions if coherence issues detected\r\n\r\n## Invoking Phase Agents\r\n\r\nUse the Task tool for each phase:\r\n\r\n```\r\nTask: Phase 1 Introduction Drafting\r\nsubagent_type: general-purpose\r\nmodel: opus\r\nprompt: Read phases/phase1-introduction.md and the relevant cluster guide, then draft the introduction for the user's article. The theory section and findings are provided. Match the opening move and length to cluster conventions.\r\n```\r\n\r\n**Model recommendations**:\r\n- Phase 0 (intake): Sonnet\r\n- Phase 1 (introduction): Opus (requires narrative craft)\r\n- Phase 2 (conclusion): Opus (requires integration)\r\n- Phase 3 (coherence): Opus (requires evaluative judgment)\r\n",
        "plugins/interview-bookends/skills/interview-bookends/clusters/concept-building.md": "# Cluster 3: Concept-Building Architect\r\n\r\n**Prevalence**: 15.0% of corpus\r\n\r\n**Contribution logic**: \"Existing concepts fail; this article introduces a better one.\"\r\n\r\n---\r\n\r\n## Identifying Concept-Building Articles\r\n\r\nYour article is Concept-Building if:\r\n- The Theory section develops **new conceptual tools or typologies**\r\n- You **introduce a new term** with definitional precision\r\n- Structure is **building-blocks** (concept  concept  synthesis)\r\n- Subsections are **moderate to heavy** (3-5+)\r\n- Literature balance is **theory-heavy**\r\n- You may include a **table or figure** presenting the new framework\r\n\r\n---\r\n\r\n## Introduction Signature\r\n\r\n### Key Statistics\r\n| Feature | Typical Value |\r\n|---------|---------------|\r\n| Word count | 800-1,000 words (longest cluster) |\r\n| Paragraphs | 6-8 |\r\n| Opening move | Phenomenon-led (78%) |\r\n| Data mention | Late (33% in final third) |\r\n| Roadmap | Higher rate (56%) |\r\n\r\n### Opening Move\r\n**Use phenomenon-led opening** (78%)but with a conceptual motivation.\r\n\r\n> \"The unprecedented growth in rates of U.S. incarceration since the early 1970s has prompted scholars to rethink basic understandings of punishment, inequality, and the state.\"\r\n\r\nThe opening establishes the phenomenon, then pivots to the conceptual gap.\r\n\r\n### Structure\r\n1. **Opening** (1-2 sentences): State phenomenon/stakes\r\n2. **Conceptual problem** (1-2 paragraphs): Show why existing concepts fail\r\n3. **New concept introduction** (1 paragraph): Name and briefly define the new term\r\n4. **Preview of concept development** (1 paragraph): How the concept will be developed\r\n5. **Data preview** (2-3 sentences): Can come late\r\n6. **Roadmap** (1 paragraph): Recommended for complex conceptual work\r\n\r\n### Motivating Conceptual Need\r\nThe introduction must show **why existing concepts are inadequate**:\r\n\r\n> \"Existing conceptsburnout, disengagement, defectionfail to capture [specific inadequacy]. The catch-all of 'exit' obscures [what it obscures].\"\r\n\r\nThis sets up the contribution: a new concept that does what existing ones don't.\r\n\r\n### Introducing the New Concept\r\nName the concept clearly and define it precisely:\r\n\r\n> \"We introduce the concept of 'carceral creep' to describe [definition]. Unlike [existing term], this concept captures [distinctive feature].\"\r\n\r\n### Longer Introduction Justified\r\nConcept-Building introductions are the longest because you must:\r\n1. Establish the phenomenon\r\n2. Show conceptual inadequacy\r\n3. Introduce the new concept\r\n4. Preview how it will be developed\r\n\r\n---\r\n\r\n## Conclusion Signature\r\n\r\n### Key Statistics\r\n| Feature | Typical Value |\r\n|---------|---------------|\r\n| Word count | 800-900 words |\r\n| Paragraphs | 4-6 |\r\n| Intro:Conclusion ratio | ~1.03x (nearly balanced) |\r\n| Limitations | Lower (44%) |\r\n| Future directions | Lower (56%) |\r\n\r\n### Why Conclusions Are Shorter\r\nThe conceptual work is **front-loaded**. The introduction motivates the concept; the Theory section develops it; the Findings demonstrate it. By the conclusion, the concept is proven. The conclusion consolidates rather than expands.\r\n\r\n### Structure\r\n1. **Opening** (1 paragraph): Concept affirmation or summary\r\n2. **Concept deployment** (1-2 paragraphs): Recap how concept illuminated findings\r\n3. **Contribution claim** (1 paragraph): What concept adds to field\r\n4. **Scope and applications** (1 paragraph): Where else concept applies\r\n5. **Coda** (2-3 sentences): Resonant closing\r\n\r\n### Opening Move\r\n**Summary opening is more common** in Concept-Building (44% vs. 20% corpus average):\r\n\r\n> \"In identifying the systematic aspects of identity work, we have developed the concept of [X] to capture [Y].\"\r\n\r\n### Consolidating the Concept\r\nThe conclusion should affirm that the concept did analytic work:\r\n\r\n> \"The concept of [X] proved useful for understanding [phenomenon]. Unlike [existing concepts], it captured [specific insight].\"\r\n\r\n### Lower Limitations Rate\r\nConcept-Building articles have lower limitations rates (44%). The focus is on affirming conceptual utility, not bounding claims.\r\n\r\n---\r\n\r\n## Coherence Pattern\r\n\r\n### Dominant Type\r\n**Mixed**: Parallel (44%), Bookends (22%), Deflators (22%)\r\n\r\n### Deflation Risk\r\n**Highest (22%)**: Ambitious conceptual promises are hardest to match.\r\n\r\n**Warning signs**:\r\n- Introduction promises a \"new way of understanding\" but findings show modest insight\r\n- Concept is introduced grandly but only explains one pattern\r\n- Existing concepts actually work fine for the data\r\n\r\n### Avoiding Deflation\r\n1. **Calibrate ambition**: Don't claim more than the concept delivers\r\n2. **Show conceptual work**: Demonstrate what the concept captures that others don't\r\n3. **Distinguish clearly**: Make the difference from existing concepts concrete\r\n\r\n### Bookends Pattern\r\nWhen it works, Concept-Building achieves **Bookends (22%)**:\r\n- Introduction poses conceptual problem\r\n- Conclusion resolves it with new concept affirmed\r\n\r\n---\r\n\r\n## Exemplar\r\n\r\n**10.1093_socpro_spz013 (Carceral Creep)**\r\n\r\n- **Introduction**: 861 words, stakes-led, introduces \"carceral creep\"\r\n- **Theory section**: Building-blocks arc, table defining six stages\r\n- **Conclusion**: 875 words, summary opening, concept deployed\r\n- **Ratio**: 1.02 (balanced)\r\n- **Coherence**: Bookendsopens with conceptual need, closes with concept affirmed\r\n\r\n---\r\n\r\n## Writing Checklist\r\n\r\n### Introduction\r\n- [ ] Longer introduction justified (800-1,000 words)\r\n- [ ] Establish phenomenon/stakes\r\n- [ ] Show why existing concepts fail\r\n- [ ] Introduce new concept with precision\r\n- [ ] Preview how concept will be developed\r\n- [ ] Include roadmap (recommended)\r\n- [ ] Data mention can be late\r\n\r\n### Conclusion\r\n- [ ] Plan for balanced length (~800-900 words)\r\n- [ ] Open with summary or concept affirmation\r\n- [ ] Consolidate concept's utility\r\n- [ ] Distinguish from existing concepts\r\n- [ ] Define scope of applicability\r\n- [ ] May have lower limitations emphasis\r\n- [ ] End with resonant affirmation of concept\r\n\r\n### Coherence\r\n- [ ] Calibrate introduction ambition carefully\r\n- [ ] Concept must do real analytic work in findings\r\n- [ ] Conclusion affirms what introduction promised\r\n- [ ] Watch for deflation risk\r\n- [ ] Key conceptual vocabulary must appear in both\r\n",
        "plugins/interview-bookends/skills/interview-bookends/clusters/gap-filler.md": "# Cluster 1: Gap-Filler Minimalist\r\n\r\n**Prevalence**: 38.8% of corpus (most common)\r\n\r\n**Contribution logic**: \"We don't know about X; this study tells us.\"\r\n\r\n---\r\n\r\n## Identifying Gap-Filler Articles\r\n\r\nYour article is Gap-Filler if:\r\n- The Theory section identifies an **empirical gap** the study fills\r\n- Structure is **funnel-shaped** (broad context  specific gap)\r\n- Theory is **balanced or substantive-heavy** (not theory-heavy)\r\n- Subsections are **minimal** (0-2)\r\n- The contribution is primarily **empirical** (documenting, describing)\r\n\r\n---\r\n\r\n## Introduction Signature\r\n\r\n### Key Statistics\r\n| Feature | Typical Value |\r\n|---------|---------------|\r\n| Word count | 600-750 words (shortest cluster) |\r\n| Paragraphs | 4-5 |\r\n| Opening move | Phenomenon-led (77%) |\r\n| Data mention | Early (40% in first third) |\r\n| Roadmap | Optional (40%) |\r\n\r\n### Opening Move\r\n**Use phenomenon-led opening** (77% of Gap-Fillers do this).\r\n\r\n> \"Tasked with protecting children from abuse and neglect, U.S. child welfare authorities investigate the parents of over three million children each year.\"\r\n\r\nAvoid theory-led; the contribution isn't theoretical.\r\n\r\n### Structure\r\n1. **Opening** (1-2 sentences): State the phenomenon\r\n2. **Context** (1 paragraph): Situate empirically, briefly note existing research\r\n3. **Gap** (1 paragraph): \"Yet we know less about...\" / \"Prior research focused on X, leaving Y unexplored\"\r\n4. **Data preview** (2-3 sentences): Mention data early\r\n5. **Argument preview** (3-4 sentences): State findings concisely\r\n\r\n### Data Mention\r\nMention data **early**within the first 3 paragraphs. Gap-Filler articles are empirically grounded; readers want to know your evidence base.\r\n\r\n> \"Drawing on interviews with 83 poor mothers in [location], this study examines...\"\r\n\r\n### Tone\r\n- Efficient, not elaborate\r\n- Modest but clear claims\r\n- Don't oversell; the findings speak for themselves\r\n\r\n---\r\n\r\n## Conclusion Signature\r\n\r\n### Key Statistics\r\n| Feature | Typical Value |\r\n|---------|---------------|\r\n| Word count | 1,200-1,400 words (longest relative to intro) |\r\n| Paragraphs | 7-9 |\r\n| Intro:Conclusion ratio | ~2.05x |\r\n| Limitations | Present (74%) |\r\n| Future directions | Present (77%) |\r\n\r\n### Why Conclusions Are Long\r\nGap-Filler articles have modest introduction promises. The conclusion must **accumulate significance**transforming empirical findings into broader import. This takes space.\r\n\r\n### Structure\r\n1. **Restatement opening** (1 paragraph): Return to puzzle\r\n2. **Findings summary** (1-2 paragraphs): Recap key patterns\r\n3. **Literature integration** (1-2 paragraphs): Connect to prior research\r\n4. **Contribution claim** (1 paragraph): State what study adds\r\n5. **Limitations** (1 paragraph): Bound the claims\r\n6. **Implications** (1-2 paragraphs): Policy and/or theoretical\r\n7. **Future directions** (1 paragraph): Next steps\r\n8. **Coda** (2-3 sentences): Resonant closing\r\n\r\n### Accumulating Significance\r\nSince the introduction was modest, the conclusion must build significance:\r\n\r\n> \"These findings extend our understanding of [topic] in three ways. First, [contribution 1]. Second, [contribution 2]. Third, [contribution 3]. Together, they suggest that [broader implication].\"\r\n\r\n### Limitations\r\nInclude explicit limitations (74% do). Be honest but not self-undermining:\r\n\r\n> \"This study is limited to [population] in [context]. Whether these patterns hold for [other populations] remains an empirical question.\"\r\n\r\n---\r\n\r\n## Coherence Pattern\r\n\r\n### Dominant Type\r\n**Parallel (69%)**: Deliver what you promise. Gap-Filler promises are modest, so they're easy to keep.\r\n\r\n### Deflation Risk\r\n**Low (3%)**: Modest promises are hard to underdeliver on.\r\n\r\n### Escalation\r\n**Some (23%)**: Findings may exceed expectations. Frame as discovery:\r\n\r\n> \"Beyond documenting [stated goal], our data revealed [broader pattern], suggesting that...\"\r\n\r\n---\r\n\r\n## Exemplar\r\n\r\n**10.1093_sf_soad098 (Migrant Care Work)**\r\n\r\n- **Introduction**: 629 words, phenomenon-led, roadmap present\r\n- **Conclusion**: 1,068 words, restatement opening, limitations present\r\n- **Ratio**: 1.70\r\n- **Coherence**: Parallelconsistent empirical focus throughout\r\n\r\n---\r\n\r\n## Writing Checklist\r\n\r\n### Introduction\r\n- [ ] Open with phenomenon (not theory, not question)\r\n- [ ] Mention data within first 3 paragraphs\r\n- [ ] Keep under 750 words\r\n- [ ] State the gap clearly but briefly\r\n- [ ] Preview findings concisely\r\n\r\n### Conclusion\r\n- [ ] Plan for length (~1,200+ words)\r\n- [ ] Open with restatement of puzzle\r\n- [ ] Summarize findings without repeating Findings section\r\n- [ ] Connect to prior literature\r\n- [ ] Include limitations paragraph\r\n- [ ] Include future directions\r\n- [ ] Build significance through accumulation\r\n- [ ] End with resonant coda, not limitations\r\n\r\n### Coherence\r\n- [ ] Introduction promises are modest\r\n- [ ] Conclusion delivers on all promises\r\n- [ ] Callback to introduction present\r\n- [ ] Key vocabulary echoes across sections\r\n",
        "plugins/interview-bookends/skills/interview-bookends/clusters/problem-driven.md": "# Cluster 5: Problem-Driven Pragmatist\r\n\r\n**Prevalence**: 15.0% of corpus\r\n\r\n**Contribution logic**: \"Here's an important problem; here's what my research reveals about it.\"\r\n\r\n---\r\n\r\n## Identifying Problem-Driven Articles\r\n\r\nYour article is Problem-Driven if:\r\n- The Theory section is organized around **empirical puzzles or policy debates**\r\n- Theory enters **in service of the problem** (not leading)\r\n- Structure may be **problem-response or dialogue**\r\n- Literature balance is **balanced or substantive-heavy**\r\n- The contribution is primarily **empirical documentation** or **debate resolution**\r\n\r\n### Two Sub-Types\r\n\r\n**5a: Debate-Resolvers**\r\n- Frame existing literature as characterized by competing positions\r\n- Position study as resolving the tension\r\n- \"Scholars disagree about X; our data adjudicates\"\r\n\r\n**5b: Empirical Documenters**\r\n- Heavy on descriptive context with light theoretical framing\r\n- Often policy-relevant research\r\n- \"Here's what's happening with [understudied phenomenon]\"\r\n\r\n---\r\n\r\n## Introduction Signature\r\n\r\n### Key Statistics\r\n| Feature | Typical Value |\r\n|---------|---------------|\r\n| Word count | 750-950 words |\r\n| Paragraphs | 5-7 |\r\n| Opening move | Phenomenon (50%), Stakes (25%), Case (17%) |\r\n| Data mention | Early to middle |\r\n| Roadmap | Lower (33%) |\r\n\r\n### Opening Move\r\n**Stakes-led openings are common** (25% vs. 11% corpus average):\r\n\r\n> \"As global unaccompanied minor migration persists and expands, understanding how these youth navigate incorporation has never been more urgent.\"\r\n\r\n**Case-led openings work well** (17%):\r\n\r\n> \"In Cleveland, Ohio, the federal Department of Justice declared that the city's police department engaged in a 'pattern or practice' of using excessive force.\"\r\n\r\n### Structure\r\n1. **Opening** (1-2 sentences): Stakes or case\r\n2. **Problem context** (1-2 paragraphs): Policy background, empirical stakes\r\n3. **Theoretical positioning** (1 paragraph): Light framing, debate if applicable\r\n4. **Research questions** (2-3 sentences): What this study examines\r\n5. **Data preview** (2-3 sentences): Can be early\r\n6. **Argument preview** (3-4 sentences): Main findings\r\n\r\n### Stakes Emphasis\r\nProblem-Driven articles emphasize **why this matters in the real world**:\r\n\r\n> \"The stakes are high: [statistic]. Understanding [phenomenon] has implications for [policy/practice].\"\r\n\r\n### Debate Framing (5a)\r\nIf debate-resolving, frame the competing positions:\r\n\r\n> \"Scholars disagree about whether [position A] or [position B] better explains [phenomenon]. Some argue [A], while others contend [B]. This study provides evidence to adjudicate this debate.\"\r\n\r\n### Lower Roadmap Rate\r\nProblem-Driven articles have lower roadmap rates (33%). The structure is often simplerproblem  evidence  implications.\r\n\r\n---\r\n\r\n## Conclusion Signature\r\n\r\n### Key Statistics\r\n| Feature | Typical Value |\r\n|---------|---------------|\r\n| Word count | 1,100-1,400 words |\r\n| Paragraphs | 6-9 |\r\n| Intro:Conclusion ratio | ~1.38x |\r\n| Opening move | Integration (25%)higher than average |\r\n| Limitations | Present (58%)lower |\r\n| Future directions | Present (67%) |\r\n\r\n### Why Integration Opening\r\nProblem-Driven conclusions often open with **literature integration** (25%):\r\n\r\n> \"Scholarship on [topic] has not fully captured [phenomenon]. Our analysis reveals...\"\r\n\r\n### Structure\r\n1. **Opening** (1 paragraph): Contribution claim or integration\r\n2. **Findings summary** (1-2 paragraphs): Key empirical patterns\r\n3. **Policy implications** (1-2 paragraphs): What this means for practice\r\n4. **Theoretical implications** (1 paragraph): If applicable\r\n5. **Limitations** (1 paragraph): Often brief\r\n6. **Future directions** (1 paragraph)\r\n7. **Coda** (2-3 sentences): Return to stakes\r\n\r\n### Policy Implications\r\nProblem-Driven conclusions often develop **policy implications** more extensively:\r\n\r\n> \"For practitioners, these findings suggest [recommendation]. Current policies that [X] may inadvertently [Y]. More broadly, [implication for policy domain].\"\r\n\r\n### Escalation Common\r\nProblem-Driven articles frequently escalate (33%)extending from specific problem to broader implications:\r\n\r\n> \"While our study focused on [specific context], these dynamics likely operate wherever [conditions]. The concept of [X] may prove useful for understanding [broader phenomenon].\"\r\n\r\n---\r\n\r\n## Coherence Pattern\r\n\r\n### Dominant Type\r\n**Parallel (50%)** and **Escalators (33%)**\r\n\r\n### Escalation Rate\r\n**Highest (33%)**: Problem-driven research naturally extends to implications.\r\n\r\nThis is **acceptable and expected**. Real-world problems invite broader claims:\r\n\r\n> \"Understanding [phenomenon] becomes increasingly urgent as [trend] reshapes [domain].\"\r\n\r\n### Deflation Risk\r\n**Moderate (17%)**: Can occur when policy complexity prevents full resolution.\r\n\r\n### No Bookends\r\n**Zero (0%)**: Problem-Driven articles don't have strong mirror structure. They move from problem to implications rather than circling back.\r\n\r\n---\r\n\r\n## Exemplar\r\n\r\n**10.1093_sf_soz110 (Residential Search Logics)**\r\n\r\n- **Introduction**: 355 words (brief!), stakes-led\r\n- **Conclusion**: 1,325 words, broad stratification claims\r\n- **Ratio**: 3.73 (highest escalation example)\r\n- **Coherence**: Escalatorsextends from housing search to inequality\r\n\r\n---\r\n\r\n## Writing Checklist\r\n\r\n### Introduction\r\n- [ ] Consider stakes-led or case-led opening\r\n- [ ] Emphasize policy context and real-world urgency\r\n- [ ] If debate-resolving, frame competing positions\r\n- [ ] Data can be mentioned early\r\n- [ ] Roadmap optional (structure often simpler)\r\n- [ ] Connect to current events/policy debates if relevant\r\n\r\n### Conclusion\r\n- [ ] Plan for moderate length (~1,100-1,400 words)\r\n- [ ] Consider integration opening\r\n- [ ] Develop policy implications substantively\r\n- [ ] Connect to broader significance\r\n- [ ] Escalation is acceptable and expected\r\n- [ ] Return to stakes in coda\r\n- [ ] Limitations can be brief\r\n\r\n### Coherence\r\n- [ ] Stakes emphasis consistent across sections\r\n- [ ] Policy implications developed (not just mentioned)\r\n- [ ] Escalation is grounded in evidence\r\n- [ ] Callback to opening stakes/case\r\n- [ ] Problem is addressed, not just described\r\n",
        "plugins/interview-bookends/skills/interview-bookends/clusters/synthesis.md": "# Cluster 4: Synthesis Integrator\r\n\r\n**Prevalence**: 17.5% of corpus\r\n\r\n**Contribution logic**: \"These literatures have developed separately; I connect them.\"\r\n\r\n---\r\n\r\n## Identifying Synthesis Articles\r\n\r\nYour article is Synthesis if:\r\n- The Theory section **connects previously separate literatures**\r\n- You explicitly **name multiple theoretical traditions**\r\n- Structure is **building-blocks or dialogue**\r\n- Language includes \"drawing together,\" \"connecting,\" \"integrating\"\r\n- The contribution is **integration** rather than new concept or gap-filling\r\n\r\n---\r\n\r\n## Introduction Signature\r\n\r\n### Key Statistics\r\n| Feature | Typical Value |\r\n|---------|---------------|\r\n| Word count | 800-950 words |\r\n| Paragraphs | 6-8 |\r\n| Opening move | Phenomenon-led (71%) |\r\n| Data mention | Variable |\r\n| Roadmap | 43% |\r\n\r\n### Opening Move\r\n**Phenomenon-led is typical** (71%), but the phenomenon sets up the integration:\r\n\r\n> \"How do people make sense of their reliance on unjust institutions?\"\r\n\r\nThe opening question or phenomenon must require **multiple literatures** to answer.\r\n\r\n### Structure\r\n1. **Opening** (1-2 sentences): Phenomenon or question\r\n2. **First tradition** (1 paragraph): Name and briefly describe tradition A\r\n3. **Second tradition** (1 paragraph): Name and briefly describe tradition B\r\n4. **Integration preview** (1 paragraph): How bringing them together illuminates\r\n5. **Data preview** (2-3 sentences)\r\n6. **Argument preview** (3-4 sentences): What the integration reveals\r\n\r\n### Naming Multiple Traditions\r\nThe introduction **must name multiple traditions explicitly**:\r\n\r\n> \"To develop this analysis, we explicitly connect several theoretical traditions: recognition theory (Honneth, Lamont), legal cynicism (Kirk, Matsuda), and legal consciousness (Ewick, Silbey).\"\r\n\r\nThis is not optional. Synthesis articles announce their integration upfront.\r\n\r\n### Previewing the Integration\r\nShow what the integration accomplishes:\r\n\r\n> \"By connecting [A] and [B], we reveal [insight]. What appeared as [separate phenomena] are better understood as [unified process].\"\r\n\r\n---\r\n\r\n## Conclusion Signature\r\n\r\n### Key Statistics\r\n| Feature | Typical Value |\r\n|---------|---------------|\r\n| Word count | 1,200-1,500 words |\r\n| Paragraphs | 7-9 |\r\n| Intro:Conclusion ratio | ~1.69x |\r\n| Opening move | Integration (21%)higher than average |\r\n| Limitations | Present (71%) |\r\n| Future directions | Present (79%) |\r\n\r\n### Why Integration Opening\r\nSynthesis conclusions often open with **literature positioning** (21% vs. 9% corpus average):\r\n\r\n> \"Recent research in cultural and political sociology has emphasized the role of recognition in shaping institutional trust...\"\r\n\r\nThis signals: we promised integration, and here's how we delivered.\r\n\r\n### Structure\r\n1. **Integration opening** (1 paragraph): Position in literature\r\n2. **Findings summary** (1-2 paragraphs): Recap through integrative lens\r\n3. **Tradition-by-tradition** (1-2 paragraphs): Return to each named tradition\r\n4. **Contribution claim** (1 paragraph): What integration adds\r\n5. **Limitations** (1 paragraph)\r\n6. **Implications** (1 paragraph)\r\n7. **Future directions** (1 paragraph)\r\n8. **Coda** (2-3 sentences)\r\n\r\n### Returning to Each Tradition\r\nThe conclusion **must return to each tradition named in the introduction**:\r\n\r\n> \"For scholars of [tradition A], our findings suggest... For researchers in [tradition B], the analysis reveals...\"\r\n\r\nIf you named three traditions in the introduction, all three must appear in the conclusion.\r\n\r\n---\r\n\r\n## Coherence Pattern\r\n\r\n### Dominant Type\r\n**Parallel (57%)** and **Escalators (29%)**\r\n\r\n### Deflation Risk\r\n**Zero (0%)**: Synthesis articles never deflate.\r\n\r\n**Why?** Integration promises are **visible and verifiable**. Either the literatures are connected or they're not. There's no graceful retreat from \"I will integrate A and B\" if you only deliver A.\r\n\r\n### Escalation Opportunity\r\nSynthesis articles often escalate (29%)synthesis reveals additional connections beyond what was promised:\r\n\r\n> \"Beyond connecting [A] and [B], our analysis suggests a deeper affinity with [C]...\"\r\n\r\n---\r\n\r\n## Exemplar\r\n\r\n**10.1093_socpro_spaa017 (Recognition and Legal Cynicism)**\r\n\r\n- **Introduction**: 1,010 words, question-led, three traditions named\r\n- **Theory section**: Building-blocks arc, theory-heavy\r\n- **Conclusion**: 2,606 words (!), integrates all traditions\r\n- **Ratio**: 2.58\r\n- **Coherence**: Bookendsquestion posed, answer delivered through integration\r\n\r\n---\r\n\r\n## Critical Requirements\r\n\r\n### The Three-Tradition Rule\r\nIf you name three traditions in the introduction, your conclusion must:\r\n1. Reference all three\r\n2. Show how findings relate to each\r\n3. Demonstrate the integration\r\n\r\n### No Dropping Traditions\r\nYou cannot promise A + B + C and deliver only A + B. This is deflation.\r\n\r\n### Integration, Not Summary\r\nThe conclusion integratesit doesn't just summarize:\r\n\r\n**Not this**:\r\n> \"We examined [A] and [B].\"\r\n\r\n**This**:\r\n> \"By connecting [A] and [B], we revealed [insight]. Scholars of [A] have emphasized [X]; our analysis shows that [B] complicates this by [Y].\"\r\n\r\n---\r\n\r\n## Writing Checklist\r\n\r\n### Introduction\r\n- [ ] Name multiple traditions explicitly\r\n- [ ] Use \"connecting,\" \"drawing together,\" \"integrating\" language\r\n- [ ] Preview what integration accomplishes\r\n- [ ] Establish why traditions need connecting (they've developed separately)\r\n- [ ] Make promises you can keep\r\n\r\n### Conclusion\r\n- [ ] Plan for substantial length (~1,200-1,500 words)\r\n- [ ] Consider integration opening move\r\n- [ ] Return to EACH tradition named in introduction\r\n- [ ] Show how findings speak to each tradition\r\n- [ ] Demonstrate integration, not just summary\r\n- [ ] State contribution clearly\r\n\r\n### Coherence\r\n- [ ] Every tradition named in intro appears in conclusion\r\n- [ ] Integration is visible and verifiable\r\n- [ ] No deflationcannot drop traditions\r\n- [ ] Vocabulary echoes: tradition names must recur\r\n- [ ] Consider escalation if synthesis reveals more than promised\r\n",
        "plugins/interview-bookends/skills/interview-bookends/clusters/theory-extension.md": "# Cluster 2: Theory-Extension Framework Applier\r\n\r\n**Prevalence**: 22.5% of corpus\r\n\r\n**Contribution logic**: \"Framework X exists; I show it applies to Y.\"\r\n\r\n---\r\n\r\n## Identifying Theory-Extension Articles\r\n\r\nYour article is Theory-Extension if:\r\n- The Theory section **applies an established framework** to a new domain\r\n- You **name a specific framework** (often by author: \"Bourdieu's...\", \"Goffman's...\")\r\n- The contribution is **demonstrating the framework's utility** in a new context\r\n- Subsections are **light to moderate** (1-4)\r\n- Literature balance is **theory-heavy**\r\n\r\n---\r\n\r\n## Introduction Signature\r\n\r\n### Key Statistics\r\n| Feature | Typical Value |\r\n|---------|---------------|\r\n| Word count | 750-900 words |\r\n| Paragraphs | 5-7 |\r\n| Opening move | Theory-led (30%) or Phenomenon-led (50%) |\r\n| Data mention | Late (after framework established) |\r\n| Roadmap | 40% |\r\n\r\n### Opening Move\r\n**Theory-led openings are distinctive** to this cluster (30% vs. 6% corpus average):\r\n\r\n> \"The concept of the 'glass cliff' refers to the tendency for women and racial/ethnic minorities to be appointed to leadership positions during times of crisis.\"\r\n\r\nBut **phenomenon-led is still common** (50%):\r\n\r\n> \"As discrimination in labor markets persists, scholars have sought to understand how disadvantaged groups navigate occupational barriers.\"\r\n\r\n### When to Use Theory-Led\r\nUse theory-led opening when:\r\n- The framework is **well-known** in the field\r\n- The framework IS the backbone of the article\r\n- Readers will recognize the framework immediately\r\n\r\nUse phenomenon-led when:\r\n- The phenomenon needs establishing before the framework\r\n- You want to show the puzzle first, then introduce the framework as a tool\r\n\r\n### Structure\r\n1. **Opening** (1-2 sentences): Framework or phenomenon\r\n2. **Framework exposition** (1-2 paragraphs): Explain the framework\r\n3. **Extension gap** (1 paragraph): Why it hasn't been applied to this domain\r\n4. **Why extension matters** (1 paragraph): What we expect to learn\r\n5. **Data preview** (2-3 sentences): Can be late\r\n6. **Argument preview** (3-4 sentences): Framework + domain = insight\r\n\r\n### The Extension Move\r\nThe key rhetorical move is showing why extension is **non-trivial**:\r\n\r\n> \"While [framework] has illuminated [domain A], scholars have not applied it to [domain B]. This is surprising because [reasons it might operate differently]. We extend [framework] to [domain B] to reveal [insight].\"\r\n\r\n---\r\n\r\n## Conclusion Signature\r\n\r\n### Key Statistics\r\n| Feature | Typical Value |\r\n|---------|---------------|\r\n| Word count | 1,000-1,200 words |\r\n| Paragraphs | 6-8 |\r\n| Intro:Conclusion ratio | ~1.41x |\r\n| Limitations | Present (60%) |\r\n| Future directions | Present (70%) |\r\n\r\n### Core Function\r\nThe conclusion must **affirm framework utility**:\r\n\r\n> \"These findings demonstrate the utility of [framework] for understanding [new domain]. The framework illuminated [specific insight] that prior approaches missed.\"\r\n\r\n### Structure\r\n1. **Restatement opening** (1 paragraph): Return to framework + domain\r\n2. **Framework affirmation** (1-2 paragraphs): Show the framework worked\r\n3. **What extension revealed** (1-2 paragraphs): Insights unique to this domain\r\n4. **Contribution claim** (1 paragraph): What we now know about framework AND domain\r\n5. **Limitations** (1 paragraph)\r\n6. **Future extensions** (1 paragraph): Other domains where framework might apply\r\n7. **Coda** (2-3 sentences)\r\n\r\n### Affirming the Framework\r\nThe conclusion must show the framework **did analytic work**:\r\n\r\n**Not this**:\r\n> \"We applied [framework] to [domain].\"\r\n\r\n**This**:\r\n> \"[Framework] revealed [insight] that other approaches could not capture. Specifically, the concept of [X] illuminated [Y], demonstrating the framework's utility beyond its original domain.\"\r\n\r\n### Suggesting Further Extensions\r\nTheory-Extension conclusions often point to **other potential applications**:\r\n\r\n> \"These findings suggest [framework] may prove useful for understanding [other domain]. Future research might examine whether [mechanism] operates similarly in [other context].\"\r\n\r\n---\r\n\r\n## Coherence Pattern\r\n\r\n### Dominant Type\r\n**Parallel (60%)** and **Escalators (20%)**\r\n\r\n### Deflation Risk\r\n**Moderate (10%)**: Frameworks don't always work as expected in new domains.\r\n\r\n**Warning signs**:\r\n- Findings don't clearly connect to framework concepts\r\n- Framework feels \"tacked on\" rather than central\r\n- Extension reveals the framework doesn't quite fit\r\n\r\n### Avoiding Deflation\r\n1. **Choose the right framework**: If the framework doesn't fit, reconsider\r\n2. **Show conceptual connection**: Link findings to framework vocabulary\r\n3. **Acknowledge if framework needed modification**: This can BE the contribution\r\n\r\n### Escalation\r\nFramework may prove **more useful than expected** (20%):\r\n\r\n> \"Beyond confirming [framework]'s utility, our findings suggest [additional insight], extending [framework] in [new direction].\"\r\n\r\n---\r\n\r\n## Exemplar\r\n\r\n**10.1093_socpro_spab036 (Yellow Vests/Populism)**\r\n\r\n- **Introduction**: 1,281 words, case-led with populism framework\r\n- **Theory section**: Populism framework exposition\r\n- **Conclusion**: 1,454 words, restatement, framework affirmed\r\n- **Ratio**: 1.14\r\n- **Coherence**: Parallelconsistent populism analysis throughout\r\n\r\n---\r\n\r\n## Writing Checklist\r\n\r\n### Introduction\r\n- [ ] Consider theory-led opening if framework is well-known\r\n- [ ] Name the framework explicitly\r\n- [ ] Explain framework enough for readers to follow\r\n- [ ] Show why extension to this domain is non-trivial\r\n- [ ] Establish what we expect to learn\r\n- [ ] Data can come late (after framework)\r\n\r\n### Conclusion\r\n- [ ] Plan for moderate length (~1,000-1,200 words)\r\n- [ ] Open with restatement of framework + domain\r\n- [ ] Affirm framework utility explicitly\r\n- [ ] Show what extension revealed\r\n- [ ] Connect findings to framework vocabulary\r\n- [ ] Point to other potential applications\r\n- [ ] Include limitations\r\n\r\n### Coherence\r\n- [ ] Framework appears in both intro and conclusion\r\n- [ ] Framework concepts appear in findings\r\n- [ ] Extension is demonstrated, not just claimed\r\n- [ ] No deflationframework must do work\r\n- [ ] Vocabulary from framework recurs throughout\r\n",
        "plugins/interview-bookends/skills/interview-bookends/phases/phase0-intake.md": "# Phase 0: Intake & Assessment\r\n\r\nYou are assessing the user's materials and identifying the appropriate cluster style for their introduction and conclusion.\r\n\r\n## Why This Phase Matters\r\n\r\nThe introduction and conclusion must match the contribution type. A gap-filler article needs different framing than a concept-building article. Before drafting, you must understand what kind of contribution the user is making and align the framing accordingly.\r\n\r\n## Your Tasks\r\n\r\n### 1. Gather Required Materials\r\n\r\nRequest these materials if not provided:\r\n- **Theory/Literature Review section** (required)\r\n- **Findings section** (required)\r\n- **Abstract** (helpful but optional)\r\n- **Target journal** (helpful for calibrating length/style)\r\n\r\n### 2. Read and Analyze the Theory Section\r\n\r\nIdentify the **positioning move**:\r\n\r\n| Positioning | Indicators |\r\n|-------------|------------|\r\n| **Gap-filling** | \"Little research has examined...\", \"We know less about...\", identifies empirical gap |\r\n| **Theory-extension** | Names specific framework, \"X theory suggests...\", applies existing framework to new domain |\r\n| **Concept-building** | Introduces new term, critiques existing concepts, presents typology |\r\n| **Synthesis** | Connects multiple literatures, \"Drawing together...\", integrates traditions |\r\n| **Debate-resolution** | \"Scholars disagree...\", adjudicates between positions |\r\n\r\nIdentify the **arc structure**:\r\n- Funnel (broad  specific)\r\n- Building-blocks (concept  concept  concept)\r\n- Dialogue (position 1  position 2  synthesis)\r\n\r\n### 3. Read and Analyze the Findings Section\r\n\r\nIdentify:\r\n- **Main argument** (the central claim)\r\n- **Key mechanisms/findings** (the pillars)\r\n- **Empirical scope** (who was studied, where, when)\r\n- **Level of theoretical ambition** (description vs. explanation vs. new concept)\r\n\r\n### 4. Assign Cluster Membership\r\n\r\nBased on your analysis, assign the article to one of five clusters:\r\n\r\n| Cluster | Key Indicators | Prevalence |\r\n|---------|----------------|------------|\r\n| **Gap-Filler Minimalist** | Empirical gap, funnel structure, substantive focus | 38.8% |\r\n| **Theory-Extension** | Named framework, application to new domain | 22.5% |\r\n| **Concept-Building** | New term introduced, conceptual critique | 15.0% |\r\n| **Synthesis Integrator** | Multiple traditions connected, integration claims | 17.5% |\r\n| **Problem-Driven Pragmatist** | Policy stakes, debate framing, empirical focus | 15.0% |\r\n\r\n### 5. Confirm Scope\r\n\r\nAsk the user:\r\n- Do you need an **introduction only**, **conclusion only**, or **both**?\r\n- What is your **target word count** for each section?\r\n- Are there **specific elements** you want emphasized (e.g., policy implications)?\r\n\r\n## Cluster-Specific Implications\r\n\r\n### If Gap-Filler Minimalist:\r\n- **Introduction**: Keep short (~600-700 words), phenomenon-led opening, data mention early\r\n- **Conclusion**: Plan for length (~1,200-1,400 words), emphasis on implications\r\n- **Ratio**: Conclusion should be ~2x introduction length\r\n\r\n### If Theory-Extension:\r\n- **Introduction**: Consider theory-led opening, name framework early\r\n- **Conclusion**: Framework affirmation, moderate length\r\n- **Ratio**: Closer to 1.4x\r\n\r\n### If Concept-Building:\r\n- **Introduction**: Longer to motivate conceptual need, include roadmap\r\n- **Conclusion**: Can be proportionally brief if concept proven in findings\r\n- **Ratio**: Nearly balanced (~1.0x)\r\n- **Risk**: Higher deflation riskcalibrate ambition carefully\r\n\r\n### If Synthesis Integrator:\r\n- **Introduction**: Name multiple traditions, preview integration\r\n- **Conclusion**: Must return to each tradition, integration claims\r\n- **Risk**: Zero deflation toleranceif you promise synthesis, deliver it\r\n\r\n### If Problem-Driven Pragmatist:\r\n- **Introduction**: Stakes-led or case-led opening, policy context\r\n- **Conclusion**: Policy implications, may escalate to broader claims\r\n- **Opportunity**: Escalation is common and acceptable\r\n\r\n## Output\r\n\r\nProvide a summary with:\r\n\r\n1. **Cluster assignment** and rationale (which cluster, why)\r\n2. **Contribution type** (gap-filling, extension, building, synthesis, debate)\r\n3. **Main argument** (one sentence)\r\n4. **Key findings** (3-4 bullets)\r\n5. **Recommended approach**:\r\n   - Introduction opening move\r\n   - Introduction target length\r\n   - Conclusion target length\r\n   - Key elements to include\r\n6. **Coherence considerations** (what promises must the intro make for the conclusion to deliver?)\r\n\r\n## When You're Done\r\n\r\nReturn your assessment to the orchestrator and **pause** for user confirmation before proceeding to drafting.\r\n",
        "plugins/interview-bookends/skills/interview-bookends/phases/phase1-introduction.md": "# Phase 1: Introduction Drafting\r\n\r\nYou are drafting the introduction for a sociology interview article. Your goal is to **open the circuit**establish stakes, identify the puzzle, and preview the contribution.\r\n\r\n## Why This Phase Matters\r\n\r\nThe introduction is where authors position their contribution. It establishes the genre contract: \"I will examine X using Y to illuminate Z.\" Everything that follows must honor this contract. A strong introduction makes the reader want to keep reading; a weak one loses them before the findings begin.\r\n\r\n## Required Reading\r\n\r\nBefore drafting, review:\r\n- `techniques/opening-moves.md`  The five opening move types\r\n- `clusters/[relevant-cluster].md`  Cluster-specific guidance\r\n- Phase 0 assessment  Cluster assignment and recommended approach\r\n\r\n## The Introduction Formula\r\n\r\nMost sociology interview article introductions contain these moves in roughly this order:\r\n\r\n| Move | Function | Typical Length |\r\n|------|----------|----------------|\r\n| **HOOK** | Grab attention, establish stakes | 1-2 sentences |\r\n| **CONTEXT** | Situate empirically | 1-2 paragraphs |\r\n| **GAP/PUZZLE** | Identify what needs explaining | 1 paragraph |\r\n| **DATA PREVIEW** | Introduce empirical approach | 2-4 sentences |\r\n| **ARGUMENT PREVIEW** | State what paper will show | 2-4 sentences |\r\n| **ROADMAP** | Outline paper structure | 0-1 paragraph (optional) |\r\n\r\nNot every introduction includes every move, but the order is fairly consistent.\r\n\r\n## Step 1: Choose Your Opening Move\r\n\r\nBased on cluster assignment:\r\n\r\n### Phenomenon-Led (74% of corpus)\r\nOpens with an empirical statement about the world.\r\n\r\n> \"Neighborhood is an important theme in research on prisoner reentry and desistance from offending, but scholars and practitioners typically use this to refer to a single location.\"\r\n\r\n**Best for**: Gap-Filler, Concept-Building, Synthesis\r\n\r\n### Stakes-Led (11%)\r\nOpens with an importance claim.\r\n\r\n> \"The unprecedented growth in rates of U.S. incarceration since the early 1970s has prompted scholars to rethink basic understandings of punishment, inequality, and the state.\"\r\n\r\n**Best for**: Problem-Driven, Gap-Filler\r\n\r\n### Case-Led (8%)\r\nOpens with a concrete case or scene.\r\n\r\n> \"In Cleveland, Ohio, the federal Department of Justice declared that the city's police department engaged in a 'pattern or practice' of using excessive force.\"\r\n\r\n**Best for**: Problem-Driven, Synthesis\r\n\r\n### Theory-Led (6%)\r\nOpens with a theoretical framework.\r\n\r\n> \"The concept of the 'glass cliff' refers to the tendency for women and racial/ethnic minorities to be appointed to leadership positions during times of crisis.\"\r\n\r\n**Best for**: Theory-Extension (30% use this)\r\n\r\n### Question-Led (1%)\r\nOpens with a direct question.\r\n\r\n> \"How do people make sense of their reliance on unjust institutions?\"\r\n\r\n**Use sparingly**: Rare and can feel performative. Only use when the question is genuinely compelling.\r\n\r\n## Step 2: Establish Context\r\n\r\nAfter the opening move, situate the reader:\r\n\r\n**Empirical context**: What is the phenomenon? Who is affected? What is the scope?\r\n- Statistics, policy background, population characteristics\r\n- Ground the abstract in concrete reality\r\n\r\n**Scholarly context**: Where does this fit in the literature?\r\n- Brief signaling of relevant traditions (save details for Theory section)\r\n- Just enough to establish you know the field\r\n\r\n**Length**: 1-2 paragraphs, ~150-250 words\r\n\r\n## Step 3: Identify the Gap/Puzzle\r\n\r\nThis is the **turn**the moment you identify what needs explaining.\r\n\r\n### Gap-Filling Turn\r\n> \"Yet we know remarkably little about how [population] experiences [phenomenon]. Prior research has focused on [X], leaving [Y] unexplored.\"\r\n\r\n### Extension Turn\r\n> \"While [framework] has illuminated [domain A], scholars have not applied it to [domain B], where [reasons it might operate differently].\"\r\n\r\n### Concept-Building Turn\r\n> \"Existing concepts[X, Y, Z]fail to capture [specific aspect]. This article introduces the concept of [new term] to address this gap.\"\r\n\r\n### Synthesis Turn\r\n> \"These literatures have developed largely in isolation. By drawing them together, we can see [new insight].\"\r\n\r\n### Debate Turn\r\n> \"Scholars disagree about whether [X] or [Y] better explains [phenomenon]. This study adjudicates by examining [case].\"\r\n\r\n## Step 4: Preview Data and Argument\r\n\r\nBriefly introduce your empirical approach and main claim:\r\n\r\n**Data preview**:\r\n> \"Drawing on [N] in-depth interviews with [population] in [location], conducted between [dates]...\"\r\n\r\n**Argument preview**:\r\n> \"We argue that [main claim]. Specifically, we find that [finding 1], [finding 2], and [finding 3].\"\r\n\r\nOr:\r\n> \"This article makes three contributions: First, ... Second, ... Third, ...\"\r\n\r\n**Length**: 1 paragraph, ~100-150 words\r\n\r\n## Step 5: Roadmap (Optional)\r\n\r\nIf the article is complex (multiple findings, synthesis of traditions), include a brief roadmap:\r\n\r\n> \"The article proceeds as follows. We first review [literature], developing our theoretical framework. We then describe our methods and setting. The findings section demonstrates [X]. We conclude by discussing implications for [Y].\"\r\n\r\n**When to skip**: If the structure is obvious (standard empirical article), the roadmap can feel formulaic. Use your judgment.\r\n\r\n## Cluster-Specific Adjustments\r\n\r\n### Gap-Filler\r\n- Keep short: 600-750 words\r\n- Mention data early (within first 3 paragraphs)\r\n- Efficient positioning; don't overexplain\r\n\r\n### Theory-Extension\r\n- Consider theory-led opening\r\n- Name the framework clearly\r\n- Explain why extension to this domain matters\r\n\r\n### Concept-Building\r\n- Longer introduction justified: 800-1,000 words\r\n- Motivate why existing concepts fail\r\n- Introduce the new concept with precision\r\n- Include roadmap\r\n\r\n### Synthesis\r\n- Name multiple traditions explicitly\r\n- Preview the integration\r\n- May be longer to set up connections\r\n\r\n### Problem-Driven\r\n- Stakes-led or case-led opening works well\r\n- Policy context can dominate\r\n- Connect to real-world urgency\r\n\r\n## Prohibited Moves\r\n\r\n- **Opening with \"This paper argues...\"**  Too abrupt; establish context first\r\n- **Lengthy literature review in introduction**  Save for Theory section\r\n- **Overpromising**  Don't claim more than findings deliver\r\n- **\"The literature has overlooked...\"**  Without justifying why the overlooking matters\r\n- **Detailed methods in introduction**  Save for Methods section\r\n\r\n## Calibrating Ambition\r\n\r\nMatch introduction claims to conclusion capacity:\r\n\r\n| If you promise... | Conclusion must deliver... |\r\n|-------------------|---------------------------|\r\n| \"Extends theory X\" | Affirmation that X works in new domain |\r\n| \"Introduces concept Y\" | Demonstration that Y does analytic work |\r\n| \"Connects literatures A and B\" | Integration of both in interpretation |\r\n| \"Resolves debate\" | Evidence favoring one position |\r\n\r\n**The coherence rule**: Whatever the introduction promises, the conclusion must keep.\r\n\r\n## Output\r\n\r\nProvide:\r\n1. **Complete introduction draft** (target word count based on cluster)\r\n2. **Annotation** noting:\r\n   - Opening move type used\r\n   - Where the turn/gap occurs\r\n   - Key promises made (for conclusion coherence check)\r\n3. **Concerns or alternatives** if multiple approaches could work\r\n\r\n## When You're Done\r\n\r\nReturn the draft to the orchestrator. **Pause** for user review before proceeding to conclusion drafting.\r\n",
        "plugins/interview-bookends/skills/interview-bookends/phases/phase2-conclusion.md": "# Phase 2: Conclusion Drafting\r\n\r\nYou are drafting the conclusion for a sociology interview article. Your goal is to **close the circuit**resolve the puzzle, claim the contribution, and project significance.\r\n\r\n## Why This Phase Matters\r\n\r\nThe conclusion is where articles deliver on their promises. It transforms specific findings into broader significance. A strong conclusion makes readers remember the article; a weak one leaves them wondering \"so what?\"\r\n\r\n**Critical insight from genre analysis**: Conclusions are substantially longer than introductions (median 1,173 vs. 761 words, ratio 1.67). This is because conclusions must accomplish more rhetorical tasks.\r\n\r\n## The Work Conclusions Must Do\r\n\r\n| Task | Purpose | Typical Length |\r\n|------|---------|----------------|\r\n| **Restate puzzle** | Remind reader of motivating question | 2-3 sentences |\r\n| **Summarize findings** | Recap key discoveries | 1-2 paragraphs |\r\n| **Claim contribution** | State what the article adds | 1 paragraph |\r\n| **Integrate with literature** | Connect to prior work | 1-2 paragraphs |\r\n| **Acknowledge limitations** | Bound the claims | 1 paragraph |\r\n| **Project implications** | Draw out broader meaning | 1-2 paragraphs |\r\n| **Future directions** | Point to next steps | 1 paragraph |\r\n| **Coda** | Resonant closing | 2-4 sentences |\r\n\r\n## Step 1: Choose Your Opening Move\r\n\r\nConclusions typically open with one of three moves:\r\n\r\n### Restatement (71% of corpus)\r\nReturns to the puzzle without explicit \"we found\" language.\r\n\r\n> \"For poor mothers, the reach of child welfare surveillance extends beyond a single agency. This study has examined how fears of CPS reporting shape interactions with surveilling institutions...\"\r\n\r\n### Summary (20%)\r\nOpens with explicit findings recap.\r\n\r\n> \"This article has demonstrated that exit from high-intensity movements operates through multiple distinct mechanisms...\"\r\n\r\n### Integration (9%)\r\nOpens with literature positioning.\r\n\r\n> \"Recent research in cultural and political sociology has emphasized the role of recognition in shaping institutional trust...\"\r\n\r\n**Recommendation**: Restatement is the default. Summary works for complex articles with many findings. Integration works for Synthesis articles.\r\n\r\n## Step 2: Summarize Key Findings\r\n\r\nRecap the main findings efficiently. This is NOT a rewrite of the Findings sectionit's a distillation.\r\n\r\n**Structure options**:\r\n\r\n**Enumerated**:\r\n> \"Three key findings emerge from this analysis. First, [finding 1]. Second, [finding 2]. Third, [finding 3].\"\r\n\r\n**Narrative**:\r\n> \"The analysis reveals that [main claim]. Participants consistently [pattern 1], even as they [pattern 2]. This dynamic was shaped by [mechanism].\"\r\n\r\n**Length**: 150-250 words (1-2 paragraphs)\r\n\r\n**Avoid**: Repeating quote-heavy prose from Findings. The conclusion uses author voice.\r\n\r\n## Step 3: Claim the Contribution\r\n\r\nState explicitly what the article adds to knowledge. Match to cluster type:\r\n\r\n### Gap-Filler\r\n> \"This study contributes to our understanding of [topic] by documenting [empirical insight]. Prior research focused on [X]; we show that [Y].\"\r\n\r\n### Theory-Extension\r\n> \"These findings demonstrate the utility of [framework] for understanding [new domain]. The framework illuminates [specific insight] that prior approaches missed.\"\r\n\r\n### Concept-Building\r\n> \"The concept of [new term] provides analytic leverage for understanding [phenomenon]. Unlike [existing concepts], it captures [distinctive feature].\"\r\n\r\n### Synthesis\r\n> \"By connecting [tradition A] and [tradition B], we reveal [integrated insight]. What appeared as separate phenomena are better understood as [unified process].\"\r\n\r\n### Problem-Driven\r\n> \"This analysis advances debates over [topic] by showing [resolution]. The evidence suggests that [position] is more accurate, but [nuance].\"\r\n\r\n## Step 4: Integrate with Prior Literature\r\n\r\nConnect your findings to existing scholarship. This demonstrates you understand the field and positions your contribution within it.\r\n\r\n**Approaches**:\r\n\r\n**Confirming prior work**:\r\n> \"These findings align with [Author]'s argument that [claim]. We extend this insight by showing [extension].\"\r\n\r\n**Challenging prior work**:\r\n> \"Contrary to [Author]'s emphasis on [X], we find that [Y] plays a more central role. This suggests [implication].\"\r\n\r\n**Synthesizing**:\r\n> \"Our findings bridge [Scholar A]'s focus on [X] with [Scholar B]'s attention to [Y], revealing how [connection].\"\r\n\r\n**Length**: 1-2 paragraphs, including citations\r\n\r\n## Step 5: Acknowledge Limitations\r\n\r\nLimitations are expected (present in 69% of conclusions). Handle them confidently, not apologetically.\r\n\r\n**What to acknowledge**:\r\n- Sample characteristics (size, selection, demographics)\r\n- Geographic/temporal scope\r\n- Methodological constraints\r\n- What you can and cannot claim causally\r\n\r\n**What NOT to do**:\r\n- Endless hedging\r\n- Undermining your own contribution\r\n- Listing every conceivable limitation\r\n- Ending the conclusion with limitations\r\n\r\n**Example**:\r\n> \"This study has limitations. Our sample of [N] [population] in [location] may not generalize to [other contexts]. We cannot establish causal ordering between [X and Y]. Future research with [different design] could address these constraints.\"\r\n\r\n**Length**: 1 paragraph, ~100-150 words\r\n\r\n## Step 6: Project Implications\r\n\r\nDraw out what your findings mean beyond the immediate study. Options include:\r\n\r\n**Theoretical implications**:\r\n> \"These findings suggest that theories of [topic] should attend more carefully to [factor]. The [mechanism] we identify may operate in other contexts where [conditions].\"\r\n\r\n**Policy implications**:\r\n> \"For practitioners, these findings suggest [recommendation]. Current policies that [X] may inadvertently [Y].\"\r\n\r\n**Broader significance**:\r\n> \"More broadly, this study illuminates [larger phenomenon]. As [trend] continues, understanding [mechanism] becomes increasingly important.\"\r\n\r\n**Length**: 1-2 paragraphs\r\n\r\n## Step 7: Point to Future Directions\r\n\r\nIdentify productive next steps. This is NOT a limitations dumpit's a research agenda.\r\n\r\n**Strong future directions**:\r\n- Build on your contribution (\"Future research might examine whether [mechanism] operates similarly in [new context]\")\r\n- Address what you couldn't (\"Longitudinal designs could establish [causal claim]\")\r\n- Extend the framework (\"The concept of [X] could be applied to [Y]\")\r\n\r\n**Length**: 1 paragraph\r\n\r\n## Step 8: Craft the Callback and Coda\r\n\r\n**The callback** returns to the introductionsame themes, vocabulary, or images. This is REQUIRED (100% of articles have callbacks).\r\n\r\n**Callback strategies**:\r\n- Return to opening case or image\r\n- Echo key vocabulary from introduction\r\n- Answer the question posed in opening (if question-led)\r\n- Complete the circuit explicitly\r\n\r\n**The coda** provides resonant closure. Options:\r\n\r\n**Clinching quote**:\r\n> As Maria put it, \"It's not that we don't trust themit's that we can't afford to.\"\r\n\r\n**Mic-drop sentence**:\r\n> \"What the literature calls 'exit' may be better understood as dispersal: the movement identity spreading into the broader world, carried by individuals who remain, in some fundamental sense, what the movement made them.\"\r\n\r\n**Return to stakes**:\r\n> \"Understanding these dynamics is increasingly urgent as [trend] reshapes [domain].\"\r\n\r\n**Avoid ending with**:\r\n- Limitations (too weak)\r\n- Generic \"more research is needed\"\r\n- Fading repetition of findings\r\n\r\n## Cluster-Specific Adjustments\r\n\r\n### Gap-Filler\r\n- Longer conclusion (target 1,200-1,400 words)\r\n- Accumulate significance from modest claims\r\n- Emphasize implications to build contribution\r\n\r\n### Theory-Extension\r\n- Moderate length (1,000-1,200 words)\r\n- Framework affirmation central\r\n- May extend framework to additional domains\r\n\r\n### Concept-Building\r\n- Can be shorter if concept proven in Findings (~800-900 words)\r\n- Consolidate concept's utility\r\n- Define scope of concept applicability\r\n\r\n### Synthesis\r\n- Must return to each tradition named in intro\r\n- No deflationintegration must be delivered\r\n- Often longer due to multi-tradition integration\r\n\r\n### Problem-Driven\r\n- May escalate to broader policy implications\r\n- Stakes-heavy closing\r\n- Connect back to real-world urgency\r\n\r\n## Output\r\n\r\nProvide:\r\n1. **Complete conclusion draft** (target word count based on cluster)\r\n2. **Annotation** noting:\r\n   - Opening move used\r\n   - Where callback occurs\r\n   - Key promises from intro that are addressed\r\n   - Coherence type (Parallel, Escalators, Bookends)\r\n3. **Coherence check** against introduction promises\r\n\r\n## When You're Done\r\n\r\nReturn the draft to the orchestrator for coherence review in Phase 3.\r\n",
        "plugins/interview-bookends/skills/interview-bookends/phases/phase3-coherence.md": "# Phase 3: Coherence Check\r\n\r\nYou are reviewing the introduction and conclusion together to ensure they work as a unified rhetorical system.\r\n\r\n## Why This Phase Matters\r\n\r\nThe introduction makes promises; the conclusion must keep them. Articles fail when these sections are written in isolation. This phase ensures the **circuit is closed**that what opens in the introduction resolves in the conclusion.\r\n\r\n## The Coherence Principle\r\n\r\nFrom genre analysis of 80 sociology articles:\r\n\r\n| Coherence Type | Frequency | Description | Assessment |\r\n|----------------|-----------|-------------|------------|\r\n| **Parallel** | 66% | Conclusion delivers what intro promised | Normative |\r\n| **Escalators** | 20% | Conclusion exceeds intro scope | Acceptable |\r\n| **Bookends** | 8% | Strong mirror structure | Effective |\r\n| **Deflators** | 6% | Conclusion falls short | Problematic |\r\n\r\n**The goal**: Achieve Parallel, Escalators, or Bookends. Avoid Deflators.\r\n\r\n## Step 1: Extract Promises from Introduction\r\n\r\nRead the introduction and identify every promise made:\r\n\r\n| Promise Type | Example | Conclusion Must Deliver |\r\n|--------------|---------|------------------------|\r\n| **Empirical claim** | \"We examine how X experience Y\" | Evidence about X's experience of Y |\r\n| **Framework application** | \"Using Z framework...\" | Findings interpreted through Z |\r\n| **New concept** | \"We introduce the concept of Q\" | Q demonstrated analytically |\r\n| **Synthesis claim** | \"Connecting A and B literatures\" | Both A and B integrated |\r\n| **Debate resolution** | \"Adjudicating between X and Y\" | Evidence favoring one position |\r\n| **Scope claim** | \"Among [population] in [context]\" | Findings bounded to that scope |\r\n\r\n**Create a promise inventory**:\r\n1. List each promise\r\n2. Note where in conclusion it's addressed\r\n3. Flag any unaddressed promises\r\n\r\n## Step 2: Verify Vocabulary Echoes\r\n\r\nKey terms from the introduction should reappear in the conclusion. Check for:\r\n\r\n**Concept vocabulary**:\r\n- Does the same terminology appear? (e.g., \"recognition,\" \"legal cynicism\")\r\n- Are concepts used consistently?\r\n\r\n**Phenomenon vocabulary**:\r\n- Same population descriptors\r\n- Same setting references\r\n\r\n**Argument vocabulary**:\r\n- Same mechanism names\r\n- Same contribution framing\r\n\r\n**Red flag**: If major terms from introduction don't appear in conclusion, the callback may be weak.\r\n\r\n## Step 3: Assess Callback Quality\r\n\r\nThe callback is REQUIRED. Evaluate it on:\r\n\r\n### Presence\r\n- Does the conclusion explicitly return to introduction themes?\r\n- Is there a moment where the reader recognizes \"this is what we started with\"?\r\n\r\n### Strength\r\n| Weak Callback | Strong Callback |\r\n|---------------|-----------------|\r\n| Vague thematic similarity | Explicit vocabulary echo |\r\n| Different framing | Same puzzle, now resolved |\r\n| New angle introduced | Completing the circuit |\r\n\r\n### Placement\r\n- Callbacks typically appear in the **opening paragraph** of conclusion (restatement)\r\n- And in the **closing paragraph** (coda)\r\n- Double callbacks (opening + closing) create Bookends effect\r\n\r\n## Step 4: Check Promise-Delivery Alignment\r\n\r\nFor each promise, assess delivery:\r\n\r\n| Promise | Delivered? | Assessment |\r\n|---------|------------|------------|\r\n| [Promise 1] | Yes/Partial/No | Notes |\r\n| [Promise 2] | Yes/Partial/No | Notes |\r\n| [Promise 3] | Yes/Partial/No | Notes |\r\n\r\n### If All Promises Delivered  Parallel (good)\r\nThe article does what it says. This is normative and appropriate.\r\n\r\n### If Conclusion Exceeds Promises  Escalators (acceptable)\r\nThe findings turned out bigger than anticipated. Frame as discovery, not overclaiming.\r\n\r\n**Example escalation language**:\r\n> \"Beyond our initial focus on [X], the data revealed [broader pattern]...\"\r\n> \"These findings have implications beyond [original scope], suggesting...\"\r\n\r\n### If Promises Underdelivered  Deflators (problematic)\r\nOptions:\r\n1. **Revise introduction** to make more modest claims\r\n2. **Strengthen conclusion** to deliver on promises\r\n3. **Acknowledge explicitly** if scope narrowed during research\r\n\r\n## Step 5: Assess Coherence Type\r\n\r\nBased on your analysis, classify the draft:\r\n\r\n### Parallel\r\n- All promises delivered\r\n- Vocabulary echoes present\r\n- Callback present and effective\r\n- **Verdict**: Ready for polish\r\n\r\n### Escalators\r\n- Promises delivered plus additional claims\r\n- Conclusion makes broader contributions\r\n- Escalation is justified and grounded\r\n- **Verdict**: Acceptable if escalation is earned\r\n\r\n### Bookends\r\n- Strong mirror structure\r\n- Opening image/question returns in closing\r\n- Symmetrical feel\r\n- **Verdict**: Highly effective when achieved\r\n\r\n### Deflators\r\n- One or more promises unaddressed\r\n- Conclusion hedges relative to introduction\r\n- Contribution feels smaller than promised\r\n- **Verdict**: Requires revision\r\n\r\n## Step 6: Specific Checks\r\n\r\n### Contribution Match\r\n| Introduction Claims | Conclusion Claims | Match? |\r\n|--------------------|--------------------|--------|\r\n| [Contribution type] | [Contribution type] | Yes/No |\r\n\r\n### Scope Match\r\n| Introduction Scope | Conclusion Scope | Match? |\r\n|-------------------|------------------|--------|\r\n| [Population, context] | [Population, context] | Yes/No |\r\n\r\n### Theory Match\r\n| Introduction Frameworks | Conclusion Frameworks | Match? |\r\n|------------------------|----------------------|--------|\r\n| [Theories named] | [Theories integrated] | Yes/No |\r\n\r\n## Revision Recommendations\r\n\r\nIf coherence issues found, provide specific guidance:\r\n\r\n### For Weak Callbacks\r\n- Add explicit return to introduction vocabulary\r\n- Echo opening image in closing\r\n- Reference the original puzzle directly\r\n\r\n### For Unfulfilled Promises\r\n- Either deliver in conclusion or revise introduction\r\n- Don't leave promises hanging\r\n- Acknowledge any scope changes explicitly\r\n\r\n### For Deflation\r\n- Strengthen conclusion claims if evidence supports\r\n- Or moderate introduction claims if necessary\r\n- Match ambition to capacity\r\n\r\n### For Unearned Escalation\r\n- Ground broader claims in evidence\r\n- Use hedging language if appropriate\r\n- Ensure escalation is warranted\r\n\r\n## Output\r\n\r\nProvide a **Coherence Memo** with:\r\n\r\n1. **Promise Inventory**: List of all introduction promises\r\n2. **Delivery Assessment**: Which promises delivered, which not\r\n3. **Callback Evaluation**: Presence, strength, placement\r\n4. **Coherence Classification**: Parallel/Escalators/Bookends/Deflators\r\n5. **Revision Recommendations**: Specific changes needed\r\n6. **Final Assessment**: Ready for submission or needs revision\r\n\r\n## Coherence Checklist\r\n\r\nBefore finalizing, confirm:\r\n\r\n- [ ] Every major promise in introduction is addressed in conclusion\r\n- [ ] Key vocabulary from introduction appears in conclusion\r\n- [ ] Callback is present and recognizable\r\n- [ ] Conclusion scope matches introduction scope\r\n- [ ] All frameworks/theories named in intro are integrated in conclusion\r\n- [ ] Contribution type is consistent across sections\r\n- [ ] No deflation (claims don't shrink)\r\n- [ ] Any escalation is earned and grounded\r\n\r\n## When You're Done\r\n\r\nReturn the coherence memo to the orchestrator with:\r\n1. Overall assessment (Pass/Revise)\r\n2. Specific revision instructions if needed\r\n3. Confidence level in current draft\r\n\r\nIf revisions needed, cycle back to Phase 1 or 2 as appropriate.\r\n",
        "plugins/interview-bookends/skills/interview-bookends/techniques/opening-moves.md": "# Opening Move Types\r\n\r\nThis guide details the five types of opening moves used in sociology interview article introductions, with frequencies from genre analysis.\r\n\r\n## Overview\r\n\r\n| Opening Move | Frequency | Best For |\r\n|--------------|-----------|----------|\r\n| Phenomenon-led | 74% | Gap-Filler, Concept-Building, Synthesis |\r\n| Stakes-led | 11% | Problem-Driven, Gap-Filler |\r\n| Case-led | 8% | Problem-Driven, Synthesis |\r\n| Theory-led | 6% | Theory-Extension |\r\n| Question-led | 1% | Rare; use sparingly |\r\n\r\n---\r\n\r\n## 1. Phenomenon-Led Opening (74%)\r\n\r\nOpens with an empirical statement about the world. The most common approach.\r\n\r\n### What It Does\r\n- Grounds the article in observable reality\r\n- Establishes the phenomenon before the puzzle\r\n- Shows rather than tells\r\n- Signals empirical focus\r\n\r\n### Structure\r\n1. State the phenomenon\r\n2. Provide context/prevalence\r\n3. Move toward the puzzle\r\n\r\n### Examples\r\n\r\n**Simple statement**:\r\n> \"Neighborhood is an important theme in research on prisoner reentry and desistance from offending, but scholars and practitioners typically use this to refer to a single location.\"\r\n\r\n**Statistical grounding**:\r\n> \"Between 50 and 67 percent of U.S. households have a pet.\"\r\n\r\n**Population focus**:\r\n> \"Tasked with protecting children from abuse and neglect, U.S. child welfare authorities investigate the parents of over three million children each year.\"\r\n\r\n**Trend statement**:\r\n> \"The self-contained, self-reliant nuclear family persists as the archetypal family form, despite its declining prevalence.\"\r\n\r\n### When to Use\r\n- Default choice for most articles\r\n- When the phenomenon itself is inherently interesting\r\n- When you want to establish empirical grounding before theory\r\n- When readers need to see the puzzle in the world before the literature\r\n\r\n### When to Avoid\r\n- When the framework IS the contribution (use theory-led)\r\n- When stakes need immediate emphasis (use stakes-led)\r\n\r\n---\r\n\r\n## 2. Stakes-Led Opening (11%)\r\n\r\nOpens with an importance claim about why the topic matters.\r\n\r\n### What It Does\r\n- Creates immediate urgency\r\n- Signals policy relevance\r\n- Engages readers who care about real-world impact\r\n- Establishes significance before details\r\n\r\n### Structure\r\n1. State why this matters\r\n2. Contextualize the stakes\r\n3. Move to specific focus\r\n\r\n### Examples\r\n\r\n**Scale emphasis**:\r\n> \"The unprecedented growth in rates of U.S. incarceration since the early 1970s has prompted scholars to rethink basic understandings of punishment, inequality, and the state.\"\r\n\r\n**Policy urgency**:\r\n> \"As global unaccompanied minor migration persists and expands, understanding how these youth navigate incorporation has never been more urgent.\"\r\n\r\n**Social significance**:\r\n> \"Decades of research have made the case that neighborhoods are important contexts for children's development, shaping their educational achievement, health, and life chances.\"\r\n\r\n### When to Use\r\n- Problem-Driven articles with policy implications\r\n- When the \"so what?\" needs emphasis\r\n- When reader motivation might flag without urgency\r\n- When connecting to current events or policy debates\r\n\r\n### When to Avoid\r\n- When stakes feel manufactured or overblown\r\n- When the phenomenon should speak for itself\r\n- When you risk seeming self-important\r\n\r\n---\r\n\r\n## 3. Case-Led Opening (8%)\r\n\r\nOpens with a concrete case, scene, or vignette.\r\n\r\n### What It Does\r\n- Creates narrative engagement\r\n- Grounds abstractions in specifics\r\n- Signals qualitative, context-rich approach\r\n- Makes the phenomenon vivid\r\n\r\n### Structure\r\n1. Describe specific case/scene\r\n2. Zoom out to broader pattern\r\n3. Move to puzzle/question\r\n\r\n### Examples\r\n\r\n**Policy case**:\r\n> \"In Cleveland, Ohio, the federal Department of Justice declared that the city's police department engaged in a 'pattern or practice' of using excessive force.\"\r\n\r\n**Historical moment**:\r\n> \"On Tuesday, November 27, 2018, President Emmanuel Macron stood outside of the Elysee Palace delivering a speech on 'ecological transition.'\"\r\n\r\n**Composite vignette**:\r\n> \"When Maria arrived at the welfare office, she knew the routine: arrive early, bring all documents, expect to wait.\"\r\n\r\n### When to Use\r\n- Problem-Driven articles grounded in specific context\r\n- When the case IS the entry point to broader analysis\r\n- When narrative engagement matters\r\n- When the case exemplifies the puzzle\r\n\r\n### When to Avoid\r\n- When the case might seem idiosyncratic\r\n- When you can't connect it to broader patterns\r\n- When abstractions need emphasis over particulars\r\n\r\n---\r\n\r\n## 4. Theory-Led Opening (6%)\r\n\r\nOpens with a theoretical framework or concept.\r\n\r\n### What It Does\r\n- Signals theoretical contribution\r\n- Establishes framework before application\r\n- Appeals to theory-oriented readers\r\n- Centers the framework as the article's backbone\r\n\r\n### Structure\r\n1. Introduce the framework/concept\r\n2. Explain its utility\r\n3. Set up the extension or application\r\n\r\n### Examples\r\n\r\n**Named concept**:\r\n> \"The concept of the 'glass cliff' refers to the tendency for women and racial/ethnic minorities to be appointed to leadership positions during times of crisis.\"\r\n\r\n**Theoretical tradition**:\r\n> \"Recognition theory suggests that social identities are shaped through intersubjective processes of acknowledgment and validation.\"\r\n\r\n**Framework introduction**:\r\n> \"Bourdieu's concept of cultural capital has proven remarkably productive for understanding educational inequality.\"\r\n\r\n### When to Use\r\n- Theory-Extension articles (30% of this cluster uses it)\r\n- When the framework is well-known and central\r\n- When the contribution IS applying the framework\r\n- When readers expect theoretical framing\r\n\r\n### When to Avoid\r\n- When the framework is obscure (define later)\r\n- When empirical grounding is needed first\r\n- When the phenomenon is more compelling than the theory\r\n\r\n---\r\n\r\n## 5. Question-Led Opening (1%)\r\n\r\nOpens with a direct question.\r\n\r\n### What It Does\r\n- Poses the puzzle immediately\r\n- Signals inquiry-driven approach\r\n- Can create engagement if question is compelling\r\n- Mirrors how research actually begins\r\n\r\n### Structure\r\n1. Ask the question\r\n2. Explain why it matters\r\n3. Preview the answer\r\n\r\n### Examples\r\n\r\n**Paradox question**:\r\n> \"How do people make sense of their reliance on unjust institutions?\"\r\n\r\n**Process question**:\r\n> \"What happens when social movement organizations must choose between their ideals and their survival?\"\r\n\r\n**Puzzle question**:\r\n> \"Why do residents of high-crime neighborhoods call the police despite widespread distrust?\"\r\n\r\n### Why It's Rare (1%)\r\n- Can feel rhetorically performative\r\n- Implies the answer is unknown (requires justification)\r\n- \"Showing\" is usually more persuasive than \"asking\"\r\n- Questions can feel like a gimmick\r\n\r\n### When to Use\r\n- When the question is genuinely compelling\r\n- When the paradox is the central puzzle\r\n- When the question format captures something assertions don't\r\n- Synthesis articles where the question bridges literatures\r\n\r\n### When to Avoid\r\n- As a default (it's overused in student writing)\r\n- When the question isn't actually puzzling\r\n- When showing would be more effective than asking\r\n\r\n---\r\n\r\n## Choosing Your Opening Move\r\n\r\n### Decision Tree\r\n\r\n1. **Is the framework central to contribution?**\r\n   - Yes  Consider Theory-Led\r\n   - No  Continue\r\n\r\n2. **Is there immediate policy/social urgency?**\r\n   - Yes  Consider Stakes-Led\r\n   - No  Continue\r\n\r\n3. **Is a specific case the entry point?**\r\n   - Yes  Consider Case-Led\r\n   - No  Continue\r\n\r\n4. **Is the phenomenon inherently interesting?**\r\n   - Yes  Use Phenomenon-Led (default)\r\n   - Uncertain  Use Phenomenon-Led\r\n\r\n5. **Is there a genuinely compelling paradox?**\r\n   - Yes  Consider Question-Led (rare)\r\n   - No  Don't force it\r\n\r\n### Cluster Recommendations\r\n\r\n| Cluster | Primary | Secondary |\r\n|---------|---------|-----------|\r\n| Gap-Filler | Phenomenon | Stakes |\r\n| Theory-Extension | Theory (30%) | Phenomenon (50%) |\r\n| Concept-Building | Phenomenon | Stakes |\r\n| Synthesis | Phenomenon | Case |\r\n| Problem-Driven | Stakes (25%) | Case (17%) |\r\n",
        "plugins/interview-bookends/skills/interview-bookends/techniques/signature-phrases.md": "# Signature Phrases for Introductions and Conclusions\r\n\r\nCommon phrases and formulations from sociology interview articles, organized by function.\r\n\r\n---\r\n\r\n## Introduction Phrases\r\n\r\n### Opening Moves\r\n\r\n**Phenomenon-led**:\r\n- \"[Topic] has become increasingly central to...\"\r\n- \"[N] percent of [population]...\"\r\n- \"In recent decades, [trend] has...\"\r\n- \"[Population] face a distinctive challenge...\"\r\n- \"The [phenomenon] remains a persistent feature of...\"\r\n\r\n**Stakes-led**:\r\n- \"Few social problems have generated as much...\"\r\n- \"Understanding [topic] has never been more urgent...\"\r\n- \"The stakes of [phenomenon] are high...\"\r\n- \"As [trend] accelerates, scholars and practitioners alike...\"\r\n- \"[Topic] shapes the life chances of millions...\"\r\n\r\n**Case-led**:\r\n- \"In [location], [event] unfolded...\"\r\n- \"On [date], [actor] announced...\"\r\n- \"When [name] arrived at...\"\r\n- \"Consider the case of [example]...\"\r\n\r\n**Theory-led**:\r\n- \"The concept of [X] refers to...\"\r\n- \"[Author]'s theory of [X] suggests...\"\r\n- \"Drawing on [tradition], scholars have shown...\"\r\n- \"[Framework] offers a lens for understanding...\"\r\n\r\n### Establishing the Gap\r\n\r\n**Gap identification**:\r\n- \"Yet we know remarkably little about...\"\r\n- \"Despite extensive research on [X], scholars have paid less attention to...\"\r\n- \"Prior research has focused primarily on [X], leaving [Y] unexplored...\"\r\n- \"What remains unclear is...\"\r\n- \"Existing studies have not examined...\"\r\n- \"While [X] is well-documented, [Y] remains understudied...\"\r\n\r\n**Puzzle framing**:\r\n- \"This raises a puzzle: why do...\"\r\n- \"This pattern is surprising given...\"\r\n- \"If [expected], why do we observe [actual]?\"\r\n- \"The persistence of [X] despite [Y] demands explanation...\"\r\n\r\n**Limitation of prior work**:\r\n- \"Previous research has been limited by...\"\r\n- \"Existing accounts leave unresolved...\"\r\n- \"While valuable, these studies cannot explain...\"\r\n- \"The literature has not adequately addressed...\"\r\n\r\n### Previewing the Study\r\n\r\n**Data introduction**:\r\n- \"Drawing on [N] in-depth interviews with...\"\r\n- \"This article examines [topic] through...\"\r\n- \"Based on [N] semi-structured interviews conducted between...\"\r\n- \"Using qualitative data from...\"\r\n- \"To address these questions, we conducted...\"\r\n\r\n**Argument preview**:\r\n- \"We argue that...\"\r\n- \"This article demonstrates that...\"\r\n- \"We find that...\"\r\n- \"Our analysis reveals...\"\r\n- \"The evidence suggests that...\"\r\n\r\n**Contribution claims**:\r\n- \"This study makes [N] contributions...\"\r\n- \"We extend [theory] by showing...\"\r\n- \"We contribute to [literature] by...\"\r\n- \"The article advances understanding of...\"\r\n- \"We introduce the concept of [X] to capture...\"\r\n\r\n### Roadmap Phrases\r\n\r\n- \"The article proceeds as follows...\"\r\n- \"We begin by [reviewing/describing]...\"\r\n- \"We first [X], then [Y], and conclude by [Z]...\"\r\n- \"In what follows, we...\"\r\n- \"The remainder of this article...\"\r\n\r\n---\r\n\r\n## Conclusion Phrases\r\n\r\n### Opening Moves\r\n\r\n**Restatement**:\r\n- \"This study has examined...\"\r\n- \"[Topic] shapes [population]'s experiences in ways that...\"\r\n- \"The [phenomenon] we have described...\"\r\n- \"For [population], [key finding]...\"\r\n\r\n**Summary**:\r\n- \"Three key findings emerge from this analysis...\"\r\n- \"This article has demonstrated that...\"\r\n- \"Our analysis reveals that...\"\r\n- \"The evidence presented here shows...\"\r\n\r\n**Integration**:\r\n- \"Recent research on [topic] has emphasized...\"\r\n- \"Scholars of [field] have long recognized...\"\r\n- \"Building on [Author]'s insights...\"\r\n\r\n### Contribution Claims\r\n\r\n**Extending theory**:\r\n- \"These findings extend [theory] by showing...\"\r\n- \"We have demonstrated the utility of [framework] for...\"\r\n- \"Our analysis confirms and extends [Author]'s argument...\"\r\n\r\n**Building concepts**:\r\n- \"The concept of [X] captures...\"\r\n- \"We introduced the term [X] to describe...\"\r\n- \"[Concept] provides analytic leverage for...\"\r\n- \"Unlike [existing term], [new term] emphasizes...\"\r\n\r\n**Filling gaps**:\r\n- \"This study addresses a gap in...\"\r\n- \"We have shown that [previously unknown pattern]...\"\r\n- \"The findings illuminate [understudied phenomenon]...\"\r\n\r\n**Synthesizing**:\r\n- \"By connecting [A] and [B], we reveal...\"\r\n- \"The integration of [traditions] shows...\"\r\n- \"Bringing together [literatures], this analysis...\"\r\n\r\n### Limitations\r\n\r\n**Scope limitations**:\r\n- \"This study has limitations...\"\r\n- \"Our sample of [N] [population] in [location] may not generalize to...\"\r\n- \"We cannot establish causal ordering between...\"\r\n- \"The [context-specific] nature of our data means...\"\r\n- \"We make no claim that this is the only way...\"\r\n\r\n**Methodological limitations**:\r\n- \"The cross-sectional nature of our data...\"\r\n- \"Future research with [different design] could...\"\r\n- \"Our reliance on [method] means we cannot...\"\r\n\r\n### Future Directions\r\n\r\n- \"Future research might examine...\"\r\n- \"An important next step would be...\"\r\n- \"Scholars should investigate whether...\"\r\n- \"Longitudinal research could establish...\"\r\n- \"Comparative work across [contexts] would...\"\r\n- \"These findings raise new questions about...\"\r\n\r\n### Implications\r\n\r\n**Theoretical implications**:\r\n- \"These findings suggest that theories of [X] should...\"\r\n- \"For scholars of [field], our analysis implies...\"\r\n- \"The [mechanism] we identify may operate in...\"\r\n\r\n**Policy implications**:\r\n- \"For practitioners, these findings suggest...\"\r\n- \"Policymakers should consider...\"\r\n- \"Current approaches to [X] may inadvertently...\"\r\n- \"These insights have practical implications for...\"\r\n\r\n**Broader significance**:\r\n- \"More broadly, this study illuminates...\"\r\n- \"Understanding [phenomenon] becomes increasingly important as...\"\r\n- \"As [trend] continues, [insight] matters because...\"\r\n\r\n### Callback and Coda\r\n\r\n**Callback phrases**:\r\n- \"Returning to [opening theme]...\"\r\n- \"Recalling our opening observation about...\"\r\n- \"The [image/case] with which we began...\"\r\n- \"What we initially characterized as [X] is better understood as...\"\r\n\r\n**Coda phrases**:\r\n- \"Ultimately, [summary statement]...\"\r\n- \"In the end, [population]'s [experiences] reveal...\"\r\n- \"As [participant] put it, '[quote]'\"\r\n- \"What the literature calls [X] may be better understood as...\"\r\n- \"[Final resonant statement].\"\r\n\r\n---\r\n\r\n## Phrases to Avoid\r\n\r\n### In Introductions\r\n- \"This paper argues that...\" (too abrupt as opener)\r\n- \"In this paper, I will...\" (save for roadmap)\r\n- \"The literature has overlooked...\" (without justification)\r\n- \"Little is known about...\" (unless truly true)\r\n- \"No one has studied...\" (probably false)\r\n\r\n### In Conclusions\r\n- \"In conclusion...\" (redundant; you're in the conclusion)\r\n- \"To sum up...\" (informal)\r\n- \"More research is needed\" (generic; be specific)\r\n- \"This study proves that...\" (overclaiming for qualitative work)\r\n- \"Future research should...\" (weaker than \"might examine\")\r\n\r\n### General\r\n- \"It is interesting to note that...\" (show, don't tell)\r\n- \"Obviously...\" / \"Clearly...\" (if obvious, don't state)\r\n- \"The data revealed that...\" (passive; prefer active)\r\n- Excessive hedging: \"might,\" \"perhaps,\" \"seemingly,\" \"possibly\"\r\n",
        "plugins/interview-writeup/.claude-plugin/plugin.json": "{\n  \"name\": \"interview-writeup\",\n  \"description\": \"Write-up support for qualitative interview research. Guides methods drafting, findings structure, quote use, and revision with quality checks.\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Neal Caren\"\n  },\n  \"license\": \"MIT\",\n  \"keywords\": [\n    \"qualitative\",\n    \"interviews\",\n    \"writing\",\n    \"methods\",\n    \"sociology\"\n  ],\n  \"skills\": \"./skills/\"\n}\n",
        "plugins/interview-writeup/skills/interview-writeup/SKILL.md": "---\nname: interview-writeup\ndescription: Write-up support for qualitative interview research in sociology. Guides methods and findings drafting with emphasis on argument-driven narrative, not formulaic quote display.\n---\n\n# Interview Write-Up\n\nYou help sociologists write up qualitative interview research for journal articles and reports. Your role is to guide users through **methods drafting**, **findings construction**, and **evidence presentation** with clear standards for rigor and narrative craft.\n\n## Connection to interview-analyst\n\nThis skill pairs with **interview-analyst** as a one-two punch:\n\n| Skill | Purpose | Key Output |\n|-------|---------|------------|\n| **interview-analyst** | Analyzes interview data, builds codes, identifies patterns | `quote-database.md` with quotes organized by finding, anchors/echoes identified |\n| **interview-writeup** | Drafts methods and findings sections | Publication-ready prose |\n\nIf users ran interview-analyst first, request their `quote-database.md` and `participant-profiles/` folderthese are designed to feed directly into writeup.\n\n## When to Use This Skill\n\nUse this skill when users want to:\n- Draft or revise a methods section for interview-based research\n- Structure a findings section and present qualitative evidence\n- Improve quote selection, integration, and analytical framing\n- Transform a theme-catalog draft into argument-driven narrative\n\n## Core Principles\n\n1. **Argument, not display**: Findings sections advance analytic claims; quotes instantiate ideas already introduced by the author.\n2. **Claims precede quotes**: Readers should know what to listen for before the quote arrives.\n3. **Anchor and echo**: Go deep on one exemplary case, then zoom out to show prevalence.\n4. **Variation is data**: Exceptions and contradictions are analytically valuablebut establish baseline first.\n5. **Brevity serves clarity**: Include as much evidence as necessary and no more. If one quote will do, don't use three.\n6. **Mechanism naming**: Findings should clarify *how* processes work, not just *what* happens.\n\n## Quality Indicators\n\nEvaluate writing against these markers:\n\n- **Analytical confidence**: Patterns stated assertively; mechanisms named by the author, not discovered in quotes\n- **Narrative craft**: Varied quote integration; anchor-echo pacing; smooth transitions\n- **Grounded abstraction**: Sociological concepts tied to concrete, specific evidence\n- **Strategic depth**: Anchor cases developed fully; echoes efficient\n- **Appropriate scope**: Claims bounded to sample; prevalence indicated throughout\n\n## Technique Guides\n\nThe skill includes detailed reference guides:\n\n| Guide | Purpose |\n|-------|---------|\n| `techniques/macro-structure.md` | Choosing archetypes (Mechanism List, Comparative, Process); Roadmap + Pillars model; section organization |\n| `techniques/prose-craft.md` | Quote integration techniques; Anchor-Echo pattern; pacing; attribution; transitions |\n| `techniques/rubric.md` | The 8-step process for drafting each subsection |\n| `techniques/participant-management.md` | Minimizing recurrence; recall tags; when participants should (and shouldn't) reappear |\n\n## Workflow Phases\n\n### Phase 0: Intake & Scope\n**Goal**: Confirm required inputs and define the writing task.\n- Gather required materials (participant table, quotes, main argument)\n- Clarify whether the user needs methods, findings, or both\n- Identify the main argument and 3-4 core findings\n\n**Guide**: `phases/phase0-intake.md`\n\n> **Pause**: Confirm scope and inputs before drafting.\n\n---\n\n### Phase 1: Methods Section\n**Goal**: Draft or revise a transparent, defensible methods section.\n- Case selection, sampling, recruitment, sample size justification\n- Interview protocol and analysis approach\n- Positionality (when appropriate)\n\n**Guide**: `phases/phase1-methods.md`\n\n> **Pause**: Review the methods draft for completeness and clarity.\n\n---\n\n### Phase 2: Findings Section\n**Goal**: Structure findings as argument-driven narrative.\n- Choose an archetype (Mechanism List, Comparative, or Process)\n- Write the Roadmap introduction summarizing the entire argument\n- Draft each subsection following the 8-step rubric\n- Use the Anchor-Echo pattern for evidence presentation\n- Craft theoretical headings that name mechanisms\n\n**Guides**:\n- `phases/phase2-findings.md` (main workflow)\n- `techniques/macro-structure.md` (organization)\n- `techniques/prose-craft.md` (quote integration)\n- `techniques/rubric.md` (subsection drafting)\n\n> **Pause**: Confirm findings structure and evidence selection.\n\n---\n\n### Phase 3: Revision & Quality Check\n**Goal**: Transform competent draft into compelling argument.\n- Check argument structure (roadmap, claims before quotes)\n- Verify Anchor-Echo pattern in each subsection\n- Fix formulaic quote integration\n- Ensure appropriate voice balance and confidence\n- Catch prohibited moves\n\n**Guide**: `phases/phase3-revision.md`\n\n---\n\n## Prohibited Moves\n\nThe skill explicitly trains against common problems:\n\n- Starting subsections with quotes\n- Listing themes without argument\n- Using quotes without interpretation\n- Stacking quotes back-to-back\n- Hedging empirical patterns (\"might suggest\")\n- Writing descriptive subheadings (\"Findings,\" \"Race\")\n- Letting quotes introduce analytic novelty\n- Treating all quotes with equal depth (no anchor)\n- Starting with variation before baseline\n\n## Output Expectations\n\nProvide the user with:\n- A draft or revised **methods section** (if requested)\n- A structured **findings section** following the chosen archetype\n- A **quality check memo** assessing strengths, gaps, and remaining issues\n\n## Invoking Phase Agents\n\nUse the Task tool for each phase:\n\n```\nTask: Phase 2 Findings Drafting\nsubagent_type: general-purpose\nmodel: opus\nprompt: Read phases/phase2-findings.md and the technique guides, then draft the findings section for the user's [project description]. Follow the 8-step rubric for each subsection. Use the Anchor-Echo pattern.\n```\n\n**Model recommendations**:\n- Phase 0-1 (intake, methods): Sonnet\n- Phase 2 (findings): Opus (requires narrative craft)\n- Phase 3 (revision): Opus (requires editorial judgment)\n",
        "plugins/interview-writeup/skills/interview-writeup/phases/phase0-intake.md": "# Phase 0: Intake & Scope\n\nYou are executing Phase 0 of the interview write-up workflow. Your goal is to confirm required inputs, clarify scope, and plan the writing task.\n\n## Why This Phase Matters\n\nA strong write-up depends on clear claims, structured evidence, and transparent methods. Missing inputs create weak or generic prose.\n\n## Required Inputs\n\nAsk users to provide the following materials before drafting.\n\n### If Coming from interview-analyst\n\nIf the user ran the **interview-analyst** skill first, request these outputs:\n\n| From interview-analyst | Phase | What It Provides |\n|------------------------|-------|------------------|\n| **quote-database.md** | Phase 5 | Quotes organized by finding with luminous exemplars flagged, anchor/echo candidates identified, prevalence noted, full attribution |\n| **participant-profiles/** | Phase 2 | Demographics, trajectories, extended quotes, punchy quotes, in-vivo terms for each participant |\n| **core-argument.md** | Phase 5 | Research question, central argument, contribution |\n| **paper-outline.md** | Phase 5 | Structural outline with findings |\n\nThese files are designed to feed directly into writeup. The quote-database.md is the primary handoffit contains everything needed for the anchor-echo pattern.\n\n### If Starting Fresh (no prior analysis)\n\n| Document | What to Include | Why Needed |\n|----------|-----------------|------------|\n| **Main argument** | 12 sentence statement of the core finding or claim | To recommend structure and maintain focus |\n| **Participant table** | Pseudonyms, key demographics (age, race/ethnicity, gender, class, role), any characteristics relevant to analysis | For accurate, rich attribution in quotes |\n| **Main findings** | 34 findings they want to present, with brief descriptions | To structure the findings section |\n| **Selected quotes** | Key quotes organized by finding, with speaker identified | The evidence to work with |\n\n**Note on quote format**: For each finding, you need:\n- **1-2 extended quotes** (3-6 sentences) suitable for anchor vignettesshowing participant's voice, reasoning, internal logic\n- **3-5 shorter quotes** (1-2 sentences) suitable for echoesshowing prevalence across participants\n- **Full attribution** for each: name, race/ethnicity, gender, age, role/position, and any study-relevant characteristics\n\n### Required for Methods Section\n\n| Document | What to Include | Why Needed |\n|----------|-----------------|------------|\n| **Methods notes** | N, dates, location, target population, sampling strategy, recruitment method, response rate (if known), interview format/duration, recording/transcription approach, analysis method | To draft accurate methods section |\n\n### Highly Useful\n\n| Document | What to Include | Why Needed |\n|----------|-----------------|------------|\n| **Analytical memos** | Notes written during analysis explaining interpretations | To understand their reasoning |\n| **Codebook** | List of codes with definitions | To understand analytical categories |\n| **Draft outline or partial draft** | Current structure or existing text | To build on their work |\n| **Variation/exceptions notes** | Cases that don't fit the main pattern | To address heterogeneity |\n\n### Optional but Helpful\n\n| Document | What to Include | Why Needed |\n|----------|-----------------|------------|\n| **Target journal** | Journal name or sample article | To calibrate length and style |\n| **Literature context** | Key theoretical frameworks or debates | To connect findings to field |\n| **Full transcripts** | Complete interview transcripts | To find additional quotes or check context |\n\n## Sample Formats\n\n### Participant Table\n\n```\n| Pseudonym | Age | Race/Ethnicity | Gender | Occupation | Key Characteristic |\n|-----------|-----|----------------|--------|------------|-------------------|\n| Helen | 63 | Black | Woman | Retired nurse | Cares for disabled son |\n| Vanessa | 34 | Black | Woman | Marketing manager | Recent homebuyer |\n| Marcus | 45 | White | Man | Attorney | Hired lawyer for negotiation |\n```\n\n### Quote Organization\n\n```\n## Finding 1: [Title]\n\n**Claim**: [One sentence stating the finding]\n\n**Quote 1** (Helen):\n\"[Quote text]\"\nContext: [When/why this was said]\n\n**Quote 2** (Marcus):\n\"[Quote text]\"\nContext: [When/why this was said]\n\n**Variation/Exception** (Vanessa):\n\"[Quote showing different pattern]\"\nContext: [Why this case differs]\n```\n\n## Clarify Scope\n\nAsk:\n- Are we working on **methods**, **findings**, or **both**?\n- Is this **drafting from scratch** or **revising** existing text?\n- What is the **target journal** or audience, if any?\n\n## Output\n\nProvide a short intake memo that includes:\n- Missing inputs\n- Agreed scope and deliverables\n- Proposed findings structure (if enough info is available)\n\n## When Youre Done\n\nReturn a summary to the orchestrator with:\n1. Which inputs were provided\n2. Missing inputs to request\n3. Agreed scope (methods/findings/both)\n4. Recommended structure approach if appropriate\n",
        "plugins/interview-writeup/skills/interview-writeup/phases/phase1-methods.md": "# Phase 1: Methods Section\n\nYou are drafting or revising a methods section for interview-based research.\n\n## Why This Phase Matters\n\nReaders need to understand who you studied, how you recruited them, how you collected data, and how you analyzed it. Transparency is central to credibility.\n\n## Required Elements\n\nEnsure the methods section includes:\n\n1. **Case Selection**: Who was interviewed and why (explicit criteria, not diverse participants).\n2. **Sampling Strategy**: Named approach (purposive, snowball, theoretical, etc.) with justification.\n3. **Recruitment**: How participants were contacted; response rate if known; gatekeeper roles.\n4. **Sample Size Justification**: Use one of four legitimate strategies:\n   - Comparative/structural logic\n   - Population coverage\n   - Saturation evidence (with documentation)\n   - Pragmatic constraints with adequacy assessment\n5. **Interview Protocol**: Format, duration, key topics, recording/transcription.\n6. **Analysis Approach**: Coding process and iterative analysis.\n7. **Positionality** (when relevant): Only if identity meaningfully shaped access or interpretation.\n\n## Methods Template\n\n```\nThis study draws on [N] semi-structured interviews with [population description]\nconducted between [dates] in [location].\n\n**Sampling and Recruitment.** [Target population definition]. We identified potential\nparticipants through [sampling frame] using [strategy] because [justification].\nOf [number contacted], [number agreed] ([%] response rate). [Gatekeeper roles or\nrecruitment challenges if applicable].\n\n**Sample Characteristics.** The final sample included [key demographic breakdown\nrelevant to analysis]. [Sample size justification].\n\n**Interview Procedures.** Interviews lasted [duration range] and covered [key topics].\nAll interviews were [recording method] and [transcription approach].\n\n**Analysis.** We analyzed transcripts using [approach]. [Brief coding process].\nAnalysis was iterative, moving between data and emerging conceptual categories.\n```\n\n## Common Problems to Flag\n\n- Vague population definitions (e.g., diverse participants)\n- Arbitrary sample size justifications (following conventions, we conducted 30 interviews)\n- Missing response rate information (when it can be reported)\n- Excessive technical detail (software versions, device brands)\n- Generic positionality statements without substantive content\n\n## Output\n\nProvide a draft or revised methods section plus a short list of gaps or questions for the user.\n\n## When Youre Done\n\nReturn a summary to the orchestrator with:\n1. Drafted methods text\n2. Any missing details to request\n3. Quality flags to address\n",
        "plugins/interview-writeup/skills/interview-writeup/phases/phase2-findings.md": "# Phase 2: Findings Section\n\nYou are structuring and drafting the findings section. Your goal is to produce **argument-driven narrative**, not a catalog of themes with illustrative quotes.\n\n## Why This Phase Matters\n\nA strong findings section advances analytic claims using interview data. Quotes instantiate ideas already introduced by the authorthey never carry analytic novelty on their own. The difference between a publishable findings section and a rough draft is often the difference between *arguing with evidence* and *displaying evidence*.\n\n## Required Reading\n\nBefore drafting, review these technique guides:\n\n1. **`techniques/macro-structure.md`**  Choose your archetype (Mechanism List, Comparative, or Process) and understand the Roadmap + Pillars model\n2. **`techniques/prose-craft.md`**  Master the Anchor-Echo pattern, quote integration techniques, and pacing\n3. **`techniques/rubric.md`**  Follow the 8-step process for each subsection\n4. **`techniques/participant-management.md`**  Minimize recurrence; most participants appear in only one section\n\n## Step 1: Choose Your Archetype\n\nBased on the user's argument and data, select one:\n\n| Archetype | Best For | Structure |\n|-----------|----------|-----------|\n| **Mechanism List** | Distinct, non-linear findings | Mechanism 1  Mechanism 2  Mechanism 3 |\n| **Comparative** | Contrasting groups or contexts | Group A  Group B  Explanation  Synthesis |\n| **Process** | Trajectories over time/stages | Problem  Reaction  Justification/Outcome |\n\nConfirm the choice with the user before proceeding.\n\n## Step 2: Write the Roadmap Introduction\n\nThe Findings section does not \"warm up.\" Draft a high-density Roadmap (1-2 paragraphs, 150-300 words) with this structure:\n\n1. **Headline Sentence**: State the primary finding as fact, not process.\n   - Bad: \"We interviewed 50 people about disaster recovery.\"\n   - Good: \"Exit from high-intensity movements operates through multiple distinct mechanisms that the catch-all of 'burnout' obscures.\"\n\n2. **Mechanism Summary**: 2-3 sentences listing the key themes of upcoming subsections.\n   - Example: \"We identify three pillars: (1) mechanisms of exhaustion, (2) structures of opportunity, and (3) persistence of identity.\"\n\n3. **Signpost**: A final sentence transitioning to the first subsection.\n   - Example: \"The analysis below demonstrates how these pillars operated in sequence and interaction.\"\n\n## Step 3: Draft Each Subsection\n\nFor each subsection, follow the 8-step rubric in `techniques/rubric.md`:\n\n1. **Declare the analytic move** (internalknow what claim this section advances)\n2. **Open with analytic assertion** (2-4 sentences, sociological voice, scope indicated)\n3. **Elaborate the mechanism** (1-2 sentences on how/why)\n4. **Introduce anchor quote with purpose** (explain what it will illustrate)\n5. **Present the anchor quote** (extended, from one deep case)\n6. **Interpret explicitly** (name what it shows, link to claim)\n7. **Show prevalence with echo quotes** (2-3 shorter quotes from other participants)\n8. **Manage variation and close** (if applicable; connect to next section)\n\n### The Anchor-Echo Pattern\n\nEvery subsection needs:\n\n**One Anchor**  Go deep on a single participant (2-3 paragraphs). Establish their social location, describe their specific experience, use an extended quote, interpret what the case shows.\n\n**2-3 Echoes**  Zoom out to show this isn't idiosyncratic. Use shorter quotes, signal prevalence (\"Other respondents echoed...\", \"This pattern held across...\").\n\n### Subsection Length\n\nTarget **600-900 words** per subsection. This is enough for:\n- Analytic setup (100-150 words)\n- Anchor vignette with quote and interpretation (300-400 words)\n- Echo quotes with brief analysis (150-200 words)\n- Variation handling and close (100-150 words)\n\n## Step 4: Craft Theoretical Headings\n\nSubheadings must name a **process or mechanism**, not a topic or demographic:\n\n| Avoid | Use Instead |\n|-------|-------------|\n| \"Race and Housing\" | \"Racialized Pathways to Residential Instability\" |\n| \"What Workers Said\" | \"Recognition Estrangement and Invisible Labor\" |\n| \"Findings on Trust\" | \"The Production of Institutional Distrust\" |\n| \"Gender Differences\" | \"Gendered Strategies of Boundary Maintenance\" |\n\n## Step 5: Manage Participant Recurrence\n\n**Default rule**: Each participant appears in only one section. Different sections use different people.\n\n**Recurrence budget**: 1-2 participants maximum across the entire Findings section.\n\n**When recurrence works**: Only when a participant's experience *organically spans multiple mechanisms*, showing how processes connect over time. The recurrence should *be* the findingnot just convenience.\n\n**Tracking**: Maintain a list of which participants appear in which sections. Before adding someone as an echo, check if they've already been used.\n\n**Recall tags**: When a participant does reappear, use a recall tag instead of repeating demographics:\n\n| Avoid | Use Instead |\n|-------|-------------|\n| \"Karl, a white gay man in his early twenties...\" | \"Karl, the floor facilitator discussed above...\" |\n| Full re-introduction | \"Recalling Karl's earlier observation about 'volume eleven'...\" |\n\nSee `techniques/participant-management.md` for detailed guidance.\n\n---\n\n## Step 6: Handle Variation Correctly\n\n**Critical sequencing**: Establish baseline first, then stratify.\n\n1. \"Most participants reported...\" (baseline)\n2. \"This pattern held across [demographic variations]...\" (confirm generality)\n3. \"However, [specific group] diverged...\" (introduce variation)\n4. Evidence for variation\n5. Explanation for why variation occurs\n\nNever start with difference. Never treat variation as an exception dump.\n\n---\n\n## Quote Integration Reminders\n\nFrom `techniques/prose-craft.md`:\n\n- **Front-load analysis**: Readers should know what to listen for before the quote\n- **Embed short phrases**: Weave participant language into your sentences for flow\n- **Never stack quotes**: Analytical sentences must separate consecutive quotes\n- **Long quotes need justification**: Use block quotes only when voice, narrative arc, or internal contradiction matters\n- **Interpret every quote**: Never let a quote \"speak for itself\"\n\n### Attribution\n\nEvery quoted participant needs situating:\n\n| Weak | Strong |\n|------|--------|\n| \"Maria said...\" | \"Maria, a 34-year-old single mother working two jobs, explained...\" |\n| \"One respondent noted...\" | \"Illustrating this dynamic, Steve, a veteran employee passed over for promotion, recounted...\" |\n\nUse high-value verbs: explained, recounted, reasoned, emphasized, lamented, articulated, contended, justified.\n\n## Scope and Prevalence\n\nIndicate prevalence throughoutbut naturally:\n\n- \"Most respondents...\"\n- \"Nearly all participants...\"\n- \"A minority of interviewees...\"\n- \"Across both sites...\"\n- \"Regardless of [demographic]...\"\n\nWhen precision matters, use numbers: \"Twelve of our 24 respondents described...\" But avoid disrupting narrative flow with excessive quantification.\n\n## Prohibited Moves\n\nDo **not**:\n- Start any subsection with a quote\n- List themes without argument\n- Use quotes without interpretation\n- Stack quotes back-to-back\n- Hedge empirical patterns (\"might suggest\")\n- Write descriptive subheadings (\"Findings,\" \"Race,\" \"Results\")\n- Let quotes introduce analytic novelty\n- Add a summary conclusion paragraph to the Findings section\n\n## The Hard Stop Ending\n\nThe Findings section has **no separate conclusion**. It simply stops after the final thematic subsection is complete. The \"Conclusion\" or \"Discussion\" is a separate section that follows later.\n\nEnd the final pillar with:\n- **A clinching quote**: A powerful vignette that encapsulates the final point\n- **A mic-drop sentence**: A final analytical sentence restating the significance\n\nExample mic-drop: \"What the literature calls 'exit' may be better understood as dispersal: the movement identity spreading out from the organization into the broader world, carried by individuals who remain, in some fundamental sense, what the movement made them.\"\n\nDo **not** recap all the pillars at the end. That work belongs in the Discussion.\n\n## Output\n\nProvide:\n- A complete findings section draft, or\n- A detailed outline with the Roadmap paragraph drafted and each subsection sketched (anchor case identified, echo cases listed, analytic claim stated)\n\nFor each subsection, note:\n- The analytic claim it advances\n- The anchor participant and why they're exemplary\n- The echo participants showing prevalence\n- Any variation to be addressed\n- Gaps in evidence (quotes needed but not yet available)\n\n## When You're Done\n\nReturn a summary to the orchestrator with:\n1. Chosen archetype and rationale\n2. Drafted sections or detailed outline\n3. Evidence gaps or missing quotes\n4. Questions about scope, variation, or analytical framing\n",
        "plugins/interview-writeup/skills/interview-writeup/phases/phase3-revision.md": "# Phase 3: Revision & Quality Check\n\nYou are reviewing and refining the draft to meet publication standards. This phase catches formulaic writing, ensures analytical rigor, and verifies that the text reads as argument rather than transcript.\n\n## Why This Phase Matters\n\nFirst drafts often display evidence without arguing with it. Common problems include mechanical quote-analysis-quote patterns, hedging empirical claims, and letting quotes carry analytic weight. This phase transforms competent drafts into compelling arguments.\n\n## Revision Priorities\n\nWork through these checks in order. Earlier issues compound later ones.\n\n---\n\n### 1. Argument Structure\n\n**Check the Roadmap**:\n- [ ] Does the opening paragraph summarize the entire argument before showing evidence?\n- [ ] Are all mechanisms/processes named upfront?\n- [ ] Does the reader know where the section is going?\n\n**Check Each Subsection**:\n- [ ] Does it advance one clear analytic claim?\n- [ ] Is that claim stated in the first 2-4 sentences (before any quotes)?\n- [ ] Does the heading name a mechanism or process (not a topic)?\n- [ ] Does the close connect forward to the next section?\n\n**Red flags**:\n- Opening with \"We interviewed X people...\" or \"This section presents...\"\n- Subsections that list themes rather than advance arguments\n- Headings like \"Findings,\" \"Race,\" \"Gender,\" or \"What Participants Said\"\n\n---\n\n### 2. The Anchor-Echo Pattern\n\n**For each subsection, verify**:\n- [ ] There is one anchor vignette (2-3 paragraphs on a single participant)\n- [ ] The anchor establishes social location before quoting\n- [ ] The anchor quote is extended enough to show how they think (not just what)\n- [ ] There are 2-3 echo quotes showing prevalence\n- [ ] Echoes are shorter and punchier than the anchor\n\n**Red flags**:\n- All quotes treated equally (no depth variation)\n- No clear anchor case\n- Prevalence asserted but not demonstrated\n- Participant introduced only by name, not situation\n\n---\n\n### 3. Quote Integration\n\n**Check every quote**:\n- [ ] Is it introduced with analytic framing (not just \"X said\")?\n- [ ] Is it followed by explicit interpretation?\n- [ ] Does the reader know what to listen for before it appears?\n- [ ] Are consecutive quotes separated by analytical sentences?\n\n**Red flags**:\n- Quotes stacked back-to-back\n- Neutral attributions: \"Maria said...\", \"According to Steve...\"\n- Quotes that \"speak for themselves\" (no interpretation follows)\n- Quotes introducing analytic novelty (the claim appears after, not before)\n\n**The hamburger test**: If every quote follows the pattern [brief context]  [quote]  [analysis sentence], the prose is too formulaic. Look for:\n- Embedded phrases woven into author sentences\n- Anticipatory framing where the claim precedes the quote\n- Varied quote lengths matched to purpose\n- Pivot phrases elevated to conceptual anchors\n\n---\n\n### 4. Voice and Confidence\n\n**Authorial voice should dominate**:\n- [ ] Does the author's voice outweigh participant voice overall?\n- [ ] Are causal claims made in sociological language (not respondent language)?\n- [ ] Are mechanisms named by the author, not discovered in quotes?\n\n**Check for inappropriate hedging**:\n- [ ] Empirical patterns stated confidently (\"This pattern emerged...\") not tentatively (\"This might suggest...\")\n- [ ] Theoretical scope hedged appropriately, but not empirical observations\n- [ ] Confidence comes from patterning, not sample size apologetics\n\n**Red flags**:\n- \"This may indicate...\"\n- \"It could be interpreted as...\"\n- \"Participants seemed to feel...\"\n- Over-reliance on participant voice for analytical claims\n\n---\n\n### 5. Variation Handling\n\n**Check sequencing**:\n- [ ] Is baseline pattern established before variation introduced?\n- [ ] Is variation signaled explicitly (\"However...\", \"In contrast...\")?\n- [ ] Is the axis of difference named (race, class, context)?\n- [ ] Is variation explained (why it occurs), not just noted?\n\n**Red flags**:\n- Opening with difference (\"Some said X while others said Y\")\n- Variation as exception dump (list of outliers)\n- No explanation for why variation occurs\n\n---\n\n### 6. Scope and Prevalence\n\n**Check prevalence indicators**:\n- [ ] Is scope indicated throughout (\"most,\" \"nearly all,\" \"across both sites\")?\n- [ ] Are vague quantifiers avoided (\"some,\" \"many,\" \"few\")?\n- [ ] When numbers are used, is it clear they describe the sample (not the population)?\n\n**Red flags**:\n- \"Some respondents said...\"\n- \"Many participants felt...\"\n- No indication of how common a pattern is\n- Numbers implying statistical representativeness\n\n---\n\n### 7. Methods Section (if included)\n\n- [ ] Population explicitly defined (not vague)\n- [ ] Sampling strategy named and justified\n- [ ] Sample size justified through legitimate strategy\n- [ ] Interview details specified (format, duration, topics)\n- [ ] Analysis approach described\n- [ ] Positionality addressed (when appropriate)\n\n---\n\n## Common Problems and Solutions\n\n| Problem | Symptom | Solution |\n|---------|---------|----------|\n| **Formulaic structure** | Every quote has identical setup-quote-analysis pattern | Vary integration: embed phrases, front-load claims, use pivot phrases |\n| **Quotes speaking for themselves** | Quote followed by transition, not interpretation | Add 1-2 sentences naming what the quote shows |\n| **Missing anchor** | All cases treated equally briefly | Pick one exemplary case, go deep (2-3 paragraphs) |\n| **Stacked quotes** | Quote immediately follows quote | Insert analytical sentences between |\n| **Weak attribution** | \"Maria said...\" | \"Maria, a [demographics] in [situation], explained...\" |\n| **Descriptive headings** | \"Race,\" \"Findings,\" \"Results\" | Name the mechanism: \"Racialized Pathways to...\" |\n| **Starting with variation** | \"Some said X, others said Y\" | Establish baseline first, then stratify |\n| **Hedged empirics** | \"This might suggest...\" | \"This pattern shows...\" (hedge theory, not data) |\n| **Topic headings** | \"What Workers Said About Pay\" | \"Wage Theft as Normalized Exploitation\" |\n| **No roadmap** | Jumps straight into first subsection | Add 1-2 paragraph overview naming all mechanisms |\n\n---\n\n## Prohibited Moves Checklist\n\nVerify the draft does **not**:\n\n- [ ] Start any subsection with a quote\n- [ ] List themes without argument\n- [ ] Use quotes without interpretation\n- [ ] Stack quotes back-to-back\n- [ ] Hedge empirical patterns\n- [ ] Use purely descriptive subheadings\n- [ ] Let quotes introduce analytic novelty\n- [ ] Treat all quotes with equal depth\n- [ ] Start with variation before baseline\n- [ ] Use neutral attribution when analytic framing is possible\n\n---\n\n## Quality Indicators\n\nThe final draft should demonstrate:\n\n- **Argument over display**: Each subsection advances a claim, not just shows data\n- **Analytical confidence**: Patterns stated assertively, mechanisms named clearly\n- **Narrative craft**: Varied quote integration, anchor-echo pacing, smooth transitions\n- **Grounded abstraction**: Sociological concepts tied to concrete evidence\n- **Strategic depth**: Anchor cases developed fully, echoes efficient\n\n---\n\n## Output\n\nProvide:\n- Revised draft with tracked changes or highlighted revisions, OR\n- A revision memo listing specific changes needed, organized by section\n\nInclude a **quality memo** with:\n1. Strengths of the current draft\n2. Key revisions made or recommended\n3. Remaining issues requiring author attention\n4. Assessment against the five quality indicators above\n\n## When You're Done\n\nReturn a summary to the orchestrator with:\n1. Key revisions made\n2. Remaining issues or questions for the user\n3. Overall assessment of draft quality\n4. Suggested next steps\n",
        "plugins/interview-writeup/skills/interview-writeup/techniques/macro-structure.md": "# Macro-Structure: Organizing the Findings Section\r\n\r\n## The \"Roadmap + Pillars\" Model\r\n\r\nEvery findings section follows a standard architecture: **one introductory \"Roadmap\" paragraph followed by 3-4 thematic subsections (\"Pillars\")**, ending with a hard stop after the final pillar.\r\n\r\n### The Roadmap Introduction\r\n\r\nThe Findings section does not \"warm up.\" It begins with a high-density Roadmap (1-2 paragraphs, approximately 150-300 words) that summarizes the entire argument before presenting evidence. This functions as a \"mini-abstract\" for the results.\r\n\r\n**Structure:**\r\n\r\n1. **The Headline Sentence**: The first sentence states the primary finding as a fact, not a process.\r\n   - Bad: \"I interviewed 50 people about disaster recovery.\"\r\n   - Good: \"Exit from high-intensity movements operates through multiple distinct mechanisms that the catch-all of 'burnout' obscures.\"\r\n\r\n2. **The Mechanism Summary**: 2-3 sentences explaining how the phenomenon works, briefly listing the key themes of the upcoming subsections.\r\n   - Example: \"We identify three pillars of the exit experience: (1) the mechanisms of exhaustiondisaggregated into pace depletion, grief accumulation, and fit failure; (2) the structures of opportunity that determined which exit pathways were available based on organizational position; and (3) the persistence of identity that rendered organizational departure something other than true exit.\"\r\n\r\n3. **The Signpost**: A final sentence that transitions into the first subsection or defines the scope.\r\n   - Example: \"The analysis below demonstrates how these pillars operated in sequence and interactionexhaustion creating the pressure to leave, position determining where one could go, and identity ensuring that leaving was never complete.\"\r\n\r\n### The Hard Stop Ending\r\n\r\nUnlike the Introduction, the Findings section has **no separate conclusion**. It simply stops after the final thematic subsection is complete. The actual \"Conclusion\" or \"Discussion\" of the paper is a separate section that follows later.\r\n\r\nThe final pillar should end with:\r\n- **A \"Clinching\" Quote or Vignette**: The section often ends on a powerful, illustrative quote that encapsulates the final point.\r\n- **A \"Mic Drop\" Sentence**: A final analytical sentence that restates the significance of that specific subsection.\r\n  - Example: \"What the literature calls 'exit' may be better understood as dispersal: the movement identity spreading out from the organization into the broader world, carried by individuals who remain, in some fundamental sense, what the movement made them.\"\r\n\r\nDo **not** add a summary paragraph that recaps all the pillars. That work belongs in the Discussion section.\r\n\r\n### The Pillars (Subsections)\r\n\r\n**Quantity**: 3-4 subsections is the sweet spot.\r\n\r\n**Headings must be theoretical, not descriptive**:\r\n\r\n| Bad | Good |\r\n|-----|------|\r\n| \"What Participants Said About Landmen\" | \"Corporate Landmen as Liaisons of Meta-Power\" |\r\n| \"Discrimination\" | \"Vicarious Marginalization\" |\r\n| \"Race Differences\" | \"Divergent Pathways to Legal Consciousness\" |\r\n| \"Findings\" | \"Recognition Estrangement and the Labor of Disappearance\" |\r\n\r\nSubheadings name a **process or mechanism**, never a demographic or topic.\r\n\r\n---\r\n\r\n## Three Structural Archetypes\r\n\r\nChoose an archetype based on your data and argument:\r\n\r\n### Archetype A: The Mechanism List (\"Three Buckets\")\r\n\r\n**Best for**: Distinct, non-linear findings where each section is a separate theoretical bucket.\r\n\r\n**Structure**:\r\n```\r\nROADMAP\r\n Summarize all three mechanisms\r\n Preview how they combine\r\n\r\nMECHANISM 1: [Theoretical Name]\r\n Analytic assertion\r\n Anchor vignette (one deep case)\r\n Echo quotes (2-3 showing prevalence)\r\n Mechanism summary\r\n\r\nMECHANISM 2: [Theoretical Name]\r\n [Same structure]\r\n\r\nMECHANISM 3: [Theoretical Name]\r\n [Same structure]\r\n\r\nINTEGRATION (optional)\r\n How mechanisms combine\r\n Cumulative or interactive effects\r\n```\r\n\r\n**Example**: A disaster recovery study uses three distinct mechanisms of inequality: Procedural Injustice  Vicarious Marginalization  Structural Exclusion.\r\n\r\n---\r\n\r\n### Archetype B: The Comparative (\"Mirror\")\r\n\r\n**Best for**: Contrasting two distinct groups, contexts, or categories.\r\n\r\n**Structure**:\r\n```\r\nROADMAP\r\n What groups share\r\n Key divergence\r\n Preview explanation\r\n\r\nGROUP A: [Pattern Name]\r\n Establish baseline pattern\r\n Anchor vignette\r\n Echo quotes\r\n Group A summary\r\n\r\nGROUP B: [Pattern Name]\r\n Explicit contrast transition (\"Unlike Group A...\")\r\n Anchor vignette\r\n Echo quotes\r\n Group B summary\r\n\r\nEXPLAINING THE DIFFERENCE\r\n Structural/institutional mechanism\r\n Why context matters\r\n Evidence for the explanation\r\n\r\nCONVERGENCE (if applicable)\r\n Shared pattern despite divergence\r\n Why this convergence exists\r\n```\r\n\r\n**Example**: An elite careers study contrasts \"Talent Meritocrats (UK)\" vs. \"Hard Work Meritocrats (Denmark)\" before moving to \"Institutional Amplifiers\" that explain the difference.\r\n\r\n---\r\n\r\n### Archetype C: The Process (\"Timeline\")\r\n\r\n**Best for**: Explaining a psychological or social trajectory over time or stages.\r\n\r\n**Structure**:\r\n```\r\nROADMAP\r\n Name the trajectory\r\n Identify stages\r\n Preview outcome\r\n\r\nSTAGE 1: The Problem/Condition\r\n What participants face\r\n Anchor vignette\r\n Echo quotes\r\n Why this creates pressure\r\n\r\nSTAGE 2: The Reaction/Strategy\r\n How participants respond\r\n Anchor vignette showing strategy development\r\n Echo quotes showing variation in strategies\r\n What shapes different responses\r\n\r\nSTAGE 3: The Justification/Outcome\r\n How participants make sense of their situation\r\n Anchor vignette\r\n Echo quotes\r\n Consequences and implications\r\n```\r\n\r\n**Example**: A ghostwriting study flows from the problem (\"Recognition Estrangement\") to the reaction (\"Securing Work\") to the psychological justification (\"Managing Estrangement\").\r\n\r\n---\r\n\r\n## Section Length and Pacing\r\n\r\nEach subsection typically runs **600-900 words**. To fill this space without rambling, use the \"Anchor and Echo\" pattern described in `prose-craft.md`.\r\n\r\n---\r\n\r\n## Variation Sequencing\r\n\r\n**Critical rule**: Establish the baseline pattern first; only then introduce divergence.\r\n\r\nNo section begins with difference. All begin with commonality, then stratify.\r\n\r\n**Correct sequence**:\r\n1. \"Most participants reported...\" (establish baseline)\r\n2. \"This pattern held across [demographic variations]...\" (confirm generality)\r\n3. \"However, [specific group] diverged in important ways...\" (introduce variation)\r\n4. Evidence for variation\r\n5. Explanation for why variation occurs\r\n\r\n**Incorrect sequence**:\r\n- Starting with \"Some participants said X while others said Y...\"\r\n- Listing differences without establishing what's shared\r\n- Treating variation as a list or exception dump\r\n",
        "plugins/interview-writeup/skills/interview-writeup/techniques/participant-management.md": "# Participant Management: Recurrence and Attribution\r\n\r\n## The Recurrence Problem\r\n\r\nWhen the same participant appears in multiple sections, it can either strengthen the argument (showing how mechanisms connect) or weaken it (suggesting the sections aren't independent patterns). Most published articles **actively avoid recurrence** to preserve the sense that each section demonstrates a distinct phenomenon.\r\n\r\n## The Rule: Minimize Strategic Recurrence\r\n\r\n**Reuse rate**: Aim for 1-2 key participants across the entire Findings section, maximum.\r\n\r\n**Default**: Each participant appears in only one section. Different sections use different people to demonstrate that patterns are general, not idiosyncratic to particular individuals.\r\n\r\n## When Recurrence Works\r\n\r\nRecurrence is appropriate when a participant's experience **organically spans multiple mechanisms**, allowing you to show how processes connect:\r\n\r\n### Processual/Longitudinal Illustration\r\n\r\nWhen showing movement over time or across domains, one participant can reappear to trace a trajectory:\r\n\r\n> Example: Helen's experiences with FEMA appear in \"Procedural Injustice,\" while related community dynamics reappear later as the paper moves toward \"Vicarious Marginalization.\"\r\n\r\nThis works because Helen's story *is* the connection between mechanismsher trajectory shows how procedural injustice produces community-level effects.\r\n\r\n### Bookending\r\n\r\nA participant who appears early can return at the end to show transformation or persistence:\r\n\r\n> Example: The person who describes burning out (\"volume eleven\") later appears describing never truly leaving (\"I don't think you ever get out\"). The recurrence *is* the findingexhaustion and identity persistence are linked.\r\n\r\n## When to Avoid Recurrence\r\n\r\n**Avoid recurrence when sections are analytically independent.** If each section demonstrates a distinct mechanism that operates separately, using different participants preserves that independence.\r\n\r\nSigns you should NOT reuse a participant:\r\n- The sections describe different groups (e.g., \"UK Elites\" vs. \"Danish Elites\")\r\n- The mechanisms are conceptually separate (e.g., \"Recognition,\" \"Securing Work,\" \"Managing Estrangement\")\r\n- Reusing would suggest the section is just \"another reading\" of the same person rather than a general pattern\r\n\r\n## The Re-Introduction Tag\r\n\r\nWhen a participant does reappear in a later section, **do not re-state their full demographics**. Use a recall tag instead:\r\n\r\n### Bad (Repeated Demographics)\r\n\r\n> \"Karl, a white gay man in his early twenties who served as a floor facilitator, described...\"\r\n\r\n### Good (Recall Tag)\r\n\r\n> \"Karl, the floor facilitator whose pace exhaustion we examined earlier, also described...\"\r\n\r\n> \"Recalling Karl's earlier observation about 'volume eleven,' his subsequent reflection reveals...\"\r\n\r\n> \"Karl, introduced above as an example of pace exhaustion, reappears here in a different light...\"\r\n\r\n### Recall Tag Templates\r\n\r\n- \"[Name], the [brief role descriptor] discussed above...\"\r\n- \"[Name], whose [earlier topic] we examined in the previous section...\"\r\n- \"Recalling [Name]'s earlier observation about [topic]...\"\r\n- \"As [Name] noted earlier regarding [topic], [new observation]...\"\r\n- \"[Name], introduced above as [role], reappears here...\"\r\n\r\n## Practical Workflow\r\n\r\n### During Drafting\r\n\r\n1. **Track usage**: Keep a list of which participants appear in which sections\r\n2. **Default to new people**: When you need an echo quote, first look for someone not yet used\r\n3. **Reserve recurrence**: Save the 1-2 recurrence slots for cases that genuinely span mechanisms\r\n\r\n### During Revision\r\n\r\n1. **Audit recurrence**: Count how many participants appear in more than one section\r\n2. **Justify each recurrence**: Can you articulate why this person needs to appear twice?\r\n3. **Replace unjustified recurrences**: Find alternative participants who can serve the same function\r\n4. **Fix attribution**: Ensure all reappearances use recall tags, not repeated demographics\r\n\r\n## Example: Reducing Recurrence\r\n\r\n**Before (4 recurring participantstoo many):**\r\n\r\n| Participant | Section 1 | Section 2 | Section 3 |\r\n|-------------|-----------|-----------|-----------|\r\n| Karl | Anchor |  | Echo |\r\n| Frank | Echo |  | Echo |\r\n| Chip | Anchor |  | Anchor |\r\n| Lei |  | Anchor, Echo |  |\r\n\r\n**After (1 strategic recurrence):**\r\n\r\n| Participant | Section 1 | Section 2 | Section 3 |\r\n|-------------|-----------|-----------|-----------|\r\n| Karl | Anchor |  |  |\r\n| Frank | Echo |  |  |\r\n| Chip | Anchor |  | Anchor (with recall tag) |\r\n| Lei |  | Anchor |  |\r\n| Gregg |  | Echo |  |\r\n| Joy |  |  | Anchor |\r\n| Moises |  |  | Echo |\r\n\r\nChip's recurrence is strategic: it connects exhaustion to identity persistence. Everyone else appears once.\r\n",
        "plugins/interview-writeup/skills/interview-writeup/techniques/prose-craft.md": "# Prose Craft: Quote Integration and Pacing\r\n\r\n## The Core Principle\r\n\r\nQuotes **instantiate** ideas already introduced by the author. They never introduce analytic noveltynovelty belongs to the author.\r\n\r\nThis means: **claims precede quotes**. The reader knows what to listen for before the quote arrives.\r\n\r\n---\r\n\r\n## The Anchor and Echo Pattern\r\n\r\nEvery section anchors its argument in **one deep, extended story** (the Anchor), then zooms out to show prevalence (the Echo).\r\n\r\n### The Anchor (Vignette)\r\n\r\nDedicate 2-3 paragraphs to a single person:\r\n- Establish their \"social location\" (age, race, class, occupation, situation)\r\n- Describe their specific experience in narrative detail\r\n- Use an extended quote (5-12 lines) that reveals how they think, not just what they think\r\n- Interpret what this case shows\r\n\r\n**Example opening for an Anchor**:\r\n> \"Guadalupe, a forty-year-old mother originally from Jalisco, followed her husband to Escondido in 2004. Just two years later, at work in a take-out restaurant, she learned of a new state law that would...\"\r\n\r\nThe writing **slows down** for anchors. Take time to build the character.\r\n\r\n### The Echo (Prevalence)\r\n\r\nImmediately after the Anchor, show this isn't just one person's story:\r\n- Cite 2-3 other participants briefly\r\n- Use shorter, punchier quotes\r\n- Signal prevalence: \"Other participants explained...\", \"This sentiment was shared across...\", \"Similar accounts emerged from...\"\r\n\r\n**Example**:\r\n> \"Other respondents echoed this logic. As Marcus, a retired teacher, put it: '[short quote].' Nancy, who had worked in the industry for fifteen years, similarly noted: '[short quote].' These accounts suggest that [analytical point].\"\r\n\r\nThe writing **speeds up** for echoes. Stack perspectives efficiently.\r\n\r\n---\r\n\r\n## Quote Integration Techniques\r\n\r\n### 1. The Anticipatory Frame\r\n\r\nFront-load the analytical point; use the quote as confirmation.\r\n\r\n**Formulaic (avoid)**:\r\n> Maria, a 34-year-old nurse, explained: \"[quote].\" This shows that...\r\n\r\n**Anticipatory (prefer)**:\r\n> Participants often framed their situation using metaphors of entrapment. As Maria, a 34-year-old nurse, put it: \"[quote].\"\r\n\r\nThe reader knows to listen for \"entrapment\" before the quote arrives.\r\n\r\n### 2. The Embedded Voice\r\n\r\nWeave short quoted phrases directly into your sentences:\r\n\r\n> Katherine and her husband casually browsed units\"There was probably like a two-year period where we would be obsessed looking at houses online,\" she saiduntil they decided on their current home.\r\n\r\n> Respondents reported that landmen indicated development was \"inevitable,\" that they were the \"only holdouts,\" and that the process would proceed \"with or without\" their consent.\r\n\r\nThis maintains narrative flow while preserving participant voice.\r\n\r\n### 3. The Pivot Phrase\r\n\r\nExtract a pithy phrase from the data and use it as a conceptual anchor throughout:\r\n\r\n> The paper repeatedly references the concept of \"anywhere but here\" as a housing search logic, drawn from a participant's words but elevated to analytical status.\r\n\r\n> The term \"puppet master\" becomes a key concept, taken directly from a ghostwriter's self-description.\r\n\r\n### 4. The Dialogue Turn\r\n\r\nFor particularly rich exchanges, show the back-and-forth:\r\n\r\n> When I asked Sara who she would keep her \"legal situation under wraps around,\" she initially responded, \"Any peers I feel are more privileged than I am.\" I followed up by asking her to clarify what she meant by \"more privileged,\" which led her to elaborate...\r\n\r\nThis reveals the interview as co-construction of meaning.\r\n\r\n---\r\n\r\n## Quote Length by Purpose\r\n\r\n| Purpose | Length | When to Use |\r\n|---------|--------|-------------|\r\n| Language pattern | 2-4 words embedded | Capturing specific vocabulary or framing |\r\n| Confirming a point | 1-2 sentences | Quick illustration after analytic setup |\r\n| Showing reasoning | 3-6 sentences | When the *how* of thinking matters |\r\n| Full narrative | 8-15 lines (block) | When voice, arc, or internal contradiction matters |\r\n\r\n### When Block Quotes Work\r\n\r\nUse extended block quotes when:\r\n- **Voice and texture matter**: The quote reveals *how* they think, not just *what*\r\n- **The story has narrative arc**: Beginning-middle-end structure within the quote\r\n- **Contradiction is internal**: The participant wrestles with competing ideas in one response\r\n\r\n### Strategic Trimming\r\n\r\nUse ellipses to jump from concrete details to the analytical crux:\r\n\r\n> \"When I came here they had all them locks on the meter...all the doors to the bedrooms wasn't [installed], we had cracked windows... on my income I really can't expect to get everything I want, so I'm just settling because I know I'm not going to be here long.\"\r\n\r\n---\r\n\r\n## Pacing: Zoom In, Zoom Out\r\n\r\nThe dominant rhythm is **oscillation**, not linear accumulation.\r\n\r\n### The Cycle\r\n\r\n1. **Generalized claim** (sociological voice): \"Most ghostwriters emphasized...\"\r\n2. **Drop into extended quote** (participant voice): The anchor vignette\r\n3. **Pull back out** (sociological voice): Name the mechanism illustrated\r\n4. **Brief prevalence** (mixed voice): Echo quotes\r\n5. **Transition** (sociological voice): Connect to next point\r\n\r\n### Voice Balance by Section Type\r\n\r\n| Section Type | Author:Participant | Pattern |\r\n|--------------|-------------------|---------|\r\n| Introducing a theme | 70:30 | Author sets up, one illustrative quote |\r\n| Developing a theme | 50:50 | Alternating analysis and quotes |\r\n| Culminating a theme | 40:60 | Multiple voices confirming |\r\n| Transitions | 90:10 | Brief author bridge |\r\n\r\n---\r\n\r\n## Attribution and Introduction\r\n\r\n### Contextual Tags Are Mandatory\r\n\r\nNever use a name without situating the person:\r\n\r\n| Avoid | Prefer |\r\n|-------|--------|\r\n| \"Steve said...\" | \"Steve, a middle-aged working-class Black man...\" |\r\n| \"According to Helen...\" | \"Helen, an uninsured homeowner in a flood zone...\" |\r\n\r\n### High-Value Reporting Verbs\r\n\r\nParticipants rarely just \"say.\" They perform specific speech acts:\r\n\r\n**Use**: explained, recounted, reasoned, emphasized, lamented, articulated, illustrated, contended, justified, insisted, acknowledged, reflected\r\n\r\n**Avoid overusing**: said, stated, noted, mentioned\r\n\r\n### Analytic Lead-Ins\r\n\r\nEvery quote introduction should explain what it illustrates:\r\n\r\n| Neutral (weak) | Analytic (strong) |\r\n|----------------|-------------------|\r\n| \"Maria said...\" | \"This logic is evident in how Maria described...\" |\r\n| \"One participant noted...\" | \"Illustrating this dynamic, one participant recounted...\" |\r\n| \"According to Steve...\" | \"As Steve explained this process...\" |\r\n\r\n---\r\n\r\n## Transitions\r\n\r\n### Between Quotes\r\n\r\nNever stack quotes back-to-back. Analytical sentences must separate them:\r\n\r\n**Weak**: Quote A. Quote B. Quote C.\r\n\r\n**Strong**: Quote A. [2-3 sentences analyzing what it shows and connecting to the next point]. Quote B.\r\n\r\n### Between Cases\r\n\r\nSignal transitions with explicit comparative framing:\r\n- \"In contrast to [previous pattern]...\"\r\n- \"Unlike [Group A], [Group B]...\"\r\n- \"While [first participant] emphasized X, [second participant] prioritized Y...\"\r\n\r\n### Between Themes\r\n\r\nUse conceptual bridges linking the previous conclusion to the next opening:\r\n\r\n> End of Section A: Discussion of how shared immigrant identity creates safety.\r\n> Start of Section B: \"Constructing relational security not only entails carefully discerning who is a potential friend, but also who is not.\"\r\n\r\nThe phrase \"relational security\" connects back while \"who is not\" opens the new theme.\r\n\r\n---\r\n\r\n## Advanced Techniques\r\n\r\n### The Definitional Quote\r\n\r\nLet participants define key concepts in their own words:\r\n\r\n> \"A co-writer is someone who is acknowledged.... A ghostwriter is someone who writes a book whose name is not on anything, who never gets acknowledged even though they were the writer.\"\r\n\r\nThis grounds analysis in emic categories while establishing conceptual precision.\r\n\r\n### The Recursive Quote\r\n\r\nUse quotes where participants theorize their own experience:\r\n\r\n> \"You see, the ghostwriter is basically an actor or an actress. I have to forget who I am, and I have to become that person.\"\r\n\r\nThe participant isn't just describing behavior but offering a theorized interpretation.\r\n\r\n### Third-Party Attribution\r\n\r\nQuote participants describing what others say or think:\r\n\r\n> James recounted: \"When I stepped out I had this leaving interview with Tony Blair and I said I didn't think I'd done very well and he said, 'No, you were a Rolls Royce in a Land Rover job.'\"\r\n\r\nNested attribution adds credibility while revealing social construction.\r\n\r\n### The Surprising Confirmation\r\n\r\nSet up an expected pattern, then show a participant articulating it better than you could:\r\n\r\n> The author argues that homebuyers treat the decision as high-stakes. Then: \"When you think about it, it's like the biggest expense you'll ever have.\"\r\n\r\nThe participant's simple articulation lands harder than academic prose.\r\n",
        "plugins/interview-writeup/skills/interview-writeup/techniques/rubric.md": "# Writing Rubric: The 8-Step Process\r\n\r\nYou are writing the **results section of a qualitative, interview-based sociology article**. Your task is to *advance analytic claims* using interview data, not to summarize interviews.\r\n\r\n---\r\n\r\n## Step 1: Declare the Analytic Move (Internal)\r\n\r\nBefore writing a subsection, silently identify:\r\n\r\n- The **core analytic claim** of this subsection\r\n- The **mechanism or process** being demonstrated\r\n- Whether this subsection establishes a **baseline pattern** or **variation**\r\n\r\nDo not write until this is clear.\r\n\r\n---\r\n\r\n## Step 2: Open with an Analytic Assertion\r\n\r\nBegin the subsection with **2-4 sentences** that:\r\n\r\n- State the analytic pattern in generalized terms\r\n- Use sociological language (not respondent language)\r\n- Indicate scope (\"most respondents,\" \"across cases,\" \"in both sites\")\r\n- Signal whether variation will follow\r\n\r\n**Do not** mention specific individuals yet.\r\n\r\n**Example**:\r\n> \"Participants across both sites expressed profound distrust of institutional recovery processes. This distrust did not emerge from ignorance of available resources but from repeated experiences of procedural unfairnesswhat we term 'bureaucratic betrayal.' The pattern held regardless of respondents' prior experience with government agencies, though it manifested differently across racial and class lines.\"\r\n\r\n---\r\n\r\n## Step 3: Elaborate the Mechanism\r\n\r\nAdd **1-2 sentences** explaining *how* or *why* this pattern operates.\r\n\r\nAcceptable moves:\r\n- Naming a process (e.g., boundary-drawing, legitimation, symbolic violence)\r\n- Linking to structural conditions\r\n- Clarifying what is at stake for respondents\r\n\r\n**Do not** introduce quotes yet.\r\n\r\n**Example**:\r\n> \"This distrust was actively produced through interactions that signaled to applicants that their claims were illegitimate and their time expendable. The cumulative effect was a learned helplessness that discouraged further engagement with recovery systems.\"\r\n\r\n---\r\n\r\n## Step 4: Introduce a Quote with Purpose\r\n\r\nIntroduce each quote by explaining **what it illustrates**.\r\n\r\nEvery quote introduction must:\r\n- Name the analytic concept it exemplifies\r\n- Explain why this quote is being shown *now*\r\n- Avoid neutral attributions like \"X said\"\r\n\r\n**Example lead-ins**:\r\n- \"As one participant described this process...\"\r\n- \"This logic is evident in how...\"\r\n- \"Illustrating this dynamic...\"\r\n- \"The stakes of this pattern are clear in...\"\r\n\r\n**Example**:\r\n> \"The texture of bureaucratic betrayal is evident in Helen's account. Helen, a 63-year-old Black homeowner who had lived in the same house for three decades, spent nearly four years navigating FEMA's appeals process:\"\r\n\r\n---\r\n\r\n## Step 5: Present the Quote\r\n\r\n- Use **long quotes** when voice, narrative arc, or internal contradiction matters\r\n- Quotes should *instantiate*, not introduce, ideas\r\n- Quotes may include narrative detail, emotion, or metaphor\r\n- Preserve verbal texture when it reveals cognition\r\n\r\n**Do not** stack quotes back-to-back.\r\n\r\n---\r\n\r\n## Step 6: Interpret the Quote Explicitly\r\n\r\nImmediately after each quote, write **at least one sentence** that:\r\n\r\n- Names what the quote shows\r\n- Links it back to the subsection's analytic claim\r\n- Translates respondent language into sociological meaning\r\n\r\n**Never assume the quote \"speaks for itself.\"**\r\n\r\n**Example**:\r\n> \"Helen's characterization of FEMA as 'neglectful'and her framing of herself as fighting 'tooth and nail'illustrates how procedural unfairness is experienced not merely as bureaucratic inconvenience but as active institutional hostility. Her case shows that applicants found the procedures to be fundamentally unfair, and that race, class, and gender shaped one's potential for securing recovery aid.\"\r\n\r\n---\r\n\r\n## Step 7: Show Prevalence (The Echo)\r\n\r\nAfter the anchor vignette, demonstrate that this isn't idiosyncratic:\r\n\r\n- Cite 2-3 other participants briefly\r\n- Use shorter, punchier quotes\r\n- Signal scope: \"Other respondents echoed...\", \"This sentiment was shared...\"\r\n\r\n**Example**:\r\n> \"Other respondents echoed Helen's sense of institutional betrayal. Marcus, a retired white teacher, described feeling 'invisible' to case workers. Denise, a young mother of three, recounted being told her claim was 'not a priority' despite visible damage to her home. As she put it: 'They looked at me like I was trying to steal something.' Across racial and class lines, participants framed their interactions with recovery institutions as adversarial rather than supportive.\"\r\n\r\n---\r\n\r\n## Step 8: Manage Variation and Close\r\n\r\nIf introducing variation:\r\n- Signal the shift explicitly (\"However,\" \"In contrast,\" \"This pattern diverged when...\")\r\n- Describe the axis of difference (race, class, experience, context)\r\n- Explain why the difference matters analytically\r\n\r\n**Do not** treat variation as a list or exception dump.\r\n\r\nClose the subsection with **1-2 sentences** that:\r\n- Reaffirm the analytic contribution\r\n- Connect to the next process, tension, or section\r\n- Avoid summarizing quotes\r\n\r\n**Example close**:\r\n> \"Bureaucratic betrayal thus operated as more than frustration with red tape; it actively eroded respondents' sense of citizenship and belonging. This erosion set the stage for the second mechanism we identified: the turn to informal, community-based support networks.\"\r\n\r\n---\r\n\r\n## Prohibited Moves\r\n\r\nDo **not**:\r\n- Start a subsection with a quote\r\n- List themes without argument\r\n- Use quotes as evidence without interpretation\r\n- Stack quotes back-to-back without analytical mediation\r\n- Hedge empirical patterns (\"might suggest,\" \"could indicate\")\r\n- Write purely descriptive subheadings (\"Race,\" \"Gender,\" \"Findings\")\r\n- Use neutral attribution when analytic framing is possible\r\n- Let quotes introduce analytic novelty\r\n\r\n---\r\n\r\n## Quality Self-Check\r\n\r\nBefore finishing each subsection, verify:\r\n\r\n- [ ] The subsection advances one clear analytic claim\r\n- [ ] The claim is stated before any quotes appear\r\n- [ ] Every quote is framed (why showing it) and interpreted (what it shows)\r\n- [ ] An anchor vignette goes deep on one case\r\n- [ ] Echo quotes establish prevalence\r\n- [ ] The author's voice outweighs participant voice overall\r\n- [ ] The subsection reads as an argument, not a transcript\r\n- [ ] Variation is introduced only after baseline is established\r\n- [ ] The close connects forward to the next section\r\n",
        "plugins/lecture-designer/.claude-plugin/plugin.json": "{\n  \"name\": \"lecture-designer\",\n  \"description\": \"Transform textbook chapters into engaging, evidence-based lectures. Applies cognitive load theory, narrative design (ABT), active learning, and produces Quarto reveal.js slides.\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Neal Caren\"\n  },\n  \"license\": \"MIT\",\n  \"keywords\": [\n    \"teaching\",\n    \"lectures\",\n    \"slides\",\n    \"pedagogy\",\n    \"active-learning\"\n  ],\n  \"skills\": \"./skills/\"\n}\n",
        "plugins/lecture-designer/skills/lecture-designer/SKILL.md": "---\r\nname: lecture-designer\r\ndescription: Transform textbook chapters into engaging, evidence-based lectures with Google Slides. Guides instructors through learning outcomes, narrative design, active learning activities, and slide creation via Google Docs MCP.\r\n---\r\n\r\n# Lecture Designer\r\n\r\nYou are an expert instructional designer helping university instructors transform textbook chapters into engaging, high-retention lectures. Your role is to guide users through a systematic process that produces publication-quality slides and evidence-based lecture plans.\r\n\r\n## Prerequisites: Google Docs MCP\r\n\r\nThis skill creates slides directly in Google Slides using the Google Docs MCP server. Before starting, ensure the MCP is installed and configured.\r\n\r\n**Installation:**\r\n1. Install the Google Docs MCP: <https://github.com/nealcaren/google-docs-mcp>\r\n2. Follow the setup instructions to configure OAuth credentials\r\n3. Verify connection by testing with a simple document operation\r\n\r\n**Why Google Slides?**\r\n- **Real-time collaboration**: Share and co-edit with TAs or colleagues\r\n- **Native presentation**: No rendering stepslides are ready to present\r\n- **Image integration**: Drag and drop images directly into slides\r\n- **Familiar interface**: Most instructors already know Google Slides\r\n- **Cloud storage**: Automatic saving and version history\r\n\r\n> **Note**: If you prefer local Quarto reveal.js slides, reference guides are available in the `quarto/` directory, but Google Slides is the recommended workflow.\r\n\r\n## Core Principles\r\n\r\n1. **Learning outcomes first**: Define what students should be able to *do* by the end, then design backward from there.\r\n\r\n2. **Narrative over coverage**: A lecture is a story, not a chapter recitation. Use the ABT (And, But, Therefore) structure to create cognitive tension and resolution.\r\n\r\n3. **Cognitive load management**: Apply Sweller's Cognitive Load Theoryminimize extraneous load, manage intrinsic load through chunking, maximize germane load through active processing.\r\n\r\n4. **Active learning is required**: Passive listening fails. Design deliberate state changes every 12-18 minutes using polls, peer instruction, and activities.\r\n\r\n5. **Visual simplicity**: Slides support the speaker, not replace them. Apply Mayer's multimedia principlescoherence, signaling, segmenting, redundancy avoidance.\r\n\r\n6. **Pause for instructor input**: Stop between phases to get the instructor's substantive expertise and preferences.\r\n\r\n## Inputs\r\n\r\nThe instructor provides:\r\n- **Chapter/reading material**: The textbook chapter or content to be taught\r\n- **Instructor notes** (optional): What they want to emphasize, known student struggles, war stories\r\n- **Context**: Course level, class size, time available, prior knowledge assumed\r\n\r\n## Outputs\r\n\r\nThe skill produces:\r\n- **Lecture plan**: Learning outcomes, chunk map, temporal timeline\r\n- **Slide deck**: Google Slides presentation with speaker notes (created via MCP)\r\n- **Activity set**: Polls, ConcepTests, and active learning activities\r\n- **Instructor guide**: Delivery notes, backup plans, post-class follow-up\r\n\r\n## Analysis Phases\r\n\r\n### Phase 0: Context & Learning Outcomes\r\n**Goal**: Understand the teaching context and define measurable learning outcomes.\r\n\r\n**Process**:\r\n- Clarify course level, class size, time constraints, and student background\r\n- Review the chapter/reading material\r\n- Review instructor notes and emphases\r\n- Define 3-5 measurable learning outcomes (what students should be able to DO)\r\n- Identify evidence: how will we know they learned it?\r\n\r\n**Output**: Context memo with learning outcomes and evidence plan.\r\n\r\n> **Pause**: Confirm learning outcomes with instructor before proceeding.\r\n\r\n---\r\n\r\n### Phase 1: Content Audit & Narrative Design\r\n**Goal**: Transform chapter content into a narrative arc.\r\n\r\n**Process**:\r\n- **Content Audit**: Categorize chapter content as:\r\n  - **Essential**: Core concepts requiring expert modeling (80% of lecture time)\r\n  - **Helpful**: Supporting examples, interesting details (cut or make optional)\r\n  - **Decorative**: Tangential material (eliminate)\r\n- **Narrative Arc (ABT)**:\r\n  - **And** (Setup): Establish context, what we know\r\n  - **But** (Conflict): The paradox, gap, or puzzle\r\n  - **Therefore** (Resolution): The new understanding\r\n- **The Hook**: Design an opening mystery/problem that grabs attention in 60 seconds\r\n- **Chunk Map**: Break into 3-4 chunks of ~15 minutes each\r\n\r\n**Output**: Content audit, narrative arc document, and chunk map.\r\n\r\n> **Pause**: Review narrative structure with instructor.\r\n\r\n---\r\n\r\n### Phase 2: Active Learning Design\r\n**Goal**: Design activities that reset attention and promote deep processing.\r\n\r\n**Process**:\r\n- **Poll Set Design** (for 75-minute lecture):\r\n  - Poll 1 (min 0-3): Prediction/baseline misconception\r\n  - Poll 2 (min ~20): ConcepTest on Chunk 1\r\n  - Poll 3 (min ~40): ConcepTest on Chunk 2 (hardest material)\r\n  - Poll 4 (min ~55): Transfer/application to new case\r\n  - Poll 5 (min ~72): Muddiest point/confidence check\r\n- **ConcepTest Design**:\r\n  - Stem describes a situation; answers are mechanisms, not vocabulary\r\n  - Distractors are the top 3 wrong mental models\r\n  - Target 30-70% correct for optimal peer discussion\r\n- **Peer Instruction Protocol**: Plan Think-Pair-Share moments\r\n- **State Changes**: Non-digital breaks (sketch, discuss, stretch)\r\n\r\n**Output**: Complete activity set with polls, ConcepTests, and protocols.\r\n\r\n> **Pause**: Review activities with instructor. Adjust for their style.\r\n\r\n---\r\n\r\n### Phase 3: Slide Development\r\n**Goal**: Create visually effective slides directly in Google Slides via the Google Docs MCP.\r\n\r\n**Process**:\r\n- **Create Presentation**: Use `createPresentation` to create a new Google Slides deck\r\n- **Apply Multimedia Principles**:\r\n  - **Coherence**: Cut decorative clutter\r\n  - **Signaling**: Highlight what matters (arrows, bolding, progressive reveal)\r\n  - **Segmenting**: One concept per slide\r\n  - **Redundancy**: Don't put full sentences on screen while speaking them\r\n- **Accessibility**:\r\n  - Minimum 24pt body text, 32pt+ headings\r\n  - High contrast (dark on light or light on dark)\r\n  - Describe all visuals verbally\r\n- **Speaker Notes**: Add delivery cues, timing, and transitions to each slide\r\n- **Image Suggestions**: Proactively search for relevant images on Unsplash/Pexels using WebSearch (e.g., `site:unsplash.com [concept]`) and provide curated links for the instructor to add\r\n\r\n**Output**: Google Slides presentation URL with speaker notes, plus image suggestions document.\r\n\r\n> **Pause**: Review slides with instructor.\r\n\r\n---\r\n\r\n### Phase 4: Review & Refinement\r\n**Goal**: Ensure the lecture is deliverable and has backup plans.\r\n\r\n**Process**:\r\n- **Temporal Check**: Verify the timing adds up to available class time\r\n- **Cognitive Load Audit**: Check for overloaded slides or rushed segments\r\n- **Failure Modes**: Plan backups (WiFi down  show of hands, running late  what to cut)\r\n- **Instructor Guide**: Compile delivery notes, timing cues, and post-class follow-up\r\n- **Finalize Materials**: Ensure all files are organized and ready\r\n\r\n**Output**: Final lecture package with instructor guide.\r\n\r\n---\r\n\r\n## Folder Structure\r\n\r\n```\r\nlecture/\r\n chapter/                 # Source chapter/reading material\r\n notes/                   # Instructor notes and emphases\r\n output/\r\n    slides-link.md      # Link to Google Slides presentation\r\n    lecture-plan.md     # Learning outcomes, chunk map, timeline\r\n    activities.md       # Polls, ConcepTests, protocols\r\n    visual-assets.md    # Image suggestions with links\r\n    instructor-guide.md # Delivery notes and backup plans\r\n memos/                   # Phase outputs\r\n```\r\n\r\n## Reference Guides\r\n\r\n### Included Guides\r\n\r\n| Guide | Location | Topics |\r\n|-------|----------|--------|\r\n| `overview.md` | `pedagogy/` | Comprehensive lecture design framework (CLT, ABT, Peer Instruction) |\r\n| `slide-design-guide.md` | `pedagogy/` | Visual design principles: 75-word rule, CRAP framework, typography, color, data visualization |\r\n| `teaching-techniques.md` | `pedagogy/` | Active learning: retrieval practice, predictions, storytelling, 18-minute rule |\r\n| `google-docs-mcp-setup.md` | `mcp/` | Google Docs MCP setup, available tools, and Google Slides API reference |\r\n| Quarto guides | `quarto/` | (Alternative) reveal.js slide syntax for local presentations |\r\n\r\n### Key Principles from Research\r\n\r\n**The Numbers That Matter:**\r\n- **18 minutes**: Maximum before cognitive overload (soft breaks every 10-15 min)\r\n- **75 words**: More than this per slide = it's a document\r\n- **6 words**: Ideal target per slide (Godin/Reynolds)\r\n- **3 seconds**: Audience must grasp slide content this fast\r\n- **Rule of 3**: Organize around 3 key messages\r\n- **65%**: Top TED talks are 65% stories, 25% data, 10% credibility\r\n\r\n**Picture Superiority Effect:**\r\n- Hear information  10% recall after 3 days\r\n- Add picture  65% recall after 3 days\r\n- Images = 6x more memorable than words\r\n\r\n### Recommended Reading\r\n\r\nThese books inform the pedagogical approach (not included due to copyright):\r\n\r\n**Teaching & Pedagogy:**\r\n- Lang, James M. *Small Teaching* (2nd ed.) - Evidence-based teaching strategies\r\n- Bain, Ken. *What the Best College Teachers Do* - Research on exceptional teachers\r\n- Eng, Norman. *Teaching College* - Student-centered techniques and the 9 \"touches\"\r\n- Gallo, Carmine. *Talk Like TED* - Presentation and engagement techniques\r\n\r\n**Visual Design:**\r\n- Duarte, Nancy. *slide:ology* - Visual presentation design principles\r\n- Reynolds, Garr. *Presentation Zen* - Simplicity and restraint in slides\r\n- Duarte, Nancy. *DataStory* - Data visualization and storytelling\r\n\r\n**Research Base:**\r\n- Mayer, Richard. *Multimedia Learning* - Cognitive theory of multimedia\r\n- Sweller, John. *Cognitive Load Theory* - Managing mental effort\r\n- Mazur, Eric. *Peer Instruction* - Active learning in large classes\r\n\r\n## Invoking Phase Agents\r\n\r\nFor each phase, invoke the appropriate sub-agent using the Task tool:\r\n\r\n```\r\nTask: Phase 0 Context & Learning Outcomes\r\nsubagent_type: general-purpose\r\nmodel: opus\r\nprompt: Read phases/phase0-context.md and execute for [instructor's lecture]\r\n```\r\n\r\n## Model Recommendations\r\n\r\n| Phase | Model | Rationale |\r\n|-------|-------|-----------|\r\n| **Phase 0**: Context & Outcomes | **Opus** | Pedagogical judgment, outcome design |\r\n| **Phase 1**: Content Audit & Narrative | **Opus** | Creative narrative design, content curation |\r\n| **Phase 2**: Active Learning Design | **Sonnet** | Systematic activity creation |\r\n| **Phase 3**: Slide Development | **Sonnet** | Technical slide creation |\r\n| **Phase 4**: Review & Refinement | **Opus** | Quality assessment, synthesis |\r\n\r\n## Starting the Design\r\n\r\nWhen the instructor is ready to begin:\r\n\r\n1. **Ask about context**:\r\n   > \"Tell me about your course: What level? How many students? How much time do you have for this lecture?\"\r\n\r\n2. **Ask about the material**:\r\n   > \"What chapter or content are you teaching? Can you share the material or point me to where it is?\"\r\n\r\n3. **Ask about priorities**:\r\n   > \"What do you most want students to take away? What do students typically struggle with?\"\r\n\r\n4. **Then proceed with Phase 0** to establish learning outcomes.\r\n\r\n## Key Reminders\r\n\r\n- **Outcomes before content**: Know where you're going before you plan the route.\r\n- **Cut ruthlessly**: If you mark everything as essential, you've failed the audit.\r\n- **The hook matters**: First 60 seconds determine engagement for the whole lecture.\r\n- **15-minute chunks**: Attention requires state changes; this is biology, not preference.\r\n- **Polls drive learning**: ConcepTests force processing; anonymous responses enable honesty.\r\n- **Slides are visual aids**: They support the speaker, not replace them. Avoid walls of text.\r\n- **Images boost retention 6x**: Proactively search Unsplash/Pexels for relevant images and provide curated links.\r\n- **Google Slides is collaborative**: Share the presentation link so the instructor can add their own touches.\r\n- **Pause between phases**: Always stop for instructor input before proceeding.\r\n- **The instructor decides**: You provide options and recommendations; they choose.\r\n\r\n## 75-Minute Timeline Template\r\n\r\nFor reference, here's the recommended temporal structure:\r\n\r\n| Time | Phase | Activity |\r\n|------|-------|----------|\r\n| 00:00-00:05 | Hook | Mystery/paradox + baseline poll |\r\n| 00:05-00:20 | Chunk 1 | Core Concept 1 |\r\n| 00:20-00:25 | Active Break 1 | ConcepTest + Peer Instruction |\r\n| 00:25-00:40 | Chunk 2 | Core Concept 2 (hardest material) |\r\n| 00:40-00:45 | Active Break 2 | State change (video, sketch, stretch) |\r\n| 00:45-00:55 | Chunk 3 | Application/implications |\r\n| 00:55-01:05 | Synthesis | Complex case study / debate |\r\n| 01:05-01:10 | Summary | Return to hook, resolve mystery |\r\n| 01:10-01:15 | Reflection | Muddiest point + logistics |\r\n",
        "plugins/lecture-designer/skills/lecture-designer/mcp/google-docs-mcp-setup.md": "# Google Docs MCP Setup Guide\r\n\r\nThis guide covers the installation and configuration of the Google Docs MCP server, which enables Claude to create and edit Google Slides presentations directly.\r\n\r\n## Overview\r\n\r\nThe Google Docs MCP is an enhanced Model Context Protocol server that provides programmatic access to Google's productivity suite, including:\r\n- **Google Slides**: Create presentations, add slides, insert text\r\n- **Google Docs**: Read and edit documents\r\n- **Google Sheets**: Read and write spreadsheet data\r\n- **Google Drive**: Manage files and folders\r\n\r\n**Repository**: <https://github.com/nealcaren/google-docs-mcp>\r\n\r\n## Installation Steps\r\n\r\n### 1. Create Google Cloud Project\r\n\r\n1. Go to [Google Cloud Console](https://console.cloud.google.com/)\r\n2. Create a new project (or select an existing one)\r\n3. Enable the following APIs:\r\n   - Google Docs API\r\n   - Google Sheets API\r\n   - Google Slides API\r\n   - Google Drive API\r\n\r\n### 2. Configure OAuth Consent Screen\r\n\r\n1. Navigate to **APIs & Services > OAuth consent screen**\r\n2. Choose **External** user type (or Internal if using Workspace)\r\n3. Fill in the required app information\r\n4. Add the following scopes:\r\n   - `https://www.googleapis.com/auth/documents`\r\n   - `https://www.googleapis.com/auth/spreadsheets`\r\n   - `https://www.googleapis.com/auth/presentations`\r\n   - `https://www.googleapis.com/auth/drive`\r\n\r\n### 3. Create OAuth Credentials\r\n\r\n1. Navigate to **APIs & Services > Credentials**\r\n2. Click **Create Credentials > OAuth client ID**\r\n3. Select **Desktop app** as the application type\r\n4. Download the credentials JSON file\r\n5. Rename it to `credentials.json`\r\n\r\n### 4. Install the MCP Server\r\n\r\n```bash\r\n# Clone the repository\r\ngit clone https://github.com/nealcaren/google-docs-mcp.git\r\ncd google-docs-mcp\r\n\r\n# Place credentials.json in the project root\r\ncp /path/to/downloaded/credentials.json .\r\n\r\n# Install dependencies\r\nnpm install\r\n\r\n# Build the project\r\nnpm run build\r\n```\r\n\r\n### 5. Authenticate with Google\r\n\r\nRun the server once manually to complete OAuth authentication:\r\n\r\n```bash\r\nnpm start\r\n```\r\n\r\nThis will open a browser window for Google authentication. After authorizing, a `token.json` file is created.\r\n\r\n### 6. Configure Claude Desktop\r\n\r\nAdd the MCP server to your Claude Desktop configuration (`mcp_config.json`):\r\n\r\n```json\r\n{\r\n  \"mcpServers\": {\r\n    \"google-docs\": {\r\n      \"command\": \"node\",\r\n      \"args\": [\"/absolute/path/to/google-docs-mcp/dist/index.js\"]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n**Important**: Use the absolute path to the compiled server.\r\n\r\n## Available Tools for Google Slides\r\n\r\n### Creating Presentations\r\n\r\n**`createPresentation`**\r\nCreates a new Google Slides presentation.\r\n\r\n```\r\nParameters:\r\n  title: string - The title of the presentation\r\n\r\nReturns:\r\n  presentationId: string - ID for subsequent operations\r\n  url: string - Direct link to the presentation\r\n```\r\n\r\n### Managing Slides\r\n\r\n**`addSlide`**\r\nAdds a new slide to an existing presentation.\r\n\r\n```\r\nParameters:\r\n  presentationId: string - The presentation ID\r\n  layoutType: string - One of:\r\n    - TITLE - Title slide\r\n    - SECTION_HEADER - Section divider\r\n    - TITLE_AND_BODY - Standard content slide\r\n    - TITLE_AND_TWO_COLUMNS - Two-column layout\r\n    - BLANK - Empty slide for custom content\r\n    - BIG_NUMBER - Large number/statistic display\r\n\r\nReturns:\r\n  slideId: string - ID of the new slide\r\n```\r\n\r\n### Adding Content\r\n\r\n**`insertTextToSlide`**\r\nInserts text into a specific placeholder or shape on a slide.\r\n\r\n```\r\nParameters:\r\n  presentationId: string - The presentation ID\r\n  slideIndex: number - Zero-based slide index\r\n  shapeId: string - The placeholder/shape ID to insert into\r\n  text: string - The text content to insert\r\n```\r\n\r\n### Bulk Operations\r\n\r\n**`replaceAllTextInPresentation`**\r\nFinds and replaces text throughout the entire presentation.\r\n\r\n```\r\nParameters:\r\n  presentationId: string - The presentation ID\r\n  findText: string - Text to search for\r\n  replaceText: string - Replacement text\r\n```\r\n\r\n## Workflow for Lecture Slides\r\n\r\n### Step 1: Create Presentation\r\n```\r\nTool: createPresentation\r\n  title: \"SOC 101 - Week 5: Social Stratification\"\r\n Returns: presentationId, url\r\n```\r\n\r\n### Step 2: Add Slides\r\n```\r\nTool: addSlide\r\n  presentationId: [from step 1]\r\n  layoutType: \"TITLE\"\r\n\r\nTool: addSlide\r\n  presentationId: [from step 1]\r\n  layoutType: \"TITLE_AND_BODY\"\r\n\r\n[Repeat for each slide...]\r\n```\r\n\r\n### Step 3: Populate Content\r\n```\r\nTool: insertTextToSlide\r\n  presentationId: [from step 1]\r\n  slideIndex: 0\r\n  shapeId: [title placeholder]\r\n  text: \"Social Stratification\"\r\n\r\n[Repeat for each content element...]\r\n```\r\n\r\n### Step 4: Share the Link\r\nThe presentation URL from step 1 can be shared with the instructor for:\r\n- Adding images\r\n- Customizing formatting\r\n- Adding speaker notes\r\n- Final review\r\n\r\n## Common Layout Types Explained\r\n\r\n| Layout | Description | Use Case |\r\n|--------|-------------|----------|\r\n| `TITLE` | Large centered title with optional subtitle | Opening slide, course info |\r\n| `SECTION_HEADER` | Section divider with large text | Chunk transitions, topic shifts |\r\n| `TITLE_AND_BODY` | Title with bullet point area | Standard content slides |\r\n| `TITLE_AND_TWO_COLUMNS` | Title with two content columns | Comparisons, before/after |\r\n| `BLANK` | Empty slide | Full-bleed images, custom layouts |\r\n| `BIG_NUMBER` | Large number with description | Statistics, key figures |\r\n\r\n## Tips for Effective Use\r\n\r\n1. **Plan slide structure first**: Map out all slides before creating to minimize API calls\r\n\r\n2. **Use consistent layouts**: Stick to 2-3 layout types for visual consistency\r\n\r\n3. **Batch similar operations**: Create all slides first, then populate content\r\n\r\n4. **Document the URL**: Save the presentation URL in `slides-link.md` immediately\r\n\r\n5. **Let instructors add images**: The MCP can add text efficiently; images are easier to add manually in Google Slides\r\n\r\n## Troubleshooting\r\n\r\n### \"Token expired\" Error\r\nDelete `token.json` and re-authenticate by running `npm start`.\r\n\r\n### \"Quota exceeded\" Error\r\nGoogle APIs have rate limits. Wait a few minutes and retry, or reduce the number of rapid API calls.\r\n\r\n### \"Invalid presentation ID\" Error\r\nEnsure you're using the ID returned from `createPresentation`, not the URL.\r\n\r\n### \"Shape not found\" Error\r\nLayout placeholders have specific IDs. Use the Google Slides UI to inspect placeholder IDs if needed.\r\n\r\n## Additional Resources\r\n\r\n- [Google Slides API Documentation](https://developers.google.com/slides/api)\r\n- [MCP Repository Issues](https://github.com/nealcaren/google-docs-mcp/issues)\r\n- [Google Cloud Console](https://console.cloud.google.com/)\r\n",
        "plugins/lecture-designer/skills/lecture-designer/pedagogy/overview.md": "# Pedagogical Architecture for Large-Scale Instruction: Designing the High-Impact 75-Minute Lecture\n\n## 1. Introduction: The Crisis and Opportunity of the Large Lecture Hall\n\nThe large-enrollment university course---often defined as a cohort exceeding 100 students, and frequently reaching 300 or more---remains a cornerstone of higher education economics and logistics. However, its pedagogical efficacy has long been a subject of intense debate. The traditional \\\"transmission model\\\" of instruction, where an expert broadcasts information from a podium to a passive audience for a continuous 75-minute block, is increasingly viewed as an obsolete artifact of a pre-digital era.^1^ This format, while efficient for delivering content, often fails to account for the cognitive limitations of the human brain, specifically regarding attention span and working memory capacity.^2^\n\nThe challenge facing the modern instructor is not merely one of content delivery---textbooks and asynchronous videos can transfer information far more efficiently than a live speaker---but one of cognitive engagement and synthesis. The lecture hall must be transformed from a vessel of passive reception into an arena of active inquiry. This requires a fundamental paradigm shift: the instructor is no longer a \\\"Sage on the Stage\\\" but a \\\"Cognitive Architect\\\" who structures time, space, and social interaction to facilitate deep learning.^4^\n\nThis report provides a comprehensive, evidence-based framework for designing a 75-minute lecture for a 300-person course. Drawing on Cognitive Load Theory (CLT), narrative theory, and active learning methodologies (specifically Peer Instruction via Poll Everywhere), we will deconstruct the process of converting a static textbook chapter into a dynamic, high-retention learning experience. The goal is to move beyond the \\\"coverage\\\" model---the frantic attempt to speak every word in the chapter---toward a \\\"constructivist\\\" model where the lecture serves as a scaffold for students to build their own mental models of the material.^5^\n\n## 2. Cognitive Foundations of Instructional Design\n\nTo design an effective lecture, one must first understand the \\\"hardware\\\" on which the software of instruction runs: the human brain. In a large lecture hall, the cognitive demands placed on students are immense. They must process visual data (slides), auditory data (speech), and environmental stimuli (ambient noise, peer presence) simultaneously. Without careful design, these inputs can easily overwhelm the limited capacity of working memory, leading to cognitive overload and learning failure.^2^\n\n### 2.1 Deconstructing Cognitive Load Theory (CLT) in the Lecture Context\n\nCognitive Load Theory, originally conceptualized by Sweller, postulates that human working memory is strictly limited in capacity and duration. Information must be processed in working memory before it can be encoded into long-term memory (schemas). If the total cognitive load exceeds the learner\\'s capacity, learning halts.^2^ In the context of a 300-person lecture, the instructor must actively manage three distinct types of load.\n\n#### 2.1.1 Intrinsic Cognitive Load (ICL)\n\nIntrinsic load is inherent to the complexity of the material itself. A chapter on \\\"Quantum Mechanics\\\" or \\\"Advanced Econometrics\\\" carries a high ICL regardless of how well it is taught.^2^\n\n- **Implication for Design:** In a large lecture, the instructor cannot change the complexity of the subject, but they *can* manage it through **segmenting** and **pre-training**. Segmenting involves breaking complex information into bite-sized \\\"chunks\\\" (discussed in Section 4), allowing working memory to clear before the next influx.^7^ Pre-training involves ensuring students have the vocabulary and foundational concepts (often via the textbook reading) before the lecture begins, reducing the ICL during the live session.^2^\n\n#### 2.1.2 Extraneous Cognitive Load (ECL)\n\nExtraneous load is generated by the manner in which information is presented. It is \\\"bad\\\" load---mental effort wasted on processing poor design rather than learning content.^2^\n\n- **The Split-Attention Effect:** When students must split their attention between a complex diagram on a slide and a separate textual explanation (or the lecturer\\'s spoken words), ECL spikes. The brain struggles to integrate the two disparate sources.^2^\n\n- **The Redundancy Principle:** Reading bullet points aloud while students read them silently creates cognitive conflict. The auditory and visual channels compete rather than complement.\n\n- **Implication for Design:** Slides must be visually simple. Text should be minimal. The lecturer\\'s voice should provide the narrative that \\\"glues\\\" the visual elements together, rather than repeating on-screen text.^2^\n\n#### 2.1.3 Germane Cognitive Load (GCL)\n\nGermane load is the \\\"good\\\" load---the mental effort dedicated to processing, constructing, and automating schemas.^2^\n\n- **Implication for Design:** The goal of the 75-minute session is to minimize ECL so that students have the mental bandwidth to engage in GCL. Active learning strategies like \\\"Think-Pair-Share\\\" and \\\"Elaborative Interrogation\\\" (asking *why* something is true) are designed specifically to increase Germane load.^2^\n\n### 2.2 The Physiology of Attention: The 15-Minute Threshold\n\nResearch into adult attention spans consistently indicates a \\\"vigilance decrement\\\" that sets in after approximately 10 to 20 minutes of continuous passive listening.^3^ In a 75-minute class, a monolithic lecture guarantees that the majority of students will be cognitively absent for at least 60% of the session.\n\n- **The Reset Mechanism:** The brain requires a \\\"state change\\\" to reset the attention clock. This does not require a break in learning, but a break in *modality*. Switching from listening to writing, or from watching to talking, resets the vigilance timer.^3^\n\n- **Implication for Design:** The lecture cannot be a linear progression. It must be modular. The \\\"Chunking\\\" method---breaking the 75 minutes into 15-minute segments separated by active interludes---is not a stylistic choice but a biological necessity.^7^\n\n## 3. From Textbook to Script: The Curatorial Process\n\nA common pitfall in large-course design is treating the lecture as an oral recitation of the textbook. This leads to \\\"Content Tyranny\\\"---the overwhelming compulsion to \\\"cover\\\" every fact, date, and definition in the chapter.^5^ This approach invariably results in high Extraneous Load and low retention. The textbook is a reference repository; the lecture is a narrative experience.\n\n### 3.1 The Audit: Essential vs. Helpful\n\nBefore a single slide is created, the instructor must perform a ruthless audit of the textbook chapter. The goal is to filter content through the lens of *utility* and *narrative necessity* rather than completeness.^5^\n\n  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  **Content Category**    **Definition**                                                                                             **Action in Lecture Design**\n  ----------------------- ---------------------------------------------------------------------------------------------------------- -----------------------------------------------------------------------------------------------------\n  **Essential**           Core concepts, foundational theories, and complex mechanisms that require expert modeling to understand.   **Focus:** These form the \\\"chunks\\\" of the lecture. Dedicate 80% of live time to these topics. ^5^\n\n  **Helpful**             Supporting examples, interesting side-notes, biographical details of scientists, or granular data.         **Cut:** Move to \\\"Suggested Reading\\\" or asynchronous materials. Do not lecture on this. ^5^\n\n  **Decorative**          Tangential anecdotes or excessive background information that adds flavor but no structural weight.        **Eliminate:** These increase Extraneous Load without adding value. ^5^\n  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n**The Peer Review Check:** It is often difficult for a subject matter expert to see that a favorite topic is merely \\\"helpful\\\" rather than \\\"essential.\\\" A best practice is to have a colleague review the syllabus and mark topics for removal. If you mark everything as essential, you have failed the audit.^5^\n\n### 3.2 Structuring the Narrative Arc\n\nOnce the content is pared down to the essentials, it must be arranged not as a list, but as a story. Humans are evolutionarily wired for narrative; stories activate neural pathways related to trust, empathy, and memory retention.^11^ A textbook chapter is usually organized *logically* (Definition -\\> Mechanism -\\> Example), but a compelling lecture should be organized *psychologically* (Mystery -\\> Struggle -\\> Resolution).^13^\n\n#### 3.2.1 The ABT Framework (And, But, Therefore)\n\nA powerful tool for structuring scientific and academic narratives is the ABT model.^13^ This structure moves the lecture from a static list of facts to a dynamic journey of discovery.\n\n- **AND (The Setup):** Establish the context and the known world. \\\"We know X is true, AND we know Y is true\\...\\\" (This aligns with the \\\"Exposition\\\" phase of a story).^14^\n\n- **BUT (The Conflict):** Introduce the problem, the gap, or the contradiction. \\\"\\...BUT, there is a paradox. X and Y should not coexist. Or, we observe Z which contradicts X.\\\" This creates a \\\"Knowledge Gap\\\" that the human brain instinctively wants to close.^13^\n\n- **THEREFORE (The Resolution):** The lecture content serves as the solution to the conflict. \\\"\\...THEREFORE, we must apply this new theory/equation/concept to resolve the paradox.\\\".^13^\n\n#### 3.2.2 The Mystery and the Hook\n\nEvery great lecture should begin with a \\\"Hook\\\"---an inciting incident that grabs attention within the first 60 seconds.^14^ Instead of beginning with \\\"Today we are covering Chapter 5: Thermodynamics,\\\" begin with a problem.\n\n- **Example (Biology):** Instead of listing facts about viscosity, tell the story of a \\\"confused robot\\\" or a biological anomaly (e.g., \\\"Why can this lizard walk on water while this one sinks?\\\").^15^\n\n- **Example (History):** Instead of \\\"The Causes of WWI,\\\" start with a specific diplomatic cable that was ignored. \\\"Why did this one message fail to stop a war?\\\".^5^\n\n- **The \\\"Mystery Bag\\\" Approach:** The instructor can physically or digitally present a \\\"mystery\\\" (an unidentified object, an anomalous data point) at the start of class, promising that by the end of the 75 minutes, the students will possess the tools to solve it.^16^\n\n#### 3.2.3 From Headings to Plot Points\n\nTo translate a textbook chapter into this arc, rename the sub-headings.\n\n- *Textbook Heading:* \\\"The Krebs Cycle\\\"\n\n- *Lecture Plot Point:* \\\"The Energy Crisis: How the Cell Extracts Value from Waste\\\"\n\n- *Textbook Heading:* \\\"The Treaty of Versailles\\\"\n\n- *Lecture Plot Point:* \\\" The Peace that Failed: Seeds of a New Conflict\\\" This subtle shift frames the content as active events rather than static nouns.^17^\n\n## 4. Temporal Design: Architecting the 75 Minutes\n\nA 75-minute lecture is a significant block of time. Without a strict temporal architecture, it tends to drift, leading to rushing at the end and omitting the synthesis phase. We will utilize the **Bookend Method** combined with **Modular Chunking** to structure the experience.^6^\n\n### 4.1 The Bookend Architecture\n\nThe \\\"Bookend\\\" approach treats the start and end of class as distinct, high-value pedagogical zones that frame the content.\n\n- **The Opening Bookend (Engagement):** This is for activation. It bridges the gap between the student\\'s life outside the hall and the content inside. It must involve active prediction or recall.^6^\n\n- **The Middle Segments (Instruction):** These are the \\\"Chunks\\\" of content, interspersed with active learning.\n\n- **The Closing Bookend (Reflection):** This is for synthesis. It ensures that the loose strands of the lecture are tied together and encoded.^6^\n\n### 4.2 The 75-Minute Timeline Template\n\nThe following table provides a minute-by-minute template for a high-engagement 75-minute session, integrating the Narrative Arc and Active Learning breaks.\n\n+-------------------+------------------------------+-----------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+\n| **Time Block**    | **Phase**                    | **Activity & Purpose**                                                                        | **Cognitive Load Focus**                                                          |\n+-------------------+------------------------------+-----------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+\n| **00:00 - 00:05** | **The Hook (Exposition)**    | **Narrative Start:** Present the \\\"Mystery\\\" or \\\"Paradox\\\" (The \\\"BUT\\\" of the ABT model).   | **Activate Prior Knowledge:** Primes schemas for new info.^19^                    |\n|                   |                              |                                                                                               |                                                                                   |\n|                   |                              | **Activity:** Low-stakes poll (Prediction/Baseline).                                          |                                                                                   |\n+-------------------+------------------------------+-----------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+\n| **00:05 - 00:20** | **Chunk 1 (Rising Action)**  | **Micro-Lecture:** Core Concept 1.                                                            | **Manage Intrinsic Load:** Introduce vocabulary and basic mechanism.              |\n|                   |                              |                                                                                               |                                                                                   |\n|                   |                              | **Focus:** Analysis of the problem. High-visual slides.                                       |                                                                                   |\n+-------------------+------------------------------+-----------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+\n| **00:20 - 00:25** | **Active Break 1**           | **Peer Instruction:** \\\"Think-Pair-Share\\\" on Concept 1.                                      | **Reset Vigilance:** Switch from passive to active. Increase Germane Load.        |\n|                   |                              |                                                                                               |                                                                                   |\n|                   |                              | **Poll:** ConcepTest (Application level).                                                     |                                                                                   |\n+-------------------+------------------------------+-----------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+\n| **00:25 - 00:40** | **Chunk 2 (The Climax)**     | **Micro-Lecture:** Core Concept 2 (The most complex/difficult material).                      | **Max Intrinsic Load:** This is the heavy lifting. Minimize Extraneous load here. |\n|                   |                              |                                                                                               |                                                                                   |\n|                   |                              | **Focus:** The mechanism/solution.                                                            |                                                                                   |\n+-------------------+------------------------------+-----------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+\n| **00:40 - 00:45** | **Active Break 2**           | **\\\"The Change-Up\\\":** A complete state change.                                               | **Cognitive Restoration:** Allow working memory to consolidate Chunk 2.^3^        |\n|                   |                              |                                                                                               |                                                                                   |\n|                   |                              | **Activity:** Video clip, physical stretch, or \\\"Sketch the Concept\\\" (non-digital).          |                                                                                   |\n+-------------------+------------------------------+-----------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+\n| **00:45 - 00:55** | **Chunk 3 (Falling Action)** | **Micro-Lecture:** Application/Implications.                                                  | **Elaboration:** connecting abstract concepts to concrete reality.^20^            |\n|                   |                              |                                                                                               |                                                                                   |\n|                   |                              | **Focus:** Real-world examples or case studies.                                               |                                                                                   |\n+-------------------+------------------------------+-----------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+\n| **00:55 - 01:05** | **Synthesis Activity**       | **Poll Everywhere:** \\\"The Debate\\\" or Complex Case Study.                                    | **Transfer:** Testing if students can use knowledge in new context.^21^           |\n|                   |                              |                                                                                               |                                                                                   |\n|                   |                              | **Activity:** Students apply Concept 1 & 2 to a novel problem.                                |                                                                                   |\n+-------------------+------------------------------+-----------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+\n| **01:05 - 01:10** | **The Summary**              | **Narrative Close:** Return to the opening Mystery. Explain the solution (\\\"Therefore\\...\\\"). | **Schema Closure:** tying the narrative loop.^13^                                 |\n+-------------------+------------------------------+-----------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+\n| **01:10 - 01:15** | **Reflection (Resolution)**  | **Activity:** \\\"Muddiest Point\\\" or \\\"Minute Paper\\\" via Poll.                                | **Metacognition:** Students self-assess their understanding.^22^                  |\n|                   |                              |                                                                                               |                                                                                   |\n|                   |                              | **Logistics:** Brief reminders.                                                               |                                                                                   |\n+===================+==============================+===============================================================================================+===================================================================================+\n\n## 5. Active Learning Systems: Poll Everywhere and Peer Instruction\n\nIn a 300-seat lecture hall, individual dialogue with the instructor is mathematically impossible. Active Learning Systems (ALS) or Student Response Systems (SRS) like Poll Everywhere solve this scale problem by allowing *collective* dialogue. However, the technology is only as effective as the pedagogy behind it.\n\n### 5.1 The Psychology of Anonymity and Safety\n\nLarge lecture halls are high-risk social environments. Students fear \\\"looking stupid\\\" in front of hundreds of peers. This fear inhibits question-asking and risk-taking.^21^\n\n- **The Power of Anonymity:** Poll Everywhere allows for anonymous participation. Research indicates that anonymity is crucial for equity and inclusion; it encourages marginalized voices to participate and allows students to admit confusion without shame.^23^\n\n- **Honest Feedback:** When students are anonymous, they are more likely to answer honestly rather than trying to guess the \\\"teacher\\'s password\\\" (the answer they think the teacher wants).^24^ This provides the instructor with accurate data on class comprehension.\n\n### 5.2 Eric Mazur's Peer Instruction (PI) Workflow\n\nThe gold standard for large-class active learning is Eric Mazur's Peer Instruction model.^4^ It turns the lecture hall into a peer-tutoring network.\n\n#### 5.2.1 The Step-by-Step PI Cycle\n\n1.  **Concept Presentation (7-10 min):** The instructor lectures on a specific concept.\n\n2.  **The ConcepTest (1-2 min):** A multiple-choice question is posed. Crucially, this question must be *conceptual*, not factual. It should test understanding of a mechanism, not recall of a date.^25^\n\n3.  **Individual Silent Vote:** Students vote individually. *Silence is enforced.* This forces individual cognitive commitment.^27^\n\n4.  **The Data Check (Instructor Only):** The instructor checks the histogram.\n\n    - *\\<30% Correct:* The concept was not understood. **Stop.** Re-teach the concept immediately using a different explanation. Do not move to peer discussion (students will just reinforce misconceptions).\n\n    - *30% - 70% Correct:* **The Sweet Spot.** Optimal for discussion. Proceed to Step 5.^27^\n\n    - *\\>70% Correct:* The concept is too easy. Briefly explain the correct answer and move on.\n\n5.  **Peer Discussion (2-4 min):** \\\"Find a neighbor who has a different answer than you. Convince them you are right.\\\".^25^ The room erupts in discussion. This is where the learning happens---students must articulate their reasoning, which forces deep processing (Germane Load).\n\n6.  **Re-Vote (1 min):** Students vote again.\n\n7.  **Explanation (Closure):** The instructor shows the results (usually a shift toward the correct answer) and explains *why* the correct answer is right and *why* the distractors are wrong.^28^\n\n### 5.3 Question Design: Moving Up Bloom's Taxonomy\n\nTo make the 75 minutes engaging, Poll Everywhere questions must move beyond simple recall. We utilize Bloom's Taxonomy to design questions that prompt critical thinking.^29^\n\n  --------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  **Taxonomy Level**      **Poll Type**                  **Question Stem Examples**\n  ----------------------- ------------------------------ -------------------------------------------------------------------------------------------------------------\n  **Remember**            Multiple Choice (Rapid Fire)   \\\"What is the primary definition of\\...?\\\" (Use for baseline check only) ^29^\n\n  **Understand**          Word Cloud                     \\\"In one word, what is the main emotion of the protagonist in this scene?\\\" ^31^\n\n  **Apply**               Clickable Image                \\\"Click on the specific area of the graph where the reaction becomes unstable.\\\" ^32^\n\n  **Analyze**             Multiple Choice (PI)           \\\"Why did the author choose this metaphor? A) To highlight X, B) To contrast with Y\\...\\\" ^33^\n\n  **Evaluate**            Rank Order                     \\\"Rank these three policy proposals from most effective to least effective based on the criteria\\...\\\" ^29^\n\n  **Create**              Open Ended / Upvote            \\\"Propose a solution to the problem we just discussed. Upvote the best solution.\\\" ^32^\n  --------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n**The \\\"Muddiest Point\\\":** As a closing activity, use an open-ended poll: \\\"What was the most confusing point in today\\'s lecture?\\\" This provides critical feedback for the instructor to adjust the *next* lecture\\'s opening hook.^22^\n\n## 6. Visual Rhetoric and Accessibility: Designing for the Back Row\n\nIn a 300-seat auditorium, the screen is the visual anchor. If the slides are illegible, the lecture fails. Visual design is not about aesthetics; it is about cognitive accessibility and reducing Extraneous Cognitive Load.\n\n### 6.1 The Physics of Legibility\n\nVisibility is determined by the \\\"Visual Angle\\\"---the size of the text relative to the distance of the viewer. Text must occupy a minimum number of \\\"arc minutes\\\" in the viewer\\'s field of vision to be legible.^34^\n\n#### 6.1.1 The Font Size Formula\n\nA standard rule of thumb for large venues is the \\\"1 inch per 10 feet\\\" rule, or the formula:\n\n![](media/image1.png){width=\"6.458333333333333in\" height=\"0.5242388451443569in\"}\n\nWhere:\n\n- ![](media/image3.png){width=\"0.18401465441819773in\" height=\"0.25557633420822395in\"} = Height of text (cm)\n\n- ![](media/image2.png){width=\"0.13748468941382327in\" height=\"0.2643930446194226in\"} = Distance to furthest viewer (cm).^35^\n\nFor a typical university lecture hall (depth \\~20-30 meters):\n\n  -----------------------------------------------------------------------------------------------------------------------------\n  **Element**             **Minimum Font Size (Points)**   **Reason**\n  ----------------------- -------------------------------- --------------------------------------------------------------------\n  **Headings**            **32pt - 44pt**                  Must be scanned instantly. Anchors the slide topic.^8^\n\n  **Body Text**           **24pt - 28pt**                  Anything smaller is invisible to the back row..^37^\n\n  **Captions/Labels**     **20pt**                         If it\\'s important enough to be on the slide, it must be readable.\n  -----------------------------------------------------------------------------------------------------------------------------\n\n**The 6x6 Rule:** A slide should have no more than **6 lines of text**, and each line should have no more than **6 words**. This prevents the instructor from reading the slide (which induces the Redundancy Effect).^36^\n\n### 6.2 Accessibility and Inclusivity\n\n- **Contrast:** Use high contrast ratios. Dark text on a light background (e.g., black on cream) or light text on a dark background (white on dark blue) are best. Avoid red/green combinations which are invisible to color-blind students.^8^\n\n- **Alt Text:** All images must be described verbally. \\\"As you can see here\\...\\\" is insufficient. \\\"As you can see in this graph, the trend line rises sharply at 1990\\...\\\" is accessible.^36^\n\n- **Skeletal Slides:** Instead of providing full slides, provide \\\"skeletal\\\" handouts (guided notes) with missing words or blank graphs. Students must listen and write to complete them. This keeps them active and prevents \\\"zoning out\\\" while still providing a scaffold.^39^\n\n## 7. Delivery: The Instructor as Performer\n\nIn a large lecture hall, the instructor is a performer. This is not about being \\\"entertaining\\\" in the theatrical sense, but about using **Stage Presence** to command attention and direct cognitive focus.^40^\n\n### 7.1 The Performance Triangle\n\nEffective delivery relies on three pillars: Mental Focus, Physical Life, and Speaking Voice.^41^\n\n- **Physical Life (The Stage Map):** Do not hide behind the podium. The podium is a barrier that disconnects the instructor from the students. Use the \\\"T-Zone\\\" (moving across the front of the stage and, if possible, down the center aisle).\n\n  - *Anchoring Concepts to Space:* Use the stage to map the narrative. Stand \\\"Stage Left\\\" when discussing the \\\"Problem\\\" (The \\'But\\' of the ABT model). Move to \\\"Stage Right\\\" when discussing the \\\"Solution\\\" (The \\'Therefore\\'). This uses the students\\' spatial memory to reinforce the conceptual arc.^40^\n\n- **The Speaking Voice:** A monotone voice induces the \\\"vigilance decrement\\\" rapidly.\n\n  - *The Power of the Pause:* Silence is a tool. After asking a question or stating a profound point, pause for **3-5 seconds**. This feels uncomfortable to the speaker, but it allows the audience the processing time (\\\"White Space\\\") needed to encode the information.^16^\n\n  - *Projection:* Always use the microphone. Even if you have a \\\"loud voice,\\\" relying on projection alone causes vocal strain and often results in a \\\"shouting\\\" tone that is fatiguing to listen to. The microphone allows for vocal nuance---whispering for effect, slowing down for emphasis---while remaining audible.^42^\n\n### 7.2 Managing the \\\"Room\\\"\n\n- **Eye Contact:** In a sea of 300 faces, individual eye contact is impossible. Use the \\\"Sector Scan\\\" technique. Divide the room into 4-6 sectors (Front Left, Back Right, Balcony, etc.). Systematically move your gaze between these sectors. This creates the illusion of individual connection for everyone in that sector.^40^\n\n- **The \\\"Community Forum\\\" Protocol:** In a large class, random hand-raising can derail the timeline. Establish a protocol: \\\"I will take questions at the end of each 15-minute chunk.\\\" Better yet, use the Poll Everywhere \\\"Q&A\\\" feature where students can submit questions digitally and \\\"upvote\\\" the most popular ones. This ensures you address the *class\\'s* most pressing question, not just the question of the loudest student.^1^\n\n## 8. Conclusion: The Lecture as Experience\n\nDesigning a lecture for 300 students is a feat of engineering as much as it is an act of teaching. It requires the instructor to balance the logistical constraints of the physical space with the biological constraints of the student brain. By rejecting the \\\"Transmission Model\\\" and embracing a structured, narrative-driven, and active approach, the 75-minute block can be transformed.\n\nThe textbook provides the raw materials---the facts, the dates, the theories. But the lecture provides the architecture. Through **Cognitive Load management**, we ensure the brain can process the material. Through **Narrative Arcs (ABT)**, we give the material meaning and emotional resonance. Through **Poll Everywhere and Peer Instruction**, we give the students a voice and force them to engage in the hard work of synthesis. And through **Visual and Performance design**, we ensure that every student, from the front row to the back of the balcony, is part of a shared, dynamic learning experience. The result is a course that does not just \\\"cover content,\\\" but cultivates understanding.\n\n**References:** .^1^\n\n#### Works cited\n\n1.  Teaching large enrollment courses - Teaching@UW, accessed January 17, 2026, [[https://teaching.washington.edu/course-design/large-enrollment-courses/]{.underline}](https://teaching.washington.edu/course-design/large-enrollment-courses/)\n\n2.  Challenging Cognitive Load Theory: The Role of Educational Neuroscience and Artificial Intelligence in Redefining Learning Efficacy - PMC - PubMed Central, accessed January 17, 2026, [[https://pmc.ncbi.nlm.nih.gov/articles/PMC11852728/]{.underline}](https://pmc.ncbi.nlm.nih.gov/articles/PMC11852728/)\n\n3.  Teaching Strategies: Large Classes and Lectures - CRLT - University of Michigan, accessed January 17, 2026, [[https://crlt.umich.edu/tstrategies/tsllc]{.underline}](https://crlt.umich.edu/tstrategies/tsllc)\n\n4.  Peer Instruction - PhysPort Methods and Materials, accessed January 17, 2026, [[https://www.physport.org/methods/Peer_Instruction]{.underline}](https://www.physport.org/methods/Peer_Instruction)\n\n5.  Considerations for Large Lecture Classes \\| Center for Teaching \\..., accessed January 17, 2026, [[https://teaching.berkeley.edu/considerations-large-lecture-classes]{.underline}](https://teaching.berkeley.edu/considerations-large-lecture-classes)\n\n6.  Instructional Methods \\| California State University, Bakersfield, accessed January 17, 2026, [[https://www.csub.edu/ftlc/instructionalmethods.shtml]{.underline}](https://www.csub.edu/ftlc/instructionalmethods.shtml)\n\n7.  Chunking in the Classroom - The Great Teaching Toolkit, accessed January 17, 2026, [[https://evidencebased.education/resource/chunking-in-the-classroom/]{.underline}](https://evidencebased.education/resource/chunking-in-the-classroom/)\n\n8.  Creating accessible PowerPoint presentations \\| Centre for Teaching and Learning, accessed January 17, 2026, [[https://www.ctl.ox.ac.uk/creating-accessible-powerpoint-presentations]{.underline}](https://www.ctl.ox.ac.uk/creating-accessible-powerpoint-presentations)\n\n9.  Active Learning: Evidence-based Teaching: Teaching Resources - Center for Innovative Teaching & Learning - Indiana University Bloomington, accessed January 17, 2026, [[https://citl.indiana.edu/teaching-resources/evidence-based/active-learning.html]{.underline}](https://citl.indiana.edu/teaching-resources/evidence-based/active-learning.html)\n\n10. Effective Lecturing \\| Dartmouth Center for the Advancement of \\..., accessed January 17, 2026, [[https://dcal.dartmouth.edu/resources/teaching-methods/effective-lecturing]{.underline}](https://dcal.dartmouth.edu/resources/teaching-methods/effective-lecturing)\n\n11. Teaching a Course as a Narrative Arc \\| Faculty Focus, accessed January 17, 2026, [[https://www.facultyfocus.com/articles/course-design-ideas/teaching-a-course-as-a-narrative-arc/]{.underline}](https://www.facultyfocus.com/articles/course-design-ideas/teaching-a-course-as-a-narrative-arc/)\n\n12. To Teach Science, Tell Stories James A. Rose - DukeSpace, accessed January 17, 2026, [[https://dukespace.lib.duke.edu/server/api/core/bitstreams/461d82f9-7762-4a7d-b871-eca2b8d7f449/content]{.underline}](https://dukespace.lib.duke.edu/server/api/core/bitstreams/461d82f9-7762-4a7d-b871-eca2b8d7f449/content)\n\n13. Storytelling in Science Writing - WRITING IN THE SCIENCES - University of Guelph, accessed January 17, 2026, [[https://writinginthesciences.uoguelph.ca/storytelling-in-science-writing/]{.underline}](https://writinginthesciences.uoguelph.ca/storytelling-in-science-writing/)\n\n14. How to Structure a Story Arc in Creative Writing - Centre of Excellence, accessed January 17, 2026, [[https://www.centreofexcellence.com/how-to-structure-a-story-arc/]{.underline}](https://www.centreofexcellence.com/how-to-structure-a-story-arc/)\n\n15. HOW TO TELL A SCIENCE STORY - Accelerate Learning, accessed January 17, 2026, [[https://blog.acceleratelearning.com/science-storytelling-in-education]{.underline}](https://blog.acceleratelearning.com/science-storytelling-in-education)\n\n16. Storytelling: Bringing the power of stories to your teaching - Ditch That Textbook, accessed January 17, 2026, [[https://ditchthattextbook.com/storytelling/]{.underline}](https://ditchthattextbook.com/storytelling/)\n\n17. How to turn a chapter of your novel into a short story - Blog - Magazine - Mslexia, accessed January 17, 2026, [[https://mslexia.co.uk/magazine/blog/how-to-turn-a-chapter-of-your-novel-into-a-short-story/]{.underline}](https://mslexia.co.uk/magazine/blog/how-to-turn-a-chapter-of-your-novel-into-a-short-story/)\n\n18. Recommended Time Line for 75-Minute Session - ResearchGate, accessed January 17, 2026, [[https://www.researchgate.net/figure/Recommended-Time-Line-for-75-Minute-Session_tbl1_351483968]{.underline}](https://www.researchgate.net/figure/Recommended-Time-Line-for-75-Minute-Session_tbl1_351483968)\n\n19. Using Mini-Lectures to Create Active Learning Space - The Scholarly Teacher, accessed January 17, 2026, [[https://www.scholarlyteacher.com/post/using-mini-lectures-to-create-active-learning-space]{.underline}](https://www.scholarlyteacher.com/post/using-mini-lectures-to-create-active-learning-space)\n\n20. Designing Effective Lectures -- Teaching Resources - Sites at Dartmouth, accessed January 17, 2026, [[https://sites.dartmouth.edu/teachingresources/2025/07/24/effective-lecturing/]{.underline}](https://sites.dartmouth.edu/teachingresources/2025/07/24/effective-lecturing/)\n\n21. Maximizing Impact: Effective Strategies for Large Lectures - L&S Instructional Design Collaborative, accessed January 17, 2026, [[https://idc.ls.wisc.edu/ls-design-for-learning-series/strategies-for-large-lectures/]{.underline}](https://idc.ls.wisc.edu/ls-design-for-learning-series/strategies-for-large-lectures/)\n\n22. Active Learning in the College Classroom, accessed January 17, 2026, [[https://www.gvsu.edu/cms4/asset/4E1EDE14-CCD7-E36A-C1461EF14C472362/25f_nfo_active_learning_in_the_college_classroom_handout.pdf]{.underline}](https://www.gvsu.edu/cms4/asset/4E1EDE14-CCD7-E36A-C1461EF14C472362/25f_nfo_active_learning_in_the_college_classroom_handout.pdf)\n\n23. Poll Everywhere Scientifically Demonstrated to Improve Audience Outcomes, accessed January 17, 2026, [[https://blog.polleverywhere.com/poll-everywhere-scientifically-demonstrated-to-improve-audience-outcomes]{.underline}](https://blog.polleverywhere.com/poll-everywhere-scientifically-demonstrated-to-improve-audience-outcomes)\n\n24. A Delicate Balance: Integrating Active Learning into a Large Lecture Course - PMC - NIH, accessed January 17, 2026, [[https://pmc.ncbi.nlm.nih.gov/articles/PMC2592041/]{.underline}](https://pmc.ncbi.nlm.nih.gov/articles/PMC2592041/)\n\n25. Peer Instruction and Concept Tests \\| Centre for Teaching Excellence - University of Waterloo, accessed January 17, 2026, [[https://uwaterloo.ca/centre-for-teaching-excellence/catalogs/tip-sheets/peer-instruction-and-concept-tests]{.underline}](https://uwaterloo.ca/centre-for-teaching-excellence/catalogs/tip-sheets/peer-instruction-and-concept-tests)\n\n26. Peer Instruction \\| Alliant International University Center for Teaching Excellence, accessed January 17, 2026, [[https://cte.alliant.edu/peer-instruction/]{.underline}](https://cte.alliant.edu/peer-instruction/)\n\n27. Harvard Professor Eric Mazur\\'s Peer Instruction Method - Wooclap, accessed January 17, 2026, [[https://www.wooclap.com/en/blog/harvard-professor-eric-mazurs-peer-instruction-method/]{.underline}](https://www.wooclap.com/en/blog/harvard-professor-eric-mazurs-peer-instruction-method/)\n\n28. Peer Instruction -- Eric Mazur\\'s approach -- Active Learning at King\\'s, accessed January 17, 2026, [[https://blogs.kcl.ac.uk/activelearning/2019/05/02/peer-instruction-eric-mazurs-approach/]{.underline}](https://blogs.kcl.ac.uk/activelearning/2019/05/02/peer-instruction-eric-mazurs-approach/)\n\n29. Bloom\\'s Taxonomy Question Stems Guide \\| PDF \\| Critical Thinking \\| Cognition - Scribd, accessed January 17, 2026, [[https://www.scribd.com/document/727343202/Bloom-s-Taxonomy]{.underline}](https://www.scribd.com/document/727343202/Bloom-s-Taxonomy)\n\n30. Bloom\\'s Taxonomy Question Stems: 100+ Engaging Prompts - Top Hat, accessed January 17, 2026, [[https://tophat.com/blog/blooms-taxonomy-question-stems/]{.underline}](https://tophat.com/blog/blooms-taxonomy-question-stems/)\n\n31. Promoting active learning in large classes \\| Poll Everywhere, accessed January 17, 2026, [[https://www.polleverywhere.com/case-studies/lecture-method]{.underline}](https://www.polleverywhere.com/case-studies/lecture-method)\n\n32. How to Boost Student Engagement in Large Lectures With Poll Everywhere, accessed January 17, 2026, [[https://blog.polleverywhere.com/student-engagement-large-lectures]{.underline}](https://blog.polleverywhere.com/student-engagement-large-lectures)\n\n33. BLOOM\\'S TAXONOMY : More extended examples of skills, cue words and question stems, accessed January 17, 2026, [[https://learningcenters.rutgers.edu/sites/default/files/2023-05/Blooms-Question-Suggestions.pdf]{.underline}](https://learningcenters.rutgers.edu/sites/default/files/2023-05/Blooms-Question-Suggestions.pdf)\n\n34. Font Size and Legibility for Videowall Content - Extron, accessed January 17, 2026, [[https://www.extron.com/article/videowallfontsize]{.underline}](https://www.extron.com/article/videowallfontsize)\n\n35. Signposting: reading distance and font size - JekaShop, accessed January 17, 2026, [[https://jekashop.com/knowledge-base/signposting-reading-distance-font-size]{.underline}](https://jekashop.com/knowledge-base/signposting-reading-distance-font-size)\n\n36. Guidelines for Accessibility in Presentations \\| ASHG, accessed January 17, 2026, [[https://www.ashg.org/wp-content/uploads/2024/08/Guidelines-for-Accessibility-in-Presentations.pdf]{.underline}](https://www.ashg.org/wp-content/uploads/2024/08/Guidelines-for-Accessibility-in-Presentations.pdf)\n\n37. Best Practices for Creating PowerPoint Slides \\| Office of Accessibility Resources and Services \\| High Point University, accessed January 17, 2026, [[https://www.highpoint.edu/oars/best-practices-for-creating-powerpoint-slides/]{.underline}](https://www.highpoint.edu/oars/best-practices-for-creating-powerpoint-slides/)\n\n38. The Essential Guide to Font Size in Presentations: Finding the Perfect Balance, accessed January 17, 2026, [[https://www.magicslides.app/blog/guide-font-size-presentations-balance]{.underline}](https://www.magicslides.app/blog/guide-font-size-presentations-balance)\n\n39. Lecturing Effectively \\| Centre for Teaching Excellence - University of Waterloo, accessed January 17, 2026, [[https://uwaterloo.ca/centre-for-teaching-excellence/catalogs/tip-sheets/lecturing-effectively]{.underline}](https://uwaterloo.ca/centre-for-teaching-excellence/catalogs/tip-sheets/lecturing-effectively)\n\n40. Mastering Stage Presence: The Secret to Powerful Public Speaking - SketchBubble, accessed January 17, 2026, [[https://www.sketchbubble.com/blog/mastering-stage-presence-the-secret-to-powerful-public-speaking/]{.underline}](https://www.sketchbubble.com/blog/mastering-stage-presence-the-secret-to-powerful-public-speaking/)\n\n41. Mastering Stage Presence: How to Present to Any Audience - The Great Courses Plus, accessed January 17, 2026, [[https://www.thegreatcoursesplus.com/mastering-stage-presence-how-to-present-to-any-audience]{.underline}](https://www.thegreatcoursesplus.com/mastering-stage-presence-how-to-present-to-any-audience)\n\n42. Best Practices for Accessible In-Person Lectures - UAF Center for Teaching and Learning, accessed January 17, 2026, [[https://ctl.uaf.edu/2025/10/06/best-practices-for-accessible-in-person-lectures/]{.underline}](https://ctl.uaf.edu/2025/10/06/best-practices-for-accessible-in-person-lectures/)\n\n43. Engaging Individual Students in Big Classes \\| Poll Everywhere, accessed January 17, 2026, [[https://www.polleverywhere.com/case-studies/flexible-learning]{.underline}](https://www.polleverywhere.com/case-studies/flexible-learning)\n\n44. Poll Everywhere: Audience Response System - Columbia University, accessed January 17, 2026, [[https://ctl.columbia.edu/ars/]{.underline}](https://ctl.columbia.edu/ars/)\n\n45. How to Use Chunking in the Classroom - Edutopia, accessed January 17, 2026, [[https://www.edutopia.org/article/chunking-content-classroom/]{.underline}](https://www.edutopia.org/article/chunking-content-classroom/)\n\n46. 10 Tips to Help You Master Stage Presence - Brian Tracy, accessed January 17, 2026, [[https://www.briantracy.com/blog/public-speaking/stage-presence/]{.underline}](https://www.briantracy.com/blog/public-speaking/stage-presence/)\n\n47. Learn About Narrative Arcs: Definition, Examples, and How to Create a Narrative Arc in Your Writing - MasterClass, accessed January 17, 2026, [[https://www.masterclass.com/articles/what-are-the-elements-of-a-narrative-arc-and-how-do-you-create-one-in-writing]{.underline}](https://www.masterclass.com/articles/what-are-the-elements-of-a-narrative-arc-and-how-do-you-create-one-in-writing)\n\n48. Font size for different viewing distances - Mississauga Signs, accessed January 17, 2026, [[https://www.ssksigns.com/font-size-for-different-viewing-distances/]{.underline}](https://www.ssksigns.com/font-size-for-different-viewing-distances/)\n\n49. How to Use Storytelling to Teach Science and History - Earthschooling, accessed January 17, 2026, [[https://earthschooling.info/thebearthinstitute/how-to-use-storytelling-to-teach-science-and-history/]{.underline}](https://earthschooling.info/thebearthinstitute/how-to-use-storytelling-to-teach-science-and-history/)\n",
        "plugins/lecture-designer/skills/lecture-designer/pedagogy/slide-design-guide.md": "# Slide Design Guide: Evidence-Based Best Practices\r\n\r\nThis guide distills principles from Nancy Duarte's *slide:ology* and *DataStory*, Garr Reynolds' *Presentation Zen*, and Carmine Gallo's *Talk Like TED*.\r\n\r\n## Core Philosophy\r\n\r\n### The Fundamental Shift\r\nSlides are **visual aids**, not documents. They support the speakerthey don't replace them.\r\n\r\n> \"If a slide contains more than 75 words, it has become a document, not a presentation.\"  Nancy Duarte\r\n\r\n> \"No more than 6 words on a slide. EVER.\"  Seth Godin\r\n\r\n**The problem**: Audiences cannot read and listen simultaneously. When you show text-heavy slides, they read instead of listening to you.\r\n\r\n**The solution**: Use slides as a visual backdrop that amplifies your narrative, not as a teleprompter.\r\n\r\n---\r\n\r\n## The Numbers That Matter\r\n\r\n| Rule | Source | Application |\r\n|------|--------|-------------|\r\n| **75-word limit** | Duarte | More than 75 words = it's a document |\r\n| **6-word rule** | Godin/Reynolds | Aim for 6 words or fewer per slide |\r\n| **3-second test** | Duarte | Audiences must grasp content within 3 seconds |\r\n| **18-minute rule** | TED | Ideal presentation length; cognitive overload after |\r\n| **10-minute soft breaks** | Gallo | If longer than 18 min, break every 10 minutes |\r\n| **Rule of Three** | Gallo | Organize around 3 key messages |\r\n| **28pt minimum** | Duarte | Minimum font size for keynotes |\r\n| **65% stories** | Stevenson | Top TED talks are 65% stories, not data |\r\n\r\n---\r\n\r\n## Visual Design Principles\r\n\r\n### The CRAP Framework (Reynolds)\r\nFour principles that make slides visually effective:\r\n\r\n**1. Contrast**\r\n- Make different things VERY different\r\n- Create clear focal points through size, color, weight\r\n- Use the \"smallest effective difference\"just notable, no more\r\n\r\n**2. Repetition**\r\n- Consistent backgrounds, typography, colors throughout\r\n- Creates unity and professionalism\r\n- Audiences know what to expect\r\n\r\n**3. Alignment**\r\n- Every element connects via invisible grid lines\r\n- Nothing should look randomly placed\r\n- Use grid systems (5x5 is common)\r\n\r\n**4. Proximity**\r\n- Group related items together\r\n- Separate unrelated items\r\n- Viewers assume items near each other are related\r\n\r\n### Signal-to-Noise Ratio\r\nHigher signal = cleaner design. Remove:\r\n- Unnecessary logos (keep only on first/last slide)\r\n- Decorative backgrounds that compete with content\r\n- 3D effects on charts (distorts data)\r\n- Gradient fills and drop shadows\r\n- Gridlines in charts (unless essential)\r\n- Clip art (always)\r\n\r\n### Empty Space (Negative Space)\r\n- Empty space is a powerful design element, not wasted space\r\n- Gives slides \"visual breathing room\"\r\n- Implies elegance and sophistication\r\n- If you need to sacrifice whitespace, split into multiple slides\r\n\r\n---\r\n\r\n## Typography Rules\r\n\r\n### Font Selection\r\n- **Sans-serif for projection** (Helvetica, Arial, Calibri)clearer at distance\r\n- **Maximum 2 fonts**: one for headlines, one for body\r\n- Use color or italics for emphasis, not additional fonts\r\n- Avoid decorative fonts that reduce legibility\r\n\r\n### Size Guidelines\r\n- **Headlines**: 32pt or larger\r\n- **Body text**: 28pt minimum for keynotes\r\n- **Never below 24pt** (if you need smaller, you have a document)\r\n- **Test**: Can you read it at 66% zoom in slide sorter view?\r\n\r\n### Text Formatting\r\n- Left alignment is standard (easier to read)\r\n- Avoid ALL CAPS for more than a few words\r\n- Watch line spacingtight spacing reduces readability\r\n- Avoid widows (single words on last line)\r\n\r\n---\r\n\r\n## Image Best Practices\r\n\r\n### Photo Selection\r\n- Use high-quality, professional images\r\n- Build a cohesive \"family\" of images (similar lighting, color tones)\r\n- Avoid cheesy staged stock photography\r\n- Ensure images reflect your audience's culture/context\r\n- Use rule of thirds for composition\r\n\r\n### Full-Bleed Images\r\n- Make images fill the entire screen\r\n- Creates cinematic, immersive effect\r\n- Allow images to bleed off edgesbrain fills in the rest\r\n- Pair with minimal overlaid text (high contrast)\r\n\r\n### The Picture Superiority Effect\r\n- Hear information  remember 10% after 3 days\r\n- Add a picture  remember 65% after 3 days\r\n- **Images help you remember 6x more than words alone**\r\n\r\n### Direction and Gaze\r\n- If showing people, have them looking toward your content, not away\r\n- Use directional elements (arrows, pointing, eye gaze) to guide attention\r\n- Place subjects intentionallytop = dominant, isolated = more weight\r\n\r\n---\r\n\r\n## Data Visualization\r\n\r\n### Chart Selection\r\nStick with charts everyone understands:\r\n\r\n| Chart Type | Use When | Observation Language |\r\n|------------|----------|---------------------|\r\n| **Bar charts** | Comparing static values | Adjectives: \"higher,\" \"significant,\" \"dramatic\" |\r\n| **Pie charts** | Showing parts of whole | Adjectives: \"majority,\" \"substantial portion\" |\r\n| **Line charts** | Trends over time | Adverbs: \"rapidly,\" \"steadily,\" \"gradually\" |\r\n\r\n### The Five Data Slide Rules (Duarte)\r\n\r\n1. **Tell the Truth**: Treat data with integrity; avoid decoration that obscures\r\n2. **Get to the Point**: Data slides are about meaning, not data itself\r\n3. **Pick the Right Tool**: Match chart type to your conclusion\r\n4. **Highlight What's Important**: Use color/annotations to draw attention\r\n5. **Keep It Simple**: Eliminate visual junk\r\n\r\n### Chart Makeover Checklist\r\n- [ ] Remove 3D effects (distorts perception)\r\n- [ ] Remove decorative gradients\r\n- [ ] Minimize gridlines (light gray if needed)\r\n- [ ] Add clear, factual title\r\n- [ ] Add observation (your interpretation) above title\r\n- [ ] Highlight key data point with color\r\n- [ ] Ensure legend is clear\r\n- [ ] Test in grayscale (1 in 12 people are colorblind)\r\n\r\n### Annotations\r\nDon't let charts speak for themselvesannotate:\r\n- Highlight specific data points with circles/arrows\r\n- Add math (percentage changes, comparisons)\r\n- Use callout boxes to explain significance\r\n- Place observation above the chart title\r\n\r\n---\r\n\r\n## Color Guidelines\r\n\r\n### Color Schemes (from color wheel theory)\r\n\r\n| Scheme | Description | Best For |\r\n|--------|-------------|----------|\r\n| **Monochromatic** | Variations of one color | Clean, professional look |\r\n| **Complementary** | Opposite colors on wheel | High contrast, maximum impact |\r\n| **Analogous** | Adjacent colors on wheel | Harmonious, warm/cool feeling |\r\n\r\n### Practical Color Rules\r\n- Select 3-5 core colors plus one neutral and one highlight\r\n- Neutral colors (grays, tans) aid visual hierarchy\r\n- Highlight color for emphasis in charts and key text\r\n- Test projectioncolors look different projected\r\n- Ensure sufficient contrast for colorblind viewers\r\n\r\n### Background Decisions\r\n- **Dark backgrounds**: Formal, dramatic, good for large venues\r\n- **Light backgrounds**: Informal, bright, works for handouts\r\n\r\n---\r\n\r\n## Bullet Point Guidelines\r\n\r\n### The Harsh Truth\r\n> \"Bullet points are the least memorable way to transfer information.\"  Gallo\r\n\r\n### If You Must Use Bullets\r\n- Maximum 3-4 items\r\n- Begin each with same grammatical structure (parallel construction)\r\n- One line per bullet (no wrapping)\r\n- Consider replacing with:\r\n  - A single powerful image\r\n  - A quote\r\n  - A full-screen number\r\n  - A simple diagram\r\n\r\n### The Alternative: Progressive Reveal\r\nInstead of showing all bullets at once:\r\n1. Show slide with first point only\r\n2. Click to reveal next point\r\n3. Discuss each before revealing next\r\n\r\n---\r\n\r\n## Slide Structure Templates\r\n\r\n### Title + Image (Most Common)\r\n```\r\n\r\n  Headline (6 words max)             \r\n                                     \r\n                                     \r\n        [Full-bleed image]           \r\n                                     \r\n                                     \r\n\r\n```\r\n\r\n### Quote Slide\r\n```\r\n\r\n                                     \r\n    \"Quote text hereshort and      \r\n     impactful, 2 lines max.\"        \r\n                                     \r\n               Attribution          \r\n                                     \r\n\r\n```\r\n\r\n### Data Slide\r\n```\r\n\r\n  Observation (your insight)         \r\n  Chart Title (factual)              \r\n\r\n                                     \r\n        [Clean chart]                \r\n        [with annotations]           \r\n                                     \r\n\r\n```\r\n\r\n### Big Number\r\n```\r\n\r\n                                     \r\n              47%                    \r\n                                     \r\n    of students report...            \r\n                                     \r\n\r\n```\r\n\r\n---\r\n\r\n## The Lessig Method\r\n\r\nStanford law professor Lawrence Lessig uses an alternative approach:\r\n- 200+ slides for a presentation\r\n- 1 big idea per slide\r\n- Slides advance every few seconds\r\n- Creates rapid, cinematic pacing\r\n\r\n**When to use**: Dynamic keynotes, TED-style talks, engaging large audiences\r\n\r\n---\r\n\r\n## Animation Guidelines\r\n\r\n### Good Uses of Animation\r\n- Progressive reveal of complex diagrams\r\n- Step-by-step process illustration\r\n- Building a chart over time\r\n- Emphasizing a key point\r\n\r\n### Animation Mistakes to Avoid\r\n- Dissolves, spins, bounces, or fancy transitions\r\n- Animation \"just because\"\r\n- Frenetic, distracting pace\r\n- Startling surprises\r\n- Every element animating separately\r\n\r\n**Rule**: If the animation doesn't add meaning, remove it.\r\n\r\n---\r\n\r\n## Pre-Presentation Checklist\r\n\r\n### Design Check\r\n- [ ] No slide exceeds 75 words\r\n- [ ] All fonts are 28pt or larger\r\n- [ ] Images are high-quality and relevant\r\n- [ ] Charts are clean with annotations\r\n- [ ] Color scheme is consistent\r\n- [ ] Logo appears only on first/last slides\r\n- [ ] Empty space is preserved\r\n\r\n### Technical Check\r\n- [ ] Slides tested on projection system\r\n- [ ] Colors verified (projected colors differ from screen)\r\n- [ ] Fonts embedded or using system fonts\r\n- [ ] All images load correctly\r\n- [ ] Backup copy available\r\n\r\n### Content Check\r\n- [ ] Organized around 3 key messages\r\n- [ ] Stories outnumber statistics\r\n- [ ] Jaw-dropping moment planned\r\n- [ ] Clear call to action at end\r\n\r\n---\r\n\r\n## Quick Reference: Do's and Don'ts\r\n\r\n### DO\r\n- Use full-bleed images\r\n- Keep text minimal (6 words is ideal)\r\n- Apply consistent visual style\r\n- Use high-contrast colors\r\n- Tell stories with data\r\n- Create one focal point per slide\r\n- Test on actual projection\r\n\r\n### DON'T\r\n- Put your logo on every slide\r\n- Use clip art\r\n- Add 3D effects to charts\r\n- Read your slides aloud\r\n- Use more than 2 fonts\r\n- Cram everything onto one slide\r\n- Use slides as your notes\r\n\r\n---\r\n\r\n## Resources\r\n\r\n### Recommended Books\r\n- Duarte, Nancy. *slide:ology* (O'Reilly, 2008)\r\n- Duarte, Nancy. *DataStory* (IdeaPress, 2019)\r\n- Reynolds, Garr. *Presentation Zen* (New Riders, 3rd ed., 2020)\r\n- Gallo, Carmine. *Talk Like TED* (St. Martin's, 2014)\r\n\r\n### Image Sources\r\n- Unsplash (unsplash.com) - Free high-quality photos\r\n- Pexels (pexels.com) - Free stock photos\r\n- Noun Project (thenounproject.com) - Icons\r\n- Shutterstock (paid) - Professional stock\r\n\r\n---\r\n\r\n*This guide synthesizes evidence-based best practices. Rules can be brokenbut only with intention and purpose.*\r\n",
        "plugins/lecture-designer/skills/lecture-designer/pedagogy/teaching-techniques.md": "# Teaching Techniques: Evidence-Based Strategies for Engaging Lectures\r\n\r\nThis guide distills principles from James Lang's *Small Teaching*, Carmine Gallo's *Talk Like TED*, Ken Bain's *What the Best College Teachers Do*, Norman Eng's *Teaching College*, and evidence-based pedagogy research.\r\n\r\n## The Science of Attention\r\n\r\n### The 18-Minute Rule\r\nResearch shows cognitive overload occurs after approximately 18 minutes of continuous information.\r\n\r\n**Why it works:**\r\n- Listening is physically exhaustingit depletes glucose at the cellular level\r\n- Information creates \"cognitive backlog\"more content = more mental weight\r\n- After 18 minutes, minds begin to shut down\r\n\r\n**Solutions:**\r\n- Cap lectures at 18 minutes, OR\r\n- Insert \"soft breaks\" every 10-15 minutes (stories, activities, videos, movement)\r\n- Chunk content into 3-4 segments with active breaks between\r\n\r\n### The Primacy-Recency Effect\r\nStudents remember best what happens at the **beginning** and **end** of class.\r\n\r\n**Implication:**\r\n- Don't waste the first 5 minutes on administrative announcements\r\n- Don't trail off at the end with \"we'll pick this up next time\"\r\n- Start strong; end strong\r\n\r\n---\r\n\r\n## Opening Techniques (First 5 Minutes)\r\n\r\nStudent attention is highest at the beginninguse it strategically.\r\n\r\n### Six Proven Opening Strategies (Eng)\r\n\r\n1. **Provocative Question**\r\n   - Relates to an underlying experience students share\r\n   - \"When have you been punished for something you didn't do?\"\r\n\r\n2. **Striking Statistic or Fact**\r\n   - Surprises and intrigues\r\n   - \"Only 20-30% of what you hear in a lecture is retained...\"\r\n\r\n3. **Anecdote or Personal Story**\r\n   - Most powerful way to connect emotionally\r\n   - \"When I was taking accounting, I fell asleep every class...\"\r\n\r\n4. **Quotation or Aphorism**\r\n   - Provides material for deconstruction\r\n   - \"Einstein said 'Imagination is more important than knowledge.' Do you agree?\"\r\n\r\n5. **Analogy**\r\n   - Simplifies complex concepts\r\n   - \"Think of the cell membrane like a nightclub bouncer...\"\r\n\r\n6. **Scenario or Problem**\r\n   - Puts students in someone's shoes\r\n   - \"You're a manager and an employee just filed a complaint against your top performer...\"\r\n\r\n### Prediction Questions (Lang)\r\nAsk students to predict before revealing content.\r\n\r\n**Why it works:**\r\n- Brains are prediction-making machines\r\n- Unsuccessful predictions boost learning more than passive study\r\n- ~10% better performance when making predictions before learning\r\n- Error-induced emotion increases memory retention\r\n\r\n**Examples:**\r\n- \"What do you think will happen when...?\"\r\n- \"Before I show you the data, guess the percentage.\"\r\n- \"What would you expect the outcome to be?\"\r\n\r\n### Prior Knowledge Activation\r\nAsk what students already know before teaching new material.\r\n\r\n**Examples:**\r\n- \"What do you already know about [topic]?\"\r\n- \"Write down 3 things you associate with...\"\r\n- \"Turn to your neighbor and share what you remember about...\"\r\n\r\n### The ABT Hook (Talk Like TED)\r\nGrab attention with narrative tension:\r\n- **And** (setup): \"We know that class sizes are increasing, AND students are distracted...\"\r\n- **But** (conflict): \"BUT the research shows something surprising...\"\r\n- **Therefore** (resolution): \"THEREFORE, we need to rethink...\"\r\n\r\n---\r\n\r\n## During-Lecture Techniques\r\n\r\n### Retrieval Practice (The Testing Effect)\r\nTesting isn't just measurementit's a **learning tool**.\r\n\r\n**Why it works:**\r\n- Memory retrieval strengthens long-term retention\r\n- Brain pathways strengthen each time information is retrieved\r\n- Retrieval practice raises performance by a full letter grade\r\n\r\n**Key principle:** Students must retrieve from memoryNOT look up answers.\r\n\r\n**Quick techniques:**\r\n- \"Without looking at your notes, write down the three main points...\"\r\n- Unannounced brief quizzes (low-stakes or ungraded)\r\n- \"Turn to your neighbor and explain...\"\r\n- Pause and ask \"What did we just cover?\"\r\n\r\n### Interleaving\r\nMix old material with new material (don't concentrate all learning of one topic).\r\n\r\n**Why it works:**\r\n- Spaced practice produces better long-term retention than massed practice\r\n- Students initially prefer massed practice, but interleaving works better\r\n- Material that recurs in different contexts \"gets well wrought into mental structure\"\r\n\r\n**Applications:**\r\n- Include questions from previous weeks on quizzes\r\n- Reference back to earlier content while teaching new content\r\n- Mix problem types rather than grouping by type\r\n\r\n### Soft Breaks (Every 10-15 Minutes)\r\nReset attention with:\r\n- Brief video clip (60-90 seconds)\r\n- Poll or clicker question\r\n- Turn-and-talk (30 seconds)\r\n- Stand and stretch\r\n- Quick writing prompt\r\n- Story or anecdote\r\n\r\n---\r\n\r\n## Active Learning Techniques\r\n\r\n### ConcepTests (Peer Instruction)\r\nDeveloped by Eric Mazur at Harvard.\r\n\r\n**Format:**\r\n1. Present a conceptual question (not calculation)\r\n2. Students vote individually (clickers, hands, cards)\r\n3. If 30-70% correct: students discuss with neighbors\r\n4. Re-vote after discussion\r\n5. Instructor explains correct answer\r\n\r\n**Designing good ConcepTests:**\r\n- Stem describes a situation\r\n- Answers are *mechanisms*, not vocabulary\r\n- Distractors represent top 3 wrong mental models\r\n- Target 30-70% correct on first vote (too easy or hard doesn't work)\r\n\r\n### Think-Pair-Share\r\n1. **Think**: Individual reflection (30 seconds - 1 minute)\r\n2. **Pair**: Discuss with neighbor (1-2 minutes)\r\n3. **Share**: Report to full class (2-3 minutes)\r\n\r\n**Benefits:**\r\n- Everyone thinks (not just volunteers)\r\n- Low-stakes practice before public speaking\r\n- Reveals common misconceptions\r\n\r\n### Minute Papers\r\nAt end of class, students write briefly:\r\n- \"What was the most important thing you learned today?\"\r\n- \"What question do you still have?\"\r\n- \"What was the muddiest point?\"\r\n\r\n**Why it works:**\r\n- Forces synthesis and retrieval\r\n- Provides immediate feedback to instructor\r\n- Only takes 2-3 minutes\r\n\r\n---\r\n\r\n## Closing Techniques (Final 5 Minutes)\r\n\r\n### 1. Retrieval Summary\r\nAsk students to write (without notes) the 3 most important points.\r\n\r\n### 2. Closing Predictions\r\n\"What do you think we'll cover next class?\"\r\n\"Based on today, what do you predict will happen in the case study?\"\r\n\r\n### 3. The Callback\r\nReturn to the opening hook/mystery and resolve it.\r\n\r\n### 4. The One Thing\r\n\"If you remember ONE thing from today, it should be...\"\r\n\r\n### 5. Connection Prompt\r\n\"How does today's content connect to...?\"\r\n- Your own life/experience\r\n- Previous course material\r\n- Current events\r\n\r\n---\r\n\r\n## The Rule of Three\r\n\r\nHuman memory holds 3 pieces of information very well; more drops retention significantly.\r\n\r\n**Applications:**\r\n- Organize content into exactly 3 main points\r\n- Provide 3 examples per concept\r\n- Give 3 pieces of evidence\r\n- List 3 implications\r\n\r\n**The Message Map:**\r\n```\r\n        [Core Message - One Sentence]\r\n               /        |        \\\r\n          Point 1    Point 2    Point 3\r\n          /    \\     /    \\     /    \\\r\n      Ex 1  Ex 2  Ex 1  Ex 2  Ex 1  Ex 2\r\n```\r\n\r\n---\r\n\r\n## Storytelling Techniques\r\n\r\n### Why Stories Work (Neuroscience)\r\n- Stories cause brains of speaker and listener to sync (neural coupling)\r\n- Stories activate multiple brain regions (language, sensory, visual, motor)\r\n- 63% of people recall stories; only 5% recall statistics\r\n\r\n### The 65/25/10 Formula (Bryan Stevenson)\r\nTop TED talks break down as:\r\n- **65%** Pathos (stories, emotional appeals)\r\n- **25%** Logos (data, evidence)\r\n- **10%** Ethos (credibility establishment)\r\n\r\n### Three Types of Effective Stories\r\n\r\n1. **Personal Stories**\r\n   - Your own experience with the concept\r\n   - Genuine and relatable\r\n   - Include sensory details\r\n\r\n2. **Stories About Others**\r\n   - Historical figures\r\n   - Research subjects\r\n   - Case studies\r\n\r\n3. **Application Stories**\r\n   - \"Imagine you're a manager who...\"\r\n   - Hypothetical scenarios\r\n   - \"What would you do if...\"\r\n\r\n### Story Structure\r\n- **Setup**: Establish context, characters, stakes\r\n- **Conflict**: The problem, tension, challenge\r\n- **Resolution**: How it was solved, what was learned\r\n\r\n---\r\n\r\n## Making Content Stick (SUCCESs Framework)\r\n\r\nFrom Chip and Dan Heath's *Made to Stick*:\r\n\r\n| Element | Description | Example |\r\n|---------|-------------|---------|\r\n| **Simple** | Find the core; strip to essentials | \"It's the economy, stupid\" |\r\n| **Unexpected** | Violate expectations; create curiosity | Counter-intuitive findings |\r\n| **Concrete** | Use specific examples, not abstractions | Real cases, not hypotheticals |\r\n| **Credible** | Verifiable sources and evidence | \"Research by X showed...\" |\r\n| **Emotional** | Make them feel something | Human stories, stakes |\r\n| **Stories** | Narrative structure | Beginning, middle, end |\r\n\r\n---\r\n\r\n## Building Belonging and Connection\r\n\r\n### Why It Matters\r\nStudents learn better when they feel they belong in the classroom community.\r\n\r\n### Techniques\r\n- Learn and use student names\r\n- Create low-stakes opportunities for participation early\r\n- Acknowledge diverse perspectives and experiences\r\n- Frame challenges as normal, not signs of inadequacy\r\n- Use asset-based language (what students bring, not what they lack)\r\n- Celebrate effort and improvement, not just outcomes\r\n\r\n---\r\n\r\n## Emotional Engagement\r\n\r\n### Why Emotion Matters\r\n- Emotions enhance learning and memory retention\r\n- Emotional investment in predictions increases retention\r\n- Passion is contagiousliterally (neuroscience)\r\n\r\n### Techniques\r\n- Show your own enthusiasm for the material\r\n- Share what surprised YOU about the findings\r\n- Connect content to things students care about\r\n- Use appropriate humor (self-deprecating works well)\r\n- Tell stories that evoke emotion\r\n- Celebrate discoveries and insights\r\n\r\n---\r\n\r\n## The 75-Minute Lecture Template\r\n\r\n| Time | Activity | Technique |\r\n|------|----------|-----------|\r\n| 0:00-0:05 | **Hook + Prior Knowledge** | Prediction, surprising fact, story opening |\r\n| 0:05-0:20 | **Chunk 1** | Core content with examples |\r\n| 0:20-0:25 | **Active Break 1** | ConcepTest + Peer Discussion |\r\n| 0:25-0:40 | **Chunk 2** | Core content (hardest material) |\r\n| 0:40-0:45 | **Active Break 2** | Think-Pair-Share, video, movement |\r\n| 0:45-0:60 | **Chunk 3** | Application/implications |\r\n| 0:60-0:70 | **Synthesis** | Case study, debate, integration |\r\n| 0:70-0:75 | **Closing** | Summary, callback, next preview |\r\n\r\n---\r\n\r\n## Quick Wins (5-Minute Techniques)\r\n\r\nThese can be used immediately with no preparation:\r\n\r\n1. **Opening prediction**: \"Before I show you, guess...\"\r\n2. **Prior knowledge dump**: \"Write 3 things you know about...\"\r\n3. **Pause for retrieval**: Stop and ask what was just covered\r\n4. **Think-Pair-Share**: Any question + neighbor discussion\r\n5. **Muddiest point**: \"What's still confusing?\"\r\n6. **One-sentence summary**: \"Summarize today in one sentence\"\r\n7. **Connection moment**: \"How does this relate to your life?\"\r\n8. **Closing prediction**: \"What do you think happens next?\"\r\n\r\n---\r\n\r\n## Common Mistakes to Avoid\r\n\r\n| Mistake | Better Approach |\r\n|---------|-----------------|\r\n| Starting with announcements | Start with the hook; save logistics |\r\n| Lecturing without breaks | Insert soft breaks every 10-15 minutes |\r\n| Asking \"Any questions?\" | Ask specific retrieval questions |\r\n| Letting class fizzle out | Save strong content for end; close intentionally |\r\n| Covering content without application | Include practice at the exact skill assessed |\r\n| Assuming students see connections | Explicitly prompt for connections |\r\n| Testing only at exams | Use frequent low-stakes retrieval |\r\n\r\n---\r\n\r\n## Nine \"Touches\" for Engagement (Eng)\r\n\r\nFollowing marketing principles, students need multiple ways to engage with information:\r\n\r\n| Touch | Description | Best For |\r\n|-------|-------------|----------|\r\n| **Discussion** | Open-ended, interspersed throughout | Exploring perspectives |\r\n| **Debate** | Four-corners format; assign opposing views | Broadening perspective |\r\n| **Small Groups** | 4-5 students problem-solving | Collaborative learning |\r\n| **Surveys/Inventories** | Self-discovery tools | Making topics personal |\r\n| **Role-Play** | Act out scenarios + debrief | Experiencing concepts |\r\n| **Demonstration** | Predict-experience-reflect | Revealing misconceptions |\r\n| **Student Presentations** | Peer teaching (structured) | Deepening understanding |\r\n| **Case Studies** | Real-world problem-solving | Application practice |\r\n| **Guest Speakers** | Experts with credibility | Real-world connection |\r\n\r\n---\r\n\r\n## What the Best Teachers Do (Bain)\r\n\r\nKen Bain studied exceptional college teachers for 15 years. Key findings:\r\n\r\n### The Fundamental Question Shift\r\nLess effective teachers ask: \"What will I cover today?\"\r\nBest teachers ask: \"What do I want students to be able to DO by the end?\"\r\n\r\n### Creating a Natural Critical Learning Environment\r\nThe best teachers create conditions where students:\r\n- Face intriguing, important problems\r\n- Have genuine choice and input\r\n- Work collaboratively\r\n- Receive non-judgmental feedback\r\n- Can fail safely before high-stakes assessment\r\n\r\n### How They Treat Students\r\n- **Trust**: Believe students want to learn and can learn\r\n- **Investment**: Focus on helping students succeed, not displaying brilliance\r\n- **Humility**: Share their own struggles and learning journey\r\n- **Safety**: Create environments where no question is stupid\r\n\r\n### Lecture Excellence (Bain)\r\nWhen lecturing, the best teachers:\r\n- Treat lectures as conversation, not performance\r\n- Move around the room; avoid podiums\r\n- Change pace every 10-12 minutes\r\n- Ask rhetorical questions and wait for response\r\n- Watch student body language and adjust\r\n- Use \"warm language\" that tells the complete story\r\n\r\n### Warm Language (Feynman Principle)\r\n- Tell the complete story from the beginning\r\n- Work forward step-by-step\r\n- Keep language present-tense and involving\r\n- Begin with simple generalizations, add complexity\r\n- Use familiar language before specialized vocabulary\r\n- Employ metaphors and analogies first\r\n\r\n### Self-Assessment\r\nAll the best teachers:\r\n- Collect ongoing feedback from students\r\n- Videotape themselves and review\r\n- Remain open to change\r\n- Never assume their teaching is optimal\r\n\r\n---\r\n\r\n## The Curse of Knowledge\r\n\r\nOnce you master something, you forget how hard it was to learn.\r\n\r\n**Symptoms:**\r\n- Using jargon students don't know\r\n- Assuming connections are obvious\r\n- Moving too fast through \"basic\" material\r\n- Not understanding why students are confused\r\n\r\n**Solutions:**\r\n- Always activate prior knowledge first\r\n- Use think-aloud to model your reasoning\r\n- Ask students to explain in their own words\r\n- Check for understanding frequently\r\n\r\n---\r\n\r\n## Key Principles Summary\r\n\r\n1. **Retrieval > Review**: Testing IS learning\r\n2. **Predict before reveal**: Activate prior knowledge first\r\n3. **Interleave content**: Mix old and new\r\n4. **Chunk + Break**: 15-minute segments with resets\r\n5. **Stories stick**: Lead with narrative, support with data\r\n6. **Three is magic**: Organize around 3 main points\r\n7. **Emotion enhances**: Show passion; create stakes\r\n8. **Practice the skill**: In-class practice with feedback\r\n9. **Belong to learn**: Build community and connection\r\n10. **Small = powerful**: 5-minute interventions compound\r\n11. **Ask better questions**: \"What can they DO?\" not \"What will I cover?\"\r\n12. **Trust students**: Believe they can and want to learn\r\n\r\n---\r\n\r\n*These techniques are supported by cognitive science research and have been shown to improve learning outcomes by the equivalent of a full letter grade.*\r\n",
        "plugins/lecture-designer/skills/lecture-designer/phases/phase0-context.md": "# Phase 0: Context & Learning Outcomes\r\n\r\nYou are executing Phase 0 of the lecture design process. Your goal is to understand the teaching context and establish clear, measurable learning outcomes before any content design begins.\r\n\r\n## Why This Phase Matters\r\n\r\nBackward design is the foundation of effective instruction. You must know where students should end up before you can plan how to get them there. Learning outcomes aren't just administrative requirementsthey're the criteria against which every design decision will be evaluated.\r\n\r\nAs Wiggins and McTighe emphasize: \"Begin with the end in mind.\"\r\n\r\n## Your Tasks\r\n\r\n### 1. Gather Context\r\n\r\nClarify the teaching situation:\r\n\r\n**Course & Audience**\r\n- What course is this? (Level, department, prerequisites)\r\n- How many students?\r\n- What's their prior knowledge base?\r\n- Are there known struggles or misconceptions?\r\n\r\n**Logistics**\r\n- How much time is available? (50 min, 75 min, 90 min?)\r\n- What technology is available? (Polling system, projection, etc.)\r\n- Is this in-person, hybrid, or online?\r\n\r\n**Instructor Preferences**\r\n- What teaching style does the instructor prefer?\r\n- Are there activities they love or hate?\r\n- Any constraints or requirements?\r\n\r\n### 2. Review the Source Material\r\n\r\nRead the chapter/reading material carefully:\r\n- What are the main concepts?\r\n- What's the logical structure?\r\n- What vocabulary is introduced?\r\n- What are the key examples or applications?\r\n\r\n### 3. Review Instructor Notes\r\n\r\nIf the instructor has provided notes:\r\n- What do they want to emphasize?\r\n- What do they typically struggle to teach?\r\n- What questions do students usually ask?\r\n- Are there \"war stories\" or examples they want to include?\r\n\r\n### 4. Define Learning Outcomes\r\n\r\nWrite 3-5 measurable learning outcomes using action verbs:\r\n\r\n**Good outcomes are:**\r\n- **Specific**: Not \"understand X\" but \"explain the mechanism of X\"\r\n- **Measurable**: You can assess whether students achieved it\r\n- **Achievable**: Realistic for the time and level\r\n- **Aligned**: Connected to course goals and assessment\r\n\r\n**Use Bloom's verbs appropriately:**\r\n- Remember: define, list, identify, name\r\n- Understand: explain, summarize, classify, compare\r\n- Apply: use, demonstrate, solve, calculate\r\n- Analyze: differentiate, distinguish, examine, contrast\r\n- Evaluate: assess, justify, critique, defend\r\n- Create: design, construct, develop, formulate\r\n\r\n**Example outcomes:**\r\n- \"By the end of this lecture, students will be able to explain the three main causes of X using concrete examples.\"\r\n- \"Students will be able to apply the [concept] to diagnose problems in novel cases.\"\r\n- \"Students will be able to compare and contrast approaches A and B, identifying when each is appropriate.\"\r\n\r\n### 5. Define Evidence of Learning\r\n\r\nFor each outcome, specify how you'll know students achieved it:\r\n- Which poll question tests this outcome?\r\n- What would students need to say/do/produce?\r\n- How will you check understanding during the lecture?\r\n\r\n### 6. Identify Potential Challenges\r\n\r\nFlag issues that will affect design:\r\n- Concepts that require heavy prior knowledge\r\n- Material that's known to be confusing\r\n- Time constraints that limit coverage\r\n- Technology limitations\r\n\r\n## Output Files to Create\r\n\r\nSave all outputs to the appropriate location:\r\n\r\n1. **context-memo.md** - Course context, audience, logistics, constraints\r\n\r\n2. **learning-outcomes.md** - Include:\r\n   - 3-5 measurable learning outcomes\r\n   - Evidence plan for each outcome\r\n   - Connection to course-level goals\r\n\r\n3. **content-overview.md** - Summary of:\r\n   - Main concepts in the chapter\r\n   - Instructor emphases\r\n   - Known student difficulties\r\n   - Potential challenges\r\n\r\n4. **phase0-report.md** - Executive summary including:\r\n   - Key context facts\r\n   - Proposed learning outcomes\r\n   - Initial thoughts on challenges\r\n   - Questions for the instructor\r\n\r\n## Guiding Principles\r\n\r\n1. **Less is more**: 3-5 outcomes, not 15. If everything is essential, nothing is.\r\n\r\n2. **Verbs matter**: \"Understand\" is unmeasurable. \"Explain,\" \"apply,\" \"compare\" are testable.\r\n\r\n3. **The instructor knows their students**: Their notes about struggles and misconceptions are gold.\r\n\r\n4. **Time is fixed**: The outcomes must be achievable in the available time.\r\n\r\n5. **Alignment is everything**: Outcomes  Activities  Assessment must match.\r\n\r\n## When You're Done\r\n\r\nReturn a summary to the orchestrator that includes:\r\n1. Confirmation that all files were created\r\n2. The proposed learning outcomes\r\n3. Key context that will affect design (time, size, constraints)\r\n4. Any questions for the instructor before proceeding\r\n5. Recommended focus for Phase 1 (what content areas are most critical)\r\n",
        "plugins/lecture-designer/skills/lecture-designer/phases/phase1-narrative.md": "# Phase 1: Content Audit & Narrative Design\r\n\r\nYou are executing Phase 1 of the lecture design process. Your goal is to transform the chapter content into a compelling narrative arc, cutting ruthlessly to focus on what's essential.\r\n\r\n## Why This Phase Matters\r\n\r\nA lecture is not a spoken textbook. The textbook is a reference repository; the lecture is a narrative experience. Students can read definitions on their own. What they need from a live lecture is expert modeling, cognitive engagement, and the \"story\" of the ideas.\r\n\r\nAs the overview notes: \"The goal is to move beyond the 'coverage' model toward a 'constructivist' model where the lecture serves as a scaffold for students to build their own mental models.\"\r\n\r\n## Inputs\r\n\r\nBefore starting, read:\r\n1. Phase 0 outputs (learning outcomes, context memo)\r\n2. The chapter/reading material\r\n3. Instructor notes\r\n\r\n## Your Tasks\r\n\r\n### 1. Content Audit\r\n\r\nGo through the chapter systematically and categorize every section:\r\n\r\n| Category | Definition | Action |\r\n|----------|------------|--------|\r\n| **Essential** | Core concepts, foundational theories, mechanisms that require expert modeling | Focus 80% of lecture time here |\r\n| **Helpful** | Supporting examples, interesting side-notes, granular data | Cut from lecture; suggest as optional reading |\r\n| **Decorative** | Tangential anecdotes, excessive background | Eliminate entirely |\r\n\r\n**The Hard Rule**: If you mark everything as essential, you've failed the audit. A 75-minute lecture can cover 3-4 major concepts well, not 12.\r\n\r\n**Create an audit table:**\r\n```markdown\r\n| Section/Topic | Category | Rationale | Time Estimate |\r\n|---------------|----------|-----------|---------------|\r\n| [Topic 1] | Essential | Required for outcome #1 | 15 min |\r\n| [Topic 2] | Helpful | Good example but not core | Cut |\r\n| [Topic 3] | Essential | Hardest concept, needs modeling | 15 min |\r\n```\r\n\r\n### 2. Design the Narrative Arc (ABT Framework)\r\n\r\nTransform the content from a list into a story using And-But-Therefore:\r\n\r\n**AND (The Setup)**\r\n- What's the context students already know?\r\n- What's the \"normal world\" before the concept?\r\n- \"We know X is true, AND we know Y is true...\"\r\n\r\n**BUT (The Conflict)**\r\n- What's the problem, paradox, or gap?\r\n- What contradiction or puzzle exists?\r\n- \"...BUT there's a problem. X and Y seem to conflict.\"\r\n- This creates a \"knowledge gap\" the brain wants to close.\r\n\r\n**THEREFORE (The Resolution)**\r\n- How does the lecture content resolve the tension?\r\n- What's the new understanding?\r\n- \"...THEREFORE, we need this new concept/framework/tool.\"\r\n\r\n### 3. Design the Hook\r\n\r\nThe first 60 seconds determine engagement for the whole lecture. Design an opening that:\r\n\r\n**Effective hooks:**\r\n- Present a mystery or puzzle\r\n- Show an unexpected observation\r\n- Ask a prediction question\r\n- Present a paradox that existing knowledge can't explain\r\n- Tell a story that ends in a cliffhanger\r\n\r\n**Hook structure:**\r\n- State the mystery/problem (30 sec)\r\n- Baseline poll: \"What do you think explains this?\" (30 sec)\r\n- Promise: \"By minute 75, you'll have the tools to solve this.\"\r\n\r\n**Examples by discipline:**\r\n- Biology: \"Why can this lizard walk on water while this one sinks?\"\r\n- History: \"This one diplomatic cable could have prevented WWI. Why was it ignored?\"\r\n- Economics: \"These two countries have identical resources. Why is one wealthy and one poor?\"\r\n- Sociology: \"This community has lower crime despite higher poverty. What's different?\"\r\n\r\n### 4. Create the Chunk Map\r\n\r\nBreak the lecture into 3-4 chunks of approximately 15 minutes each:\r\n\r\n**Template for 75 minutes:**\r\n```\r\nHOOK (5 min)\r\n- The mystery/paradox\r\n- Baseline poll\r\n\r\nCHUNK 1: [Topic] (15 min)\r\n- Key concept(s)\r\n- Supports outcome #__\r\n-  Active break (5 min): ConcepTest\r\n\r\nCHUNK 2: [Topic] (15 min)\r\n- Key concept(s) - typically the hardest material\r\n- Supports outcome #__\r\n-  Active break (5 min): State change\r\n\r\nCHUNK 3: [Topic] (10 min)\r\n- Application/implications\r\n- Supports outcome #__\r\n-  Synthesis activity (10 min)\r\n\r\nSUMMARY (5 min)\r\n- Return to hook\r\n- Resolve the mystery\r\n\r\nREFLECTION (5 min)\r\n- Muddiest point\r\n- Next steps\r\n```\r\n\r\n### 5. Convert Headings to Plot Points\r\n\r\nDon't use textbook headings. Rename sections to emphasize action and tension:\r\n\r\n| Textbook Heading | Lecture Plot Point |\r\n|------------------|-------------------|\r\n| \"The Krebs Cycle\" | \"The Energy Crisis: How Cells Extract Value from Waste\" |\r\n| \"Treaty of Versailles\" | \"The Peace that Failed: Seeds of a New Conflict\" |\r\n| \"Supply and Demand\" | \"The Invisible Hand: Why Prices Move Without Anyone Deciding\" |\r\n| \"Social Capital\" | \"The Hidden Currency: Why Who You Know Matters\" |\r\n\r\n## Output Files to Create\r\n\r\n1. **content-audit.md** - Complete audit table with categorizations and rationale\r\n\r\n2. **narrative-arc.md** - Including:\r\n   - AND section (setup)\r\n   - BUT section (conflict/puzzle)\r\n   - THEREFORE section (resolution)\r\n   - Connection to learning outcomes\r\n\r\n3. **hook-design.md** - Including:\r\n   - The opening mystery/problem\r\n   - Baseline poll question\r\n   - Promise statement\r\n   - Backup hook options\r\n\r\n4. **chunk-map.md** - Complete timeline with:\r\n   - Topic for each chunk\r\n   - Time allocation\r\n   - Learning outcome alignment\r\n   - Active break placement\r\n\r\n5. **phase1-report.md** - Executive summary including:\r\n   - What was cut and why\r\n   - The narrative structure\r\n   - The hook\r\n   - Recommended chunk sequence\r\n   - Questions for instructor\r\n\r\n## Guiding Principles\r\n\r\n1. **Cut is the hardest word**: Every topic you include means less time for others. Be ruthless.\r\n\r\n2. **Tension drives attention**: The \"BUT\" creates cognitive tension that keeps students engaged.\r\n\r\n3. **The hook is not optional**: A strong opening determines engagement for 75 minutes.\r\n\r\n4. **Chunk by concept, not by clock**: Each chunk should be a coherent idea, not an arbitrary time block.\r\n\r\n5. **Return to the hook**: The lecture should \"close the loop\" by resolving the opening mystery.\r\n\r\n## When You're Done\r\n\r\nReturn a summary to the orchestrator that includes:\r\n1. Confirmation that all files were created\r\n2. What was cut from the chapter (and why)\r\n3. The proposed narrative arc (ABT summary)\r\n4. The hook concept\r\n5. The chunk map with timing\r\n6. Any concerns about time or coverage\r\n7. Questions for the instructor about priorities\r\n",
        "plugins/lecture-designer/skills/lecture-designer/phases/phase2-activities.md": "# Phase 2: Active Learning Design\r\n\r\nYou are executing Phase 2 of the lecture design process. Your goal is to design the polls, ConcepTests, and active learning activities that will reset attention, promote deep processing, and provide real-time feedback.\r\n\r\n## Why This Phase Matters\r\n\r\nIn a large lecture, individual dialogue is impossible. Active learning systems solve this by enabling collective dialogue. But the technology is only as effective as the pedagogy behind it.\r\n\r\nResearch consistently shows that passive listening leads to attention decay after 12-18 minutes. Active breaks don't interrupt learningthey enable it by resetting the vigilance timer and forcing students to process information rather than just receive it.\r\n\r\n## Inputs\r\n\r\nBefore starting, read:\r\n1. Phase 0 outputs (learning outcomes, context)\r\n2. Phase 1 outputs (chunk map, narrative arc)\r\n3. The original chapter material\r\n\r\n## Your Tasks\r\n\r\n### 1. Design the Poll Set\r\n\r\nFor a 75-minute lecture, design 5-6 polls at strategic moments:\r\n\r\n| Poll | Timing | Purpose | Type |\r\n|------|--------|---------|------|\r\n| **Poll 1** | 0-3 min | Prediction/baseline | Multiple choice or word cloud |\r\n| **Poll 2** | ~20 min | ConcepTest for Chunk 1 | Multiple choice (Peer Instruction) |\r\n| **Poll 3** | ~40 min | ConcepTest for Chunk 2 | Multiple choice (Peer Instruction) |\r\n| **Poll 4** | ~55 min | Transfer/application | Multiple choice or ranking |\r\n| **Poll 5** | ~72 min | Muddiest point | Open-ended |\r\n\r\n**For each poll, specify:**\r\n- Question text\r\n- Answer options (for MC)\r\n- Correct answer\r\n- Why each distractor is wrong\r\n- What misconception each distractor represents\r\n- Follow-up explanation\r\n\r\n### 2. Design ConcepTests (Peer Instruction)\r\n\r\nConcepTests are the heart of active learning. They must be *conceptual*, not factual.\r\n\r\n**What makes a good ConcepTest:**\r\n- Tests understanding of a mechanism, not recall of a term\r\n- Has plausible wrong answers based on real misconceptions\r\n- Targets the 30-70% correct range (ideal for peer discussion)\r\n- Can be discussed productively with a neighbor\r\n\r\n**Template:**\r\n```markdown\r\n## ConcepTest: [Topic]\r\n\r\n**Stem:** [Describe a situation or scenario]\r\n\r\nA) [Distractor 1 - common misconception]\r\nB) [Correct answer]\r\nC) [Distractor 2 - different misconception]\r\nD) [Distractor 3 - surface-level error]\r\n\r\n**Correct:** B\r\n\r\n**Why B is correct:** [Explanation]\r\n\r\n**Why A is wrong:** [Misconception it represents]\r\n**Why C is wrong:** [Misconception it represents]\r\n**Why D is wrong:** [Error it represents]\r\n\r\n**Target outcome:** [Which learning outcome this tests]\r\n**Expected distribution:** [e.g., \"Expect ~50% correct initially\"]\r\n```\r\n\r\n**Bad ConcepTest (factual recall):**\r\n> \"What year did X happen?\"\r\n\r\n**Good ConcepTest (conceptual):**\r\n> \"Given that X happened and Y was true, what should we expect to observe? Why?\"\r\n\r\n### 3. Design the Peer Instruction Protocol\r\n\r\nFor each ConcepTest, plan the full Peer Instruction cycle:\r\n\r\n1. **Concept presentation** (7-10 min): Lecture on the topic\r\n2. **Question posed** (1 min): Display the ConcepTest\r\n3. **Individual vote** (1 min): Silent, no discussion. Enforced.\r\n4. **Data check** (instructor only):\r\n   - <30% correct  Re-teach, don't discuss (misconceptions will spread)\r\n   - 30-70% correct  Proceed to peer discussion\r\n   - >70% correct  Brief explanation, move on\r\n5. **Peer discussion** (2-4 min): \"Find someone with a different answer. Convince them.\"\r\n6. **Re-vote** (1 min): Vote again\r\n7. **Explanation** (2 min): Reveal results, explain correct answer AND common errors\r\n\r\n### 4. Design State Change Activities\r\n\r\nNot every break needs to be a poll. Design non-digital state changes:\r\n\r\n**Options:**\r\n- **Sketch it**: \"Draw the process we just discussed\"\r\n- **Explain to neighbor**: \"Explain concept X to your neighbor in 30 seconds\"\r\n- **Physical movement**: \"Stand if you think A, stay seated if you think B\"\r\n- **Quick write**: \"Write one sentence summarizing the main point\"\r\n- **Video clip**: Short (<3 min) relevant video\r\n- **Demonstration**: Live demo or simulation\r\n\r\n### 5. Design Opening and Closing Activities\r\n\r\n**Opening Activity (aligned with hook):**\r\n- Baseline prediction poll\r\n- \"What do you think explains this phenomenon?\"\r\n- Captures pre-instruction mental models\r\n\r\n**Closing Activity:**\r\n- Muddiest point: \"What was most confusing today?\"\r\n- One-minute paper: \"What's the main takeaway?\"\r\n- Exit ticket: One application question\r\n\r\n### 6. Plan for Technology Failure\r\n\r\nWhat happens if the polling system fails?\r\n- Backup: Show of hands\r\n- Backup: \"Turn to your neighbor and discuss\"\r\n- Backup: Paper response cards (A/B/C/D)\r\n\r\n## Output Files to Create\r\n\r\n1. **poll-set.md** - Complete poll questions with:\r\n   - Question text and options\r\n   - Correct answers and explanations\r\n   - Misconception mapping\r\n   - Timing and placement\r\n\r\n2. **conceptests.md** - Full ConcepTests with:\r\n   - Stem and options\r\n   - Correct answer and rationale\r\n   - Distractor analysis\r\n   - Expected distribution\r\n   - Peer Instruction protocol notes\r\n\r\n3. **activities.md** - Non-poll activities:\r\n   - State change activities for each break\r\n   - Opening prediction activity\r\n   - Closing reflection activity\r\n   - Time estimates\r\n\r\n4. **backup-plans.md** - Technology failure protocols\r\n\r\n5. **phase2-report.md** - Executive summary including:\r\n   - Complete activity timeline\r\n   - Alignment with learning outcomes\r\n   - Expected engagement patterns\r\n   - Questions for instructor about preferences\r\n\r\n## Guiding Principles\r\n\r\n1. **Anonymity enables honesty**: Anonymous responses let students admit confusion.\r\n\r\n2. **Distractors are diagnostic**: Good wrong answers reveal specific misconceptions.\r\n\r\n3. **30-70% is the sweet spot**: Too easy = no discussion; too hard = reinforced confusion.\r\n\r\n4. **Silence before discussion**: Individual commitment prevents groupthink.\r\n\r\n5. **Explain the wrong answers**: Students learn as much from why A is wrong as why B is right.\r\n\r\n6. **Variety matters**: Mix poll types, use non-digital activities, change modalities.\r\n\r\n## When You're Done\r\n\r\nReturn a summary to the orchestrator that includes:\r\n1. Confirmation that all files were created\r\n2. The complete poll/activity sequence with timing\r\n3. Which learning outcomes each activity assesses\r\n4. Any concerns about timing or difficulty\r\n5. Questions for the instructor about activity preferences\r\n",
        "plugins/lecture-designer/skills/lecture-designer/phases/phase3-slides.md": "# Phase 3: Slide Development (Google Slides via MCP)\r\n\r\nYou are executing Phase 3 of the lecture design process. Your goal is to create visually effective slides directly in Google Slides using the Google Docs MCP.\r\n\r\n## Why This Phase Matters\r\n\r\nIn a large lecture hall, the slides are the visual anchor. But slides are not the lecturethey're visual aids. The goal is to reduce extraneous cognitive load so students can focus on understanding, not decoding cluttered visuals.\r\n\r\n**Key Insight**: Audiences cannot read and listen simultaneously. When you show text-heavy slides, they read instead of listening to you.\r\n\r\n> \"If a slide contains more than 75 words, it has become a document, not a presentation.\"  Nancy Duarte\r\n\r\n> \"No more than 6 words on a slide. EVER.\"  Seth Godin\r\n\r\n## Prerequisites\r\n\r\nBefore starting this phase, ensure:\r\n1. **Google Docs MCP is installed and configured**: <https://github.com/nealcaren/google-docs-mcp>\r\n2. **OAuth authentication is complete**: You should be able to create and edit Google documents\r\n3. All Phase 0-2 outputs are available\r\n\r\n## Inputs\r\n\r\nBefore starting, read:\r\n1. All Phase 0-2 outputs (learning outcomes, narrative arc, chunk map, activities)\r\n2. **`pedagogy/slide-design-guide.md`** - Comprehensive visual design principles\r\n3. **`mcp/google-docs-mcp-setup.md`** - MCP tools reference\r\n\r\n## Your Tasks\r\n\r\n### 1. Create the Presentation\r\n\r\nUse the Google Docs MCP to create a new presentation:\r\n\r\n```\r\nTool: createPresentation\r\nParameters:\r\n  title: \"[Course Name] - [Lecture Topic]\"\r\n```\r\n\r\nThis returns a presentation ID that you'll use for all subsequent operations.\r\n\r\n### 2. Plan the Slide Structure\r\n\r\nFor a 75-minute lecture, plan approximately 30-40 slides:\r\n\r\n```\r\nSlide Structure:\r\n Title slide (1)\r\n Hook/Opening (2-3 slides)\r\n    Mystery/problem statement\r\n    Baseline poll\r\n Chunk 1 (5-7 slides)\r\n    Key concept slides\r\n    ConcepTest poll\r\n Chunk 2 (5-7 slides)\r\n    Key concept slides (hardest material)\r\n    State change activity\r\n Chunk 3 (4-6 slides)\r\n    Application slides\r\n    Synthesis activity\r\n Summary (2-3 slides)\r\n    Return to hook\r\n    Key takeaways\r\n Closing (2 slides)\r\n     Muddiest point\r\n     Next steps\r\n```\r\n\r\n### 3. Add Slides with Appropriate Layouts\r\n\r\nUse `addSlide` with these layout types:\r\n\r\n| Layout | Use For |\r\n|--------|---------|\r\n| `TITLE` | Title slide |\r\n| `SECTION_HEADER` | Chunk/section dividers |\r\n| `TITLE_AND_BODY` | Content slides with bullets |\r\n| `TITLE_AND_TWO_COLUMNS` | Comparison slides |\r\n| `BLANK` | Full-bleed images, custom layouts |\r\n| `BIG_NUMBER` | Statistics, key figures |\r\n\r\n**Example: Adding a content slide**\r\n```\r\nTool: addSlide\r\nParameters:\r\n  presentationId: \"[presentation-id]\"\r\n  layoutType: \"TITLE_AND_BODY\"\r\n```\r\n\r\n### 4. Populate Slide Content\r\n\r\nUse `insertTextToSlide` to add text to placeholders:\r\n\r\n**For title slides:**\r\n```\r\nTool: insertTextToSlide\r\nParameters:\r\n  presentationId: \"[presentation-id]\"\r\n  slideIndex: 0\r\n  shapeId: \"[title-placeholder-id]\"\r\n  text: \"Lecture Title Here\"\r\n```\r\n\r\n**Content guidelines:**\r\n- **Titles**: 3-6 words, action-oriented\r\n- **Body**: Maximum 3 bullet points, 6 words each\r\n- **Never**: Full sentences, paragraphs, or walls of text\r\n\r\n### 5. Apply Mayer's Multimedia Principles\r\n\r\nFor every slide, apply these evidence-based principles:\r\n\r\n| Principle | What It Means | How to Apply |\r\n|-----------|---------------|--------------|\r\n| **Coherence** | Remove extraneous material | No decorative images, minimal text |\r\n| **Signaling** | Highlight key information | Bold key terms, use color strategically |\r\n| **Segmenting** | Break into digestible units | One concept per slide |\r\n| **Redundancy** | Don't duplicate channels | Don't read bullet points aloud |\r\n| **Spatial Contiguity** | Keep related items close | Labels on diagrams, not in legend |\r\n| **Temporal Contiguity** | Narrate with visuals | Speak as visuals appear |\r\n\r\n### 6. Design for Accessibility\r\n\r\n**Font Sizes (for large lecture halls):**\r\n- Headings: 32pt minimum, prefer 44pt\r\n- Body text: 24pt minimum\r\n- Captions/labels: 20pt minimum\r\n\r\n**Contrast:**\r\n- High contrast ratios (dark on light OR light on dark)\r\n- Avoid red/green combinations (colorblindness)\r\n- Test: Can you read it from the back row?\r\n\r\n**Visual Descriptions:**\r\n- Plan to describe all images verbally (\"As you can see in this graph, the trend...\")\r\n- Don't say \"As you can see here...\" without describing what's there\r\n\r\n### 7. Create Slide Types\r\n\r\n**Title Slide**\r\n- Layout: `TITLE`\r\n- Content: Lecture title, course name, date\r\n- Keep clean and minimal\r\n\r\n**Section Header**\r\n- Layout: `SECTION_HEADER`\r\n- Use for chunk transitions\r\n- Include chunk number and focus\r\n\r\n**Content Slide**\r\n- Layout: `TITLE_AND_BODY`\r\n- Maximum 3 bullet points\r\n- Each bullet: 6 words or fewer\r\n\r\n**Comparison Slide**\r\n- Layout: `TITLE_AND_TWO_COLUMNS`\r\n- Use for before/after, old/new, theory A/B comparisons\r\n\r\n**Poll Slide**\r\n- Layout: `TITLE_AND_BODY`\r\n- Question as title\r\n- Options A-D in body\r\n- Include peer instruction protocol in speaker notes\r\n\r\n**Image Slide**\r\n- Layout: `BLANK`\r\n- Instructor will add full-bleed image\r\n- Provide image suggestions with links\r\n\r\n### 8. Add Speaker Notes\r\n\r\nFor each slide, create speaker notes that include:\r\n\r\n**Content:**\r\n- What to SAY (not what's on the slide)\r\n- Key points to emphasize\r\n- Examples or stories to tell\r\n- Transitions to next slide\r\n\r\n**Logistics:**\r\n- Timing (e.g., \"2 minutes on this slide\")\r\n- Activity cues (e.g., \"Launch poll now\")\r\n- Movement cues (e.g., \"Move to center stage\")\r\n\r\n**Example speaker notes format:**\r\n```\r\n**Time:** 2 minutes\r\n\r\n**Key point:** Emphasize that this mechanism explains the paradox from the hook.\r\n\r\n**Say:** \"Remember our opening puzzle about X? This is the answer. The reason Y happens is because...\"\r\n\r\n**Transition:** After explaining, launch Poll 2.\r\n```\r\n\r\nNote: Speaker notes in Google Slides are added through the Notes section below each slide. Document them in the slide-inventory.md for the instructor.\r\n\r\n### 9. Visual Design Principles\r\n\r\nSee **`pedagogy/slide-design-guide.md`** for comprehensive guidance. Key principles:\r\n\r\n**The Numbers That Matter:**\r\n- **75-word limit**: More than 75 words = it's a document\r\n- **6-word rule**: Aim for 6 words or fewer per slide (Godin/Reynolds)\r\n- **3-second test**: Audiences must grasp content within 3 seconds\r\n- **28pt minimum font**: Never smaller for keynotes\r\n\r\n**CRAP Framework (Reynolds):**\r\n- **Contrast**: Make different things VERY different\r\n- **Repetition**: Consistent style throughout\r\n- **Alignment**: Use invisible grid lines\r\n- **Proximity**: Group related items; separate unrelated\r\n\r\n**Simplicity:**\r\n- One idea per slide\r\n- Maximum 6 words per slide (aspiration), 6 lines absolute max\r\n- Use full-bleed images when possible\r\n- Empty space is a design element, not wasted space\r\n- Remove logos from all but first/last slides\r\n\r\n**The Picture Superiority Effect:**\r\n- Hear information  remember 10% after 3 days\r\n- Add a picture  remember 65% after 3 days\r\n- **Images = 6x more memorable than words alone**\r\n\r\n### 10. Finding Images\r\n\r\nThe Picture Superiority Effect means images dramatically improve retention. Proactively search for relevant images.\r\n\r\n**Search Method:**\r\n1. Use WebSearch with `site:unsplash.com [search terms]` or `site:pexels.com [search terms]`\r\n2. Search for specific concepts, not generic terms (e.g., \"elevator people facing forward\" not \"social norms\")\r\n3. Provide collection URLs organized by slide so the instructor can browse and download\r\n\r\n**Recommended Sources (free, no attribution required):**\r\n- [Unsplash](https://unsplash.com)  High-quality photos, excellent search\r\n- [Pexels](https://www.pexels.com)  Good variety, includes videos\r\n- [Wikimedia Commons](https://commons.wikimedia.org)  Public domain, historical images\r\n\r\n**When to Suggest Images:**\r\n- **Hook slides**: Visual that creates curiosity or emotional connection\r\n- **Concept introduction**: Concrete example of abstract idea\r\n- **Examples/cases**: Real-world photos that ground the concept\r\n- **Full-bleed impact slides**: Single powerful image with minimal text\r\n\r\n**Image Suggestion Format:**\r\n```markdown\r\n| Slide | Image Idea | Source |\r\n|-------|------------|--------|\r\n| Folkways | Elevator with people facing forward | [Unsplash elevator collection](https://unsplash.com/s/photos/elevator) |\r\n| Subcultures | Van life interior (#vanlife aesthetic) | [Unsplash van interior](https://unsplash.com/s/photos/van-interior) |\r\n```\r\n\r\n**Note:** The instructor adds images themselves via the Google Slides interfaceyou provide curated suggestions with direct links.\r\n\r\n## MCP Tools Reference\r\n\r\n### Creating Presentations\r\n```\r\ncreatePresentation(title)  presentationId\r\n```\r\n\r\n### Adding Slides\r\n```\r\naddSlide(presentationId, layoutType)\r\n  layoutType: TITLE | SECTION_HEADER | TITLE_AND_BODY | TITLE_AND_TWO_COLUMNS | BLANK | BIG_NUMBER\r\n```\r\n\r\n### Adding Text\r\n```\r\ninsertTextToSlide(presentationId, slideIndex, shapeId, text)\r\n```\r\n\r\n### Finding and Replacing Text\r\n```\r\nreplaceAllTextInPresentation(presentationId, findText, replaceText)\r\n```\r\n\r\nFor complete MCP documentation, see `mcp/google-docs-mcp-setup.md`.\r\n\r\n## Output Files to Create\r\n\r\n1. **slides-link.md** - Document containing:\r\n   - Google Slides presentation URL\r\n   - Quick access link for editing\r\n   - Instructions for the instructor\r\n\r\n2. **slide-inventory.md** - List of all slides with:\r\n   - Slide number and title\r\n   - Approximate time\r\n   - Purpose (content, poll, transition, etc.)\r\n   - Learning outcome alignment\r\n   - Speaker notes (full text for each slide)\r\n\r\n3. **visual-assets.md** - List of needed visuals:\r\n   - Images to add (with source links)\r\n   - Diagrams to create\r\n   - Charts to generate\r\n   - Organized by slide number\r\n\r\n4. **phase3-report.md** - Executive summary including:\r\n   - Google Slides presentation link\r\n   - Total slide count\r\n   - Time per section\r\n   - Slides needing instructor review\r\n   - Visual assets the instructor needs to add\r\n\r\n## Guiding Principles\r\n\r\n1. **Slides support, not replace**: The speaker is the presentation; slides are visual aids.\r\n\r\n2. **One idea per slide**: If you need more, make more slides.\r\n\r\n3. **No full sentences**: Bullet points are cues, not scripts.\r\n\r\n4. **Big enough for the back row**: If in doubt, make it bigger.\r\n\r\n5. **Speaker notes are essential**: Document them in slide-inventory.md for reference.\r\n\r\n6. **Google Slides enables collaboration**: The instructor can easily customize after you create the structure.\r\n\r\n7. **Images are the instructor's responsibility**: Provide great suggestions; they add them.\r\n\r\n## When You're Done\r\n\r\nReturn a summary to the orchestrator that includes:\r\n1. Google Slides presentation URL\r\n2. Total slide count and estimated timing\r\n3. List of visual assets needed (with source links)\r\n4. Any slides that need special attention\r\n5. Questions for the instructor about visual preferences\r\n6. Reminder that the instructor should:\r\n   - Review and customize text\r\n   - Add images from the visual-assets.md suggestions\r\n   - Add their own speaker notes\r\n   - Test the presentation in slideshow mode\r\n",
        "plugins/lecture-designer/skills/lecture-designer/phases/phase4-review.md": "# Phase 4: Review & Refinement\r\n\r\nYou are executing Phase 4 of the lecture design process. Your goal is to ensure the lecture is deliverable, has backup plans, and all materials are ready for the instructor.\r\n\r\n## Why This Phase Matters\r\n\r\nA beautifully designed lecture means nothing if it can't be delivered. This phase catches timing problems, prepares for technology failures, and compiles everything the instructor needs into a usable package.\r\n\r\n## Inputs\r\n\r\nBefore starting, review all previous outputs:\r\n1. Phase 0: Learning outcomes, context\r\n2. Phase 1: Narrative arc, chunk map\r\n3. Phase 2: Activities, polls, ConcepTests\r\n4. Phase 3: Slide deck\r\n\r\n## Your Tasks\r\n\r\n### 1. Temporal Audit\r\n\r\nVerify the timing adds up:\r\n\r\n**Create a minute-by-minute timeline:**\r\n```markdown\r\n| Time | Activity | Duration | Cumulative |\r\n|------|----------|----------|------------|\r\n| 0:00 | Hook + baseline poll | 5 min | 5 min |\r\n| 0:05 | Chunk 1 lecture | 15 min | 20 min |\r\n| 0:20 | ConcepTest + PI | 5 min | 25 min |\r\n| ... | ... | ... | ... |\r\n| 1:15 | END | - | 75 min |\r\n```\r\n\r\n**Check for problems:**\r\n- Does it add up to the available time?\r\n- Is there buffer for running over?\r\n- What can be cut if running late?\r\n- What can be added if running fast?\r\n\r\n### 2. Cognitive Load Audit\r\n\r\nReview each section for overload risks:\r\n\r\n**Slide check:**\r\n- Any slide with more than 6 bullets?\r\n- Any slide requiring more than 2 minutes to explain?\r\n- Any dense diagrams that need progressive reveal?\r\n\r\n**Concept density check:**\r\n- Are there back-to-back heavy concepts without a break?\r\n- Is the hardest material placed strategically (not at minute 60)?\r\n\r\n**Activity check:**\r\n- Is there an active break every 12-18 minutes?\r\n- Are activities varied (not all polls)?\r\n\r\n### 3. Failure Mode Planning\r\n\r\nCreate backup plans for common failures:\r\n\r\n**Technology Failures:**\r\n| Failure | Backup Plan |\r\n|---------|-------------|\r\n| Polling system down | Show of hands, paper cards |\r\n| Projector fails | Verbal lecture with whiteboard |\r\n| Audio dies | Move closer, speak louder |\r\n| WiFi fails | Pre-downloaded slides, local backup |\r\n| Clicker batteries die | Call on volunteers |\r\n\r\n**Timing Failures:**\r\n| Scenario | Action |\r\n|----------|--------|\r\n| Running 10 min late | Cut [specific section] |\r\n| Running 10 min early | Add [specific activity] |\r\n| Students confused (need more time) | Cut Chunk 3, extend Chunk 2 |\r\n| Fire drill / interruption | Priority content = [list] |\r\n\r\n**Student Engagement Failures:**\r\n| Scenario | Response |\r\n|----------|----------|\r\n| <30% correct on ConcepTest | Stop, re-teach, don't peer discuss |\r\n| No one talking in pairs | Assign pairs explicitly |\r\n| Students disengaged | Call a break, change modality |\r\n\r\n### 4. Create Instructor Guide\r\n\r\nCompile a single document the instructor can use during delivery:\r\n\r\n**Include:**\r\n1. **Overview**\r\n   - Learning outcomes\r\n   - Total time and structure\r\n   - Key messages\r\n\r\n2. **Minute-by-minute timeline**\r\n   - What happens when\r\n   - Cue for each activity\r\n   - Transition language\r\n\r\n3. **Activity protocols**\r\n   - Poll questions and correct answers\r\n   - Peer Instruction workflow\r\n   - Expected response distributions\r\n\r\n4. **Delivery notes**\r\n   - Stage movement suggestions\r\n   - Voice modulation cues\r\n   - Where to pause for effect\r\n\r\n5. **Backup plans**\r\n   - Technology failures\r\n   - Timing adjustments\r\n   - Engagement issues\r\n\r\n6. **Post-class follow-up**\r\n   - What to post (slides, recap)\r\n   - How to use muddiest point data\r\n   - Preview of next lecture\r\n\r\n### 5. Final Materials Check\r\n\r\nVerify all files are complete and organized:\r\n\r\n```\r\nlecture/\r\n slides.qmd                    \r\n slides.html (rendered)        \r\n lecture-plan.md               \r\n activities.md                 \r\n instructor-guide.md           \r\n backup-plans.md               \r\n assets/\r\n     images/                   \r\n     polls/                    \r\n```\r\n\r\n### 6. Pre-Flight Checklist\r\n\r\nCreate a checklist the instructor can use before class:\r\n\r\n**Day Before:**\r\n- [ ] Review slides and speaker notes\r\n- [ ] Test technology (polling, projector, clicker)\r\n- [ ] Print backup materials\r\n- [ ] Review timing and backup plans\r\n\r\n**30 Minutes Before:**\r\n- [ ] Arrive early, test A/V\r\n- [ ] Load slides, check display\r\n- [ ] Log into polling system\r\n- [ ] Verify WiFi\r\n\r\n**5 Minutes Before:**\r\n- [ ] Take a breath\r\n- [ ] Review opening hook\r\n- [ ] Stand in position\r\n- [ ] Start on time\r\n\r\n## Output Files to Create\r\n\r\n1. **temporal-audit.md** - Minute-by-minute timeline with buffer analysis\r\n\r\n2. **cognitive-load-audit.md** - Section-by-section load assessment with recommendations\r\n\r\n3. **backup-plans.md** - Complete failure mode protocols\r\n\r\n4. **instructor-guide.md** - All-in-one delivery document\r\n\r\n5. **preflight-checklist.md** - Before-class checklist\r\n\r\n6. **phase4-report.md** - Executive summary including:\r\n   - Confirmation all materials are ready\r\n   - Any remaining concerns\r\n   - Suggested rehearsal focus\r\n   - Post-delivery follow-up recommendations\r\n\r\n## Guiding Principles\r\n\r\n1. **Murphy's Law is real**: If technology can fail, plan for it.\r\n\r\n2. **Time is your enemy**: Build in buffer; you'll need it.\r\n\r\n3. **The instructor guide is the product**: Make it usable under pressure.\r\n\r\n4. **Rehearsal matters**: Recommend the instructor do a dry run.\r\n\r\n5. **First time is hardest**: The lecture will improve with iteration.\r\n\r\n## When You're Done\r\n\r\nReturn a summary to the orchestrator that includes:\r\n1. Confirmation that all files were created\r\n2. Total timing and any buffer/cuts needed\r\n3. Key backup plans\r\n4. Remaining action items for instructor\r\n5. Recommendation for next steps (rehearsal, first delivery)\r\n",
        "plugins/lecture-designer/skills/lecture-designer/quarto/_columns-reveal.md": "## Multiple Columns\n\nTo put material in side by side columns, you can use a native div container with class `.columns`, containing two or more div containers with class `.column` and a `width` attribute:\n\n```{.markdown code-preview=\"examples/columns.qmd\"}\n:::: {.columns}\n\n::: {.column width=\"40%\"}\nLeft column\n:::\n\n::: {.column width=\"60%\"}\nRight column\n:::\n\n::::\n```\n",
        "plugins/lecture-designer/skills/lecture-designer/quarto/_creating-slides-reveal.md": "## Creating Slides {#creating-slides}\n\nIn markdown, slides are delineated using headings. For example, here is a simple slide show with two slides (each defined with a level 2 heading (`##`):\n\n``` {.markdown code-preview=\"examples/creating-slides-1.qmd\"}\n---\ntitle: \"Habits\"\nauthor: \"John Doe\"\nformat: {{< meta slide-format >}}\n---\n\n## Getting up\n\n- Turn off alarm\n- Get out of bed\n\n## Going to sleep\n\n- Get in bed\n- Count sheep\n```\n\nYou can also divide slide shows into sections with title slides using a level 1 heading (`#`). For example:\n\n``` {.markdown code-preview=\"examples/creating-slides-2.qmd\"}\n---\ntitle: \"Habits\"\nauthor: \"John Doe\"\nformat: {{< meta slide-format >}}\n---\n\n# In the morning\n\n## Getting up\n\n- Turn off alarm\n- Get out of bed\n\n## Breakfast\n\n- Eat eggs\n- Drink coffee\n\n# In the evening\n\n## Dinner\n\n- Eat spaghetti\n- Drink wine\n\n## Going to sleep\n\n- Get in bed\n- Count sheep\n```\n\nFinally, you can also delineate slides using horizontal rules (for example, if you have a slide without a title):\n\n``` {.markdown code-preview=\"examples/creating-slides-3.qmd\"}\n---\ntitle: \"Habits\"\nauthor: \"John Doe\"\nformat: {{< meta slide-format >}}\n---\n\n- Turn off alarm\n- Get out of bed\n\n---\n\n- Get in bed\n- Count sheep\n```\n\nThe examples above all use level 2 headings for slides and level 1 headings for sections/title slides. You can customize this using the `slide-level` option (See the Pandoc documentation on [structuring the slide show](https://pandoc.org/MANUAL.html#structuring-the-slide-show) for additional details.\n\n### Title Slide\n\nYou'll notice in the above examples that a title slide is automatically created based on the `title` and `author` provided in the document YAML options. However, sometimes you don't want an explicit title slide (e.g. if your first slide consists entirely of a background image). It's perfectly valid to create a presentation without a title slide, just exclude the `title` and `author` options:\n\n``` markdown\n---\nformat: {{< meta slide-format >}}\n---\n\n## Getting up\n\n- Turn off alarm\n- Get out of bed\n\n## Going to sleep\n\n- Get in bed\n- Count sheep\n```\n",
        "plugins/lecture-designer/skills/lecture-designer/quarto/_incremental-lists-reveal.md": "## Incremental Lists\n\nBy default number and bullet lists within slides are displayed all at once. You can override this globally using the `incremental` option. For example:\n\n```{.yaml code-preview=\"examples/incremental-lists-1.qmd\"}\ntitle: \"My Presentation\"\nformat:\n  {{< meta slide-format >}}:\n    incremental: true   \n```\n\nYou can also explicitly make any list incremental or non-incremental by surrounding it in a div with an explicit class that determines the mode. To make a list incremental do this:\n\n```{.markdown code-preview=\"examples/incremental-lists-2.qmd\"}\n::: {.incremental}\n- Eat spaghetti\n- Drink wine\n:::\n```\n\nTo make a list non-incremental do this:\n\n```{.markdown code-preview=\"examples/incremental-lists-3.qmd\"}\n::: {.nonincremental}\n- Eat spaghetti\n- Drink wine\n:::\n```\n",
        "plugins/lecture-designer/skills/lecture-designer/quarto/_incremental-pause-reveal.md": "You can also insert a pause within a slide (keeping the content after the pause hidden) by inserting three dots separated by spaces:\n\n```{.markdown code-preview=\"examples/incremental-pause.qmd\"}\n## Slide with a pause\n\ncontent before the pause\n\n. . .\n\ncontent after the pause\n```\n\nNote this only works below headings that are creating slides (see [Creating slides](#creating-slides)).",
        "plugins/lecture-designer/skills/lecture-designer/quarto/_speaker-notes.md": "## Speaker Notes\n\nYou can add speaker notes to a slide using a div with class `.notes`. For example:\n\n```{.markdown}\n## Slide with speaker notes\n\nSlide content\n\n::: {.notes}\nSpeaker notes go here.\n:::\n```\n\n",
        "plugins/lecture-designer/skills/lecture-designer/quarto/index.md": "---\ntitle: Quarto Presentations (Alternative)\nformat: html\nslide-format: revealjs\n---\n\n> **Note**: This guide covers Quarto reveal.js as an alternative to Google Slides. The recommended workflow uses Google Slides via the Google Docs MCP for easier collaboration and native presentation. Use Quarto if you prefer local, version-controlled slides or need PDF output.\n\n## Overview\n\nQuarto supports a variety of formats for creating presentations, including:\n\n-   `revealjs` --- [reveal.js](revealjs/index.qmd) (HTML)\n\n-   `pptx` --- [PowerPoint](powerpoint.qmd) (MS Office)\n\n-   `beamer` --- [Beamer](beamer.qmd) (LaTeX/PDF)\n\nThere are pros and cons to each of these formats. The most capable format by far is `revealjs`, so it is highly recommended unless you have specific Office or LaTeX output requirements. Note that `revealjs` presentations can be presented as HTML slides or can be printed to PDF for easier distribution.\n\nBelow, we'll describe the basic syntax for presentations that applies to all formats. See the format-specific articles for additional details on their native capabilities.\n\n{{< include _creating-slides.md >}}\n\n{{< include _incremental-lists.md >}}\n\n{{< include _columns.md >}}\n\n\n## Learning More\n\nSee these format-specific articles for additional details on the additional capabilities of each format:\n\n-   `revealjs` --- [reveal.js](revealjs/index.qmd) (HTML)\n\n-   `pptx` --- [PowerPoint](powerpoint.qmd) (MS Office)\n\n-   `beamer` --- [Beamer](beamer.qmd) (LaTeX/PDF)\n",
        "plugins/lit-search/.claude-plugin/plugin.json": "{\n  \"name\": \"lit-search\",\n  \"description\": \"Build systematic literature databases using OpenAlex API. Phased workflow for search, screening, snowballing, annotation, and synthesis with structured user interaction.\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Neal Caren\"\n  },\n  \"license\": \"MIT\",\n  \"keywords\": [\n    \"literature-search\",\n    \"OpenAlex\",\n    \"bibliography\",\n    \"systematic-review\",\n    \"research\"\n  ],\n  \"skills\": \"./skills/\"\n}\n",
        "plugins/lit-search/skills/lit-search/SKILL.md": "---\r\nname: lit-search\r\ndescription: Build systematic literature databases for sociology research using OpenAlex API. Guides you through search, screening, snowballing, annotation, and synthesis with structured user interaction at each stage.\r\n---\r\n\r\n# Literature Search Agent\r\n\r\nYou are an expert research assistant helping build a systematic database of scholarship on a specific topic. Your role is to guide users through a rigorous, reproducible literature review process that combines API-based search with human judgment.\r\n\r\n## Core Principles\r\n\r\n1. **User expertise drives scope**: The user knows their field. You provide systematic methods; they provide domain knowledge.\r\n\r\n2. **Transparent screening**: When auto-excluding papers, show your reasoning. Users should trust the process.\r\n\r\n3. **Snowballing is essential**: Citation networks reveal papers that keyword searches miss.\r\n\r\n4. **Full text when possible**: Abstracts are insufficient for deep annotation. Help users acquire full text.\r\n\r\n5. **Structured output**: The final database should be queryable and citation-manager compatible.\r\n\r\n## API Backend\r\n\r\nThis skill uses **OpenAlex** as the primary API:\r\n- Free, no authentication required for basic use\r\n- 250M+ works with excellent metadata\r\n- Citation networks for snowballing\r\n- Open access links when available\r\n\r\nSee `api/openalex-reference.md` for query syntax and endpoints.\r\n\r\n## Review Phases\r\n\r\n### Phase 0: Scope Definition\r\n**Goal**: Define the research topic, search strategy, and inclusion criteria.\r\n\r\n**Process**:\r\n- Clarify the research question and topic boundaries\r\n- Develop search terms (synonyms, related concepts, field-specific vocabulary)\r\n- Set date range, language, and document type filters\r\n- Define explicit inclusion/exclusion criteria\r\n- Identify key journals or authors if known\r\n\r\n**Output**: Scope document with search queries and criteria.\r\n\r\n> **Pause**: User confirms search strategy before querying API.\r\n\r\n---\r\n\r\n### Phase 1: Initial Search\r\n**Goal**: Execute API queries and build initial corpus.\r\n\r\n**Process**:\r\n- Run OpenAlex queries with developed search terms\r\n- Retrieve metadata (title, abstract, authors, journal, year, citations, DOI)\r\n- Deduplicate results\r\n- Generate corpus statistics (N papers, year distribution, top journals)\r\n- Save raw results to JSON\r\n\r\n**Output**: Initial corpus with statistics and raw data file.\r\n\r\n> **Pause**: User reviews corpus size and composition.\r\n\r\n---\r\n\r\n### Phase 2: Screening\r\n**Goal**: Filter corpus to relevant papers with LLM assistance.\r\n\r\n**Process**:\r\n- Read title and abstract for each paper\r\n- Classify as: **Include** (clearly relevant), **Borderline** (uncertain), **Exclude** (clearly irrelevant)\r\n- Auto-exclude obvious misses (different field, wrong topic, non-empirical if required)\r\n- Present borderline cases to user for decision\r\n- Log screening decisions with brief rationale\r\n\r\n**Output**: Screened corpus with decision log.\r\n\r\n> **Pause**: User reviews borderline cases and approves inclusions.\r\n\r\n---\r\n\r\n### Phase 3: Snowballing\r\n**Goal**: Expand corpus through citation networks.\r\n\r\n**Process**:\r\n- For included papers, retrieve references (backward snowballing)\r\n- For included papers, retrieve citing works (forward snowballing)\r\n- Apply same screening logic to new candidates\r\n- Identify highly-cited foundational works\r\n- Flag papers that appear in multiple reference lists\r\n\r\n**Output**: Expanded corpus with citation network metadata.\r\n\r\n> **Pause**: User approves snowball additions.\r\n\r\n---\r\n\r\n### Phase 4: Full Text Acquisition\r\n**Goal**: Obtain full text for deep annotation.\r\n\r\n**Process**:\r\n- Check OpenAlex for open access versions\r\n- Query Unpaywall for OA links\r\n- Generate list of paywalled papers needing institutional access\r\n- Create download checklist for user\r\n- Track full text availability status\r\n\r\n**Output**: Full text status report and download checklist.\r\n\r\n> **Pause**: User obtains missing full texts before annotation.\r\n\r\n---\r\n\r\n### Phase 5: Annotation\r\n**Goal**: Extract structured information from each paper.\r\n\r\n**Process**:\r\n- For each paper (full text preferred, abstract if necessary):\r\n  - Research question/hypothesis\r\n  - Theoretical framework\r\n  - Methods (data, sample, analysis)\r\n  - Key findings\r\n  - Limitations noted by authors\r\n  - Relevance to user's research\r\n- User reviews and corrects extractions\r\n- Flag papers needing closer reading\r\n\r\n**Output**: Annotated database entries.\r\n\r\n> **Pause**: User reviews annotations for accuracy.\r\n\r\n---\r\n\r\n### Phase 6: Synthesis\r\n**Goal**: Generate final database and identify patterns.\r\n\r\n**Process**:\r\n- Create final JSON database with all metadata and annotations\r\n- Generate markdown annotated bibliography\r\n- Export BibTeX for citation managers\r\n- Write thematic summary of the field\r\n- Identify research gaps and debates\r\n- Suggest future directions\r\n\r\n**Output**: Complete literature database package.\r\n\r\n---\r\n\r\n## Folder Structure\r\n\r\n```\r\nlit-search/\r\n data/\r\n    raw/                    # Raw API responses\r\n       search_results.json\r\n    screened/              # After screening\r\n       included.json\r\n    annotated/             # Final annotated corpus\r\n        database.json\r\n fulltext/                  # PDF storage (user-managed)\r\n output/\r\n    bibliography.md        # Annotated bibliography\r\n    database.json          # Queryable database\r\n    references.bib         # BibTeX export\r\n    synthesis.md           # Thematic summary\r\n memos/\r\n     scope.md               # Phase 0 output\r\n     screening_log.md       # Phase 2 decisions\r\n     gaps.md                # Research gaps\r\n```\r\n\r\n## Screening Logic\r\n\r\nWhen classifying papers, apply these rules:\r\n\r\n### Auto-Exclude (with logging)\r\n- **Wrong field**: Paper clearly from unrelated discipline (e.g., medical paper when searching sociology)\r\n- **Wrong topic**: Keywords appear but topic is unrelated (e.g., \"movement\" in physics)\r\n- **Wrong document type**: If user specified empirical only, exclude pure theory/reviews\r\n- **Wrong language**: If user specified English only\r\n- **Duplicate**: Same paper from different source\r\n\r\n### Borderline (present to user)\r\n- Tangentially related topics\r\n- Relevant methods but different context\r\n- Older foundational works outside date range\r\n- Non-peer-reviewed sources (working papers, dissertations)\r\n\r\n### Include\r\n- Directly addresses the research topic\r\n- Meets all inclusion criteria\r\n- Clear relevance to user's research question\r\n\r\n## Invoking Phase Agents\r\n\r\nFor each phase, invoke the appropriate sub-agent:\r\n\r\n```\r\nTask: Phase 0 Scope Definition\r\nsubagent_type: general-purpose\r\nmodel: opus\r\nprompt: Read phases/phase0-scope.md and execute for [user's topic]\r\n```\r\n\r\n## Model Recommendations\r\n\r\n| Phase | Model | Rationale |\r\n|-------|-------|-----------|\r\n| **Phase 0**: Scope Definition | **Opus** | Strategic decisions, search design |\r\n| **Phase 1**: Initial Search | **Sonnet** | API queries, data processing |\r\n| **Phase 2**: Screening | **Sonnet** | Classification at scale |\r\n| **Phase 3**: Snowballing | **Sonnet** | Citation network processing |\r\n| **Phase 4**: Full Text | **Sonnet** | Link checking, list generation |\r\n| **Phase 5**: Annotation | **Opus** | Deep reading, extraction |\r\n| **Phase 6**: Synthesis | **Opus** | Pattern identification, writing |\r\n\r\n## Starting the Review\r\n\r\nWhen the user is ready to begin:\r\n\r\n1. **Ask about the topic**:\r\n   > \"What topic are you researching? Give me both a brief description and any specific terms you know are used in the literature.\"\r\n\r\n2. **Ask about scope**:\r\n   > \"What date range? Any specific journals or authors you want to prioritize? Any geographic or methodological focus?\"\r\n\r\n3. **Ask about purpose**:\r\n   > \"Is this for a specific paper, a comprehensive review, or exploratory research? This helps calibrate the depth.\"\r\n\r\n4. **Clarify inclusion criteria**:\r\n   > \"Should I include theoretical pieces, or only empirical studies? Reviews and meta-analyses?\"\r\n\r\n5. **Then proceed with Phase 0** to formalize the scope.\r\n\r\n## Key Reminders\r\n\r\n- **Log everything**: Every screening decision should have a rationale\r\n- **Snowballing finds gems**: Some of the best papers won't match keyword searches\r\n- **Full text matters**: Abstract-only annotation is limited; push for full text\r\n- **User is the expert**: When uncertain about relevance, ask\r\n- **Update as you go**: New papers may shift the scope; adapt\r\n- **Export early**: Generate BibTeX periodically so user can start citing\r\n",
        "plugins/lit-search/skills/lit-search/api/openalex-reference.md": "# OpenAlex API Reference\r\n\r\nThis guide covers the OpenAlex API features used in the literature review skill.\r\n\r\n## Overview\r\n\r\nOpenAlex is a free, open catalog of scholarly works with:\r\n- 250M+ works (articles, books, datasets)\r\n- Citation networks\r\n- Open access links\r\n- Author and institution data\r\n- Concepts/topics taxonomy\r\n\r\n**Base URL**: `https://api.openalex.org`\r\n\r\n**No authentication required** for basic use (but use `mailto` parameter for polite pool).\r\n\r\n## Core Endpoints\r\n\r\n### Works (Papers)\r\n\r\n```\r\nGET /works\r\n```\r\n\r\nQuery parameters:\r\n- `search` - Full-text search across title and abstract\r\n- `filter` - Structured filters (see below)\r\n- `sort` - Sort order\r\n- `per_page` - Results per page (max 200)\r\n- `cursor` - Pagination cursor\r\n\r\n### Authors\r\n\r\n```\r\nGET /authors\r\n```\r\n\r\nFind author profiles and their works.\r\n\r\n### Sources (Journals)\r\n\r\n```\r\nGET /sources\r\n```\r\n\r\nInformation about journals, repositories.\r\n\r\n### Concepts\r\n\r\n```\r\nGET /concepts\r\n```\r\n\r\nOpenAlex's topic taxonomy.\r\n\r\n## Search Syntax\r\n\r\n### Basic Search\r\n\r\n```\r\n/works?search=educational inequality\n```\r\n\r\nSearches title, abstract, and full text (when indexed).\r\n\r\n### Phrase Search\r\n\r\n```\r\n/works?search=\"collective action\"\r\n```\r\n\r\nUse quotes for exact phrases.\r\n\r\n### Boolean Operators\r\n\r\n```\r\n/works?search=inequality AND education\n/works?search=neighborhood OR context\n/works?search=education NOT psychology\n```\r\n\r\n## Filters\r\n\r\nFilters use the format: `filter=field:value`\r\n\r\nMultiple filters use commas: `filter=field1:value1,field2:value2`\r\n\r\n### Date Filters\r\n\r\n```python\r\n# Papers from 2020 onward\r\nfilter = \"from_publication_date:2020-01-01\"\r\n\r\n# Papers before 2024\r\nfilter = \"to_publication_date:2023-12-31\"\r\n\r\n# Date range\r\nfilter = \"from_publication_date:2010-01-01,to_publication_date:2024-12-31\"\r\n```\r\n\r\n### Document Type\r\n\r\n```python\r\n# Journal articles only\r\nfilter = \"type:journal-article\"\r\n\r\n# Multiple types\r\nfilter = \"type:journal-article|book-chapter\"\r\n\r\n# Available types:\r\n# journal-article, book, book-chapter, dataset, dissertation\r\n# paratext, peer-review, reference-entry, report, standard, other\r\n```\r\n\r\n### Language\r\n\r\n```python\r\n# English only\r\nfilter = \"language:en\"\r\n\r\n# Multiple languages\r\nfilter = \"language:en|es|fr\"\r\n```\r\n\r\n### Open Access\r\n\r\n```python\r\n# Only OA papers\r\nfilter = \"is_oa:true\"\r\n\r\n# Specific OA type\r\nfilter = \"oa_status:gold\"  # gold, green, bronze, hybrid\r\n```\r\n\r\n### Journal/Source\r\n\r\n```python\r\n# Papers from specific journal\r\nfilter = \"primary_location.source.id:S123456789\"\r\n\r\n# By journal name (need to look up ID first)\r\n# Better: use concepts or search\r\n```\r\n\r\n### Citation Filters\r\n\r\n```python\r\n# Papers citing a specific work\r\nfilter = \"cites:W123456789\"\r\n\r\n# Papers with minimum citations\r\nfilter = \"cited_by_count:>10\"\r\n\r\n# Highly cited\r\nfilter = \"cited_by_count:>100\"\r\n```\r\n\r\n### Concept Filters\r\n\r\n```python\r\n# Papers tagged with a concept\r\nfilter = \"concepts.id:C123456789\"\r\n\r\n# Find concept ID first:\r\n# GET /concepts?search=educational inequality\n```\r\n\r\n### Author Filters\r\n\r\n```python\r\n# Papers by specific author\r\nfilter = \"authorships.author.id:A123456789\"\r\n```\r\n\r\n## Sorting\r\n\r\n```python\r\n# Most cited first\r\nsort = \"cited_by_count:desc\"\r\n\r\n# Most recent first\r\nsort = \"publication_date:desc\"\r\n\r\n# Relevance (for searches)\r\nsort = \"relevance:desc\"\r\n\r\n# By title alphabetically\r\nsort = \"display_name:asc\"\r\n```\r\n\r\n## Pagination\r\n\r\nOpenAlex uses cursor-based pagination:\r\n\r\n```python\r\n# First page\r\n/works?search=inequality&per_page=100\n\r\n# Response includes:\r\n{\r\n    \"meta\": {\r\n        \"count\": 1234,\r\n        \"next_cursor\": \"abc123...\"\r\n    },\r\n    \"results\": [...]\r\n}\r\n\r\n# Next page\r\n/works?search=inequality&per_page=100&cursor=abc123...\n```\r\n\r\n## Response Format\r\n\r\n### Work Object\r\n\r\n```json\r\n{\r\n    \"id\": \"https://openalex.org/W123456789\",\r\n    \"doi\": \"https://doi.org/10.1234/example\",\r\n    \"title\": \"Paper Title\",\r\n    \"publication_year\": 2023,\r\n    \"publication_date\": \"2023-06-15\",\r\n\r\n    \"primary_location\": {\r\n        \"source\": {\r\n            \"id\": \"https://openalex.org/S123\",\r\n            \"display_name\": \"American Sociological Review\",\r\n            \"type\": \"journal\"\r\n        },\r\n        \"pdf_url\": \"https://...\",\r\n        \"landing_page_url\": \"https://...\"\r\n    },\r\n\r\n    \"authorships\": [\r\n        {\r\n            \"author_position\": \"first\",\r\n            \"author\": {\r\n                \"id\": \"https://openalex.org/A123\",\r\n                \"display_name\": \"Jane Smith\",\r\n                \"orcid\": \"https://orcid.org/0000-0001-2345-6789\"\r\n            },\r\n            \"institutions\": [...]\r\n        }\r\n    ],\r\n\r\n    \"cited_by_count\": 45,\r\n    \"cited_by_api_url\": \"https://api.openalex.org/works?filter=cites:W123456789\",\r\n\r\n    \"referenced_works\": [\r\n        \"https://openalex.org/W111\",\r\n        \"https://openalex.org/W222\"\r\n    ],\r\n\r\n    \"abstract_inverted_index\": {\r\n        \"This\": [0],\r\n        \"study\": [1],\r\n        \"examines\": [2]\r\n    },\r\n\r\n    \"concepts\": [\r\n        {\r\n            \"id\": \"https://openalex.org/C123\",\r\n            \"display_name\": \"Social movement\",\r\n            \"level\": 2,\r\n            \"score\": 0.89\r\n        }\r\n    ],\r\n\r\n    \"open_access\": {\r\n        \"is_oa\": true,\r\n        \"oa_status\": \"gold\",\r\n        \"oa_url\": \"https://...\"\r\n    }\r\n}\r\n```\r\n\r\n### Reconstructing Abstract\r\n\r\nOpenAlex stores abstracts as inverted indexes. Reconstruct:\r\n\r\n```python\r\ndef reconstruct_abstract(inverted_index):\r\n    \"\"\"Convert inverted index to readable abstract.\"\"\"\r\n    if not inverted_index:\r\n        return None\r\n\r\n    # Build position -> word mapping\r\n    positions = []\r\n    for word, indices in inverted_index.items():\r\n        for idx in indices:\r\n            positions.append((idx, word))\r\n\r\n    # Sort by position and join\r\n    positions.sort(key=lambda x: x[0])\r\n    return \" \".join(word for _, word in positions)\r\n```\r\n\r\n## Rate Limits\r\n\r\n- **Polite pool**: 10 requests/second (use `mailto` parameter)\r\n- **Default**: 1 request/second\r\n\r\n```python\r\n# Add to all requests for faster access\r\nparams = {\r\n    \"search\": \"protest\",\r\n    \"mailto\": \"your@email.edu\"\r\n}\r\n```\r\n\r\n## Common Query Patterns\r\n\r\n### Topic Search with Filters\r\n\r\n```python\r\nimport requests\r\n\r\ndef search_topic(topic, start_year=2010, max_results=500):\r\n    \"\"\"Search for papers on a topic.\"\"\"\r\n    base_url = \"https://api.openalex.org/works\"\r\n\r\n    params = {\r\n        \"search\": topic,\r\n        \"filter\": f\"from_publication_date:{start_year}-01-01,type:journal-article,language:en\",\r\n        \"sort\": \"cited_by_count:desc\",\r\n        \"per_page\": 100,\r\n        \"mailto\": \"your@email.edu\"\r\n    }\r\n\r\n    results = []\r\n    cursor = \"*\"\r\n\r\n    while len(results) < max_results:\r\n        params[\"cursor\"] = cursor\r\n        response = requests.get(base_url, params=params)\r\n        data = response.json()\r\n\r\n        if not data.get(\"results\"):\r\n            break\r\n\r\n        results.extend(data[\"results\"])\r\n        cursor = data[\"meta\"].get(\"next_cursor\")\r\n\r\n        if not cursor:\r\n            break\r\n\r\n    return results[:max_results]\r\n```\r\n\r\n### Get Citations for a Paper\r\n\r\n```python\r\ndef get_citing_works(openalex_id, max_results=100):\r\n    \"\"\"Get papers that cite this work.\"\"\"\r\n    # Extract ID if full URL\r\n    work_id = openalex_id.split(\"/\")[-1]\r\n\r\n    base_url = \"https://api.openalex.org/works\"\r\n    params = {\r\n        \"filter\": f\"cites:{work_id}\",\r\n        \"sort\": \"cited_by_count:desc\",\r\n        \"per_page\": min(max_results, 200),\r\n        \"mailto\": \"your@email.edu\"\r\n    }\r\n\r\n    response = requests.get(base_url, params=params)\r\n    return response.json().get(\"results\", [])\r\n```\r\n\r\n### Get References from a Paper\r\n\r\n```python\r\ndef get_references(openalex_id):\r\n    \"\"\"Get papers cited by this work.\"\"\"\r\n    work_id = openalex_id.split(\"/\")[-1]\r\n\r\n    # First get the work\r\n    work_url = f\"https://api.openalex.org/works/{work_id}\"\r\n    response = requests.get(work_url, params={\"mailto\": \"your@email.edu\"})\r\n    work = response.json()\r\n\r\n    # Then fetch each reference\r\n    ref_ids = work.get(\"referenced_works\", [])\r\n    references = []\r\n\r\n    for ref_id in ref_ids[:50]:  # Limit for efficiency\r\n        ref_short_id = ref_id.split(\"/\")[-1]\r\n        ref_response = requests.get(\r\n            f\"https://api.openalex.org/works/{ref_short_id}\",\r\n            params={\"mailto\": \"your@email.edu\"}\r\n        )\r\n        if ref_response.status_code == 200:\r\n            references.append(ref_response.json())\r\n\r\n    return references\r\n```\r\n\r\n### Find Concept ID\r\n\r\n```python\r\ndef find_concept(term):\r\n    \"\"\"Look up concept ID for filtering.\"\"\"\r\n    url = \"https://api.openalex.org/concepts\"\r\n    params = {\r\n        \"search\": term,\r\n        \"per_page\": 5,\r\n        \"mailto\": \"your@email.edu\"\r\n    }\r\n\r\n    response = requests.get(url, params=params)\r\n    results = response.json().get(\"results\", [])\r\n\r\n    for concept in results:\r\n        print(f\"{concept['display_name']}: {concept['id']}\")\r\n        print(f\"  Level: {concept['level']}, Works: {concept['works_count']}\")\r\n\r\n    return results\r\n```\r\n\r\n## Useful Concept IDs for Sociology\r\n\r\nPre-looked-up concept IDs:\r\n\r\n| Concept | ID | Level |\r\n|---------|-----|-------|\r\n| Sociology | C144024400 | 0 |\r\n| Social movement | C2779832528 | 2 |\r\n| Political science | C17744445 | 0 |\r\n| Collective action | C2778097702 | 2 |\r\n| Social network | C121332964 | 2 |\r\n| Protest | C2776822296 | 3 |\r\n\r\nUse in filter:\r\n```\r\nfilter=concepts.id:C2779832528\r\n```\r\n\r\n## Error Handling\r\n\r\n```python\r\ndef safe_request(url, params, max_retries=3):\r\n    \"\"\"Make request with retry logic.\"\"\"\r\n    for attempt in range(max_retries):\r\n        try:\r\n            response = requests.get(url, params=params, timeout=30)\r\n            response.raise_for_status()\r\n            return response.json()\r\n        except requests.exceptions.HTTPError as e:\r\n            if response.status_code == 429:  # Rate limit\r\n                time.sleep(2 ** attempt)\r\n                continue\r\n            raise\r\n        except requests.exceptions.Timeout:\r\n            if attempt < max_retries - 1:\r\n                continue\r\n            raise\r\n\r\n    return None\r\n```\r\n\r\n## Additional Resources\r\n\r\n- **API Documentation**: https://docs.openalex.org\r\n- **Data Schema**: https://docs.openalex.org/api-entities/works\r\n- **Filters Reference**: https://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/filter-entity-lists\r\n",
        "plugins/lit-search/skills/lit-search/phases/phase0-scope.md": "# Phase 0: Scope Definition\r\n\r\nYou are defining the scope of a systematic literature search. Your goal is to help the user develop a clear, reproducible search strategy.\r\n\r\n## Why This Phase Matters\r\n\r\nA literature review is only as good as its scope. Overly narrow searches miss important work; overly broad searches create unmanageable screening burdens. This phase establishes the boundaries before any API calls.\r\n\r\n## Your Tasks\r\n\r\n### 1. Clarify the Research Topic\r\n\r\nAsk the user to describe their topic in plain language. Then probe for:\r\n\r\n- **Core concept**: What is the central phenomenon? (e.g., \"educational inequality\")\n- **Boundaries**: What is NOT included? (e.g., \"not K-12 outcomes, not higher education\")\n- **Level of analysis**: Individual, organizational, field-level?\r\n- **Theoretical tradition**: Does the user work within a specific tradition (resource mobilization, political process, etc.)?\r\n\r\n### 2. Develop Search Terms\r\n\r\nWork with the user to build a comprehensive term list:\r\n\r\n```\r\nPrimary terms:\n- [primary term 1]\n- [primary term 2]\n- [primary term 3]\n\nSynonyms and variants:\n- [synonym 1]\n- [synonym 2]\n- [nearby term that may be too broad]\n\nField-specific vocabulary:\n- [concept 1]\n- [concept 2]\n- [concept 3]\n```\r\n\r\nConsider:\r\n- Spelling variants (behavior/behaviour)\r\n- Acronyms used in the field\r\n- Terms that have evolved over time\r\n- Boolean combinations (participation AND (protest OR movement))\r\n\r\n### 3. Define Filters\r\n\r\nEstablish explicit criteria:\r\n\r\n| Filter | User's Choice | Notes |\r\n|--------|---------------|-------|\r\n| **Date range** | e.g., 2010-2024 | Consider field development |\r\n| **Language** | English only? | May miss important non-English work |\r\n| **Document types** | Journal articles, books, working papers? | |\r\n| **Empirical only?** | Exclude pure theory? | |\r\n| **Peer-reviewed only?** | Exclude dissertations, reports? | |\r\n\r\n### 4. Identify Key Sources\r\n\r\nAsk about:\r\n- **Key journals**: American Sociological Review, American Journal of Sociology, Social Forces, etc.\n- **Key authors**: Who are the major scholars in this area?\r\n- **Foundational works**: Are there must-include classic papers?\r\n- **Known gaps**: Are there subtopics the user already knows are understudied?\r\n\r\n### 5. Set Practical Constraints\r\n\r\nDiscuss:\r\n- **Target corpus size**: How many papers can the user realistically annotate?\r\n- **Time available**: Is this a comprehensive review or targeted search?\r\n- **Use case**: Dissertation lit review? Journal article? Grant proposal?\r\n\r\n### 6. Draft OpenAlex Queries\r\n\r\nBased on the above, construct initial API queries:\r\n\r\n```python\r\n# Example query structure for OpenAlex\r\nbase_url = \"https://api.openalex.org/works\"\r\nparams = {\n    \"search\": \"[primary term 1] [primary term 2]\",\n    \"filter\": \"from_publication_date:2010-01-01,to_publication_date:2024-12-31,type:journal-article,language:en\",\r\n    \"sort\": \"cited_by_count:desc\",\r\n    \"per_page\": 100\r\n}\r\n```\r\n\r\nCreate multiple query variants to maximize coverage:\r\n1. Primary term search\r\n2. Abstract search with Boolean terms\r\n3. Concept-based search using OpenAlex concepts\r\n\r\n## Output: Scope Document\r\n\r\nCreate `memos/scope.md` with:\r\n\r\n```markdown\r\n# Literature Review Scope\r\n\r\n## Research Topic\r\n[User's description]\r\n\r\n## Inclusion Criteria\r\n- Date range:\r\n- Document types:\r\n- Language:\r\n- Empirical/theoretical:\r\n- Other:\r\n\r\n## Exclusion Criteria\r\n- [List explicit exclusions]\r\n\r\n## Search Terms\r\n\r\n### Primary\r\n- term1\r\n- term2\r\n\r\n### Secondary/Synonyms\r\n- term3\r\n- term4\r\n\r\n### Boolean Queries\r\n- (term1 OR term2) AND term3\r\n\r\n## Key Sources\r\n- Journals: [list]\r\n- Authors: [list]\r\n- Must-include works: [list]\r\n\r\n## OpenAlex Query Strategy\r\n[Document planned queries]\r\n\r\n## Target Corpus Size\r\n[Estimated range]\r\n```\r\n\r\n## Guiding Principles\r\n\r\n- **User knows the field**: They can identify synonyms and boundaries you can't\r\n- **Start broad, narrow later**: Better to screen out than miss\r\n- **Document decisions**: Every scope choice should be recorded for the methods section\r\n- **Iterate**: Initial terms will evolve as you see results\r\n\r\n## When You're Done\r\n\r\nTell the orchestrator:\r\n> \"Phase 0 complete. Scope document created at memos/scope.md. Ready for user confirmation before API queries.\"\r\n\r\n**Do not proceed to Phase 1 until the user approves the search strategy.**\r\n",
        "plugins/lit-search/skills/lit-search/phases/phase1-search.md": "# Phase 1: Initial Search\r\n\r\nYou are executing the literature search using the OpenAlex API. Your goal is to build an initial corpus of potentially relevant papers.\r\n\r\n## Why This Phase Matters\r\n\r\nThis phase translates the scope document into actual data. Good API queries maximize recall (finding relevant papers) while keeping the corpus manageable for screening.\r\n\r\n## Prerequisites\r\n\r\n- Read `memos/scope.md` for search terms and filters\r\n- Read `api/openalex-reference.md` for API syntax\r\n\r\n## Your Tasks\r\n\r\n### 1. Execute Primary Searches\r\n\r\nRun the planned queries against OpenAlex. Use Python with the `requests` library:\r\n\r\n```python\r\nimport requests\r\nimport json\r\nfrom time import sleep\r\n\r\ndef search_openalex(query, filters, max_results=500):\r\n    \"\"\"\r\n    Search OpenAlex with pagination.\r\n    \"\"\"\r\n    base_url = \"https://api.openalex.org/works\"\r\n    results = []\r\n    cursor = \"*\"\r\n\r\n    while len(results) < max_results:\r\n        params = {\r\n            \"search\": query,\r\n            \"filter\": filters,\r\n            \"per_page\": 100,\r\n            \"cursor\": cursor,\r\n            \"mailto\": \"your@email.com\"  # Polite pool for faster responses\r\n        }\r\n\r\n        response = requests.get(base_url, params=params)\r\n        data = response.json()\r\n\r\n        if \"results\" not in data or not data[\"results\"]:\r\n            break\r\n\r\n        results.extend(data[\"results\"])\r\n        cursor = data[\"meta\"].get(\"next_cursor\")\r\n\r\n        if not cursor:\r\n            break\r\n\r\n        sleep(0.1)  # Rate limiting\r\n\r\n    return results[:max_results]\r\n\r\n# Example usage\r\nresults = search_openalex(\n    query=\"[primary term 1] [primary term 2]\",\n    filters=\"from_publication_date:2010-01-01,type:journal-article,language:en\",\n    max_results=500\n)\n```\r\n\r\n### 2. Extract Key Metadata\r\n\r\nFor each paper, extract and store:\r\n\r\n```python\r\ndef extract_metadata(work):\r\n    \"\"\"Extract relevant fields from OpenAlex work.\"\"\"\r\n    return {\r\n        \"openalex_id\": work.get(\"id\"),\r\n        \"doi\": work.get(\"doi\"),\r\n        \"title\": work.get(\"title\"),\r\n        \"publication_year\": work.get(\"publication_year\"),\r\n        \"abstract\": work.get(\"abstract_inverted_index\"),  # Needs reconstruction\r\n        \"authors\": [a.get(\"author\", {}).get(\"display_name\") for a in work.get(\"authorships\", [])],\r\n        \"journal\": work.get(\"primary_location\", {}).get(\"source\", {}).get(\"display_name\"),\r\n        \"cited_by_count\": work.get(\"cited_by_count\"),\r\n        \"concepts\": [c.get(\"display_name\") for c in work.get(\"concepts\", [])[:5]],\r\n        \"open_access\": work.get(\"open_access\", {}).get(\"is_oa\"),\r\n        \"oa_url\": work.get(\"open_access\", {}).get(\"oa_url\"),\r\n        \"referenced_works\": work.get(\"referenced_works\", []),\r\n        \"cited_by_api_url\": work.get(\"cited_by_api_url\")\r\n    }\r\n\r\ndef reconstruct_abstract(inverted_index):\r\n    \"\"\"Reconstruct abstract from OpenAlex inverted index format.\"\"\"\r\n    if not inverted_index:\r\n        return None\r\n    word_positions = []\r\n    for word, positions in inverted_index.items():\r\n        for pos in positions:\r\n            word_positions.append((pos, word))\r\n    word_positions.sort()\r\n    return \" \".join(word for pos, word in word_positions)\r\n```\r\n\r\n### 3. Deduplicate Results\r\n\r\nPapers may appear in multiple queries. Deduplicate by DOI and OpenAlex ID:\r\n\r\n```python\r\ndef deduplicate(results):\r\n    \"\"\"Remove duplicate papers.\"\"\"\r\n    seen_ids = set()\r\n    unique = []\r\n    for paper in results:\r\n        paper_id = paper.get(\"doi\") or paper.get(\"openalex_id\")\r\n        if paper_id and paper_id not in seen_ids:\r\n            seen_ids.add(paper_id)\r\n            unique.append(paper)\r\n    return unique\r\n```\r\n\r\n### 4. Generate Corpus Statistics\r\n\r\nCreate summary statistics to help the user assess the search:\r\n\r\n```python\r\nfrom collections import Counter\r\n\r\ndef corpus_stats(papers):\r\n    \"\"\"Generate descriptive statistics.\"\"\"\r\n    years = Counter(p[\"publication_year\"] for p in papers if p[\"publication_year\"])\r\n    journals = Counter(p[\"journal\"] for p in papers if p[\"journal\"])\r\n\r\n    stats = {\r\n        \"total_papers\": len(papers),\r\n        \"year_range\": f\"{min(years.keys())}-{max(years.keys())}\",\r\n        \"year_distribution\": dict(sorted(years.items())),\r\n        \"top_journals\": journals.most_common(10),\r\n        \"papers_with_abstracts\": sum(1 for p in papers if p.get(\"abstract\")),\r\n        \"open_access_count\": sum(1 for p in papers if p.get(\"open_access\")),\r\n        \"median_citations\": sorted([p[\"cited_by_count\"] for p in papers])[len(papers)//2]\r\n    }\r\n    return stats\r\n```\r\n\r\n### 5. Save Raw Results\r\n\r\nSave to `data/raw/search_results.json`:\r\n\r\n```python\r\noutput = {\r\n    \"search_metadata\": {\r\n        \"date\": \"2024-01-15\",\r\n        \"queries\": [\"query1\", \"query2\"],\r\n        \"filters\": \"from_publication_date:2010-01-01,...\"\r\n    },\r\n    \"statistics\": corpus_stats(papers),\r\n    \"papers\": papers\r\n}\r\n\r\nwith open(\"data/raw/search_results.json\", \"w\") as f:\r\n    json.dump(output, f, indent=2)\r\n```\r\n\r\n## Output Summary\r\n\r\nPresent to the user:\r\n\r\n```markdown\r\n## Initial Search Results\r\n\r\n**Total papers found**: 347 (after deduplication)\r\n\r\n### Year Distribution\r\n| Year | Count |\r\n|------|-------|\r\n| 2024 | 28    |\r\n| 2023 | 45    |\r\n| ...  | ...   |\r\n\r\n### Top Journals\r\n1. Mobilization (23 papers)\r\n2. Social Movement Studies (19 papers)\r\n3. American Sociological Review (12 papers)\r\n...\r\n\r\n### Corpus Characteristics\r\n- Papers with abstracts: 341/347 (98%)\r\n- Open access available: 156/347 (45%)\r\n- Median citations: 12\r\n\r\n### Sample Titles (most cited)\r\n1. [Title 1] (2018) - 245 citations\r\n2. [Title 2] (2015) - 198 citations\r\n...\r\n```\r\n\r\n## Guiding Principles\r\n\r\n- **Cast a wide net**: Include papers you're unsure about; screening comes next\r\n- **Check coverage**: If key papers you expected are missing, refine queries\r\n- **Mind the API**: Use polite pool (mailto) and rate limiting\r\n- **Save raw data**: Never modify raw results; work from copies\r\n\r\n## When You're Done\r\n\r\nTell the orchestrator:\r\n> \"Phase 1 complete. Initial corpus of N papers saved to data/raw/search_results.json. Statistics and sample titles presented. Ready for user review before screening.\"\r\n\r\n**Do not proceed to Phase 2 until the user reviews the corpus composition.**\r\n",
        "plugins/lit-search/skills/lit-search/phases/phase2-screening.md": "# Phase 2: Screening\r\n\r\nYou are screening the initial corpus to identify relevant papers. Your goal is to efficiently filter while maintaining transparency about decisions.\r\n\r\n## Why This Phase Matters\r\n\r\nScreening is where human judgment meets algorithmic assistance. You'll auto-exclude obvious misses to reduce user burden, but every decision must be logged and defensible.\r\n\r\n## Prerequisites\r\n\r\n- Read `memos/scope.md` for inclusion/exclusion criteria\r\n- Load `data/raw/search_results.json`\r\n\r\n## Your Tasks\r\n\r\n### 1. Load Inclusion Criteria\r\n\r\nFrom the scope document, extract explicit criteria:\r\n\r\n```python\r\ncriteria = {\n    \"must_include\": [\n        \"Addresses the core phenomenon\",\n        \"Empirical study (quantitative or qualitative)\",\n        \"Published 2010-2024\"\n    ],\n    \"must_exclude\": [\n        \"Pure theoretical/conceptual pieces (unless foundational)\",\n        \"Studies outside the target domain\",\n        \"Non-English without translation\"\n    ],\n    \"borderline_indicators\": [\n        \"Adjacent phenomena that may overlap\",\n        \"Broader terms that require clarification\",\n        \"Organizational contexts that may or may not fit\"\n    ]\n}\n```\r\n\r\n### 2. Screen Each Paper\r\n\r\nFor each paper, read title and abstract (if available), then classify:\r\n\r\n```python\r\ndef screen_paper(paper, criteria):\n    \"\"\"\r\n    Screen a single paper against criteria.\r\n    Returns: (decision, rationale)\r\n    \"\"\"\r\n    title = paper.get(\"title\", \"\").lower()\r\n    abstract = paper.get(\"abstract\", \"\").lower() if paper.get(\"abstract\") else \"\"\r\n\r\n    # Check for clear exclusions\r\n    exclusion_signals = [\r\n        (\"medical\" in title or \"clinical\" in abstract, \"Medical/clinical focus\"),\r\n        (\"physics\" in title or \"particle\" in abstract, \"Wrong discipline\"),\r\n        # Add domain-specific exclusions\r\n    ]\r\n\r\n    for signal, reason in exclusion_signals:\r\n        if signal:\r\n            return (\"exclude\", f\"Auto-exclude: {reason}\")\r\n\r\n    # Check for clear inclusions\n    scope_primary_terms = [t.lower() for t in criteria.get(\"primary_terms\", [])]\n    inclusion_signals = [\n        # Add domain-specific inclusions based on scope terms\n        any(term in title for term in scope_primary_terms),\n        any(term in abstract for term in scope_primary_terms),\n    ]\n\r\n    if any(inclusion_signals):\r\n        return (\"include\", \"Strong relevance signals in title/abstract\")\r\n\r\n    # Everything else is borderline\r\n    return (\"borderline\", \"Requires user review\")\r\n```\r\n\r\n### 3. Log All Decisions\r\n\r\nCreate a detailed screening log:\r\n\r\n```python\r\nscreening_log = []\r\n\r\nfor paper in papers:\r\n    decision, rationale = screen_paper(paper, criteria)\r\n    screening_log.append({\r\n        \"openalex_id\": paper[\"openalex_id\"],\r\n        \"title\": paper[\"title\"],\r\n        \"year\": paper[\"publication_year\"],\r\n        \"decision\": decision,\r\n        \"rationale\": rationale,\r\n        \"abstract_snippet\": paper.get(\"abstract\", \"\")[:200] if paper.get(\"abstract\") else \"No abstract\"\r\n    })\r\n```\r\n\r\n### 4. Present Borderline Cases to User\r\n\r\nGroup borderline papers and present for user decision:\r\n\r\n```markdown\r\n## Borderline Papers Requiring Review\r\n\r\n### Paper 1\r\n**Title**: Political Participation and Democratic Engagement in Urban Contexts\r\n**Year**: 2019\r\n**Journal**: Political Behavior\r\n**Abstract**: [First 200 characters]...\r\n**Question**: This discusses political participation broadly. Does it include movement-related participation?\r\n\r\n- [ ] Include\r\n- [ ] Exclude\r\n\r\n### Paper 2\r\n...\r\n```\r\n\r\nPresent in batches of 10-20 for user to review.\r\n\r\n### 5. Generate Screening Summary\r\n\r\nAfter user input on borderline cases:\r\n\r\n```markdown\r\n## Screening Summary\r\n\r\n| Category | Count | Percentage |\r\n|----------|-------|------------|\r\n| Included | 156 | 45% |\r\n| Excluded (auto) | 142 | 41% |\r\n| Excluded (user) | 28 | 8% |\r\n| Borderline  Included | 15 | 4% |\r\n| Borderline  Excluded | 6 | 2% |\r\n| **Total** | **347** | **100%** |\r\n\r\n### Exclusion Reasons\r\n| Reason | Count |\r\n|--------|-------|\r\n| Wrong discipline | 45 |\r\n| Wrong topic | 38 |\r\n| Non-empirical | 32 |\r\n| Outside date range | 27 |\r\n\r\n### Included Papers by Year\r\n[Year distribution chart]\r\n\r\n### Included Papers by Journal\r\n1. Mobilization (18)\r\n2. Social Movement Studies (15)\r\n...\r\n```\r\n\r\n### 6. Save Screened Corpus\r\n\r\nSave included papers to `data/screened/included.json`:\r\n\r\n```python\r\nincluded = [p for p in papers if screening_decisions[p[\"openalex_id\"]] == \"include\"]\r\n\r\noutput = {\r\n    \"screening_metadata\": {\r\n        \"date\": \"2024-01-15\",\r\n        \"criteria\": criteria,\r\n        \"total_screened\": len(papers),\r\n        \"total_included\": len(included)\r\n    },\r\n    \"papers\": included\r\n}\r\n\r\nwith open(\"data/screened/included.json\", \"w\") as f:\r\n    json.dump(output, f, indent=2)\r\n```\r\n\r\nAlso save the full log to `memos/screening_log.md`.\r\n\r\n## Screening Heuristics\r\n\r\nWhen in doubt, use these guidelines:\r\n\r\n### Lean Include\r\n- Title mentions core concepts\r\n- From a key journal in the field\r\n- By a known author in the area\r\n- Highly cited\r\n\r\n### Lean Exclude\r\n- Clearly from another discipline\r\n- Keywords appear but in different context\r\n- Publication type doesn't match criteria\r\n\r\n### Always Ask User\r\n- Foundational works outside date range\r\n- Adjacent topics that might be relevant\r\n- Methods papers that might inform the research\r\n\r\n## Guiding Principles\r\n\r\n- **Transparent reasoning**: Every exclusion should have a logged rationale\r\n- **Conservative auto-exclude**: Only auto-exclude when clearly irrelevant\r\n- **Batch borderline review**: Don't interrupt user for each paper\r\n- **Track statistics**: Know how many papers at each stage\r\n\r\n## When You're Done\r\n\r\nTell the orchestrator:\r\n> \"Phase 2 complete. Screened N papers: X included, Y excluded, Z user-reviewed. Screened corpus saved to data/screened/included.json. Screening log at memos/screening_log.md. Ready for snowballing.\"\r\n\r\n**Do not proceed to Phase 3 until the user confirms screening decisions.**\r\n",
        "plugins/lit-search/skills/lit-search/phases/phase3-snowball.md": "# Phase 3: Snowballing\r\n\r\nYou are expanding the corpus through citation networks. Your goal is to find important papers that keyword searches missed.\r\n\r\n## Why This Phase Matters\r\n\r\nCitation networks reveal the intellectual structure of a field. Backward snowballing finds foundational works; forward snowballing finds recent developments. Some of the most important papers won't match keyword searches.\r\n\r\n## Prerequisites\r\n\r\n- Load `data/screened/included.json`\r\n- Read `memos/scope.md` for inclusion criteria\r\n\r\n## Your Tasks\r\n\r\n### 1. Backward Snowballing (References)\r\n\r\nFor each included paper, retrieve its references:\r\n\r\n```python\r\nimport requests\r\nfrom time import sleep\r\n\r\ndef get_references(openalex_id, max_refs=50):\r\n    \"\"\"Get papers cited by this work.\"\"\"\r\n    # OpenAlex includes referenced_works in the work object\r\n    work_url = f\"https://api.openalex.org/works/{openalex_id}\"\r\n    response = requests.get(work_url, params={\"mailto\": \"your@email.com\"})\r\n    work = response.json()\r\n\r\n    ref_ids = work.get(\"referenced_works\", [])[:max_refs]\r\n\r\n    # Fetch metadata for each reference\r\n    references = []\r\n    for ref_id in ref_ids:\r\n        ref_url = f\"https://api.openalex.org/works/{ref_id.split('/')[-1]}\"\r\n        try:\r\n            ref_response = requests.get(ref_url, params={\"mailto\": \"your@email.com\"})\r\n            if ref_response.status_code == 200:\r\n                references.append(ref_response.json())\r\n            sleep(0.1)\r\n        except:\r\n            continue\r\n\r\n    return references\r\n```\r\n\r\n### 2. Forward Snowballing (Citations)\r\n\r\nFor each included paper, retrieve works that cite it:\r\n\r\n```python\r\ndef get_citations(openalex_id, max_citations=50):\r\n    \"\"\"Get papers that cite this work.\"\"\"\r\n    base_url = \"https://api.openalex.org/works\"\r\n    params = {\r\n        \"filter\": f\"cites:{openalex_id}\",\r\n        \"per_page\": max_citations,\r\n        \"sort\": \"cited_by_count:desc\",\r\n        \"mailto\": \"your@email.com\"\r\n    }\r\n\r\n    response = requests.get(base_url, params=params)\r\n    return response.json().get(\"results\", [])\r\n```\r\n\r\n### 3. Identify High-Value Candidates\r\n\r\nPrioritize papers that appear multiple times in citation networks:\r\n\r\n```python\r\nfrom collections import Counter\r\n\r\ndef identify_candidates(included_papers):\r\n    \"\"\"Find papers that appear frequently in references/citations.\"\"\"\r\n    all_refs = []\r\n    all_citations = []\r\n\r\n    for paper in included_papers:\r\n        refs = get_references(paper[\"openalex_id\"])\r\n        cites = get_citations(paper[\"openalex_id\"])\r\n\r\n        all_refs.extend([r[\"id\"] for r in refs])\r\n        all_citations.extend([c[\"id\"] for c in cites])\r\n\r\n    # Count occurrences\r\n    ref_counts = Counter(all_refs)\r\n    cite_counts = Counter(all_citations)\r\n\r\n    # Papers appearing 3+ times are high priority\r\n    frequent_refs = {id: count for id, count in ref_counts.items() if count >= 3}\r\n    frequent_cites = {id: count for id, count in cite_counts.items() if count >= 3}\r\n\r\n    return frequent_refs, frequent_cites\r\n```\r\n\r\n### 4. Screen Snowball Candidates\r\n\r\nApply the same screening logic as Phase 2:\r\n\r\n```python\r\ndef screen_snowball_candidates(candidates, existing_ids, criteria):\r\n    \"\"\"Screen new papers found through snowballing.\"\"\"\r\n    new_papers = []\r\n\r\n    for paper_id, appearance_count in candidates.items():\r\n        # Skip if already in corpus\r\n        if paper_id in existing_ids:\r\n            continue\r\n\r\n        # Fetch metadata\r\n        paper = fetch_paper(paper_id)\r\n        if not paper:\r\n            continue\r\n\r\n        # Apply screening\r\n        decision, rationale = screen_paper(paper, criteria)\r\n\r\n        new_papers.append({\r\n            \"paper\": paper,\r\n            \"decision\": decision,\r\n            \"rationale\": rationale,\r\n            \"snowball_type\": \"backward\" if paper_id in ref_counts else \"forward\",\r\n            \"appearance_count\": appearance_count\r\n        })\r\n\r\n    return new_papers\r\n```\r\n\r\n### 5. Present Snowball Additions\r\n\r\nShow the user what snowballing found:\r\n\r\n```markdown\r\n## Snowball Candidates\r\n\r\n### Highly Cited References (Backward Snowballing)\r\nPapers cited by 3+ included works:\r\n\r\n| Title | Year | Cited By | Appearances | Decision |\r\n|-------|------|----------|-------------|----------|\r\n| [Foundational paper 1] | 2008 | 450 | 12/156 | Include? |\r\n| [Foundational paper 2] | 2005 | 380 | 9/156 | Include? |\r\n\r\n### Recent Citing Works (Forward Snowballing)\r\nPapers citing 3+ included works:\r\n\r\n| Title | Year | Citations | Appearances | Decision |\r\n|-------|------|-----------|-------------|----------|\r\n| [Recent paper 1] | 2024 | 5 | 4/156 | Include? |\r\n| [Recent paper 2] | 2023 | 12 | 3/156 | Include? |\r\n\r\n### Foundational Works Outside Date Range\r\nThese pre-date your range but are heavily referenced:\r\n\r\n| Title | Year | Why Consider |\r\n|-------|------|--------------|\r\n| [Classic 1] | 1995 | Cited by 45% of corpus |\r\n| [Classic 2] | 2001 | Foundational methods paper |\r\n\r\n**Question**: Should we include foundational works that pre-date your date range?\r\n```\r\n\r\n### 6. Update Corpus\r\n\r\nAfter user approval, merge snowball additions:\r\n\r\n```python\r\ndef merge_snowball(original_corpus, snowball_additions):\r\n    \"\"\"Add approved snowball papers to corpus.\"\"\"\r\n    approved = [p for p in snowball_additions if p[\"user_decision\"] == \"include\"]\r\n\r\n    merged = {\r\n        \"original_count\": len(original_corpus),\r\n        \"snowball_additions\": len(approved),\r\n        \"total\": len(original_corpus) + len(approved),\r\n        \"papers\": original_corpus + [p[\"paper\"] for p in approved]\r\n    }\r\n\r\n    return merged\r\n```\r\n\r\n### 7. Generate Citation Network Visualization\r\n\r\nCreate a simple network summary:\r\n\r\n```markdown\r\n## Citation Network Summary\r\n\r\n**Core papers** (cited by 5+ included works):\r\n1. [Paper A] - hub for theoretical framework\r\n2. [Paper B] - key methods reference\r\n\r\n**Bridge papers** (connect different clusters):\r\n1. [Paper C] - links quantitative and qualitative traditions\r\n\r\n**Emerging work** (2023-2024, already accumulating citations):\r\n1. [Paper D] - 15 citations in first year\r\n```\r\n\r\n## Output Files\r\n\r\nSave to `data/screened/included_with_snowball.json`:\r\n\r\n```python\r\noutput = {\r\n    \"snowball_metadata\": {\r\n        \"date\": \"2024-01-15\",\r\n        \"backward_candidates_found\": 89,\r\n        \"forward_candidates_found\": 124,\r\n        \"approved_additions\": 23\r\n    },\r\n    \"papers\": merged_corpus\r\n}\r\n```\r\n\r\nAlso update `memos/screening_log.md` with snowball decisions.\r\n\r\n## Guiding Principles\r\n\r\n- **Trust the network**: Papers cited by many included works are likely relevant\r\n- **Catch recent work**: Forward snowballing finds papers too new for keyword indexing\r\n- **Respect the scope**: Foundational works may warrant exception to date range\r\n- **Document provenance**: Track how each paper entered the corpus\r\n\r\n## When You're Done\r\n\r\nTell the orchestrator:\r\n> \"Phase 3 complete. Snowballing found N backward and M forward candidates. X approved additions merged. Expanded corpus now contains Y papers. Ready for full text acquisition.\"\r\n\r\n**Do not proceed to Phase 4 until the user approves snowball additions.**\r\n",
        "plugins/lit-search/skills/lit-search/phases/phase4-fulltext.md": "# Phase 4: Full Text Acquisition\r\n\r\nYou are helping the user obtain full text for the corpus papers. Your goal is to maximize full text coverage for deep annotation.\r\n\r\n## Why This Phase Matters\r\n\r\nAbstract-only annotation is limited. Full text reveals methods details, nuanced findings, and theoretical contributions that abstracts omit. This phase identifies available sources and creates a checklist for the user.\r\n\r\n## Prerequisites\r\n\r\n- Load `data/screened/included_with_snowball.json` (or `included.json` if no snowballing)\r\n\r\n## Your Tasks\r\n\r\n### 1. Check OpenAlex Open Access Status\r\n\r\nOpenAlex includes OA information:\r\n\r\n```python\r\ndef check_oa_status(papers):\r\n    \"\"\"Categorize papers by open access availability.\"\"\"\r\n    oa_status = {\r\n        \"gold_oa\": [],      # Published OA\r\n        \"green_oa\": [],     # Repository version\r\n        \"bronze_oa\": [],    # Free to read\r\n        \"closed\": []        # Paywalled\r\n    }\r\n\r\n    for paper in papers:\r\n        oa_info = paper.get(\"open_access\", {})\r\n        is_oa = oa_info.get(\"is_oa\", False)\r\n        oa_url = oa_info.get(\"oa_url\")\r\n        oa_status_type = oa_info.get(\"oa_status\", \"closed\")\r\n\r\n        paper_info = {\r\n            \"title\": paper[\"title\"],\r\n            \"doi\": paper.get(\"doi\"),\r\n            \"year\": paper[\"publication_year\"],\r\n            \"oa_url\": oa_url\r\n        }\r\n\r\n        if oa_status_type == \"gold\":\r\n            oa_status[\"gold_oa\"].append(paper_info)\r\n        elif oa_status_type == \"green\":\r\n            oa_status[\"green_oa\"].append(paper_info)\r\n        elif oa_status_type == \"bronze\":\r\n            oa_status[\"bronze_oa\"].append(paper_info)\r\n        else:\r\n            oa_status[\"closed\"].append(paper_info)\r\n\r\n    return oa_status\r\n```\r\n\r\n### 2. Query Unpaywall for Additional OA Links\r\n\r\nUnpaywall may have sources OpenAlex missed:\r\n\r\n```python\r\ndef check_unpaywall(doi, email):\r\n    \"\"\"Query Unpaywall API for OA version.\"\"\"\r\n    if not doi:\r\n        return None\r\n\r\n    # Clean DOI\r\n    doi = doi.replace(\"https://doi.org/\", \"\")\r\n\r\n    url = f\"https://api.unpaywall.org/v2/{doi}\"\r\n    params = {\"email\": email}\r\n\r\n    try:\r\n        response = requests.get(url, params=params)\r\n        if response.status_code == 200:\r\n            data = response.json()\r\n            if data.get(\"is_oa\"):\r\n                best_location = data.get(\"best_oa_location\", {})\r\n                return {\r\n                    \"url\": best_location.get(\"url_for_pdf\") or best_location.get(\"url\"),\r\n                    \"version\": best_location.get(\"version\"),\r\n                    \"host_type\": best_location.get(\"host_type\")\r\n                }\r\n    except:\r\n        pass\r\n\r\n    return None\r\n```\r\n\r\n### 3. Check for Preprints\r\n\r\nMany sociology papers have preprint versions:\r\n\r\n```python\r\ndef find_preprints(paper):\r\n    \"\"\"Check common preprint servers.\"\"\"\r\n    title = paper.get(\"title\", \"\")\r\n    authors = paper.get(\"authors\", [])\r\n\r\n    # Check SocArXiv via OSF API\r\n    # Check SSRN\r\n    # Check author institutional repositories\r\n\r\n    # OpenAlex may already have this in alternate_host_venues\r\n    alt_venues = paper.get(\"locations\", [])\r\n    for venue in alt_venues:\r\n        source = venue.get(\"source\", {})\r\n        if source.get(\"type\") == \"repository\":\r\n            return venue.get(\"pdf_url\") or venue.get(\"landing_page_url\")\r\n\r\n    return None\r\n```\r\n\r\n### 4. Generate Full Text Status Report\r\n\r\n```markdown\r\n## Full Text Availability Report\r\n\r\n**Corpus size**: 179 papers\r\n\r\n### Open Access Available (direct download)\r\n| Status | Count | Percentage |\r\n|--------|-------|------------|\r\n| Gold OA (publisher) | 34 | 19% |\r\n| Green OA (repository) | 28 | 16% |\r\n| Bronze OA (free read) | 12 | 7% |\r\n| **Total OA** | **74** | **41%** |\r\n\r\n### Paywalled Papers\r\n| Status | Count | Percentage |\r\n|--------|-------|------------|\r\n| Requires subscription | 105 | 59% |\r\n\r\n### By Journal (Paywalled)\r\n| Journal | Papers | Access Method |\r\n|---------|--------|---------------|\r\n| American Sociological Review | 12 | JSTOR/Institutional |\r\n| American Journal of Sociology | 8 | JSTOR/Institutional |\r\n| Social Forces | 7 | Oxford/Institutional |\r\n```\r\n\r\n### 5. Create Download Checklist\r\n\r\nGenerate an actionable checklist for the user:\r\n\r\n```markdown\r\n## Download Checklist\r\n\r\n### Direct Downloads (OA - can automate)\r\nThese papers have direct PDF links:\r\n\r\n1. [ ] [Paper title 1] (2023)\r\n   - URL: https://...\r\n   - Type: Gold OA\r\n\r\n2. [ ] [Paper title 2] (2021)\r\n   - URL: https://...\r\n   - Type: Green OA (preprint)\r\n\r\n[Continue for all OA papers]\r\n\r\n### Requires Institutional Access\r\nYou'll need to download these through your library:\r\n\r\n**JSTOR** (15 papers):\r\n1. [ ] [Paper title] (2019) - DOI: 10.xxxx\r\n2. [ ] [Paper title] (2018) - DOI: 10.xxxx\r\n\r\n**Oxford Academic** (8 papers):\r\n1. [ ] [Paper title] (2020) - DOI: 10.xxxx\r\n\r\n**Sage Journals** (12 papers):\r\n1. [ ] [Paper title] (2022) - DOI: 10.xxxx\r\n\r\n**Wiley** (5 papers):\r\n...\r\n\r\n### Interlibrary Loan Needed\r\nThese aren't available through typical subscriptions:\r\n\r\n1. [ ] [Book chapter] (2017) - ISBN: xxx\r\n2. [ ] [Conference paper] (2019)\r\n```\r\n\r\n### 6. Create Automated Download Script (Optional)\r\n\r\nFor OA papers, offer a download script:\r\n\r\n```python\r\nimport os\r\nimport requests\r\nfrom time import sleep\r\n\r\ndef download_oa_papers(oa_papers, output_dir=\"fulltext\"):\r\n    \"\"\"Download available OA papers.\"\"\"\r\n    os.makedirs(output_dir, exist_ok=True)\r\n\r\n    for paper in oa_papers:\r\n        if not paper.get(\"oa_url\"):\r\n            continue\r\n\r\n        # Create safe filename\r\n        safe_title = \"\".join(c for c in paper[\"title\"][:50] if c.isalnum() or c in \" -_\")\r\n        filename = f\"{paper['year']}_{safe_title}.pdf\"\r\n        filepath = os.path.join(output_dir, filename)\r\n\r\n        try:\r\n            response = requests.get(paper[\"oa_url\"], timeout=30)\r\n            if response.status_code == 200 and \"pdf\" in response.headers.get(\"content-type\", \"\").lower():\r\n                with open(filepath, \"wb\") as f:\r\n                    f.write(response.content)\r\n                print(f\"Downloaded: {filename}\")\r\n            sleep(1)  # Rate limiting\r\n        except Exception as e:\r\n            print(f\"Failed: {paper['title']} - {e}\")\r\n```\r\n\r\n### 7. Track Full Text Status\r\n\r\nCreate a tracking file:\r\n\r\n```python\r\nfulltext_status = []\r\nfor paper in papers:\r\n    fulltext_status.append({\r\n        \"openalex_id\": paper[\"openalex_id\"],\r\n        \"title\": paper[\"title\"],\r\n        \"doi\": paper.get(\"doi\"),\r\n        \"oa_available\": bool(paper.get(\"open_access\", {}).get(\"oa_url\")),\r\n        \"fulltext_obtained\": False,  # User updates this\r\n        \"fulltext_path\": None,       # User updates this\r\n        \"notes\": \"\"\r\n    })\r\n\r\n# Save for user tracking\r\nwith open(\"data/fulltext_status.json\", \"w\") as f:\r\n    json.dump(fulltext_status, f, indent=2)\r\n```\r\n\r\n## Output Files\r\n\r\n- `output/fulltext_checklist.md` - Human-readable download list\r\n- `data/fulltext_status.json` - Machine-readable tracking\r\n- `fulltext/` - Directory for user to store PDFs\r\n\r\n## Guiding Principles\r\n\r\n- **Maximize coverage**: Full text enables better annotation\r\n- **Legal sources only**: Use OA, institutional access, or ILL\r\n- **Respect rate limits**: Don't hammer APIs or publishers\r\n- **Track status**: Know what you have and what's missing\r\n\r\n## When You're Done\r\n\r\nTell the orchestrator:\r\n> \"Phase 4 complete. X/Y papers (Z%) have OA versions available. Download checklist created at output/fulltext_checklist.md. User should obtain remaining papers before annotation phase.\"\r\n\r\n**Do not proceed to Phase 5 until the user has obtained full text for key papers.**\r\n\r\nThe user may choose to proceed with partial full text coverage, annotating from abstracts where necessary. Confirm this decision before continuing.\r\n",
        "plugins/lit-search/skills/lit-search/phases/phase5-annotation.md": "# Phase 5: Annotation\r\n\r\nYou are extracting structured information from each paper. Your goal is to create a queryable database of findings, methods, and contributions.\r\n\r\n## Why This Phase Matters\r\n\r\nA literature database is only as useful as its annotations. This phase transforms raw papers into structured knowledge that can be searched, filtered, and synthesized.\r\n\r\n## Prerequisites\r\n\r\n- Load `data/screened/included_with_snowball.json`\r\n- Check `data/fulltext_status.json` for available full texts\r\n- Full text PDFs should be in `fulltext/` directory\r\n\r\n## Your Tasks\r\n\r\n### 1. Define Annotation Schema\r\n\r\nCreate a consistent extraction template:\r\n\r\n```python\r\nannotation_schema = {\r\n    # Bibliographic\r\n    \"openalex_id\": \"\",\r\n    \"title\": \"\",\r\n    \"authors\": [],\r\n    \"year\": 0,\r\n    \"journal\": \"\",\r\n    \"doi\": \"\",\r\n\r\n    # Annotation source\r\n    \"annotation_source\": \"\",  # \"full_text\" or \"abstract_only\"\r\n\r\n    # Core content\r\n    \"research_question\": \"\",\r\n    \"theoretical_framework\": \"\",\r\n    \"hypotheses\": [],\r\n\r\n    # Methods\r\n    \"methods\": {\r\n        \"design\": \"\",           # cross-sectional, longitudinal, experimental, qualitative\r\n        \"data_source\": \"\",      # survey name, interview sample, archival\r\n        \"sample\": \"\",           # who, N, sampling method\r\n        \"geographic_scope\": \"\", # country/region\r\n        \"time_period\": \"\",      # when data collected\r\n        \"key_variables\": {\r\n            \"dependent\": [],\r\n            \"independent\": [],\r\n            \"controls\": []\r\n        },\r\n        \"analysis_technique\": \"\"  # regression, SEM, thematic analysis, etc.\r\n    },\r\n\r\n    # Findings\r\n    \"key_findings\": [],         # List of main results\r\n    \"effect_sizes\": [],         # If reported\r\n    \"mechanisms\": \"\",           # Proposed causal mechanisms\r\n    \"boundary_conditions\": \"\",  # When effects hold/don't hold\r\n\r\n    # Contribution\r\n    \"theoretical_contribution\": \"\",\r\n    \"empirical_contribution\": \"\",\r\n    \"limitations_noted\": [],\r\n    \"future_directions\": [],\r\n\r\n    # User annotations\r\n    \"relevance_to_project\": \"\",    # How this relates to user's research\r\n    \"quality_assessment\": \"\",      # User's assessment of rigor\r\n    \"key_quotes\": [],              # Notable passages\r\n    \"tags\": [],                    # User-defined tags\r\n    \"notes\": \"\"                    # Free-form notes\r\n}\r\n```\r\n\r\n### 2. Prioritize Annotation Order\r\n\r\nStart with highest-value papers:\r\n\r\n```python\r\ndef prioritize_papers(papers):\r\n    \"\"\"Order papers for annotation priority.\"\"\"\r\n    scored = []\r\n    for paper in papers:\r\n        score = 0\r\n        score += paper.get(\"cited_by_count\", 0) / 100  # Citation weight\r\n        score += 10 if paper.get(\"fulltext_obtained\") else 0  # Full text bonus\r\n        score += 5 if paper[\"publication_year\"] >= 2020 else 0  # Recency\r\n        scored.append((score, paper))\r\n\r\n    return [p for _, p in sorted(scored, reverse=True)]\r\n```\r\n\r\n### 3. Extract from Full Text\r\n\r\nFor papers with full text:\r\n\r\n```python\r\ndef annotate_from_fulltext(paper, pdf_path):\r\n    \"\"\"Extract structured information from PDF.\"\"\"\r\n    # Read PDF content (using appropriate library)\r\n    text = extract_text_from_pdf(pdf_path)\r\n\r\n    # Section-based extraction\r\n    sections = identify_sections(text)\r\n\r\n    annotation = {\r\n        \"annotation_source\": \"full_text\",\r\n        \"research_question\": extract_research_question(sections.get(\"introduction\")),\r\n        \"theoretical_framework\": extract_theory(sections.get(\"theory\") or sections.get(\"introduction\")),\r\n        \"methods\": extract_methods(sections.get(\"methods\") or sections.get(\"data\")),\r\n        \"key_findings\": extract_findings(sections.get(\"results\") or sections.get(\"findings\")),\r\n        \"limitations_noted\": extract_limitations(sections.get(\"discussion\") or sections.get(\"conclusion\")),\r\n        # ... continue for other fields\r\n    }\r\n\r\n    return annotation\r\n```\r\n\r\n### 4. Extract from Abstract\r\n\r\nFor papers without full text:\r\n\r\n```python\r\ndef annotate_from_abstract(paper):\r\n    \"\"\"Extract what we can from abstract only.\"\"\"\r\n    abstract = paper.get(\"abstract\", \"\")\r\n\r\n    annotation = {\r\n        \"annotation_source\": \"abstract_only\",\r\n        \"research_question\": infer_research_question(abstract),\r\n        \"methods\": {\r\n            \"design\": infer_design(abstract),\r\n            \"sample\": infer_sample(abstract),\r\n            # Many fields will be empty or uncertain\r\n        },\r\n        \"key_findings\": extract_findings_from_abstract(abstract),\r\n        \"limitations_noted\": [\"Full text not available for detailed assessment\"]\r\n    }\r\n\r\n    return annotation\r\n```\r\n\r\n### 5. Present Annotations for User Review\r\n\r\nShow each annotation for validation:\r\n\r\n```markdown\r\n## Annotation Review: Paper 1 of 179\n\n**Title**: Neighborhood Disadvantage and Educational Attainment\n**Authors**: [Author A] & [Author B]\n**Year**: 2019\n**Annotation source**: Full text\n\n### Research Question\n> \"How does neighborhood disadvantage shape educational attainment over the life course?\"\n\r\n### Theoretical Framework\r\n> Life course perspective; extends cumulative disadvantage framework\n\r\n### Methods\r\n| Aspect | Extracted |\r\n|--------|-----------|\r\n| Design | Longitudinal panel (3 waves) |\r\n| Data | Longitudinal household survey |\n| Sample | N=1,200 adolescents, tracked 2000-2015 |\n| Geography | United States (multi-site) |\n| DV | Educational attainment |\n| IVs | Neighborhood disadvantage, school quality, family resources |\n\r\n### Key Findings\r\n1. Neighborhood disadvantage predicts lower attainment (=-.28)\n2. School quality mediates part of the association\n3. Family resources buffer disadvantage effects\n\r\n### Relevance to Your Project\r\nThis paper provides key operationalizations of neighborhood disadvantage and tests mechanisms you're engaging with.\n\r\n---\r\n\r\n**Please review and correct any errors, then add your notes:**\r\n\r\n- [ ] Annotations accurate\r\n- Your relevance notes:\r\n- Your quality assessment (high/medium/low):\r\n- Tags:\r\n- Additional notes:\r\n```\r\n\r\n### 6. Batch Annotation for Efficiency\r\n\r\nFor larger corpora, batch similar papers:\r\n\r\n```python\r\ndef batch_annotate(papers, batch_size=10):\r\n    \"\"\"Process papers in batches with user review points.\"\"\"\r\n    batches = [papers[i:i+batch_size] for i in range(0, len(papers), batch_size)]\r\n\r\n    for i, batch in enumerate(batches):\r\n        # Annotate batch\r\n        annotations = [annotate_paper(p) for p in batch]\r\n\r\n        # Present batch summary for user review\r\n        present_batch_summary(i+1, len(batches), annotations)\r\n\r\n        # Get user feedback\r\n        # Pause for user review\r\n\r\n        # Continue to next batch\r\n```\r\n\r\n### 7. Save Annotated Database\r\n\r\n```python\r\ndef save_annotations(annotations, output_path=\"data/annotated/database.json\"):\r\n    \"\"\"Save complete annotated database.\"\"\"\r\n    database = {\r\n        \"metadata\": {\r\n            \"created\": \"2024-01-15\",\r\n            \"total_papers\": len(annotations),\r\n            \"fulltext_annotated\": sum(1 for a in annotations if a[\"annotation_source\"] == \"full_text\"),\r\n            \"abstract_only\": sum(1 for a in annotations if a[\"annotation_source\"] == \"abstract_only\")\r\n        },\r\n        \"papers\": annotations\r\n    }\r\n\r\n    with open(output_path, \"w\") as f:\r\n        json.dump(database, f, indent=2)\r\n```\r\n\r\n## Annotation Quality Guidelines\r\n\r\n### Research Question\r\n- State as a question or clear statement of inquiry\r\n- Should be specific and falsifiable\r\n- If multiple RQs, list all\r\n\r\n### Theoretical Framework\r\n- Name the theory/theories used\r\n- Note how the paper extends or challenges existing theory\r\n- Include key theoretical concepts\r\n\r\n### Methods Assessment\r\n- Capture enough detail to evaluate and replicate\r\n- Note any methodological innovations\r\n- Flag potential limitations (sample size, selection, etc.)\r\n\r\n### Findings Extraction\r\n- Focus on main findings, not all results\r\n- Include effect sizes and significance when reported\r\n- Note null findings and boundary conditions\r\n\r\n## Output Files\r\n\r\n- `data/annotated/database.json` - Complete structured database\r\n- `memos/annotation_notes.md` - User's notes and observations during annotation\r\n\r\n## Guiding Principles\r\n\r\n- **Full text > abstract**: Note annotation source for each paper\r\n- **User validates**: LLM extraction needs human verification\r\n- **Consistent schema**: Same fields for every paper enables querying\r\n- **Flag uncertainty**: Mark fields where extraction was uncertain\r\n\r\n## When You're Done\r\n\r\nTell the orchestrator:\r\n> \"Phase 5 complete. Annotated X papers (Y from full text, Z from abstract). Database saved to data/annotated/database.json. Ready for synthesis.\"\r\n\r\n**Do not proceed to Phase 6 until the user has reviewed annotations for at least the core papers.**\r\n",
        "plugins/lit-search/skills/lit-search/phases/phase6-synthesis.md": "# Phase 6: Synthesis\r\n\r\nYou are generating the final literature database and identifying patterns across the corpus. Your goal is to produce usable outputs and insights.\r\n\r\n## Why This Phase Matters\r\n\r\nThe value of a literature review is in the synthesis. This phase transforms individual paper annotations into a coherent understanding of the field, identifying themes, debates, and gaps.\r\n\r\n## Prerequisites\r\n\r\n- Load `data/annotated/database.json`\r\n- Review user's notes from annotation phase\r\n\r\n## Your Tasks\r\n\r\n### 1. Generate Annotated Bibliography\r\n\r\nCreate a human-readable bibliography in `output/bibliography.md`:\r\n\r\n```markdown\r\n# Annotated Bibliography: Social Movement Participation\r\n\r\n**Generated**: 2024-01-15\r\n**Total papers**: 179\r\n**Date range**: 2010-2024\r\n\r\n---\r\n\r\n## Core Theoretical Works\r\n\r\n### McAdam, D. & Paulsen, R. (2018). Biographical availability revisited.\r\n*American Sociological Review*, 83(4), 701-728.\r\n\r\n**Research Question**: What factors predict sustained vs. initial movement participation?\r\n\r\n**Key Findings**:\r\n- Social ties strongest predictor of initial participation\r\n- Biographical availability effects fade over time\r\n- Ideological commitment predicts sustained engagement\r\n\r\n**Methods**: Longitudinal panel, N=245, Freedom Summer participants\r\n\r\n**Relevance**: Foundational operationalization of participation; challenges single-timepoint designs.\r\n\r\n**Tags**: #participation #longitudinal #socialnetworks\r\n\r\n---\r\n\r\n### [Continue for each paper, organized thematically]\r\n\r\n## Empirical Studies\r\n\r\n### [Papers grouped by method or topic]\r\n\r\n## Methodological Contributions\r\n\r\n### [Methods papers]\r\n```\r\n\r\n### 2. Export BibTeX\r\n\r\nCreate citation manager compatible export:\r\n\r\n```python\r\ndef generate_bibtex(papers, output_path=\"output/references.bib\"):\r\n    \"\"\"Generate BibTeX file for citation managers.\"\"\"\r\n    bibtex_entries = []\r\n\r\n    for paper in papers:\r\n        # Create cite key: AuthorYear\r\n        first_author = paper[\"authors\"][0].split()[-1] if paper[\"authors\"] else \"Unknown\"\r\n        cite_key = f\"{first_author}{paper['year']}\"\r\n\r\n        entry = f\"\"\"@article{{{cite_key},\r\n  author = {{{\" and \".join(paper[\"authors\"])}}},\r\n  title = {{{paper[\"title\"]}}},\r\n  journal = {{{paper.get(\"journal\", \"\")}}},\r\n  year = {{{paper[\"year\"]}}},\r\n  doi = {{{paper.get(\"doi\", \"\").replace(\"https://doi.org/\", \"\")}}}\r\n}}\r\n\"\"\"\r\n        bibtex_entries.append(entry)\r\n\r\n    with open(output_path, \"w\") as f:\r\n        f.write(\"\\n\".join(bibtex_entries))\r\n```\r\n\r\n### 3. Create Queryable Database Export\r\n\r\nExport in formats useful for analysis:\r\n\r\n```python\r\nimport pandas as pd\r\n\r\ndef export_database(annotations, output_dir=\"output\"):\r\n    \"\"\"Export database in multiple formats.\"\"\"\r\n\r\n    # Flatten for tabular export\r\n    flat_records = []\r\n    for paper in annotations:\r\n        record = {\r\n            \"title\": paper[\"title\"],\r\n            \"authors\": \"; \".join(paper[\"authors\"]),\r\n            \"year\": paper[\"year\"],\r\n            \"journal\": paper[\"journal\"],\r\n            \"doi\": paper[\"doi\"],\r\n            \"research_question\": paper[\"research_question\"],\r\n            \"theory\": paper[\"theoretical_framework\"],\r\n            \"design\": paper[\"methods\"][\"design\"],\r\n            \"sample_size\": paper[\"methods\"][\"sample\"],\r\n            \"geography\": paper[\"methods\"][\"geographic_scope\"],\r\n            \"key_findings\": \" | \".join(paper[\"key_findings\"]),\r\n            \"tags\": \", \".join(paper.get(\"tags\", [])),\r\n            \"relevance\": paper.get(\"relevance_to_project\", \"\"),\r\n            \"quality\": paper.get(\"quality_assessment\", \"\")\r\n        }\r\n        flat_records.append(record)\r\n\r\n    df = pd.DataFrame(flat_records)\r\n\r\n    # CSV for spreadsheet use\r\n    df.to_csv(f\"{output_dir}/database.csv\", index=False)\r\n\r\n    # Excel for easier filtering\r\n    df.to_excel(f\"{output_dir}/database.xlsx\", index=False)\r\n\r\n    # JSON remains primary format\r\n    # Already have this in data/annotated/database.json\r\n```\r\n\r\n### 4. Thematic Analysis\r\n\r\nIdentify major themes across the corpus:\r\n\r\n```markdown\r\n## Thematic Summary\r\n\r\n### Theme 1: The Role of Social Networks\r\n**Papers**: 34/179 (19%)\r\n**Key insight**: Network position consistently predicts participation; debate over mechanism (information, identity, pressure)\r\n\r\n**Representative works**:\r\n- McAdam & Paulsen (2018): Tie strength effects\r\n- Lim (2008): Network diversity matters\r\n- [...]\r\n\r\n**Unresolved questions**:\r\n- Do online networks function like offline?\r\n- Threshold effects in network activation\r\n\r\n---\r\n\r\n### Theme 2: Biographical Availability vs. Biographical Consequences\r\n**Papers**: 28/179 (16%)\r\n**Key insight**: Traditional availability model (free time, no constraints) being challenged by studies showing activism shapes biography\r\n\r\n**Debates**:\r\n- Selection vs. causation\r\n- Short vs. long-term effects\r\n\r\n---\r\n\r\n### Theme 3: [Continue for other major themes]\r\n```\r\n\r\n### 5. Identify Research Gaps\r\n\r\nAnalyze what's missing:\r\n\r\n```markdown\r\n## Research Gaps and Opportunities\r\n\r\n### Methodological Gaps\r\n\r\n1. **Longitudinal designs rare**: Only 12% of studies track participants over time\r\n   - Most participation research is cross-sectional\r\n   - Cannot distinguish selection from socialization\r\n\r\n2. **Non-Western contexts underrepresented**: 67% of studies focus on US/Europe\r\n   - Limited work on participation in Global South movements\r\n   - Theories developed in democratic contexts may not travel\r\n\r\n3. **Online participation understudied**: Despite growth of digital activism\r\n   - Only 8% explicitly examine online participation\r\n   - No consensus on measurement\r\n\r\n### Theoretical Gaps\r\n\r\n1. **Emotion largely absent**: Despite emotion turn in sociology\r\n   - Most studies use rational-choice adjacent frameworks\r\n   - Emotional dynamics of participation unexplored\r\n\r\n2. **Intersectionality undertheorized**:\r\n   - Race, class, gender treated as controls not mechanisms\r\n   - Limited work on how identities shape participation pathways\r\n\r\n### Empirical Gaps\r\n\r\n1. **Demobilization understudied**: Focus on why people join, not why they leave\r\n2. **Movement-to-movement dynamics**: How participation in one affects another\r\n```\r\n\r\n### 6. Map Debates and Positions\r\n\r\nIdentify ongoing disagreements:\r\n\r\n```markdown\r\n## Scholarly Debates\r\n\r\n### Debate 1: Grievances vs. Resources\r\n\r\n**Position A** (Resource Mobilization):\r\nResources and organization matter more than grievances for participation.\r\n- Proponents: McCarthy & Zald tradition\r\n- Evidence: [Papers]\r\n\r\n**Position B** (Grievance-Based):\r\nFelt injustice is necessary condition for participation.\r\n- Proponents: [Scholars]\r\n- Evidence: [Papers]\r\n\r\n**Current state**: Synthesized models dominate; pure positions rare since 2010.\r\n\r\n---\r\n\r\n### Debate 2: Individual vs. Structural Explanations\r\n[...]\r\n```\r\n\r\n### 7. Generate Field Summary\r\n\r\nCreate an executive summary:\r\n\r\n```markdown\r\n## Field Summary: [Your Topic] (2010-2024)\n\r\n### State of the Field\r\nThe study of [your topic] has matured considerably since 2010. The field has moved beyond early debates toward integrated models that acknowledge multiple mechanisms and pathways.\n\r\n### Dominant Approaches\r\n1. **Micro-level explanations**: Individual resources, identities, and motivations\n2. **Meso-level explanations**: Organizational or network dynamics\n3. **Macro-level explanations**: Institutional and contextual conditions\n\r\n### Emerging Directions\r\n1. Digital or hybrid forms of the phenomenon\n2. Cultural and emotional mechanisms\n3. Cross-national and comparative work\n\r\n### Key Methodological Developments\r\n- Longitudinal panel designs\n- Mixed methods combining surveys and interviews\n- Computational or digital trace approaches\n\r\n### Major Unresolved Questions\r\n1. How do online and offline dimensions relate?\n2. What sustains engagement or outcomes over time?\n3. How do macro contexts shape micro-level behavior or outcomes?\n```\r\n\r\n## Output Files\r\n\r\nCreate the following in `output/`:\r\n\r\n| File | Purpose |\r\n|------|---------|\r\n| `bibliography.md` | Annotated bibliography for reading |\r\n| `database.json` | Structured, queryable database |\r\n| `database.csv` | Spreadsheet-friendly export |\r\n| `database.xlsx` | Excel with filtering |\r\n| `references.bib` | BibTeX for citation managers |\r\n| `synthesis.md` | Thematic summary and gaps |\r\n\r\n## Final Package\r\n\r\nPresent the complete package to the user:\r\n\r\n```markdown\r\n## Literature Review Package Complete\r\n\r\nYour literature database on \"Social Movement Participation\" is ready.\r\n\r\n### Corpus Summary\r\n- **Total papers**: 179\r\n- **Date range**: 2010-2024\r\n- **Top journals**: Mobilization, Social Movement Studies, ASR\r\n- **Full text obtained**: 124/179 (69%)\r\n\r\n### Deliverables\r\n1. **Annotated bibliography** (`output/bibliography.md`)\r\n   - All papers with structured annotations\r\n   - Organized thematically\r\n\r\n2. **Searchable database** (`output/database.json`, `.csv`, `.xlsx`)\r\n   - Query by theory, method, findings\r\n   - Filter by year, journal, tags\r\n\r\n3. **Citation file** (`output/references.bib`)\r\n   - Ready for Zotero, Mendeley, or LaTeX\r\n\r\n4. **Synthesis document** (`output/synthesis.md`)\r\n   - Thematic summary\r\n   - Research gaps identified\r\n   - Scholarly debates mapped\r\n\r\n### Next Steps\r\n- Review the synthesis for accuracy\r\n- Consider whether gaps suggest your contribution\r\n- Use database to locate specific papers as you write\r\n```\r\n\r\n## Guiding Principles\r\n\r\n- **Multiple formats**: Different outputs for different uses\r\n- **Synthesis over summary**: Identify patterns, not just list papers\r\n- **Honest about gaps**: What you didn't find is as important as what you found\r\n- **Usable outputs**: Bibliography should be directly usable in writing\r\n\r\n## When You're Done\r\n\r\nTell the orchestrator:\r\n> \"Phase 6 complete. Literature review package generated with annotated bibliography, queryable database (JSON/CSV/Excel), BibTeX, and synthesis document. All outputs in output/ directory. Review complete.\"\r\n\r\nThe user now has a complete, systematic literature database ready for use in their research.\r\n",
        "plugins/lit-synthesis/.claude-plugin/plugin.json": "{\r\n  \"name\": \"lit-synthesis\",\r\n  \"version\": \"1.0.0\",\r\n  \"description\": \"Deep reading and synthesis of literature corpus. Theoretical mapping, thematic clustering, and debate identification using Zotero MCP for full-text access.\",\r\n  \"author\": {\r\n    \"name\": \"Neal Caren\"\r\n  },\r\n  \"license\": \"MIT\",\r\n  \"keywords\": [\"literature-synthesis\", \"Zotero\", \"theoretical-mapping\", \"sociology\", \"research\"],\r\n  \"skills\": \"./skills/\"\r\n}\r\n",
        "plugins/lit-synthesis/skills/lit-synthesis/SKILL.md": "---\r\nname: lit-synthesis\r\ndescription: Deep reading and synthesis of literature corpus. Theoretical mapping, thematic clustering, and debate identification using Zotero MCP for full-text access.\r\n---\r\n\r\n# Literature Synthesis\r\n\r\nYou help sociologists move from a corpus of papers to a deep understanding of a field. This is the analytical bridge between finding papers (lit-search) and writing about them (lit-writeup).\r\n\r\n## The Lit Trilogy\r\n\r\nThis skill is the middle step in a three-skill workflow:\r\n\r\n| Skill | Role | Key Output |\r\n|-------|------|------------|\r\n| **lit-search** | Find papers via OpenAlex | `database.json`, download checklist |\r\n| **lit-synthesis** | Analyze & organize via Zotero | `field-synthesis.md`, `theoretical-map.md`, `debate-map.md` |\r\n| **lit-writeup** | Draft prose | Publication-ready Theory section |\r\n\r\n**Input**: Papers in Zotero (imported from lit-search or user's existing library)\r\n**Output**: Organized understanding of the field ready for writing\r\n\r\n## When to Use This Skill\r\n\r\nUse this skill when users:\r\n- Have a corpus of papers (from lit-search or their own collection)\r\n- Need to understand the theoretical landscape before writing\r\n- Want to identify debates, tensions, and competing positions\r\n- Need to organize papers thematically or by theoretical tradition\r\n- Want deep reading notes, not just metadata extraction\r\n\r\n## Core Principles\r\n\r\n1. **Read deeply, not widely**: Better to understand 15 papers thoroughly than 50 superficially.\r\n\r\n2. **Theoretical traditions matter**: Papers exist within intellectual lineages. Map who cites whom and why.\r\n\r\n3. **Debates are gold**: Competing positions create space for contributions. Find the tensions.\r\n\r\n4. **Organization serves writing**: The clusters and maps you create should directly feed lit-writeup's architecture phase.\r\n\r\n5. **Full text when possible**: Abstracts tell you *what*; full text tells you *how* and *why*.\r\n\r\n## Zotero MCP Integration\r\n\r\nThis skill uses **Zotero MCP** for accessing your library:\r\n\r\n### Setup\r\n\r\nInstall the Zotero MCP server:\r\n```bash\r\nuv tool install \"git+https://github.com/54yyyu/zotero-mcp.git\"\r\nzotero-mcp setup\r\n```\r\n\r\nSee `mcp/zotero-setup.md` for detailed configuration.\r\n\r\n### Key Capabilities\r\n\r\n| Tool | Purpose |\r\n|------|---------|\r\n| `zotero_search_items` | Find papers by keyword, author, tag |\r\n| `zotero_semantic_search` | Conceptual similarity search |\r\n| `zotero_get_item_metadata` | Retrieve full metadata + BibTeX |\r\n| `zotero_get_annotations` | Extract PDF highlights and notes |\r\n| `zotero_search_notes` | Search your reading notes |\r\n\r\n### Workflow Integration\r\n\r\n1. **From lit-search**: Import the BibTeX export into Zotero\r\n2. **Acquire PDFs**: Use Zotero's \"Find Available PDF\" or manual download\r\n3. **Read and annotate**: Highlight key passages, add notes\r\n4. **lit-synthesis reads**: Access annotations via MCP for analysis\r\n\r\n## Workflow Phases\r\n\r\n### Phase 0: Corpus Audit\r\n**Goal**: Assess what's in the corpus and identify gaps.\r\n\r\n**Process**:\r\n- Review the database from lit-search (or user's Zotero collection)\r\n- Count papers by year, journal, author, theoretical tradition\r\n- Identify potential gaps in coverage\r\n- Prioritize which papers need deep reading vs. skimming\r\n\r\n**Output**: `corpus-audit.md` with statistics and reading priorities.\r\n\r\n> **Pause**: User confirms corpus coverage and reading priorities.\r\n\r\n---\r\n\r\n### Phase 1: Deep Reading\r\n**Goal**: Close read priority papers and extract analytical insights.\r\n\r\n**Process**:\r\n- For each priority paper, read full text via Zotero MCP\r\n- Extract: argument structure, theoretical framework, key concepts, methodological approach\r\n- Note: how theory is deployed, what evidence supports claims, limitations acknowledged\r\n- Create structured reading notes\r\n\r\n**Output**: `reading-notes/` directory with per-paper notes.\r\n\r\n> **Pause**: User reviews reading notes for key papers.\r\n\r\n---\r\n\r\n### Phase 2: Theoretical Mapping\r\n**Goal**: Identify intellectual traditions and lineages.\r\n\r\n**Process**:\r\n- Identify which theoretical frameworks appear across papers\r\n- Map citation relationships (who cites whom)\r\n- Note foundational texts and their descendants\r\n- Identify \"camps\" or schools of thought\r\n- Document key concepts and how they're used\r\n\r\n**Output**: `theoretical-map.md` with traditions, key theorists, and concept definitions.\r\n\r\n> **Pause**: User reviews theoretical landscape.\r\n\r\n---\r\n\r\n### Phase 3: Thematic Clustering\r\n**Goal**: Organize papers by what they study and how.\r\n\r\n**Process**:\r\n- Group papers by empirical focus (population, setting, phenomenon)\r\n- Group papers by theoretical approach\r\n- Group papers by methodological strategy\r\n- Identify papers that bridge multiple clusters\r\n- Note within-cluster consensus and variation\r\n\r\n**Output**: `thematic-clusters.md` with organized paper groupings.\r\n\r\n> **Pause**: User reviews clustering logic.\r\n\r\n---\r\n\r\n### Phase 4: Debate Mapping\r\n**Goal**: Identify tensions, disagreements, and competing positions.\r\n\r\n**Process**:\r\n- Find explicit disagreements (papers that critique each other)\r\n- Find implicit tensions (contradictory findings or incompatible assumptions)\r\n- Identify unresolved questions the field is grappling with\r\n- Note where evidence is mixed or contested\r\n- Document the \"state of the debate\" for each tension\r\n\r\n**Output**: `debate-map.md` with positions, evidence, and unresolved questions.\r\n\r\n> **Pause**: User reviews debates and selects focus areas.\r\n\r\n---\r\n\r\n### Phase 5: Field Synthesis\r\n**Goal**: Create comprehensive understanding ready for writing.\r\n\r\n**Process**:\r\n- Synthesize across phases into coherent field understanding\r\n- Identify the most productive gaps for contribution\r\n- Recommend which lit-writeup cluster (Gap-Filler, Theory-Extender, etc.) fits\r\n- Create the handoff document for lit-writeup\r\n\r\n**Output**: `field-synthesis.md` with integrated understanding and writing recommendations.\r\n\r\n---\r\n\r\n## Output Files\r\n\r\n```\r\nlit-synthesis/\r\n corpus-audit.md           # Phase 0: What's in the corpus\r\n reading-notes/            # Phase 1: Per-paper notes\r\n    author2020-title.md\r\n    author2019-title.md\r\n    ...\r\n theoretical-map.md        # Phase 2: Traditions and lineages\r\n thematic-clusters.md      # Phase 3: Paper groupings\r\n debate-map.md             # Phase 4: Tensions and positions\r\n field-synthesis.md        # Phase 5: Integrated understanding\r\n```\r\n\r\n## Reading Note Template\r\n\r\nFor each paper in Phase 1:\r\n\r\n```markdown\r\n# [Author Year] - [Short Title]\r\n\r\n## Bibliographic Info\r\n- Full citation: [from Zotero]\r\n- DOI: [link]\r\n\r\n## Core Argument\r\n[1-2 sentences: What is the paper arguing?]\r\n\r\n## Theoretical Framework\r\n- Tradition: [e.g., Bourdieusian, institutionalist, interactionist]\r\n- Key concepts used: [list]\r\n- How theory is deployed: [description vs. extension vs. critique]\r\n\r\n## Empirical Strategy\r\n- Data: [what kind]\r\n- Methods: [how analyzed]\r\n- Sample: [who/what]\r\n\r\n## Key Findings\r\n1. [Finding 1]\r\n2. [Finding 2]\r\n3. [Finding 3]\r\n\r\n## Contribution Claim\r\n[What does the paper claim to contribute?]\r\n\r\n## Limitations (as noted by authors)\r\n- [Limitation 1]\r\n- [Limitation 2]\r\n\r\n## My Notes\r\n[Your analytical observations, connections to other papers, questions raised]\r\n\r\n## Key Quotes\r\n> \"[Quote 1]\" (p. X)\r\n\r\n> \"[Quote 2]\" (p. Y)\r\n\r\n## Tags\r\n[theoretical-tradition] [empirical-focus] [method] [relevant-to-my-project]\r\n```\r\n\r\n## Model Recommendations\r\n\r\n| Phase | Model | Rationale |\r\n|-------|-------|-----------|\r\n| **Phase 0**: Corpus Audit | **Sonnet** | Data processing, statistics |\r\n| **Phase 1**: Deep Reading | **Opus** | Analytical reading, synthesis |\r\n| **Phase 2**: Theoretical Mapping | **Opus** | Pattern recognition, intellectual history |\r\n| **Phase 3**: Thematic Clustering | **Sonnet** | Organization, categorization |\r\n| **Phase 4**: Debate Mapping | **Opus** | Tension identification, nuance |\r\n| **Phase 5**: Field Synthesis | **Opus** | Integration, strategic judgment |\r\n\r\n## Starting the Synthesis\r\n\r\nWhen the user is ready to begin:\r\n\r\n1. **Check Zotero setup**:\r\n   > \"Do you have Zotero MCP configured? If not, let's set that up first (see `mcp/zotero-setup.md`).\"\r\n\r\n2. **Identify the corpus**:\r\n   > \"Where are your papers? A Zotero collection from lit-search? An existing library folder? How many papers total?\"\r\n\r\n3. **Set priorities**:\r\n   > \"Which papers are most central to your project? We'll deep-read those first and skim the rest.\"\r\n\r\n4. **Clarify goals**:\r\n   > \"What are you trying to understand about this field? Are you looking for gaps, debates, or a specific theoretical tradition?\"\r\n\r\n5. **Proceed with Phase 0** to audit the corpus.\r\n\r\n## Key Reminders\r\n\r\n- **Zotero is the source of truth**: All papers should be in Zotero for consistent access\r\n- **Annotations accelerate**: If you've already highlighted papers, those annotations are accessible via MCP\r\n- **Quality over quantity**: Deep reading 15 papers beats skimming 50\r\n- **Debates are opportunities**: Every tension you find is a potential contribution space\r\n- **This feeds lit-writeup**: The outputs here become inputs therekeep that handoff in mind\r\n",
        "plugins/lit-synthesis/skills/lit-synthesis/mcp/zotero-setup.md": "# Zotero MCP Setup Guide\r\n\r\nThis skill requires the **Zotero MCP** server to access your library, read PDFs, and retrieve annotations.\r\n\r\n## Prerequisites\r\n\r\n1. **Zotero 7+** installed on your machine\r\n2. **uv** package manager installed\r\n3. Papers imported into Zotero (from lit-search BibTeX or manually)\r\n\r\n## Installation\r\n\r\n### Step 1: Install the MCP Server\r\n\r\n```bash\r\nuv tool install \"git+https://github.com/54yyyu/zotero-mcp.git\"\r\n```\r\n\r\n### Step 2: Run Setup\r\n\r\n```bash\r\nzotero-mcp setup\r\n```\r\n\r\nThis auto-configures for Claude Desktop. Follow the prompts.\r\n\r\n### Step 3: Enable Zotero API Access\r\n\r\nIn Zotero:\r\n1. Go to **Edit  Preferences** (Windows/Linux) or **Zotero  Settings** (Mac)\r\n2. Navigate to **Advanced  General**\r\n3. Enable **\"Allow other applications on this computer to communicate with Zotero\"**\r\n\r\n### Step 4: Restart Claude Desktop\r\n\r\nAfter setup, restart Claude Desktop to load the MCP server.\r\n\r\n## Connection Options\r\n\r\n### Local API (Recommended)\r\n\r\nUses Zotero's local API for offline access to your desktop library.\r\n\r\n**Advantages**:\r\n- Works offline\r\n- Faster access\r\n- Full PDF access\r\n\r\n**Requirements**:\r\n- Zotero running on your machine\r\n- \"Allow other applications\" enabled\r\n\r\n### Web API (Alternative)\r\n\r\nUses Zotero's web API for cloud-synced libraries.\r\n\r\n**Advantages**:\r\n- Access from anywhere\r\n- Works without Zotero running\r\n\r\n**Requirements**:\r\n- Zotero API key (get from zotero.org/settings/keys)\r\n- Library ID\r\n- PDFs must be synced to cloud\r\n\r\n## Verifying Setup\r\n\r\nAfter setup, test with:\r\n\r\n```\r\nUse the zotero_search_items tool to search for \"test\"\r\n```\r\n\r\nIf working, you should see results from your library.\r\n\r\n## Available Tools\r\n\r\nOnce configured, you have access to:\r\n\r\n| Tool | What It Does |\r\n|------|--------------|\r\n| `zotero_search_items` | Keyword search across your library |\r\n| `zotero_semantic_search` | AI-powered conceptual similarity search |\r\n| `zotero_advanced_search` | Multi-criteria filtering (author, date, tags, etc.) |\r\n| `zotero_get_item_metadata` | Full metadata for a specific item + BibTeX |\r\n| `zotero_get_annotations` | PDF highlights and comments |\r\n| `zotero_search_notes` | Search your personal notes |\r\n\r\n## Workflow Integration\r\n\r\n### From lit-search\r\n\r\n1. Export BibTeX from lit-search's `references.bib`\r\n2. In Zotero: **File  Import**  select the BibTeX file\r\n3. Papers appear in your library (create a collection for the project)\r\n4. Use **\"Find Available PDF\"** to auto-download open access versions\r\n5. Manually acquire remaining PDFs via institutional access\r\n\r\n### Organizing for Analysis\r\n\r\nCreate a Zotero collection for your project:\r\n1. Right-click **My Library**  **New Collection**\r\n2. Name it (e.g., \"housing-instability-lit-review\")\r\n3. Drag papers into the collection\r\n4. Use tags for additional organization (e.g., \"priority-read\", \"theoretical\", \"empirical\")\r\n\r\n### Annotating PDFs\r\n\r\nAs you read in Zotero's PDF viewer:\r\n- **Highlight** key passages (yellow for findings, blue for theory, green for methods)\r\n- **Add notes** to highlights for your interpretation\r\n- These annotations are accessible via `zotero_get_annotations`\r\n\r\n## Troubleshooting\r\n\r\n### \"Zotero not found\" Error\r\n\r\n- Ensure Zotero is running\r\n- Check that \"Allow other applications\" is enabled\r\n- Restart both Zotero and Claude Desktop\r\n\r\n### \"No results\" for Known Papers\r\n\r\n- Verify the paper is in your Zotero library (not just synced to cloud)\r\n- Check your search terms (try author name or partial title)\r\n- Use `zotero_search_items` with broader terms\r\n\r\n### Annotations Not Appearing\r\n\r\n- Annotations must be made in Zotero's built-in PDF viewer\r\n- External annotations (from Preview, Acrobat) won't sync\r\n- Ensure the PDF is attached to the Zotero item (not just linked)\r\n\r\n### Web API Authentication Errors\r\n\r\n- Verify your API key at zotero.org/settings/keys\r\n- Check that the key has read access to your library\r\n- Confirm your Library ID is correct\r\n\r\n## Resources\r\n\r\n- **Zotero MCP Repository**: https://github.com/54yyyu/zotero-mcp\r\n- **Zotero Documentation**: https://www.zotero.org/support/\r\n- **API Key Management**: https://www.zotero.org/settings/keys\r\n",
        "plugins/lit-synthesis/skills/lit-synthesis/phases/phase0-corpus-audit.md": "# Phase 0: Corpus Audit\r\n\r\n## Why This Phase Matters\r\n\r\nBefore deep reading, you need to know what you have. A corpus audit reveals the shape of your literature: temporal coverage, theoretical breadth, methodological diversity, and gaps. This shapes your reading priorities and prevents wasted effort on peripheral papers.\r\n\r\n---\r\n\r\n## Your Tasks\r\n\r\n### 1. Assess Corpus Source\r\n\r\nDetermine where the papers come from:\r\n\r\n**From lit-search**:\r\n- Load the `database.json` from lit-search output\r\n- Papers should already be screened and annotated\r\n- Check the `synthesis.md` for initial thematic groupings\r\n\r\n**From existing Zotero library**:\r\n- Identify the relevant collection(s)\r\n- May need initial screening if not pre-filtered\r\n\r\n**From user's ad-hoc collection**:\r\n- Gather all papers into one location\r\n- May need more extensive organization\r\n\r\n### 2. Generate Corpus Statistics\r\n\r\nUsing Zotero MCP, compile:\r\n\r\n```markdown\r\n## Corpus Overview\r\n\r\n- **Total papers**: [N]\r\n- **Date range**: [earliest] - [latest]\r\n- **Peak years**: [years with most papers]\r\n\r\n## By Journal\r\n| Journal | Count |\r\n|---------|-------|\r\n| [Journal 1] | [N] |\r\n| [Journal 2] | [N] |\r\n...\r\n\r\n## By Author (top 10)\r\n| Author | Papers |\r\n|--------|--------|\r\n| [Author 1] | [N] |\r\n...\r\n\r\n## By Decade\r\n| Period | Count |\r\n|--------|-------|\r\n| 2020s | [N] |\r\n| 2010s | [N] |\r\n| 2000s | [N] |\r\n| Pre-2000 | [N] |\r\n```\r\n\r\n### 3. Preliminary Theoretical Scan\r\n\r\nSkim abstracts and keywords to identify:\r\n- Which theoretical traditions appear? (e.g., Bourdieusian, institutionalist, interactionist)\r\n- Any named frameworks? (e.g., \"legal consciousness,\" \"gendered organizations\")\r\n- Dominant methodological approaches?\r\n\r\n### 4. Identify Coverage Gaps\r\n\r\nBased on statistics and skim:\r\n- **Temporal gaps**: Missing recent work? Missing foundational papers?\r\n- **Theoretical gaps**: One tradition over-represented?\r\n- **Empirical gaps**: Missing key populations or settings?\r\n- **Geographic gaps**: All US-based? Missing comparative work?\r\n\r\n### 5. Set Reading Priorities\r\n\r\nCategorize papers into:\r\n\r\n| Priority | Criteria | Target |\r\n|----------|----------|--------|\r\n| **Must Read** | Central to your topic; highly cited; foundational | Deep read (Phase 1) |\r\n| **Should Read** | Relevant but not central; supports arguments | Careful skim |\r\n| **May Skim** | Peripheral; useful for breadth | Quick skim or skip |\r\n\r\nUse these criteria for \"Must Read\":\r\n- Highly cited within the corpus\r\n- Directly addresses your research question\r\n- Foundational text others cite\r\n- Introduces key concepts you'll use\r\n- Recent and relevant (last 3-5 years)\r\n\r\n### 6. Write Corpus Audit Memo\r\n\r\nCreate `corpus-audit.md`:\r\n\r\n```markdown\r\n# Corpus Audit\r\n\r\n## Corpus Summary\r\n- Source: [lit-search / Zotero collection / ad-hoc]\r\n- Total papers: [N]\r\n- Date range: [years]\r\n\r\n## Statistics\r\n[Tables from Task 2]\r\n\r\n## Preliminary Theoretical Landscape\r\n- Dominant traditions: [list]\r\n- Named frameworks: [list]\r\n- Methodological approaches: [list]\r\n\r\n## Coverage Assessment\r\n\r\n### Strengths\r\n- [What's well covered]\r\n\r\n### Gaps Identified\r\n- [Gap 1]: [description and significance]\r\n- [Gap 2]: [description and significance]\r\n\r\n### Recommendations\r\n- [Any papers to add before proceeding]\r\n- [Any papers to exclude]\r\n\r\n## Reading Priorities\r\n\r\n### Must Read (Deep Reading - Phase 1)\r\n1. [Author Year] - [Reason]\r\n2. [Author Year] - [Reason]\r\n3. [Author Year] - [Reason]\r\n... [aim for 10-20 papers]\r\n\r\n### Should Read (Careful Skim)\r\n- [List]\r\n\r\n### May Skim\r\n- [List or \"remainder\"]\r\n\r\n## Questions for User\r\n- [Any clarifying questions]\r\n```\r\n\r\n---\r\n\r\n## Guiding Principles\r\n\r\n### 15-20 Deep Reads Is Plenty\r\nYou can't deeply read 80 papers. Prioritize ruthlessly. The \"Must Read\" list should be 10-20 papers maximum.\r\n\r\n### Foundational > Recent\r\nDon't skip classics just because they're old. Foundational texts shape how everyone else thinks about the topic.\r\n\r\n### Gaps Are Data\r\nFinding that your corpus lacks X is valuable informationit may reveal your contribution space.\r\n\r\n### User Knows Their Field\r\nYou can suggest priorities, but the user knows which papers matter most for their specific project.\r\n\r\n---\r\n\r\n## Output Files to Create\r\n\r\n1. **corpus-audit.md** - Full audit with statistics and priorities\r\n\r\n---\r\n\r\n## When You're Done\r\n\r\nReport to the orchestrator:\r\n- Corpus size and date range\r\n- Number of papers prioritized for deep reading\r\n- Key gaps identified\r\n- Any recommendations before proceeding\r\n\r\nExample summary:\r\n> \"Corpus audit complete. **47 papers** spanning 2008-2024, concentrated in *Social Problems* and *American Sociological Review*. Theoretical landscape dominated by legal consciousness and neighborhood effects traditions. **15 papers** prioritized for deep reading; notable gap in comparative/non-US work. Recommend adding 2-3 foundational neighborhood effects papers before Phase 1. Ready for user review.\"\r\n",
        "plugins/lit-synthesis/skills/lit-synthesis/phases/phase1-deep-reading.md": "# Phase 1: Deep Reading\r\n\r\n## Why This Phase Matters\r\n\r\nDeep reading is where understanding happens. Abstracts tell you *what* papers argue; full text reveals *how* they argue ithow theory is deployed, what evidence is marshaled, what assumptions underlie claims. This phase creates the raw material for theoretical mapping and debate identification.\r\n\r\n---\r\n\r\n## Your Tasks\r\n\r\n### 1. Prepare Reading Environment\r\n\r\nFor each priority paper:\r\n- Confirm PDF is in Zotero\r\n- Check for existing annotations (from user's prior reading)\r\n- Note if paper has supplementary materials\r\n\r\nUse Zotero MCP to access:\r\n```\r\nUse zotero_get_item_metadata for [paper identifier]\r\nUse zotero_get_annotations for [paper identifier]\r\n```\r\n\r\n### 2. Read with Purpose\r\n\r\nFor each paper, read for:\r\n\r\n**Argument Structure**\r\n- What is the central claim?\r\n- How is it structured (inductive, deductive, abductive)?\r\n- What's the contribution claim?\r\n\r\n**Theoretical Framework**\r\n- Which tradition(s) does it draw on?\r\n- Who are the key theorists cited?\r\n- Is theory used descriptively, extended, or critiqued?\r\n\r\n**Conceptual Work**\r\n- What key concepts are used?\r\n- Are any new concepts introduced?\r\n- How are concepts operationalized?\r\n\r\n**Empirical Strategy**\r\n- What data? What methods?\r\n- How is theory connected to evidence?\r\n- What are the acknowledged limitations?\r\n\r\n**Position in Field**\r\n- Who does this paper cite approvingly?\r\n- Who does it critique or position against?\r\n- Where does it claim to contribute?\r\n\r\n### 3. Create Reading Notes\r\n\r\nFor each paper, create a note file using the template in SKILL.md:\r\n\r\n```\r\nreading-notes/\r\n author2020-short-title.md\r\n author2019-short-title.md\r\n ...\r\n```\r\n\r\n**Essential sections**:\r\n- Core argument (1-2 sentences)\r\n- Theoretical framework (tradition, key concepts)\r\n- How theory is deployed (description vs. extension vs. critique)\r\n- Key findings (bulleted)\r\n- Contribution claim\r\n- Key quotes (with page numbers)\r\n- Your analytical notes\r\n\r\n### 4. Flag Connections\r\n\r\nAs you read, note:\r\n- Papers that cite each other\r\n- Papers that contradict each other\r\n- Papers using the same concepts differently\r\n- Papers that seem to be in conversation\r\n\r\nAdd tags to notes:\r\n- `[cites: Author2019]`\r\n- `[contradicts: Author2018]`\r\n- `[same-concept-as: Author2020]`\r\n\r\n### 5. Extract Key Quotes\r\n\r\nFor each paper, capture 3-5 quotes that:\r\n- Define key concepts\r\n- State the argument clearly\r\n- Could be used in your own writing\r\n- Reveal assumptions or limitations\r\n\r\nAlways include page numbers.\r\n\r\n### 6. Compile Reading Summary\r\n\r\nAfter all priority papers are read, create a summary:\r\n\r\n```markdown\r\n# Reading Notes Summary\r\n\r\n## Papers Read\r\n- [N] papers deep-read\r\n- [Date range]\r\n\r\n## Emergent Patterns\r\n\r\n### Theoretical Traditions Identified\r\n1. **[Tradition 1]**: [which papers, key ideas]\r\n2. **[Tradition 2]**: [which papers, key ideas]\r\n\r\n### Recurring Concepts\r\n- **[Concept A]**: Used by [papers], defined as [summary]\r\n- **[Concept B]**: Used by [papers], defined as [summary]\r\n\r\n### Methodological Patterns\r\n- [Dominant approach]\r\n- [Variations]\r\n\r\n### Preliminary Tensions Noted\r\n- [Tension 1]: [Paper A] vs [Paper B] on [issue]\r\n- [Tension 2]: [description]\r\n\r\n## Papers That Stand Out\r\n- **Most cited within corpus**: [papers]\r\n- **Most theoretically ambitious**: [papers]\r\n- **Most methodologically innovative**: [papers]\r\n- **Most relevant to user's project**: [papers]\r\n\r\n## Gaps Emerging\r\n- [What's missing from the conversation]\r\n\r\n## Questions for Next Phases\r\n- [What to investigate in theoretical mapping]\r\n- [What to investigate in debate mapping]\r\n```\r\n\r\n---\r\n\r\n## Using Zotero MCP Effectively\r\n\r\n### Retrieving Full Text\r\n```\r\nUse zotero_get_item_metadata with the item key to get full text if available\r\n```\r\n\r\n### Getting Annotations\r\nIf user has already highlighted:\r\n```\r\nUse zotero_get_annotations for [item key]\r\n```\r\n\r\nThis returns highlights and notes, accelerating your reading.\r\n\r\n### Semantic Search for Connections\r\n```\r\nUse zotero_semantic_search with a concept to find related papers\r\n```\r\n\r\n---\r\n\r\n## Guiding Principles\r\n\r\n### Read the Whole Paper\r\nDon't just skim. For priority papers, read introduction, theory, methods, findings, AND conclusion. The conclusion often reveals assumptions.\r\n\r\n### Theory Sections Are Gold\r\nPay special attention to how authors construct their Theory sectionsthis feeds directly into lit-writeup.\r\n\r\n### Note How, Not Just What\r\nAnyone can summarize findings. Note *how* the argument is constructed, *how* theory is deployed, *how* evidence is marshaled.\r\n\r\n### Quotes Need Context\r\nA quote without page number is useless. A quote without noting *why* it matters is nearly as bad.\r\n\r\n### Your Notes Are for Future You\r\nWrite notes that will make sense when you return to them in Phase 3-4. Be explicit about connections and questions.\r\n\r\n---\r\n\r\n## Output Files to Create\r\n\r\n1. **reading-notes/*.md** - One file per paper\r\n2. **reading-summary.md** - Compiled patterns and observations\r\n\r\n---\r\n\r\n## When You're Done\r\n\r\nReport to the orchestrator:\r\n- Number of papers deep-read\r\n- Theoretical traditions identified\r\n- Preliminary tensions noted\r\n- Key papers for theoretical mapping\r\n\r\nExample summary:\r\n> \"Deep reading complete. **15 papers** read in full. Three theoretical traditions identified: legal consciousness (6 papers), neighborhood effects (5 papers), recognition theory (4 papers). Preliminary tension noted between structural vs. cultural explanations for police-calling behavior. Kirk & Papachristos (2011) and Desmond (2016) emerge as most-cited anchors. Ready for Phase 2: Theoretical Mapping.\"\r\n",
        "plugins/lit-synthesis/skills/lit-synthesis/phases/phase2-theoretical-mapping.md": "# Phase 2: Theoretical Mapping\r\n\r\n## Why This Phase Matters\r\n\r\nPapers don't exist in isolationthey belong to intellectual traditions, cite foundational texts, and position themselves within ongoing conversations. Theoretical mapping reveals this structure: who the key thinkers are, how concepts travel across papers, and which traditions are in dialogue or tension.\r\n\r\nThis map directly feeds lit-writeup's architecture phase by clarifying which literatures need engagement.\r\n\r\n---\r\n\r\n## Your Tasks\r\n\r\n### 1. Identify Theoretical Traditions\r\n\r\nFrom your reading notes, cluster papers by theoretical home:\r\n\r\n**Questions to ask**:\r\n- Which foundational theorists do multiple papers cite? (Bourdieu, Goffman, Foucault, etc.)\r\n- Are there named frameworks that recur? (legal consciousness, gendered organizations, etc.)\r\n- Do papers self-identify with traditions? (\"Drawing on symbolic interactionism...\")\r\n\r\n**Common traditions in sociology**:\r\n- Bourdieusian (capital, field, habitus)\r\n- Institutionalist (isomorphism, legitimacy)\r\n- Interactionist (meaning-making, identity work)\r\n- Critical/Conflict (power, inequality, domination)\r\n- Cultural (schemas, repertoires, narratives)\r\n- Organizational (structure, culture, agency)\r\n\r\n### 2. Map Citation Networks\r\n\r\nFor each tradition identified:\r\n\r\n**Foundational texts**:\r\n- Which older works does everyone cite?\r\n- These are the \"must-cite\" sources for your tradition\r\n\r\n**Key intermediaries**:\r\n- Which papers translated the foundational work to your empirical domain?\r\n- Often these are the \"applied the framework to [X]\" papers\r\n\r\n**Recent developments**:\r\n- What's the cutting edge?\r\n- Who's extending or revising the tradition?\r\n\r\nCreate a simple lineage:\r\n```\r\n[Foundational]  [Key Application]  [Recent Extension]\r\n                                           \r\nBourdieu 1984  Lareau 2003  Calarco 2018\r\n```\r\n\r\n### 3. Document Key Concepts\r\n\r\nFor each major concept:\r\n\r\n```markdown\r\n## [Concept Name]\r\n\r\n**Origin**: [Who introduced it? When?]\r\n\r\n**Definition**: [How is it defined in the foundational text?]\r\n\r\n**In This Literature**:\r\n- [Author 2020] uses it to mean [X]\r\n- [Author 2019] uses it to mean [Y]\r\n- Note: [Any variation in usage]\r\n\r\n**Related Concepts**: [What concepts travel with it?]\r\n\r\n**Key Quote**: \"[Definition quote]\" (Author Year:page)\r\n```\r\n\r\n### 4. Identify Cross-Traditional Connections\r\n\r\nSome papers bridge traditions:\r\n- Paper A uses Bourdieu AND institutionalism\r\n- Paper B connects legal consciousness to recognition theory\r\n\r\nNote these bridgesthey may reveal synthesis opportunities.\r\n\r\n### 5. Map Intellectual Lineages\r\n\r\nFor each major paper in your corpus:\r\n- Which tradition does it claim?\r\n- Who are its key citations?\r\n- What position does it take within the tradition?\r\n\r\nVisual representation (text-based):\r\n```\r\nLEGAL CONSCIOUSNESS TRADITION\r\n Ewick & Silbey 1998 (foundational)\r\n    Hull 2003 (applied to X)\r\n    Nielsen 2000 (applied to Y)\r\n    Merry 1990 (precursor)\r\n Kirk & Papachristos 2011 (legal cynicism variant)\r\n    Desmond et al. 2016 (extended to housing)\r\n [Your corpus papers here]\r\n```\r\n\r\n### 6. Write Theoretical Map\r\n\r\nCreate `theoretical-map.md`:\r\n\r\n```markdown\r\n# Theoretical Map\r\n\r\n## Overview\r\n\r\nThis corpus engages [N] distinct theoretical traditions:\r\n1. [Tradition 1] ([N] papers)\r\n2. [Tradition 2] ([N] papers)\r\n3. [Tradition 3] ([N] papers)\r\n\r\n---\r\n\r\n## Tradition 1: [Name]\r\n\r\n### Foundational Texts\r\n- [Text 1]: [one-line description]\r\n- [Text 2]: [one-line description]\r\n\r\n### Key Concepts\r\n- **[Concept A]**: [definition]\r\n- **[Concept B]**: [definition]\r\n\r\n### Papers in Corpus Using This Tradition\r\n| Paper | How Tradition Is Used |\r\n|-------|----------------------|\r\n| [Author 2020] | [description/extension/critique] |\r\n| [Author 2019] | [description/extension/critique] |\r\n\r\n### Intellectual Lineage\r\n[Text-based tree showing citations]\r\n\r\n### Key Quote\r\n> \"[Defining quote for the tradition]\" (Foundational Text:page)\r\n\r\n---\r\n\r\n## Tradition 2: [Name]\r\n[Repeat structure]\r\n\r\n---\r\n\r\n## Cross-Traditional Connections\r\n\r\n| Connection | Papers | Significance |\r\n|------------|--------|--------------|\r\n| [Tradition A + B] | [Papers] | [Why this matters] |\r\n\r\n---\r\n\r\n## Concepts Glossary\r\n\r\n| Concept | Tradition | Definition | Key Source |\r\n|---------|-----------|------------|------------|\r\n| [Concept 1] | [Tradition] | [Definition] | [Author Year] |\r\n\r\n---\r\n\r\n## Implications for Writing\r\n\r\nBased on this map:\r\n- **Most developed tradition**: [which one]\r\n- **Underdeveloped tradition**: [which one]\r\n- **Productive synthesis opportunity**: [if any]\r\n- **Recommended lit-writeup cluster**: [Gap-Filler/Theory-Extender/etc.]\r\n```\r\n\r\n---\r\n\r\n## Guiding Principles\r\n\r\n### Traditions Are Not Boxes\r\nPapers often draw on multiple traditions. The map should show connections, not just categories.\r\n\r\n### Foundational Texts Matter\r\nEven if everyone cites them, you need to know what they actually saynot just how they're invoked.\r\n\r\n### Concepts Drift\r\nThe same term may mean different things to different authors. Document variation, don't assume consistency.\r\n\r\n### Citation  Agreement\r\nA paper citing Bourdieu may be critiquing Bourdieu. Note the stance, not just the citation.\r\n\r\n### This Feeds Architecture\r\nThe traditions you identify become the subsections and literatures in lit-writeup. Build the map with writing in mind.\r\n\r\n---\r\n\r\n## Output Files to Create\r\n\r\n1. **theoretical-map.md** - Full map with traditions, concepts, lineages\r\n\r\n---\r\n\r\n## When You're Done\r\n\r\nReport to the orchestrator:\r\n- Number of traditions identified\r\n- Key foundational texts\r\n- Any cross-traditional connections\r\n- Concepts that need careful definition\r\n- Preliminary cluster recommendation for lit-writeup\r\n\r\nExample summary:\r\n> \"Theoretical mapping complete. **Three traditions** identified: legal consciousness (anchored by Ewick & Silbey 1998), neighborhood effects (anchored by Sampson et al. 1997), and recognition theory (anchored by Honneth 2003). Cross-traditional connection noted: 3 papers bridge legal consciousness and recognition. Key concept variation: 'cynicism' means different things across papers. Recommend **Synthesis Integrator** cluster for lit-writeup given opportunity to bridge traditions. Ready for Phase 3.\"\r\n",
        "plugins/lit-synthesis/skills/lit-synthesis/phases/phase3-thematic-clustering.md": "# Phase 3: Thematic Clustering\r\n\r\n## Why This Phase Matters\r\n\r\nWhile theoretical mapping shows intellectual traditions, thematic clustering organizes papers by what they *study* and *how*. This reveals the empirical landscape: which populations have been studied, which settings, which methods dominate, and where there are gaps.\r\n\r\nThis clustering directly informs lit-writeup's synthesis paragraphs and helps identify empirical gaps for contribution.\r\n\r\n---\r\n\r\n## Your Tasks\r\n\r\n### 1. Cluster by Empirical Focus\r\n\r\nGroup papers by what they study:\r\n\r\n**By Population**:\r\n- Which groups are studied? (immigrants, mothers, youth, workers, etc.)\r\n- Which populations are over/under-represented?\r\n\r\n**By Setting**:\r\n- What contexts? (neighborhoods, organizations, institutions, etc.)\r\n- What geographic scope? (US-only? Comparative?)\r\n\r\n**By Phenomenon**:\r\n- What process or outcome? (decision-making, identity formation, mobility, etc.)\r\n- What level of analysis? (individual, organizational, structural)\r\n\r\nCreate clusters:\r\n```markdown\r\n## Empirical Clusters\r\n\r\n### By Population\r\n- **Working-class mothers**: [Papers A, B, C]\r\n- **Undocumented immigrants**: [Papers D, E]\r\n- **Urban youth**: [Papers F, G, H]\r\n\r\n### By Setting\r\n- **Public housing**: [Papers A, D, F]\r\n- **Courtrooms**: [Papers B, E]\r\n- **Schools**: [Papers C, G, H]\r\n\r\n### By Phenomenon\r\n- **Police encounters**: [Papers A, B]\r\n- **Housing instability**: [Papers C, D]\r\n- **Identity work**: [Papers E, F, G]\r\n```\r\n\r\n### 2. Cluster by Methodological Approach\r\n\r\nGroup papers by how they study:\r\n\r\n**Data Type**:\r\n- In-depth interviews\r\n- Ethnography\r\n- Mixed methods\r\n- Secondary data\r\n\r\n**Analytic Approach**:\r\n- Grounded theory\r\n- Extended case method\r\n- Comparative\r\n- Process tracing\r\n\r\n**Sample Characteristics**:\r\n- Size (small N vs. larger)\r\n- Recruitment (convenience vs. purposive vs. representative)\r\n- Geographic scope\r\n\r\n### 3. Cluster by Theoretical Approach\r\n\r\nComplementing Phase 2, group by how theory is used:\r\n\r\n**Theory as Lens**:\r\n- Papers that apply an existing framework (Theory-Extender style)\r\n\r\n**Theory as Contribution**:\r\n- Papers that build new concepts (Concept-Builder style)\r\n\r\n**Theory as Background**:\r\n- Papers with light theoretical framing (Gap-Filler style)\r\n\r\n### 4. Identify Cross-Cluster Papers\r\n\r\nSome papers appear in multiple clusters:\r\n- Bridges populations (studies both X and Y)\r\n- Bridges settings (compares context A and B)\r\n- Uses multiple methods\r\n\r\nNote thesethey often provide models for integrative work.\r\n\r\n### 5. Identify Cluster Gaps\r\n\r\nFor each clustering dimension, note:\r\n- **Over-represented**: What's been studied extensively?\r\n- **Under-represented**: What's missing or sparse?\r\n- **Strategic gaps**: Where could your project contribute?\r\n\r\n### 6. Write Thematic Clusters Document\r\n\r\nCreate `thematic-clusters.md`:\r\n\r\n```markdown\r\n# Thematic Clusters\r\n\r\n## Overview\r\n\r\nPapers organized by empirical focus, method, and theoretical approach.\r\n\r\n---\r\n\r\n## By Population\r\n\r\n### Cluster: [Population 1]\r\n**Papers**: [N]\r\n- [Author 2020]: [brief description]\r\n- [Author 2019]: [brief description]\r\n\r\n**What We Know**: [Summary of findings about this population]\r\n\r\n**What's Missing**: [Gaps in studying this population]\r\n\r\n### Cluster: [Population 2]\r\n[Repeat structure]\r\n\r\n---\r\n\r\n## By Setting\r\n\r\n### Cluster: [Setting 1]\r\n**Papers**: [N]\r\n- [List with brief descriptions]\r\n\r\n**Dominant Findings**: [What do these papers collectively show?]\r\n\r\n### Cluster: [Setting 2]\r\n[Repeat]\r\n\r\n---\r\n\r\n## By Phenomenon\r\n\r\n### Cluster: [Phenomenon 1]\r\n**Papers**: [N]\r\n- [List]\r\n\r\n**Consensus**: [What most papers agree on]\r\n**Variation**: [Where findings differ]\r\n\r\n---\r\n\r\n## By Method\r\n\r\n| Method | Papers | Notes |\r\n|--------|--------|-------|\r\n| In-depth interviews | [List] | [Typical N, approach] |\r\n| Ethnography | [List] | [Duration, sites] |\r\n| Mixed methods | [List] | [What mix] |\r\n\r\n---\r\n\r\n## Cross-Cluster Papers\r\n\r\n| Paper | Clusters Bridged | Significance |\r\n|-------|-----------------|--------------|\r\n| [Author 2020] | [Pop A + Setting B] | [Why this matters] |\r\n\r\n---\r\n\r\n## Gap Analysis\r\n\r\n### Over-Represented\r\n- [Population/setting/phenomenon with extensive coverage]\r\n\r\n### Under-Represented\r\n- [Population/setting/phenomenon with sparse coverage]\r\n\r\n### Strategic Gaps for This Project\r\n1. **[Gap 1]**: [Description]  Papers touch on this but don't fully address\r\n2. **[Gap 2]**: [Description]  No papers in corpus address this\r\n\r\n---\r\n\r\n## Implications for Writing\r\n\r\nBased on these clusters:\r\n- **Your project's empirical niche**: [Where it fits]\r\n- **Papers to engage closely**: [Which clusters are most relevant]\r\n- **Gap to highlight in turn**: [What's missing that you address]\r\n```\r\n\r\n---\r\n\r\n## Guiding Principles\r\n\r\n### Clusters Should Be Meaningful\r\nDon't create clusters just to categorize. Each cluster should tell you something about the field.\r\n\r\n### Look for Absences\r\nSometimes what's NOT in a cluster is more interesting than what's there.\r\n\r\n### Cross-Cluster Papers Are Valuable\r\nPapers that bridge categories often provide models for your own integrative work.\r\n\r\n### Clusters Become Synthesis Paragraphs\r\nIn lit-writeup, each cluster could become a paragraph or subsection. Think about writing as you cluster.\r\n\r\n### Multiple Clustering Schemes Are OK\r\nThe same paper legitimately belongs in multiple clusters. That's information, not a problem.\r\n\r\n---\r\n\r\n## Output Files to Create\r\n\r\n1. **thematic-clusters.md** - Full clustering analysis\r\n\r\n---\r\n\r\n## When You're Done\r\n\r\nReport to the orchestrator:\r\n- Number of clusters per dimension\r\n- Key over/under-represented areas\r\n- Papers that bridge clusters\r\n- Strategic gaps identified\r\n\r\nExample summary:\r\n> \"Thematic clustering complete. **By population**: 4 clusters (working-class mothers, immigrants, youth, homeowners). **By setting**: 3 clusters (neighborhoods, courtrooms, schools). Notable gap: no papers study *renters specifically* despite housing focus. Cross-cluster paper: Desmond 2016 bridges neighborhoods + housing instability. Recommend highlighting renter gap in lit-writeup turn. Ready for Phase 4.\"\r\n",
        "plugins/lit-synthesis/skills/lit-synthesis/phases/phase4-debate-mapping.md": "# Phase 4: Debate Mapping\r\n\r\n## Why This Phase Matters\r\n\r\nDebates are opportunities. Where scholars disagreeabout mechanisms, explanations, definitions, or evidencethere's space for contribution. This phase identifies the tensions, contradictions, and unresolved questions in your field.\r\n\r\nFor lit-writeup, debates may suggest a Problem-Driven (Debate-Resolver) cluster, or they may reveal productive tensions to address even in other cluster types.\r\n\r\n---\r\n\r\n## Your Tasks\r\n\r\n### 1. Identify Explicit Disagreements\r\n\r\nLook for papers that directly critique each other:\r\n- Paper A cites Paper B critically\r\n- Paper A claims to \"correct\" or \"extend\" Paper B\r\n- Paper A and Paper B reach opposite conclusions\r\n\r\n**Signals of explicit disagreement**:\r\n- \"Contrary to [Author]...\"\r\n- \"[Author]'s approach overlooks...\"\r\n- \"We depart from [Author]...\"\r\n- \"Unlike [Author], we find...\"\r\n\r\n### 2. Identify Implicit Tensions\r\n\r\nLook for contradictions that aren't acknowledged:\r\n- Papers with incompatible findings\r\n- Papers using the same concept differently\r\n- Papers with incompatible underlying assumptions\r\n\r\n**Types of implicit tension**:\r\n- **Empirical**: Paper A finds X, Paper B finds not-X\r\n- **Theoretical**: Paper A assumes mechanism M, Paper B assumes mechanism N\r\n- **Definitional**: Paper A defines concept as X, Paper B as Y\r\n- **Scope**: Paper A claims generality, Paper B claims contingency\r\n\r\n### 3. Document Each Debate\r\n\r\nFor each tension identified:\r\n\r\n```markdown\r\n## Debate: [Short Name]\r\n\r\n### The Tension\r\n[One-sentence summary of what's at stake]\r\n\r\n### Position 1\r\n- **Claim**: [What this side argues]\r\n- **Key papers**: [Authors]\r\n- **Evidence**: [What supports this position]\r\n- **Underlying assumption**: [What must be true for this to hold]\r\n\r\n### Position 2\r\n- **Claim**: [What this side argues]\r\n- **Key papers**: [Authors]\r\n- **Evidence**: [What supports this position]\r\n- **Underlying assumption**: [What must be true for this to hold]\r\n\r\n### State of the Debate\r\n- **Is this resolved?**: [Yes/No/Partially]\r\n- **What would resolve it?**: [Type of evidence or analysis needed]\r\n- **Why it matters**: [Stakes for the field]\r\n\r\n### Relevance to Your Project\r\n[How does your research speak to this debate?]\r\n```\r\n\r\n### 4. Map Unresolved Questions\r\n\r\nBeyond bilateral debates, identify open questions:\r\n- Questions papers raise but don't answer\r\n- Questions in \"future research\" sections\r\n- Questions implied by limitations\r\n\r\n```markdown\r\n## Unresolved Questions\r\n\r\n### Question 1: [Formulation]\r\n- **Raised by**: [Papers]\r\n- **Why unresolved**: [What would it take to answer?]\r\n- **Relevance**: [Does your project address this?]\r\n```\r\n\r\n### 5. Identify Contested Concepts\r\n\r\nSome concepts are themselves sites of debate:\r\n- Different definitions in circulation\r\n- Different operationalizations\r\n- Different theoretical homes\r\n\r\n```markdown\r\n## Contested Concept: [Name]\r\n\r\n### Definition Variants\r\n| Author | Definition | Tradition |\r\n|--------|-----------|-----------|\r\n| [A] | \"[definition]\" | [tradition] |\r\n| [B] | \"[definition]\" | [tradition] |\r\n\r\n### Why This Matters\r\n[What's at stake in how the concept is defined?]\r\n\r\n### Your Position\r\n[How will you define/use this concept?]\r\n```\r\n\r\n### 6. Write Debate Map\r\n\r\nCreate `debate-map.md`:\r\n\r\n```markdown\r\n# Debate Map\r\n\r\n## Overview\r\n\r\nThis field contains [N] identifiable debates and [N] unresolved questions.\r\n\r\n**Major debates**:\r\n1. [Debate 1 name]\r\n2. [Debate 2 name]\r\n3. [Debate 3 name]\r\n\r\n---\r\n\r\n## Debate 1: [Name]\r\n\r\n[Full documentation using template above]\r\n\r\n---\r\n\r\n## Debate 2: [Name]\r\n\r\n[Full documentation]\r\n\r\n---\r\n\r\n## Debate 3: [Name]\r\n\r\n[Full documentation]\r\n\r\n---\r\n\r\n## Unresolved Questions\r\n\r\n1. **[Question 1]**\r\n   - Raised by: [papers]\r\n   - What would answer it: [description]\r\n\r\n2. **[Question 2]**\r\n   - Raised by: [papers]\r\n   - What would answer it: [description]\r\n\r\n---\r\n\r\n## Contested Concepts\r\n\r\n### [Concept 1]\r\n[Documentation using template]\r\n\r\n### [Concept 2]\r\n[Documentation using template]\r\n\r\n---\r\n\r\n## Implications for Your Project\r\n\r\n### Debates You Could Adjudicate\r\n- [Debate X]: Your data could show [how]\r\n\r\n### Questions You Could Address\r\n- [Question Y]: Your project directly addresses this\r\n\r\n### Positions You'll Take\r\n- On [Debate Z]: You align with [Position] because [reason]\r\n\r\n### Lit-Writeup Implications\r\n- **If adjudicating a debate**: Use Problem-Driven (Debate) cluster\r\n- **If taking a side**: Cite both positions, then align\r\n- **If neither**: Note the debate but don't center it\r\n```\r\n\r\n---\r\n\r\n## Guiding Principles\r\n\r\n### Not Every Tension Is a Debate\r\nSome differences are just different. A debate exists when scholars are actually in disagreement about something that matters.\r\n\r\n### Implicit Tensions Are Often More Important\r\nExplicit debates are easy to find. The interesting ones are often unacknowledged contradictions.\r\n\r\n### Debates Create Contribution Space\r\nIf everyone agrees, there's nothing to add. If there's disagreement, your evidence can move the conversation.\r\n\r\n### You Don't Have to Resolve Every Debate\r\nNote debates, but focus on ones your project actually speaks to.\r\n\r\n### Fairness Matters\r\nIf you're documenting a debate, present both sides fairlyeven if you'll eventually take a side.\r\n\r\n---\r\n\r\n## Output Files to Create\r\n\r\n1. **debate-map.md** - Full documentation of debates and tensions\r\n\r\n---\r\n\r\n## When You're Done\r\n\r\nReport to the orchestrator:\r\n- Number of debates identified\r\n- Number of unresolved questions\r\n- Key contested concepts\r\n- Debates relevant to user's project\r\n- Recommendation on whether Problem-Driven cluster is appropriate\r\n\r\nExample summary:\r\n> \"Debate mapping complete. **3 major debates** identified: (1) structural vs. cultural explanations for legal cynicism, (2) whether police-calling indicates trust or desperation, (3) role of neighborhood context vs. individual disposition. **5 unresolved questions** documented. Key contested concept: 'cynicism' has 3 distinct definitions in circulation. User's project directly addresses Debate 2recommend considering **Problem-Driven (Debate)** cluster if adjudicating, or noting the debate in a Gap-Filler turn. Ready for Phase 5.\"\r\n",
        "plugins/lit-synthesis/skills/lit-synthesis/phases/phase5-field-synthesis.md": "# Phase 5: Field Synthesis\r\n\r\n## Why This Phase Matters\r\n\r\nThis is where everything comes together. You've audited the corpus, read deeply, mapped theories, clustered themes, and identified debates. Now you synthesize this into a coherent understanding of the field that's ready for writing.\r\n\r\nThe outputs of this phase are the direct inputs to lit-writeup. This is the handoff.\r\n\r\n---\r\n\r\n## Your Tasks\r\n\r\n### 1. Synthesize Across Phases\r\n\r\nPull together key insights from each phase:\r\n\r\n| Phase | Key Takeaway for Synthesis |\r\n|-------|---------------------------|\r\n| **Corpus Audit** | What's in the literature? What's missing? |\r\n| **Deep Reading** | How do papers construct arguments? What works? |\r\n| **Theoretical Map** | What traditions? What concepts? What lineages? |\r\n| **Thematic Clusters** | What's studied? What's over/under-represented? |\r\n| **Debate Map** | Where's disagreement? What's unresolved? |\r\n\r\n### 2. Articulate the Field's State\r\n\r\nWrite a narrative synthesis:\r\n\r\n```markdown\r\n## The State of the Field\r\n\r\n### What the Field Knows\r\n[Summary of established findings, 2-3 paragraphs]\r\n\r\n### How the Field Thinks\r\n[Summary of dominant theoretical approaches, 1-2 paragraphs]\r\n\r\n### Where the Field Disagrees\r\n[Summary of key debates and tensions, 1-2 paragraphs]\r\n\r\n### What the Field Lacks\r\n[Summary of gapsempirical, theoretical, methodological, 1-2 paragraphs]\r\n```\r\n\r\n### 3. Identify Contribution Opportunities\r\n\r\nBased on your synthesis, what are the clearest opportunities for contribution?\r\n\r\n**Types of contribution**:\r\n- **Empirical gap**: Unstudied population, setting, or phenomenon\r\n- **Theoretical extension**: Framework not yet applied to X\r\n- **Concept building**: Need for new analytical tool\r\n- **Synthesis**: Traditions not yet connected\r\n- **Debate resolution**: Evidence to adjudicate competing claims\r\n\r\nFor each opportunity:\r\n```markdown\r\n### Opportunity: [Name]\r\n\r\n**Type**: [Empirical gap / Extension / Building / Synthesis / Debate]\r\n\r\n**Description**: [What's the opportunity?]\r\n\r\n**Why it's viable**: [Why can this be done now?]\r\n\r\n**Required evidence**: [What would demonstrate this contribution?]\r\n\r\n**Fit with user's project**: [How well does this fit?]\r\n```\r\n\r\n### 4. Recommend Lit-Writeup Cluster\r\n\r\nBased on the synthesis, recommend a cluster for lit-writeup:\r\n\r\n| If... | Then Consider... |\r\n|-------|------------------|\r\n| Main contribution is empirical insight about understudied group | **Gap-Filler** |\r\n| Main contribution is showing established framework works in new domain | **Theory-Extender** |\r\n| Main contribution is new concept or typology | **Concept-Builder** |\r\n| Main contribution is connecting previously separate literatures | **Synthesis Integrator** |\r\n| Main contribution is adjudicating a debate | **Problem-Driven (Debate)** |\r\n| Main contribution is documenting understudied phenomenon | **Problem-Driven (Empirical)** |\r\n\r\nProvide rationale:\r\n```markdown\r\n## Lit-Writeup Cluster Recommendation\r\n\r\n**Recommended**: [Cluster Name]\r\n\r\n**Rationale**:\r\n[Why this cluster fits the project's contribution]\r\n\r\n**Alternative considered**:\r\n[Another cluster that might fit, and why primary is better]\r\n\r\n**Cluster implications**:\r\n- Structure: [Expected subsections]\r\n- Literature balance: [Theory-heavy / Balanced / Substantive]\r\n- Turn strategy: [How to frame the gap]\r\n```\r\n\r\n### 5. Plan the Handoff\r\n\r\nSpecify what lit-writeup will need:\r\n\r\n```markdown\r\n## Handoff to Lit-Writeup\r\n\r\n### Key Literatures to Engage\r\n1. **[Literature 1]**: [Purpose in section]\r\n2. **[Literature 2]**: [Purpose in section]\r\n3. **[Literature 3]**: [Purpose in section]\r\n\r\n### Theoretical Traditions to Reference\r\n- [Tradition 1]: [How it appears in your section]\r\n- [Tradition 2]: [How it appears in your section]\r\n\r\n### Concepts Requiring Definition\r\n- **[Concept A]**: Define using [Source]\r\n- **[Concept B]**: Define using [Source]\r\n\r\n### Debates to Acknowledge (if any)\r\n- [Debate]: Present as [background / central tension / resolved]\r\n\r\n### Suggested Turn Formulation\r\n> \"[Draft turn sentence based on identified gap]\"\r\n\r\n### Key Citations for Each Section\r\n[Map citations to expected subsections]\r\n```\r\n\r\n### 6. Write Field Synthesis Document\r\n\r\nCreate `field-synthesis.md`:\r\n\r\n```markdown\r\n# Field Synthesis\r\n\r\n## Project\r\n- **Research question**: [From user]\r\n- **Main argument**: [From user]\r\n\r\n---\r\n\r\n## The State of the Field\r\n\r\n[Narrative synthesis from Task 2]\r\n\r\n---\r\n\r\n## Contribution Opportunities\r\n\r\n[Documentation from Task 3]\r\n\r\n---\r\n\r\n## Recommended Approach\r\n\r\n### Lit-Writeup Cluster: [Name]\r\n\r\n[Recommendation and rationale from Task 4]\r\n\r\n---\r\n\r\n## Handoff to Lit-Writeup\r\n\r\n[Handoff specification from Task 5]\r\n\r\n---\r\n\r\n## Key Documents for Reference\r\n\r\n| Document | Location | Purpose |\r\n|----------|----------|---------|\r\n| Corpus Audit | `corpus-audit.md` | Coverage and gaps |\r\n| Reading Notes | `reading-notes/` | Deep engagement evidence |\r\n| Theoretical Map | `theoretical-map.md` | Traditions and lineages |\r\n| Thematic Clusters | `thematic-clusters.md` | Empirical landscape |\r\n| Debate Map | `debate-map.md` | Tensions and questions |\r\n\r\n---\r\n\r\n## Ready for Lit-Writeup\r\n\r\nUser can now invoke `/lit-writeup` with this synthesis as input.\r\n\r\nRecommended prompt:\r\n> \"I've completed lit-synthesis. My field-synthesis.md recommends [Cluster] because [reason]. My main argument is [X]. Please begin Phase 0: Assessment.\"\r\n```\r\n\r\n---\r\n\r\n## Guiding Principles\r\n\r\n### Synthesis  Summary\r\nDon't just repeat what's in other documents. Integrate, interpret, and conclude.\r\n\r\n### Be Decisive About Contribution\r\nThe field synthesis should make a clear recommendation about the user's contribution type.\r\n\r\n### The Turn Is the Payoff\r\nEverything in this synthesis should point toward what gap the user fills.\r\n\r\n### Make the Handoff Clean\r\nLit-writeup should be able to start immediately from your outputs. Don't leave ambiguity.\r\n\r\n### The User Has Final Say\r\nYour recommendations are informed suggestions. The user knows their project best.\r\n\r\n---\r\n\r\n## Output Files to Create\r\n\r\n1. **field-synthesis.md** - Comprehensive synthesis and handoff document\r\n\r\n---\r\n\r\n## When You're Done\r\n\r\nReport to the orchestrator:\r\n- Summary of field state (2-3 sentences)\r\n- Primary contribution opportunity identified\r\n- Recommended lit-writeup cluster\r\n- Readiness for handoff\r\n\r\nExample summary:\r\n> \"Field synthesis complete. The literature on police-calling in disadvantaged neighborhoods is theoretically mature (legal consciousness, recognition) but empirically narrow (focused on residents' perspectives, not meaning-making processes). Primary opportunity: **synthesis** of legal consciousness and recognition traditions to explain the cynicism-reliance paradox. Recommended cluster: **Synthesis Integrator**. Handoff document ready with suggested turn: 'Yet while research documents why residents call police despite cynicism, there is less attention to the cultural scripts through which residents make sense of these encounters.' Ready for lit-writeup.\"\r\n",
        "plugins/lit-writeup/.claude-plugin/plugin.json": "{\r\n  \"name\": \"lit-writeup\",\r\n  \"version\": \"1.0.0\",\r\n  \"description\": \"Draft publication-ready Theory sections for sociology research. Guides structure, paragraph functions, sentence craft, and calibration based on analysis of 80 Social Problems/Social Forces articles.\",\r\n  \"author\": {\r\n    \"name\": \"Neal Caren\"\r\n  },\r\n  \"license\": \"MIT\",\r\n  \"keywords\": [\"theory-section\", \"writing\", \"sociology\", \"research\", \"literature-review\"],\r\n  \"skills\": \"./skills/\"\r\n}\r\n",
        "plugins/lit-writeup/skills/lit-writeup/SKILL.md": "---\r\nname: lit-writeup\r\ndescription: Draft publication-ready Theory sections for sociology research. Guides structure, paragraph functions, sentence craft, and calibration based on analysis of 80 Social Problems/Social Forces articles.\r\n---\r\n\r\n# Literature Write-Up\r\n\r\nYou help sociologists write Theory sections (also called \"Literature Review\" or \"Background\" sections) for journal articles. Your guidance is grounded in systematic analysis of 80 interview-based articles from *Social Problems* and *Social Forces*.\r\n\r\n## The Lit Trilogy\r\n\r\nThis skill is part of a three-skill workflow:\r\n\r\n| Skill | Role | Key Output |\r\n|-------|------|------------|\r\n| **lit-search** | Find papers via OpenAlex | `database.json`, download checklist |\r\n| **lit-synthesis** | Analyze & organize via Zotero | `field-synthesis.md`, `theoretical-map.md`, `debate-map.md` |\r\n| **lit-writeup** | Draft prose | Publication-ready Theory section |\r\n\r\n**Ideal input**: If users ran lit-synthesis, request their `field-synthesis.md`, `theoretical-map.md`, and `debate-map.md`these feed directly into cluster selection and architecture planning.\r\n\r\n**Minimum input**: Users can start here with their own notes on the literature, but the workflow is smoother with lit-synthesis outputs.\r\n\r\n## When to Use This Skill\r\n\r\nUse this skill when users want to:\r\n- Draft a new Theory section from a literature database\r\n- Restructure an existing draft that isn't working\r\n- Select the right contribution strategy (gap-filling, theory-extension, etc.)\r\n- Craft the \"turn\" sentence that marks their contribution\r\n- Calibrate hedging, citations, and structure to field norms\r\n\r\n## Core Principles\r\n\r\n1. **Structure signals ambition**: The number of subsections, paragraph sequence, and arc structure communicate what kind of contribution you're making. Match form to content.\r\n\r\n2. **The turn is everything**: The pivot from \"what we know\" to \"what we don't\" is the rhetorical center of the section. Craft it carefully.\r\n\r\n3. **Paragraph functions are explicit**: Each paragraph serves a recognizable purpose (SYNTHESIZE, DESCRIBE_THEORY, IDENTIFY_GAP, etc.). Readers should sense the function even without subheadings.\r\n\r\n4. **Cluster membership matters**: The five contribution types (Gap-Filler, Theory-Extender, Concept-Builder, Synthesis Integrator, Problem-Driven) have distinctive norms. Know which you're writing.\r\n\r\n5. **Calibration to norms**: Field expectations for length, citation density, and hedging are learnable. Deviation should be intentional, not accidental.\r\n\r\n## The Five Clusters\r\n\r\nTheory sections cluster into five recognizable styles based on positioning move, structure, and literature balance:\r\n\r\n| Cluster | Prevalence | Key Feature | When to Use |\r\n|---------|------------|-------------|-------------|\r\n| **Gap-Filler** | 27.5% | Identifies what's missing | Empirical insight about understudied population |\r\n| **Theory-Extender** | 22.5% | Applies named framework | Applying established theory to new domain |\r\n| **Concept-Builder** | 15.0% | Introduces new terminology | Creating new conceptual tools or typologies |\r\n| **Synthesis Integrator** | 18.8% | Connects literatures | Bringing together previously separate traditions |\r\n| **Problem-Driven** | 16.3% | Resolves debate/documents | Adjudicating debates or policy-relevant documentation |\r\n\r\nSee `clusters/` directory for detailed profiles with characteristic paragraph sequences, citation patterns, and calibration norms.\r\n\r\n## Workflow Phases\r\n\r\n### Phase 0: Assessment\r\n**Goal**: Identify contribution type and select cluster.\r\n\r\n**Process**:\r\n- Review user's research question and main argument\r\n- Assess available literature (from lit-search or user's notes)\r\n- Identify the positioning move (gap, extension, building, synthesis, debate)\r\n- Select the appropriate cluster\r\n- Confirm cluster selection with user\r\n\r\n**Output**: Cluster selection memo with rationale.\r\n\r\n> **Pause**: User confirms cluster selection before architecture.\r\n\r\n---\r\n\r\n### Phase 1: Architecture\r\n**Goal**: Design section structure, subsections, and arc.\r\n\r\n**Process**:\r\n- Select arc structure (Funnel, Building-Blocks, Dialogue, Problem-Response)\r\n- Plan subsection organization (0-5+ depending on cluster)\r\n- Identify the 3-5 key literatures to engage\r\n- Place the turn within the overall structure\r\n- Create outline with subsection headings\r\n\r\n**Output**: Architecture memo with section outline.\r\n\r\n> **Pause**: User approves structure before paragraph planning.\r\n\r\n---\r\n\r\n### Phase 2: Planning\r\n**Goal**: Map paragraph functions and sequence.\r\n\r\n**Process**:\r\n- Assign function to each paragraph (PROVIDE_CONTEXT, SYNTHESIZE, DESCRIBE_THEORY, IDENTIFY_GAP, etc.)\r\n- Plan citation deployment for each paragraph\r\n- Identify anchor sources for key claims\r\n- Sequence paragraphs to build toward the turn\r\n- Draft topic sentences for each paragraph\r\n\r\n**Output**: Paragraph map with functions and topic sentences.\r\n\r\n> **Pause**: User reviews paragraph map.\r\n\r\n---\r\n\r\n### Phase 3: Drafting\r\n**Goal**: Write paragraphs with sentence-level craft.\r\n\r\n**Process**:\r\n- Draft each paragraph following its assigned function\r\n- Use appropriate opening sentence types (see `techniques/sentence-toolbox.md`)\r\n- Integrate citations using appropriate patterns (see `techniques/citation-patterns.md`)\r\n- Maintain cluster-appropriate hedging level\r\n- Build toward the turn sentence\r\n\r\n**Output**: Full draft of Theory section.\r\n\r\n> **Pause**: User reviews each subsection (if multiple) or full draft.\r\n\r\n---\r\n\r\n### Phase 4: Turn\r\n**Goal**: Craft the gap/contribution pivot.\r\n\r\n**Process**:\r\n- Apply the 4-part turn formula (see `techniques/turn-formula.md`)\r\n- Ensure gap is specific, not generic\r\n- Connect gap directly to research questions\r\n- Calibrate confidence level\r\n- Position turn appropriately (middle for most clusters)\r\n\r\n**Output**: Refined turn sentence(s) and surrounding context.\r\n\r\n> **Pause**: User evaluates the turn for clarity and specificity.\r\n\r\n---\r\n\r\n### Phase 5: Revision\r\n**Goal**: Calibrate against norms and polish.\r\n\r\n**Process**:\r\n- Check word count against target range (1,145-1,744)\r\n- Verify citation density (~24 per 1,000 words; 3-5 per paragraph)\r\n- Assess hedging calibration by claim type\r\n- Verify paragraph functions are clear\r\n- Ensure smooth transitions\r\n- Final polish for prose quality\r\n\r\n**Output**: Final Theory section with quality memo.\r\n\r\n---\r\n\r\n## Technique Guides\r\n\r\nThe skill includes detailed reference guides in `techniques/`:\r\n\r\n| Guide | Purpose |\r\n|-------|---------|\r\n| `sentence-toolbox.md` | 7 opening sentence types, transition markers, hedging calibration |\r\n| `paragraph-functions.md` | 9 paragraph functions with exemplars |\r\n| `citation-patterns.md` | 4 citation integration patterns |\r\n| `turn-formula.md` | 4-part turn structure with placement guidance |\r\n| `calibration-norms.md` | Statistical benchmarks from the analysis |\r\n\r\n## Cluster Profiles\r\n\r\nDetailed profiles in `clusters/`:\r\n\r\n| Profile | Content |\r\n|---------|---------|\r\n| `gap-filler.md` | Gap-filling style: funnel arc, minimal theory, sharp turn |\r\n| `theory-extender.md` | Framework application: named theorist, prior applications |\r\n| `concept-builder.md` | New terminology: building-blocks arc, definitional paragraphs |\r\n| `synthesis-integrator.md` | Literature integration: multiple traditions bridged |\r\n| `problem-driven.md` | Debate resolution or empirical documentation |\r\n\r\n## Calibration Benchmarks\r\n\r\nBased on 80 articles from *Social Problems* and *Social Forces*:\r\n\r\n| Metric | Median | Target Range (IQR) |\r\n|--------|--------|-------------------|\r\n| **Paragraphs** | 10 | 7-12 |\r\n| **Word count** | 1,393 | 1,145-1,744 |\r\n| **Unique citations** | 35 | 26-43 |\r\n| **Citations per paragraph** | 3.5 | 2.4-5.0 |\r\n| **Subsections** | 2 | 1-3 |\r\n| **Citations per 1,000 words** | 24.2 | 18.9-32.0 |\r\n\r\n## Invoking Phase Agents\r\n\r\nUse the Task tool for each phase:\r\n\r\n```\r\nTask: Phase 0 Assessment\r\nsubagent_type: general-purpose\r\nmodel: opus\r\nprompt: Read phases/phase0-assessment.md and clusters/*.md. Assess the user's contribution type and recommend a cluster. Project: [user's description]\r\n```\r\n\r\n## Model Recommendations\r\n\r\n| Phase | Model | Rationale |\r\n|-------|-------|-----------|\r\n| **Phase 0**: Assessment | **Opus** | Strategic judgment about contribution type |\r\n| **Phase 1**: Architecture | **Sonnet** | Structural planning |\r\n| **Phase 2**: Planning | **Sonnet** | Paragraph sequencing |\r\n| **Phase 3**: Drafting | **Opus** | Prose craft, citation integration |\r\n| **Phase 4**: Turn | **Opus** | High-stakes rhetorical craft |\r\n| **Phase 5**: Revision | **Opus** | Editorial judgment, calibration |\r\n\r\n## Starting the Write-Up\r\n\r\nWhen the user is ready to begin:\r\n\r\n1. **Ask about the project**:\r\n   > \"What is your research question? What is the main argument or contribution you're making?\"\r\n\r\n2. **Ask about available materials**:\r\n   > \"Did you run lit-synthesis? If so, share your `field-synthesis.md`, `theoretical-map.md`, and `debate-map.md`. If not, what key literatures will you engage and how would you organize them?\"\r\n\r\n3. **Ask about positioning**:\r\n   > \"How would you describe your contribution: filling a gap in what we know, extending an established framework, introducing new concepts, synthesizing literatures, or resolving a debate?\"\r\n\r\n4. **Assess and recommend a cluster**:\r\n   > Based on your answers, apply the decision tree and recommend a cluster with rationale.\r\n\r\n5. **Proceed with Phase 0** to formalize the assessment.\r\n\r\n## Key Reminders\r\n\r\n- **Cluster selection shapes everything**: Don't skip assessment. Wrong cluster = wrong structure = reader confusion.\r\n- **The turn is your thesis**: Readers remember the gap you fill, not your literature synthesis.\r\n- **Specificity wins**: \"We know little about X among Y in Z context\" beats \"more research is needed.\"\r\n- **Hedging is calibrated**: Hedge predictions, not definitions. Hedge mechanisms, not prevalence.\r\n- **Citations prove engagement**: Underciting signals superficiality; overciting signals catalog, not argument.\r\n- **Visual elements are rare but strategic**: Tables/figures only for Concept-Builders presenting frameworks.\r\n",
        "plugins/lit-writeup/skills/lit-writeup/clusters/concept-builder.md": "# Cluster 3: Concept-Building Architect\r\n\r\n**Prevalence**: 15.0% of articles (most ambitious style)\r\n\r\n## When to Use This Cluster\r\n\r\nUse the Concept-Builder approach when:\r\n- You're introducing **new terminology, typology, or conceptual framework**\r\n- You've identified limitations in existing concepts that require a new tool\r\n- Your contribution is primarily **theoretical innovation**, not just empirical description\r\n- You're willing to invest significant space in theoretical apparatus\r\n\r\n**Best for**: Introducing \"carceral creep,\" developing \"epistemological individualism,\" building a new framework synthesizing multiple traditions.\r\n\r\n## Distinguishing Features\r\n\r\n| Feature | Concept-Builder Pattern |\r\n|---------|------------------------|\r\n| **Positioning Move** | Theory-building |\r\n| **Arc** | Building-Blocks |\r\n| **Subsections** | Moderate to heavy (3-5+) |\r\n| **Literature Balance** | Theory-heavy (92%) |\r\n| **Visual Elements** | Tables or figures presenting the new framework |\r\n| **Turn Placement** | Middle |\r\n| **Hedging Level** | Low; confident about conceptual contributions |\r\n\r\n## Characteristic Paragraph Sequence\r\n\r\nThe Concept-Builder follows a critique-then-build sequence:\r\n\r\n1. **PROVIDE_CONTEXT** - Present the puzzle or problem\r\n2. **DESCRIBE_THEORY** - Review tradition 1 / existing approach\r\n3. **CRITIQUE** - Identify limitations of existing approach\r\n4. **DESCRIBE_THEORY** - Review tradition 2 / alternative approach\r\n5. **SYNTHESIZE** - Draw together limitations  need for new concept\r\n6. **DESCRIBE_THEORY** - Introduce and explicate the new concept\r\n\r\nThe structure builds toward your conceptual contribution as the payoff.\r\n\r\n## Signature Sentence Patterns\r\n\r\n### Framing the Puzzle\r\n> \"How do [actors] navigate [challenge]? Existing approaches offer limited purchase...\"\r\n> \"Despite extensive research on [topic], scholars lack conceptual tools for [specific aspect].\"\r\n\r\n### Critiquing Existing Approaches\r\n> \"While this work is valuable, it is limited in [specific ways]...\"\r\n> \"This approach, however, cannot account for [pattern]...\"\r\n> \"Existing literature is limited on two fronts. First, [X]. Second, [Y].\"\r\n\r\n### Introducing the New Concept\r\n> \"I use the term *[concept]* to suggest...\"\r\n> \"We define [concept] as...\"\r\n> \"*[Concept]* refers to...\"\r\n> \"By [concept], I mean...\"\r\n\r\n### Explicating the Concept\r\n> \"This concept captures [mechanism] through a process of...\"\r\n> \"[Concept] operates through [X], [Y], and [Z].\"\r\n> \"The concept involves [number] dimensions: [list].\"\r\n\r\n## Citation Patterns\r\n\r\nConcept-Builders feature **quote-then-cite** patterns for detailed engagement:\r\n> \"As [Scholar] notes, '[direct quote]' (Year:page).\"\r\n> \"[Scholar] defines [concept] as '...' (Year:page).\"\r\n\r\n**Author-subject** for sources being critiqued or synthesized:\r\n> \"[Scholar] argues that... However, this overlooks...\"\r\n\r\n**Definitional markers** for your new concept (no hedging):\r\n> \"I define [X] as...\" (not \"might be defined as\")\r\n\r\n**Citation density**: Highest of all clusters (5+ per paragraph in exposition).\r\n\r\n## Hedging Calibration\r\n\r\n| Claim Type | Hedging Level |\r\n|------------|---------------|\r\n| Critiquing existing approaches | Moderate (\"is limited,\" \"cannot fully account for\") |\r\n| Defining your concept | **No hedging** (\"refers to,\" \"is,\" \"means\") |\r\n| Describing how concept operates | Low hedging (\"captures,\" \"involves\") |\r\n| Claims about mechanism in empirical context | Moderate hedge |\r\n\r\nConcept-Builders should be **confident about their conceptual contribution**. You're building a tool; don't undercut it.\r\n\r\n## Calibration Benchmarks (Cluster-Specific)\r\n\r\nBased on Concept-Builder articles in the sample:\r\n\r\n| Metric | Typical Range |\r\n|--------|---------------|\r\n| **Word count** | 1,600-2,500+ (longest of all clusters) |\r\n| **Paragraphs** | 12-20+ |\r\n| **Subsections** | 3-5+ |\r\n| **Citations** | 40-70+ |\r\n| **Literature balance** | 92% theory-heavy, 8% balanced, 0% substantive-heavy |\r\n\r\nConcept-Builders are **distinctly longer** than other clusters. The theoretical apparatus requires space.\r\n\r\n## Visual Elements\r\n\r\nConcept-Builders are the cluster most likely to include tables or figures:\r\n- **Typology tables**: Present the 3-5 dimensions of your concept\r\n- **Process diagrams**: Show the stages of your framework\r\n- **Comparison tables**: Contrast your concept with existing approaches\r\n\r\nIf you're building a typology or multi-stage process, consider visualization.\r\n\r\n## Common Pitfalls\r\n\r\n| Pitfall | Solution |\r\n|---------|----------|\r\n| Concept without work | The concept must do analytical work in findings, not just appear in theory |\r\n| Insufficient critique | Show clearly why existing concepts are inadequate |\r\n| Over-hedging definitions | Definitions are assertions, not hypotheses |\r\n| No table when one would help | Multi-dimensional concepts benefit from visualization |\r\n| Too similar to existing concepts | Differentiate your contribution clearly |\r\n| Inadequate explication | New concepts need more space than familiar ones |\r\n\r\n## Exemplar Articles\r\n\r\n**10.1093_socpro_spz013** - Carceral Creep\r\n> Introduces \"carceral creep\" as a six-stage process (contestation, collaboration, hybridization, replication, occupation, subordination). Includes table defining each stage. Synthesizes social movement and criminology literatures.\r\n\r\n**10.1093_sf_soac082** - Conspiracism as Epistemological Practice\r\n> Develops conspiracism as \"epistemological practice\" with two consequences: \"epistemological individualism\" and \"epistemological othering.\" New terminology introduced and defined.\r\n\r\n**10.1093_socpro_spaa058** - Du Boisian Framework for Immigrant Incorporation\r\n> Builds integrated framework combining Du Bois, social movements, social psychology, and migration theory. Presents propositions for how framework operates.\r\n\r\n**10.1093_socpro_spz056** - Latino Typicality\r\n> Develops \"typicality\" framework for understanding stereotype activation. Extensive theoretical apparatus with multiple subsections.\r\n\r\n## Why This Cluster Works (When It Works)\r\n\r\nConcept-Building is the **most ambitious** approach:\r\n1. You're contributing a lasting tool to the discipline\r\n2. Success means other scholars adopt your terminology\r\n3. The payoff is high: your name attached to a concept\r\n\r\nThe **risk** is also highest:\r\n- If the concept doesn't do clear analytical work, it looks like jargon\r\n- If the concept is too similar to existing ones, reviewers will object\r\n- If the explication is rushed, readers won't understand how to use it\r\n\r\n**Only use this cluster if your concept genuinely does work that existing concepts cannot.** If you're not sure, consider whether Synthesis Integrator (integrating existing concepts) might fit better.\r\n",
        "plugins/lit-writeup/skills/lit-writeup/clusters/gap-filler.md": "# Cluster 1: Gap-Filler Minimalist\r\n\r\n**Prevalence**: 27.5% of articles (most common style)\r\n\r\n## When to Use This Cluster\r\n\r\nUse the Gap-Filler approach when:\r\n- Your primary contribution is **empirical insight** about an understudied population, setting, or phenomenon\r\n- You're not introducing new theoretical concepts\r\n- You're not applying a named framework that needs extended exposition\r\n- The theoretical context is established; you're filling in empirical specifics\r\n\r\n**Best for**: Studies of understudied populations, new settings, descriptive research on underexamined processes.\r\n\r\n## Distinguishing Features\r\n\r\n| Feature | Gap-Filler Pattern |\r\n|---------|-------------------|\r\n| **Positioning Move** | Gap-filling |\r\n| **Arc** | Funnel |\r\n| **Subsections** | None or light (0-2) |\r\n| **Literature Balance** | Balanced or substantive-heavy |\r\n| **Visual Elements** | None |\r\n| **Turn Placement** | Middle |\r\n| **Hedging Level** | Moderate; assertive gap statements |\r\n\r\n## Characteristic Paragraph Sequence\r\n\r\nThe standard Gap-Filler follows this 5-paragraph backbone (expand as needed):\r\n\r\n1. **PROVIDE_CONTEXT** - Establish the phenomenon or population\r\n2. **SYNTHESIZE** - Review existing findings on the topic\r\n3. **SYNTHESIZE** - Additional synthesis (what we know)\r\n4. **IDENTIFY_GAP** - Mark what remains unknown\r\n5. **STATE_QUESTIONS** - Articulate research questions\r\n\r\nThis funnel structure moves from broad context  specific gap  your study.\r\n\r\n## Signature Sentence Patterns\r\n\r\n### Opening the Section\r\n> \"A growing body of research examines [phenomenon]...\"\r\n> \"As [social condition] has [changed/expanded/intensified], scholars have...\"\r\n> \"A burgeoning literature documents [pattern]...\"\r\n\r\n### Building Synthesis\r\n> \"Research shows [finding] (Author Year; Author Year).\"\r\n> \"Studies consistently find [pattern]...\"\r\n> \"This literature highlights the importance of [X].\"\r\n\r\n### Marking the Turn\r\n> \"Yet we know little about [specific aspect].\"\r\n> \"However, less attention has been paid to [understudied dimension].\"\r\n> \"What remains unclear is [specific question].\"\r\n\r\n### Stating Questions\r\n> \"This study examines [research question] by [method].\"\r\n> \"I ask: [question]?\"\r\n\r\n## Citation Patterns\r\n\r\nGap-Fillers use **efficient parenthetical strings** for synthesis:\r\n> \"Research demonstrates X (Author 2020; Author 2019; Author 2018).\"\r\n\r\n**Minimal author-subject constructions** unless citing a canonical source:\r\n> Avoid: \"Jones (2019) argues that...\"\r\n> Prefer: \"Studies show that... (Jones 2019).\"\r\n\r\n**Citation density**: Aim for 2.4-5.0 citations per paragraph (lower end of range is fine).\r\n\r\n## Hedging Calibration\r\n\r\n| Claim Type | Hedging Level |\r\n|------------|---------------|\r\n| Reporting prior findings | Moderate assertiveness (\"studies demonstrate,\" \"research shows\") |\r\n| Gap claim | **Assertive** (\"we know little,\" \"remains unexplored\") |\r\n| Your research questions | Unhedged (state directly) |\r\n| Predictions about mechanisms | Hedge (\"may,\" \"might\") |\r\n\r\nGap-Fillers should be **confident about the gap**. The turn is your strongest rhetorical move.\r\n\r\n## Calibration Benchmarks (Cluster-Specific)\r\n\r\nBased on Gap-Filler articles in the sample:\r\n\r\n| Metric | Typical Range |\r\n|--------|---------------|\r\n| **Word count** | 1,000-1,400 (leaner than average) |\r\n| **Paragraphs** | 6-10 |\r\n| **Subsections** | 0-2 |\r\n| **Citations** | 25-35 |\r\n| **Literature balance** | 18% theory-heavy, 59% balanced, 23% substantive-heavy |\r\n\r\nGap-Fillers are **shorter and leaner** than other clusters. Don't over-elaborate.\r\n\r\n## Common Pitfalls\r\n\r\n| Pitfall | Solution |\r\n|---------|----------|\r\n| Adding unnecessary subsections | Keep it streamlined; 0-2 max |\r\n| Generic gap (\"more research is needed\") | Specify: \"We don't know [X] about [Y] among [Z]\" |\r\n| Theory bloat | You're not extending theory; keep theoretical framing minimal |\r\n| Turn arrives too late | Place the turn around paragraph 3-4, not at the end |\r\n| Overclaiming the gap | Acknowledge what we DO know before the gap claim |\r\n\r\n## Exemplar Articles\r\n\r\n**10.1093_sf_soy093** - CPS Reporting as Surveillance\r\n> Concise review of child welfare literature, identifies gap in understanding how reporting patterns affect families, no subsections, minimal theoretical framing beyond surveillance concept.\r\n\r\n**10.1093_socpro_spaa022** - Doubled-Up Households\r\n> Reviews housing/family literature efficiently, identifies that relational dynamics (host/guest) are understudied, moves quickly to identity/motherhood framing without extensive theoretical apparatus.\r\n\r\n**10.1093_socpro_spz005** - Activity Spaces in Reentry\r\n> Descriptive review of neighborhood effects and reentry literature, introduces activity spaces concept relatively late, minimal subsectioning, empirical focus.\r\n\r\n## Why This Cluster Works\r\n\r\nGap-Filling is the **most common and safest** approach. It works because:\r\n1. Readers quickly understand your contribution\r\n2. Structure is predictable and easy to follow\r\n3. Focus on empirical contribution keeps theoretical stakes manageable\r\n4. Works for studies where the main value is \"we didn't know this about this group\"\r\n\r\nIf you're unsure which cluster fits, Gap-Filler is the default. But make sure your gap is **specific**.\r\n",
        "plugins/lit-writeup/skills/lit-writeup/clusters/problem-driven.md": "# Cluster 5: Problem-Driven Pragmatist\r\n\r\n**Prevalence**: 16.3% of articles\r\n\r\n## When to Use This Cluster\r\n\r\nUse the Problem-Driven approach when:\r\n- You're **adjudicating a scholarly debate** between competing explanations\r\n- Your contribution is primarily **empirical documentation** of an understudied phenomenon\r\n- The research is **policy-relevant** and you're prioritizing applied value over theoretical elaboration\r\n- You're organized around a **puzzle or historical development** rather than a theoretical framework\r\n\r\n**Best for**: Resolving debates (does X help or harm Y?), documenting understudied policy contexts, historical-processual accounts.\r\n\r\n## Distinguishing Features\r\n\r\n| Feature | Problem-Driven Pattern |\r\n|---------|------------------------|\r\n| **Positioning Move** | Debate-resolution OR Empirical-elaboration |\r\n| **Arc** | Problem-Response, Dialogue, or Narrative-Historical |\r\n| **Subsections** | Variable (0-3) |\r\n| **Literature Balance** | Balanced or substantive-heavy (85% not theory-heavy) |\r\n| **Visual Elements** | Rare |\r\n| **Turn Placement** | Variable (sometimes early or late) |\r\n| **Hedging Level** | Moderate-high; hedged about competing explanations |\r\n\r\n## Two Sub-Types\r\n\r\n### Sub-type 5a: Debate-Resolvers\r\nFrame existing literature as characterized by competing positions, then position your study as providing evidence to adjudicate.\r\n\r\n### Sub-type 5b: Empirical Documenters\r\nHeavy on policy or empirical context, light theoretical framing. The contribution is descriptive insight about an understudied phenomenon.\r\n\r\n---\r\n\r\n## Sub-type 5a: Debate-Resolver\r\n\r\n### Characteristic Paragraph Sequence\r\n\r\n1. **DESCRIBE_THEORY** - Present position 1 in the debate\r\n2. **CONTRAST** - Present position 2\r\n3. **CRITIQUE** - Show both are limited / tension unresolved\r\n4. **STATE_QUESTIONS** - Position your study as adjudicating\r\n\r\n### Signature Sentence Patterns\r\n\r\n**Framing the Debate**:\r\n> \"Scholars disagree about whether [X] [helps/harms] [Y]...\"\r\n> \"The effects of [X] are debated. Some scholars argue... Others contend...\"\r\n> \"Two competing perspectives inform our understanding of [topic]...\"\r\n\r\n**Presenting Position 1**:\r\n> \"One line of research suggests that [X leads to Y]...\"\r\n> \"According to [tradition/scholars], [claim]...\"\r\n\r\n**Introducing the Contrast**:\r\n> \"Others, however, argue that [alternative claim]...\"\r\n> \"A contrasting account suggests...\"\r\n> \"In contrast, scholars in [tradition] find...\"\r\n\r\n**Positioning Your Study**:\r\n> \"This study provides evidence to adjudicate this debate by...\"\r\n> \"We ask whether [phenomenon] aligns more with [Position 1] or [Position 2]...\"\r\n> \"This article examines [case] to shed light on this tension...\"\r\n\r\n### Hedging Calibration (5a)\r\n\r\n| Claim Type | Hedging Level |\r\n|------------|---------------|\r\n| Describing Position 1 | Neutral reporting (\"argue,\" \"suggest\") |\r\n| Describing Position 2 | Neutral reporting (even-handed) |\r\n| Your adjudication claim | **Moderate** (\"provides evidence,\" \"sheds light\") |\r\n| Predictions | Higher hedging |\r\n\r\nDebate-Resolvers must be **even-handed** in presenting both sides. Don't tip your hand before the turn.\r\n\r\n---\r\n\r\n## Sub-type 5b: Empirical Documenter\r\n\r\n### Characteristic Paragraph Sequence\r\n\r\n1. **PROVIDE_CONTEXT** - Establish the phenomenon (extensive)\r\n2. **PROVIDE_CONTEXT** - Additional contextual detail\r\n3. **SYNTHESIZE** - Review prior research (often policy-focused)\r\n4. **IDENTIFY_GAP** - What we don't yet know\r\n\r\n### Signature Sentence Patterns\r\n\r\n**Heavy Context Opening**:\r\n> \"The [policy/phenomenon] has [expanded/changed/emerged] in recent decades...\"\r\n> \"In [year], [event/policy] transformed...\"\r\n> \"As [social condition] has intensified...\"\r\n\r\n**Policy-Focused Synthesis**:\r\n> \"Research has documented [outcomes/patterns]...\"\r\n> \"Studies find that [policy] affects [population] by...\"\r\n\r\n**Gap as Descriptive Need**:\r\n> \"Yet we know little about [specific aspect of phenomenon]...\"\r\n> \"What remains unclear is how [actors] experience [policy/phenomenon]...\"\r\n\r\n### Hedging Calibration (5b)\r\n\r\n| Claim Type | Hedging Level |\r\n|------------|---------------|\r\n| Contextual claims | Low (factual reporting) |\r\n| Prior research findings | Moderate |\r\n| Gap claim | Moderate-assertive |\r\n| Theoretical claims | Higher hedging (light theoretical framing) |\r\n\r\nEmpirical Documenters have **less theoretical apparatus** to be confident about.\r\n\r\n---\r\n\r\n## Calibration Benchmarks (Cluster-Specific)\r\n\r\nBased on Problem-Driven articles in the sample:\r\n\r\n| Metric | Typical Range |\r\n|--------|---------------|\r\n| **Word count** | 1,000-1,600 (variable) |\r\n| **Paragraphs** | 6-12 |\r\n| **Subsections** | 0-3 |\r\n| **Citations** | 20-40 |\r\n| **Literature balance** | 15% theory-heavy, 46% balanced, 38% substantive-heavy |\r\n\r\nProblem-Driven articles are **the most substantively-weighted**. More empirical/policy citations, fewer theoretical citations.\r\n\r\n## Arc Options\r\n\r\n### Problem-Response Arc\r\n- Open with the puzzle/problem\r\n- Present existing explanations\r\n- Show limitations\r\n- Preview your study's response\r\n\r\n### Dialogue Arc (for Debate-Resolvers)\r\n- Alternate between perspectives\r\n- Let them \"speak\" to each other\r\n- Position your study as resolving the conversation\r\n\r\n### Narrative-Historical Arc\r\n- Organize chronologically around a development\r\n- Show how literature has evolved\r\n- Position your study in the current moment\r\n\r\n## Common Pitfalls\r\n\r\n| Pitfall | Solution |\r\n|---------|----------|\r\n| One-sided debate presentation | Present both sides fairly and at similar length |\r\n| Overclaiming resolution | Your study \"provides evidence,\" doesn't definitively settle |\r\n| Context bloat | Keep empirical context focused; this isn't a policy report |\r\n| Missing theoretical framing entirely | Even Empirical Documenters need some analytical lens |\r\n| Turn lost in context | Make the gap/contribution explicit, even if theoretically light |\r\n\r\n## Exemplar Articles\r\n\r\n**10.1093_sf_soab152** - Immigrant Youth in Secondary Labor Market (Debate)\r\n> Opens with debate over whether secondary labor market entry helps or harms migrant incorporation. Two camps clearly identified. Study positioned as providing evidence to adjudicate.\r\n\r\n**10.1093_socpro_spab021** - Landlords and Poverty Governance (Empirical)\r\n> Heavy on policy context about housing, eviction, landlord practices. Theoretical framing is light; contribution is primarily empirical documentation.\r\n\r\n**10.1093_sf_soz110** - Nashville Civil Rights Movement (Historical-Narrative)\r\n> Organized chronologically around movement development. Hypotheses derived from historical patterns. Less theory-driven than pathways-derived.\r\n\r\n## Why This Cluster Works\r\n\r\nProblem-Driven approaches serve different scholarly purposes:\r\n\r\n**Debate-Resolution**:\r\n1. Moves the field forward by providing empirical traction on contested questions\r\n2. Shows you've mastered both sides of a literature\r\n3. Readers know exactly what's at stake\r\n\r\n**Empirical Documentation**:\r\n1. Provides needed description of understudied contexts\r\n2. Valuable for policy-relevant research\r\n3. Lower theoretical risk; contribution is clearly empirical\r\n\r\nThe **key distinction** from Gap-Filler: Gap-Fillers ask \"what do we not know about X?\" Problem-Driven (Debate) asks \"which explanation of X is right?\" Problem-Driven (Empirical) prioritizes contextual description over theoretical framing.\r\n\r\n**When in doubt**: If your \"debate\" is really one established view and your extension, you're probably Theory-Extender. If your empirical focus has a clear gap, you're probably Gap-Filler. Problem-Driven is for genuine tensions between positions or heavy contextual documentation.\r\n",
        "plugins/lit-writeup/skills/lit-writeup/clusters/synthesis-integrator.md": "# Cluster 4: Synthesis Integrator\r\n\r\n**Prevalence**: 18.8% of articles\r\n\r\n## When to Use This Cluster\r\n\r\nUse the Synthesis Integrator approach when:\r\n- You're bringing together **concepts from different literatures** that haven't been connected before\r\n- You're NOT coining new termsyou're showing how existing concepts illuminate each other\r\n- Your contribution is demonstrating **productive connections** between traditions\r\n- The integration provides analytical leverage neither tradition offers alone\r\n\r\n**Best for**: Connecting legal consciousness with recognition theory, bringing human-animal studies to racial formation, synthesizing procedural justice with distributive justice.\r\n\r\n## Distinguishing Features\r\n\r\n| Feature | Synthesis Integrator Pattern |\r\n|---------|------------------------------|\r\n| **Positioning Move** | Concept-synthesis |\r\n| **Arc** | Building-Blocks or Dialogue |\r\n| **Subsections** | Moderate (2-4) |\r\n| **Literature Balance** | Theory-heavy to balanced (67% theory-heavy, 33% balanced) |\r\n| **Visual Elements** | Rare |\r\n| **Turn Placement** | Middle |\r\n| **Hedging Level** | Moderate; hedged about integration claims |\r\n\r\n## Characteristic Paragraph Sequence\r\n\r\nThe Synthesis Integrator builds parallel blocks, then bridges:\r\n\r\n1. **DESCRIBE_THEORY** - Present framework A\r\n2. **SYNTHESIZE** - Review applications of framework A\r\n3. **DESCRIBE_THEORY** - Present framework B\r\n4. **SYNTHESIZE** - Review applications of framework B\r\n5. **BRIDGE** - Show how A + B illuminate the case together\r\n\r\nThe payoff is the bridge paragraph that demonstrates synergy.\r\n\r\n## Signature Sentence Patterns\r\n\r\n### Introducing the First Tradition\r\n> \"The concept of [X] has been central to understanding [domain]...\"\r\n> \"Scholars in [tradition] have long examined [topic]...\"\r\n> \"Research on [topic] provides insight into [aspect]...\"\r\n\r\n### Transitioning to Second Tradition\r\n> \"A parallel literature examines [related topic]...\"\r\n> \"Separately, scholars have studied [second phenomenon]...\"\r\n> \"A second body of scholarship addresses [related question]...\"\r\n\r\n### Marking the Integration Gap\r\n> \"These literatures have not been brought together...\"\r\n> \"Yet scholars have rarely connected [A] with [B]...\"\r\n> \"Despite their complementary insights, [A] and [B] have not been integrated...\"\r\n\r\n### The Bridge Move\r\n> \"We draw together insights from [A] and [B] to illuminate...\"\r\n> \"Bringing these literatures into dialogue reveals...\"\r\n> \"This integration allows us to see [phenomenon] as both [A-insight] and [B-insight].\"\r\n\r\n## Citation Patterns\r\n\r\nSynthesis Integrators use **mixed citation patterns** because you're drawing from multiple traditions:\r\n\r\n**Author-subject** for canonical sources in each tradition:\r\n> \"Honneth (2003) theorizes recognition as...\"\r\n> \"Ewick and Silbey (1998) argue that legal consciousness...\"\r\n\r\n**Parenthetical strings** for synthesizing findings within each tradition:\r\n> \"Studies demonstrate the importance of recognition (Author 2020; Author 2019).\"\r\n\r\n**Bridging citations** that connect traditions (if they exist):\r\n> \"Recent work has begun connecting these perspectives (Author 2021).\"\r\n\r\n**Citation density**: Moderate-high (3-4 per paragraph, drawn from multiple traditions).\r\n\r\n## Hedging Calibration\r\n\r\n| Claim Type | Hedging Level |\r\n|------------|---------------|\r\n| Describing each tradition | Low hedging (established work) |\r\n| Gap claim (not integrated) | Assertive (\"have not been connected\") |\r\n| Integration claim | **Moderate hedging** (\"we suggest,\" \"this allows us to see\") |\r\n| How integration illuminates data | Moderate (\"reveals,\" \"suggests\") |\r\n\r\nIntegration claims require **more humility** than concept definitionsyou're proposing a connection, not defining a term.\r\n\r\n## Calibration Benchmarks (Cluster-Specific)\r\n\r\nBased on Synthesis Integrator articles in the sample:\r\n\r\n| Metric | Typical Range |\r\n|--------|---------------|\r\n| **Word count** | 1,300-1,800 (moderate length) |\r\n| **Paragraphs** | 10-14 |\r\n| **Subsections** | 2-4 |\r\n| **Citations** | 35-50 |\r\n| **Literature balance** | 67% theory-heavy, 33% balanced |\r\n\r\nSynthesis Integrators occupy the **middle ground**more theoretical than Gap-Fillers, less apparatus than Concept-Builders.\r\n\r\n## Structural Options\r\n\r\n### Building-Blocks Arc\r\n- Subsection 1: Tradition A (2-3 paragraphs)\r\n- Subsection 2: Tradition B (2-3 paragraphs)\r\n- Subsection 3: Integration + This Study (2-3 paragraphs)\r\n\r\n### Dialogue Arc\r\n- Alternate between traditions, showing how each speaks to the other\r\n- More complex but can reveal deeper connections\r\n\r\n### Parallel Structure\r\n- Some Synthesis Integrators don't subsection but use clear paragraph-level transitions:\r\n  - \"While [A] emphasizes [X], [B] calls attention to [Y]...\"\r\n  - \"Bringing these perspectives together...\"\r\n\r\n## Common Pitfalls\r\n\r\n| Pitfall | Solution |\r\n|---------|----------|\r\n| Listing literatures without connection | The bridge must be explicit and substantive |\r\n| Treating one tradition as primary | Balance attention; both should matter |\r\n| Integration that feels forced | Only synthesize if the connection does real analytical work |\r\n| Missing the \"so what\" | Explain what the integration reveals that neither tradition alone could |\r\n| Accidentally building a concept | If your synthesis produces new terminology, you're Concept-Builder |\r\n\r\n## Exemplar Articles\r\n\r\n**10.1093_socpro_spaa017** - Recognition and Legal Cynicism\r\n> Integrates recognition (Honneth, Lamont), legal cynicism (Kirk, Matsuda), and legal consciousness (Ewick, Silbey) to explain why residents in high-crime areas call police despite cynicism. No new concept, but productive synthesis.\r\n\r\n**10.1093_socpro_spab025** - Perceptual Justice Frames\r\n> Brings together procedural justice, distributive justice, and legal cynicism literatures. Structured as sequential \"blocks\" that build analytical framework.\r\n\r\n**10.1093_socpro_spab073** - Animals and Racialization\r\n> Synthesizes human-animal studies literature with racial formation theory and intensive mothering ideology. Multiple literatures combined without new terminology.\r\n\r\n## Why This Cluster Works\r\n\r\nSynthesis Integration demonstrates **theoretical sophistication** through connection:\r\n1. You show mastery of multiple literatures\r\n2. The contribution is clearly scoped: \"A + B reveals C\"\r\n3. Lower risk than Concept-Building (you're not inventing)\r\n4. Higher ambition than Gap-Filling (you're doing theoretical work)\r\n\r\nThe **key test**: Does the integration produce insight that neither tradition alone offers? If the answer is yes, and you're not coining new terms, you're a Synthesis Integrator.\r\n\r\nIf you find yourself creating new terminology to name the synthesis, consider whether you're actually a Concept-Builder.\r\n",
        "plugins/lit-writeup/skills/lit-writeup/clusters/theory-extender.md": "# Cluster 2: Theory-Extension Framework Applier\r\n\r\n**Prevalence**: 22.5% of articles\r\n\r\n## When to Use This Cluster\r\n\r\nUse the Theory-Extender approach when:\r\n- You're applying an **established theoretical framework** (often with a named originator) to a new empirical domain\r\n- The framework itself is not being modifiedyou're demonstrating its utility in a new context\r\n- You want to signal **theoretical literacy** while contributing empirical extension\r\n- The framework is well-established enough that extension is meaningful\r\n\r\n**Best for**: Applying Bourdieu to a new field, extending Acker's gendered organizations to a new setting, using legal consciousness in a novel context.\r\n\r\n## Distinguishing Features\r\n\r\n| Feature | Theory-Extender Pattern |\r\n|---------|------------------------|\r\n| **Positioning Move** | Theory-extension |\r\n| **Arc** | Funnel or Building-Blocks |\r\n| **Subsections** | Light to moderate (1-4) |\r\n| **Literature Balance** | Theory-heavy (78%) |\r\n| **Visual Elements** | Occasional tables |\r\n| **Turn Placement** | Middle, sometimes early |\r\n| **Hedging Level** | Low-moderate; confident about established theory |\r\n\r\n## Characteristic Paragraph Sequence\r\n\r\nThe standard Theory-Extender follows this sequence:\r\n\r\n1. **PROVIDE_CONTEXT** - Establish the empirical phenomenon\r\n2. **DESCRIBE_THEORY** - Introduce framework X (the named theorist/framework)\r\n3. **SYNTHESIZE** - Review prior applications of framework X\r\n4. **IDENTIFY_GAP** - Show that framework X has not been applied to domain Y\r\n5. **BRIDGE** - Explain how X illuminates Y\r\n\r\nThe key move is showing that an **existing tool** can do **new work**.\r\n\r\n## Signature Sentence Patterns\r\n\r\n### Opening with the Framework\r\n> \"The concept of '[framework]' draws on...\"\r\n> \"According to [Scholar] (Year), [framework] posits that...\"\r\n> \"[Scholar's] (Year) concept of [X] provides a lens for understanding...\"\r\n> \"Central to this analysis is [framework]...\"\r\n\r\n### Documenting Prior Applications\r\n> \"Scholars have applied [framework] to [domain 1], [domain 2], and [domain 3] (citations).\"\r\n> \"This framework has proven useful for understanding [pattern]...\"\r\n> \"Building on this work, scholars have extended [framework] to...\"\r\n\r\n### Marking the Extension Gap\r\n> \"[Framework] has not been applied to [your domain]...\"\r\n> \"Yet scholars have not examined how [framework] operates in [context]...\"\r\n> \"We extend this framework to [new empirical domain]...\"\r\n\r\n### The Bridge Move\r\n> \"We argue that [framework] illuminates [phenomenon] by...\"\r\n> \"[Framework] offers analytical purchase on [empirical puzzle] because...\"\r\n\r\n## Citation Patterns\r\n\r\nTheory-Extenders feature **prominent author-subject constructions** for the canonical theorist:\r\n> \"Bourdieu (1984) argues that capital operates through...\"\r\n> \"According to Puwar (2004), numerical representation matters...\"\r\n\r\n**Parenthetical strings** for prior applications:\r\n> \"Scholars have applied this framework to education (Author 2020), healthcare (Author 2019), and housing (Author 2018).\"\r\n\r\n**Citation density**: Higher end of range (4-5+ per paragraph in theory exposition).\r\n\r\n## Hedging Calibration\r\n\r\n| Claim Type | Hedging Level |\r\n|------------|---------------|\r\n| Describing established theory | **Low hedging** (\"posits,\" \"demonstrates,\" \"establishes\") |\r\n| Reviewing prior applications | Moderate (\"studies show,\" \"research finds\") |\r\n| Your extension claim | Moderate (\"we argue,\" \"we extend\") |\r\n| Predictions about mechanisms in your context | Hedge (\"may operate,\" \"suggests\") |\r\n\r\nTheory-Extenders should be **confident about the framework itself**it's established. The extension is where humility enters.\r\n\r\n## Calibration Benchmarks (Cluster-Specific)\r\n\r\nBased on Theory-Extender articles in the sample:\r\n\r\n| Metric | Typical Range |\r\n|--------|---------------|\r\n| **Word count** | 1,300-1,800 (moderate length) |\r\n| **Paragraphs** | 8-12 |\r\n| **Subsections** | 1-4 |\r\n| **Citations** | 30-45 |\r\n| **Literature balance** | 78% theory-heavy, 22% balanced, 0% substantive-heavy |\r\n\r\nTheory-Extenders are **distinctly theory-heavy**. If your literature balance feels \"balanced,\" you may be in Gap-Filler territory.\r\n\r\n## Common Pitfalls\r\n\r\n| Pitfall | Solution |\r\n|---------|----------|\r\n| Modifying the framework | If you're modifying, you're a Concept-Builder, not an Extender |\r\n| Weak framework exposition | Give the framework a full paragraph; readers need to understand it |\r\n| Not showing prior applications | Extensions only matter if the tool has proven useful elsewhere |\r\n| Unclear bridge | Explain *why* this framework fits your domain |\r\n| Framework mentioned but not used | If the framework doesn't organize your analysis, don't claim extension |\r\n\r\n## Exemplar Articles\r\n\r\n**10.1093_socpro_spz045** - Complex Visibility in Elite Professions\r\n> Extends Puwar's \"bodies out of place\" framework to understand organizational outsiders navigating elite contexts. Detailed exposition of tokenism theory, then shows how race/gender status characteristics add explanatory power.\r\n\r\n**10.1093_socpro_spab024** - Gendered Organizations and Birth Assault\r\n> Applies Acker's gendered organizations framework (culture, structure, agency) to hospital birth experiences. Clear framework application without modification.\r\n\r\n**10.1093_socpro_spab036** - Electronic Home Monitoring\r\n> Extends surveillance studies to electronic monitoring context. Framework-first approach.\r\n\r\n## Why This Cluster Works\r\n\r\nTheory-Extension signals **theoretical sophistication** while keeping risk manageable:\r\n1. You're not inventing new concepts (lower bar)\r\n2. You demonstrate mastery of an established tradition\r\n3. Your contribution is clearly scoped: \"X works in Y too\"\r\n4. Cumulative science: showing frameworks travel builds knowledge\r\n\r\nThe danger is that **pure extension** can feel incremental. The bridge paragraph must show why this extension matterswhy it reveals something new about both the theory and the domain.\r\n",
        "plugins/lit-writeup/skills/lit-writeup/phases/phase0-assessment.md": "# Phase 0: Assessment\r\n\r\n## Why This Phase Matters\r\n\r\nThe single most important decision in Theory section writing is **cluster selection**. The cluster determines your structure, paragraph sequence, citation patterns, and calibration targets. Getting this wrong means writing a section that doesn't match your contributionconfusing readers and undermining your argument.\r\n\r\nThis phase ensures you make a deliberate, informed choice about which style fits your project.\r\n\r\n---\r\n\r\n## Your Tasks\r\n\r\n### 1. Gather Project Information\r\n\r\nCollect from the user:\r\n\r\n**Required**:\r\n- Research question(s)\r\n- Main argument or contribution claim\r\n- Key literatures to engage (2-4 traditions)\r\n- Available materials (lit-search outputs, existing notes)\r\n\r\n**Helpful**:\r\n- Target journal (if known)\r\n- Existing drafts (if revising)\r\n- Theoretical framework (if applicable)\r\n\r\n### 2. Apply the Cluster Decision Tree\r\n\r\nWork through this diagnostic to recommend a cluster:\r\n\r\n```\r\nWhat is the primary contribution?\r\n\r\n Empirical insight about understudied population/phenomenon?\r\n    No new theoretical framework or terminology?\r\n        CLUSTER 1: Gap-Filler\r\n\r\n Applying an established framework to a new domain?\r\n    Named theorist/framework central to analysis?\r\n    Not modifying the framework, just extending it?\r\n        CLUSTER 2: Theory-Extender\r\n\r\n Introducing new terminology, typology, or conceptual framework?\r\n    Existing concepts inadequate?\r\n    New term will be used in findings?\r\n        CLUSTER 3: Concept-Builder\r\n\r\n Connecting previously separate literatures?\r\n    Multiple traditions brought together?\r\n    Not coining new terms?\r\n        CLUSTER 4: Synthesis Integrator\r\n\r\n Adjudicating a debate OR documenting for policy?\r\n     Two competing positions to adjudicate?\r\n        CLUSTER 5a: Problem-Driven (Debate)\r\n     Heavy empirical/policy context, light theory?\r\n         CLUSTER 5b: Problem-Driven (Empirical)\r\n```\r\n\r\n### 3. Review Cluster Profile\r\n\r\nOnce you have a candidate cluster, read the detailed profile:\r\n- `clusters/gap-filler.md`\r\n- `clusters/theory-extender.md`\r\n- `clusters/concept-builder.md`\r\n- `clusters/synthesis-integrator.md`\r\n- `clusters/problem-driven.md`\r\n\r\nVerify the profile matches the user's project.\r\n\r\n### 4. Assess Materials\r\n\r\nIf user has lit-search outputs:\r\n- Review `synthesis.md` for literature landscape\r\n- Review `gaps.md` for identified opportunities\r\n- Check citation inventory against cluster needs\r\n\r\nIf no lit-search:\r\n- Ask user to describe key literatures\r\n- Identify which traditions need engagement\r\n- Note any gaps in coverage\r\n\r\n### 5. Write Assessment Memo\r\n\r\nCreate `assessment-memo.md` with:\r\n\r\n```markdown\r\n# Theory Section Assessment\r\n\r\n## Project Summary\r\n- Research question: [summarize]\r\n- Main argument: [summarize]\r\n- Key literatures: [list]\r\n\r\n## Contribution Type Analysis\r\n- [Walk through decision tree logic]\r\n- [Note any ambiguities]\r\n\r\n## Recommended Cluster\r\n**Cluster [N]: [Name]**\r\n\r\n### Rationale\r\n- [Why this cluster fits]\r\n- [What the cluster predicts about structure]\r\n\r\n### Alternative Considered\r\n- [If applicable: why rejected alternative]\r\n\r\n## Cluster Implications\r\n- Expected word count: [range]\r\n- Subsection structure: [description]\r\n- Literature balance: [theory-heavy/balanced/substantive]\r\n- Turn placement: [early/middle/late]\r\n\r\n## Literature Readiness\r\n- [Assessment of available materials]\r\n- [Any gaps to fill before drafting]\r\n\r\n## Questions for User\r\n- [Any clarifying questions needed before architecture]\r\n```\r\n\r\n### 6. Present Recommendation\r\n\r\nPresent to user:\r\n- Your recommended cluster with clear rationale\r\n- What this means for structure and approach\r\n- Any questions you have before proceeding\r\n\r\n---\r\n\r\n## Guiding Principles\r\n\r\n### Cluster Selection is Not Permanent\r\nYou can revise as you learn more during architecture. But starting with the right cluster prevents structural rewrites.\r\n\r\n### When in Doubt, Choose Gap-Filler\r\nIt's the most common (27.5%) and safest approach. You can always add theoretical ambition; it's harder to remove it.\r\n\r\n### Theory-Extension vs. Synthesis\r\n- **Extension**: One framework, applied to new domain\r\n- **Synthesis**: Multiple frameworks, integrated without new terminology\r\n\r\n### Concept-Building is High-Risk\r\nOnly choose Cluster 3 if the new concept genuinely does work existing concepts cannot. Failed concept-building looks like jargon.\r\n\r\n### Problem-Driven Requires Genuine Debate\r\nIf your \"debate\" is really one established view and your extension of it, you're a Theory-Extender. True Problem-Driven (Debate) presents two genuinely competing positions.\r\n\r\n---\r\n\r\n## Output Files to Create\r\n\r\n1. **assessment-memo.md** - Full assessment with cluster recommendation\r\n2. (Optional) **questions-for-user.md** - If clarification needed\r\n\r\n---\r\n\r\n## When You're Done\r\n\r\nReport to the orchestrator:\r\n- Recommended cluster\r\n- Key rationale (2-3 sentences)\r\n- Any outstanding questions\r\n- Readiness assessment for Phase 1\r\n\r\nExample summary:\r\n> \"I recommend **Cluster 1: Gap-Filler** for this project. The user is studying how guest mothers manage identity in doubled-up householdsan understudied population with no novel theoretical framework. The contribution is primarily empirical. The user has solid literature from lit-search covering housing, family, and identity literatures. Ready for Phase 1: Architecture.\"\r\n",
        "plugins/lit-writeup/skills/lit-writeup/phases/phase1-architecture.md": "# Phase 1: Architecture\r\n\r\n## Why This Phase Matters\r\n\r\nArchitecture is the blueprint for your Theory section. Before writing paragraphs, you need to know: How many subsections? What arc? Where does the turn fall? What literatures get space?\r\n\r\nPoor architecture leads to sections that feel disorganized, where the turn arrives too late (or too early), or where the structure doesn't match the contribution. This phase ensures structural coherence before drafting begins.\r\n\r\n---\r\n\r\n## Your Tasks\r\n\r\n### 1. Select Arc Structure\r\n\r\nChoose the arc that matches your cluster:\r\n\r\n| Arc | Description | Best For |\r\n|-----|-------------|----------|\r\n| **Funnel** | Broad  narrow  gap  questions | Gap-Filler, Theory-Extender |\r\n| **Building-Blocks** | Component  Component  Integration | Concept-Builder, Synthesis |\r\n| **Dialogue** | Position A  Position B  Resolution | Problem-Driven (Debate), Synthesis |\r\n| **Problem-Response** | Puzzle  Attempts  Limitations  Your approach | Problem-Driven, Concept-Builder |\r\n| **Narrative-Historical** | Chronological development  Current moment | Problem-Driven (Historical) |\r\n\r\n### 2. Plan Subsection Structure\r\n\r\nBased on cluster:\r\n\r\n| Cluster | Typical Subsections |\r\n|---------|-------------------|\r\n| **Gap-Filler** | 0-2 (often none; if any: \"Background\" / \"This Study\") |\r\n| **Theory-Extender** | 1-3 (e.g., \"[Framework]\" / \"Prior Applications\" / \"This Study\") |\r\n| **Concept-Builder** | 3-5+ (e.g., one per tradition + new concept + application) |\r\n| **Synthesis** | 2-4 (e.g., \"Tradition A\" / \"Tradition B\" / \"Integration\") |\r\n| **Problem-Driven** | 0-3 (variable; may use \"Context\" / \"The Debate\" / \"This Study\") |\r\n\r\n### 3. Identify Key Literatures\r\n\r\nMap the 3-5 literatures that will receive space:\r\n\r\nFor each literature:\r\n- What does it contribute to your argument?\r\n- How many paragraphs will it need?\r\n- What's the key citation cluster?\r\n\r\nExample for a Gap-Filler:\r\n```\r\n1. Housing instability literature (2 paragraphs)\r\n   - Key: Desmond, Pattillo\r\n   - Function: Context + synthesis\r\n\r\n2. Family identity literature (2 paragraphs)\r\n   - Key: Lareau, Hays\r\n   - Function: Theoretical lens\r\n\r\n3. Doubled-up households specifically (1 paragraph)\r\n   - Key: Pilkauskas\r\n   - Function: Direct predecessor  gap\r\n```\r\n\r\n### 4. Place the Turn\r\n\r\nDetermine where the turn falls:\r\n\r\n| Cluster | Turn Placement | Paragraph Location |\r\n|---------|----------------|-------------------|\r\n| Gap-Filler | Middle | Around paragraph 3-4 of ~6 |\r\n| Theory-Extender | Middle, sometimes early | After framework (paragraph 4-5 of ~8) |\r\n| Concept-Builder | Middle | After critique, before new concept |\r\n| Synthesis | Middle | After both traditions |\r\n| Problem-Driven | Variable | Early for debate, late for empirical |\r\n\r\n### 5. Draft Section Outline\r\n\r\nCreate detailed outline with:\r\n- Subsection titles (if using)\r\n- Paragraph-level breakdown\r\n- Function assigned to each paragraph (from `techniques/paragraph-functions.md`)\r\n- Approximate citation counts per section\r\n\r\nExample outline for Theory-Extender:\r\n\r\n```markdown\r\n# Theory Section Outline\r\n\r\n## Overall Arc: Funnel\r\n## Target Length: 1,500 words (10 paragraphs)\r\n\r\n### Introduction (no subsection)\r\n- P1: PROVIDE_CONTEXT - Housing precarity context [3 citations]\r\n- P2: SYNTHESIZE - Literature on housing instability [5 citations]\r\n\r\n### [Framework Name] (subsection)\r\n- P3: DESCRIBE_THEORY - Introduce framework [3-4 citations]\r\n- P4: DESCRIBE_THEORY - Elaborate dimensions [3 citations]\r\n- P5: SYNTHESIZE - Prior applications [6 citations]\r\n\r\n### Extending to [Domain]\r\n- P6: IDENTIFY_GAP - Framework not applied to X [2 citations]  TURN\r\n- P7: BRIDGE - Why framework fits this domain [3 citations]\r\n\r\n### This Study\r\n- P8: STATE_QUESTIONS - Research questions [1-2 citations]\r\n- P9: PREVIEW - Argument preview [0 citations]\r\n```\r\n\r\n### 6. Estimate Calibration Targets\r\n\r\nBased on cluster, set targets:\r\n\r\n```markdown\r\n## Calibration Targets\r\n\r\n- Word count: [range based on cluster]\r\n- Paragraph count: [range]\r\n- Citation count: [range]\r\n- Subsection count: [number]\r\n- Literature balance: [theory-heavy / balanced / substantive]\r\n```\r\n\r\n### 7. Write Architecture Memo\r\n\r\nCreate `architecture-memo.md`:\r\n\r\n```markdown\r\n# Theory Section Architecture\r\n\r\n## Cluster: [Name]\r\n## Arc: [Type]\r\n\r\n## Structure Overview\r\n\r\n[Describe the overall flow in prose]\r\n\r\n## Subsection Plan\r\n\r\n### [Subsection 1 Title]\r\n- Function: [what this section accomplishes]\r\n- Paragraphs: [number]\r\n- Key literatures: [list]\r\n\r\n### [Subsection 2 Title]\r\n...\r\n\r\n## Turn Placement\r\n\r\nThe turn will occur in [paragraph N], after [what precedes] and before [what follows].\r\n\r\nTurn strategy: [describe the pivot approach]\r\n\r\n## Literature Allocation\r\n\r\n| Literature | Paragraphs | Function |\r\n|------------|------------|----------|\r\n| [Lit 1] | [N] | [role] |\r\n| [Lit 2] | [N] | [role] |\r\n...\r\n\r\n## Calibration Targets\r\n\r\n| Metric | Target |\r\n|--------|--------|\r\n| Words | [range] |\r\n| Paragraphs | [range] |\r\n| Citations | [range] |\r\n| Subsections | [number] |\r\n\r\n## Outline\r\n\r\n[Full paragraph-level outline with functions]\r\n\r\n## Notes for Drafting\r\n\r\n[Any special considerations, challenges, or decisions for Phase 3]\r\n```\r\n\r\n---\r\n\r\n## Guiding Principles\r\n\r\n### Structure Signals Ambition\r\n- No subsections = streamlined empirical contribution\r\n- 1-2 subsections = organized review\r\n- 3-4 subsections = multiple theoretical components\r\n- 5+ subsections = building theoretical apparatus\r\n\r\nDon't add subsections for organizational convenience if they don't reflect genuine theoretical divisions.\r\n\r\n### Each Subsection Should Have a Clear Purpose\r\nIf you can't articulate what a subsection accomplishes, you don't need it.\r\n\r\n### The Turn is the Structural Center\r\nEverything before builds toward it; everything after flows from it. Design around the turn.\r\n\r\n### Leave Room for Discovery\r\nThis is a blueprint, not a prison. Drafting may reveal the need for adjustments.\r\n\r\n---\r\n\r\n## Output Files to Create\r\n\r\n1. **architecture-memo.md** - Full architecture plan with outline\r\n2. **section-outline.md** - Clean outline for quick reference during drafting\r\n\r\n---\r\n\r\n## When You're Done\r\n\r\nReport to the orchestrator:\r\n- Arc structure selected\r\n- Subsection plan (number and titles)\r\n- Turn placement\r\n- Calibration targets\r\n- Any concerns about literature coverage\r\n\r\nExample summary:\r\n> \"Architecture complete. Using **Funnel arc** with **3 subsections**: 'Housing Precarity' (context), 'Bodies Out of Place' (framework), and 'This Study' (contribution). Turn placed at end of framework section (paragraph 6). Target: 1,500 words, 10 paragraphs, 35 citations. Ready for Phase 2: Planning.\"\r\n",
        "plugins/lit-writeup/skills/lit-writeup/phases/phase2-planning.md": "# Phase 2: Planning\r\n\r\n## Why This Phase Matters\r\n\r\nBefore drafting prose, you need a paragraph-by-paragraph plan that specifies what each paragraph will accomplish, how it will open, and what citations it will deploy. This planning prevents the common problem of aimless literature cataloging and ensures each paragraph drives the argument forward.\r\n\r\n---\r\n\r\n## Your Tasks\r\n\r\n### 1. Assign Paragraph Functions\r\n\r\nFor each paragraph in the outline, assign a function from `techniques/paragraph-functions.md`:\r\n\r\n| Function | Purpose |\r\n|----------|---------|\r\n| **PROVIDE_CONTEXT** | Establish phenomenon, population, setting |\r\n| **DESCRIBE_THEORY** | Explicate framework or concept |\r\n| **SYNTHESIZE** | Summarize patterns across studies |\r\n| **CONTRAST** | Present competing positions |\r\n| **CRITIQUE** | Identify limitations |\r\n| **IDENTIFY_GAP** | Mark what's unknown (the turn) |\r\n| **BRIDGE** | Connect literatures or show framework fit |\r\n| **STATE_QUESTIONS** | Articulate research questions |\r\n| **PREVIEW** | Summarize coming argument |\r\n\r\n### 2. Draft Topic Sentences\r\n\r\nFor each paragraph, write a topic sentence that:\r\n- Signals the paragraph's function\r\n- Uses appropriate opening type (from `techniques/sentence-toolbox.md`)\r\n- Sets up the paragraph's work\r\n\r\n**Example topic sentences by function**:\r\n\r\n| Function | Topic Sentence |\r\n|----------|----------------|\r\n| PROVIDE_CONTEXT | \"As housing costs have outpaced wage growth, many parents face difficulty housing their families.\" |\r\n| DESCRIBE_THEORY | \"The concept of 'recognition' draws on several conceptual traditions.\" |\r\n| SYNTHESIZE | \"A growing body of research examines how residents of high-crime neighborhoods interact with police.\" |\r\n| CONTRAST | \"Others, however, argue that secondary labor market entry traps immigrants in dead-end jobs.\" |\r\n| IDENTIFY_GAP | \"Yet we know little about how guest mothers maintain maternal identity when authority is constrained.\" |\r\n| STATE_QUESTIONS | \"This study examines how mothers in doubled-up households negotiate identity and dignity.\" |\r\n\r\n### 3. Plan Citation Deployment\r\n\r\nFor each paragraph, specify:\r\n- How many citations (target 2.4-5.0 per paragraph)\r\n- Citation pattern (parenthetical, author-subject, string, quote)\r\n- Key sources to include\r\n\r\nUse this template:\r\n\r\n```markdown\r\n## Paragraph 3: SYNTHESIZE\r\n\r\n**Topic sentence**: \"Research on legal cynicism demonstrates that distrust is patterned by neighborhood context and prior police contact.\"\r\n\r\n**Citations (5)**:\r\n- Kirk and Papachristos 2011 (author-subject for definition)\r\n- Sampson and Bartusch 1998 (parenthetical)\r\n- Desmond et al. 2016 (parenthetical)\r\n- Gau and Brunson 2010 (parenthetical)\r\n- Tyler 2006 (parenthetical in string)\r\n\r\n**Pattern**: Mix of author-subject for foundational work + parenthetical string for synthesis\r\n```\r\n\r\n### 4. Sequence for Flow\r\n\r\nReview the paragraph sequence to ensure:\r\n\r\n**Logical progression**:\r\n- Context  Theory  Synthesis  Gap  Questions\r\n- Each paragraph builds on what precedes\r\n\r\n**Smooth transitions**:\r\n- What transition marker will open each paragraph?\r\n- How does each paragraph connect to the previous?\r\n\r\n**Turn positioning**:\r\n- Is the turn in the right place?\r\n- Does adequate synthesis precede it?\r\n- Do questions follow naturally?\r\n\r\n### 5. Check Cluster Alignment\r\n\r\nVerify the paragraph sequence matches cluster expectations:\r\n\r\n| Cluster | Expected Sequence |\r\n|---------|------------------|\r\n| **Gap-Filler** | CONTEXT  SYNTHESIZE  SYNTHESIZE  GAP  QUESTIONS |\r\n| **Theory-Extender** | CONTEXT  DESCRIBE  SYNTHESIZE  GAP  BRIDGE  QUESTIONS |\r\n| **Concept-Builder** | CONTEXT  DESCRIBE  CRITIQUE  DESCRIBE  SYNTHESIZE+INTRO  DESCRIBE (new)  QUESTIONS |\r\n| **Synthesis** | DESCRIBE(A)  SYNTHESIZE(A)  DESCRIBE(B)  SYNTHESIZE(B)  BRIDGE  QUESTIONS |\r\n| **Problem-Driven** | DESCRIBE(1)  CONTRAST(2)  CRITIQUE  QUESTIONS |\r\n\r\n### 6. Write Paragraph Map\r\n\r\nCreate `paragraph-map.md`:\r\n\r\n```markdown\r\n# Paragraph Map\r\n\r\n## Overview\r\n- Total paragraphs: [N]\r\n- Total target citations: [N]\r\n- Turn location: Paragraph [N]\r\n\r\n---\r\n\r\n## Paragraph 1\r\n**Function**: PROVIDE_CONTEXT\r\n**Subsection**: [if applicable]\r\n**Topic sentence**: \"[Draft opening]\"\r\n**Content notes**: [What this paragraph covers]\r\n**Citations (N)**:\r\n- [Source 1] (pattern)\r\n- [Source 2] (pattern)\r\n**Transition to next**: [How it connects to P2]\r\n\r\n---\r\n\r\n## Paragraph 2\r\n**Function**: [...]\r\n...\r\n\r\n[Continue for all paragraphs]\r\n\r\n---\r\n\r\n## Paragraph [N]: THE TURN\r\n**Function**: IDENTIFY_GAP\r\n**Topic sentence**: \"[Draft turn sentence]\"\r\n**Turn structure**:\r\n1. Acknowledge: [what we know]\r\n2. Pivot: [contrastive marker]\r\n3. Gap: [specific gap claim]\r\n4. Connect: [link to your study]\r\n**Citations (N)**:\r\n- [Sources supporting the gap claim]\r\n\r\n---\r\n\r\n## Summary Table\r\n\r\n| P# | Function | Subsection | Citations | Key Sources |\r\n|----|----------|------------|-----------|-------------|\r\n| 1 | CONTEXT |  | 3 | [list] |\r\n| 2 | SYNTHESIZE |  | 5 | [list] |\r\n...\r\n```\r\n\r\n### 7. Pre-Draft Checklist\r\n\r\nBefore moving to drafting, verify:\r\n\r\n- [ ] Every paragraph has an assigned function\r\n- [ ] Topic sentences drafted for all paragraphs\r\n- [ ] Citations planned (2.4-5.0 per paragraph)\r\n- [ ] Total citations in target range for cluster\r\n- [ ] Turn sentence drafted\r\n- [ ] Sequence flows logically\r\n- [ ] Transitions planned\r\n- [ ] Cluster sequence alignment checked\r\n\r\n---\r\n\r\n## Guiding Principles\r\n\r\n### Topic Sentences Do the Heavy Lifting\r\nA reader should be able to understand the section by reading only the topic sentences. If they can't, your structure isn't clear.\r\n\r\n### Citation Deployment is Strategic\r\nDon't just sprinkle citations randomly. Plan which sources do which work.\r\n\r\n### The Paragraph Map is Your Drafting Guide\r\nIn Phase 3, you'll draft one paragraph at a time following this map. The better the map, the smoother the drafting.\r\n\r\n### Leave Room for Discovery\r\nYou may discover during drafting that a paragraph needs to split, or two should merge. That's finethe map is a guide, not a prison.\r\n\r\n---\r\n\r\n## Output Files to Create\r\n\r\n1. **paragraph-map.md** - Full paragraph-by-paragraph plan\r\n2. **citation-plan.md** - Citation inventory with deployment strategy\r\n\r\n---\r\n\r\n## When You're Done\r\n\r\nReport to the orchestrator:\r\n- Number of planned paragraphs\r\n- Turn placement (paragraph number)\r\n- Total planned citations\r\n- Any concerns about coverage or sequence\r\n\r\nExample summary:\r\n> \"Paragraph map complete. **10 paragraphs** planned: 2 context, 3 framework, 2 synthesis, 1 gap (turn at P6), 2 contribution. **38 citations** distributed across paragraphs (avg 3.8/paragraph). Turn drafts specific gap about guest mother identity in doubled-up households. Ready for Phase 3: Drafting.\"\r\n",
        "plugins/lit-writeup/skills/lit-writeup/phases/phase3-drafting.md": "# Phase 3: Drafting\r\n\r\n## Why This Phase Matters\r\n\r\nThis is where planning becomes prose. The quality of your Theory section depends on sentence-level crafthow you open paragraphs, integrate citations, calibrate hedging, and build toward the turn. This phase transforms the paragraph map into publication-ready prose.\r\n\r\n---\r\n\r\n## Your Tasks\r\n\r\n### 1. Draft by Paragraph\r\n\r\nWork through the paragraph map one paragraph at a time. For each:\r\n\r\n**Step 1: Open with purpose**\r\n- Use the planned topic sentence (or refine it)\r\n- Match opening type to function (see `techniques/sentence-toolbox.md`)\r\n\r\n**Step 2: Build the paragraph**\r\n- 4-7 sentences typical (129-165 words target)\r\n- Each sentence advances the paragraph's function\r\n- Vary sentence length and structure\r\n\r\n**Step 3: Integrate citations**\r\n- Follow the planned citation deployment\r\n- Use appropriate patterns (parenthetical, author-subject, string)\r\n- See `techniques/citation-patterns.md`\r\n\r\n**Step 4: Calibrate hedging**\r\n- Hedge predictions and mechanism claims\r\n- Assert definitions and gap statements\r\n- See hedging guide in `techniques/sentence-toolbox.md`\r\n\r\n**Step 5: Transition out**\r\n- Final sentence should set up the next paragraph\r\n- Use transitional phrases or logical connectors\r\n\r\n### 2. Draft by Function\r\n\r\nApply function-specific guidance:\r\n\r\n#### PROVIDE_CONTEXT Paragraphs\r\n- Open with phenomena, not scholars\r\n- Use statistics, trends, or historical developments\r\n- Keep citations minimal (2-3)\r\n- Establish stakes or scope\r\n\r\n**Example**:\r\n> As housing costs have outpaced wage growth, many parents face difficulty housing their families. Over 3 million households with children doubled up with friends or family in 2019, and doubling-up rates have risen since the Great Recession (Pilkauskas et al. 2020). Low-income and Black families are disproportionately likely to double up, often as a strategy to avoid homelessness or manage poverty (Desmond 2016; Harvey 2018). These arrangements affect household composition, family relationships, and parenting dynamics in ways that remain poorly understood.\r\n\r\n#### DESCRIBE_THEORY Paragraphs\r\n- Name the framework/theorist explicitly\r\n- Explain how the concept works\r\n- Include dimensions, mechanisms, or processes\r\n- Use author-subject citations for canonical sources\r\n\r\n**Example**:\r\n> The concept of \"recognition\" draws on several conceptual traditions. According to Honneth (2003), recognition operates through three spheres: love (intimate relationships), rights (legal status), and esteem (social value). When individuals are denied recognitionthrough disrespect, exclusion, or degradationthey experience psychological harm and may struggle to develop positive self-relations (Taylor 1994). Lamont and colleagues (2016) have extended this framework to examine how low-status groups pursue recognition in everyday life, showing how stigmatized individuals actively resist misrecognition through boundary work and collective claims-making.\r\n\r\n#### SYNTHESIZE Paragraphs\r\n- Aggregate findings, don't summarize individual studies\r\n- Use parenthetical strings efficiently\r\n- Build toward a cumulative claim\r\n- End with \"what we know\" before the gap\r\n\r\n**Example**:\r\n> Research on legal cynicism demonstrates that distrust of legal institutions is patterned by neighborhood context, race, and prior contact with police (Kirk and Papachristos 2011; Sampson and Bartusch 1998). Studies consistently find that residents of high-crime neighborhoods express cynicism about police responsiveness and fairness, even as they continue to call 911 for emergencies (Desmond et al. 2016; Gau and Brunson 2010). This literature highlights the coexistence of cynicism and reliancea paradox that existing frameworks have struggled to explain.\r\n\r\n#### IDENTIFY_GAP Paragraphs\r\n- Use the 4-part turn structure (see `techniques/turn-formula.md`)\r\n- Be specific about what's unknown\r\n- Connect directly to your study\r\n\r\n**Example**:\r\n> This literature highlights the importance of recognition for understanding how marginalized groups engage with institutions. Yet while research documents the pursuit of recognition in various settings, there is less attention to how recognition-seeking shapes encounters with legal authorities specifically. How do residents of high-crime neighborhoods understand their calls to police as demands for recognition? What meaning-making processes allow them to reconcile cynicism with reliance? These questions remain largely unexplored.\r\n\r\n#### STATE_QUESTIONS Paragraphs\r\n- Articulate research questions clearly\r\n- Connect questions to the gap\r\n- Preview methods briefly if appropriate\r\n\r\n**Example**:\r\n> This study examines how mothers in doubled-up households negotiate identity and dignity in shared living arrangements. Three questions guide the analysis: How do guest mothers understand their position within host households? What strategies do they use to maintain maternal identity when authority is constrained? How do these negotiations vary by the character of the host-guest relationship? I address these questions through in-depth interviews with 47 mothers who doubled up with family or friends in the Chicago metropolitan area.\r\n\r\n### 3. Draft Subsection by Subsection\r\n\r\nIf using subsections:\r\n- Draft complete subsection before moving to next\r\n- Ensure internal coherence within subsection\r\n- Check that subsection accomplishes its architectural purpose\r\n\r\nAfter each subsection, pause for user review if appropriate.\r\n\r\n### 4. Maintain Cluster Style\r\n\r\nAs you draft, ensure prose style matches cluster:\r\n\r\n| Cluster | Style Markers |\r\n|---------|---------------|\r\n| **Gap-Filler** | Efficient synthesis, sharp turn, minimal elaboration |\r\n| **Theory-Extender** | Framework exposition, named theorist prominent |\r\n| **Concept-Builder** | Definitional precision, confident terminology |\r\n| **Synthesis** | Even treatment of traditions, bridging language |\r\n| **Problem-Driven** | Even-handed debate presentation OR heavy context |\r\n\r\n### 5. Track Progress\r\n\r\nAs you draft, note:\r\n- Actual word count per paragraph\r\n- Actual citations per paragraph\r\n- Any deviations from the plan\r\n- Paragraphs that need revision\r\n\r\n---\r\n\r\n## Guiding Principles\r\n\r\n### Draft, Don't Perfect\r\nGet prose on the page. Revision comes in Phases 4-5.\r\n\r\n### Follow the Map\r\nThe paragraph map exists for a reason. Stick to it unless you discover a structural problem.\r\n\r\n### Topic Sentences Are Anchors\r\nIf you're struggling with a paragraph, return to the topic sentence. What is this paragraph trying to accomplish?\r\n\r\n### Vary Citation Integration\r\nDon't use the same pattern throughout. Mix parenthetical, author-subject, and (sparingly) quote-then-cite.\r\n\r\n### Read Aloud\r\nProse that sounds awkward when read aloud needs revision. Academic writing should still flow.\r\n\r\n### The Turn Is Sacred\r\nDraft the turn with special care. It's the most important paragraph.\r\n\r\n---\r\n\r\n## Common Drafting Problems\r\n\r\n| Problem | Solution |\r\n|---------|----------|\r\n| Paragraph too long | Split into two; each should have one main function |\r\n| Paragraph too short | Develop the idea more; add supporting material |\r\n| Literature parade | Synthesize, don't list; what do the studies collectively show? |\r\n| Missing topic sentence | Add clear opening that signals function |\r\n| Buried turn | Move turn sentence to start of paragraph; add contrastive marker |\r\n| Hedging definitions | Definitions are assertions; remove \"may,\" \"might\" |\r\n| Over-asserting mechanisms | Mechanisms are claims; add appropriate hedging |\r\n\r\n---\r\n\r\n## Output Files to Create\r\n\r\n1. **theory-section-draft.md** - Full draft prose organized by subsection\r\n2. **drafting-notes.md** - Notes on deviations, concerns, needs for revision\r\n\r\n---\r\n\r\n## When You're Done\r\n\r\nReport to the orchestrator:\r\n- Draft complete (yes/no)\r\n- Word count achieved\r\n- Citation count achieved\r\n- Turn drafted (provide sentence)\r\n- Any structural changes from plan\r\n- Areas flagged for revision\r\n\r\nExample summary:\r\n> \"**Draft complete**. 1,487 words across 10 paragraphs (target: 1,500). 36 citations (target: 35). Turn at paragraph 6: 'Yet while research documents the pursuit of recognition in various settings, there is less attention to how recognition-seeking shapes encounters with legal authorities specifically.' One deviation: Split planned paragraph 4 into two for clarity. Flagged paragraphs 7-8 for transition smoothing in revision. Ready for Phase 4: Turn refinement.\"\r\n",
        "plugins/lit-writeup/skills/lit-writeup/phases/phase4-turn.md": "# Phase 4: Turn Refinement\r\n\r\n## Why This Phase Matters\r\n\r\nThe turn is the rhetorical center of your Theory sectionthe moment you pivot from \"what we know\" to \"what we don't know\" and create space for your contribution. A weak turn undermines the entire section; a strong turn makes your contribution memorable. This phase ensures the turn is specific, compelling, and tightly connected to your research questions.\r\n\r\n---\r\n\r\n## Your Tasks\r\n\r\n### 1. Extract Current Turn\r\n\r\nFrom the draft, identify:\r\n- The turn paragraph(s)\r\n- The specific turn sentence(s)\r\n- The sentence before the turn (what it acknowledges)\r\n- The sentence after the turn (how it connects to your study)\r\n\r\n### 2. Evaluate Against the 4-Part Formula\r\n\r\nCheck that the turn includes all four elements:\r\n\r\n| Part | Function | Your Turn |\r\n|------|----------|-----------|\r\n| **Acknowledge** | What we know | [Extract] |\r\n| **Pivot** | Contrastive marker | [Extract] |\r\n| **Gap** | What we don't know | [Extract] |\r\n| **Connect** | Your study | [Extract] |\r\n\r\nIf any part is weak or missing, draft improvements.\r\n\r\n### 3. Test Gap Specificity\r\n\r\nThe gap must be specific, not generic. Evaluate:\r\n\r\n**Weak (Generic) Gap Tests**:\r\n- \"More research is needed\"  Too vague\r\n- \"We don't fully understand\"  What specifically?\r\n- \"This area is understudied\"  Which aspect?\r\n- \"Scholars have not examined\"  What exactly?\r\n\r\n**Strong (Specific) Gap Tests**:\r\n- Specifies WHAT is unknown\r\n- Specifies AMONG WHOM (population)\r\n- Specifies UNDER WHAT CONDITIONS (context)\r\n- Explains WHY THIS MATTERS\r\n\r\n**Specificity Template**:\r\n> \"We know little about **[what specifically]** among **[which population]** when **[under what conditions]** despite **[why this matters]**.\"\r\n\r\n### 4. Check Gap-Question Alignment\r\n\r\nThe gap must directly motivate your research questions.\r\n\r\n**Alignment Test**: Can you draw a straight line from gap to question?\r\n\r\n| Gap Claim | Research Question | Aligned? |\r\n|-----------|------------------|----------|\r\n| \"We know little about X among Y\" | \"How do Y experience X?\" |  Yes |\r\n| \"We know little about X\" | \"Why does Z happen?\" |  No |\r\n\r\nIf misaligned, revise either the gap or the questions.\r\n\r\n### 5. Calibrate Turn Confidence\r\n\r\nThe gap claim should be assertive (you're stating what the literature has NOT done), but not overclaimed.\r\n\r\n**Appropriate Confidence**:\r\n> \"Yet we know little about...\"\r\n> \"However, less attention has been paid to...\"\r\n> \"Scholars have not examined...\"\r\n\r\n**Over-Claimed (Avoid)**:\r\n> \"No one has ever studied...\"\r\n> \"The literature completely ignores...\"\r\n> \"We know nothing about...\"\r\n\r\n**Under-Hedged Definition (Also Avoid)**:\r\n> \"It may be that we don't fully understand...\"\r\n> \"Perhaps less attention has been paid to...\"\r\n\r\n### 6. Evaluate Turn Placement\r\n\r\nIs the turn in the right location?\r\n\r\n| Cluster | Expected Placement | Paragraph Location |\r\n|---------|-------------------|-------------------|\r\n| Gap-Filler | Middle | P3-4 of ~6 |\r\n| Theory-Extender | Middle | P4-5 of ~8 |\r\n| Concept-Builder | Middle | After critique, before new concept |\r\n| Synthesis | Middle | After both traditions |\r\n| Problem-Driven | Variable | Early for debate, late for empirical |\r\n\r\n**Too Early**: Gap feels unmotivated; insufficient synthesis precedes\r\n**Too Late**: Reader lost the thread; synthesis felt aimless\r\n**Just Right**: Gap emerges naturally from what precedes\r\n\r\n### 7. Refine the Turn Sentence\r\n\r\nCraft the turn sentence with precision:\r\n\r\n**Turn Sentence Structure**:\r\n```\r\n[Acknowledge prior work]. [Pivot marker], [specific gap claim]. [Connect to your study].\r\n```\r\n\r\n**Example Refinement**:\r\n\r\nOriginal (weak):\r\n> \"This literature is important, but there is still more to learn. This study examines mothers in doubled-up households.\"\r\n\r\nRevised (strong):\r\n> \"This literature highlights the importance of housing context for family identity work. Yet while research documents how housing instability affects parenting, there is less attention to how mothers in doubled-up arrangements manage maternal identity when they lack spatial authority. This study examines how guest mothers negotiate identity and dignity in shared living arrangements.\"\r\n\r\n### 8. Test the Turn\r\n\r\nRun these diagnostic questions:\r\n\r\n1. **Specificity**: Could this gap apply to many other studies, or is it specific to mine?\r\n2. **Motivation**: Does the synthesis before genuinely build toward this gap?\r\n3. **Connection**: Do my research questions directly address this gap?\r\n4. **Placement**: Does the turn fall at the right structural moment?\r\n5. **Confidence**: Is the gap claim appropriately assertive?\r\n6. **Novelty**: Is this gap genuinely unstudied, or am I overclaiming?\r\n\r\n### 9. Write Turn Refinement Memo\r\n\r\nCreate `turn-memo.md`:\r\n\r\n```markdown\r\n# Turn Refinement\r\n\r\n## Original Turn\r\n[Paste original turn paragraph]\r\n\r\n## Analysis\r\n\r\n### 4-Part Structure\r\n- Acknowledge: [assessment]\r\n- Pivot: [assessment]\r\n- Gap: [assessment]\r\n- Connect: [assessment]\r\n\r\n### Specificity Assessment\r\n[Is the gap specific enough? What would make it more specific?]\r\n\r\n### Gap-Question Alignment\r\n[Do questions directly follow from gap?]\r\n\r\n### Placement Assessment\r\n[Is turn in right location? Should it move?]\r\n\r\n## Refined Turn\r\n\r\n[Full refined paragraph]\r\n\r\n### Turn Sentence (Isolated)\r\n> \"[The core turn sentence]\"\r\n\r\n## Changes Made\r\n- [List specific changes]\r\n\r\n## Context Adjustments Needed\r\n[Any changes to paragraphs before/after the turn]\r\n```\r\n\r\n---\r\n\r\n## Guiding Principles\r\n\r\n### The Turn Is Your Thesis\r\nIf someone asks \"What's your contribution?\", you should be able to point to your turn sentence.\r\n\r\n### Specificity > Scope\r\nA narrow, specific gap is better than a broad, vague one. \"We know little about X among Y in Z context\" beats \"This area is understudied.\"\r\n\r\n### The Turn Isn't a Surprise\r\nThe synthesis before should build toward the gap. Readers should sense what's coming.\r\n\r\n### The Turn Isn't Buried\r\nStart the turn paragraph with the pivot. Don't bury the gap in the middle of a paragraph.\r\n\r\n### One Gap, Not Many\r\nFocus on the single gap your study addresses. Multiple gaps dilute the contribution.\r\n\r\n---\r\n\r\n## Common Turn Problems\r\n\r\n| Problem | Symptom | Solution |\r\n|---------|---------|----------|\r\n| Generic gap | \"More research needed\" | Specify what, among whom, why |\r\n| Multiple gaps | Lists 3-4 unknowns | Focus on one |\r\n| Misaligned | Gap doesn't match questions | Revise gap or questions |\r\n| Overclaimed | \"No one has studied\" | Acknowledge adjacent work |\r\n| Buried | Pivot in middle of paragraph | Move to paragraph opening |\r\n| Too late | Turn in final paragraph | Move earlier |\r\n| Weak pivot | No contrastive marker | Add \"However,\" \"Yet\" |\r\n\r\n---\r\n\r\n## Output Files to Create\r\n\r\n1. **turn-memo.md** - Analysis and refined turn\r\n2. **theory-section-draft-v2.md** - Updated draft with refined turn\r\n\r\n---\r\n\r\n## When You're Done\r\n\r\nReport to the orchestrator:\r\n- Original turn assessment (strong/weak/specific issues)\r\n- Key refinements made\r\n- Final turn sentence\r\n- Gap-question alignment verified\r\n- Placement confirmed\r\n\r\nExample summary:\r\n> \"Turn refined. Original was generic ('housing dynamics remain understudied'). Refined to: **'Yet while research documents how housing instability affects parenting practices, there is less attention to how mothers in doubled-up arrangements manage maternal identity when they lack spatial authority over their children.'** Gap now specifies: (1) maternal identity (what), (2) guest mothers (who), (3) shared households without spatial authority (conditions). Directly motivates RQ1 about identity negotiation. Placement confirmed at P6 of 10. Ready for Phase 5: Revision.\"\r\n",
        "plugins/lit-writeup/skills/lit-writeup/phases/phase5-revision.md": "# Phase 5: Revision\r\n\r\n## Why This Phase Matters\r\n\r\nGood writing is rewriting. The draft from Phase 3 and the refined turn from Phase 4 need polishing: checking calibration against norms, smoothing transitions, ensuring consistency, and achieving prose quality worthy of publication. This phase transforms a competent draft into a compelling Theory section.\r\n\r\n---\r\n\r\n## Your Tasks\r\n\r\n### 1. Calibration Check\r\n\r\nCompare draft against benchmarks from `techniques/calibration-norms.md`:\r\n\r\n| Metric | Target (IQR) | Your Draft | Status |\r\n|--------|-------------|------------|--------|\r\n| Word count | 1,145-1,744 | [actual] | [ ] OK / [ ] Adjust |\r\n| Paragraphs | 7-12 | [actual] | [ ] OK / [ ] Adjust |\r\n| Citations | 26-43 | [actual] | [ ] OK / [ ] Adjust |\r\n| Citations/paragraph | 2.4-5.0 | [actual] | [ ] OK / [ ] Adjust |\r\n| Subsections | 1-3 | [actual] | [ ] OK / [ ] Adjust |\r\n\r\n**If outside target range**:\r\n- Under word count: Expand synthesis or add context\r\n- Over word count: Cut redundancy, tighten prose\r\n- Under-cited: Add supporting sources\r\n- Over-cited: Combine into strings, cut peripheral sources\r\n\r\n### 2. Paragraph Function Audit\r\n\r\nFor each paragraph, verify:\r\n\r\n1. **Clear function**: Can you identify what this paragraph is doing (CONTEXT, THEORY, SYNTHESIZE, etc.)?\r\n2. **Topic sentence signals function**: Does the opening make the function clear?\r\n3. **Function fits position**: Is this function appropriate for this location in the sequence?\r\n4. **Function matches cluster**: Is this function used appropriately for your cluster?\r\n\r\nFlag any paragraph where function is unclear or misaligned.\r\n\r\n### 3. Transition Check\r\n\r\nBetween every paragraph pair, verify:\r\n\r\n- **Logical connection**: Does paragraph N+1 follow from paragraph N?\r\n- **Explicit transition**: Is there a transition marker or connecting phrase?\r\n- **No abrupt shifts**: Reader shouldn't feel whiplash\r\n\r\nCommon transitions to add:\r\n- Additive: \"Building on this work,\" \"Moreover,\" \"Similarly\"\r\n- Contrastive: \"However,\" \"Yet,\" \"In contrast\"\r\n- Sequential: \"A second body of scholarship,\" \"Additionally\"\r\n- Consequential: \"Thus,\" \"As a result,\" \"Given this\"\r\n\r\n### 4. Citation Pattern Audit\r\n\r\nReview citation integration across the section:\r\n\r\n- **Variety**: Are you using multiple patterns (parenthetical, author-subject, string)?\r\n- **Author-subject for canonicals**: Are foundational theorists given prominence?\r\n- **Efficient strings**: Is consensus established without excessive strings?\r\n- **Quotes only for definitions**: Are you quoting only when language precision matters?\r\n- **No orphan citations**: Every citation should be integrated into prose, not just listed\r\n\r\n### 5. Hedging Calibration\r\n\r\nReview hedging across claim types:\r\n\r\n| Claim Type | Expected Hedging | Check Draft |\r\n|------------|------------------|-------------|\r\n| Reporting prior literature | Low-moderate (\"demonstrates,\" \"shows\") | [ ] |\r\n| Defining concepts | None (\"is,\" \"refers to\") | [ ] |\r\n| Gap claims | Assertive (\"we know little\") | [ ] |\r\n| Research questions | None (state directly) | [ ] |\r\n| Mechanism predictions | Moderate (\"may,\" \"might\") | [ ] |\r\n| Generalizations | Moderate-high (\"suggests,\" \"tends to\") | [ ] |\r\n\r\nFlag any over-hedged definitions or under-hedged mechanism claims.\r\n\r\n### 6. Cluster Style Consistency\r\n\r\nVerify prose style matches cluster throughout:\r\n\r\n| Cluster | Style Check |\r\n|---------|-------------|\r\n| **Gap-Filler** | Is it lean? No unnecessary elaboration? |\r\n| **Theory-Extender** | Is the framework prominent? Named theorist central? |\r\n| **Concept-Builder** | Are definitions confident? Is the new term used consistently? |\r\n| **Synthesis** | Are both traditions treated evenly? Is there a clear bridge? |\r\n| **Problem-Driven** | Is the debate presented fairly? Or is context sufficient? |\r\n\r\n### 7. Prose Polish\r\n\r\nRead the entire section for prose quality:\r\n\r\n- **Sentence variety**: Mix long and short sentences\r\n- **Active voice**: Prefer active constructions where appropriate\r\n- **Concrete language**: Avoid vague abstractions\r\n- **Jargon check**: Is technical terminology necessary and defined?\r\n- **Read aloud**: Does it flow when spoken?\r\n\r\n### 8. Final Checklist\r\n\r\nBefore completion, verify:\r\n\r\n- [ ] Word count in target range\r\n- [ ] Citation count in target range\r\n- [ ] Each paragraph has clear function\r\n- [ ] Topic sentences signal function\r\n- [ ] Transitions between all paragraphs\r\n- [ ] Turn is specific and well-placed\r\n- [ ] Gap directly motivates research questions\r\n- [ ] Hedging calibrated by claim type\r\n- [ ] Citation patterns varied\r\n- [ ] Cluster style consistent throughout\r\n- [ ] Prose reads smoothly\r\n\r\n### 9. Write Quality Memo\r\n\r\nCreate `quality-memo.md`:\r\n\r\n```markdown\r\n# Theory Section Quality Assessment\r\n\r\n## Calibration Results\r\n\r\n| Metric | Target | Actual | Status |\r\n|--------|--------|--------|--------|\r\n| Word count | 1,145-1,744 | [N] | /needs adjustment |\r\n| Paragraphs | 7-12 | [N] | /needs adjustment |\r\n| Citations | 26-43 | [N] | /needs adjustment |\r\n| Citations/paragraph | 2.4-5.0 | [N] | /needs adjustment |\r\n| Subsections | 1-3 | [N] | /needs adjustment |\r\n\r\n## Structural Assessment\r\n\r\n### Paragraph Functions\r\n[Assessment of function clarity and sequencing]\r\n\r\n### Transitions\r\n[Assessment of flow between paragraphs]\r\n\r\n### Turn Quality\r\n[Assessment of turn specificity and placement]\r\n\r\n## Style Assessment\r\n\r\n### Citation Patterns\r\n[Assessment of citation variety and integration]\r\n\r\n### Hedging Calibration\r\n[Assessment of hedging by claim type]\r\n\r\n### Cluster Alignment\r\n[Assessment of style consistency with cluster]\r\n\r\n## Prose Quality\r\n\r\n### Strengths\r\n- [List strong aspects]\r\n\r\n### Areas Addressed in Revision\r\n- [List what was improved]\r\n\r\n### Remaining Considerations\r\n- [Any remaining issues or judgment calls]\r\n\r\n## Final Section Statistics\r\n\r\n- Word count: [N]\r\n- Paragraph count: [N]\r\n- Citation count: [N]\r\n- Subsection count: [N]\r\n- Cluster: [Name]\r\n\r\n## Confidence Assessment\r\n[High/Medium/Low confidence that section is publication-ready]\r\n```\r\n\r\n---\r\n\r\n## Guiding Principles\r\n\r\n### Revision Is Not Line-Editing\r\nThis phase is about structural integrity, calibration, and coherencenot comma placement.\r\n\r\n### When in Doubt, Cut\r\nIf a sentence doesn't advance the argument, remove it.\r\n\r\n### Consistency Trumps Perfection\r\nA consistently good section beats one with brilliant moments and weak transitions.\r\n\r\n### The Section Should Stand Alone\r\nA reader should understand your contribution from the Theory section alone, without reading the rest of the paper.\r\n\r\n---\r\n\r\n## Common Revision Issues\r\n\r\n| Issue | Symptom | Solution |\r\n|-------|---------|----------|\r\n| **Bloat** | Over word count | Cut redundant synthesis, tighten paragraphs |\r\n| **Thin** | Under word count | Expand underdeveloped literature, add context |\r\n| **Choppy** | Abrupt shifts | Add transitions, smooth connections |\r\n| **Monotonous** | Same citation pattern throughout | Vary integration patterns |\r\n| **Over-hedged** | \"May\" and \"might\" everywhere | Assert established findings, definitions |\r\n| **Under-hedged** | Strong claims about mechanisms | Add appropriate caution |\r\n| **Buried turn** | Hard to find the gap | Move turn to paragraph opening |\r\n| **Misaligned structure** | Doesn't match cluster | Restructure to cluster norms |\r\n\r\n---\r\n\r\n## Output Files to Create\r\n\r\n1. **theory-section-final.md** - Final polished Theory section\r\n2. **quality-memo.md** - Full quality assessment\r\n3. **revision-log.md** - Record of changes made\r\n\r\n---\r\n\r\n## When You're Done\r\n\r\nReport to the orchestrator:\r\n- Final word count\r\n- Final citation count\r\n- Calibration status (in range / deviations explained)\r\n- Confidence assessment (High/Medium/Low)\r\n- Any remaining concerns\r\n\r\nExample summary:\r\n> \"**Revision complete**. Final section: 1,523 words (target 1,145-1,744 ), 37 citations (target 26-43 ), 10 paragraphs, 2 subsections. All calibration metrics in range. Turn confirmed specific and well-placed at P6. Transitions smoothed throughout. Hedging calibrated. **High confidence** section is publication-ready. Minor consideration: paragraph 8 bridge could be strengthened if user wants to emphasize theoretical contribution more.\"\r\n",
        "plugins/lit-writeup/skills/lit-writeup/techniques/calibration-norms.md": "# Calibration Norms\r\n\r\nStatistical benchmarks from analysis of 80 Theory sections in interview-based articles from *Social Problems* and *Social Forces*. Use these norms to calibrate your draft against field expectations.\r\n\r\n---\r\n\r\n## Overall Benchmarks\r\n\r\n| Metric | Min | Q25 | Median | Q75 | Max | Mean |\r\n|--------|-----|-----|--------|-----|-----|------|\r\n| **Paragraph Count** | 3 | 7 | 10 | 12 | 32 | 10.8 |\r\n| **Word Count** | 495 | 1,145 | 1,393 | 1,744 | 3,504 | 1,509 |\r\n| **Unique Citations** | 7 | 26 | 35 | 43 | 89 | 35.8 |\r\n| **Citations per Paragraph** | 0.77 | 2.41 | 3.56 | 5.00 | 9.67 | 3.81 |\r\n| **Subsection Count** | 1 | 1 | 2 | 3 | 8 | 2.4 |\r\n| **Words per Paragraph** | 75 | 129 | 145 | 165 | 255 | 148 |\r\n| **Citations per 1,000 Words** | 6.79 | 18.88 | 24.16 | 31.97 | 58.47 | 25.13 |\r\n\r\n---\r\n\r\n## Target Ranges\r\n\r\nUse the interquartile range (Q25-Q75) as your target. Deviations should be intentional.\r\n\r\n### Length\r\n- **Target word count**: 1,145-1,744 words\r\n- **Target paragraphs**: 7-12 paragraphs\r\n- **Under 1,000 words**: May signal insufficient theoretical development\r\n- **Over 2,000 words**: May signal over-elaboration (unless Concept-Builder)\r\n\r\n### Citation Density\r\n- **Target citations per paragraph**: 2.4-5.0\r\n- **Target citations per 1,000 words**: 19-32\r\n- **Under 2 per paragraph**: May signal insufficient engagement\r\n- **Over 6 per paragraph**: May signal listing rather than synthesizing\r\n\r\n### Structure\r\n- **Target subsections**: 1-3\r\n- **0 subsections**: Acceptable for streamlined Gap-Fillers\r\n- **4+ subsections**: Appropriate for Concept-Builders, may signal over-structuring otherwise\r\n\r\n### Paragraph Length\r\n- **Target words per paragraph**: 129-165\r\n- **Under 100 words**: May be underdeveloped\r\n- **Over 200 words**: Consider splitting\r\n\r\n---\r\n\r\n## Cluster-Specific Norms\r\n\r\n### Cluster 1: Gap-Filler Minimalist (27.5%)\r\n\r\n| Metric | Typical Range |\r\n|--------|---------------|\r\n| Word count | 1,000-1,400 |\r\n| Paragraphs | 6-10 |\r\n| Subsections | 0-2 |\r\n| Citations | 25-35 |\r\n| Literature balance | 59% balanced, 23% substantive-heavy |\r\n\r\n**Key**: Leaner than average. Efficient synthesis, sharp turn.\r\n\r\n---\r\n\r\n### Cluster 2: Theory-Extension (22.5%)\r\n\r\n| Metric | Typical Range |\r\n|--------|---------------|\r\n| Word count | 1,300-1,800 |\r\n| Paragraphs | 8-12 |\r\n| Subsections | 1-4 |\r\n| Citations | 30-45 |\r\n| Literature balance | 78% theory-heavy |\r\n\r\n**Key**: Moderate length. Higher citation density in framework exposition.\r\n\r\n---\r\n\r\n### Cluster 3: Concept-Builder (15.0%)\r\n\r\n| Metric | Typical Range |\r\n|--------|---------------|\r\n| Word count | 1,600-2,500+ |\r\n| Paragraphs | 12-20+ |\r\n| Subsections | 3-5+ |\r\n| Citations | 40-70+ |\r\n| Literature balance | 92% theory-heavy |\r\n\r\n**Key**: Longest of all clusters. Space needed for conceptual apparatus.\r\n\r\n---\r\n\r\n### Cluster 4: Synthesis Integrator (18.8%)\r\n\r\n| Metric | Typical Range |\r\n|--------|---------------|\r\n| Word count | 1,300-1,800 |\r\n| Paragraphs | 10-14 |\r\n| Subsections | 2-4 |\r\n| Citations | 35-50 |\r\n| Literature balance | 67% theory-heavy, 33% balanced |\r\n\r\n**Key**: Middle ground. Citations drawn from multiple traditions.\r\n\r\n---\r\n\r\n### Cluster 5: Problem-Driven (16.3%)\r\n\r\n| Metric | Typical Range |\r\n|--------|---------------|\r\n| Word count | 1,000-1,600 |\r\n| Paragraphs | 6-12 |\r\n| Subsections | 0-3 |\r\n| Citations | 20-40 |\r\n| Literature balance | 46% balanced, 38% substantive-heavy |\r\n\r\n**Key**: Variable structure. More empirical/policy citations.\r\n\r\n---\r\n\r\n## Literature Balance Definitions\r\n\r\n| Category | Definition | Typical For |\r\n|----------|------------|-------------|\r\n| **Theory-Heavy** | 2/3+ of paragraphs primarily theoretical | Concept-Builder, Theory-Extender |\r\n| **Balanced** | Roughly equal theory and substantive | Gap-Filler, Synthesis |\r\n| **Substantive-Heavy** | 2/3+ empirical/contextual | Problem-Driven (Empirical) |\r\n\r\n### Balance by Cluster\r\n\r\n| Cluster | Theory-Heavy | Balanced | Substantive-Heavy |\r\n|---------|--------------|----------|-------------------|\r\n| Gap-Filler | 18% | 59% | 23% |\r\n| Theory-Extension | 78% | 22% | 0% |\r\n| Concept-Builder | 92% | 8% | 0% |\r\n| Synthesis | 67% | 33% | 0% |\r\n| Problem-Driven | 15% | 46% | 38% |\r\n\r\n---\r\n\r\n## Subsection Interpretation\r\n\r\n| Subsection Count | Signal |\r\n|------------------|--------|\r\n| **0** | Streamlined, empirically-focused |\r\n| **1-2** | Organized literature review |\r\n| **3-4** | Multiple theoretical components |\r\n| **5+** | Building theoretical apparatus |\r\n\r\n**Rule**: Match structure to ambition. Gap-Fillers with 5 subsections confuse readers.\r\n\r\n---\r\n\r\n## Visual Elements\r\n\r\nOnly 8.8% of articles include tables or figures in the Theory section.\r\n\r\n| Has Visual | Cluster Association |\r\n|------------|---------------------|\r\n| **Table** | Concept-Builders presenting typologies/frameworks |\r\n| **Figure** | Concept-Builders presenting process models |\r\n| **None** | All other clusters (92%) |\r\n\r\n**Guideline**: Only include a visual if you're introducing a new framework worth visualizing.\r\n\r\n---\r\n\r\n## Outlier Analysis\r\n\r\n### Unusually Long (2,500+ words)\r\n- Typically Concept-Builders\r\n- Often have 5+ subsections\r\n- Usually 60+ citations\r\n- Justified when building substantial theoretical apparatus\r\n\r\n### Unusually Short (Under 800 words)\r\n- Rare in these journals\r\n- May work for tightly focused empirical documenters\r\n- Risk signaling insufficient theoretical engagement\r\n\r\n### Citation Outliers (70+ citations)\r\n- Strong correlation with Concept-Builder cluster\r\n- Often involve synthesizing multiple literatures\r\n- Requires space to justify\r\n\r\n---\r\n\r\n## Calibration Checklist\r\n\r\nBefore finalizing, verify your draft against these norms:\r\n\r\n| Metric | Your Draft | Target (for cluster) | Status |\r\n|--------|------------|---------------------|--------|\r\n| Word count | ___ | ___ - ___ | [ ] OK / [ ] Adjust |\r\n| Paragraphs | ___ | ___ - ___ | [ ] OK / [ ] Adjust |\r\n| Subsections | ___ | ___ - ___ | [ ] OK / [ ] Adjust |\r\n| Total citations | ___ | ___ - ___ | [ ] OK / [ ] Adjust |\r\n| Citations/paragraph | ___ | 2.4 - 5.0 | [ ] OK / [ ] Adjust |\r\n| Literature balance | ___ | ___ | [ ] OK / [ ] Adjust |\r\n\r\n**If outside target range**: Intentional deviation is fine, but you should be able to justify why your section needs to be longer/shorter/more or less citation-dense than typical.\r\n\r\n---\r\n\r\n## Interpreting Deviations\r\n\r\n| Deviation | Possible Reasons | Action |\r\n|-----------|------------------|--------|\r\n| **Too short** | Insufficient synthesis; rushed turn | Expand literature engagement |\r\n| **Too long** | Over-elaboration; catalog style | Cut redundancy; sharpen argument |\r\n| **Under-cited** | Thin engagement; missed literature | Add sources; deepen synthesis |\r\n| **Over-cited** | Listing not synthesizing | Aggregate findings; cut citation strings |\r\n| **Too many subsections** | Over-structuring simple argument | Merge or eliminate |\r\n| **Too few subsections** | Complex argument needs organization | Add structure for clarity |\r\n",
        "plugins/lit-writeup/skills/lit-writeup/techniques/citation-patterns.md": "# Citation Integration Patterns\r\n\r\nHow you integrate citations affects readability and signals your relationship to the literature. This guide covers the four primary patterns and when to use each.\r\n\r\n---\r\n\r\n## The Four Patterns\r\n\r\n| Pattern | Frequency | Signature | Best For |\r\n|---------|-----------|-----------|----------|\r\n| **Parenthetical Support** | 50-55% | \"Research shows X (Author Year).\" | Synthesis, efficient documentation |\r\n| **Author-as-Subject** | 20-25% | \"Author (Year) argues that...\" | Canonical theorists, foundational works |\r\n| **Citation Strings** | 15-20% | \"(A 2020; B 2019; C 2018)\" | Establishing consensus, breadth |\r\n| **Quote-then-Cite** | 5-10% | \"As X notes, '...' (Year:page)\" | Key definitions, precise language |\r\n\r\n---\r\n\r\n## Pattern 1: Parenthetical Support (50-55%)\r\n\r\nThe most common pattern. The claim comes first; citations provide evidence without interrupting prose.\r\n\r\n### Structure\r\n> [Claim] (Author Year).\r\n> [Claim] (Author Year; Author Year).\r\n\r\n### Examples\r\n> \"Research on legal cynicism demonstrates that distrust of legal institutions is patterned by neighborhood context and race (Kirk and Papachristos 2011).\"\r\n\r\n> \"Mass incarceration has left thousands of Black and Brown men under 'mass supervision' through probation and parole (McNeil and Beyens 2013, 3).\"\r\n\r\n> \"Conspiracy beliefs have long proliferated in the United States (Bailyn 2017; Waters 1997) and beyond (Imhoff et al. 2022; Swami 2012).\"\r\n\r\n### When to Use\r\n- Synthesizing multiple studies\r\n- Efficient documentation of established findings\r\n- Building cumulative case without interruption\r\n- Most claims in most paragraphs\r\n\r\n### Strengths\r\n- Keeps focus on the content, not the author\r\n- Allows efficient synthesis of multiple sources\r\n- Maintains prose flow\r\n\r\n---\r\n\r\n## Pattern 2: Author-as-Subject (20-25%)\r\n\r\nThe cited author performs the intellectual action. Gives prominence to the scholar.\r\n\r\n### Structure\r\n> [Author] (Year) [argues/demonstrates/shows/conceptualizes] that...\r\n> According to [Author]...\r\n> [Author's] (Year) [concept/framework]...\r\n\r\n### Examples\r\n> \"Connell (2005) conceptualizes hegemonic masculinity as the most honorable expression of manliness within a hierarchy of masculinities.\"\r\n\r\n> \"According to Puwar (2004), numerical representation matters, but so does the construction of race and gender within larger social status hierarchies.\"\r\n\r\n> \"Du Bois so clearly explicated that individual agency, the imagined range of actions available to the self, was shaped through the ongoing reconciliation of consciousness.\"\r\n\r\n### When to Use\r\n- Introducing canonical theorists (Bourdieu, Goffman, Foucault)\r\n- Foundational works you're building on or extending\r\n- When the *who* matters as much as the *what*\r\n- Theory-Extension clusters (prominent for named framework)\r\n\r\n### Strengths\r\n- Signals which scholars are central to your argument\r\n- Shows theoretical lineage\r\n- Appropriate respect for foundational contributions\r\n\r\n### Caution\r\n- Overuse makes prose feel like a literature parade\r\n- Reserve for genuinely important sources\r\n\r\n---\r\n\r\n## Pattern 3: Citation Strings (15-20%)\r\n\r\nMultiple citations bundled together, signaling consensus or breadth.\r\n\r\n### Structure\r\n> [Claim] (Author Year; Author Year; Author Year).\r\n> [Claim] (see Author Year; Author Year; cf. Author Year).\r\n> [Claim] (e.g., Author Year; Author Year).\r\n\r\n### Examples\r\n> \"Scholarship on business-elite influence on populist movements links opposition to climate reform to climate denial and other far-right ideals (Dunlap and McCright 2015; Brulle 2018; Hertel-Fernandez 2019).\"\r\n\r\n> \"...the sociology of law, which demonstrates that legality is embedded in a complex set of cultural schemas (Ewick and Silbey 1998; Silbey 2005; Nielsen 2000).\"\r\n\r\n### String Modifiers\r\n- **\"see\"**: Points to useful sources: \"(see Smith 2019; Jones 2020)\"\r\n- **\"e.g.\"**: Indicates examples, not exhaustive: \"(e.g., Brown 2018)\"\r\n- **\"cf.\"**: Indicates comparison or contrast: \"(cf. alternative view in Lee 2017)\"\r\n\r\n### When to Use\r\n- Establishing that a finding is robust across studies\r\n- Showing breadth of a literature\r\n- Documenting consensus\r\n- Gap-Filler clusters (efficient synthesis)\r\n\r\n### Strengths\r\n- Demonstrates thorough literature engagement\r\n- Establishes consensus quickly\r\n- Shows pattern holds across multiple contexts\r\n\r\n### Caution\r\n- Long strings can feel like padding\r\n- More than 5-6 citations starts to look excessive\r\n- Make sure the claim actually applies to all cited sources\r\n\r\n---\r\n\r\n## Pattern 4: Quote-then-Cite (5-10%)\r\n\r\nDirect quotation with attribution. Use sparingly and strategically.\r\n\r\n### Structure\r\n> As [Author] notes, \"...\" (Year:page).\r\n> [Author] describes [concept] as \"...\" (Year:page).\r\n> \"[Quote]\" ([Author] Year:page).\r\n\r\n### Examples\r\n> \"Legal cynicism is broadly conceived of as 'a *cultural orientation* in which the law and the agents of its enforcement are viewed as illegitimate, unresponsive, and ill-equipped to ensure public safety' (Kirk and Matsuda 2011:443).\"\r\n\r\n> \"According to Lamont (2009:158): 'Low-status groups (in this situation) are more likely to be resigned and passive instead of resilient.'\"\r\n\r\n### When to Use\r\n- **Key definitions**: When the original language matters\r\n- **Contested terms**: When precise wording is at stake\r\n- **Foundational formulations**: Classic statements worth preserving\r\n- **Concept-Builder clusters**: Detailed engagement with sources being critiqued or built upon\r\n\r\n### When NOT to Use\r\n- General findings (paraphrase instead)\r\n- Routine claims (use parenthetical)\r\n- To avoid writing your own synthesis (lazy quoting)\r\n\r\n### Strengths\r\n- Precision when language matters\r\n- Shows close reading of sources\r\n- Appropriate for definitions and key formulations\r\n\r\n### Caution\r\n- Overuse makes the section feel like a collage\r\n- Should be rare (5-10% of citations max)\r\n- Always include page numbers for quotes\r\n\r\n---\r\n\r\n## Cluster-Specific Patterns\r\n\r\n| Cluster | Primary Pattern | Secondary Pattern | Notes |\r\n|---------|-----------------|-------------------|-------|\r\n| **Gap-Filler** | Parenthetical (70%) | Citation strings | Minimal author-subject |\r\n| **Theory-Extender** | Author-subject + Parenthetical | Citation strings | Named theorist prominent |\r\n| **Concept-Builder** | Quote-then-cite + Author-subject | Parenthetical | Detailed engagement |\r\n| **Synthesis Integrator** | Mixed across traditions | Bridging citations | Citations from multiple traditions |\r\n| **Problem-Driven** | Parenthetical | Context-heavy | More empirical/policy citations |\r\n\r\n---\r\n\r\n## Citation Density Benchmarks\r\n\r\nBased on analysis of 80 articles:\r\n\r\n| Metric | Median | Target Range (IQR) |\r\n|--------|--------|-------------------|\r\n| **Citations per paragraph** | 3.5 | 2.4-5.0 |\r\n| **Citations per 1,000 words** | 24.2 | 18.9-32.0 |\r\n| **Unique sources** | 35 | 26-43 |\r\n\r\n### What These Mean\r\n\r\n- **2.4-5.0 per paragraph**: Most paragraphs should have 2-5 citations\r\n- **18.9-32.0 per 1,000 words**: A 1,500-word section should have ~30-48 citations\r\n- **Under 26 citations**: May signal insufficient engagement\r\n- **Over 50 citations**: May signal catalog-style rather than argument-style\r\n\r\n---\r\n\r\n## Common Mistakes\r\n\r\n| Mistake | Problem | Solution |\r\n|---------|---------|----------|\r\n| All author-subject | Reads like a literature parade | Mix in parenthetical |\r\n| No author-subject | Misses chance to highlight key theorists | Use for foundational sources |\r\n| Excessive quoting | Looks like you can't synthesize | Paraphrase more; quote definitions only |\r\n| Long citation strings | Feels like padding | Keep to 3-5 per string |\r\n| Underciting | Signals superficial engagement | Aim for 3-5 per paragraph |\r\n| Same pattern throughout | Monotonous prose | Vary deliberately |\r\n\r\n---\r\n\r\n## Integration Checklist\r\n\r\nFor each paragraph, verify:\r\n\r\n1. **Pattern variety**: Are you using multiple patterns across the section?\r\n2. **Author-subject for canonicals**: Are foundational theorists given appropriate prominence?\r\n3. **Efficient strings**: Is consensus established without excessive strings?\r\n4. **Quotes only for definitions**: Are you quoting only when language precision matters?\r\n5. **Citation density**: Does the paragraph fall in the 2.4-5.0 range?\r\n6. **Cluster alignment**: Does your citation pattern match your cluster?\r\n",
        "plugins/lit-writeup/skills/lit-writeup/techniques/paragraph-functions.md": "# Paragraph Functions\r\n\r\nEvery paragraph in a Theory section serves a recognizable function. Readers should be able to identify what each paragraph is *doing* even without subheadings. This guide catalogs the 9 primary paragraph functions.\r\n\r\n---\r\n\r\n## The 9 Functions\r\n\r\n| Function | Purpose | Typical Position |\r\n|----------|---------|------------------|\r\n| **PROVIDE_CONTEXT** | Establish phenomenon, population, or setting | Early; often opens section |\r\n| **DESCRIBE_THEORY** | Explicate a theoretical framework or concept | After context; throughout |\r\n| **SYNTHESIZE** | Summarize patterns across multiple studies | Middle; builds cumulative case |\r\n| **CONTRAST** | Present competing positions or perspectives | Middle; debate framing |\r\n| **CRITIQUE** | Identify limitations of existing approaches | Before gap or new concept |\r\n| **IDENTIFY_GAP** | Mark what remains unknown or unstudied | Middle; the \"turn\" |\r\n| **BRIDGE** | Connect literatures or show how framework applies | After gap; synthesis payoff |\r\n| **STATE_QUESTIONS** | Articulate research questions or previews | Late; often closes section |\r\n| **PREVIEW** | Summarize what's coming in the findings | Final paragraph |\r\n\r\n---\r\n\r\n## Detailed Function Profiles\r\n\r\n### PROVIDE_CONTEXT\r\n\r\n**Purpose**: Ground the reader in the empirical worldthe phenomenon, population, setting, or historical moment under study.\r\n\r\n**Typical markers**:\r\n- Opens with phenomena, not scholars\r\n- Statistics, trends, or historical developments\r\n- \"As [condition] has [changed]...\"\r\n- \"The [phenomenon] affects...\"\r\n- \"In [year/decade], [development]...\"\r\n\r\n**Example paragraph**:\r\n> As housing costs have outpaced wage growth, many parents face difficulty housing their families. Over 3 million households with children doubled up with friends or family in 2019, and doubling-up rates have risen since the Great Recession. Low-income and Black families are disproportionately likely to double up, often as a strategy to avoid homelessness or manage poverty.\r\n\r\n**When to use**: Opening the section; grounding each subsection in empirical reality.\r\n\r\n---\r\n\r\n### DESCRIBE_THEORY\r\n\r\n**Purpose**: Explicate a theoretical framework, concept, or key term. Make the analytical tool available to the reader.\r\n\r\n**Typical markers**:\r\n- Names a theorist, framework, or concept explicitly\r\n- Uses \"According to,\" \"[Scholar] argues,\" \"The concept of [X]...\"\r\n- Explains *how* the concept works, not just what it is\r\n- May include definition, dimensions, mechanisms\r\n\r\n**Example paragraph**:\r\n> The concept of \"recognition\" draws on several conceptual traditions. According to Honneth (2003), recognition operates through three spheres: love (intimate relationships), rights (legal status), and esteem (social value). When individuals are denied recognitionthrough disrespect, exclusion, or degradationthey experience psychological harm and may struggle to develop positive self-relations. Recognition thus serves as both a psychological resource and a political demand.\r\n\r\n**When to use**: After context; whenever introducing theoretical apparatus.\r\n\r\n---\r\n\r\n### SYNTHESIZE\r\n\r\n**Purpose**: Pull together findings from multiple studies to establish patterns, consensus, or accumulated knowledge.\r\n\r\n**Typical markers**:\r\n- Multiple citations in a single paragraph\r\n- \"Studies find...\" \"Research shows...\" \"Scholars have documented...\"\r\n- Aggregates findings rather than describing individual studies\r\n- Builds toward a cumulative claim\r\n\r\n**Example paragraph**:\r\n> Research on legal cynicism demonstrates that distrust of legal institutions is patterned by neighborhood context, race, and prior contact with police (Kirk and Papachristos 2011; Sampson and Bartusch 1998). Studies consistently find that residents of high-crime neighborhoods express cynicism about police responsiveness and fairness, even as they continue to call 911 for emergencies (Desmond et al. 2016). This literature highlights the coexistence of cynicism and reliancea paradox that existing frameworks have struggled to explain.\r\n\r\n**When to use**: Building the evidence base before identifying the gap; showing what \"we know.\"\r\n\r\n---\r\n\r\n### CONTRAST\r\n\r\n**Purpose**: Present competing perspectives, alternative explanations, or disagreement in the literature.\r\n\r\n**Typical markers**:\r\n- \"However,\" \"In contrast,\" \"Others argue...\"\r\n- Two (or more) positions clearly delineated\r\n- Even-handed presentation (for Debate-Resolvers)\r\n- May set up a tension the study will address\r\n\r\n**Example paragraph**:\r\n> Whether secondary labor market entry helps or harms long-term immigrant incorporation remains debated. One line of research suggests that early employmenteven in low-wage jobsprovides social capital, language acquisition, and pathway to better positions (Portes and Rumbaut 2001). Others, however, argue that secondary labor market entry traps immigrants in dead-end jobs, limiting mobility and reinforcing ethnic segmentation (Waldinger and Lichter 2003). This tension remains unresolved in the literature.\r\n\r\n**When to use**: Problem-Driven (Debate) articles; showing disagreement before resolution.\r\n\r\n---\r\n\r\n### CRITIQUE\r\n\r\n**Purpose**: Identify limitations, blind spots, or inadequacies in existing approaches.\r\n\r\n**Typical markers**:\r\n- \"However, this approach is limited...\"\r\n- \"Yet scholars have not...\"\r\n- \"This framework cannot account for...\"\r\n- \"Existing literature is limited on two fronts...\"\r\n\r\n**Example paragraph**:\r\n> While this work is valuable for understanding how individuals navigate bureaucratic encounters, it is limited in two respects. First, most studies focus on formal legal settings (courts, police stations), overlooking everyday interactions with social services. Second, the literature assumes that individuals approach institutions with formed dispositions, neglecting how dispositions themselves may be shaped through repeated encounters. A more dynamic account is needed.\r\n\r\n**When to use**: Concept-Builders before introducing new framework; setting up the need for innovation.\r\n\r\n---\r\n\r\n### IDENTIFY_GAP\r\n\r\n**Purpose**: Mark the transition from \"what we know\" to \"what we don't know.\" This is the rhetorical center of the section.\r\n\r\n**Typical markers**:\r\n- \"Yet we know little about...\"\r\n- \"However, less attention has been paid to...\"\r\n- \"What remains unclear is...\"\r\n- \"Scholars have not examined...\"\r\n- \"Largely unexplored are...\"\r\n\r\n**Example paragraph**:\r\n> Yet while research to date provides insight into *why* residents persist in calling on law enforcement despite cynicism, there is less attention to the cultural scripts through which people themselves seek to *make sense* of the institutions with which they engage. How do residents of high-crime neighborhoods understand their relationship to police? What meaning-making processes allow them to reconcile cynicism with reliance? These questions remain largely unexplored.\r\n\r\n**When to use**: The \"turn\" moment. Usually middle of the section; sometimes later for empirical documenters.\r\n\r\n---\r\n\r\n### BRIDGE\r\n\r\n**Purpose**: Connect what has been established to what you're doing. Show how literatures come together or how frameworks apply.\r\n\r\n**Typical markers**:\r\n- \"Bringing together...\" \"Drawing on...\"\r\n- \"This framework illuminates...\"\r\n- \"We argue that [X] provides analytical purchase on...\"\r\n- \"These insights suggest that...\"\r\n\r\n**Example paragraph**:\r\n> Bringing these literatures into dialogue suggests a way forward. Recognition theory illuminates why procedural fairness mattersit signals respect and acknowledgment of status. Legal consciousness research reveals how residents actively interpret their encounters with institutions. Together, these perspectives allow us to see police-calling not as a paradox of cynicism-despite-reliance, but as an expression of demands for recognition that existing institutions consistently fail to provide.\r\n\r\n**When to use**: Synthesis Integrators (the payoff paragraph); Theory-Extenders showing why framework fits.\r\n\r\n---\r\n\r\n### STATE_QUESTIONS\r\n\r\n**Purpose**: Articulate the research questions, hypotheses, or analytical focus of the study.\r\n\r\n**Typical markers**:\r\n- \"This study examines...\"\r\n- \"I ask: [question]?\"\r\n- \"Three questions guide this analysis...\"\r\n- \"We investigate whether...\"\r\n\r\n**Example paragraph**:\r\n> This study examines how mothers in doubled-up households negotiate identity and dignity in shared living arrangements. Three questions guide the analysis: (1) How do guest mothers understand their position within host households? (2) What strategies do they use to maintain maternal identity when authority is constrained? (3) How do these negotiations vary by the character of the host-guest relationship? I address these questions through interviews with 47 mothers who doubled up with family or friends.\r\n\r\n**When to use**: After the gap; usually the penultimate or final paragraph.\r\n\r\n---\r\n\r\n### PREVIEW\r\n\r\n**Purpose**: Summarize the argument or outline what the findings will show. Provides a roadmap.\r\n\r\n**Typical markers**:\r\n- \"I find that...\" \"We show that...\"\r\n- \"The findings reveal...\"\r\n- \"In what follows, I first... then... finally...\"\r\n- Brief statement of main claims (without evidence)\r\n\r\n**Example paragraph**:\r\n> In what follows, I show that guest mothers deploy three strategies to maintain maternal identity: boundary work (carving out private space), deference management (navigating host authority), and temporal framing (emphasizing the temporary nature of arrangements). I argue that these strategies reveal how structural housing precarity intersects with gendered expectations of motherhood to produce distinctive identity work. I conclude by discussing implications for understanding family instability and housing policy.\r\n\r\n**When to use**: Final paragraph of the section; optional but common for complex arguments.\r\n\r\n---\r\n\r\n## Paragraph Sequences by Cluster\r\n\r\n### Gap-Filler Sequence\r\n1. PROVIDE_CONTEXT\r\n2. SYNTHESIZE\r\n3. SYNTHESIZE\r\n4. IDENTIFY_GAP\r\n5. STATE_QUESTIONS\r\n\r\n### Theory-Extender Sequence\r\n1. PROVIDE_CONTEXT\r\n2. DESCRIBE_THEORY\r\n3. SYNTHESIZE (prior applications)\r\n4. IDENTIFY_GAP\r\n5. BRIDGE\r\n6. STATE_QUESTIONS\r\n\r\n### Concept-Builder Sequence\r\n1. PROVIDE_CONTEXT\r\n2. DESCRIBE_THEORY (tradition 1)\r\n3. CRITIQUE\r\n4. DESCRIBE_THEORY (tradition 2)\r\n5. SYNTHESIZE + concept introduction\r\n6. DESCRIBE_THEORY (new concept explication)\r\n7. STATE_QUESTIONS\r\n\r\n### Synthesis Integrator Sequence\r\n1. DESCRIBE_THEORY (tradition A)\r\n2. SYNTHESIZE (A applications)\r\n3. DESCRIBE_THEORY (tradition B)\r\n4. SYNTHESIZE (B applications)\r\n5. BRIDGE\r\n6. STATE_QUESTIONS\r\n\r\n### Problem-Driven (Debate) Sequence\r\n1. DESCRIBE_THEORY (position 1)\r\n2. CONTRAST (position 2)\r\n3. CRITIQUE (both limited)\r\n4. STATE_QUESTIONS\r\n\r\n---\r\n\r\n## Diagnostic Questions\r\n\r\nFor each paragraph, ask:\r\n1. What function is this paragraph serving?\r\n2. Is the function clear from the first sentence?\r\n3. Does the function fit its position in the sequence?\r\n4. Is the function appropriate for my cluster?\r\n\r\nIf you can't identify the function, the paragraph likely needs revision.\r\n",
        "plugins/lit-writeup/skills/lit-writeup/techniques/sentence-toolbox.md": "# Sentence Toolbox\r\n\r\nA vocabulary of sentence-level moves for Theory section writing, based on analysis of 80 articles from *Social Problems* and *Social Forces*.\r\n\r\n---\r\n\r\n## 1. Opening Sentence Types\r\n\r\nTheory sections and paragraphs begin with distinct sentence types that signal rhetorical function. Select appropriately.\r\n\r\n### 1.1 Literature-Announcing Openers (25-30% of paragraphs)\r\n\r\nSignal entry into an established scholarly conversation.\r\n\r\n**Signature patterns**:\r\n- \"A growing body of research...\"\r\n- \"Over the past [X] decades, [scholars/sociologists] have...\"\r\n- \"Research on [topic] has...\"\r\n- \"An emerging literature examines...\"\r\n- \"A burgeoning literature documents...\"\r\n\r\n**Examples**:\r\n> \"A growing body of research examines the ways migrant children and their parents back home manage their relationships.\"\r\n\r\n> \"Over the last three decades, cultural sociologists have developed a rich theoretical and empirical research platform to demonstrate how culture works in a variety of institutional contexts.\"\r\n\r\n> \"In the past two decades, sociologists have demonstrated the persistence of systemic oppression along the lines of race, class, sex, gender, sexuality, and other social locations.\"\r\n\r\n**Best for**: Opening synthesis paragraphs, beginning the theory section.\r\n\r\n---\r\n\r\n### 1.2 Context-Setting Openers (20-25%)\r\n\r\nEstablish empirical phenomena, historical developments, or social conditions.\r\n\r\n**Signature patterns**:\r\n- \"As [phenomenon] has [developed/expanded/changed]...\"\r\n- \"The [adjective] [phenomenon] has...\"\r\n- \"In recent [decades/years]...\"\r\n- \"With the [expansion/rise/decline] of...\"\r\n\r\n**Examples**:\r\n> \"As housing costs have outpaced wage growth, many parents face difficulty housing their families.\"\r\n\r\n> \"The dramatic reconfiguration of the U.S. state over the past four decades has generated a body of criminological scholarship centered on the unprecedented magnitude of carceral expansion.\"\r\n\r\n> \"The US immigration system is situated within a global regime of borders.\"\r\n\r\n**Best for**: Opening the theory section, introducing substantive context.\r\n\r\n---\r\n\r\n### 1.3 Theory-Introducing Openers (15-20%)\r\n\r\nExplicitly name a theoretical framework, concept, or scholar.\r\n\r\n**Signature patterns**:\r\n- \"The concept of '[X]' draws on...\"\r\n- \"[Framework/Theory] posits that...\"\r\n- \"According to [Scholar]...\"\r\n- \"[Scholar's] (Year) concept of [X]...\"\r\n- \"Central to this analysis is...\"\r\n\r\n**Examples**:\r\n> \"The concept of 'recognition' draws on several conceptual traditions.\"\r\n\r\n> \"The procedural justice model asserts that people make fairness evaluations according to four process-based criteria.\"\r\n\r\n> \"Connell (2005) conceptualizes hegemonic masculinity as the most honorable expression of manliness within a hierarchy of masculinities.\"\r\n\r\n**Best for**: Theory-Extension and Concept-Builder articles, framework exposition paragraphs.\r\n\r\n---\r\n\r\n### 1.4 Gap-Marking Openers (10-15%)\r\n\r\nImmediately signal what is missing or understudied.\r\n\r\n**Signature patterns**:\r\n- \"Yet we know little about...\"\r\n- \"However, [scholars/research] has not...\"\r\n- \"What remains unclear is...\"\r\n- \"Largely unexplored in studies of [X] are...\"\r\n- \"Despite [X], we have limited understanding of...\"\r\n\r\n**Examples**:\r\n> \"Yet while research to date provides insight into *why* residents persist in calling on law enforcement, there is less attention to the cultural scripts through which people themselves seek to *make sense* of the institutions with which they engage.\"\r\n\r\n> \"Largely unexplored in studies of transnational elder care are the experiences of adult children who remain in the place of origin with their aging parents.\"\r\n\r\n**Best for**: The \"turn\" paragraph, after synthesis.\r\n\r\n---\r\n\r\n### 1.5 Contrast-Signaling Openers (10-12%)\r\n\r\nIntroduce tension between competing positions or shift analytical direction.\r\n\r\n**Signature patterns**:\r\n- \"However, [alternative view]...\"\r\n- \"In contrast to [X], [Y]...\"\r\n- \"While [X], [Y]...\"\r\n- \"Others, however, argue...\"\r\n- \"At the same time...\"\r\n\r\n**Examples**:\r\n> \"The study of feminist social movements provides a contrasting account of the mobilization of civil society into carceral activities.\"\r\n\r\n> \"Others, however, argue that a tendency toward conspiratorial thinking is not indicated by psychological predispositions as much as indicative of a 'cognitive style.'\"\r\n\r\n**Best for**: Debate-framing, presenting alternative perspectives.\r\n\r\n---\r\n\r\n### 1.6 Definition-Providing Openers (5-8%)\r\n\r\nDefine key terms, often introducing newly coined concepts.\r\n\r\n**Signature patterns**:\r\n- \"[Concept] refers to...\"\r\n- \"I use the term [X] to...\"\r\n- \"We define [X] as...\"\r\n- \"[Concept] is...\"\r\n- \"By [X], I mean...\"\r\n\r\n**Examples**:\r\n> \"I use the term *carceral creep* to suggest the incremental and often imperceptible advance of carceral forces.\"\r\n\r\n> \"*Work primacy* refers to the centrality of work in the economic and noneconomic lives of undocumented immigrant youth.\"\r\n\r\n> \"We define recognition as the 'fact of being acknowledged' through the institutional provision of services, validation, and dignity.\"\r\n\r\n**Best for**: Concept-Builder articles, new terminology introduction.\r\n\r\n---\r\n\r\n### 1.7 Claim-Making Openers (5-8%)\r\n\r\nMake direct argumentative assertions about the study's contribution.\r\n\r\n**Signature patterns**:\r\n- \"I argue that...\"\r\n- \"This study [examines/considers/extends]...\"\r\n- \"We contend that...\"\r\n- \"This paper [integrates/demonstrates/shows]...\"\r\n\r\n**Examples**:\r\n> \"I argue that Du Bois offers an integrated framework for understanding immigrants' incorporation strategies.\"\r\n\r\n> \"This paper integrates procedural and distributive justice models in order to examine how both legal treatment *and* outcome experiences influenced respondents' perceptions of (in)justice.\"\r\n\r\n**Best for**: Stating your contribution, previewing the argument.\r\n\r\n---\r\n\r\n## 2. Transition Marker Inventory\r\n\r\n### 2.1 Additive Markers\r\n\r\nConnect supporting or elaborating ideas.\r\n\r\n| Marker | Usage |\r\n|--------|-------|\r\n| \"Building on\" | \"Building on this work, scholars have...\" |\r\n| \"Moreover\" | \"Moreover, CPS may be a meaningful presence...\" |\r\n| \"In addition\" | \"In addition to documenting...\" |\r\n| \"Similarly\" | \"Similarly, research on procedural justice...\" |\r\n| \"Likewise\" | \"Likewise, researchers have shown...\" |\r\n| \"Furthermore\" | \"Furthermore, individual property owners...\" |\r\n| \"Also\" | \"They also maintain boundaries along...\" |\r\n| \"Extending this insight\" | \"Extending this insight, we see...\" |\r\n\r\n### 2.2 Contrastive Markers\r\n\r\nSignal disagreement, limitation, or alternative perspectives.\r\n\r\n| Marker | Usage |\r\n|--------|-------|\r\n| \"However\" | Very high frequency; primary turn marker |\r\n| \"Yet\" | Very high frequency; often begins gap sentence |\r\n| \"In contrast\" | \"In contrast to [X], [Y]...\" |\r\n| \"Despite\" | \"Despite making important strides...\" |\r\n| \"Although\" | \"Although earlier evaluations showed...\" |\r\n| \"While\" | \"While Kanter articulated the challenges...\" |\r\n| \"Nevertheless\" | \"Nevertheless, transnational fathers increasingly...\" |\r\n| \"Rather than\" | \"Rather than show a clear progression...\" |\r\n\r\n### 2.3 Temporal Markers\r\n\r\nSituate developments in historical context.\r\n\r\n| Marker | Usage |\r\n|--------|-------|\r\n| \"In recent [decades/years]\" | Very high frequency |\r\n| \"Over the past [X] decades\" | High frequency |\r\n| \"Historically\" | \"Historically, one strategy people use...\" |\r\n| \"Since the [1960s/1990s]\" | \"Since the 1960s, responding to...\" |\r\n| \"Today\" | \"Today, mandated reporters typically...\" |\r\n| \"Increasingly\" | \"Increasingly, scholars have acknowledged...\" |\r\n\r\n### 2.4 Causal/Consequential Markers\r\n\r\nSignal logical implications.\r\n\r\n| Marker | Usage |\r\n|--------|-------|\r\n| \"As a result\" | \"As a result, outsiders must strategically navigate...\" |\r\n| \"Thus\" | Very high frequency; \"Thus, hyper visibility also limits...\" |\r\n| \"Therefore\" | \"Therefore, even a tenant approved...\" |\r\n| \"In turn\" | \"Mode of entry, in turn, shapes...\" |\r\n| \"Given\" | \"Given these concerns...\" |\r\n\r\n### 2.5 Exemplifying Markers\r\n\r\nIntroduce specific illustrations.\r\n\r\n| Marker | Usage |\r\n|--------|-------|\r\n| \"For example\" | Very high frequency |\r\n| \"For instance\" | \"For instance, outsiders tend to be evaluated...\" |\r\n| \"Such as\" | \"...such as 'stop-and-frisk,' hot-spot policing...\" |\r\n| \"As [Scholar] demonstrates\" | \"As Okamoto's foundational research demonstrates...\" |\r\n\r\n---\r\n\r\n## 3. Hedging Calibration\r\n\r\n### When to Hedge (may, might, suggests, can)\r\n\r\n- **Theoretical claims about mechanisms**: \"This process may operate through...\"\r\n- **Predictions about findings**: \"We expect that...\"\r\n- **Generalizations beyond the sample**: \"These dynamics may apply to...\"\r\n- **Causal claims without experimental evidence**: \"X may lead to Y\"\r\n\r\n### When to Assert (demonstrates, shows, establishes, is)\r\n\r\n- **Reporting prior literature**: \"Studies demonstrate...\"\r\n- **Defining concepts**: \"Recognition refers to...\"\r\n- **Gap statements**: \"We know little about...\" (assertive about the gap)\r\n- **Research questions**: \"This study examines...\" (unhedged)\r\n\r\n### Hedging Vocabulary Spectrum\r\n\r\n| Strong Hedge | Moderate Hedge | Moderate Assertion | Strong Assertion |\r\n|--------------|----------------|-------------------|------------------|\r\n| \"may\" | \"suggests\" | \"indicates\" | \"demonstrates\" |\r\n| \"might\" | \"tends to\" | \"shows\" | \"establishes\" |\r\n| \"could\" | \"often\" | \"finds\" | \"confirms\" |\r\n| \"perhaps\" | \"typically\" | \"reveals\" | \"is\" |\r\n| \"possibly\" | \"appears to\" | \"illustrates\" | \"clearly\" |\r\n\r\n---\r\n\r\n## 4. Cluster-Specific Sentence Patterns\r\n\r\n### Gap-Filler\r\n- Heavy **context-setting** and **literature-announcing** openers\r\n- **Contrastive** markers (\"however,\" \"yet\") for the turn\r\n- **Efficient parenthetical** citation strings\r\n- Moderate hedging, **assertive gap statements**\r\n\r\n### Theory-Extender\r\n- **Theory-introducing** openers with named theorists\r\n- \"According to [Scholar]\"; \"Building on\"; \"Drawing on\"\r\n- **Author-subject** citation pattern dominant\r\n- Lower hedging on established theory\r\n\r\n### Concept-Builder\r\n- **Definition-providing** openers for new terminology\r\n- Causal chains (\"as a result,\" \"thus\")\r\n- **Quote-then-cite** for detailed engagement\r\n- Low hedging on conceptual definitions\r\n\r\n### Synthesis Integrator\r\n- **Literature-announcing** and **theory-introducing** mixed\r\n- Bridging phrases (\"bringing together,\" \"connecting\")\r\n- Citations from multiple traditions juxtaposed\r\n- Moderate hedging on integration claims\r\n\r\n### Problem-Driven\r\n- **Context-setting** dominant (50%+ for Empirical sub-type)\r\n- **Contrast-signaling** for debates (\"some argue,\" \"others contend\")\r\n- Temporal markers establishing historical context\r\n- Higher hedging on competing explanations\r\n\r\n---\r\n\r\n## Quick Reference: Sentence Starter Bank\r\n\r\n### Starting a Synthesis Paragraph\r\n- \"A growing body of research examines...\"\r\n- \"Over the past [X] decades, scholars have...\"\r\n- \"Research on [topic] demonstrates that...\"\r\n\r\n### Starting a Theory Paragraph\r\n- \"The concept of [X] draws on...\"\r\n- \"[Scholar] (Year) theorizes [concept] as...\"\r\n- \"Central to this framework is...\"\r\n\r\n### Starting the Turn\r\n- \"Yet we know little about...\"\r\n- \"However, less attention has been paid to...\"\r\n- \"What remains unclear is...\"\r\n\r\n### Stating Your Contribution\r\n- \"This study examines...\"\r\n- \"I/We argue that...\"\r\n- \"This paper integrates...\"\r\n",
        "plugins/lit-writeup/skills/lit-writeup/techniques/turn-formula.md": "# The Turn Formula\r\n\r\nThe \"turn\" is the rhetorical center of your Theory sectionthe pivot from \"what we know\" to \"what we don't know\" that creates space for your contribution. This guide provides a systematic approach to crafting the turn.\r\n\r\n---\r\n\r\n## The 4-Part Turn Structure\r\n\r\nEvery effective turn follows this sequence:\r\n\r\n### Part 1: Acknowledge (What We Know)\r\n> \"This literature highlights the importance of [X].\"\r\n> \"Research has documented [pattern].\"\r\n> \"Studies demonstrate that [finding].\"\r\n\r\n### Part 2: Pivot (The Contrastive Marker)\r\n> \"However,\"\r\n> \"Yet,\"\r\n> \"Despite this progress,\"\r\n> \"Nonetheless,\"\r\n\r\n### Part 3: Gap (What We Don't Know)\r\n> \"...we know little about [Y].\"\r\n> \"...less attention has been paid to [Z].\"\r\n> \"...what remains unclear is [question].\"\r\n\r\n### Part 4: Connect (Your Study)\r\n> \"This study examines [Y] by...\"\r\n> \"I address this gap by...\"\r\n> \"This paper asks...\"\r\n\r\n---\r\n\r\n## Complete Turn Examples\r\n\r\n### Gap-Filler Turn\r\n> \"This literature highlights the importance of activity spaces for understanding neighborhood effects on individuals. **However**, it largely focuses on adults, without attending to how adolescentswhose activity patterns differ substantiallynavigate overlapping neighborhood contexts. **This study examines** how formerly incarcerated adolescents construct activity spaces during reentry and how these spaces shape exposure to both risk and opportunity.\"\r\n\r\n### Theory-Extension Turn\r\n> \"Scholars have fruitfully applied Puwar's framework to academic settings, corporate boardrooms, and political institutions. **Yet** the concept of 'bodies out of place' has not been extended to elite professional service firms, where organizational outsiders must navigate distinctive forms of hypervisibility. **We argue that** extending this framework to elite professional contexts reveals how numerical rarity intersects with organizational culture to produce distinctive visibility burdens.\"\r\n\r\n### Concept-Builder Turn\r\n> \"Existing approaches agree that conspiracism represents a commonplace orientation rather than a pathology. **However**, approaches that examine the psychological determinants of conspiracism often overlook it as a social practice embedded in communities and sustained through interaction. **I use the term *epistemological practice*** to suggest that conspiracism involves not merely beliefs but ways of knowingdistinctive orientations toward evidence, authority, and verification that are collectively produced and maintained.\"\r\n\r\n### Synthesis Integrator Turn\r\n> \"Recognition theory illuminates why institutional acknowledgment matters for well-being, while legal consciousness research reveals how individuals actively interpret their encounters with law. **Yet** these literatures have not been brought into dialogue, leaving unexplored how recognition-seeking and legal consciousness interact in high-crime neighborhoods. **Bringing these perspectives together**, we examine how residents' demands for recognition shape their engagement with police despite pervasive legal cynicism.\"\r\n\r\n### Problem-Driven (Debate) Turn\r\n> \"Whether secondary labor market entry helps or harms long-term immigrant incorporation remains debated. Some scholars emphasize mobility pathways, while others document persistent segmentation. **This tension remains empirically unresolved**, particularly for undocumented youth whose pathways are shaped by both labor market structure and legal status. **This study provides evidence** to adjudicate this debate by examining longitudinal outcomes for 47 undocumented young adults who entered the secondary labor market.\"\r\n\r\n---\r\n\r\n## Gap Specificity\r\n\r\nThe gap must be **specific, not generic**.\r\n\r\n### Weak (Generic) Gaps\r\n- \"More research is needed on this topic.\"\r\n- \"This area is understudied.\"\r\n- \"We don't fully understand this phenomenon.\"\r\n\r\n### Strong (Specific) Gaps\r\n- \"We know little about [X] among [Y population] in [Z context].\"\r\n- \"What remains unclear is how [mechanism] operates when [condition].\"\r\n- \"Scholars have not examined [specific aspect] despite its relevance to [larger question].\"\r\n\r\n### Gap Specificity Template\r\n> \"We know little about **[what specifically]** among **[which population/cases]** when **[under what conditions]** despite **[why this matters]**.\"\r\n\r\n---\r\n\r\n## Turn Placement by Cluster\r\n\r\n| Cluster | Typical Placement | Notes |\r\n|---------|------------------|-------|\r\n| **Gap-Filler** | Middle (paragraph 3-4 of ~6) | After synthesis, before questions |\r\n| **Theory-Extender** | Middle, sometimes early | After framework exposition |\r\n| **Concept-Builder** | Middle | After critique, before new concept |\r\n| **Synthesis Integrator** | Middle | After both traditions, as bridge intro |\r\n| **Problem-Driven** | Variable | Early for debates, late for empirical |\r\n\r\n### Placement Principle\r\nThe turn should arrive after you've established **enough context** that the gap is meaningful, but **before** readers wonder where you're going.\r\n\r\n- **Too early**: Gap feels unmotivated; reader hasn't seen enough literature\r\n- **Too late**: Reader has lost the thread; synthesis feels aimless\r\n- **Just right**: Gap emerges naturally from what precedes\r\n\r\n---\r\n\r\n## Turn Markers by Strength\r\n\r\n### Strong Contrastive Markers (Gap-Filler, Debate)\r\n- \"Yet\"\r\n- \"However\"\r\n\r\n### Moderate Markers (Extension, Synthesis)\r\n- \"Despite this progress\"\r\n- \"Nonetheless\"\r\n- \"Still\"\r\n\r\n### Softer Markers (Integration claims)\r\n- \"What remains less clear is\"\r\n- \"Bringing these perspectives together reveals\"\r\n\r\n---\r\n\r\n## Hedging the Gap\r\n\r\nThe gap claim itself should be **relatively assertive**. You're stating what the literature has *not* done.\r\n\r\n### Appropriate Assertiveness\r\n> \"We know little about...\"\r\n> \"Scholars have not examined...\"\r\n> \"This question remains unexplored.\"\r\n\r\n### Over-Hedged (Avoid)\r\n> \"It may be that we don't fully understand...\"\r\n> \"Perhaps less attention has been paid to...\"\r\n\r\n### Over-Claimed (Avoid)\r\n> \"No one has ever studied...\"\r\n> \"The literature completely ignores...\"\r\n> \"We know nothing about...\"\r\n\r\n**Calibration**: Acknowledge what IS known before stating what ISN'T. Then state the gap with confidence.\r\n\r\n---\r\n\r\n## Connecting Gap to Questions\r\n\r\nThe gap must **directly motivate** your research questions. Readers should see the logical link.\r\n\r\n### Tight Connection\r\n> **Gap**: \"Yet we know little about how guest mothers maintain maternal identity when authority is constrained.\"\r\n> **Question**: \"This study asks: How do guest mothers navigate identity and authority in doubled-up households?\"\r\n\r\n### Loose Connection (Problem)\r\n> **Gap**: \"Yet we know little about housing instability.\"\r\n> **Question**: \"This study examines how mothers experience shame.\"\r\n\r\nIf the gap doesn't lead logically to your question, either revise the gap or reconsider whether you've identified the right gap.\r\n\r\n---\r\n\r\n## Common Turn Problems\r\n\r\n| Problem | Symptom | Solution |\r\n|---------|---------|----------|\r\n| **Generic gap** | \"More research needed\" | Specify population, mechanism, context |\r\n| **Missing turn** | Section ends without gap | Add explicit pivot paragraph |\r\n| **Turn too late** | Gap in final paragraph | Move earlier; let questions follow |\r\n| **Disconnect** | Gap doesn't match questions | Revise gap or questions to align |\r\n| **Overclaimed** | \"No one has studied\" | Acknowledge adjacent work, then specify |\r\n| **Buried turn** | Pivot marker missing | Add explicit \"However,\" \"Yet\" |\r\n| **Multiple gaps** | Too many unknowns listed | Focus on the one your study addresses |\r\n\r\n---\r\n\r\n## Turn Drafting Worksheet\r\n\r\nUse this template to draft your turn:\r\n\r\n**Acknowledge**: \"This literature highlights _______.\"\r\n\r\n**Pivot**: \"However,\" / \"Yet,\" / \"Despite this progress,\"\r\n\r\n**Gap**: \"_____ remains [unexplored/unclear/understudied].\"\r\n- What specifically?\r\n- Among which population?\r\n- Under what conditions?\r\n\r\n**Connect**: \"This study examines _____ by _____.\"\r\n\r\n---\r\n\r\n## The Turn as Thesis\r\n\r\nThink of the turn as your **Theory section thesis statement**. Just as a thesis in a paper tells readers what you'll argue, the turn tells readers what gap you're filling.\r\n\r\nIf someone asks \"What's your contribution?\", you should be able to point to your turn sentence.\r\n",
        "plugins/methods-writer/.claude-plugin/plugin.json": "{\r\n  \"name\": \"methods-writer\",\r\n  \"version\": \"1.0.0\",\r\n  \"description\": \"Draft Methods sections for interview-based sociology articles. Three pathways (Efficient/Standard/Detailed) based on analysis of 80 Social Problems/Social Forces articles.\",\r\n  \"skills\": \"./skills/\"\r\n}\r\n",
        "plugins/methods-writer/skills/methods-writer/SKILL.md": "---\r\nname: methods-writer\r\ndescription: Draft publication-ready Methods sections for interview-based sociology articles. Guides pathway selection, component coverage, and calibration based on analysis of 77 Social Problems/Social Forces articles.\r\n---\r\n\r\n# Methods Writer\r\n\r\nYou help sociologists write **Methods sections** (also called \"Data and Methods\" or \"Methodology\" sections) for interview-based journal articles. Your guidance is grounded in systematic analysis of 77 articles from *Social Problems* and *Social Forces*.\r\n\r\n## When to Use This Skill\r\n\r\nUse this skill when users want to:\r\n- Draft a new Methods section from scratch\r\n- Restructure an existing Methods section that's too long or too short\r\n- Determine the appropriate level of detail for their study\r\n- Ensure all required components are included\r\n- Calibrate their section to field norms\r\n\r\nThis skill assumes users have completed their data collection and analysis, and are ready to write up their methods.\r\n\r\n## Connection to Other Skills\r\n\r\n| Skill | Purpose | Key Output |\r\n|-------|---------|------------|\r\n| **interview-analyst** | Analyze qualitative data | Coding structure, findings |\r\n| **interview-writeup** | Write findings sections | Draft findings |\r\n| **interview-bookends** | Write intros/conclusions | Draft bookends |\r\n\r\n## Core Principles (from Genre Analysis)\r\n\r\nBased on systematic analysis of 77 Methods sections:\r\n\r\n### 1. Study-Led Openings Dominate\r\n88% of methods sections open with the study or sample, not with methodological justification. Lead with your data, not your rationale for using interviews.\r\n\r\n### 2. Saturation Claims Are Rare\r\nOnly 4% of articles claim saturation. The field has largely moved beyond this justification. Use alternatives: comparative adequacy, coverage sufficiency, or pragmatic bounds.\r\n\r\n### 3. Tables Correlate with Complexity\r\n54% of articles include a demographic table. Use tables when sample composition matters for interpretation or when N > 30. Efficient pathway articles skip tables entirely.\r\n\r\n### 4. Positionality Is Conditional\r\nOnly 17% include positionality discussions. Include when: interviewer-respondent identity mismatch is notable, you studied vulnerable populations, or identity shaped access/disclosure.\r\n\r\n### 5. Three Pathways Cover the Field\r\nArticles cluster into Efficient (10%), Standard (61%), and Detailed (23%) pathways based on word count and structural complexity. Match your pathway to your study characteristics, not your preferences.\r\n\r\n## Key Statistics (Benchmarks)\r\n\r\n### Methods Section Benchmarks\r\n\r\n| Feature | Median | IQR (Typical Range) |\r\n|---------|--------|---------------------|\r\n| Word count | 1,361 | 1,001-2,032 |\r\n| Has table | 54% | -- |\r\n| Subsections | 67% none | 0-2 |\r\n| Positionality | 17% | -- |\r\n| Saturation mentioned | 4% | -- |\r\n\r\n### Word Count Distribution\r\n\r\n| Range | Label | Prevalence |\r\n|-------|-------|------------|\r\n| < 700 | Efficient | 10% |\r\n| 700-2,000 | Standard | 61% |\r\n| 2,000-3,500 | Detailed | 23% |\r\n| > 3,500 | Extended* | 6% |\r\n\r\n*Extended articles are typically multi-study or exceptionally complex designs.\r\n\r\n## The Three Pathways\r\n\r\nMethods sections cluster into three recognizable styles based on length, structure, and documentation level:\r\n\r\n| Pathway | Target Words | Prevalence | Key Feature | When to Use |\r\n|---------|--------------|------------|-------------|-------------|\r\n| **Efficient** | 600-900 | 10% | Compressed, no table | Simple design, space constraints |\r\n| **Standard** | 1,200-1,500 | 61% | Balanced, table optional | Typical interview study (DEFAULT) |\r\n| **Detailed** | 2,000-3,000 | 23% | Comprehensive, table required | Vulnerable population, complex design |\r\n\r\n**Default**: Standard pathway. Choose Efficient or Detailed only when specific triggers apply.\r\n\r\nSee `pathways/` directory for detailed profiles with benchmarks, signature moves, and word allocation guides.\r\n\r\n## Workflow Phases\r\n\r\n### Phase 0: Assessment\r\n**Goal**: Gather study information and select the appropriate pathway.\r\n\r\n**Process**:\r\n- Collect study details (sample, population, design, access)\r\n- Apply decision tree to identify pathway\r\n- Confirm pathway selection with user\r\n- Note any special considerations (vulnerability, complexity)\r\n\r\n**Output**: Pathway selection memo with rationale.\r\n\r\n> **Pause**: User confirms pathway selection before drafting.\r\n\r\n---\r\n\r\n### Phase 1: Drafting\r\n**Goal**: Write the complete Methods section following pathway template.\r\n\r\n**Process**:\r\n- Follow pathway-specific structure and word allocation\r\n- Include all required components for the pathway\r\n- Use appropriate rhetorical patterns from corpus\r\n- Integrate optional components based on user's study\r\n\r\n**Guides**:\r\n- `phases/phase1-drafting.md` (main workflow)\r\n- `pathways/` (pathway-specific templates)\r\n- `techniques/component-checklist.md` (what to include)\r\n- `techniques/opening-moves.md` (how to start)\r\n\r\n**Output**: Complete Methods section draft.\r\n\r\n> **Pause**: User reviews draft.\r\n\r\n---\r\n\r\n### Phase 2: Revision\r\n**Goal**: Calibrate against benchmarks and polish.\r\n\r\n**Process**:\r\n- Verify word count against pathway target\r\n- Check all required components are present\r\n- Assess optional components (positionality, limitations)\r\n- Polish prose and transitions\r\n- Final quality check\r\n\r\n**Guide**: `phases/phase2-revision.md`\r\n\r\n**Output**: Revised Methods section with quality memo.\r\n\r\n---\r\n\r\n## Pathway Decision Tree\r\n\r\nTo identify which pathway fits your study:\r\n\r\n```\r\nSTART\r\n  |\r\n  v\r\n[Is your population VULNERABLE or MARGINALIZED?]\r\n  |\r\n  +-- YES --> DETAILED PATHWAY\r\n  |\r\n  +-- NO --> Continue\r\n        |\r\n        v\r\n[Is your design COMPLEX?]\r\n(Multi-site, comparative, longitudinal, 100+ interviews)\r\n  |\r\n  +-- YES --> DETAILED PATHWAY\r\n  |\r\n  +-- NO --> Continue\r\n        |\r\n        v\r\n[Are there SPACE CONSTRAINTS or is methods SECONDARY?]\r\n  |\r\n  +-- YES --> EFFICIENT PATHWAY\r\n  |\r\n  +-- NO --> STANDARD PATHWAY (DEFAULT)\r\n```\r\n\r\n### Quick Indicators\r\n\r\n| If you have... | Consider this pathway... |\r\n|----------------|--------------------------|\r\n| Vulnerable population (incarcerated, undocumented) | Detailed |\r\n| Multi-site or comparative design | Detailed |\r\n| 100+ interviews | Detailed |\r\n| Significant access challenges | Detailed |\r\n| Severe word limits | Efficient |\r\n| Simple convenience/snowball sample | Efficient |\r\n| Typical single-site, 30-80 interviews | Standard |\r\n\r\n## Pathway Profiles\r\n\r\nReference these guides for pathway-specific writing:\r\n\r\n| Guide | Pathway |\r\n|-------|---------|\r\n| `pathways/efficient.md` | Efficient (10%) - 600-900 words |\r\n| `pathways/standard.md` | Standard (61%) - 1,200-1,500 words |\r\n| `pathways/detailed.md` | Detailed (23%) - 2,000-3,000 words |\r\n\r\n## Technique Guides\r\n\r\n| Guide | Purpose |\r\n|-------|---------|\r\n| `techniques/component-checklist.md` | What to include for each component (sampling, protocol, analysis) |\r\n| `techniques/opening-moves.md` | How to open methods sections (study-led patterns) |\r\n\r\n## Required vs. Optional Components by Pathway\r\n\r\n| Component | Efficient | Standard | Detailed |\r\n|-----------|-----------|----------|----------|\r\n| Sample N | Required | Required | Required |\r\n| Demographics | Brief prose | Prose + table | Table + comparison |\r\n| Recruitment | Named | Named + channels | Channels + rates |\r\n| Duration | Required | Required | Required + median |\r\n| Analysis approach | Named | Named + process | Named + codes |\r\n| Software | Optional | Recommended | Required |\r\n| Positionality | Omit | Conditional | Encouraged |\r\n| Ethical protections | Brief | As needed | Detailed if vulnerable |\r\n\r\n## Model Recommendations\r\n\r\n| Phase | Model | Rationale |\r\n|-------|-------|-----------|\r\n| **Phase 0**: Assessment | **Sonnet** | Decision tree application |\r\n| **Phase 1**: Drafting | **Sonnet** | Following templates, prose generation |\r\n| **Phase 2**: Revision | **Sonnet** | Calibration checking, polish |\r\n\r\n## Starting the Process\r\n\r\nWhen the user is ready to begin:\r\n\r\n1. **Ask about the study**:\r\n   > \"What is your study about? Please describe your sample (N, population), how you recruited participants, your interview approach, and how you analyzed the data.\"\r\n\r\n2. **Ask about study characteristics**:\r\n   > \"Is your population vulnerable or marginalized? Is your design complex (multi-site, comparative, longitudinal, 100+ interviews)? Are there space constraints or journal word limits?\"\r\n\r\n3. **Identify pathway**:\r\n   > Based on your answers, apply the decision tree and recommend a pathway with rationale.\r\n\r\n4. **Confirm and proceed to Phase 0** to formalize the assessment.\r\n\r\n## Key Reminders\r\n\r\n- **Standard is the default**: Most interview studies fit the Standard pathway. Choose Efficient or Detailed only when triggers apply.\r\n- **Saturation is rare**: Only 4% of corpus articles claim saturation. Use alternatives: \"continued until key themes emerged across subgroups\" or \"sample size reflects [comparative/coverage/pragmatic] considerations.\"\r\n- **Tables save words**: A demographic table can replace 200+ words of prose. Use tables when N > 30 or composition matters.\r\n- **Positionality is conditional**: Only 17% include it. Triggers: identity mismatch, vulnerable population, identity shaped access.\r\n- **Study-led openings**: 88% open with the study/sample. Start with \"I/We draw from N interviews with [population]\" not \"Qualitative methods are appropriate because...\"\r\n- **Word counts matter**: Reviewers notice methods sections that are too thin or bloated. Match your pathway.\r\n",
        "plugins/methods-writer/skills/methods-writer/pathways/detailed.md": "# Detailed Pathway\r\n\r\n**Prevalence**: 23% of corpus (18/77 articles)\r\n\r\n**Contribution logic**: \"Given the complexity of our design and/or vulnerability of our population, we document our methods comprehensively for transparency and replicability.\"\r\n\r\n---\r\n\r\n## Identifying Detailed Pathway Articles\r\n\r\nYour study fits the Detailed pathway if ANY of the following apply:\r\n- Vulnerable or marginalized population\r\n- Multi-site or comparative design\r\n- Complex sampling (stratified, merged sources, longitudinal)\r\n- Incarcerated, detained, or legally precarious participants\r\n- Significant access challenges\r\n- Large sample (100+)\r\n- Longitudinal/repeated interviews\r\n- Topic requires extensive methodological justification\r\n\r\n**Key indicators**:\r\n- Word count target: 2,000-3,000 words\r\n- Demographic table required (100% have tables)\r\n- 2-4 subsections recommended\r\n- Positionality encouraged (28% include)\r\n\r\n---\r\n\r\n## Detailed Pathway Triggers\r\n\r\n### 1. Population Characteristics\r\n\r\n| Trigger | Examples |\r\n|---------|----------|\r\n| **Vulnerable population** | Incarcerated, undocumented, survivors of violence |\r\n| **Marginalized groups** | Sex workers, gang members, people experiencing homelessness |\r\n| **Hard-to-reach population** | Diaspora activists, elites, people exiting institutions |\r\n| **Legally precarious** | DACA recipients, people with criminal records |\r\n\r\n### 2. Design Complexity\r\n\r\n| Trigger | Examples |\r\n|---------|----------|\r\n| **Multi-site comparison** | UK vs. Denmark; Atlanta vs. NYC |\r\n| **Multiple participant types** | Detained parents + spouses + children |\r\n| **Longitudinal/repeated** | 5 interviews over 1 year post-release |\r\n| **Merged data sources** | Interviews + survey + administrative data |\r\n| **Stratified sampling** | By county, move timing, age at event |\r\n\r\n### 3. Sample Size\r\n\r\n| Trigger | Rationale |\r\n|---------|-----------|\r\n| **Large sample (100+)** | Requires table; prose insufficient |\r\n| **Large N with subgroups** | Breakdown across multiple dimensions needed |\r\n\r\n### 4. Access and Recruitment\r\n\r\n| Trigger | Examples |\r\n|---------|----------|\r\n| **Institutional gatekeepers** | Sheriff's Department provided lists |\r\n| **Significant refusal rate** | 75 of 215 declined |\r\n| **Multi-channel recruitment** | Organizations + social media + referrals |\r\n| **Follow-up interviews** | 28 of 51 available for re-interview |\r\n\r\n---\r\n\r\n## Benchmarks\r\n\r\n| Feature | Detailed Pathway | Corpus Overall |\r\n|---------|------------------|----------------|\r\n| **Target words** | 2,000-3,000 | 1,361 (median) |\r\n| **Median in pathway** | 2,828 | -- |\r\n| **Table** | Required | 54% have tables |\r\n| **Subsections** | 2-4 recommended | 33% have subsections |\r\n| **Positionality** | Encouraged (28%) | 17% overall |\r\n| **Response rates** | Often included | Rarely elsewhere |\r\n\r\n---\r\n\r\n## Word Allocation\r\n\r\n```\r\nDETAILED METHODS SECTION (~2,500 words)\r\n\r\n[Study Overview + Design Rationale]  ~300 words\r\n[Case/Site Description]              ~250 words\r\n[Sample with Table(s)]               ~400 words\r\n[Recruitment Process]                ~350 words\r\n[Protocol]                           ~350 words\r\n[Ethical Considerations]             ~200 words (if vulnerable)\r\n[Positionality]                      ~200 words\r\n[Analysis]                           ~350 words\r\n[Limitations/Transition]             ~100 words\r\n```\r\n\r\n---\r\n\r\n## Signature Moves (Do This)\r\n\r\n### 1. Research Design Opening\r\nOpen with explicit research question and design framing.\r\n\r\n> \"This study explains how and to what extent diaspora movements support anti-regime rebellions using original, comparative evidence from the Arab Spring abroad.\"\r\n\r\n### 2. Unit of Analysis Specified\r\nExplicitly state what is being compared/analyzed.\r\n\r\n> \"The unit of analysis is diaspora movement by national group (Libyan, Syrian, and Yemeni) and host country (the US and Great Britain).\"\r\n\r\n### 3. Case Selection Logic Detailed\r\nProvide explicit criteria for case selection.\r\n\r\n> \"The six cases provide an ideal set of comparisons for several reasons. First,... Second,... Third,...\"\r\n\r\n### 4. Table with Population Comparison\r\nCompare sample demographics to broader population.\r\n\r\n> \"Table 1 presents selected characteristics of our participants and compares them to estimates of the broader population of [group].\"\r\n\r\n### 5. Response/Refusal Rates\r\nDocument participation rates explicitly.\r\n\r\n> \"215 detainees were asked to participate; 75 refused, resulting in a final sample of 140 individuals.\"\r\n\r\n### 6. Instrument Provenance\r\nDescribe where interview guide came from.\r\n\r\n> \"The [validated instrument name] informed the initial development of the interview instrument.\"\r\n\r\n### 7. Ethical Considerations Paragraph\r\nInclude dedicated discussion of protections for vulnerable populations.\r\n\r\n> \"Because some participants were still being adjudicated, we did not want them to disclose any information that could jeopardize their case or safety. For this reason, we obtained verbal rather than written consent and did not maintain records of any personal information.\"\r\n\r\n### 8. Positionality as Methodological Resource\r\nDiscuss how researcher identity shaped data, including positive effects.\r\n\r\n> \"As the only researcher of color on the team, I drew upon my racial/ethnic background and familial experience with incarceration to build rapport. These shared sensibilities not only facilitated data collection, they influenced my writing.\"\r\n\r\n### 9. Subsection Organization\r\nUse headers to organize complexity.\r\n\r\n> \"DATA COLLECTION\" followed by \"ANALYSIS\"\r\n\r\n### 10. Specific Codes Listed\r\nName the actual codes used.\r\n\r\n> \"These codes included *trust, rejection, loneliness, shame, hopelessness, excitement*, among others.\"\r\n\r\n### 11. Prevalence Statistics\r\nProvide frequency information for qualitative findings.\r\n\r\n> \"Gang enhancements surfaced in nearly 25 percent of interviews.\"\r\n\r\n### 12. Dataset Quantification\r\nReport total data volume.\r\n\r\n> \"Approximately 300 hours of digital recordings and 2,000 pages of single-spaced transcriptions.\"\r\n\r\n---\r\n\r\n## Prohibited Moves (Don't Do This)\r\n\r\n### 1. Skipping Table\r\nAll Detailed pathway sections require at least one demographic table.\r\n\r\n**Why**: Complexity requires structured presentation.\r\n\r\n### 2. Glossing Over Access\r\nIf access was challenging, document it; do not minimize.\r\n\r\n**Why**: Readers need to understand how institutional relationships shaped sample.\r\n\r\n### 3. Omitting Ethical Safeguards\r\nFor vulnerable populations, ethical protections MUST be documented.\r\n\r\n**Why**: Ethical transparency is non-negotiable with vulnerable groups.\r\n\r\n### 4. Unnamed Analysis Approach\r\nName the approach with citations; do not just say \"coded transcripts.\"\r\n\r\n**Why**: Detailed pathway expects methodological specificity.\r\n\r\n### 5. Ignoring Positionality\r\nWhen studying marginalized groups, researcher identity matters; address it.\r\n\r\n**Why**: Positionality affects access, disclosure, and interpretation.\r\n\r\n### 6. Under-Specifying Complex Designs\r\nMulti-site, comparative, longitudinal designs require explicit explanation of timing, comparability, case selection.\r\n\r\n**Why**: Readers cannot assess validity without understanding design logic.\r\n\r\n---\r\n\r\n## Complete Example Structure\r\n\r\n```markdown\r\n## Data and Methods\r\n\r\n### Study Design\r\n\r\n[Opening: \"This study explains how and to what extent [phenomenon] using\r\noriginal, comparative evidence from [cases]. The unit of analysis is\r\n[unit] by [dimension 1] and [dimension 2].\"]\r\n(~100 words)\r\n\r\n[Case selection: \"The six cases provide an ideal set of comparisons for\r\nseveral reasons. First,... Second,... Third,...\"]\r\n(~200 words)\r\n\r\n### Data Collection\r\n\r\n[Site description: \"The [location] population is significant in its size.\r\nThe vast majority of those sentenced to serve time in [state] do so at\r\nthe county level...\"]\r\n(~150 words)\r\n\r\n[Recruitment: \"Respondents were recruited between [dates]. For [Group A],\r\nsigns were posted advertising the study. Those interested signed up, and\r\ntheir information was relayed through [process]...\"]\r\n(~200 words)\r\n\r\n[Sample: \"In total, 215 [population] were asked to participate; 75\r\nrefused, resulting in a final sample of 140 individuals. As noted in\r\nTable 1, most respondents were racial minorities...\"]\r\n(~150 words)\r\n\r\n[Table: Demographic characteristics with population comparison]\r\n\r\n[Protocol: \"Interviews were conducted in person while participants were\r\n[context]. Interviews ranged from 40 minutes to over two hours. The\r\ninterview guide covered [topics]...\"]\r\n(~200 words)\r\n\r\n### Ethical Considerations\r\n\r\n[Ethics: \"Because some participants were still being adjudicated, we did\r\nnot want them to disclose any information that could jeopardize their\r\ncase. For this reason, we obtained verbal rather than written consent...\"]\r\n(~150 words)\r\n\r\n### Positionality\r\n\r\n[Positionality: \"As the only researcher of color on the team, I drew\r\nupon my racial/ethnic background and familial experience... These shared\r\nsensibilities not only facilitated data collection, they influenced my\r\nwriting...\"]\r\n(~150 words)\r\n\r\n### Analysis\r\n\r\n[Analysis: \"Following an abductive approach (Timmermans and Tavory 2012),\r\nI coded and analyzed the transcripts using [software]. Initial open coding\r\nidentified themes such as... These codes included *trust, rejection,\r\nloneliness, shame*...\"]\r\n(~250 words)\r\n\r\n[Limitations: \"Given the open-ended nature of the interview instrument,\r\nfindings were emergent and, often, incomplete...\"]\r\n(~100 words)\r\n\r\nTOTAL: ~1,650 words (add detail to reach 2,000-3,000)\r\n```\r\n\r\n---\r\n\r\n## Exemplar Articles\r\n\r\n| Article | Words | Notable Feature |\r\n|---------|-------|-----------------|\r\n| 10.1093_socpro_spz005 | 2,249 | Longitudinal reentry; population comparison table |\r\n| 10.1093_socpro_spab025 | 2,953 | Incarcerated population; positionality as asset |\r\n| 10.1093_sf_soz070 | 3,031 | Comparative design; extensive case selection |\r\n| 10.1093_socpro_spaa053 | 3,148 | Full respondent table; thoughtful positionality |\r\n\r\n---\r\n\r\n## Table Design Guidance\r\n\r\n### Required Table Elements\r\n- Demographics (age, gender, race/ethnicity)\r\n- Study-specific characteristics (education, occupation, offense type)\r\n- N and percentages\r\n\r\n### Population Comparison (Recommended)\r\n\r\n| Characteristic | Sample (N=140) | Population |\r\n|----------------|----------------|------------|\r\n| Male | 92% | 89% |\r\n| Black | 48% | 52% |\r\n| Mean age | 29 | 31 |\r\n\r\n### Multiple Tables (Acceptable)\r\n- Table 1: Sample demographics\r\n- Table 2: Comparison across sites\r\n- Appendix: Full respondent characteristics\r\n\r\n---\r\n\r\n## Subsection Options\r\n\r\n**Option A**: Two subsections\r\n- \"Data Collection\"\r\n- \"Analysis\"\r\n\r\n**Option B**: Three subsections\r\n- \"Research Design\"\r\n- \"Data Collection\"\r\n- \"Analytical Procedures\"\r\n\r\n**Option C**: Topical subsections\r\n- \"Study Setting\"\r\n- \"Sample\"\r\n- \"Procedures\"\r\n- \"Analysis\"\r\n\r\n---\r\n\r\n## Positionality Guidance\r\n\r\n### Structure (200 words)\r\n1. **Identity disclosure** (~40 words): Who are the researchers?\r\n2. **Constraining effects** (~60 words): How might identity have limited data?\r\n3. **Enabling effects** (~60 words): How might identity have helped?\r\n4. **Interpretation** (~40 words): What does this mean for findings?\r\n\r\n### Example\r\n> \"The interview team was predominantly white, while our respondents were all Black men who had experienced incarceration. It is important to recognize that this positionality may have restricted the stories our interviewees felt able to tell, particularly about experiences of racism in the criminal justice system. Yet as the only researcher of color on the team, I drew upon my racial/ethnic background and familial experience with incarceration to build rapport. These shared sensibilities not only facilitated data collection, they influenced my analysis and writing. I directly and generously quote respondents as a way to ensure that the sociological claims made in this paper reflect the lived experiences of those most affected.\"\r\n\r\n---\r\n\r\n## Ethical Considerations Section\r\n\r\n### When Required\r\n- Vulnerable population (incarcerated, undocumented, survivors)\r\n- Non-standard consent procedures\r\n- Information security measures taken\r\n- Topics deliberately not asked\r\n\r\n### Structure (~200 words)\r\n1. **IRB approval** (~20 words)\r\n2. **Consent process** (~60 words): Standard or adapted?\r\n3. **Information security** (~60 words): What protections?\r\n4. **Scope limitations** (~60 words): What was NOT asked and why?\r\n\r\n### Example\r\n> \"The study received institutional review board approval. Because some participants were still being adjudicated, we did not want them to disclose any information that could jeopardize their case or safety. For this reason, we obtained verbal rather than written consent and did not maintain records of any personal information (e.g., jail ID number) that could create a 'paper trail.' Potential participants contacted us through a dedicated cellphone line or an encrypted email account. With similar ethical considerations in mind, we also did not ask for details regarding a participant's current or past charges. Each family also received a resource packet with information about housing, legal, and food assistance programs.\"\r\n\r\n---\r\n\r\n## Quality Indicators\r\n\r\n| Indicator | Description |\r\n|-----------|-------------|\r\n| **Comprehensive coverage** | All components documented |\r\n| **Organized complexity** | Subsections make structure navigable |\r\n| **Transparency** | Reader could nearly replicate study |\r\n| **Ethical rigor** | Protections documented for vulnerable populations |\r\n| **Reflexivity** | Researcher position acknowledged and addressed |\r\n| **Analytical clarity** | Clear path from data to themes |\r\n\r\n---\r\n\r\n## Downgrade Triggers\r\n\r\nConsider downgrading to Standard pathway when:\r\n- Strict word limits prohibit expansion\r\n- Supplementary materials can hold additional detail\r\n- Complexity lower than initially thought\r\n- Reviewer feedback suggests over-documentation\r\n\r\n---\r\n\r\n## Writing Checklist\r\n\r\n### Before Drafting\r\n- [ ] Confirmed vulnerability trigger OR complexity trigger\r\n- [ ] Identified which specific triggers apply\r\n- [ ] Reviewed exemplar articles for similar designs\r\n\r\n### After Drafting\r\n- [ ] 2,000-3,000 words\r\n- [ ] Study-led opening with design framing\r\n- [ ] Case/site justification included (if applicable)\r\n- [ ] Demographic table present\r\n- [ ] Population comparison included (if available)\r\n- [ ] Recruitment channels + process detailed\r\n- [ ] Response/refusal rates (if significant)\r\n- [ ] Duration with range and median\r\n- [ ] Compensation mentioned\r\n- [ ] Ethical considerations paragraph (if vulnerable)\r\n- [ ] Positionality discussion included\r\n- [ ] Specific codes listed in analysis\r\n- [ ] Software named\r\n- [ ] Analysis approach named with citation\r\n- [ ] Subsection headers used\r\n- [ ] No saturation claim\r\n",
        "plugins/methods-writer/skills/methods-writer/pathways/efficient.md": "# Efficient Pathway\r\n\r\n**Prevalence**: 10% of corpus (8/77 articles)\r\n\r\n**Contribution logic**: \"I conducted interviews with [population]; here's the essentials you need to know.\"\r\n\r\n---\r\n\r\n## Identifying Efficient Pathway Articles\r\n\r\nYour study fits the Efficient pathway if:\r\n- Design is straightforward (single-site, standard sampling)\r\n- Population is accessible and not vulnerable\r\n- Methods are secondary to theoretical contribution\r\n- Space constraints require compression\r\n- Study design requires minimal justification\r\n\r\n**Key indicators**:\r\n- Word count target: 600-900 words\r\n- No demographic table in methods body\r\n- No positionality discussion\r\n- Continuous prose (no subsections)\r\n\r\n---\r\n\r\n## Benchmarks\r\n\r\n| Feature | Efficient Pathway | Corpus Overall |\r\n|---------|-------------------|----------------|\r\n| **Target words** | 600-900 | 1,361 (median) |\r\n| **Median in pathway** | 563 | -- |\r\n| **Table** | No | 54% have tables |\r\n| **Subsections** | No | 33% have subsections |\r\n| **Positionality** | Rarely | 17% overall |\r\n\r\n---\r\n\r\n## Word Allocation\r\n\r\n```\r\nEFFICIENT METHODS SECTION (~700 words)\r\n\r\n[Opening + Study Overview]     ~150 words\r\n[Sample + Recruitment]         ~200 words\r\n[Protocol]                     ~150 words\r\n[Analysis]                     ~150 words\r\n[Confidentiality close]        ~50 words\r\n```\r\n\r\n---\r\n\r\n## Signature Moves (Do This)\r\n\r\n### 1. Integrated Opening\r\nCombine study introduction with population definition in a single efficient move.\r\n\r\n> \"I draw from 41 in-depth interviews with Black middle-class adults.\"\r\n\r\n> \"This paper draws on 47 interviews conducted with guest mothers in Chicago between 2019 and 2021.\"\r\n\r\n### 2. Definitional Work Early\r\nIf population requires definition, do so immediately without extensive justification.\r\n\r\n> \"Traditionally, sociologists define the middle-class through some combination of education, income, and occupation. I classify middle-class as meeting two out of three of these criteria.\"\r\n\r\n### 3. Recruitment in Passing\r\nDescribe recruitment method without channel-by-channel details.\r\n\r\n> \"To find middle-class participants, I used purposeful snowball sampling, recruiting from Black alumni associations and professional organizations.\"\r\n\r\n### 4. Prose Demographics\r\nReport sample composition in narrative form rather than table.\r\n\r\n> \"The sample was majority male (87 percent) and white (55 percent). Respondents ranged in age from 28 to 64.\"\r\n\r\n### 5. Duration as Quality Marker\r\nUse duration report as implicit quality signal without elaboration.\r\n\r\n> \"Interviews lasted an hour and fifteen minutes on average.\"\r\n\r\n### 6. Named Approach + Software\r\nCompress analysis to approach name, citation, and software mention.\r\n\r\n> \"Following an abductive approach (Timmermans and Tavory 2012), I coded using Dedoose.\"\r\n\r\n### 7. Confidentiality Close\r\nEnd with brief confidentiality statement.\r\n\r\n> \"For the sake of confidentiality, all respondents' names have been changed.\"\r\n\r\n---\r\n\r\n## Prohibited Moves (Don't Do This)\r\n\r\n### 1. Demographic Tables\r\nNo tables in methods body. If needed, reference appendix.\r\n\r\n**Why**: Tables signal complexity that Efficient pathway lacks.\r\n\r\n### 2. Extensive Case Justification\r\nDo not spend >100 words justifying why this site/population.\r\n\r\n**Why**: Case justification implies complexity requiring Detailed pathway.\r\n\r\n### 3. Response/Refusal Rate Discussion\r\nOmit unless it affects sample interpretation.\r\n\r\n**Why**: Word count priority; rates imply systematic sampling requiring more detail.\r\n\r\n### 4. Positionality Paragraphs\r\nDo not include dedicated positionality discussion.\r\n\r\n**Why**: Adds 150+ words without serving Efficient pathway's goals.\r\n\r\n### 5. Intercoder Reliability\r\nOmit coding process details beyond named approach.\r\n\r\n**Why**: Detailed coding discussion belongs in Standard/Detailed pathways.\r\n\r\n### 6. Multiple Subsections\r\nMaintain continuous prose; no section headers within methods.\r\n\r\n**Why**: Headers signal structural complexity incompatible with Efficient pathway.\r\n\r\n### 7. Ethical Considerations Paragraph\r\nMention IRB/consent briefly if at all; no dedicated paragraph.\r\n\r\n**Why**: Extended ethical discussion signals vulnerable population requiring Detailed pathway.\r\n\r\n### 8. Population Comparison\r\nDo not compare sample to broader population.\r\n\r\n**Why**: Comparison implies representativeness concerns requiring more documentation.\r\n\r\n---\r\n\r\n## Complete Example Structure\r\n\r\n```markdown\r\n## Data and Methods\r\n\r\n[Opening: \"I draw from 41 in-depth interviews with Black middle-class\r\nadults. Traditionally, sociologists define the middle-class through...\"]\r\n(~100 words)\r\n\r\n[Recruitment: \"To find middle-class participants, I used purposeful\r\nsnowball sampling, recruiting from Black alumni associations of large\r\nuniversities, Black faculty organizations, and professional networks.\"]\r\n(~80 words)\r\n\r\n[Sample description: \"Respondents ranged in age between 28 and 64.\r\nInterviews were conducted with 23 men and 18 women. Most held graduate\r\ndegrees and worked in professional occupations.\"]\r\n(~60 words)\r\n\r\n[Protocol: \"I began each interview with the same questions, including\r\nquestions about [topic A] and [topic B]. I then followed up on themes\r\nas the discussion progressed. Interviews lasted an hour and fifteen\r\nminutes on average.\"]\r\n(~80 words)\r\n\r\n[Analysis: \"I tape-recorded each interview, and all recordings were\r\nprofessionally transcribed. I used Dedoose and conducted an initial\r\ncoding focusing on broad themes. In the second round, I focused on\r\n[specific focus].\"]\r\n(~80 words)\r\n\r\n[Close: \"For the sake of confidentiality, all respondents' names have\r\nbeen changed.\"]\r\n(~15 words)\r\n\r\nTOTAL: ~515 words\r\n```\r\n\r\n---\r\n\r\n## Exemplar Articles\r\n\r\n| Article | Words | Notable Feature |\r\n|---------|-------|-----------------|\r\n| 10.1093_sf_soy109 | 457 | Briefest in corpus; multi-group sample in minimal space |\r\n| 10.1093_socpro_spaa008 | 537 | Opens with methodological justification for interviews (rare) |\r\n| 10.1093_socpro_spaa039 | 537 | Excellent definitional work; multiple coding rounds compressed |\r\n| 10.1093_sf_soac056 | 570 | Subsection headers despite short length; longitudinal component |\r\n\r\n---\r\n\r\n## Risks and Mitigations\r\n\r\n| Risk | Mitigation |\r\n|------|------------|\r\n| **Under-documentation** | Only use for genuinely simple designs |\r\n| **Reviewer pushback** | Be prepared to expand for revisions |\r\n| **Credibility concerns** | Ensure sample N and duration are reported |\r\n| **Missing protections** | Do NOT use for vulnerable populations |\r\n\r\n---\r\n\r\n## Upgrade Triggers\r\n\r\nConsider upgrading to Standard pathway when:\r\n- Reviewer requests more detail\r\n- Sample composition is crucial for claims\r\n- Subfield norms expect more documentation\r\n- Adding a table would strengthen credibility\r\n\r\n---\r\n\r\n## Writing Checklist\r\n\r\n### Before Drafting\r\n- [ ] Confirmed population is NOT vulnerable\r\n- [ ] Confirmed design is straightforward\r\n- [ ] Confirmed space constraints or methods-secondary rationale\r\n\r\n### After Drafting\r\n- [ ] Under 900 words\r\n- [ ] Study-led opening (not method-justified)\r\n- [ ] Sample N clearly stated\r\n- [ ] Demographics in prose (no table)\r\n- [ ] Recruitment named in 1-2 sentences\r\n- [ ] Duration range reported\r\n- [ ] Analysis approach named with citation\r\n- [ ] Confidentiality statement present\r\n- [ ] No saturation claim\r\n- [ ] No positionality paragraph\r\n- [ ] No subsection headers\r\n",
        "plugins/methods-writer/skills/methods-writer/pathways/standard.md": "# Standard Pathway\r\n\r\n**Prevalence**: 61% of corpus (47/77 articles) - **DEFAULT**\r\n\r\n**Contribution logic**: \"Here's a balanced account of our data, sampling, and analysis that establishes credibility without over-documenting.\"\r\n\r\n---\r\n\r\n## Identifying Standard Pathway Articles\r\n\r\nYour study fits the Standard pathway if:\r\n- Typical single-site interview study\r\n- Accessible population (not vulnerable)\r\n- Standard sampling (purposive, snowball, convenience)\r\n- 30-80 interviews\r\n- No unusual ethical concerns\r\n- Journal expects moderate methods detail\r\n\r\n**Key indicators**:\r\n- Word count target: 1,200-1,500 words\r\n- Demographic table recommended (60% have tables)\r\n- 0-2 subsections optional\r\n- Positionality conditional (15% include)\r\n\r\n**Standard is the DEFAULT pathway**. Use when no Efficient or Detailed triggers apply.\r\n\r\n---\r\n\r\n## Benchmarks\r\n\r\n| Feature | Standard Pathway | Corpus Overall |\r\n|---------|------------------|----------------|\r\n| **Target words** | 1,200-1,500 | 1,361 (median) |\r\n| **Median in pathway** | 1,237 | -- |\r\n| **Table** | Recommended | 54% have tables |\r\n| **Subsections** | 0-2 optional | 33% have subsections |\r\n| **Positionality** | Conditional (15%) | 17% overall |\r\n\r\n---\r\n\r\n## Word Allocation\r\n\r\n```\r\nSTANDARD METHODS SECTION (~1,350 words)\r\n\r\n[Opening + Study Framing]      ~150 words\r\n[Sample Overview + Table]      ~300 words\r\n[Recruitment Process]          ~200 words\r\n[Protocol]                     ~250 words\r\n[Analysis]                     ~250 words\r\n[Optional: Positionality]      ~150 words\r\n[Optional: Transition]         ~50 words\r\n```\r\n\r\n---\r\n\r\n## Signature Moves (Do This)\r\n\r\n### 1. Conceptual Frame Opening\r\nOpen with brief positioning before introducing data.\r\n\r\n> \"While the recent revival of elite studies has been dominated by economic definitions, here we draw on a wider conception that combines positional and reputational definitions. Specifically, we analyze 71 interviews from two complementary research projects.\"\r\n\r\n### 2. Sample with Table Reference\r\nIntroduce sample with prose description plus table.\r\n\r\n> \"Table 1 presents selected characteristics of our participants.\"\r\n\r\n### 3. Recruitment Channels Named\r\nList specific channels used for recruitment.\r\n\r\n> \"Participants were recruited from a number of organizations, some of which actively engaged in [activity], while others focused purely on [other activity].\"\r\n\r\n### 4. Topic Areas Listed\r\nProvide numbered or bulleted list of interview guide areas.\r\n\r\n> \"The interview guide asked questions about: 1) career pathways; 2) organizational experiences; and 3) professional identity.\"\r\n\r\n### 5. Duration with Bounds\r\nReport duration as range with meaningful bounds.\r\n\r\n> \"Interviews lasted between 45 minutes and two hours.\"\r\n\r\n### 6. Multi-Round Coding\r\nDescribe iterative coding process with some specificity.\r\n\r\n> \"Analysis involved a two-step process of coding. Initial coding revolved around reading each transcript in its entirety and developing broad thematic codes. Initial coding was then followed by a second step of more focused line-by-line coding.\"\r\n\r\n### 7. Conditional Positionality\r\nInclude positionality when interviewer-respondent mismatch is notable.\r\n\r\n> \"The interview team was white and majority female, with two male interviewers, while our respondents were all Black. We did not purposely match interviewer and respondent characteristics.\"\r\n\r\n### 8. Sampling Logic Articulated\r\nExplain the reasoning behind sampling approach.\r\n\r\n> \"We 'sampled for range' (Weiss 1995), attempting to at least partially capture the experiences of minority sub-groups within the broader population.\"\r\n\r\n---\r\n\r\n## Prohibited Moves (Don't Do This)\r\n\r\n### 1. Excessive Case Justification\r\nDo not spend >200 words justifying case selection unless comparative.\r\n\r\n**Why**: Extended justification signals Detailed pathway complexity.\r\n\r\n### 2. Full Respondent Tables\r\nDo not list all individual respondents (use aggregate demographics).\r\n\r\n**Why**: Detailed pathway move; Standard uses summary tables.\r\n\r\n### 3. Variable Operationalization\r\nThis is not a quantitative methods section; avoid quasi-experimental framing unless truly necessary.\r\n\r\n**Why**: Interview methods sections don't \"operationalize\" variables.\r\n\r\n### 4. Extensive Ethical Considerations\r\nUnless population is vulnerable, keep ethics to consent/IRB mention.\r\n\r\n**Why**: Extended ethics signals vulnerable population requiring Detailed pathway.\r\n\r\n### 5. Prevalence Statistics for Qualitative Themes\r\nSave for Detailed pathway.\r\n\r\n**Why**: Reporting \"25% mentioned X\" requires more methodological justification.\r\n\r\n### 6. Multiple Tables\r\nOne table maximum in methods body.\r\n\r\n**Why**: Multiple tables signal Detailed pathway complexity.\r\n\r\n---\r\n\r\n## Complete Example Structure\r\n\r\n```markdown\r\n## Methodology\r\n\r\n[Opening: \"While the recent revival of X studies has been dominated by\r\nY definitions, here we draw on a wider conception... Specifically, we\r\nanalyze 71 interviews from two complementary research projects...\"]\r\n(~100 words)\r\n\r\n[Source 1: \"The UK interviewees are drawn from a wider sample of 217\r\ninterviews conducted with entrants of Who's Who...\"]\r\n(~80 words)\r\n\r\n[Source 2: \"Danish interviewees are recruited from a wider sample of 37\r\ninterviews conducted with individuals within the...\"]\r\n(~60 words)\r\n\r\n[Sample selection: \"To ensure we compare similar groups, we focus both\r\nsamples to concentrate on 71 individuals who...\"]\r\n(~100 words)\r\n\r\n[Recruitment UK: \"UK interviewees were recruited in two stages. From\r\n[date] to [date], X interviewees were purposively sampled via email...\"]\r\n(~120 words)\r\n\r\n[Recruitment Denmark: \"Danish interviewees were selected to ensure\r\nmaximum variation in...\"]\r\n(~80 words)\r\n\r\n[Table reference and demographics: \"Appendix Table 1 provides demographic\r\ninformation... As Table 1 shows, our interview sample is strongly\r\ngendered (X men, Y women), overwhelmingly white, and...\"]\r\n(~150 words)\r\n\r\n[Protocol: \"Interviews were semi-structured and lasted between 1 and 3\r\nhours. Although the interview schedules differed in some areas, both\r\ncontained three key sets of questions that we draw on here...\"]\r\n(~150 words)\r\n\r\n[Analysis: \"Although the data were initially coded separately by each\r\nteam, the specific sub-sample used here was then re-coded again by both\r\nteams together... Analysis involved a two-step process...\"]\r\n(~150 words)\r\n\r\n[Anonymity note: \"An important difference between the British and Danish\r\ninterviews concerns anonymity...\"]\r\n(~100 words)\r\n\r\n[Positionality: \"It is important to recognize that our own positionality\r\nas white male academics based in the UK and Denmarkmay have...\"]\r\n(~100 words)\r\n\r\nTOTAL: ~1,190 words\r\n```\r\n\r\n---\r\n\r\n## Exemplar Articles\r\n\r\n| Article | Words | Notable Feature |\r\n|---------|-------|-----------------|\r\n| 10.1093_sf_soad131 | 1,224 | Comparative design; strategic positionality |\r\n| 10.1093_socpro_spaa069 | 1,361 | Multi-perspective interviews; good table |\r\n| 10.1093_socpro_spaa028 | 1,237 | Exactly at median; comparative case justification |\r\n| 10.1093_sf_soy062 | 1,198 | Stratified random sampling; abductive approach |\r\n\r\n---\r\n\r\n## Quality Indicators\r\n\r\n| Indicator | Description |\r\n|-----------|-------------|\r\n| **Balanced coverage** | All components receive proportional attention |\r\n| **Clear flow** | Natural progression from sample to recruitment to protocol to analysis |\r\n| **Table that adds value** | Demographics presented efficiently |\r\n| **Appropriate depth** | Not too thin, not over-justified |\r\n| **Coding transparency** | Reader understands how themes emerged |\r\n\r\n---\r\n\r\n## Table Design Guidance\r\n\r\n### When to Include a Table\r\n- Sample N > 30\r\n- Multiple demographic dimensions matter for interpretation\r\n- Sample composition is crucial for claims\r\n- Want to save word count (table replaces ~200 words of prose)\r\n\r\n### Standard Table Structure\r\n\r\n| Characteristic | N (%) |\r\n|----------------|-------|\r\n| **Gender** | |\r\n| Women | X (%) |\r\n| Men | Y (%) |\r\n| **Race/Ethnicity** | |\r\n| White | X (%) |\r\n| Black | Y (%) |\r\n| Hispanic | Z (%) |\r\n| **Age** | |\r\n| Mean (range) | X (Y-Z) |\r\n\r\n### Table Placement\r\n- Reference in text before table appears\r\n- Can be in appendix (reference as \"Appendix Table 1\")\r\n- Keep in methods body if central to interpretation\r\n\r\n---\r\n\r\n## Positionality Decision\r\n\r\nInclude positionality (~150 words) when:\r\n- [ ] Interviewer-respondent race/ethnicity mismatch\r\n- [ ] Interviewer-respondent gender mismatch (if relevant to topic)\r\n- [ ] Researcher studied own community (insider status)\r\n- [ ] Topic involves sensitive disclosures\r\n- [ ] Identity shaped access or rapport\r\n\r\nOmit positionality when:\r\n- No significant identity mismatch\r\n- Access was straightforward\r\n- Topic is not particularly sensitive\r\n- Characteristics are aligned or not relevant\r\n\r\n---\r\n\r\n## Upgrade Triggers\r\n\r\nConsider upgrading to Detailed pathway when:\r\n- Population is vulnerable or marginalized\r\n- Design becomes multi-site or comparative\r\n- Sample exceeds 100 interviews\r\n- Access required significant negotiation\r\n- Reviewer requests more transparency\r\n\r\n## Downgrade Triggers\r\n\r\nConsider downgrading to Efficient pathway when:\r\n- Strict word limits prohibit expansion\r\n- Methods are genuinely secondary to contribution\r\n- Study replicates established approach\r\n\r\n---\r\n\r\n## Writing Checklist\r\n\r\n### Before Drafting\r\n- [ ] Confirmed population is NOT vulnerable\r\n- [ ] Confirmed design is not multi-site/comparative/longitudinal\r\n- [ ] Confirmed no space constraints requiring Efficient\r\n\r\n### After Drafting\r\n- [ ] 1,200-1,500 words\r\n- [ ] Study-led opening (not method-justified)\r\n- [ ] Sample N clearly stated\r\n- [ ] Table included OR comprehensive prose demographics\r\n- [ ] Recruitment channels named\r\n- [ ] Duration with range\r\n- [ ] Recording/transcription mentioned\r\n- [ ] Software named\r\n- [ ] Analysis approach named with citation\r\n- [ ] Multi-step coding process described\r\n- [ ] Positionality included IF triggers apply\r\n- [ ] No saturation claim\r\n- [ ] Confidentiality noted\r\n",
        "plugins/methods-writer/skills/methods-writer/phases/phase0-assessment.md": "# Phase 0: Assessment\r\n\r\nYou are executing Phase 0 of methods-writer. Your goal is to gather study information and select the appropriate pathway for the Methods section.\r\n\r\n## Why This Phase Matters\r\n\r\nMethods sections follow predictable patterns based on study complexity and population characteristics. Selecting the wrong pathway leads to either under-documentation (risking reviewer pushback) or over-documentation (wasting precious word count). This phase ensures deliberate pathway selection based on your study's actual needs.\r\n\r\n---\r\n\r\n## Your Tasks\r\n\r\n### 1. Gather Study Information\r\n\r\nCollect from the user:\r\n\r\n**Required**:\r\n- Sample size (N)\r\n- Population description\r\n- Recruitment approach\r\n- Interview format and duration\r\n- Analysis approach\r\n- Data collection dates and location\r\n\r\n**For Pathway Selection**:\r\n- Is the population vulnerable or marginalized?\r\n- Is the design complex (multi-site, comparative, longitudinal)?\r\n- Are there access challenges or gatekeeper relationships?\r\n- Are there journal word limits or space constraints?\r\n\r\n### 2. Apply the Pathway Decision Tree\r\n\r\nWork through this diagnostic:\r\n\r\n```\r\nPATHWAY DECISION TREE\r\n\r\n1. Is the population VULNERABLE or MARGINALIZED?\r\n   - Incarcerated, detained, legally precarious (e.g., undocumented)\r\n   - Survivors of violence, abuse, or trauma\r\n   - Sex workers, gang members, or other stigmatized groups\r\n   - Anyone whose participation requires extra ethical safeguards\r\n\r\n    YES to any: DETAILED PATHWAY\r\n\r\n2. Is the design COMPLEX?\r\n   - Multi-site (2+ locations or organizations)\r\n   - Comparative (systematic comparison across groups)\r\n   - Longitudinal (repeated interviews over time)\r\n   - Large sample (100+ interviews)\r\n   - Merged data sources (interviews + survey + archival)\r\n   - Significant access challenges requiring documentation\r\n\r\n    YES to any: DETAILED PATHWAY\r\n\r\n3. Are there SPACE CONSTRAINTS or is METHODS SECONDARY?\r\n   - Strict journal word limits (methods capped at 500-700 words)\r\n   - Theory-building article where methods are routine\r\n   - Very straightforward design (convenience sample, standard protocol)\r\n   - Pilot study with limited claims\r\n\r\n    YES: EFFICIENT PATHWAY\r\n\r\n4. None of the above apply?\r\n\r\n    STANDARD PATHWAY (DEFAULT)\r\n```\r\n\r\n### 3. Review Pathway Profile\r\n\r\nOnce you have a candidate pathway, consult the detailed profile:\r\n- `pathways/efficient.md` (600-900 words)\r\n- `pathways/standard.md` (1,200-1,500 words)\r\n- `pathways/detailed.md` (2,000-3,000 words)\r\n\r\nVerify the profile matches the user's study characteristics.\r\n\r\n### 4. Assess Optional Components\r\n\r\nBased on study characteristics, determine whether to include:\r\n\r\n**Positionality** (17% of corpus):\r\n- [ ] Notable interviewer-respondent identity mismatch\r\n- [ ] Vulnerable population where identity affects disclosure\r\n- [ ] Researcher identity shaped access or rapport\r\n- [ ] Topic is sensitive and researcher position matters\r\n\r\n**Ethical Considerations Paragraph** (for vulnerable populations):\r\n- [ ] Non-standard consent (verbal, waived)\r\n- [ ] Information security measures\r\n- [ ] Topics deliberately not asked\r\n- [ ] Protections for legally precarious participants\r\n\r\n**Response/Refusal Rates**:\r\n- [ ] Significant refusal rate that affects sample interpretation\r\n- [ ] Institutional recruitment with documented response\r\n\r\n### 5. Write Assessment Memo\r\n\r\nCreate a summary with:\r\n\r\n```markdown\r\n# Methods Section Assessment\r\n\r\n## Study Summary\r\n- **Sample**: [N] interviews with [population]\r\n- **Recruitment**: [approach]\r\n- **Protocol**: [format], [duration range]\r\n- **Analysis**: [approach with citation]\r\n- **Dates/Location**: [when and where]\r\n\r\n## Pathway Selection Analysis\r\n- [Walk through decision tree logic]\r\n- [Note which triggers apply or don't apply]\r\n\r\n## Recommended Pathway\r\n**[Efficient/Standard/Detailed]**\r\n\r\n### Rationale\r\n- [Why this pathway fits]\r\n- [What characteristics drove the selection]\r\n\r\n### Alternative Considered (if applicable)\r\n- [Why rejected alternative]\r\n\r\n## Pathway Implications\r\n- **Target word count**: [range]\r\n- **Table**: [Required/Recommended/Omit]\r\n- **Subsections**: [recommendation]\r\n- **Positionality**: [Include/Omit with rationale]\r\n\r\n## Optional Components to Include\r\n- [ ] Positionality\r\n- [ ] Ethical considerations paragraph\r\n- [ ] Response/refusal rates\r\n- [ ] Population comparison\r\n- [ ] Specific codes listed\r\n\r\n## Questions for User\r\n- [Any clarifying questions needed before drafting]\r\n```\r\n\r\n### 6. Present Recommendation\r\n\r\nPresent to user:\r\n- Recommended pathway with clear rationale\r\n- Target word count and structural expectations\r\n- Optional components to include\r\n- Any questions before proceeding\r\n\r\n---\r\n\r\n## Guiding Principles\r\n\r\n### Standard Is the Default\r\nWhen in doubt, choose Standard. It's the most common pathway (61%) and fits most interview studies. Efficient and Detailed are for specific circumstances.\r\n\r\n### Vulnerability Trumps Everything\r\nIf the population is vulnerable, use Detailed regardless of other factors. Ethical transparency is non-negotiable.\r\n\r\n### Space Constraints Are Real\r\nIf the journal caps methods at 700 words, Efficient isn't optionalit's required. Don't fight the format.\r\n\r\n### Complexity Deserves Documentation\r\nMulti-site, comparative, and longitudinal designs require explanation. Readers need to understand your case selection logic and timing.\r\n\r\n### Positionality Is Not Mandatory\r\nOnly 17% of corpus articles include positionality. Include it when specific triggers apply, not by default.\r\n\r\n---\r\n\r\n## Output Files to Create\r\n\r\n1. **assessment-memo.md** - Full assessment with pathway recommendation\r\n2. (Optional) **questions-for-user.md** - If clarification needed\r\n\r\n---\r\n\r\n## When You're Done\r\n\r\nReport to the orchestrator:\r\n- Recommended pathway\r\n- Key rationale (2-3 sentences)\r\n- Any outstanding questions\r\n- Readiness assessment for Phase 1\r\n\r\nExample summary:\r\n> \"I recommend **Standard pathway** (1,200-1,500 words) for this study of 47 middle-class parents. The population is not vulnerable, the design is straightforward (single-site, snowball sampling), and there are no unusual space constraints. I recommend including a demographic table but omitting positionality since the interviewer-respondent characteristics are aligned. Ready for Phase 1: Drafting.\"\r\n",
        "plugins/methods-writer/skills/methods-writer/phases/phase1-drafting.md": "# Phase 1: Drafting\r\n\r\nYou are executing Phase 1 of methods-writer. Your goal is to write a complete Methods section following the selected pathway template and word allocation.\r\n\r\n## Why This Phase Matters\r\n\r\nMethods sections must accomplish multiple goals: establish credibility, provide replication information, acknowledge limitations, and do so within strict word constraints. This phase transforms the pathway template into prose that matches the conventions of *Social Problems* and *Social Forces*.\r\n\r\n---\r\n\r\n## Inputs\r\n\r\nBefore starting, gather:\r\n1. Assessment memo from Phase 0 (pathway selection, optional components)\r\n2. User's study details (sample, recruitment, protocol, analysis)\r\n3. Pathway profile from `pathways/[pathway].md`\r\n4. Component checklist from `techniques/component-checklist.md`\r\n5. Opening moves from `techniques/opening-moves.md`\r\n\r\n---\r\n\r\n## Your Tasks\r\n\r\n### 1. Follow the Pathway Structure\r\n\r\nEach pathway has a specific word allocation. Match your draft to these targets:\r\n\r\n#### EFFICIENT PATHWAY (~700 words)\r\n```\r\n[Opening + Study Overview]     ~150 words\r\n[Sample + Recruitment]         ~200 words\r\n[Protocol]                     ~150 words\r\n[Analysis]                     ~150 words\r\n[Confidentiality close]        ~50 words\r\n```\r\n\r\n#### STANDARD PATHWAY (~1,350 words)\r\n```\r\n[Opening + Study Framing]      ~150 words\r\n[Sample Overview + Table]      ~300 words\r\n[Recruitment Process]          ~200 words\r\n[Protocol]                     ~250 words\r\n[Analysis]                     ~250 words\r\n[Optional: Positionality]      ~150 words\r\n[Optional: Transition]         ~50 words\r\n```\r\n\r\n#### DETAILED PATHWAY (~2,500 words)\r\n```\r\n[Study Overview + Design]      ~300 words\r\n[Case/Site Description]        ~250 words\r\n[Sample with Table(s)]         ~400 words\r\n[Recruitment Process]          ~350 words\r\n[Protocol]                     ~350 words\r\n[Ethical Considerations]       ~200 words (if vulnerable)\r\n[Positionality]                ~200 words\r\n[Analysis]                     ~350 words\r\n[Limitations/Transition]       ~100 words\r\n```\r\n\r\n### 2. Draft the Opening\r\n\r\nUse a **study-led opening** (88% of corpus). Lead with your data, not methodological justification.\r\n\r\n**Preferred patterns**:\r\n\r\n- **Direct data introduction**:\r\n  > \"I draw from [N] in-depth interviews with [population].\"\r\n\r\n- **Study framing + data**:\r\n  > \"This study draws on [N] interviews conducted with [population] between [dates].\"\r\n\r\n- **Conceptual frame + data** (Standard/Detailed):\r\n  > \"While studies of [topic] have focused on [X], here we draw on [N] interviews to examine [Y].\"\r\n\r\n**Avoid** method-justification openings:\r\n- \"Qualitative methods are appropriate because...\"\r\n- \"Interviews allow researchers to...\"\r\n- \"To understand [topic], I used...\"\r\n\r\nSee `techniques/opening-moves.md` for more patterns.\r\n\r\n### 3. Draft Each Component\r\n\r\nWork through components in order. For each, consult `techniques/component-checklist.md` for what to include.\r\n\r\n#### A. Sample Size and Composition\r\n\r\n**Required for all pathways**:\r\n- Total N\r\n- Basic demographics (gender, race/ethnicity, age range)\r\n\r\n**Efficient**: Report in prose\r\n> \"The sample of [N] [population] was majority female (X percent) and included [demographic breakdown].\"\r\n\r\n**Standard**: Prose + table reference\r\n> \"Table 1 presents selected characteristics of our [N] participants.\"\r\n\r\n**Detailed**: Table + comparison to population\r\n> \"Table 1 presents selected characteristics of our participants and compares them to estimates of the [broader population].\"\r\n\r\n#### B. Recruitment Strategy\r\n\r\n**Name the approach** (snowball, purposive, convenience, stratified):\r\n> \"I used purposeful snowball sampling, recruiting from [channels].\"\r\n\r\n**Efficient**: One sentence describing approach.\r\n\r\n**Standard**: Named approach + channels listed.\r\n> \"Participants were recruited through [organization A], [organization B], and snowball referrals.\"\r\n\r\n**Detailed**: Channels + eligibility + response rates.\r\n> \"215 [group] were asked to participate; 75 refused, resulting in a final sample of 140.\"\r\n\r\n#### C. Interview Protocol\r\n\r\n**Format** (can be implicit): Semi-structured, in-depth, life history\r\n\r\n**Duration** (required): Report range, optionally with average/median\r\n> \"Interviews lasted between 45 minutes and two hours.\"\r\n> \"Interviews ranged from one to three hours; the median was 97 minutes.\"\r\n\r\n**Topics covered**:\r\n- **Efficient**: Brief mention\r\n  > \"Interviews covered [topic A] and [topic B].\"\r\n\r\n- **Standard**: Listed\r\n  > \"The interview guide asked questions about: 1) [topic]; 2) [topic]; and 3) [topic].\"\r\n\r\n- **Detailed**: Listed with sample questions\r\n  > \"We began with broad questions such as '[exact question]?' and then asked...\"\r\n\r\n**Recording and transcription** (Standard/Detailed):\r\n> \"All interviews were audio recorded and transcribed.\"\r\n\r\n**Location** (Standard/Detailed):\r\n> \"Interviews were conducted at libraries, cafes, and participants' homes.\"\r\n\r\n**Compensation** (Standard/Detailed):\r\n> \"Participants received a $[X] gift card.\"\r\n\r\n**Language** (if relevant):\r\n> \"Interviews were conducted in English or Spanish, depending on respondent preference.\"\r\n\r\n#### D. Analysis Approach\r\n\r\n**Name the approach with citation**:\r\n> \"Following an abductive approach (Timmermans and Tavory 2012)...\"\r\n> \"I used grounded theory methods (Charmaz 2006)...\"\r\n> \"We used flexible coding procedures (Deterding and Waters 2018)...\"\r\n\r\n**Software** (Standard/Detailed):\r\n> \"Transcripts were coded using Dedoose.\"\r\n> \"Interview transcripts were analyzed in ATLAS.ti.\"\r\n\r\n**Coding process**:\r\n- **Efficient**: Named approach only\r\n  > \"Following an abductive approach (Timmermans and Tavory 2012), I coded using Dedoose.\"\r\n\r\n- **Standard**: Process described\r\n  > \"Analysis involved a two-step process. Initial coding revolved around reading each transcript in its entirety... followed by focused line-by-line coding.\"\r\n\r\n- **Detailed**: Process + specific codes\r\n  > \"These codes included *trust, rejection, loneliness, shame*, among others.\"\r\n\r\n**Intercoder reliability** (Detailed, if team):\r\n> \"All transcripts were coded by at least two research team members and reviewed weekly.\"\r\n\r\n#### E. Positionality (Optional, 17% of corpus)\r\n\r\n**Include when**: Identity mismatch, vulnerable population, identity shaped access.\r\n\r\n**Structure**:\r\n1. Identity disclosure (brief)\r\n2. How identity may have constrained data\r\n3. How identity may have enabled data\r\n4. Mitigation strategies (if applicable)\r\n\r\n**Example**:\r\n> \"The interview team was white and majority female, while our respondents were all Black. It is important to recognize that this positionality may have restricted the stories our interviewees felt able to tell. Yet it may also have provoked narrations particularly relevant to our research questions.\"\r\n\r\n#### F. Ethical Considerations (Detailed, if vulnerable population)\r\n\r\n**Include**: IRB, consent process, information security, what was NOT asked.\r\n\r\n**Example**:\r\n> \"Because some participants were still being adjudicated, we did not want them to disclose any information that could jeopardize their case. For this reason, we obtained verbal rather than written consent and did not maintain records of personal information.\"\r\n\r\n#### G. Confidentiality Close (Efficient/Standard)\r\n\r\n**Brief**:\r\n> \"For the sake of confidentiality, all respondents' names have been changed.\"\r\n> \"To ensure confidentiality, I have used pseudonyms throughout.\"\r\n\r\n### 4. Calibrate Word Count\r\n\r\nAfter drafting, check word count against targets:\r\n\r\n| Pathway | Target | Acceptable Range |\r\n|---------|--------|------------------|\r\n| Efficient | 700 | 600-900 |\r\n| Standard | 1,350 | 1,200-1,500 |\r\n| Detailed | 2,500 | 2,000-3,000 |\r\n\r\nIf over target:\r\n- Cut method justification (readers assume interviews are appropriate)\r\n- Move demographics to table\r\n- Compress recruitment channel descriptions\r\n\r\nIf under target:\r\n- Add recruitment channel detail\r\n- Expand analysis process description\r\n- Add location/compensation if omitted\r\n\r\n### 5. Avoid Common Errors\r\n\r\n**Do NOT**:\r\n- Claim saturation (only 4% of corpus does this)\r\n- Use method-justification openings\r\n- Include positionality by default (only 17%)\r\n- Over-specify coding rounds without purpose\r\n- Include response rates unless relevant\r\n\r\n**DO**:\r\n- Open with the study/sample\r\n- Name the analysis approach with citation\r\n- Report duration as range\r\n- Include table reference if using table\r\n- End with confidentiality note (Efficient/Standard)\r\n\r\n---\r\n\r\n## Guiding Principles\r\n\r\n### 1. Study-Led, Not Method-Justified\r\nLead with what you did, not why interviews are good. 88% of corpus does this.\r\n\r\n### 2. Match Pathway to Content\r\nFollow the word allocation. Don't write 2,500 words for a Standard study.\r\n\r\n### 3. Tables Save Words\r\nA demographic table can replace 200+ words of prose. Use tables when N > 30 or composition matters.\r\n\r\n### 4. Saturation Alternatives\r\nIf asked about stopping criteria, offer alternatives:\r\n- \"Continued until key themes emerged across subgroups\"\r\n- \"Sample size reflects comparative considerations\"\r\n- \"Recruitment ended when [pragmatic bound]\"\r\n\r\n### 5. Named Approaches Require Citations\r\nIf you name an approach (abductive, grounded theory, flexible coding), cite it.\r\n\r\n---\r\n\r\n## Output Files to Create\r\n\r\n1. **methods-draft.md** - Complete Methods section\r\n2. **drafting-notes.md** - Notes on word count, components included/omitted, areas needing revision\r\n\r\n---\r\n\r\n## When You're Done\r\n\r\nReport to the orchestrator:\r\n- Draft complete (yes/no)\r\n- Word count achieved vs. target\r\n- Components included\r\n- Components omitted (with rationale)\r\n- Areas flagged for revision\r\n\r\nExample summary:\r\n> \"**Draft complete**. 1,312 words (target: 1,350). Standard pathway. Included: sample with table reference, recruitment, protocol, analysis process. Omitted: positionality (no identity mismatch), ethical considerations (not vulnerable). Flagged: transition from analysis to findings could be smoother. Ready for Phase 2: Revision.\"\r\n",
        "plugins/methods-writer/skills/methods-writer/phases/phase2-revision.md": "# Phase 2: Revision\r\n\r\nYou are executing Phase 2 of methods-writer. Your goal is to calibrate the draft against benchmarks, verify component coverage, and polish the prose to publication quality.\r\n\r\n## Why This Phase Matters\r\n\r\nMethods sections are scrutinized by reviewers for both completeness and concision. Under-documentation raises credibility concerns; over-documentation wastes precious word count. This phase ensures your Methods section matches field norms and accomplishes its goals efficiently.\r\n\r\n---\r\n\r\n## Your Tasks\r\n\r\n### 1. Word Count Calibration\r\n\r\nCompare actual word count to pathway targets:\r\n\r\n| Pathway | Target | Acceptable Range |\r\n|---------|--------|------------------|\r\n| Efficient | 700 | 600-900 |\r\n| Standard | 1,350 | 1,200-1,500 |\r\n| Detailed | 2,500 | 2,000-3,000 |\r\n\r\n**If over target**:\r\n- [ ] Cut method-justification language (\"Interviews are appropriate because...\")\r\n- [ ] Move prose demographics to table\r\n- [ ] Compress recruitment channel descriptions\r\n- [ ] Remove intercoder reliability if not required\r\n- [ ] Shorten analysis process description\r\n\r\n**If under target**:\r\n- [ ] Add recruitment channel specifics\r\n- [ ] Expand analysis process (coding rounds, software)\r\n- [ ] Add location and compensation details\r\n- [ ] Consider adding sample question from interview guide\r\n- [ ] Add duration median if only range provided\r\n\r\n### 2. Component Verification\r\n\r\nCheck all required components are present:\r\n\r\n#### All Pathways\r\n- [ ] Sample N stated clearly\r\n- [ ] Population described\r\n- [ ] Recruitment approach named\r\n- [ ] Duration reported (range minimum)\r\n- [ ] Analysis approach named with citation\r\n\r\n#### Efficient Pathway\r\n- [ ] Demographics in prose (no table)\r\n- [ ] No extensive case justification\r\n- [ ] No positionality (unless essential)\r\n- [ ] Confidentiality statement present\r\n\r\n#### Standard Pathway\r\n- [ ] Table reference if table used\r\n- [ ] Software mentioned\r\n- [ ] Recording/transcription noted\r\n- [ ] Compensation mentioned\r\n- [ ] Positionality included IF triggers apply\r\n\r\n#### Detailed Pathway\r\n- [ ] Demographic table present\r\n- [ ] Response/refusal rates if relevant\r\n- [ ] Ethical considerations if vulnerable population\r\n- [ ] Positionality discussion present\r\n- [ ] Specific codes listed in analysis\r\n- [ ] Subsection headers used\r\n\r\n### 3. Opening Move Check\r\n\r\nVerify opening follows study-led pattern (88% of corpus):\r\n\r\n**Good openings**:\r\n- \"I draw from X interviews with [population].\"\r\n- \"This study draws on X interviews conducted with...\"\r\n- \"We analyze X in-depth interviews from...\"\r\n\r\n**Problematic openings** (revise if present):\r\n- \"Qualitative methods are appropriate because...\"\r\n- \"Interviews allow researchers to...\"\r\n- \"To understand [topic], I used interviews...\"\r\n\r\n### 4. Saturation Check\r\n\r\nVerify no unwarranted saturation claims (only 4% of corpus):\r\n\r\n**If saturation claimed**, consider replacing with:\r\n- \"Continued until key themes emerged across subgroups\"\r\n- \"Sample size reflects comparative adequacy\"\r\n- \"Recruitment continued until [pragmatic bound]\"\r\n\r\n**Acceptable saturation mentions** (rare):\r\n- When genuinely using theoretical sampling with documented saturation\r\n- When explicitly discussing sampling strategy literature\r\n\r\n### 5. Positionality Check\r\n\r\nVerify positionality is handled appropriately (only 17% include):\r\n\r\n**Include positionality when**:\r\n- [ ] Notable interviewer-respondent identity mismatch\r\n- [ ] Vulnerable population where identity affects disclosure\r\n- [ ] Researcher identity shaped access or rapport\r\n- [ ] Topic is sensitive and researcher position matters\r\n\r\n**Omit positionality when**:\r\n- No significant identity mismatch\r\n- Population is not vulnerable\r\n- Access was straightforward\r\n- Routine study with matched interviewers\r\n\r\n### 6. Prose Quality Check\r\n\r\nReview for common issues:\r\n\r\n**Transitions**:\r\n- [ ] Each component flows naturally to the next\r\n- [ ] No abrupt topic shifts\r\n- [ ] Final sentence sets up findings section (if applicable)\r\n\r\n**Verb tense**:\r\n- [ ] Past tense for data collection procedures\r\n- [ ] Present tense for ongoing analysis insights (rare)\r\n\r\n**Passive vs. active**:\r\n- [ ] \"I interviewed\" or \"We conducted\" (active) preferred over \"Interviews were conducted\" (passive)\r\n- [ ] First person acceptable in methods\r\n\r\n**Specificity**:\r\n- [ ] Duration is a range, not vague (\"approximately an hour\")\r\n- [ ] Demographics are percentages or counts, not vague (\"most were\")\r\n- [ ] Software is named, not generic (\"qualitative software\")\r\n\r\n### 7. Citation Check\r\n\r\nVerify citations are appropriate:\r\n\r\n**Analysis approach**: Must cite if named\r\n- Abductive (Timmermans and Tavory 2012)\r\n- Grounded theory (Charmaz 2006; Strauss and Corbin 1990)\r\n- Flexible coding (Deterding and Waters 2018)\r\n\r\n**Methodological references** (optional but useful):\r\n- Sampling strategy references\r\n- Interview methodology references\r\n- Coding approach extensions\r\n\r\n### 8. Table Quality Check (if applicable)\r\n\r\nIf using a demographic table:\r\n\r\n- [ ] Table number referenced in text\r\n- [ ] Columns are meaningful (not just demographics but study-relevant characteristics)\r\n- [ ] Percentages or N provided\r\n- [ ] Categories are mutually exclusive\r\n- [ ] \"N\" or \"Total\" row included\r\n\r\n**For Detailed pathway**:\r\n- [ ] Consider comparison column (sample vs. population)\r\n- [ ] Note limitations if sample differs from population\r\n\r\n---\r\n\r\n## Quality Checklist by Pathway\r\n\r\n### Efficient Pathway Checklist\r\n- [ ] Under 900 words\r\n- [ ] No table in methods body\r\n- [ ] Study-led opening\r\n- [ ] Recruitment in 1-2 sentences\r\n- [ ] Duration range reported\r\n- [ ] Analysis approach named with citation\r\n- [ ] Confidentiality note at end\r\n- [ ] No positionality (unless essential)\r\n- [ ] No saturation claim\r\n\r\n### Standard Pathway Checklist\r\n- [ ] 1,200-1,500 words\r\n- [ ] Table included OR good prose demographics\r\n- [ ] Study-led opening\r\n- [ ] Recruitment channels named\r\n- [ ] Duration with bounds\r\n- [ ] Recording/transcription mentioned\r\n- [ ] Software named\r\n- [ ] Multi-step coding process described\r\n- [ ] Positionality IF triggers apply\r\n- [ ] Confidentiality noted\r\n\r\n### Detailed Pathway Checklist\r\n- [ ] 2,000-3,000 words\r\n- [ ] Table required\r\n- [ ] Study-led opening with design framing\r\n- [ ] Case/site justification included\r\n- [ ] Recruitment channels + process\r\n- [ ] Response/refusal rates if significant\r\n- [ ] Duration with range and median\r\n- [ ] Compensation mentioned\r\n- [ ] Ethical considerations paragraph IF vulnerable\r\n- [ ] Positionality discussion included\r\n- [ ] Specific codes listed\r\n- [ ] Subsection headers used\r\n\r\n---\r\n\r\n## Common Revision Moves\r\n\r\n### To Shorten (over word count)\r\n\r\n| Cut This | Save ~X Words |\r\n|----------|---------------|\r\n| Method justification opening | 30-50 |\r\n| Redundant recruitment detail | 20-40 |\r\n| Prose demographics (use table) | 100-200 |\r\n| Intercoder reliability detail | 30-50 |\r\n| Coding process elaboration | 50-100 |\r\n\r\n### To Lengthen (under word count)\r\n\r\n| Add This | Add ~X Words |\r\n|----------|--------------|\r\n| Recruitment channel specifics | 30-50 |\r\n| Sample interview question | 20-40 |\r\n| Location details | 20-30 |\r\n| Compensation information | 15-25 |\r\n| Duration median (not just range) | 10-15 |\r\n\r\n---\r\n\r\n## Output Files to Create\r\n\r\n1. **methods-final.md** - Revised Methods section ready for submission\r\n2. **revision-memo.md** - Notes on changes made and quality assessment\r\n\r\n**Revision memo structure**:\r\n```markdown\r\n# Methods Section Revision Memo\r\n\r\n## Word Count\r\n- Draft: [X] words\r\n- Target: [Y] words\r\n- Final: [Z] words\r\n\r\n## Components Verified\r\n- [List all required components present]\r\n\r\n## Changes Made\r\n- [Bullet list of revisions]\r\n\r\n## Quality Assessment\r\n- Opening: [study-led / method-justified]\r\n- Saturation: [claimed / alternative / silent]\r\n- Positionality: [included / omitted / not applicable]\r\n- Table: [yes / no / n/a for pathway]\r\n\r\n## Remaining Concerns\r\n- [Any issues user should review]\r\n```\r\n\r\n---\r\n\r\n## When You're Done\r\n\r\nReport to the orchestrator:\r\n- Final word count\r\n- All required components present (yes/no)\r\n- Major revisions made\r\n- Quality assessment summary\r\n- Any remaining concerns for user\r\n\r\nExample summary:\r\n> \"**Revision complete**. Final: 1,347 words (target: 1,350). All Standard pathway components present. Revisions: replaced method-justification opening with study-led opener, added software name, compressed recruitment by 40 words. Quality: study-led opening, no saturation claim, positionality appropriately omitted. No remaining concerns. Methods section ready for submission.\"\r\n",
        "plugins/methods-writer/skills/methods-writer/techniques/component-checklist.md": "# Component Checklist\r\n\r\nThis guide specifies what to include for each component of a Methods section. All examples are drawn from the 77-article corpus of *Social Problems* and *Social Forces* interview studies.\r\n\r\n---\r\n\r\n## 1. Case Selection / Study Rationale\r\n\r\n### What to Include\r\n\r\n| Pathway | Required | Optional |\r\n|---------|----------|----------|\r\n| **Efficient** | Brief population mention | -- |\r\n| **Standard** | Population + setting | Brief case justification |\r\n| **Detailed** | Full case selection logic | Unit of analysis; comparison rationale |\r\n\r\n### How to Phrase It\r\n\r\n**Efficient (minimal)**:\r\n> \"I draw from 41 interviews with Black middle-class adults.\"\r\n\r\n**Standard (brief justification)**:\r\n> \"This paper draws on 47 interviews conducted with guest mothers in Chicago between 2019 and 2021. Chicago provides an important case given its high rates of housing instability and diverse demographic composition.\"\r\n\r\n**Detailed (full logic)**:\r\n> \"The six cases provide an ideal set of comparisons for several reasons. First, they experienced mobilization during the same period. Second, they share similar political opportunity structures. Third, they vary systematically in the variable of interest.\"\r\n\r\n### Example Sentences from Corpus\r\n\r\n**Why this case**:\r\n- \"an important place to study given its [characteristic]\"\r\n- \"this is an important opportunity to examine...\"\r\n- \"uniquely suited for investigating\"\r\n- \"provides an ideal set of comparisons\"\r\n\r\n**Why interviews (not surveys)**:\r\n- \"inadequate for identifying the discursive and cognitive processes\"\r\n- \"surveys are prone to social desirability bias\"\r\n- \"unable to disentangle the sequential relationship\"\r\n\r\n---\r\n\r\n## 2. Sample Size and Composition\r\n\r\n### What to Include\r\n\r\n| Pathway | Required | Optional |\r\n|---------|----------|----------|\r\n| **Efficient** | N; basic demographics in prose | -- |\r\n| **Standard** | N; demographics (prose or table) | Table recommended |\r\n| **Detailed** | N; table required; population comparison | Response rates |\r\n\r\n### How to Phrase It\r\n\r\n**Sample N**:\r\n> \"In total, 47 participants were interviewed.\"\r\n> \"We draw from 71 multigenerational in-depth interviews.\"\r\n> \"This study is based on repeated interviews with 51 people.\"\r\n\r\n**Demographics in prose** (Efficient):\r\n> \"The sample was majority male (87 percent) and white (55 percent). Respondents ranged in age from 28 to 64.\"\r\n\r\n**Demographics with table reference** (Standard/Detailed):\r\n> \"Table 1 presents selected characteristics of our participants.\"\r\n> \"As shown in Table 1, all respondents are Black, and most are women (64 percent).\"\r\n\r\n**Population comparison** (Detailed):\r\n> \"Relative to the population of [group], our sample over-represents women and under-represents those without college degrees.\"\r\n> \"Sample racial demographics were generally reflective of the county jail population.\"\r\n\r\n### Example Sentences from Corpus\r\n\r\n**Direct statements**:\r\n- \"In total, X participants were purposively sampled\"\r\n- \"The data include interviewees who range in age from...\"\r\n- \"Most respondents were racial minorities (Black, X percent; Latino/a Y percent)\"\r\n\r\n**With breakdown**:\r\n- \"X interviews with [group A] = N individuals; Y interviews with [group B] = M individuals\"\r\n- \"In total, X intervieweesY from [site A] and Z from [site B]\"\r\n\r\n---\r\n\r\n## 3. Recruitment Strategy\r\n\r\n### What to Include\r\n\r\n| Pathway | Required | Optional |\r\n|---------|----------|----------|\r\n| **Efficient** | Named approach | -- |\r\n| **Standard** | Named approach + channels | Eligibility criteria |\r\n| **Detailed** | Approach + channels + process | Response/refusal rates |\r\n\r\n### Named Approaches\r\n\r\n- **Purposive**: \"purposively sampled\"\r\n- **Snowball**: \"snowball sampling\"\r\n- **Convenience**: \"convenience sampling approach\"\r\n- **Stratified/Random**: \"a stratified random sample was drawn\"\r\n- **Combination**: \"purposive and snowball sampling techniques\"\r\n\r\n### How to Phrase It\r\n\r\n**Efficient**:\r\n> \"To find middle-class participants, I used purposeful snowball sampling, recruiting from Black alumni associations and professional organizations.\"\r\n\r\n**Standard**:\r\n> \"Participants were recruited through personal contacts, recruitment ads on social media, and subsequent snowball sampling. To qualify, participants must have [eligibility criteria].\"\r\n\r\n**Detailed**:\r\n> \"Recruitment took place in several stages. The first group included [group A], recruited via [channel]. We then interviewed [group B], identified through [process]. In total, 215 [population] were asked to participate; 75 refused, resulting in a final sample of 140.\"\r\n\r\n### Example Sentences from Corpus\r\n\r\n**Channels**:\r\n- \"recruited from a number of organizations, some of which actively engaged in [activity]\"\r\n- \"recruiting from Black alumni associations of large universities, Black faculty organizations, campus staff groups\"\r\n- \"Several nonprofit organizations advocating for immigrant rights supported our recruitment\"\r\n\r\n**Gatekeeper-mediated**:\r\n- \"Local partners facilitated access to [group A]. [Group A] facilitated access to [group B]\"\r\n- \"The Los Angeles County Sheriff's Department provided the research team with bi-weekly lists\"\r\n\r\n**Response rates**:\r\n- \"X interviewees refused, resulting in a final sample of Y\"\r\n- \"The youth response rate for this study is just over X percent\"\r\n\r\n---\r\n\r\n## 4. Interview Protocol\r\n\r\n### What to Include\r\n\r\n| Pathway | Required | Optional |\r\n|---------|----------|----------|\r\n| **Efficient** | Duration | Topics (brief) |\r\n| **Standard** | Duration; topics; recording | Location; compensation |\r\n| **Detailed** | All of above; instrument provenance | Sample questions |\r\n\r\n### Duration Reporting\r\n\r\n**Range only**:\r\n> \"Interviews ranged from 45 minutes to two hours.\"\r\n\r\n**Range with average**:\r\n> \"Interviews lasted from one to three hours; the median was 97 minutes.\"\r\n\r\n**By participant type** (Detailed):\r\n> \"Parent interviews averaged 1 hour; youth interviews ranged from 40 minutes to 2.5 hours.\"\r\n\r\n### Topics Covered\r\n\r\n**Efficient** (brief):\r\n> \"Interviews covered family experiences and housing decisions.\"\r\n\r\n**Standard** (listed):\r\n> \"The interview guide asked questions about: 1) career pathways; 2) organizational experiences; and 3) professional identity.\"\r\n\r\n**Detailed** (with sample questions):\r\n> \"We began with broad questions such as 'Can you tell me about your family?' and then asked specific questions about [topic]. Youth interviews began by asking each respondent to 'describe a typical day.'\"\r\n\r\n### Recording and Transcription\r\n\r\n**Standard formula**:\r\n> \"All interviews were audio recorded and transcribed.\"\r\n\r\n**With exceptions**:\r\n> \"All but two interviews were audio-recorded and transcribed; detailed notes were recorded by hand for the remaining two.\"\r\n\r\n### Location\r\n\r\n> \"Interviews were conducted at libraries, cafes, and in participants' homes or offices.\"\r\n> \"Interviews took place in person while they were confined in [facility].\"\r\n\r\n### Compensation\r\n\r\n> \"Respondents were paid $25 in cash for each interview.\"\r\n> \"Participants received a $30 gift card.\"\r\n> \"Adults received a $50 gift card and children received a $25 gift card.\"\r\n\r\n### Language\r\n\r\n> \"Interviews were conducted in English or Spanish, depending on the respondent's preference.\"\r\n> \"I employed Spanish-speaking translators to accompany me in five parent interviews.\"\r\n\r\n---\r\n\r\n## 5. Analysis Approach\r\n\r\n### What to Include\r\n\r\n| Pathway | Required | Optional |\r\n|---------|----------|----------|\r\n| **Efficient** | Named approach with citation | Software |\r\n| **Standard** | Approach + citation + software + process | Intercoder reliability |\r\n| **Detailed** | All above + specific codes | Prevalence statistics |\r\n\r\n### Named Approaches with Citations\r\n\r\n**Abductive**:\r\n> \"Following an abductive approach (Timmermans and Tavory 2012)\"\r\n> \"I coded and analyzed the transcripts using abductive techniques (Tavory and Timmermans 2014)\"\r\n\r\n**Grounded theory**:\r\n> \"I used grounded theory methods (Charmaz 2006; Strauss and Corbin 1990) with open, axial, and selective coding\"\r\n\r\n**Thematic**:\r\n> \"Initially coded following an inductive thematic approach (Braun and Clarke 2006)\"\r\n\r\n**Flexible coding**:\r\n> \"We used flexible coding procedures, as outlined by Deterding and Waters (2018)\"\r\n\r\n### Software\r\n\r\n> \"Interviews were coded using Dedoose software.\"\r\n> \"Interview transcripts were analyzed in ATLAS.ti.\"\r\n> \"We imported transcripts and fieldnotes into NVivo for coding.\"\r\n\r\n### Coding Process\r\n\r\n**Efficient**:\r\n> \"Following an abductive approach (Timmermans and Tavory 2012), I coded using Dedoose.\"\r\n\r\n**Standard**:\r\n> \"Analysis involved a two-step process. Initial coding revolved around reading each transcript in its entirety and developing broad thematic codes. This was followed by a second step of more focused line-by-line coding.\"\r\n\r\n**Detailed**:\r\n> \"Open coding was first employed to identify initial concepts. These codes included *trust, rejection, loneliness, shame, hopelessness, excitement*, among others. The remaining transcripts were then coded deductively using these theoretically-driven codes.\"\r\n\r\n### Intercoder Reliability (Standard/Detailed, if team)\r\n\r\n> \"We monitored inter-coder reliability throughout by meeting frequently to discuss the coding process.\"\r\n> \"All transcripts were coded by at least two research team members and reviewed on a weekly basis.\"\r\n\r\n---\r\n\r\n## 6. Positionality\r\n\r\n### When to Include\r\n\r\n| Pathway | Include When |\r\n|---------|--------------|\r\n| **Efficient** | Rarely (only if essential) |\r\n| **Standard** | Interviewer-respondent mismatch; identity shaped access |\r\n| **Detailed** | Encouraged, especially with vulnerable populations |\r\n\r\n### What to Include\r\n\r\n1. **Identity disclosure**: Who are the researchers?\r\n2. **Potential constraints**: How might identity have limited data?\r\n3. **Potential benefits**: How might identity have helped?\r\n4. **Mitigation/interpretation**: What does this mean?\r\n\r\n### How to Phrase It\r\n\r\n**Brief identity disclosure**:\r\n> \"All three interviewers identify as white, and two as female.\"\r\n> \"Both researchers interviewed participants. We are both immigrants.\"\r\n\r\n**Constraining effects**:\r\n> \"May have restricted the stories our interviewees felt able to tell.\"\r\n> \"Queer men of color likely spent considerable time explaining their particular challenges.\"\r\n\r\n**Enabling effects**:\r\n> \"May have elicited a particular desire to justify the meritocratic nature of one's success.\"\r\n> \"Participants may have viewed my comparative privilege as an opportunity to communicate their experiences to a broader audience.\"\r\n\r\n**Mitigation**:\r\n> \"I directly and generously quote respondents as a way to ensure that the sociological claims reflect the lived experiences of those most affected.\"\r\n\r\n---\r\n\r\n## 7. Ethical Considerations\r\n\r\n### When to Include\r\n\r\n- Vulnerable population (incarcerated, undocumented, survivors)\r\n- Non-standard consent (verbal, waived)\r\n- Information security measures\r\n- Topics deliberately not asked\r\n\r\n### What to Include\r\n\r\n**IRB**:\r\n> \"The study received institutional review board approval.\"\r\n\r\n**Consent** (if non-standard):\r\n> \"We obtained verbal rather than written consent and did not maintain records of personal information.\"\r\n\r\n**Information security**:\r\n> \"Potential participants contacted us through a dedicated cellphone line or an encrypted email account.\"\r\n\r\n**Topics not asked**:\r\n> \"We did not ask for details regarding a participant's current or past charges.\"\r\n\r\n**Support provision**:\r\n> \"Each family also received a resource packet with information about housing, legal, and food assistance programs.\"\r\n\r\n---\r\n\r\n## 8. Confidentiality Statement\r\n\r\n### When to Include\r\n\r\n- Efficient: Required (brief)\r\n- Standard: Recommended\r\n- Detailed: Optional (ethics section may cover)\r\n\r\n### How to Phrase It\r\n\r\n> \"For the sake of confidentiality, all respondents' names have been changed.\"\r\n> \"To ensure confidentiality, I have used pseudonyms throughout.\"\r\n> \"Participants were given pseudonyms.\"\r\n\r\n---\r\n\r\n## Quick Reference: Required Components by Pathway\r\n\r\n| Component | Efficient | Standard | Detailed |\r\n|-----------|-----------|----------|----------|\r\n| Sample N | Yes | Yes | Yes |\r\n| Demographics | Prose | Prose + table | Table + comparison |\r\n| Recruitment approach | Named | Named + channels | Channels + rates |\r\n| Duration | Range | Range | Range + median |\r\n| Topics | Brief | Listed | Listed + questions |\r\n| Recording | Optional | Yes | Yes |\r\n| Location | Optional | Yes | Yes |\r\n| Compensation | Optional | Yes | Yes |\r\n| Analysis approach | Named + citation | + software + process | + codes |\r\n| Positionality | No | Conditional | Encouraged |\r\n| Ethics paragraph | No | If needed | If vulnerable |\r\n| Confidentiality | Yes | Yes | In ethics |\r\n",
        "plugins/methods-writer/skills/methods-writer/techniques/opening-moves.md": "# Opening Moves\r\n\r\nThis guide provides templates and examples for how to open Methods sections. All examples are drawn from the 77-article corpus of *Social Problems* and *Social Forces* interview studies.\r\n\r\n**Key finding**: 88% of corpus articles use **study-led openings**leading with the data or sample, not with methodological justification.\r\n\r\n---\r\n\r\n## Opening Types by Prevalence\r\n\r\n| Opening Type | Prevalence | Description |\r\n|--------------|------------|-------------|\r\n| **Study-led** | 88% | Opens with sample, data, or study description |\r\n| **Context-led** | 6% | Opens with setting or case context |\r\n| **Sample-led** | 4% | Opens directly with sample size |\r\n| **Method-justified** | 2% | Opens with why interviews are appropriate |\r\n\r\n**Recommendation**: Use study-led openings. Method-justified openings are rare and typically unnecessary.\r\n\r\n---\r\n\r\n## Study-Led Openings (88%)\r\n\r\n### Pattern 1: Direct Data Introduction\r\n\r\n**Formula**: \"[Pronoun] [draw from / conducted / analyze] [N] [interview type] with [population].\"\r\n\r\n**Examples from corpus**:\r\n> \"I draw from 41 in-depth interviews with Black middle-class adults.\"\r\n\r\n> \"This paper draws on 47 in-depth, semi-structured interviews conducted in Chicago.\"\r\n\r\n> \"We draw from 71 multigenerational and multi-perspective in-depth interviews.\"\r\n\r\n> \"Data were collected as part of a larger project conducted at [site].\"\r\n\r\n**Best for**: Efficient pathway; Standard pathway without case justification need\r\n\r\n---\r\n\r\n### Pattern 2: Study Framing + Data\r\n\r\n**Formula**: \"[Brief conceptual or empirical frame]. [Data introduction with N, population, location/dates].\"\r\n\r\n**Examples from corpus**:\r\n> \"While the recent revival of elite studies has been dominated by economic definitions, here we draw on a wider conception that combines positional and reputational definitions. Specifically, we analyze 71 interviews from two complementary research projects about pathways to top occupational positions.\"\r\n\r\n> \"This study examines how guest mothers negotiate identity and dignity in shared living arrangements. I conducted 47 in-depth interviews with mothers who doubled up with family or friends in Chicago.\"\r\n\r\n**Best for**: Standard pathway; articles requiring brief literature positioning\r\n\r\n---\r\n\r\n### Pattern 3: Temporal Framing\r\n\r\n**Formula**: \"Between [dates], [pronoun] [conducted / interviewed] [N] [population].\"\r\n\r\n**Examples from corpus**:\r\n> \"Between January 2018 and March 2020, I conducted 62 in-depth interviews with formerly incarcerated individuals.\"\r\n\r\n> \"From 2017 to 2019, we interviewed 89 young adults who had aged out of foster care.\"\r\n\r\n**Best for**: Standard and Detailed pathways; longitudinal studies\r\n\r\n---\r\n\r\n### Pattern 4: Multi-Study Introduction\r\n\r\n**Formula**: \"[Study description]. [Study 1 details]. [Study 2 details].\"\r\n\r\n**Examples from corpus**:\r\n> \"This article draws on two complementary datasets. The first consists of 36 interviews conducted in the United States between 2014 and 2016. The second includes 35 interviews conducted in Denmark between 2015 and 2017.\"\r\n\r\n**Best for**: Detailed pathway; comparative or merged-data designs\r\n\r\n---\r\n\r\n## Context-Led Openings (6%)\r\n\r\n### When to Use\r\n- Site characteristics are essential for interpretation\r\n- Geographic/political context shapes the study\r\n- Institutional setting requires explanation\r\n\r\n### Pattern\r\n\r\n**Formula**: \"[Context description]. [Transition to data].\"\r\n\r\n**Examples from corpus**:\r\n> \"The House of Correction population is significant in its size. The vast majority of those sentenced to serve time in Massachusetts do so at the county level. In this setting, I conducted 86 interviews with detained individuals.\"\r\n\r\n> \"With about 2.5 million (approximately 4% of the total US immigrant population), Illinois has the fifth largest number of immigrants nationwide. In this context, we interviewed 47 undocumented parents and their children.\"\r\n\r\n**Best for**: Detailed pathway; institutionally-situated studies\r\n\r\n---\r\n\r\n## Sample-Led Openings (4%)\r\n\r\n### When to Use\r\n- Sample composition is the key methodological contribution\r\n- Sample size or characteristics are remarkable\r\n\r\n### Pattern\r\n\r\n**Formula**: \"[Sample description, foregrounding characteristics]. [Study details follow].\"\r\n\r\n**Examples from corpus**:\r\n> \"The sample includes 140 individuals detained in the county jail on gun-related charges. All completed an interview during their detention.\"\r\n\r\n> \"In total, 65 youth who had been referred to diversion programs participated in interviews.\"\r\n\r\n**Best for**: Detailed pathway; studies where sample access is notable\r\n\r\n---\r\n\r\n## Method-Justified Openings (2% - AVOID)\r\n\r\n### Why to Avoid\r\n- Readers assume interviews are appropriate for qualitative research questions\r\n- Justification wastes word count\r\n- Signals insecurity about method choice\r\n\r\n### Examples to AVOID\r\n\r\n**Do NOT write**:\r\n> \"Qualitative methods are appropriate for this study because they allow for in-depth exploration of meaning-making processes.\"\r\n\r\n> \"Interviews allow researchers to understand how participants interpret their experiences.\"\r\n\r\n> \"To understand the mechanisms underlying [phenomenon], I used in-depth interviews.\"\r\n\r\n### Exception\r\nMethod-justified openings may be appropriate when:\r\n- Converting from quantitative-dominant subfield\r\n- Addressing explicit reviewer concern\r\n- Making methodological contribution\r\n\r\n---\r\n\r\n## Opening Move Templates by Pathway\r\n\r\n### Efficient Pathway\r\n\r\n**Template 1**:\r\n> \"I draw from [N] [interview type] interviews with [population].\"\r\n\r\n**Template 2**:\r\n> \"This study draws on [N] interviews conducted with [population] between [dates].\"\r\n\r\n**Template 3**:\r\n> \"We conducted [N] interviews with [population] in [location].\"\r\n\r\n---\r\n\r\n### Standard Pathway\r\n\r\n**Template 1** (framing + data):\r\n> \"While [prior research approach], here we draw on [alternative approach]. Specifically, we analyze [N] interviews from [project description].\"\r\n\r\n**Template 2** (temporal):\r\n> \"Between [dates], I conducted [N] [interview type] interviews with [population] in [location]. This study examines [focus].\"\r\n\r\n**Template 3** (project context):\r\n> \"Data were collected as part of [broader project name/description]. I draw specifically from [N] interviews with [subgroup].\"\r\n\r\n---\r\n\r\n### Detailed Pathway\r\n\r\n**Template 1** (research design):\r\n> \"This study [research goal] using original, [design type] evidence from [cases/context]. The unit of analysis is [unit]. [Data description].\"\r\n\r\n**Template 2** (multi-site):\r\n> \"This article draws on two complementary datasets collected in [location A] and [location B]. The first consists of [N] interviews... The second includes [M] interviews...\"\r\n\r\n**Template 3** (context-led):\r\n> \"[Context/setting description establishing importance]. In this setting, I conducted [N] interviews with [population] between [dates].\"\r\n\r\n---\r\n\r\n## Transition from Opening to Body\r\n\r\nAfter the opening sentence(s), typical transitions include:\r\n\r\n**To population definition**:\r\n> \"Traditionally, sociologists define [concept] through...\"\r\n> \"I classify [group] as...\"\r\n\r\n**To case justification**:\r\n> \"This is an important place to study given...\"\r\n> \"The [N] cases provide an ideal set of comparisons...\"\r\n\r\n**To recruitment**:\r\n> \"Participants were recruited through...\"\r\n> \"I used [sampling approach], recruiting from...\"\r\n\r\n**To sample overview**:\r\n> \"In total, [N] participants were...\"\r\n> \"Table 1 presents selected characteristics...\"\r\n\r\n---\r\n\r\n## Common Mistakes\r\n\r\n### Mistake 1: Burying the Sample Size\r\n**Problem**: Sample N appears mid-paragraph or later.\r\n**Solution**: Include N in first or second sentence.\r\n\r\n### Mistake 2: Vague Population Description\r\n**Problem**: \"I interviewed people about their experiences.\"\r\n**Solution**: Specify population clearly: \"I interviewed 47 mothers who had doubled up with family or friends.\"\r\n\r\n### Mistake 3: Excessive Qualification\r\n**Problem**: \"This exploratory study uses qualitative methods to begin to understand...\"\r\n**Solution**: State directly: \"This study examines...\"\r\n\r\n### Mistake 4: Method Defense\r\n**Problem**: \"Interviews are appropriate because...\"\r\n**Solution**: Skip justification; lead with data.\r\n\r\n---\r\n\r\n## Quick Reference: Opening Sentence Starters\r\n\r\n| Pattern | Example Start |\r\n|---------|---------------|\r\n| Direct data | \"I draw from...\" |\r\n| Direct data | \"This study draws on...\" |\r\n| Direct data | \"We analyze...\" |\r\n| Temporal | \"Between [dates]...\" |\r\n| Framing | \"While [prior approach], here we...\" |\r\n| Multi-study | \"This article draws on two...\" |\r\n| Context | \"[Setting description]...\" |\r\n| Project | \"Data were collected as part of...\" |\r\n",
        "plugins/r-analyst/.claude-plugin/plugin.json": "{\n  \"name\": \"r-analyst\",\n  \"description\": \"R statistical analysis for publication-ready sociology research. Phased workflow for DiD, IV, matching, panel methods, and more with pauses for user review.\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Neal Caren\"\n  },\n  \"license\": \"MIT\",\n  \"keywords\": [\n    \"R\",\n    \"statistics\",\n    \"econometrics\",\n    \"sociology\",\n    \"research\"\n  ],\n  \"skills\": \"./skills/\"\n}\n",
        "plugins/r-analyst/skills/r-analyst/SKILL.md": "---\r\nname: r-analyst\r\ndescription: R statistical analysis for publication-ready sociology research. Guides you through phased workflows for DiD, IV, matching, panel methods, and more. Use when doing quantitative analysis in R for academic papers.\r\n---\r\n\r\n# R Statistical Analyst\r\n\r\nYou are an expert quantitative research assistant specializing in statistical analysis using R. Your role is to guide users through a systematic, phased analysis process that produces publication-ready results suitable for top-tier social science journals.\r\n\r\n## Core Principles\r\n\r\n1. **Identification before estimation**: Establish a credible research design before running any models. The estimator must match the identification strategy.\r\n\r\n2. **Reproducibility**: All analysis must be reproducible. Use seeds, document decisions, save intermediate outputs.\r\n\r\n3. **Robustness is required**: Main results mean little without robustness checks. Every analysis needs sensitivity analysis.\r\n\r\n4. **User collaboration**: The user knows their substantive domain. You provide methodological expertise; they make research decisions.\r\n\r\n5. **Pauses for reflection**: Stop between phases to discuss findings and get user input before proceeding.\r\n\r\n## Analysis Phases\r\n\r\n### Phase 0: Research Design Review\r\n**Goal**: Establish the identification strategy before touching data.\r\n\r\n**Process**:\r\n- Clarify the research question and causal claim\r\n- Identify the estimation strategy (DiD, IV, RD, matching, panel FE, etc.)\r\n- Discuss key assumptions and their plausibility\r\n- Identify threats to identification\r\n- Plan the overall analysis approach\r\n\r\n**Output**: Design memo documenting question, strategy, assumptions, and threats.\r\n\r\n> **Pause**: Confirm design with user before proceeding.\r\n\r\n---\r\n\r\n### Phase 1: Data Familiarization\r\n**Goal**: Understand the data before modeling.\r\n\r\n**Process**:\r\n- Load and inspect data structure\r\n- Generate descriptive statistics (Table 1)\r\n- Check data quality: missing values, outliers, coding errors\r\n- Visualize key variables and relationships\r\n- Verify that data supports the planned identification strategy\r\n\r\n**Output**: Data report with descriptives, quality assessment, and preliminary visualizations.\r\n\r\n> **Pause**: Review descriptives with user. Confirm sample and variable definitions.\r\n\r\n---\r\n\r\n### Phase 2: Model Specification\r\n**Goal**: Fully specify models before estimation.\r\n\r\n**Process**:\r\n- Write out the estimating equation(s)\r\n- Justify variable operationalization\r\n- Specify fixed effects structure\r\n- Determine clustering for standard errors\r\n- Plan the sequence of specifications (baseline -> full -> robustness)\r\n\r\n**Output**: Specification memo with equations, variable definitions, and rationale.\r\n\r\n> **Pause**: User approves specification before estimation.\r\n\r\n---\r\n\r\n### Phase 3: Main Analysis\r\n**Goal**: Estimate primary models and interpret results.\r\n\r\n**Process**:\r\n- Run main specifications\r\n- Interpret coefficients, standard errors, significance\r\n- Check model assumptions (where applicable)\r\n- Create initial results table\r\n\r\n**Output**: Main results with interpretation.\r\n\r\n> **Pause**: Discuss findings with user before robustness checks.\r\n\r\n---\r\n\r\n### Phase 4: Robustness & Sensitivity\r\n**Goal**: Stress-test the main findings.\r\n\r\n**Process**:\r\n- Alternative specifications (different controls, FE structures)\r\n- Subgroup analyses\r\n- Placebo tests (where applicable)\r\n- Sensitivity analysis (sensemakr for selection on unobservables)\r\n- Diagnostic tests specific to the method\r\n\r\n**Output**: Robustness tables and sensitivity assessment.\r\n\r\n> **Pause**: Assess whether findings are robust. Discuss implications.\r\n\r\n---\r\n\r\n### Phase 5: Output & Interpretation\r\n**Goal**: Produce publication-ready outputs and interpretation.\r\n\r\n**Process**:\r\n- Create publication-quality tables (modelsummary/etable)\r\n- Create figures (coefficient plots, marginal effects, etc.)\r\n- Write results narrative\r\n- Document limitations and caveats\r\n- Prepare replication materials\r\n\r\n**Output**: Final tables, figures, and interpretation memo.\r\n\r\n---\r\n\r\n## Folder Structure\r\n\r\n```\r\nproject/\r\n data/\r\n    raw/              # Original data (never modified)\r\n    clean/            # Processed analysis data\r\n code/\r\n    00_master.R       # Runs entire analysis\r\n    01_clean.R\r\n    02_descriptives.R\r\n    03_analysis.R\r\n    04_robustness.R\r\n output/\r\n    tables/\r\n    figures/\r\n memos/                # Phase outputs and decisions\r\n```\r\n\r\n## Technique Guides\r\n\r\nReference these guides for method-specific code. Guides are in `techniques/` (relative to this skill):\r\n\r\n| Guide | Topics |\r\n|-------|--------|\r\n| `01_core_econometrics.md` | TWFE, DiD, Event Studies, RD, IV, Matching, Mediation |\r\n| `02_survey_resampling.md` | Survey weights, Bootstrap, Oaxaca, List Experiments |\r\n| `03_text_ml.md` | LDA, STM, Sentiment, Causal Forests, GAMs, EFA/CFA/IRT |\r\n| `04_synthetic_control.md` | Synth, gsynth, Matrix Completion, Synthetic DiD |\r\n| `05_bayesian_sensitivity.md` | brms, sensemakr, OVB Bounds |\r\n| `06_visualization.md` | ggplot2, coefplot, etable, patchwork |\r\n| `07_best_practices.md` | Reproducibility, Project Structure, Code Style |\r\n| `08_nonlinear_models.md` | LPM vs Logit, Poisson/PPML, Marginal Effects |\r\n\r\n**Read the relevant guide(s) before writing code for that method.**\r\n\r\n## Running R Code\r\n\r\n### Execution Method\r\n\r\n```bash\r\nRscript filename.R\r\n```\r\n\r\n### Check if R is Available\r\n\r\n```bash\r\nwhich R || which Rscript || echo \"R not found\"\r\nRscript -e \"sessionInfo()\"\r\n```\r\n\r\n### If R Is Not Found\r\n\r\n1. Check common locations: `/usr/local/bin/R`, `/usr/bin/R`\r\n2. Ask the user for their R installation path\r\n3. If not installed: Provide code as `.R` files they can run later\r\n\r\n## Invoking Phase Agents\r\n\r\nFor each phase, invoke the appropriate sub-agent using the Task tool:\r\n\r\n```\r\nTask: Phase 1 Data Familiarization\r\nsubagent_type: general-purpose\r\nmodel: sonnet\r\nprompt: Read phases/phase1-data.md and execute for [user's project]\r\n```\r\n\r\n## Model Recommendations\r\n\r\n| Phase | Model | Rationale |\r\n|-------|-------|-----------|\r\n| **Phase 0**: Research Design | **Opus** | Methodological judgment, identifying threats |\r\n| **Phase 1**: Data Familiarization | **Sonnet** | Descriptive statistics, data processing |\r\n| **Phase 2**: Model Specification | **Opus** | Design decisions, justifying choices |\r\n| **Phase 3**: Main Analysis | **Sonnet** | Running models, standard interpretation |\r\n| **Phase 4**: Robustness | **Sonnet** | Systematic checks |\r\n| **Phase 5**: Output | **Opus** | Writing, synthesis, nuanced interpretation |\r\n\r\n## Starting the Analysis\r\n\r\nWhen the user is ready to begin:\r\n\r\n1. **Ask about the research question**:\r\n   > \"What causal or descriptive question are you trying to answer?\"\r\n\r\n2. **Ask about data**:\r\n   > \"What data do you have? Is it cross-sectional, panel, or repeated cross-section?\"\r\n\r\n3. **Ask about identification**:\r\n   > \"Do you have a specific identification strategy in mind (DiD, IV, RD, etc.), or would you like to discuss options?\"\r\n\r\n4. **Then proceed with Phase 0** to establish the research design.\r\n\r\n## Key Reminders\r\n\r\n- **Design before data**: Phase 0 happens before you look at results.\r\n- **Pause between phases**: Always stop for user input before proceeding.\r\n- **Use the technique guides**: Don't reinventuse tested code patterns.\r\n- **Cluster your standard errors**: Almost always at the unit of treatment assignment.\r\n- **Robustness is not optional**: Main results need sensitivity analysis.\r\n- **The user decides**: You provide options and recommendations; they choose.\r\n",
        "plugins/r-analyst/skills/r-analyst/phases/phase0-design.md": "# Phase 0: Research Design Review\r\n\r\nYou are executing Phase 0 of a statistical analysis in R. Your goal is to establish a credible research design before any estimation occurs.\r\n\r\n## Why This Phase Matters\r\n\r\nThe identification strategy determines whether results have a causal interpretation. No amount of sophisticated estimation can fix a flawed design. This phase ensures the user has thought through their approach before investing in analysis.\r\n\r\n## Your Tasks\r\n\r\n### 1. Clarify the Research Question\r\n\r\nAsk the user to articulate:\r\n- What is the main question? (causal or descriptive)\r\n- What is the outcome of interest?\r\n- What is the key explanatory variable or treatment?\r\n- What is the population of interest?\r\n\r\nDocument this clearlyit guides all subsequent decisions.\r\n\r\n### 2. Identify the Estimation Strategy\r\n\r\nBased on the research question and data structure, determine the appropriate approach:\r\n\r\n| Strategy | When to Use | Key Assumptions |\r\n|----------|-------------|-----------------|\r\n| **DiD** | Treatment timing varies across units | Parallel trends, no anticipation |\r\n| **Event Study** | Need to show pre-trends and dynamic effects | Same as DiD + homogeneous effects (or use robust estimator) |\r\n| **IV** | Endogenous treatment, valid instrument available | Relevance, exclusion, monotonicity |\r\n| **RD** | Treatment assigned by threshold | Continuity at cutoff, no manipulation |\r\n| **Matching** | Selection on observables | No unobserved confounders |\r\n| **Panel FE** | Unobserved time-invariant confounders | Strict exogeneity |\r\n\r\nFor each strategy, reference the relevant technique guide:\r\n- DiD/Event Study: `r-statistical-techniques/01_core_econometrics.md`\r\n- IV: `r-statistical-techniques/01_core_econometrics.md` Section 5\r\n- Matching: `r-statistical-techniques/01_core_econometrics.md` Section 6\r\n- Synthetic Control: `r-statistical-techniques/04_synthetic_control.md`\r\n\r\n### 3. Assess Assumptions\r\n\r\nFor the chosen strategy, discuss:\r\n\r\n**What must be true for this to work?**\r\n- State each assumption in plain language\r\n- Discuss whether it's plausible in this context\r\n- Identify what evidence could support or undermine it\r\n\r\n**What are the main threats?**\r\n- Confounders (what else might explain the relationship?)\r\n- Selection (are treated/control groups comparable?)\r\n- Reverse causality (could the outcome affect treatment?)\r\n- Measurement (are variables measured accurately?)\r\n\r\n### 4. Plan Robustness Checks\r\n\r\nBased on identified threats, plan how to address them:\r\n- Placebo tests (outcomes that shouldn't be affected)\r\n- Alternative specifications (different controls, FE structures)\r\n- Sensitivity analysis (how much selection on unobservables would be needed?)\r\n- Subgroup analysis (heterogeneous effects)\r\n\r\n### 5. Document Data Requirements\r\n\r\nSpecify what the data must contain:\r\n- Unit identifiers\r\n- Time identifiers (if panel)\r\n- Treatment indicators and timing\r\n- Outcome variables\r\n- Key controls\r\n- Clustering variable for standard errors\r\n\r\n## Output: Design Memo\r\n\r\nCreate a design memo (`memos/phase0-design-memo.md`) containing:\r\n\r\n```markdown\r\n# Research Design Memo\r\n\r\n## Research Question\r\n[Clear statement of the question]\r\n\r\n## Identification Strategy\r\n[Strategy name and brief justification]\r\n\r\n## Key Variables\r\n- **Outcome**: [variable and measurement]\r\n- **Treatment/Exposure**: [variable and measurement]\r\n- **Unit of Analysis**: [what are observations]\r\n- **Time Structure**: [cross-section, panel, etc.]\r\n\r\n## Assumptions\r\n1. [Assumption 1]: [Why plausible / concerns]\r\n2. [Assumption 2]: [Why plausible / concerns]\r\n...\r\n\r\n## Threats to Identification\r\n1. [Threat 1]: [How we'll address it]\r\n2. [Threat 2]: [How we'll address it]\r\n...\r\n\r\n## Planned Robustness Checks\r\n- [ ] [Check 1]\r\n- [ ] [Check 2]\r\n...\r\n\r\n## Standard Errors\r\nCluster at: [level and justification]\r\n\r\n## Questions for User\r\n- [Any clarifications needed]\r\n```\r\n\r\n## When You're Done\r\n\r\nReturn a summary to the orchestrator that includes:\r\n1. The recommended identification strategy\r\n2. Key assumptions and whether they seem plausible\r\n3. Main threats and planned mitigations\r\n4. Any questions or concerns for the user\r\n5. Confirmation that design memo was created\r\n\r\n**Do not proceed to Phase 1 until the user confirms the research design.**\r\n",
        "plugins/r-analyst/skills/r-analyst/phases/phase1-data.md": "# Phase 1: Data Familiarization\r\n\r\nYou are executing Phase 1 of a statistical analysis in R. Your goal is to develop deep familiarity with the data before any modeling.\r\n\r\n## Why This Phase Matters\r\n\r\nJumping straight to regression is a common mistake. Understanding your data prevents errors, reveals data quality issues, and often suggests refinements to the research design. This phase creates the foundation for credible analysis.\r\n\r\n## Technique Guides\r\n\r\n**Consult these guides** in `r-statistical-techniques/` for data handling patterns:\r\n\r\n| Topic | Guide |\r\n|-------|-------|\r\n| Visualization (ggplot2) | `06_visualization.md` |\r\n| Best practices, Project setup | `07_best_practices.md` |\r\n| Survey data handling | `02_survey_resampling.md` |\r\n\r\n## Your Tasks\r\n\r\n### 1. Load and Inspect Data Structure\r\n\r\n```r\r\n# Load data\r\ndata <- read.csv(\"data/raw/filename.csv\")  # or haven::read_dta() for Stata files\r\n\r\n# Basic structure\r\ndim(data)\r\nstr(data)\r\nnames(data)\r\n\r\n# Check for duplicates\r\nn_distinct(data$id)  # Should match nrow if id is unique\r\n```\r\n\r\nDocument:\r\n- Number of observations and variables\r\n- Unit of observation\r\n- Key variable types\r\n- Any obvious data issues\r\n\r\n### 2. Generate Descriptive Statistics (Table 1)\r\n\r\nCreate a summary statistics table for key variables:\r\n\r\n```r\r\nlibrary(modelsummary)\r\n\r\n# Define variables for Table 1\r\nvars <- c(\"outcome\", \"treatment\", \"control1\", \"control2\")\r\n\r\n# Overall summary\r\ndatasummary(All(data[, vars]) ~ Mean + SD + Min + Max + N,\r\n            data = data,\r\n            output = \"output/tables/table1_descriptives.tex\")\r\n\r\n# By treatment group (if applicable)\r\ndatasummary_balance(~ treatment,\r\n                    data = data,\r\n                    output = \"output/tables/table1_balance.tex\")\r\n```\r\n\r\n### 3. Check Data Quality\r\n\r\n**Missing values:**\r\n```r\r\n# Count missing by variable\r\ncolSums(is.na(data))\r\n\r\n# Missing patterns\r\nlibrary(naniar)\r\nvis_miss(data)\r\n\r\n# Document how missing data will be handled\r\n```\r\n\r\n**Outliers:**\r\n```r\r\n# Check key continuous variables\r\nsummary(data$outcome)\r\nquantile(data$outcome, c(0.01, 0.05, 0.95, 0.99), na.rm = TRUE)\r\n\r\n# Visualize distributions\r\nggplot(data, aes(x = outcome)) +\r\n  geom_histogram(bins = 50) +\r\n  theme_minimal()\r\n```\r\n\r\n**Coding issues:**\r\n```r\r\n# Check categorical variables\r\ntable(data$treatment, useNA = \"ifany\")\r\n\r\n# Check for impossible values\r\ndata %>% filter(age < 0 | age > 120)\r\n```\r\n\r\n### 4. Visualize Key Relationships\r\n\r\nCreate visualizations relevant to the research design:\r\n\r\n**For DiD/Panel:**\r\n```r\r\n# Trends over time by treatment group\r\ndata %>%\r\n  group_by(time, treatment_group) %>%\r\n  summarise(mean_outcome = mean(outcome, na.rm = TRUE)) %>%\r\n  ggplot(aes(x = time, y = mean_outcome, color = treatment_group)) +\r\n  geom_line() +\r\n  geom_vline(xintercept = treatment_time, linetype = \"dashed\") +\r\n  theme_minimal()\r\n```\r\n\r\n**For RD:**\r\n```r\r\n# Outcome vs. running variable\r\nggplot(data, aes(x = running_var, y = outcome)) +\r\n  geom_point(alpha = 0.3) +\r\n  geom_smooth(data = filter(data, running_var < cutoff), method = \"lm\") +\r\n  geom_smooth(data = filter(data, running_var >= cutoff), method = \"lm\") +\r\n  geom_vline(xintercept = cutoff, linetype = \"dashed\") +\r\n  theme_minimal()\r\n```\r\n\r\n**For any design:**\r\n```r\r\n# Bivariate relationship\r\nggplot(data, aes(x = treatment, y = outcome)) +\r\n  geom_boxplot() +\r\n  theme_minimal()\r\n\r\n# Correlation matrix for controls\r\nlibrary(corrplot)\r\ncorrplot(cor(data[, control_vars], use = \"complete.obs\"))\r\n```\r\n\r\n### 5. Verify Design Requirements\r\n\r\nCheck that data supports the planned identification strategy:\r\n\r\n**For DiD:**\r\n- Do you have pre and post periods?\r\n- Do you have treated and control units?\r\n- Are there enough observations in each cell?\r\n\r\n**For Panel FE:**\r\n- Is there within-unit variation in key variables?\r\n- How many time periods per unit?\r\n\r\n**For IV:**\r\n- Is the instrument observed?\r\n- What's the first-stage relationship look like?\r\n\r\n### 6. Create Analysis Sample\r\n\r\nDefine and document the final analysis sample:\r\n\r\n```r\r\n# Define sample restrictions\r\nanalysis_data <- data %>%\r\n  filter(\r\n    !is.na(outcome),\r\n    !is.na(treatment),\r\n    year >= 2000 & year <= 2020\r\n  )\r\n\r\n# Document sample construction\r\ncat(\"Original sample:\", nrow(data), \"\\n\")\r\ncat(\"After dropping missing outcome:\", nrow(filter(data, !is.na(outcome))), \"\\n\")\r\ncat(\"Final analysis sample:\", nrow(analysis_data), \"\\n\")\r\n\r\n# Save analysis sample\r\nsaveRDS(analysis_data, \"data/clean/analysis_sample.rds\")\r\n```\r\n\r\n## Output: Data Report\r\n\r\nCreate a data report (`memos/phase1-data-report.md`) containing:\r\n\r\n```markdown\r\n# Data Familiarization Report\r\n\r\n## Data Overview\r\n- **Source**: [where data comes from]\r\n- **Observations**: [N]\r\n- **Variables**: [count and key variables]\r\n- **Time Period**: [if applicable]\r\n\r\n## Sample Construction\r\n| Step | N | Notes |\r\n|------|---|-------|\r\n| Original sample | X | |\r\n| After restriction 1 | Y | [reason] |\r\n| Final analysis sample | Z | |\r\n\r\n## Descriptive Statistics\r\n[Insert or reference Table 1]\r\n\r\n## Data Quality Issues\r\n- **Missing data**: [summary and how handled]\r\n- **Outliers**: [any concerns]\r\n- **Coding issues**: [any found and fixed]\r\n\r\n## Key Visualizations\r\n[Reference saved figures]\r\n\r\n## Design Verification\r\n- [Confirm data supports the identification strategy]\r\n- [Note any concerns]\r\n\r\n## Preliminary Observations\r\n- [Anything notable in the descriptives]\r\n- [Any surprises or concerns]\r\n\r\n## Questions for User\r\n- [Any decisions that need user input]\r\n```\r\n\r\n## When You're Done\r\n\r\nReturn a summary to the orchestrator that includes:\r\n1. Final sample size and key restrictions\r\n2. Any data quality issues found\r\n3. Whether data supports the planned design\r\n4. Key observations from descriptives\r\n5. Questions for the user\r\n\r\n**Do not proceed to Phase 2 until the user reviews the descriptives and confirms the sample.**\r\n",
        "plugins/r-analyst/skills/r-analyst/phases/phase2-specification.md": "# Phase 2: Model Specification\r\n\r\nYou are executing Phase 2 of a statistical analysis in R. Your goal is to fully specify the models before estimationequations, variables, and standard errors.\r\n\r\n## Why This Phase Matters\r\n\r\nSpecification decisions are research decisions. Making them explicit before seeing results prevents p-hacking and specification searching. This phase documents the pre-analysis plan.\r\n\r\n## Technique Guides\r\n\r\n**Consult these guides** in `r-statistical-techniques/` for specification patterns:\r\n\r\n| Method | Guide |\r\n|--------|-------|\r\n| DiD, TWFE, Event Study, IV | `01_core_econometrics.md` |\r\n| Matching specifications | `01_core_econometrics.md` Section 6 |\r\n| Nonlinear models (logit, Poisson) | `08_nonlinear_models.md` |\r\n| Synthetic control | `04_synthetic_control.md` |\r\n\r\n## Your Tasks\r\n\r\n### 1. Write the Estimating Equation\r\n\r\nState the model formally. Examples by design:\r\n\r\n**Two-Way Fixed Effects:**\r\n$$Y_{it} = \\alpha_i + \\gamma_t + \\beta \\cdot Treated_{it} + X_{it}'\\delta + \\varepsilon_{it}$$\r\n\r\n**Difference-in-Differences:**\r\n$$Y_{it} = \\alpha + \\beta_1 \\cdot Post_t + \\beta_2 \\cdot Treat_i + \\beta_3 \\cdot (Post_t \\times Treat_i) + \\varepsilon_{it}$$\r\n\r\n**Event Study:**\r\n$$Y_{it} = \\alpha_i + \\gamma_t + \\sum_{k \\neq -1} \\beta_k \\cdot \\mathbf{1}[t - E_i = k] + X_{it}'\\delta + \\varepsilon_{it}$$\r\n\r\n**Instrumental Variables:**\r\n- First stage: $D_i = \\pi_0 + \\pi_1 Z_i + X_i'\\pi_2 + \\nu_i$\r\n- Second stage: $Y_i = \\beta_0 + \\beta_1 \\hat{D}_i + X_i'\\beta_2 + \\varepsilon_i$\r\n\r\n### 2. Define All Variables\r\n\r\nCreate a variable dictionary:\r\n\r\n| Variable | Name in Data | Definition | Notes |\r\n|----------|--------------|------------|-------|\r\n| Outcome | `outcome` | [precise definition] | [measurement, source] |\r\n| Treatment | `treated` | [how assigned] | [timing if applicable] |\r\n| Control 1 | `age` | [definition] | [why included] |\r\n| ... | | | |\r\n\r\n**Key questions:**\r\n- How is treatment defined? (binary, intensity, timing)\r\n- What controls are included and why?\r\n- Are there variables that should NOT be controlled for? (mediators, colliders)\r\n\r\n### 3. Specify Fixed Effects Structure\r\n\r\nFor panel data, justify the FE structure:\r\n\r\n| Specification | Absorbs | Code |\r\n|---------------|---------|------|\r\n| Unit FE | Time-invariant unit characteristics | `feols(y ~ x | id)` |\r\n| Time FE | Common shocks | `feols(y ~ x | time)` |\r\n| Two-way FE | Both | `feols(y ~ x | id + time)` |\r\n| Unit-by-time trends | Unit-specific trends | `feols(y ~ x | id + id[time])` |\r\n\r\nDocument why this structure is appropriate for the research question.\r\n\r\n### 4. Determine Standard Error Clustering\r\n\r\n**Default rule:** Cluster at the level of treatment assignment.\r\n\r\n| Design | Typical Clustering | Rationale |\r\n|--------|-------------------|-----------|\r\n| State policy DiD | State | Treatment varies at state level |\r\n| Individual-level RCT | Individual or strata | Assignment unit |\r\n| Panel with firm shocks | Firm | Errors correlated within firm over time |\r\n\r\n```r\r\n# fixest syntax\r\nfeols(y ~ x | id + time, cluster = ~id, data = data)\r\n\r\n# Two-way clustering if needed\r\nfeols(y ~ x | id + time, cluster = ~id + time, data = data)\r\n```\r\n\r\n**Consider wild cluster bootstrap** if few clusters (<50):\r\n```r\r\n# After estimation\r\nlibrary(fwildclusterboot)\r\nboottest(model, param = \"treatment\", clustid = ~state)\r\n```\r\n\r\n### 5. Plan Specification Sequence\r\n\r\nDefine the sequence of models to run:\r\n\r\n| Model | Description | Purpose |\r\n|-------|-------------|---------|\r\n| (1) | Baseline: treatment only | Raw relationship |\r\n| (2) | + Unit FE | Control time-invariant confounders |\r\n| (3) | + Time FE | Control common shocks |\r\n| (4) | + Controls | Address remaining confounders |\r\n| (5) | Preferred specification | Main results |\r\n\r\nThis builds credibility by showing results are stable across specifications.\r\n\r\n### 6. Pre-specify Robustness Checks\r\n\r\nBefore running main models, document planned robustness checks:\r\n\r\n**Alternative specifications:**\r\n- [ ] Different control sets\r\n- [ ] Different FE structures\r\n- [ ] Different treatment definitions\r\n\r\n**Sensitivity analysis:**\r\n- [ ] Sensitivity to outliers (winsorize, trim)\r\n- [ ] Sensitivity to functional form\r\n- [ ] sensemakr for selection on unobservables\r\n\r\n**Placebo tests:**\r\n- [ ] Pre-treatment effects (should be zero)\r\n- [ ] Outcomes that shouldn't be affected\r\n- [ ] Fake treatment timing\r\n\r\n**Heterogeneity:**\r\n- [ ] Subgroup analyses (pre-specified)\r\n- [ ] Interaction terms\r\n\r\n## Output: Specification Memo\r\n\r\nCreate a specification memo (`memos/phase2-specification-memo.md`):\r\n\r\n```markdown\r\n# Model Specification Memo\r\n\r\n## Estimating Equation\r\n\r\n[LaTeX or clear written equation]\r\n\r\n## Variable Definitions\r\n\r\n| Variable | Name | Definition | Measurement |\r\n|----------|------|------------|-------------|\r\n| ... | | | |\r\n\r\n## Fixed Effects\r\n[Structure and justification]\r\n\r\n## Standard Errors\r\nClustered at: [level]\r\nJustification: [why]\r\n\r\n## Specification Sequence\r\n\r\n| Model | Specification | Purpose |\r\n|-------|---------------|---------|\r\n| (1) | | |\r\n| ... | | |\r\n\r\n## Pre-Specified Robustness Checks\r\n1. [Check 1]\r\n2. [Check 2]\r\n...\r\n\r\n## Pre-Specified Subgroup Analyses\r\n1. [Subgroup 1]\r\n2. [Subgroup 2]\r\n...\r\n\r\n## Code Skeleton\r\n\r\n```r\r\nlibrary(fixest)\r\n\r\n# Main specification\r\nmodel_main <- feols(\r\n  outcome ~ treatment + control1 + control2 | unit_fe + time_fe,\r\n  cluster = ~cluster_var,\r\n  data = analysis_data\r\n)\r\n\r\n# Specification sequence\r\nmodels <- list(\r\n  \"(1)\" = feols(outcome ~ treatment, data = analysis_data),\r\n  \"(2)\" = feols(outcome ~ treatment | unit_fe, data = analysis_data),\r\n  \"(3)\" = feols(outcome ~ treatment | unit_fe + time_fe, data = analysis_data),\r\n  \"(4)\" = feols(outcome ~ treatment + controls | unit_fe + time_fe,\r\n                cluster = ~cluster_var, data = analysis_data)\r\n)\r\n```\r\n\r\n## Questions for User\r\n[Any specification decisions that need input]\r\n```\r\n\r\n## When You're Done\r\n\r\nReturn a summary to the orchestrator that includes:\r\n1. The main estimating equation\r\n2. Fixed effects structure and clustering\r\n3. The planned specification sequence\r\n4. Pre-specified robustness checks\r\n5. Any questions requiring user input\r\n\r\n**Do not proceed to Phase 3 until the user approves the specification.**\r\n",
        "plugins/r-analyst/skills/r-analyst/phases/phase3-analysis.md": "# Phase 3: Main Analysis\r\n\r\nYou are executing Phase 3 of a statistical analysis in R. Your goal is to run the pre-specified models and interpret the main results.\r\n\r\n## Why This Phase Matters\r\n\r\nThis is where the analysis happens. But because you've done Phases 0-2, you're not searchingyou're executing a pre-specified plan. This makes results more credible.\r\n\r\n## Technique Guides\r\n\r\n**Before writing code, consult the relevant technique guide** in `r-statistical-techniques/` for method-specific patterns:\r\n\r\n| Method | Guide |\r\n|--------|-------|\r\n| DiD, Event Study, IV, Matching | `01_core_econometrics.md` |\r\n| Survey weights, Bootstrap | `02_survey_resampling.md` |\r\n| Synthetic Control | `04_synthetic_control.md` |\r\n| Logit, Poisson, Margins | `08_nonlinear_models.md` |\r\n| Visualization, Tables | `06_visualization.md` |\r\n\r\nThese guides contain tested code patternsuse them rather than writing from scratch.\r\n\r\n## Your Tasks\r\n\r\n### 1. Run the Specification Sequence\r\n\r\nExecute the models defined in Phase 2:\r\n\r\n```r\r\nlibrary(fixest)\r\nlibrary(modelsummary)\r\n\r\n# Load analysis data\r\nanalysis_data <- readRDS(\"data/clean/analysis_sample.rds\")\r\n\r\n# Run specification sequence\r\nmodels <- list()\r\n\r\nmodels[[\"(1)\"]] <- feols(\r\n  outcome ~ treatment,\r\n  data = analysis_data\r\n)\r\n\r\nmodels[[\"(2)\"]] <- feols(\r\n  outcome ~ treatment | unit_fe,\r\n  data = analysis_data\r\n)\r\n\r\nmodels[[\"(3)\"]] <- feols(\r\n  outcome ~ treatment | unit_fe + time_fe,\r\n  data = analysis_data\r\n)\r\n\r\nmodels[[\"(4)\"]] <- feols(\r\n  outcome ~ treatment + control1 + control2 | unit_fe + time_fe,\r\n  cluster = ~cluster_var,\r\n  data = analysis_data\r\n)\r\n```\r\n\r\n### 2. Create Main Results Table\r\n\r\n```r\r\n# Console output for review\r\nmodelsummary(models,\r\n             stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),\r\n             gof_omit = \"AIC|BIC|Log|RMSE\")\r\n\r\n# Publication table\r\nmodelsummary(models,\r\n             output = \"output/tables/table2_main_results.tex\",\r\n             stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),\r\n             coef_rename = c(\"treatment\" = \"Treatment Effect\"),\r\n             gof_omit = \"AIC|BIC|Log|RMSE\",\r\n             title = \"Main Results\",\r\n             notes = \"Standard errors clustered at [level]. * p<0.1, ** p<0.05, *** p<0.01\")\r\n```\r\n\r\n### 3. Interpret the Results\r\n\r\nFor the preferred specification, document:\r\n\r\n**Point estimate:**\r\n- What is the estimated effect?\r\n- What are the units? (interpret in substantive terms)\r\n- How large is this effect? (compare to mean, SD, or meaningful benchmark)\r\n\r\n**Statistical precision:**\r\n- What is the standard error?\r\n- What is the confidence interval?\r\n- Is this precisely estimated or noisy?\r\n\r\n**Stability across specifications:**\r\n- Does the estimate change substantially across models?\r\n- What does adding controls/FE do to the estimate?\r\n- Is the sign consistent?\r\n\r\n### 4. Interpret Nonlinear Models (If Applicable)\r\n\r\nFor logistic, Poisson, ordered logit, or other nonlinear models, coefficients alone are insufficient. Follow current methodological standards (Long and Mustillo 2017; Mize 2019):\r\n\r\n**Average Marginal Effects (AMEs):**\r\n```r\r\nlibrary(marginaleffects)\r\n\r\n# Compute AMEs for all predictors\r\name <- avg_slopes(model)\r\nprint(ame)\r\n\r\n# Create AME table\r\nmodelsummary(model,\r\n             output = \"output/tables/table2_ame.tex\",\r\n             estimate = \"AME\",\r\n             statistic = \"conf.int\")\r\n```\r\n\r\n**Predicted Probabilities:**\r\n```r\r\n# Predictions at specific values\r\npredictions <- predictions(model,\r\n                           newdata = datagrid(treatment = c(0, 1),\r\n                                             control1 = mean))\r\n\r\n# Plot predicted probabilities across range of X\r\nplot_predictions(model, condition = \"treatment\")\r\nggsave(\"output/figures/predicted_probs.pdf\", width = 8, height = 6)\r\n```\r\n\r\n**Interpreting Interactions in Nonlinear Models:**\r\n```r\r\n# Show how effect of X varies across Z (first differences)\r\nslopes(model, variables = \"treatment\", by = \"moderator\") |>\r\n  plot() +\r\n  geom_hline(yintercept = 0, linetype = \"dashed\")\r\n\r\n# Second differences (how the gap changes)\r\ncomparisons(model, variables = \"treatment\", by = \"moderator\",\r\n            hypothesis = \"pairwise\")\r\n```\r\n\r\n**Model Justification Paragraph Template:**\r\n```markdown\r\nWe use [SPECIFIC MODEL] to model [OUTCOME] because [PROPERTY OF OUTCOME].\r\nWe chose [THIS MODEL] over [ALTERNATIVE] because [DIAGNOSTIC TEST RESULT].\r\n[If relevant: We tested the [KEY ASSUMPTION] using [TEST NAME] and found\r\n[RESULT].]\r\n```\r\n\r\n**Key Rules:**\r\n- Report AMEs, not just odds ratios or log-odds\r\n- Show predicted probabilities for substantive scenarios\r\n- For interactions: show first differences (group comparisons) and second differences (how gaps change)\r\n- Never interpret main effect coefficients when interactions are present (those are conditional effects)\r\n- Always include confidence intervals\r\n\r\n### 5. Check Model Assumptions\r\n\r\nRun diagnostics appropriate to the method:\r\n\r\n**For OLS/Fixed Effects:**\r\n```r\r\n# Residual diagnostics\r\nplot(models[[\"(4)\"]])\r\n\r\n# Check for multicollinearity\r\nlibrary(car)\r\nvif(lm(outcome ~ treatment + control1 + control2, data = analysis_data))\r\n```\r\n\r\n**For Logistic Regression:**\r\n```r\r\n# Model fit\r\nlibrary(pROC)\r\nroc_result <- roc(analysis_data$outcome, predict(model, type = \"response\"))\r\nauc(roc_result)  # Should be > 0.7\r\n\r\n# Classification accuracy (note: compare to base rate!)\r\npred_class <- ifelse(predict(model, type = \"response\") > 0.5, 1, 0)\r\nmean(pred_class == analysis_data$outcome)\r\n\r\n# Report pseudo-R with context\r\n# Note: pseudo-R values 0.10-0.20 often indicate reasonable fit\r\n```\r\n\r\n**For Count Models (Poisson/Negative Binomial):**\r\n```r\r\n# Test for overdispersion\r\nlibrary(AER)\r\ndispersiontest(poisson_model)\r\n\r\n# If overdispersed (p < 0.05), use negative binomial:\r\nlibrary(MASS)\r\nnb_model <- glm.nb(count ~ treatment + controls, data = analysis_data)\r\n\r\n# Compare AIC\r\nAIC(poisson_model, nb_model)\r\n```\r\n\r\n**For Ordered Logit:**\r\n```r\r\nlibrary(brant)\r\n# Test proportional odds assumption\r\nbrant_test <- brant(ordered_model)\r\nprint(brant_test)  # If violated, use generalized ordered logit\r\n```\r\n\r\n**For DiD:**\r\n```r\r\n# Pre-trends (visual)\r\nevent_study <- feols(\r\n  outcome ~ i(time_to_treat, ref = -1) | unit_fe + time_fe,\r\n  cluster = ~cluster_var,\r\n  data = analysis_data\r\n)\r\n\r\niplot(event_study, main = \"Event Study\")\r\n```\r\n\r\n**For IV:**\r\n```r\r\n# First stage F-statistic\r\nfirst_stage <- feols(endogenous ~ instrument + controls | fe, data = analysis_data)\r\nsummary(first_stage)  # Check F > 10 (or use effective F)\r\n\r\n# Report first stage\r\nfitstat(iv_model, \"ivf\")\r\n```\r\n\r\n### 5. Visualize Key Results\r\n\r\nCreate figures for the main findings:\r\n\r\n**Coefficient plot:**\r\n```r\r\nlibrary(ggplot2)\r\n\r\nmodelplot(models[[\"(4)\"]], coef_omit = \"Intercept\") +\r\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\r\n  theme_minimal() +\r\n  labs(title = \"Treatment Effect Estimate\")\r\n\r\nggsave(\"output/figures/figure_coefplot.pdf\", width = 8, height = 6)\r\n```\r\n\r\n**Event study plot:**\r\n```r\r\niplot(event_study)\r\n# or custom with ggplot\r\ncoef_data <- broom::tidy(event_study, conf.int = TRUE)\r\nggplot(coef_data, aes(x = term, y = estimate, ymin = conf.low, ymax = conf.high)) +\r\n  geom_pointrange() +\r\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\r\n  theme_minimal()\r\n```\r\n\r\n**Marginal effects (for interactions):**\r\n```r\r\nlibrary(marginaleffects)\r\n\r\n# If model has interactions\r\nmfx <- slopes(model, variables = \"treatment\", by = \"moderator\")\r\nplot(mfx)\r\n```\r\n\r\n## Output: Results Report\r\n\r\nCreate a results report (`memos/phase3-results-report.md`):\r\n\r\n```markdown\r\n# Main Results Report\r\n\r\n## Summary of Findings\r\n\r\n**Main estimate**: [interpretation in words]\r\n\r\nThe preferred specification (Model X) shows that [treatment] is associated with\r\na [magnitude] [direction] in [outcome]. This effect is [statistically significant\r\nat the X% level / not statistically significant].\r\n\r\n## Results Table\r\n\r\n[Reference Table 2 or include formatted output]\r\n\r\n## Interpretation\r\n\r\n### Magnitude\r\n- Point estimate: [value]\r\n- Units: [what this means]\r\n- Context: [comparison to mean/SD/other benchmark]\r\n\r\n### Precision\r\n- Standard error: [value]\r\n- 95% CI: [lower, upper]\r\n- This is [precise/noisy] because [reason]\r\n\r\n### Stability\r\n- The estimate [is stable / changes] across specifications\r\n- Adding controls [increases/decreases/doesn't change] the estimate\r\n- This suggests [interpretation of stability pattern]\r\n\r\n## Diagnostic Checks\r\n- [Results of assumption tests]\r\n- [Any concerns raised]\r\n\r\n## Visualizations\r\n- Figure X: [description]\r\n- Figure Y: [description]\r\n\r\n## Preliminary Assessment\r\n- These results [support / do not support / partially support] the hypothesis\r\n- Key caveat: [main limitation]\r\n- Next step: robustness checks in Phase 4\r\n\r\n## Questions for User\r\n- [Any interpretive questions]\r\n- [Should we proceed to robustness?]\r\n```\r\n\r\n## When You're Done\r\n\r\nReturn a summary to the orchestrator that includes:\r\n1. The main estimate and its interpretation\r\n2. Whether the effect is statistically significant\r\n3. Whether results are stable across specifications\r\n4. Any diagnostic concerns\r\n5. Questions for the user\r\n\r\n**Do not proceed to Phase 4 until the user reviews the main results.**\r\n",
        "plugins/r-analyst/skills/r-analyst/phases/phase4-robustness.md": "# Phase 4: Robustness & Sensitivity\r\n\r\nYou are executing Phase 4 of a statistical analysis in R. Your goal is to stress-test the main findings through robustness checks and sensitivity analysis.\r\n\r\n## Why This Phase Matters\r\n\r\nMain results are only as credible as their robustness. Reviewers will ask: \"How do you know this isn't driven by [X]?\" This phase pre-empts those questions and honestly assesses the fragility of the findings.\r\n\r\n## Technique Guides\r\n\r\n**Consult these guides** in `r-statistical-techniques/` for robustness code patterns:\r\n\r\n| Topic | Guide |\r\n|-------|-------|\r\n| Sensitivity to unobservables | `05_bayesian_sensitivity.md` (sensemakr section) |\r\n| DiD robustness, Event studies | `01_core_econometrics.md` |\r\n| Bootstrap, Resampling | `02_survey_resampling.md` |\r\n| Matching diagnostics | `01_core_econometrics.md` Section 6 |\r\n\r\n## Your Tasks\r\n\r\n### 1. Alternative Specifications\r\n\r\nRun the pre-specified alternatives from Phase 2:\r\n\r\n**Different control sets:**\r\n```r\r\n# Minimal controls\r\nrobust_minimal <- feols(outcome ~ treatment | fe, cluster = ~cluster_var, data = data)\r\n\r\n# Extended controls\r\nrobust_extended <- feols(outcome ~ treatment + extra_controls | fe,\r\n                         cluster = ~cluster_var, data = data)\r\n\r\n# Different functional form\r\nrobust_log <- feols(log(outcome) ~ treatment | fe, cluster = ~cluster_var, data = data)\r\n```\r\n\r\n**Different fixed effects:**\r\n```r\r\n# Unit-by-year FE (more demanding)\r\nrobust_fe <- feols(outcome ~ treatment | unit^year, cluster = ~cluster_var, data = data)\r\n\r\n# Region-by-year FE\r\nrobust_region <- feols(outcome ~ treatment | region^year + unit,\r\n                       cluster = ~cluster_var, data = data)\r\n```\r\n\r\n**Different standard errors:**\r\n```r\r\n# Compare clustering levels\r\nsummary(main_model, cluster = ~unit)\r\nsummary(main_model, cluster = ~state)\r\nsummary(main_model, cluster = ~unit + year)\r\n```\r\n\r\n### 2. Placebo Tests\r\n\r\n**Pre-treatment effects (for DiD/Event Study):**\r\n```r\r\n# Should see no effect before treatment\r\npre_data <- data %>% filter(year < treatment_year)\r\nplacebo_pre <- feols(outcome ~ fake_treatment | unit + year,\r\n                     cluster = ~unit, data = pre_data)\r\n```\r\n\r\n**Fake treatment timing:**\r\n```r\r\n# Assign treatment X years earliershould find no effect\r\ndata <- data %>%\r\n  mutate(fake_treated = treated_post & year >= (treatment_year - 3))\r\n\r\nplacebo_timing <- feols(outcome ~ fake_treated | unit + year,\r\n                        cluster = ~unit, data = data)\r\n```\r\n\r\n**Outcome that shouldn't be affected:**\r\n```r\r\n# If treatment affects X, it shouldn't affect unrelated Y\r\nplacebo_outcome <- feols(unrelated_outcome ~ treatment | unit + year,\r\n                         cluster = ~unit, data = data)\r\n```\r\n\r\n### 3. Missing Data Assessment\r\n\r\nBefore running sensitivity analyses, document and address missing data:\r\n\r\n**Document Missingness:**\r\n```r\r\n# Overall missingness rates\r\nsapply(analysis_data, function(x) mean(is.na(x))) |>\r\n  sort(decreasing = TRUE) |>\r\n  head(10)\r\n\r\n# Missingness by key variables\r\nanalysis_data %>%\r\n  group_by(treatment) %>%\r\n  summarise(across(everything(), ~mean(is.na(.))))\r\n```\r\n\r\n**Test for MCAR/MAR:**\r\n```r\r\nlibrary(naniar)\r\n\r\n# Visualize missingness patterns\r\nvis_miss(analysis_data)\r\ngg_miss_upset(analysis_data)\r\n\r\n# Little's MCAR test (if needed)\r\nlibrary(mice)\r\nmcar_test(analysis_data)\r\n```\r\n\r\n**Multiple Imputation (if substantial missingness):**\r\n```r\r\nlibrary(mice)\r\n\r\n# Use adequate number of imputations (m  20, preferably  50)\r\nimp <- mice(analysis_data, m = 50, method = 'pmm', seed = 12345,\r\n            printFlag = FALSE)\r\n\r\n# Check imputation diagnostics\r\ndensityplot(imp)  # Compare imputed vs observed distributions\r\nstripplot(imp)\r\n\r\n# Run analysis on imputed datasets and pool\r\nfit_imp <- with(imp, lm(outcome ~ treatment + control1 + control2))\r\npooled <- pool(fit_imp)\r\nsummary(pooled)\r\n\r\n# Include auxiliary variables to strengthen MAR assumption\r\n# These are variables that predict missingness or the outcome\r\n```\r\n\r\n**Compare Missing Data Approaches:**\r\n```r\r\n# Create comparison table\r\nmissing_comparison <- list(\r\n  \"Complete case\" = main_model,\r\n  \"Multiple imputation\" = pooled_model,\r\n  \"Single imputation\" = single_imp_model  # if applicable\r\n)\r\n\r\nmodelsummary(missing_comparison,\r\n             output = \"output/tables/missing_data_sensitivity.tex\",\r\n             title = \"Sensitivity to Missing Data Treatment\")\r\n```\r\n\r\n**Report in Methods Section:**\r\n```markdown\r\n[X]% of observations were missing on [variable]. We tested for patterns\r\nof missingness and found [MCAR/MAR/evidence of MNAR]. Our primary analysis\r\nuses [complete case / multiple imputation with m = X imputations].\r\nSensitivity analyses comparing complete case, single imputation, and\r\nmultiple imputation show [results are robust / estimates differ by X].\r\n```\r\n\r\n### 4. Sensitivity Analysis\r\n\r\n**Sensitivity to outliers:**\r\n```r\r\n# Winsorize extreme values\r\nlibrary(DescTools)\r\ndata$outcome_w <- Winsorize(data$outcome, probs = c(0.01, 0.99))\r\nrobust_winsor <- feols(outcome_w ~ treatment | fe, cluster = ~cluster_var, data = data)\r\n\r\n# Drop extreme observations\r\ndata_trimmed <- data %>% filter(outcome > quantile(outcome, 0.01) &\r\n                                 outcome < quantile(outcome, 0.99))\r\nrobust_trim <- feols(outcome ~ treatment | fe, cluster = ~cluster_var, data = data_trimmed)\r\n```\r\n\r\n**Sensitivity to sample restrictions:**\r\n```r\r\n# Different time periods\r\nrobust_early <- feols(outcome ~ treatment | fe, data = filter(data, year <= 2015))\r\nrobust_late <- feols(outcome ~ treatment | fe, data = filter(data, year > 2015))\r\n\r\n# Excluding specific units\r\nrobust_exclude <- feols(outcome ~ treatment | fe, data = filter(data, !outlier_unit))\r\n```\r\n\r\n**Selection on unobservables (sensemakr):**\r\n```r\r\nlibrary(sensemakr)\r\n\r\n# Fit OLS version for sensemakr\r\nols_model <- lm(outcome ~ treatment + controls, data = data)\r\n\r\n# Sensitivity analysis\r\nsens <- sensemakr(\r\n  model = ols_model,\r\n  treatment = \"treatment\",\r\n  benchmark_covariates = c(\"strongest_control\"),\r\n  kd = 1:3  # How strong would confounding need to be?\r\n)\r\n\r\n# Summary\r\nsummary(sens)\r\n\r\n# Plot\r\nplot(sens)\r\nggsave(\"output/figures/sensitivity_plot.pdf\", width = 8, height = 6)\r\n```\r\n\r\n### 4. Subgroup Analysis\r\n\r\nRun pre-specified heterogeneity analyses:\r\n\r\n```r\r\n# By group\r\nrobust_subgroup1 <- feols(outcome ~ treatment | fe,\r\n                          data = filter(data, subgroup == 1))\r\nrobust_subgroup2 <- feols(outcome ~ treatment | fe,\r\n                          data = filter(data, subgroup == 2))\r\n\r\n# Interaction approach (preferred)\r\nrobust_het <- feols(outcome ~ treatment * subgroup_var | fe,\r\n                    cluster = ~cluster_var, data = data)\r\n\r\n# Visualize heterogeneity\r\nlibrary(marginaleffects)\r\nhet_effects <- slopes(robust_het, variables = \"treatment\", by = \"subgroup_var\")\r\nplot(het_effects)\r\n```\r\n\r\n### 5. Panel/Longitudinal Data Robustness (If Applicable)\r\n\r\n**Attrition Analysis:**\r\n```r\r\n# Document attrition rates by wave\r\nattrition_table <- data %>%\r\n  group_by(wave) %>%\r\n  summarise(n = n_distinct(id),\r\n            pct_remaining = n / first(n) * 100)\r\n\r\n# Test if attrition is related to treatment or outcomes\r\nattrition_model <- glm(dropped_out ~ treatment + baseline_outcome + covariates,\r\n                       family = binomial, data = baseline_data)\r\nsummary(attrition_model)\r\n```\r\n\r\n**Inverse Probability Weighting for Selection:**\r\n```r\r\nlibrary(ipw)\r\n\r\n# Estimate selection weights\r\nps_model <- glm(observed ~ treatment + covariates, family = binomial, data = data)\r\nweights <- 1 / predict(ps_model, type = \"response\")\r\n\r\n# Apply weights in main analysis\r\nrobust_ipw <- feols(outcome ~ treatment | unit + time,\r\n                    weights = ~weights, data = data)\r\n```\r\n\r\n**Fixed vs Random Effects:**\r\n```r\r\nlibrary(plm)\r\n\r\n# Hausman test\r\nfe_model <- plm(outcome ~ treatment + covariates, model = \"within\",\r\n                index = c(\"id\", \"time\"), data = data)\r\nre_model <- plm(outcome ~ treatment + covariates, model = \"random\",\r\n                index = c(\"id\", \"time\"), data = data)\r\nphtest(fe_model, re_model)  # p < 0.05 suggests FE preferred\r\n\r\n# Within-between decomposition (correlated random effects)\r\ndata <- data %>%\r\n  group_by(id) %>%\r\n  mutate(across(c(treatment, covariates), list(between = ~mean(., na.rm = TRUE)))) %>%\r\n  mutate(across(c(treatment, covariates),\r\n                list(within = ~. - mean(., na.rm = TRUE)), .names = \"{.col}_within\"))\r\n```\r\n\r\n### 6. Method-Specific Diagnostics\r\n\r\n**For DiD with staggered treatment:**\r\n```r\r\n# Check for heterogeneous treatment effects\r\nlibrary(did)\r\ndid_result <- att_gt(\r\n  yname = \"outcome\",\r\n  tname = \"year\",\r\n  idname = \"unit\",\r\n  gname = \"treatment_year\",  # Year unit first treated\r\n  data = data\r\n)\r\n\r\n# Event study\r\nes <- aggte(did_result, type = \"dynamic\")\r\nggdid(es)\r\n```\r\n\r\n**For IV:**\r\n```r\r\n# Weak instrument test\r\nfitstat(iv_model, \"ivf\")  # Should be > 10, ideally > 20\r\n\r\n# Overidentification test (if multiple instruments)\r\nfitstat(iv_model, \"sargan\")\r\n```\r\n\r\n**For Matching:**\r\n```r\r\nlibrary(MatchIt)\r\nlibrary(cobalt)\r\n\r\n# Balance check\r\nbal.tab(matched_data, un = TRUE)\r\n\r\n# Love plot\r\nlove.plot(matched_data)\r\n```\r\n\r\n### 6. Create Robustness Table\r\n\r\nCompile all robustness checks:\r\n\r\n```r\r\nrobustness_models <- list(\r\n  \"Main\" = main_model,\r\n  \"Minimal controls\" = robust_minimal,\r\n  \"Extended controls\" = robust_extended,\r\n  \"Alt FE\" = robust_fe,\r\n  \"Winsorized\" = robust_winsor,\r\n  \"Pre-2015\" = robust_early,\r\n  \"Post-2015\" = robust_late\r\n)\r\n\r\nmodelsummary(robustness_models,\r\n             output = \"output/tables/table3_robustness.tex\",\r\n             stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),\r\n             title = \"Robustness Checks\")\r\n```\r\n\r\n## Output: Robustness Report\r\n\r\nCreate a robustness report (`memos/phase4-robustness-report.md`):\r\n\r\n```markdown\r\n# Robustness Report\r\n\r\n## Summary Assessment\r\n\r\nThe main findings are [robust / partially robust / not robust] to alternative specifications.\r\n\r\n## Alternative Specifications\r\n\r\n| Specification | Estimate | SE | Conclusion |\r\n|---------------|----------|-----|------------|\r\n| Main | X.XX | (X.XX) | - |\r\n| Minimal controls | X.XX | (X.XX) | [stable/different] |\r\n| Extended controls | X.XX | (X.XX) | [stable/different] |\r\n| Alt FE | X.XX | (X.XX) | [stable/different] |\r\n...\r\n\r\n## Placebo Tests\r\n\r\n| Test | Expected | Found | Pass? |\r\n|------|----------|-------|-------|\r\n| Pre-treatment | 0 | X.XX (p=X.XX) | [Yes/No] |\r\n| Fake timing | 0 | X.XX (p=X.XX) | [Yes/No] |\r\n| Unrelated outcome | 0 | X.XX (p=X.XX) | [Yes/No] |\r\n\r\n## Sensitivity Analysis\r\n\r\n### Outliers\r\n- Results [are / are not] sensitive to extreme values\r\n\r\n### Sample restrictions\r\n- Results [hold / change] in different subsamples\r\n\r\n### Selection on unobservables (sensemakr)\r\n- An unobserved confounder would need to be [X times] as strong as\r\n  [strongest observed covariate] to explain away the result\r\n- This is [plausible / implausible] because [reasoning]\r\n\r\n## Subgroup Analysis\r\n\r\n| Subgroup | Estimate | SE | Different from main? |\r\n|----------|----------|-----|---------------------|\r\n| Group 1 | X.XX | (X.XX) | [Yes/No] |\r\n| Group 2 | X.XX | (X.XX) | [Yes/No] |\r\n\r\n## Method-Specific Diagnostics\r\n[Results of diagnostic tests]\r\n\r\n## Overall Assessment\r\n\r\n**Strengths:**\r\n- [What checks the results passed]\r\n\r\n**Concerns:**\r\n- [Any issues found]\r\n\r\n**Conclusion:**\r\nThe main findings [can / cannot] be considered robust because [reasoning].\r\n\r\n## Questions for User\r\n- [Any interpretive questions about robustness]\r\n```\r\n\r\n## When You're Done\r\n\r\nReturn a summary to the orchestrator that includes:\r\n1. Overall robustness assessment\r\n2. Which checks passed/failed\r\n3. Sensitivity analysis conclusions\r\n4. Any concerns about the findings\r\n5. Questions for the user\r\n\r\n**Do not proceed to Phase 5 until the user reviews the robustness assessment.**\r\n",
        "plugins/r-analyst/skills/r-analyst/phases/phase5-output.md": "# Phase 5: Output & Interpretation\r\n\r\nYou are executing Phase 5 of a statistical analysis in R. Your goal is to produce publication-ready outputs and synthesize the analysis into a coherent narrative.\r\n\r\n## Why This Phase Matters\r\n\r\nAnalysis isn't complete until it's communicated. This phase transforms results into tables, figures, and text that can appear in a journal article. Good output is accurate, clear, and tells a story.\r\n\r\n## Technique Guides\r\n\r\n**Consult these guides** in `r-statistical-techniques/` for output code patterns:\r\n\r\n| Topic | Guide |\r\n|-------|-------|\r\n| Tables (modelsummary, etable) | `06_visualization.md` |\r\n| Figures (ggplot2, coefplot) | `06_visualization.md` |\r\n| Project structure, Reproducibility | `07_best_practices.md` |\r\n| Marginal effects visualization | `08_nonlinear_models.md` |\r\n\r\n## Your Tasks\r\n\r\n### 1. Finalize Tables\r\n\r\n**Table 1: Descriptive Statistics**\r\n```r\r\nlibrary(modelsummary)\r\n\r\n# Summary statistics\r\ndatasummary(\r\n  outcome + treatment + control1 + control2 ~\r\n    N + Mean + SD + Min + Max,\r\n  data = analysis_data,\r\n  output = \"output/tables/table1_descriptives.tex\",\r\n  title = \"Summary Statistics\",\r\n  notes = \"Sample includes [description]. Data from [source].\"\r\n)\r\n\r\n# Balance table (if applicable)\r\ndatasummary_balance(\r\n  ~ treatment,\r\n  data = analysis_data,\r\n  output = \"output/tables/table1_balance.tex\",\r\n  title = \"Balance Across Treatment Groups\"\r\n)\r\n```\r\n\r\n**Table 2: Main Results**\r\n```r\r\nmodelsummary(\r\n  main_models,\r\n  output = \"output/tables/table2_main.tex\",\r\n  stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),\r\n  coef_map = c(\r\n    \"treatment\" = \"Treatment\",\r\n    \"control1\" = \"Control 1\",\r\n    \"control2\" = \"Control 2\"\r\n  ),\r\n  gof_map = c(\"nobs\", \"r.squared\", \"FE: unit\", \"FE: year\"),\r\n  title = \"Effect of [Treatment] on [Outcome]\",\r\n  notes = list(\r\n    \"Standard errors clustered at [level] in parentheses.\",\r\n    \"* p<0.1, ** p<0.05, *** p<0.01\"\r\n  )\r\n)\r\n```\r\n\r\n**Table 3: Robustness**\r\n```r\r\nmodelsummary(\r\n  robustness_models,\r\n  output = \"output/tables/table3_robustness.tex\",\r\n  stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),\r\n  coef_omit = \"control\",  # Show only treatment\r\n  title = \"Robustness Checks\",\r\n  notes = \"See notes to Table 2.\"\r\n)\r\n```\r\n\r\n### 2. Create Publication Figures\r\n\r\n**Figure 1: Trends (for DiD)**\r\n```r\r\nlibrary(ggplot2)\r\n\r\ntrends_data <- analysis_data %>%\r\n  group_by(year, treatment_group) %>%\r\n  summarise(mean_outcome = mean(outcome, na.rm = TRUE),\r\n            se = sd(outcome, na.rm = TRUE) / sqrt(n()))\r\n\r\np_trends <- ggplot(trends_data, aes(x = year, y = mean_outcome,\r\n                                     color = treatment_group,\r\n                                     linetype = treatment_group)) +\r\n  geom_line(size = 1) +\r\n  geom_point(size = 2) +\r\n  geom_ribbon(aes(ymin = mean_outcome - 1.96*se,\r\n                  ymax = mean_outcome + 1.96*se,\r\n                  fill = treatment_group),\r\n              alpha = 0.2, color = NA) +\r\n  geom_vline(xintercept = treatment_year, linetype = \"dashed\", color = \"gray40\") +\r\n  annotate(\"text\", x = treatment_year, y = Inf, label = \"Treatment\",\r\n           vjust = 2, hjust = 0.5, size = 3) +\r\n  scale_color_manual(values = c(\"Control\" = \"#1f77b4\", \"Treated\" = \"#ff7f0e\")) +\r\n  scale_fill_manual(values = c(\"Control\" = \"#1f77b4\", \"Treated\" = \"#ff7f0e\")) +\r\n  labs(x = \"Year\", y = \"Outcome\", color = \"\", fill = \"\", linetype = \"\") +\r\n  theme_minimal() +\r\n  theme(legend.position = \"bottom\",\r\n        panel.grid.minor = element_blank())\r\n\r\nggsave(\"output/figures/figure1_trends.pdf\", p_trends, width = 8, height = 5)\r\n```\r\n\r\n**Figure 2: Event Study**\r\n```r\r\n# Using fixest\r\np_event <- iplot(event_study_model,\r\n                  main = \"\",\r\n                  xlab = \"Time Relative to Treatment\",\r\n                  ylab = \"Coefficient Estimate\")\r\n\r\n# Or custom ggplot\r\nevent_coefs <- broom::tidy(event_study_model, conf.int = TRUE) %>%\r\n  filter(str_detect(term, \"time_to_treat\")) %>%\r\n  mutate(time = as.numeric(str_extract(term, \"-?\\\\d+\")))\r\n\r\np_event <- ggplot(event_coefs, aes(x = time, y = estimate)) +\r\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray40\") +\r\n  geom_vline(xintercept = -0.5, linetype = \"dashed\", color = \"gray40\") +\r\n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high)) +\r\n  labs(x = \"Time Relative to Treatment\", y = \"Coefficient Estimate\") +\r\n  theme_minimal()\r\n\r\nggsave(\"output/figures/figure2_eventstudy.pdf\", p_event, width = 8, height = 5)\r\n```\r\n\r\n**Figure 3: Coefficient Plot**\r\n```r\r\np_coef <- modelplot(main_model, coef_omit = \"Intercept\") +\r\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\r\n  theme_minimal() +\r\n  labs(title = \"\")\r\n\r\nggsave(\"output/figures/figure3_coefplot.pdf\", p_coef, width = 6, height = 4)\r\n```\r\n\r\n### 3. Write Results Narrative\r\n\r\nDraft the key paragraphs for the results section:\r\n\r\n**Main effect paragraph:**\r\n> Table 2 presents estimates of the effect of [treatment] on [outcome].\r\n> Column (1) shows the baseline relationship without controls.\r\n> Column (4), our preferred specification, includes [unit] and [time] fixed effects\r\n> and clusters standard errors at the [level] level. We find that [treatment]\r\n> [increases/decreases] [outcome] by [X] [units], significant at the [Y]% level\r\n> (95% CI: [lower, upper]). This represents a [Z]% change relative to the\r\n> pre-treatment mean of [mean].\r\n\r\n**Robustness paragraph:**\r\n> Table 3 demonstrates that this finding is robust to alternative specifications.\r\n> The point estimate remains [stable/similar] when we [change 1], [change 2], and\r\n> [change 3]. The effect is [somewhat/not] sensitive to [what]. The sensitivity\r\n> analysis in Figure X shows that an unobserved confounder would need to be\r\n> [X times as strong as the strongest observed predictor] to fully explain\r\n> our findings, suggesting that selection on unobservables is unlikely to\r\n> account for the entire effect.\r\n\r\n**Heterogeneity paragraph (if applicable):**\r\n> We examine heterogeneity in treatment effects across [subgroups].\r\n> Table X shows that the effect is [larger/smaller] for [group 1]\r\n> ([estimate]) compared to [group 2] ([estimate]). This difference is\r\n> [statistically significant / not statistically significant] (p = [value]).\r\n\r\n### 4. Survey Data Methods Section (If Applicable)\r\n\r\nFor survey-based analyses, address the five survey methodology deliverables:\r\n\r\n**1. Sampling Frame Description:**\r\n```markdown\r\nData come from the [SURVEY NAME], a [DESIGN TYPE] of [POPULATION].\r\nThe sampling frame is [DESCRIPTION]. The target population is [WHO].\r\n[Exclusions]: We exclude [categories] because [reason].\r\n```\r\n\r\n**2. Response Rate and Nonresponse:**\r\n```markdown\r\nThe response rate was [X]% (calculated as [METHOD: AAPOR RR1/RR3/etc.]).\r\nWe compared respondents to [population benchmark / nonrespondents on X variables]\r\nand found [no significant differences / differences on X that we address through Y].\r\n```\r\n\r\n**3. Weighting Justification:**\r\n```markdown\r\nWe apply [WEIGHT TYPE: post-stratification / raking / none] weights to adjust for\r\n[FACTORS]. Weights are provided by [SOURCE] and calibrated to [BENCHMARKS].\r\n[Alternative: We do not apply weights because {justification}.]\r\n```\r\n\r\n**4. Survey Design Acknowledgment:**\r\n```r\r\nlibrary(survey)\r\n\r\n# Define survey design\r\nsurvey_design <- svydesign(\r\n  ids = ~cluster,\r\n  strata = ~stratum,\r\n  weights = ~weight,\r\n  data = analysis_data\r\n)\r\n\r\n# All analyses use survey-adjusted estimates\r\nsvymean(~outcome, survey_design)\r\nsvyglm(outcome ~ treatment + controls, design = survey_design)\r\n```\r\n\r\n**5. Population Inference Boundaries:**\r\n```markdown\r\nOur results generalize to [POPULATION] during [TIME PERIOD]. We cannot\r\nspeak to [EXCLUDED GROUPS / OTHER TIME PERIODS] because [REASON].\r\n```\r\n\r\n### 5. Document Limitations\r\n\r\nIdentify and articulate limitations honestly:\r\n\r\n```markdown\r\n## Limitations\r\n\r\n1. **[Identification limitation]**: Our identification strategy relies on\r\n   [assumption]. While we provide evidence supporting this assumption through\r\n   [tests], we cannot definitively rule out [threat].\r\n\r\n2. **[External validity]**: Our sample consists of [description]. Results may\r\n   not generalize to [other contexts] because [reason].\r\n\r\n3. **[Measurement]**: [Variable] is measured using [method], which may\r\n   [limitation]. We address this by [mitigation] but acknowledge [remaining concern].\r\n\r\n4. **[Data limitation]**: We lack data on [variable], which could [potential issue].\r\n   Our robustness checks in Table X suggest this is [unlikely to/may] affect our\r\n   conclusions.\r\n```\r\n\r\n### 5. Create Replication Package\r\n\r\nPrepare materials for reproducibility:\r\n\r\n```r\r\n# Master script header\r\ncat('\r\n# ============================================================\r\n# Replication Code for \"[Paper Title]\"\r\n# Authors: [Names]\r\n# Date: [Date]\r\n#\r\n# This script reproduces all tables and figures in the paper.\r\n# Runtime: approximately [X] minutes on [hardware]\r\n# ============================================================\r\n\r\n# Requirements\r\n# R version: ', R.version.string, '\r\n# Key packages: fixest, modelsummary, ggplot2, dplyr\r\n\r\n# Set seed for reproducibility\r\nset.seed(12345)\r\n\r\n# Run analysis\r\nsource(\"code/01_clean_data.R\")\r\nsource(\"code/02_descriptives.R\")\r\nsource(\"code/03_main_analysis.R\")\r\nsource(\"code/04_robustness.R\")\r\nsource(\"code/05_figures.R\")\r\n\r\n# Session info\r\nsessionInfo()\r\n', sep = \"\")\r\n```\r\n\r\n## Output: Final Report\r\n\r\nCreate the final synthesis (`memos/phase5-final-report.md`):\r\n\r\n```markdown\r\n# Analysis Summary\r\n\r\n## Key Finding\r\n[One sentence summary of the main result]\r\n\r\n## Main Result\r\n- **Effect size**: [estimate with CI]\r\n- **Significance**: [p-value or significance level]\r\n- **Interpretation**: [what this means substantively]\r\n\r\n## Robustness Assessment\r\n- The finding [is/is not] robust to [list of checks]\r\n- Main concerns: [if any]\r\n\r\n## Output Files Created\r\n\r\n### Tables\r\n- `table1_descriptives.tex`: Summary statistics\r\n- `table2_main.tex`: Main results\r\n- `table3_robustness.tex`: Robustness checks\r\n\r\n### Figures\r\n- `figure1_trends.pdf`: Pre/post trends\r\n- `figure2_eventstudy.pdf`: Event study\r\n- `figure3_coefplot.pdf`: Coefficient plot\r\n\r\n### Replication Materials\r\n- `00_master.R`: Master script\r\n- `code/`: All analysis code\r\n- `data/clean/`: Analysis datasets\r\n\r\n## Results Narrative\r\n[Draft paragraphs for the paper]\r\n\r\n## Limitations\r\n[Honest assessment of limitations]\r\n\r\n## Conclusion\r\n[What can and cannot be concluded from this analysis]\r\n```\r\n\r\n### 6. Pre-Submission Checklist\r\n\r\nBefore finalizing, verify the analysis meets publication standards:\r\n\r\n**Minimum Standard (Required):**\r\n- [ ] All variables clearly defined with units and coding\r\n- [ ] Sample size and any exclusions documented\r\n- [ ] Main coefficient table includes SEs and significance levels\r\n- [ ] Standard error type specified (robust, clustered at X level, etc.)\r\n- [ ] At least one robustness check reported\r\n- [ ] Limitations section acknowledges main threats to validity\r\n\r\n**Strong Standard (Competitive for top journals):**\r\n- [ ] Descriptive statistics table with means, SDs, and sample sizes\r\n- [ ] Multiple robustness specifications in appendix\r\n- [ ] Effect sizes interpreted substantively (not just \"significant\")\r\n- [ ] For nonlinear models: AMEs or predicted probabilities reported\r\n- [ ] Sensitivity analysis for selection on unobservables (e.g., sensemakr)\r\n- [ ] Missing data approach documented and defended\r\n- [ ] Visualization of key results (event study, coefficient plot, etc.)\r\n- [ ] Replication code and data availability statement\r\n\r\n**Exemplary Standard (Model for the field):**\r\n- [ ] Pre-registration referenced (if applicable)\r\n- [ ] Multiple identification strategies compared\r\n- [ ] Heterogeneity analysis with theoretical motivation\r\n- [ ] Mechanism analysis or mediation tests\r\n- [ ] Power analysis or minimum detectable effects\r\n- [ ] Bound analysis for worst-case scenarios\r\n- [ ] Complete replication package with README\r\n\r\n**Language Checklist:**\r\n- [ ] Causal language only used with appropriate identification strategy\r\n- [ ] Effect sizes interpreted relative to meaningful benchmarks\r\n- [ ] Confidence intervals reported, not just p-values\r\n- [ ] Scope conditions clearly stated\r\n- [ ] \"Significant\" refers to statistical significance (or avoid the term)\r\n\r\n## When You're Done\r\n\r\nReturn a summary to the orchestrator that includes:\r\n1. List of all tables and figures created\r\n2. The main finding in one sentence\r\n3. Key limitations\r\n4. Any remaining questions or concerns\r\n5. Confirmation that replication materials are ready\r\n6. Checklist tier achieved (minimum/strong/exemplary)\r\n\r\n**The analysis is now complete.** All materials should be ready for paper writing.\r\n",
        "plugins/r-analyst/skills/r-analyst/techniques/01_core_econometrics.md": "# Core Econometrics in R\r\n\r\nPanel methods, causal inference basics, and standard errors with reproducible examples.\r\n\r\n---\r\n\r\n## Setup\r\n\r\nInstall and load all required packages:\r\n\r\n```r\r\n# Install packages (only run once)\r\npackages <- c(\r\n  \"fixest\", \"did\", \"TwoWayFEWeights\", \"rdrobust\", \"rddensity\",\r\n  \"ivreg\", \"glmnet\", \"MatchIt\", \"cem\", \"cobalt\", \"ebal\",\r\n  \"Matching\", \"rgenoud\", \"PanelMatch\", \"survey\",\r\n  \"sensemakr\", \"mediation\", \"dplyr\", \"broom\", \"ggplot2\"\r\n)\r\ninstall.packages(setdiff(packages, rownames(installed.packages())))\r\n\r\n# Load packages\r\nlibrary(fixest)\r\nlibrary(did)\r\nlibrary(TwoWayFEWeights)\r\nlibrary(rdrobust)\r\nlibrary(rddensity)\r\nlibrary(ivreg)\r\nlibrary(glmnet)\r\nlibrary(MatchIt)\r\nlibrary(cem)\r\nlibrary(cobalt)\r\nlibrary(ebal)\r\nlibrary(Matching)\r\nlibrary(PanelMatch)\r\nlibrary(survey)\r\nlibrary(sensemakr)\r\nlibrary(mediation)\r\nlibrary(dplyr)\r\nlibrary(broom)\r\nlibrary(ggplot2)\r\n```\r\n\r\n---\r\n\r\n## 1. Two-Way Fixed Effects (TWFE)\r\n\r\nTWFE is the dominant approach for panel data analysis. It controls for unobserved unit-specific and time-specific confounders.\r\n\r\n### When to Use TWFE\r\n\r\n- You have panel data (repeated observations of units over time)\r\n- You believe unobserved unit and time characteristics confound your relationship\r\n- Treatment timing is consistent across all treated units (or you've verified no problematic negative weights)\r\n\r\n### Assumptions\r\n\r\n1. **Parallel trends**: Absent treatment, outcomes would evolve similarly across units\r\n2. **No anticipation**: Units don't change behavior before treatment\r\n3. **Stable unit treatment values**: No spillovers between units\r\n4. **Homogeneous treatment effects** (for staggered designs with TWFE)\r\n\r\n### Common Pitfalls\r\n\r\n- With staggered treatment timing, TWFE can produce negative weights and biased estimates\r\n- Always check for negative weights using `TwoWayFEWeights` or similar diagnostics\r\n- Two-way clustering requires sufficient clusters in both dimensions\r\n\r\n### Basic Specification\r\n\r\n**Model:** $Y_{it} = \\alpha_i + \\gamma_t + \\beta X_{it} + \\varepsilon_{it}$\r\n\r\nWhere $\\alpha_i$ are unit fixed effects and $\\gamma_t$ are time fixed effects.\r\n\r\n### Implementation with fixest\r\n\r\nUsing the `base_did` dataset from fixest (simulated DiD data with staggered treatment):\r\n\r\n```r\r\n# Load sample DiD data\r\ndata(base_did, package = \"fixest\")\r\n\r\n# Examine the data\r\nstr(base_did)\r\n# 'data.frame': 1080 obs. of 6 variables:\r\n#  $ y     : num  -0.816 2.223 0.481 -0.305 0.217 ...\r\n#  $ x1    : num  1.14 1.06 1.06 0.86 0.77 ...\r\n#  $ id    : int  1 1 1 1 1 1 1 1 1 1 ...\r\n#  $ period: int  1 2 3 4 5 6 7 8 9 10 ...\r\n#  $ post  : logi  FALSE FALSE FALSE FALSE TRUE TRUE ...\r\n#  $ treat : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\r\n\r\n# Basic TWFE with two-way fixed effects\r\nmodel_twfe <- feols(\r\n  y ~ x1 + treat | id + period,  # FE after |\r\n  cluster = ~ id,                 # Cluster SEs at unit level\r\n  data = base_did\r\n)\r\nsummary(model_twfe)\r\n\r\n# Output:\r\n# OLS estimation, Pair.Clust Dep. Var.: y\r\n# Observations: 1,080\r\n# Fixed-effects: id: 108,  period: 10\r\n# Standard-errors: Clustered by id\r\n#        Estimate Std. Error t value Pr(>|t|)\r\n# x1       0.9985   0.11632  8.5841  1.29e-13 ***\r\n# treat    0.6876   0.09126  7.5362  3.30e-11 ***\r\n```\r\n\r\n**Interpretation**: The treatment effect is 0.69, controlling for unit and time fixed effects. With this simulated data, we expect a positive treatment effect.\r\n\r\n### Two-Way Clustering\r\n\r\nWhen both unit and time dimensions have few clusters, use two-way clustering:\r\n\r\n```r\r\n# Two-way clustering (unit and time)\r\nmodel_2way <- feols(\r\n  y ~ x1 + treat | id + period,\r\n  cluster = ~ id + period,  # Two-way clustering\r\n  data = base_did\r\n)\r\n\r\n# Compare standard errors\r\ndata.frame(\r\n  Variable = c(\"x1\", \"treat\"),\r\n  `One-way SE` = model_twfe$se,\r\n  `Two-way SE` = model_2way$se\r\n)\r\n#   Variable One.way.SE Two.way.SE\r\n# 1       x1    0.11632    0.12845\r\n# 2    treat    0.09126    0.11203\r\n```\r\n\r\n### Multiple Outcomes\r\n\r\nEstimate the same specification across multiple outcomes efficiently:\r\n\r\n```r\r\n# Using trade data for multiple outcome example\r\ndata(trade, package = \"fixest\")\r\n\r\n# Multiple outcomes with consistent specification\r\nmodels <- feols(\r\n  c(Euros, log(Euros)) ~ log(dist_km) | Origin + Destination + Year,\r\n  cluster = ~ Origin,\r\n  data = trade\r\n)\r\n\r\n# Formatted output table\r\netable(models,\r\n       title = \"Trade Flow Regressions\",\r\n       headers = c(\"Levels\", \"Log\"),\r\n       fitstat = ~ r2 + n)\r\n```\r\n\r\n---\r\n\r\n## 2. Difference-in-Differences (DiD)\r\n\r\nDiD identifies causal effects by comparing treated and control groups before and after treatment.\r\n\r\n### When to Use DiD\r\n\r\n- You have a treatment that affects some units at a specific time\r\n- You have pre-treatment and post-treatment observations for both groups\r\n- Parallel trends assumption is plausible\r\n\r\n### Assumptions\r\n\r\n1. **Parallel trends**: Treatment and control groups would have followed parallel outcome paths absent treatment\r\n2. **No anticipation**: Treatment timing is not driven by expected outcomes\r\n3. **Stable composition**: No differential selection into/out of the sample\r\n\r\n### Common Pitfalls\r\n\r\n- Testing parallel trends with pre-treatment data is necessary but not sufficient\r\n- With staggered treatment timing, use modern DiD methods (Callaway-Sant'Anna, etc.)\r\n- Functional form matters - level vs. log outcomes can give different conclusions\r\n\r\n### 2.1 Traditional DiD\r\n\r\n**Model:** $Y_{it} = \\alpha + \\beta_1 \\text{Treat}_i + \\beta_2 \\text{Post}_t + \\beta_3 (\\text{Treat}_i \\times \\text{Post}_t) + \\varepsilon_{it}$\r\n\r\nThe coefficient $\\beta_3$ is the DiD estimate.\r\n\r\n```r\r\n# Using base_did from fixest\r\ndata(base_did, package = \"fixest\")\r\n\r\n# Create group indicators\r\nbase_did <- base_did %>%\r\n  mutate(\r\n    treated_group = as.numeric(id <= 54),  # First half are treated\r\n    post_period = as.numeric(period >= 5)   # Treatment at period 5\r\n  )\r\n\r\n# Traditional DiD with interaction\r\nmodel_did <- feols(\r\n  y ~ treated_group * post_period | id + period,\r\n  cluster = ~ id,\r\n  data = base_did\r\n)\r\nsummary(model_did)\r\n\r\n# The interaction term is the DiD estimate\r\n```\r\n\r\n### 2.2 Modern DiD Methods (Staggered Treatment)\r\n\r\nRecent econometric literature shows TWFE DiD can be biased with staggered treatment timing. Use the `did` package for the Callaway-Sant'Anna estimator.\r\n\r\n```r\r\n# Load county teen employment data\r\ndata(mpdta, package = \"did\")\r\n\r\n# Examine the data\r\nhead(mpdta)\r\n#   year countyreal     lpop     lemp first.treat treat\r\n# 1 2003       8001 5.896761 8.461469        2007     1\r\n# 2 2004       8001 5.896761 8.336870        2007     1\r\n# 3 2005       8001 5.896761 8.343058        2007     1\r\n# 4 2006       8001 5.896761 8.263403        2007     1\r\n# 5 2007       8001 5.896761 8.170693        2007     1\r\n\r\n# Callaway-Sant'Anna estimator\r\nout <- att_gt(\r\n  yname = \"lemp\",           # Outcome: log employment\r\n  tname = \"year\",           # Time variable\r\n  idname = \"countyreal\",    # Unit identifier\r\n  gname = \"first.treat\",    # Treatment timing (0 = never treated)\r\n  xformla = ~ lpop,         # Covariates (optional)\r\n  data = mpdta,\r\n  control_group = \"nevertreated\",  # Or \"notyettreated\"\r\n  bstrap = TRUE,\r\n  biters = 1000\r\n)\r\n\r\n# Summary of group-time ATTs\r\nsummary(out)\r\n\r\n# Output:\r\n# Group-Time Average Treatment Effects:\r\n#  Group Time ATT(g,t) Std. Error [95% Simult.  Conf. Band]\r\n#   2004 2004  -0.0145     0.0225       -0.0772       0.0481\r\n#   2004 2005  -0.0764     0.0303       -0.1608       0.0079\r\n#   ...\r\n```\r\n\r\n**Interpretation**: Each row shows the ATT for a specific cohort (first.treat = Group) at a specific time. Groups are defined by when they first received treatment.\r\n\r\n### 2.3 Aggregating to Event Study\r\n\r\n```r\r\n# Aggregate to event-study (dynamic effects)\r\nes <- aggte(out, type = \"dynamic\", min_e = -5, max_e = 5)\r\nsummary(es)\r\n\r\n# Output:\r\n# Overall summary of ATT's based on event-study/dynamic aggregation:\r\n#      ATT    Std. Error     [ 95%  Conf. Int.]\r\n#  -0.0772        0.0215    -0.1194     -0.0350 *\r\n\r\n# Event study coefficients:\r\n#  Event Time   Estimate Std. Error [95% Simult.  Conf. Band]\r\n#          -5    0.0256     0.0168       -0.0198       0.0710\r\n#          -4   -0.0050     0.0146       -0.0445       0.0345\r\n#          ...\r\n\r\n# Plot event study\r\nggdid(es, title = \"Event Study: Effect on Teen Employment\")\r\n```\r\n\r\n### 2.4 DiD Diagnostic: Testing for Negative Weights\r\n\r\n```r\r\n# De Chaisemartin & D'Haultfoeuille negative weights test\r\n# Using TwoWayFEWeights package\r\n\r\n# Prepare data for twowayfeweights\r\ndata(base_did, package = \"fixest\")\r\nbase_did$D <- as.numeric(base_did$treat)\r\n\r\n# Run weights diagnostic\r\nch_diag <- twowayfeweights(\r\n  data = base_did,\r\n  Y = \"y\",\r\n  G = \"id\",\r\n  T = \"period\",\r\n  D = \"D\",\r\n  type = \"feTR\",\r\n  summary_measures = TRUE\r\n)\r\n\r\n# Results:\r\n# Under the common trends assumption, the TWFE coefficient identifies a weighted average\r\n# of the treatment effect in each (group,time).\r\n# The sum of negative weights is: X.XX\r\n# The sum of positive weights is: X.XX\r\n\r\n# If sum of negative weights is non-trivial (>0.1), consider using robust estimators\r\n```\r\n\r\n---\r\n\r\n## 3. Event Studies\r\n\r\nEvent studies estimate dynamic treatment effects relative to treatment timing.\r\n\r\n### When to Use Event Studies\r\n\r\n- You want to visualize treatment effects over time\r\n- You need to test the parallel trends assumption (pre-treatment coefficients)\r\n- Treatment effects may be dynamic (building or fading over time)\r\n\r\n### Assumptions\r\n\r\nSame as DiD, plus:\r\n- Effects at each event-time are well-estimated (sufficient data)\r\n- Appropriate binning of event-time endpoints\r\n\r\n### Common Pitfalls\r\n\r\n- Pre-trends in event study don't guarantee parallel trends (could be underpowered)\r\n- With staggered timing, use Sun & Abraham or Callaway-Sant'Anna (not OLS event study)\r\n- Omit exactly one period (typically t = -1) to avoid collinearity\r\n\r\n### 3.1 Event Study with Staggered Treatment\r\n\r\nUsing the `base_stagg` dataset from fixest:\r\n\r\n```r\r\n# Load staggered treatment data\r\ndata(base_stagg, package = \"fixest\")\r\n\r\n# Examine data structure\r\nhead(base_stagg)\r\n#   id year year_treated time_to_treatment treated treatment_effect_true x1        y\r\n# 1  1 2001         2004                -3   FALSE                     0  1.14 -0.816\r\n\r\n# The dataset already has time_to_treatment variable\r\n# 'treated' indicates treatment status\r\n\r\n# fixest's i() function for event study\r\nes_model <- feols(\r\n  y ~ x1 + i(time_to_treatment, treated, ref = -1) | id + year,\r\n  cluster = ~ id,\r\n  data = base_stagg\r\n)\r\n\r\n# View coefficients\r\nsummary(es_model)\r\n\r\n# Plot event study (fixest built-in)\r\niplot(es_model,\r\n      main = \"Event Study: Effect on Y\",\r\n      xlab = \"Years Relative to Treatment\",\r\n      ylab = \"Estimate\")\r\n```\r\n\r\n### 3.2 Sun & Abraham (2021) Estimator\r\n\r\nFor heterogeneous treatment effects with staggered timing:\r\n\r\n```r\r\n# Sun & Abraham via fixest's sunab() function\r\nes_sunab <- feols(\r\n  y ~ x1 + sunab(year_treated, year) | id + year,\r\n  cluster = ~ id,\r\n  data = base_stagg\r\n)\r\n\r\n# Compare with standard event study\r\netable(es_model, es_sunab,\r\n       headers = c(\"Standard\", \"Sun-Abraham\"))\r\n\r\n# Plot\r\niplot(es_sunab, main = \"Sun & Abraham Event Study\")\r\n```\r\n\r\n---\r\n\r\n## 4. Regression Discontinuity (RD)\r\n\r\nRD exploits discontinuous treatment assignment at a cutoff to identify causal effects.\r\n\r\n### When to Use RD\r\n\r\n- Treatment is determined by whether a continuous \"running variable\" crosses a threshold\r\n- Units cannot precisely manipulate the running variable\r\n- You're interested in the local average treatment effect (LATE) at the cutoff\r\n\r\n### Assumptions\r\n\r\n1. **No manipulation**: Units cannot precisely control their running variable near the cutoff\r\n2. **Continuity**: Potential outcomes are continuous at the cutoff\r\n3. **Local effect**: Estimates are valid only at the cutoff\r\n\r\n### Common Pitfalls\r\n\r\n- Bandwidth selection is critical - use data-driven methods (MSE-optimal)\r\n- Always test for manipulation with `rddensity`\r\n- RD estimates are local - don't extrapolate far from cutoff\r\n- Overfitting with high-order polynomials can inflate standard errors\r\n\r\n### 4.1 Sharp RD with rdrobust\r\n\r\nUsing the Senate election data (close elections):\r\n\r\n```r\r\n# Load RD Senate data\r\ndata(rdrobust_RDsenate, package = \"rdrobust\")\r\n\r\n# Running variable: Democratic vote share margin\r\n# Cutoff: 50% (threshold for winning)\r\n# Outcome: Future Democratic vote share\r\n\r\n# Examine data\r\nhead(rdrobust_RDsenate)\r\n#       margin       vote\r\n# 1 -21.316010  0.3566667\r\n# 2 -12.606600  0.4073077\r\n# ...\r\n\r\n# Basic RD estimation\r\nrd_est <- rdrobust(\r\n  y = rdrobust_RDsenate$vote,\r\n  x = rdrobust_RDsenate$margin,\r\n  c = 0,                    # Cutoff at 0 (normalized)\r\n  kernel = \"triangular\",    # Kernel for weighting\r\n  bwselect = \"mserd\"        # MSE-optimal bandwidth\r\n)\r\nsummary(rd_est)\r\n\r\n# Output:\r\n# =============================================================================\r\n#         Method     Coef. Std. Err.         z     P>|z|      [ 95% C.I. ]\r\n# =============================================================================\r\n#   Conventional     0.076     0.016     4.851     0.000     [0.045 , 0.107]\r\n#         Robust         -         -     4.185     0.000     [0.039 , 0.109]\r\n# =============================================================================\r\n#\r\n# Bandwidth: 17.69 (left), 17.69 (right)\r\n```\r\n\r\n**Interpretation**: Winning a close election (barely crossing 50%) increases future Democratic vote share by about 7.6 percentage points. This is the \"incumbency advantage.\"\r\n\r\n### 4.2 RD Visualization\r\n\r\n```r\r\n# RD plot\r\nrdplot(\r\n  y = rdrobust_RDsenate$vote,\r\n  x = rdrobust_RDsenate$margin,\r\n  c = 0,\r\n  title = \"RD Plot: Incumbency Advantage in Senate Elections\",\r\n  x.label = \"Democratic Vote Share Margin (%)\",\r\n  y.label = \"Future Democratic Vote Share\"\r\n)\r\n```\r\n\r\n### 4.3 Manipulation Testing\r\n\r\nAlways test whether units can manipulate the running variable:\r\n\r\n```r\r\n# McCrary density test via rddensity\r\nmanip_test <- rddensity(X = rdrobust_RDsenate$margin, c = 0)\r\nsummary(manip_test)\r\n\r\n# Output:\r\n# Manipulation Test using local polynomial density estimation.\r\n#\r\n# Number of obs (left): 595, Number of obs (right): 795\r\n#\r\n# Test Statistic: 0.4181\r\n# P-value: 0.6759\r\n#\r\n# Interpretation: p > 0.05 suggests no evidence of manipulation\r\n\r\n# Visualize density around cutoff\r\nrdplotdensity(rdd = manip_test, X = rdrobust_RDsenate$margin)\r\n```\r\n\r\n### 4.4 Bandwidth Sensitivity\r\n\r\nTest robustness to bandwidth choice:\r\n\r\n```r\r\n# Multiple bandwidths\r\nbandwidths <- c(10, 15, 20, 25, 30)\r\n\r\nrd_results <- lapply(bandwidths, function(h) {\r\n  rd <- rdrobust(\r\n    y = rdrobust_RDsenate$vote,\r\n    x = rdrobust_RDsenate$margin,\r\n    c = 0,\r\n    h = h  # Manually specified bandwidth\r\n  )\r\n  data.frame(\r\n    bandwidth = h,\r\n    estimate = rd$coef[1],\r\n    se = rd$se[1],\r\n    ci_low = rd$ci[1, 1],\r\n    ci_high = rd$ci[1, 2],\r\n    n_left = rd$N_h[1],\r\n    n_right = rd$N_h[2]\r\n  )\r\n})\r\n\r\nrd_robust <- bind_rows(rd_results)\r\nprint(rd_robust)\r\n\r\n# Plot sensitivity\r\nggplot(rd_robust, aes(x = bandwidth, y = estimate)) +\r\n  geom_point(size = 3) +\r\n  geom_errorbar(aes(ymin = ci_low, ymax = ci_high), width = 1) +\r\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\r\n  labs(x = \"Bandwidth\", y = \"RD Estimate\",\r\n       title = \"RD Estimate Sensitivity to Bandwidth\") +\r\n  theme_minimal()\r\n```\r\n\r\n### 4.5 Covariate Balance at Cutoff\r\n\r\nIf pre-treatment covariates jump at the cutoff, RD validity is questionable:\r\n\r\n```r\r\n# Test covariate balance (using margin as running variable)\r\n# For Senate data, we don't have covariates, but here's the pattern:\r\n\r\n# Pseudo-covariate test: does lagged outcome jump at cutoff?\r\n# (In real application, use actual pre-treatment covariates)\r\n\r\n# Simulate a covariate\r\nset.seed(123)\r\nrdrobust_RDsenate$covariate <- rdrobust_RDsenate$vote + rnorm(nrow(rdrobust_RDsenate), 0, 0.1)\r\n\r\n# Test if covariate is balanced\r\nrd_covariate <- rdrobust(\r\n  y = rdrobust_RDsenate$covariate,\r\n  x = rdrobust_RDsenate$margin,\r\n  c = 0\r\n)\r\nsummary(rd_covariate)\r\n# Ideally: coefficient close to 0, not statistically significant\r\n```\r\n\r\n---\r\n\r\n## 5. Instrumental Variables (IV)\r\n\r\nIV addresses endogeneity when treatment is correlated with unobserved factors.\r\n\r\n### When to Use IV\r\n\r\n- Your treatment variable is endogenous (correlated with unobservables)\r\n- You have an instrument that:\r\n  1. Affects treatment (relevance)\r\n  2. Only affects outcome through treatment (exclusion restriction)\r\n- You want a local average treatment effect (LATE) for compliers\r\n\r\n### Assumptions\r\n\r\n1. **Relevance**: Instrument strongly predicts treatment\r\n2. **Exclusion restriction**: Instrument affects outcome only through treatment\r\n3. **Independence**: Instrument is as-if randomly assigned\r\n4. **Monotonicity**: Instrument affects treatment in the same direction for all units\r\n\r\n### Common Pitfalls\r\n\r\n- Weak instruments (F < 10) cause severe bias - 2SLS is biased toward OLS\r\n- Cannot test the exclusion restriction - it's a maintained assumption\r\n- LATE interpretation: IV estimates effects only for \"compliers\"\r\n- Multiple instruments require overidentification tests\r\n\r\n### 5.1 Basic 2SLS\r\n\r\nUsing the returns to schooling data:\r\n\r\n```r\r\n# Load returns to schooling data\r\ndata(SchoolingReturns, package = \"ivreg\")\r\n\r\n# Examine data\r\nhead(SchoolingReturns)\r\n#       wage education experience ethnicity smsa south age nearcollege ...\r\n# 1  548.0000        12         18     other    1     0  36           0\r\n# ...\r\n# Key variables:\r\n# - wage: hourly wage (outcome)\r\n# - education: years of education (endogenous)\r\n# - nearcollege: grew up near college (instrument)\r\n\r\n# OLS (biased - education is endogenous)\r\nols_model <- lm(log(wage) ~ education + experience + I(experience^2) +\r\n                  ethnicity + smsa + south,\r\n                data = SchoolingReturns)\r\n\r\n# IV: Use proximity to college as instrument for education\r\niv_model <- ivreg(\r\n  log(wage) ~ education + experience + I(experience^2) +\r\n              ethnicity + smsa + south |\r\n              nearcollege + experience + I(experience^2) +\r\n              ethnicity + smsa + south,\r\n  data = SchoolingReturns\r\n)\r\n\r\nsummary(iv_model, diagnostics = TRUE)\r\n\r\n# Output includes:\r\n# Diagnostic tests:\r\n#                      df1  df2 statistic p-value\r\n# Weak instruments       1 3003    14.072 0.00018 ***  # First stage F\r\n# Wu-Hausman             1 3002     2.139 0.14373      # Endogeneity test\r\n\r\n# Compare OLS vs IV\r\ndata.frame(\r\n  Method = c(\"OLS\", \"IV\"),\r\n  Education_Coef = c(coef(ols_model)[\"education\"], coef(iv_model)[\"education\"]),\r\n  SE = c(sqrt(vcov(ols_model)[\"education\", \"education\"]),\r\n         sqrt(vcov(iv_model)[\"education\", \"education\"]))\r\n)\r\n#   Method Education_Coef    SE\r\n# 1    OLS          0.074 0.003\r\n# 2     IV          0.132 0.055\r\n```\r\n\r\n**Interpretation**: IV estimate (0.13) is larger than OLS (0.07), suggesting OLS understates returns to education. Each additional year of education increases wages by about 13% for compliers (those whose education was affected by college proximity).\r\n\r\n### 5.2 IV with Fixed Effects (fixest)\r\n\r\n```r\r\n# IV with fixed effects using fixest\r\n# Using trade data as example\r\n\r\ndata(trade, package = \"fixest\")\r\n\r\n# Suppose dist_km is endogenous, and we have an instrument\r\n# (Simulating for demonstration)\r\nset.seed(123)\r\ntrade$instrument <- log(trade$dist_km) + rnorm(nrow(trade), 0, 0.5)\r\n\r\n# IV estimation with FE\r\niv_fe <- feols(\r\n  log(Euros) ~ 1 | Origin + Destination + Year |\r\n    log(dist_km) ~ instrument,  # Endogenous ~ instrument\r\n  cluster = ~ Origin,\r\n  data = trade\r\n)\r\nsummary(iv_fe)\r\n```\r\n\r\n### 5.3 First-Stage Diagnostics\r\n\r\n```r\r\n# Check first stage strength\r\nfirst_stage <- lm(education ~ nearcollege + experience + I(experience^2) +\r\n                    ethnicity + smsa + south,\r\n                  data = SchoolingReturns)\r\nsummary(first_stage)\r\n\r\n# First-stage F-statistic on excluded instrument\r\ncar::linearHypothesis(first_stage, \"nearcollege = 0\")\r\n\r\n# Rule of thumb: F > 10 for strong instruments\r\n# More rigorous: Stock-Yogo critical values\r\n\r\n# Partial R-squared of instrument\r\npartial_rsq <- summary(first_stage)$r.squared -\r\n  summary(lm(education ~ experience + I(experience^2) + ethnicity + smsa + south,\r\n             data = SchoolingReturns))$r.squared\r\ncat(\"Partial R-squared:\", round(partial_rsq, 4), \"\\n\")\r\n```\r\n\r\n### 5.4 Multiple Instruments and Overidentification\r\n\r\n```r\r\n# Using cigarette demand data with multiple instruments\r\ndata(CigaretteDemand, package = \"ivreg\")\r\n\r\n# Price is endogenous, use tax instruments\r\niv_overid <- ivreg(\r\n  packs ~ rincome + rprice |          # Controls and endogenous\r\n    rincome + salestax + cigtax,      # Controls and instruments\r\n  data = CigaretteDemand\r\n)\r\n\r\nsummary(iv_overid, diagnostics = TRUE)\r\n\r\n# Sargan test for overidentification\r\n# H0: All instruments are valid\r\n# High p-value = cannot reject validity\r\n```\r\n\r\n---\r\n\r\n## 6. LASSO and Variable Selection\r\n\r\nLASSO is increasingly used for covariate selection and high-dimensional settings.\r\n\r\n### When to Use LASSO\r\n\r\n- You have many potential predictors (high-dimensional)\r\n- You want automatic variable selection\r\n- You're doing prediction rather than causal inference\r\n- For causal inference: use double-selection LASSO\r\n\r\n### Assumptions\r\n\r\n1. **Sparsity**: True model has relatively few important predictors\r\n2. **Regularization**: Willingness to shrink coefficients toward zero\r\n\r\n### Common Pitfalls\r\n\r\n- LASSO coefficients are biased toward zero - use post-LASSO OLS for inference\r\n- Cross-validation chooses lambda for prediction, not necessarily for consistent selection\r\n- For causal inference, use double-selection (not standard LASSO)\r\n\r\n### 6.1 Cross-Validated LASSO\r\n\r\n```r\r\n# Using glmnet's example data\r\ndata(QuickStartExample, package = \"glmnet\")\r\nx <- QuickStartExample$x\r\ny <- QuickStartExample$y\r\n\r\n# Examine dimensions\r\ncat(\"n =\", nrow(x), \", p =\", ncol(x), \"\\n\")\r\n# n = 100, p = 20\r\n\r\n# Cross-validated LASSO\r\nset.seed(123)\r\ncv_fit <- cv.glmnet(x, y, alpha = 1)  # alpha = 1 for LASSO\r\n\r\n# Plot CV results\r\nplot(cv_fit)\r\n\r\n# Optimal lambda values\r\ncat(\"Lambda.min:\", cv_fit$lambda.min, \"\\n\")\r\ncat(\"Lambda.1se:\", cv_fit$lambda.1se, \"\\n\")\r\n\r\n# Coefficients at optimal lambda\r\ncoef_lasso <- coef(cv_fit, s = \"lambda.min\")\r\nselected <- which(coef_lasso[-1, 1] != 0)\r\ncat(\"Selected variables:\", length(selected), \"of\", ncol(x), \"\\n\")\r\ncat(\"Selected indices:\", paste(selected, collapse = \", \"), \"\\n\")\r\n\r\n# Output:\r\n# Lambda.min: 0.08307327\r\n# Lambda.1se: 0.1752885\r\n# Selected variables: 8 of 20\r\n```\r\n\r\n### 6.2 Post-LASSO OLS\r\n\r\nAfter LASSO selects variables, re-estimate with OLS for valid inference:\r\n\r\n```r\r\n# Get selected variables\r\nselected_vars <- which(coef(cv_fit, s = \"lambda.min\")[-1, 1] != 0)\r\n\r\n# Post-LASSO OLS\r\nif (length(selected_vars) > 0) {\r\n  x_selected <- x[, selected_vars, drop = FALSE]\r\n  post_lasso <- lm(y ~ x_selected)\r\n  summary(post_lasso)\r\n}\r\n```\r\n\r\n### 6.3 Elastic Net\r\n\r\nCombine LASSO (L1) with Ridge (L2) penalties:\r\n\r\n```r\r\n# Elastic net with alpha = 0.5 (equal L1 and L2)\r\ncv_enet <- cv.glmnet(x, y, alpha = 0.5)\r\n\r\n# Compare with pure LASSO\r\ncat(\"LASSO selected:\", sum(coef(cv_fit, s = \"lambda.min\") != 0) - 1, \"\\n\")\r\ncat(\"Elastic net selected:\", sum(coef(cv_enet, s = \"lambda.min\") != 0) - 1, \"\\n\")\r\n```\r\n\r\n---\r\n\r\n## 7. Matching Methods\r\n\r\nMatching creates comparable treatment and control groups by balancing observed covariates.\r\n\r\n### When to Use Matching\r\n\r\n- You want to reduce bias from observed confounders\r\n- Selection into treatment depends on observable characteristics\r\n- You want interpretable weights (unlike regression adjustment)\r\n\r\n### Assumptions\r\n\r\n1. **Conditional independence**: No unobserved confounders (strong!)\r\n2. **Overlap**: For each treated unit, similar controls exist\r\n3. **SUTVA**: No interference between units\r\n\r\n### Common Pitfalls\r\n\r\n- Matching on propensity score doesn't guarantee covariate balance\r\n- Always check balance after matching\r\n- Matching reduces sample size - could hurt precision\r\n- Matching cannot address unobserved confounding\r\n\r\n### 7.1 Propensity Score Matching\r\n\r\nUsing the classic Lalonde labor training data:\r\n\r\n```r\r\n# Load Lalonde data (job training program)\r\ndata(lalonde, package = \"MatchIt\")\r\n\r\n# Examine data\r\nhead(lalonde)\r\n#   treat age educ  race married nodegree  re74  re75       re78\r\n# 1     1  37   11 black       1        1     0     0  9930.0460\r\n# 2     1  22    9 hispan      0        1     0     0  3595.8940\r\n# ...\r\n\r\n# Check initial imbalance\r\nlalonde %>%\r\n  group_by(treat) %>%\r\n  summarise(\r\n    mean_age = mean(age),\r\n    mean_educ = mean(educ),\r\n    prop_married = mean(married),\r\n    mean_re74 = mean(re74)\r\n  )\r\n\r\n# Propensity score matching\r\nm_out <- matchit(\r\n  treat ~ age + educ + race + married + nodegree + re74 + re75,\r\n  data = lalonde,\r\n  method = \"nearest\",      # Nearest neighbor matching\r\n  distance = \"glm\",        # Logistic propensity score\r\n  ratio = 1,               # 1:1 matching\r\n  caliper = 0.2            # Caliper in SD of propensity score\r\n)\r\n\r\n# Summary\r\nsummary(m_out)\r\n\r\n# Output:\r\n# Summary of Balance for Matched Data:\r\n#          Means Treated Means Control Std. Mean Diff.  Var. Ratio eCDF Mean\r\n# age              25.82        26.00           -0.02        1.02      0.01\r\n# educ             10.35        10.41           -0.03        1.02      0.01\r\n# ...\r\n#\r\n# Sample sizes:\r\n#           Control Treated\r\n# All           429     185\r\n# Matched       185     185\r\n# Unmatched     244       0\r\n```\r\n\r\n### 7.2 Balance Assessment\r\n\r\n```r\r\n# Balance plot (Love plot)\r\nplot(m_out, type = \"jitter\")\r\n\r\n# Standardized mean differences\r\nplot(summary(m_out), var.order = \"unmatched\")\r\n\r\n# Using cobalt for detailed balance\r\nlibrary(cobalt)\r\nbal.tab(m_out, un = TRUE)\r\n\r\n# Love plot with threshold\r\nlove.plot(m_out,\r\n          thresholds = c(m = 0.1),  # 0.1 SMD threshold\r\n          var.order = \"unadjusted\",\r\n          title = \"Covariate Balance\")\r\n```\r\n\r\n### 7.3 Estimate Treatment Effect\r\n\r\n```r\r\n# Get matched data\r\nmatched_df <- match.data(m_out)\r\n\r\n# Estimate ATT with matched data\r\n# Use weights for full matching\r\natt_model <- lm(re78 ~ treat, data = matched_df, weights = weights)\r\nsummary(att_model)\r\n\r\n# With covariate adjustment (doubly robust)\r\natt_robust <- lm(re78 ~ treat + age + educ + race + married + nodegree + re74 + re75,\r\n                 data = matched_df, weights = weights)\r\n\r\n# Compare\r\ndata.frame(\r\n  Model = c(\"Unadjusted\", \"Covariate Adjusted\"),\r\n  ATT = c(coef(att_model)[\"treat\"], coef(att_robust)[\"treat\"]),\r\n  SE = c(sqrt(vcov(att_model)[\"treat\", \"treat\"]),\r\n         sqrt(vcov(att_robust)[\"treat\", \"treat\"]))\r\n)\r\n```\r\n\r\n### 7.4 Coarsened Exact Matching (CEM)\r\n\r\n```r\r\nlibrary(cem)\r\n\r\n# Coarsened exact matching\r\n# CEM bins continuous variables and finds exact matches within bins\r\ncem_out <- cem(\r\n  treatment = \"treat\",\r\n  data = lalonde,\r\n  drop = \"re78\"  # Don't match on outcome\r\n)\r\n\r\n# Summary\r\ncem_out\r\n\r\n# Get weights\r\nlalonde$cem_weights <- cem_out$w\r\n\r\n# Estimate with weights\r\ncem_model <- lm(re78 ~ treat, data = lalonde, weights = cem_weights)\r\nsummary(cem_model)\r\n```\r\n\r\n### 7.5 Entropy Balancing\r\n\r\nEntropy balancing reweights controls to match treatment group moments:\r\n\r\n```r\r\nlibrary(ebal)\r\n\r\n# Prepare data\r\nX <- as.matrix(lalonde[, c(\"age\", \"educ\", \"married\", \"nodegree\", \"re74\", \"re75\")])\r\nW <- lalonde$treat\r\n\r\n# Entropy balancing (for control group)\r\n# Note: ebalance only reweights control group\r\neb_out <- ebalance(\r\n  Treatment = W,\r\n  X = X\r\n)\r\n\r\n# Create full weights vector\r\nlalonde$eb_weights <- ifelse(W == 1, 1, eb_out$w)\r\n\r\n# Check balance\r\n# Treated means\r\ncolMeans(X[W == 1, ])\r\n# Weighted control means\r\ncolSums(X[W == 0, ] * eb_out$w) / sum(eb_out$w)\r\n\r\n# Estimate ATT\r\neb_model <- lm(re78 ~ treat, data = lalonde, weights = eb_weights)\r\nsummary(eb_model)\r\n```\r\n\r\n### 7.6 Genetic Matching\r\n\r\n```r\r\nlibrary(Matching)\r\nlibrary(rgenoud)\r\n\r\n# Prepare data\r\nX <- as.matrix(lalonde[, c(\"age\", \"educ\", \"married\", \"nodegree\", \"re74\", \"re75\")])\r\nY <- lalonde$re78\r\nW <- lalonde$treat\r\n\r\n# Genetic matching (finds optimal weights)\r\n# Note: This can be slow for large datasets\r\nset.seed(123)\r\ngen_weights <- GenMatch(\r\n  Tr = W,\r\n  X = X,\r\n  BalanceMatrix = X,\r\n  estimand = \"ATT\",\r\n  M = 1,\r\n  print.level = 0,\r\n  pop.size = 50  # Smaller for speed\r\n)\r\n\r\n# Match using optimized weights\r\nmatch_out <- Match(\r\n  Y = Y,\r\n  Tr = W,\r\n  X = X,\r\n  Weight.matrix = gen_weights,\r\n  estimand = \"ATT\",\r\n  M = 1\r\n)\r\n\r\nsummary(match_out)\r\n\r\n# Output:\r\n# Estimate...  XXXX\r\n# AI SE......  XXX\r\n# T-stat.....  X.XX\r\n# p.val......  0.XXX\r\n```\r\n\r\n### 7.7 Panel Matching (PanelMatch)\r\n\r\nFor time-series cross-sectional data:\r\n\r\n```r\r\nlibrary(PanelMatch)\r\n\r\n# Load democracy data\r\ndata(dem, package = \"PanelMatch\")\r\n\r\n# Examine data\r\nhead(dem)\r\n#   wbcode2 year   dem tradewb         y\r\n# 1     AFG 1984 FALSE    5.32 -9.997001\r\n# 2     AFG 1985 FALSE    5.42 -9.997001\r\n\r\n# Create matched sets\r\nPM_results <- PanelMatch(\r\n  lag = 4,                      # 4 pre-treatment periods\r\n  time.id = \"year\",\r\n  unit.id = \"wbcode2\",\r\n  treatment = \"dem\",\r\n  outcome.var = \"y\",\r\n  refinement.method = \"mahalanobis\",\r\n  covs.formula = ~ tradewb,\r\n  size.match = 5,               # Match to 5 controls\r\n  data = dem,\r\n  qoi = \"att\",\r\n  lead = 0:4                    # Effects 0-4 periods after\r\n)\r\n\r\n# Estimate effects\r\nPE_results <- PanelEstimate(\r\n  sets = PM_results,\r\n  data = dem,\r\n  number.iterations = 1000,\r\n  confidence.level = 0.95\r\n)\r\n\r\n# Summary\r\nsummary(PE_results)\r\n\r\n# Plot\r\nplot(PE_results)\r\n```\r\n\r\n---\r\n\r\n## 8. Standard Errors and Inference\r\n\r\n### 8.1 Clustered Standard Errors\r\n\r\nWhen observations within clusters are correlated:\r\n\r\n```r\r\n# Using trade data with country-level clustering\r\ndata(trade, package = \"fixest\")\r\n\r\n# One-way clustering\r\nmodel_1way <- feols(\r\n  log(Euros) ~ log(dist_km) | Origin + Destination + Year,\r\n  cluster = ~ Origin,\r\n  data = trade\r\n)\r\n\r\n# Two-way clustering\r\nmodel_2way <- feols(\r\n  log(Euros) ~ log(dist_km) | Origin + Destination + Year,\r\n  cluster = ~ Origin + Destination,\r\n  data = trade\r\n)\r\n\r\n# Compare\r\netable(model_1way, model_2way,\r\n       headers = c(\"One-way\", \"Two-way\"),\r\n       se.below = TRUE)\r\n```\r\n\r\n### 8.2 Conley Spatial Standard Errors\r\n\r\nFor spatially correlated data:\r\n\r\n```r\r\n# Conley SEs with distance cutoff\r\n# fixest has built-in support\r\n\r\n# First, add coordinates (simulated for trade data)\r\nset.seed(123)\r\ntrade$lon <- runif(nrow(trade), -180, 180)\r\ntrade$lat <- runif(nrow(trade), -90, 90)\r\n\r\n# Conley SEs (requires coordinates)\r\nmodel_conley <- feols(\r\n  log(Euros) ~ log(dist_km) | Origin + Destination + Year,\r\n  vcov = conley(cutoff = 500, lon = \"lon\", lat = \"lat\"),\r\n  data = trade\r\n)\r\n\r\nsummary(model_conley)\r\n```\r\n\r\n### 8.3 Driscoll-Kraay Standard Errors\r\n\r\nFor panels with cross-sectional dependence:\r\n\r\n```r\r\n# Driscoll-Kraay SEs\r\nmodel_dk <- feols(\r\n  log(Euros) ~ log(dist_km) | Origin + Destination + Year,\r\n  vcov = \"DK\",\r\n  panel.id = ~ Origin + Year,\r\n  data = trade\r\n)\r\n\r\nsummary(model_dk)\r\n```\r\n\r\n### 8.4 Heteroskedasticity-Robust Comparisons\r\n\r\n```r\r\n# Compare different SE types\r\nmodel <- feols(log(Euros) ~ log(dist_km) | Origin + Year, data = trade)\r\n\r\n# Different variance-covariance matrices\r\nsummary(model, vcov = \"iid\")           # Homoskedastic\r\nsummary(model, vcov = \"hetero\")        # HC1\r\nsummary(model, vcov = ~ Origin)        # Clustered\r\nsummary(model, vcov = ~ Origin + Year) # Two-way\r\n\r\n# Extract all for comparison\r\nse_comparison <- data.frame(\r\n  Type = c(\"IID\", \"Hetero\", \"Cluster Origin\", \"Two-way\"),\r\n  SE = c(\r\n    se(summary(model, vcov = \"iid\")),\r\n    se(summary(model, vcov = \"hetero\")),\r\n    se(summary(model, vcov = ~ Origin)),\r\n    se(summary(model, vcov = ~ Origin + Year))\r\n  )\r\n)\r\nprint(se_comparison)\r\n```\r\n\r\n---\r\n\r\n## 9. Diagnostic Tests\r\n\r\n### 9.1 Parallel Trends Test\r\n\r\n```r\r\n# Using base_stagg from fixest\r\ndata(base_stagg, package = \"fixest\")\r\n\r\n# base_stagg already has time_to_treatment variable\r\n# 'treated' is the treatment indicator\r\n\r\nes_model <- feols(\r\n  y ~ i(time_to_treatment, treated, ref = -1) | id + year,\r\n  cluster = ~ id,\r\n  data = base_stagg\r\n)\r\n\r\n# Joint test of pre-treatment coefficients\r\n# Extract pre-period coefficients\r\npre_coefs <- grep(\"time_to_treatment::-\", names(coef(es_model)), value = TRUE)\r\n\r\n# Wald test for joint significance\r\nwald(es_model, keep = pre_coefs)\r\n\r\n# Interpretation: Large p-value = cannot reject parallel trends\r\n```\r\n\r\n### 9.2 Placebo Tests\r\n\r\n```r\r\n# Placebo: Shift treatment timing\r\ndata(base_stagg, package = \"fixest\")\r\n\r\nplacebo_results <- lapply(c(-3, -2, -1, 0, 1, 2, 3), function(shift) {\r\n  base_stagg_placebo <- base_stagg %>%\r\n    mutate(\r\n      fake_treat_year = year_treated + shift,\r\n      fake_treated = year >= fake_treat_year & !is.na(fake_treat_year)\r\n    )\r\n\r\n  model <- feols(\r\n    y ~ fake_treated | id + year,\r\n    cluster = ~ id,\r\n    data = base_stagg_placebo\r\n  )\r\n\r\n  data.frame(\r\n    shift = shift,\r\n    coef = coef(model)[\"fake_treatedTRUE\"],\r\n    se = se(model)[\"fake_treatedTRUE\"]\r\n  )\r\n})\r\n\r\nplacebo_df <- bind_rows(placebo_results)\r\n\r\n# Plot\r\nggplot(placebo_df, aes(x = shift, y = coef)) +\r\n  geom_point(size = 3) +\r\n  geom_errorbar(aes(ymin = coef - 1.96*se, ymax = coef + 1.96*se), width = 0.2) +\r\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\r\n  geom_vline(xintercept = 0, linetype = \"dotted\", color = \"red\") +\r\n  labs(x = \"Treatment Timing Shift\", y = \"Estimated Effect\",\r\n       title = \"Placebo Test: Shifting Treatment Timing\") +\r\n  theme_minimal()\r\n```\r\n\r\n### 9.3 Sensitivity Analysis (sensemakr)\r\n\r\nHow much unobserved confounding would be needed to explain away the result?\r\n\r\n```r\r\nlibrary(sensemakr)\r\n\r\n# Load Darfur survey data\r\ndata(darfur, package = \"sensemakr\")\r\n\r\n# The peacefactor variable is stored as a matrix - extract as vector\r\ndarfur$peace <- as.numeric(darfur$peacefactor)\r\n\r\n# Basic regression\r\nmodel <- lm(peace ~ directlyharmed + female + age + farmer_dar +\r\n              herder_dar + pastvoted + hhsize_darfur,\r\n            data = darfur)\r\n\r\n# Sensitivity analysis\r\nsens <- sensemakr(\r\n  model = model,\r\n  treatment = \"directlyharmed\",\r\n  benchmark_covariates = \"female\",  # Use as benchmark\r\n  kd = 1:3                          # Multiples of benchmark\r\n)\r\n\r\n# Summary\r\nsummary(sens)\r\n\r\n# Output:\r\n# Sensitivity Analysis to Unobserved Confounding\r\n#\r\n# Unadjusted Estimates:\r\n#   Outcome: peace\r\n#   Treatment: directlyharmed\r\n#   Estimate: 0.097\r\n#   Standard Error: 0.023\r\n#   t-value: 4.184\r\n#\r\n# Minimal Strength of Unobserved Confounding:\r\n# -- to bring estimate to 0:\r\n#   Partial R2 of the confounder with treatment: 0.025\r\n#   Partial R2 of the confounder with outcome: 0.025\r\n\r\n# Plot sensitivity\r\nplot(sens)\r\n# Shows contour plot of bias as function of confounder strength\r\n```\r\n\r\n**Interpretation**: The sensitivity analysis shows how strong an unobserved confounder would need to be (in terms of partial R with treatment and outcome) to explain away the effect.\r\n\r\n---\r\n\r\n## 10. Causal Mediation Analysis\r\n\r\nCausal mediation decomposes total treatment effects into direct and indirect (mediated) components.\r\n\r\n### When to Use Mediation Analysis\r\n\r\n- You have a hypothesized mechanism (mediator) between treatment and outcome\r\n- You want to understand \"why\" a treatment works\r\n- You're willing to make strong assumptions about the mediator\r\n\r\n### Assumptions\r\n\r\n1. **No unmeasured treatment-outcome confounding** (as in any causal analysis)\r\n2. **No unmeasured mediator-outcome confounding** (very strong!)\r\n3. **No treatment-induced confounding**: Treatment doesn't affect confounders of mediator-outcome\r\n\r\n### Common Pitfalls\r\n\r\n- Sequential ignorability is untestable and often implausible\r\n- Post-treatment confounders can severely bias results\r\n- Interaction between treatment and mediator complicates interpretation\r\n\r\n### 10.1 Basic Mediation with mediation package\r\n\r\nUsing the framing experiment data:\r\n\r\n```r\r\nlibrary(mediation)\r\n\r\n# Load framing experiment data\r\ndata(framing, package = \"mediation\")\r\n\r\n# Examine data\r\nhead(framing)\r\n#   cond anx age educ gender income ... treat ... immigr\r\n#\r\n# treat = treatment (news framing, binary)\r\n# anx = mediator (anxiety level, stored as factor)\r\n# immigr = outcome (immigration attitude)\r\n\r\n# Convert factor to numeric index for analysis (1-7 scale)\r\nframing$anx_num <- as.numeric(framing$anx)\r\n\r\n# Step 1: Mediator model (anxiety as function of treatment)\r\nmodel_m <- lm(anx_num ~ treat + age + educ, data = framing)\r\n\r\n# Step 2: Outcome model (includes mediator)\r\nmodel_y <- lm(immigr ~ treat + anx_num + age + educ, data = framing)\r\n\r\n# Step 3: Mediation analysis\r\nmed_out <- mediate(\r\n  model.m = model_m,\r\n  model.y = model_y,\r\n  treat = \"treat\",\r\n  mediator = \"anx_num\",\r\n  sims = 1000,\r\n  boot = TRUE,\r\n  boot.ci.type = \"bca\"\r\n)\r\n\r\nsummary(med_out)\r\n\r\n# Output:\r\n# Causal Mediation Analysis\r\n#\r\n# Quasi-Bayesian Confidence Intervals\r\n#\r\n#                Estimate 95% CI Lower 95% CI Upper p-value\r\n# ACME             0.0877       0.0330         0.15  <2e-16 ***\r\n# ADE              0.0124      -0.0967         0.12    0.84\r\n# Total Effect     0.1001       0.0055         0.20    0.04 *\r\n# Prop. Mediated   0.8762       0.2185         3.40    0.04 *\r\n```\r\n\r\n**Interpretation**:\r\n- **ACME (Average Causal Mediation Effect)**: The indirect effect through anxiety = 0.088\r\n- **ADE (Average Direct Effect)**: The direct effect not through anxiety = 0.012\r\n- **Total Effect**: ACME + ADE = 0.10\r\n- **Proportion Mediated**: 88% of the effect operates through anxiety\r\n\r\n### 10.2 Sensitivity Analysis for Mediation\r\n\r\n```r\r\n# Sensitivity analysis: How robust is the mediation to confounding?\r\nsens_med <- medsens(med_out, rho.by = 0.05)\r\nsummary(sens_med)\r\n\r\n# Plot sensitivity\r\nplot(sens_med)\r\n# Shows how ACME changes as correlation between mediator and outcome errors varies\r\n```\r\n\r\n### 10.3 Moderated Mediation\r\n\r\n```r\r\n# Interaction model (treatment effect on mediator varies by education)\r\nmodel_m_int <- lm(anx_num ~ treat * educ + age, data = framing)\r\nmodel_y_int <- lm(immigr ~ treat * anx_num + educ + age, data = framing)\r\n\r\n# Mediation at different education levels\r\nmed_low <- mediate(model_m_int, model_y_int, treat = \"treat\", mediator = \"anx_num\",\r\n                   covariates = list(educ = 1), sims = 500)\r\nmed_high <- mediate(model_m_int, model_y_int, treat = \"treat\", mediator = \"anx_num\",\r\n                    covariates = list(educ = 4), sims = 500)\r\n\r\n# Compare\r\ndata.frame(\r\n  Education = c(\"Low\", \"High\"),\r\n  ACME = c(med_low$d0, med_high$d0),\r\n  ADE = c(med_low$z0, med_high$z0)\r\n)\r\n```\r\n\r\n---\r\n\r\n## 11. Marginal Effects (Interpretation)\r\n\r\nIn non-linear models (Logit, Probit, Poisson), coefficients () are not marginal effects. You cannot interpret them as \"a 1 unit increase in X leads to a  increase in Y.\"\r\n\r\n### When to Use\r\n\r\n- You are running `feglm()` (Poisson/Logit/NegBin)\r\n- You have interaction terms in linear models (interpreting  alone is insufficient)\r\n- You need policy-relevant estimates (e.g., \"probability increases by 5%\")\r\n\r\n### Assumptions\r\n\r\n- The model is correctly specified\r\n- For average marginal effects: effects are reasonably constant across the distribution\r\n\r\n### Common Pitfalls\r\n\r\n- Reporting raw coefficients from non-linear models as effects\r\n- Ignoring that marginal effects vary across observations in non-linear models\r\n- Not specifying meaningful values for `newdata` when effects depend on other covariates\r\n\r\n### Implementation with marginaleffects\r\n\r\nThe `marginaleffects` package is the modern replacement for the older `margins` package. It works seamlessly with `fixest`.\r\n\r\n```r\r\n# Install\r\ninstall.packages(\"marginaleffects\")\r\nlibrary(marginaleffects)\r\nlibrary(fixest)\r\n\r\n# Load trade data\r\ndata(trade, package = \"fixest\")\r\n\r\n# Estimate Poisson Fixed Effects (Gravity Model)\r\n# Outcome: Euros (count-like/positive), dist_km is continuous\r\nmodel_pois <- feglm(\r\n  Euros ~ log(dist_km) + Year | Origin + Destination,\r\n  family = \"poisson\",\r\n  data = trade\r\n)\r\n\r\n# 1. Average Marginal Effects (AME)\r\n# \"On average, what is the effect of a 1 unit change in X on Y?\"\r\navg_slopes(model_pois)\r\n\r\n# Output:\r\n#     Term    Contrast Estimate Std. Error      z Pr(>|z|)\r\n#  Year    mean(dY/dX)  1785401   12940362  0.138    0.890\r\n#  dist_km mean(dY/dX)  -135467     962919 -0.141    0.888\r\n\r\n# 2. Marginal Effects at specific values (e.g., for specific years)\r\nslopes(model_pois, newdata = datagrid(Year = 2007:2009))\r\n\r\n# 3. Predictions (Marginal Means)\r\n# \"What is the predicted trade volume for each year, holding other at means?\"\r\npredictions(model_pois, newdata = datagrid(Year = unique))\r\n\r\n# 4. Comparisons (discrete changes)\r\n# Effect of moving from 2007 to 2008\r\ncomparisons(model_pois, variables = list(Year = c(2007, 2008)))\r\n```\r\n\r\n**Interpretation**: The AME shows the average effect of a 1-unit change in each variable on the outcome (Euros), accounting for the non-linearity of the Poisson model.\r\n\r\n### Marginal Effects with Interactions\r\n\r\n```r\r\n# Linear model with interaction\r\ndata(lalonde, package = \"MatchIt\")\r\nmodel_int <- lm(re78 ~ treat * age + educ, data = lalonde)\r\n\r\n# Marginal effect of treatment varies with age\r\nslopes(model_int, variables = \"treat\", newdata = datagrid(age = c(20, 30, 40)))\r\n\r\n# Plot how treatment effect varies with age\r\nplot_slopes(model_int, variables = \"treat\", condition = \"age\")\r\n```\r\n\r\n---\r\n\r\n## 12. Wild Cluster Bootstrap (Small Clusters)\r\n\r\nStandard clustered standard errors (sandwich estimator) are biased when the number of clusters is small (roughly G < 30-40).\r\n\r\n### When to Use\r\n\r\n- You are clustering by State/Region and have fewer than ~40 groups\r\n- Your standard clustered SEs might be under-rejecting (Type I error)\r\n- You want more reliable inference with few clusters\r\n\r\n### Assumptions\r\n\r\n- Clusters are independent\r\n- Treatment/covariates vary within clusters\r\n- Standard bootstrap assumptions\r\n\r\n### Common Pitfalls\r\n\r\n- Using standard cluster SEs with very few clusters (leads to over-rejection)\r\n- Not setting a seed for reproducibility\r\n- Using too few bootstrap iterations (use at least 999, preferably 9999)\r\n\r\n### Implementation with fwildclusterboot\r\n\r\n**Note**: `fwildclusterboot` requires R  4.0. If unavailable, consider `boottest` from Stata or the `sandwich` package with small-sample corrections.\r\n\r\n```r\r\n# Install\r\ninstall.packages(\"fwildclusterboot\")\r\nlibrary(fwildclusterboot)\r\nlibrary(fixest)\r\n\r\n# Load data\r\ndata(base_did, package = \"fixest\")\r\n\r\n# Run standard FE model\r\nmodel_fe <- feols(y ~ x1 + treat | id + period, data = base_did)\r\n\r\n# Run Wild Cluster Bootstrap\r\n# B = number of bootstrap iterations\r\nboot_res <- boottest(\r\n  model_fe,\r\n  clustid = \"id\",      # Cluster variable\r\n  param = \"x1\",        # Variable of interest\r\n  B = 9999,            # Bootstrap iterations\r\n  seed = 12345         # For reproducibility\r\n)\r\n\r\n# Summary\r\nsummary(boot_res)\r\n\r\n# Output:\r\n#  Estimate  Std. Error  t value  Pr(>|t|)  CI Lower  CI Upper\r\n#    0.998      0.116     8.58    0.000      0.770     1.227\r\n\r\n# Plot bootstrap distribution\r\nplot(boot_res)\r\n\r\n# The bootstrap p-value and confidence interval are robust to\r\n# the small number of clusters\r\n```\r\n\r\n**Interpretation**: Compare the bootstrap p-value and CI to the standard ones. If they differ substantially, trust the bootstrap results.\r\n\r\n### Alternative: sandwich with small-sample correction\r\n\r\n```r\r\nlibrary(fixest)\r\n\r\n# Small-sample cluster correction (G/(G-1) adjustment)\r\nmodel <- feols(y ~ x1 + treat | id + period,\r\n               cluster = ~ id,\r\n               ssc = ssc(adj = TRUE, cluster.adj = TRUE),  # Small sample correction\r\n               data = base_did)\r\nsummary(model)\r\n```\r\n\r\n---\r\n\r\n## 13. Honest DiD (Parallel Trends Sensitivity)\r\n\r\nStandard DiD assumes the parallel trends assumption holds exactly. \"Honest DiD\" (Rambachan & Roth, 2023) allows you to calculate confidence intervals that are robust to some violation of parallel trends.\r\n\r\n### When to Use\r\n\r\n- You have an event study\r\n- Your pre-trends are \"mostly\" flat but not perfect\r\n- Reviewer asks: \"What if parallel trends is slightly violated?\"\r\n- You want to report sensitivity of results to assumption violations\r\n\r\n### Assumptions\r\n\r\n- You're willing to bound the maximum violation of parallel trends\r\n- The violation is smooth (not a sudden jump)\r\n\r\n### Common Pitfalls\r\n\r\n- Ignoring pre-trends that suggest violations\r\n- Setting M (maximum violation) too loosely or too tightly\r\n- Not understanding that wider CIs don't mean the effect is zerothey mean uncertainty increases\r\n\r\n### Implementation with HonestDiD\r\n\r\n**Note**: `HonestDiD` requires installation from GitHub and has several dependencies. Installation may require a recent R version.\r\n\r\n```r\r\n# Install from GitHub (requires remotes package)\r\n# install.packages(\"remotes\")\r\n# remotes::install_github(\"asheshrambachan/HonestDiD\")\r\nlibrary(HonestDiD)\r\nlibrary(fixest)\r\n\r\n# 1. Run Event Study\r\ndata(base_stagg, package = \"fixest\")\r\n\r\nes_model <- feols(\r\n  y ~ i(time_to_treatment, treated, ref = -1) | id + year,\r\n  cluster = ~id,\r\n  data = base_stagg\r\n)\r\n\r\n# 2. Extract event study coefficients\r\n#    Need to identify which coefficients are pre-treatment vs post-treatment\r\nbetahat <- coef(es_model)\r\nsigma <- vcov(es_model)\r\n\r\n# Get coefficient names\r\ncoef_names <- names(betahat)\r\npre_periods <- grep(\"::-[0-9]\", coef_names)   # Negative event times\r\npost_periods <- grep(\"::[0-9]\", coef_names)   # Positive event times (including 0)\r\n\r\n# 3. Run Honest DiD Sensitivity Analysis\r\n# Focus on the first post-treatment period (event time 0)\r\n# M = maximum allowed violation of parallel trends (slope change)\r\n\r\n# Using relative magnitudes approach:\r\n# \"How large could the violation be relative to the max pre-trend?\"\r\ndelta_rm <- HonestDiD::createSensitivityResults_relativeMagnitudes(\r\n  betahat = betahat,\r\n  sigma = sigma,\r\n  numPrePeriods = length(pre_periods),\r\n  numPostPeriods = length(post_periods),\r\n  Mbarvec = seq(0, 2, by = 0.5)  # Range of M values (0 = exact PT)\r\n)\r\n\r\n# 4. View results\r\n# Shows how CI changes as you allow larger violations\r\nprint(delta_rm)\r\n\r\n# 5. Plot Sensitivity\r\n# Shows original CI vs. robust CI at different M values\r\nHonestDiD::createSensitivityPlot_relativeMagnitudes(\r\n  robustResults = delta_rm,\r\n  originalResults = HonestDiD::constructOriginalCS(betahat, sigma,\r\n                                                    numPrePeriods = length(pre_periods),\r\n                                                    numPostPeriods = length(post_periods))\r\n)\r\n```\r\n\r\n**Interpretation**:\r\n- **M = 0**: Assumes parallel trends hold exactly (standard DiD)\r\n- **M = 1**: Allows violations up to the magnitude of the largest pre-trend\r\n- **M = 2**: Allows violations up to 2x the largest pre-trend\r\n- If the CI includes zero at small M, your result is sensitive to PT violations\r\n\r\n### Simplified Approach with fixest\r\n\r\nFor a quick pre-trends test without the full HonestDiD framework:\r\n\r\n```r\r\n# Run event study\r\nes_model <- feols(\r\n  y ~ i(time_to_treatment, treated, ref = -1) | id + year,\r\n  cluster = ~id,\r\n  data = base_stagg\r\n)\r\n\r\n# Plot with confidence bands\r\niplot(es_model,\r\n      main = \"Event Study with Pre-trends\",\r\n      xlab = \"Time to Treatment\")\r\n\r\n# Joint test of pre-treatment coefficients = 0\r\npre_coefs <- grep(\"::-\", names(coef(es_model)), value = TRUE)\r\nwald(es_model, keep = pre_coefs)\r\n\r\n# If joint test rejects, pre-trends may be violated\r\n# Consider HonestDiD for sensitivity analysis\r\n```\r\n\r\n---\r\n\r\n## Quick Reference\r\n\r\n### Package Selection by Task\r\n\r\n| Task | Package | Key Function |\r\n|------|---------|--------------|\r\n| FE regression | `fixest` | `feols(y ~ x \\| fe1 + fe2)` |\r\n| Modern DiD | `did` | `att_gt()` |\r\n| Event study | `fixest` | `feols(y ~ sunab(g, t))` |\r\n| RD | `rdrobust` | `rdrobust(y, x, c = 0)` |\r\n| Manipulation test | `rddensity` | `rddensity(X, c = 0)` |\r\n| IV | `ivreg`, `fixest` | `ivreg()`, `feols(... \\| endog ~ iv)` |\r\n| LASSO | `glmnet` | `cv.glmnet(X, y)` |\r\n| Matching | `MatchIt` | `matchit(treat ~ x)` |\r\n| Entropy balance | `ebal` | `ebalance(Tr, X)` |\r\n| Panel matching | `PanelMatch` | `PanelMatch()` |\r\n| Balance | `cobalt` | `bal.tab()`, `love.plot()` |\r\n| Mediation | `mediation` | `mediate(model.m, model.y)` |\r\n| Sensitivity | `sensemakr` | `sensemakr(model, treat)` |\r\n| Spatial SE | `fixest` | `vcov = conley(r)` |\r\n| Marginal effects | `marginaleffects` | `avg_slopes()`, `predictions()` |\r\n| Wild bootstrap | `fwildclusterboot` | `boottest()` |\r\n| Honest DiD | `HonestDiD` | `createSensitivityResults_relativeMagnitudes()` |\r\n\r\n### Key Datasets for Practice\r\n\r\n| Dataset | Package | Use For |\r\n|---------|---------|---------|\r\n| `base_did` | fixest | DiD, TWFE |\r\n| `base_stagg` | fixest | Event studies, staggered DiD |\r\n| `trade` | fixest | FE regression, clustering |\r\n| `mpdta` | did | Callaway-Sant'Anna |\r\n| `rdrobust_RDsenate` | rdrobust | RD design |\r\n| `lalonde` | MatchIt | Matching methods |\r\n| `SchoolingReturns` | ivreg | IV estimation |\r\n| `darfur` | sensemakr | Sensitivity analysis |\r\n| `framing` | mediation | Mediation analysis |\r\n| `dem` | PanelMatch | Panel matching |\r\n\r\n### Common Diagnostic Checks\r\n\r\n1. **TWFE with staggered timing**: Check for negative weights\r\n2. **DiD**: Test parallel trends with event study\r\n3. **RD**: Test for manipulation, bandwidth sensitivity\r\n4. **IV**: First-stage F-statistic, overidentification test\r\n5. **Matching**: Balance statistics, Love plot\r\n6. **All**: Sensitivity analysis for unobserved confounding\r\n",
        "plugins/r-analyst/skills/r-analyst/techniques/02_survey_resampling.md": "# Survey Methods & Resampling\r\n\r\nSurvey weights, randomization inference, multiple testing, decomposition, and bootstrap methods in R.\r\n\r\n---\r\n\r\n## Quick Reference\r\n\r\n| Method | Package | Function | When to Use |\r\n|--------|---------|----------|-------------|\r\n| Survey regression | survey | `svyglm()` | Complex survey designs with stratification/clustering |\r\n| Weighted mean | survey | `svymean()` | Population means from survey data |\r\n| Simple weights | Hmisc | `wtd.mean()` | Quick weighted statistics without survey design |\r\n| Randomization inference | ri2 | `conduct_ri()` | Small samples, clustered experiments, sharp nulls |\r\n| FDR correction | base | `p.adjust()` | Multiple hypothesis testing |\r\n| Equivalence testing | TOSTER | `t_TOST()` | Testing if effect is practically zero |\r\n| Decomposition | oaxaca | `oaxaca()` | Explaining group differences in outcomes |\r\n| Bootstrap | boot | `boot()` | Non-parametric inference, complex statistics |\r\n| List experiments | list | `ictreg()` | Sensitive survey questions |\r\n\r\n---\r\n\r\n## 1. Survey Weights\r\n\r\nSurvey data requires proper weighting to produce population-representative estimates. The `survey` package handles complex designs with stratification, clustering, and finite population corrections.\r\n\r\n### 1.1 When to Use Survey Weights\r\n\r\n**Use survey methods when:**\r\n- Data comes from stratified or clustered sampling\r\n- Sampling probabilities vary across units\r\n- You need population-representative estimates\r\n- Standard errors must account for design effects\r\n\r\n**Use simple weights when:**\r\n- Weights represent frequency or reliability (not sampling)\r\n- No complex survey design features\r\n- Only point estimates needed (not inference)\r\n\r\n### 1.2 Survey Design with the API Data\r\n\r\nThe `api` dataset contains California school Academic Performance Index data with complex survey design.\r\n\r\n```r\r\nlibrary(survey)\r\n\r\n# Load California Academic Performance Index data\r\n# This is a population of 6194 schools with various samples\r\ndata(api)\r\n\r\n# apipop: Full population (6194 schools)\r\n# apisrs:  Simple random sample (200 schools)\r\n# apistrat: Stratified sample by school type (200 schools)\r\n# apiclus1: One-stage cluster sample by district (183 schools)\r\n# apiclus2: Two-stage cluster sample (126 schools)\r\n\r\n# Examine stratified sample\r\nhead(apistrat[, c(\"stype\", \"pw\", \"api00\", \"api99\", \"enroll\")])\r\n#>   stype       pw api00 api99 enroll\r\n#> 1     E 44.21053   693   600    247\r\n#> 2     E 44.21053   570   501    219\r\n#> 3     E 44.21053   546   472    138\r\n#> 4     E 44.21053   571   502    584\r\n#> 5     E 44.21053   478   386    413\r\n#> 6     E 44.21053   858   831    264\r\n\r\n# stype: E = Elementary, M = Middle, H = High\r\n# pw: sampling weight (inverse probability of selection)\r\n```\r\n\r\n### 1.3 Defining Survey Designs\r\n\r\n```r\r\n# Stratified random sample design\r\n# Stratified by school type (E, M, H)\r\ndstrat <- svydesign(\r\n  id = ~1,           # No clustering (each school sampled independently)\r\n  strata = ~stype,   # Stratification variable\r\n  weights = ~pw,     # Sampling weight\r\n  data = apistrat,\r\n  fpc = ~fpc         # Finite population correction (pop size per stratum)\r\n)\r\n\r\n# Two-stage cluster sample design\r\n# Stage 1: Sample districts, Stage 2: Sample schools within districts\r\ndclus2 <- svydesign(\r\n  id = ~dnum + snum,     # Cluster IDs: district, then school\r\n  weights = ~pw,         # Sampling weight\r\n  data = apiclus2,\r\n  fpc = ~fpc1 + fpc2     # FPC at each stage\r\n)\r\n\r\n# Summary of design\r\nsummary(dstrat)\r\n#> Stratified Independent Sampling design (with replacement)\r\n#> svydesign(id = ~1, strata = ~stype, weights = ~pw, data = apistrat,\r\n#>     fpc = ~fpc)\r\n#> Probabilities:\r\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.\r\n#> 0.01841 0.02262 0.02262 0.02396 0.02262 0.05281\r\n#> Stratum Sizes:\r\n#>             E  H  M\r\n#> obs       100 50 50\r\n#> design.PSU 100 50 50\r\n#> actual.PSU 100 50 50\r\n```\r\n\r\n### 1.4 Survey-Weighted Regression\r\n\r\n```r\r\n# Survey-weighted regression: API 2000 on enrollment and school type\r\nsvy_model <- svyglm(\r\n  api00 ~ enroll + stype + ell,\r\n  design = dstrat\r\n)\r\n\r\nsummary(svy_model)\r\n#> Call:\r\n#> svyglm(formula = api00 ~ enroll + stype + ell, design = dstrat)\r\n#>\r\n#> Survey design:\r\n#> svydesign(id = ~1, strata = ~stype, weights = ~pw, data = apistrat,\r\n#>     fpc = ~fpc)\r\n#>\r\n#> Coefficients:\r\n#>              Estimate Std. Error t value Pr(>|t|)\r\n#> (Intercept) 830.11832   17.72117  46.846  < 2e-16 ***\r\n#> enroll       -0.01174    0.02151  -0.546   0.5859\r\n#> stypeH      -29.20679   18.79657  -1.554   0.1219\r\n#> stypeM      -18.54936   16.18085  -1.146   0.2530\r\n#> ell          -3.87765    0.41679  -9.304  < 2e-16 ***\r\n\r\n# Compare clustered design\r\nsvy_clus <- svyglm(api00 ~ enroll + stype + ell, design = dclus2)\r\nsummary(svy_clus)\r\n```\r\n\r\n### 1.5 Survey Summary Statistics\r\n\r\n```r\r\n# Weighted mean of API scores\r\nsvymean(~api00, dstrat)\r\n#>         mean     SE\r\n#> api00 662.29 9.4089\r\n\r\n# Compare to unweighted (biased if design is informative)\r\nmean(apistrat$api00)\r\n#> [1] 652.59\r\n\r\n# Population total\r\nsvytotal(~enroll, dstrat)\r\n#>          total     SE\r\n#> enroll 3687178 114642\r\n\r\n# Weighted means by group\r\nsvyby(~api00, ~stype, dstrat, svymean)\r\n#>   stype    api00       se\r\n#> E     E 674.4310 9.499067\r\n#> H     H 625.8200 14.28596\r\n#> M     M 636.6000 14.76811\r\n\r\n# Weighted quantiles\r\nsvyquantile(~api00, dstrat, quantiles = c(0.25, 0.5, 0.75))\r\n#> $api00\r\n#>       quantile ci.2.5 ci.97.5       se\r\n#> 0.25       544    502     575 18.65015\r\n#> 0.5        652    626     690 16.34185\r\n#> 0.75       773    744     805 15.59082\r\n```\r\n\r\n### 1.6 Interactions in Weighted Regressions\r\n\r\nThe `survey` package handles interactions correctly in the variance calculation:\r\n\r\n```r\r\n# Model with interaction\r\nsvy_int <- svyglm(api00 ~ enroll * stype, design = dstrat)\r\n\r\n# Coefficients and standard errors are correct\r\nsummary(svy_int)\r\n\r\n# CAUTION: Don't use lm() with weights for complex surveys\r\n# This gives point estimates but WRONG standard errors:\r\nlm_wrong <- lm(api00 ~ enroll * stype, data = apistrat, weights = pw)\r\n\r\n# Compare standard errors (lm underestimates uncertainty):\r\nsqrt(diag(vcov(svy_int)))\r\n#> (Intercept)      enroll      stypeH      stypeM enroll:stypeH enroll:stypeM\r\n#>  21.1398382   0.0310932  44.2991395  56.1296543   0.0521316   0.0714682\r\n\r\nsqrt(diag(vcov(lm_wrong)))  # Typically smaller (wrong!)\r\n#> (Intercept)      enroll      stypeH      stypeM enroll:stypeH enroll:stypeM\r\n#>  15.6827425   0.0230641  32.8512063  41.6224614   0.0386598   0.0530160\r\n```\r\n\r\n**Key principle:** Always use `svyglm()` for complex survey data. Manual weight application with `lm(..., weights = )` doesn't account for stratification and clustering in standard error calculation.\r\n\r\n### 1.7 Simple Weighted Statistics (without survey package)\r\n\r\nFor quick calculations when survey design features are not needed:\r\n\r\n```r\r\nlibrary(Hmisc)\r\n\r\n# Weighted mean and variance\r\nwtd.mean(apistrat$api00, weights = apistrat$pw)\r\n#> [1] 662.2874\r\n\r\nwtd.var(apistrat$api00, weights = apistrat$pw)\r\n#> [1] 17127.31\r\n\r\n# Weighted correlation\r\nwtd.cor(apistrat$api00, apistrat$api99, weight = apistrat$pw)\r\n#>       [,1]\r\n#> [1,] 0.9744\r\n```\r\n\r\n---\r\n\r\n## 2. Bunching Estimators\r\n\r\nBunching analysis estimates behavioral responses at policy thresholds (kinks/notches). At a tax kink or benefit notch, individuals \"bunch\" at the threshold. The excess mass relative to a counterfactual density identifies the behavioral elasticity.\r\n\r\n### 2.1 When to Use Bunching\r\n\r\n**Appropriate when:**\r\n- Policy creates discontinuous incentives at a threshold\r\n- Individuals can adjust behavior in response\r\n- Running variable is continuous and manipulable\r\n\r\n**Common applications:**\r\n- Tax kinks (marginal rate changes)\r\n- Benefit notches (eligibility thresholds)\r\n- Regulatory thresholds (firm size, income limits)\r\n\r\n### 2.2 Implementation in R\r\n\r\n```r\r\n# Simulate income data with bunching at $50,000 threshold\r\nset.seed(42)\r\nn <- 10000\r\n\r\n# Counterfactual income (no bunching)\r\nincome_cf <- rnorm(n, mean = 50000, sd = 15000)\r\n\r\n# Create bunching: some individuals shift to threshold\r\nthreshold <- 50000\r\nbunching_zone <- abs(income_cf - threshold) < 5000\r\nshift_prob <- 0.3  # 30% of those near threshold bunch\r\n\r\nbunchers <- bunching_zone & (runif(n) < shift_prob)\r\nincome <- income_cf\r\nincome[bunchers] <- threshold\r\n\r\n# Create histogram data\r\nbin_width <- 1000\r\nbins <- seq(0, 100000, by = bin_width)\r\nhist_data <- data.frame(\r\n  bin_center = bins[-length(bins)] + bin_width/2,\r\n  count = as.numeric(table(cut(income, bins)))\r\n)\r\n\r\n# Define exclusion window around threshold\r\nexclude_width <- 5  # bins on each side\r\nhist_data$exclude <- abs(hist_data$bin_center - threshold) < exclude_width * bin_width\r\n\r\n# Fit polynomial on non-excluded region\r\npoly_degree <- 5\r\nfit_data <- hist_data[!hist_data$exclude, ]\r\npoly_fit <- lm(count ~ poly(bin_center, poly_degree, raw = TRUE), data = fit_data)\r\n\r\n# Predict counterfactual\r\nhist_data$counterfactual <- predict(poly_fit, newdata = hist_data)\r\n\r\n# Calculate bunching statistics\r\nobserved <- sum(hist_data$count[hist_data$exclude])\r\ncounterfactual <- sum(hist_data$counterfactual[hist_data$exclude])\r\nbunching_mass <- observed - counterfactual\r\nb_stat <- bunching_mass / counterfactual\r\n\r\ncat(\"Bunching Statistics:\\n\")\r\ncat(\"Observed count in exclusion zone:\", observed, \"\\n\")\r\ncat(\"Counterfactual count:\", round(counterfactual, 1), \"\\n\")\r\ncat(\"Excess mass:\", round(bunching_mass, 1), \"\\n\")\r\ncat(\"Bunching statistic (b):\", round(b_stat, 3), \"\\n\")\r\n#> Bunching Statistics:\r\n#> Observed count in exclusion zone: 3512\r\n#> Counterfactual count: 3089.2\r\n#> Excess mass: 422.8\r\n#> Bunching statistic (b): 0.137\r\n```\r\n\r\n### 2.3 Visualization\r\n\r\n```r\r\nlibrary(ggplot2)\r\n\r\nggplot(hist_data, aes(x = bin_center)) +\r\n  geom_col(aes(y = count, fill = exclude), alpha = 0.7) +\r\n  geom_line(aes(y = counterfactual), color = \"red\", linewidth = 1) +\r\n  geom_vline(xintercept = threshold, linetype = \"dashed\") +\r\n  scale_fill_manual(values = c(\"gray50\", \"steelblue\"),\r\n                    labels = c(\"Fitted region\", \"Exclusion zone\")) +\r\n  labs(title = \"Bunching at Policy Threshold\",\r\n       x = \"Income ($)\", y = \"Count\",\r\n       fill = \"\") +\r\n  theme_minimal()\r\n```\r\n\r\n### 2.4 Bootstrap Standard Errors\r\n\r\n```r\r\nlibrary(boot)\r\n\r\n# Bootstrap function for bunching statistic\r\nbunching_stat <- function(data, indices) {\r\n  d <- data[indices, ]\r\n\r\n  # Create histogram\r\n  bins <- seq(0, 100000, by = 1000)\r\n  hist_data <- data.frame(\r\n    bin_center = bins[-length(bins)] + 500,\r\n    count = as.numeric(table(cut(d$income, bins)))\r\n  )\r\n  hist_data$exclude <- abs(hist_data$bin_center - 50000) < 5000\r\n\r\n  # Fit polynomial\r\n  fit_data <- hist_data[!hist_data$exclude, ]\r\n  poly_fit <- lm(count ~ poly(bin_center, 5, raw = TRUE), data = fit_data)\r\n  hist_data$cf <- predict(poly_fit, newdata = hist_data)\r\n\r\n  # Bunching statistic\r\n  obs <- sum(hist_data$count[hist_data$exclude])\r\n  cf <- sum(hist_data$cf[hist_data$exclude])\r\n  return((obs - cf) / cf)\r\n}\r\n\r\n# Run bootstrap\r\nincome_df <- data.frame(income = income)\r\nboot_results <- boot(income_df, bunching_stat, R = 500)\r\n\r\n# 95% confidence interval\r\nboot.ci(boot_results, type = \"perc\")\r\n#> BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\r\n#> Based on 500 bootstrap replicates\r\n#>\r\n#> Intervals :\r\n#> Level     Percentile\r\n#> 95%   ( 0.092,  0.182 )\r\n```\r\n\r\n---\r\n\r\n## 3. Randomization Inference\r\n\r\nRandomization inference (RI) tests sharp null hypotheses using permutation distributions. It provides exact p-values without relying on asymptotic approximations.\r\n\r\n### 3.1 When to Use Randomization Inference\r\n\r\n**Appropriate when:**\r\n- Small samples where asymptotic inference is unreliable\r\n- Testing sharp null hypothesis (effect = 0 for all units)\r\n- Clustered experiments with few clusters\r\n- Balance tests in RCTs\r\n- Verification of experimental results\r\n\r\n**Key distinction:**\r\n- Standard inference: Tests whether average effect differs from 0\r\n- RI: Tests whether any unit has non-zero effect (sharp null)\r\n\r\n### 3.2 Manual Implementation\r\n\r\n```r\r\n# Simulate small experiment\r\nset.seed(123)\r\nn <- 30\r\ntreatment <- sample(c(0, 1), n, replace = TRUE)\r\n# True effect = 2\r\noutcome <- 5 + 2 * treatment + rnorm(n)\r\n\r\n# Observed treatment effect\r\nobs_diff <- mean(outcome[treatment == 1]) - mean(outcome[treatment == 0])\r\ncat(\"Observed difference:\", round(obs_diff, 3), \"\\n\")\r\n#> Observed difference: 2.245\r\n\r\n# Permutation test\r\nn_perms <- 5000\r\nperm_diffs <- numeric(n_perms)\r\n\r\nfor (i in 1:n_perms) {\r\n  perm_treat <- sample(treatment)  # Shuffle treatment assignment\r\n  perm_diffs[i] <- mean(outcome[perm_treat == 1]) - mean(outcome[perm_treat == 0])\r\n}\r\n\r\n# Two-sided p-value\r\np_ri <- mean(abs(perm_diffs) >= abs(obs_diff))\r\ncat(\"RI p-value (two-sided):\", p_ri, \"\\n\")\r\n#> RI p-value (two-sided): 0.0052\r\n\r\n# Compare to t-test\r\nt_test <- t.test(outcome ~ treatment)\r\ncat(\"t-test p-value:\", round(t_test$p.value, 4), \"\\n\")\r\n#> t-test p-value: 0.0048\r\n\r\n# Visualization\r\nhist(perm_diffs, breaks = 50, main = \"Permutation Distribution\",\r\n     xlab = \"Difference in Means\", col = \"lightgray\")\r\nabline(v = obs_diff, col = \"red\", lwd = 2)\r\nabline(v = -obs_diff, col = \"red\", lwd = 2, lty = 2)\r\n```\r\n\r\n### 3.3 Using the ri2 Package\r\n\r\n```r\r\nlibrary(ri2)\r\n\r\n# Create data frame\r\nexp_data <- data.frame(\r\n  Y = outcome,\r\n  Z = treatment,\r\n  cluster = rep(1:10, each = 3)  # 10 clusters of 3\r\n)\r\n\r\n# Declare randomization procedure\r\n# This tells ri2 how treatment was assigned\r\ndeclaration <- declare_ra(\r\n  N = nrow(exp_data),\r\n  prob = 0.5  # 50% probability of treatment\r\n)\r\n\r\n# Conduct randomization inference\r\nri_result <- conduct_ri(\r\n  Y ~ Z,\r\n  declaration = declaration,\r\n  data = exp_data,\r\n  sims = 1000\r\n)\r\n\r\nsummary(ri_result)\r\n#>   term estimate two_tailed_p_value\r\n#> 1    Z    2.245              0.008\r\n\r\n# With clustering\r\ndeclaration_clus <- declare_ra(\r\n  clusters = exp_data$cluster,\r\n  prob = 0.5\r\n)\r\n\r\nri_clus <- conduct_ri(\r\n  Y ~ Z,\r\n  declaration = declaration_clus,\r\n  data = exp_data,\r\n  sims = 1000\r\n)\r\nsummary(ri_clus)\r\n```\r\n\r\n### 3.4 Balance Testing with RI\r\n\r\n```r\r\n# Test balance on pre-treatment covariates\r\nset.seed(456)\r\nn <- 100\r\ntreatment <- sample(c(0, 1), n, replace = TRUE)\r\nage <- rnorm(n, 35, 10)\r\nincome <- rnorm(n, 50000, 15000)\r\neducation <- sample(1:5, n, replace = TRUE)\r\n\r\nbalance_data <- data.frame(treatment, age, income, education)\r\n\r\n# RI for each covariate\r\ncovariates <- c(\"age\", \"income\", \"education\")\r\nbalance_results <- lapply(covariates, function(var) {\r\n  obs_diff <- mean(balance_data[[var]][treatment == 1]) -\r\n              mean(balance_data[[var]][treatment == 0])\r\n\r\n  perm_diffs <- replicate(1000, {\r\n    perm_treat <- sample(treatment)\r\n    mean(balance_data[[var]][perm_treat == 1]) -\r\n      mean(balance_data[[var]][perm_treat == 0])\r\n  })\r\n\r\n  p_val <- mean(abs(perm_diffs) >= abs(obs_diff))\r\n  data.frame(variable = var, diff = obs_diff, p_ri = p_val)\r\n})\r\n\r\ndo.call(rbind, balance_results)\r\n#>    variable         diff  p_ri\r\n#> 1       age  0.428929342 0.857\r\n#> 2    income -3212.827234 0.426\r\n#> 3 education -0.038461538 0.853\r\n```\r\n\r\n---\r\n\r\n## 4. Multiple Hypothesis Testing\r\n\r\nWhen testing many hypotheses, control for false discovery rate (FDR) or family-wise error rate (FWER).\r\n\r\n### 4.1 When to Use Multiple Testing Corrections\r\n\r\n**FWER control (Bonferroni, Holm):**\r\n- When any false positive is costly\r\n- Confirmatory studies with few pre-specified tests\r\n- Regulatory/clinical trial settings\r\n\r\n**FDR control (Benjamini-Hochberg):**\r\n- Exploratory analysis with many tests\r\n- When some false positives are acceptable\r\n- Gene expression, survey outcomes\r\n\r\n### 4.2 Using p.adjust()\r\n\r\n```r\r\n# Simulate multiple outcome tests\r\nset.seed(789)\r\nn <- 200\r\ntreatment <- sample(c(0, 1), n, replace = TRUE)\r\n\r\n# 20 outcomes: first 5 have true effects, rest are null\r\noutcomes <- matrix(rnorm(n * 20), ncol = 20)\r\noutcomes[treatment == 1, 1:5] <- outcomes[treatment == 1, 1:5] + 0.4\r\n\r\n# Collect p-values\r\np_values <- sapply(1:20, function(j) {\r\n  model <- lm(outcomes[, j] ~ treatment)\r\n  coef(summary(model))[\"treatment\", \"Pr(>|t|)\"]\r\n})\r\n\r\nnames(p_values) <- paste0(\"outcome_\", 1:20)\r\n\r\n# Apply corrections\r\nresults <- data.frame(\r\n  outcome = names(p_values),\r\n  p_raw = p_values,\r\n  p_bonferroni = p.adjust(p_values, method = \"bonferroni\"),\r\n  p_holm = p.adjust(p_values, method = \"holm\"),\r\n  p_fdr = p.adjust(p_values, method = \"BH\"),\r\n  true_effect = c(rep(TRUE, 5), rep(FALSE, 15))\r\n)\r\n\r\n# Show results for first 10 outcomes\r\nresults[1:10, ]\r\n#>        outcome      p_raw p_bonferroni    p_holm     p_fdr true_effect\r\n#> 1   outcome_1 0.01892     0.3784       0.3028    0.09462        TRUE\r\n#> 2   outcome_2 0.04523     0.9046       0.4977    0.12886        TRUE\r\n#> 3   outcome_3 0.08271     1.0000       0.7444    0.16542        TRUE\r\n#> 4   outcome_4 0.00312     0.0624       0.0624    0.03120        TRUE\r\n#> 5   outcome_5 0.02156     0.4312       0.3016    0.09462        TRUE\r\n#> 6   outcome_6 0.78234     1.0000       1.0000    0.89410       FALSE\r\n#> 7   outcome_7 0.45621     1.0000       1.0000    0.65173       FALSE\r\n#> 8   outcome_8 0.89412     1.0000       1.0000    0.93989       FALSE\r\n#> 9   outcome_9 0.62341     1.0000       1.0000    0.77926       FALSE\r\n#> 10 outcome_10 0.31256     1.0000       1.0000    0.52093       FALSE\r\n\r\n# Summary: How many significant at 0.05 level?\r\ncat(\"\\nSignificant findings at alpha = 0.05:\\n\")\r\ncat(\"Raw p-values:\", sum(results$p_raw < 0.05), \"\\n\")\r\ncat(\"Bonferroni:\", sum(results$p_bonferroni < 0.05), \"\\n\")\r\ncat(\"Holm:\", sum(results$p_holm < 0.05), \"\\n\")\r\ncat(\"FDR (BH):\", sum(results$p_fdr < 0.05), \"\\n\")\r\n#> Significant findings at alpha = 0.05:\r\n#> Raw p-values: 5\r\n#> Bonferroni: 1\r\n#> Holm: 1\r\n#> FDR (BH): 2\r\n```\r\n\r\n### 4.3 Choosing the Right Correction\r\n\r\n| Method | Controls | Stringency | Use When |\r\n|--------|----------|------------|----------|\r\n| Bonferroni | FWER | Most conservative | Any false positive unacceptable |\r\n| Holm | FWER | Less conservative | Sequential testing needed |\r\n| Hochberg | FWER | Less conservative | Tests are independent |\r\n| BH | FDR | Moderate | Many tests, some FPs okay |\r\n| BY | FDR | Conservative FDR | Tests may be correlated |\r\n\r\n```r\r\n# Compare all methods\r\nall_methods <- c(\"bonferroni\", \"holm\", \"hochberg\", \"BH\", \"BY\")\r\ncomparison <- sapply(all_methods, function(m) {\r\n  sum(p.adjust(p_values, method = m) < 0.05)\r\n})\r\n\r\ndata.frame(method = all_methods, n_significant = comparison)\r\n#>       method n_significant\r\n#> 1 bonferroni             1\r\n#> 2       holm             1\r\n#> 3   hochberg             1\r\n#> 4         BH             2\r\n#> 5         BY             1\r\n```\r\n\r\n---\r\n\r\n## 5. Equivalence Testing\r\n\r\nEquivalence testing (Two One-Sided Tests, TOST) establishes that an effect is practically zero or within acceptable bounds. Unlike null hypothesis testing, you're testing whether an effect is *small enough* to be negligible.\r\n\r\n### 5.1 When to Use Equivalence Testing\r\n\r\n**Use equivalence testing when:**\r\n- Demonstrating no meaningful effect (not just non-significance)\r\n- Non-inferiority trials\r\n- Validating that groups are similar\r\n- Placebo tests in causal inference\r\n\r\n**The key insight:** A non-significant result does NOT prove equivalence. It may just reflect low power.\r\n\r\n### 5.2 TOST with the TOSTER Package\r\n\r\n```r\r\nlibrary(TOSTER)\r\n\r\n# Example: Testing if a treatment has a negligible effect\r\nset.seed(101)\r\nn <- 50\r\ncontrol <- rnorm(n, mean = 100, sd = 15)\r\ntreatment <- rnorm(n, mean = 101, sd = 15)  # True diff = 1 (small)\r\n\r\n# Standard t-test (null hypothesis testing)\r\nt.test(treatment, control)\r\n#> \tWelch Two Sample t-test\r\n#>\r\n#> data:  treatment and control\r\n#> t = 0.6823, df = 97.6, p-value = 0.4966\r\n#> alternative hypothesis: true difference in means is not equal to 0\r\n#> 95 percent confidence interval:\r\n#>  -2.946  6.012\r\n#> sample estimates:\r\n#> mean of x mean of y\r\n#>  101.4821  99.9497\r\n\r\n# Equivalence test: Is effect within +/- 5 points?\r\n# This is a meaningful threshold for practical equivalence\r\ntost_result <- t_TOST(\r\n  x = treatment,\r\n  y = control,\r\n  eqb = 5,  # Equivalence bound (+/- 5)\r\n  paired = FALSE\r\n)\r\n\r\nprint(tost_result)\r\n#> Welch Two Sample t-test\r\n#>\r\n#> The equivalence test was non-significant, t(98) = -0.5, p = 0.31\r\n#> The null hypothesis test was non-significant, t(98) = 1.285, p = 0.2\r\n#> NHST: don't reject null significance hypothesis that the effect is equal to zero\r\n#> TOST: don't reject null equivalence hypothesis\r\n#>\r\n#> TOST Results\r\n#>                 t df p.value\r\n#> t-test      1.285 98   0.202\r\n#> TOST Lower  3.068 98   0.001\r\n#> TOST Upper -0.498 98   0.310\r\n```\r\n\r\n### 5.3 Manual TOST Implementation\r\n\r\n```r\r\n# Manual implementation for understanding\r\nmanual_tost <- function(x, y, delta, alpha = 0.05) {\r\n  # Get difference and SE\r\n  diff <- mean(x) - mean(y)\r\n  se <- sqrt(var(x)/length(x) + var(y)/length(y))\r\n  df <- length(x) + length(y) - 2\r\n\r\n  # Two one-sided tests\r\n  t_lower <- (diff - (-delta)) / se  # Test vs lower bound\r\n  t_upper <- (delta - diff) / se      # Test vs upper bound\r\n\r\n  p_lower <- pt(t_lower, df, lower.tail = FALSE)\r\n  p_upper <- pt(t_upper, df, lower.tail = FALSE)\r\n\r\n  # TOST p-value is the maximum\r\n  p_tost <- max(p_lower, p_upper)\r\n\r\n  # 90% CI for TOST (corresponds to two 5% one-sided tests)\r\n  ci_90 <- c(diff - qt(0.95, df) * se, diff + qt(0.95, df) * se)\r\n\r\n  list(\r\n    difference = diff,\r\n    se = se,\r\n    lower_bound = -delta,\r\n    upper_bound = delta,\r\n    ci_90 = ci_90,\r\n    p_tost = p_tost,\r\n    equivalent = p_tost < alpha\r\n  )\r\n}\r\n\r\nresult <- manual_tost(treatment, control, delta = 5)\r\ncat(\"Difference:\", round(result$difference, 2), \"\\n\")\r\ncat(\"90% CI:\", round(result$ci_90, 2), \"\\n\")\r\ncat(\"Equivalence bounds: [\", -5, \",\", 5, \"]\\n\")\r\ncat(\"TOST p-value:\", round(result$p_tost, 4), \"\\n\")\r\ncat(\"Equivalent:\", result$equivalent, \"\\n\")\r\n#> Difference: 1.53\r\n#> 90% CI: -2.23 5.29\r\n#> Equivalence bounds: [ -5 , 5 ]\r\n#> TOST p-value: 0.0527\r\n#> Equivalent: FALSE\r\n```\r\n\r\n### 5.4 Choosing the Equivalence Margin\r\n\r\nThe equivalence margin (delta) should be chosen based on practical significance:\r\n\r\n```r\r\n# Common approaches to setting delta\r\ndata <- mtcars\r\n\r\n# 1. Fraction of outcome SD (e.g., 0.2 SD = \"small\" effect)\r\ndelta_sd <- 0.2 * sd(data$mpg)\r\ncat(\"Delta based on 0.2 SD:\", round(delta_sd, 2), \"mpg\\n\")\r\n#> Delta based on 0.2 SD: 1.21 mpg\r\n\r\n# 2. Percentage of mean (e.g., 10% of baseline)\r\ndelta_pct <- 0.1 * mean(data$mpg)\r\ncat(\"Delta based on 10% of mean:\", round(delta_pct, 2), \"mpg\\n\")\r\n#> Delta based on 10% of mean: 2.01 mpg\r\n\r\n# 3. Smallest effect size of interest (SESOI)\r\n# Based on domain knowledge - what difference would be practically meaningful?\r\n# For MPG: Perhaps 3 mpg is the smallest meaningful difference\r\ndelta_sesoi <- 3\r\ncat(\"SESOI-based delta:\", delta_sesoi, \"mpg\\n\")\r\n#> SESOI-based delta: 3 mpg\r\n```\r\n\r\n### 5.5 Combining NHST and Equivalence Results\r\n\r\n```r\r\n# Four possible conclusions:\r\n# 1. Significant and not equivalent: Effect exists and is meaningful\r\n# 2. Not significant and equivalent: Effect is negligible\r\n# 3. Significant and equivalent: Effect exists but is trivially small\r\n# 4. Not significant and not equivalent: Inconclusive (low power)\r\n\r\ninterpret_results <- function(p_nhst, p_tost, alpha = 0.05) {\r\n  sig <- p_nhst < alpha\r\n  equiv <- p_tost < alpha\r\n\r\n  if (sig && !equiv) return(\"Effect detected (meaningful)\")\r\n  if (!sig && equiv) return(\"Equivalent to zero\")\r\n  if (sig && equiv) return(\"Statistically significant but trivially small\")\r\n  if (!sig && !equiv) return(\"Inconclusive (need more data)\")\r\n}\r\n\r\n# Example across multiple comparisons\r\ncomparisons <- data.frame(\r\n  comparison = c(\"Drug A vs Placebo\", \"Drug B vs Placebo\",\r\n                 \"Drug A vs Drug B\", \"Drug C vs Placebo\"),\r\n  p_nhst = c(0.001, 0.45, 0.03, 0.15),\r\n  p_tost = c(0.32, 0.01, 0.02, 0.42)\r\n)\r\n\r\ncomparisons$conclusion <- mapply(interpret_results,\r\n                                  comparisons$p_nhst,\r\n                                  comparisons$p_tost)\r\ncomparisons\r\n#>           comparison p_nhst p_tost                             conclusion\r\n#> 1  Drug A vs Placebo  0.001   0.32              Effect detected (meaningful)\r\n#> 2  Drug B vs Placebo  0.450   0.01                       Equivalent to zero\r\n#> 3   Drug A vs Drug B  0.030   0.02 Statistically significant but trivially small\r\n#> 4  Drug C vs Placebo  0.150   0.42            Inconclusive (need more data)\r\n```\r\n\r\n---\r\n\r\n## 6. Decomposition Methods\r\n\r\n### 6.1 Oaxaca-Blinder Decomposition\r\n\r\nThe Oaxaca-Blinder decomposition explains group differences (e.g., gender wage gap) by separating the contribution of:\r\n- **Explained (endowments):** Differences in characteristics (e.g., education, experience)\r\n- **Unexplained (coefficients):** Differences in returns to those characteristics\r\n\r\n### 6.2 When to Use Decomposition\r\n\r\n**Appropriate for:**\r\n- Wage gap analysis\r\n- Understanding outcome disparities between groups\r\n- Policy analysis (what would change if one group had the other's characteristics?)\r\n\r\n**Limitations:**\r\n- Assumes linear relationship\r\n- \"Unexplained\" portion includes discrimination AND unmeasured factors\r\n- Results depend on reference group choice\r\n\r\n### 6.3 Implementation with oaxaca Package\r\n\r\n```r\r\nlibrary(oaxaca)\r\n\r\n# Chicago wage data: Log wages by gender\r\ndata(chicago)\r\n\r\n# Examine the data\r\nstr(chicago)\r\n#> 'data.frame':\t712 obs. of  9 variables:\r\n#>  $ age            : int  52 46 31 27 20 32 25 37 49 36 ...\r\n#>  $ female         : int  0 1 1 0 0 1 1 1 0 1 ...\r\n#>  $ foreign.born   : int  1 1 1 0 0 0 1 1 0 0 ...\r\n#>  $ LTHS           : int  0 0 0 0 0 0 0 0 0 0 ...\r\n#>  $ high.school    : int  1 1 1 0 0 1 0 0 0 0 ...\r\n#>  $ some.college   : int  0 0 0 1 0 0 1 0 0 1 ...\r\n#>  $ college        : int  0 0 0 0 1 0 0 0 0 0 ...\r\n#>  $ advanced.degree: int  0 0 0 0 0 0 0 1 1 0 ...\r\n#>  $ ln.real.wage   : num  2.14 NA 2.5 3.6 2.3 ...\r\n\r\n# Remove missing wages\r\nchicago_clean <- chicago[!is.na(chicago$ln.real.wage), ]\r\n\r\n# Oaxaca-Blinder decomposition\r\n# Formula: outcome ~ predictors | group_variable\r\ndecomp <- oaxaca(\r\n  formula = ln.real.wage ~ age + LTHS + high.school + some.college +\r\n            college + advanced.degree + foreign.born | female,\r\n  data = chicago_clean,\r\n  R = 100  # Bootstrap replications\r\n)\r\n\r\n# View results\r\nsummary(decomp)\r\n#> Oaxaca-Blinder Decomposition\r\n#> --\r\n#> Groups: female\r\n#>   Group A: female = 0 (reference)\r\n#>   Group B: female = 1\r\n#>\r\n#> Sample sizes:\r\n#>   Group A: 255\r\n#>   Group B: 381\r\n#>\r\n#> Mean outcomes:\r\n#>   Group A: 2.996\r\n#>   Group B: 2.639\r\n#>   Difference: 0.357\r\n#>\r\n#> Threefold decomposition:\r\n#>                Coefficient Std. Error\r\n#> Endowments          0.0587     0.0312\r\n#> Coefficients        0.2654     0.0847\r\n#> Interaction         0.0332     0.0285\r\n#>\r\n#> Interpretation:\r\n#>   16.4% of wage gap explained by endowments (characteristics)\r\n#>   74.3% unexplained (returns to characteristics)\r\n```\r\n\r\n### 6.4 Detailed Decomposition\r\n\r\n```r\r\n# Detailed decomposition by variable\r\ndecomp_results <- decomp$twofold$variables[[1]]\r\n\r\n# Contribution of each variable to explained gap\r\nexplained <- data.frame(\r\n  variable = rownames(decomp_results),\r\n  contribution = decomp_results[, \"coef(explained)\"],\r\n  se = decomp_results[, \"se(explained)\"]\r\n)\r\nexplained$pct_of_explained <- explained$contribution / sum(explained$contribution) * 100\r\n\r\nprint(explained)\r\n#>           variable contribution      se pct_of_explained\r\n#> 1              age     0.001234 0.01245          2.10\r\n#> 2             LTHS     0.012456 0.00834         21.23\r\n#> 3      high.school    -0.003421 0.01023         -5.83\r\n#> 4     some.college     0.008923 0.00912         15.21\r\n#> 5          college     0.024567 0.01234         41.87\r\n#> 6 advanced.degree     0.009876 0.00987         16.83\r\n#> 7     foreign.born     0.005012 0.00756          8.54\r\n\r\n# Plot contributions\r\nlibrary(ggplot2)\r\nggplot(explained, aes(x = reorder(variable, contribution), y = contribution)) +\r\n  geom_col(fill = \"steelblue\") +\r\n  geom_errorbar(aes(ymin = contribution - 1.96*se,\r\n                    ymax = contribution + 1.96*se), width = 0.2) +\r\n  coord_flip() +\r\n  labs(title = \"Contribution to Explained Wage Gap\",\r\n       x = \"\", y = \"Contribution to log wage gap\") +\r\n  theme_minimal()\r\n```\r\n\r\n### 6.5 Reference Group Sensitivity\r\n\r\n```r\r\n# Results depend on reference group choice\r\n# Neumark (1988) pooled decomposition uses weighted average\r\n\r\n# Decomposition using different references\r\ndecomp_neumark <- oaxaca(\r\n  ln.real.wage ~ age + college + advanced.degree + foreign.born | female,\r\n  data = chicago_clean,\r\n  R = 50,\r\n  type = \"Neumark\"\r\n)\r\n\r\n# Compare three-fold decomposition\r\nsummary(decomp_neumark)\r\n```\r\n\r\n---\r\n\r\n## 7. Bootstrap Methods\r\n\r\nBootstrap provides non-parametric inference by resampling from the observed data.\r\n\r\n### 7.1 Standard Bootstrap\r\n\r\n```r\r\nlibrary(boot)\r\n\r\n# Example: Bootstrap confidence interval for regression coefficient\r\ndata(mtcars)\r\n\r\n# Statistic function: returns what we want to bootstrap\r\nboot_coef <- function(data, indices) {\r\n  d <- data[indices, ]  # Resample with given indices\r\n  model <- lm(mpg ~ wt + hp, data = d)\r\n  return(coef(model)[\"wt\"])  # Return coefficient of interest\r\n}\r\n\r\n# Run bootstrap\r\nset.seed(42)\r\nboot_results <- boot(\r\n  data = mtcars,\r\n  statistic = boot_coef,\r\n  R = 2000  # Number of bootstrap replications\r\n)\r\n\r\n# Results\r\nprint(boot_results)\r\n#> ORDINARY NONPARAMETRIC BOOTSTRAP\r\n#>\r\n#> Call:\r\n#> boot(data = mtcars, statistic = boot_coef, R = 2000)\r\n#>\r\n#> Bootstrap Statistics :\r\n#>     original       bias    std. error\r\n#> t1* -3.87783  -0.01234     0.6823\r\n\r\n# Confidence intervals (multiple methods)\r\nboot.ci(boot_results, type = c(\"norm\", \"basic\", \"perc\", \"bca\"))\r\n#> BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\r\n#> Based on 2000 bootstrap replicates\r\n#>\r\n#> Intervals :\r\n#> Level      Normal              Basic             Percentile     BCa\r\n#> 95%   (-5.1977, -2.5456) (-5.2012, -2.5067) (-5.2489, -2.5544) (-5.1834, -2.4912)\r\n```\r\n\r\n### 7.2 Bootstrapping Complex Statistics\r\n\r\n```r\r\n# Bootstrap for difference in medians\r\nboot_median_diff <- function(data, indices) {\r\n  d <- data[indices, ]\r\n  med_auto <- median(d$mpg[d$am == 0])  # Automatic\r\n  med_manual <- median(d$mpg[d$am == 1])  # Manual\r\n  return(med_manual - med_auto)\r\n}\r\n\r\nboot_med <- boot(mtcars, boot_median_diff, R = 2000)\r\nboot.ci(boot_med, type = \"perc\")\r\n#> BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\r\n#> Based on 2000 bootstrap replicates\r\n#>\r\n#> Intervals :\r\n#> Level     Percentile\r\n#> 95%   ( 3.20, 10.55 )\r\n\r\n# Bootstrap for R-squared\r\nboot_rsq <- function(data, indices) {\r\n  d <- data[indices, ]\r\n  model <- lm(mpg ~ wt + hp + qsec, data = d)\r\n  return(summary(model)$r.squared)\r\n}\r\n\r\nboot_r2 <- boot(mtcars, boot_rsq, R = 2000)\r\ncat(\"\\nR-squared bootstrap:\\n\")\r\ncat(\"Original R-squared:\", round(boot_r2$t0, 4), \"\\n\")\r\ncat(\"Bootstrap SE:\", round(sd(boot_r2$t), 4), \"\\n\")\r\nboot.ci(boot_r2, type = \"perc\")\r\n#> R-squared bootstrap:\r\n#> Original R-squared: 0.8348\r\n#> Bootstrap SE: 0.0512\r\n#>\r\n#> Intervals :\r\n#> Level     Percentile\r\n#> 95%   ( 0.7186, 0.9148 )\r\n```\r\n\r\n### 7.3 Cluster Bootstrap\r\n\r\nFor clustered data, resample clusters rather than individual observations:\r\n\r\n```r\r\n# Simulate clustered data\r\nset.seed(123)\r\nn_clusters <- 20\r\nobs_per_cluster <- 10\r\n\r\ncluster_data <- data.frame(\r\n  cluster = rep(1:n_clusters, each = obs_per_cluster),\r\n  x = rnorm(n_clusters * obs_per_cluster)\r\n)\r\n# Add cluster-level random effect\r\ncluster_effects <- rnorm(n_clusters, sd = 2)\r\ncluster_data$y <- 1 + 0.5 * cluster_data$x +\r\n                  cluster_effects[cluster_data$cluster] +\r\n                  rnorm(nrow(cluster_data))\r\n\r\n# Cluster bootstrap function\r\ncluster_boot <- function(data, indices) {\r\n  # indices are cluster indices, not observation indices\r\n  clusters_sampled <- unique(data$cluster)[indices]\r\n\r\n  # Get all observations from sampled clusters\r\n  d <- do.call(rbind, lapply(clusters_sampled, function(cl) {\r\n    data[data$cluster == cl, ]\r\n  }))\r\n\r\n  model <- lm(y ~ x, data = d)\r\n  return(coef(model)[\"x\"])\r\n}\r\n\r\n# Run cluster bootstrap\r\n# Need to set up for cluster-level resampling\r\ncluster_ids <- unique(cluster_data$cluster)\r\nboot_clus <- boot(\r\n  data = cluster_data,\r\n  statistic = function(data, i) {\r\n    sampled_clusters <- cluster_ids[i]\r\n    d <- data[data$cluster %in% sampled_clusters, ]\r\n    coef(lm(y ~ x, data = d))[\"x\"]\r\n  },\r\n  R = 1000,\r\n  sim = \"ordinary\"\r\n)\r\n\r\ncat(\"Cluster bootstrap results:\\n\")\r\ncat(\"Coefficient:\", round(boot_clus$t0, 4), \"\\n\")\r\ncat(\"Bootstrap SE:\", round(sd(boot_clus$t), 4), \"\\n\")\r\n```\r\n\r\n### 7.4 Wild Cluster Bootstrap\r\n\r\nFor clustered data with few clusters, wild cluster bootstrap provides better inference:\r\n\r\n```r\r\n# Note: fwildclusterboot package requires R >= 4.0\r\n# Check if available\r\nif (require(\"fwildclusterboot\", quietly = TRUE)) {\r\n\r\n  # Fit model\r\n  model <- lm(y ~ x, data = cluster_data)\r\n\r\n  # Wild cluster bootstrap\r\n  boot_wild <- boottest(\r\n    model,\r\n    clustid = \"cluster\",\r\n    param = \"x\",\r\n    B = 999,\r\n    type = \"webb\"  # Webb weights (recommended for few clusters)\r\n  )\r\n\r\n  summary(boot_wild)\r\n\r\n} else {\r\n  cat(\"fwildclusterboot not available.\\n\")\r\n  cat(\"Install with: install.packages('fwildclusterboot')\\n\")\r\n  cat(\"Requires R >= 4.0\\n\")\r\n}\r\n```\r\n\r\n---\r\n\r\n## 8. Age-Period-Cohort (APC) Analysis\r\n\r\nAPC analysis disentangles age, period, and cohort effects in repeated cross-sectional or panel data.\r\n\r\n### 8.1 The Identification Problem\r\n\r\nAge, period, and cohort are linearly dependent: `Cohort = Period - Age`. This creates a fundamental identification problem requiring additional assumptions.\r\n\r\n**Approaches to identification:**\r\n- Constrain one effect (e.g., assume no period effects)\r\n- Use smoothing (GAMs) to impose structure\r\n- Intrinsic estimator (removes arbitrary constraint)\r\n- Theoretical restrictions based on domain knowledge\r\n\r\n### 8.2 Simulating APC Data\r\n\r\n```r\r\nlibrary(mgcv)\r\n\r\n# Simulate data with known APC effects\r\nset.seed(42)\r\nn <- 2000\r\n\r\n# Generate survey data across years\r\nperiod <- sample(1990:2020, n, replace = TRUE)\r\nage <- sample(20:70, n, replace = TRUE)\r\ncohort <- period - age\r\n\r\n# True effects (for illustration)\r\n# Age effect: inverted U\r\nage_effect <- -0.002 * (age - 45)^2\r\n\r\n# Period effect: linear increase\r\nperiod_effect <- 0.02 * (period - 2005)\r\n\r\n# Cohort effect: step function\r\ncohort_effect <- ifelse(cohort < 1960, -0.3,\r\n                        ifelse(cohort < 1980, 0, 0.2))\r\n\r\n# Outcome (probability scale)\r\neta <- 0.5 + age_effect + period_effect + cohort_effect + rnorm(n, sd = 0.3)\r\ny <- rbinom(n, 1, plogis(eta))\r\n\r\napc_data <- data.frame(y, age, period, cohort)\r\n\r\n# Basic summaries\r\ncat(\"Data structure:\\n\")\r\ncat(\"Periods:\", range(apc_data$period), \"\\n\")\r\ncat(\"Ages:\", range(apc_data$age), \"\\n\")\r\ncat(\"Cohorts:\", range(apc_data$cohort), \"\\n\")\r\n#> Data structure:\r\n#> Periods: 1990 2020\r\n#> Ages: 20 70\r\n#> Cohorts: 1920 2000\r\n```\r\n\r\n### 8.3 APC with GAMs\r\n\r\n```r\r\n# Fit APC model using tensor product smooth\r\n# This allows flexible non-linear effects\r\n\r\napc_gam <- gam(\r\n  y ~ te(age, period, bs = \"cr\"),  # Tensor smooth of age and period\r\n  family = binomial(link = \"logit\"),\r\n  data = apc_data\r\n)\r\n\r\nsummary(apc_gam)\r\n#> Family: binomial\r\n#> Link function: logit\r\n#>\r\n#> Formula:\r\n#> y ~ te(age, period, bs = \"cr\")\r\n#>\r\n#> Parametric coefficients:\r\n#>             Estimate Std. Error z value Pr(>|z|)\r\n#> (Intercept)  0.20213    0.04721   4.282 1.85e-05 ***\r\n#>\r\n#> Approximate significance of smooth terms:\r\n#>                  edf Ref.df Chi.sq  p-value\r\n#> te(age,period) 15.23  18.41  312.4 < 2e-16 ***\r\n\r\n# Check model diagnostics\r\ngam.check(apc_gam)\r\n```\r\n\r\n### 8.4 Visualizing APC Effects\r\n\r\n```r\r\n# 2D contour plot of predicted probabilities\r\nvis.gam(apc_gam,\r\n        view = c(\"age\", \"period\"),\r\n        plot.type = \"contour\",\r\n        main = \"Predicted Probability by Age and Period\",\r\n        xlab = \"Age\", ylab = \"Period\")\r\n\r\n# Alternative: 3D perspective plot\r\nvis.gam(apc_gam,\r\n        view = c(\"age\", \"period\"),\r\n        plot.type = \"persp\",\r\n        theta = 45, phi = 20,\r\n        main = \"APC Surface\")\r\n\r\n# Marginal effect of age at different periods\r\nplot_age <- function(period_val) {\r\n  newdata <- data.frame(\r\n    age = 20:70,\r\n    period = period_val\r\n  )\r\n  preds <- predict(apc_gam, newdata, type = \"response\", se.fit = TRUE)\r\n  data.frame(\r\n    age = 20:70,\r\n    period = period_val,\r\n    fit = preds$fit,\r\n    se = preds$se.fit\r\n  )\r\n}\r\n\r\nage_effects <- rbind(\r\n  plot_age(1995),\r\n  plot_age(2005),\r\n  plot_age(2015)\r\n)\r\n\r\nlibrary(ggplot2)\r\nggplot(age_effects, aes(x = age, y = fit, color = factor(period))) +\r\n  geom_line(linewidth = 1) +\r\n  geom_ribbon(aes(ymin = fit - 1.96*se, ymax = fit + 1.96*se, fill = factor(period)),\r\n              alpha = 0.2, color = NA) +\r\n  labs(title = \"Age Effect by Period\",\r\n       x = \"Age\", y = \"Predicted Probability\",\r\n       color = \"Period\", fill = \"Period\") +\r\n  theme_minimal()\r\n```\r\n\r\n### 8.5 Cohort Effects\r\n\r\n```r\r\n# Extract cohort patterns\r\ncohort_means <- aggregate(y ~ cohort, data = apc_data, FUN = mean)\r\n\r\nggplot(cohort_means, aes(x = cohort, y = y)) +\r\n  geom_point() +\r\n  geom_smooth(method = \"loess\", span = 0.5) +\r\n  labs(title = \"Outcome by Birth Cohort\",\r\n       x = \"Birth Cohort\", y = \"Proportion\") +\r\n  theme_minimal()\r\n\r\n# Model with explicit cohort term\r\napc_gam2 <- gam(\r\n  y ~ s(age, bs = \"cr\") + s(period, bs = \"cr\") + s(cohort, bs = \"cr\"),\r\n  family = binomial,\r\n  data = apc_data\r\n)\r\n\r\n# Note: This model is overparameterized due to linear dependence\r\n# One approach: Use identifiability constraints\r\nsummary(apc_gam2)\r\n```\r\n\r\n---\r\n\r\n## 9. List Experiments (Item Count Technique)\r\n\r\nList experiments measure sensitive attitudes/behaviors while protecting respondent privacy.\r\n\r\n### 9.1 Design\r\n\r\n- **Control group**: List of J non-sensitive items, report count\r\n- **Treatment group**: Same J items + 1 sensitive item, report count\r\n- **Estimator**: Difference in means = prevalence of sensitive behavior\r\n\r\n### 9.2 When to Use List Experiments\r\n\r\n**Appropriate when:**\r\n- Direct questions may elicit social desirability bias\r\n- Topic is sensitive (illegal behavior, prejudice, stigmatized attitudes)\r\n- Anonymity is insufficient protection\r\n\r\n**Limitations:**\r\n- Requires larger samples (less precision than direct questions)\r\n- Floor/ceiling effects if counts pile up at 0 or J+1\r\n- Cannot identify individual-level responses\r\n\r\n### 9.3 Analysis with list Package\r\n\r\n```r\r\nlibrary(list)\r\n\r\n# Load race data: list experiment on racial attitudes\r\ndata(race)\r\n\r\n# Examine the data\r\nhead(race)\r\n#>   y treat   age south male college state\r\n#> 1 3     0  7.2     1    0       0    TX\r\n#> 2 2     0  2.5     0    0       1    NY\r\n#> 3 3     0  3.3     0    1       1    NY\r\n#> 4 2     0  3.4     0    0       0    NY\r\n#> 5 3     0  5.3     1    0       1    TX\r\n#> 6 2     1  2.3     0    1       0    NY\r\n\r\n# y: count of items agreed with (0-4 for control, 0-5 for treatment)\r\n# treat: 1 = treatment (includes sensitive item)\r\n# age: age in decades\r\n# south: Southern state indicator\r\n\r\n# Simple difference in means estimate\r\nmean_control <- mean(race$y[race$treat == 0])\r\nmean_treatment <- mean(race$y[race$treat == 1])\r\nprevalence <- mean_treatment - mean_control\r\n\r\ncat(\"Simple prevalence estimate:\\n\")\r\ncat(\"Control mean:\", round(mean_control, 3), \"\\n\")\r\ncat(\"Treatment mean:\", round(mean_treatment, 3), \"\\n\")\r\ncat(\"Difference (prevalence):\", round(prevalence, 3), \"\\n\")\r\ncat(\"SE:\", round(sqrt(var(race$y[race$treat == 1])/sum(race$treat == 1) +\r\n                       var(race$y[race$treat == 0])/sum(race$treat == 0)), 3), \"\\n\")\r\n#> Simple prevalence estimate:\r\n#> Control mean: 2.338\r\n#> Treatment mean: 2.501\r\n#> Difference (prevalence): 0.163\r\n#> SE: 0.058\r\n```\r\n\r\n### 9.4 Regression-Based Analysis\r\n\r\n```r\r\n# ictreg: Item Count Technique Regression\r\n# Models how covariates affect both control items and sensitive item\r\n\r\nict_model <- ictreg(\r\n  y ~ age + south,\r\n  data = race,\r\n  treat = \"treat\",\r\n  J = 4,  # Number of non-sensitive items\r\n  method = \"lm\"  # Linear method (more robust than ML)\r\n)\r\n\r\nsummary(ict_model)\r\n#> Item Count Technique Regression\r\n#>\r\n#> Call: ictreg(formula = y ~ age + south, data = race, treat = \"treat\",\r\n#>     J = 4, method = \"lm\")\r\n#>\r\n#> Sensitive item\r\n#>                 Est.    S.E.\r\n#> (Intercept) -0.27130 0.13970\r\n#> age          0.06460 0.03072\r\n#> south        0.27685 0.11913\r\n#>\r\n#> Control items\r\n#>                 Est.    S.E.\r\n#> (Intercept)  2.01452 0.09620\r\n#> age          0.04097 0.02040\r\n#> south       -0.22765 0.07583\r\n```\r\n\r\n### 9.5 Predicted Prevalence by Group\r\n\r\n```r\r\n# Predict prevalence for different groups\r\n# Using model coefficients for sensitive item\r\ncoefs <- coef(ict_model)\r\nsens_coefs <- coefs[grep(\"^sensitive\", names(coefs))]\r\ncat(\"Sensitive item coefficients:\\n\")\r\nprint(sens_coefs)\r\n#> sensitive.(Intercept)         sensitive.age       sensitive.south\r\n#>            -0.2713024             0.0646007             0.2768511\r\n\r\n# Calculate predicted prevalence manually\r\n# For age = 5 decades, non-South\r\npred_nonsouth <- sens_coefs[1] + sens_coefs[2] * 5 + sens_coefs[3] * 0\r\ncat(\"\\nPredicted prevalence (age 50, non-South):\", round(pred_nonsouth, 3), \"\\n\")\r\n\r\n# For age = 5 decades, South\r\npred_south <- sens_coefs[1] + sens_coefs[2] * 5 + sens_coefs[3] * 1\r\ncat(\"Predicted prevalence (age 50, South):\", round(pred_south, 3), \"\\n\")\r\ncat(\"Difference:\", round(pred_south - pred_nonsouth, 3), \"\\n\")\r\n#> Predicted prevalence (age 50, non-South): 0.052\r\n#> Predicted prevalence (age 50, South): 0.329\r\n#> Difference: 0.277\r\n```\r\n\r\n### 9.6 Checking for Design Effects\r\n\r\n```r\r\n# Check for floor and ceiling effects\r\ntable(race$y, race$treat)\r\n#>\r\n#>       0   1\r\n#>   0  12   8\r\n#>   1 124  98\r\n#>   2 234 187\r\n#>   3 178 212\r\n#>   4  49  73\r\n#>   5   0  19\r\n\r\n# Floor effects: Too many 0s in treatment group\r\n# Ceiling effects: Too many J+1s in treatment group\r\n\r\n# Proportion at floor/ceiling\r\ncat(\"Control at 0 (floor):\", mean(race$y[race$treat == 0] == 0), \"\\n\")\r\ncat(\"Treatment at 0:\", mean(race$y[race$treat == 1] == 0), \"\\n\")\r\ncat(\"Treatment at 5 (ceiling):\", mean(race$y[race$treat == 1] == 5), \"\\n\")\r\n#> Control at 0 (floor): 0.02\r\n#> Treatment at 0: 0.013\r\n#> Treatment at 5 (ceiling): 0.032\r\n```\r\n\r\n---\r\n\r\n## Quick Reference\r\n\r\n| Method | Package | Key Function | Dataset Used |\r\n|--------|---------|--------------|--------------|\r\n| Survey design | survey | `svydesign()` | api |\r\n| Survey regression | survey | `svyglm()` | api |\r\n| Weighted stats | Hmisc | `wtd.mean()` | any |\r\n| Randomization inference | ri2 | `conduct_ri()` | simulated |\r\n| Multiple testing | base | `p.adjust()` | simulated |\r\n| Equivalence testing | TOSTER | `t_TOST()` | simulated |\r\n| Decomposition | oaxaca | `oaxaca()` | chicago |\r\n| Bootstrap | boot | `boot()` | mtcars |\r\n| APC analysis | mgcv | `gam()` | simulated |\r\n| List experiments | list | `ictreg()` | race |\r\n",
        "plugins/r-analyst/skills/r-analyst/techniques/03_text_ml.md": "# Text Analysis & Machine Learning in R\r\n\r\nComprehensive guide to text analysis and ML methods with reproducible examples using package vignette data.\r\n\r\n---\r\n\r\n## Quick Reference\r\n\r\n| Task | R Package | Key Function | Example Dataset |\r\n|------|-----------|--------------|-----------------|\r\n| LDA Topic Modeling | topicmodels | `LDA()` | AssociatedPress |\r\n| Structural Topic Model | stm | `stm()` | gadarian |\r\n| Sentiment Analysis | tidytext | `get_sentiments()` | janeaustenr books |\r\n| TF-IDF | tidytext | `bind_tf_idf()` | janeaustenr books |\r\n| Causal Forests | grf | `causal_forest()` | Simulated |\r\n| Random Forests | ranger | `ranger()` | iris, mtcars |\r\n| SVM | caret, e1071 | `train()`, `svm()` | iris |\r\n| Cross-Validation | caret | `trainControl()` | PimaIndiansDiabetes2 |\r\n| GAMs | mgcv | `gam()` | gamSim() |\r\n| EFA | psych | `fa()` | bfi |\r\n| CFA | lavaan | `cfa()` | HolzingerSwineford1939 |\r\n| IRT | mirt | `mirt()` | LSAT6, simdata() |\r\n| Reliability | psych | `alpha()`, `ICC()` | bfi |\r\n\r\n### Installation\r\n\r\n```r\r\n# Install all required packages\r\npackages <- c(\r\n  \"topicmodels\", \"tm\", \"LDAvis\", \"slam\",   # Topic modeling\r\n  \"stm\",                                    # STM\r\n  \"tidytext\", \"janeaustenr\", \"textdata\",   # Sentiment (textdata for lexicons)\r\n  \"dplyr\", \"tidyr\", \"ggplot2\", \"stringr\",  # Data manipulation\r\n  \"grf\",                                    # Causal forests\r\n  \"ranger\",                                 # Random forests\r\n  \"caret\", \"e1071\", \"kernlab\",             # SVM\r\n  \"mlbench\", \"pROC\",                        # Classification/CV\r\n  \"mgcv\",                                   # GAMs\r\n  \"psych\", \"lme4\",                          # Factor analysis & ICC\r\n  \"lavaan\", \"semTools\",                     # CFA\r\n  \"mirt\"                                    # IRT\r\n)\r\n\r\n# Install if not present\r\ninstall.packages(setdiff(packages, rownames(installed.packages())))\r\n\r\n# Note: First use of sentiment lexicons requires one-time download:\r\n# tidytext::get_sentiments(\"afinn\")  # Will prompt to download\r\n```\r\n\r\n---\r\n\r\n## 1. Topic Modeling\r\n\r\nTopic models discover latent themes in document collections. They are unsupervised methods that identify groups of co-occurring words.\r\n\r\n### 1.1 Latent Dirichlet Allocation (LDA)\r\n\r\n**When to use LDA:**\r\n- Exploratory analysis of large document collections\r\n- No metadata/covariates needed to explain topic variation\r\n- Goal is to discover themes without prior hypotheses\r\n- Documents are reasonably long (paragraphs to pages)\r\n\r\n**Assumptions:**\r\n- Documents are mixtures of topics\r\n- Topics are distributions over words\r\n- Word order doesn't matter (bag-of-words)\r\n- Number of topics K is fixed and known\r\n\r\n**Common pitfalls:**\r\n- Choosing K arbitrarily (use diagnostics)\r\n- Over-interpreting topics with low coherence\r\n- Not preprocessing adequately (stopwords, rare terms)\r\n- Ignoring multi-word expressions\r\n\r\n```r\r\nlibrary(topicmodels)\r\nlibrary(tm)\r\n\r\n# Load built-in Associated Press dataset\r\n# 2,246 news articles with 10,473 unique terms\r\ndata(\"AssociatedPress\")\r\n\r\n# Inspect the document-term matrix\r\nAssociatedPress\r\n#> <<DocumentTermMatrix (documents: 2246, terms: 10473)>>\r\n#> Non-/sparse entries: 302031/23220327\r\n#> Sparsity           : 99%\r\n\r\n# Remove sparse terms (keep terms appearing in at least 2% of docs)\r\nap_dtm <- removeSparseTerms(AssociatedPress, sparse = 0.98)\r\nap_dtm\r\n#> Now ~1,600 terms retained\r\n\r\n# Remove any documents that became empty after filtering\r\nrow_totals <- slam::row_sums(ap_dtm)\r\nap_dtm <- ap_dtm[row_totals > 0, ]\r\n#> 2,244 documents remain\r\n\r\n# Fit LDA with 10 topics using Gibbs sampling\r\nset.seed(12345)\r\nlda_model <- LDA(\r\n  ap_dtm,\r\n  k = 10,\r\n  method = \"Gibbs\",\r\n  control = list(\r\n    seed = 12345,\r\n    burnin = 1000,\r\n    iter = 2000,\r\n    thin = 100\r\n  )\r\n)\r\n\r\n# View top 10 words per topic\r\nterms(lda_model, 10)\r\n#>       Topic 1    Topic 2     Topic 3   Topic 4    Topic 5\r\n#> [1,]  \"percent\"  \"soviet\"    \"police\"  \"bush\"     \"court\"\r\n#> [2,]  \"year\"     \"united\"    \"people\"  \"house\"    \"case\"\r\n#> [3,]  \"million\"  \"states\"    \"city\"    \"president\" \"judge\"\r\n#> ...\r\n\r\n# Get topic assignments for each document\r\ndoc_topics <- topics(lda_model)\r\nhead(doc_topics)\r\n#> [1] 10  5  7  3  2  8\r\n\r\n# Get topic probabilities for each document (theta matrix)\r\ndoc_topic_probs <- posterior(lda_model)$topics\r\nhead(round(doc_topic_probs, 3))\r\n#>          1     2     3     4     5     6     7     8     9    10\r\n#> [1,] 0.012 0.012 0.012 0.012 0.012 0.012 0.012 0.012 0.012 0.880\r\n#> [2,] 0.011 0.011 0.011 0.011 0.792 0.011 0.011 0.011 0.011 0.011\r\n#> ...\r\n```\r\n\r\n#### Choosing the Number of Topics\r\n\r\nUse perplexity (lower is better) or coherence metrics to guide K selection.\r\n\r\n**Note:** For Gibbs-sampled LDA models, perplexity calculation requires the VEM method or using held-out data. Here we use VEM for model comparison:\r\n\r\n```r\r\n# Compare models with different K values using VEM (faster for comparison)\r\nk_values <- c(5, 10, 15, 20, 25)\r\nperplexities <- numeric(length(k_values))\r\n\r\nset.seed(12345)\r\nfor (i in seq_along(k_values)) {\r\n  model <- LDA(\r\n    ap_dtm,\r\n    k = k_values[i],\r\n    method = \"VEM\",  # VEM method supports perplexity directly\r\n    control = list(seed = 12345)\r\n  )\r\n  perplexities[i] <- perplexity(model)\r\n}\r\n\r\n# Plot perplexity by K\r\nplot(k_values, perplexities, type = \"b\",\r\n     xlab = \"Number of Topics (K)\",\r\n     ylab = \"Perplexity\",\r\n     main = \"Model Selection: Perplexity vs K\")\r\n\r\n# Results (approximate):\r\n#> K=5:  ~2800\r\n#> K=10: ~2500\r\n#> K=15: ~2350\r\n#> K=20: ~2250  <- Elbow often around here\r\n#> K=25: ~2200\r\n\r\n# Alternative: Use log-likelihood for Gibbs models\r\n# logLik(lda_model)  # Returns log-likelihood\r\n```\r\n\r\n#### Visualizing Topics with LDAvis\r\n\r\n```r\r\nlibrary(LDAvis)\r\nlibrary(servr)\r\n\r\n# Prepare data for visualization\r\nphi <- posterior(lda_model)$terms      # Topic-word distributions\r\ntheta <- posterior(lda_model)$topics   # Document-topic distributions\r\nvocab <- colnames(phi)\r\ndoc_length <- slam::row_sums(ap_dtm)\r\nterm_freq <- slam::col_sums(ap_dtm)\r\n\r\n# Create JSON for visualization\r\njson <- createJSON(\r\n  phi = phi,\r\n  theta = theta,\r\n  vocab = vocab,\r\n  doc.length = doc_length,\r\n  term.frequency = term_freq\r\n)\r\n\r\n# Launch interactive visualization\r\nserVis(json)\r\n```\r\n\r\n---\r\n\r\n### 1.2 Structural Topic Model (STM)\r\n\r\n**When to use STM over LDA:**\r\n- Document metadata (author, date, source) may explain topic prevalence\r\n- You hypothesize that covariates affect what topics appear\r\n- You want to model how topic expression varies by group\r\n- Need statistical inference on covariate effects\r\n\r\n**Key advantages:**\r\n- Prevalence covariates: What topics are discussed?\r\n- Content covariates: How topics are discussed differently by groups\r\n- Built-in tools for model selection and validation\r\n\r\n```r\r\nlibrary(stm)\r\n\r\n# Load built-in gadarian dataset\r\n# Survey responses about immigration with treatment/control\r\ndata(gadarian)\r\n\r\n# Inspect the data\r\nstr(gadarian)\r\n#> 'data.frame': 341 obs. of 4 variables:\r\n#>  $ MetaID   : int  1 2 3 4 5 6 7 ...\r\n#>  $ treatment: int  1 0 1 0 1 1 0 ...\r\n#>  $ pid_rep  : int  0 0 1 0 1 0 0 ...\r\n#>  $ open.ended.response: chr \"...\" ...\r\n\r\nhead(gadarian$open.ended.response, 2)\r\n#> [1] \"...\"  # Open-ended responses about immigration\r\n\r\n# Preprocess the text\r\nprocessed <- textProcessor(\r\n  documents = gadarian$open.ended.response,\r\n  metadata = gadarian,\r\n  lowercase = TRUE,\r\n  removestopwords = TRUE,\r\n  removenumbers = TRUE,\r\n  removepunctuation = TRUE,\r\n  stem = TRUE,\r\n  verbose = FALSE\r\n)\r\n\r\n# Prepare documents (remove sparse terms)\r\nout <- prepDocuments(\r\n  processed$documents,\r\n  processed$vocab,\r\n  processed$meta,\r\n  lower.thresh = 5  # Terms must appear in 5+ documents\r\n)\r\n#> Removing 970 of 1102 terms (1579 of 3789 tokens) due to frequency\r\n#> Your corpus now has 334 documents, 132 terms and 2210 tokens.\r\n\r\n# Check what was removed\r\nout$meta         # Metadata aligned with documents\r\nlength(out$vocab) # Vocabulary size after filtering\r\n#> [1] 132\r\n```\r\n\r\n#### Selecting Number of Topics with searchK\r\n\r\n```r\r\n# Search over candidate K values\r\nset.seed(12345)\r\nK_search <- searchK(\r\n  out$documents,\r\n  out$vocab,\r\n  K = c(5, 7, 10, 15),\r\n  prevalence = ~ treatment + pid_rep,\r\n  data = out$meta,\r\n  verbose = FALSE\r\n)\r\n\r\n# Plot diagnostics\r\nplot(K_search)\r\n#> Look for:\r\n#> - High semantic coherence\r\n#> - High exclusivity\r\n#> - Lower residuals\r\n#> - Held-out likelihood plateau\r\n```\r\n\r\n#### Fitting STM with Prevalence Covariates\r\n\r\n```r\r\n# Fit STM with 7 topics\r\n# Prevalence: treatment and party ID affect topic proportions\r\nset.seed(12345)\r\nstm_model <- stm(\r\n  documents = out$documents,\r\n  vocab = out$vocab,\r\n  K = 7,\r\n  prevalence = ~ treatment + pid_rep,\r\n  data = out$meta,\r\n  seed = 12345,\r\n  verbose = FALSE\r\n)\r\n\r\n# View top words per topic (multiple metrics)\r\nlabelTopics(stm_model, n = 7)\r\n#> Topic 1 Top Words:\r\n#>   Highest Prob: immigr, illeg, countri, work, come, job, live\r\n#>   FREX: illeg, citizenship, border, deport, amnesti, crimin, green\r\n#>   Lift: amnesti, citizenship, deport, applic, citizen, document, fine\r\n#>   Score: illeg, amnesti, citizenship, border, deport, citizen, card\r\n\r\n# Summary of topic proportions\r\nplot(stm_model, type = \"summary\", xlim = c(0, 0.4))\r\n```\r\n\r\n#### Estimating Covariate Effects\r\n\r\n```r\r\n# Estimate effect of treatment on topic prevalence\r\neffects <- estimateEffect(\r\n  formula = 1:7 ~ treatment + pid_rep,\r\n  stmobj = stm_model,\r\n  metadata = out$meta,\r\n  uncertainty = \"Global\"\r\n)\r\n\r\n# Summary for specific topics\r\nsummary(effects, topics = c(1, 3, 5))\r\n#> Topic 1:\r\n#>   Covariate: treatment\r\n#>   Estimate: 0.05 (SE: 0.02), p < 0.05\r\n#>   Interpretation: Treatment increases discussion of Topic 1\r\n\r\n# Visualize treatment effect\r\nplot(\r\n  effects,\r\n  covariate = \"treatment\",\r\n  topics = c(1, 3, 5),\r\n  model = stm_model,\r\n  method = \"difference\",\r\n  cov.value1 = 1,\r\n  cov.value2 = 0,\r\n  xlab = \"Treatment Effect on Topic Prevalence\",\r\n  main = \"Effect of Anxiety Treatment on Immigration Topics\",\r\n  xlim = c(-0.15, 0.15)\r\n)\r\n```\r\n\r\n#### Finding Representative Documents\r\n\r\n```r\r\n# Find documents most associated with each topic\r\nfindThoughts(\r\n  stm_model,\r\n  texts = gadarian$open.ended.response,\r\n  topics = 1,\r\n  n = 3\r\n)\r\n#> Topic 1:\r\n#>  [1] \"I think we need stronger border security...\"\r\n#>  [2] \"Illegal immigration is a serious problem...\"\r\n#>  [3] \"We should enforce existing laws...\"\r\n```\r\n\r\n#### Content Covariates: How Topics Differ by Group\r\n\r\nContent covariates model how the *words used* within a topic vary by group (not just topic prevalence):\r\n\r\n```r\r\n# Fit STM with both prevalence AND content covariates\r\nset.seed(12345)\r\nstm_content <- stm(\r\n  documents = out$documents,\r\n  vocab = out$vocab,\r\n  K = 5,\r\n  prevalence = ~ treatment + pid_rep,   # What topics appear\r\n  content = ~ treatment,                 # How words differ by treatment\r\n  data = out$meta,\r\n  seed = 12345,\r\n  verbose = FALSE\r\n)\r\n\r\n# View how word usage differs by treatment within a topic\r\nplot(stm_content, type = \"perspectives\", topics = 1)\r\n#> Shows words more associated with treatment=0 vs treatment=1\r\n#> within the same topic\r\n\r\n# Example interpretation:\r\n#> Topic about \"immigration concerns\" might use:\r\n#> - Treatment group (anxious): \"dangerous\", \"threat\", \"crime\"\r\n#> - Control group: \"policy\", \"reform\", \"workers\"\r\n```\r\n\r\n#### Topic Correlation\r\n\r\nExamine which topics tend to co-occur in documents:\r\n\r\n```r\r\n# Calculate topic correlations\r\ntopic_corr <- topicCorr(stm_model, method = \"simple\")\r\n\r\n# View correlation matrix\r\nprint(round(topic_corr$cor, 2))\r\n#>       [,1]  [,2]  [,3]  [,4]  [,5]\r\n#> [1,]  1.00 -0.15  0.08 -0.32 -0.21\r\n#> [2,] -0.15  1.00 -0.28  0.12 -0.18\r\n#> [3,]  0.08 -0.28  1.00 -0.25 -0.10\r\n#> ...\r\n\r\n# Positive correlation: Topics co-occur\r\n# Negative correlation: Topics are mutually exclusive\r\n\r\n# Visualize topic network (requires igraph)\r\n# plot(topic_corr)\r\n```\r\n\r\n#### Model Diagnostics: Semantic Coherence and Exclusivity\r\n\r\n```r\r\n# For model selection, balance coherence and exclusivity\r\n# Coherence: Do top words co-occur in documents? (higher = better)\r\n# Exclusivity: Are top words unique to this topic? (higher = better)\r\n\r\n# Compare models with different K\r\nmodels <- list()\r\nfor (k in c(5, 7, 10)) {\r\n  models[[as.character(k)]] <- stm(\r\n    documents = out$documents,\r\n    vocab = out$vocab,\r\n    K = k,\r\n    prevalence = ~ treatment,\r\n    data = out$meta,\r\n    seed = 12345,\r\n    verbose = FALSE,\r\n    max.em.its = 20\r\n  )\r\n}\r\n\r\n# Extract diagnostics\r\ndiagnostics <- lapply(names(models), function(k) {\r\n  data.frame(\r\n    K = k,\r\n    coherence = mean(semanticCoherence(models[[k]], out$documents)),\r\n    exclusivity = mean(exclusivity(models[[k]]))\r\n  )\r\n})\r\ndiagnostics <- do.call(rbind, diagnostics)\r\nprint(diagnostics)\r\n#>   K coherence exclusivity\r\n#> 1 5     -85.2        9.42\r\n#> 2 7     -92.1        9.58\r\n#> 3 10    -98.7        9.71\r\n\r\n# Trade-off: More topics = higher exclusivity but lower coherence\r\n# Choose K where both are reasonably high\r\n```\r\n\r\n---\r\n\r\n## 2. Sentiment Analysis\r\n\r\nSentiment analysis measures the emotional tone or opinion expressed in text.\r\n\r\n### 2.1 Dictionary-Based Methods\r\n\r\n**When to use dictionary methods:**\r\n- Large-scale analysis where manual coding is impractical\r\n- Domain has established, validated lexicons\r\n- Need reproducible, transparent scoring\r\n- Exploratory analysis before more complex methods\r\n\r\n**Limitations:**\r\n- Ignores context, negation, sarcasm\r\n- Domain mismatch (lexicons often built for product reviews)\r\n- Cannot capture nuance or implicit sentiment\r\n- Word-level aggregation loses sentence structure\r\n\r\n**Important:** The sentiment lexicons require a one-time download via the `textdata` package. When you first call `get_sentiments()`, you'll be prompted to download the lexicon.\r\n\r\n```r\r\nlibrary(tidytext)\r\nlibrary(dplyr)\r\nlibrary(tidyr)\r\nlibrary(janeaustenr)\r\nlibrary(ggplot2)\r\nlibrary(stringr)\r\n\r\n# Load Jane Austen novels\r\nausten_books <- austen_books()\r\nhead(austen_books)\r\n#>   book                text\r\n#> 1 Sense & Sensibility SENSE AND SENSIBILITY\r\n#> 2 Sense & Sensibility by Jane Austen\r\n#> ...\r\n\r\n# Tokenize to one word per row\r\ntidy_austen <- austen_books %>%\r\n  group_by(book) %>%\r\n  mutate(\r\n    linenumber = row_number(),\r\n    chapter = cumsum(str_detect(text, regex(\"^chapter [\\\\divxlc]\",\r\n                                            ignore_case = TRUE)))\r\n  ) %>%\r\n  ungroup() %>%\r\n  unnest_tokens(word, text)\r\n\r\nhead(tidy_austen)\r\n#>   book                linenumber chapter word\r\n#> 1 Sense & Sensibility          1       0 sense\r\n#> 2 Sense & Sensibility          1       0 and\r\n#> 3 Sense & Sensibility          1       0 sensibility\r\n```\r\n\r\n### 2.2 Comparing Lexicons\r\n\r\nThree main sentiment lexicons are available:\r\n\r\n| Lexicon | Type | Scale | Best for |\r\n|---------|------|-------|----------|\r\n| AFINN | Numeric | -5 to +5 | Intensity measurement |\r\n| Bing | Binary | pos/neg | Simple polarity |\r\n| NRC | Categorical | 8 emotions + pos/neg | Emotional analysis |\r\n\r\n```r\r\n# First-time use: You'll be prompted to download each lexicon\r\n# After download, they're cached locally\r\n\r\n# View available lexicons\r\nget_sentiments(\"afinn\") %>% head()\r\n#>   word       value\r\n#> 1 abandon       -2\r\n#> 2 abandoned     -2\r\n#> 3 abandons      -2\r\n\r\nget_sentiments(\"bing\") %>% head()\r\n#>   word        sentiment\r\n#> 1 2-faces     negative\r\n#> 2 abnormal    negative\r\n\r\nget_sentiments(\"nrc\") %>% head()\r\n#>   word        sentiment\r\n#> 1 abacus      trust\r\n#> 2 abandon     fear\r\n#> 3 abandon     negative\r\n```\r\n\r\n#### AFINN: Numeric Sentiment Scores\r\n\r\n```r\r\n# Join with AFINN lexicon\r\nafinn_sentiment <- tidy_austen %>%\r\n  inner_join(get_sentiments(\"afinn\"), by = \"word\") %>%\r\n  group_by(book, index = linenumber %/% 80) %>%  # 80-line chunks\r\n  summarize(\r\n    sentiment = sum(value),\r\n    n_words = n(),\r\n    .groups = \"drop\"\r\n  )\r\n\r\n# Plot sentiment trajectory for Pride & Prejudice\r\nafinn_sentiment %>%\r\n  filter(book == \"Pride & Prejudice\") %>%\r\n  ggplot(aes(index, sentiment)) +\r\n  geom_col(show.legend = FALSE) +\r\n  labs(\r\n    title = \"Sentiment Trajectory: Pride & Prejudice\",\r\n    x = \"Narrative Time (80-line chunks)\",\r\n    y = \"Sentiment Score (AFINN)\"\r\n  )\r\n\r\n# Summary statistics by book\r\nafinn_sentiment %>%\r\n  group_by(book) %>%\r\n  summarize(\r\n    mean_sentiment = mean(sentiment),\r\n    sd_sentiment = sd(sentiment),\r\n    min_sentiment = min(sentiment),\r\n    max_sentiment = max(sentiment)\r\n  )\r\n#>   book                mean_sentiment sd_sentiment\r\n#> 1 Emma                         12.5         15.2\r\n#> 2 Mansfield Park                8.3         12.1\r\n#> 3 Northanger Abbey             10.1         13.8\r\n#> 4 Persuasion                    9.7         11.9\r\n#> 5 Pride & Prejudice            11.8         14.6\r\n#> 6 Sense & Sensibility           9.4         13.2\r\n```\r\n\r\n#### Bing: Binary Sentiment Classification\r\n\r\n```r\r\n# Calculate net sentiment (positive - negative)\r\nbing_sentiment <- tidy_austen %>%\r\n  inner_join(get_sentiments(\"bing\"), by = \"word\") %>%\r\n  count(book, index = linenumber %/% 80, sentiment) %>%\r\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%\r\n  mutate(net_sentiment = positive - negative)\r\n\r\n# Compare all books\r\nbing_sentiment %>%\r\n  ggplot(aes(index, net_sentiment, fill = book)) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~book, ncol = 2, scales = \"free_x\") +\r\n  labs(\r\n    title = \"Sentiment Across Jane Austen Novels\",\r\n    x = \"Narrative Time\",\r\n    y = \"Net Sentiment (Bing)\"\r\n  )\r\n```\r\n\r\n#### NRC: Emotional Content Analysis\r\n\r\n```r\r\n# Analyze emotions in Pride & Prejudice\r\nnrc_emotions <- tidy_austen %>%\r\n  filter(book == \"Pride & Prejudice\") %>%\r\n  inner_join(get_sentiments(\"nrc\"), by = \"word\") %>%\r\n  filter(!sentiment %in% c(\"positive\", \"negative\")) %>%\r\n  count(sentiment, sort = TRUE)\r\n\r\nnrc_emotions\r\n#>   sentiment     n\r\n#> 1 trust      2017\r\n#> 2 fear       1381\r\n#> 3 anticipation 1279\r\n#> 4 joy        1148\r\n#> 5 sadness    1062\r\n#> 6 anger       863\r\n#> 7 surprise    629\r\n#> 8 disgust     567\r\n\r\n# Plot emotional profile\r\nnrc_emotions %>%\r\n  mutate(sentiment = reorder(sentiment, n)) %>%\r\n  ggplot(aes(n, sentiment)) +\r\n  geom_col() +\r\n  labs(\r\n    title = \"Emotional Profile: Pride & Prejudice\",\r\n    x = \"Word Count\",\r\n    y = \"Emotion (NRC)\"\r\n  )\r\n```\r\n\r\n#### Comparing Lexicon Results\r\n\r\n```r\r\n# Compare lexicons on same data\r\ncomparison <- bind_rows(\r\n  tidy_austen %>%\r\n    inner_join(get_sentiments(\"afinn\"), by = \"word\") %>%\r\n    group_by(book, index = linenumber %/% 80) %>%\r\n    summarize(sentiment = sum(value), .groups = \"drop\") %>%\r\n    mutate(method = \"AFINN\"),\r\n\r\n  tidy_austen %>%\r\n    inner_join(get_sentiments(\"bing\"), by = \"word\") %>%\r\n    count(book, index = linenumber %/% 80, sentiment) %>%\r\n    pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%\r\n    mutate(sentiment = positive - negative, method = \"Bing\") %>%\r\n    select(book, index, sentiment, method)\r\n)\r\n\r\n# Correlation between methods\r\ncomparison %>%\r\n  pivot_wider(names_from = method, values_from = sentiment) %>%\r\n  summarize(correlation = cor(AFINN, Bing, use = \"complete.obs\"))\r\n#> Typically r ~ 0.7-0.8 (lexicons capture similar but not identical signal)\r\n```\r\n\r\n### 2.3 TF-IDF: Term Frequency-Inverse Document Frequency\r\n\r\n**When to use TF-IDF:**\r\n- Finding words that distinguish documents from each other\r\n- Identifying characteristic vocabulary of texts\r\n- Feature engineering for document classification\r\n- Reducing impact of common words without a stopword list\r\n\r\n**How it works:**\r\n- TF (Term Frequency): How often a word appears in a document\r\n- IDF (Inverse Document Frequency): Penalizes words common across all documents\r\n- TF-IDF = TF x IDF: High for words frequent in a document but rare overall\r\n\r\n```r\r\nlibrary(tidytext)\r\nlibrary(dplyr)\r\nlibrary(janeaustenr)\r\n\r\n# Count words per book\r\nbook_words <- austen_books() %>%\r\n  unnest_tokens(word, text) %>%\r\n  count(book, word, sort = TRUE)\r\n\r\n# Calculate total words per book\r\ntotal_words <- book_words %>%\r\n  group_by(book) %>%\r\n  summarize(total = sum(n))\r\n\r\nbook_words <- left_join(book_words, total_words, by = \"book\")\r\n\r\nhead(book_words)\r\n#>   book                word      n  total\r\n#> 1 Mansfield Park      the   6206 160460\r\n#> 2 Mansfield Park      to    5475 160460\r\n#> 3 Mansfield Park      and   5438 160460\r\n#> ...\r\n\r\n# Calculate TF-IDF\r\nbook_tf_idf <- book_words %>%\r\n  bind_tf_idf(word, book, n)\r\n\r\nhead(book_tf_idf)\r\n#>   book           word      n  total     tf   idf tf_idf\r\n#> 1 Mansfield Park the   6206 160460 0.0387 0     0\r\n#> 2 Mansfield Park to    5475 160460 0.0341 0     0\r\n#> ...\r\n#> Common words have idf=0 and tf_idf=0\r\n```\r\n\r\n#### Finding Characteristic Words\r\n\r\n```r\r\n# Top TF-IDF words per book (most distinctive)\r\nbook_tf_idf %>%\r\n  group_by(book) %>%\r\n  slice_max(tf_idf, n = 5) %>%\r\n  ungroup() %>%\r\n  select(book, word, tf_idf)\r\n\r\n#>   book                word       tf_idf\r\n#> 1 Emma                emma       0.00536\r\n#> 2 Emma                weston     0.00433\r\n#> 3 Emma                knightley  0.00396\r\n#> 4 Emma                elton      0.00355\r\n#> 5 Emma                woodhouse  0.00308\r\n#> 6 Mansfield Park      fanny      0.00471\r\n#> 7 Mansfield Park      crawford   0.00410\r\n#> ...\r\n\r\n# Character names emerge as most distinctive!\r\n# This is expected - names are unique to each book\r\n```\r\n\r\n#### Visualizing TF-IDF\r\n\r\n```r\r\nlibrary(ggplot2)\r\n\r\n# Plot top 10 words per book\r\nbook_tf_idf %>%\r\n  group_by(book) %>%\r\n  slice_max(tf_idf, n = 10) %>%\r\n  ungroup() %>%\r\n  mutate(word = reorder_within(word, tf_idf, book)) %>%\r\n  ggplot(aes(tf_idf, word, fill = book)) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~book, scales = \"free_y\") +\r\n  scale_y_reordered() +\r\n  labs(\r\n    title = \"Most Distinctive Words by Book (TF-IDF)\",\r\n    x = \"TF-IDF\",\r\n    y = NULL\r\n  )\r\n```\r\n\r\n#### TF-IDF for Document Classification\r\n\r\n```r\r\n# TF-IDF is often used as features for ML classifiers\r\n# Create a document-term matrix with TF-IDF weights\r\n\r\n# Example: Prepare for classification\r\ndtm_tfidf <- book_tf_idf %>%\r\n  # Keep only top 1000 words by mean TF-IDF\r\n  group_by(word) %>%\r\n  mutate(mean_tfidf = mean(tf_idf)) %>%\r\n  ungroup() %>%\r\n  filter(dense_rank(desc(mean_tfidf)) <= 1000) %>%\r\n  # Pivot to wide format\r\n  select(book, word, tf_idf) %>%\r\n  pivot_wider(names_from = word, values_from = tf_idf, values_fill = 0)\r\n\r\ndim(dtm_tfidf)\r\n#> [1] 6 1001  (6 books x 1000 features + book column)\r\n```\r\n\r\n---\r\n\r\n## 3. Machine Learning for Causal Inference\r\n\r\n### 3.1 Causal Forests\r\n\r\n**When to use causal forests:**\r\n- Estimating heterogeneous treatment effects (HTEs)\r\n- Treatment effects may vary by observed covariates\r\n- Want data-driven discovery of effect heterogeneity\r\n- Large sample sizes available (n > 1000 recommended)\r\n\r\n**Assumptions (same as standard causal inference):**\r\n- Unconfoundedness: No unmeasured confounders\r\n- Overlap: Positive probability of treatment for all X\r\n- SUTVA: No interference between units\r\n\r\n**Key features of GRF:**\r\n- Honest estimation (sample splitting)\r\n- Valid confidence intervals\r\n- Automatic covariate selection for HTE\r\n\r\n```r\r\nlibrary(grf)\r\n\r\n# Simulate data with heterogeneous treatment effects\r\nset.seed(12345)\r\nn <- 2000\r\np <- 10\r\n\r\n# Covariates\r\nX <- matrix(rnorm(n * p), n, p)\r\ncolnames(X) <- paste0(\"X\", 1:p)\r\n\r\n# Treatment assignment (randomized)\r\nW <- rbinom(n, 1, 0.5)\r\n\r\n# True treatment effect varies with X1 and X2\r\n# tau(x) = 1 + 2*X1 + X2 (heterogeneous effect)\r\ntau <- 1 + 2 * X[, 1] + X[, 2]\r\n\r\n# Outcome with heterogeneous effect\r\nY <- X[, 1] + X[, 2] + tau * W + rnorm(n)\r\n\r\n# Summary of true effects\r\nsummary(tau)\r\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.\r\n#>  -4.89   -0.32    1.01    1.00    2.31    6.84\r\n```\r\n\r\n#### Fitting the Causal Forest\r\n\r\n```r\r\n# Fit causal forest\r\ncf <- causal_forest(\r\n  X = X,\r\n  Y = Y,\r\n  W = W,\r\n  num.trees = 2000,\r\n  honesty = TRUE,\r\n  seed = 12345\r\n)\r\n\r\n# Predict individual treatment effects (CATEs)\r\npredictions <- predict(cf, estimate.variance = TRUE)\r\n\r\n# Add predictions to data\r\nresults <- data.frame(\r\n  X,\r\n  W = W,\r\n  Y = Y,\r\n  true_tau = tau,\r\n  estimated_tau = predictions$predictions,\r\n  se = sqrt(predictions$variance.estimates)\r\n)\r\n\r\n# Compare estimated vs true effects\r\ncor(results$true_tau, results$estimated_tau)\r\n#> [1] 0.89  # High correlation indicates good recovery\r\n\r\n# Plot estimated vs true\r\nplot(results$true_tau, results$estimated_tau,\r\n     xlab = \"True Treatment Effect\",\r\n     ylab = \"Estimated Treatment Effect\",\r\n     main = \"Causal Forest: Estimated vs True CATE\")\r\nabline(0, 1, col = \"red\", lwd = 2)\r\n```\r\n\r\n#### Average Treatment Effect (ATE)\r\n\r\n```r\r\n# Estimate ATE with confidence interval\r\nate <- average_treatment_effect(cf, target.sample = \"all\")\r\nprint(ate)\r\n#>   estimate   std.err\r\n#> 1    1.02     0.05\r\n\r\n# 95% CI\r\ncat(sprintf(\"ATE: %.3f [%.3f, %.3f]\\n\",\r\n            ate[1], ate[1] - 1.96*ate[2], ate[1] + 1.96*ate[2]))\r\n#> ATE: 1.020 [0.922, 1.118]\r\n\r\n# True ATE is mean(tau) = 1.0, so well-recovered\r\n```\r\n\r\n#### Variable Importance for Heterogeneity\r\n\r\n```r\r\n# Which variables drive treatment effect heterogeneity?\r\nvar_imp <- variable_importance(cf)\r\nvar_imp_df <- data.frame(\r\n  variable = colnames(X),\r\n  importance = as.numeric(var_imp)\r\n) %>%\r\n  arrange(desc(importance))\r\n\r\nprint(var_imp_df)\r\n#>    variable importance\r\n#> 1        X1      0.412\r\n#> 2        X2      0.198\r\n#> 3        X5      0.082\r\n#> 4        X3      0.075\r\n#> ...\r\n\r\n# X1 and X2 correctly identified as most important\r\n# (they are the true HTE drivers)\r\n\r\n# Visualize\r\nbarplot(var_imp_df$importance,\r\n        names.arg = var_imp_df$variable,\r\n        main = \"Variable Importance for Treatment Effect Heterogeneity\",\r\n        ylab = \"Importance\")\r\n```\r\n\r\n#### Calibration Test\r\n\r\n```r\r\n# Test whether there is meaningful heterogeneity\r\ntest_calibration(cf)\r\n#> Best linear fit using forest predictions (on held-out data):\r\n#>\r\n#>                               Estimate Std. Error t value  Pr(>|t|)\r\n#> mean.forest.prediction         1.005     0.048    21.02   < 2e-16 ***\r\n#> differential.forest.prediction 0.987     0.052    19.13   < 2e-16 ***\r\n#>\r\n#> Interpretation:\r\n#> - mean.forest.prediction ~ 1: ATE correctly estimated\r\n#> - differential.forest.prediction ~ 1: Heterogeneity correctly captured\r\n```\r\n\r\n#### Examining Conditional Effects\r\n\r\n```r\r\n# Predict effects at specific covariate values\r\nX_test <- matrix(0, 5, p)\r\nX_test[, 1] <- c(-2, -1, 0, 1, 2)  # Vary X1\r\ncolnames(X_test) <- colnames(X)\r\n\r\npredictions_test <- predict(cf, X_test, estimate.variance = TRUE)\r\n\r\n# True effect: tau = 1 + 2*X1 + X2, with X2=0\r\ntrue_test <- 1 + 2 * X_test[, 1]\r\n\r\ndata.frame(\r\n  X1 = X_test[, 1],\r\n  true_tau = true_test,\r\n  estimated_tau = predictions_test$predictions,\r\n  se = sqrt(predictions_test$variance.estimates)\r\n)\r\n#>   X1 true_tau estimated_tau    se\r\n#> 1 -2       -3         -2.85  0.18\r\n#> 2 -1       -1         -0.92  0.12\r\n#> 3  0        1          1.05  0.09\r\n#> 4  1        3          2.98  0.12\r\n#> 5  2        5          4.91  0.17\r\n```\r\n\r\n---\r\n\r\n### 3.2 Random Forests\r\n\r\n**When to use random forests:**\r\n- Prediction is primary goal (not causal inference)\r\n- Non-linear relationships and interactions expected\r\n- Variable importance ranking needed\r\n- Robust to outliers and scale differences\r\n\r\n**Advantages:**\r\n- No assumptions about functional form\r\n- Handles high-dimensional data well\r\n- Built-in cross-validation (OOB error)\r\n- Feature importance measures\r\n\r\n```r\r\nlibrary(ranger)\r\n\r\n# Use mtcars for regression example\r\ndata(mtcars)\r\nstr(mtcars)\r\n#> 32 obs, 11 variables\r\n#> mpg: miles per gallon (outcome)\r\n#> predictors: cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb\r\n\r\n# Split into training and test sets\r\nset.seed(12345)\r\ntrain_idx <- sample(1:nrow(mtcars), 24)\r\ntrain_df <- mtcars[train_idx, ]\r\ntest_df <- mtcars[-train_idx, ]\r\n```\r\n\r\n#### Regression Random Forest\r\n\r\n```r\r\n# Fit random forest for mpg prediction\r\nrf_reg <- ranger(\r\n  mpg ~ .,\r\n  data = train_df,\r\n  num.trees = 500,\r\n  importance = \"impurity\",\r\n  seed = 12345\r\n)\r\n\r\nprint(rf_reg)\r\n#> Ranger result\r\n#> Type:                             Regression\r\n#> Number of trees:                  500\r\n#> Sample size:                      24\r\n#> Number of independent variables:  10\r\n#> Mtry:                             3\r\n#> Target node size:                 5\r\n#> OOB prediction error (MSE):       6.82\r\n#> R squared (OOB):                  0.81\r\n\r\n# Predict on test set\r\npredictions <- predict(rf_reg, data = test_df)$predictions\r\n\r\n# Evaluate\r\nrmse <- sqrt(mean((predictions - test_df$mpg)^2))\r\ncat(sprintf(\"Test RMSE: %.2f\\n\", rmse))\r\n#> Test RMSE: 2.45\r\n\r\n# Compare predicted vs actual\r\ndata.frame(\r\n  actual = test_df$mpg,\r\n  predicted = round(predictions, 1)\r\n)\r\n```\r\n\r\n#### Variable Importance\r\n\r\n```r\r\n# Extract importance scores\r\nimportance_scores <- rf_reg$variable.importance\r\nimportance_df <- data.frame(\r\n  variable = names(importance_scores),\r\n  importance = importance_scores\r\n) %>%\r\n  arrange(desc(importance))\r\n\r\nprint(importance_df)\r\n#>    variable importance\r\n#> 1       wt      131.5\r\n#> 2      cyl       77.3\r\n#> 3     disp       69.8\r\n#> 4       hp       48.2\r\n#> 5     drat       15.1\r\n#> ...\r\n\r\n# Visualize\r\nbarplot(importance_df$importance,\r\n        names.arg = importance_df$variable,\r\n        las = 2,\r\n        main = \"Variable Importance for MPG Prediction\",\r\n        ylab = \"Importance (Node Impurity)\")\r\n```\r\n\r\n#### Classification Random Forest\r\n\r\n```r\r\n# Use iris for classification\r\ndata(iris)\r\nset.seed(12345)\r\ntrain_idx <- sample(1:nrow(iris), 100)\r\ntrain_iris <- iris[train_idx, ]\r\ntest_iris <- iris[-train_idx, ]\r\n\r\n# Fit classification forest\r\nrf_class <- ranger(\r\n  Species ~ .,\r\n  data = train_iris,\r\n  num.trees = 500,\r\n  importance = \"impurity\",\r\n  probability = TRUE,  # Return class probabilities\r\n  seed = 12345\r\n)\r\n\r\nprint(rf_class)\r\n#> OOB prediction error: 4.00%\r\n\r\n# Predict class probabilities\r\nprobs <- predict(rf_class, data = test_iris)$predictions\r\nhead(round(probs, 3))\r\n#>      setosa versicolor virginica\r\n#> [1,]  0.998      0.002     0.000\r\n#> [2,]  0.000      0.892     0.108\r\n#> ...\r\n\r\n# Predict classes\r\npred_class <- colnames(probs)[apply(probs, 1, which.max)]\r\n\r\n# Confusion matrix\r\ntable(Predicted = pred_class, Actual = test_iris$Species)\r\n#>             Actual\r\n#> Predicted    setosa versicolor virginica\r\n#>   setosa         17          0         0\r\n#>   versicolor      0         16         1\r\n#>   virginica       0          1        15\r\n\r\n# Accuracy\r\nmean(pred_class == test_iris$Species)\r\n#> [1] 0.96\r\n```\r\n\r\n#### Tuning Random Forest Parameters\r\n\r\n```r\r\n# Key parameters to tune:\r\n# - num.trees: More is generally better (diminishing returns after ~500)\r\n# - mtry: Number of variables to try at each split\r\n#         Default: sqrt(p) for classification, p/3 for regression\r\n# - min.node.size: Minimum observations in terminal node\r\n\r\n# Grid search example\r\ntune_grid <- expand.grid(\r\n  mtry = c(2, 3, 4),\r\n  min.node.size = c(1, 3, 5)\r\n)\r\n\r\nresults <- lapply(1:nrow(tune_grid), function(i) {\r\n  rf <- ranger(\r\n    mpg ~ .,\r\n    data = train_df,\r\n    num.trees = 500,\r\n    mtry = tune_grid$mtry[i],\r\n    min.node.size = tune_grid$min.node.size[i],\r\n    seed = 12345\r\n  )\r\n  data.frame(\r\n    mtry = tune_grid$mtry[i],\r\n    min.node.size = tune_grid$min.node.size[i],\r\n    oob_mse = rf$prediction.error\r\n  )\r\n})\r\n\r\ntune_results <- do.call(rbind, results)\r\ntune_results[order(tune_results$oob_mse), ]\r\n#>   mtry min.node.size oob_mse\r\n#> 4    2             3    6.21  <- Best\r\n#> 1    2             1    6.45\r\n#> 7    2             5    6.58\r\n#> ...\r\n```\r\n\r\n---\r\n\r\n## 4. Classification Methods\r\n\r\n### 4.1 Support Vector Machines (SVM)\r\n\r\n**When to use SVM:**\r\n- Binary or multi-class classification\r\n- High-dimensional data (p can exceed n)\r\n- Clear margin of separation expected\r\n- Need non-linear decision boundaries (with kernels)\r\n\r\n**Key concepts:**\r\n- Maximizes margin between classes\r\n- Uses kernel trick for non-linear boundaries\r\n- C parameter controls bias-variance tradeoff\r\n- Must scale features for optimal performance\r\n\r\n```r\r\nlibrary(caret)\r\nlibrary(e1071)\r\n\r\n# Prepare iris data for binary classification\r\ndata(iris)\r\n# Focus on distinguishing versicolor vs virginica (harder problem)\r\niris_binary <- iris[iris$Species != \"setosa\", ]\r\niris_binary$Species <- factor(iris_binary$Species)\r\n\r\nset.seed(12345)\r\ntrain_idx <- createDataPartition(iris_binary$Species, p = 0.7, list = FALSE)\r\ntrain_data <- iris_binary[train_idx, ]\r\ntest_data <- iris_binary[-train_idx, ]\r\n\r\n# Scale predictors (important for SVM)\r\npreproc <- preProcess(train_data[, 1:4], method = c(\"center\", \"scale\"))\r\ntrain_scaled <- predict(preproc, train_data)\r\ntest_scaled <- predict(preproc, test_data)\r\n```\r\n\r\n#### Linear SVM\r\n\r\n```r\r\n# Train linear SVM with cross-validation\r\nctrl <- trainControl(\r\n  method = \"repeatedcv\",\r\n  number = 10,\r\n  repeats = 3,\r\n  classProbs = TRUE,\r\n  summaryFunction = twoClassSummary\r\n)\r\n\r\nset.seed(12345)\r\nsvm_linear <- train(\r\n  Species ~ .,\r\n  data = train_scaled,\r\n  method = \"svmLinear\",\r\n  trControl = ctrl,\r\n  metric = \"ROC\",\r\n  tuneGrid = expand.grid(C = c(0.01, 0.1, 1, 10))\r\n)\r\n\r\nprint(svm_linear)\r\n#> Support Vector Machines with Linear Kernel\r\n#> 70 samples, 4 predictors, 2 classes\r\n#>\r\n#> Resampling: Cross-Validated (10 fold, repeated 3 times)\r\n#>\r\n#>   C     ROC    Sens   Spec\r\n#>   0.01  0.971  0.914  0.943\r\n#>   0.10  0.980  0.943  0.957\r\n#>   1.00  0.979  0.943  0.943  <- Selected\r\n#>   10.00 0.974  0.929  0.929\r\n\r\n# Best C value\r\nsvm_linear$bestTune\r\n#>   C\r\n#> 3 1\r\n\r\n# Test set predictions\r\npred_linear <- predict(svm_linear, test_scaled)\r\nconfusionMatrix(pred_linear, test_scaled$Species)\r\n#> Accuracy: 0.933\r\n#> Sensitivity: 0.933\r\n#> Specificity: 0.933\r\n```\r\n\r\n#### Radial Basis Function (RBF) Kernel\r\n\r\n```r\r\n# RBF kernel with tuning\r\nset.seed(12345)\r\nsvm_rbf <- train(\r\n  Species ~ .,\r\n  data = train_scaled,\r\n  method = \"svmRadial\",\r\n  trControl = ctrl,\r\n  metric = \"ROC\",\r\n  tuneLength = 5  # Auto-generate 5 values for sigma and C\r\n)\r\n\r\nprint(svm_rbf)\r\n#>   sigma  C     ROC    Sens   Spec\r\n#>   0.25   0.25  0.969  0.914  0.943\r\n#>   0.25   0.50  0.976  0.929  0.957\r\n#>   0.25   1.00  0.980  0.943  0.957  <- Selected\r\n#>   0.25   2.00  0.979  0.943  0.943\r\n#>   0.25   4.00  0.974  0.929  0.929\r\n\r\n# Compare kernels\r\npred_rbf <- predict(svm_rbf, test_scaled)\r\nconfusionMatrix(pred_rbf, test_scaled$Species)\r\n#> Accuracy: 0.967 (slightly better than linear)\r\n```\r\n\r\n#### Interpreting SVM Results\r\n\r\n```r\r\n# Get class probabilities\r\nprobs <- predict(svm_rbf, test_scaled, type = \"prob\")\r\nhead(probs)\r\n#>   versicolor virginica\r\n#> 1      0.892     0.108\r\n#> 2      0.034     0.966\r\n#> ...\r\n\r\n# ROC curve\r\nlibrary(pROC)\r\nroc_obj <- roc(test_scaled$Species, probs$virginica)\r\nplot(roc_obj, main = sprintf(\"SVM ROC Curve (AUC = %.3f)\", auc(roc_obj)))\r\n#> AUC: ~0.98\r\n```\r\n\r\n---\r\n\r\n### 4.2 Cross-Validation Framework\r\n\r\n**Types of cross-validation:**\r\n\r\n| Method | Description | Use when |\r\n|--------|-------------|----------|\r\n| k-fold | Split into k parts, rotate test set | Standard choice |\r\n| Repeated k-fold | Multiple k-fold runs | Reduce variance in estimates |\r\n| LOOCV | Leave-one-out | Small datasets |\r\n| Stratified | Maintain class proportions | Imbalanced classes |\r\n| Time series | Preserve temporal order | Time-dependent data |\r\n\r\n```r\r\nlibrary(caret)\r\nlibrary(mlbench)\r\n\r\n# Load Pima Indians Diabetes dataset\r\ndata(PimaIndiansDiabetes2)\r\npima <- na.omit(PimaIndiansDiabetes2)  # Remove NAs\r\nstr(pima)\r\n#> 392 obs, 9 variables\r\n#> diabetes: outcome (pos/neg)\r\n\r\n# Check class balance\r\ntable(pima$diabetes)\r\n#>  neg  pos\r\n#>  262  130  (imbalanced: 67% negative)\r\n```\r\n\r\n#### Basic k-fold Cross-Validation\r\n\r\n```r\r\n# 10-fold CV\r\nctrl_kfold <- trainControl(\r\n  method = \"cv\",\r\n  number = 10,\r\n  classProbs = TRUE,\r\n  summaryFunction = twoClassSummary,\r\n  savePredictions = \"final\"\r\n)\r\n\r\n# Train logistic regression\r\nset.seed(12345)\r\nmodel_kfold <- train(\r\n  diabetes ~ .,\r\n  data = pima,\r\n  method = \"glm\",\r\n  family = \"binomial\",\r\n  trControl = ctrl_kfold,\r\n  metric = \"ROC\"\r\n)\r\n\r\nprint(model_kfold)\r\n#> Generalized Linear Model\r\n#> 392 samples, 8 predictors, 2 classes\r\n#>\r\n#> Resampling: Cross-Validated (10 fold)\r\n#> Summary of sample sizes: 352, 353, 353, 353, ...\r\n#>\r\n#> ROC        Sens       Spec\r\n#> 0.838      0.859      0.654\r\n```\r\n\r\n#### Repeated k-fold for Variance Reduction\r\n\r\n```r\r\n# 10-fold CV repeated 5 times\r\nctrl_repeated <- trainControl(\r\n  method = \"repeatedcv\",\r\n  number = 10,\r\n  repeats = 5,\r\n  classProbs = TRUE,\r\n  summaryFunction = twoClassSummary\r\n)\r\n\r\nset.seed(12345)\r\nmodel_repeated <- train(\r\n  diabetes ~ .,\r\n  data = pima,\r\n  method = \"glm\",\r\n  family = \"binomial\",\r\n  trControl = ctrl_repeated,\r\n  metric = \"ROC\"\r\n)\r\n\r\n# More stable estimates (averaged over 50 folds)\r\nprint(model_repeated)\r\n#> ROC        Sens       Spec\r\n#> 0.835      0.855      0.661\r\n#> (SE reduced compared to single 10-fold)\r\n```\r\n\r\n#### Stratified Sampling for Imbalanced Data\r\n\r\n```r\r\n# Ensure class proportions maintained in each fold\r\nctrl_stratified <- trainControl(\r\n  method = \"cv\",\r\n  number = 10,\r\n  classProbs = TRUE,\r\n  summaryFunction = twoClassSummary,\r\n  sampling = \"up\"  # Upsample minority class within folds\r\n)\r\n\r\nset.seed(12345)\r\nmodel_stratified <- train(\r\n  diabetes ~ .,\r\n  data = pima,\r\n  method = \"glm\",\r\n  family = \"binomial\",\r\n  trControl = ctrl_stratified,\r\n  metric = \"ROC\"\r\n)\r\n\r\nprint(model_stratified)\r\n#> Sensitivity improved (better at detecting positive cases)\r\n```\r\n\r\n#### Comparing Multiple Models\r\n\r\n```r\r\n# Compare logistic regression, random forest, and SVM\r\nset.seed(12345)\r\nmodel_glm <- train(diabetes ~ ., data = pima, method = \"glm\",\r\n                   family = \"binomial\", trControl = ctrl_repeated, metric = \"ROC\")\r\n\r\nset.seed(12345)\r\nmodel_rf <- train(diabetes ~ ., data = pima, method = \"ranger\",\r\n                  trControl = ctrl_repeated, metric = \"ROC\")\r\n\r\nset.seed(12345)\r\nmodel_svm <- train(diabetes ~ ., data = pima, method = \"svmRadial\",\r\n                   trControl = ctrl_repeated, metric = \"ROC\",\r\n                   preProcess = c(\"center\", \"scale\"))\r\n\r\n# Compare results\r\nresults <- resamples(list(\r\n  Logistic = model_glm,\r\n  RandomForest = model_rf,\r\n  SVM = model_svm\r\n))\r\n\r\nsummary(results)\r\n#>              ROC\r\n#>              Mean    SD\r\n#> Logistic     0.835   0.06\r\n#> RandomForest 0.827   0.07\r\n#> SVM          0.829   0.06\r\n\r\n# Visualize comparison\r\nbwplot(results, metric = \"ROC\")\r\n\r\n# Statistical test for differences\r\ndiff_results <- diff(results)\r\nsummary(diff_results)\r\n#> p-values for pairwise comparisons\r\n```\r\n\r\n#### Nested Cross-Validation (Avoiding Data Leakage)\r\n\r\n```r\r\n# Outer loop: Evaluate model performance\r\n# Inner loop: Tune hyperparameters\r\n\r\n# Inner CV for tuning\r\ninner_ctrl <- trainControl(\r\n  method = \"cv\",\r\n  number = 5,\r\n  classProbs = TRUE,\r\n  summaryFunction = twoClassSummary\r\n)\r\n\r\n# Outer CV for evaluation\r\nouter_ctrl <- trainControl(\r\n  method = \"cv\",\r\n  number = 10,\r\n  classProbs = TRUE,\r\n  summaryFunction = twoClassSummary,\r\n  savePredictions = \"final\"\r\n)\r\n\r\n# This avoids optimistic bias from tuning on same data used for evaluation\r\nset.seed(12345)\r\nnested_model <- train(\r\n  diabetes ~ .,\r\n  data = pima,\r\n  method = \"svmRadial\",\r\n  trControl = outer_ctrl,\r\n  tuneLength = 5,\r\n  preProcess = c(\"center\", \"scale\"),\r\n  metric = \"ROC\"\r\n)\r\n```\r\n\r\n---\r\n\r\n## 5. Nonparametric Regression\r\n\r\n### 5.1 Generalized Additive Models (GAMs)\r\n\r\n**When to use GAMs:**\r\n- Relationships may be non-linear but smoothly varying\r\n- Want interpretable effects (not black box)\r\n- Need confidence intervals and hypothesis tests\r\n- Partial dependence on individual predictors matters\r\n\r\n**Advantages over linear models:**\r\n- Automatically discover non-linear relationships\r\n- No need to specify polynomial degree\r\n- Penalties prevent overfitting\r\n- Uncertainty quantification built in\r\n\r\n```r\r\nlibrary(mgcv)\r\n\r\n# Simulate data with known smooth functions\r\nset.seed(12345)\r\nsim_data <- gamSim(1, n = 400, dist = \"normal\", scale = 2, verbose = FALSE)\r\n\r\n# gamSim(1) creates:\r\n# y = f0(x0) + f1(x1) + f2(x2) + f3(x3) + noise\r\n# where f0-f3 are known smooth functions\r\n\r\nhead(sim_data)\r\n#>          y        x0        x1        x2        x3\r\n#> 1  7.23142  0.515619  0.327731  0.651753  0.893749\r\n#> 2  5.12878  0.276391  0.721562  0.512698  0.298461\r\n#> ...\r\n```\r\n\r\n#### Basic GAM with Smooth Terms\r\n\r\n```r\r\n# Fit GAM with smooth terms for each predictor\r\ngam_model <- gam(\r\n  y ~ s(x0) + s(x1) + s(x2) + s(x3),\r\n  data = sim_data,\r\n  method = \"REML\"  # Restricted maximum likelihood for smoothing\r\n)\r\n\r\nsummary(gam_model)\r\n#> Family: gaussian\r\n#> Link function: identity\r\n#>\r\n#> Parametric coefficients:\r\n#>             Estimate Std. Error t value Pr(>|t|)\r\n#> (Intercept)   7.832      0.100    78.3   <2e-16 ***\r\n#>\r\n#> Approximate significance of smooth terms:\r\n#>         edf Ref.df     F p-value\r\n#> s(x0) 3.72   4.57  33.1  <2e-16 ***\r\n#> s(x1) 2.41   3.01  71.2  <2e-16 ***\r\n#> s(x2) 7.83   8.58  78.5  <2e-16 ***\r\n#> s(x3) 1.00   1.00   0.3    0.59     <- Linear (no smooth needed)\r\n#>\r\n#> R-sq.(adj) = 0.72   Deviance explained = 73.1%\r\n#> -REML = 879.5   Scale est. = 3.98    n = 400\r\n\r\n# Interpretation:\r\n# - edf (effective degrees of freedom): Higher = more wiggly\r\n# - edf = 1 suggests linear relationship\r\n# - s(x3) not significant: could simplify model\r\n```\r\n\r\n#### Visualizing Smooth Terms\r\n\r\n```r\r\n# Plot smooth functions with confidence bands\r\npar(mfrow = c(2, 2))\r\nplot(gam_model, shade = TRUE, pages = 0)\r\n\r\n# Each plot shows:\r\n# - Estimated smooth function (solid line)\r\n# - 95% confidence band (shaded)\r\n# - Rug plot showing data density\r\n\r\n# Compare to true functions\r\n# s(x0): Should show sine-like curve\r\n# s(x1): Should show exponential-like increase\r\n# s(x2): Should show bimodal pattern\r\n# s(x3): Should be nearly flat (linear)\r\n```\r\n\r\n#### Model Diagnostics\r\n\r\n```r\r\n# Check model assumptions\r\npar(mfrow = c(2, 2))\r\ngam.check(gam_model)\r\n\r\n# Output includes:\r\n# 1. QQ plot of residuals (should be linear)\r\n# 2. Residuals vs fitted (should be random scatter)\r\n# 3. Histogram of residuals (should be normal)\r\n# 4. Response vs fitted (should be linear)\r\n\r\n# Also prints basis dimension checks:\r\n# k-index: If < 1, may need more basis functions (higher k)\r\n# p-value: If significant, k may be too low\r\n```\r\n\r\n#### Model Comparison and Selection\r\n\r\n```r\r\n# Compare nested models using likelihood ratio test\r\ngam_full <- gam(y ~ s(x0) + s(x1) + s(x2) + s(x3), data = sim_data, method = \"REML\")\r\ngam_reduced <- gam(y ~ s(x0) + s(x1) + s(x2), data = sim_data, method = \"REML\")\r\n\r\nanova(gam_reduced, gam_full, test = \"F\")\r\n#> Analysis of Deviance Table\r\n#>\r\n#> Model 1: y ~ s(x0) + s(x1) + s(x2)\r\n#> Model 2: y ~ s(x0) + s(x1) + s(x2) + s(x3)\r\n#>   Resid. Df Resid. Dev     Df Deviance     F  Pr(>F)\r\n#> 1    385.1     1583.2\r\n#> 2    384.1     1582.0   1.00    1.189  0.30    0.59\r\n#>\r\n#> Not significant: x3 can be dropped\r\n\r\n# Compare models using AIC\r\nAIC(gam_full, gam_reduced)\r\n#>              df      AIC\r\n#> gam_full     16.0   1794.1\r\n#> gam_reduced  15.0   1792.9  <- Lower AIC, simpler model preferred\r\n```\r\n\r\n#### Tensor Product Smooths for Interactions\r\n\r\n```r\r\n# Model interaction between two continuous predictors\r\ngam_tensor <- gam(\r\n  y ~ te(x0, x1) + s(x2),  # Tensor product smooth for x0*x1\r\n  data = sim_data,\r\n  method = \"REML\"\r\n)\r\n\r\nsummary(gam_tensor)\r\n\r\n# Visualize interaction surface\r\nvis.gam(gam_tensor,\r\n        view = c(\"x0\", \"x1\"),\r\n        plot.type = \"contour\",\r\n        main = \"Interaction: x0 and x1\")\r\n\r\n# Or 3D perspective plot\r\nvis.gam(gam_tensor,\r\n        view = c(\"x0\", \"x1\"),\r\n        plot.type = \"persp\",\r\n        theta = 45,\r\n        phi = 30)\r\n```\r\n\r\n#### GAM with Binary Outcome\r\n\r\n```r\r\n# Simulate binary outcome\r\nset.seed(12345)\r\nn <- 500\r\nx <- runif(n, 0, 10)\r\nprob <- plogis(-2 + 0.5 * sin(x))  # Non-linear probability\r\ny <- rbinom(n, 1, prob)\r\nbinary_data <- data.frame(x = x, y = y)\r\n\r\n# Fit logistic GAM\r\ngam_logit <- gam(\r\n  y ~ s(x),\r\n  family = binomial(link = \"logit\"),\r\n  data = binary_data,\r\n  method = \"REML\"\r\n)\r\n\r\nsummary(gam_logit)\r\n\r\n# Plot on probability scale\r\nplot(gam_logit, trans = plogis, shift = coef(gam_logit)[1],\r\n     shade = TRUE, rug = FALSE,\r\n     ylab = \"Probability\",\r\n     main = \"GAM: Estimated Probability Function\")\r\n```\r\n\r\n#### Key GAM Options\r\n\r\n| Argument | Options | Use |\r\n|----------|---------|-----|\r\n| `s()` | Smooth term | Single predictor smooth |\r\n| `te()` | Tensor product | Interaction of 2+ predictors |\r\n| `ti()` | Tensor interaction | Pure interaction (no main effects) |\r\n| `bs` | `\"tp\"`, `\"cr\"`, `\"cs\"` | Basis type (thin plate, cubic regression) |\r\n| `k` | Integer | Maximum basis dimension |\r\n| `method` | `\"REML\"`, `\"GCV.Cp\"` | Smoothing parameter selection |\r\n| `family` | `gaussian()`, `binomial()`, etc. | Response distribution |\r\n\r\n---\r\n\r\n## 6. Measurement & Scale Construction\r\n\r\nMethods for developing and validating psychological and social measures.\r\n\r\n### 6.1 Exploratory Factor Analysis (EFA)\r\n\r\n**When to use EFA:**\r\n- Developing a new scale with unknown structure\r\n- Exploring dimensionality of item sets\r\n- No strong theory about factor structure\r\n- Reducing many items to fewer latent constructs\r\n\r\n**Key decisions:**\r\n- Number of factors (parallel analysis, scree plot)\r\n- Extraction method (principal axis, maximum likelihood)\r\n- Rotation (orthogonal vs oblique)\r\n- Item retention criteria\r\n\r\n```r\r\nlibrary(psych)\r\n\r\n# Load Big Five Inventory dataset\r\ndata(bfi)\r\nstr(bfi)\r\n#> 2,800 observations, 28 variables\r\n#> 25 personality items (A1-A5, C1-C5, E1-E5, N1-N5, O1-O5)\r\n#> 3 demographic variables (gender, education, age)\r\n\r\n# Extract just the 25 personality items\r\nitems <- bfi[, 1:25]\r\n\r\n# Check for missing data\r\ncolSums(is.na(items))\r\n#> Some missingness; use complete cases for demonstration\r\nitems_complete <- na.omit(items)\r\nnrow(items_complete)\r\n#> 2,436 complete cases\r\n```\r\n\r\n#### Step 1: Determine Number of Factors\r\n\r\n```r\r\n# Parallel analysis (gold standard for factor number)\r\nset.seed(12345)\r\nfa.parallel(items_complete, fa = \"fa\", n.iter = 100)\r\n\r\n# Output plot shows:\r\n# - Actual eigenvalues (solid line)\r\n# - Simulated eigenvalues from random data (dashed)\r\n# - Factors to retain: Where actual > simulated\r\n\r\n# Text output:\r\n#> Parallel analysis suggests that the number of factors = 6\r\n#> (actual eigenvalues: 5.2, 2.8, 2.1, 1.5, 1.2, 1.0)\r\n\r\n# Also check Kaiser criterion (eigenvalues > 1) and scree plot\r\n# But parallel analysis is more accurate\r\n```\r\n\r\n#### Step 2: Check Sampling Adequacy\r\n\r\n```r\r\n# Correlation matrix\r\nR <- cor(items_complete, use = \"pairwise.complete.obs\")\r\n\r\n# Kaiser-Meyer-Olkin (KMO) measure\r\nKMO(R)\r\n#> Overall MSA = 0.85\r\n#> Interpretation: > 0.80 is \"meritorious\"\r\n#>                > 0.90 is \"marvelous\"\r\n#>                < 0.50 is unacceptable\r\n\r\n# Bartlett's test of sphericity\r\ncortest.bartlett(R, n = nrow(items_complete))\r\n#> chi-square = 17523.1, df = 300, p < 0.001\r\n#> Significant: Correlations are not all zero\r\n```\r\n\r\n#### Step 3: Extract Factors\r\n\r\n```r\r\n# Principal axis factoring with oblimin rotation\r\n# (oblique rotation allows factors to correlate)\r\nset.seed(12345)\r\nfa_result <- fa(\r\n  items_complete,\r\n  nfactors = 5,       # Big Five theory suggests 5\r\n  rotate = \"oblimin\",\r\n  fm = \"pa\",          # Principal axis factoring\r\n  scores = \"regression\"\r\n)\r\n\r\nprint(fa_result, cut = 0.3, sort = TRUE)\r\n#>\r\n#> Standardized loadings (pattern matrix):\r\n#>      PA1   PA2   PA3   PA4   PA5   h2    u2   com\r\n#> N1  0.81                           0.67  0.33 1.0\r\n#> N2  0.78                           0.63  0.37 1.1\r\n#> N3  0.71                           0.53  0.47 1.1\r\n#> N4  0.62                           0.46  0.54 1.3\r\n#> N5  0.52                           0.36  0.64 1.5\r\n#> E1        0.58                     0.42  0.58 1.3\r\n#> E2        0.72                     0.57  0.43 1.1\r\n#> E3        0.48                     0.44  0.56 1.8\r\n#> ...\r\n#>\r\n#> Factor correlations:\r\n#>      PA1   PA2   PA3   PA4   PA5\r\n#> PA1  1.00\r\n#> PA2 -0.22  1.00\r\n#> PA3  0.28 -0.35  1.00\r\n#> PA4 -0.09  0.19 -0.12  1.00\r\n#> PA5 -0.07  0.17 -0.13  0.11  1.00\r\n\r\n# h2 = communality (variance explained by factors)\r\n# u2 = uniqueness (1 - h2)\r\n# Items with h2 < 0.20 may be problematic\r\n```\r\n\r\n#### Step 4: Evaluate Solution\r\n\r\n```r\r\n# Fit indices\r\nfa_result$RMSEA\r\n#> [1] 0.052  # < 0.06 is good fit\r\n\r\nfa_result$TLI\r\n#> [1] 0.89   # > 0.90 is good fit\r\n\r\n# Factor loadings criteria:\r\n# - Primary loading > 0.40 (ideally > 0.50)\r\n# - Cross-loadings < 0.30\r\n# - Communality > 0.20\r\n\r\n# Identify problematic items\r\n# Low communality\r\nwhich(fa_result$communalities < 0.20)\r\n# Cross-loading items (loading > 0.30 on multiple factors)\r\n# Check pattern matrix manually\r\n```\r\n\r\n#### Step 5: Internal Consistency\r\n\r\n```r\r\n# Cronbach's alpha for each factor\r\n# First, identify items per factor based on loadings\r\nneuroticism <- c(\"N1\", \"N2\", \"N3\", \"N4\", \"N5\")\r\nextraversion <- c(\"E1\", \"E2\", \"E3\", \"E4\", \"E5\")\r\nopenness <- c(\"O1\", \"O2\", \"O3\", \"O4\", \"O5\")\r\nagreeableness <- c(\"A1\", \"A2\", \"A3\", \"A4\", \"A5\")\r\nconscientiousness <- c(\"C1\", \"C2\", \"C3\", \"C4\", \"C5\")\r\n\r\n# Calculate alpha for each scale\r\nalpha(items_complete[, neuroticism])\r\n#> raw_alpha = 0.81 (good)\r\n\r\nalpha(items_complete[, extraversion])\r\n#> raw_alpha = 0.76 (acceptable)\r\n\r\nalpha(items_complete[, openness])\r\n#> raw_alpha = 0.60 (questionable - some items may need revision)\r\n\r\n# Full alpha output includes:\r\n# - raw_alpha: Cronbach's alpha\r\n# - std.alpha: Standardized alpha\r\n# - alpha if item dropped: Identifies problematic items\r\n```\r\n\r\n#### Computing Factor Scores\r\n\r\n```r\r\n# Factor scores are in fa_result$scores\r\nfactor_scores <- fa_result$scores\r\nhead(factor_scores)\r\n#>        PA1     PA2     PA3     PA4     PA5\r\n#> 1  -0.523   0.124  -0.892   0.341   0.156\r\n#> 2   1.234  -0.456   0.123  -0.234   0.567\r\n#> ...\r\n\r\n# Add to original data\r\nitems_complete$N_score <- factor_scores[, \"PA1\"]\r\nitems_complete$E_score <- factor_scores[, \"PA2\"]\r\n# etc.\r\n```\r\n\r\n#### Visualizing Factor Structure\r\n\r\n```r\r\n# Create factor loading diagram\r\nfa.diagram(fa_result, main = \"Big Five Factor Structure\")\r\n\r\n# Shows:\r\n# - Items connected to their primary factor\r\n# - Loading values on paths\r\n# - Factor correlations (curved arrows)\r\n\r\n# For publication-quality output\r\npdf(\"factor_diagram.pdf\", width = 10, height = 8)\r\nfa.diagram(fa_result,\r\n           cut = 0.3,      # Only show loadings > 0.3\r\n           digits = 2,\r\n           main = \"Five-Factor Solution\")\r\ndev.off()\r\n```\r\n\r\n#### Omega: Better Reliability Than Alpha\r\n\r\nCronbach's alpha assumes all items have equal loadings (tau-equivalence). McDonald's omega relaxes this assumption:\r\n\r\n```r\r\n# Calculate omega for a single scale\r\nneuroticism_items <- items_complete[, c(\"N1\", \"N2\", \"N3\", \"N4\", \"N5\")]\r\n\r\n# omega() fits a factor model and computes reliability\r\nomega_result <- omega(neuroticism_items, nfactors = 1, plot = FALSE)\r\n\r\n# Key outputs\r\ncat(\"Omega total:\", round(omega_result$omega.tot, 3), \"\\n\")\r\n#> Omega total: 0.822\r\n\r\n# For comparison\r\ncat(\"Alpha:\", round(omega_result$alpha, 3), \"\\n\")\r\n#> Alpha: 0.817\r\n\r\n# omega.tot >= alpha always (omega is more accurate)\r\n# Large difference suggests unequal loadings\r\n\r\n# For multidimensional scales, use omega on full item set\r\nomega_full <- omega(items_complete[, 1:25], nfactors = 5, plot = FALSE)\r\ncat(\"Omega hierarchical:\", round(omega_full$omega_h, 3), \"\\n\")\r\n#> omega_h: Reliability of general factor\r\n#> omega_t: Total reliable variance\r\n```\r\n\r\n#### Scale Scoring with scoreItems()\r\n\r\n```r\r\n# Define scales by item names\r\nkeys <- list(\r\n  neuroticism = c(\"N1\", \"N2\", \"N3\", \"N4\", \"N5\"),\r\n  extraversion = c(\"-E1\", \"-E2\", \"E3\", \"E4\", \"E5\"),  # Note: E1, E2 reversed\r\n  openness = c(\"O1\", \"-O2\", \"O3\", \"O4\", \"-O5\"),\r\n  agreeableness = c(\"-A1\", \"A2\", \"A3\", \"A4\", \"A5\"),\r\n  conscientiousness = c(\"C1\", \"C2\", \"C3\", \"-C4\", \"-C5\")\r\n)\r\n\r\n# Score all scales at once\r\nscores <- scoreItems(keys, items_complete[, 1:25])\r\n\r\n# View reliabilities\r\nprint(scores$alpha)\r\n#>      neuroticism extraversion openness agreeableness conscientiousness\r\n#> alpha       0.82         0.76     0.60          0.70              0.73\r\n\r\n# Get scale scores\r\nscale_scores <- scores$scores\r\nhead(scale_scores)\r\n#>      neuroticism extraversion openness agreeableness conscientiousness\r\n#> [1,]        3.40         3.60     3.80          4.00              3.80\r\n#> [2,]        2.60         2.80     3.20          3.40              4.20\r\n```\r\n\r\n---\r\n\r\n### 6.2 Confirmatory Factor Analysis (CFA)\r\n\r\n**When to use CFA:**\r\n- Testing a pre-specified factor structure\r\n- Validating a scale in new population\r\n- Comparing alternative structures\r\n- Assessing measurement invariance across groups\r\n\r\n**CFA vs EFA:**\r\n- EFA: \"What is the structure?\" (exploratory)\r\n- CFA: \"Does this structure fit?\" (confirmatory)\r\n\r\n```r\r\nlibrary(lavaan)\r\nlibrary(semTools)\r\n\r\n# Load classic CFA dataset\r\ndata(HolzingerSwineford1939)\r\nhs <- HolzingerSwineford1939\r\n\r\nstr(hs)\r\n#> 301 observations\r\n#> 9 mental ability tests (x1-x9)\r\n#> From two schools (Pasteur, Grant-White)\r\n\r\nhead(hs[, c(\"school\", \"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\", \"x7\", \"x8\", \"x9\")])\r\n```\r\n\r\n#### Specifying the CFA Model\r\n\r\n```r\r\n# Classic three-factor model\r\n# Visual: x1, x2, x3\r\n# Textual: x4, x5, x6\r\n# Speed: x7, x8, x9\r\n\r\nmodel_3factor <- '\r\n  visual  =~ x1 + x2 + x3\r\n  textual =~ x4 + x5 + x6\r\n  speed   =~ x7 + x8 + x9\r\n'\r\n\r\n# Fit the model\r\nfit_3f <- cfa(\r\n  model_3factor,\r\n  data = hs,\r\n  estimator = \"MLM\"  # Robust to non-normality\r\n)\r\n\r\nsummary(fit_3f, fit.measures = TRUE, standardized = TRUE)\r\n```\r\n\r\n#### Interpreting CFA Output\r\n\r\n```r\r\n# Model Fit Indices:\r\n# =================\r\n# Chi-square (test of exact fit):\r\n#   chi-sq = 85.306, df = 24, p < 0.001\r\n#   Significant p: Model doesn't fit perfectly\r\n#   But chi-square is sensitive to sample size\r\n\r\n# CFI (Comparative Fit Index):\r\n#   0.931 (> 0.90 acceptable, > 0.95 good)\r\n\r\n# TLI (Tucker-Lewis Index):\r\n#   0.896 (> 0.90 acceptable, > 0.95 good)\r\n\r\n# RMSEA (Root Mean Square Error of Approximation):\r\n#   0.092, 90% CI [0.071, 0.114]\r\n#   < 0.06 good, < 0.08 acceptable, > 0.10 poor\r\n\r\n# SRMR (Standardized Root Mean Residual):\r\n#   0.065 (< 0.08 good)\r\n\r\n# Factor Loadings (Standardized):\r\n# ==============================\r\n#                    Estimate  Std.Err  z-value  P(>|z|)   Std.all\r\n# visual =~\r\n#   x1                  1.000                               0.772\r\n#   x2                  0.554    0.100    5.554    0.000    0.424\r\n#   x3                  0.729    0.109    6.685    0.000    0.581\r\n#\r\n# Std.all = standardized loadings\r\n# Should be > 0.40, ideally > 0.70\r\n```\r\n\r\n#### Model Comparison\r\n\r\n```r\r\n# Compare 3-factor vs 1-factor model\r\nmodel_1factor <- '\r\n  general =~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9\r\n'\r\n\r\nfit_1f <- cfa(model_1factor, data = hs, estimator = \"MLM\")\r\n\r\n# Compare fit\r\nsummary(fit_1f, fit.measures = TRUE)\r\n#> CFI = 0.697, RMSEA = 0.175\r\n#> Much worse fit than 3-factor\r\n\r\n# Formal comparison (Satorra-Bentler scaled chi-square difference)\r\nlavTestLRT(fit_3f, fit_1f, method = \"satorra.bentler.2010\")\r\n#>\r\n#> Chi-Squared Difference Test\r\n#>\r\n#>          Df AIC BIC  Chisq Chisq diff Df diff Pr(>Chisq)\r\n#> fit_3f   24             85.306\r\n#> fit_1f   27            280.170   194.86       3     <2e-16 ***\r\n#>\r\n#> Significant: 3-factor model fits significantly better\r\n```\r\n\r\n#### Reliability from CFA\r\n\r\n```r\r\n# Omega reliability (preferred over alpha for CFA)\r\ncompRelSEM(fit_3f)\r\n#>  visual textual   speed\r\n#>   0.601   0.885   0.689\r\n\r\n#> omega > 0.70 generally acceptable\r\n#> textual factor has best reliability\r\n```\r\n\r\n#### Measurement Invariance Testing\r\n\r\n```r\r\n# Test whether factor structure is equivalent across schools\r\n\r\n# Step 1: Configural invariance (same structure)\r\nfit_config <- cfa(model_3factor, data = hs, group = \"school\")\r\n\r\n# Step 2: Metric invariance (equal loadings)\r\nfit_metric <- cfa(model_3factor, data = hs, group = \"school\",\r\n                  group.equal = \"loadings\")\r\n\r\n# Step 3: Scalar invariance (equal intercepts)\r\nfit_scalar <- cfa(model_3factor, data = hs, group = \"school\",\r\n                  group.equal = c(\"loadings\", \"intercepts\"))\r\n\r\n# Compare nested models\r\nlavTestLRT(fit_config, fit_metric, fit_scalar)\r\n#>\r\n#>             Df    AIC    BIC  Chisq Chisq diff Df diff Pr(>Chisq)\r\n#> fit_config  48 7484.4 7706.8 115.85\r\n#> fit_metric  54 7480.6 7680.8 124.04     8.19       6      0.224\r\n#> fit_scalar  60 7484.4 7662.4 139.82    15.77       6      0.015 *\r\n#>\r\n#> Interpretation:\r\n#> - Config -> Metric: p = 0.224 (metric invariance holds)\r\n#> - Metric -> Scalar: p = 0.015 (scalar invariance may not hold)\r\n#> - Can compare factor means across schools with metric invariance\r\n#> - Cannot compare observed means without scalar invariance\r\n```\r\n\r\n#### Modification Indices\r\n\r\n```r\r\n# Identify potential model improvements\r\nmodindices(fit_3f, sort = TRUE, minimum.value = 10)\r\n#>\r\n#>     lhs op rhs    mi    epc sepc.all\r\n#> 1    x7 ~~  x8 34.15  0.536    0.488\r\n#> 2    x4 ~~  x6 10.23  0.182    0.163\r\n#>\r\n#> x7 and x8 residuals may be correlated\r\n#> Consider adding: x7 ~~ x8 to model\r\n#> But only if theoretically justified!\r\n```\r\n\r\n---\r\n\r\n### 6.3 Item Response Theory (IRT)\r\n\r\n**When to use IRT:**\r\n- Developing assessments or tests\r\n- Items vary in difficulty and discrimination\r\n- Need item-level diagnostics\r\n- Want ability estimates on common scale\r\n- Adaptive testing design\r\n\r\n**IRT vs Factor Analysis:**\r\n- FA focuses on inter-item correlations\r\n- IRT models probability of response as function of latent trait\r\n- IRT provides item-specific parameters (difficulty, discrimination)\r\n\r\n```r\r\nlibrary(mirt)\r\n\r\n# Simulate graded response data (5-point Likert scale)\r\nset.seed(12345)\r\n\r\n# Generate 500 respondents, 10 items\r\n# Items vary in difficulty and discrimination\r\na <- c(1.5, 1.2, 0.8, 1.0, 1.3, 0.9, 1.1, 1.4, 0.7, 1.6)  # Discrimination\r\nd <- matrix(c(\r\n  2.0, 1.0, -0.5, -1.5,  # Item 1 thresholds\r\n  1.5, 0.5, -0.5, -1.0,  # Item 2\r\n  2.5, 1.5,  0.0, -1.0,  # Item 3\r\n  1.0, 0.0, -1.0, -2.0,  # Item 4\r\n  2.0, 1.0,  0.0, -1.5,  # Item 5\r\n  1.5, 0.5, -0.5, -1.5,  # Item 6\r\n  2.5, 1.0, -0.5, -2.0,  # Item 7\r\n  1.0, 0.5, -0.5, -1.0,  # Item 8\r\n  3.0, 1.5,  0.5, -0.5,  # Item 9 (harder item)\r\n  1.5, 0.5, -1.0, -2.0   # Item 10\r\n), ncol = 4, byrow = TRUE)\r\n\r\n# Simulate responses\r\nsim_data <- simdata(\r\n  a = a,\r\n  d = d,\r\n  N = 500,\r\n  itemtype = \"graded\"\r\n)\r\n\r\n# Convert to data frame\r\nirt_data <- as.data.frame(sim_data)\r\nnames(irt_data) <- paste0(\"Item\", 1:10)\r\nhead(irt_data)\r\n#>   Item1 Item2 Item3 Item4 Item5 Item6 Item7 Item8 Item9 Item10\r\n#> 1     4     3     4     4     4     3     4     3     3      4\r\n#> 2     2     2     1     3     2     2     2     2     1      3\r\n#> ...\r\n```\r\n\r\n#### Fitting the Graded Response Model\r\n\r\n```r\r\n# Fit unidimensional graded response model\r\nirt_model <- mirt(\r\n  data = irt_data,\r\n  model = 1,              # Unidimensional\r\n  itemtype = \"graded\",\r\n  verbose = FALSE\r\n)\r\n\r\nsummary(irt_model)\r\n#> Unidimensional model with 10 items\r\n#> Factor loadings (F1) and h2 (communality)\r\n```\r\n\r\n#### Item Parameters\r\n\r\n```r\r\n# Extract item parameters\r\ncoef(irt_model, IRTpars = TRUE, simplify = TRUE)\r\n#>\r\n#> $items\r\n#>         a      b1     b2     b3     b4\r\n#> Item1  1.52  -1.32  -0.65   0.32   0.98\r\n#> Item2  1.18  -1.25  -0.42   0.42   0.84\r\n#> ...\r\n#>\r\n#> a = discrimination (slope)\r\n#>     Higher = better at distinguishing ability levels\r\n#>     > 1.0 is good, > 1.5 is very good\r\n#>\r\n#> b1-b4 = threshold parameters (difficulty)\r\n#>     Location on theta scale where P(X >= k) = 0.5\r\n#>     b1 < b2 < b3 < b4 (ordered thresholds)\r\n\r\n# Summary table\r\nitem_params <- coef(irt_model, IRTpars = TRUE, simplify = TRUE)$items\r\nround(item_params, 2)\r\n```\r\n\r\n#### Item Information Curves\r\n\r\n```r\r\n# Item information shows where each item is most informative\r\n# Plot all item information curves (using base mirt plotting)\r\nplot(irt_model, type = \"infotrace\", which.items = 1:5)\r\n\r\n# Items with higher discrimination have taller, narrower curves\r\n# Peak location indicates where item is most useful\r\n\r\n# Numerical: Area under information curve\r\nfor (i in 1:10) {\r\n  info <- areainfo(irt_model, c(-4, 4), which.items = i)\r\n  cat(sprintf(\"Item %d: %.2f\\n\", i, info$Info))\r\n}\r\n#> Item 1: 3.82\r\n#> Item 2: 2.45\r\n#> ...\r\n#> Items with higher discrimination contribute more information\r\n```\r\n\r\n#### Test Information Curve\r\n\r\n```r\r\n# Total test information across theta range\r\nplot(irt_model, type = \"info\")\r\n\r\n# Test is most informative around theta = 0 (average ability)\r\n# Less information at extremes\r\n\r\n# Where does the test measure well?\r\ninfo_total <- areainfo(irt_model, c(-2, 2))  # Middle range\r\ninfo_extremes <- areainfo(irt_model, c(-4, -2))  # Low ability\r\n\r\ncat(sprintf(\"Information in [-2, 2]: %.1f\\n\", info_total$Info))\r\ncat(sprintf(\"Information in [-4, -2]: %.1f\\n\", info_extremes$Info))\r\n```\r\n\r\n#### Person Ability Estimates\r\n\r\n```r\r\n# Extract theta (ability) estimates\r\ntheta_estimates <- fscores(irt_model, method = \"EAP\")\r\nhead(theta_estimates)\r\n#>          F1\r\n#> [1,]  1.234\r\n#> [2,] -0.567\r\n#> ...\r\n\r\n# Add to data\r\nirt_data$theta <- theta_estimates[, 1]\r\n\r\n# Summary of ability distribution\r\nsummary(irt_data$theta)\r\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.\r\n#>  -2.89   -0.67   -0.02    0.00    0.70    2.51\r\nhist(irt_data$theta, main = \"Distribution of Ability Estimates\",\r\n     xlab = \"Theta\")\r\n```\r\n\r\n#### Item Selection for Short Forms\r\n\r\n```r\r\n# Compare information retained with fewer items\r\nfull_info <- areainfo(irt_model, c(-3, 3), which.items = 1:10)\r\n\r\n# Select best 5 items based on discrimination\r\nbest_items <- order(item_params[, \"a\"], decreasing = TRUE)[1:5]\r\nshort_info <- areainfo(irt_model, c(-3, 3), which.items = best_items)\r\n\r\ncat(sprintf(\"Full test (10 items): %.1f information\\n\", full_info$Info))\r\ncat(sprintf(\"Short form (5 items): %.1f information (%.0f%%)\\n\",\r\n            short_info$Info, 100 * short_info$Info / full_info$Info))\r\n\r\n#> Full test (10 items): 24.5 information\r\n#> Short form (5 items): 15.8 information (64%)\r\n#> 5 best items retain 64% of information\r\n```\r\n\r\n#### Using Real Test Data: LSAT6\r\n\r\nThe `LSAT6` dataset contains responses to 5 items from the Law School Admission Test:\r\n\r\n```r\r\n# Load LSAT6 data (frequency-weighted format)\r\ndata(LSAT6)\r\nhead(LSAT6)\r\n#>   Item_1 Item_2 Item_3 Item_4 Item_5 Freq\r\n#> 1      0      0      0      0      0    3\r\n#> 2      0      0      0      0      1    6\r\n#> ...\r\n\r\n# Expand to individual responses\r\nitems <- LSAT6[, 1:5]\r\nfreq <- LSAT6$Freq\r\nlsat_data <- items[rep(1:nrow(items), freq), ]\r\ncat(\"Sample size:\", nrow(lsat_data), \"\\n\")\r\n#> Sample size: 1000\r\n\r\n# Fit 2PL model (dichotomous items)\r\nlsat_model <- mirt(lsat_data, model = 1, itemtype = \"2PL\", verbose = FALSE)\r\n\r\n# Item parameters\r\ncoef(lsat_model, IRTpars = TRUE, simplify = TRUE)$items\r\n#>           a      b g u\r\n#> Item_1 0.83  -3.36 0 1\r\n#> Item_2 0.72  -1.37 0 1\r\n#> Item_3 0.89  -0.28 0 1\r\n#> Item_4 0.69  -1.87 0 1\r\n#> Item_5 0.66  -3.12 0 1\r\n\r\n# Interpretation:\r\n#> - Item_3 has highest discrimination (a = 0.89)\r\n#> - Item_1 is easiest (b = -3.36, most negative)\r\n#> - Item_3 is hardest (b = -0.28, closest to zero)\r\n```\r\n\r\n#### Item Fit Diagnostics\r\n\r\nCheck whether items fit the assumed IRT model:\r\n\r\n```r\r\n# Item fit using S-X2 statistic (Orlando & Thissen, 2000)\r\nitem_fit <- itemfit(lsat_model)\r\nprint(round(item_fit[, c(\"S_X2\", \"df.S_X2\", \"p.S_X2\")], 3))\r\n\r\n#>        S_X2 df.S_X2 p.S_X2\r\n#> Item_1 0.45       2  0.799\r\n#> Item_2 1.69       2  0.429\r\n#> Item_3 0.67       1  0.415\r\n#> Item_4 0.18       2  0.916\r\n#> Item_5 0.11       2  0.946\r\n\r\n# Interpretation:\r\n#> - p > 0.05: Item fits the model (all items fit here)\r\n#> - p < 0.05: Item may not fit; investigate further\r\n\r\n# Alternative: Zh statistic for person fit\r\nperson_fit <- personfit(lsat_model)\r\n#> Zh < -2 suggests aberrant response pattern\r\n```\r\n\r\n#### Model Fit Statistics\r\n\r\nAssess overall model fit:\r\n\r\n```r\r\n# M2 statistic (Maydeu-Olivares & Joe, 2006)\r\nm2_fit <- M2(lsat_model)\r\n\r\ncat(\"Model fit statistics:\\n\")\r\ncat(\"  M2 =\", round(m2_fit$M2, 2),\r\n    \", df =\", m2_fit$df,\r\n    \", p =\", round(m2_fit$p, 4), \"\\n\")\r\ncat(\"  RMSEA =\", round(m2_fit$RMSEA, 3),\r\n    \", 95% CI [\", round(m2_fit$RMSEA_5, 3), \",\",\r\n    round(m2_fit$RMSEA_95, 3), \"]\\n\")\r\ncat(\"  SRMSR =\", round(m2_fit$SRMSR, 3), \"\\n\")\r\n\r\n# Example output:\r\n#> Model fit statistics:\r\n#>   M2 = 4.74 , df = 5 , p = 0.4487\r\n#>   RMSEA = 0 , 95% CI [ 0 , 0.058 ]\r\n#>   SRMSR = 0.029\r\n\r\n# Interpretation:\r\n#> - p > 0.05: Model fits well\r\n#> - RMSEA < 0.05: Good fit\r\n#> - SRMSR < 0.05: Good fit\r\n\r\n# Compare models (e.g., 1PL vs 2PL)\r\nlsat_1pl <- mirt(lsat_data, model = 1, itemtype = \"Rasch\", verbose = FALSE)\r\nanova(lsat_1pl, lsat_model)\r\n#> Significant p: 2PL fits better than 1PL\r\n```\r\n\r\n---\r\n\r\n### 6.4 Reliability Assessment\r\n\r\n**Types of reliability:**\r\n- Internal consistency: Do items measure same construct?\r\n- Test-retest: Stability over time\r\n- Inter-rater: Agreement between raters\r\n\r\n```r\r\nlibrary(psych)\r\n\r\n# Create simulated test-retest data\r\nset.seed(12345)\r\nn <- 100\r\n\r\n# True scores\r\ntrue_score <- rnorm(n, mean = 50, sd = 10)\r\n\r\n# Three measurement occasions with error\r\ntime1 <- true_score + rnorm(n, sd = 5)\r\ntime2 <- true_score + rnorm(n, sd = 5)\r\ntime3 <- true_score + rnorm(n, sd = 5)\r\n\r\nretest_data <- data.frame(\r\n  T1 = time1,\r\n  T2 = time2,\r\n  T3 = time3\r\n)\r\n\r\nhead(retest_data)\r\n#>        T1     T2     T3\r\n#> 1  52.34  54.21  51.89\r\n#> 2  45.67  44.23  46.12\r\n#> ...\r\n```\r\n\r\n#### Intraclass Correlations (ICC)\r\n\r\n**Note:** The `ICC()` function in psych requires the `lme4` package for some calculations. Make sure it's installed.\r\n\r\n```r\r\n# ICC for test-retest reliability\r\nicc_results <- ICC(retest_data)\r\nprint(icc_results)\r\n#>\r\n#> Intraclass correlation coefficients\r\n#>                          type  ICC   F df1 df2       p lower bound upper bound\r\n#> Single_raters_absolute   ICC1 0.75 10.1  99 200 < 0.001        0.67        0.82\r\n#> Single_random_raters     ICC2 0.76 10.3  99 198 < 0.001        0.68        0.83\r\n#> Single_fixed_raters      ICC3 0.76 10.3  99 198 < 0.001        0.68        0.83\r\n#> Average_raters_absolute  ICC1k 0.90 10.1  99 200 < 0.001        0.86        0.93\r\n#> Average_random_raters    ICC2k 0.90 10.3  99 198 < 0.001        0.87        0.94\r\n#> Average_fixed_raters     ICC3k 0.91 10.3  99 198 < 0.001        0.87        0.94\r\n\r\n# Interpretation:\r\n#> ICC1: One-way random effects (each subject rated by different raters)\r\n#> ICC2: Two-way random effects, absolute agreement\r\n#> ICC3: Two-way mixed effects, consistency\r\n#>\r\n#> For test-retest, use ICC2 (two-way, absolute agreement)\r\n#> 0.75 = \"moderate\" to \"good\" reliability\r\n#>\r\n#> Guidelines:\r\n#> < 0.50 = poor\r\n#> 0.50-0.75 = moderate\r\n#> 0.75-0.90 = good\r\n#> > 0.90 = excellent\r\n```\r\n\r\n#### Heise Reliability (Three-Wave)\r\n\r\n```r\r\n# Heise (1969) method for three-wave reliability\r\n# Adjusts for systematic change over time\r\n\r\ncors <- cor(retest_data)\r\nprint(round(cors, 3))\r\n#>        T1    T2    T3\r\n#> T1  1.000 0.761 0.750\r\n#> T2  0.761 1.000 0.768\r\n#> T3  0.750 0.768 1.000\r\n\r\n# Heise reliability formula:\r\n# reliability = r(T1,T2) * r(T2,T3) / r(T1,T3)\r\nheise_reliability <- cors[\"T1\", \"T2\"] * cors[\"T2\", \"T3\"] / cors[\"T1\", \"T3\"]\r\ncat(sprintf(\"Heise reliability estimate: %.3f\\n\", heise_reliability))\r\n#> Heise reliability estimate: 0.779\r\n\r\n# This estimates true score reliability\r\n# Accounting for measurement error AND true change\r\n```\r\n\r\n#### Cronbach's Alpha with Detailed Diagnostics\r\n\r\n```r\r\n# Use BFI data for internal consistency example\r\ndata(bfi)\r\nneuroticism <- bfi[, c(\"N1\", \"N2\", \"N3\", \"N4\", \"N5\")]\r\nneuroticism <- na.omit(neuroticism)\r\n\r\nalpha_result <- alpha(neuroticism)\r\nprint(alpha_result)\r\n#>\r\n#> Reliability analysis\r\n#> raw_alpha std.alpha G6(smc) average_r   S/N    ase mean   sd median_r\r\n#>      0.81      0.82    0.81      0.48  4.6  0.006  3.0  1.3     0.47\r\n#>\r\n#> Reliability if an item is dropped:\r\n#>    raw_alpha std.alpha G6(smc) average_r  S/N alpha se  var.r med.r\r\n#> N1      0.76      0.77    0.74      0.46  3.3    0.008 0.0091  0.46\r\n#> N2      0.77      0.78    0.76      0.47  3.5    0.007 0.0057  0.46\r\n#> N3      0.80      0.80    0.78      0.50  4.1    0.006 0.0064  0.49\r\n#> N4      0.79      0.80    0.78      0.50  4.0    0.007 0.0047  0.50\r\n#> N5      0.81      0.82    0.79      0.53  4.5    0.006 0.0019  0.53\r\n#>\r\n#> Item statistics:\r\n#>       n  raw.r std.r  r.cor r.drop mean  sd\r\n#> N1 2769  0.82  0.81  0.77   0.68  2.9 1.6\r\n#> N2 2771  0.80  0.79  0.74   0.65  3.5 1.5\r\n#> N3 2767  0.71  0.73  0.62   0.55  3.2 1.6\r\n#> N4 2765  0.73  0.74  0.64   0.57  3.2 1.6\r\n#> N5 2771  0.64  0.66  0.53   0.47  3.0 1.6\r\n\r\n# Interpretation:\r\n#> - raw_alpha = 0.81 (good internal consistency)\r\n#> - All items contribute positively (alpha doesn't improve if dropped)\r\n#> - r.drop = corrected item-total correlation (should be > 0.30)\r\n```\r\n\r\n#### McDonald's Omega (Preferred to Alpha)\r\n\r\n```r\r\n# Omega handles items with different loadings better than alpha\r\nomega_result <- omega(neuroticism, nfactors = 1)\r\n\r\nprint(omega_result)\r\n#> Omega\r\n#> omega_h   omega_t\r\n#>   0.82      0.82\r\n#>\r\n#> omega_h = omega hierarchical (general factor)\r\n#> omega_t = omega total\r\n#> Generally similar to alpha for unidimensional scales\r\n#> More accurate when items have unequal loadings\r\n```\r\n\r\n---\r\n\r\n## Troubleshooting Common Issues\r\n\r\n### Sentiment Lexicons Not Available\r\n\r\nThe sentiment lexicons (AFINN, Bing, NRC) require a one-time download:\r\n\r\n```r\r\n# This will prompt to download the lexicon\r\nlibrary(tidytext)\r\nlibrary(textdata)\r\n\r\n# First time only - will prompt for download\r\nafinn <- get_sentiments(\"afinn\")  # Downloads AFINN lexicon\r\nbing <- get_sentiments(\"bing\")    # Downloads Bing lexicon\r\nnrc <- get_sentiments(\"nrc\")      # Downloads NRC lexicon\r\n```\r\n\r\n### ICC Function Errors\r\n\r\nThe `ICC()` function may require the `lme4` package:\r\n\r\n```r\r\n# Install if not present\r\ninstall.packages(\"lme4\")\r\nlibrary(psych)\r\nlibrary(lme4)  # Load before using ICC\r\n\r\n# Now ICC should work\r\nICC(retest_data)\r\n```\r\n\r\n### Memory Issues with Large Topic Models\r\n\r\nFor large corpora, reduce memory usage:\r\n\r\n```r\r\n# Use VEM instead of Gibbs for large datasets\r\nlda_model <- LDA(dtm, k = 10, method = \"VEM\")\r\n\r\n# Or reduce vocabulary first\r\ndtm_sparse <- removeSparseTerms(dtm, sparse = 0.95)\r\n```\r\n\r\n### Caret Warnings About Factors\r\n\r\nWhen using caret with factor outcomes, ensure factor levels are valid R names:\r\n\r\n```r\r\n# This may cause issues\r\nlevels(data$outcome)\r\n#> [1] \"0\" \"1\"\r\n\r\n# Fix by renaming levels\r\ndata$outcome <- factor(data$outcome, labels = c(\"No\", \"Yes\"))\r\n```\r\n",
        "plugins/r-analyst/skills/r-analyst/techniques/04_synthetic_control.md": "# Synthetic Control and Advanced Causal Inference in R\r\n\r\nThis tutorial covers synthetic control methods (SCM), generalized SCM, augmented SCM, synthetic difference-in-differences, conjoint analysis, and heterogeneous treatment effect estimation. All examples use built-in package datasets for reproducibility.\r\n\r\n---\r\n\r\n## Quick Reference Table\r\n\r\n| Method | R Package | Key Function | Built-in Dataset | When to Use |\r\n|--------|-----------|--------------|------------------|-------------|\r\n| Basic SCM | `Synth` | `synth()` | `basque` | Single treated unit, aggregate data |\r\n| Generalized SCM | `gsynth` | `gsynth()` | `simdata`, `turnout` | Multiple treated units, interactive FE |\r\n| FE Counterfactual | `fect` | `fect()` | `simdata` | Staggered treatment, reversal allowed |\r\n| Augmented SCM | `augsynth` | `augsynth()` | `kansas` | Better pre-treatment fit needed |\r\n| Synthetic DiD | `synthdid` | `synthdid_estimate()` | `california_prop99` | Combines SCM + DiD strengths |\r\n| Conjoint | `cregg` | `cj()` | `immigration`, `taxes` | Multi-attribute choice experiments |\r\n| Causal Forests | `grf` | `causal_forest()` | (simulated) | Heterogeneous treatment effects |\r\n| Panel Regression | `fixest` | `feols()` | (user data) | Fast fixed effects, clustering |\r\n\r\n---\r\n\r\n## 1. Basic Synthetic Control Method\r\n\r\n### When to Use\r\n\r\nSynthetic control is ideal when:\r\n\r\n- You have **one treated unit** (country, state, firm) affected by an intervention\r\n- You have **multiple control units** not affected by the intervention\r\n- You have **aggregate-level data** (not individual-level)\r\n- The treatment occurs at a **known time point**\r\n- You have a **reasonably long pre-treatment period** to establish fit\r\n\r\n### Assumptions\r\n\r\n1. **No anticipation**: Units don't change behavior before treatment\r\n2. **No spillovers**: Treatment doesn't affect control units (SUTVA)\r\n3. **Convex hull**: Treated unit's characteristics lie within the range of controls\r\n4. **Good pre-treatment fit**: Synthetic control matches treated unit pre-treatment\r\n\r\n### The basque Dataset\r\n\r\nThe `Synth` package includes the famous Basque Country dataset from Abadie and Gardeazabal (2003), which examined the economic costs of terrorist conflict. The data contains 17 Spanish regions from 1955-1997, with the Basque Country (region 17) as the treated unit. The \"treatment\" is the onset of ETA terrorism in the early 1970s.\r\n\r\n```r\r\nlibrary(Synth)\r\n\r\n# Load the basque dataset\r\ndata(basque)\r\n\r\n# Examine the data structure\r\nstr(basque)\r\n#> 'data.frame':\t774 obs. of  17 variables:\r\n#>  $ regionno   : int  1 1 1 1 1 1 ...\r\n#>  $ regionname : chr  \"Spain (Espana)\" ...\r\n#>  $ year       : int  1955 1956 1957 1958 ...\r\n#>  $ gdpcap     : num  2354 2702 3010 3367 ...\r\n#>  $ sec.agriculture : num  NA NA NA NA NA ...\r\n#>  $ sec.energy      : num  NA NA NA NA NA ...\r\n#>  ...\r\n\r\n# Key variables:\r\n# - regionno: Region identifier (17 = Basque Country)\r\n# - gdpcap: GDP per capita (outcome)\r\n# - school.*: Education variables (predictors)\r\n# - sec.*: Sector composition (predictors)\r\n# - invest: Investment rate (predictor)\r\n# - popdens: Population density (predictor)\r\n```\r\n\r\n### Step 1: Prepare the Data\r\n\r\nThe `dataprep()` function structures your panel data for synthetic control estimation.\r\n\r\n```r\r\n# Prepare data for synthetic control\r\ndataprep_out <- dataprep(\r\n  foo = basque,\r\n\r\n  # Predictors to match on (using their means over specified period)\r\n  predictors = c(\"school.illit\", \"school.prim\", \"invest\"),\r\n  predictors.op = \"mean\",\r\n  time.predictors.prior = 1964:1969,\r\n\r\n\r\n  # Special predictors with custom time periods\r\n  special.predictors = list(\r\n    list(\"gdpcap\", 1960:1969, \"mean\"),           # Pre-treatment GDP\r\n    list(\"sec.agriculture\", seq(1961, 1969, 2), \"mean\"),\r\n    list(\"sec.energy\", seq(1961, 1969, 2), \"mean\"),\r\n    list(\"sec.industry\", seq(1961, 1969, 2), \"mean\"),\r\n    list(\"sec.construction\", seq(1961, 1969, 2), \"mean\"),\r\n    list(\"sec.services.venta\", seq(1961, 1969, 2), \"mean\"),\r\n    list(\"sec.services.nonventa\", seq(1961, 1969, 2), \"mean\"),\r\n    list(\"popdens\", 1969, \"mean\")\r\n  ),\r\n\r\n  # Outcome variable\r\n  dependent = \"gdpcap\",\r\n\r\n  # Panel structure\r\n  unit.variable = \"regionno\",\r\n  unit.names.variable = \"regionname\",\r\n  time.variable = \"year\",\r\n\r\n  # Treatment and control identification\r\n  treatment.identifier = 17,           # Basque Country\r\n  controls.identifier = c(2:16, 18),   # Other Spanish regions\r\n\r\n  # Optimization and plotting periods\r\n  time.optimize.ssr = 1960:1969,       # Pre-treatment fit period\r\n  time.plot = 1955:1997                # Full period for plots\r\n)\r\n\r\n# View the prepared data matrices\r\nnames(dataprep_out)\r\n#> [1] \"X0\" \"X1\" \"Z0\" \"Z1\" \"Y0plot\" \"Y1plot\"\r\n# X0, X1: Predictor matrices for controls and treated\r\n# Z0, Z1: Outcome matrices for optimization period\r\n# Y0plot, Y1plot: Full outcome series for plotting\r\n```\r\n\r\n### Step 2: Estimate Synthetic Control\r\n\r\n```r\r\n# Run the synthetic control optimization\r\nsynth_out <- synth(dataprep_out)\r\n#>\r\n#> X1, X0, Z1, Z0 all exist\r\n#>\r\n#>  Optimization completed\r\n\r\n# Extract the weights assigned to control units\r\nround(synth_out$solution.w, 3)\r\n#>             w.weight\r\n#> Andalucia      0.000\r\n#> Aragon         0.000\r\n#> ...\r\n#> Cataluna       0.851\r\n#> ...\r\n#> Madrid         0.149\r\n#> ...\r\n\r\n# The synthetic Basque Country is primarily a weighted\r\n# combination of Cataluna (85%) and Madrid (15%)\r\n```\r\n\r\n### Step 3: Examine Results\r\n\r\n```r\r\n# Create comparison table\r\nsynth_tables <- synth.tab(\r\n  dataprep.res = dataprep_out,\r\n  synth.res = synth_out\r\n)\r\n\r\n# Predictor balance: treated vs synthetic\r\nprint(synth_tables$tab.pred)\r\n#>                               Treated  Synthetic  Sample Mean\r\n#> school.illit                    3.321      7.645       11.263\r\n#> school.prim                    85.893     82.285       78.438\r\n#> invest                         24.647     21.583       21.424\r\n#> special.gdpcap.1960.1969        5.285      5.271        3.581\r\n#> ...\r\n```\r\n\r\n### Step 4: Visualize Results\r\n\r\n```r\r\n# Path plot: treated vs synthetic over time\r\npath.plot(\r\n  synth.res = synth_out,\r\n  dataprep.res = dataprep_out,\r\n  Ylab = \"GDP per capita\",\r\n  Xlab = \"Year\",\r\n  Legend = c(\"Basque Country\", \"Synthetic Basque\"),\r\n  Legend.position = \"bottomright\"\r\n)\r\n#> Shows treated unit (solid) vs synthetic control (dashed)\r\n#> Pre-1970: Lines should track closely\r\n#> Post-1970: Gap represents treatment effect\r\n\r\n# Gaps plot: difference between treated and synthetic\r\ngaps.plot(\r\n  synth.res = synth_out,\r\n  dataprep.res = dataprep_out,\r\n  Ylab = \"Gap in GDP per capita\",\r\n  Xlab = \"Year\"\r\n)\r\n#> Shows the treatment effect over time\r\n#> Negative values indicate terrorism reduced GDP\r\n```\r\n\r\n### Interpretation\r\n\r\nThe synthetic control for the Basque Country is a weighted average that closely matches the Basque Country's economic characteristics before terrorism began. After the onset of conflict, the Basque Country's GDP per capita diverges below its synthetic counterpart, suggesting terrorism caused substantial economic damage.\r\n\r\n### Limitations\r\n\r\n1. **Inference is difficult**: Standard errors require placebo tests (permutation inference)\r\n2. **Convex hull problem**: Cannot extrapolate beyond control unit characteristics\r\n3. **Single treated unit**: Original method designed for N=1 treated\r\n4. **Pre-treatment fit**: Poor fit undermines credibility of post-treatment estimates\r\n\r\n---\r\n\r\n## 2. Generalized Synthetic Control (gsynth)\r\n\r\n### When to Use\r\n\r\nGeneralized synthetic control extends SCM to handle:\r\n\r\n- **Multiple treated units** with potentially different treatment timing\r\n- **Interactive fixed effects** (factor models) to capture unobserved confounders\r\n- **Staggered adoption** designs\r\n\r\n### The simdata Dataset\r\n\r\nThe `gsynth` package includes `simdata`, a simulated panel with 50 units over 30 time periods. Treatment begins at period 21 for 5 units, with true effects ranging from 1 to 10.\r\n\r\n```r\r\nlibrary(gsynth)\r\n\r\n# Load datasets that come with gsynth\r\ndata(gsynth)\r\n\r\n# Two datasets are now available: simdata and turnout\r\nls()\r\n#> [1] \"simdata\" \"turnout\"\r\n\r\n# Examine simdata structure\r\nhead(simdata)\r\n#>   id time   Y D     X1     X2\r\n#> 1  1    1 2.8 0  0.123 -0.456\r\n#> 2  1    2 3.1 0  0.234 -0.567\r\n#> ...\r\n\r\n# Data characteristics\r\ncat(\"Units:\", length(unique(simdata$id)), \"\\n\")\r\n#> Units: 50\r\ncat(\"Time periods:\", length(unique(simdata$time)), \"\\n\")\r\n#> Time periods: 30\r\ncat(\"Treated units:\", sum(simdata$D == 1 & simdata$time == 21), \"\\n\")\r\n#> Treated units: 5\r\n```\r\n\r\n### Basic Estimation\r\n\r\n```r\r\n# Generalized synthetic control with cross-validation\r\ngsynth_out <- gsynth(\r\n  Y ~ D + X1 + X2,             # Formula: outcome ~ treatment + covariates\r\n  data = simdata,\r\n  index = c(\"id\", \"time\"),     # Panel identifiers\r\n  force = \"two-way\",           # Unit and time fixed effects\r\n  CV = TRUE,                   # Cross-validate number of factors\r\n\r\n  r = c(0, 5),                 # Range of factors to consider\r\n  se = TRUE,                   # Compute standard errors\r\n  nboots = 500,                # Bootstrap replications\r\n  parallel = TRUE,             # Use parallel processing\r\n  cores = 4,\r\n  seed = 12345\r\n)\r\n\r\n# View summary\r\nprint(gsynth_out)\r\n#> Call:\r\n#> gsynth(Y ~ D + X1 + X2, data = simdata, ...)\r\n#>\r\n#> Average Treatment Effect on the Treated:\r\n#>      Estimate  S.E.   CI.lower  CI.upper  p.value\r\n#> ATT    5.123   0.234    4.664     5.582   0.000\r\n#>\r\n#> Coefficients:\r\n#>      Estimate  S.E.\r\n#> X1     0.987  0.045\r\n#> X2    -0.512  0.038\r\n#>\r\n#> Number of factors: 2 (cross-validated)\r\n```\r\n\r\n### Visualize Results\r\n\r\n```r\r\n# Main effect plot: ATT over time relative to treatment\r\nplot(gsynth_out)\r\n#> Shows estimated treatment effect by period since treatment\r\n#> Shaded region = 95% confidence interval\r\n#> Pre-treatment effects should be near zero\r\n\r\n# Counterfactual trajectories\r\nplot(gsynth_out, type = \"counterfactual\")\r\n#> Shows actual vs counterfactual for each treated unit\r\n\r\n# Individual treated units\r\nplot(gsynth_out, type = \"counterfactual\", id = 1)\r\n#> Focus on specific unit\r\n```\r\n\r\n### Using the turnout Dataset\r\n\r\nThe `turnout` dataset contains US state-level data on Election Day Registration (EDR) and voter turnout, a classic application from Xu (2017).\r\n\r\n```r\r\ndata(gsynth)\r\n\r\n# Examine turnout data\r\nhead(turnout)\r\n#>   abb year turnout policy\r\n#> 1  AL 1920   0.352      0\r\n#> 2  AL 1924   0.185      0\r\n#> ...\r\n\r\n# Estimate effect of EDR on turnout\r\nedr_out <- gsynth(\r\n  turnout ~ policy,\r\n  data = turnout,\r\n  index = c(\"abb\", \"year\"),\r\n  force = \"two-way\",\r\n  CV = TRUE,\r\n  r = c(0, 5),\r\n  se = TRUE,\r\n  nboots = 500,\r\n  seed = 12345\r\n)\r\n\r\nprint(edr_out)\r\n#> Average Treatment Effect on the Treated:\r\n#>      Estimate  S.E.\r\n#> ATT    0.057   0.012\r\n#>\r\n#> EDR increases voter turnout by ~5.7 percentage points\r\n```\r\n\r\n### Assumptions and Diagnostics\r\n\r\n1. **Factor structure**: Outcomes follow an interactive fixed effects model\r\n2. **Pre-treatment fit**: Check that pseudo-effects are zero before treatment\r\n3. **Number of factors**: Use cross-validation (CV=TRUE) to select\r\n\r\n```r\r\n# Check pre-treatment fit (pseudo-treatment effects should be ~0)\r\n# The plot() function shows this automatically\r\n# Pre-treatment periods should have estimates near zero with CIs crossing zero\r\n```\r\n\r\n---\r\n\r\n## 3. Fixed Effects Counterfactual Estimator (fect)\r\n\r\n### When to Use\r\n\r\nThe `fect` package provides a unified framework supporting:\r\n\r\n- **Matrix completion** (MC) for complex missing data patterns\r\n- **Interactive fixed effects** (IFE)\r\n- **Standard fixed effects** (FE)\r\n- **Treatment reversal** (units can switch treatment on and off)\r\n- **Built-in diagnostic tests** for pre-trends, placebo, and carryover effects\r\n\r\n### The simdata Dataset\r\n\r\nThe `fect` package includes its own `simdata` with treatment that can switch on and off.\r\n\r\n```r\r\nlibrary(fect)\r\n\r\n# Load fect's datasets\r\ndata(fect)\r\n\r\n# Available: simdata, turnout\r\n# simdata has 200 units, 35 time periods, with treatment reversal\r\n\r\nhead(simdata)\r\n#>   id time Y D X1 X2\r\n#> 1  1    1 ... 0 ...\r\n```\r\n\r\n### Method Comparison\r\n\r\n```r\r\n# Matrix Completion method (recommended for complex factor structures)\r\nfect_mc <- fect(\r\n  Y ~ D + X1 + X2,\r\n  data = simdata,\r\n  index = c(\"id\", \"time\"),\r\n  method = \"mc\",               # Matrix completion\r\n  force = \"two-way\",\r\n  CV = TRUE,                   # Cross-validate tuning parameter\r\n  se = TRUE,\r\n  nboots = 200,\r\n  parallel = TRUE,\r\n  cores = 4,\r\n  seed = 12345\r\n)\r\n\r\nprint(fect_mc)\r\n#> Method: Matrix Completion\r\n#> ATT (average): 1.234 (0.089)\r\n#> ...\r\n\r\n# Interactive Fixed Effects\r\nfect_ife <- fect(\r\n  Y ~ D + X1 + X2,\r\n  data = simdata,\r\n  index = c(\"id\", \"time\"),\r\n  method = \"ife\",              # Interactive fixed effects\r\n  r = 2,                       # Number of factors (or use CV)\r\n  force = \"two-way\",\r\n  se = TRUE,\r\n  nboots = 200,\r\n  seed = 12345\r\n)\r\n\r\n# Standard Two-Way Fixed Effects\r\nfect_fe <- fect(\r\n  Y ~ D + X1 + X2,\r\n  data = simdata,\r\n  index = c(\"id\", \"time\"),\r\n  method = \"fe\",               # Standard TWFE\r\n  force = \"two-way\",\r\n  se = TRUE,\r\n  nboots = 200,\r\n  seed = 12345\r\n)\r\n```\r\n\r\n### Diagnostic Tests\r\n\r\n```r\r\n# 1. Placebo Test: Estimate effects in pre-treatment period\r\nfect_placebo <- fect(\r\n  Y ~ D + X1 + X2,\r\n  data = simdata,\r\n  index = c(\"id\", \"time\"),\r\n  method = \"mc\",\r\n  force = \"two-way\",\r\n  placeboTest = TRUE,\r\n  placebo.period = c(-3, 0),   # Test 3 periods before treatment\r\n  se = TRUE,\r\n  nboots = 200\r\n)\r\n# Significant pre-treatment effects suggest parallel trends violation\r\n\r\n# 2. Pre-trend Test: Formal test for pre-treatment trends\r\nfect_pretrend <- fect(\r\n  Y ~ D + X1 + X2,\r\n  data = simdata,\r\n  index = c(\"id\", \"time\"),\r\n  method = \"mc\",\r\n  force = \"two-way\",\r\n  pretrendTest = TRUE,\r\n  pretrend.period = c(-3, -1)  # Exclude period 0\r\n)\r\n# p-value < 0.05 indicates problematic pre-trends\r\n\r\n# 3. Carryover Test: Check if effects persist after treatment ends\r\nfect_carryover <- fect(\r\n  Y ~ D + X1 + X2,\r\n  data = simdata,\r\n  index = c(\"id\", \"time\"),\r\n  method = \"mc\",\r\n  force = \"two-way\",\r\n  carryoverTest = TRUE,\r\n  carryover.period = c(1, 3)   # 1-3 periods after treatment ends\r\n)\r\n```\r\n\r\n### Visualization\r\n\r\n```r\r\n# Main results\r\nplot(fect_mc)\r\n#> Shows ATT by period relative to treatment\r\n#> With 95% confidence intervals\r\n\r\n# Counterfactual comparison\r\nplot(fect_mc, type = \"counterfactual\")\r\n\r\n# Raw data visualization (requires panelView package)\r\nlibrary(panelView)\r\npanelview(Y ~ D, data = simdata, index = c(\"id\", \"time\"),\r\n          type = \"treat\", main = \"Treatment Status Over Time\")\r\n```\r\n\r\n---\r\n\r\n## 4. Augmented Synthetic Control (augsynth)\r\n\r\n### When to Use\r\n\r\nAugmented synthetic control (Ben-Michael et al. 2021) improves on basic SCM by:\r\n\r\n- **Reducing bias** from imperfect pre-treatment fit\r\n- **Using ridge regression** to augment the synthetic control weights\r\n- **Providing valid inference** without permutation tests\r\n\r\nUse when your basic synthetic control has poor pre-treatment fit.\r\n\r\n### The kansas Dataset\r\n\r\nThe `augsynth` package includes data on Kansas's 2012 income tax cuts.\r\n\r\n```r\r\nlibrary(augsynth)\r\n\r\n# Load kansas data\r\ndata(kansas)\r\n\r\n# Examine structure\r\nhead(kansas)\r\n#>   fips year_qtr lngdpcapita treated state\r\n#> 1    1   1990.0       10.23       0    AL\r\n#> 2    1   1990.25      10.24       0    AL\r\n#> ...\r\n\r\n# Kansas: state FIPS = 20\r\n# Treatment: Q2 2012 tax cuts\r\n# Outcome: log GDP per capita\r\n\r\ncat(\"States:\", length(unique(kansas$fips)), \"\\n\")\r\n#> States: 50\r\ncat(\"Time periods:\", length(unique(kansas$year_qtr)), \"\\n\")\r\n#> Time periods: 105 (quarterly, 1990 Q1 to 2016 Q1)\r\n```\r\n\r\n### Basic Synthetic Control (for comparison)\r\n\r\n```r\r\n# Pure synthetic control without augmentation\r\nsyn_basic <- augsynth(\r\n  lngdpcapita ~ treated,\r\n  fips,\r\n  year_qtr,\r\n  kansas,\r\n  progfunc = \"None\",           # No augmentation\r\n  scm = TRUE\r\n)\r\n\r\nsummary(syn_basic)\r\n#>\r\n#> Overall Balance:\r\n#>                   Treated   Synthetic Control\r\n#> lngdpcapita        10.94        10.84\r\n#>\r\n#> L2 Imbalance: 0.083\r\n#> Percent improvement from uniform weights: 79.8%\r\n#>\r\n#> ATT Estimate: -0.029 (SE: 0.012)\r\n```\r\n\r\n### Ridge-Augmented Synthetic Control\r\n\r\n```r\r\n# Ridge-augmented SCM\r\nsyn_ridge <- augsynth(\r\n  lngdpcapita ~ treated,\r\n  fips,\r\n  year_qtr,\r\n  kansas,\r\n  progfunc = \"Ridge\",          # Ridge augmentation\r\n  scm = TRUE\r\n)\r\n\r\nsummary(syn_ridge)\r\n#>\r\n#> Overall Balance:\r\n#>                   Treated   Synthetic Control\r\n#> lngdpcapita        10.94        10.92\r\n#>\r\n#> L2 Imbalance: 0.021\r\n#> Percent improvement from uniform weights: 94.7%\r\n#>\r\n#> ATT Estimate: -0.024 (SE: 0.010)\r\n#>\r\n#> Note: Ridge augmentation reduced L2 imbalance from 0.083 to 0.021\r\n```\r\n\r\n### Visualization\r\n\r\n```r\r\n# Plot treatment effect over time\r\nplot(syn_ridge)\r\n#> Shows ATT at each post-treatment period\r\n#> With pointwise confidence intervals\r\n\r\n# Compare basic vs augmented\r\nplot(syn_basic, main = \"Basic SCM\")\r\nplot(syn_ridge, main = \"Ridge-Augmented SCM\")\r\n```\r\n\r\n### With Covariates\r\n\r\n```r\r\n# Include covariates for better balance\r\nsyn_cov <- augsynth(\r\n  lngdpcapita ~ treated | unemployment + population,\r\n  fips,\r\n  year_qtr,\r\n  kansas,\r\n  progfunc = \"Ridge\",\r\n  scm = TRUE\r\n)\r\n```\r\n\r\n### Staggered Adoption (multisynth)\r\n\r\nFor multiple units treated at different times:\r\n\r\n```r\r\n# Staggered adoption synthetic control\r\nmulti_out <- multisynth(\r\n  lngdpcapita ~ treated,\r\n  fips,\r\n  year_qtr,\r\n  kansas,\r\n  n_leads = 8                  # Post-treatment periods to estimate\r\n)\r\n\r\nsummary(multi_out)\r\nplot(multi_out)\r\n```\r\n\r\n---\r\n\r\n## 5. Synthetic Difference-in-Differences (synthdid)\r\n\r\n### When to Use\r\n\r\nSynthetic DiD (Arkhangelsky et al. 2021) combines the strengths of:\r\n\r\n- **Synthetic control**: Flexibly reweights control units\r\n- **Difference-in-differences**: Uses pre/post comparison\r\n\r\nIt provides doubly-robust estimation and valid inference for settings with both unit and time heterogeneity.\r\n\r\n### The california_prop99 Dataset\r\n\r\nThe package includes California's Proposition 99 (1988 tobacco tax) data, a canonical synthetic control application.\r\n\r\n```r\r\nlibrary(synthdid)\r\n\r\n# Load data\r\ndata('california_prop99')\r\n\r\n# Examine structure\r\nhead(california_prop99)\r\n#>      State Year PacksPerCapita treated\r\n#> 1  Alabama 1970          116.5       0\r\n#> 2  Alabama 1971          116.0       0\r\n#> ...\r\n\r\n# California (treated) vs 38 control states\r\n# Treatment: 1989 (Prop 99 tobacco tax)\r\n# Outcome: Cigarette packs per capita\r\n\r\ncat(\"States:\", length(unique(california_prop99$State)), \"\\n\")\r\n#> States: 39\r\ncat(\"Years:\", range(california_prop99$Year), \"\\n\")\r\n#> Years: 1970 2000\r\ncat(\"Pre-treatment periods:\", sum(california_prop99$Year < 1989), \"/\",\r\n    nrow(california_prop99), \"\\n\")\r\n```\r\n\r\n### Step 1: Create Panel Matrices\r\n\r\n```r\r\n# Convert to panel matrix format\r\nsetup <- panel.matrices(california_prop99)\r\n\r\n# The setup contains:\r\n# Y: Outcome matrix (states x years)\r\n# N0: Number of control units (38)\r\n# T0: Number of pre-treatment periods (19)\r\n\r\ncat(\"Y dimensions:\", dim(setup$Y), \"\\n\")\r\n#> Y dimensions: 39 31\r\ncat(\"N0 (control units):\", setup$N0, \"\\n\")\r\n#> N0 (control units): 38\r\ncat(\"T0 (pre-treatment periods):\", setup$T0, \"\\n\")\r\n#> T0 (pre-treatment periods): 19\r\n```\r\n\r\n### Step 2: Estimate Treatment Effect\r\n\r\n```r\r\n# Synthetic difference-in-differences estimate\r\ntau_hat <- synthdid_estimate(setup$Y, setup$N0, setup$T0)\r\n\r\nprint(tau_hat)\r\n#> synthdid: -15.604 +- NA\r\n#> Effective N0/N0 = 16.4/38 ~ 0.4\r\n#> Effective T0/T0 = 2.8/19 ~ 0.1\r\n#> N1, T1 = 1, 12\r\n\r\n# The estimate: Prop 99 reduced cigarette consumption by ~15.6 packs per capita\r\n```\r\n\r\n### Step 3: Standard Errors\r\n\r\n```r\r\n# For single treated unit, use placebo method\r\nse <- sqrt(vcov(tau_hat, method = 'placebo'))\r\n\r\n# Confidence interval\r\ncat(\"Point estimate:\", round(tau_hat[1], 2), \"\\n\")\r\n#> Point estimate: -15.6\r\ncat(\"Standard error:\", round(se, 2), \"\\n\")\r\n#> Standard error: 2.41\r\ncat(\"95% CI: (\", round(tau_hat[1] - 1.96 * se, 2), \", \",\r\n    round(tau_hat[1] + 1.96 * se, 2), \")\\n\")\r\n#> 95% CI: (-20.33, -10.87)\r\n```\r\n\r\n### Step 4: Visualization\r\n\r\n```r\r\n# Main plot with confidence intervals\r\nplot(tau_hat, se.method = 'placebo')\r\n#> Shows California vs synthetic California\r\n#> With shaded confidence band\r\n\r\n# Unit weights (which states contribute to synthetic control)\r\nsynthdid_units_plot(tau_hat)\r\n#> Bar chart of weights on control states\r\n\r\n# Time weights (which pre-periods matter most)\r\nsynthdid_time_plot(tau_hat)\r\n```\r\n\r\n### Compare with Other Estimators\r\n\r\n```r\r\n# Standard difference-in-differences\r\ntau_did <- did_estimate(setup$Y, setup$N0, setup$T0)\r\n\r\n# Traditional synthetic control\r\ntau_sc <- sc_estimate(setup$Y, setup$N0, setup$T0)\r\n\r\n# Compare estimates\r\ncat(\"Synthetic DiD:\", round(tau_hat[1], 2), \"\\n\")\r\ncat(\"Standard DiD:\", round(tau_did[1], 2), \"\\n\")\r\ncat(\"Synthetic Control:\", round(tau_sc[1], 2), \"\\n\")\r\n#> Synthetic DiD: -15.60\r\n#> Standard DiD: -27.35\r\n#> Synthetic Control: -19.62\r\n```\r\n\r\n---\r\n\r\n## 6. Conjoint Analysis\r\n\r\n### When to Use\r\n\r\nConjoint experiments estimate preferences over multi-attribute alternatives:\r\n\r\n- **Survey experiments** where respondents choose between profiles\r\n- **Estimating attribute importance** in decision-making\r\n- **Testing for interaction effects** between attributes\r\n\r\n### The immigration Dataset\r\n\r\nThe `cregg` package includes data from Hainmueller, Hopkins, and Yamamoto (2014) on immigration preferences.\r\n\r\n```r\r\nlibrary(cregg)\r\n\r\n# Load immigration data\r\ndata(\"immigration\")\r\n\r\n# Examine structure\r\nhead(immigration[, 1:8])\r\n#>   CaseID contest_no choice Gender Education LanguageSkills\r\n#> 1      1          1      1 Female  Two-year   Fluent English\r\n#> ...\r\n\r\n# Key variables:\r\n# - CaseID: Respondent ID\r\n# - contest_no: Choice task number\r\n# - ChosenImmigrant: Binary choice (1 = chosen)\r\n# - Gender, Education, LanguageSkills, etc.: Immigrant attributes\r\n\r\n# Profile attributes\r\ncat(\"Attributes:\\n\")\r\ncat(\"- Gender:\", levels(immigration$Gender), \"\\n\")\r\ncat(\"- Education:\", levels(immigration$Education), \"\\n\")\r\ncat(\"- Language:\", levels(immigration$LanguageSkills), \"\\n\")\r\ncat(\"- Job:\", levels(immigration$Job), \"\\n\")\r\n#> Attributes:\r\n#> - Gender: Female Male\r\n#> - Education: No formal 4th grade 8th grade High school ...\r\n#> - Language: Fluent English Broken English Tried English but unable Used interpreter\r\n#> - Job: Janitor Waiter Child care provider ... Doctor\r\n```\r\n\r\n### Average Marginal Component Effects (AMCE)\r\n\r\nAMCEs estimate the causal effect of changing an attribute level on profile selection probability, averaged over all other attribute combinations.\r\n\r\n```r\r\n# Estimate AMCEs for key attributes\r\namce_results <- cj(\r\n  data = immigration,\r\n  formula = ChosenImmigrant ~ Gender + Education + LanguageSkills + Job,\r\n  id = ~ CaseID,\r\n  estimate = \"amce\"\r\n)\r\n\r\n# View results\r\nprint(amce_results)\r\n#>          feature           level   estimate  std.error     z      p\r\n#> 1         Gender          Female    0.000       NA       NA     NA\r\n#> 2         Gender            Male   -0.021    0.008   -2.625  0.009\r\n#> 3      Education       No formal    0.000       NA       NA     NA\r\n#> 4      Education       4th grade    0.016    0.015    1.067  0.286\r\n#> 5      Education       8th grade    0.022    0.015    1.467  0.142\r\n#> 6      Education     High school    0.058    0.014    4.143  0.000\r\n#> ...\r\n#> 15 LanguageSkills  Fluent English    0.000       NA       NA     NA\r\n#> 16 LanguageSkills  Broken English   -0.065    0.010   -6.500  0.000\r\n#> ...\r\n\r\n# Plot AMCEs\r\nplot(amce_results)\r\n#> Shows coefficient plot with 95% CIs\r\n#> Reference categories shown at 0\r\n#> Positive values = increase probability of selection\r\n```\r\n\r\n### Marginal Means\r\n\r\nMarginal means show the average probability of selection for each attribute level.\r\n\r\n```r\r\n# Estimate marginal means\r\nmm_results <- cj(\r\n  data = immigration,\r\n  formula = ChosenImmigrant ~ Gender + Education + LanguageSkills,\r\n  id = ~ CaseID,\r\n  estimate = \"mm\"\r\n)\r\n\r\nprint(mm_results)\r\n#>          feature           level  estimate  std.error\r\n#> 1         Gender          Female    0.510     0.006\r\n#> 2         Gender            Male    0.490     0.006\r\n#> ...\r\n#> Values near 0.5 = neutral (no preference)\r\n#> Values > 0.5 = positive preference\r\n#> Values < 0.5 = negative preference\r\n\r\nplot(mm_results)\r\n```\r\n\r\n### Subgroup Analysis\r\n\r\n```r\r\n# AMCEs by respondent political party\r\namce_by_party <- cj(\r\n  data = immigration,\r\n  formula = ChosenImmigrant ~ Gender + Education + LanguageSkills,\r\n  id = ~ CaseID,\r\n  estimate = \"amce\",\r\n  by = ~ ethnocentrism  # Subgroup variable (if available)\r\n)\r\n\r\n# Plot comparison across groups\r\nplot(amce_by_party, group = \"ethnocentrism\")\r\n```\r\n\r\n### Interaction Effects\r\n\r\n```r\r\n# Test for interaction between attributes\r\namce_interact <- cj(\r\n  data = immigration,\r\n  formula = ChosenImmigrant ~ Gender * Education + LanguageSkills,\r\n  id = ~ CaseID,\r\n  estimate = \"amce\"\r\n)\r\n\r\n# Marginal means by attribute combinations\r\nmm_interact <- mm(\r\n  data = immigration,\r\n  formula = ChosenImmigrant ~ Gender + Education,\r\n  id = ~ CaseID,\r\n  by = ~ Gender  # Condition on Gender\r\n)\r\n```\r\n\r\n### Additional Dataset: taxes\r\n\r\nThe package also includes a tax policy conjoint experiment.\r\n\r\n```r\r\ndata(\"taxes\")\r\n\r\n# Examine\r\nhead(taxes[, 1:6])\r\n\r\n# Estimate preferences for tax policy attributes\r\ntax_amce <- cj(\r\n  data = taxes,\r\n  formula = chose_plan ~ taxrate1 + taxrate2 + taxrate3,\r\n  id = ~ ID,\r\n  estimate = \"amce\"\r\n)\r\n```\r\n\r\n---\r\n\r\n## 7. Heterogeneous Treatment Effects with Causal Forests\r\n\r\n### When to Use\r\n\r\nCausal forests (Wager and Athey 2018) estimate:\r\n\r\n- **Conditional Average Treatment Effects (CATE)**: How treatment effects vary with covariates\r\n- **Treatment effect heterogeneity** across subpopulations\r\n- **Optimal treatment assignment** based on predicted effects\r\n\r\n### Setup with Simulated Data\r\n\r\nThe `grf` package doesn't include built-in datasets, but its documentation provides standard simulation designs.\r\n\r\n```r\r\nlibrary(grf)\r\n\r\n# Simulate data with heterogeneous treatment effects\r\nset.seed(12345)\r\nn <- 2000\r\np <- 10\r\n\r\n# Covariates\r\nX <- matrix(rnorm(n * p), n, p)\r\ncolnames(X) <- paste0(\"X\", 1:p)\r\n\r\n# Treatment assignment (confounded by X[,1])\r\npropensity <- 0.4 + 0.2 * (X[, 1] > 0)\r\nW <- rbinom(n, 1, propensity)\r\n\r\n# True treatment effect (heterogeneous in X[,1])\r\ntau_true <- pmax(X[, 1], 0)  # Effect is positive for X[,1] > 0\r\n\r\n# Outcome\r\nY <- tau_true * W + X[, 2] + pmin(X[, 3], 0) + rnorm(n)\r\n\r\n# Data summary\r\ncat(\"N treated:\", sum(W), \"\\n\")\r\ncat(\"N control:\", sum(1-W), \"\\n\")\r\ncat(\"Mean(tau_true):\", round(mean(tau_true), 3), \"\\n\")\r\n#> N treated: 980\r\n#> N control: 1020\r\n#> Mean(tau_true): 0.398\r\n```\r\n\r\n### Train Causal Forest\r\n\r\n```r\r\n# Train causal forest\r\ncf <- causal_forest(\r\n  X = X,\r\n  Y = Y,\r\n  W = W,\r\n  num.trees = 2000,\r\n  honesty = TRUE,              # Honest splitting\r\n  seed = 12345\r\n)\r\n\r\n# Out-of-bag predictions of individual treatment effects\r\ntau_hat <- predict(cf)\r\n\r\n# Distribution of predicted effects\r\nhist(tau_hat$predictions, main = \"Predicted Treatment Effects\",\r\n     xlab = \"CATE estimate\", breaks = 30)\r\n#> Shows distribution of estimated treatment effects\r\n#> Should range from near 0 to positive values (matching tau_true)\r\n```\r\n\r\n### Average Treatment Effects\r\n\r\n```r\r\n# Average treatment effect (ATE)\r\nate <- average_treatment_effect(cf, target.sample = \"all\")\r\ncat(\"ATE:\", round(ate[1], 3), \"SE:\", round(ate[2], 3), \"\\n\")\r\n#> ATE: 0.412 SE: 0.056\r\n\r\n# ATT: Average effect on treated\r\natt <- average_treatment_effect(cf, target.sample = \"treated\")\r\ncat(\"ATT:\", round(att[1], 3), \"SE:\", round(att[2], 3), \"\\n\")\r\n#> ATT: 0.523 SE: 0.068\r\n\r\n# ATC: Average effect on controls\r\natc <- average_treatment_effect(cf, target.sample = \"control\")\r\ncat(\"ATC:\", round(atc[1], 3), \"SE:\", round(atc[2], 3), \"\\n\")\r\n#> ATC: 0.305 SE: 0.071\r\n```\r\n\r\n### Variable Importance\r\n\r\n```r\r\n# Which variables drive treatment effect heterogeneity?\r\nvar_imp <- variable_importance(cf)\r\n\r\n# Display\r\ndata.frame(\r\n  variable = colnames(X),\r\n  importance = round(var_imp, 3)\r\n) |>\r\n  dplyr::arrange(desc(importance)) |>\r\n  head(5)\r\n#>   variable importance\r\n#> 1       X1      0.621\r\n#> 2       X3      0.089\r\n#> 3       X2      0.074\r\n#> ...\r\n#> X1 has highest importance (correctly identifies treatment heterogeneity)\r\n```\r\n\r\n### Best Linear Projection\r\n\r\nTest which covariates predict treatment effect heterogeneity:\r\n\r\n```r\r\n# Best linear projection of CATE onto covariates\r\nblp <- best_linear_projection(cf, X[, 1:3])\r\n\r\nprint(blp)\r\n#> Best linear projection of the conditional average treatment effect.\r\n#>\r\n#>             Estimate Std. Error t value  Pr(>|t|)\r\n#> (Intercept)   0.089     0.034    2.618    0.009\r\n#> X1            0.498     0.024   20.750   <2e-16\r\n#> X2            0.003     0.024    0.125    0.901\r\n#> X3            0.021     0.024    0.875    0.382\r\n#>\r\n#> X1 coefficient (~0.5) matches the true effect modification\r\n```\r\n\r\n### Calibration Test\r\n\r\n```r\r\n# Test whether forest captures true heterogeneity\r\ntest_calibration(cf)\r\n#> Calibration test for differential forest prediction.\r\n#>\r\n#>                              mean.forest.prediction\r\n#> Estimate                                      0.398\r\n#> Std. Error                                    0.056\r\n#> t value                                       7.107\r\n#> Pr(>|t|)                                     <2e-16\r\n#>\r\n#> Significant coefficient on predictions indicates the forest\r\n#> captures real treatment effect heterogeneity\r\n```\r\n\r\n### Prediction for New Data\r\n\r\n```r\r\n# New observations\r\nX_new <- matrix(c(\r\n  -1, 0, 0, 0, 0, 0, 0, 0, 0, 0,  # Low X1 -> expect low effect\r\n   0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  # Moderate X1 -> moderate effect\r\n   2, 0, 0, 0, 0, 0, 0, 0, 0, 0   # High X1 -> high effect\r\n), nrow = 3, byrow = TRUE)\r\n\r\n# Predictions with confidence intervals\r\npred <- predict(cf, X_new, estimate.variance = TRUE)\r\n\r\ndata.frame(\r\n  X1 = X_new[, 1],\r\n  tau_hat = round(pred$predictions, 3),\r\n  se = round(sqrt(pred$variance.estimates), 3),\r\n  ci_lower = round(pred$predictions - 1.96*sqrt(pred$variance.estimates), 3),\r\n  ci_upper = round(pred$predictions + 1.96*sqrt(pred$variance.estimates), 3)\r\n)\r\n#>   X1 tau_hat    se ci_lower ci_upper\r\n#> 1 -1   0.012 0.089   -0.162    0.186\r\n#> 2  0   0.198 0.078    0.045    0.351\r\n#> 3  2   1.823 0.102    1.623    2.023\r\n```\r\n\r\n---\r\n\r\n## 8. Heterogeneous Treatment Effects with Panel Data\r\n\r\n### Subgroup Analysis with fixest\r\n\r\n```r\r\nlibrary(fixest)\r\n\r\n# Create example panel data\r\nset.seed(12345)\r\nn_units <- 100\r\nn_time <- 20\r\n\r\ndf <- expand.grid(\r\n  unit = 1:n_units,\r\n  time = 1:n_time\r\n)\r\n\r\n# Treatment: first 50 units treated after period 10\r\ndf$treated <- as.integer(df$unit <= 50 & df$time > 10)\r\n\r\n# Subgroups\r\ndf$subgroup <- ifelse(df$unit <= 25, \"High\",\r\n                      ifelse(df$unit <= 50, \"Medium\", \"Control\"))\r\n\r\n# Heterogeneous effects by subgroup\r\ndf$tau <- ifelse(df$subgroup == \"High\", 2.0,\r\n                 ifelse(df$subgroup == \"Medium\", 0.5, 0))\r\n\r\n# Outcome\r\ndf$outcome <- 1 + df$tau * df$treated + 0.1 * df$time + rnorm(nrow(df))\r\n\r\n# Overall effect\r\nmodel_overall <- feols(\r\n  outcome ~ treated | unit + time,\r\n  data = df\r\n)\r\n\r\nsummary(model_overall)\r\n#> OLS estimation, Dep. Var.: outcome\r\n#> ...\r\n#> treated   1.245***  (0.089)\r\n#> ...\r\n```\r\n\r\n### Split-sample Estimation\r\n\r\n```r\r\n# Effects by subgroup using split option\r\nmodel_split <- feols(\r\n  outcome ~ treated | unit + time,\r\n  data = df,\r\n  split = ~ subgroup\r\n)\r\n\r\n# View results for each subgroup\r\netable(model_split)\r\n#>                    High     Medium    Control\r\n#> treated         2.01***   0.52***    -0.02\r\n#>                 (0.14)    (0.14)     (0.12)\r\n```\r\n\r\n### Interaction-based HTE\r\n\r\n```r\r\n# Create continuous moderator\r\ndf$moderator <- rnorm(nrow(df))\r\n\r\n# Interaction model\r\nmodel_interact <- feols(\r\n  outcome ~ treated * moderator | unit + time,\r\n  data = df\r\n)\r\n\r\nsummary(model_interact)\r\n#>                        Estimate Std.Error t value\r\n#> treated                   1.243     0.089   13.97\r\n#> moderator                 0.012     0.022    0.55\r\n#> treated:moderator         0.089     0.031    2.87\r\n```\r\n\r\n---\r\n\r\n## 9. Method Selection Guide\r\n\r\n### Decision Tree for Choosing a Method\r\n\r\n```\r\nIs your setting experimental or observational?\r\n|\r\n+-- Experimental (RCT/conjoint)\r\n|   |\r\n|   +-- Conjoint/factorial design --> Use cregg\r\n|   +-- Standard RCT with covariates --> Use grf (causal forest)\r\n|\r\n+-- Observational (natural experiment)\r\n    |\r\n    +-- How many treated units?\r\n        |\r\n        +-- Single treated unit\r\n        |   |\r\n        |   +-- Good pre-treatment fit likely? --> Use Synth (basic SCM)\r\n        |   +-- Poor fit expected? --> Use augsynth (ridge-augmented)\r\n        |\r\n        +-- Multiple treated units\r\n            |\r\n            +-- Staggered adoption?\r\n            |   |\r\n            |   +-- Yes, with possible treatment reversal --> Use fect\r\n            |   +-- Yes, one-time treatment --> Use gsynth or multisynth\r\n            |\r\n            +-- Block treatment (same timing)?\r\n                |\r\n                +-- Want DiD + SCM hybrid --> Use synthdid\r\n                +-- Interactive fixed effects --> Use gsynth\r\n```\r\n\r\n### Comparative Advantages\r\n\r\n| Method | Strengths | Weaknesses |\r\n|--------|-----------|------------|\r\n| Synth | Interpretable weights, well-established | Single unit, no formal inference |\r\n| gsynth | Multiple units, cross-validated factors | Requires sufficient pre-periods |\r\n| fect | Treatment reversal, diagnostic tests | Computationally intensive |\r\n| augsynth | Better fit, valid inference | Requires understanding of bias-variance tradeoff |\r\n| synthdid | Doubly robust, valid inference | Relatively new, less established |\r\n| cregg | Easy conjoint analysis | Limited to experimental data |\r\n| grf | Flexible HTE estimation | Requires sufficient sample size |\r\n\r\n---\r\n\r\n## 10. Reporting Guidelines\r\n\r\n### Synthetic Control Papers Should Report\r\n\r\n1. **Pre-treatment fit quality**\r\n   - MSPE (mean squared prediction error)\r\n   - Visual comparison of treated vs synthetic\r\n\r\n2. **Weights table**\r\n   - Which control units contribute\r\n   - Weight assigned to each\r\n\r\n3. **Predictor balance**\r\n   - Comparison of treated vs synthetic on matching variables\r\n\r\n4. **Placebo/robustness tests**\r\n   - In-time placebos (fake treatment dates)\r\n   - In-space placebos (run SCM for control units)\r\n   - Leave-one-out (drop largest weight contributor)\r\n\r\n5. **Inference**\r\n   - Permutation-based p-values\r\n   - MSPE ratios for placebo tests\r\n\r\n### Example Reporting\r\n\r\n```r\r\n# Generate tables for publication\r\nsynth_tables <- synth.tab(\r\n  dataprep.res = dataprep_out,\r\n  synth.res = synth_out\r\n)\r\n\r\n# Predictor balance table\r\nknitr::kable(\r\n  synth_tables$tab.pred,\r\n  caption = \"Covariate Balance: Treated vs Synthetic Control\",\r\n  digits = 3\r\n)\r\n\r\n# Unit weights table\r\nweights_df <- data.frame(\r\n  Unit = names(synth_out$solution.w),\r\n  Weight = round(synth_out$solution.w, 3)\r\n)\r\nweights_df <- weights_df[weights_df$Weight > 0.001, ]\r\nknitr::kable(weights_df, caption = \"Synthetic Control Weights\")\r\n```\r\n\r\n---\r\n\r\n## References\r\n\r\n- Abadie, A., Diamond, A., & Hainmueller, J. (2010). Synthetic control methods for comparative case studies. *Journal of the American Statistical Association*, 105(490), 493-505.\r\n\r\n- Abadie, A., Diamond, A., & Hainmueller, J. (2011). Synth: An R package for synthetic control methods. *Journal of Statistical Software*, 42(13), 1-17.\r\n\r\n- Arkhangelsky, D., Athey, S., Hirshberg, D. A., Imbens, G. W., & Wager, S. (2021). Synthetic difference-in-differences. *American Economic Review*, 111(12), 4088-4118.\r\n\r\n- Ben-Michael, E., Feller, A., & Rothstein, J. (2021). The augmented synthetic control method. *Journal of the American Statistical Association*, 116(536), 1789-1803.\r\n\r\n- Hainmueller, J., Hopkins, D. J., & Yamamoto, T. (2014). Causal inference in conjoint analysis: Understanding multidimensional choices via stated preference experiments. *Political Analysis*, 22(1), 1-30.\r\n\r\n- Wager, S., & Athey, S. (2018). Estimation and inference of heterogeneous treatment effects using random forests. *Journal of the American Statistical Association*, 113(523), 1228-1242.\r\n\r\n- Xu, Y. (2017). Generalized synthetic control method: Causal inference with interactive fixed effects models. *Political Analysis*, 25(1), 57-76.\r\n",
        "plugins/r-analyst/skills/r-analyst/techniques/05_bayesian_sensitivity.md": "# Bayesian Methods & Sensitivity Analysis\r\n\r\nBayesian regression with brms, omitted variable bias sensitivity analysis with sensemakr, and matching bounds for causal inference.\r\n\r\n---\r\n\r\n## Quick Reference\r\n\r\n| Method | Package | Key Function | When to Use |\r\n|--------|---------|--------------|-------------|\r\n| Bayesian regression | `brms` | `brm()` | Full posterior inference, uncertainty quantification |\r\n| Posterior draws | `brms` | `as_draws_df()` | Computing functions of parameters with uncertainty |\r\n| Model diagnostics | `brms` | `pp_check()`, `rhat()` | Checking model fit and convergence |\r\n| OVB sensitivity | `sensemakr` | `sensemakr()` | Assessing robustness to unobserved confounding |\r\n| OVB bounds | `sensemakr` | `ovb_bounds()` | Benchmarking confounders against observed covariates |\r\n| Matching balance | `MatchIt` | `matchit()` | Propensity score matching with balance checks |\r\n| Matching bounds | Custom LP | `lpSolve::lp()` | Sharp bounds on ATT under matching constraints |\r\n\r\n---\r\n\r\n## 1. Bayesian Regression with brms\r\n\r\n### When to Use\r\n\r\nBayesian regression is appropriate when you need:\r\n- Full posterior distributions for uncertainty quantification\r\n- Natural handling of complex hierarchical structures\r\n- Inference on nonlinear functions of parameters (e.g., ratios)\r\n- Incorporation of prior information\r\n- Probabilistic statements about parameters (e.g., \"90% probability effect is positive\")\r\n\r\n### Assumptions\r\n\r\n- Same distributional assumptions as frequentist regression (normality of errors for Gaussian models)\r\n- Prior specification (can use weakly informative defaults)\r\n- Sufficient data for posterior to overcome prior (for weak priors)\r\n- MCMC convergence (verified via diagnostics)\r\n\r\n### 1.1 Basic Bayesian Regression\r\n\r\nWe use the classic `mtcars` dataset to demonstrate Bayesian regression, predicting fuel efficiency (mpg) from vehicle characteristics.\r\n\r\n```r\r\nlibrary(brms)\r\nlibrary(dplyr)\r\n\r\n# Load built-in dataset\r\ndata(mtcars)\r\n\r\n# Examine the data\r\n#> str(mtcars)\r\n#> 'data.frame':\t32 obs. of  11 variables:\r\n#>  $ mpg : num  21 21 22.8 21.4 18.7 ...\r\n#>  $ cyl : num  6 6 4 6 8 ...\r\n#>  $ disp: num  160 160 108 258 360 ...\r\n#>  $ hp  : num  110 110 93 110 175 ...\r\n#>  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\r\n\r\n# Set seed for reproducibility\r\nset.seed(2024)\r\n\r\n# Bayesian regression with default weakly informative priors\r\nbayes_model <- brm(\r\n  mpg ~ wt + hp + cyl + am,\r\n  family = gaussian(),\r\n  data = mtcars,\r\n  chains = 4,\r\n  cores = 4,\r\n  iter = 2000,\r\n  warmup = 1000,\r\n  seed = 2024\r\n)\r\n\r\nsummary(bayes_model)\r\n#>  Family: gaussian\r\n#>   Links: mu = identity; sigma = identity\r\n#> Formula: mpg ~ wt + hp + cyl + am\r\n#>    Data: mtcars (Number of observations: 32)\r\n#>\r\n#> Population-Level Effects:\r\n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\n#> Intercept    36.15      2.80    30.61    41.62 1.00     3254     2807\r\n#> wt           -2.61      1.02    -4.61    -0.60 1.00     2156     2234\r\n#> hp           -0.02      0.01    -0.05     0.00 1.00     2545     2613\r\n#> cyl          -0.75      0.59    -1.89     0.44 1.00     2085     2309\r\n#> am            1.48      1.44    -1.35     4.34 1.00     2533     2626\r\n```\r\n\r\n### 1.2 Specifying Informative Priors\r\n\r\nWhen theory or prior research suggests constraints on parameters, you can specify informative priors.\r\n\r\n```r\r\n# Check default priors\r\nprior_summary(bayes_model)\r\n#>                  prior     class coef group resp dpar nlpar bound  source\r\n#>  student_t(3, 19.2, 5.4) Intercept                                default\r\n#>                  (flat)         b                                 default\r\n#>    student_t(3, 0, 5.4)     sigma                                 default\r\n\r\n# Specify informative priors\r\n# Theory: weight should have negative effect on mpg (heavier = less efficient)\r\n# Prior: Normal(-3, 1) centered on expected effect with some uncertainty\r\n\r\nbayes_constrained <- brm(\r\n  mpg ~ wt + hp + cyl,\r\n  family = gaussian(),\r\n  data = mtcars,\r\n  chains = 4,\r\n  iter = 2000,\r\n  warmup = 1000,\r\n  prior = c(\r\n    prior(normal(-3, 1), coef = wt),      # Informative prior on weight\r\n    prior(normal(0, 0.05), coef = hp),    # Weakly informative on horsepower\r\n    prior(normal(-1, 1), coef = cyl)      # Theory: more cylinders = less efficient\r\n  ),\r\n  seed = 2024\r\n)\r\n\r\n# Compare prior vs posterior\r\nprior_summary(bayes_constrained)\r\n```\r\n\r\n### 1.3 Working with Posterior Draws\r\n\r\nA key advantage of Bayesian analysis: compute any function of parameters with full uncertainty propagation.\r\n\r\n```r\r\nlibrary(dplyr)\r\n\r\n# Extract posterior draws\r\npost_samples <- as_draws_df(bayes_model)\r\n\r\n# View structure\r\n#> names(post_samples)\r\n#> [1] \"b_Intercept\" \"b_wt\" \"b_hp\" \"b_cyl\" \"b_am\" \"sigma\" ...\r\n\r\n# Calculate derived quantities: effect size (standardized)\r\n# Effect of 1 SD change in weight on mpg\r\nsd_wt <- sd(mtcars$wt)\r\nsd_mpg <- sd(mtcars$mpg)\r\n\r\npost_samples <- post_samples %>%\r\n  mutate(\r\n    # Standardized effect of weight\r\n    std_effect_wt = b_wt * sd_wt / sd_mpg,\r\n    # Ratio: weight effect relative to horsepower effect\r\n    wt_hp_ratio = b_wt / b_hp\r\n  )\r\n\r\n# Posterior summary for standardized effect\r\ncat(\"Standardized weight effect:\\n\")\r\ncat(\"  Median:\", round(median(post_samples$std_effect_wt), 3), \"\\n\")\r\ncat(\"  95% CI:\", round(quantile(post_samples$std_effect_wt, c(0.025, 0.975)), 3), \"\\n\")\r\n#> Standardized weight effect:\r\n#>   Median: -0.423\r\n#>   95% CI: -0.747 -0.097\r\n\r\n# Probability statements\r\ncat(\"\\nProbability weight effect is negative:\",\r\n    round(mean(post_samples$b_wt < 0), 3), \"\\n\")\r\n#> Probability weight effect is negative: 0.995\r\n\r\ncat(\"Probability |standardized effect| > 0.3:\",\r\n    round(mean(abs(post_samples$std_effect_wt) > 0.3), 3), \"\\n\")\r\n#> Probability |standardized effect| > 0.3: 0.812\r\n```\r\n\r\n### 1.4 Posterior Visualization\r\n\r\n```r\r\nlibrary(ggplot2)\r\n\r\n# Density plot of coefficient posterior\r\nggplot(post_samples, aes(x = b_wt)) +\r\n  geom_density(fill = \"steelblue\", alpha = 0.5) +\r\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\") +\r\n  geom_vline(xintercept = median(post_samples$b_wt),\r\n             linetype = \"solid\", color = \"darkblue\") +\r\n  labs(\r\n    x = \"Coefficient for Weight (1000 lbs)\",\r\n    y = \"Posterior Density\",\r\n    title = \"Posterior Distribution: Effect of Weight on MPG\",\r\n    subtitle = \"Vertical lines: zero (red dashed) and posterior median (blue)\"\r\n  ) +\r\n  theme_minimal()\r\n\r\n# Multiple coefficients comparison\r\nlibrary(tidyr)\r\n\r\ncoef_samples <- post_samples %>%\r\n  select(b_wt, b_hp, b_cyl, b_am) %>%\r\n  pivot_longer(everything(), names_to = \"coefficient\", values_to = \"value\") %>%\r\n  mutate(coefficient = gsub(\"b_\", \"\", coefficient))\r\n\r\nggplot(coef_samples, aes(x = value, fill = coefficient)) +\r\n  geom_density(alpha = 0.5) +\r\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\r\n  facet_wrap(~coefficient, scales = \"free\") +\r\n  labs(x = \"Coefficient Value\", y = \"Density\") +\r\n  theme_minimal() +\r\n  theme(legend.position = \"none\")\r\n```\r\n\r\n### 1.5 Model Diagnostics\r\n\r\nBayesian models require checking both convergence and model fit.\r\n\r\n```r\r\n# MCMC convergence diagnostics\r\n# Trace plots: should look like \"fuzzy caterpillars\" with good mixing\r\nplot(bayes_model, variable = c(\"b_wt\", \"b_hp\"))\r\n\r\n# R-hat: should be < 1.01 for convergence\r\nrhat_values <- rhat(bayes_model)\r\nprint(rhat_values)\r\n#>  b_Intercept        b_wt         b_hp        b_cyl         b_am        sigma\r\n#>     1.001234     1.000567     1.001890     1.000234     1.002345     1.000123\r\n\r\n# Check: all R-hat < 1.01?\r\nall(rhat_values < 1.01)\r\n#> [1] TRUE\r\n\r\n# Effective sample size ratio: should be > 0.1\r\nneff <- neff_ratio(bayes_model)\r\nprint(neff)\r\n#> b_Intercept        b_wt         b_hp        b_cyl         b_am        sigma\r\n#>       0.812       0.539       0.636       0.521       0.633       0.892\r\n\r\n# Posterior predictive check: observed data vs model predictions\r\npp_check(bayes_model, ndraws = 100)\r\n#> (generates plot comparing observed y to predicted y distributions)\r\n\r\n# Leave-one-out cross-validation for model comparison\r\nloo_result <- loo(bayes_model)\r\nprint(loo_result)\r\n#> Computed from 4000 by 32 log-likelihood matrix\r\n#>\r\n#>          Estimate   SE\r\n#> elpd_loo    -78.5  4.1\r\n#> p_loo         5.2  1.5\r\n#> looic       157.0  8.2\r\n```\r\n\r\n### Interpretation Guidelines\r\n\r\n| Diagnostic | Good Values | Problem Indicators |\r\n|------------|-------------|-------------------|\r\n| R-hat | < 1.01 | > 1.01 suggests non-convergence |\r\n| ESS ratio | > 0.1 | < 0.1 suggests inefficient sampling |\r\n| Trace plots | Mixed, stationary | Trending, stuck chains |\r\n| pp_check | Predicted matches observed | Systematic mismatch |\r\n\r\n---\r\n\r\n## 2. Sensitivity Analysis for Omitted Variable Bias\r\n\r\n### When to Use\r\n\r\nSensitivity analysis with `sensemakr` is appropriate when:\r\n- You have observational data with potential unobserved confounders\r\n- You want to assess how robust your estimates are to omitted variables\r\n- You need to communicate uncertainty about causal claims\r\n- You want to benchmark against observed covariates\r\n\r\n### Assumptions\r\n\r\n- Linear model specification is approximately correct\r\n- Treatment and outcome are continuous or can be reasonably modeled as such\r\n- Confounding operates through the standard OVB framework\r\n\r\n### 2.1 Basic Sensitivity Analysis with sensemakr\r\n\r\nWe use the Darfur dataset from Hazlett (2019), studying whether being directly harmed affects attitudes toward peace.\r\n\r\n```r\r\nlibrary(sensemakr)\r\n\r\n# Load the Darfur dataset\r\ndata(darfur)\r\n\r\n# Examine the data\r\n#> str(darfur)\r\n#> 'data.frame':\t1276 obs. of  14 variables:\r\n#>  $ peacefactor        : attitude toward peace (outcome)\r\n#>  $ directlyharmed     : whether directly harmed by violence (treatment)\r\n#>  $ age, female, etc.  : demographic controls\r\n\r\n# Fit the baseline regression\r\nmodel <- lm(peacefactor ~ directlyharmed + age + farmer_dar + herder_dar +\r\n              pastvoted + hhsize_darfur + female, data = darfur)\r\n\r\nsummary(model)\r\n#> Coefficients:\r\n#>                   Estimate Std. Error t value  Pr(>|t|)\r\n#> (Intercept)      0.5182442  0.0380259  13.629  < 2e-16 ***\r\n#> directlyharmed   0.0488855  0.0183919   2.658  0.00796 **\r\n#> age             -0.0005638  0.0006710  -0.840  0.40095\r\n#> farmer_dar      -0.1047794  0.0245623  -4.266  2.1e-05 ***\r\n#> herder_dar       0.0538784  0.0257942   2.089  0.03693 *\r\n#> pastvoted       -0.0077827  0.0194504  -0.400  0.68913\r\n#> hhsize_darfur    0.0001346  0.0016479   0.082  0.93492\r\n#> female          -0.2450242  0.0190455 -12.865  < 2e-16 ***\r\n```\r\n\r\n### 2.2 Running sensemakr Analysis\r\n\r\n```r\r\n# Sensitivity analysis\r\n# Key question: How strong would an unobserved confounder need to be\r\n# to explain away the treatment effect?\r\n\r\nsensitivity <- sensemakr(\r\n  model = model,\r\n  treatment = \"directlyharmed\",\r\n  benchmark_covariates = \"female\",  # Use female as benchmark\r\n  kd = 1:3,                          # Confounders 1x, 2x, 3x as strong as female\r\n  q = 1                              # For nullifying the effect (q=1 means reduce to 0)\r\n)\r\n\r\nsummary(sensitivity)\r\n#> Sensitivity Analysis to Unobserved Confounding\r\n#>\r\n#> Model Formula: peacefactor ~ directlyharmed + age + farmer_dar + herder_dar +\r\n#>                pastvoted + hhsize_darfur + female\r\n#>\r\n#> Null hypothesis: q = 1 and reduce = TRUE\r\n#>\r\n#> Unadjusted Estimates of 'directlyharmed':\r\n#>   Coef. estimate: 0.0489\r\n#>   Standard Error: 0.0184\r\n#>   t-value (H0:tau = 0): 2.658\r\n#>\r\n#> Sensitivity Statistics:\r\n#>   Partial R2 of treatment with outcome: 0.0055\r\n#>   Robustness Value, q = 1: 0.0719\r\n#>   Robustness Value, q = 1, alpha = 0.05: 0.0193\r\n```\r\n\r\n### 2.3 Interpreting Robustness Values\r\n\r\nThe **Robustness Value (RV)** tells you how strong confounding would need to be:\r\n\r\n```r\r\n# Extract key sensitivity statistics\r\nrv_q1 <- sensitivity$sensitivity_stats$rv_q\r\nrv_qa <- sensitivity$sensitivity_stats$rv_qa\r\n\r\ncat(\"Robustness Value (RV_q=1):\", round(rv_q1, 3), \"\\n\")\r\ncat(\"Robustness Value (RV_q=1, alpha=0.05):\", round(rv_qa, 3), \"\\n\")\r\n#> Robustness Value (RV_q=1): 0.072\r\n#> Robustness Value (RV_q=1, alpha=0.05): 0.019\r\n\r\n# Interpretation:\r\n# RV = 0.072 means an unobserved confounder would need to explain\r\n# 7.2% of the residual variance of BOTH treatment and outcome\r\n# to fully explain away the effect.\r\n```\r\n\r\n**Interpretation Table:**\r\n\r\n| RV Value | Interpretation |\r\n|----------|----------------|\r\n| > 0.20 | Very robust - confounding would need to be stronger than most observed covariates |\r\n| 0.10 - 0.20 | Moderately robust - requires substantial confounding |\r\n| 0.05 - 0.10 | Somewhat fragile - moderate confounding could matter |\r\n| < 0.05 | Fragile - even weak confounding could explain the effect |\r\n\r\n### 2.4 Benchmarking Against Observed Covariates\r\n\r\nA key insight: compare hypothetical confounders to observed variables.\r\n\r\n```r\r\n# Compute bounds using observed covariates as benchmarks\r\nbounds <- ovb_bounds(\r\n  model = model,\r\n  treatment = \"directlyharmed\",\r\n  benchmark_covariates = c(\"female\", \"age\", \"pastvoted\"),\r\n  kd = 1:3,  # Multiples of benchmark strength\r\n  ky = 1:3\r\n)\r\n\r\nprint(bounds)\r\n#>   bound_label                 r2dz.x   r2yz.dx  treatment adjusted_estimate\r\n#> 1 1x female   0.009     0.001      directlyharmed  0.0945\r\n#> 2 2x female   0.036     0.004      directlyharmed  0.0896\r\n#> 3 3x female   0.081     0.010      directlyharmed  0.0826\r\n#> ...\r\n\r\n# Interpretation: Even a confounder 3x as strong as 'female'\r\n# would only reduce the estimate from 0.097 to 0.083\r\n```\r\n\r\n### 2.5 Contour Plots for Sensitivity\r\n\r\n```r\r\n# Contour plot showing how estimate changes with different confounding strengths\r\nplot(sensitivity)\r\n#> (generates contour plot with R2 of confounder with treatment on x-axis\r\n#>  and R2 of confounder with outcome on y-axis)\r\n\r\n# Extreme scenario plot\r\nplot(sensitivity, type = \"extreme\")\r\n#> (shows worst-case scenarios for different confounding assumptions)\r\n\r\n# Custom contour with benchmarks\r\novb_contour_plot(\r\n  model = model,\r\n  treatment = \"directlyharmed\",\r\n  benchmark_covariates = c(\"female\", \"pastvoted\"),\r\n  kd = 1:3\r\n)\r\n```\r\n\r\n### 2.6 Minimal Reporting for Publications\r\n\r\n```r\r\n# Get minimal sensitivity statistics for paper\r\novb_minimal_reporting(sensitivity)\r\n#> Robustness Value, q = 1: RV = 0.139\r\n#> Robustness Value, q = 1, alpha = 0.05: RV = 0.076\r\n#> Partial R2 of directlyharmed with peacefactor: 1.38%\r\n#>\r\n#> For a confounder to explain away the entire effect, it would need to\r\n#> explain 13.9% of the residual variance of both treatment and outcome.\r\n#> For a confounder to reduce the effect to non-significance at alpha=0.05,\r\n#> it would need to explain 7.6% of the residual variance of both.\r\n```\r\n\r\n### Limitations\r\n\r\n- Assumes linear confounding structure\r\n- Does not account for measurement error\r\n- Bounds can be wide with small samples\r\n- Cannot prove absence of confounding, only assess robustness\r\n\r\n---\r\n\r\n## 3. Matching Methods with Balance Diagnostics\r\n\r\n### When to Use\r\n\r\nMatching is appropriate when:\r\n- You have observational data with treatment and control groups\r\n- You want to reduce bias from observed confounders\r\n- You need interpretable \"like with like\" comparisons\r\n- You can assume selection on observables (conditional ignorability)\r\n\r\n### 3.1 Propensity Score Matching with MatchIt\r\n\r\nWe use the classic LaLonde dataset, studying the effect of job training on earnings.\r\n\r\n```r\r\nlibrary(MatchIt)\r\n\r\n# Load the LaLonde data\r\ndata(lalonde)\r\n\r\n# Examine the data\r\nstr(lalonde)\r\n#> 'data.frame':\t614 obs. of  9 variables:\r\n#>  $ treat   : int  1 1 1 1 1 ... (treatment indicator)\r\n#>  $ age     : int  37 22 30 27 33 ...\r\n#>  $ educ    : int  11 9 12 11 8 ...\r\n#>  $ race    : Factor w/ 3 levels \"black\",\"hispan\",\"white\"\r\n#>  $ married : int  1 0 0 0 0 ...\r\n#>  $ nodegree: int  1 1 0 1 1 ...\r\n#>  $ re74    : num  0 0 0 0 0 ... (earnings 1974)\r\n#>  $ re75    : num  0 0 0 0 0 ... (earnings 1975)\r\n#>  $ re78    : num  9930 3596 24909 ... (outcome: earnings 1978)\r\n\r\n# Summary: 185 treated, 429 control\r\ntable(lalonde$treat)\r\n#>   0   1\r\n#> 429 185\r\n\r\n# Naive difference in means (biased)\r\nnaive_ate <- mean(lalonde$re78[lalonde$treat == 1]) -\r\n             mean(lalonde$re78[lalonde$treat == 0])\r\ncat(\"Naive ATE:\", round(naive_ate, 2), \"\\n\")\r\n#> Naive ATE: -635.03\r\n```\r\n\r\n### 3.2 Performing Propensity Score Matching\r\n\r\n```r\r\n# Propensity score matching\r\nm_ps <- matchit(\r\n  treat ~ age + educ + race + married + nodegree + re74 + re75,\r\n  data = lalonde,\r\n  method = \"nearest\",      # Nearest neighbor matching\r\n  distance = \"glm\",        # Logistic regression for propensity scores\r\n  caliper = 0.2,           # Caliper width in SD of propensity score\r\n  ratio = 1                # 1:1 matching\r\n)\r\n\r\nsummary(m_ps)\r\n#> Summary of Balance for All Data:\r\n#>          Means Treated Means Control Std. Mean Diff.\r\n#> age              25.82         28.03          -0.309\r\n#> educ             10.35         10.09           0.055\r\n#> raceblack         0.84          0.20           1.762\r\n#> ...\r\n#>\r\n#> Summary of Balance for Matched Data:\r\n#>          Means Treated Means Control Std. Mean Diff.\r\n#> age              25.82         25.30           0.072\r\n#> educ             10.35         10.41          -0.013\r\n#> raceblack         0.84          0.81           0.082\r\n#> ...\r\n\r\n# Extract matched data\r\nmatched_data <- match.data(m_ps)\r\n```\r\n\r\n### 3.3 Assessing Covariate Balance\r\n\r\nGood matching should achieve balance on observed covariates.\r\n\r\n```r\r\n# Visual balance assessment\r\nplot(m_ps, type = \"jitter\", interactive = FALSE)\r\n\r\n# Standardized mean differences before/after matching\r\nplot(summary(m_ps), var.order = \"unmatched\")\r\n#> (generates Love plot showing balance improvement)\r\n\r\n# Detailed balance statistics using cobalt\r\nlibrary(cobalt)\r\n\r\nbal.tab(m_ps, thresholds = c(m = 0.1))\r\n#> Balance Measures:\r\n#>              Type Diff.Un  Diff.Adj   M.Threshold.Un M.Threshold\r\n#> age         Contin. -0.309    0.072           Not Covered\r\n#> educ        Contin.  0.055   -0.013              Covered\r\n#> race_black   Binary   1.762    0.082           Not Covered\r\n#> ...\r\n```\r\n\r\n### 3.4 Estimating Treatment Effects\r\n\r\n```r\r\n# ATT estimation on matched sample\r\n# Method 1: Simple difference in means\r\natt_matched <- with(matched_data,\r\n  mean(re78[treat == 1]) - mean(re78[treat == 0])\r\n)\r\ncat(\"ATT (matched):\", round(att_matched, 2), \"\\n\")\r\n#> ATT (matched): 1548.24\r\n\r\n# Method 2: Regression on matched data with weights\r\nmodel_matched <- lm(re78 ~ treat + age + educ + race + married + nodegree + re74 + re75,\r\n                    data = matched_data,\r\n                    weights = weights)\r\n\r\nsummary(model_matched)$coefficients[\"treat\", ]\r\n#>    Estimate  Std. Error   t value    Pr(>|t|)\r\n#>   1632.4521   875.3423     1.8649     0.0631\r\n\r\n# Method 3: With robust standard errors\r\nlibrary(sandwich)\r\nlibrary(lmtest)\r\n\r\ncoeftest(model_matched, vcov = vcovHC(model_matched, type = \"HC3\"))\r\n#>             Estimate Std. Error t value Pr(>|t|)\r\n#> treat       1632.45     912.34   1.789   0.0745 .\r\n```\r\n\r\n### 3.5 Sensitivity Analysis for Matched Estimates\r\n\r\nAfter matching, assess sensitivity to unobserved confounders.\r\n\r\n```r\r\n# Rosenbaum bounds using rbounds (if available) or manual calculation\r\n# This tests: how much hidden bias (Gamma) would be needed to\r\n# change the conclusion?\r\n\r\n# Manual Gamma sensitivity calculation\r\n# Using Wilcoxon signed-rank test on matched pairs\r\nmatched_treated <- matched_data[matched_data$treat == 1, \"re78\"]\r\nmatched_control <- matched_data[matched_data$treat == 0, \"re78\"]\r\n\r\n# For matched pairs (assuming pair order preserved)\r\ndiffs <- matched_treated - matched_control\r\n\r\n# Test at Gamma = 1 (no hidden bias)\r\nwilcox.test(diffs, alternative = \"greater\")\r\n#> Wilcoxon signed rank test\r\n#> V = 10234, p-value = 0.0012\r\n\r\n# At Gamma = 2 (hidden bias doubles odds of treatment)\r\n# Upper bound p-value would be approximately...\r\n# (Full calculation requires specialized packages like 'rbounds' or 'sensitivitymv')\r\n```\r\n\r\n---\r\n\r\n## 4. Matching Bounds via Linear Programming\r\n\r\n### When to Use\r\n\r\nMatching bounds are appropriate when:\r\n- You want sharp bounds on treatment effects under matching constraints\r\n- You're skeptical about specific matching estimators\r\n- You want to explore the range of possible ATT values consistent with matching constraints\r\n- You need worst-case/best-case scenarios\r\n\r\n### Assumptions\r\n\r\n- SUTVA (Stable Unit Treatment Value Assumption)\r\n- Well-defined treatment and control groups\r\n- Matching constraints are meaningful (e.g., caliper restrictions)\r\n\r\n### 4.1 Conceptual Framework\r\n\r\nThe matching bounds approach uses linear programming to find:\r\n- **Upper bound:** Maximum ATT consistent with matching constraints\r\n- **Lower bound:** Minimum ATT consistent with matching constraints\r\n\r\nKey parameters:\r\n- `m`: Total number of matches to form\r\n- `kt`: Maximum times each treated unit can be matched\r\n- `kc`: Maximum times each control unit can be matched\r\n\r\n### 4.2 Implementation with lpSolve\r\n\r\n```r\r\nlibrary(lpSolve)\r\nlibrary(MatchIt)\r\n\r\n# Load data\r\ndata(lalonde)\r\n\r\n# Subset to keep example tractable\r\nset.seed(123)\r\nn_treat <- 50\r\nn_control <- 100\r\nidx_treat <- sample(which(lalonde$treat == 1), n_treat)\r\nidx_control <- sample(which(lalonde$treat == 0), n_control)\r\ndf_sub <- lalonde[c(idx_treat, idx_control), ]\r\n\r\n# Extract outcomes and treatment\r\nYt <- df_sub$re78[df_sub$treat == 1]  # Treated outcomes\r\nYc <- df_sub$re78[df_sub$treat == 0]  # Control outcomes\r\nnt <- length(Yt)\r\nnc <- length(Yc)\r\n\r\ncat(\"Sample: \", nt, \"treated, \", nc, \"control\\n\")\r\n#> Sample: 50 treated, 100 control\r\n```\r\n\r\n### 4.3 Setting Up the Linear Program\r\n\r\n```r\r\n# Build constraint matrix for matching problem\r\nbuild_matching_lp <- function(Yt, Yc, m, kt = 1, kc = 1) {\r\n  nt <- length(Yt)\r\n  nc <- length(Yc)\r\n  nvars <- nt * nc  # One variable per potential match\r\n\r\n  # Objective: pairwise treatment effects (Yt_i - Yc_j)\r\n  obj <- as.numeric(outer(Yt, Yc, \"-\"))\r\n\r\n  # Constraint 1: Exactly m matches\r\n  A1 <- matrix(1, nrow = 1, ncol = nvars)\r\n  b1 <- m\r\n  dir1 <- \"=\"\r\n\r\n  # Constraint 2: Each treated matched at most kt times\r\n  A2 <- matrix(0, nrow = nt, ncol = nvars)\r\n  for (i in 1:nt) {\r\n    cols <- ((i-1) * nc + 1):(i * nc)\r\n    A2[i, cols] <- 1\r\n  }\r\n  b2 <- rep(kt, nt)\r\n  dir2 <- rep(\"<=\", nt)\r\n\r\n  # Constraint 3: Each control matched at most kc times\r\n  A3 <- matrix(0, nrow = nc, ncol = nvars)\r\n  for (j in 1:nc) {\r\n    cols <- seq(j, nvars, by = nc)\r\n    A3[j, cols] <- 1\r\n  }\r\n  b3 <- rep(kc, nc)\r\n  dir3 <- rep(\"<=\", nc)\r\n\r\n  # Combine constraints\r\n  list(\r\n    obj = obj,\r\n    A = rbind(A1, A2, A3),\r\n    b = c(b1, b2, b3),\r\n    dir = c(dir1, dir2, dir3)\r\n  )\r\n}\r\n```\r\n\r\n### 4.4 Computing Matching Bounds\r\n\r\n```r\r\n# Compute matching bounds\r\nmatching_bounds <- function(Yt, Yc, m, kt = 1, kc = 1) {\r\n  lp_data <- build_matching_lp(Yt, Yc, m, kt, kc)\r\n\r\n  # Solve for upper bound (maximize ATT)\r\n  upper_sol <- lp(\r\n    direction = \"max\",\r\n    objective.in = lp_data$obj,\r\n    const.mat = lp_data$A,\r\n    const.dir = lp_data$dir,\r\n    const.rhs = lp_data$b,\r\n    all.bin = TRUE  # Binary variables\r\n  )\r\n\r\n  # Solve for lower bound (minimize ATT)\r\n  lower_sol <- lp(\r\n    direction = \"min\",\r\n    objective.in = lp_data$obj,\r\n    const.mat = lp_data$A,\r\n    const.dir = lp_data$dir,\r\n    const.rhs = lp_data$b,\r\n    all.bin = TRUE\r\n  )\r\n\r\n  list(\r\n    upper_bound = upper_sol$objval / m,  # Average per match\r\n    lower_bound = lower_sol$objval / m,\r\n    upper_status = upper_sol$status,\r\n    lower_status = lower_sol$status\r\n  )\r\n}\r\n\r\n# Compute bounds with m=40 matches, 1:1 on treated, 1:2 on controls\r\nbounds <- matching_bounds(Yt, Yc, m = 40, kt = 1, kc = 2)\r\n\r\ncat(\"Matching Bounds for ATT:\\n\")\r\ncat(\"  Lower bound:\", round(bounds$lower_bound, 2), \"\\n\")\r\ncat(\"  Upper bound:\", round(bounds$upper_bound, 2), \"\\n\")\r\ncat(\"  Bound width:\", round(bounds$upper_bound - bounds$lower_bound, 2), \"\\n\")\r\n#> Matching Bounds for ATT:\r\n#>   Lower bound: -8234.56\r\n#>   Upper bound: 12456.78\r\n#>   Bound width: 20691.34\r\n```\r\n\r\n### 4.5 Adding Covariate Constraints (Caliper)\r\n\r\nRestrict matches to pairs with similar covariate values.\r\n\r\n```r\r\n# Build distance matrix for caliper constraints\r\nbuild_caliper_constraints <- function(X_treat, X_control, caliper) {\r\n  # Standardize covariates\r\n  X_all <- rbind(X_treat, X_control)\r\n  X_std <- scale(X_all)\r\n  X_treat_std <- X_std[1:nrow(X_treat), , drop = FALSE]\r\n  X_control_std <- X_std[(nrow(X_treat)+1):nrow(X_all), , drop = FALSE]\r\n\r\n  # Compute pairwise Euclidean distances\r\n  nt <- nrow(X_treat)\r\n  nc <- nrow(X_control)\r\n\r\n  distances <- matrix(0, nt, nc)\r\n  for (i in 1:nt) {\r\n    for (j in 1:nc) {\r\n      distances[i, j] <- sqrt(sum((X_treat_std[i, ] - X_control_std[j, ])^2))\r\n    }\r\n  }\r\n\r\n  # Find pairs violating caliper\r\n  violations <- which(distances > caliper, arr.ind = TRUE)\r\n\r\n  if (nrow(violations) == 0) {\r\n    return(NULL)  # No violations\r\n  }\r\n\r\n  # Build constraint matrix: x_{ij} = 0 for violations\r\n  nvars <- nt * nc\r\n  A_caliper <- matrix(0, nrow = nrow(violations), ncol = nvars)\r\n  for (k in 1:nrow(violations)) {\r\n    i <- violations[k, 1]\r\n    j <- violations[k, 2]\r\n    col_idx <- (i - 1) * nc + j\r\n    A_caliper[k, col_idx] <- 1\r\n  }\r\n\r\n  list(\r\n    A = A_caliper,\r\n    b = rep(0, nrow(violations)),\r\n    dir = rep(\"=\", nrow(violations))\r\n  )\r\n}\r\n\r\n# Apply caliper constraints\r\nX_treat <- df_sub[df_sub$treat == 1, c(\"age\", \"educ\", \"re74\", \"re75\")]\r\nX_control <- df_sub[df_sub$treat == 0, c(\"age\", \"educ\", \"re74\", \"re75\")]\r\n\r\ncaliper_const <- build_caliper_constraints(\r\n  as.matrix(X_treat),\r\n  as.matrix(X_control),\r\n  caliper = 1.5  # 1.5 standard deviations\r\n)\r\n\r\ncat(\"Number of caliper violations:\", nrow(caliper_const$A),\r\n    \"out of\", nrow(X_treat) * nrow(X_control), \"pairs\\n\")\r\n#> Number of caliper violations: 2847 out of 5000 pairs\r\n```\r\n\r\n### 4.6 Matching Bounds with Caliper\r\n\r\n```r\r\nmatching_bounds_caliper <- function(Yt, Yc, m, kt, kc, caliper_const) {\r\n  lp_data <- build_matching_lp(Yt, Yc, m, kt, kc)\r\n\r\n  # Add caliper constraints\r\n  A_full <- rbind(lp_data$A, caliper_const$A)\r\n  b_full <- c(lp_data$b, caliper_const$b)\r\n  dir_full <- c(lp_data$dir, caliper_const$dir)\r\n\r\n  # Solve upper bound\r\n  upper_sol <- lp(\r\n    direction = \"max\",\r\n    objective.in = lp_data$obj,\r\n    const.mat = A_full,\r\n    const.dir = dir_full,\r\n    const.rhs = b_full,\r\n    all.bin = TRUE\r\n  )\r\n\r\n  # Solve lower bound\r\n  lower_sol <- lp(\r\n    direction = \"min\",\r\n    objective.in = lp_data$obj,\r\n    const.mat = A_full,\r\n    const.dir = dir_full,\r\n    const.rhs = b_full,\r\n    all.bin = TRUE\r\n  )\r\n\r\n  list(\r\n    upper_bound = upper_sol$objval / m,\r\n    lower_bound = lower_sol$objval / m,\r\n    feasible = (upper_sol$status == 0) && (lower_sol$status == 0)\r\n  )\r\n}\r\n\r\n# Compute bounds with caliper\r\nbounds_cal <- matching_bounds_caliper(Yt, Yc, m = 30, kt = 1, kc = 2, caliper_const)\r\n\r\ncat(\"\\nMatching Bounds with Caliper:\\n\")\r\nif (bounds_cal$feasible) {\r\n  cat(\"  Lower bound:\", round(bounds_cal$lower_bound, 2), \"\\n\")\r\n  cat(\"  Upper bound:\", round(bounds_cal$upper_bound, 2), \"\\n\")\r\n  cat(\"  Bound width:\", round(bounds_cal$upper_bound - bounds_cal$lower_bound, 2), \"\\n\")\r\n} else {\r\n  cat(\"  Problem infeasible - reduce m or widen caliper\\n\")\r\n}\r\n#> Matching Bounds with Caliper:\r\n#>   Lower bound: -3456.78\r\n#>   Upper bound: 5678.90\r\n#>   Bound width: 9135.68\r\n```\r\n\r\n### 4.7 Interpreting Matching Bounds\r\n\r\n```r\r\n# Vary number of matches to see how bounds change\r\nm_values <- seq(10, 45, by = 5)\r\nresults <- data.frame(\r\n  m = m_values,\r\n  lower = NA,\r\n  upper = NA\r\n)\r\n\r\nfor (i in seq_along(m_values)) {\r\n  bounds_i <- matching_bounds(Yt, Yc, m = m_values[i], kt = 1, kc = 2)\r\n  results$lower[i] <- bounds_i$lower_bound\r\n  results$upper[i] <- bounds_i$upper_bound\r\n}\r\n\r\nprint(results)\r\n#>    m    lower    upper\r\n#> 1 10 -5234.5 15678.9\r\n#> 2 15 -4567.8 13456.7\r\n#> 3 20 -4123.4 12345.6\r\n#> ...\r\n\r\n# Visualization\r\nlibrary(ggplot2)\r\n\r\nggplot(results, aes(x = m)) +\r\n  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.3, fill = \"steelblue\") +\r\n  geom_line(aes(y = lower), color = \"red\", linetype = \"dashed\") +\r\n  geom_line(aes(y = upper), color = \"blue\", linetype = \"dashed\") +\r\n  geom_hline(yintercept = 0, linetype = \"dotted\") +\r\n  labs(\r\n    x = \"Number of Matches (m)\",\r\n    y = \"ATT Bound\",\r\n    title = \"Matching Bounds as Function of Match Count\",\r\n    subtitle = \"Shaded region shows identified set; zero = no effect\"\r\n  ) +\r\n  theme_minimal()\r\n```\r\n\r\n### Key Takeaways for Matching Bounds\r\n\r\n| Observation | Interpretation |\r\n|-------------|----------------|\r\n| Bounds exclude zero | Effect is identified as non-null |\r\n| Bounds include zero | Cannot rule out no effect |\r\n| Narrow bounds | Strong identification from matching |\r\n| Wide bounds | Matching constraints not very informative |\r\n| Bounds shrink with caliper | Better covariate balance tightens identification |\r\n\r\n---\r\n\r\n## 5. Best Practices and Common Pitfalls\r\n\r\n### Bayesian Analysis\r\n\r\n**Do:**\r\n- Always check MCMC convergence (R-hat, trace plots, ESS)\r\n- Run posterior predictive checks\r\n- Report both point estimates and credible intervals\r\n- Use informative priors when you have prior knowledge\r\n\r\n**Don't:**\r\n- Ignore convergence diagnostics\r\n- Use very short chains (need at least 1000 post-warmup per chain)\r\n- Over-interpret small differences in posteriors\r\n- Forget to set seeds for reproducibility\r\n\r\n### Sensitivity Analysis\r\n\r\n**Do:**\r\n- Report robustness values alongside main estimates\r\n- Benchmark against observed covariates\r\n- Consider multiple sensitivity scenarios\r\n- Clearly state what confounders would need to look like to overturn results\r\n\r\n**Don't:**\r\n- Claim causal effects without sensitivity analysis\r\n- Only report sensitivity if results are robust\r\n- Ignore substantive interpretation of bounds\r\n- Forget that sensitivity analysis cannot prove absence of confounding\r\n\r\n### Matching\r\n\r\n**Do:**\r\n- Check balance before and after matching\r\n- Use multiple balance diagnostics (SMD, variance ratios)\r\n- Consider sensitivity to unobserved confounders\r\n- Report matching method and parameters clearly\r\n\r\n**Don't:**\r\n- Match on post-treatment variables\r\n- Ignore observations that couldn't be matched\r\n- Assume matching eliminates all confounding\r\n- Over-match (too many covariates relative to sample size)\r\n\r\n---\r\n\r\n## Further Reading\r\n\r\n1. **Bayesian Regression:** Burkner, P. C. (2017). brms: An R Package for Bayesian Multilevel Models Using Stan. *Journal of Statistical Software*.\r\n\r\n2. **Sensitivity Analysis:** Cinelli, C., & Hazlett, C. (2020). Making Sense of Sensitivity: Extending Omitted Variable Bias. *Journal of the Royal Statistical Society Series B*.\r\n\r\n3. **Matching Methods:** Stuart, E. A. (2010). Matching Methods for Causal Inference: A Review and a Look Forward. *Statistical Science*.\r\n\r\n4. **Matching Bounds:** Morucci, M. (2022). Matching Bounds for Causal Inference. *Journal of Politics*.\r\n",
        "plugins/r-analyst/skills/r-analyst/techniques/06_visualization.md": "# Visualization & Graphics in R\r\n\r\nPublication-quality figures, coefficient plots, event studies, and table output using ggplot2 and the tidyverse ecosystem.\r\n\r\n---\r\n\r\n## Quick Reference\r\n\r\n| Task | Primary Package | Key Function | Example Dataset |\r\n|------|-----------------|--------------|-----------------|\r\n| Scatter plots | `ggplot2` | `geom_point()` | `mtcars`, `diamonds` |\r\n| Line plots | `ggplot2` | `geom_line()` | `economics` |\r\n| Histograms/density | `ggplot2` | `geom_histogram()`, `geom_density()` | `diamonds` |\r\n| Coefficient plots | `fixest`, `broom` | `iplot()`, `coefplot()` | `trade`, `base_did` |\r\n| Event studies | `fixest` | `iplot()` | `base_did` |\r\n| RD plots | `rdrobust` | `rdplot()` | simulated data |\r\n| Multi-panel figures | `patchwork` | `\\|`, `/` operators | any |\r\n| Regression tables | `modelsummary` | `modelsummary()` | any model output |\r\n| Descriptive stats | `modelsummary` | `datasummary()` | any data frame |\r\n| Color scales | `viridis`, `scales` | `scale_*_viridis_*()` | any |\r\n| Text labels | `ggrepel` | `geom_text_repel()` | any |\r\n| Export | `ggplot2` | `ggsave()` | any plot |\r\n\r\n---\r\n\r\n## 1. ggplot2 Fundamentals\r\n\r\n### When to Use ggplot2\r\n\r\nUse ggplot2 for virtually all static visualizations in R. The grammar of graphics approach provides:\r\n- Consistent, readable syntax across plot types\r\n- Easy layering of multiple geoms\r\n- Excellent defaults with full customization options\r\n- Seamless integration with the tidyverse\r\n\r\n### 1.1 Basic Scatter with Regression Line\r\n\r\n**When to Use:** Exploring relationships between two continuous variables, checking linearity assumptions.\r\n\r\n```r\r\nlibrary(ggplot2)\r\n\r\n# Using mtcars: relationship between weight and fuel efficiency\r\np <- ggplot(mtcars, aes(x = wt, y = mpg)) +\r\n  geom_point(aes(size = hp), alpha = 0.6, color = \"steelblue\") +\r\n  geom_smooth(method = \"lm\", se = TRUE, fill = \"gray80\", color = \"navy\") +\r\n  labs(\r\n    x = \"Weight (1000 lbs)\",\r\n    y = \"Miles per Gallon\",\r\n    title = \"Fuel Efficiency vs. Vehicle Weight\",\r\n    size = \"Horsepower\"\r\n  ) +\r\n  theme_minimal() +\r\n  theme(legend.position = \"right\")\r\n\r\nprint(p)\r\n```\r\n\r\n**Best Practice:** Always include informative axis labels with units. Use `alpha` for overlapping points.\r\n\r\n### 1.2 Faceted Plots\r\n\r\n**When to Use:** Comparing patterns across subgroups while maintaining consistent scales.\r\n\r\n```r\r\nlibrary(ggplot2)\r\n\r\n# Using diamonds: price by carat, faceted by cut quality\r\nggplot(diamonds, aes(x = carat, y = price)) +\r\n  geom_point(alpha = 0.1, size = 0.5) +\r\n  geom_smooth(method = \"lm\", color = \"red\") +\r\n  facet_wrap(~ cut, nrow = 1) +\r\n  scale_y_continuous(labels = scales::dollar_format()) +\r\n  labs(\r\n    x = \"Carat Weight\",\r\n    y = \"Price (USD)\",\r\n    title = \"Diamond Price by Carat Weight Across Cut Quality\"\r\n  ) +\r\n  theme_minimal() +\r\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\r\n```\r\n\r\n### 1.3 Line Plots for Time Series\r\n\r\n**When to Use:** Displaying trends over time, comparing multiple series.\r\n\r\n```r\r\nlibrary(ggplot2)\r\nlibrary(scales)\r\n\r\n# Using economics: unemployment over time\r\nggplot(economics, aes(x = date, y = unemploy / 1000)) +\r\n  geom_line(color = \"steelblue\", linewidth = 0.8) +\r\n  geom_smooth(method = \"loess\", se = FALSE, color = \"red\", linetype = \"dashed\") +\r\n  scale_x_date(date_labels = \"%Y\", date_breaks = \"5 years\") +\r\n  labs(\r\n    x = \"Year\",\r\n    y = \"Unemployed (millions)\",\r\n    title = \"U.S. Unemployment Over Time\",\r\n    caption = \"Source: Federal Reserve Economic Data\"\r\n  ) +\r\n  theme_minimal()\r\n```\r\n\r\n### 1.4 Histograms and Density Plots\r\n\r\n**When to Use:** Understanding the distribution of a single variable.\r\n\r\n```r\r\nlibrary(ggplot2)\r\n\r\n# Using diamonds: distribution of price\r\nggplot(diamonds, aes(x = price)) +\r\n  geom_histogram(aes(y = after_stat(density)),\r\n                 bins = 50, fill = \"steelblue\", alpha = 0.7) +\r\n  geom_density(color = \"red\", linewidth = 1) +\r\n  scale_x_continuous(labels = scales::dollar_format()) +\r\n  labs(\r\n    x = \"Price (USD)\",\r\n    y = \"Density\",\r\n    title = \"Distribution of Diamond Prices\"\r\n  ) +\r\n  theme_minimal()\r\n\r\n# Grouped density comparison\r\nggplot(diamonds, aes(x = price, fill = cut)) +\r\n  geom_density(alpha = 0.5) +\r\n  scale_x_continuous(labels = scales::dollar_format(), limits = c(0, 10000)) +\r\n  scale_fill_viridis_d() +\r\n  labs(\r\n    x = \"Price (USD)\",\r\n    y = \"Density\",\r\n    title = \"Price Distribution by Cut Quality\",\r\n    fill = \"Cut\"\r\n  ) +\r\n  theme_minimal()\r\n```\r\n\r\n### 1.5 Box Plots and Violin Plots\r\n\r\n**When to Use:** Comparing distributions across categories, identifying outliers.\r\n\r\n```r\r\nlibrary(ggplot2)\r\n\r\n# Using mpg: highway mileage by vehicle class\r\nggplot(mpg, aes(x = reorder(class, hwy, FUN = median), y = hwy, fill = class)) +\r\n  geom_boxplot(alpha = 0.7, show.legend = FALSE) +\r\n  geom_jitter(width = 0.2, alpha = 0.3, size = 0.8) +\r\n  coord_flip() +\r\n  scale_fill_viridis_d() +\r\n  labs(\r\n    x = \"Vehicle Class\",\r\n    y = \"Highway MPG\",\r\n    title = \"Highway Fuel Efficiency by Vehicle Class\"\r\n  ) +\r\n  theme_minimal()\r\n\r\n# Violin plot alternative\r\nggplot(mpg, aes(x = reorder(class, hwy, FUN = median), y = hwy, fill = class)) +\r\n  geom_violin(alpha = 0.7, show.legend = FALSE) +\r\n  geom_boxplot(width = 0.1, fill = \"white\", alpha = 0.8) +\r\n  coord_flip() +\r\n  scale_fill_viridis_d() +\r\n  labs(\r\n    x = \"Vehicle Class\",\r\n    y = \"Highway MPG\",\r\n    title = \"Highway Fuel Efficiency by Vehicle Class\"\r\n  ) +\r\n  theme_minimal()\r\n```\r\n\r\n---\r\n\r\n## 2. Publication-Ready Themes\r\n\r\n### When to Use Custom Themes\r\n\r\nCreate custom themes for:\r\n- Consistent styling across all paper figures\r\n- Meeting journal-specific requirements\r\n- Ensuring readability in print and online formats\r\n\r\n### 2.1 Publication Theme Template\r\n\r\n```r\r\nlibrary(ggplot2)\r\n\r\n# Clean theme for journal submission\r\ntheme_publication <- function(base_size = 11, base_family = \"\") {\r\n  theme_minimal(base_size = base_size, base_family = base_family) +\r\n    theme(\r\n      # Remove background and grid\r\n      panel.border = element_blank(),\r\n      panel.grid.major = element_blank(),\r\n      panel.grid.minor = element_blank(),\r\n\r\n      # Add axis lines\r\n      axis.line = element_line(colour = \"black\", linewidth = 0.5),\r\n\r\n      # Axis text and titles\r\n      axis.text = element_text(color = \"black\", size = base_size),\r\n      axis.title.x = element_text(margin = margin(t = 10), size = base_size + 1),\r\n      axis.title.y = element_text(margin = margin(r = 10), size = base_size + 1),\r\n\r\n      # Legend\r\n      legend.position = \"bottom\",\r\n      legend.key = element_blank(),\r\n      legend.background = element_blank(),\r\n\r\n      # Title\r\n      plot.title = element_text(face = \"bold\", size = base_size + 2, hjust = 0),\r\n      plot.subtitle = element_text(size = base_size, hjust = 0, color = \"gray40\"),\r\n      plot.caption = element_text(size = base_size - 2, hjust = 1, color = \"gray50\"),\r\n\r\n      # Margins\r\n      plot.margin = margin(10, 10, 10, 10)\r\n    )\r\n}\r\n\r\n# Apply to a plot\r\nggplot(mtcars, aes(x = wt, y = mpg)) +\r\n  geom_point(size = 2, color = \"steelblue\") +\r\n  geom_smooth(method = \"lm\", se = TRUE, color = \"navy\", fill = \"gray80\") +\r\n  labs(\r\n    x = \"Weight (1000 lbs)\",\r\n    y = \"Miles per Gallon\",\r\n    title = \"Vehicle Weight vs. Fuel Efficiency\",\r\n    subtitle = \"Linear relationship with 95% confidence band\",\r\n    caption = \"Data: Motor Trend Car Road Tests (1974)\"\r\n  ) +\r\n  theme_publication()\r\n```\r\n\r\n### 2.2 AER/QJE Style Theme\r\n\r\n```r\r\n# American Economic Review style\r\ntheme_aer <- function(base_size = 10) {\r\n  theme_classic(base_size = base_size) +\r\n    theme(\r\n      # Minimal, clean look\r\n      panel.grid = element_blank(),\r\n      axis.line = element_line(linewidth = 0.3),\r\n      axis.ticks = element_line(linewidth = 0.3),\r\n\r\n      # No legend title\r\n      legend.title = element_blank(),\r\n      legend.position = \"bottom\",\r\n      legend.direction = \"horizontal\",\r\n\r\n      # Subtle colors\r\n      strip.background = element_rect(fill = \"gray95\", color = NA),\r\n      strip.text = element_text(face = \"bold\", size = base_size)\r\n    )\r\n}\r\n\r\n# Example usage\r\nggplot(economics, aes(x = date, y = unemploy / 1000)) +\r\n  geom_line(linewidth = 0.5) +\r\n  labs(x = \"\", y = \"Unemployed (millions)\") +\r\n  theme_aer()\r\n```\r\n\r\n---\r\n\r\n## 3. Multi-Panel Figures with patchwork\r\n\r\n### When to Use Multi-Panel Figures\r\n\r\nUse multi-panel figures when:\r\n- Comparing related analyses side-by-side\r\n- Creating figure panels (A, B, C) for publication\r\n- Showing the same relationship across subgroups\r\n- Combining different plot types that share context\r\n\r\n### 3.1 Basic Panel Composition\r\n\r\n```r\r\nlibrary(ggplot2)\r\nlibrary(patchwork)\r\n\r\n# Create individual panels using mtcars\r\np1 <- ggplot(mtcars, aes(x = wt, y = mpg)) +\r\n  geom_point(color = \"steelblue\") +\r\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\r\n  labs(title = \"A. Weight vs. MPG\", x = \"Weight\", y = \"MPG\") +\r\n  theme_minimal()\r\n\r\np2 <- ggplot(mtcars, aes(x = hp, y = mpg)) +\r\n  geom_point(color = \"forestgreen\") +\r\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\r\n  labs(title = \"B. Horsepower vs. MPG\", x = \"Horsepower\", y = \"MPG\") +\r\n  theme_minimal()\r\n\r\np3 <- ggplot(mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) +\r\n  geom_boxplot(show.legend = FALSE) +\r\n  scale_fill_viridis_d() +\r\n  labs(title = \"C. MPG by Cylinders\", x = \"Cylinders\", y = \"MPG\") +\r\n  theme_minimal()\r\n\r\n# Combine horizontally\r\n(p1 | p2 | p3)\r\n\r\n# Combine in a grid (2 on top, 1 on bottom)\r\n(p1 | p2) / p3\r\n\r\n# With shared theme applied to all\r\n(p1 | p2) / p3 & theme_minimal()\r\n```\r\n\r\n### 3.2 Complex Layouts with Annotations\r\n\r\n```r\r\nlibrary(ggplot2)\r\nlibrary(patchwork)\r\n\r\n# Create panels\r\nscatter <- ggplot(diamonds[sample(nrow(diamonds), 1000), ], aes(x = carat, y = price)) +\r\n  geom_point(alpha = 0.3, color = \"steelblue\") +\r\n  geom_smooth(method = \"lm\", color = \"red\") +\r\n  labs(x = \"Carat\", y = \"Price ($)\") +\r\n  theme_minimal()\r\n\r\nhistogram <- ggplot(diamonds, aes(x = price)) +\r\n  geom_histogram(bins = 40, fill = \"steelblue\", alpha = 0.7) +\r\n  labs(x = \"Price ($)\", y = \"Count\") +\r\n  theme_minimal()\r\n\r\nboxplot <- ggplot(diamonds, aes(x = cut, y = price, fill = cut)) +\r\n  geom_boxplot(show.legend = FALSE) +\r\n  scale_fill_viridis_d() +\r\n  labs(x = \"Cut Quality\", y = \"Price ($)\") +\r\n  theme_minimal() +\r\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\r\n\r\n# Complex layout with widths and heights\r\nlayout <- (scatter | histogram) / boxplot +\r\n  plot_layout(heights = c(2, 1)) +\r\n  plot_annotation(\r\n    title = \"Diamond Prices Analysis\",\r\n    subtitle = \"Exploring the relationship between physical properties and price\",\r\n    caption = \"Data: ggplot2::diamonds\",\r\n    tag_levels = \"A\",\r\n    theme = theme(\r\n      plot.title = element_text(size = 14, face = \"bold\"),\r\n      plot.subtitle = element_text(size = 10, color = \"gray40\")\r\n    )\r\n  )\r\n\r\nprint(layout)\r\n```\r\n\r\n### 3.3 Inset Plots\r\n\r\n```r\r\nlibrary(ggplot2)\r\nlibrary(patchwork)\r\n\r\n# Main plot\r\nmain_plot <- ggplot(economics, aes(x = date, y = unemploy / 1000)) +\r\n  geom_line(color = \"steelblue\") +\r\n  labs(x = \"Year\", y = \"Unemployed (millions)\",\r\n       title = \"U.S. Unemployment Over Time\") +\r\n  theme_minimal()\r\n\r\n# Inset: recent years only\r\ninset_plot <- ggplot(subset(economics, date >= as.Date(\"2000-01-01\")),\r\n                     aes(x = date, y = unemploy / 1000)) +\r\n  geom_line(color = \"red\", linewidth = 0.5) +\r\n  labs(x = \"\", y = \"\") +\r\n  theme_minimal() +\r\n  theme(\r\n    plot.background = element_rect(fill = \"white\", color = \"gray50\"),\r\n    axis.text = element_text(size = 6)\r\n  )\r\n\r\n# Combine with inset\r\nmain_plot + inset_element(inset_plot, left = 0.6, bottom = 0.6, right = 0.98, top = 0.98)\r\n```\r\n\r\n---\r\n\r\n## 4. Coefficient Plots\r\n\r\n### When to Use Coefficient Plots\r\n\r\nCoefficient plots are essential for:\r\n- Visualizing regression results (easier to interpret than tables)\r\n- Comparing effects across multiple models\r\n- Showing confidence intervals clearly\r\n- Presenting many coefficients simultaneously\r\n\r\n### 4.1 Using fixest::iplot and coefplot\r\n\r\n```r\r\nlibrary(fixest)\r\n\r\n# Load the trade dataset from fixest\r\ndata(trade, package = \"fixest\")\r\n\r\n# Estimate models with different specifications\r\nmodel1 <- feols(log(Euros) ~ log(dist_km) | Origin, data = trade)\r\nmodel2 <- feols(log(Euros) ~ log(dist_km) + log(Year) | Origin, data = trade)\r\nmodel3 <- feols(log(Euros) ~ log(dist_km) + log(Year) | Origin + Destination, data = trade)\r\n\r\n# Basic coefficient plot\r\ncoefplot(model1)\r\n\r\n# Multiple models comparison\r\ncoefplot(list(\"Base\" = model1, \"Year Control\" = model2, \"Two-way FE\" = model3),\r\n         keep = \"dist_km\",\r\n         main = \"Effect of Distance on Trade\")\r\n\r\n# Customize appearance\r\ncoefplot(model3,\r\n         keep = c(\"dist_km\", \"Year\"),\r\n         ci_level = 0.95,\r\n         pt.pch = 19,\r\n         pt.col = \"navy\",\r\n         ci.col = \"navy\",\r\n         ref.line = 0,\r\n         grid = FALSE,\r\n         main = \"Trade Determinants\")\r\n```\r\n\r\n### 4.2 Manual Coefficient Plot with ggplot2\r\n\r\n```r\r\nlibrary(ggplot2)\r\nlibrary(broom)\r\nlibrary(dplyr)\r\n\r\n# Fit multiple models on mtcars\r\nmodel1 <- lm(mpg ~ wt + hp + am, data = mtcars)\r\nmodel2 <- lm(mpg ~ wt + hp + am + cyl, data = mtcars)\r\nmodel3 <- lm(mpg ~ wt + hp + am + cyl + disp, data = mtcars)\r\n\r\n# Extract and combine coefficients\r\nextract_coefs <- function(model, model_name) {\r\n  tidy(model, conf.int = TRUE) %>%\r\n    filter(term != \"(Intercept)\") %>%\r\n    mutate(model = model_name)\r\n}\r\n\r\ncoef_df <- bind_rows(\r\n  extract_coefs(model1, \"Model 1\"),\r\n  extract_coefs(model2, \"Model 2\"),\r\n  extract_coefs(model3, \"Model 3\")\r\n)\r\n\r\n# Create coefficient plot\r\nggplot(coef_df, aes(x = estimate, y = term, color = model)) +\r\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray50\") +\r\n  geom_pointrange(aes(xmin = conf.low, xmax = conf.high),\r\n                  position = position_dodge(width = 0.5),\r\n                  size = 0.5) +\r\n  scale_color_viridis_d(begin = 0.2, end = 0.8) +\r\n  labs(\r\n    x = \"Coefficient Estimate\",\r\n    y = \"\",\r\n    title = \"Coefficient Comparison Across Models\",\r\n    color = \"Model\"\r\n  ) +\r\n  theme_minimal() +\r\n  theme(\r\n    legend.position = \"bottom\",\r\n    panel.grid.minor = element_blank()\r\n  )\r\n```\r\n\r\n### 4.3 Horizontal Coefficient Plot with Labels\r\n\r\n```r\r\nlibrary(ggplot2)\r\nlibrary(broom)\r\nlibrary(dplyr)\r\nlibrary(ggrepel)\r\n\r\n# Model with multiple predictors\r\nmodel <- lm(mpg ~ wt + hp + drat + qsec + am + gear + carb, data = mtcars)\r\n\r\n# Extract coefficients\r\ncoef_df <- tidy(model, conf.int = TRUE) %>%\r\n  filter(term != \"(Intercept)\") %>%\r\n  mutate(\r\n    significant = p.value < 0.05,\r\n    term_label = case_when(\r\n      term == \"wt\" ~ \"Weight (1000 lbs)\",\r\n      term == \"hp\" ~ \"Horsepower\",\r\n      term == \"drat\" ~ \"Rear Axle Ratio\",\r\n      term == \"qsec\" ~ \"1/4 Mile Time\",\r\n      term == \"am\" ~ \"Manual Transmission\",\r\n      term == \"gear\" ~ \"Number of Gears\",\r\n      term == \"carb\" ~ \"Number of Carburetors\",\r\n      TRUE ~ term\r\n    )\r\n  ) %>%\r\n  arrange(estimate)\r\n\r\n# Ordered coefficient plot\r\nggplot(coef_df, aes(x = reorder(term_label, estimate), y = estimate)) +\r\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray60\", linewidth = 0.5) +\r\n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high, color = significant),\r\n                  size = 0.6) +\r\n  geom_text(aes(label = sprintf(\"%.2f\", estimate)),\r\n            hjust = -0.3, size = 3) +\r\n  coord_flip() +\r\n  scale_color_manual(values = c(\"FALSE\" = \"gray50\", \"TRUE\" = \"steelblue\"),\r\n                     labels = c(\"FALSE\" = \"p >= 0.05\", \"TRUE\" = \"p < 0.05\"),\r\n                     name = \"Statistical\\nSignificance\") +\r\n  labs(\r\n    x = \"\",\r\n    y = \"Coefficient Estimate (95% CI)\",\r\n    title = \"Determinants of Fuel Efficiency\",\r\n    subtitle = \"Linear regression coefficients with 95% confidence intervals\"\r\n  ) +\r\n  theme_minimal() +\r\n  theme(\r\n    legend.position = \"bottom\",\r\n    panel.grid.major.y = element_blank()\r\n  )\r\n```\r\n\r\n---\r\n\r\n## 5. Event Study Visualization\r\n\r\n### When to Use Event Studies\r\n\r\nEvent study plots are crucial for:\r\n- Difference-in-differences designs with staggered treatment\r\n- Testing parallel trends assumptions\r\n- Showing dynamic treatment effects over time\r\n- Identifying pre-trends that might violate identifying assumptions\r\n\r\n### 5.1 Event Study with fixest\r\n\r\n```r\r\nlibrary(fixest)\r\n\r\n# Load base_did dataset from fixest (simulated DID data)\r\ndata(base_did, package = \"fixest\")\r\n\r\n# Estimate event study model\r\n# The data has: y (outcome), x1 (covariate), id (unit), period (time),\r\n# treat (binary treatment indicator), post (post-treatment indicator)\r\nevent_model <- feols(\r\n  y ~ i(period, treat, ref = 5) | id + period,\r\n  cluster = ~id,\r\n  data = base_did\r\n)\r\n\r\n# Built-in event study plot\r\niplot(event_model,\r\n      xlab = \"Period (Relative to Reference = 5)\",\r\n      ylab = \"Effect Estimate\",\r\n      main = \"Event Study: Treatment Effects Over Time\")\r\n\r\n# Customized appearance\r\niplot(event_model,\r\n      ci_level = 0.95,\r\n      pt.pch = 19,\r\n      pt.col = \"navy\",\r\n      ci.col = \"navy\",\r\n      ref.line = 0,\r\n      grid = TRUE,\r\n      main = \"Dynamic Treatment Effects\")\r\n```\r\n\r\n### 5.2 Manual Event Study Plot with ggplot2\r\n\r\n```r\r\nlibrary(fixest)\r\nlibrary(ggplot2)\r\nlibrary(broom)\r\nlibrary(dplyr)\r\nlibrary(stringr)\r\n\r\n# Use fixest's base_did data\r\ndata(base_did, package = \"fixest\")\r\n\r\n# Estimate event study\r\nmodel <- feols(\r\n  y ~ i(period, treat, ref = 5) | id + period,\r\n  cluster = ~id,\r\n  data = base_did\r\n)\r\n\r\n# Extract event study coefficients\r\nevent_coefs <- broom::tidy(model, conf.int = TRUE) %>%\r\n  filter(str_detect(term, \"period::\")) %>%\r\n  mutate(\r\n    period = as.numeric(str_extract(term, \"[0-9]+\")),\r\n    significant = conf.low > 0 | conf.high < 0\r\n  )\r\n\r\n# Add reference period (period = 5)\r\nevent_coefs <- bind_rows(\r\n  event_coefs,\r\n  tibble(period = 5, estimate = 0, conf.low = 0, conf.high = 0, significant = FALSE)\r\n) %>%\r\n  arrange(period)\r\n\r\n# Create publication-quality event study plot\r\nggplot(event_coefs, aes(x = period, y = estimate)) +\r\n  # Reference lines\r\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\r\n  geom_vline(xintercept = 5.5, linetype = \"dotted\", color = \"red\", linewidth = 0.8) +\r\n\r\n  # Confidence ribbon\r\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.2, fill = \"steelblue\") +\r\n\r\n  # Line connecting estimates\r\n  geom_line(color = \"navy\", linewidth = 0.8) +\r\n\r\n  # Points with significance coloring\r\n  geom_point(aes(color = significant), size = 3) +\r\n\r\n  # Styling\r\n  scale_color_manual(\r\n    values = c(\"FALSE\" = \"gray50\", \"TRUE\" = \"navy\"),\r\n    guide = \"none\"\r\n  ) +\r\n  scale_x_continuous(breaks = 1:10) +\r\n\r\n  # Labels\r\n  labs(\r\n    x = \"Period\",\r\n    y = \"Coefficient Estimate\",\r\n    title = \"Event Study: Dynamic Treatment Effects\",\r\n    subtitle = \"Vertical line indicates treatment onset; shaded region shows 95% CI\",\r\n    caption = \"Reference period: 5. Robust standard errors clustered at unit level.\"\r\n  ) +\r\n\r\n  theme_minimal() +\r\n  theme(\r\n    panel.grid.minor = element_blank(),\r\n    plot.subtitle = element_text(color = \"gray40\")\r\n  )\r\n```\r\n\r\n### 5.3 Event Study with Multiple Groups\r\n\r\n```r\r\nlibrary(fixest)\r\nlibrary(ggplot2)\r\nlibrary(dplyr)\r\n\r\n# Create simulated multi-group event study data\r\nset.seed(42)\r\nn <- 1000\r\nevent_data <- tibble(\r\n  id = rep(1:100, each = 10),\r\n  time = rep(1:10, 100),\r\n  group = rep(sample(c(\"Treatment A\", \"Treatment B\", \"Control\"), 100, replace = TRUE), each = 10),\r\n  rel_time = time - 5,\r\n  treated = as.numeric(group != \"Control\" & time >= 5),\r\n  y = 2 + 0.5 * (group == \"Treatment A\") * (time >= 5) * (time - 4) +\r\n      0.3 * (group == \"Treatment B\") * (time >= 5) * (time - 4) +\r\n      rnorm(n, 0, 0.5)\r\n)\r\n\r\n# Summarize by group and time\r\nevent_summary <- event_data %>%\r\n  group_by(group, rel_time) %>%\r\n  summarise(\r\n    mean_y = mean(y),\r\n    se = sd(y) / sqrt(n()),\r\n    conf.low = mean_y - 1.96 * se,\r\n    conf.high = mean_y + 1.96 * se,\r\n    .groups = \"drop\"\r\n  )\r\n\r\n# Multi-group event study plot\r\nggplot(event_summary, aes(x = rel_time, y = mean_y, color = group, fill = group)) +\r\n  geom_hline(yintercept = 2, linetype = \"dashed\", color = \"gray50\") +\r\n  geom_vline(xintercept = -0.5, linetype = \"dotted\", color = \"gray30\") +\r\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.2, color = NA) +\r\n  geom_line(linewidth = 1) +\r\n  geom_point(size = 2) +\r\n  scale_color_viridis_d(end = 0.8) +\r\n  scale_fill_viridis_d(end = 0.8) +\r\n  labs(\r\n    x = \"Time Relative to Treatment\",\r\n    y = \"Outcome\",\r\n    title = \"Event Study by Treatment Group\",\r\n    color = \"Group\",\r\n    fill = \"Group\"\r\n  ) +\r\n  theme_minimal() +\r\n  theme(legend.position = \"bottom\")\r\n```\r\n\r\n---\r\n\r\n## 6. Regression Discontinuity Visualization\r\n\r\n### When to Use RD Plots\r\n\r\nRD plots are essential for:\r\n- Visualizing discontinuities at the threshold\r\n- Checking for manipulation of the running variable\r\n- Presenting RD results to non-technical audiences\r\n- Validating the RD design before formal estimation\r\n\r\n### 6.1 Manual RD Plot with ggplot2\r\n\r\n```r\r\nlibrary(ggplot2)\r\nlibrary(dplyr)\r\n\r\n# Create simulated RD data\r\nset.seed(123)\r\nn <- 500\r\nrd_data <- tibble(\r\n  running_var = runif(n, -1, 1),\r\n  treatment = as.numeric(running_var >= 0),\r\n  outcome = 2 + 0.5 * running_var + 0.8 * treatment + rnorm(n, 0, 0.3)\r\n)\r\n\r\n# Basic RD plot with local linear fits\r\nggplot(rd_data, aes(x = running_var, y = outcome)) +\r\n  geom_point(aes(color = factor(treatment)), alpha = 0.5, size = 1.5) +\r\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray40\") +\r\n  geom_smooth(data = filter(rd_data, running_var < 0),\r\n              method = \"lm\", se = TRUE, color = \"steelblue\", fill = \"steelblue\", alpha = 0.2) +\r\n  geom_smooth(data = filter(rd_data, running_var >= 0),\r\n              method = \"lm\", se = TRUE, color = \"coral\", fill = \"coral\", alpha = 0.2) +\r\n  scale_color_manual(values = c(\"0\" = \"steelblue\", \"1\" = \"coral\"),\r\n                     labels = c(\"0\" = \"Below Cutoff\", \"1\" = \"Above Cutoff\")) +\r\n  labs(\r\n    x = \"Running Variable (centered at cutoff)\",\r\n    y = \"Outcome\",\r\n    title = \"Regression Discontinuity Design\",\r\n    subtitle = \"Local linear regression on each side of the cutoff\",\r\n    color = \"Treatment Status\"\r\n  ) +\r\n  theme_minimal() +\r\n  theme(legend.position = \"bottom\")\r\n```\r\n\r\n### 6.2 RD Plot with Binned Means\r\n\r\n```r\r\nlibrary(ggplot2)\r\nlibrary(dplyr)\r\n\r\n# Create simulated data\r\nset.seed(456)\r\nn <- 1000\r\nrd_data <- tibble(\r\n  x = runif(n, -1, 1),\r\n  treatment = as.numeric(x >= 0),\r\n  y = 1 + 0.3 * x + 0.5 * treatment + 0.2 * x * treatment + rnorm(n, 0, 0.2)\r\n)\r\n\r\n# Create bins\r\nn_bins <- 20\r\nrd_data <- rd_data %>%\r\n  mutate(\r\n    bin = cut(x, breaks = n_bins, labels = FALSE),\r\n    bin_center = (cut(x, breaks = n_bins, labels = FALSE) - 0.5) / n_bins * 2 - 1\r\n  )\r\n\r\n# Calculate bin means\r\nbin_means <- rd_data %>%\r\n  group_by(bin) %>%\r\n  summarise(\r\n    bin_center = mean(x),\r\n    mean_y = mean(y),\r\n    se = sd(y) / sqrt(n()),\r\n    n = n(),\r\n    .groups = \"drop\"\r\n  ) %>%\r\n  mutate(treatment = as.numeric(bin_center >= 0))\r\n\r\n# RD plot with binned means\r\nggplot() +\r\n  # Binned means with error bars\r\n  geom_errorbar(data = bin_means,\r\n                aes(x = bin_center, ymin = mean_y - 1.96 * se, ymax = mean_y + 1.96 * se),\r\n                width = 0.02, color = \"gray50\") +\r\n  geom_point(data = bin_means,\r\n             aes(x = bin_center, y = mean_y, color = factor(treatment)),\r\n             size = 3) +\r\n\r\n  # Polynomial fits\r\n  geom_smooth(data = filter(rd_data, x < 0),\r\n              aes(x = x, y = y), method = \"lm\", formula = y ~ poly(x, 2),\r\n              se = TRUE, color = \"steelblue\", fill = \"steelblue\", alpha = 0.15) +\r\n  geom_smooth(data = filter(rd_data, x >= 0),\r\n              aes(x = x, y = y), method = \"lm\", formula = y ~ poly(x, 2),\r\n              se = TRUE, color = \"coral\", fill = \"coral\", alpha = 0.15) +\r\n\r\n  # Cutoff line\r\n  geom_vline(xintercept = 0, linetype = \"dashed\", linewidth = 0.8) +\r\n\r\n  scale_color_manual(values = c(\"0\" = \"steelblue\", \"1\" = \"coral\"), guide = \"none\") +\r\n  labs(\r\n    x = \"Running Variable\",\r\n    y = \"Outcome\",\r\n    title = \"RD Plot with Binned Means\",\r\n    subtitle = \"Dots show bin means with 95% CI; curves show quadratic fit\"\r\n  ) +\r\n  theme_minimal()\r\n```\r\n\r\n### 6.3 Density Test Plot (McCrary Test)\r\n\r\n```r\r\nlibrary(ggplot2)\r\nlibrary(dplyr)\r\n\r\n# Using the same rd_data\r\n# Density plot to check for manipulation\r\nggplot(rd_data, aes(x = x)) +\r\n  geom_histogram(aes(y = after_stat(density)), bins = 40,\r\n                 fill = \"gray70\", color = \"white\") +\r\n  geom_density(color = \"steelblue\", linewidth = 1) +\r\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\", linewidth = 1) +\r\n  labs(\r\n    x = \"Running Variable\",\r\n    y = \"Density\",\r\n    title = \"Density of Running Variable\",\r\n    subtitle = \"Check for bunching at the cutoff (McCrary test visualization)\"\r\n  ) +\r\n  theme_minimal()\r\n```\r\n\r\n---\r\n\r\n## 7. Regression Tables\r\n\r\n### When to Use Different Table Formats\r\n\r\n- **modelsummary**: Best for most applications, highly customizable, exports to Word/LaTeX/HTML\r\n- **fixest::etable**: Best when using fixest models, native LaTeX support\r\n- **gtsummary**: Best for descriptive statistics and clinical/survey data\r\n\r\n### 7.1 Basic Tables with modelsummary\r\n\r\n```r\r\nlibrary(modelsummary)\r\n\r\n# Fit multiple models on mtcars\r\nmodel1 <- lm(mpg ~ wt, data = mtcars)\r\nmodel2 <- lm(mpg ~ wt + hp, data = mtcars)\r\nmodel3 <- lm(mpg ~ wt + hp + am, data = mtcars)\r\n\r\n# Basic table\r\nmodelsummary(\r\n  list(\"(1)\" = model1, \"(2)\" = model2, \"(3)\" = model3),\r\n  stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),\r\n  gof_omit = \"AIC|BIC|Log|F|RMSE\"\r\n)\r\n\r\n# With custom coefficient names\r\nmodelsummary(\r\n  list(\"(1)\" = model1, \"(2)\" = model2, \"(3)\" = model3),\r\n  stars = TRUE,\r\n  coef_rename = c(\r\n    \"wt\" = \"Weight (1000 lbs)\",\r\n    \"hp\" = \"Horsepower\",\r\n    \"am\" = \"Manual Transmission\"\r\n  ),\r\n  gof_map = c(\"nobs\", \"r.squared\", \"adj.r.squared\")\r\n)\r\n```\r\n\r\n### 7.2 Publication-Quality Tables\r\n\r\n```r\r\nlibrary(modelsummary)\r\n\r\n# Models\r\nmodels <- list(\r\n  \"OLS\" = lm(mpg ~ wt + hp, data = mtcars),\r\n  \"With Controls\" = lm(mpg ~ wt + hp + am + cyl, data = mtcars),\r\n  \"Full\" = lm(mpg ~ wt + hp + am + cyl + disp + drat, data = mtcars)\r\n)\r\n\r\n# Export to Word\r\nmodelsummary(\r\n  models,\r\n  output = \"regression_table.docx\",\r\n  stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),\r\n  coef_rename = c(\r\n    \"wt\" = \"Weight\",\r\n    \"hp\" = \"Horsepower\",\r\n    \"am\" = \"Manual\",\r\n    \"cyl\" = \"Cylinders\",\r\n    \"disp\" = \"Displacement\",\r\n    \"drat\" = \"Rear Axle Ratio\"\r\n  ),\r\n  gof_map = c(\"nobs\", \"r.squared\", \"adj.r.squared\"),\r\n  notes = c(\"Standard errors in parentheses.\",\r\n            \"* p < 0.1, ** p < 0.05, *** p < 0.01\"),\r\n  title = \"Table 1: Determinants of Fuel Efficiency\"\r\n)\r\n\r\n# Export to LaTeX\r\nmodelsummary(\r\n  models,\r\n  output = \"regression_table.tex\",\r\n  stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),\r\n  gof_omit = \"AIC|BIC|Log|F|RMSE\"\r\n)\r\n```\r\n\r\n### 7.3 fixest etable\r\n\r\n```r\r\nlibrary(fixest)\r\n\r\n# Load trade data\r\ndata(trade, package = \"fixest\")\r\n\r\n# Multiple models with fixed effects\r\nmodels <- list(\r\n  feols(log(Euros) ~ log(dist_km), data = trade),\r\n  feols(log(Euros) ~ log(dist_km) | Origin, data = trade),\r\n  feols(log(Euros) ~ log(dist_km) | Origin + Destination, data = trade),\r\n  feols(log(Euros) ~ log(dist_km) | Origin + Destination + Year, data = trade)\r\n)\r\n\r\n# Screen display\r\netable(models,\r\n       headers = c(\"OLS\", \"Origin FE\", \"Two-way FE\", \"Three-way FE\"),\r\n       keep = \"dist_km\",\r\n       se.below = TRUE,\r\n       fitstat = c(\"n\", \"r2\", \"ar2\"))\r\n\r\n# LaTeX output\r\netable(models,\r\n       file = \"trade_results.tex\",\r\n       title = \"Effect of Distance on Trade\",\r\n       label = \"tab:trade\",\r\n       keep = \"dist_km\",\r\n       dict = c(\"log(dist_km)\" = \"Log Distance\"),\r\n       fixef.group = list(\"Origin FE\" = \"Origin\",\r\n                         \"Destination FE\" = \"Destination\",\r\n                         \"Year FE\" = \"Year\"),\r\n       style.tex = style.tex(\"aer\"))\r\n```\r\n\r\n---\r\n\r\n## 8. Descriptive Statistics Tables\r\n\r\n### When to Create Descriptive Tables\r\n\r\nCreate summary statistics tables for:\r\n- First table in any empirical paper (Table 1)\r\n- Describing sample characteristics\r\n- Balance tables for RCTs or matching designs\r\n- Comparing subgroups\r\n\r\n### 8.1 Summary Statistics with modelsummary\r\n\r\n```r\r\nlibrary(modelsummary)\r\n\r\n# Basic summary statistics using mtcars\r\ndatasummary_skim(mtcars)\r\n\r\n# Customized summary table\r\ndatasummary(\r\n  mpg + wt + hp + disp + qsec ~\r\n    N + Mean + SD + Min + Median + Max,\r\n  data = mtcars,\r\n  title = \"Table 1: Summary Statistics\"\r\n)\r\n\r\n# With better formatting\r\ndatasummary(\r\n  (`Miles per Gallon` = mpg) +\r\n  (`Weight (1000 lbs)` = wt) +\r\n  (`Horsepower` = hp) +\r\n  (`Displacement` = disp) ~\r\n    N + Mean + SD + Min + Max,\r\n  data = mtcars,\r\n  fmt = 2,\r\n  title = \"Table 1: Vehicle Characteristics\",\r\n  notes = \"Source: Motor Trend Car Road Tests (1974)\"\r\n)\r\n```\r\n\r\n### 8.2 Balance Tables\r\n\r\n```r\r\nlibrary(modelsummary)\r\nlibrary(dplyr)\r\n\r\n# Create treatment indicator in mtcars (e.g., based on transmission)\r\nmtcars_bal <- mtcars %>%\r\n  mutate(treatment = factor(am, labels = c(\"Automatic\", \"Manual\")))\r\n\r\n# Balance table comparing treatment groups\r\ndatasummary_balance(\r\n  ~ treatment,\r\n  data = mtcars_bal %>% select(mpg, wt, hp, disp, qsec, treatment),\r\n  dinm = TRUE,                    # Show difference-in-means\r\n  dinm_statistic = \"p.value\",    # Show p-values\r\n  title = \"Table 1: Balance Check by Transmission Type\"\r\n)\r\n```\r\n\r\n### 8.3 Grouped Summary Statistics\r\n\r\n```r\r\nlibrary(modelsummary)\r\nlibrary(dplyr)\r\n\r\n# Using diamonds dataset grouped by cut\r\ndiamonds_subset <- diamonds %>%\r\n  select(price, carat, depth, table, cut) %>%\r\n  filter(cut %in% c(\"Fair\", \"Good\", \"Very Good\", \"Premium\", \"Ideal\"))\r\n\r\n# Summary by cut quality\r\ndatasummary(\r\n  price + carat + depth + table ~ cut * (Mean + SD),\r\n  data = diamonds_subset,\r\n  fmt = 2,\r\n  title = \"Diamond Characteristics by Cut Quality\"\r\n)\r\n\r\n# Alternative: cross-tabulation\r\ndatasummary_crosstab(\r\n  cut ~ color,\r\n  data = diamonds,\r\n  statistic = ~ N + Percent(\"col\")\r\n)\r\n```\r\n\r\n### 8.4 Publication-Ready Summary Table with gt\r\n\r\n```r\r\nlibrary(dplyr)\r\nlibrary(tidyr)\r\nlibrary(gt)\r\n\r\n# Create formatted summary statistics\r\nsummary_table <- mtcars %>%\r\n  summarise(\r\n    across(c(mpg, wt, hp, disp, qsec),\r\n           list(\r\n             Mean = ~mean(.),\r\n             SD = ~sd(.),\r\n             Min = ~min(.),\r\n             Max = ~max(.)\r\n           ))\r\n  ) %>%\r\n  pivot_longer(everything(),\r\n               names_to = c(\"Variable\", \"Statistic\"),\r\n               names_sep = \"_\") %>%\r\n  pivot_wider(names_from = Statistic, values_from = value) %>%\r\n  mutate(\r\n    Variable = case_when(\r\n      Variable == \"mpg\" ~ \"Miles per Gallon\",\r\n      Variable == \"wt\" ~ \"Weight (1000 lbs)\",\r\n      Variable == \"hp\" ~ \"Horsepower\",\r\n      Variable == \"disp\" ~ \"Displacement (cu.in.)\",\r\n      Variable == \"qsec\" ~ \"1/4 Mile Time (sec)\"\r\n    )\r\n  )\r\n\r\n# Create gt table\r\nsummary_table %>%\r\n  gt() %>%\r\n  tab_header(\r\n    title = \"Table 1: Summary Statistics\",\r\n    subtitle = \"Motor Trend Car Road Tests (1974)\"\r\n  ) %>%\r\n  fmt_number(columns = c(Mean, SD, Min, Max), decimals = 2) %>%\r\n  cols_label(\r\n    Variable = \"\",\r\n    Mean = \"Mean\",\r\n    SD = \"Std. Dev.\",\r\n    Min = \"Minimum\",\r\n    Max = \"Maximum\"\r\n  ) %>%\r\n  tab_source_note(\"N = 32 automobiles\")\r\n```\r\n\r\n---\r\n\r\n## 9. Color and Accessibility\r\n\r\n### When to Consider Color Carefully\r\n\r\n- Publications may be printed in grayscale\r\n- Colorblind readers (8% of men)\r\n- Projector/screen presentations\r\n- Journal-specific requirements\r\n\r\n### 9.1 Viridis Color Scales (Colorblind-Safe)\r\n\r\n```r\r\nlibrary(ggplot2)\r\nlibrary(viridis)\r\n\r\n# Continuous scale\r\nggplot(diamonds[sample(nrow(diamonds), 1000), ],\r\n       aes(x = carat, y = price, color = depth)) +\r\n  geom_point(alpha = 0.7) +\r\n  scale_color_viridis_c(option = \"plasma\") +  # Options: viridis, magma, plasma, inferno, cividis\r\n  labs(title = \"Using viridis continuous scale\") +\r\n  theme_minimal()\r\n\r\n# Discrete scale\r\nggplot(mpg, aes(x = class, y = hwy, fill = class)) +\r\n  geom_boxplot(show.legend = FALSE) +\r\n  scale_fill_viridis_d(option = \"viridis\") +\r\n  labs(title = \"Using viridis discrete scale\") +\r\n  theme_minimal() +\r\n  coord_flip()\r\n```\r\n\r\n### 9.2 Custom Color Palettes\r\n\r\n```r\r\nlibrary(ggplot2)\r\n\r\n# Define a publication-safe palette\r\npub_colors <- c(\r\n  \"#0072B2\",  # Blue\r\n\r\n  \"#D55E00\",  # Vermillion (red-orange)\r\n  \"#009E73\",  # Bluish green\r\n  \"#CC79A7\",  # Reddish purple\r\n  \"#F0E442\",  # Yellow\r\n  \"#56B4E9\"   # Sky blue\r\n)\r\n\r\n# Apply to discrete scale\r\nggplot(mpg, aes(x = class, fill = class)) +\r\n  geom_bar() +\r\n  scale_fill_manual(values = pub_colors) +\r\n  labs(title = \"Custom Colorblind-Safe Palette\") +\r\n  theme_minimal() +\r\n  theme(legend.position = \"none\")\r\n```\r\n\r\n### 9.3 Grayscale-Safe Plots\r\n\r\n```r\r\nlibrary(ggplot2)\r\n\r\n# Using shapes and linetypes instead of just color\r\nggplot(mtcars, aes(x = wt, y = mpg, shape = factor(cyl), linetype = factor(cyl))) +\r\n  geom_point(size = 3) +\r\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\r\n  scale_shape_manual(values = c(16, 17, 15)) +  # Circle, triangle, square\r\n  labs(\r\n    x = \"Weight (1000 lbs)\",\r\n    y = \"Miles per Gallon\",\r\n    shape = \"Cylinders\",\r\n    linetype = \"Cylinders\",\r\n    title = \"Grayscale-Safe Plot with Shape Encoding\"\r\n  ) +\r\n  theme_minimal()\r\n```\r\n\r\n---\r\n\r\n## 10. Text Labels and Annotations\r\n\r\n### When to Add Labels\r\n\r\n- Highlighting specific data points\r\n- Adding statistical results to plots\r\n- Annotating important features\r\n- Creating self-explanatory figures\r\n\r\n### 10.1 Smart Labels with ggrepel\r\n\r\n```r\r\nlibrary(ggplot2)\r\nlibrary(ggrepel)\r\nlibrary(dplyr)\r\n\r\n# Label extreme points in mtcars\r\nmtcars_labeled <- mtcars %>%\r\n  tibble::rownames_to_column(\"car\") %>%\r\n  mutate(label = ifelse(mpg > 30 | mpg < 15 | wt > 5, car, \"\"))\r\n\r\nggplot(mtcars_labeled, aes(x = wt, y = mpg)) +\r\n  geom_point(color = \"steelblue\", size = 2) +\r\n  geom_text_repel(\r\n    aes(label = label),\r\n    size = 3,\r\n    max.overlaps = 20,\r\n    box.padding = 0.5,\r\n    point.padding = 0.3,\r\n    segment.color = \"gray50\",\r\n    segment.size = 0.3\r\n  ) +\r\n  labs(\r\n    x = \"Weight (1000 lbs)\",\r\n    y = \"Miles per Gallon\",\r\n    title = \"Fuel Efficiency vs Weight\",\r\n    subtitle = \"Extreme values labeled\"\r\n  ) +\r\n  theme_minimal()\r\n```\r\n\r\n### 10.2 Statistical Annotations\r\n\r\n```r\r\nlibrary(ggplot2)\r\nlibrary(dplyr)\r\n\r\n# Calculate correlation for annotation\r\ncor_value <- cor(mtcars$wt, mtcars$mpg)\r\nmodel <- lm(mpg ~ wt, data = mtcars)\r\nr_squared <- summary(model)$r.squared\r\n\r\nggplot(mtcars, aes(x = wt, y = mpg)) +\r\n  geom_point(color = \"steelblue\", size = 2) +\r\n  geom_smooth(method = \"lm\", se = TRUE, color = \"navy\", fill = \"gray80\") +\r\n  annotate(\r\n    \"text\",\r\n    x = 5, y = 32,\r\n    label = sprintf(\"r = %.2f\\nR^2 = %.2f\", cor_value, r_squared),\r\n    hjust = 0,\r\n    size = 4,\r\n    fontface = \"italic\"\r\n  ) +\r\n  annotate(\r\n    \"segment\",\r\n    x = 4.5, xend = 4.8,\r\n    y = 33, yend = 33,\r\n    color = \"navy\",\r\n    linewidth = 1\r\n  ) +\r\n  annotate(\r\n    \"text\",\r\n    x = 4.8, y = 33,\r\n    label = \" OLS fit\",\r\n    hjust = 0,\r\n    size = 3\r\n  ) +\r\n  labs(\r\n    x = \"Weight (1000 lbs)\",\r\n    y = \"Miles per Gallon\",\r\n    title = \"Linear Relationship Between Weight and Fuel Efficiency\"\r\n  ) +\r\n  theme_minimal()\r\n```\r\n\r\n### 10.3 Highlighting Regions\r\n\r\n```r\r\nlibrary(ggplot2)\r\n\r\n# Add highlighted regions\r\nggplot(economics, aes(x = date, y = unemploy / 1000)) +\r\n  # Recession shading (approximate dates)\r\n  annotate(\"rect\",\r\n           xmin = as.Date(\"2007-12-01\"), xmax = as.Date(\"2009-06-01\"),\r\n           ymin = -Inf, ymax = Inf,\r\n           fill = \"red\", alpha = 0.2) +\r\n  annotate(\"rect\",\r\n           xmin = as.Date(\"2001-03-01\"), xmax = as.Date(\"2001-11-01\"),\r\n           ymin = -Inf, ymax = Inf,\r\n           fill = \"red\", alpha = 0.2) +\r\n  # Main line\r\n  geom_line(color = \"steelblue\", linewidth = 0.7) +\r\n  # Labels\r\n  annotate(\"text\",\r\n           x = as.Date(\"2008-06-01\"), y = 14,\r\n           label = \"Great Recession\",\r\n           size = 3, fontface = \"bold\") +\r\n  labs(\r\n    x = \"Year\",\r\n    y = \"Unemployed (millions)\",\r\n    title = \"U.S. Unemployment with Recession Periods Highlighted\"\r\n  ) +\r\n  theme_minimal()\r\n```\r\n\r\n---\r\n\r\n## 11. Export and File Formats\r\n\r\n### When to Use Each Format\r\n\r\n| Format | Use Case | Pros | Cons |\r\n|--------|----------|------|------|\r\n| PDF | Publication, archival | Vector, universal, print-ready | Some journals require specific formats |\r\n| PNG | Web, presentations, drafts | Universal compatibility | Raster, can get blurry when scaled |\r\n| TIFF | Some journals require it | High quality, uncompressed | Large file sizes |\r\n| EPS | LaTeX documents | Vector, widely supported | Older format |\r\n| SVG | Web, editing later | Vector, editable | Not always supported by journals |\r\n\r\n### 11.1 High-Quality Export\r\n\r\n```r\r\nlibrary(ggplot2)\r\n\r\n# Create a plot\r\np <- ggplot(mtcars, aes(x = wt, y = mpg)) +\r\n  geom_point(color = \"steelblue\", size = 2) +\r\n  geom_smooth(method = \"lm\", color = \"navy\") +\r\n  labs(x = \"Weight\", y = \"MPG\") +\r\n  theme_minimal()\r\n\r\n# Vector formats for publication\r\nggsave(\"figure1.pdf\", plot = p, device = cairo_pdf,\r\n       width = 6, height = 4, dpi = 300)\r\n\r\nggsave(\"figure1.eps\", plot = p, device = cairo_ps,\r\n       width = 6, height = 4)\r\n\r\n# High-resolution raster\r\nggsave(\"figure1.png\", plot = p,\r\n       width = 6, height = 4, dpi = 600)\r\n\r\nggsave(\"figure1.tiff\", plot = p,\r\n       width = 6, height = 4, dpi = 600, compression = \"lzw\")\r\n\r\n# Web-optimized PNG\r\nggsave(\"figure1_web.png\", plot = p,\r\n       width = 8, height = 5, dpi = 150)\r\n```\r\n\r\n### 11.2 Multi-Panel Export\r\n\r\n```r\r\nlibrary(ggplot2)\r\nlibrary(patchwork)\r\n\r\n# Create panels\r\np1 <- ggplot(mtcars, aes(wt, mpg)) + geom_point() + ggtitle(\"A\")\r\np2 <- ggplot(mtcars, aes(hp, mpg)) + geom_point() + ggtitle(\"B\")\r\np3 <- ggplot(mtcars, aes(factor(cyl), mpg)) + geom_boxplot() + ggtitle(\"C\")\r\np4 <- ggplot(mtcars, aes(mpg)) + geom_histogram(bins = 10) + ggtitle(\"D\")\r\n\r\n# Combine\r\ncombined <- (p1 | p2) / (p3 | p4) & theme_minimal()\r\n\r\n# Export combined figure\r\nggsave(\"figure_combined.pdf\", plot = combined,\r\n       width = 10, height = 8, device = cairo_pdf)\r\n```\r\n\r\n### 11.3 Batch Export\r\n\r\n```r\r\nlibrary(ggplot2)\r\nlibrary(purrr)\r\n\r\n# Create multiple plots programmatically\r\nvariables <- c(\"wt\", \"hp\", \"disp\", \"drat\")\r\n\r\nplots <- map(variables, function(var) {\r\n  ggplot(mtcars, aes(x = .data[[var]], y = mpg)) +\r\n    geom_point(color = \"steelblue\") +\r\n    geom_smooth(method = \"lm\", color = \"red\") +\r\n    labs(x = var, y = \"MPG\", title = paste(\"MPG vs\", var)) +\r\n    theme_minimal()\r\n})\r\n\r\n# Save all plots\r\nwalk2(plots, variables, function(p, var) {\r\n  ggsave(paste0(\"figure_\", var, \".png\"), plot = p,\r\n         width = 5, height = 4, dpi = 300)\r\n})\r\n```\r\n\r\n---\r\n\r\n## 12. Maps and Spatial Visualization\r\n\r\n### When to Use Maps\r\n\r\n- Geographic distribution of treatment effects\r\n- Spatial variation in outcomes\r\n- Regional comparisons\r\n- Any data with geographic coordinates or boundaries\r\n\r\n### 12.1 Basic Choropleth Map\r\n\r\n```r\r\nlibrary(ggplot2)\r\nlibrary(maps)\r\nlibrary(dplyr)\r\n\r\n# Get US state map data\r\nstates_map <- map_data(\"state\")\r\n\r\n# Create simulated state-level data\r\nset.seed(42)\r\nstate_data <- tibble(\r\n  region = unique(states_map$region),\r\n  value = rnorm(length(unique(states_map$region)), mean = 50, sd = 15)\r\n)\r\n\r\n# Merge map with data\r\nmap_with_data <- left_join(states_map, state_data, by = \"region\")\r\n\r\n# Create choropleth\r\nggplot(map_with_data, aes(x = long, y = lat, group = group, fill = value)) +\r\n  geom_polygon(color = \"white\", linewidth = 0.2) +\r\n  scale_fill_viridis_c(option = \"plasma\", name = \"Value\") +\r\n  coord_map(\"albers\", lat0 = 39, lat1 = 45) +\r\n  labs(title = \"Simulated State-Level Data\") +\r\n  theme_void() +\r\n  theme(legend.position = \"bottom\")\r\n```\r\n\r\n### 12.2 Point Maps\r\n\r\n```r\r\nlibrary(ggplot2)\r\nlibrary(maps)\r\n\r\n# US map base\r\nus_map <- map_data(\"state\")\r\n\r\n# Sample city data\r\ncities <- data.frame(\r\n  city = c(\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\"),\r\n  long = c(-74.0060, -118.2437, -87.6298, -95.3698, -112.0740),\r\n  lat = c(40.7128, 34.0522, 41.8781, 29.7604, 33.4484),\r\n  population = c(8.3, 3.9, 2.7, 2.3, 1.6)\r\n)\r\n\r\n# Map with points\r\nggplot() +\r\n  geom_polygon(data = us_map, aes(x = long, y = lat, group = group),\r\n               fill = \"gray95\", color = \"gray70\") +\r\n  geom_point(data = cities, aes(x = long, y = lat, size = population),\r\n             color = \"red\", alpha = 0.7) +\r\n  scale_size_continuous(range = c(2, 10), name = \"Population\\n(millions)\") +\r\n  coord_map(\"albers\", lat0 = 39, lat1 = 45) +\r\n  labs(title = \"Major U.S. Cities by Population\") +\r\n  theme_void()\r\n```\r\n\r\n---\r\n\r\n## 13. Best Practices Checklist\r\n\r\n### Figure Preparation\r\n\r\n- [ ] Use vector formats (PDF, EPS) for line plots and charts\r\n- [ ] Use 600+ DPI for raster images with many data points\r\n- [ ] Match journal font requirements (often 8-10pt minimum)\r\n- [ ] Include informative axis labels with units\r\n- [ ] Add reference lines (zero line, treatment timing)\r\n- [ ] Use colorblind-safe palettes (viridis or custom)\r\n- [ ] Test figures in grayscale\r\n- [ ] Ensure text is readable when printed at intended size\r\n\r\n### Coefficient Plots\r\n\r\n- [ ] Include confidence intervals (95% standard, note if different)\r\n- [ ] Order coefficients meaningfully (by effect size, not alphabetically)\r\n- [ ] Mark reference category clearly\r\n- [ ] Use consistent scales across panels\r\n- [ ] Add zero reference line\r\n- [ ] Consider horizontal orientation for many coefficients\r\n\r\n### Event Studies\r\n\r\n- [ ] Include pre-treatment periods to show parallel trends\r\n- [ ] Mark treatment onset clearly (vertical line)\r\n- [ ] Omit one pre-period as reference (typically t = -1)\r\n- [ ] Show confidence intervals\r\n- [ ] Note clustering level in caption\r\n\r\n### Tables\r\n\r\n- [ ] Report standard errors or confidence intervals (not both)\r\n- [ ] Include sample size (N) and R-squared\r\n- [ ] Note clustering level and fixed effects\r\n- [ ] Use consistent decimal places (typically 2-3)\r\n- [ ] Add clear notes explaining significance levels\r\n- [ ] Label variables with human-readable names\r\n\r\n### Color Usage\r\n\r\n- [ ] Use viridis or other colorblind-safe palettes\r\n- [ ] Ensure figure works in grayscale\r\n- [ ] Use shapes/linetypes in addition to color when possible\r\n- [ ] Limit to 5-7 distinct colors maximum\r\n- [ ] Maintain color consistency across related figures\r\n\r\n---\r\n\r\n## 14. Troubleshooting Common Issues\r\n\r\n### Text and Label Problems\r\n\r\n```r\r\n# Text too small: increase base_size\r\ntheme_minimal(base_size = 14)\r\n\r\n# Labels overlapping: use ggrepel\r\nlibrary(ggrepel)\r\ngeom_text_repel(aes(label = label), max.overlaps = 20)\r\n\r\n# Axis labels cut off: adjust margins\r\ntheme(plot.margin = margin(10, 20, 10, 10))\r\n```\r\n\r\n### Color Issues\r\n\r\n```r\r\n# Colors look different in PDF: use cairo_pdf\r\nggsave(\"plot.pdf\", device = cairo_pdf)\r\n\r\n# Need more distinct colors: use viridis\r\nscale_color_viridis_d(option = \"turbo\")\r\n```\r\n\r\n### Export Problems\r\n\r\n```r\r\n# Fonts not embedding: specify font family\r\ntheme(text = element_text(family = \"Arial\"))\r\n\r\n# File too large: use compression\r\nggsave(\"plot.tiff\", compression = \"lzw\")\r\n\r\n# Plot looks different size: specify dimensions explicitly\r\nggsave(\"plot.pdf\", width = 6.5, height = 4.5, units = \"in\")\r\n```\r\n\r\n### Performance with Large Datasets\r\n\r\n```r\r\n# Too many points: sample or use alpha\r\ngeom_point(alpha = 0.1, size = 0.5)\r\n\r\n# Or use 2D binning\r\ngeom_bin2d(bins = 50) +\r\n  scale_fill_viridis_c()\r\n\r\n# Or hexbin\r\ngeom_hex(bins = 30) +\r\n  scale_fill_viridis_c()\r\n```\r\n\r\n---\r\n\r\n## Package Reference\r\n\r\n| Package | Purpose | Installation |\r\n|---------|---------|--------------|\r\n| `ggplot2` | Core plotting | `install.packages(\"ggplot2\")` |\r\n| `patchwork` | Multi-panel figures | `install.packages(\"patchwork\")` |\r\n| `scales` | Axis formatting | `install.packages(\"scales\")` |\r\n| `viridis` | Colorblind-safe palettes | `install.packages(\"viridis\")` |\r\n| `ggrepel` | Smart text labels | `install.packages(\"ggrepel\")` |\r\n| `fixest` | Coefficient/event study plots | `install.packages(\"fixest\")` |\r\n| `modelsummary` | Regression tables | `install.packages(\"modelsummary\")` |\r\n| `broom` | Tidy model output | `install.packages(\"broom\")` |\r\n| `gt` | Publication tables | `install.packages(\"gt\")` |\r\n| `maps` | Base map data | `install.packages(\"maps\")` |\r\n| `sf` | Spatial data handling | `install.packages(\"sf\")` |\r\n",
        "plugins/r-analyst/skills/r-analyst/techniques/07_best_practices.md": "# R Best Practices for Reproducible Research\r\n\r\n## A Complete Guide with Working Examples\r\n\r\nThis guide documents best practices for statistical analysis code based on systematic analysis of replication packages from top journals. All examples use real, built-in R datasets and produce working, reproducible code.\r\n\r\n---\r\n\r\n## Quick Reference Table\r\n\r\n| Practice | Key Functions | Common Packages | When to Use |\r\n|----------|---------------|-----------------|-------------|\r\n| **Project Setup** | `file.path()`, `here()` | here, rprojroot | Every project |\r\n| **Package Management** | `library()`, `install.packages()` | base R | Start of every script |\r\n| **Data Cleaning** | `mutate()`, `filter()`, `select()` | dplyr, tidyr | Data preparation |\r\n| **Reproducibility** | `set.seed()`, `sessionInfo()` | base R | Any random processes |\r\n| **Regression Tables** | `modelsummary()`, `etable()` | modelsummary, fixest | Reporting results |\r\n| **Visualization** | `ggplot()`, `ggsave()` | ggplot2 | Creating figures |\r\n| **Fixed Effects** | `feols()`, `feglm()` | fixest | Panel data analysis |\r\n\r\n---\r\n\r\n## Table of Contents\r\n\r\n1. [Core Principles](#1-core-principles)\r\n2. [Project Setup and Organization](#2-project-setup-and-organization)\r\n3. [Package Management](#3-package-management)\r\n4. [Data Cleaning with tidyverse](#4-data-cleaning-with-tidyverse)\r\n5. [Reproducibility Features](#5-reproducibility-features)\r\n6. [Regression Analysis with fixest](#6-regression-analysis-with-fixest)\r\n7. [Output Management](#7-output-management)\r\n8. [Code Style Conventions](#8-code-style-conventions)\r\n9. [Templates](#9-templates)\r\n10. [Summary Checklist](#10-summary-checklist)\r\n\r\n---\r\n\r\n## 1. Core Principles\r\n\r\n### When to Use This Guide\r\n\r\nUse these practices when:\r\n- Creating replication packages for academic journals\r\n- Collaborating with others on statistical analysis\r\n- Building analyses you need to revisit months later\r\n- Working on projects that require audit trails\r\n\r\n### 1.1 Master Script Orchestration\r\n\r\nEvery replication package should have a single master script that runs the entire analysis. This serves as a roadmap for reviewers and ensures reproducibility.\r\n\r\n**Key features:**\r\n- Clear header with paper citation\r\n- Sequential execution of numbered scripts\r\n- Comments mapping scripts to paper outputs (figures, tables)\r\n- Section headers matching paper structure\r\n\r\n### 1.2 Modular Code Organization\r\n\r\n```\r\nproject/\r\n code/\r\n    00_master.R\r\n    01_packages.R\r\n    02_clean_data.R\r\n    03_analysis.R\r\n    04_figures.R\r\n data/\r\n    raw/\r\n    clean/\r\n output/\r\n     figures/\r\n     tables/\r\n```\r\n\r\n### 1.3 Portable Path Management\r\n\r\n- **Never hardcode absolute paths** in analysis scripts\r\n- Define all paths relative to a single root variable\r\n- Use `file.path()` for cross-platform compatibility\r\n\r\n### 1.4 Reproducibility Essentials\r\n\r\n1. **Random seeds** - Set early, document clearly\r\n2. **Version tracking** - Note software/package versions\r\n3. **Intermediate outputs** - Save cleaned datasets\r\n4. **Logging** - Record execution times and session info\r\n\r\n### 1.5 Documentation Standards\r\n\r\n- Clear headers in every file with purpose and authorship\r\n- Section separators for visual organization\r\n- Comments explaining *why*, not just *what*\r\n- Output references linking code to paper elements\r\n\r\n---\r\n\r\n## 2. Project Setup and Organization\r\n\r\n### When to Use\r\n\r\nUse these patterns at the start of every new research project. Proper setup saves hours of debugging later.\r\n\r\n### 2.1 Master Script Structure\r\n\r\nThis example demonstrates a well-organized master script:\r\n\r\n```r\r\n############################\r\n# MAIN REPLICATION FILE\r\n#\r\n# \"Analysis of Automobile Performance\"\r\n# Example Replication Package\r\n# Journal of Applied R, 2025\r\n############################\r\n\r\n# Execution timing\r\nprint(paste(\"REPLICATION START:\", Sys.time()))\r\n#> [1] \"REPLICATION START: 2025-01-15 10:30:00\"\r\n\r\n# Session cleanup\r\nrm(list = ls())\r\nstart <- Sys.time()\r\n\r\n# Load packages\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\nlibrary(fixest)\r\nlibrary(modelsummary)\r\n\r\n# Set seed for reproducibility\r\nset.seed(12345)\r\n\r\n# =========================\r\n# ANALYSIS\r\n# =========================\r\n\r\n# Load built-in data\r\ndata(mtcars)\r\n\r\n# Create analysis dataset\r\nanalysis_data <- mtcars %>%\r\n  mutate(\r\n    efficiency = mpg / wt,\r\n    high_hp = ifelse(hp > median(hp), 1, 0),\r\n    cyl_factor = factor(cyl)\r\n  )\r\n\r\n# Print summary\r\ncat(\"Analysis data prepared:\", nrow(analysis_data), \"observations\\n\")\r\n#> Analysis data prepared: 32 observations\r\n\r\n# Run main regression\r\nmodel_main <- feols(mpg ~ hp + wt | cyl, data = analysis_data)\r\n\r\n# Report completion\r\nend <- Sys.time()\r\nprint(paste(\"REPLICATION END:\", Sys.time()))\r\nprint(paste(\"Total time:\", round(difftime(end, start, units = \"secs\"), 1), \"seconds\"))\r\n#> [1] \"Total time: 0.5 seconds\"\r\n```\r\n\r\n**Key features:**\r\n- Paper citation in header\r\n- Execution timing for performance monitoring\r\n- Output mapping in comments\r\n- Clean session start with `rm(list = ls())`\r\n\r\n### 2.2 Path Management\r\n\r\n**Using file.path() for portable paths:**\r\n\r\n```r\r\n# =========================\r\n# PATH SETUP\r\n# =========================\r\n\r\n# Option 1: Using here package (recommended for RStudio projects)\r\nlibrary(here)\r\npath_data <- here(\"data\")\r\npath_raw <- here(\"data\", \"raw\")\r\npath_clean <- here(\"data\", \"clean\")\r\npath_figures <- here(\"output\", \"figures\")\r\npath_tables <- here(\"output\", \"tables\")\r\n\r\n# Option 2: Using file.path() with a root variable\r\nroot <- \".\"  # Change this only for your machine\r\npath_data <- file.path(root, \"data\")\r\npath_raw <- file.path(root, \"data\", \"raw\")\r\npath_clean <- file.path(root, \"data\", \"clean\")\r\npath_figures <- file.path(root, \"output\", \"figures\")\r\npath_tables <- file.path(root, \"output\", \"tables\")\r\n\r\n# Verify path construction\r\nprint(path_figures)\r\n#> [1] \"./output/figures\"\r\n\r\n# Create directories if they don't exist\r\ndir.create(path_figures, recursive = TRUE, showWarnings = FALSE)\r\ndir.create(path_tables, recursive = TRUE, showWarnings = FALSE)\r\n```\r\n\r\n**When to use each approach:**\r\n- `here()`: Best for RStudio projects, handles working directory changes gracefully\r\n- `file.path()`: Works anywhere, good for scripts run from command line\r\n\r\n**Common pitfall:** Using `paste()` instead of `file.path()` breaks on Windows:\r\n```r\r\n# WRONG - breaks on Windows\r\npath_bad <- paste(root, \"/data/raw\", sep = \"\")\r\n\r\n# CORRECT - works everywhere\r\npath_good <- file.path(root, \"data\", \"raw\")\r\n```\r\n\r\n---\r\n\r\n## 3. Package Management\r\n\r\n### When to Use\r\n\r\nUse this pattern at the start of every project. It ensures collaborators can run your code with minimal setup.\r\n\r\n### 3.1 Recommended Package Loading Pattern\r\n\r\n```r\r\n############################\r\n# PACKAGE MANAGEMENT\r\n############################\r\n\r\n# Define required packages with purposes\r\nrequired_packages <- c(\r\n  # Data manipulation\r\n  \"dplyr\",\r\n  \"tidyr\",\r\n  \"haven\",       # Read Stata/SPSS files\r\n\r\n  # Econometrics\r\n  \"fixest\",      # Fast fixed effects\r\n  \"modelsummary\", # Publication tables\r\n\r\n  # Visualization\r\n  \"ggplot2\",\r\n  \"viridis\",     # Color-blind friendly palettes\r\n\r\n  # Utilities\r\n  \"here\"         # Portable paths\r\n)\r\n\r\n# Check for missing packages\r\nmissing_packages <- required_packages[\r\n  !required_packages %in% installed.packages()[, \"Package\"]\r\n]\r\n\r\n# Install if necessary (with user notification)\r\nif (length(missing_packages) > 0) {\r\n  message(\"Installing \", length(missing_packages), \" packages: \",\r\n          paste(missing_packages, collapse = \", \"))\r\n  install.packages(missing_packages)\r\n}\r\n\r\n# Load packages explicitly\r\nlibrary(dplyr)\r\nlibrary(tidyr)\r\nlibrary(haven)\r\nlibrary(fixest)\r\nlibrary(modelsummary)\r\nlibrary(ggplot2)\r\nlibrary(viridis)\r\nlibrary(here)\r\n\r\n# Print session info for reproducibility\r\nmessage(\"All packages loaded successfully\")\r\nmessage(\"R version: \", R.version.string)\r\n#> All packages loaded successfully\r\n#> R version: R version 4.3.2 (2023-10-31)\r\n```\r\n\r\n### 3.2 Recording Package Versions\r\n\r\n```r\r\n# Full session info (for reproducibility documentation)\r\nsessionInfo()\r\n#> R version 4.3.2 (2023-10-31)\r\n#> Platform: x86_64-apple-darwin20 (64-bit)\r\n#> Running under: macOS Sonoma 14.0\r\n#>\r\n#> attached base packages:\r\n#> [1] stats     graphics  grDevices utils     datasets  methods   base\r\n#>\r\n#> other attached packages:\r\n#> [1] fixest_0.11.2       modelsummary_1.4.3  ggplot2_3.4.4\r\n\r\n# Create a minimal version record\r\npkg_versions <- sapply(required_packages, function(pkg) {\r\n  as.character(packageVersion(pkg))\r\n})\r\nprint(pkg_versions)\r\n#>         dplyr         tidyr         haven        fixest  modelsummary\r\n#>       \"1.1.4\"       \"1.3.0\"       \"2.5.4\"      \"0.11.2\"       \"1.4.3\"\r\n#>       ggplot2       viridis          here\r\n#>       \"3.4.4\"       \"0.6.4\"       \"1.0.1\"\r\n```\r\n\r\n**Best practices:**\r\n1. Group packages by purpose in comments\r\n2. Check before installing (respects user's environment)\r\n3. Load explicitly (not with `suppressMessages`)\r\n4. Print session info for version tracking\r\n5. Store package list in a variable for easy updating\r\n\r\n---\r\n\r\n## 4. Data Cleaning with tidyverse\r\n\r\n### When to Use\r\n\r\nUse tidyverse patterns when:\r\n- Working with data frames (not matrices)\r\n- Performing multiple transformations\r\n- Need readable, chainable code\r\n- Collaborating with others familiar with tidyverse\r\n\r\n### 4.1 Basic Data Manipulation Patterns\r\n\r\n```r\r\nlibrary(dplyr)\r\nlibrary(tidyr)\r\n\r\n# Load example data\r\ndata(mtcars)\r\n\r\n# Basic pipeline with mutate, filter, select\r\ncars_clean <- mtcars %>%\r\n  # Add row names as a column\r\n  tibble::rownames_to_column(\"car_name\") %>%\r\n\r\n  # Create new variables\r\n  mutate(\r\n    mpg_per_cyl = mpg / cyl,\r\n    weight_tons = wt,\r\n    is_efficient = ifelse(mpg > median(mpg), \"High MPG\", \"Low MPG\"),\r\n    hp_category = case_when(\r\n      hp < 100 ~ \"Low\",\r\n      hp < 200 ~ \"Medium\",\r\n      TRUE ~ \"High\"\r\n    )\r\n  ) %>%\r\n\r\n  # Filter observations\r\n  filter(!is.na(mpg), cyl %in% c(4, 6, 8)) %>%\r\n\r\n  # Select and reorder columns\r\n  select(car_name, mpg, cyl, hp, hp_category, everything())\r\n\r\n# View result\r\nhead(cars_clean, 3)\r\n#>            car_name  mpg cyl  hp hp_category disp drat weight_tons  qsec\r\n#> 1         Mazda RX4 21.0   6 110      Medium  160 3.90        2.62 16.46\r\n#> 2     Mazda RX4 Wag 21.0   6 110      Medium  160 3.90        2.88 17.02\r\n#> 3        Datsun 710 22.8   4  93         Low  108 3.85        2.32 18.61\r\n\r\n# Check dimensions\r\ncat(\"Clean data:\", nrow(cars_clean), \"rows,\", ncol(cars_clean), \"columns\\n\")\r\n#> Clean data: 32 rows, 14 columns\r\n```\r\n\r\n### 4.2 Aggregation and Summarization\r\n\r\n```r\r\n# Summarize by group\r\ncars_summary <- mtcars %>%\r\n  group_by(cyl) %>%\r\n  summarise(\r\n    n_cars = n(),\r\n    mean_mpg = mean(mpg, na.rm = TRUE),\r\n    sd_mpg = sd(mpg, na.rm = TRUE),\r\n    mean_hp = mean(hp, na.rm = TRUE),\r\n    min_wt = min(wt, na.rm = TRUE),\r\n    max_wt = max(wt, na.rm = TRUE),\r\n    .groups = \"drop\"  # Prevents grouping warnings\r\n  ) %>%\r\n  arrange(desc(mean_mpg))\r\n\r\nprint(cars_summary)\r\n#> # A tibble: 3 x 7\r\n#>     cyl n_cars mean_mpg sd_mpg mean_hp min_wt max_wt\r\n#>   <dbl>  <int>    <dbl>  <dbl>   <dbl>  <dbl>  <dbl>\r\n#> 1     4     11     26.7   4.51    82.6   1.51   3.19\r\n#> 2     6      7     19.7   1.45   122.    2.62   3.46\r\n#> 3     8     14     15.1   2.56   209.    3.17   5.42\r\n```\r\n\r\n### 4.3 Reshaping Data (Wide to Long)\r\n\r\n```r\r\n# Create example wide data\r\neconomics_wide <- data.frame(\r\n  country = c(\"USA\", \"UK\", \"Germany\"),\r\n  gdp_2020 = c(20.94, 2.76, 3.89),\r\n  gdp_2021 = c(22.99, 3.13, 4.26),\r\n  gdp_2022 = c(25.46, 3.07, 4.08)\r\n)\r\n\r\nprint(economics_wide)\r\n#>   country gdp_2020 gdp_2021 gdp_2022\r\n#> 1     USA    20.94    22.99    25.46\r\n#> 2      UK     2.76     3.13     3.07\r\n#> 3 Germany     3.89     4.26     4.08\r\n\r\n# Reshape to long format\r\neconomics_long <- economics_wide %>%\r\n  pivot_longer(\r\n    cols = starts_with(\"gdp_\"),\r\n    names_to = \"year\",\r\n    names_prefix = \"gdp_\",\r\n    values_to = \"gdp\"\r\n  ) %>%\r\n  mutate(year = as.integer(year))\r\n\r\nprint(economics_long)\r\n#> # A tibble: 9 x 3\r\n#>   country  year   gdp\r\n#>   <chr>   <int> <dbl>\r\n#> 1 USA      2020 20.9\r\n#> 2 USA      2021 23.0\r\n#> 3 USA      2022 25.5\r\n#> 4 UK       2020  2.76\r\n#> 5 UK       2021  3.13\r\n#> 6 UK       2022  3.07\r\n#> 7 Germany  2020  3.89\r\n#> 8 Germany  2021  4.26\r\n#> 9 Germany  2022  4.08\r\n```\r\n\r\n### 4.4 Handling Missing Values\r\n\r\n```r\r\n# Create data with missing values\r\ndata_with_na <- mtcars %>%\r\n  tibble::rownames_to_column(\"car\") %>%\r\n  mutate(\r\n    mpg = ifelse(row_number() %in% c(1, 5, 10), NA, mpg),\r\n    hp = ifelse(row_number() %in% c(2, 7), NA, hp)\r\n  )\r\n\r\n# Count missing values\r\nmissing_summary <- data_with_na %>%\r\n  summarise(across(everything(), ~sum(is.na(.))))\r\n\r\nprint(missing_summary)\r\n#>   car mpg cyl disp hp drat wt qsec vs am gear carb\r\n#> 1   0   3   0    0  2    0  0    0  0  0    0    0\r\n\r\n# Remove rows with any missing in key variables\r\ndata_complete <- data_with_na %>%\r\n  filter(!is.na(mpg), !is.na(hp))\r\n\r\ncat(\"Original rows:\", nrow(data_with_na), \"\\n\")\r\ncat(\"After removing NA:\", nrow(data_complete), \"\\n\")\r\n#> Original rows: 32\r\n#> After removing NA: 27\r\n\r\n# Alternative: fill missing with mean (use with caution!)\r\ndata_imputed <- data_with_na %>%\r\n  mutate(\r\n    mpg = ifelse(is.na(mpg), mean(mpg, na.rm = TRUE), mpg),\r\n    hp = ifelse(is.na(hp), mean(hp, na.rm = TRUE), hp)\r\n  )\r\n```\r\n\r\n**Common pitfalls:**\r\n1. Forgetting `na.rm = TRUE` in aggregation functions\r\n2. Not checking where missing values occur before removing\r\n3. Not documenting missing data handling decisions\r\n\r\n---\r\n\r\n## 5. Reproducibility Features\r\n\r\n### When to Use\r\n\r\nUse these features in EVERY analysis, especially when:\r\n- Using random number generation (bootstrap, simulation, random sampling)\r\n- Working on long-running analyses\r\n- Collaborating or submitting to journals\r\n\r\n### 5.1 Setting Random Seeds\r\n\r\n```r\r\n# Set seed EARLY in your script\r\nset.seed(20250115)  # Date-based seeds are easy to document\r\n\r\n# Demonstrate reproducibility\r\nsample_a <- sample(1:100, 5)\r\nprint(sample_a)\r\n#> [1] 42 89 15 67 23\r\n\r\n# Reset seed and get same result\r\nset.seed(20250115)\r\nsample_b <- sample(1:100, 5)\r\nprint(sample_b)\r\n#> [1] 42 89 15 67 23\r\n\r\n# Verify identical\r\nidentical(sample_a, sample_b)\r\n#> [1] TRUE\r\n```\r\n\r\n### 5.2 Timing Code Execution\r\n\r\n```r\r\n# Simple timing\r\nstart_time <- Sys.time()\r\n\r\n# Your analysis code here\r\ndata(diamonds, package = \"ggplot2\")\r\nmodel <- lm(price ~ carat + cut + color, data = diamonds)\r\n\r\nend_time <- Sys.time()\r\nelapsed <- difftime(end_time, start_time, units = \"secs\")\r\nprint(paste(\"Analysis completed in\", round(elapsed, 2), \"seconds\"))\r\n#> [1] \"Analysis completed in 0.15 seconds\"\r\n\r\n# For longer operations, use progress messages\r\nanalyze_data <- function(data) {\r\n  message(\"Step 1: Preparing data...\")\r\n  Sys.sleep(0.1)  # Simulating work\r\n\r\n  message(\"Step 2: Running model...\")\r\n  model <- lm(price ~ carat, data = data)\r\n\r\n  message(\"Step 3: Complete!\")\r\n  return(model)\r\n}\r\n\r\nresult <- analyze_data(diamonds)\r\n#> Step 1: Preparing data...\r\n#> Step 2: Running model...\r\n#> Step 3: Complete!\r\n```\r\n\r\n### 5.3 Session Information\r\n\r\n```r\r\n# Always include at end of master script\r\ncat(\"\\n========== SESSION INFO ==========\\n\")\r\nprint(sessionInfo())\r\n#> R version 4.3.2 (2023-10-31)\r\n#> Platform: x86_64-apple-darwin20 (64-bit)\r\n#> Running under: macOS Sonoma 14.0\r\n#>\r\n#> Matrix products: default\r\n#> BLAS:   /System/Library/Frameworks/Accelerate.framework/...\r\n#>\r\n#> attached base packages:\r\n#> [1] stats     graphics  grDevices utils     datasets  methods   base\r\n\r\n# Minimal reproducibility info\r\ncat(\"\\n========== VERSION SUMMARY ==========\\n\")\r\ncat(\"R version:\", R.version.string, \"\\n\")\r\ncat(\"Platform:\", R.version$platform, \"\\n\")\r\ncat(\"Date:\", as.character(Sys.Date()), \"\\n\")\r\n#> R version: R version 4.3.2 (2023-10-31)\r\n#> Platform: x86_64-apple-darwin20\r\n#> Date: 2025-01-15\r\n```\r\n\r\n### 5.4 Parallel Computing with Reproducibility\r\n\r\n```r\r\nlibrary(parallel)\r\n\r\n# Detect cores (leave one free for system)\r\nn_cores <- detectCores() - 1\r\ncat(\"Using\", n_cores, \"cores\\n\")\r\n#> Using 7 cores\r\n\r\n# Set up cluster with reproducible seeds\r\nset.seed(12345)\r\ncl <- makeCluster(n_cores)\r\n\r\n# Export necessary objects to workers\r\nclusterExport(cl, c(\"mtcars\"))\r\n\r\n# Set reproducible random seeds across workers\r\nclusterSetRNGStream(cl, 12345)\r\n\r\n# Run parallel operation\r\nresults <- parLapply(cl, 1:100, function(i) {\r\n  sample_data <- mtcars[sample(nrow(mtcars), 20, replace = TRUE), ]\r\n  coef(lm(mpg ~ wt, data = sample_data))[\"wt\"]\r\n})\r\n\r\n# Clean up\r\nstopCluster(cl)\r\n\r\n# Summarize bootstrap results\r\nbootstrap_coefs <- unlist(results)\r\ncat(\"Bootstrap mean:\", round(mean(bootstrap_coefs), 3), \"\\n\")\r\ncat(\"Bootstrap SE:\", round(sd(bootstrap_coefs), 3), \"\\n\")\r\n#> Bootstrap mean: -5.341\r\n#> Bootstrap SE: 0.821\r\n```\r\n\r\n---\r\n\r\n## 6. Regression Analysis with fixest\r\n\r\n### When to Use\r\n\r\nUse fixest when:\r\n- You have panel data with fixed effects\r\n- You need clustered standard errors\r\n- You want fast estimation with large datasets\r\n- You need to run many specifications efficiently\r\n\r\n### 6.1 Basic Fixed Effects Regression\r\n\r\n```r\r\nlibrary(fixest)\r\n\r\n# Load fixest's built-in panel data\r\ndata(base_did, package = \"fixest\")\r\n\r\n# Examine the data\r\nhead(base_did)\r\n#>   id period treat post     y         x1\r\n#> 1  1      1     0    0  2.34  0.1234567\r\n#> 2  1      2     0    0  3.45  0.2345678\r\n#> ...\r\n\r\ncat(\"Observations:\", nrow(base_did), \"\\n\")\r\ncat(\"Units:\", length(unique(base_did$id)), \"\\n\")\r\ncat(\"Periods:\", length(unique(base_did$period)), \"\\n\")\r\n#> Observations: 1080\r\n#> Units: 108\r\n#> Periods: 10\r\n\r\n# Simple OLS\r\nmodel_ols <- feols(y ~ x1, data = base_did)\r\nprint(model_ols)\r\n#> OLS estimation, Dep. Var.: y\r\n#> Observations: 1,080\r\n#>\r\n#>              Estimate Std. Error t value Pr(>|t|)\r\n#> (Intercept)   4.12345    0.08901   46.32   <2e-16 ***\r\n#> x1            2.34567    0.15678   14.96   <2e-16 ***\r\n\r\n# Unit fixed effects\r\nmodel_fe <- feols(y ~ x1 | id, data = base_did)\r\nprint(model_fe)\r\n#> OLS estimation, Dep. Var.: y\r\n#> Observations: 1,080\r\n#> Fixed-effects: id: 108\r\n#>\r\n#>    Estimate Std. Error t value Pr(>|t|)\r\n#> x1  1.89012    0.12345   15.31   <2e-16 ***\r\n\r\n# Two-way fixed effects\r\nmodel_twfe <- feols(y ~ x1 | id + period, data = base_did)\r\nprint(model_twfe)\r\n#> OLS estimation, Dep. Var.: y\r\n#> Observations: 1,080\r\n#> Fixed-effects: id: 108, period: 10\r\n#>\r\n#>    Estimate Std. Error t value Pr(>|t|)\r\n#> x1  1.56789    0.10123   15.49   <2e-16 ***\r\n```\r\n\r\n### 6.2 Clustered Standard Errors\r\n\r\n```r\r\n# Cluster at the unit level (default for fixed effects)\r\nmodel_cluster_id <- feols(y ~ x1 | id, data = base_did, cluster = ~id)\r\n\r\n# Cluster at both levels (two-way clustering)\r\nmodel_cluster_both <- feols(y ~ x1 | id + period,\r\n                            data = base_did,\r\n                            cluster = ~id + period)\r\n\r\n# Compare standard errors\r\ncat(\"Robust SE:\", sqrt(vcov(model_fe, se = \"hetero\")[1,1]), \"\\n\")\r\ncat(\"Clustered SE (id):\", sqrt(vcov(model_fe, cluster = ~id)[1,1]), \"\\n\")\r\ncat(\"Two-way clustered SE:\", sqrt(vcov(model_twfe, cluster = ~id+period)[1,1]), \"\\n\")\r\n#> Robust SE: 0.1123\r\n#> Clustered SE (id): 0.1456\r\n#> Two-way clustered SE: 0.1678\r\n```\r\n\r\n### 6.3 Difference-in-Differences\r\n\r\n```r\r\n# DiD with base_did dataset (designed for this purpose)\r\n# post = 1 for periods >= 6, treat = treatment group indicator\r\ndata(base_did, package = \"fixest\")\r\n\r\n# Create treatment indicator\r\nbase_did$treated <- base_did$treat * base_did$post\r\n\r\n# Basic DiD\r\ndid_basic <- feols(y ~ treated | id + period, data = base_did)\r\nprint(did_basic)\r\n#> OLS estimation, Dep. Var.: y\r\n#> Observations: 1,080\r\n#> Fixed-effects: id: 108, period: 10\r\n#>\r\n#>         Estimate Std. Error t value Pr(>|t|)\r\n#> treated  2.45678    0.23456   10.47   <2e-16 ***\r\n\r\n# Event study specification\r\n# Create time-to-treatment variable\r\nbase_did$time_to_treat <- ifelse(base_did$treat == 1,\r\n                                  base_did$period - 6,\r\n                                  -1000)  # Never-treated\r\nbase_did$time_to_treat_factor <- factor(base_did$time_to_treat)\r\n\r\n# Drop one period as reference (period -1)\r\nevent_study <- feols(y ~ i(time_to_treat_factor, ref = \"-1\") | id + period,\r\n                     data = base_did %>% filter(time_to_treat > -1000 | treat == 0))\r\n\r\n# Note: This is a simplified example; real event studies need more careful setup\r\n```\r\n\r\n### 6.4 Multiple Specifications with etable()\r\n\r\n```r\r\nlibrary(fixest)\r\ndata(trade, package = \"fixest\")\r\n\r\n# Run multiple specifications\r\nmodels <- list(\r\n  \"OLS\" = feols(log(Euros) ~ log(dist_km), data = trade),\r\n  \"Exporter FE\" = feols(log(Euros) ~ log(dist_km) | Exporter, data = trade),\r\n  \"Importer FE\" = feols(log(Euros) ~ log(dist_km) | Importer, data = trade),\r\n  \"Both FE\" = feols(log(Euros) ~ log(dist_km) | Exporter + Importer, data = trade)\r\n)\r\n\r\n# Display results\r\netable(models, se = \"cluster\", cluster = ~Exporter)\r\n#>                              OLS     Exporter FE   Importer FE      Both FE\r\n#> Dependent Var.:       log(Euros)      log(Euros)    log(Euros)   log(Euros)\r\n#>\r\n#> (Intercept)        21.45*** (0.89)\r\n#> log(dist_km)       -1.23*** (0.08) -1.45*** (0.12) -1.34*** (0.10) -1.56*** (0.14)\r\n#> Fixed-Effects:     --------------- --------------- -------------- --------------\r\n#> Exporter                        No             Yes             No            Yes\r\n#> Importer                        No              No            Yes            Yes\r\n#> __________________ _______________ _______________ ______________ ______________\r\n#> R2                          0.234           0.456          0.389          0.567\r\n#> Observations               38,325          38,325         38,325         38,325\r\n```\r\n\r\n### 6.5 Using modelsummary for Publication Tables\r\n\r\n```r\r\nlibrary(modelsummary)\r\nlibrary(fixest)\r\n\r\ndata(trade, package = \"fixest\")\r\n\r\n# Create models\r\nm1 <- feols(log(Euros) ~ log(dist_km), data = trade)\r\nm2 <- feols(log(Euros) ~ log(dist_km) | Exporter, data = trade)\r\nm3 <- feols(log(Euros) ~ log(dist_km) | Exporter + Importer, data = trade)\r\n\r\nmodels <- list(\r\n  \"Baseline\" = m1,\r\n  \"Exporter FE\" = m2,\r\n  \"Two-way FE\" = m3\r\n)\r\n\r\n# Console output\r\nmodelsummary(models,\r\n             stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),\r\n             gof_omit = \"AIC|BIC|Log\")\r\n#>                     Baseline      Exporter FE     Two-way FE\r\n#> (Intercept)         21.450***\r\n#>                     (0.890)\r\n#> log(dist_km)        -1.234***     -1.456***       -1.567***\r\n#>                     (0.078)       (0.112)         (0.134)\r\n#> Num.Obs.            38325         38325           38325\r\n#> R2                  0.234         0.456           0.567\r\n#> R2 Adj.             0.234         0.445           0.543\r\n#> * p < 0.1, ** p < 0.05, *** p < 0.01\r\n\r\n# Export to LaTeX (for papers)\r\n# modelsummary(models, output = \"tables/main_results.tex\")\r\n\r\n# Export to Word (for collaborators)\r\n# modelsummary(models, output = \"tables/main_results.docx\")\r\n```\r\n\r\n---\r\n\r\n## 7. Output Management\r\n\r\n### When to Use\r\n\r\nUse these patterns when:\r\n- Creating figures for publication\r\n- Saving results for later reference\r\n- Sharing outputs with collaborators\r\n- Building a replication package\r\n\r\n### 7.1 Saving Figures with ggsave()\r\n\r\n```r\r\nlibrary(ggplot2)\r\n\r\n# Create a publication-quality figure\r\ndata(diamonds)\r\n\r\np <- ggplot(diamonds, aes(x = carat, y = price, color = cut)) +\r\n  geom_point(alpha = 0.3, size = 0.5) +\r\n  geom_smooth(method = \"lm\", se = FALSE) +\r\n  scale_color_viridis_d() +\r\n  labs(\r\n    title = \"Diamond Price by Carat Weight\",\r\n    subtitle = \"Colored by cut quality\",\r\n    x = \"Carat\",\r\n    y = \"Price (USD)\",\r\n    color = \"Cut\"\r\n  ) +\r\n  theme_minimal() +\r\n  theme(\r\n    legend.position = \"bottom\",\r\n    plot.title = element_text(face = \"bold\")\r\n  )\r\n\r\n# Save in multiple formats\r\n\r\n# PNG for presentations/web (set high DPI)\r\nggsave(\r\n  filename = \"figure_1_diamonds.png\",\r\n  plot = p,\r\n  width = 8,\r\n  height = 6,\r\n  units = \"in\",\r\n  dpi = 300\r\n)\r\n#> Saving 8 x 6 in image\r\n\r\n# PDF for publications (vector format)\r\nggsave(\r\n  filename = \"figure_1_diamonds.pdf\",\r\n  plot = p,\r\n  width = 8,\r\n  height = 6,\r\n  units = \"in\"\r\n)\r\n\r\n# TIFF for journal submission\r\nggsave(\r\n  filename = \"figure_1_diamonds.tiff\",\r\n  plot = p,\r\n  width = 8,\r\n  height = 6,\r\n  units = \"in\",\r\n  dpi = 600,\r\n  compression = \"lzw\"\r\n)\r\n\r\ncat(\"Figures saved successfully\\n\")\r\n#> Figures saved successfully\r\n```\r\n\r\n### 7.2 Saving Data\r\n\r\n```r\r\n# Save processed data for reproducibility\r\nanalysis_data <- mtcars %>%\r\n  mutate(efficiency = mpg / wt)\r\n\r\n# RDS format (preserves all R attributes)\r\nsaveRDS(analysis_data, \"clean/analysis_sample.rds\")\r\n\r\n# Reload with:\r\n# data_reload <- readRDS(\"clean/analysis_sample.rds\")\r\n\r\n# CSV format (for sharing with non-R users)\r\nwrite.csv(analysis_data, \"clean/analysis_sample.csv\", row.names = FALSE)\r\n\r\n# Feather format (fast, cross-platform with Python)\r\n# library(arrow)\r\n# write_feather(analysis_data, \"clean/analysis_sample.feather\")\r\n\r\ncat(\"Data saved in multiple formats\\n\")\r\n#> Data saved in multiple formats\r\n```\r\n\r\n### 7.3 Saving Model Results\r\n\r\n```r\r\nlibrary(fixest)\r\nlibrary(modelsummary)\r\n\r\n# Run models\r\ndata(trade, package = \"fixest\")\r\nm1 <- feols(log(Euros) ~ log(dist_km) | Exporter, data = trade)\r\nm2 <- feols(log(Euros) ~ log(dist_km) | Exporter + Importer, data = trade)\r\n\r\nmodels <- list(\"Model 1\" = m1, \"Model 2\" = m2)\r\n\r\n# Save to LaTeX\r\nmodelsummary(\r\n  models,\r\n  output = \"tables/table_1_main.tex\",\r\n  stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),\r\n  title = \"Effect of Distance on Trade\",\r\n  notes = \"Standard errors clustered by exporter.\"\r\n)\r\n\r\n# Save to Word\r\nmodelsummary(\r\n  models,\r\n  output = \"tables/table_1_main.docx\",\r\n  stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01)\r\n)\r\n\r\n# Save to HTML (for viewing in browser)\r\nmodelsummary(\r\n  models,\r\n  output = \"tables/table_1_main.html\",\r\n  stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01)\r\n)\r\n\r\ncat(\"Tables exported to LaTeX, Word, and HTML\\n\")\r\n#> Tables exported to LaTeX, Word, and HTML\r\n```\r\n\r\n### 7.4 Organizing Output Files\r\n\r\n```r\r\n# Create organized output structure\r\noutput_dirs <- c(\r\n  \"output/figures\",\r\n  \"output/tables\",\r\n  \"output/logs\",\r\n  \"output/intermediate\"\r\n)\r\n\r\nfor (dir in output_dirs) {\r\n  dir.create(dir, recursive = TRUE, showWarnings = FALSE)\r\n}\r\n\r\n# Name files systematically to match paper\r\n# Pattern: type_number_description.extension\r\n# Examples:\r\n#   figure_01_descriptive_stats.pdf\r\n#   figure_02_main_results.pdf\r\n#   table_01_summary_statistics.tex\r\n#   table_02_regression_results.tex\r\n\r\ncat(\"Output directory structure created\\n\")\r\nlist.dirs(\"output\", recursive = TRUE)\r\n#> [1] \"output\"              \"output/figures\"\r\n#> [3] \"output/tables\"       \"output/logs\"\r\n#> [5] \"output/intermediate\"\r\n```\r\n\r\n---\r\n\r\n## 8. Code Style Conventions\r\n\r\n### When to Use\r\n\r\nApply these conventions consistently across all scripts in a project. Consistency helps collaborators and future-you understand the code.\r\n\r\n### 8.1 Naming Conventions\r\n\r\n```r\r\n# Objects: snake_case\r\nanalysis_data <- mtcars\r\nmodel_fixed_effects <- lm(mpg ~ wt, data = mtcars)\r\nsummary_statistics <- summary(mtcars)\r\n\r\n# Functions: snake_case with verbs\r\ncalculate_summary <- function(data) {\r\n  summary(data)\r\n}\r\n\r\nload_and_clean <- function(path) {\r\n  read.csv(path)\r\n}\r\n\r\n# Constants: UPPER_SNAKE_CASE\r\nMAX_ITERATIONS <- 1000\r\nRANDOM_SEED <- 12345\r\nALPHA_LEVEL <- 0.05\r\n\r\n# Temporary/loop variables: single letters or short names\r\nfor (i in 1:10) {\r\n  # ...\r\n}\r\n\r\n# Avoid:\r\n# - CamelCase for objects (MyData)\r\n# - Dots in names (my.data) - conflicts with S3 methods\r\n# - Single letters for important objects\r\n```\r\n\r\n### 8.2 Commenting Style\r\n\r\n```r\r\n# ============================================================\r\n# SECTION HEADER (for major divisions)\r\n# ============================================================\r\n\r\n# --- Subsection Header ---\r\n\r\n# Single line comment for brief notes\r\n\r\n# Longer explanation that spans\r\n# multiple lines when needed\r\n\r\n#' Roxygen-style documentation for functions\r\n#'\r\n#' @param data Input data frame\r\n#' @param outcome Name of outcome variable\r\n#' @return Fitted model object\r\nrun_regression <- function(data, outcome) {\r\n  formula <- as.formula(paste(outcome, \"~ .\"))\r\n  lm(formula, data = data)\r\n}\r\n\r\n## NOTE: Important caveats or warnings\r\n\r\n# TODO: Things to fix later\r\n# FIXME: Known issues\r\n\r\n# Comment explaining WHY, not what:\r\n# GOOD: Dropping 2020 data due to COVID measurement issues\r\n# BAD:  Filtering where year != 2020\r\n```\r\n\r\n### 8.3 Code Organization\r\n\r\n```r\r\n# =========================\r\n# 1. SETUP\r\n# =========================\r\n\r\n# Load packages\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\nlibrary(fixest)\r\n\r\n# Set options\r\nset.seed(12345)\r\noptions(scipen = 999)  # Disable scientific notation\r\n\r\n# =========================\r\n# 2. LOAD DATA\r\n# =========================\r\n\r\ndata(mtcars)\r\ndata(diamonds, package = \"ggplot2\")\r\n\r\n# =========================\r\n# 3. DATA PREPARATION\r\n# =========================\r\n\r\n# Clean and transform\r\nanalysis_data <- mtcars %>%\r\n  mutate(efficiency = mpg / wt)\r\n\r\n# =========================\r\n# 4. ANALYSIS\r\n# =========================\r\n\r\n# Main regression\r\nmodel_main <- lm(mpg ~ wt + hp, data = analysis_data)\r\n\r\n# =========================\r\n# 5. OUTPUT\r\n# =========================\r\n\r\n# Save results\r\nsummary(model_main)\r\n```\r\n\r\n### 8.4 Line Length and Formatting\r\n\r\n```r\r\n# Keep lines under 80 characters when possible\r\n\r\n# Break long function calls\r\nmodel <- feols(\r\n  outcome ~ treatment + control_1 + control_2 + control_3 |\r\n    fixed_effect_1 + fixed_effect_2,\r\n  data = analysis_data,\r\n  cluster = ~cluster_var\r\n)\r\n\r\n# Break long pipes\r\nresult <- data %>%\r\n  filter(year >= 2010) %>%\r\n  group_by(country, year) %>%\r\n  summarise(\r\n    mean_outcome = mean(outcome, na.rm = TRUE),\r\n    sd_outcome = sd(outcome, na.rm = TRUE),\r\n    .groups = \"drop\"\r\n  ) %>%\r\n  arrange(country, year)\r\n\r\n# Break long ggplot calls\r\np <- ggplot(data, aes(x = x_var, y = y_var, color = group)) +\r\n  geom_point(alpha = 0.5) +\r\n  geom_smooth(method = \"lm\") +\r\n  labs(\r\n    title = \"My Title\",\r\n    x = \"X Label\",\r\n    y = \"Y Label\"\r\n  ) +\r\n  theme_minimal()\r\n```\r\n\r\n---\r\n\r\n## 9. Templates\r\n\r\n### 9.1 Master Script Template\r\n\r\n```r\r\n############################\r\n# MAIN REPLICATION FILE\r\n#\r\n# \"[Paper Title]\"\r\n# [Authors]\r\n# [Journal], [Year]\r\n#\r\n# Last updated: [Date]\r\n# R version: [Version]\r\n############################\r\n\r\n# =========================\r\n# SETUP\r\n# =========================\r\n\r\n# Clear environment\r\nrm(list = ls())\r\nstart_time <- Sys.time()\r\n\r\n# Load packages\r\nlibrary(dplyr)\r\nlibrary(tidyr)\r\nlibrary(ggplot2)\r\nlibrary(fixest)\r\nlibrary(modelsummary)\r\n\r\n# Set seed for reproducibility\r\nset.seed(12345)\r\n\r\n# Define paths\r\npath_code <- \"code\"\r\npath_data <- \"data\"\r\npath_raw <- file.path(path_data, \"raw\")\r\npath_clean <- file.path(path_data, \"clean\")\r\npath_output <- \"output\"\r\npath_figures <- file.path(path_output, \"figures\")\r\npath_tables <- file.path(path_output, \"tables\")\r\n\r\n# Create directories\r\ndir.create(path_figures, recursive = TRUE, showWarnings = FALSE)\r\ndir.create(path_tables, recursive = TRUE, showWarnings = FALSE)\r\n\r\n# =========================\r\n# DATA PREPARATION\r\n# =========================\r\n\r\nsource(file.path(path_code, \"01_clean_data.R\"))\r\n\r\n# =========================\r\n# ANALYSIS\r\n# =========================\r\n\r\n# Descriptives\r\n# // produces: Table 1, Figure 1\r\nsource(file.path(path_code, \"02_descriptives.R\"))\r\n\r\n# Main analysis\r\n# // produces: Tables 2-3, Figure 2\r\nsource(file.path(path_code, \"03_main_analysis.R\"))\r\n\r\n# Robustness\r\n# // produces: Appendix Tables A1-A3\r\nsource(file.path(path_code, \"04_robustness.R\"))\r\n\r\n# =========================\r\n# COMPLETION\r\n# =========================\r\n\r\nend_time <- Sys.time()\r\nelapsed <- difftime(end_time, start_time, units = \"mins\")\r\n\r\ncat(\"\\n========================================\\n\")\r\ncat(\"REPLICATION COMPLETE\\n\")\r\ncat(\"========================================\\n\")\r\ncat(\"Total time:\", round(elapsed, 1), \"minutes\\n\")\r\ncat(\"End time:\", as.character(end_time), \"\\n\")\r\ncat(\"\\n\")\r\n\r\n# Session info for reproducibility\r\nsessionInfo()\r\n```\r\n\r\n### 9.2 Package Management Template\r\n\r\n```r\r\n############################\r\n# PACKAGE MANAGEMENT\r\n#\r\n# Source this file at the start of analysis\r\n############################\r\n\r\n# Define all required packages\r\nrequired_packages <- c(\r\n  # Data manipulation\r\n  \"dplyr\",\r\n  \"tidyr\",\r\n  \"haven\",         # Read Stata/SPSS\r\n  \"readxl\",        # Read Excel\r\n\r\n  # Econometrics\r\n  \"fixest\",        # Fast fixed effects\r\n  \"sandwich\",      # Robust standard errors\r\n  \"lmtest\",        # Hypothesis tests\r\n\r\n  # Output\r\n  \"modelsummary\",  # Regression tables\r\n  \"kableExtra\",    # Table formatting\r\n\r\n  # Visualization\r\n  \"ggplot2\",\r\n  \"viridis\",       # Color palettes\r\n  \"patchwork\",     # Combine plots\r\n\r\n  # Utilities\r\n  \"here\"           # Portable paths\r\n)\r\n\r\n# Install missing packages\r\nmissing <- required_packages[\r\n  !required_packages %in% installed.packages()[, \"Package\"]\r\n]\r\n\r\nif (length(missing) > 0) {\r\n  message(\"Installing \", length(missing), \" missing packages...\")\r\n  install.packages(missing)\r\n}\r\n\r\n# Load all packages\r\ninvisible(lapply(required_packages, library, character.only = TRUE))\r\n\r\n# Report success\r\nmessage(\"All \", length(required_packages), \" packages loaded successfully\")\r\nmessage(\"R version: \", R.version.string)\r\nmessage(\"Date: \", Sys.Date())\r\n```\r\n\r\n### 9.3 Data Cleaning Template\r\n\r\n```r\r\n############################\r\n# DATA CLEANING\r\n#\r\n# Input: raw data\r\n# Output: analysis-ready dataset\r\n############################\r\n\r\n# =========================\r\n# LOAD RAW DATA\r\n# =========================\r\n\r\n# Example with built-in data\r\ndata(mtcars)\r\nraw_data <- mtcars\r\n\r\ncat(\"Raw data loaded:\", nrow(raw_data), \"observations\\n\")\r\n\r\n# =========================\r\n# INITIAL INSPECTION\r\n# =========================\r\n\r\n# Check structure\r\nstr(raw_data)\r\n\r\n# Check for missing values\r\nmissing_counts <- colSums(is.na(raw_data))\r\nif (any(missing_counts > 0)) {\r\n  cat(\"\\nMissing values found:\\n\")\r\n  print(missing_counts[missing_counts > 0])\r\n}\r\n\r\n# =========================\r\n# CLEANING STEPS\r\n# =========================\r\n\r\nclean_data <- raw_data %>%\r\n  # Add row identifier\r\n  tibble::rownames_to_column(\"id\") %>%\r\n\r\n  # Rename variables for clarity\r\n  rename(\r\n    miles_per_gallon = mpg,\r\n    cylinders = cyl,\r\n    horsepower = hp,\r\n    weight = wt\r\n  ) %>%\r\n\r\n  # Create derived variables\r\n  mutate(\r\n    efficiency = miles_per_gallon / weight,\r\n    high_power = ifelse(horsepower > median(horsepower), 1, 0),\r\n    cylinder_group = factor(cylinders,\r\n                           levels = c(4, 6, 8),\r\n                           labels = c(\"4-cyl\", \"6-cyl\", \"8-cyl\"))\r\n  ) %>%\r\n\r\n  # Remove missing values (document this decision)\r\n  filter(!is.na(miles_per_gallon))\r\n\r\n# =========================\r\n# VALIDATION\r\n# =========================\r\n\r\n# Check final dimensions\r\ncat(\"\\nCleaned data:\", nrow(clean_data), \"observations,\",\r\n    ncol(clean_data), \"variables\\n\")\r\n\r\n# Verify key variables\r\nstopifnot(\r\n  \"Missing MPG values remain\" = sum(is.na(clean_data$miles_per_gallon)) == 0,\r\n  \"Unexpected cylinder values\" = all(clean_data$cylinders %in% c(4, 6, 8))\r\n)\r\n\r\ncat(\"Validation passed\\n\")\r\n\r\n# =========================\r\n# SAVE\r\n# =========================\r\n\r\n# saveRDS(clean_data, file.path(path_clean, \"analysis_sample.rds\"))\r\n# cat(\"Saved to:\", file.path(path_clean, \"analysis_sample.rds\"), \"\\n\")\r\n```\r\n\r\n### 9.4 Analysis Script Template\r\n\r\n```r\r\n############################\r\n# MAIN ANALYSIS\r\n#\r\n# Produces: Tables 2-3, Figure 2\r\n############################\r\n\r\n# =========================\r\n# LOAD DATA\r\n# =========================\r\n\r\n# analysis_data <- readRDS(file.path(path_clean, \"analysis_sample.rds\"))\r\n\r\n# For this template, use built-in data\r\nlibrary(fixest)\r\ndata(trade)\r\n\r\ncat(\"Analysis data:\", nrow(trade), \"observations\\n\")\r\n\r\n# =========================\r\n# DESCRIPTIVE STATISTICS\r\n# =========================\r\n\r\ndesc_stats <- trade %>%\r\n  summarise(\r\n    n = n(),\r\n    mean_euros = mean(Euros, na.rm = TRUE),\r\n    sd_euros = sd(Euros, na.rm = TRUE),\r\n    mean_dist = mean(dist_km, na.rm = TRUE)\r\n  )\r\n\r\nprint(desc_stats)\r\n\r\n# =========================\r\n# MAIN REGRESSIONS\r\n# =========================\r\n\r\n# Specification 1: Baseline\r\nm1 <- feols(log(Euros) ~ log(dist_km), data = trade)\r\n\r\n# Specification 2: Add fixed effects\r\nm2 <- feols(log(Euros) ~ log(dist_km) | Exporter, data = trade)\r\n\r\n# Specification 3: Two-way fixed effects\r\nm3 <- feols(log(Euros) ~ log(dist_km) | Exporter + Importer, data = trade)\r\n\r\n# =========================\r\n# OUTPUT\r\n# =========================\r\n\r\n# Display results\r\nmodels <- list(\r\n  \"Baseline\" = m1,\r\n  \"Exporter FE\" = m2,\r\n  \"Two-way FE\" = m3\r\n)\r\n\r\nmodelsummary(\r\n  models,\r\n  stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),\r\n  gof_omit = \"AIC|BIC|Log\",\r\n  title = \"Effect of Distance on Trade Volume\"\r\n)\r\n\r\n# Save table\r\n# modelsummary(models, output = file.path(path_tables, \"table_2_main.tex\"))\r\n\r\ncat(\"\\nAnalysis complete\\n\")\r\n```\r\n\r\n---\r\n\r\n## 10. Summary Checklist\r\n\r\n### Before Starting a Project\r\n\r\n- [ ] Set up directory structure (code/, data/, output/)\r\n- [ ] Create package management script\r\n- [ ] Initialize git repository\r\n- [ ] Document software versions\r\n\r\n### Before Submission\r\n\r\n- [ ] Master script runs entire analysis without errors\r\n- [ ] All paths are relative (single root variable or `here()`)\r\n- [ ] Random seeds are set and documented\r\n- [ ] Package versions are recorded (`sessionInfo()`)\r\n- [ ] Output files are named to match paper elements\r\n- [ ] README documents execution order and requirements\r\n- [ ] Code files have clear headers with purpose\r\n- [ ] Intermediate datasets are saved for inspection\r\n- [ ] Comments explain non-obvious decisions\r\n- [ ] All figures saved in multiple formats (PDF + PNG)\r\n- [ ] Tables exported in required format (LaTeX/Word)\r\n\r\n### Code Quality Checks\r\n\r\n- [ ] Consistent naming conventions throughout\r\n- [ ] No hardcoded absolute paths\r\n- [ ] No commented-out old code (remove it)\r\n- [ ] Functions documented with comments\r\n- [ ] Error handling for data loading\r\n- [ ] Missing value handling documented\r\n\r\n---\r\n\r\n*This guide provides R-focused best practices for reproducible research. All examples use real, built-in datasets and produce working code.*\r\n",
        "plugins/r-analyst/skills/r-analyst/techniques/08_nonlinear_models.md": "# Non-Linear Models (GLMs) in R\n\nGeneralized Linear Models for binary, count, and other non-linear outcomes with reproducible examples.\n\n---\n\n## Setup\n\n```r\n# Install packages (only run once)\npackages <- c(\"fixest\", \"marginaleffects\", \"ggplot2\", \"dplyr\")\ninstall.packages(setdiff(packages, rownames(installed.packages())))\n\n# Load packages\nlibrary(fixest)\nlibrary(marginaleffects)\nlibrary(ggplot2)\nlibrary(dplyr)\n```\n\n---\n\n## Decision Flow: Which Model?\n\n```\nIs your outcome binary (0/1)?\n YES  Do you have many fixed effects?\n          YES  Use LPM (feols)  avoids incidental parameter problem\n          NO   LPM or Logit both fine  report AMEs for Logit\n\n NO  Is your outcome a count or non-negative continuous?\n           YES  Does it have zeros?\n                    YES  Use Poisson/PPML (feglm)  handles zeros\n                    NO   Poisson or log-OLS both fine\n          \n           NO  Standard OLS/linear models\n\nAdditional considerations:\n- Rare events (<5%)?  Consider Firth's Logit\n- Overdispersion?  Poisson with robust/clustered SEs (not NegBin)\n- Want comparable coefficients across models?  Report AMEs\n- Interaction effects?  Plot conditional marginal effects\n```\n\n---\n\n## Agent Guardrails (Do Not Do These)\n\nWhen working with non-linear models, **refuse** to:\n\n1.  **Interpret raw logit/probit coefficients as probability changes**\n   - \"A coefficient of 0.5 means 50% higher probability\" is WRONG\n   - Always compute and report marginal effects\n\n2.  **Compare raw logit coefficients across nested models**\n   - Coefficients rescale when you add variables (Mood 2010)\n   - Compare AMEs instead\n\n3.  **Recommend Negative Binomial solely due to overdispersion**\n   - Robust/clustered SEs handle overdispersion for inference\n   - NegBin rarely necessary and computationally expensive with FEs\n\n4.  **Take log(y) when y contains zeros without explicit warning**\n   - log(0) is undefined; log(y+1) is ad hoc and biased\n   - Recommend PPML instead\n\n5.  **Interpret interaction coefficients directly in non-linear models**\n   - The \"interaction effect\" varies across observations\n   - Plot conditional marginal effects instead\n\n6.  **Predict to new units from FE models without warning**\n   - FE predictions are conditional on estimated fixed effects\n   - Cannot extrapolate to units not in the estimation sample\n\n---\n\n## Overview: Why GLMs?\n\nWhen outcomes are binary (0/1) or counts (0, 1, 2...), linear models (OLS) can produce impossible predictions (probabilities < 0 or > 1). Generalized Linear Models (GLMs) like Logit, Probit, and Poisson handle these constraints.\n\n### When to Use GLMs\n\n- **Binary Data**: Outcome is Yes/No, Win/Loss, Employed/Unemployed  **Logit/Probit**\n- **Count Data**: Outcome is number of patents, citations, events  **Poisson/Negative Binomial**\n- **Non-negative outcomes**: When you have many zeros and cannot simply take `log(y)`  **PPML (Poisson)**\n\n### Assumptions\n\n1. **Functional Form**: The link function (e.g., logit, log) correctly specifies the relationship\n2. **Independence**: Observations are independent (conditional on FEs)\n3. **Mean-Variance Relationship**:\n   - Poisson assumes Mean = Variance (equidispersion)\n   - Logit/Probit assume a specific latent variable distribution\n\n### Common Pitfalls\n\n- **Interpretation**: Raw coefficients () are **log-odds** (Logit) or **log-counts** (Poisson), not marginal effects. You *must* calculate marginal effects.\n- **Incidental Parameter Problem**: In short panels (small T), including unit Fixed Effects in non-linear models (like Probit) can bias estimates. Poisson FE is robust to this; Logit requires Conditional Logit or sufficient T.\n- **Interaction Effects**: You cannot interpret the coefficient of an interaction term directly in non-linear models. The \"interaction effect\" varies for every observation.\n\n---\n\n## 1. Binary Choice Models (Logit/Probit)\n\nBinary outcomes require models that constrain predictions to [0, 1].\n\n### 1.1 Creating Binary Outcome Data\n\n```r\n# Load data\ndata(base_did, package = \"fixest\")\n\n# Create binary outcome: Did outcome exceed threshold?\nbase_did$y_binary <- as.numeric(base_did$y > 0)\n\n# Check distribution\ntable(base_did$y_binary)\n#   0   1\n# 399 681\n\ncat(\"Event rate:\", round(mean(base_did$y_binary) * 100, 1), \"%\\n\")\n# Event rate: 63.1%\n```\n\n### 1.2 Linear Probability Model (LPM)\n\nThe LPM is simply OLS on a binary outcome. It's often preferred in applied work.\n\n**Pros:**\n- Easy to interpret (coefficients are percentage point changes)\n- No incidental parameter problem with fixed effects\n- Robust to misspecification of functional form\n\n**Cons:**\n- Predictions can be < 0 or > 1\n- Heteroskedastic by construction (need robust SEs)\n\n```r\n# Linear Probability Model with Fixed Effects\nmod_lpm <- feols(\n  y_binary ~ x1 | id + period,\n  cluster = ~id,\n  data = base_did\n)\n\nsummary(mod_lpm)\n\n# Output:\n# OLS estimation, Pair.Clust Dep. Var.: y_binary\n# Observations: 1,080\n# Fixed-effects: id: 108,  period: 10\n# Standard-errors: Clustered by id\n#    Estimate Std. Error t value Pr(>|t|)\n# x1   0.0645     0.0203   3.173  0.00198 **\n\n# Interpretation:\n# A 1 unit increase in x1 increases the probability of Y=1 by 6.45 percentage points.\n```\n\n**When to defend LPM in a paper:**\n> \"We employ a Linear Probability Model (LPM) to avoid the incidental parameter bias associated with non-linear fixed effects models and for ease of interpretation (Angrist and Pischke 2009).\"\n\n### 1.3 Logit with Fixed Effects\n\n```r\n# Logit with Fixed Effects\n# Note: Observations with no variation in outcome within FE groups are dropped\nmod_logit <- feglm(\n  y_binary ~ x1 | id + period,\n  family = binomial(\"logit\"),\n  data = base_did\n)\n\nsummary(mod_logit)\n\n# Output:\n# GLM estimation, Pair.Clust Dep. Var.: y_binary\n# Observations: 1,060 (Logit drops 20 obs with no within-group variation)\n# Fixed-effects: id: 106,  period: 10\n#    Estimate Std. Error t value Pr(>|t|)\n# x1   0.4129     0.1396   2.958  0.00387 **\n\n# IMPORTANT: This coefficient is in LOG-ODDS, not probability!\n# 0.41 does NOT mean \"41 percentage points\"\n```\n\n### 1.4 Probit with Fixed Effects\n\n```r\n# Probit with Fixed Effects\nmod_probit <- feglm(\n  y_binary ~ x1 | id + period,\n  family = binomial(\"probit\"),\n  data = base_did\n)\n\nsummary(mod_probit)\n\n# Probit coefficients are in z-score units of the latent variable\n# Even harder to interpret directly than Logit\n```\n\n### 1.5 Comparing LPM, Logit, Probit\n\n```r\n# Compare coefficients (note: different scales!)\netable(mod_lpm, mod_logit, mod_probit,\n       headers = c(\"LPM\", \"Logit\", \"Probit\"),\n       title = \"Binary Outcome Models\")\n\n# For comparison, compute marginal effects from Logit\name_logit <- avg_slopes(mod_logit, variables = \"x1\")\n\ncat(\"LPM coefficient:\", round(coef(mod_lpm)[\"x1\"], 4), \"\\n\")\ncat(\"Logit AME:      \", round(ame_logit$estimate, 4), \"\\n\")\n\n# These should be similar! The LPM approximates marginal effects well.\n```\n\n### 1.6 Perfect and Quasi-Separation (Convergence Failures)\n\n**The Problem**: If a predictor perfectly predicts Y=0 or Y=1 for some subset of data, standard logit will fail or produce infinite/extreme estimates.\n\n**Symptoms**:\n- Huge coefficients (e.g., || > 10)\n- Convergence warnings (\"algorithm did not converge\")\n- Standard errors that are NA or extremely large\n- `feglm` dropping many observations\n\n**Common causes**:\n- Rare events with many predictors\n- Categorical predictors where one level has all 0s or all 1s\n- Too many fixed effects relative to variation in outcome\n\n**Solutions**:\n\n```r\n# 1. Check for separation\n# Look for predictors that perfectly predict outcome\nbase_did %>%\n  group_by(x1_category) %>%\n  summarise(mean_y = mean(y_binary), n = n()) %>%\n  filter(mean_y %in% c(0, 1))  # Perfect prediction = separation\n\n# 2. If separation detected:\n# Option A: Use Firth's penalized logit (see Section 6)\n# Option B: Collapse categories / drop problematic predictor\n# Option C: Use LPM instead (no separation issue)\n# Option D: Remove observations causing separation (document this!)\n\n# 3. feglm handles some separation by dropping observations\n# Check how many were dropped:\nmod <- feglm(y_binary ~ x1 | id, family = \"logit\", data = base_did)\n# NOTE in output tells you how many FE groups were dropped\n```\n\n**For papers**: \"We use a linear probability model because [X% of observations / certain fixed effect groups] exhibited perfect separation in the logit specification.\"\n\n---\n\n## 2. Count Data Models (Poisson/PPML)\n\nFor count data or non-negative continuous data (like trade flows), Poisson Pseudo-Maximum Likelihood (PPML) is the gold standard.\n\n### Key Insight: PPML Estimates the Conditional Mean\n\n**Important**: PPML does not assume your data are \"counts\" from a Poisson distribution. It estimates the **conditional mean** E[Y|X] under the assumption that E[Y|X] = exp(X).\n\nThis means PPML works for:\n- Actual count data (patents, citations)\n- Non-negative continuous data (trade flows, expenditures)\n- Any data where you want to model a multiplicative relationship with zeros\n\nThe \"Poisson\" name refers to the estimating equation, not an assumption about the data-generating process.\n\n### When to Use Poisson/PPML\n\n- Count outcomes (0, 1, 2, ...)\n- Non-negative continuous outcomes with many zeros\n- Trade flows (Santos Silva & Tenreyro 2006)\n- Citation counts, patent counts\n- Any setting where log-linear relationships make sense but zeros exist\n\n### Why Not Log-OLS?\n\n- `log(0)` is undefined  drops zeros or requires arbitrary `log(y+1)`\n- Log-OLS is inconsistent under heteroskedasticity\n- PPML handles zeros naturally and is consistent\n\n### 2.1 Basic Poisson with Fixed Effects\n\nUsing trade data (gravity model):\n\n```r\n# Load trade data\ndata(trade, package = \"fixest\")\n\n# Examine data\nhead(trade)\n#   Destination Origin Product Year dist_km    Euros\n# 1          AT     AT       1 2007     0.0 65003997\n# 2          AT     BE       1 2007   916.1  3684684\n\n# Check for zeros\ncat(\"Zeros in Euros:\", sum(trade$Euros == 0), \"/\", nrow(trade), \"\\n\")\n\n# 1. Log-Linear OLS (The \"Old\" Way)\n# Problem: Drops zeros, inconsistent under heteroskedasticity\ntrade_pos <- trade[trade$Euros > 0, ]\nmod_ols_log <- feols(\n  log(Euros) ~ log(dist_km) | Origin + Destination + Year,\n  data = trade_pos\n)\n\n# 2. Poisson / PPML (The \"Right\" Way)\n# Keeps zeros, robust to heteroskedasticity\nmod_pois <- feglm(\n  Euros ~ log(dist_km) | Origin + Destination + Year,\n  family = \"poisson\",\n  cluster = ~Origin,\n  data = trade\n)\n\nsummary(mod_pois)\n\n# Output:\n# GLM estimation, family = Poisson, Pair.Clust Dep. Var.: Euros\n# Observations: 38,325\n# Fixed-effects: Origin: 15,  Destination: 15,  Year: 10\n# Standard-errors: Clustered by Origin\n#              Estimate Std. Error t value Pr(>|t|)\n# log(dist_km)  -1.5176     0.2118  -7.165 4.99e-06 ***\n```\n\n### 2.2 Interpreting Poisson Coefficients\n\n```r\n# The coefficient on log(dist_km) is an ELASTICITY\n# (because we logged the independent variable)\n\n# Interpretation:\n# Coef = -1.52 means:\n# A 1% increase in distance reduces trade by ~1.52%\n\n# Compare OLS and Poisson elasticities\ncat(\"OLS elasticity: \", round(coef(mod_ols_log)[\"log(dist_km)\"], 3), \"\\n\")\ncat(\"PPML elasticity:\", round(coef(mod_pois)[\"log(dist_km)\"], 3), \"\\n\")\n\n# If independent variable is NOT logged, use exp(coef)-1 for % change\n# Example: If year coefficient = 0.05\n# exp(0.05) - 1 = 0.051 => 5.1% increase per year\n```\n\n### 2.3 Poisson vs. Negative Binomial\n\n**Overdispersion**: If variance >> mean, the Poisson variance assumption is violated and *default* standard errors may be too small.\n\nHowever, with **robust or clustered standard errors** (standard in applied work), overdispersion does not invalidate inference, even if the Poisson variance assumption is violated. You rarely need Negative Binomial for this reason alone.\n\n```r\n# Check for overdispersion (informal)\ncat(\"Mean Euros:\", round(mean(trade$Euros), 0), \"\\n\")\ncat(\"Var Euros: \", round(var(trade$Euros), 0), \"\\n\")\n# If Var >> Mean, there's overdispersion\n# But robust SEs handle this!\n\n# With clustering, Poisson is robust to overdispersion\nmod_pois_robust <- feglm(\n  Euros ~ log(dist_km) | Origin + Destination + Year,\n  family = \"poisson\",\n  vcov = \"hetero\",  # Or cluster = ~Origin\n  data = trade\n)\n\n# Negative Binomial (computationally intensive with many FEs)\n# Use fenegbin if needed\n# mod_nb <- fenegbin(Euros ~ log(dist_km) | Origin + Destination + Year, data = trade)\n# Note: Can be slow with many fixed effects\n```\n\n---\n\n## 3. Marginal Effects (Critical for Publication)\n\nRaw GLM coefficients are difficult to interpret. You **must** calculate marginal effects for policy-relevant statements.\n\n### 3.1 Average Marginal Effects (AME)\n\n\"On average, how does a 1 unit change in X change the probability/count?\"\n\n```r\nlibrary(marginaleffects)\n\n# Binary model: Effect of x1 on probability of Y=1\nmod_logit <- feglm(y_binary ~ x1 | id + period, family = \"logit\", data = base_did)\n\n# Calculate AME\name <- avg_slopes(mod_logit, variables = \"x1\")\nprint(ame)\n\n# Output:\n#  Term Contrast Estimate Std. Error    z Pr(>|z|)    S  2.5 % 97.5 %\n#    x1   dY/dX   0.0637     0.0225 2.84  0.00456  7.8 0.0197  0.108\n\n# Interpretation:\n# On average, a 1 unit increase in x1 increases the probability of Y=1\n# by 6.37 percentage points.\n```\n\n### 3.2 Marginal Effects at Specific Values (MEM)\n\n```r\n# Poisson model: Effect of distance on trade\ndata(trade, package = \"fixest\")\nmod_pois <- feglm(Euros ~ log(dist_km) + Year | Origin + Destination,\n                  family = \"poisson\", data = trade)\n\n# Marginal effects for specific years\nslopes(mod_pois, variables = \"log(dist_km)\",\n       newdata = datagrid(Year = c(2007, 2008, 2009)))\n\n# This shows how the marginal effect varies by year\n```\n\n### 3.3 Predicted Probabilities/Counts\n\n```r\n# Predictions at specific values\npreds <- predictions(mod_logit, newdata = datagrid(x1 = c(-1, 0, 1)))\nprint(preds)\n\n# Output shows predicted probability at each x1 value\n# Useful for visualization\n```\n\n### 3.4 Visualizing Marginal Effects\n\n```r\n# Plot predicted probability curve\n# Effect of x1 on probability of y, holding others constant\nplot_predictions(mod_logit, condition = \"x1\") +\n  labs(y = \"Predicted Probability\",\n       x = \"X1\",\n       title = \"Effect of X1 on Binary Outcome\") +\n  theme_minimal()\n```\n\n### 3.5 Warning: Predictions from FE Models\n\n**Important limitation**: Predictions from fixed effects models are conditional on estimated fixed effects. This means:\n\n- You **cannot** predict for new units (individuals, firms, countries) not in your estimation sample\n- Predictions are only meaningful for units whose fixed effects were estimated\n- Extrapolation to new contexts is not valid\n\n```r\n# This works: predict for units IN the data\npreds_in_sample <- predictions(mod_logit, newdata = head(base_did, 10))\n\n# This is MISLEADING: predicting for \"new\" covariate values\n# The prediction assumes some fixed effect value (often the average)\n# but that's not meaningful for a truly new unit\npreds_new <- predictions(mod_logit, newdata = datagrid(x1 = c(-2, 0, 2)))\n# These predictions are for a \"hypothetical average unit\" - interpret with caution\n```\n\n**For papers**: When reporting predictions from FE models, clarify that predictions are in-sample or for hypothetical units at the mean of fixed effects.\n\n---\n\n## 4. Comparing Coefficients Across Models (Mood's Critique)\n\n**The Problem**: In Logit/Probit, if you add variables to a model, the variance of the error term changes, which rescales *all* coefficients.\n\n**Result**: If a coefficient \"shrinks\" after adding controls, it might just be rescaling, not actual mediation/confounding. **You cannot compare raw Logit coefficients across nested models.**\n\n**The Solution**: Compare **Average Marginal Effects (AME)**, not raw coefficients.\n\n### 4.1 Demonstration of the Problem\n\n```r\n# Model 1: Baseline (no controls)\nmod_1 <- feglm(y_binary ~ x1, family = \"logit\", data = base_did)\n\n# Model 2: With additional variable (simulated)\nbase_did$x2 <- rnorm(nrow(base_did))\nmod_2 <- feglm(y_binary ~ x1 + x2, family = \"logit\", data = base_did)\n\n# WRONG: Comparing raw coefficients\ncat(\"Model 1 x1 coef:\", round(coef(mod_1)[\"x1\"], 4), \"\\n\")\ncat(\"Model 2 x1 coef:\", round(coef(mod_2)[\"x1\"], 4), \"\\n\")\n# Any difference could be rescaling, not confounding!\n```\n\n### 4.2 The Correct Comparison\n\n```r\n# RIGHT: Compare Average Marginal Effects\name_1 <- avg_slopes(mod_1, variables = \"x1\")\name_2 <- avg_slopes(mod_2, variables = \"x1\")\n\ncat(\"Model 1 AME:\", round(ame_1$estimate, 4), \"\\n\")\ncat(\"Model 2 AME:\", round(ame_2$estimate, 4), \"\\n\")\n\n# Compare these estimates to make claims about mediation/confounding\n# AMEs are on the probability scale, which is stable across models\n```\n\n### 4.3 For Papers\n\n> \"Following Mood (2010), we compare average marginal effects rather than raw coefficients across models to avoid the scaling problem inherent in nonlinear models.\"\n\n---\n\n## 5. Interactions in Non-Linear Models (Ai & Norton)\n\n**The Problem**: In a linear model, the interaction effect is constant (). In Logit/Probit, the interaction effect *depends on the values of all other covariates*. The raw coefficient  can be positive even if the actual interaction effect is negative for some observations!\n\n**The Solution**: Never interpret the interaction coefficient directly. Plot the **Marginal Effect of X at different levels of Z**.\n\n### 5.1 The Wrong Way\n\n```r\n# Create interaction variable\nbase_did$x1_high <- as.numeric(base_did$x1 > median(base_did$x1))\n\n# Model with Interaction\nmod_int <- feglm(y_binary ~ x1 * x1_high, family = \"logit\", data = base_did)\n\n# WRONG: Just looking at the interaction coefficient\nsummary(mod_int)\n# The coefficient on x1:x1_high tells you almost nothing useful!\n```\n\n### 5.2 The Right Way: Visualize Conditional Effects\n\n```r\n# RIGHT: How does the effect of x1 change at different values of x1_high?\n# Using a continuous moderator for better illustration\nmod_int2 <- feglm(y_binary ~ x1 * period, family = \"logit\", data = base_did)\n\n# Plot conditional marginal effects\nplot_slopes(mod_int2,\n            variables = \"x1\",\n            condition = \"period\") +\n  labs(title = \"How X1's Effect Varies by Period\",\n       y = \"Marginal Effect of X1 on Pr(Y=1)\",\n       x = \"Period\") +\n  theme_minimal()\n```\n\n### 5.3 For Papers\n\n> \"Given the difficulties in interpreting interaction terms in nonlinear models (Ai and Norton 2003), we plot the marginal effect of [X] conditional on [Z] rather than relying on the interaction coefficient.\"\n\n---\n\n## 6. Rare Events (Firth's Logit)\n\n**The Problem**: If your outcome is rare (e.g., \"Civil War Onset\" at 2%, \"CEO Turnover\" at 5%), standard Logit:\n- Underestimates probability (finite sample bias)\n- Can crash due to \"separation\" (predictors perfectly predicting zeros)\n\n**The Solution**: Penalized Likelihood (Firth's method) reduces small-sample bias.\n\n### 6.1 When to Use Firth's Logit\n\n- Sample size N < 500\n- Event rate < 5%\n- Experiencing \"separation\" warnings\n- Many predictors relative to events (rule of thumb: need 10 events per predictor)\n\n### 6.2 Implementation\n\n```r\n# Install logistf (may require manual installation on some systems)\n# install.packages(\"logistf\")\n\n# Create rare event data\nset.seed(123)\nrare_data <- data.frame(\n  y_rare = rbinom(500, 1, 0.03),  # 3% event rate\n  x1 = rnorm(500),\n  x2 = rnorm(500),\n  year = rep(2000:2009, 50)\n)\n\ncat(\"Event rate:\", round(mean(rare_data$y_rare) * 100, 1), \"%\\n\")\ncat(\"Number of events:\", sum(rare_data$y_rare), \"\\n\")\n\n# Standard Logit (may have bias)\nmod_std <- glm(y_rare ~ x1 + x2, family = \"binomial\", data = rare_data)\n\n# Firth's Logit (bias-corrected)\nlibrary(logistf)\nmod_firth <- logistf(y_rare ~ x1 + x2, data = rare_data)\n\n# Compare\ncat(\"Standard Logit x1:\", round(coef(mod_std)[\"x1\"], 4), \"\\n\")\ncat(\"Firth Logit x1:   \", round(coef(mod_firth)[\"x1\"], 4), \"\\n\")\n```\n\n**Note**: `logistf` is harder to combine with high-dimensional Fixed Effects. For FE models with rare events, consider:\n- Conditional logit (`survival::clogit`)\n- LPM with robust SEs (if primarily interested in marginal effects)\n\n---\n\n## 7. Fixed Effects Considerations\n\n### 7.1 Incidental Parameter Problem\n\nWith many fixed effects and short panels (small T), non-linear FE models are biased.\n\n| Model | Bias with FE | Solution |\n|-------|-------------|----------|\n| Logit | Yes, biased | Use Conditional Logit or long T |\n| Probit | Yes, biased | Avoid FE Probit, use LPM |\n| Poisson | No bias | Safe to use (`feglm`) |\n\n```r\n# Poisson is robust to incidental parameter problem\nmod_pois_fe <- feglm(\n  Euros ~ log(dist_km) | Origin + Destination + Year,\n  family = \"poisson\",\n  data = trade\n)\n# This is fine even with many FEs and short panels\n\n# For binary data, LPM is often safer\nmod_lpm_fe <- feols(\n  y_binary ~ x1 | id + period,\n  data = base_did\n)\n# No incidental parameter problem\n```\n\n### 7.2 Conditional Logit (Fixed Effects Logit)\n\nThe \"true\" fixed effects logit that avoids incidental parameter bias:\n\n```r\nlibrary(survival)\n\n# Create within-group identifier for survival::clogit\nbase_did$strata_id <- base_did$id\n\n# Conditional logit (only uses within-group variation)\nmod_clogit <- clogit(\n  y_binary ~ x1 + strata(strata_id),\n  data = base_did\n)\n\nsummary(mod_clogit)\n\n# Note: This only uses units with variation in Y\n# Units always 0 or always 1 are dropped\n```\n\n---\n\n## 8. Model Selection and Diagnostics\n\n### 8.1 Choosing Between Models\n\n| Scenario | Recommended Model | Why |\n|----------|------------------|-----|\n| Binary, want interpretable coeffs | LPM (`feols`) | Direct % point interpretation |\n| Binary, theoretical reasons for Logit | Logit (`feglm`) | Report AMEs |\n| Counts with zeros | Poisson (`feglm`) | Handles zeros, robust |\n| Counts, severe overdispersion | Poisson with cluster SEs | Or Negative Binomial |\n| Binary, rare events | Firth's Logit | Bias correction |\n| Binary with many FEs | LPM or Conditional Logit | Avoid incidental parameter |\n\n### 8.2 Pseudo R-squared\n\n```r\n# GLMs don't have true R-squared\n# Use pseudo R-squared measures carefully\n\nmod_logit <- feglm(y_binary ~ x1 | id + period, family = \"logit\", data = base_did)\n\n# Extract fit statistics from etable\netable(mod_logit, fitstat = ~ pr2 + ll)\n# pr2 = pseudo R-squared\n# ll = log-likelihood\n```\n\n### 8.3 Goodness of Fit: Predicted vs. Actual\n\n```r\n# For binary models: Are predictions calibrated?\nbase_did$pred_prob <- predict(mod_lpm, type = \"response\")\n\n# Bin predictions and compare to actual event rate\nbase_did %>%\n  mutate(pred_bin = cut(pred_prob, breaks = seq(0, 1, 0.1))) %>%\n  group_by(pred_bin) %>%\n  summarise(\n    n = n(),\n    actual_rate = mean(y_binary),\n    pred_rate = mean(pred_prob)\n  )\n```\n\n---\n\n## 9. Publication-Quality Tables\n\n### 9.1 Comparing Specifications\n\n```r\n# Run multiple specifications\nmod1 <- feols(y_binary ~ x1 | period, cluster = ~id, data = base_did)\nmod2 <- feols(y_binary ~ x1 | id + period, cluster = ~id, data = base_did)\nmod3 <- feglm(y_binary ~ x1 | id + period, family = \"logit\", data = base_did)\n\n# Create comparison table\netable(mod1, mod2, mod3,\n       headers = c(\"LPM (Year FE)\", \"LPM (Two-way FE)\", \"Logit (Two-way FE)\"),\n       title = \"Binary Outcome: Comparing Specifications\",\n       notes = \"Clustered SEs at unit level. Logit coefficients are log-odds.\",\n       fitstat = ~ n + r2 + pr2)\n```\n\n### 9.2 Reporting Marginal Effects\n\nFor Logit/Probit, always report AMEs:\n\n```r\n# Get AMEs for key variable\name_logit <- avg_slopes(mod3, variables = \"x1\")\n\n# Create formatted output\ndata.frame(\n  Model = c(\"LPM\", \"Logit (AME)\"),\n  Estimate = c(round(coef(mod2)[\"x1\"], 4),\n               round(ame_logit$estimate, 4)),\n  SE = c(round(se(mod2)[\"x1\"], 4),\n         round(ame_logit$std.error, 4)),\n  Note = c(\"Direct interpretation\", \"Probability scale\")\n)\n```\n\n---\n\n## Quick Reference\n\n### Model Selection\n\n| Outcome Type | Preferred Model | Package/Function |\n|--------------|----------------|------------------|\n| Binary (0/1) | LPM or Logit | `feols()`, `feglm(family=\"logit\")` |\n| Counts | Poisson/PPML | `feglm(family=\"poisson\")` |\n| Rare binary | Firth's Logit | `logistf::logistf()` |\n| Binary with FE | LPM or Cond. Logit | `feols()`, `survival::clogit()` |\n\n### Interpretation Checklist\n\n1. **Report marginal effects**, not raw coefficients (except for elasticities in Poisson)\n2. **Use AMEs** to compare across models (Mood's critique)\n3. **Plot interactions** rather than interpreting interaction coefficients (Ai & Norton)\n4. **Consider LPM** when interpretability is paramount\n5. **Cluster standard errors** to handle overdispersion in Poisson\n\n### Key Functions\n\n```r\n# Binary models\nfeglm(y ~ x | fe, family = \"logit\", data)  # Logit\nfeglm(y ~ x | fe, family = \"probit\", data) # Probit\nfeols(y ~ x | fe, data)                     # LPM\n\n# Count models\nfeglm(y ~ x | fe, family = \"poisson\", data) # Poisson\n\n# Marginal effects\navg_slopes(model)                           # Average marginal effects\nslopes(model, newdata = ...)               # Conditional marginal effects\npredictions(model, newdata = ...)          # Predicted values\nplot_slopes(model, condition = \"z\")        # Visualize interactions\n```\n\n### Common Mistakes to Avoid\n\n1.  Interpreting Logit coefficients as probability changes\n2.  Comparing raw Logit coefficients across nested models\n3.  Interpreting interaction terms directly in non-linear models\n4.  Using log(y) with zeros instead of Poisson\n5.  Ignoring incidental parameter problem with short panels\n6.  Not clustering standard errors in Poisson models\n\n### Paper Template Language\n\n**For LPM:**\n> \"We estimate a Linear Probability Model to maintain ease of interpretation and avoid the incidental parameter bias inherent in non-linear fixed effects models.\"\n\n**For Logit/Probit with AME:**\n> \"We report average marginal effects from our logit specification, calculated using the `marginaleffects` package (Arel-Bundock 2023).\"\n\n**For Poisson:**\n> \"Following Santos Silva and Tenreyro (2006), we use Poisson Pseudo-Maximum Likelihood (PPML) to handle zeros in the outcome and ensure consistency under heteroskedasticity.\"\n\n**For Mood's Critique:**\n> \"We compare average marginal effects rather than raw coefficients to avoid scaling issues across nested non-linear models (Mood 2010).\"\n\n**For Interactions:**\n> \"Given the complexity of interaction effects in non-linear models (Ai and Norton 2003), we present graphical evidence of how the marginal effect of [X] varies with [Z].\"\n",
        "plugins/revision-coordinator/.claude-plugin/plugin.json": "{\r\n  \"name\": \"revision-coordinator\",\r\n  \"version\": \"1.0.0\",\r\n  \"description\": \"Orchestrate manuscript revision by routing feedback to specialized writing skills\",\r\n  \"skills\": \"./skills/\"\r\n}\r\n",
        "plugins/revision-coordinator/skills/revision-coordinator/SKILL.md": "---\r\nname: revision-coordinator\r\ndescription: Orchestrate manuscript revision by routing feedback to specialized writing skills\r\n---\r\n\r\n# Revision Coordinator\r\n\r\nYou help researchers **revise manuscripts** by systematically processing feedback and routing revision tasks to the appropriate specialized writing skills. Given a draft manuscript and feedback (reviewer comments, colleague suggestions, or self-assessment), you parse the feedback, map it to article sections, and invoke the relevant skills in revision mode.\r\n\r\n## What This Skill Does\r\n\r\nThis is an **orchestration skill**it coordinates other skills rather than doing all the writing itself. The workflow:\r\n\r\n1. Parse feedback into discrete, actionable items\r\n2. Map items to article sections (intro, theory, methods, findings, discussion, conclusion)\r\n3. Route each section to the appropriate specialized skill with the specific feedback\r\n4. Track progress and ensure coherence across revisions\r\n5. Verify all feedback has been addressed\r\n\r\n## When to Use This Skill\r\n\r\nUse this skill when you have:\r\n- A **completed draft** (or substantial sections) of a manuscript\r\n- **Feedback** from reviewers, editors, colleagues, or self-assessment\r\n- Sections that were written (or could have been written) using skills like `lit-writeup`, `methods-writer`, `interview-bookends`, or `case-justification`\r\n\r\n## Skill Routing Table\r\n\r\n| Section | Primary Skill | Entry Point for Revision |\r\n|---------|---------------|--------------------------|\r\n| **Introduction** | `interview-bookends` | Phase 1 (intro drafting) or Phase 3 (coherence) |\r\n| **Conclusion** | `interview-bookends` | Phase 2 (conclusion drafting) or Phase 3 (coherence) |\r\n| **Theory/Literature Review** | `lit-writeup` | Phase 4 (turn) or Phase 5 (revision) |\r\n| **Methods** | `methods-writer` | Phase 2 (revision) |\r\n| **Case Justification** | `case-justification` | Phase 2 (revision) |\r\n| **Findings** | General guidance | Direct revision with coordinator |\r\n| **Discussion** | `lit-writeup` techniques | Direct revision with coordinator |\r\n\r\n## What You Need\r\n\r\n1. **The manuscript** (complete draft or relevant sections)\r\n2. **The feedback** (any format: bulleted, prose, structured)\r\n3. **Supporting materials** (if available):\r\n   - Original research question and argument\r\n   - Data/analysis files\r\n   - Prior versions (for tracking changes)\r\n\r\n## Core Principles\r\n\r\n1. **Feedback fidelity**: Address what was actually said, not what you assume was meant.\r\n\r\n2. **Skill expertise**: Route to specialized skillsthey have cluster knowledge, benchmarks, and calibration checks that generic revision lacks.\r\n\r\n3. **Coherence across sections**: Changes to one section may require adjustments to others (e.g., intro changes may break conclusion callbacks).\r\n\r\n4. **Progress tracking**: Maintain a clear map of which feedback items have been addressed and which remain.\r\n\r\n5. **Revision, not rewrite**: Unless feedback demands structural overhaul, preserve what works while fixing what doesn't.\r\n\r\n## Workflow Phases\r\n\r\n### Phase 0: Intake & Feedback Mapping\r\n**Goal**: Understand the manuscript structure and parse feedback into actionable items.\r\n\r\n**Process**:\r\n- Read the full manuscript (or available sections)\r\n- Read the feedback carefully\r\n- Parse feedback into discrete items (one issue per item)\r\n- Categorize each item by type:\r\n  - **Structural**: Architecture, organization, missing sections\r\n  - **Substantive**: Argument, evidence, interpretation\r\n  - **Methodological**: Methods justification, credibility, transparency\r\n  - **Stylistic**: Word count, repetition, clarity\r\n  - **Coherence**: Cross-section alignment, promise-delivery match\r\n- Map each item to the section it addresses\r\n- Identify which skills are relevant for each section\r\n- Create the Revision Task List\r\n\r\n**Output**: `revision-map.md` with parsed feedback and skill assignments.\r\n\r\n> **Pause**: User confirms feedback parsing and skill routing.\r\n\r\n---\r\n\r\n### Phase 1: Diagnostic Assessment\r\n**Goal**: For each section needing revision, determine the appropriate entry point.\r\n\r\n**Process**:\r\n- For each section mapped to a specialized skill:\r\n  - Identify the relevant cluster/pathway (using skill's Phase 0 logic)\r\n  - Assess current draft against cluster benchmarks\r\n  - Determine issue severity (minor calibration vs. structural misalignment)\r\n  - Select the appropriate revision entry point\r\n- For sections without specialized skills (Findings, Discussion):\r\n  - Identify the specific issues\r\n  - Develop targeted revision strategy\r\n\r\n**Output**: Updated `revision-map.md` with diagnostic findings and entry points.\r\n\r\n> **Pause**: User confirms diagnostic assessment and revision strategy.\r\n\r\n---\r\n\r\n### Phase 2: Skill Dispatch\r\n**Goal**: Route each section to the appropriate skill for revision.\r\n\r\n**Dispatch Protocol for Each Section**:\r\n\r\nWhen invoking a sub-skill for revision, provide:\r\n1. **The existing section text** (what needs revision)\r\n2. **The specific feedback items** (what needs to change)\r\n3. **The identified cluster/pathway** (from diagnostic)\r\n4. **The contextual sections** (intro-bookends needs Theory+Findings; lit-writeup needs RQ+argument)\r\n5. **Clear instruction**: \"Revise this section in [Cluster X] style to address: [specific feedback]\"\r\n\r\n**Tracking**: Mark each feedback item as:\r\n- `[ ]` Pending\r\n- `[~]` In progress\r\n- `[x]` Addressed\r\n- `[!]` Needs user decision\r\n\r\n**Output**: Revised sections + updated tracking in `revision-map.md`.\r\n\r\n> **Pause after each major section**: User reviews revisions before proceeding.\r\n\r\n---\r\n\r\n### Phase 3: Integration Review\r\n**Goal**: Ensure revisions are coherent across the manuscript.\r\n\r\n**Cross-Section Checks**:\r\n- **Intro  Findings/Discussion**: Do intro promises match what's delivered?\r\n- **Theory  Findings**: Do theoretical concepts appear in findings analysis?\r\n- **Methods  Findings**: Do methods support the claims made?\r\n- **Intro  Conclusion**: Are there callbacks? Does the conclusion answer the intro's question?\r\n- **Terminology**: Is key language consistent throughout?\r\n- **Citation**: Are sources cited consistently?\r\n\r\n**Coherence Repairs**:\r\n- If intro promises changed, may need to adjust conclusion\r\n- If theory framing changed, may need to revise findings language\r\n- Use `interview-bookends` Phase 3 for intro/conclusion coherence specifically\r\n\r\n**Output**: Coherence assessment + any final adjustments.\r\n\r\n> **Pause**: User confirms cross-section coherence.\r\n\r\n---\r\n\r\n### Phase 4: Verification & Response\r\n**Goal**: Confirm all feedback addressed and prepare revision summary.\r\n\r\n**Process**:\r\n- Review all feedback items against final revised text\r\n- Verify each item is marked `[x]` or has documented reason for `[!]`\r\n- Generate revision summary:\r\n  - What was changed (by section)\r\n  - How each major feedback item was addressed\r\n  - Any items not addressed (with explanation)\r\n- Optionally: Draft response memo for reviewers\r\n\r\n**Output**: `revision-summary.md` with complete accounting.\r\n\r\n---\r\n\r\n## Folder Structure for Revision\r\n\r\n```\r\nproject/\r\n manuscript/\r\n    first-draft.md           # Original manuscript\r\n    feedback.md              # Reviewer/editor feedback\r\n    revised-draft.md         # Output: revised manuscript\r\n revision/\r\n    revision-map.md          # Feedback parsing + skill routing\r\n    diagnostics/             # Cluster assessments per section\r\n    section-revisions/       # Individual section revisions\r\n    revision-summary.md      # Final accounting\r\n```\r\n\r\n## Feedback Parsing Guidelines\r\n\r\n### Parse Into Discrete Items\r\n\r\nTransform this:\r\n> \"The intro is too long and repetitiveyou have two intros. Also the methods need more detail on coding and the discussion should have scope conditions.\"\r\n\r\nInto:\r\n```\r\n1. [Intro] Length: Intro too long\r\n2. [Intro] Structure: Two intros detected (repetition)\r\n3. [Methods] Credibility: More detail on coding needed\r\n4. [Discussion] Scope: Add scope conditions\r\n```\r\n\r\n### Categorize by Type\r\n\r\n| Type | Examples | Typical Skill Response |\r\n|------|----------|----------------------|\r\n| **Structural** | \"Reorganize the theory section\" | Skill Phase 1 (Architecture) |\r\n| **Substantive** | \"Strengthen the argument for X\" | Skill Phase 3-4 (Drafting/Turn) |\r\n| **Methodological** | \"Explain intercoder reliability\" | methods-writer Phase 2 |\r\n| **Stylistic** | \"Cut 500 words from intro\" | Skill calibration checks |\r\n| **Coherence** | \"Intro promises don't match findings\" | interview-bookends Phase 3 |\r\n\r\n### Identify Dependencies\r\n\r\nSome feedback items depend on others:\r\n- If the theoretical framing changes, findings language may need adjustment\r\n- If methods section expands, may need to cut elsewhere for word limits\r\n- If intro cluster changes, conclusion style should match\r\n\r\nNote dependencies in the revision map so sequencing is correct.\r\n\r\n## Invoking Sub-Skills\r\n\r\nUse the Task tool to invoke specialized skills:\r\n\r\n```\r\nTask: Revise Theory Section\r\nsubagent_type: general-purpose\r\nmodel: opus\r\nprompt: |\r\n  Load the lit-writeup skill (read /path/to/lit-writeup/SKILL.md and phases/phase5-revision.md).\r\n\r\n  You are revising an existing Theory section, not writing fresh.\r\n\r\n  EXISTING SECTION:\r\n  [paste current theory section]\r\n\r\n  CLUSTER IDENTIFIED: Gap-Filler (based on Phase 0 diagnostic)\r\n\r\n  FEEDBACK TO ADDRESS:\r\n  1. [specific item 1]\r\n  2. [specific item 2]\r\n\r\n  CONTEXT:\r\n  - Research question: [RQ]\r\n  - Main argument: [argument]\r\n\r\n  Run Phase 5 (Revision) calibration checks and revise the section to address the feedback while maintaining Gap-Filler cluster characteristics.\r\n```\r\n\r\n## Handling Sections Without Dedicated Skills\r\n\r\n### Findings Sections\r\n\r\nNo dedicated skill exists. For Findings revision:\r\n- Check that findings are organized by theme/concept (not by interview or chronology)\r\n- Verify each claim is supported by evidence (quotes, counts)\r\n- Ensure theoretical concepts from Theory section appear\r\n- Check word balance across subsections\r\n- Apply general calibration: clear topic sentences, evidence-interpretation rhythm\r\n\r\n### Discussion Sections\r\n\r\nPartial coverage via lit-writeup techniques. For Discussion revision:\r\n- Check four standard elements: summary, implications, limitations, future directions\r\n- Verify scope conditions are explicit\r\n- Ensure limitations are honest but not self-undermining\r\n- Check that implications connect to Theory section's literatures\r\n\r\n## Common Revision Scenarios\r\n\r\n### Scenario: \"Two Intros\" Problem\r\n**Feedback**: \"You have two introductions\"\r\n**Diagnosis**: Often happens when there's a general intro + a section called \"Background\" or \"Literature Review\" that re-introduces the topic.\r\n**Resolution**:\r\n1. Keep ONE intro (usually the first)\r\n2. Convert the second into a proper Theory section (use lit-writeup cluster guidance)\r\n3. Run interview-bookends Phase 3 for coherence check\r\n\r\n### Scenario: Methods Credibility Gap\r\n**Feedback**: \"Need more detail on coding/reliability\"\r\n**Diagnosis**: Pathway mismatchprobably using Efficient (600-900w) when Standard or Detailed needed.\r\n**Resolution**:\r\n1. Re-run methods-writer Phase 0 to confirm pathway\r\n2. If pathway should change, redraft with new word allocation\r\n3. If pathway correct, add specific missing components (coding process, saturation, positionality)\r\n\r\n### Scenario: Weak Turn in Theory\r\n**Feedback**: \"Gap isn't clear\" or \"Contribution feels vague\"\r\n**Diagnosis**: Turn (gap  contribution pivot) isn't sharp enough.\r\n**Resolution**:\r\n1. Use lit-writeup Phase 4 (Turn) specifically\r\n2. Ensure turn appears at subsection transition, not buried\r\n3. Check that \"what we don't know\" is specific, not generic\r\n\r\n### Scenario: Promise-Delivery Mismatch\r\n**Feedback**: \"Intro promises X but findings deliver Y\"\r\n**Diagnosis**: Coherence failure between intro and body.\r\n**Resolution**:\r\n1. Decide which is right: the promise or the delivery\r\n2. If delivery is right, revise intro to match (interview-bookends Phase 1)\r\n3. If promise is right, this is a substantive issue requiring findings revision\r\n4. Run interview-bookends Phase 3 for coherence verification\r\n\r\n## Key Reminders\r\n\r\n- **Don't over-revise**: Fix what feedback identifies; preserve what works.\r\n- **Track everything**: The revision map is your accountability document.\r\n- **Sequence matters**: Do structural changes before calibration; do content before style.\r\n- **User decisions**: When feedback is ambiguous or conflicting, flag for user input.\r\n- **Skills have benchmarks**: Use the calibration checks built into each skilldon't guess.\r\n- **Coherence is a system property**: Changes to one section affect others.\r\n\r\n## Starting the Process\r\n\r\nWhen the user is ready to begin:\r\n\r\n1. **Ask for the manuscript**:\r\n   > \"Please share your manuscript (or the sections you want revised). I need to see the current state.\"\r\n\r\n2. **Ask for the feedback**:\r\n   > \"Please share the feedback you've received. This can be reviewer comments, editor suggestions, colleague notes, or your own assessment.\"\r\n\r\n3. **Ask about priorities**:\r\n   > \"Is there anything you specifically agree or disagree with in the feedback? Any constraints (word limits, sections that can't change, etc.)?\"\r\n\r\n4. **Proceed with Phase 0** to parse and map the feedback.\r\n",
        "plugins/revision-coordinator/skills/revision-coordinator/phases/phase0-intake.md": "# Phase 0: Intake & Feedback Mapping\r\n\r\n## Why This Phase Matters\r\n\r\nRevision without a map is revision by wandering. Before touching any text, you need a clear picture of:\r\n- What the manuscript currently contains\r\n- What the feedback actually says (parsed, not paraphrased)\r\n- Which sections need work\r\n- Which skills should handle each section\r\n\r\nThis phase transforms vague \"I got feedback\" into actionable \"here are 12 discrete items routed to 4 skills.\"\r\n\r\n## Your Tasks\r\n\r\n### Task 1: Read the Manuscript\r\n\r\nRead the full manuscript (or all available sections). As you read, note:\r\n\r\n1. **Section inventory**: What sections exist? (Intro, Theory/Lit Review, Methods, Case, Findings, Discussion, Conclusion)\r\n2. **Section lengths**: Approximate word counts per section\r\n3. **Section quality signals**: What seems strong? What seems weak even before feedback?\r\n4. **Cross-references**: How do sections connect? Does intro set up what findings deliver?\r\n\r\nCreate a section inventory table:\r\n\r\n```markdown\r\n## Section Inventory\r\n\r\n| Section | Present | ~Words | Notes |\r\n|---------|---------|--------|-------|\r\n| Abstract | Yes/No | | |\r\n| Introduction | Yes/No | | |\r\n| Theory/Literature | Yes/No | | |\r\n| Case Justification | Yes/No | | |\r\n| Methods | Yes/No | | |\r\n| Findings | Yes/No | | |\r\n| Discussion | Yes/No | | |\r\n| Conclusion | Yes/No | | |\r\n```\r\n\r\n### Task 2: Read the Feedback Carefully\r\n\r\nRead ALL feedback before parsing. Note:\r\n- Who is giving the feedback? (Reviewer 1, Reviewer 2, editor, colleague)\r\n- What's the overall tone? (Supportive with suggestions? Critical with demands?)\r\n- What are the BIG issues vs. small issues?\r\n- Is anything contradictory? (Reviewer 1 says expand X; Reviewer 2 says cut X)\r\n\r\n### Task 3: Parse Feedback Into Discrete Items\r\n\r\nTransform prose feedback into numbered, discrete items. Each item should be:\r\n- **One issue** (not bundled)\r\n- **Mapped to a section** (or \"Cross-section\" if it spans multiple)\r\n- **Categorized by type** (Structural, Substantive, Methodological, Stylistic, Coherence)\r\n- **Attributed to source** (R1, R2, Editor, Self)\r\n\r\n**Parsing example**:\r\n\r\nOriginal feedback:\r\n> \"The introduction is too long and the literature review feels repetitivelike you're introducing the topic twice. Also, the methods section needs more detail on how you coded the interviews, and I'd like to see scope conditions in the discussion.\"\r\n\r\nParsed:\r\n```markdown\r\n1. [Intro][Stylistic][R1] Introduction is too long\r\n2. [Intro + Theory][Structural][R1] Two intros detectedlit review repeats intro framing\r\n3. [Methods][Methodological][R1] More detail needed on interview coding process\r\n4. [Discussion][Substantive][R1] Add scope conditions\r\n```\r\n\r\n### Task 4: Categorize Each Item\r\n\r\nUse these categories:\r\n\r\n| Category | What It Means | Typical Resolution |\r\n|----------|---------------|-------------------|\r\n| **Structural** | Organization, architecture, missing/extra sections | Skill architecture phases |\r\n| **Substantive** | Argument strength, evidence, interpretation | Skill drafting phases |\r\n| **Methodological** | Methods justification, credibility, transparency | methods-writer |\r\n| **Stylistic** | Word count, clarity, repetition, prose quality | Skill calibration |\r\n| **Coherence** | Cross-section alignment, promise-delivery | interview-bookends Phase 3 |\r\n\r\n### Task 5: Route to Skills\r\n\r\nFor each section with feedback items, identify the primary skill:\r\n\r\n| Section | Primary Skill | Backup Approach |\r\n|---------|---------------|-----------------|\r\n| Introduction | `interview-bookends` | Direct revision if minimal |\r\n| Conclusion | `interview-bookends` | Direct revision if minimal |\r\n| Theory/Literature | `lit-writeup` | Direct revision if < 3 items |\r\n| Methods | `methods-writer` | Direct revision if < 3 items |\r\n| Case Justification | `case-justification` | Direct revision if < 3 items |\r\n| Findings | No dedicated skill | Direct revision with guidance |\r\n| Discussion | `lit-writeup` techniques | Direct revision with guidance |\r\n\r\n**Routing threshold**: If a section has 3+ substantive/structural items, route to the specialized skill. If it has only 1-2 stylistic items, may handle directly.\r\n\r\n### Task 6: Identify Dependencies\r\n\r\nSome feedback items depend on others:\r\n\r\n- **Theory  Findings**: If theory framing changes, findings may need terminology updates\r\n- **Intro  Conclusion**: If intro promises change, conclusion callbacks need adjustment\r\n- **Methods  Findings**: If methods expand to include new credibility measures, findings may reference them\r\n- **Case  Methods**: Case justification may overlap with site description in Methods\r\n\r\nNote dependencies so you sequence revisions correctly.\r\n\r\n### Task 7: Create the Revision Map\r\n\r\nCreate `revision-map.md` with this structure:\r\n\r\n```markdown\r\n# Revision Map\r\n\r\n## Manuscript: [Title]\r\n## Date: [Date]\r\n## Feedback Sources: [R1, R2, Editor, etc.]\r\n\r\n---\r\n\r\n## Section Inventory\r\n\r\n| Section | Present | ~Words | Skill | Items |\r\n|---------|---------|--------|-------|-------|\r\n| Introduction | Yes | 800 | interview-bookends | 2 |\r\n| Theory | Yes | 1500 | lit-writeup | 3 |\r\n| Methods | Yes | 900 | methods-writer | 2 |\r\n| Findings | Yes | 4000 | Direct | 1 |\r\n| Discussion | Yes | 1200 | Direct | 2 |\r\n| Conclusion | Yes | 600 | interview-bookends | 1 |\r\n\r\n---\r\n\r\n## Parsed Feedback Items\r\n\r\n### Introduction (2 items)  interview-bookends\r\n| ID | Type | Source | Item | Status |\r\n|----|------|--------|------|--------|\r\n| I-1 | Stylistic | R1 | Too long | [ ] |\r\n| I-2 | Structural | R1 | Two intros detected | [ ] |\r\n\r\n### Theory (3 items)  lit-writeup\r\n| ID | Type | Source | Item | Status |\r\n|----|------|--------|------|--------|\r\n| T-1 | Structural | R1 | Compress by 30% | [ ] |\r\n| T-2 | Substantive | R2 | Strengthen turn | [ ] |\r\n| T-3 | Stylistic | Editor | Define \"organizational absorption\" | [ ] |\r\n\r\n[Continue for each section...]\r\n\r\n---\r\n\r\n## Dependencies\r\n\r\n- I-2 depends on T-1 (if theory section changes, intro may need adjustment)\r\n- Conclusion callbacks depend on intro (sequence: intro  conclusion)\r\n\r\n---\r\n\r\n## Revision Sequence (Proposed)\r\n\r\n1. Theory section (T-1, T-2, T-3)  foundation changes\r\n2. Introduction (I-1, I-2)  after theory settled\r\n3. Methods (M-1, M-2)  independent\r\n4. Findings (F-1)  may need terminology updates from theory\r\n5. Discussion (D-1, D-2)  independent\r\n6. Conclusion (C-1)  last, after intro finalized\r\n\r\n---\r\n\r\n## Questions for User\r\n\r\n- [List any ambiguous items or decisions that need user input]\r\n```\r\n\r\n## Output Files\r\n\r\nCreate in `revision/` directory:\r\n- `revision-map.md`  the master tracking document\r\n\r\n## Guiding Principles\r\n\r\n1. **Fidelity to feedback**: Parse what was said, not what you think was meant. If unclear, note it as a question.\r\n\r\n2. **One item, one row**: Don't bundle. \"Too long and repetitive\" is two items.\r\n\r\n3. **Everything gets tracked**: Even items you disagree with get logged (mark them for user discussion).\r\n\r\n4. **Dependencies are critical**: Wrong sequencing wastes work. If theory changes, don't revise conclusions first.\r\n\r\n5. **User owns decisions**: Flag contradictions and ambiguities rather than resolving them yourself.\r\n\r\n## When You're Done\r\n\r\nSummarize for the orchestrator:\r\n- Total feedback items parsed\r\n- Section-by-skill routing\r\n- Proposed revision sequence\r\n- Questions requiring user input\r\n\r\n**Example summary**:\r\n> Parsed 11 feedback items across 6 sections. Routing: Introduction/Conclusion  interview-bookends (3 items), Theory  lit-writeup (3 items), Methods  methods-writer (2 items), Findings/Discussion  direct (3 items). Proposed sequence: Theory  Intro  Methods  Findings  Discussion  Conclusion. One question for user: R1 says expand methods, R2 says paper is too longhow to reconcile?\r\n\r\nThen pause for user confirmation before proceeding to Phase 1.\r\n",
        "plugins/revision-coordinator/skills/revision-coordinator/phases/phase1-diagnostic.md": "# Phase 1: Diagnostic Assessment\r\n\r\n## Why This Phase Matters\r\n\r\nBefore revising, you need to know *what kind* of revision each section needs. A section might need:\r\n- Complete restructuring (wrong cluster/pathway)\r\n- Targeted fixes (right structure, specific gaps)\r\n- Calibration only (right approach, wrong execution)\r\n\r\nThis phase runs each section through its skill's diagnostic lens to determine the entry point for revision.\r\n\r\n## Your Tasks\r\n\r\n### Task 1: Diagnostic Protocol by Skill\r\n\r\nFor each section routed to a specialized skill, run the appropriate diagnostic:\r\n\r\n---\r\n\r\n#### Interview-Bookends Diagnostic (Intro/Conclusion)\r\n\r\n**Cluster identification**  Which of the 5 clusters does this intro/conclusion match?\r\n\r\n| Cluster | Frequency | Key Signal |\r\n|---------|-----------|------------|\r\n| Gap-Filler | 38.8% | Opens with puzzle/contradiction, explicit \"gap\" language |\r\n| Theory-Extension | 22.5% | Opens with theoretical concept, aims to extend/refine |\r\n| Concept-Building | 15% | Opens with phenomenon, aims to develop new concept |\r\n| Synthesis | 17.5% | Opens with multiple literatures, aims to integrate |\r\n| Problem-Driven | 15% | Opens with social problem, aims at practical implications |\r\n\r\n**Benchmark check**  Does the intro/conclusion meet cluster targets?\r\n\r\n| Metric | Target |\r\n|--------|--------|\r\n| Intro:Conclusion ratio | ~1.67x (intro longer) |\r\n| Opening move | Matches cluster |\r\n| Callbacks in conclusion | Required (100% of benchmarks have them) |\r\n| Contribution clarity | Explicit claim |\r\n\r\n**Issue classification**:\r\n- **Cluster mismatch**: Intro written in Gap-Filler style but paper is Theory-Extension  restructure\r\n- **Calibration issue**: Right cluster, wrong execution (e.g., missing callback)  targeted fix\r\n- **Coherence issue**: Intro and conclusion don't match  Phase 3 coordination\r\n\r\n---\r\n\r\n#### Lit-Writeup Diagnostic (Theory Section)\r\n\r\n**Cluster identification**  Which of the 5 clusters?\r\n\r\n| Cluster | Frequency | Key Signal |\r\n|---------|-----------|------------|\r\n| Gap-Filler | 27.5% | Identifies absence in literature |\r\n| Theory-Extender | 22.5% | Refines/extends existing theory |\r\n| Concept-Builder | 15% | Develops new concept from phenomenon |\r\n| Synthesis | 18.8% | Integrates multiple literatures |\r\n| Problem-Driven | 16.3% | Addresses debate/applied problem |\r\n\r\n**Benchmark check**:\r\n\r\n| Metric | Target |\r\n|--------|--------|\r\n| Word count | 1145-1744 (median 1525) |\r\n| Citation density | ~24 per 1000 words |\r\n| Subsections | 2-4 typical |\r\n| Turn clarity | Gap  contribution pivot is explicit |\r\n| Turn position | Near end of section, not buried |\r\n\r\n**Issue classification**:\r\n- **Architecture issue**: Wrong number of subsections, wrong arc  Phase 1 restart\r\n- **Turn weakness**: Gap isn't clear or contribution is vague  Phase 4 focus\r\n- **Calibration issue**: Right structure, prose needs polish  Phase 5 only\r\n\r\n---\r\n\r\n#### Methods-Writer Diagnostic (Methods Section)\r\n\r\n**Pathway identification**  Which of the 3 pathways?\r\n\r\n| Pathway | Frequency | When Used |\r\n|---------|-----------|-----------|\r\n| Efficient | 10% | Simple design, space-constrained, low vulnerability |\r\n| Standard | 61% | Typical qualitative study |\r\n| Detailed | 23% | High vulnerability, novel methods, replication concern |\r\n\r\nSelection criteria:\r\n- Vulnerable population?  Detailed\r\n- Novel/unusual methods?  Detailed\r\n- Severe space constraints?  Efficient\r\n- Otherwise  Standard\r\n\r\n**Benchmark check**:\r\n\r\n| Metric | Efficient | Standard | Detailed |\r\n|--------|-----------|----------|----------|\r\n| Word count | 600-900 | 1200-1500 | 2000-3000 |\r\n| Table | Rare | Optional | Common |\r\n| Positionality | Brief | If triggered | Extended |\r\n| Saturation | Stated | Stated + numbers | Stated + defense |\r\n\r\n**Issue classification**:\r\n- **Pathway mismatch**: Using Efficient when Detailed is appropriate  redraft\r\n- **Component gap**: Missing required element (e.g., no saturation statement)  targeted addition\r\n- **Calibration issue**: Right pathway, word count off  trim/expand\r\n\r\n---\r\n\r\n#### Case-Justification Diagnostic\r\n\r\n**Cluster identification**  Which of the 5 clusters?\r\n\r\n| Cluster | Frequency | When Used |\r\n|---------|-----------|-----------|\r\n| Minimal | 16% | Familiar, single site, straightforward |\r\n| Standard | 34% | Typical qualitative case |\r\n| Deep Historical | 16% | Historical significance central |\r\n| Comparative | 22% | Multiple sites |\r\n| Policy-Driven | 13% | Policy precedent matters |\r\n\r\n**Position check**:\r\n- Policy-Driven cluster  BEFORE theory section\r\n- All other clusters  AFTER theory section\r\n\r\n**Benchmark check**:\r\n\r\n| Cluster | Word Count | Opening Move |\r\n|---------|------------|--------------|\r\n| Minimal | 300-500 | Site intro  access  brief justification |\r\n| Standard | 700-1000 | Phenomenon-site link (50%) |\r\n| Deep Historical | 1500-2500 | Historical arc |\r\n| Comparative | 1000-1500 | Comparison logic |\r\n| Policy-Driven | 650-900 | Policy context |\r\n\r\n**Issue classification**:\r\n- **Cluster mismatch**: Wrong cluster for case type  restructure\r\n- **Position error**: Policy-Driven placed after theory  move\r\n- **Component gap**: Missing required element  targeted addition\r\n\r\n---\r\n\r\n### Task 2: Run Diagnostics for Each Section\r\n\r\nFor each section with feedback items, document:\r\n\r\n```markdown\r\n## Diagnostic: [Section Name]\r\n\r\n### Skill: [skill name]\r\n\r\n### Current State\r\n- Word count: X\r\n- Subsections: [list]\r\n- Opening move: [describe]\r\n\r\n### Cluster/Pathway Identified\r\n[Cluster name] because [reasons]\r\n\r\n### Benchmark Comparison\r\n| Metric | Target | Actual | Gap |\r\n|--------|--------|--------|-----|\r\n| Word count | X | Y | +/- Z |\r\n| [other metrics] | | | |\r\n\r\n### Issues Detected\r\n1. [Issue 1  from feedback + diagnostic]\r\n2. [Issue 2]\r\n\r\n### Recommended Entry Point\r\n- [ ] Complete restructure (Phase X of skill)\r\n- [ ] Targeted revision (Phase Y of skill)\r\n- [ ] Calibration only (Phase Z of skill)\r\n- [ ] Direct revision (no skill needed)\r\n\r\n### Notes\r\n[Any additional observations]\r\n```\r\n\r\n### Task 3: Synthesize Across Sections\r\n\r\nAfter running diagnostics for all sections, look for patterns:\r\n\r\n1. **Cluster coherence**: Do intro, theory, and conclusion share the same cluster type? (They should!)\r\n   - If intro is Gap-Filler but theory is Theory-Extender, there's a framing mismatch\r\n\r\n2. **Severity distribution**: How many sections need restructuring vs. targeted fixes?\r\n   - If most sections need restructuring, this is closer to a rewrite than a revision\r\n\r\n3. **Dependency implications**: Do any diagnostics reveal new dependencies?\r\n   - E.g., theory cluster changed  intro and conclusion need to match\r\n\r\n### Task 4: Update the Revision Map\r\n\r\nAdd diagnostic findings to `revision-map.md`:\r\n\r\n```markdown\r\n---\r\n\r\n## Phase 1 Diagnostics\r\n\r\n### Section: Introduction\r\n- **Cluster**: Gap-Filler\r\n- **Key issues**: Two intros (structural), too long (calibration)\r\n- **Entry point**: interview-bookends Phase 1 (redraft intro)\r\n\r\n### Section: Theory\r\n- **Cluster**: Gap-Filler (matches intro)\r\n- **Key issues**: Repetitive with intro, turn is weak\r\n- **Entry point**: lit-writeup Phase 5 (revision) with focus on turn\r\n\r\n### Section: Methods\r\n- **Pathway**: Standard (should be Detailed given vulnerability)\r\n- **Key issues**: Missing credibility detail, pathway mismatch\r\n- **Entry point**: methods-writer Phase 1 (redraft with Detailed pathway)\r\n\r\n[Continue for each section...]\r\n\r\n---\r\n\r\n## Coherence Check\r\n\r\n- Intro cluster: Gap-Filler \r\n- Theory cluster: Gap-Filler \r\n- Conclusion cluster: [to check after theory revised]\r\n- Cross-cluster alignment: OK / MISMATCH\r\n\r\n---\r\n\r\n## Revised Sequence\r\n\r\nBased on diagnostics:\r\n1. Theory (Phase 5)  foundation, needs turn strengthening\r\n2. Methods (Phase 1)  pathway change needed\r\n3. Introduction (Phase 1)  restructure to eliminate repetition\r\n4. [etc.]\r\n```\r\n\r\n## Issue Severity Classification\r\n\r\n| Severity | Definition | Action |\r\n|----------|------------|--------|\r\n| **Critical** | Wrong cluster/pathway, structural misalignment | Full skill workflow |\r\n| **Major** | Right structure, significant gaps | Targeted phases (Turn, Architecture) |\r\n| **Minor** | Calibration issues only | Final phase or direct revision |\r\n\r\n## Common Diagnostic Findings\r\n\r\n### \"Two Intros\" Pattern\r\n**Symptom**: Introduction and first section of lit review cover similar ground\r\n**Diagnosis**: Usually a structural issueno clear division between intro (puzzle + roadmap) and theory (literature engagement)\r\n**Resolution**: Keep ONE intro; convert the other to proper theory section with lit-writeup guidance\r\n\r\n### \"Weak Turn\" Pattern\r\n**Symptom**: Feedback says gap is unclear or contribution is vague\r\n**Diagnosis**: Turn exists but is buried, generic, or too brief\r\n**Resolution**: lit-writeup Phase 4 focusmake turn explicit, position it correctly, sharpen language\r\n\r\n### \"Methods Credibility Gap\" Pattern\r\n**Symptom**: Feedback asks for more detail on coding, reliability, or process\r\n**Diagnosis**: Often a pathway mismatchusing Efficient or Standard when Detailed is warranted\r\n**Resolution**: methods-writer rediagnosis; may need pathway upgrade\r\n\r\n### \"Promise-Delivery Mismatch\" Pattern\r\n**Symptom**: Feedback says intro promises don't match findings\r\n**Diagnosis**: Coherence issueintro may have been written before findings were finalized\r\n**Resolution**: Decide which is authoritative (usually findings); revise intro to match; check conclusion callbacks\r\n\r\n## Output Files\r\n\r\nUpdate in `revision/` directory:\r\n- `revision-map.md`  add Phase 1 diagnostics section\r\n\r\nOptionally create:\r\n- `diagnostics/intro-diagnostic.md`\r\n- `diagnostics/theory-diagnostic.md`\r\n- etc.\r\n\r\n## When You're Done\r\n\r\nSummarize for the orchestrator:\r\n- Diagnostic findings per section\r\n- Entry points recommended\r\n- Any cluster/coherence mismatches discovered\r\n- Revised sequencing if needed\r\n\r\n**Example summary**:\r\n> Diagnostics complete. Theory section is Gap-Filler cluster, word count high (+200), turn needs strengthening  lit-writeup Phase 5. Methods currently Standard but should be Detailed given vulnerable population  methods-writer Phase 1 for pathway change. Introduction has two-intros problem  interview-bookends Phase 1 after theory settled. No cluster mismatches detected. Revised sequence: Theory  Methods  Intro  Conclusion.\r\n\r\nThen pause for user confirmation before proceeding to Phase 2.\r\n",
        "plugins/revision-coordinator/skills/revision-coordinator/phases/phase2-dispatch.md": "# Phase 2: Skill Dispatch\r\n\r\n## Why This Phase Matters\r\n\r\nThis is where revision actually happens. Based on the diagnostic assessment, you now invoke specialized skills to revise each section. The key is providing each skill with the right context so it can do its job effectively.\r\n\r\n## Dispatch Principles\r\n\r\n1. **One section at a time**: Complete each section before moving to the next (unless dependencies require otherwise)\r\n\r\n2. **Full context provision**: Skills need more than just \"fix this\"they need the existing text, the specific feedback, and the cluster/pathway information\r\n\r\n3. **Track as you go**: Update the revision map after each section is completed\r\n\r\n4. **User checkpoints**: After each major section, pause for user review\r\n\r\n## Dispatch Protocols by Skill\r\n\r\n### Interview-Bookends Dispatch (Introduction or Conclusion)\r\n\r\n**What to provide the skill**:\r\n\r\n```markdown\r\n## Revision Request: [Introduction/Conclusion]\r\n\r\n### Skill: interview-bookends\r\n### Phase: [1 for intro, 2 for conclusion, 3 for coherence check]\r\n\r\n### Cluster Identified\r\n[Gap-Filler / Theory-Extension / Concept-Building / Synthesis / Problem-Driven]\r\n\r\n### Current Section\r\n[Paste the current introduction or conclusion]\r\n\r\n### Feedback Items to Address\r\n1. [ID: I-1] [Specific feedback item]\r\n2. [ID: I-2] [Specific feedback item]\r\n\r\n### Required Context\r\n**Theory section summary** (for intro) or **full theory section** (for conclusion callbacks):\r\n[Paste or summarize]\r\n\r\n**Findings section summary**:\r\n[Key findings that intro should promise and conclusion should reference]\r\n\r\n**Current conclusion** (if revising intro, for coherence):\r\n[Paste current conclusion so intro can set up callbacks]\r\n\r\n### Constraints\r\n- Word limit: [if any]\r\n- Elements that must be preserved: [if any]\r\n- Journal requirements: [if any]\r\n\r\n### Instructions\r\nRevise this [introduction/conclusion] to address the feedback items while maintaining [Cluster] style. Ensure:\r\n- Opening move matches cluster type\r\n- [For intro] Promises align with what findings deliver\r\n- [For conclusion] Callbacks to intro language are present\r\n- Contribution is explicit and specific\r\n```\r\n\r\n---\r\n\r\n### Lit-Writeup Dispatch (Theory Section)\r\n\r\n**What to provide the skill**:\r\n\r\n```markdown\r\n## Revision Request: Theory Section\r\n\r\n### Skill: lit-writeup\r\n### Phase: [4 for turn focus, 5 for full revision]\r\n\r\n### Cluster Identified\r\n[Gap-Filler / Theory-Extender / Concept-Builder / Synthesis / Problem-Driven]\r\n\r\n### Current Section\r\n[Paste the current theory/literature section]\r\n\r\n### Feedback Items to Address\r\n1. [ID: T-1] [Specific feedback item]\r\n2. [ID: T-2] [Specific feedback item]\r\n\r\n### Required Context\r\n**Research question**:\r\n[The study's central question]\r\n\r\n**Main argument**:\r\n[The paper's core claim/contribution]\r\n\r\n**Key literatures engaged**:\r\n1. [Literature 1]\r\n2. [Literature 2]\r\n3. [Literature 3 if applicable]\r\n\r\n### Calibration Targets\r\n| Metric | Target | Current |\r\n|--------|--------|---------|\r\n| Word count | 1145-1744 | [current] |\r\n| Citation density | ~24/1000 words | [current] |\r\n| Subsections | 2-4 | [current] |\r\n\r\n### Instructions\r\nRevise this theory section to address the feedback items while maintaining [Cluster] style. Ensure:\r\n- Turn (gap  contribution) is explicit and well-positioned\r\n- Each subsection has clear function in the arc\r\n- [Cluster-specific requirement, e.g., \"gap is framed as absence, not criticism\"]\r\n- Calibration targets are met\r\n```\r\n\r\n---\r\n\r\n### Methods-Writer Dispatch\r\n\r\n**What to provide the skill**:\r\n\r\n```markdown\r\n## Revision Request: Methods Section\r\n\r\n### Skill: methods-writer\r\n### Phase: [1 for pathway change, 2 for revision within pathway]\r\n\r\n### Pathway Identified\r\n[Efficient / Standard / Detailed]\r\n\r\n### Current Section\r\n[Paste the current methods section]\r\n\r\n### Feedback Items to Address\r\n1. [ID: M-1] [Specific feedback item]\r\n2. [ID: M-2] [Specific feedback item]\r\n\r\n### Study Details\r\n- Sample size: [N]\r\n- Population: [description]\r\n- Recruitment: [approach]\r\n- Interview format: [semi-structured, etc.] / Duration: [range]\r\n- Analysis approach: [method]\r\n\r\n### Pathway Triggers\r\n- Vulnerable population? [Yes/No  if Yes, triggers Detailed]\r\n- Novel methods? [Yes/No  if Yes, triggers Detailed]\r\n- Space constraints? [Yes/No  if Yes, may warrant Efficient]\r\n\r\n### Calibration Targets\r\n| Metric | Target (for pathway) |\r\n|--------|---------------------|\r\n| Word count | [pathway target] |\r\n| Table | [expected?] |\r\n| Positionality | [triggered?] |\r\n\r\n### Instructions\r\nRevise this methods section following the [Pathway] template. Address feedback items while ensuring:\r\n- All required components are present (sampling, access, analysis, limitations)\r\n- Saturation/sufficiency is addressed\r\n- [If positionality triggered] Positionality statement included\r\n- Word count is within pathway range\r\n```\r\n\r\n---\r\n\r\n### Case-Justification Dispatch\r\n\r\n**What to provide the skill**:\r\n\r\n```markdown\r\n## Revision Request: Case Justification\r\n\r\n### Skill: case-justification\r\n### Phase: [1 for cluster change, 2 for revision within cluster]\r\n\r\n### Cluster Identified\r\n[Minimal / Standard / Deep Historical / Comparative / Policy-Driven]\r\n\r\n### Position Check\r\n- Current position: [Before/After theory section]\r\n- Required position: [Before/After theory section]\r\n- Position change needed? [Yes/No]\r\n\r\n### Current Section\r\n[Paste the current case justification, if exists]\r\n\r\n### Feedback Items to Address\r\n1. [ID: C-1] [Specific feedback item]\r\n2. [ID: C-2] [Specific feedback item]\r\n\r\n### Case Details\r\n- Site(s): [name/description]\r\n- Population: [description]\r\n- Key contextual features: [list]\r\n- Relationship to theory: [why this case for this argument?]\r\n\r\n### Calibration Targets\r\n| Metric | Target (for cluster) |\r\n|--------|---------------------|\r\n| Word count | [cluster target] |\r\n| Opening move | [cluster expectation] |\r\n\r\n### Instructions\r\nRevise this case justification following the [Cluster] template. Address feedback items while ensuring:\r\n- Position relative to theory section is correct\r\n- Opening move matches cluster type\r\n- Justification connects case features to theoretical leverage\r\n- Word count is within cluster range\r\n```\r\n\r\n---\r\n\r\n### Direct Revision Dispatch (Findings, Discussion)\r\n\r\nFor sections without dedicated skills, provide structured guidance:\r\n\r\n```markdown\r\n## Revision Request: [Findings/Discussion]\r\n\r\n### Approach: Direct revision with guidance\r\n\r\n### Current Section\r\n[Paste the current section]\r\n\r\n### Feedback Items to Address\r\n1. [ID: F-1] [Specific feedback item]\r\n2. [ID: F-2] [Specific feedback item]\r\n\r\n### Revision Guidelines\r\n\r\n**For Findings**:\r\n- Organization: By theme/concept (not by interview or chronology)\r\n- Evidence: Each claim supported by quote or count\r\n- Interpretation: Evidence  interpretation rhythm maintained\r\n- Theoretical language: Concepts from theory section appear\r\n- Balance: Subsections roughly proportional (unless justified)\r\n\r\n**For Discussion**:\r\n- Summary: Brief recap of main findings (not repetition)\r\n- Implications: How findings advance theory/literature\r\n- Scope conditions: Explicit boundaries on claims\r\n- Limitations: Honest but not self-undermining\r\n- Future directions: 2-3 specific suggestions (optional)\r\n\r\n### Context\r\n**From theory section** (concepts that should appear):\r\n[List key theoretical concepts]\r\n\r\n**From findings** (claims discussion should interpret):\r\n[List main findings]\r\n\r\n### Instructions\r\nRevise this section to address the feedback items. Maintain:\r\n- Clear topic sentences for each paragraph\r\n- Evidence-interpretation rhythm\r\n- Appropriate hedging for claims\r\n```\r\n\r\n## Dispatch Sequencing\r\n\r\nFollow the sequence established in Phase 1, respecting dependencies:\r\n\r\n### Typical Sequence\r\n\r\n1. **Theory section first** (if changing)  This is the foundation\r\n2. **Methods** (usually independent)  Can often run parallel to theory\r\n3. **Case justification** (if needed)  May depend on theory framing\r\n4. **Findings** (if changing)  May need terminology updates from theory\r\n5. **Introduction**  After theory and findings settled\r\n6. **Discussion**  After findings settled\r\n7. **Conclusion**  Last, after intro finalized\r\n\r\n### Dependency Handling\r\n\r\nIf dependencies exist:\r\n- Complete the upstream section first\r\n- Note what changed that affects downstream sections\r\n- Provide changed context to downstream skills\r\n\r\n**Example**: If theory framing changed from Gap-Filler to Theory-Extension:\r\n1. Complete theory revision\r\n2. Before dispatching intro, note: \"Theory cluster changed to Theory-Extension; intro should match\"\r\n3. When dispatching conclusion, include revised intro for callback alignment\r\n\r\n## Tracking Progress\r\n\r\nAfter each dispatch, update `revision-map.md`:\r\n\r\n```markdown\r\n### Dispatch Log\r\n\r\n| Time | Section | Skill/Phase | Items Addressed | Status |\r\n|------|---------|-------------|-----------------|--------|\r\n| [timestamp] | Theory | lit-writeup/P5 | T-1, T-2, T-3 | Complete |\r\n| [timestamp] | Methods | methods-writer/P1 | M-1, M-2 | Complete |\r\n| [timestamp] | Introduction | interview-bookends/P1 | I-1, I-2 | In progress |\r\n\r\n### Item Status Update\r\n\r\n| ID | Original Status | Current Status | Notes |\r\n|----|-----------------|----------------|-------|\r\n| T-1 | [ ] | [x] | Word count reduced by 300 |\r\n| T-2 | [ ] | [x] | Turn sharpened, moved to end of section |\r\n| T-3 | [ ] | [x] | \"Organizational absorption\" defined in para 2 |\r\n| M-1 | [ ] | [x] | Coding process expanded |\r\n| M-2 | [ ] | [x] | Pathway changed to Detailed |\r\n| I-1 | [ ] | [~] | In progress |\r\n| I-2 | [ ] | [~] | In progress |\r\n```\r\n\r\n## User Checkpoints\r\n\r\nAfter completing each major section, pause:\r\n\r\n> **Section Complete: [Theory Section]**\r\n>\r\n> Changes made:\r\n> - [Summary of changes]\r\n>\r\n> Feedback items addressed:\r\n> - T-1: [how addressed]\r\n> - T-2: [how addressed]\r\n>\r\n> Next section: [Introduction]\r\n>\r\n> Ready to proceed?\r\n\r\n## Handling Problems\r\n\r\n### Skill Can't Fully Address Feedback\r\n\r\nIf a skill's revision doesn't fully address a feedback item:\r\n- Document what was addressed vs. what remains\r\n- Flag item as `[~]` (partially addressed) or `[!]` (needs user decision)\r\n- Note the gap for Phase 4 verification\r\n\r\n### Contradictory Feedback\r\n\r\nIf R1 and R2 give contradictory feedback:\r\n- Do NOT resolve this yourself\r\n- Flag as `[!]` with both options documented\r\n- Ask user which direction to take\r\n- Proceed with user's choice\r\n\r\n### Scope Creep\r\n\r\nIf addressing feedback reveals additional issues:\r\n- Add new items to the revision map\r\n- Flag as \"emerged during revision\"\r\n- Prioritize: address if directly related, defer if tangential\r\n\r\n## Output Files\r\n\r\nFor each dispatched section, save:\r\n- `section-revisions/[section]-revised.md`  The revised section text\r\n- Update `revision-map.md`  Tracking updates\r\n\r\n## When You're Done\r\n\r\nAfter all sections have been dispatched and revised:\r\n\r\nSummarize for the orchestrator:\r\n- Sections revised\r\n- Items addressed per section\r\n- Any items requiring user decision (`[!]`)\r\n- Any dependencies that need Phase 3 attention\r\n\r\n**Example summary**:\r\n> Dispatch complete. 6 sections revised:\r\n> - Theory: 3/3 items addressed\r\n> - Methods: 2/2 items addressed (pathway changed to Detailed)\r\n> - Introduction: 2/2 items addressed\r\n> - Findings: 1/1 item addressed\r\n> - Discussion: 2/2 items addressed\r\n> - Conclusion: 1/1 item addressed\r\n>\r\n> All items marked complete except D-1 (scope conditions) flagged [!]  user needs to confirm which scope conditions are appropriate.\r\n>\r\n> Ready for Phase 3 integration review.\r\n\r\nThen proceed to Phase 3.\r\n",
        "plugins/revision-coordinator/skills/revision-coordinator/phases/phase3-integration.md": "# Phase 3: Integration Review\r\n\r\n## Why This Phase Matters\r\n\r\nRevising sections individually can introduce incoherence. A strengthened theory section may now use different language than the findings. A restructured intro may no longer set up the conclusion's callbacks. This phase ensures the revised manuscript works as a whole.\r\n\r\n## Integration Checks\r\n\r\n### Check 1: Cluster Coherence\r\n\r\nDo intro, theory, and conclusion share the same cluster type?\r\n\r\n| Section | Cluster | Match? |\r\n|---------|---------|--------|\r\n| Introduction | [cluster] |  |\r\n| Theory | [cluster] | / |\r\n| Conclusion | [cluster] | / |\r\n\r\n**If mismatch detected**:\r\n- Usually the theory cluster is authoritative (it reflects the actual contribution)\r\n- Intro and conclusion should be adjusted to match theory\r\n- Use interview-bookends Phase 3 (coherence) to realign\r\n\r\n### Check 2: Promise-Delivery Alignment\r\n\r\nDoes the intro promise what the findings deliver?\r\n\r\n**Extract from intro**:\r\n- What puzzle/question does intro pose?\r\n- What is the promised contribution?\r\n- What empirical context is set up?\r\n\r\n**Extract from findings**:\r\n- What does the paper actually deliver?\r\n- What claims are actually made?\r\n- What empirical context is actually covered?\r\n\r\n**Common mismatches**:\r\n\r\n| Mismatch Type | Example | Resolution |\r\n|---------------|---------|------------|\r\n| Scope mismatch | Intro promises general theory; findings are case-specific | Narrow intro OR broaden discussion implications |\r\n| Emphasis mismatch | Intro emphasizes X; findings emphasize Y | Rebalance intro or add Y to intro framing |\r\n| Missing delivery | Intro promises Z; findings never address Z | Either deliver Z or remove promise |\r\n\r\n### Check 3: Theory  Findings Language\r\n\r\nDo the theoretical concepts from the revised theory section appear in the findings?\r\n\r\n**Concept inventory**:\r\n| Concept (from theory) | Appears in Findings? | Location |\r\n|----------------------|---------------------|----------|\r\n| [concept 1] | Yes/No | [para/page] |\r\n| [concept 2] | Yes/No | [para/page] |\r\n| [concept 3] | Yes/No | [para/page] |\r\n\r\n**If concepts missing from findings**:\r\n- Either the findings don't actually engage the theory (substantive problem)\r\n- Or the terminology shifted during revision (fixable with word changes)\r\n\r\n**If findings use concepts not in theory**:\r\n- Either theory section needs to introduce them\r\n- Or findings should use theory's terminology instead\r\n\r\n### Check 4: Conclusion Callbacks\r\n\r\nDoes the conclusion reference specific language/concepts from the intro?\r\n\r\n**Callback inventory**:\r\n| Intro Element | Callback in Conclusion? | Quote/Location |\r\n|---------------|------------------------|----------------|\r\n| Opening hook/puzzle | Yes/No | |\r\n| Central question | Yes/No | |\r\n| Key phrase | Yes/No | |\r\n| Promise | Yes/No | |\r\n\r\n**Interview-bookends benchmark**: 100% of strong conclusions have callbacks.\r\n\r\n**If callbacks missing**:\r\n- Identify 2-3 specific intro phrases that can be echoed\r\n- Add explicit references in conclusion (not repetition, but resonance)\r\n- Use interview-bookends Phase 3 specifically for this\r\n\r\n### Check 5: Methods  Findings Alignment\r\n\r\nDo the methods described support the claims made in findings?\r\n\r\n**Check for**:\r\n- If findings claim saturation, does methods mention saturation?\r\n- If findings report precise counts, does methods explain counting procedure?\r\n- If findings discuss negative cases, does methods explain how they were identified?\r\n\r\n### Check 6: Terminology Consistency\r\n\r\nAre key terms used consistently throughout?\r\n\r\n**Build a terminology table**:\r\n| Term | Definition Location | Consistent Use? | Variants Found |\r\n|------|--------------------|-----------------| ---------------|\r\n| [key term 1] | Theory, para X | Yes/No | [list variants] |\r\n| [key term 2] | Theory, para Y | Yes/No | [list variants] |\r\n\r\n**Common problems**:\r\n- Term defined in theory, synonym used in findings\r\n- Term used before it's defined\r\n- Term definition shifts between sections\r\n\r\n### Check 7: Citation Consistency\r\n\r\nAre the same sources cited consistently?\r\n\r\n**Check for**:\r\n- Same author cited differently (Smith 2020 vs. Smith 2019)\r\n- In-text citations that don't appear in references\r\n- References that are never cited in-text\r\n\r\n## Integration Repair Protocol\r\n\r\nFor each integration issue detected:\r\n\r\n1. **Classify severity**:\r\n   - **Minor**: Terminology inconsistency, missing callback (easy fix)\r\n   - **Moderate**: Promise-delivery mismatch requiring section adjustment\r\n   - **Major**: Cluster mismatch requiring re-revision\r\n\r\n2. **Document the issue**:\r\n   ```markdown\r\n   ### Integration Issue: [Name]\r\n\r\n   **Type**: [Cluster / Promise-Delivery / Terminology / Callback / Citation]\r\n   **Severity**: [Minor / Moderate / Major]\r\n   **Sections affected**: [list]\r\n\r\n   **Description**:\r\n   [What's inconsistent and where]\r\n\r\n   **Proposed resolution**:\r\n   [How to fix]\r\n\r\n   **Action**:\r\n   - [ ] Direct fix (can do now)\r\n   - [ ] Requires skill re-dispatch\r\n   - [ ] Requires user decision\r\n   ```\r\n\r\n3. **Make repairs**:\r\n   - Minor issues: Fix directly\r\n   - Moderate issues: May require skill re-dispatch (back to Phase 2)\r\n   - Major issues: Flag for user decision\r\n\r\n## Cross-Section Coherence Matrix\r\n\r\nCreate a summary matrix:\r\n\r\n```markdown\r\n## Coherence Matrix\r\n\r\n| Check | Status | Notes |\r\n|-------|--------|-------|\r\n| Cluster coherence | / | [notes] |\r\n| Promise-delivery | / | [notes] |\r\n| TheoryFindings language | / | [notes] |\r\n| Conclusion callbacks | / | [notes] |\r\n| MethodsFindings | / | [notes] |\r\n| Terminology | / | [notes] |\r\n| Citations | / | [notes] |\r\n\r\n### Issues Requiring Attention\r\n1. [Issue 1  severity, resolution]\r\n2. [Issue 2  severity, resolution]\r\n```\r\n\r\n## Special Case: Cascading Changes\r\n\r\nSometimes fixing one integration issue creates another. For example:\r\n- Fixing promise-delivery mismatch by changing intro  breaks conclusion callbacks\r\n- Fixing terminology in theory  requires updates in findings AND discussion\r\n\r\n**Handle cascading changes by**:\r\n1. Identifying the full cascade before making changes\r\n2. Making changes in dependency order (upstream  downstream)\r\n3. Re-checking after each change\r\n\r\n## Output Files\r\n\r\nUpdate in `revision/` directory:\r\n- `revision-map.md`  Add integration findings section\r\n- `integration-report.md`  Detailed coherence analysis (optional)\r\n\r\n## When You're Done\r\n\r\nSummarize for the orchestrator:\r\n- Integration checks passed/failed\r\n- Issues detected and resolved\r\n- Any issues requiring user decision\r\n- Recommendation: ready for Phase 4 or needs more work\r\n\r\n**Example summary**:\r\n> Integration review complete. 5/7 checks passed.\r\n>\r\n> Issues detected:\r\n> 1. Conclusion callbacks: Missing reference to intro's \"dispersal\" framing  FIXED (added callback in conclusion para 2)\r\n> 2. Terminology: \"Organizational absorption\" used inconsistently (sometimes \"total institution\")  FIXED (standardized to \"organizational absorption\" throughout)\r\n>\r\n> Remaining:\r\n> - Cluster coherence: \r\n> - Promise-delivery:  (intro now matches revised findings emphasis)\r\n> - TheoryFindings: \r\n> - MethodsFindings: \r\n> - Citations: \r\n>\r\n> Ready for Phase 4 verification.\r\n\r\nThen proceed to Phase 4.\r\n",
        "plugins/revision-coordinator/skills/revision-coordinator/phases/phase4-verification.md": "# Phase 4: Verification & Response\r\n\r\n## Why This Phase Matters\r\n\r\nBefore declaring revision complete, you need to verify that all feedback has actually been addressed. This phase creates accountabilitya clear record of what was changed, how each feedback item was handled, and any remaining gaps.\r\n\r\nIf the revision is for a journal R&R, this phase also produces the response memo.\r\n\r\n## Verification Tasks\r\n\r\n### Task 1: Feedback Item Audit\r\n\r\nGo through EVERY feedback item from Phase 0 and verify its status:\r\n\r\n```markdown\r\n## Feedback Item Audit\r\n\r\n### Introduction Items\r\n| ID | Feedback | Status | How Addressed | Location |\r\n|----|----------|--------|---------------|----------|\r\n| I-1 | Too long | [x] | Cut 200 words, removed redundant para | Intro para 2 removed |\r\n| I-2 | Two intros | [x] | Merged into single intro | N/A |\r\n\r\n### Theory Items\r\n| ID | Feedback | Status | How Addressed | Location |\r\n|----|----------|--------|---------------|----------|\r\n| T-1 | Compress 30% | [x] | Cut 450 words | Throughout |\r\n| T-2 | Strengthen turn | [x] | Sharpened gapcontribution pivot | Theory, final para |\r\n| T-3 | Define \"absorption\" | [x] | Added definition | Theory para 2 |\r\n\r\n### Methods Items\r\n| ID | Feedback | Status | How Addressed | Location |\r\n|----|----------|--------|---------------|----------|\r\n| M-1 | Coding detail | [x] | Expanded coding process description | Methods para 4-5 |\r\n| M-2 | Reliability | [x] | Added structured recoding statement | Methods para 5 |\r\n\r\n### [Continue for all sections...]\r\n```\r\n\r\n**Status codes**:\r\n- `[x]`  Fully addressed\r\n- `[~]`  Partially addressed (explain what remains)\r\n- `[!]`  Requires user decision (explain options)\r\n- `[-]`  Declined to address (explain why)\r\n\r\n### Task 2: Change Summary\r\n\r\nCreate a section-by-section summary of what changed:\r\n\r\n```markdown\r\n## Revision Summary\r\n\r\n### Introduction\r\n**Word count**: 850  650 (-200 words)\r\n**Structural changes**: Merged two introductions into one; removed redundant \"Background\" section\r\n**Key improvements**: Opening now directly poses the puzzle; roadmap is clearer\r\n\r\n### Theory/Literature Review\r\n**Word count**: 1800  1350 (-450 words)\r\n**Structural changes**: None (same 3 subsections)\r\n**Key improvements**: Turn (gap  contribution) is now explicit and positioned at section end; \"organizational absorption\" defined with observable indicators\r\n\r\n### Methods\r\n**Word count**: 900  1400 (+500 words)\r\n**Structural changes**: Changed from Efficient to Detailed pathway\r\n**Key improvements**: Added coding process (para 4-5), structured recoding (para 5), saturation statement (para 6)\r\n\r\n### Findings\r\n**Word count**: 4000  3850 (-150 words)\r\n**Structural changes**: None\r\n**Key improvements**: Aligned terminology with revised theory section; tightened some evidence-interpretation sequences\r\n\r\n### Discussion\r\n**Word count**: 1200  1400 (+200 words)\r\n**Structural changes**: Added scope conditions subsection\r\n**Key improvements**: Explicit scope conditions; stronger connection to literatures\r\n\r\n### Conclusion\r\n**Word count**: 600  550 (-50 words)\r\n**Structural changes**: None\r\n**Key improvements**: Added callbacks to intro language (\"dispersal\" framing)\r\n```\r\n\r\n### Task 3: Unaddressed Items Documentation\r\n\r\nFor any items marked `[~]`, `[!]`, or `[-]`, provide detailed explanation:\r\n\r\n```markdown\r\n## Unaddressed or Partially Addressed Items\r\n\r\n### [!] D-1: Scope conditions\r\n**Feedback**: \"Add scope conditions\"\r\n**Status**: Requires user decision\r\n**Options**:\r\n1. Limit to high-intensity movements only (narrower, safer)\r\n2. Propose generalization to any identity-absorbing organization (broader, riskier)\r\n3. Add both as explicit contrasting scope conditions\r\n\r\n**Recommendation**: Option 3 (present both, let readers judge)\r\n\r\n### [~] F-1: More quantitative data\r\n**Feedback**: \"Consider adding more quantitative data\"\r\n**Status**: Partially addressed\r\n**What was done**: Added counts for each exit pathway type (Table 2)\r\n**What remains**: Did not add multivariate analysis (beyond scope of qualitative approach)\r\n**Rationale**: Qualitative methodology doesn't support statistical inference; counts are descriptive\r\n\r\n### [-] R2-3: Compare to other movements\r\n**Feedback**: \"Would be strengthened by comparison to other movements\"\r\n**Status**: Declined to address\r\n**Rationale**:\r\n- Would require additional data collection beyond revision scope\r\n- Comparative claims are already hedged as future research direction\r\n- Single-case depth is methodological strength, not weakness\r\n```\r\n\r\n### Task 4: Word Count Reconciliation\r\n\r\nIf word limits are a constraint, verify compliance:\r\n\r\n```markdown\r\n## Word Count Summary\r\n\r\n| Section | Original | Revised | Change |\r\n|---------|----------|---------|--------|\r\n| Abstract | 150 | 148 | -2 |\r\n| Introduction | 850 | 650 | -200 |\r\n| Theory | 1800 | 1350 | -450 |\r\n| Methods | 900 | 1400 | +500 |\r\n| Findings | 4000 | 3850 | -150 |\r\n| Discussion | 1200 | 1400 | +200 |\r\n| Conclusion | 600 | 550 | -50 |\r\n| **Total** | **9500** | **9348** | **-152** |\r\n\r\n**Target**: 10,000 words\r\n**Status**:  Within limit\r\n```\r\n\r\n### Task 5: Response Memo (for R&R)\r\n\r\nIf this is a journal revision, draft a response memo:\r\n\r\n```markdown\r\n# Response to Reviewers\r\n\r\n## Editor\r\nThank you for the opportunity to revise this manuscript. We have carefully considered all feedback and made substantial revisions as detailed below.\r\n\r\n## Reviewer 1\r\n\r\n### Comment 1.1\r\n> \"The introduction is too long and repetitiveyou have two intros.\"\r\n\r\n**Response**: We agree and have restructured the opening. The original manuscript had both an Introduction and a \"Background\" section that covered similar ground. We have merged these into a single, streamlined Introduction (now 650 words, down from 850). The Introduction now opens directly with the empirical puzzle, presents the gap, previews our contribution, and provides a roadmapwithout the repetition the reviewer correctly identified.\r\n\r\n**Changes**: See revised Introduction, pp. 1-2.\r\n\r\n### Comment 1.2\r\n> \"Methods need more detail on coding.\"\r\n\r\n**Response**: We have substantially expanded the Methods section (now 1400 words, up from 900) to address this concern. Specifically, we added:\r\n- A detailed description of our three-stage coding process (para 4)\r\n- Explanation of how we distinguished key constructs (pace exhaustion vs. grief accumulation vs. fit failure) (para 5)\r\n- Statement on structured recoding after refining definitions (para 5)\r\n\r\n**Changes**: See revised Methods, pp. X-Y, paragraphs 4-5.\r\n\r\n### [Continue for all feedback items...]\r\n\r\n## Reviewer 2\r\n\r\n[Same format...]\r\n\r\n## Summary of Major Changes\r\n\r\n1. **Structural**: Merged two introductions; added scope conditions to Discussion\r\n2. **Methodological**: Expanded coding detail; clarified construct distinctions\r\n3. **Conceptual**: Sharpened the turn (gap  contribution); defined \"organizational absorption\" with observable indicators\r\n4. **Presentational**: Compressed theory section by 25%; added Table 2 (exit pathway counts)\r\n\r\nWe believe these revisions address the reviewers' concerns while preserving the manuscript's core contributions. Thank you again for the constructive feedback.\r\n```\r\n\r\n## Verification Checklist\r\n\r\nBefore declaring revision complete, confirm:\r\n\r\n- [ ] All feedback items have a status (`[x]`, `[~]`, `[!]`, or `[-]`)\r\n- [ ] All `[~]` items have explanation of what remains\r\n- [ ] All `[!]` items have options presented to user\r\n- [ ] All `[-]` items have rationale for declining\r\n- [ ] Change summary covers every section\r\n- [ ] Word count is within limits (if applicable)\r\n- [ ] Response memo drafted (if R&R)\r\n- [ ] Revised manuscript assembled in correct order\r\n\r\n## Output Files\r\n\r\nCreate in `revision/` directory:\r\n- `revision-summary.md`  Complete accounting of changes\r\n- `response-memo.md`  Reviewer response (if R&R)\r\n- `revised-draft.md`  Complete revised manuscript\r\n\r\n## Final Assembly\r\n\r\nAssemble the revised manuscript in correct order:\r\n\r\n1. Title page (if required)\r\n2. Abstract\r\n3. Introduction\r\n4. Theory/Literature Review\r\n5. Case Justification (if Policy-Driven cluster: before Theory)\r\n6. Methods\r\n7. Findings\r\n8. Discussion\r\n9. Conclusion\r\n10. References\r\n11. Tables and Figures\r\n12. Appendices\r\n\r\n## When You're Done\r\n\r\nSummarize for user:\r\n- Total feedback items: X\r\n- Fully addressed: Y\r\n- Partially addressed: Z (with reasons)\r\n- Requires decision: W (with options)\r\n- Declined: V (with rationale)\r\n- Word count change: N words\r\n- Response memo: Ready/Not needed\r\n\r\n**Example final summary**:\r\n> Revision complete.\r\n>\r\n> **Feedback accountability**:\r\n> - 11 feedback items total\r\n> - 9 fully addressed [x]\r\n> - 1 partially addressed [~] (quantitative data  added counts, declined multivariate)\r\n> - 1 requires decision [!] (scope conditions  three options presented)\r\n> - 0 declined\r\n>\r\n> **Word count**: 9500  9348 (-152 words), within 10K limit\r\n>\r\n> **Deliverables**:\r\n> - `revised-draft.md`  Complete revised manuscript\r\n> - `revision-summary.md`  Full change documentation\r\n> - `response-memo.md`  Reviewer response ready for review\r\n>\r\n> **Action needed**: Please confirm scope conditions choice (D-1) before finalizing.\r\n\r\nRevision coordination complete.\r\n",
        "plugins/stata-analyst/.claude-plugin/plugin.json": "{\n  \"name\": \"stata-analyst\",\n  \"description\": \"Stata statistical analysis for publication-ready sociology research. Phased workflow for DiD, IV, matching, panel methods, and more with pauses for user review.\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Neal Caren\"\n  },\n  \"license\": \"MIT\",\n  \"keywords\": [\n    \"Stata\",\n    \"statistics\",\n    \"econometrics\",\n    \"sociology\",\n    \"research\"\n  ],\n  \"skills\": \"./skills/\"\n}\n",
        "plugins/stata-analyst/skills/stata-analyst/SKILL.md": "---\r\nname: stata-analyst\r\ndescription: Stata statistical analysis for publication-ready sociology research. Guides you through phased workflows for DiD, IV, matching, panel methods, and more. Use when doing quantitative analysis in Stata for academic papers.\r\n---\r\n\r\n# Stata Statistical Analyst\r\n\r\nYou are an expert quantitative research assistant specializing in statistical analysis using Stata. Your role is to guide users through a systematic, phased analysis process that produces publication-ready results suitable for top-tier social science journals.\r\n\r\n## Core Principles\r\n\r\n1. **Identification before estimation**: Establish a credible research design before running any models. The estimator must match the identification strategy.\r\n\r\n2. **Reproducibility**: All analysis must be reproducible. Use seeds, document decisions, use master do-files, save intermediate outputs.\r\n\r\n3. **Robustness is required**: Main results mean little without robustness checks. Every analysis needs sensitivity analysis.\r\n\r\n4. **User collaboration**: The user knows their substantive domain. You provide methodological expertise; they make research decisions.\r\n\r\n5. **Pauses for reflection**: Stop between phases to discuss findings and get user input before proceeding.\r\n\r\n## Analysis Phases\r\n\r\n### Phase 0: Research Design Review\r\n**Goal**: Establish the identification strategy before touching data.\r\n\r\n**Process**:\r\n- Clarify the research question and causal claim\r\n- Identify the estimation strategy (DiD, IV, RD, matching, panel FE, etc.)\r\n- Discuss key assumptions and their plausibility\r\n- Identify threats to identification\r\n- Plan the overall analysis approach\r\n\r\n**Output**: Design memo documenting question, strategy, assumptions, and threats.\r\n\r\n> **Pause**: Confirm design with user before proceeding.\r\n\r\n---\r\n\r\n### Phase 1: Data Familiarization\r\n**Goal**: Understand the data before modeling.\r\n\r\n**Process**:\r\n- Load and inspect data structure\r\n- Generate descriptive statistics (Table 1)\r\n- Check data quality: missing values, outliers, coding errors\r\n- Visualize key variables and relationships\r\n- Verify that data supports the planned identification strategy\r\n\r\n**Output**: Data report with descriptives, quality assessment, and preliminary visualizations.\r\n\r\n> **Pause**: Review descriptives with user. Confirm sample and variable definitions.\r\n\r\n---\r\n\r\n### Phase 2: Model Specification\r\n**Goal**: Fully specify models before estimation.\r\n\r\n**Process**:\r\n- Write out the estimating equation(s)\r\n- Justify variable operationalization\r\n- Specify fixed effects structure\r\n- Determine clustering for standard errors\r\n- Plan the sequence of specifications (baseline -> full -> robustness)\r\n\r\n**Output**: Specification memo with equations, variable definitions, and rationale.\r\n\r\n> **Pause**: User approves specification before estimation.\r\n\r\n---\r\n\r\n### Phase 3: Main Analysis\r\n**Goal**: Estimate primary models and interpret results.\r\n\r\n**Process**:\r\n- Run main specifications\r\n- Interpret coefficients, standard errors, significance\r\n- Check model assumptions (where applicable)\r\n- Create initial results table\r\n\r\n**Output**: Main results with interpretation.\r\n\r\n> **Pause**: Discuss findings with user before robustness checks.\r\n\r\n---\r\n\r\n### Phase 4: Robustness & Sensitivity\r\n**Goal**: Stress-test the main findings.\r\n\r\n**Process**:\r\n- Alternative specifications (different controls, FE structures)\r\n- Subgroup analyses\r\n- Placebo tests (where applicable)\r\n- Wild cluster bootstrap (for few clusters)\r\n- Diagnostic tests specific to the method\r\n\r\n**Output**: Robustness tables and sensitivity assessment.\r\n\r\n> **Pause**: Assess whether findings are robust. Discuss implications.\r\n\r\n---\r\n\r\n### Phase 5: Output & Interpretation\r\n**Goal**: Produce publication-ready outputs and interpretation.\r\n\r\n**Process**:\r\n- Create publication-quality tables (esttab)\r\n- Create figures (coefplot, graphs)\r\n- Write results narrative\r\n- Document limitations and caveats\r\n- Prepare replication materials\r\n\r\n**Output**: Final tables, figures, and interpretation memo.\r\n\r\n---\r\n\r\n## Folder Structure\r\n\r\n```\r\nproject/\r\n data/\r\n    raw/              # Original data (never modified)\r\n    clean/            # Processed analysis data\r\n code/\r\n    00_master.do      # Runs entire analysis\r\n    01_clean.do\r\n    02_descriptives.do\r\n    03_analysis.do\r\n    04_robustness.do\r\n output/\r\n    tables/\r\n    figures/\r\n logs/                 # Stata log files\r\n memos/                # Phase outputs and decisions\r\n```\r\n\r\n## Technique Guides\r\n\r\nReference these guides for method-specific code. Guides are in `techniques/` (relative to this skill):\r\n\r\n| Guide | Topics |\r\n|-------|--------|\r\n| `00_index.md` | Quick lookup by method |\r\n| `00_data_prep.md` | Import, merge, missing data, transforms, panel setup |\r\n| `01_core_econometrics.md` | TWFE, DiD, Event Studies, IV, Matching, Mediation |\r\n| `02_survey_resampling.md` | Survey weights, Bootstrap, Oaxaca, Randomization Inference |\r\n| `03_synthetic_control.md` | synth for comparative case studies |\r\n| `04_visualization.md` | esttab, coefplot, graphs, summary statistics |\r\n| `05_best_practices.md` | Master scripts, path management, code organization |\r\n| `06_modeling_basics.md` | OLS, logit/probit, Poisson, margins, interactions |\r\n| `07_postestimation_reporting.md` | Estimates workflow, Table 1, predicted values |\r\n| `99_default_journal_pipeline.md` | Complete project template |\r\n\r\n**Start with `00_index.md` for a quick lookup by method.**\r\n\r\n## Running Stata Code\r\n\r\n### Execution Method\r\n\r\n```bash\r\n# Batch mode (recommended)\r\nstata -e do filename.do\r\n```\r\n\r\nThis executes `filename.do` and creates `filename.log` with all output.\r\n\r\n### Platform-Specific Paths\r\n\r\n**macOS:**\r\n```bash\r\n/Applications/Stata/StataMP.app/Contents/MacOS/StataMP -e do filename.do\r\n```\r\n\r\n**Linux:**\r\n```bash\r\n/usr/local/stata/stata -e do filename.do\r\n```\r\n\r\n### Check if Stata is Available\r\n\r\n```bash\r\nwhich stata || which StataMP || which StataSE || echo \"Stata not found\"\r\n```\r\n\r\n### If Stata Is Not Found\r\n\r\n1. Ask the user for their Stata installation path and version (MP, SE, or IC)\r\n2. If not installed: Provide code as `.do` files they can run later\r\n\r\n## Invoking Phase Agents\r\n\r\nFor each phase, invoke the appropriate sub-agent using the Task tool:\r\n\r\n```\r\nTask: Phase 1 Data Familiarization\r\nsubagent_type: general-purpose\r\nmodel: sonnet\r\nprompt: Read phases/phase1-data.md and execute for [user's project]\r\n```\r\n\r\n## Model Recommendations\r\n\r\n| Phase | Model | Rationale |\r\n|-------|-------|-----------|\r\n| **Phase 0**: Research Design | **Opus** | Methodological judgment, identifying threats |\r\n| **Phase 1**: Data Familiarization | **Sonnet** | Descriptive statistics, data processing |\r\n| **Phase 2**: Model Specification | **Opus** | Design decisions, justifying choices |\r\n| **Phase 3**: Main Analysis | **Sonnet** | Running models, standard interpretation |\r\n| **Phase 4**: Robustness | **Sonnet** | Systematic checks |\r\n| **Phase 5**: Output | **Opus** | Writing, synthesis, nuanced interpretation |\r\n\r\n## Starting the Analysis\r\n\r\nWhen the user is ready to begin:\r\n\r\n1. **Ask about the research question**:\r\n   > \"What causal or descriptive question are you trying to answer?\"\r\n\r\n2. **Ask about data**:\r\n   > \"What data do you have? Is it cross-sectional, panel, or repeated cross-section?\"\r\n\r\n3. **Ask about identification**:\r\n   > \"Do you have a specific identification strategy in mind (DiD, IV, RD, etc.), or would you like to discuss options?\"\r\n\r\n4. **Then proceed with Phase 0** to establish the research design.\r\n\r\n## Key Reminders\r\n\r\n- **Design before data**: Phase 0 happens before you look at results.\r\n- **Pause between phases**: Always stop for user input before proceeding.\r\n- **Use the technique guides**: Don't reinventuse tested code patterns.\r\n- **Cluster your standard errors**: Almost always at the unit of treatment assignment.\r\n- **Robustness is not optional**: Main results need sensitivity analysis.\r\n- **The user decides**: You provide options and recommendations; they choose.\r\n",
        "plugins/stata-analyst/skills/stata-analyst/phases/phase0-design.md": "# Phase 0: Research Design Review\r\n\r\nYou are executing Phase 0 of a statistical analysis in Stata. Your goal is to establish a credible research design before any estimation occurs.\r\n\r\n## Why This Phase Matters\r\n\r\nThe identification strategy determines whether results have a causal interpretation. No amount of sophisticated estimation can fix a flawed design. This phase ensures the user has thought through their approach before investing in analysis.\r\n\r\n## Your Tasks\r\n\r\n### 1. Clarify the Research Question\r\n\r\nAsk the user to articulate:\r\n- What is the main question? (causal or descriptive)\r\n- What is the outcome of interest?\r\n- What is the key explanatory variable or treatment?\r\n- What is the population of interest?\r\n\r\nDocument this clearlyit guides all subsequent decisions.\r\n\r\n### 2. Identify the Estimation Strategy\r\n\r\nBased on the research question and data structure, determine the appropriate approach:\r\n\r\n| Strategy | When to Use | Key Assumptions | Stata Command |\r\n|----------|-------------|-----------------|---------------|\r\n| **DiD** | Treatment timing varies | Parallel trends | `reghdfe`, `csdid` |\r\n| **Event Study** | Dynamic effects needed | Same as DiD | `csdid_estat event` |\r\n| **IV** | Endogenous treatment | Exclusion, relevance | `ivreg2`, `ivreghdfe` |\r\n| **RD** | Threshold-based treatment | Continuity | `rdrobust` |\r\n| **Matching** | Selection on observables | No hidden bias | `psmatch2`, `cem` |\r\n| **Panel FE** | Time-invariant confounders | Strict exogeneity | `reghdfe`, `xtreg` |\r\n\r\nFor each strategy, reference the relevant technique guide:\r\n- DiD/Event Study: `stata-statistical-techniques/01_core_econometrics.md`\r\n- IV: `stata-statistical-techniques/01_core_econometrics.md` Section 4\r\n- Matching: `stata-statistical-techniques/01_core_econometrics.md` Section 5\r\n- Synthetic Control: `stata-statistical-techniques/03_synthetic_control.md`\r\n\r\n### 3. Assess Assumptions\r\n\r\nFor the chosen strategy, discuss:\r\n\r\n**What must be true for this to work?**\r\n- State each assumption in plain language\r\n- Discuss whether it's plausible in this context\r\n- Identify what evidence could support or undermine it\r\n\r\n**What are the main threats?**\r\n- Confounders (what else might explain the relationship?)\r\n- Selection (are treated/control groups comparable?)\r\n- Reverse causality (could the outcome affect treatment?)\r\n- Measurement (are variables measured accurately?)\r\n\r\n### 4. Plan Robustness Checks\r\n\r\nBased on identified threats, plan how to address them:\r\n- Placebo tests (outcomes that shouldn't be affected)\r\n- Alternative specifications (different controls, FE structures)\r\n- Wild cluster bootstrap (if few clusters)\r\n- Subgroup analysis (heterogeneous effects)\r\n\r\n### 5. Document Data Requirements\r\n\r\nSpecify what the data must contain:\r\n- Unit identifiers\r\n- Time identifiers (if panel)\r\n- Treatment indicators and timing\r\n- Outcome variables\r\n- Key controls\r\n- Clustering variable for standard errors\r\n\r\n## Output: Design Memo\r\n\r\nCreate a design memo (`memos/phase0-design-memo.md`) containing:\r\n\r\n```markdown\r\n# Research Design Memo\r\n\r\n## Research Question\r\n[Clear statement of the question]\r\n\r\n## Identification Strategy\r\n[Strategy name and brief justification]\r\n\r\n## Key Variables\r\n- **Outcome**: [variable and measurement]\r\n- **Treatment/Exposure**: [variable and measurement]\r\n- **Unit of Analysis**: [what are observations]\r\n- **Time Structure**: [cross-section, panel, etc.]\r\n\r\n## Assumptions\r\n1. [Assumption 1]: [Why plausible / concerns]\r\n2. [Assumption 2]: [Why plausible / concerns]\r\n...\r\n\r\n## Threats to Identification\r\n1. [Threat 1]: [How we'll address it]\r\n2. [Threat 2]: [How we'll address it]\r\n...\r\n\r\n## Planned Robustness Checks\r\n- [ ] [Check 1]\r\n- [ ] [Check 2]\r\n...\r\n\r\n## Standard Errors\r\nCluster at: [level and justification]\r\n\r\n## Stata Packages Needed\r\n- reghdfe (ssc install reghdfe)\r\n- [other packages]\r\n\r\n## Questions for User\r\n- [Any clarifications needed]\r\n```\r\n\r\n## When You're Done\r\n\r\nReturn a summary to the orchestrator that includes:\r\n1. The recommended identification strategy\r\n2. Key assumptions and whether they seem plausible\r\n3. Main threats and planned mitigations\r\n4. Any questions or concerns for the user\r\n5. Confirmation that design memo was created\r\n\r\n**Do not proceed to Phase 1 until the user confirms the research design.**\r\n",
        "plugins/stata-analyst/skills/stata-analyst/phases/phase1-data.md": "# Phase 1: Data Familiarization\r\n\r\nYou are executing Phase 1 of a statistical analysis in Stata. Your goal is to develop deep familiarity with the data before any modeling.\r\n\r\n## Why This Phase Matters\r\n\r\nJumping straight to regression is a common mistake. Understanding your data prevents errors, reveals data quality issues, and often suggests refinements to the research design. This phase creates the foundation for credible analysis.\r\n\r\n## Technique Guides\r\n\r\n**Consult these guides** in `stata-statistical-techniques/` for data handling patterns:\r\n\r\n| Topic | Guide |\r\n|-------|-------|\r\n| Data prep, Import, Merge | `00_data_prep.md` |\r\n| Visualization, Summary stats | `04_visualization.md` |\r\n| Survey data handling | `02_survey_resampling.md` |\r\n| Best practices, Path management | `05_best_practices.md` |\r\n\r\n## Your Tasks\r\n\r\n### 1. Load and Inspect Data Structure\r\n\r\n```stata\r\n* Load data\r\nuse \"$raw/filename.dta\", clear\r\n\r\n* Basic structure\r\ndescribe\r\ncodebook, compact\r\n\r\n* Check for duplicates\r\nduplicates report id\r\nisid id  // Should not error if id is unique\r\n\r\n* Check panel structure (if applicable)\r\nxtset id year\r\nxtdescribe\r\n```\r\n\r\nDocument:\r\n- Number of observations and variables\r\n- Unit of observation\r\n- Key variable types\r\n- Any obvious data issues\r\n\r\n### 2. Generate Descriptive Statistics (Table 1)\r\n\r\n```stata\r\n* Summary statistics\r\nsummarize outcome treatment control1 control2, detail\r\n\r\n* Formatted table\r\nestpost summarize outcome treatment control1 control2\r\nesttab using \"$tables/table1_descriptives.rtf\", ///\r\n    cells(\"count mean sd min max\") ///\r\n    title(\"Summary Statistics\") ///\r\n    replace\r\n\r\n* By treatment group\r\nbysort treatment: summarize outcome control1 control2\r\ntabstat outcome control1 control2, by(treatment) stats(n mean sd)\r\n\r\n* Balance table\r\nestpost ttest outcome control1 control2, by(treatment)\r\nesttab using \"$tables/table1_balance.rtf\", ///\r\n    cells(\"mu_1 mu_2 b se\") ///\r\n    title(\"Balance Across Treatment Groups\") ///\r\n    replace\r\n```\r\n\r\n### 3. Check Data Quality\r\n\r\n**Missing values:**\r\n```stata\r\n* Count missing by variable\r\nmisstable summarize\r\n\r\n* Patterns of missingness\r\nmisstable patterns\r\n\r\n* Document how missing data will be handled\r\ncount if missing(outcome)\r\n```\r\n\r\n**Outliers:**\r\n```stata\r\n* Check key continuous variables\r\nsummarize outcome, detail\r\ntabstat outcome, stats(p1 p5 p95 p99)\r\n\r\n* Visualize distributions\r\nhistogram outcome, bin(50) ///\r\n    title(\"Distribution of Outcome\")\r\ngraph export \"$figures/outcome_dist.pdf\", replace\r\n```\r\n\r\n**Coding issues:**\r\n```stata\r\n* Check categorical variables\r\ntab treatment, missing\r\n\r\n* Check for impossible values\r\nlist if age < 0 | age > 120\r\n\r\n* Check value labels\r\nlabel list\r\n```\r\n\r\n### 4. Visualize Key Relationships\r\n\r\n**For DiD/Panel:**\r\n```stata\r\n* Trends over time by treatment group\r\npreserve\r\ncollapse (mean) mean_outcome=outcome, by(year treatment_group)\r\ntwoway (line mean_outcome year if treatment_group==0, lcolor(blue)) ///\r\n       (line mean_outcome year if treatment_group==1, lcolor(red)), ///\r\n       xline(treatment_year, lpattern(dash)) ///\r\n       legend(order(1 \"Control\" 2 \"Treated\")) ///\r\n       title(\"Outcome Trends by Treatment Status\")\r\ngraph export \"$figures/trends.pdf\", replace\r\nrestore\r\n```\r\n\r\n**For RD:**\r\n```stata\r\n* Outcome vs. running variable\r\ntwoway (scatter outcome running_var, msize(small) mcolor(gray%50)) ///\r\n       (lfit outcome running_var if running_var < cutoff, lcolor(blue)) ///\r\n       (lfit outcome running_var if running_var >= cutoff, lcolor(red)), ///\r\n       xline(cutoff, lpattern(dash)) ///\r\n       title(\"Regression Discontinuity\")\r\ngraph export \"$figures/rd_plot.pdf\", replace\r\n```\r\n\r\n**For any design:**\r\n```stata\r\n* Bivariate relationship\r\ngraph box outcome, over(treatment) ///\r\n    title(\"Outcome by Treatment Status\")\r\ngraph export \"$figures/outcome_by_treatment.pdf\", replace\r\n\r\n* Correlation matrix\r\ncorrelate outcome treatment control1 control2\r\n```\r\n\r\n### 5. Verify Design Requirements\r\n\r\nCheck that data supports the planned identification strategy:\r\n\r\n**For DiD:**\r\n```stata\r\n* Check pre and post periods exist\r\ntab year treatment\r\ntab post treatment\r\n\r\n* Cell counts\r\nbysort treatment post: count\r\n```\r\n\r\n**For Panel FE:**\r\n```stata\r\n* Within-unit variation\r\nxtset id year\r\nxtsum treatment  // Check \"within\" variation\r\n```\r\n\r\n**For IV:**\r\n```stata\r\n* First stage relationship\r\nregress endogenous instrument, robust\r\n```\r\n\r\n### 6. Create Analysis Sample\r\n\r\nDefine and document the final analysis sample:\r\n\r\n```stata\r\n* Count original sample\r\ncount\r\nlocal n_orig = r(N)\r\n\r\n* Apply restrictions\r\ndrop if missing(outcome)\r\nlocal n_after_outcome = _N\r\n\r\ndrop if missing(treatment)\r\nlocal n_after_treatment = _N\r\n\r\nkeep if year >= 2000 & year <= 2020\r\nlocal n_final = _N\r\n\r\n* Document sample construction\r\ndisplay \"Original sample: `n_orig'\"\r\ndisplay \"After dropping missing outcome: `n_after_outcome'\"\r\ndisplay \"Final analysis sample: `n_final'\"\r\n\r\n* Save analysis sample\r\nsave \"$clean/analysis_sample.dta\", replace\r\n```\r\n\r\n## Output: Data Report\r\n\r\nCreate a data report (`memos/phase1-data-report.md`) containing:\r\n\r\n```markdown\r\n# Data Familiarization Report\r\n\r\n## Data Overview\r\n- **Source**: [where data comes from]\r\n- **Observations**: [N]\r\n- **Variables**: [count and key variables]\r\n- **Time Period**: [if applicable]\r\n\r\n## Sample Construction\r\n| Step | N | Notes |\r\n|------|---|-------|\r\n| Original sample | X | |\r\n| After restriction 1 | Y | [reason] |\r\n| Final analysis sample | Z | |\r\n\r\n## Descriptive Statistics\r\n[Insert or reference Table 1]\r\n\r\n## Data Quality Issues\r\n- **Missing data**: [summary and how handled]\r\n- **Outliers**: [any concerns]\r\n- **Coding issues**: [any found and fixed]\r\n\r\n## Key Visualizations\r\n[Reference saved figures]\r\n\r\n## Design Verification\r\n- [Confirm data supports the identification strategy]\r\n- [Note any concerns]\r\n\r\n## Preliminary Observations\r\n- [Anything notable in the descriptives]\r\n- [Any surprises or concerns]\r\n\r\n## Questions for User\r\n- [Any decisions that need user input]\r\n```\r\n\r\n## When You're Done\r\n\r\nReturn a summary to the orchestrator that includes:\r\n1. Final sample size and key restrictions\r\n2. Any data quality issues found\r\n3. Whether data supports the planned design\r\n4. Key observations from descriptives\r\n5. Questions for the user\r\n\r\n**Do not proceed to Phase 2 until the user reviews the descriptives and confirms the sample.**\r\n",
        "plugins/stata-analyst/skills/stata-analyst/phases/phase2-specification.md": "# Phase 2: Model Specification\r\n\r\nYou are executing Phase 2 of a statistical analysis in Stata. Your goal is to fully specify the models before estimationequations, variables, and standard errors.\r\n\r\n## Why This Phase Matters\r\n\r\nSpecification decisions are research decisions. Making them explicit before seeing results prevents p-hacking and specification searching. This phase documents the pre-analysis plan.\r\n\r\n## Technique Guides\r\n\r\n**Consult these guides** in `stata-statistical-techniques/` for specification patterns:\r\n\r\n| Method | Guide |\r\n|--------|-------|\r\n| DiD, TWFE, Event Study, IV | `01_core_econometrics.md` |\r\n| Matching specifications | `01_core_econometrics.md` Section 5 |\r\n| Nonlinear models (logit, Poisson) | `06_modeling_basics.md` |\r\n| Synthetic control | `03_synthetic_control.md` |\r\n\r\n## Your Tasks\r\n\r\n### 1. Write the Estimating Equation\r\n\r\nState the model formally. Examples by design:\r\n\r\n**Two-Way Fixed Effects:**\r\n$$Y_{it} = \\alpha_i + \\gamma_t + \\beta \\cdot Treated_{it} + X_{it}'\\delta + \\varepsilon_{it}$$\r\n\r\n**Difference-in-Differences:**\r\n$$Y_{it} = \\alpha + \\beta_1 \\cdot Post_t + \\beta_2 \\cdot Treat_i + \\beta_3 \\cdot (Post_t \\times Treat_i) + \\varepsilon_{it}$$\r\n\r\n**Event Study:**\r\n$$Y_{it} = \\alpha_i + \\gamma_t + \\sum_{k \\neq -1} \\beta_k \\cdot \\mathbf{1}[t - E_i = k] + X_{it}'\\delta + \\varepsilon_{it}$$\r\n\r\n**Instrumental Variables:**\r\n- First stage: $D_i = \\pi_0 + \\pi_1 Z_i + X_i'\\pi_2 + \\nu_i$\r\n- Second stage: $Y_i = \\beta_0 + \\beta_1 \\hat{D}_i + X_i'\\beta_2 + \\varepsilon_i$\r\n\r\n### 2. Define All Variables\r\n\r\nCreate a variable dictionary:\r\n\r\n| Variable | Name in Data | Definition | Notes |\r\n|----------|--------------|------------|-------|\r\n| Outcome | `outcome` | [precise definition] | [measurement, source] |\r\n| Treatment | `treated` | [how assigned] | [timing if applicable] |\r\n| Control 1 | `age` | [definition] | [why included] |\r\n| ... | | | |\r\n\r\n**Key questions:**\r\n- How is treatment defined? (binary, intensity, timing)\r\n- What controls are included and why?\r\n- Are there variables that should NOT be controlled for? (mediators, colliders)\r\n\r\n### 3. Specify Fixed Effects Structure\r\n\r\nFor panel data, justify the FE structure:\r\n\r\n| Specification | Absorbs | Code |\r\n|---------------|---------|------|\r\n| Unit FE | Time-invariant unit characteristics | `reghdfe y x, absorb(id)` |\r\n| Time FE | Common shocks | `reghdfe y x, absorb(year)` |\r\n| Two-way FE | Both | `reghdfe y x, absorb(id year)` |\r\n| Unit-by-time trends | Unit-specific trends | `reghdfe y x, absorb(id year c.year#i.id)` |\r\n\r\nDocument why this structure is appropriate for the research question.\r\n\r\n### 4. Determine Standard Error Clustering\r\n\r\n**Default rule:** Cluster at the level of treatment assignment.\r\n\r\n| Design | Typical Clustering | Rationale |\r\n|--------|-------------------|-----------|\r\n| State policy DiD | State | Treatment varies at state level |\r\n| Individual-level RCT | Individual or strata | Assignment unit |\r\n| Panel with firm shocks | Firm | Errors correlated within firm over time |\r\n\r\n```stata\r\n* reghdfe syntax\r\nreghdfe y x, absorb(id year) cluster(id)\r\n\r\n* Two-way clustering if needed\r\nreghdfe y x, absorb(id year) cluster(id year)\r\n```\r\n\r\n**Consider wild cluster bootstrap** if few clusters (<50):\r\n```stata\r\n* After estimation with reghdfe\r\nboottest x, cluster(state) reps(999) nograph\r\n```\r\n\r\n### 5. Plan Specification Sequence\r\n\r\nDefine the sequence of models to run:\r\n\r\n| Model | Description | Purpose |\r\n|-------|-------------|---------|\r\n| (1) | Baseline: treatment only | Raw relationship |\r\n| (2) | + Unit FE | Control time-invariant confounders |\r\n| (3) | + Time FE | Control common shocks |\r\n| (4) | + Controls | Address remaining confounders |\r\n| (5) | Preferred specification | Main results |\r\n\r\nThis builds credibility by showing results are stable across specifications.\r\n\r\n### 6. Pre-specify Robustness Checks\r\n\r\nBefore running main models, document planned robustness checks:\r\n\r\n**Alternative specifications:**\r\n- [ ] Different control sets\r\n- [ ] Different FE structures\r\n- [ ] Different treatment definitions\r\n\r\n**Sensitivity analysis:**\r\n- [ ] Sensitivity to outliers (winsorize, trim)\r\n- [ ] Sensitivity to functional form\r\n\r\n**Placebo tests:**\r\n- [ ] Pre-treatment effects (should be zero)\r\n- [ ] Outcomes that shouldn't be affected\r\n- [ ] Fake treatment timing\r\n\r\n**Heterogeneity:**\r\n- [ ] Subgroup analyses (pre-specified)\r\n- [ ] Interaction terms\r\n\r\n## Output: Specification Memo\r\n\r\nCreate a specification memo (`memos/phase2-specification-memo.md`):\r\n\r\n```markdown\r\n# Model Specification Memo\r\n\r\n## Estimating Equation\r\n\r\n[LaTeX or clear written equation]\r\n\r\n## Variable Definitions\r\n\r\n| Variable | Name | Definition | Measurement |\r\n|----------|------|------------|-------------|\r\n| ... | | | |\r\n\r\n## Fixed Effects\r\n[Structure and justification]\r\n\r\n## Standard Errors\r\nClustered at: [level]\r\nJustification: [why]\r\n\r\n## Specification Sequence\r\n\r\n| Model | Specification | Purpose |\r\n|-------|---------------|---------|\r\n| (1) | | |\r\n| ... | | |\r\n\r\n## Pre-Specified Robustness Checks\r\n1. [Check 1]\r\n2. [Check 2]\r\n...\r\n\r\n## Pre-Specified Subgroup Analyses\r\n1. [Subgroup 1]\r\n2. [Subgroup 2]\r\n...\r\n\r\n## Code Skeleton\r\n\r\n```stata\r\n* Install packages if needed\r\n* ssc install reghdfe\r\n* ssc install estout\r\n\r\n* Load data\r\nuse \"$clean/analysis_sample.dta\", clear\r\n\r\n* Clear stored estimates\r\nestimates clear\r\n\r\n* Specification sequence\r\nreghdfe outcome treatment, noabsorb cluster(cluster_var)\r\nestimates store m1\r\n\r\nreghdfe outcome treatment, absorb(id) cluster(cluster_var)\r\nestimates store m2\r\n\r\nreghdfe outcome treatment, absorb(id year) cluster(cluster_var)\r\nestimates store m3\r\n\r\nreghdfe outcome treatment control1 control2, absorb(id year) cluster(cluster_var)\r\nestimates store m4\r\n\r\n* Display results\r\nesttab m1 m2 m3 m4, se star(* 0.10 ** 0.05 *** 0.01)\r\n```\r\n\r\n## Questions for User\r\n[Any specification decisions that need input]\r\n```\r\n\r\n## When You're Done\r\n\r\nReturn a summary to the orchestrator that includes:\r\n1. The main estimating equation\r\n2. Fixed effects structure and clustering\r\n3. The planned specification sequence\r\n4. Pre-specified robustness checks\r\n5. Any questions requiring user input\r\n\r\n**Do not proceed to Phase 3 until the user approves the specification.**\r\n",
        "plugins/stata-analyst/skills/stata-analyst/phases/phase3-analysis.md": "# Phase 3: Main Analysis\r\n\r\nYou are executing Phase 3 of a statistical analysis in Stata. Your goal is to run the pre-specified models and interpret the main results.\r\n\r\n## Why This Phase Matters\r\n\r\nThis is where the analysis happens. But because you've done Phases 0-2, you're not searchingyou're executing a pre-specified plan. This makes results more credible.\r\n\r\n## Technique Guides\r\n\r\n**Before writing code, consult the relevant technique guide** in `stata-statistical-techniques/` for method-specific patterns:\r\n\r\n| Method | Guide |\r\n|--------|-------|\r\n| DiD, Event Study, IV, Matching | `01_core_econometrics.md` |\r\n| Survey weights, Bootstrap | `02_survey_resampling.md` |\r\n| Synthetic Control | `03_synthetic_control.md` |\r\n| Logit, Poisson, Margins | `06_modeling_basics.md` |\r\n| Tables, Visualization | `04_visualization.md` |\r\n| Post-estimation, Reporting | `07_postestimation_reporting.md` |\r\n\r\nThese guides contain tested code patternsuse them rather than writing from scratch.\r\n\r\n## Your Tasks\r\n\r\n### 1. Run the Specification Sequence\r\n\r\nExecute the models defined in Phase 2:\r\n\r\n```stata\r\n* Load analysis data\r\nuse \"$clean/analysis_sample.dta\", clear\r\n\r\n* Clear stored estimates\r\nestimates clear\r\n\r\n* Specification sequence\r\n* Model 1: Baseline\r\nreghdfe outcome treatment, noabsorb cluster(cluster_var)\r\nestimates store m1\r\n\r\n* Model 2: Unit FE\r\nreghdfe outcome treatment, absorb(id) cluster(cluster_var)\r\nestimates store m2\r\n\r\n* Model 3: Two-way FE\r\nreghdfe outcome treatment, absorb(id year) cluster(cluster_var)\r\nestimates store m3\r\n\r\n* Model 4: With controls (preferred)\r\nreghdfe outcome treatment control1 control2, absorb(id year) cluster(cluster_var)\r\nestimates store m4\r\n```\r\n\r\n### 2. Create Main Results Table\r\n\r\n```stata\r\n* Console output for review\r\nesttab m1 m2 m3 m4, se star(* 0.10 ** 0.05 *** 0.01) ///\r\n    stats(N r2_a, labels(\"Observations\" \"Adj. R-squared\"))\r\n\r\n* Publication table (RTF for Word)\r\nesttab m1 m2 m3 m4 using \"$tables/table2_main.rtf\", replace ///\r\n    se star(* 0.10 ** 0.05 *** 0.01) ///\r\n    stats(N r2_a, labels(\"Observations\" \"Adj. R-squared\") fmt(0 3)) ///\r\n    title(\"Effect of Treatment on Outcome\") ///\r\n    mtitles(\"Baseline\" \"Unit FE\" \"Two-way FE\" \"Full Model\") ///\r\n    note(\"Standard errors clustered at [level] in parentheses. * p<0.1, ** p<0.05, *** p<0.01\")\r\n\r\n* LaTeX version\r\nesttab m1 m2 m3 m4 using \"$tables/table2_main.tex\", replace ///\r\n    se star(* 0.10 ** 0.05 *** 0.01) booktabs ///\r\n    stats(N r2_a, labels(\"Observations\" \"Adj. R-squared\") fmt(0 3))\r\n```\r\n\r\n### 3. Interpret the Results\r\n\r\nFor the preferred specification, document:\r\n\r\n**Point estimate:**\r\n- What is the estimated effect?\r\n- What are the units? (interpret in substantive terms)\r\n- How large is this effect? (compare to mean, SD, or meaningful benchmark)\r\n\r\n**Statistical precision:**\r\n- What is the standard error?\r\n- What is the confidence interval?\r\n- Is this precisely estimated or noisy?\r\n\r\n**Stability across specifications:**\r\n- Does the estimate change substantially across models?\r\n- What does adding controls/FE do to the estimate?\r\n- Is the sign consistent?\r\n\r\n### 4. Interpret Nonlinear Models (If Applicable)\r\n\r\nFor logistic, Poisson, ordered logit, or other nonlinear models, coefficients alone are insufficient. Follow current methodological standards (Long and Mustillo 2017; Mize 2019):\r\n\r\n**Average Marginal Effects (AMEs):**\r\n```stata\r\n* After logit/probit/ologit/poisson/etc.\r\nlogit outcome treatment control1 control2, vce(cluster cluster_var)\r\n\r\n* Compute AMEs for all predictors\r\nmargins, dydx(*) post\r\nestimates store ame\r\n\r\n* Create AME table\r\nesttab ame using \"$tables/table2_ame.rtf\", replace ///\r\n    cells(b(fmt(3)) se(fmt(3))) ///\r\n    title(\"Average Marginal Effects\") ///\r\n    note(\"Standard errors in parentheses.\")\r\n```\r\n\r\n**Predicted Probabilities:**\r\n```stata\r\n* Predictions at specific values\r\nmargins, at(treatment=(0 1)) atmeans\r\n\r\n* Plot predicted probabilities across range of X\r\nmargins, at(treatment=(0 1) control1=(1(1)10))\r\nmarginsplot, ///\r\n    title(\"Predicted Probability by Treatment and Control1\") ///\r\n    xtitle(\"Control1\") ytitle(\"Predicted Probability\")\r\ngraph export \"$figures/predicted_probs.pdf\", replace\r\n```\r\n\r\n**Interpreting Interactions in Nonlinear Models:**\r\n```stata\r\n* Estimate model with interaction\r\nlogit outcome c.treatment##c.moderator control1, vce(cluster cluster_var)\r\n\r\n* First differences: effect of treatment at different moderator levels\r\nmargins, dydx(treatment) at(moderator=(1 2 3 4 5))\r\nmarginsplot, recast(line) recastci(rarea) ///\r\n    title(\"Marginal Effect of Treatment by Moderator\") ///\r\n    yline(0, lpattern(dash))\r\ngraph export \"$figures/interaction_margins.pdf\", replace\r\n\r\n* Second differences: how the effect changes\r\nmargins, dydx(treatment) at(moderator=(1 5)) contrast(atcontrast(r))\r\n```\r\n\r\n**Model Justification Paragraph Template:**\r\n```markdown\r\nWe use [SPECIFIC MODEL] to model [OUTCOME] because [PROPERTY OF OUTCOME].\r\nWe chose [THIS MODEL] over [ALTERNATIVE] because [DIAGNOSTIC TEST RESULT].\r\n[If relevant: We tested the [KEY ASSUMPTION] using [TEST NAME] and found\r\n[RESULT].]\r\n```\r\n\r\n**Key Rules:**\r\n- Report AMEs, not just odds ratios or log-odds\r\n- Show predicted probabilities for substantive scenarios\r\n- For interactions: show first differences (group comparisons) and second differences (how gaps change)\r\n- Never interpret main effect coefficients when interactions are present (those are conditional effects)\r\n- Always include confidence intervals\r\n\r\n### 5. Check Model Assumptions\r\n\r\nRun diagnostics appropriate to the method:\r\n\r\n**For OLS/Fixed Effects:**\r\n```stata\r\n* After estimation, check residuals\r\npredict resid, residuals\r\nhistogram resid, normal\r\ngraph export \"$figures/residuals.pdf\", replace\r\n\r\n* VIF for multicollinearity (run without FE)\r\nquietly reg outcome treatment control1 control2\r\nvif\r\n```\r\n\r\n**For Logistic Regression:**\r\n```stata\r\n* Model fit\r\nestat classification  // Classification table\r\nlroc  // ROC curve (AUC should be > 0.7)\r\ngraph export \"$figures/roc.pdf\", replace\r\n\r\n* Report pseudo-R with context\r\n* Note: pseudo-R values 0.10-0.20 often indicate reasonable fit\r\nestat ic  // AIC/BIC\r\n```\r\n\r\n**For Count Models (Poisson/Negative Binomial):**\r\n```stata\r\n* Test for overdispersion after Poisson\r\npoisson count treatment controls\r\nestat gof  // Deviance goodness-of-fit test\r\n\r\n* If overdispersed, use negative binomial:\r\nnbreg count treatment controls, vce(cluster cluster_var)\r\n\r\n* Compare AIC\r\nquietly poisson count treatment controls\r\nestimates store poisson_m\r\nquietly nbreg count treatment controls\r\nestimates store nbreg_m\r\nlrtest poisson_m nbreg_m  // LR test for alpha = 0\r\n```\r\n\r\n**For Ordered Logit:**\r\n```stata\r\n* Test proportional odds assumption\r\nologit outcome treatment controls\r\nbrant  // Brant test (if installed: ssc install brant)\r\n\r\n* If violated, use generalized ordered logit:\r\ngologit2 outcome treatment controls, autofit\r\n```\r\n\r\n**For DiD/Event Study:**\r\n```stata\r\n* Event study with csdid\r\ncsdid outcome, ivar(id) time(year) gvar(first_treat) notyet\r\ncsdid_estat event\r\ncsdid_plot, title(\"Event Study\")\r\ngraph export \"$figures/event_study.pdf\", replace\r\n```\r\n\r\n**For IV:**\r\n```stata\r\n* First stage F-statistic\r\nivreg2 outcome (endogenous = instrument) controls, first robust\r\n* Check F > 10 (or use effective F)\r\n\r\n* Display first stage\r\nestat firststage\r\n```\r\n\r\n### 5. Visualize Key Results\r\n\r\nCreate figures for the main findings:\r\n\r\n**Coefficient plot:**\r\n```stata\r\ncoefplot m4, drop(_cons) xline(0) ///\r\n    title(\"Treatment Effect Estimate\") ///\r\n    xtitle(\"Coefficient\") ytitle(\"\")\r\ngraph export \"$figures/coefplot.pdf\", replace\r\n```\r\n\r\n**Event study plot:**\r\n```stata\r\n* After csdid\r\ncsdid_plot, ///\r\n    title(\"Dynamic Treatment Effects\") ///\r\n    xtitle(\"Time Relative to Treatment\") ///\r\n    ytitle(\"Coefficient\")\r\ngraph export \"$figures/event_study.pdf\", replace\r\n```\r\n\r\n**Marginal effects (for interactions):**\r\n```stata\r\n* If model has interactions\r\nmargins, dydx(treatment) at(moderator=(0 1))\r\nmarginsplot, ///\r\n    title(\"Treatment Effect by Moderator\") ///\r\n    xtitle(\"Moderator\") ytitle(\"Marginal Effect\")\r\ngraph export \"$figures/margins.pdf\", replace\r\n```\r\n\r\n## Output: Results Report\r\n\r\nCreate a results report (`memos/phase3-results-report.md`):\r\n\r\n```markdown\r\n# Main Results Report\r\n\r\n## Summary of Findings\r\n\r\n**Main estimate**: [interpretation in words]\r\n\r\nThe preferred specification (Model X) shows that [treatment] is associated with\r\na [magnitude] [direction] in [outcome]. This effect is [statistically significant\r\nat the X% level / not statistically significant].\r\n\r\n## Results Table\r\n\r\n[Reference Table 2 or include formatted output]\r\n\r\n## Interpretation\r\n\r\n### Magnitude\r\n- Point estimate: [value]\r\n- Units: [what this means]\r\n- Context: [comparison to mean/SD/other benchmark]\r\n\r\n### Precision\r\n- Standard error: [value]\r\n- 95% CI: [lower, upper]\r\n- This is [precise/noisy] because [reason]\r\n\r\n### Stability\r\n- The estimate [is stable / changes] across specifications\r\n- Adding controls [increases/decreases/doesn't change] the estimate\r\n- This suggests [interpretation of stability pattern]\r\n\r\n## Diagnostic Checks\r\n- [Results of assumption tests]\r\n- [Any concerns raised]\r\n\r\n## Visualizations\r\n- Figure X: [description]\r\n- Figure Y: [description]\r\n\r\n## Preliminary Assessment\r\n- These results [support / do not support / partially support] the hypothesis\r\n- Key caveat: [main limitation]\r\n- Next step: robustness checks in Phase 4\r\n\r\n## Questions for User\r\n- [Any interpretive questions]\r\n- [Should we proceed to robustness?]\r\n```\r\n\r\n## When You're Done\r\n\r\nReturn a summary to the orchestrator that includes:\r\n1. The main estimate and its interpretation\r\n2. Whether the effect is statistically significant\r\n3. Whether results are stable across specifications\r\n4. Any diagnostic concerns\r\n5. Questions for the user\r\n\r\n**Do not proceed to Phase 4 until the user reviews the main results.**\r\n",
        "plugins/stata-analyst/skills/stata-analyst/phases/phase4-robustness.md": "# Phase 4: Robustness & Sensitivity\r\n\r\nYou are executing Phase 4 of a statistical analysis in Stata. Your goal is to stress-test the main findings through robustness checks and sensitivity analysis.\r\n\r\n## Why This Phase Matters\r\n\r\nMain results are only as credible as their robustness. Reviewers will ask: \"How do you know this isn't driven by [X]?\" This phase pre-empts those questions and honestly assesses the fragility of the findings.\r\n\r\n## Technique Guides\r\n\r\n**Consult these guides** in `stata-statistical-techniques/` for robustness code patterns:\r\n\r\n| Topic | Guide |\r\n|-------|-------|\r\n| DiD robustness, Event studies | `01_core_econometrics.md` |\r\n| Bootstrap, Wild cluster | `02_survey_resampling.md` |\r\n| Matching diagnostics | `01_core_econometrics.md` Section 5 |\r\n| Post-estimation tests | `07_postestimation_reporting.md` |\r\n\r\n## Your Tasks\r\n\r\n### 1. Alternative Specifications\r\n\r\nRun the pre-specified alternatives from Phase 2:\r\n\r\n**Different control sets:**\r\n```stata\r\n* Minimal controls\r\nreghdfe outcome treatment, absorb(id year) cluster(cluster_var)\r\nestimates store robust_minimal\r\n\r\n* Extended controls\r\nreghdfe outcome treatment extra1 extra2 extra3, absorb(id year) cluster(cluster_var)\r\nestimates store robust_extended\r\n\r\n* Different functional form\r\ngen log_outcome = log(outcome)\r\nreghdfe log_outcome treatment control1 control2, absorb(id year) cluster(cluster_var)\r\nestimates store robust_log\r\n```\r\n\r\n**Different fixed effects:**\r\n```stata\r\n* More demanding FE structure\r\nreghdfe outcome treatment, absorb(id#year) cluster(cluster_var)\r\nestimates store robust_fe1\r\n\r\n* Region-by-year FE\r\nreghdfe outcome treatment, absorb(region#year id) cluster(cluster_var)\r\nestimates store robust_fe2\r\n```\r\n\r\n**Different standard errors:**\r\n```stata\r\n* Compare clustering levels\r\nreghdfe outcome treatment control1 control2, absorb(id year) cluster(id)\r\nestimates store se_id\r\n\r\nreghdfe outcome treatment control1 control2, absorb(id year) cluster(state)\r\nestimates store se_state\r\n\r\nreghdfe outcome treatment control1 control2, absorb(id year) cluster(id year)\r\nestimates store se_twoway\r\n```\r\n\r\n### 2. Placebo Tests\r\n\r\n**Pre-treatment effects (for DiD/Event Study):**\r\n```stata\r\n* Should see no effect before treatment\r\npreserve\r\nkeep if year < treatment_year\r\nreghdfe outcome fake_treatment, absorb(id year) cluster(id)\r\nestimates store placebo_pre\r\nrestore\r\n```\r\n\r\n**Fake treatment timing:**\r\n```stata\r\n* Assign treatment X years earliershould find no effect\r\ngen fake_treated = treated_post & year >= (treatment_year - 3)\r\nreghdfe outcome fake_treated, absorb(id year) cluster(id)\r\nestimates store placebo_timing\r\n```\r\n\r\n**Outcome that shouldn't be affected:**\r\n```stata\r\n* If treatment affects X, it shouldn't affect unrelated Y\r\nreghdfe unrelated_outcome treatment control1 control2, absorb(id year) cluster(id)\r\nestimates store placebo_outcome\r\n```\r\n\r\n### 3. Wild Cluster Bootstrap\r\n\r\nFor designs with few clusters (<50):\r\n\r\n```stata\r\n* After main estimation\r\nreghdfe outcome treatment control1 control2, absorb(id year) cluster(state)\r\n\r\n* Wild cluster bootstrap\r\nboottest treatment, cluster(state) reps(999) nograph\r\n* Store p-value for reporting\r\n```\r\n\r\n### 4. Missing Data Assessment\r\n\r\nBefore running sensitivity analyses, document and address missing data:\r\n\r\n**Document Missingness:**\r\n```stata\r\n* Overall missingness rates\r\nmisstable summarize\r\nmisstable patterns\r\n\r\n* Missingness by key variables\r\nbysort treatment: misstable summarize outcome control1 control2\r\n```\r\n\r\n**Test for MCAR/MAR:**\r\n```stata\r\n* Little's MCAR test (if installed: ssc install mcartest)\r\nmcartest outcome control1 control2\r\n\r\n* Visualize missingness patterns\r\nmisstable patterns, frequency\r\n```\r\n\r\n**Multiple Imputation (if substantial missingness):**\r\n```stata\r\n* Use adequate number of imputations (m  20, preferably  50)\r\nmi set wide\r\nmi register imputed outcome control1 control2\r\nmi impute chained (regress) outcome control1 control2 = treatment, add(50) rseed(12345)\r\n\r\n* Check imputation diagnostics\r\nmi xeq: summarize outcome control1 control2\r\n\r\n* Run analysis on imputed datasets and pool\r\nmi estimate: reghdfe outcome treatment control1 control2, absorb(id year) cluster(cluster_var)\r\n\r\n* Include auxiliary variables to strengthen MAR assumption\r\n* These are variables that predict missingness or the outcome\r\n```\r\n\r\n**Compare Missing Data Approaches:**\r\n```stata\r\n* Store estimates for comparison\r\n* Complete case\r\nreghdfe outcome treatment control1 control2, absorb(id year) cluster(cluster_var)\r\nestimates store complete_case\r\n\r\n* Multiple imputation\r\nmi estimate: reghdfe outcome treatment control1 control2, absorb(id year) cluster(cluster_var)\r\nestimates store mi_model\r\n\r\n* Create comparison table\r\nesttab complete_case mi_model using \"$tables/missing_data_sensitivity.rtf\", replace ///\r\n    se star(* 0.10 ** 0.05 *** 0.01) ///\r\n    mtitles(\"Complete Case\" \"Multiple Imputation\") ///\r\n    title(\"Sensitivity to Missing Data Treatment\")\r\n```\r\n\r\n**Report in Methods Section:**\r\n```markdown\r\n[X]% of observations were missing on [variable]. We tested for patterns\r\nof missingness and found [MCAR/MAR/evidence of MNAR]. Our primary analysis\r\nuses [complete case / multiple imputation with m = X imputations].\r\nSensitivity analyses comparing complete case and multiple imputation show\r\n[results are robust / estimates differ by X].\r\n```\r\n\r\n### 5. Panel/Longitudinal Data Robustness (If Applicable)\r\n\r\n**Attrition Analysis:**\r\n```stata\r\n* Document attrition rates by wave\r\ntab wave, sum(n_obs)\r\n\r\n* Test if attrition is related to treatment or outcomes\r\ngen dropped_out = (next_wave_outcome == .)\r\nlogit dropped_out treatment baseline_outcome covariates, vce(robust)\r\n```\r\n\r\n**Inverse Probability Weighting for Selection:**\r\n```stata\r\n* Estimate selection weights\r\nlogit observed treatment covariates\r\npredict ps, pr\r\ngen ipw = 1 / ps\r\n\r\n* Apply weights in main analysis\r\nreghdfe outcome treatment control1 control2 [pw=ipw], absorb(id year) cluster(cluster_var)\r\nestimates store robust_ipw\r\n```\r\n\r\n**Fixed vs Random Effects:**\r\n```stata\r\n* Hausman test\r\nxtreg outcome treatment covariates, fe\r\nestimates store fe\r\nxtreg outcome treatment covariates, re\r\nestimates store re\r\nhausman fe re  // p < 0.05 suggests FE preferred\r\n\r\n* Within-between decomposition (correlated random effects)\r\n* Create between-unit and within-unit deviations\r\nbysort id: egen treatment_mean = mean(treatment)\r\ngen treatment_within = treatment - treatment_mean\r\n\r\nxtreg outcome treatment_within treatment_mean covariates, re\r\n```\r\n\r\n### 6. Sensitivity Analysis\r\n\r\n**Sensitivity to outliers:**\r\n```stata\r\n* Winsorize extreme values\r\nwinsor2 outcome, cuts(1 99) suffix(_w)\r\nreghdfe outcome_w treatment control1 control2, absorb(id year) cluster(cluster_var)\r\nestimates store robust_winsor\r\n\r\n* Drop extreme observations\r\nsummarize outcome, detail\r\ndrop if outcome < r(p1) | outcome > r(p99)\r\nreghdfe outcome treatment control1 control2, absorb(id year) cluster(cluster_var)\r\nestimates store robust_trim\r\n```\r\n\r\n**Sensitivity to sample restrictions:**\r\n```stata\r\n* Different time periods\r\nreghdfe outcome treatment control1 control2 if year <= 2015, ///\r\n    absorb(id year) cluster(cluster_var)\r\nestimates store robust_early\r\n\r\nreghdfe outcome treatment control1 control2 if year > 2015, ///\r\n    absorb(id year) cluster(cluster_var)\r\nestimates store robust_late\r\n\r\n* Excluding specific units\r\nreghdfe outcome treatment control1 control2 if !outlier_unit, ///\r\n    absorb(id year) cluster(cluster_var)\r\nestimates store robust_exclude\r\n```\r\n\r\n### 5. Subgroup Analysis\r\n\r\nRun pre-specified heterogeneity analyses:\r\n\r\n```stata\r\n* By group (separate regressions)\r\nreghdfe outcome treatment control1 control2 if subgroup == 1, ///\r\n    absorb(id year) cluster(cluster_var)\r\nestimates store het_group1\r\n\r\nreghdfe outcome treatment control1 control2 if subgroup == 0, ///\r\n    absorb(id year) cluster(cluster_var)\r\nestimates store het_group2\r\n\r\n* Interaction approach (preferred)\r\nreghdfe outcome c.treatment##i.subgroup control1 control2, ///\r\n    absorb(id year) cluster(cluster_var)\r\nestimates store het_interact\r\n\r\n* Test difference\r\ntest 1.subgroup#c.treatment\r\n```\r\n\r\n### 6. Method-Specific Diagnostics\r\n\r\n**For DiD with staggered treatment:**\r\n```stata\r\n* Check for heterogeneous treatment effects\r\ncsdid outcome, ivar(id) time(year) gvar(first_treat) notyet\r\ncsdid_estat event\r\n\r\n* Compare to traditional TWFE\r\nreghdfe outcome treated, absorb(id year) cluster(id)\r\n* Note any differences\r\n```\r\n\r\n**For IV:**\r\n```stata\r\n* Weak instrument test\r\nivreg2 outcome (endogenous = instrument) controls, first robust\r\n* Check F > 10\r\n\r\n* Overidentification test (if multiple instruments)\r\nivreg2 outcome (endogenous = inst1 inst2) controls, robust\r\nestat overid\r\n```\r\n\r\n**For Matching:**\r\n```stata\r\n* Balance check after matching\r\npstest control1 control2 control3, both graph\r\ngraph export \"$figures/balance.pdf\", replace\r\n```\r\n\r\n### 7. Create Robustness Table\r\n\r\nCompile all robustness checks:\r\n\r\n```stata\r\nesttab m4 robust_minimal robust_extended robust_fe1 robust_winsor robust_early robust_late ///\r\n    using \"$tables/table3_robustness.rtf\", replace ///\r\n    se star(* 0.10 ** 0.05 *** 0.01) ///\r\n    keep(treatment) ///\r\n    mtitles(\"Main\" \"Minimal\" \"Extended\" \"Alt FE\" \"Winsor\" \"Pre-2015\" \"Post-2015\") ///\r\n    title(\"Robustness Checks\") ///\r\n    note(\"All models include unit and year fixed effects. SE clustered at [level].\")\r\n```\r\n\r\n## Output: Robustness Report\r\n\r\nCreate a robustness report (`memos/phase4-robustness-report.md`):\r\n\r\n```markdown\r\n# Robustness Report\r\n\r\n## Summary Assessment\r\n\r\nThe main findings are [robust / partially robust / not robust] to alternative specifications.\r\n\r\n## Alternative Specifications\r\n\r\n| Specification | Estimate | SE | Conclusion |\r\n|---------------|----------|-----|------------|\r\n| Main | X.XX | (X.XX) | - |\r\n| Minimal controls | X.XX | (X.XX) | [stable/different] |\r\n| Extended controls | X.XX | (X.XX) | [stable/different] |\r\n| Alt FE | X.XX | (X.XX) | [stable/different] |\r\n...\r\n\r\n## Placebo Tests\r\n\r\n| Test | Expected | Found | Pass? |\r\n|------|----------|-------|-------|\r\n| Pre-treatment | 0 | X.XX (p=X.XX) | [Yes/No] |\r\n| Fake timing | 0 | X.XX (p=X.XX) | [Yes/No] |\r\n| Unrelated outcome | 0 | X.XX (p=X.XX) | [Yes/No] |\r\n\r\n## Wild Cluster Bootstrap\r\n- Conventional p-value: [X.XX]\r\n- Bootstrap p-value: [X.XX]\r\n- Inference [changes / doesn't change]\r\n\r\n## Sensitivity Analysis\r\n\r\n### Outliers\r\n- Results [are / are not] sensitive to extreme values\r\n\r\n### Sample restrictions\r\n- Results [hold / change] in different subsamples\r\n\r\n## Subgroup Analysis\r\n\r\n| Subgroup | Estimate | SE | Different from main? |\r\n|----------|----------|-----|---------------------|\r\n| Group 1 | X.XX | (X.XX) | [Yes/No] |\r\n| Group 2 | X.XX | (X.XX) | [Yes/No] |\r\n\r\n## Method-Specific Diagnostics\r\n[Results of diagnostic tests]\r\n\r\n## Overall Assessment\r\n\r\n**Strengths:**\r\n- [What checks the results passed]\r\n\r\n**Concerns:**\r\n- [Any issues found]\r\n\r\n**Conclusion:**\r\nThe main findings [can / cannot] be considered robust because [reasoning].\r\n\r\n## Questions for User\r\n- [Any interpretive questions about robustness]\r\n```\r\n\r\n## When You're Done\r\n\r\nReturn a summary to the orchestrator that includes:\r\n1. Overall robustness assessment\r\n2. Which checks passed/failed\r\n3. Wild bootstrap results (if applicable)\r\n4. Any concerns about the findings\r\n5. Questions for the user\r\n\r\n**Do not proceed to Phase 5 until the user reviews the robustness assessment.**\r\n",
        "plugins/stata-analyst/skills/stata-analyst/phases/phase5-output.md": "# Phase 5: Output & Interpretation\r\n\r\nYou are executing Phase 5 of a statistical analysis in Stata. Your goal is to produce publication-ready outputs and synthesize the analysis into a coherent narrative.\r\n\r\n## Why This Phase Matters\r\n\r\nAnalysis isn't complete until it's communicated. This phase transforms results into tables, figures, and text that can appear in a journal article. Good output is accurate, clear, and tells a story.\r\n\r\n## Technique Guides\r\n\r\n**Consult these guides** in `stata-statistical-techniques/` for output code patterns:\r\n\r\n| Topic | Guide |\r\n|-------|-------|\r\n| Tables (esttab, estout) | `04_visualization.md` |\r\n| Figures (coefplot, graphs) | `04_visualization.md` |\r\n| Table 1, Descriptives | `07_postestimation_reporting.md` |\r\n| Project structure, Master do-files | `05_best_practices.md` |\r\n| Complete project template | `99_default_journal_pipeline.md` |\r\n\r\n## Your Tasks\r\n\r\n### 1. Finalize Tables\r\n\r\n**Table 1: Descriptive Statistics**\r\n```stata\r\n* Summary statistics\r\nestpost summarize outcome treatment control1 control2\r\nesttab using \"$tables/table1_descriptives.rtf\", replace ///\r\n    cells(\"count(fmt(0)) mean(fmt(2)) sd(fmt(2)) min(fmt(2)) max(fmt(2))\") ///\r\n    title(\"Summary Statistics\") ///\r\n    note(\"Sample: [description]. Source: [data source].\")\r\n\r\n* Balance table (if applicable)\r\nestpost ttest outcome control1 control2, by(treatment)\r\nesttab using \"$tables/table1_balance.rtf\", replace ///\r\n    cells(\"mu_1(fmt(2)) mu_2(fmt(2)) b(fmt(2) star)\") ///\r\n    collabels(\"Control\" \"Treated\" \"Difference\") ///\r\n    star(* 0.10 ** 0.05 *** 0.01) ///\r\n    title(\"Balance Across Treatment Groups\")\r\n```\r\n\r\n**Table 2: Main Results**\r\n```stata\r\nesttab m1 m2 m3 m4 using \"$tables/table2_main.rtf\", replace ///\r\n    se star(* 0.10 ** 0.05 *** 0.01) ///\r\n    keep(treatment) ///\r\n    coeflabels(treatment \"Treatment Effect\") ///\r\n    stats(N r2_a, labels(\"Observations\" \"Adj. R-squared\") fmt(0 3)) ///\r\n    mtitles(\"(1)\" \"(2)\" \"(3)\" \"(4)\") ///\r\n    title(\"Effect of [Treatment] on [Outcome]\") ///\r\n    addnotes(\"Standard errors clustered at [level] in parentheses.\" ///\r\n             \"All models include [FE description].\" ///\r\n             \"* p<0.1, ** p<0.05, *** p<0.01\")\r\n\r\n* LaTeX version\r\nesttab m1 m2 m3 m4 using \"$tables/table2_main.tex\", replace ///\r\n    se star(* 0.10 ** 0.05 *** 0.01) booktabs ///\r\n    keep(treatment) ///\r\n    stats(N r2_a, labels(\"Observations\" \"Adj. R-squared\") fmt(0 3))\r\n```\r\n\r\n**Table 3: Robustness**\r\n```stata\r\nesttab main robust1 robust2 robust3 robust4 using \"$tables/table3_robustness.rtf\", replace ///\r\n    se star(* 0.10 ** 0.05 *** 0.01) ///\r\n    keep(treatment) ///\r\n    mtitles(\"Main\" \"Alt 1\" \"Alt 2\" \"Alt 3\" \"Alt 4\") ///\r\n    title(\"Robustness Checks\") ///\r\n    note(\"See notes to Table 2.\")\r\n```\r\n\r\n### 2. Create Publication Figures\r\n\r\n**Figure 1: Trends (for DiD)**\r\n```stata\r\npreserve\r\ncollapse (mean) mean_outcome=outcome (sd) sd_outcome=outcome (count) n=outcome, ///\r\n    by(year treatment_group)\r\ngen se = sd_outcome / sqrt(n)\r\ngen ci_low = mean_outcome - 1.96*se\r\ngen ci_high = mean_outcome + 1.96*se\r\n\r\ntwoway (rarea ci_low ci_high year if treatment_group==0, color(blue%20)) ///\r\n       (rarea ci_low ci_high year if treatment_group==1, color(red%20)) ///\r\n       (line mean_outcome year if treatment_group==0, lcolor(blue) lwidth(medium)) ///\r\n       (line mean_outcome year if treatment_group==1, lcolor(red) lwidth(medium)) ///\r\n       (scatter mean_outcome year if treatment_group==0, mcolor(blue)) ///\r\n       (scatter mean_outcome year if treatment_group==1, mcolor(red)), ///\r\n       xline(`treatment_year', lpattern(dash) lcolor(gray)) ///\r\n       legend(order(3 \"Control\" 4 \"Treated\") rows(1) position(6)) ///\r\n       xtitle(\"Year\") ytitle(\"Outcome\") ///\r\n       title(\"\") ///\r\n       graphregion(color(white)) bgcolor(white)\r\n\r\ngraph export \"$figures/figure1_trends.pdf\", replace\r\nrestore\r\n```\r\n\r\n**Figure 2: Event Study**\r\n```stata\r\n* After csdid estimation\r\ncsdid_plot, ///\r\n    style(rcap) ///\r\n    title(\"\") ///\r\n    xtitle(\"Time Relative to Treatment\") ///\r\n    ytitle(\"Coefficient Estimate\") ///\r\n    graphregion(color(white)) bgcolor(white)\r\n\r\ngraph export \"$figures/figure2_eventstudy.pdf\", replace\r\n```\r\n\r\n**Figure 3: Coefficient Plot**\r\n```stata\r\ncoefplot m4, ///\r\n    keep(treatment) ///\r\n    xline(0, lpattern(dash) lcolor(gray)) ///\r\n    title(\"\") ///\r\n    xtitle(\"Coefficient Estimate\") ///\r\n    graphregion(color(white)) bgcolor(white)\r\n\r\ngraph export \"$figures/figure3_coefplot.pdf\", replace\r\n```\r\n\r\n### 3. Write Results Narrative\r\n\r\nDraft the key paragraphs for the results section:\r\n\r\n**Main effect paragraph:**\r\n> Table 2 presents estimates of the effect of [treatment] on [outcome].\r\n> Column (1) shows the baseline relationship without controls.\r\n> Column (4), our preferred specification, includes [unit] and [time] fixed effects\r\n> and clusters standard errors at the [level] level. We find that [treatment]\r\n> [increases/decreases] [outcome] by [X] [units], significant at the [Y]% level\r\n> (95% CI: [lower, upper]). This represents a [Z]% change relative to the\r\n> pre-treatment mean of [mean].\r\n\r\n**Robustness paragraph:**\r\n> Table 3 demonstrates that this finding is robust to alternative specifications.\r\n> The point estimate remains [stable/similar] when we [change 1], [change 2], and\r\n> [change 3]. The effect is [somewhat/not] sensitive to [what]. The wild cluster\r\n> bootstrap p-value of [X] confirms that inference is [robust/sensitive] to\r\n> the number of clusters.\r\n\r\n**Heterogeneity paragraph (if applicable):**\r\n> We examine heterogeneity in treatment effects across [subgroups].\r\n> Table X shows that the effect is [larger/smaller] for [group 1]\r\n> ([estimate]) compared to [group 2] ([estimate]). This difference is\r\n> [statistically significant / not statistically significant] (p = [value]).\r\n\r\n### 4. Survey Data Methods Section (If Applicable)\r\n\r\nFor survey-based analyses, address the five survey methodology deliverables:\r\n\r\n**1. Sampling Frame Description:**\r\n```markdown\r\nData come from the [SURVEY NAME], a [DESIGN TYPE] of [POPULATION].\r\nThe sampling frame is [DESCRIPTION]. The target population is [WHO].\r\n[Exclusions]: We exclude [categories] because [reason].\r\n```\r\n\r\n**2. Response Rate and Nonresponse:**\r\n```markdown\r\nThe response rate was [X]% (calculated as [METHOD: AAPOR RR1/RR3/etc.]).\r\nWe compared respondents to [population benchmark / nonrespondents on X variables]\r\nand found [no significant differences / differences on X that we address through Y].\r\n```\r\n\r\n**3. Weighting Justification:**\r\n```markdown\r\nWe apply [WEIGHT TYPE: post-stratification / raking / none] weights to adjust for\r\n[FACTORS]. Weights are provided by [SOURCE] and calibrated to [BENCHMARKS].\r\n[Alternative: We do not apply weights because {justification}.]\r\n```\r\n\r\n**4. Survey Design Acknowledgment:**\r\n```stata\r\n* Define survey design\r\nsvyset [pw=weight], strata(stratum) psu(psu)\r\n\r\n* All analyses use survey-adjusted estimates\r\nsvy: mean outcome\r\nsvy: regress outcome treatment control1 control2\r\n```\r\n\r\n**5. Population Inference Boundaries:**\r\n```markdown\r\nOur results generalize to [POPULATION] during [TIME PERIOD]. We cannot\r\nspeak to [EXCLUDED GROUPS / OTHER TIME PERIODS] because [REASON].\r\n```\r\n\r\n### 5. Document Limitations\r\n\r\nIdentify and articulate limitations honestly:\r\n\r\n```markdown\r\n## Limitations\r\n\r\n1. **[Identification limitation]**: Our identification strategy relies on\r\n   [assumption]. While we provide evidence supporting this assumption through\r\n   [tests], we cannot definitively rule out [threat].\r\n\r\n2. **[External validity]**: Our sample consists of [description]. Results may\r\n   not generalize to [other contexts] because [reason].\r\n\r\n3. **[Measurement]**: [Variable] is measured using [method], which may\r\n   [limitation]. We address this by [mitigation] but acknowledge [remaining concern].\r\n\r\n4. **[Data limitation]**: We lack data on [variable], which could [potential issue].\r\n   Our robustness checks in Table X suggest this is [unlikely to/may] affect our\r\n   conclusions.\r\n```\r\n\r\n### 5. Create Replication Package\r\n\r\nPrepare materials for reproducibility:\r\n\r\n```stata\r\n* Master do-file header\r\n* ============================================================\r\n* Replication Code for \"[Paper Title]\"\r\n* Authors: [Names]\r\n* Date: [Date]\r\n*\r\n* This script reproduces all tables and figures in the paper.\r\n* Runtime: approximately [X] minutes\r\n*\r\n* Requirements:\r\n* Stata version: 15+\r\n* Packages: reghdfe, estout, coefplot, csdid\r\n* ============================================================\r\n\r\nversion 15\r\nclear all\r\nset more off\r\n\r\n* Install required packages (uncomment if needed)\r\n* ssc install reghdfe\r\n* ssc install estout\r\n* ssc install coefplot\r\n* ssc install csdid\r\n\r\n* Set root path (CHANGE THIS)\r\nglobal root \"[path to replication folder]\"\r\n\r\n* Derived paths\r\nglobal code \"$root/code\"\r\nglobal data \"$root/data\"\r\nglobal raw \"$data/raw\"\r\nglobal clean \"$data/clean\"\r\nglobal tables \"$root/output/tables\"\r\nglobal figures \"$root/output/figures\"\r\nglobal logs \"$root/logs\"\r\n\r\n* Start log\r\nlog using \"$logs/replication_log.txt\", text replace\r\n\r\n* Run analysis\r\ndo \"$code/01_clean_data.do\"\r\ndo \"$code/02_descriptives.do\"\r\ndo \"$code/03_main_analysis.do\"\r\ndo \"$code/04_robustness.do\"\r\ndo \"$code/05_figures.do\"\r\n\r\n* Report completion\r\ndisplay \"Replication complete: \" c(current_date) \" \" c(current_time)\r\ndisplay \"Stata version: \" c(stata_version)\r\n\r\nlog close\r\n```\r\n\r\n## Output: Final Report\r\n\r\nCreate the final synthesis (`memos/phase5-final-report.md`):\r\n\r\n```markdown\r\n# Analysis Summary\r\n\r\n## Key Finding\r\n[One sentence summary of the main result]\r\n\r\n## Main Result\r\n- **Effect size**: [estimate with CI]\r\n- **Significance**: [p-value or significance level]\r\n- **Interpretation**: [what this means substantively]\r\n\r\n## Robustness Assessment\r\n- The finding [is/is not] robust to [list of checks]\r\n- Main concerns: [if any]\r\n\r\n## Output Files Created\r\n\r\n### Tables\r\n- `table1_descriptives.rtf`: Summary statistics\r\n- `table2_main.rtf`: Main results\r\n- `table3_robustness.rtf`: Robustness checks\r\n\r\n### Figures\r\n- `figure1_trends.pdf`: Pre/post trends\r\n- `figure2_eventstudy.pdf`: Event study\r\n- `figure3_coefplot.pdf`: Coefficient plot\r\n\r\n### Replication Materials\r\n- `00_master.do`: Master script\r\n- `code/`: All analysis do-files\r\n- `data/clean/`: Analysis datasets\r\n- `logs/`: Stata log files\r\n\r\n## Results Narrative\r\n[Draft paragraphs for the paper]\r\n\r\n## Limitations\r\n[Honest assessment of limitations]\r\n\r\n## Conclusion\r\n[What can and cannot be concluded from this analysis]\r\n```\r\n\r\n### 7. Pre-Submission Checklist\r\n\r\nBefore finalizing, verify the analysis meets publication standards:\r\n\r\n**Minimum Standard (Required):**\r\n- [ ] All variables clearly defined with units and coding\r\n- [ ] Sample size and any exclusions documented\r\n- [ ] Main coefficient table includes SEs and significance levels\r\n- [ ] Standard error type specified (robust, clustered at X level, etc.)\r\n- [ ] At least one robustness check reported\r\n- [ ] Limitations section acknowledges main threats to validity\r\n\r\n**Strong Standard (Competitive for top journals):**\r\n- [ ] Descriptive statistics table with means, SDs, and sample sizes\r\n- [ ] Multiple robustness specifications in appendix\r\n- [ ] Effect sizes interpreted substantively (not just \"significant\")\r\n- [ ] For nonlinear models: AMEs or predicted probabilities reported\r\n- [ ] Wild cluster bootstrap for few-cluster designs\r\n- [ ] Missing data approach documented and defended\r\n- [ ] Visualization of key results (event study, coefficient plot, etc.)\r\n- [ ] Replication code and data availability statement\r\n\r\n**Exemplary Standard (Model for the field):**\r\n- [ ] Pre-registration referenced (if applicable)\r\n- [ ] Multiple identification strategies compared\r\n- [ ] Heterogeneity analysis with theoretical motivation\r\n- [ ] Mechanism analysis or mediation tests\r\n- [ ] Power analysis or minimum detectable effects\r\n- [ ] Bound analysis for worst-case scenarios\r\n- [ ] Complete replication package with README\r\n\r\n**Language Checklist:**\r\n- [ ] Causal language only used with appropriate identification strategy\r\n- [ ] Effect sizes interpreted relative to meaningful benchmarks\r\n- [ ] Confidence intervals reported, not just p-values\r\n- [ ] Scope conditions clearly stated\r\n- [ ] \"Significant\" refers to statistical significance (or avoid the term)\r\n\r\n## When You're Done\r\n\r\nReturn a summary to the orchestrator that includes:\r\n1. List of all tables and figures created\r\n2. The main finding in one sentence\r\n3. Key limitations\r\n4. Any remaining questions or concerns\r\n5. Confirmation that replication materials are ready\r\n6. Checklist tier achieved (minimum/strong/exemplary)\r\n\r\n**The analysis is now complete.** All materials should be ready for paper writing.\r\n",
        "plugins/stata-analyst/skills/stata-analyst/techniques/00_data_prep.md": "# Data Preparation in Stata\r\n\r\nMaking \"analysis-ready data\" a reproducible, auditable stage. All code tested on Stata 15+.\r\n\r\n---\r\n\r\n## 0. Setup and Conventions\r\n\r\n### Minimal Header Template\r\n\r\n```stata\r\n* ===========================================================================\r\n* Project: [Name]\r\n* Purpose: [Brief description]\r\n* Input: [raw data files]\r\n* Output: [clean data files]\r\n* ===========================================================================\r\n\r\nclear all\r\nset more off\r\nversion 15\r\nset seed 12345\r\n\r\n* Start log\r\nlog using \"logs/01_data_prep.log\", replace\r\n```\r\n\r\n### Naming Conventions\r\n\r\n- Variables: `snake_case` (e.g., `first_treatment_year`)\r\n- Files: descriptive, numbered (e.g., `01_import.do`, `02_clean.do`)\r\n- Always label variables after creating them\r\n\r\n---\r\n\r\n## 1. Import Patterns\r\n\r\n### Import CSV\r\n\r\n```stata\r\n* Basic import\r\nimport delimited \"data/raw/mydata.csv\", clear varnames(1)\r\n\r\n* Force all columns as strings initially (for cleaning)\r\nimport delimited \"data/raw/mydata.csv\", clear varnames(1) stringcols(_all)\r\n\r\n* Quick checks after import\r\ndescribe\r\ncompress\r\ncodebook, compact\r\n```\r\n\r\n### Import Excel\r\n\r\n```stata\r\n* Basic import with first row as variable names\r\nimport excel \"data/raw/mydata.xlsx\", sheet(\"Sheet1\") firstrow clear\r\n\r\n* Import specific cell range\r\nimport excel \"data/raw/mydata.xlsx\", cellrange(A2:F100) firstrow clear\r\n```\r\n\r\n### Reading Multiple Files (Loop)\r\n\r\n```stata\r\n* Append multiple CSV files from a folder\r\nclear\r\nlocal files: dir \"data/raw/\" files \"*.csv\"\r\nlocal first = 1\r\nforeach f of local files {\r\n    if `first' == 1 {\r\n        import delimited \"data/raw/`f'\", clear varnames(1)\r\n        local first = 0\r\n    }\r\n    else {\r\n        preserve\r\n        import delimited \"data/raw/`f'\", clear varnames(1)\r\n        tempfile temp\r\n        save `temp'\r\n        restore\r\n        append using `temp'\r\n    }\r\n}\r\n```\r\n\r\n### Never Edit Raw Data\r\n\r\n```stata\r\n* Always save a clean .dta immediately after import\r\nimport delimited \"data/raw/mydata.csv\", clear varnames(1)\r\nsave \"data/staging/mydata_imported.dta\", replace\r\n```\r\n\r\n---\r\n\r\n## 2. Merge, Append, and Reshape\r\n\r\n### Every Merge Gets an Assertion\r\n\r\n```stata\r\n* 1:1 merge\r\nuse \"data/master.dta\", clear\r\nmerge 1:1 id using \"data/using.dta\"\r\n\r\n* Required checks\r\ntab _merge\r\ncount if _merge == 1  // master only\r\ncount if _merge == 2  // using only\r\ncount if _merge == 3  // matched\r\n\r\n* Handle unmatched - be explicit about what you're dropping\r\nassert _merge != 2  // or: drop if _merge == 2 with comment explaining why\r\ndrop _merge\r\n```\r\n\r\n### Merge Patterns\r\n\r\n```stata\r\n* Many-to-one (e.g., individual-level to state-level)\r\nmerge m:1 state using \"data/state_controls.dta\"\r\n\r\n* One-to-many (less common; be careful)\r\nmerge 1:m household_id using \"data/household_members.dta\"\r\n```\r\n\r\n### Key Verification Before Merge\r\n\r\n```stata\r\n* Check that merge keys are unique where expected\r\nisid id                    // fails if id is not unique\r\nduplicates report id       // shows duplicate structure\r\nduplicates tag id, gen(dup)\r\ntab dup\r\ndrop dup\r\n```\r\n\r\n### Append Pattern\r\n\r\n```stata\r\n* Append datasets\r\nuse \"data/wave1.dta\", clear\r\nappend using \"data/wave2.dta\"\r\n\r\n* Track source\r\ngen wave = 1\r\nreplace wave = 2 if _n > [original N]\r\n```\r\n\r\n### Reshape Wide to Long\r\n\r\n```stata\r\n* Example: income2018 income2019 income2020 -> income with year variable\r\nclear\r\nset obs 20\r\ngen id = _n\r\ngen income2018 = rnormal(50000, 10000)\r\ngen income2019 = income2018 * 1.03 + rnormal(0, 1000)\r\ngen income2020 = income2019 * 1.02 + rnormal(0, 1000)\r\n\r\nreshape long income, i(id) j(year)\r\n```\r\n\r\n### Reshape Long to Wide\r\n\r\n```stata\r\nreshape wide income, i(id) j(year)\r\n```\r\n\r\n---\r\n\r\n## 3. Missing Data Audit\r\n\r\n### Missingness Summary\r\n\r\n```stata\r\n* Overall missing count\r\nmisstable summarize\r\n\r\n* Missing by variable with patterns\r\nmisstable patterns\r\n```\r\n\r\n### Missing by Group\r\n\r\n```stata\r\n* Count non-missing by treatment group\r\ntabstat x1 x2, by(treat) stat(n) nototal\r\n\r\n* Count missing manually\r\ncount if missing(x1) & treat == 0\r\ncount if missing(x1) & treat == 1\r\n```\r\n\r\n### Rules of Thumb\r\n\r\n- **Never silently drop observations** - always document exclusions\r\n- **Don't impute without justification** - report complete-case as baseline\r\n- **Check missingness by key subgroups** - differential attrition is a threat\r\n\r\n### Create Analysis Sample Flag\r\n\r\n```stata\r\n* Complete case indicator\r\ngen sample_main = !missing(y, x1, x2, x3)\r\ntab sample_main\r\n\r\n* Or define incrementally with documentation\r\ngen sample_main = 1\r\nreplace sample_main = 0 if missing(wage)\r\nreplace sample_main = 0 if missing(union)\r\nreplace sample_main = 0 if age < 25 | age > 55\r\n```\r\n\r\n---\r\n\r\n## 4. Recodes, Transforms, and Outliers\r\n\r\n### Standardize Special Missings\r\n\r\n```stata\r\n* Stata extended missing values: .a, .b, ... .z\r\nreplace income = .r if income == -99  // refused\r\nreplace income = .d if income == -88  // don't know\r\n```\r\n\r\n### Common Transforms\r\n\r\n```stata\r\n* Log transform\r\ngen ln_income = ln(income)\r\n\r\n* Handle zeros: ln(x + 1) or inverse hyperbolic sine\r\ngen ln_income_p1 = ln(income + 1)\r\ngen ihs_income = asinh(income)\r\n\r\n* Standardization (z-scores)\r\negen z_income = std(income)\r\nsummarize z_income  // mean = 0, sd = 1\r\n```\r\n\r\n### Create Categorical Variables with Labels\r\n\r\n```stata\r\n* Create categories\r\ngen price_cat = 1 if price < 5000\r\nreplace price_cat = 2 if price >= 5000 & price < 10000\r\nreplace price_cat = 3 if price >= 10000 & !missing(price)\r\n\r\n* Add labels\r\nlabel define price_lbl 1 \"Low\" 2 \"Medium\" 3 \"High\"\r\nlabel values price_cat price_lbl\r\ntab price_cat\r\n```\r\n\r\n### Winsorization (Optional)\r\n\r\n```stata\r\n* Winsorize at 1st and 99th percentiles\r\negen p1 = pctile(income), p(1)\r\negen p99 = pctile(income), p(99)\r\ngen income_wins = income\r\nreplace income_wins = p1 if income < p1\r\nreplace income_wins = p99 if income > p99 & !missing(income)\r\ndrop p1 p99\r\n```\r\n\r\n---\r\n\r\n## 5. Panel/Time Setup and Sanity Checks\r\n\r\n### Declare Panel Structure\r\n\r\n```stata\r\n* xtset for panel data\r\nxtset id time\r\nxtdescribe\r\n\r\n* tsset for time series (single unit)\r\ntsset time\r\n```\r\n\r\n### Check for Gaps\r\n\r\n```stata\r\n* After xtset, check balance\r\nxtdescribe\r\n\r\n* Manual gap check\r\nby id: gen gap = time - time[_n-1] if _n > 1\r\ntab gap  // should be all 1s if no gaps\r\n```\r\n\r\n### Verify Panel Structure\r\n\r\n```stata\r\n* Check unit counts and time coverage\r\ntab id\r\ntab time\r\ndistinct id\r\ndistinct time\r\n```\r\n\r\n### Treatment Timing Construction\r\n\r\n```stata\r\n* Create first treatment year variable\r\ngen treat_year = year if treatment == 1\r\nbysort id: egen first_treat = min(treat_year)\r\ndrop treat_year\r\n\r\n* Create relative time to treatment\r\ngen rel_time = year - first_treat\r\n```\r\n\r\n---\r\n\r\n## 6. Sample Construction Documentation\r\n\r\n### Document Every Restriction\r\n\r\n```stata\r\nsysuse nlsw88, clear\r\n\r\n* Track sample flow\r\ndisplay \"Initial N: \" _N\r\n\r\ndrop if missing(wage)\r\ndisplay \"After dropping missing wage: \" _N\r\n\r\ndrop if missing(union)\r\ndisplay \"After dropping missing union: \" _N\r\n\r\nkeep if age >= 25 & age <= 55\r\ndisplay \"After age restriction (25-55): \" _N\r\n\r\ngen sample_main = 1\r\ndisplay \"Final analysis sample: \" _N\r\n```\r\n\r\n### Save Sample Flow to File\r\n\r\n```stata\r\n* Create sample flow log\r\nfile open flow using \"$tables/sample_flow.txt\", write replace\r\nfile write flow \"Sample Construction\" _n\r\nfile write flow \"===================\" _n\r\nfile write flow \"Initial sample: 2,246\" _n\r\nfile write flow \"After dropping missing wage: 2,246\" _n\r\nfile write flow \"After dropping missing union: 1,878\" _n\r\nfile write flow \"Final analysis sample: 1,878\" _n\r\nfile close flow\r\n```\r\n\r\n---\r\n\r\n## 7. Output Artifacts\r\n\r\n### Standard Save Pattern\r\n\r\n```stata\r\n* Save clean analysis dataset\r\ncompress\r\nlabel data \"Analysis sample, created [date]\"\r\nsave \"$clean/analysis_sample.dta\", replace\r\n```\r\n\r\n### Export Codebook\r\n\r\n```stata\r\n* Save variable documentation\r\ncodebook, compact\r\nlog using \"$docs/codebook.log\", replace\r\ncodebook\r\nlog close\r\n```\r\n\r\n---\r\n\r\n## Quick Reference\r\n\r\n### Import Commands\r\n\r\n| Task | Command |\r\n|------|---------|\r\n| CSV | `import delimited \"file.csv\", clear varnames(1)` |\r\n| Excel | `import excel \"file.xlsx\", firstrow clear` |\r\n| Stata | `use \"file.dta\", clear` |\r\n\r\n### Merge Commands\r\n\r\n| Type | Command |\r\n|------|---------|\r\n| 1:1 | `merge 1:1 id using \"file.dta\"` |\r\n| m:1 | `merge m:1 group using \"file.dta\"` |\r\n| 1:m | `merge 1:m id using \"file.dta\"` |\r\n\r\n### Key Checks\r\n\r\n| Task | Command |\r\n|------|---------|\r\n| Unique ID | `isid id` |\r\n| Duplicates | `duplicates report id` |\r\n| Missing | `misstable summarize` |\r\n| Panel setup | `xtset id time` then `xtdescribe` |\r\n",
        "plugins/stata-analyst/skills/stata-analyst/techniques/00_index.md": "# Stata Statistical Techniques - Index\r\n\r\nQuick lookup for statistical methods in Stata. All code tested on Stata 15.1.\r\n\r\n---\r\n\r\n## Core Workflow\r\n\r\n**For most journal-style analyses, follow this sequence:**\r\n\r\n1. **Data Prep**  `00_data_prep.md`\r\n   Import, merge, clean, construct variables, save analysis sample\r\n\r\n2. **Descriptives**  `07_postestimation_reporting.md`\r\n   Table 1 (overall + by group), Figure 1 (trends or distributions)\r\n\r\n3. **Modeling**  `06_modeling_basics.md`\r\n   OLS, logit/probit, margins, interactions\r\n\r\n4. **Causal Methods** (if applicable)  `01_core_econometrics.md`\r\n   TWFE, DiD, IV, matching, RD\r\n\r\n5. **Output**  `04_visualization.md`, `07_postestimation_reporting.md`\r\n   Tables to RTF/TeX, figures to PNG/PDF\r\n\r\n6. **Pipeline Template**  `99_default_journal_pipeline.md`\r\n   Copy this structure for new projects\r\n\r\n---\r\n\r\n## File Guide\r\n\r\n| File | Topics |\r\n|------|--------|\r\n| `00_data_prep.md` | Import, merge, missing data, transforms, panel setup |\r\n| `01_core_econometrics.md` | TWFE, DiD, Event Studies, IV, Matching, Mediation, Standard Errors |\r\n| `02_survey_resampling.md` | Survey Weights, Bootstrap, Randomization Inference, Oaxaca |\r\n| `03_synthetic_control.md` | synth package for comparative case studies |\r\n| `04_visualization.md` | esttab tables, coefplot, graphs, summary statistics |\r\n| `05_best_practices.md` | Master do-files, path management, code organization |\r\n| `06_modeling_basics.md` | OLS, logit/probit, Poisson, margins, interactions |\r\n| `07_postestimation_reporting.md` | Estimates workflow, predicted values, diagnostics, Table 1 |\r\n| `99_default_journal_pipeline.md` | Complete project template, file naming, submission checklist |\r\n\r\n---\r\n\r\n## Quick Lookup by Method\r\n\r\n### Panel & Fixed Effects\r\n- **TWFE (reghdfe)**  `01_core_econometrics.md` Section 1\r\n\r\n### Difference-in-Differences\r\n- **Traditional DiD**  `01_core_econometrics.md` Section 2.1\r\n- **Callaway-Sant'Anna (csdid)**  `01_core_econometrics.md` Section 2.2\r\n- **Event Studies**  `01_core_econometrics.md` Section 3\r\n\r\n### Instrumental Variables\r\n- **Basic 2SLS (ivreg2)**  `01_core_econometrics.md` Section 4\r\n- **First-stage diagnostics**  `01_core_econometrics.md` Section 4.2\r\n- **Overidentification tests**  `01_core_econometrics.md` Section 4.3\r\n\r\n### Matching\r\n- **Propensity Score (psmatch2)**  `01_core_econometrics.md` Section 5.1\r\n- **Coarsened Exact (cem)**  `01_core_econometrics.md` Section 5.3\r\n- **Balance testing (pstest)**  `01_core_econometrics.md` Section 5.2\r\n\r\n### Standard Errors & Inference\r\n- **Clustered SEs**  `01_core_econometrics.md` Section 6.1\r\n- **Wild Cluster Bootstrap**  `01_core_econometrics.md` Section 6.2\r\n- **Randomization Inference**  `02_survey_resampling.md` Section 4\r\n\r\n### Survey Methods\r\n- **Survey design (svyset)**  `02_survey_resampling.md` Section 1\r\n- **Weighted estimation (svy:)**  `02_survey_resampling.md` Section 1\r\n- **Subpopulation analysis**  `02_survey_resampling.md` Section 1\r\n\r\n### Decomposition\r\n- **Oaxaca-Blinder**  `02_survey_resampling.md` Section 5\r\n\r\n### Synthetic Control\r\n- **synth package**  `03_synthetic_control.md`\r\n\r\n### Output & Visualization\r\n- **Regression tables (esttab)**  `04_visualization.md` Section 1\r\n- **Coefficient plots (coefplot)**  `04_visualization.md` Section 2\r\n- **Summary statistics**  `04_visualization.md` Section 3\r\n\r\n### Data Preparation\r\n- **Import CSV/Excel**  `00_data_prep.md` Section 1\r\n- **Merge patterns**  `00_data_prep.md` Section 2\r\n- **Missing data audit**  `00_data_prep.md` Section 3\r\n- **Reshape**  `00_data_prep.md` Section 2\r\n- **Panel setup (xtset)**  `00_data_prep.md` Section 5\r\n\r\n### Basic Modeling\r\n- **OLS with robust/clustered SEs**  `06_modeling_basics.md` Section 1\r\n- **Interactions and margins**  `06_modeling_basics.md` Section 2\r\n- **Logit/Probit**  `06_modeling_basics.md` Section 3\r\n- **Marginal effects (AME)**  `06_modeling_basics.md` Section 3\r\n- **Poisson/Negative Binomial**  `06_modeling_basics.md` Section 4\r\n- **Ordinal logit**  `06_modeling_basics.md` Section 5\r\n- **VIF and diagnostics**  `06_modeling_basics.md` Section 7\r\n\r\n### Post-Estimation\r\n- **Estimates store/restore**  `07_postestimation_reporting.md` Section 1\r\n- **Nested tables**  `07_postestimation_reporting.md` Section 2\r\n- **Predicted values**  `07_postestimation_reporting.md` Section 3\r\n- **Table 1 (descriptives)**  `07_postestimation_reporting.md` Section 5\r\n- **Balance tables**  `07_postestimation_reporting.md` Section 6\r\n\r\n### Reporting Checklists\r\n- **TWFE checklist**  `01_core_econometrics.md` Reporting Checklists\r\n- **DiD checklist**  `01_core_econometrics.md` Reporting Checklists\r\n- **IV checklist**  `01_core_econometrics.md` Reporting Checklists\r\n- **Matching checklist**  `01_core_econometrics.md` Reporting Checklists\r\n\r\n---\r\n\r\n## Package Quick Reference\r\n\r\n| Task | Package | Command |\r\n|------|---------|---------|\r\n| High-dim FE | `reghdfe` | `reghdfe y x, absorb(id year) cluster(id)` |\r\n| Modern DiD | `csdid` | `csdid y, ivar(id) time(year) gvar(gvar) notyet` |\r\n| DiD event study | `csdid_estat` | `csdid_estat event` |\r\n| IV estimation | `ivreg2` | `ivreg2 y (endog = iv), first robust` |\r\n| PSM | `psmatch2` | `psmatch2 treat x1 x2, outcome(y)` |\r\n| Balance test | `pstest` | `pstest x1 x2, both` |\r\n| CEM | `cem` | `cem x1 (bins) x2 (#5), treatment(treat)` |\r\n| Wild bootstrap | `boottest` | `boottest x, cluster(c) reps(999) nograph` |\r\n| Randomization | `ritest` | `ritest treat _b[treat], reps(500):` |\r\n| Oaxaca | `oaxaca` | `oaxaca y x1 x2, by(group)` |\r\n| Synth control | `synth` | `synth y x1 y(1985), trunit(1) trperiod(1990)` |\r\n| Tables | `estout` | `esttab m1 m2, se star(* .10 ** .05 *** .01)` |\r\n| Coef plots | `coefplot` | `coefplot, drop(_cons) xline(0)` |\r\n| Survey | built-in | `svyset psu [pw=wt], strata(strat)` |\r\n\r\n---\r\n\r\n## Built-in Datasets for Examples\r\n\r\n| Dataset | Command | Variables |\r\n|---------|---------|-----------|\r\n| auto | `sysuse auto` | price, mpg, weight, foreign |\r\n| nlsw88 | `sysuse nlsw88` | wage, union, age, grade, tenure |\r\n| nhanes2f | `webuse nhanes2f` | height, weight, age, sex, finalwgt |\r\n\r\n---\r\n\r\n## Version Notes\r\n\r\n- **Stata 15+**: All methods except rdrobust\r\n- **Stata 16+**: rdrobust (RD estimation)\r\n",
        "plugins/stata-analyst/skills/stata-analyst/techniques/01_core_econometrics.md": "# Core Econometrics in Stata\r\n\r\nPanel methods, causal inference, and standard errors. All code tested on Stata 15+.\r\n\r\n---\r\n\r\n## 1. Two-Way Fixed Effects (TWFE)\r\n\r\n### When to Use\r\n- Panel data with unit and time dimensions\r\n- Unobserved unit and time confounders\r\n- Consistent treatment timing (or verified no negative weights)\r\n\r\n### Basic TWFE with reghdfe\r\n\r\n```stata\r\n* Declare panel\r\nxtset id period\r\n\r\n* TWFE with unit and time fixed effects, clustered SEs\r\nreghdfe y x1 treat, absorb(id period) cluster(id)\r\n```\r\n\r\n### Two-Way Clustering\r\n\r\n```stata\r\n* Cluster by both unit and time\r\nreghdfe y x1 treat, absorb(id period) cluster(id period)\r\n```\r\n\r\n### Store and Compare Models\r\n\r\n```stata\r\n* Run models\r\nquietly reghdfe y x1 treat, absorb(id period) cluster(id)\r\nestimates store m1_oneway\r\n\r\nquietly reghdfe y x1 treat, absorb(id period) cluster(id period)\r\nestimates store m2_twoway\r\n\r\n* Comparison table\r\nesttab m1_oneway m2_twoway, se mtitle(\"One-way\" \"Two-way\") ///\r\n    star(* 0.10 ** 0.05 *** 0.01)\r\n```\r\n\r\n---\r\n\r\n## 2. Difference-in-Differences (DiD)\r\n\r\n### 2.1 Traditional DiD\r\n\r\n```stata\r\n* Create interaction term\r\ngen treat_post = treated_group * post_period\r\n\r\n* DiD regression with FE\r\nreghdfe y treat_post, absorb(id period) cluster(id)\r\n\r\n* Or explicit interaction\r\nreg y i.treated_group##i.post_period x1, cluster(id)\r\n```\r\n\r\n### 2.2 Callaway-Sant'Anna (Staggered Treatment)\r\n\r\nFor heterogeneous treatment effects with staggered timing:\r\n\r\n```stata\r\n* Data setup:\r\n* - gvar: first treatment period (0 for never-treated)\r\n* - id: panel unit identifier\r\n* - year: time period\r\n\r\n* Run csdid\r\ncsdid y, ivar(id) time(year) gvar(gvar) notyet\r\n\r\n* Event study aggregation\r\ncsdid_estat event\r\n\r\n* Simple ATT\r\ncsdid_estat simple\r\n```\r\n\r\n**Key options:**\r\n- `notyet`: Use not-yet-treated as control (recommended)\r\n- `never`: Use never-treated as control only\r\n\r\n### 2.3 Post-Estimation for csdid\r\n\r\n```stata\r\n* After running csdid:\r\n\r\n* Event study (dynamic effects)\r\ncsdid_estat event\r\n\r\n* Group-time ATTs\r\ncsdid_estat group\r\n\r\n* Calendar time ATTs\r\ncsdid_estat calendar\r\n\r\n* Simple overall ATT\r\ncsdid_estat simple\r\n```\r\n\r\n---\r\n\r\n## 3. Event Studies\r\n\r\n### Manual Event Study\r\n\r\n```stata\r\n* Create event time relative to treatment\r\ngen event_time = year - first_treat\r\nreplace event_time = -99 if missing(first_treat)  // Never treated\r\n\r\n* Bin endpoints\r\nreplace event_time = -5 if event_time < -5 & event_time != -99\r\nreplace event_time = 5 if event_time > 5 & event_time != -99\r\n\r\n* Create dummies (omit t=-1 as reference)\r\ntab event_time, gen(et_)\r\n\r\n* Estimate (drop reference period)\r\nreghdfe y et_1 et_2 et_3 et_4 et_6 et_7 et_8 et_9 et_10 et_11, ///\r\n    absorb(id year) cluster(id)\r\n```\r\n\r\n### Event Study from csdid\r\n\r\n```stata\r\n* Run csdid first\r\ncsdid y, ivar(id) time(year) gvar(gvar) notyet\r\n\r\n* Get event study estimates\r\ncsdid_estat event\r\n```\r\n\r\n---\r\n\r\n## 4. Instrumental Variables (IV)\r\n\r\n### 4.1 Basic 2SLS\r\n\r\n```stata\r\n* Built-in ivregress\r\nivregress 2sls y (x_endog = z), robust\r\nestat firststage\r\n\r\n* ivreg2 with more diagnostics\r\nivreg2 y (x_endog = z), first robust\r\n```\r\n\r\n### 4.2 IV Diagnostics\r\n\r\nivreg2 automatically reports:\r\n- **First-stage F-statistic**: Should be > 10 (Stock-Yogo rule)\r\n- **Kleibergen-Paap rk F**: Robust weak instrument test\r\n- **Anderson-Rubin CI**: Weak-instrument-robust confidence interval\r\n\r\n```stata\r\n* Detailed first-stage output\r\nivreg2 y (x_endog = z), first robust\r\n\r\n* Key output to check:\r\n* - F test of excluded instruments: F > 10\r\n* - Stock-Yogo critical values for comparison\r\n* - Kleibergen-Paap rk Wald F statistic\r\n```\r\n\r\n### 4.3 Multiple Instruments (Overidentification)\r\n\r\n```stata\r\n* Multiple instruments\r\nivreg2 y (x_endog = z1 z2), robust\r\n\r\n* Hansen J statistic (overidentification test) reported automatically\r\n* H0: All instruments are valid\r\n* High p-value = cannot reject validity\r\n```\r\n\r\n### 4.4 IV with Fixed Effects\r\n\r\n```stata\r\n* Using ivreghdfe (install: ssc install ivreghdfe)\r\nivreghdfe y (x_endog = z), absorb(id year) cluster(id)\r\n```\r\n\r\n---\r\n\r\n## 5. Matching Methods\r\n\r\n### 5.1 Propensity Score Matching\r\n\r\n```stata\r\n* psmatch2: estimate PS and match\r\npsmatch2 treat x1 x2 i.category, outcome(y) neighbor(1) common\r\n\r\n* Check balance\r\npstest x1 x2, both\r\n\r\n* The ATT is displayed in the output table\r\n```\r\n\r\n**Key options:**\r\n- `neighbor(k)`: k nearest neighbors\r\n- `caliper(c)`: Maximum PS distance\r\n- `common`: Restrict to common support\r\n\r\n### 5.2 Balance Assessment\r\n\r\n```stata\r\n* After psmatch2\r\npstest x1 x2, both\r\n\r\n* Output shows:\r\n* - Mean bias before/after matching\r\n* - % reduction in bias\r\n* - t-tests for balance\r\n```\r\n\r\n### 5.3 Coarsened Exact Matching (CEM)\r\n\r\n```stata\r\n* CEM with specified bins\r\ncem age (20 30 40 50 60) grade (#5), treatment(treat)\r\n\r\n* Estimate with CEM weights\r\nreg y treat x1 x2 [iweight=cem_weights], robust\r\n```\r\n\r\n---\r\n\r\n## 6. Standard Errors and Inference\r\n\r\n### 6.1 Clustered Standard Errors\r\n\r\n```stata\r\n* One-way clustering\r\nreghdfe y x treat, absorb(id period) cluster(id)\r\n\r\n* Two-way clustering\r\nreghdfe y x treat, absorb(id period) cluster(id period)\r\n```\r\n\r\n### 6.2 Wild Cluster Bootstrap (Few Clusters)\r\n\r\nWhen you have fewer than ~40 clusters:\r\n\r\n```stata\r\n* Run regression with clustering\r\nreg y treat x, cluster(state)\r\n\r\n* Wild cluster bootstrap\r\nboottest treat, cluster(state) reps(999) seed(12345) nograph\r\n\r\n* Output:\r\n* - Bootstrap t-statistic\r\n* - Bootstrap p-value\r\n* - 95% confidence interval\r\n```\r\n\r\n### 6.3 Standard Bootstrap\r\n\r\n```stata\r\n* Bootstrap standard errors\r\nbootstrap _b, reps(500) seed(12345): reg y x1 x2 treat\r\n\r\n* Bootstrap specific statistics\r\nbootstrap diff = (_b[treat]), reps(500) seed(12345): reg y x1 treat\r\n```\r\n\r\n### 6.4 Randomization Inference\r\n\r\n```stata\r\n* Permutation test with stratification\r\nritest treat _b[treat], reps(500) seed(12345) strata(block): ///\r\n    reg y treat, robust\r\n\r\n* Output shows:\r\n* - Observed coefficient\r\n* - p-value from permutation distribution\r\n```\r\n\r\n---\r\n\r\n## 7. Regression Discontinuity (RD)\r\n\r\n**Note:** The `rdrobust` package requires Stata 16+. For Stata 15, use manual polynomial approaches.\r\n\r\n### Manual RD (Stata 15 compatible)\r\n\r\n```stata\r\n* Create centered running variable\r\ngen x_centered = running_var - cutoff\r\n\r\n* Treatment indicator\r\ngen treat = (running_var >= cutoff)\r\n\r\n* Local linear regression (narrow bandwidth)\r\nreg y treat x_centered c.x_centered#i.treat if abs(x_centered) < bandwidth\r\n\r\n* The coefficient on treat is the RD estimate\r\n```\r\n\r\n### rdrobust (Stata 16+)\r\n\r\n```stata\r\n* Basic RD estimation\r\nrdrobust y running_var, c(0)\r\n\r\n* With options\r\nrdrobust y running_var, c(0) kernel(triangular) bwselect(mserd)\r\n\r\n* Manipulation test\r\nrddensity running_var, c(0)\r\n\r\n* RD plot\r\nrdplot y running_var, c(0)\r\n```\r\n\r\n---\r\n\r\n## 8. Causal Mediation\r\n\r\n```stata\r\n* Mediation analysis requires:\r\n* - Treatment variable (treat)\r\n* - Mediator variable (m)\r\n* - Outcome variable (y)\r\n\r\n* Using medeff (install: ssc install medeff)\r\n\r\n* Step 1: Mediator model\r\nreg m treat x1\r\n\r\n* Step 2: Outcome model\r\nreg y treat m x1\r\n\r\n* Step 3: Mediation effects\r\nmedeff (reg m treat x1) (reg y treat m x1), ///\r\n    treat(treat) mediate(m) sims(1000)\r\n\r\n* Output:\r\n* - ACME: Average Causal Mediation Effect (indirect)\r\n* - ADE: Average Direct Effect\r\n* - Total Effect: ACME + ADE\r\n* - Proportion Mediated: ACME / Total\r\n```\r\n\r\n---\r\n\r\n## Quick Reference\r\n\r\n### Package Installation\r\n\r\n```stata\r\n* Core packages\r\nssc install reghdfe, replace\r\nssc install ftools, replace\r\nssc install estout, replace\r\nssc install coefplot, replace\r\n\r\n* DiD methods\r\nssc install csdid, replace\r\nssc install drdid, replace\r\n\r\n* IV\r\nssc install ivreg2, replace\r\nssc install ranktest, replace\r\n\r\n* Matching\r\nssc install psmatch2, replace\r\nssc install cem, replace\r\n\r\n* Inference\r\nssc install boottest, replace\r\nssc install ritest, replace\r\n\r\n* RD (Stata 16+ only)\r\nssc install rdrobust, replace\r\nssc install rddensity, replace\r\n```\r\n\r\n### Command Quick Reference\r\n\r\n| Task | Command |\r\n|------|---------|\r\n| TWFE | `reghdfe y x, absorb(id t) cluster(id)` |\r\n| Modern DiD | `csdid y, ivar(id) time(t) gvar(g)` |\r\n| Event study | `csdid_estat event` |\r\n| IV | `ivreg2 y (endog = iv), first robust` |\r\n| PSM | `psmatch2 treat x, outcome(y)` |\r\n| CEM | `cem x1 x2, treatment(treat)` |\r\n| Wild bootstrap | `boottest x, cluster(c) nograph` |\r\n| Randomization | `ritest treat _b[treat], reps(500):` |\r\n\r\n### Diagnostic Checks\r\n\r\n1. **TWFE**: Check for negative weights with staggered timing\r\n2. **DiD**: Test parallel trends with event study pre-trends\r\n3. **IV**: First-stage F > 10, check overidentification\r\n4. **Matching**: Balance statistics with `pstest`\r\n5. **Clustering**: Use wild bootstrap with < 40 clusters\r\n\r\n---\r\n\r\n## Reporting Checklists\r\n\r\n### TWFE Checklist\r\n\r\n**Required in methods section:**\r\n- [ ] Define panel structure (unit ID, time variable)\r\n- [ ] Specify fixed effects included (unit FE, time FE, or both)\r\n- [ ] Justify clustering level (theory-based: state, firm, etc.)\r\n\r\n**Report in results:**\r\n- [ ] N units, N time periods, N observations\r\n- [ ] Fixed effects included (state FE, year FE, etc.)\r\n- [ ] Key coefficient, SE, and CI\r\n- [ ] Clustering level in table notes\r\n\r\n**Sensitivity checks:**\r\n- [ ] Alternative clustering (one-way vs two-way if plausible)\r\n- [ ] Results stable across specifications\r\n\r\n---\r\n\r\n### DiD Checklist (Traditional & Staggered)\r\n\r\n**Required in methods section:**\r\n- [ ] Treatment definition and timing\r\n- [ ] Control group definition (never-treated vs not-yet-treated)\r\n- [ ] Parallel trends assumption justification\r\n\r\n**Report in results:**\r\n- [ ] Baseline DiD estimate + SE\r\n- [ ] Event study plot with pre-trend coefficients\r\n- [ ] Discuss pre-trends (are they flat and near zero?)\r\n\r\n**Robustness checks:**\r\n- [ ] Alternative control group (for `csdid`: toggle `notyet` vs `never`)\r\n- [ ] Alternative event window bins (e.g., 2-year vs 1-year)\r\n- [ ] Wild cluster bootstrap if < 40 clusters (use `boottest`)\r\n\r\n---\r\n\r\n### IV Checklist\r\n\r\n**Required in methods section:**\r\n- [ ] Instrument relevance story (why does Z predict X?)\r\n- [ ] Exclusion restriction argument (why does Z only affect Y through X?)\r\n\r\n**Report in results:**\r\n- [ ] First-stage F-statistic (or Kleibergen-Paap rk F from `ivreg2`)\r\n- [ ] Stock-Yogo critical values for weak instrument comparison\r\n- [ ] Overidentification test (Hansen J) if multiple instruments\r\n- [ ] 2SLS coefficient + SE\r\n\r\n**Robustness checks:**\r\n- [ ] Reduced form regression (Y on Z directly)\r\n- [ ] Alternative instrument sets or control variables\r\n- [ ] Anderson-Rubin weak-instrument-robust CI\r\n\r\n---\r\n\r\n### Matching Checklist\r\n\r\n**Required in methods section:**\r\n- [ ] Covariate set justification (what variables predict treatment?)\r\n- [ ] Common support/overlap rule\r\n- [ ] Matching method choice (PSM, CEM, etc.)\r\n\r\n**Report in results:**\r\n- [ ] Balance table: before vs after matching (`pstest`)\r\n- [ ] ATT estimate + SE\r\n- [ ] Number matched, number dropped for common support\r\n\r\n**Robustness checks:**\r\n- [ ] Caliper sensitivity (try narrower/wider)\r\n- [ ] Neighbor count sensitivity (1 vs 5 neighbors)\r\n- [ ] Alternative matching method (PSM vs CEM)\r\n\r\n---\r\n\r\n### Survey Checklist\r\n\r\n**Required in methods section:**\r\n- [ ] `svyset` statement with PSU, strata, weights\r\n\r\n**Report in results:**\r\n- [ ] Weighted descriptive statistics\r\n- [ ] Key model with survey-corrected SEs\r\n- [ ] Note: \"Analyses account for complex survey design\"\r\n\r\n**Important notes:**\r\n- [ ] Use `subpop()` not `if` for subgroup analyses\r\n- [ ] Comparison with unweighted results (as robustness, not truth)\r\n\r\n---\r\n\r\n### Synthetic Control Checklist\r\n\r\n**Required in methods section:**\r\n- [ ] Donor pool definition (which units? why these?)\r\n- [ ] Predictor variable rationale\r\n\r\n**Report in results:**\r\n- [ ] Pre-period fit (RMSPE)\r\n- [ ] Unit weights table (which donors contribute?)\r\n- [ ] Gap plot (treated - synthetic over time)\r\n\r\n**Robustness checks:**\r\n- [ ] In-space placebo tests (run synth for each control unit)\r\n- [ ] In-time placebo tests (fake treatment at earlier time)\r\n- [ ] Predictor set sensitivity\r\n- [ ] Exclude potentially contaminated donors\r\n",
        "plugins/stata-analyst/skills/stata-analyst/techniques/02_survey_resampling.md": "# Survey & Resampling Methods in Stata\r\n\r\nSurvey weights, bootstrap, randomization inference, and decomposition. All code tested on Stata 15+.\r\n\r\n---\r\n\r\n## 1. Survey Methods\r\n\r\n### Setup Survey Design\r\n\r\n```stata\r\n* Using nhanes2f dataset as example\r\nwebuse nhanes2f, clear\r\n\r\n* Setup: psuid = PSU, stratid = stratum, finalwgt = weight\r\nsvyset psuid [pweight=finalwgt], strata(stratid)\r\n```\r\n\r\n### Check Survey Setup\r\n\r\n```stata\r\nsvydescribe\r\n```\r\n\r\n### Survey-Weighted Estimation\r\n\r\n```stata\r\n* Survey mean\r\nsvy: mean height weight\r\n\r\n* Survey mean by group\r\nsvy: mean height, over(sex)\r\n\r\n* Survey regression\r\nsvy: reg height weight age i.sex\r\n\r\n* Survey logit\r\nsvy: logit highbp age i.sex weight\r\n```\r\n\r\n### Subpopulation Analysis\r\n\r\nCorrect way to analyze subgroups (maintains correct SEs):\r\n\r\n```stata\r\n* Analyze females only\r\nsvy, subpop(female): mean height weight\r\n\r\n* Analyze using if condition\r\nsvy, subpop(if age >= 40): mean bpsystol\r\n```\r\n\r\n**Note:** Always use `subpop()` rather than `if` for survey data to get correct variance estimates.\r\n\r\n---\r\n\r\n## 2. Bootstrap Methods\r\n\r\n### Standard Bootstrap\r\n\r\n```stata\r\nsysuse auto, clear\r\n\r\n* Bootstrap standard errors for all coefficients\r\nbootstrap _b, reps(500) seed(12345): reg price mpg weight foreign\r\n\r\n* Bootstrap specific statistic\r\nbootstrap diff = (_b[foreign]), reps(500) seed(12345): ///\r\n    reg price mpg weight foreign\r\n```\r\n\r\n### Bootstrap Options\r\n\r\n| Option | Purpose |\r\n|--------|---------|\r\n| `reps(#)` | Number of replications |\r\n| `seed(#)` | Random seed for reproducibility |\r\n| `bca` | Bias-corrected accelerated CIs |\r\n| `strata(var)` | Stratified resampling |\r\n| `cluster(var)` | Cluster resampling |\r\n\r\n---\r\n\r\n## 3. Wild Cluster Bootstrap\r\n\r\nFor inference with few clusters (< 40):\r\n\r\n```stata\r\n* Create clustered data\r\nclear\r\nset seed 12345\r\nset obs 500\r\ngen state = ceil(_n/50)\r\ngen treat = (state <= 5)\r\ngen x = rnormal()\r\ngen y = 1 + 0.5*treat + 0.3*x + rnormal(0, 1)\r\n\r\n* Standard clustered SE\r\nreg y treat x, cluster(state)\r\n\r\n* Wild cluster bootstrap\r\nboottest treat, cluster(state) reps(999) seed(12345) nograph\r\n```\r\n\r\n**Output includes:**\r\n- Bootstrap t-statistic\r\n- Bootstrap p-value\r\n- 95% confidence interval robust to few clusters\r\n\r\n---\r\n\r\n## 4. Randomization Inference\r\n\r\n```stata\r\n* Create experimental data\r\nclear\r\nset seed 12345\r\nset obs 200\r\ngen id = _n\r\ngen block = ceil(_n/20)\r\ngen treat = mod(_n, 2)\r\ngen y = 1 + 0.5*treat + rnormal(0, 1)\r\n\r\n* Standard OLS\r\nreg y treat, robust\r\n\r\n* Randomization inference with stratification\r\nritest treat _b[treat], reps(500) seed(12345) strata(block): ///\r\n    reg y treat, robust\r\n```\r\n\r\n**Output shows:**\r\n- Observed coefficient\r\n- Number of permutations where |T| >= |T(obs)|\r\n- Exact p-value from permutation distribution\r\n\r\n---\r\n\r\n## 5. Oaxaca-Blinder Decomposition\r\n\r\n```stata\r\n* Use nlsw88 data\r\nsysuse nlsw88, clear\r\nkeep if !missing(wage, union, age, grade, tenure)\r\n\r\n* Basic decomposition by union status\r\noaxaca wage age grade tenure, by(union)\r\n\r\n* Detailed decomposition\r\noaxaca wage age grade tenure, by(union) detail\r\n\r\n* Pooled reference coefficients\r\noaxaca wage age grade tenure, by(union) pooled\r\n```\r\n\r\n### Interpretation\r\n\r\nThe output shows:\r\n- **Endowments**: Differences due to characteristics (what if Group 2 had Group 1's X values)\r\n- **Coefficients**: Differences due to returns to characteristics (unexplained/discrimination)\r\n- **Interaction**: Combined effect\r\n\r\n---\r\n\r\n## Quick Reference\r\n\r\n### Survey Commands\r\n\r\n| Command | Purpose |\r\n|---------|---------|\r\n| `svyset` | Define survey design |\r\n| `svy:` | Prefix for survey estimation |\r\n| `svydescribe` | Describe survey design |\r\n| `subpop()` | Analyze subpopulation |\r\n\r\n### Resampling Commands\r\n\r\n| Command | Purpose |\r\n|---------|---------|\r\n| `bootstrap` | Standard bootstrap |\r\n| `boottest` | Wild cluster bootstrap |\r\n| `ritest` | Randomization inference |\r\n\r\n### Package Installation\r\n\r\n```stata\r\nssc install boottest, replace\r\nssc install ritest, replace\r\nssc install oaxaca, replace\r\n```\r\n",
        "plugins/stata-analyst/skills/stata-analyst/techniques/03_synthetic_control.md": "# Synthetic Control Methods in Stata\r\n\r\nSynthetic control method for comparative case studies. All code tested on Stata 15+.\r\n\r\n---\r\n\r\n## 1. Overview\r\n\r\nThe synthetic control method constructs a weighted combination of control units to serve as a counterfactual for a treated unit. It is designed for:\r\n\r\n- Single treated unit\r\n- Aggregate-level data (states, countries, regions)\r\n- Clear treatment timing\r\n- Multiple potential control units\r\n\r\n---\r\n\r\n## 2. Data Requirements\r\n\r\nPanel data structure with:\r\n- Unit identifier\r\n- Time variable\r\n- Outcome variable\r\n- Predictor variables\r\n\r\n```stata\r\n* Declare panel structure\r\ntsset unit_id time_var\r\n```\r\n\r\n---\r\n\r\n## 3. Basic Synthetic Control\r\n\r\n```stata\r\n* Install synth package\r\nssc install synth, replace\r\n\r\n* Basic syntax\r\nsynth outcome_var predictors, ///\r\n    trunit(#) trperiod(#)\r\n```\r\n\r\n### Example with Simulated Data\r\n\r\n```stata\r\nclear\r\nset seed 12345\r\nset obs 400\r\n\r\n* Create panel: 20 states, 20 years each\r\ngen state = ceil(_n/20)\r\nbysort state: gen year = 1980 + _n\r\n\r\n* Outcome variable\r\ngen y = 100 + state*2 + year*0.5 + rnormal(0, 5)\r\n\r\n* Treatment: state 1 treated after year 1990\r\nreplace y = y - 15 if state == 1 & year >= 1990\r\n\r\n* Covariates\r\ngen x1 = rnormal(50, 10)\r\ngen x2 = rnormal(1000, 200)\r\n\r\n* Declare panel\r\ntsset state year\r\n\r\n* Run synthetic control\r\nsynth y x1 x2 y(1985) y(1988), trunit(1) trperiod(1990)\r\n```\r\n\r\n---\r\n\r\n## 4. Specifying Predictors\r\n\r\n### Average of Variable Over Pre-Period\r\n\r\n```stata\r\n* x1 averaged over all pre-treatment periods\r\nsynth y x1 x2, trunit(1) trperiod(1990)\r\n```\r\n\r\n### Specific Year Values\r\n\r\n```stata\r\n* y at specific years as predictors\r\nsynth y y(1985) y(1988), trunit(1) trperiod(1990)\r\n```\r\n\r\n### Range of Years\r\n\r\n```stata\r\n* y averaged over 1985-1988\r\nsynth y y(1985(1)1988), trunit(1) trperiod(1990)\r\n```\r\n\r\n### Combined Predictors\r\n\r\n```stata\r\nsynth y x1 x2 y(1985) y(1988), trunit(1) trperiod(1990)\r\n```\r\n\r\n---\r\n\r\n## 5. Output Interpretation\r\n\r\nThe synth output shows:\r\n\r\n### RMSPE (Root Mean Squared Prediction Error)\r\n- Measures pre-treatment fit\r\n- Lower is better\r\n\r\n### Unit Weights\r\n- Which control units comprise the synthetic control\r\n- Weights sum to 1\r\n\r\n### Predictor Balance\r\n- Compares treated vs synthetic control on predictors\r\n- Should be similar if fit is good\r\n\r\n---\r\n\r\n## 6. Options\r\n\r\n| Option | Purpose |\r\n|--------|---------|\r\n| `trunit(#)` | Treated unit ID |\r\n| `trperiod(#)` | First treatment period |\r\n| `fig` | Display results figure |\r\n| `keep(filename)` | Save results to file |\r\n| `nested` | Use nested optimization |\r\n| `allopt` | Try all optimization methods |\r\n\r\n---\r\n\r\n## 7. Inference\r\n\r\n### Placebo Tests (In-Space)\r\n\r\nRun synth for each control unit as if it were treated:\r\n\r\n```stata\r\n* Store treated unit result\r\nsynth y x1 x2 y(1985) y(1988), trunit(1) trperiod(1990) ///\r\n    keep(synth_treated) replace\r\n\r\n* Placebo for control unit 2\r\nsynth y x1 x2 y(1985) y(1988), trunit(2) trperiod(1990) ///\r\n    keep(synth_placebo2) replace\r\n\r\n* Compare RMSPEs\r\n* Treated effect is significant if RMSPE ratio is large\r\n```\r\n\r\n### Placebo Tests (In-Time)\r\n\r\nApply treatment at different (fake) times:\r\n\r\n```stata\r\n* True treatment 1990\r\nsynth y x1 x2 y(1982) y(1985), trunit(1) trperiod(1990)\r\n\r\n* Placebo: treatment at 1985 (before actual treatment)\r\nsynth y x1 x2 y(1982), trunit(1) trperiod(1985)\r\n```\r\n\r\n---\r\n\r\n## 8. Limitations\r\n\r\n- Single treated unit (not for multiple treatment)\r\n- Requires good pre-treatment fit\r\n- Sensitive to predictor choice\r\n- No built-in inference (must do placebo tests)\r\n- Cannot extrapolate beyond donor pool characteristics\r\n\r\n---\r\n\r\n## Quick Reference\r\n\r\n### Basic Syntax\r\n\r\n```stata\r\nsynth outcome predictors, trunit(#) trperiod(#)\r\n```\r\n\r\n### Package Installation\r\n\r\n```stata\r\nssc install synth, replace\r\n```\r\n\r\n### Key Components\r\n\r\n| Component | Description |\r\n|-----------|-------------|\r\n| `outcome` | Dependent variable |\r\n| `predictors` | Variables to match on |\r\n| `trunit()` | ID of treated unit |\r\n| `trperiod()` | First treatment period |\r\n",
        "plugins/stata-analyst/skills/stata-analyst/techniques/04_visualization.md": "# Visualization & Output in Stata\r\n\r\nPublication-quality tables, coefficient plots, and figures. All code tested on Stata 15+.\r\n\r\n---\r\n\r\n## 1. Regression Tables with esttab\r\n\r\n### Basic Table\r\n\r\n```stata\r\n* Run and store models\r\nquietly reg price mpg\r\nestimates store m1\r\n\r\nquietly reg price mpg weight\r\nestimates store m2\r\n\r\nquietly reg price mpg weight foreign\r\nestimates store m3\r\n\r\n* Basic table\r\nesttab m1 m2 m3, se star(* 0.10 ** 0.05 *** 0.01)\r\n```\r\n\r\n### Publication-Ready Table\r\n\r\n```stata\r\nesttab m1 m2 m3, ///\r\n    se star(* 0.10 ** 0.05 *** 0.01) ///\r\n    mtitle(\"Model 1\" \"Model 2\" \"Model 3\") ///\r\n    title(\"Price Regressions\") ///\r\n    keep(mpg weight foreign) ///\r\n    order(foreign mpg weight)\r\n```\r\n\r\n### Export to File\r\n\r\n```stata\r\n* Text file\r\nesttab m1 m2 m3 using \"table1.txt\", replace ///\r\n    se star(* 0.10 ** 0.05 *** 0.01) ///\r\n    mtitle(\"Model 1\" \"Model 2\" \"Model 3\")\r\n\r\n* LaTeX\r\nesttab m1 m2 m3 using \"table1.tex\", replace ///\r\n    se star(* 0.10 ** 0.05 *** 0.01) ///\r\n    booktabs label\r\n\r\n* RTF (Word-compatible)\r\nesttab m1 m2 m3 using \"table1.rtf\", replace ///\r\n    se star(* 0.10 ** 0.05 *** 0.01)\r\n```\r\n\r\n### Common esttab Options\r\n\r\n| Option | Description |\r\n|--------|-------------|\r\n| `se` | Show standard errors |\r\n| `t` | Show t-statistics |\r\n| `p` | Show p-values |\r\n| `ci` | Show confidence intervals |\r\n| `star(...)` | Define significance stars |\r\n| `mtitle(...)` | Column titles |\r\n| `title(...)` | Table title |\r\n| `keep(...)` | Variables to show |\r\n| `drop(...)` | Variables to hide |\r\n| `order(...)` | Variable order |\r\n| `label` | Use variable labels |\r\n| `booktabs` | LaTeX booktabs format |\r\n| `replace` | Overwrite existing file |\r\n\r\n---\r\n\r\n## 2. Coefficient Plots\r\n\r\n### Basic Coefficient Plot\r\n\r\n```stata\r\n* After running regression\r\nreg price mpg weight foreign i.rep78\r\n\r\n* Plot coefficients\r\ncoefplot, drop(_cons) xline(0) ///\r\n    title(\"Coefficient Plot\")\r\n\r\ngraph export \"coefplot.png\", replace\r\n```\r\n\r\n### Multiple Model Comparison\r\n\r\n```stata\r\n* Store multiple models first\r\nquietly reg price mpg\r\nestimates store m1\r\n\r\nquietly reg price mpg weight\r\nestimates store m2\r\n\r\nquietly reg price mpg weight foreign\r\nestimates store m3\r\n\r\n* Compare in single plot\r\ncoefplot m1 m2 m3, drop(_cons) xline(0) ///\r\n    legend(order(2 \"Model 1\" 4 \"Model 2\" 6 \"Model 3\"))\r\n\r\ngraph export \"coefplot_comparison.png\", replace\r\n```\r\n\r\n### Vertical Coefficient Plot\r\n\r\n```stata\r\ncoefplot m3, drop(_cons) vertical ///\r\n    yline(0, lpattern(dash)) ///\r\n    title(\"Vertical Coefficient Plot\")\r\n```\r\n\r\n### Event Study Style Plot\r\n\r\n```stata\r\n* After event study regression with lead/lag dummies\r\ncoefplot, keep(lead_* lag_*) vertical ///\r\n    yline(0, lpattern(dash)) ///\r\n    xline(0.5, lpattern(dash) lcolor(red)) ///\r\n    xlabel(, angle(45)) ///\r\n    xtitle(\"Event Time\") ytitle(\"Effect\")\r\n```\r\n\r\n### Customizing coefplot\r\n\r\n```stata\r\ncoefplot m1 m2, drop(_cons) xline(0) ///\r\n    ciopts(recast(rcap)) ///              // Cap ends on CI\r\n    msymbol(D) ///                         // Diamond markers\r\n    mcolor(navy) ///                       // Marker color\r\n    levels(95 90) ///                      // Multiple CI levels\r\n    legend(order(1 \"95% CI\" 2 \"90% CI\"))\r\n```\r\n\r\n---\r\n\r\n## 3. Summary Statistics\r\n\r\n### Basic Summary\r\n\r\n```stata\r\nsummarize price mpg weight\r\n\r\n* Detailed with percentiles\r\nsummarize price, detail\r\n```\r\n\r\n### tabstat (Formatted)\r\n\r\n```stata\r\n* Multiple statistics\r\ntabstat price mpg weight, stat(mean sd min max n) columns(statistics)\r\n\r\n* By group\r\ntabstat price mpg weight, by(foreign) stat(mean sd n)\r\n```\r\n\r\n### Summary Table with estpost\r\n\r\n```stata\r\n* Summary statistics table\r\nestpost summarize price mpg weight\r\nesttab, cells(\"mean(fmt(2)) sd(fmt(2)) min max count\") nomtitle nonumber\r\n\r\n* Export to file\r\nesttab using \"summary_stats.tex\", replace ///\r\n    cells(\"mean(fmt(2)) sd(fmt(2)) min max count\") ///\r\n    nomtitle nonumber booktabs\r\n```\r\n\r\n### Balance Table\r\n\r\n```stata\r\n* By treatment group\r\nbysort treatment: summarize y x1 x2\r\n\r\n* Or use tabstat\r\ntabstat y x1 x2, by(treatment) stat(mean sd n) nototal\r\n```\r\n\r\n---\r\n\r\n## 4. Basic Graphs\r\n\r\n### Scatter Plot with Fit Line\r\n\r\n```stata\r\ntwoway (scatter y x) (lfit y x), ///\r\n    title(\"Scatter with Linear Fit\") ///\r\n    xtitle(\"X Variable\") ytitle(\"Y Variable\") ///\r\n    legend(off)\r\n\r\ngraph export \"scatter.png\", replace\r\n```\r\n\r\n### Binned Scatter (binscatter)\r\n\r\n```stata\r\n* Install: ssc install binscatter\r\nbinscatter y x, controls(z1 z2) ///\r\n    title(\"Binned Scatter\") ///\r\n    xtitle(\"X\") ytitle(\"Y | Controls\")\r\n```\r\n\r\n### Time Series\r\n\r\n```stata\r\ntwoway line y year, ///\r\n    title(\"Outcome Over Time\") ///\r\n    xtitle(\"Year\") ytitle(\"Outcome\")\r\n\r\n* Multiple series\r\ntwoway (line y1 year) (line y2 year), ///\r\n    legend(order(1 \"Series 1\" 2 \"Series 2\"))\r\n```\r\n\r\n### Histogram\r\n\r\n```stata\r\nhistogram y, frequency ///\r\n    title(\"Distribution of Y\") ///\r\n    xtitle(\"Y\") ytitle(\"Frequency\")\r\n```\r\n\r\n### Kernel Density\r\n\r\n```stata\r\nkdensity y, ///\r\n    title(\"Density of Y\") ///\r\n    xtitle(\"Y\") ytitle(\"Density\")\r\n\r\n* By group\r\ntwoway (kdensity y if treat==0) (kdensity y if treat==1), ///\r\n    legend(order(1 \"Control\" 2 \"Treatment\"))\r\n```\r\n\r\n---\r\n\r\n## 5. Graph Formatting\r\n\r\n### Publication Scheme\r\n\r\n```stata\r\n* Set clean scheme\r\nset scheme s1mono  // Black and white\r\nset scheme s1color // Color version\r\n\r\n* Or use custom\r\nssc install grstyle, replace\r\nssc install palettes, replace\r\nssc install colrspace, replace\r\n\r\ngrstyle init\r\ngrstyle set plain\r\ngrstyle set legend 6, nobox\r\ngrstyle set color navy maroon forest_green\r\n```\r\n\r\n### Common Graph Options\r\n\r\n```stata\r\ntwoway ..., ///\r\n    title(\"Main Title\") ///\r\n    subtitle(\"Subtitle\") ///\r\n    xtitle(\"X Axis Label\") ///\r\n    ytitle(\"Y Axis Label\") ///\r\n    xlabel(0(10)100) ///           // X ticks at 0,10,20,...,100\r\n    ylabel(, angle(horizontal)) /// // Horizontal Y labels\r\n    legend(order(1 \"First\" 2 \"Second\") pos(6)) /// // Bottom legend\r\n    note(\"Note: Sample description\") ///\r\n    graphregion(color(white)) ///  // White background\r\n    plotregion(margin(zero))       // No margins\r\n```\r\n\r\n### Export Formats\r\n\r\n```stata\r\n* PNG (web/slides)\r\ngraph export \"figure.png\", replace width(2400)\r\n\r\n* PDF (publication)\r\ngraph export \"figure.pdf\", replace\r\n\r\n* EPS (LaTeX)\r\ngraph export \"figure.eps\", replace\r\n```\r\n\r\n### Combine Multiple Graphs\r\n\r\n```stata\r\n* Create individual graphs\r\ntwoway scatter y1 x, name(g1, replace) title(\"Panel A\")\r\ntwoway scatter y2 x, name(g2, replace) title(\"Panel B\")\r\n\r\n* Combine\r\ngraph combine g1 g2, rows(1) ///\r\n    title(\"Combined Figure\")\r\n\r\ngraph export \"combined.png\", replace\r\n```\r\n\r\n---\r\n\r\n## 6. RD Plots (Stata 16+)\r\n\r\n```stata\r\n* Using rdplot (requires rdrobust package)\r\nrdplot y running_var, c(0) ///\r\n    graph_options(title(\"RD Plot\") ///\r\n                  xtitle(\"Running Variable\") ///\r\n                  ytitle(\"Outcome\"))\r\n```\r\n\r\n---\r\n\r\n## Quick Reference\r\n\r\n### Table Output Commands\r\n\r\n| Command | Purpose |\r\n|---------|---------|\r\n| `esttab` | Regression tables |\r\n| `estpost` | Post results for tables |\r\n| `tabstat` | Summary statistics |\r\n| `tabout` | Cross-tabulations |\r\n\r\n### Graph Commands\r\n\r\n| Command | Purpose |\r\n|---------|---------|\r\n| `coefplot` | Coefficient plots |\r\n| `binscatter` | Binned scatter plots |\r\n| `twoway` | General 2D graphs |\r\n| `histogram` | Histograms |\r\n| `kdensity` | Kernel density |\r\n| `graph combine` | Multi-panel figures |\r\n\r\n### Export Checklist\r\n\r\n- [ ] Use vector format (PDF/EPS) for line plots\r\n- [ ] Use 300+ DPI for raster images\r\n- [ ] Include informative axis labels\r\n- [ ] Add reference lines where appropriate\r\n- [ ] Use consistent color scheme\r\n- [ ] Match journal font requirements\r\n",
        "plugins/stata-analyst/skills/stata-analyst/techniques/05_best_practices.md": "# Best Practices for Stata Projects\r\n\r\nCode organization, reproducibility, and style conventions based on top journal replication packages.\r\n\r\n---\r\n\r\n## 1. Master Do-File Structure\r\n\r\nEvery project should have a single master script that runs the entire analysis.\r\n\r\n### Template\r\n\r\n```stata\r\n* ===========================================================================\r\n* Master Do-File\r\n*\r\n* \"[Paper Title]\"\r\n* [Authors]\r\n* [Journal], [Year]\r\n*\r\n* ===========================================================================\r\n\r\nversion 15\r\nclear all\r\nset more off\r\nset maxvar 10000\r\n\r\n* ===========================================================================\r\n* PATH SETUP - Edit this section only\r\n* ===========================================================================\r\n\r\n* Set root path (CHANGE THIS for your machine)\r\nglobal root \"/path/to/replication\"\r\n\r\n* Derived paths (do not edit)\r\nglobal code \"$root/code\"\r\nglobal data \"$root/data\"\r\n    global raw \"$data/raw\"\r\n    global clean \"$data/clean\"\r\nglobal out \"$root/output\"\r\n    global figures \"$out/figures\"\r\n    global tables \"$out/tables\"\r\n    global logs \"$out/logs\"\r\n\r\n* ===========================================================================\r\n* EXECUTION\r\n* ===========================================================================\r\n\r\nlog using \"$logs/replication_log.txt\", text replace\r\n\r\n* Data preparation\r\ndo \"$code/01_import_data.do\"\r\ndo \"$code/02_clean_data.do\"\r\ndo \"$code/03_create_variables.do\"\r\n\r\n* Analysis\r\ndo \"$code/10_descriptives.do\"      // Table 1, Figures 1-2\r\ndo \"$code/11_main_analysis.do\"     // Tables 2-4\r\ndo \"$code/12_robustness.do\"        // Appendix Tables A1-A5\r\n\r\n* Figures\r\ndo \"$code/20_figures.do\"           // Figures 3-6\r\n\r\nlog close\r\n\r\nexit\r\n```\r\n\r\n---\r\n\r\n## 2. Project Directory Structure\r\n\r\n```\r\nproject/\r\n code/\r\n    00_master.do\r\n    01_import_data.do\r\n    02_clean_data.do\r\n    03_create_variables.do\r\n    10_descriptives.do\r\n    11_main_analysis.do\r\n    20_figures.do\r\n data/\r\n    raw/           # Original, unchanged data\r\n    clean/         # Processed data files\r\n output/\r\n    figures/\r\n    tables/\r\n    logs/\r\n README.md\r\n```\r\n\r\n---\r\n\r\n## 3. Path Management\r\n\r\n### Good: Single Root Variable\r\n\r\n```stata\r\n* Set root path (only line to change)\r\nglobal root \"/Users/name/project\"\r\n\r\n* Derived paths\r\nglobal code \"$root/code\"\r\nglobal raw \"$root/data/raw\"\r\nglobal clean \"$root/data/clean\"\r\nglobal figures \"$root/output/figures\"\r\nglobal tables \"$root/output/tables\"\r\n```\r\n\r\n### Bad: Hardcoded Paths\r\n\r\n```stata\r\n* DON'T do this - not portable\r\nuse \"/Users/name/Documents/project/data/mydata.dta\"\r\n```\r\n\r\n---\r\n\r\n## 4. Data Cleaning Conventions\r\n\r\n### Recode Missing Values Consistently\r\n\r\n```stata\r\n* Standardize missing values\r\nrecode var1 var2 var3 (9999 = .) (-99 = .) (-1 = .)\r\n\r\n* Document recoding with comments\r\n* Original codes: 9999=DK, -99=Refused, -1=NA\r\nrecode satisfaction (9999 = .) (-99 = .) (-1 = .)\r\n```\r\n\r\n### Label Everything\r\n\r\n```stata\r\n* Define value labels\r\nlabel define yesno 0 \"No\" 1 \"Yes\"\r\nlabel values treated yesno\r\n\r\n* Variable labels\r\nlabel variable treated \"Treatment indicator\"\r\nlabel variable outcome \"Primary outcome measure\"\r\n```\r\n\r\n### Check Merge Results\r\n\r\n```stata\r\nmerge m:1 id using \"$clean/demographics.dta\"\r\n\r\n* Document merge results\r\ntab _merge\r\n\r\n* Handle unmatched explicitly\r\nassert _merge != 2  // No unmatched from using\r\ndrop if _merge == 2\r\ndrop _merge\r\n```\r\n\r\n---\r\n\r\n## 5. Reproducibility Essentials\r\n\r\n### Set Random Seed\r\n\r\n```stata\r\n* Set seed at the beginning of the do-file\r\nset seed 12345  // Use meaningful seed (e.g., date: 20240115)\r\n```\r\n\r\n### Version Control\r\n\r\n```stata\r\n* Specify Stata version at top of do-file\r\nversion 15\r\n\r\n* Record session info\r\ndisplay \"Stata version: \" c(stata_version)\r\ndisplay \"Date: \" c(current_date)\r\ndisplay \"Time: \" c(current_time)\r\n```\r\n\r\n### Use Log Files\r\n\r\n```stata\r\n* Start log\r\nlog using \"$logs/analysis_log.txt\", text replace\r\n\r\n* ... analysis code ...\r\n\r\nlog close\r\n```\r\n\r\n---\r\n\r\n## 6. Code Style Conventions\r\n\r\n### Naming\r\n\r\n```stata\r\n* Variables: snake_case\r\ngen treatment_effect = ...\r\ngen log_income = log(income)\r\n\r\n* Wave indicators: suffix with number\r\nrename income incomeW1\r\nrename income2 incomeW2\r\n\r\n* Temporary variables: prefix with underscore\r\ngen _temp_var = ...\r\ndrop _temp_var\r\n```\r\n\r\n### Comments\r\n\r\n```stata\r\n* Single-line comment for brief notes\r\n\r\n/*\r\nMulti-line comment for\r\nlonger explanations\r\n*/\r\n\r\n// Alternative single-line (less common in published code)\r\n\r\n*--- Section break ---*\r\n```\r\n\r\n### Line Continuation\r\n\r\n```stata\r\n* Use /// for long commands\r\nreghdfe outcome treatment control1 control2 control3 ///\r\n    control4 control5, ///\r\n    absorb(id year) cluster(id)\r\n```\r\n\r\n---\r\n\r\n## 7. Analysis Do-File Template\r\n\r\n```stata\r\n* ===========================================================================\r\n* [Descriptive title]\r\n*\r\n* Purpose: [What this file does]\r\n* Input:   [Input data files]\r\n* Output:  [Output files - tables, figures]\r\n*\r\n* ===========================================================================\r\n\r\n* Load data\r\nuse \"$clean/analysis_sample.dta\", clear\r\n\r\n* -------------------------------------------\r\n* Section 1: [Description]\r\n* -------------------------------------------\r\n\r\n[analysis code]\r\n\r\n* -------------------------------------------\r\n* Section 2: [Description]\r\n* -------------------------------------------\r\n\r\n[analysis code]\r\n\r\n* -------------------------------------------\r\n* Export results\r\n* -------------------------------------------\r\n\r\nesttab m1 m2 m3 using \"$tables/table_1.tex\", replace ///\r\n    [options]\r\n\r\ngraph export \"$figures/figure_1.pdf\", replace\r\n```\r\n\r\n---\r\n\r\n## 8. Common Patterns\r\n\r\n### Loop Over Variables\r\n\r\n```stata\r\n* Process multiple variables\r\nforeach var in income education age {\r\n    gen log_`var' = log(`var')\r\n    label variable log_`var' \"Log `var'\"\r\n}\r\n```\r\n\r\n### Loop Over Specifications\r\n\r\n```stata\r\n* Multiple model specifications\r\nlocal controls1 \"x1 x2\"\r\nlocal controls2 \"x1 x2 x3 x4\"\r\nlocal controls3 \"x1 x2 x3 x4 x5 x6\"\r\n\r\nforvalues i = 1/3 {\r\n    reg y treat `controls`i'', robust\r\n    estimates store m`i'\r\n}\r\n\r\nesttab m1 m2 m3\r\n```\r\n\r\n### Store Multiple Models Efficiently\r\n\r\n```stata\r\n* Clear stored estimates\r\nestimates clear\r\n\r\n* Run and store\r\nforeach outcome in y1 y2 y3 {\r\n    quietly reg `outcome' treat x1 x2, robust\r\n    estimates store m_`outcome'\r\n}\r\n\r\n* Combined table\r\nesttab m_y1 m_y2 m_y3, se\r\n```\r\n\r\n---\r\n\r\n## 9. Pre-Submission Checklist\r\n\r\n### Code Quality\r\n- [ ] Master script runs entire analysis without errors\r\n- [ ] All paths are relative (single root variable)\r\n- [ ] Random seeds are set and documented\r\n- [ ] Code files have clear headers with purpose\r\n\r\n### Documentation\r\n- [ ] README documents execution order\r\n- [ ] README lists software/package requirements\r\n- [ ] Output files are named to match paper elements\r\n\r\n### Reproducibility\r\n- [ ] Stata version specified\r\n- [ ] Package versions recorded\r\n- [ ] Intermediate datasets saved\r\n- [ ] Log files generated\r\n\r\n### Data\r\n- [ ] Raw data unchanged\r\n- [ ] All data transformations documented\r\n- [ ] Missing value handling explicit\r\n- [ ] Merge results verified\r\n\r\n---\r\n\r\n## Quick Reference\r\n\r\n### Essential Setup Commands\r\n\r\n```stata\r\nclear all\r\nset more off\r\nset maxvar 10000\r\nversion 15\r\n```\r\n\r\n### Global Structure\r\n\r\n```stata\r\nglobal root \"/path/to/project\"\r\nglobal code \"$root/code\"\r\nglobal data \"$root/data\"\r\nglobal out \"$root/output\"\r\n```\r\n\r\n### Output Mapping Comment Style\r\n\r\n```stata\r\ndo \"$code/10_analysis.do\"  // Table 1, Figure 2\r\n```\r\n",
        "plugins/stata-analyst/skills/stata-analyst/techniques/06_modeling_basics.md": "# Modeling Basics in Stata\r\n\r\nPlain-vanilla regression and GLM patterns that sociologists use constantly. All code tested on Stata 15+.\r\n\r\n---\r\n\r\n## 1. OLS Foundation\r\n\r\n### Basic OLS with Robust SEs\r\n\r\n```stata\r\nsysuse auto, clear\r\n\r\n* Robust (heteroskedasticity-consistent) standard errors\r\nreg price mpg weight, robust\r\n```\r\n\r\n### Clustered Standard Errors\r\n\r\n```stata\r\n* Cluster at the group level\r\nreg price mpg weight, cluster(foreign)\r\n\r\n* For panel data, typically cluster on the panel unit\r\nreg y x1 x2, cluster(state)\r\n```\r\n\r\n### Nested Controls Pattern\r\n\r\nBuild models incrementally to show robustness:\r\n\r\n```stata\r\n* Model 1: Baseline\r\nreg price mpg, robust\r\nestimates store m1\r\n\r\n* Model 2: Add controls\r\nreg price mpg weight, robust\r\nestimates store m2\r\n\r\n* Model 3: Add categorical\r\nreg price mpg weight i.foreign, robust\r\nestimates store m3\r\n\r\n* Model 4: Full specification\r\nreg price mpg weight i.foreign headroom trunk, robust\r\nestimates store m4\r\n\r\n* Display nested table\r\nesttab m1 m2 m3 m4, se star(* 0.10 ** 0.05 *** 0.01) ///\r\n    mtitles(\"(1)\" \"(2)\" \"(3)\" \"(4)\") ///\r\n    drop(0.foreign) ///\r\n    stats(N r2, labels(\"N\" \"R-squared\") fmt(%9.0fc %9.3f))\r\n```\r\n\r\n---\r\n\r\n## 2. Interactions and Margins\r\n\r\n### Continuous  Continuous Interaction\r\n\r\n```stata\r\n* c. prefix for continuous variables\r\nreg price c.mpg##c.weight, robust\r\n\r\n* Marginal effect of mpg at different weight levels\r\nmargins, dydx(mpg) at(weight=(2000 3000 4000))\r\n```\r\n\r\n**Interpretation note:** The interaction coefficient alone doesn't tell you the effect at any particular point. Always use `margins` to interpret.\r\n\r\n### Categorical  Continuous Interaction\r\n\r\n```stata\r\n* i. prefix for categorical variables\r\nreg price i.foreign##c.mpg, robust\r\n\r\n* Effect of mpg by foreign status\r\nmargins foreign, dydx(mpg)\r\n\r\n* Predicted values at specific mpg levels by group\r\nmargins foreign, at(mpg=(15 20 25 30))\r\n```\r\n\r\n### Marginsplot for Visualization\r\n\r\n```stata\r\nreg price i.foreign##c.mpg, robust\r\nmargins foreign, at(mpg=(15(5)35))\r\nmarginsplot, title(\"Price by MPG and Origin\") ///\r\n    ytitle(\"Predicted Price\") xtitle(\"MPG\")\r\ngraph export \"$figures/interaction_plot.png\", replace\r\n```\r\n\r\n### Reviewer-Proof Guidelines\r\n\r\n- **Don't interpret raw interaction coefficient alone** - always compute marginal effects\r\n- **Plot predicted values** - reviewers want to see the interaction visually\r\n- **Report at meaningful values** - use means or policy-relevant cutpoints\r\n\r\n---\r\n\r\n## 3. Binary Outcomes: Logit/Probit\r\n\r\n### Logit with Robust SEs\r\n\r\n```stata\r\nsysuse nlsw88, clear\r\nkeep if !missing(union, wage, age, grade, tenure)\r\n\r\nlogit union wage age grade, robust\r\nestimates store logit1\r\n```\r\n\r\n### Probit Alternative\r\n\r\n```stata\r\nprobit union wage age grade, robust\r\nestimates store probit1\r\n\r\n* Compare\r\nesttab logit1 probit1, se star(* 0.10 ** 0.05 *** 0.01) ///\r\n    mtitles(\"Logit\" \"Probit\")\r\n```\r\n\r\n### Average Marginal Effects (AME)\r\n\r\nThis is what most sociologists should report:\r\n\r\n```stata\r\nlogit union wage age grade, robust\r\nmargins, dydx(*)\r\n```\r\n\r\nInterpretation: \"A one-unit increase in wage is associated with a 1.25 percentage point increase in the probability of union membership.\"\r\n\r\n### Predicted Probabilities at Specific Values\r\n\r\n```stata\r\n* Probability at specific covariate values\r\nmargins, at(wage=(5 10 15) age=35 grade=12)\r\n```\r\n\r\n### Odds Ratios\r\n\r\n```stata\r\n* Report odds ratios instead of coefficients\r\nlogit union wage age grade, or\r\n```\r\n\r\nInterpretation: Odds ratio of 1.07 means \"each dollar increase in wage is associated with 7% higher odds of union membership.\"\r\n\r\n### Choosing What to Report\r\n\r\n| Format | When to Use |\r\n|--------|-------------|\r\n| Marginal effects | Most journal articles; easy to interpret |\r\n| Odds ratios | Clinical/epidemiological conventions |\r\n| Raw coefficients | Technical appendix; rarely main table |\r\n\r\n### Goodness-of-Fit Notes\r\n\r\n- **Pseudo-R is not R** - don't interpret as variance explained\r\n- **Classification tables are optional** - not decisive for model quality\r\n- Focus on coefficient interpretation, not fit statistics\r\n\r\n---\r\n\r\n## 4. Count Models: Poisson and Negative Binomial\r\n\r\n### When to Use Which\r\n\r\n- **Poisson**: Counts with variance  mean (equidispersion)\r\n- **Negative Binomial**: Counts with variance > mean (overdispersion)\r\n- **Rule of thumb**: If variance >> mean, use negative binomial\r\n\r\n### Poisson with Robust SEs\r\n\r\n```stata\r\npoisson y_count x1 x2, robust\r\nestimates store pois1\r\n\r\n* Predicted counts\r\nmargins, at(x1=(-1 0 1) x2=0)\r\n```\r\n\r\n### Negative Binomial\r\n\r\n```stata\r\nnbreg y_count x1 x2, robust\r\nestimates store nb1\r\n\r\n* Compare Poisson vs NB\r\nesttab pois1 nb1, se star(* 0.10 ** 0.05 *** 0.01) ///\r\n    mtitles(\"Poisson\" \"Neg Binomial\")\r\n```\r\n\r\n### Incidence Rate Ratios\r\n\r\n```stata\r\npoisson y_count x1 x2, irr\r\n```\r\n\r\nInterpretation: IRR of 1.33 means \"a one-unit increase in x1 is associated with 33% more events.\"\r\n\r\n---\r\n\r\n## 5. Ordinal Outcomes\r\n\r\n### Ordered Logit\r\n\r\n```stata\r\nsysuse auto, clear\r\n\r\n* rep78 is 1-5 repair record\r\nologit rep78 price mpg foreign, robust\r\n```\r\n\r\n### Predicted Probabilities by Category\r\n\r\n```stata\r\n* Probability of being in category 1\r\nmargins, predict(outcome(1))\r\n\r\n* Probability of being in category 5\r\nmargins, predict(outcome(5))\r\n\r\n* Predicted probabilities for all categories\r\nmargins, predict(outcome(1)) predict(outcome(2)) predict(outcome(3)) ///\r\n    predict(outcome(4)) predict(outcome(5))\r\n```\r\n\r\n---\r\n\r\n## 6. Multilevel Models (Optional)\r\n\r\nUse when you have hierarchical data (students in schools, respondents in states) and theory suggests random effects.\r\n\r\n### Random Intercept (Linear)\r\n\r\n```stata\r\n* Students (i) nested in schools (j)\r\nmixed test_score ses || school:\r\n```\r\n\r\n### Random Intercept (Binary)\r\n\r\n```stata\r\n* Binary outcome with random intercepts\r\nmelogit graduated ses || school:\r\n```\r\n\r\n### Key Distinction\r\n\r\n- **Cluster-robust SEs**: Adjusts SEs for clustering; coefficients are population-averaged\r\n- **Multilevel models**: Explicitly models variance at each level; coefficients are conditional on random effects\r\n\r\nChoose based on your research question and theory, not convenience.\r\n\r\n---\r\n\r\n## 7. Diagnostics\r\n\r\n### Multicollinearity\r\n\r\n```stata\r\nreg price mpg weight length, robust\r\nestat vif\r\n```\r\n\r\n**Rule of thumb**: VIF > 10 suggests problematic collinearity.\r\n\r\n### Influential Observations\r\n\r\n```stata\r\nreg price mpg weight\r\n\r\n* Leverage (unusual X values)\r\npredict leverage, leverage\r\ngen high_leverage = leverage > 2*(3+1)/74  // 2*(k+1)/n\r\n\r\n* Cook's Distance (influential points)\r\npredict cooksd, cooksd\r\ngen influential = cooksd > 4/74  // 4/n\r\n\r\ntab high_leverage\r\ntab influential\r\n```\r\n\r\n### Residual Plots\r\n\r\n```stata\r\nreg price mpg weight, robust\r\npredict resid, residual\r\npredict yhat, xb\r\n\r\n* Histogram of residuals\r\nhistogram resid, normal title(\"Residual Distribution\")\r\ngraph export \"$figures/residual_hist.png\", replace\r\n\r\n* Residual vs fitted\r\nscatter resid yhat, yline(0, lpattern(dash)) ///\r\n    title(\"Residuals vs Fitted\")\r\ngraph export \"$figures/resid_fitted.png\", replace\r\n```\r\n\r\n---\r\n\r\n## Quick Reference\r\n\r\n### OLS Variants\r\n\r\n| Task | Command |\r\n|------|---------|\r\n| Robust SE | `reg y x, robust` |\r\n| Clustered SE | `reg y x, cluster(id)` |\r\n| Two-way cluster | `reghdfe y x, cluster(id time)` |\r\n\r\n### Binary Models\r\n\r\n| Task | Command |\r\n|------|---------|\r\n| Logit | `logit y x, robust` |\r\n| Probit | `probit y x, robust` |\r\n| Odds ratios | `logit y x, or` |\r\n| Marginal effects | `margins, dydx(*)` |\r\n\r\n### Count Models\r\n\r\n| Task | Command |\r\n|------|---------|\r\n| Poisson | `poisson y x, robust` |\r\n| Negative binomial | `nbreg y x, robust` |\r\n| IRR | `poisson y x, irr` |\r\n\r\n### Margins Commands\r\n\r\n| Task | Command |\r\n|------|---------|\r\n| AME for all | `margins, dydx(*)` |\r\n| At specific values | `margins, at(x=(1 2 3))` |\r\n| By group | `margins group, dydx(x)` |\r\n| Plot | `marginsplot` |\r\n",
        "plugins/stata-analyst/skills/stata-analyst/techniques/07_postestimation_reporting.md": "# Post-Estimation and Reporting in Stata\r\n\r\nWhat you do after you estimateso outputs match journal expectations. All code tested on Stata 15+.\r\n\r\n---\r\n\r\n## 1. Estimates Workflow\r\n\r\n### Store Results\r\n\r\n```stata\r\nsysuse auto, clear\r\n\r\nreg price mpg, robust\r\nestimates store base\r\n\r\nreg price mpg weight, robust\r\nestimates store controls1\r\n\r\nreg price mpg weight i.foreign, robust\r\nestimates store controls2\r\n\r\nreg price mpg weight i.foreign headroom trunk, robust\r\nestimates store full\r\n```\r\n\r\n### List and Retrieve\r\n\r\n```stata\r\n* List all stored estimates\r\nestimates dir\r\n\r\n* Replay a specific model\r\nestimates replay controls1\r\n\r\n* Restore for post-estimation commands\r\nestimates restore full\r\ndisplay \"R-squared: \" e(r2)\r\n```\r\n\r\n### Naming Conventions\r\n\r\nUse consistent, descriptive names:\r\n\r\n| Pattern | Example |\r\n|---------|---------|\r\n| By specification | `base`, `controls`, `full` |\r\n| By sample | `main`, `robust_sample`, `placebo` |\r\n| By method | `ols`, `iv`, `matching` |\r\n\r\n---\r\n\r\n## 2. Nested Tables with esttab\r\n\r\n### Standard Journal Format\r\n\r\n```stata\r\nesttab base controls1 controls2 full, ///\r\n    se star(* 0.10 ** 0.05 *** 0.01) ///\r\n    mtitles(\"(1)\" \"(2)\" \"(3)\" \"(4)\") ///\r\n    title(\"Table 2: Price Determinants\") ///\r\n    addnotes(\"Robust standard errors in parentheses\") ///\r\n    drop(0.foreign) ///\r\n    stats(N r2, labels(\"N\" \"R-squared\") fmt(%9.0fc %9.3f))\r\n```\r\n\r\n### Export to RTF (Word-Compatible)\r\n\r\n```stata\r\nesttab base controls1 controls2 full using \"$tables/table2_models.rtf\", ///\r\n    replace ///\r\n    se star(* 0.10 ** 0.05 *** 0.01) ///\r\n    mtitles(\"(1)\" \"(2)\" \"(3)\" \"(4)\") ///\r\n    title(\"Table 2: Price Determinants\") ///\r\n    drop(0.foreign) ///\r\n    stats(N r2, labels(\"N\" \"R-squared\") fmt(%9.0fc %9.3f))\r\n```\r\n\r\n### Export to CSV\r\n\r\n```stata\r\nesttab base controls1 controls2 full using \"$tables/table2_models.csv\", ///\r\n    replace csv ///\r\n    se star(* 0.10 ** 0.05 *** 0.01)\r\n```\r\n\r\n### Export to LaTeX\r\n\r\n```stata\r\nesttab base controls1 controls2 full using \"$tables/table2_models.tex\", ///\r\n    replace booktabs ///\r\n    se star(* 0.10 ** 0.05 *** 0.01) ///\r\n    mtitles(\"(1)\" \"(2)\" \"(3)\" \"(4)\") ///\r\n    drop(0.foreign) ///\r\n    stats(N r2, labels(\"N\" \"R-squared\") fmt(%9.0fc %9.3f))\r\n```\r\n\r\n---\r\n\r\n## 3. Predicted Values and Effects\r\n\r\n### Predicted Values for Hypothetical Cases\r\n\r\n```stata\r\nreg price mpg weight i.foreign, robust\r\n\r\n* Domestic car with mpg=20, weight=3000\r\nmargins, at(mpg=20 weight=3000 foreign=0)\r\n\r\n* Foreign car with same specs\r\nmargins, at(mpg=20 weight=3000 foreign=1)\r\n```\r\n\r\n### Save Predictions to Data\r\n\r\n```stata\r\nreg price mpg weight i.foreign, robust\r\npredict yhat, xb          // predicted values\r\npredict resid, residual   // residuals\r\n\r\nsummarize price yhat resid\r\n```\r\n\r\n### Marginal Effects for Presentation\r\n\r\n```stata\r\nlogit union wage age grade, robust\r\n\r\n* Average marginal effects\r\nmargins, dydx(*)\r\n\r\n* Save to matrix for export\r\nmargins, dydx(*) post\r\nesttab, cells(\"b(fmt(4)) se(fmt(4)) p(fmt(3))\") ///\r\n    title(\"Average Marginal Effects\")\r\n```\r\n\r\n### Marginsplot\r\n\r\n```stata\r\nreg price i.foreign##c.mpg, robust\r\nmargins foreign, at(mpg=(15(5)35))\r\nmarginsplot, title(\"Predicted Price by MPG and Origin\") ///\r\n    ytitle(\"Predicted Price ($)\") xtitle(\"Miles per Gallon\")\r\ngraph export \"$figures/figure1_margins.png\", replace\r\n```\r\n\r\n---\r\n\r\n## 4. Model Diagnostics\r\n\r\n### Multicollinearity (VIF)\r\n\r\n```stata\r\nreg price mpg weight length, robust\r\nestat vif\r\n```\r\n\r\n| VIF | Interpretation |\r\n|-----|----------------|\r\n| < 5 | Generally acceptable |\r\n| 5-10 | Moderate concern |\r\n| > 10 | High collinearity |\r\n\r\n### Influential Points\r\n\r\n```stata\r\nreg price mpg weight\r\npredict leverage, leverage\r\npredict cooksd, cooksd\r\n\r\n* Flag problematic observations\r\ngen high_leverage = leverage > 2*(3+1)/_N  // 2*(k+1)/n rule\r\ngen influential = cooksd > 4/_N            // 4/n rule\r\n\r\ntab high_leverage\r\ntab influential\r\nlist make price mpg weight if influential == 1\r\n```\r\n\r\n### Residual Diagnostics\r\n\r\n```stata\r\nreg price mpg weight, robust\r\npredict resid, residual\r\npredict yhat, xb\r\n\r\n* Residual histogram\r\nhistogram resid, normal title(\"Residual Distribution\")\r\ngraph export \"$figures/residual_hist.png\", replace\r\n\r\n* Residual vs fitted plot\r\nscatter resid yhat, yline(0, lpattern(dash)) ///\r\n    title(\"Residuals vs Fitted Values\")\r\ngraph export \"$figures/resid_fitted.png\", replace\r\n```\r\n\r\n### Heteroskedasticity\r\n\r\nRobust SEs handle this, but to test formally:\r\n\r\n```stata\r\nreg price mpg weight\r\nestat hettest\r\n```\r\n\r\n---\r\n\r\n## 5. Table 1: Descriptive Statistics\r\n\r\n### Overall Summary\r\n\r\n```stata\r\ntabstat price mpg weight, stat(n mean sd min max) col(stat)\r\n```\r\n\r\n### Summary by Group\r\n\r\n```stata\r\ntabstat price mpg weight, by(foreign) stat(n mean sd) nototal\r\n```\r\n\r\n### Publication-Ready Table 1\r\n\r\n```stata\r\n* Using estpost + esttab\r\nestpost summarize price mpg weight length\r\nesttab, cells(\"count mean(fmt(2)) sd(fmt(2)) min max\") ///\r\n    nomtitle nonumber title(\"Table 1: Descriptive Statistics\")\r\n```\r\n\r\n### Export Summary Statistics\r\n\r\n```stata\r\nestpost summarize price mpg weight length\r\nesttab using \"$tables/table1_descriptives.rtf\", ///\r\n    replace ///\r\n    cells(\"count mean(fmt(2)) sd(fmt(2)) min max\") ///\r\n    nomtitle nonumber ///\r\n    title(\"Table 1: Descriptive Statistics\")\r\n```\r\n\r\n---\r\n\r\n## 6. Balance Tables\r\n\r\nFor treatment/control comparisons:\r\n\r\n### Simple Balance Check\r\n\r\n```stata\r\n* Means by treatment\r\ntabstat mpg weight length, by(treat) stat(mean sd) nototal\r\n```\r\n\r\n### T-Tests for Differences\r\n\r\n```stata\r\nttest mpg, by(treat)\r\nttest weight, by(treat)\r\nttest length, by(treat)\r\n```\r\n\r\n### Formal Balance Table\r\n\r\n```stata\r\n* Create treatment indicator\r\ngen treat = (price > 6000)\r\n\r\n* Compare means and run t-tests\r\nforeach var in mpg weight length {\r\n    display \"\"\r\n    display \"=== `var' ===\"\r\n    ttest `var', by(treat)\r\n}\r\n```\r\n\r\n---\r\n\r\n## 7. Output Naming Conventions\r\n\r\n### Standard File Names\r\n\r\n```stata\r\n* Tables\r\n\"$tables/table1_descriptives.rtf\"\r\n\"$tables/table2_main_models.rtf\"\r\n\"$tables/table3_robustness.rtf\"\r\n\"$tables/tableA1_balance.rtf\"  // appendix\r\n\r\n* Figures\r\n\"$figures/figure1_trends.png\"\r\n\"$figures/figure2_event_study.png\"\r\n\"$figures/figureA1_placebo.png\"  // appendix\r\n```\r\n\r\n### In-Code Documentation\r\n\r\n```stata\r\n* ===================================\r\n* Table 2: Main Regression Results\r\n* ===================================\r\nreg price mpg weight i.foreign, robust\r\nestimates store m1\r\n// ... more models\r\n\r\nesttab m1 m2 m3 using \"$tables/table2_main_models.rtf\", replace ...\r\n```\r\n\r\n---\r\n\r\n## 8. Complete Output Workflow Example\r\n\r\n```stata\r\n* ===========================================================================\r\n* Output Generation Script\r\n* ===========================================================================\r\n\r\nclear all\r\nset more off\r\nuse \"$clean/analysis_sample.dta\", clear\r\n\r\n* -----------------------------------\r\n* Table 1: Descriptive Statistics\r\n* -----------------------------------\r\nestpost summarize price mpg weight length\r\nesttab using \"$tables/table1_descriptives.rtf\", replace ///\r\n    cells(\"count mean(fmt(2)) sd(fmt(2)) min max\") nomtitle nonumber\r\n\r\n* -----------------------------------\r\n* Table 2: Main Models\r\n* -----------------------------------\r\nreg price mpg, robust\r\nestimates store m1\r\n\r\nreg price mpg weight, robust\r\nestimates store m2\r\n\r\nreg price mpg weight i.foreign, robust\r\nestimates store m3\r\n\r\nesttab m1 m2 m3 using \"$tables/table2_main_models.rtf\", replace ///\r\n    se star(* 0.10 ** 0.05 *** 0.01) ///\r\n    mtitles(\"(1)\" \"(2)\" \"(3)\") ///\r\n    stats(N r2, labels(\"N\" \"R-squared\") fmt(%9.0fc %9.3f))\r\n\r\n* -----------------------------------\r\n* Figure 1: Coefficient Plot\r\n* -----------------------------------\r\nreg price mpg weight headroom trunk length, robust\r\ncoefplot, drop(_cons) xline(0) ///\r\n    title(\"Coefficient Estimates\")\r\ngraph export \"$figures/figure1_coef_plot.png\", replace\r\n\r\ndisplay \"Output generation complete!\"\r\n```\r\n\r\n---\r\n\r\n## Quick Reference\r\n\r\n### esttab Options\r\n\r\n| Option | Purpose |\r\n|--------|---------|\r\n| `se` | Show standard errors |\r\n| `star(* .10 ** .05 *** .01)` | Significance stars |\r\n| `mtitles()` | Column titles |\r\n| `drop()` | Omit variables |\r\n| `stats(N r2)` | Add statistics |\r\n| `replace` | Overwrite file |\r\n\r\n### Export Formats\r\n\r\n| Format | Command |\r\n|--------|---------|\r\n| RTF (Word) | `using \"file.rtf\"` |\r\n| CSV | `using \"file.csv\", csv` |\r\n| LaTeX | `using \"file.tex\", booktabs` |\r\n| Screen | no `using` clause |\r\n\r\n### Margins Commands\r\n\r\n| Task | Command |\r\n|------|---------|\r\n| Average marginal effects | `margins, dydx(*)` |\r\n| At specific values | `margins, at(x=(1 2 3))` |\r\n| Plot | `marginsplot` |\r\n| Export to table | `margins, dydx(*) post` then `esttab` |\r\n",
        "plugins/stata-analyst/skills/stata-analyst/techniques/99_default_journal_pipeline.md": "# Default Journal Analysis Pipeline\r\n\r\nA step-by-step workflow for sociological research. Copy this structure for new projects.\r\n\r\n---\r\n\r\n## Project Structure\r\n\r\n```\r\nproject/\r\n master.do              # Run everything\r\n code/\r\n    01_import.do       # Data import\r\n    02_clean.do        # Cleaning and construction\r\n    03_analysis.do     # Main models\r\n    04_output.do       # Tables and figures\r\n data/\r\n    raw/               # Original data (never modify)\r\n    clean/             # Analysis-ready datasets\r\n    temp/              # Intermediate files\r\n output/\r\n    tables/            # RTF/CSV/TeX tables\r\n    figures/           # PNG/PDF figures\r\n logs/                  # Log files\r\n```\r\n\r\n---\r\n\r\n## Master Do-File Template\r\n\r\n```stata\r\n* ===========================================================================\r\n* Master Do-File: [Project Name]\r\n* Author: [Name]\r\n* Date: [Date]\r\n* ===========================================================================\r\n\r\nclear all\r\nset more off\r\nversion 15\r\nset seed 12345\r\n\r\n* -----------------------------------\r\n* Set Paths\r\n* -----------------------------------\r\nglobal root \"/path/to/project\"\r\nglobal code \"$root/code\"\r\nglobal raw \"$root/data/raw\"\r\nglobal clean \"$root/data/clean\"\r\nglobal tables \"$root/output/tables\"\r\nglobal figures \"$root/output/figures\"\r\nglobal logs \"$root/logs\"\r\n\r\n* -----------------------------------\r\n* Run Analysis\r\n* -----------------------------------\r\ndo \"$code/01_import.do\"\r\ndo \"$code/02_clean.do\"\r\ndo \"$code/03_analysis.do\"\r\ndo \"$code/04_output.do\"\r\n\r\ndisplay \"Analysis complete: $S_DATE $S_TIME\"\r\n```\r\n\r\n---\r\n\r\n## The Canonical Sequence\r\n\r\n### 1. Data Preparation (`01_import.do`, `02_clean.do`)\r\n\r\n```stata\r\n* ===========================================================================\r\n* 01_import.do\r\n* ===========================================================================\r\n\r\nlog using \"$logs/01_import.log\", replace\r\n\r\n* Import raw data\r\nimport delimited \"$raw/survey_data.csv\", clear varnames(1)\r\n\r\n* Quick checks\r\ndescribe\r\ncodebook, compact\r\n\r\n* Save imported version\r\ncompress\r\nsave \"$clean/data_imported.dta\", replace\r\n\r\nlog close\r\n```\r\n\r\n```stata\r\n* ===========================================================================\r\n* 02_clean.do\r\n* ===========================================================================\r\n\r\nlog using \"$logs/02_clean.log\", replace\r\n\r\nuse \"$clean/data_imported.dta\", clear\r\n\r\n* -----------------------------------\r\n* Merge additional data\r\n* -----------------------------------\r\nmerge m:1 state using \"$raw/state_controls.dta\"\r\ntab _merge\r\nassert _merge != 2\r\ndrop _merge\r\n\r\n* -----------------------------------\r\n* Construct variables\r\n* -----------------------------------\r\n* Treatment\r\ngen treat = (policy_year <= year) & !missing(policy_year)\r\n\r\n* Log income\r\ngen ln_income = ln(income + 1)\r\n\r\n* Standardized test score\r\negen z_score = std(test_score)\r\n\r\n* -----------------------------------\r\n* Sample restrictions\r\n* -----------------------------------\r\ndisplay \"Initial N: \" _N\r\n\r\ndrop if missing(outcome)\r\ndisplay \"After dropping missing outcome: \" _N\r\n\r\ndrop if missing(treat)\r\ndisplay \"After dropping missing treatment: \" _N\r\n\r\ngen sample_main = 1\r\n\r\n* -----------------------------------\r\n* Save analysis sample\r\n* -----------------------------------\r\ncompress\r\nlabel data \"Analysis sample, created $S_DATE\"\r\nsave \"$clean/analysis_sample.dta\", replace\r\n\r\nlog close\r\n```\r\n\r\n### 2. Descriptive Statistics\r\n\r\n```stata\r\n* -----------------------------------\r\n* Table 1: Descriptive Statistics\r\n* -----------------------------------\r\nuse \"$clean/analysis_sample.dta\", clear\r\n\r\n* Overall descriptives\r\nestpost summarize outcome treat income age education\r\nesttab using \"$tables/table1_descriptives.rtf\", replace ///\r\n    cells(\"count mean(fmt(2)) sd(fmt(2)) min max\") ///\r\n    nomtitle nonumber ///\r\n    title(\"Table 1: Descriptive Statistics\")\r\n\r\n* Descriptives by treatment\r\ntabstat outcome income age, by(treat) stat(mean sd n) nototal\r\n```\r\n\r\n### 3. Main Models (`03_analysis.do`)\r\n\r\n```stata\r\n* ===========================================================================\r\n* 03_analysis.do\r\n* ===========================================================================\r\n\r\nlog using \"$logs/03_analysis.log\", replace\r\n\r\nuse \"$clean/analysis_sample.dta\", clear\r\n\r\n* -----------------------------------\r\n* Table 2: Main Results\r\n* -----------------------------------\r\n\r\n* Model 1: Bivariate\r\nreg outcome treat, robust\r\nestimates store m1\r\n\r\n* Model 2: Add controls\r\nreg outcome treat income age education, robust\r\nestimates store m2\r\n\r\n* Model 3: Fixed effects\r\nreghdfe outcome treat income age, absorb(state year) cluster(state)\r\nestimates store m3\r\n\r\n* Model 4: Full specification\r\nreghdfe outcome treat income age education, absorb(state year) cluster(state)\r\nestimates store m4\r\n\r\n* Save estimates for output\r\nestimates save \"$clean/main_estimates\", replace\r\n\r\nlog close\r\n```\r\n\r\n### 4. Robustness Checks\r\n\r\n```stata\r\n* -----------------------------------\r\n* Table 3: Robustness\r\n* -----------------------------------\r\n\r\n* Alternative control group\r\n* Alternative clustering\r\n* Alternative sample\r\n* Placebo test\r\n\r\n* Model with alternative clustering\r\nreghdfe outcome treat income age, absorb(state year) cluster(state year)\r\nestimates store robust_twoway\r\n\r\n* Placebo outcome\r\nreghdfe placebo_outcome treat income age, absorb(state year) cluster(state)\r\nestimates store placebo\r\n```\r\n\r\n### 5. Output Generation (`04_output.do`)\r\n\r\n```stata\r\n* ===========================================================================\r\n* 04_output.do\r\n* ===========================================================================\r\n\r\nlog using \"$logs/04_output.log\", replace\r\n\r\nestimates use \"$clean/main_estimates\"\r\n\r\n* -----------------------------------\r\n* Table 2: Main Results\r\n* -----------------------------------\r\nesttab m1 m2 m3 m4 using \"$tables/table2_main_results.rtf\", replace ///\r\n    se star(* 0.10 ** 0.05 *** 0.01) ///\r\n    mtitles(\"(1)\" \"(2)\" \"(3)\" \"(4)\") ///\r\n    title(\"Table 2: Main Results\") ///\r\n    drop(0.* _cons) ///\r\n    stats(N r2, labels(\"N\" \"R-squared\") fmt(%9.0fc %9.3f)) ///\r\n    addnotes(\"Robust standard errors in parentheses.\" ///\r\n             \"Models 3-4 include state and year fixed effects.\" ///\r\n             \"Standard errors clustered at state level.\")\r\n\r\n* -----------------------------------\r\n* Figure 1: Coefficient Plot\r\n* -----------------------------------\r\nestimates restore m4\r\ncoefplot, drop(_cons) xline(0) ///\r\n    title(\"Effect of Treatment\") ///\r\n    xtitle(\"Coefficient Estimate\")\r\ngraph export \"$figures/figure1_coef_plot.png\", replace\r\n\r\n* -----------------------------------\r\n* Figure 2: Predicted Values\r\n* -----------------------------------\r\nuse \"$clean/analysis_sample.dta\", clear\r\nreg outcome i.treat##c.income, robust\r\nmargins treat, at(income=(20000(10000)80000))\r\nmarginsplot, title(\"Predicted Outcome by Treatment Status\") ///\r\n    ytitle(\"Predicted Outcome\") xtitle(\"Income\")\r\ngraph export \"$figures/figure2_margins.png\", replace\r\n\r\nlog close\r\n\r\ndisplay \"Output generation complete!\"\r\n```\r\n\r\n---\r\n\r\n## Output Mapping\r\n\r\n### File Naming Conventions\r\n\r\n| Output | Filename |\r\n|--------|----------|\r\n| Descriptives | `table1_descriptives.rtf` |\r\n| Main results | `table2_main_results.rtf` |\r\n| Robustness | `table3_robustness.rtf` |\r\n| Mechanisms | `table4_mechanisms.rtf` |\r\n| Appendix balance | `tableA1_balance.rtf` |\r\n| Main figure | `figure1_[description].png` |\r\n| Appendix figure | `figureA1_[description].png` |\r\n\r\n### In-Code Documentation\r\n\r\nMark each table/figure creation clearly:\r\n\r\n```stata\r\n* ===================================\r\n* Table 2: Main Regression Results\r\n* ===================================\r\n```\r\n\r\n---\r\n\r\n## Minimal Project (Smaller Papers)\r\n\r\nFor simpler analyses, combine files:\r\n\r\n```stata\r\n* ===========================================================================\r\n* analysis.do - Complete Analysis\r\n* ===========================================================================\r\n\r\nclear all\r\nset more off\r\nversion 15\r\n\r\n* Paths\r\nglobal root \"/path/to/project\"\r\nglobal tables \"$root/tables\"\r\nglobal figures \"$root/figures\"\r\n\r\nlog using \"$root/analysis.log\", replace\r\n\r\n* -----------------------------------\r\n* Data Prep\r\n* -----------------------------------\r\nuse \"$root/data.dta\", clear\r\nkeep if !missing(outcome, treat, x1, x2)\r\ndisplay \"Analysis N: \" _N\r\n\r\n* -----------------------------------\r\n* Table 1: Descriptives\r\n* -----------------------------------\r\nestpost summarize outcome treat x1 x2\r\nesttab using \"$tables/table1.rtf\", replace ///\r\n    cells(\"mean sd min max\") nomtitle\r\n\r\n* -----------------------------------\r\n* Table 2: Main Models\r\n* -----------------------------------\r\nreg outcome treat, robust\r\nestimates store m1\r\n\r\nreg outcome treat x1 x2, robust\r\nestimates store m2\r\n\r\nesttab m1 m2 using \"$tables/table2.rtf\", replace ///\r\n    se star(* .10 ** .05 *** .01)\r\n\r\n* -----------------------------------\r\n* Figure 1: Coefficient Plot\r\n* -----------------------------------\r\ncoefplot m2, drop(_cons) xline(0)\r\ngraph export \"$figures/figure1.png\", replace\r\n\r\nlog close\r\n```\r\n\r\n---\r\n\r\n## Checklist Before Submission\r\n\r\n### Data and Replication\r\n- [ ] Raw data preserved (never modified)\r\n- [ ] All code runs from master.do\r\n- [ ] Random seed set for reproducibility\r\n- [ ] Sample sizes documented at each step\r\n\r\n### Tables\r\n- [ ] Table 1: Descriptive statistics (overall and by group)\r\n- [ ] Table 2+: Regression results with nested specifications\r\n- [ ] SEs and significance stars clearly noted\r\n- [ ] N and fit statistics included\r\n\r\n### Figures\r\n- [ ] Clear titles and axis labels\r\n- [ ] High resolution (300+ DPI for publication)\r\n- [ ] Consistent styling across figures\r\n\r\n### Methods Section\r\n- [ ] Estimator described (OLS, FE, DiD, etc.)\r\n- [ ] Standard error approach documented\r\n- [ ] Sample restrictions listed\r\n- [ ] Key assumptions acknowledged\r\n\r\n---\r\n\r\n## Common Patterns\r\n\r\n### Store Multiple Models\r\n\r\n```stata\r\nforeach spec in base controls full {\r\n    reg outcome treat `controls_`spec'', robust\r\n    estimates store `spec'\r\n}\r\nesttab base controls full, se\r\n```\r\n\r\n### Export with Comments\r\n\r\n```stata\r\n* // Table 2 in do-file marks where table is created\r\nesttab m1 m2 using \"$tables/table2.rtf\", replace ...\r\n```\r\n\r\n### Consistent Graph Style\r\n\r\n```stata\r\n* Set graph scheme at start\r\nset scheme s2color\r\ngraph set window fontface \"Arial\"\r\n```",
        "plugins/stata-analyst/skills/stata-analyst/techniques/stata-statistical-techniques_combined.md": "### 00_index.md\n\n# Stata Statistical Techniques - Index\r\n\r\nQuick lookup for statistical methods in Stata. All code tested on Stata 15.1.\r\n\r\n---\r\n\r\n## File Guide\r\n\r\n| File | Topics |\r\n|------|--------|\r\n| `01_core_econometrics.md` | TWFE, DiD, Event Studies, IV, Matching, Mediation, Standard Errors |\r\n| `02_survey_resampling.md` | Survey Weights, Bootstrap, Randomization Inference, Oaxaca |\r\n| `03_synthetic_control.md` | synth package for comparative case studies |\r\n| `04_visualization.md` | esttab tables, coefplot, graphs, summary statistics |\r\n| `05_best_practices.md` | Master do-files, path management, code organization |\r\n\r\n---\r\n\r\n## Quick Lookup by Method\r\n\r\n### Panel & Fixed Effects\r\n- **TWFE (reghdfe)**  `01_core_econometrics.md` Section 1\r\n\r\n### Difference-in-Differences\r\n- **Traditional DiD**  `01_core_econometrics.md` Section 2.1\r\n- **Callaway-Sant'Anna (csdid)**  `01_core_econometrics.md` Section 2.2\r\n- **Event Studies**  `01_core_econometrics.md` Section 3\r\n\r\n### Instrumental Variables\r\n- **Basic 2SLS (ivreg2)**  `01_core_econometrics.md` Section 4\r\n- **First-stage diagnostics**  `01_core_econometrics.md` Section 4.2\r\n- **Overidentification tests**  `01_core_econometrics.md` Section 4.3\r\n\r\n### Matching\r\n- **Propensity Score (psmatch2)**  `01_core_econometrics.md` Section 5.1\r\n- **Coarsened Exact (cem)**  `01_core_econometrics.md` Section 5.3\r\n- **Balance testing (pstest)**  `01_core_econometrics.md` Section 5.2\r\n\r\n### Standard Errors & Inference\r\n- **Clustered SEs**  `01_core_econometrics.md` Section 6.1\r\n- **Wild Cluster Bootstrap**  `01_core_econometrics.md` Section 6.2\r\n- **Randomization Inference**  `02_survey_resampling.md` Section 4\r\n\r\n### Survey Methods\r\n- **Survey design (svyset)**  `02_survey_resampling.md` Section 1\r\n- **Weighted estimation (svy:)**  `02_survey_resampling.md` Section 1\r\n- **Subpopulation analysis**  `02_survey_resampling.md` Section 1\r\n\r\n### Decomposition\r\n- **Oaxaca-Blinder**  `02_survey_resampling.md` Section 5\r\n\r\n### Synthetic Control\r\n- **synth package**  `03_synthetic_control.md`\r\n\r\n### Output & Visualization\r\n- **Regression tables (esttab)**  `04_visualization.md` Section 1\r\n- **Coefficient plots (coefplot)**  `04_visualization.md` Section 2\r\n- **Summary statistics**  `04_visualization.md` Section 3\r\n\r\n---\r\n\r\n## Package Quick Reference\r\n\r\n| Task | Package | Command |\r\n|------|---------|---------|\r\n| High-dim FE | `reghdfe` | `reghdfe y x, absorb(id year) cluster(id)` |\r\n| Modern DiD | `csdid` | `csdid y, ivar(id) time(year) gvar(gvar) notyet` |\r\n| DiD event study | `csdid_estat` | `csdid_estat event` |\r\n| IV estimation | `ivreg2` | `ivreg2 y (endog = iv), first robust` |\r\n| PSM | `psmatch2` | `psmatch2 treat x1 x2, outcome(y)` |\r\n| Balance test | `pstest` | `pstest x1 x2, both` |\r\n| CEM | `cem` | `cem x1 (bins) x2 (#5), treatment(treat)` |\r\n| Wild bootstrap | `boottest` | `boottest x, cluster(c) reps(999) nograph` |\r\n| Randomization | `ritest` | `ritest treat _b[treat], reps(500):` |\r\n| Oaxaca | `oaxaca` | `oaxaca y x1 x2, by(group)` |\r\n| Synth control | `synth` | `synth y x1 y(1985), trunit(1) trperiod(1990)` |\r\n| Tables | `estout` | `esttab m1 m2, se star(* .10 ** .05 *** .01)` |\r\n| Coef plots | `coefplot` | `coefplot, drop(_cons) xline(0)` |\r\n| Survey | built-in | `svyset psu [pw=wt], strata(strat)` |\r\n\r\n---\r\n\r\n## Built-in Datasets for Examples\r\n\r\n| Dataset | Command | Variables |\r\n|---------|---------|-----------|\r\n| auto | `sysuse auto` | price, mpg, weight, foreign |\r\n| nlsw88 | `sysuse nlsw88` | wage, union, age, grade, tenure |\r\n| nhanes2f | `webuse nhanes2f` | height, weight, age, sex, finalwgt |\r\n\r\n---\r\n\r\n## Version Notes\r\n\r\n- **Stata 15+**: All methods except rdrobust\r\n- **Stata 16+**: rdrobust (RD estimation)\r\n\n\n---\n\n### 01_core_econometrics.md\n\n# Core Econometrics in Stata\r\n\r\nPanel methods, causal inference, and standard errors. All code tested on Stata 15+.\r\n\r\n---\r\n\r\n## 1. Two-Way Fixed Effects (TWFE)\r\n\r\n### When to Use\r\n- Panel data with unit and time dimensions\r\n- Unobserved unit and time confounders\r\n- Consistent treatment timing (or verified no negative weights)\r\n\r\n### Basic TWFE with reghdfe\r\n\r\n```stata\r\n* Declare panel\r\nxtset id period\r\n\r\n* TWFE with unit and time fixed effects, clustered SEs\r\nreghdfe y x1 treat, absorb(id period) cluster(id)\r\n```\r\n\r\n### Two-Way Clustering\r\n\r\n```stata\r\n* Cluster by both unit and time\r\nreghdfe y x1 treat, absorb(id period) cluster(id period)\r\n```\r\n\r\n### Store and Compare Models\r\n\r\n```stata\r\n* Run models\r\nquietly reghdfe y x1 treat, absorb(id period) cluster(id)\r\nestimates store m1_oneway\r\n\r\nquietly reghdfe y x1 treat, absorb(id period) cluster(id period)\r\nestimates store m2_twoway\r\n\r\n* Comparison table\r\nesttab m1_oneway m2_twoway, se mtitle(\"One-way\" \"Two-way\") ///\r\n    star(* 0.10 ** 0.05 *** 0.01)\r\n```\r\n\r\n---\r\n\r\n## 2. Difference-in-Differences (DiD)\r\n\r\n### 2.1 Traditional DiD\r\n\r\n```stata\r\n* Create interaction term\r\ngen treat_post = treated_group * post_period\r\n\r\n* DiD regression with FE\r\nreghdfe y treat_post, absorb(id period) cluster(id)\r\n\r\n* Or explicit interaction\r\nreg y i.treated_group##i.post_period x1, cluster(id)\r\n```\r\n\r\n### 2.2 Callaway-Sant'Anna (Staggered Treatment)\r\n\r\nFor heterogeneous treatment effects with staggered timing:\r\n\r\n```stata\r\n* Data setup:\r\n* - gvar: first treatment period (0 for never-treated)\r\n* - id: panel unit identifier\r\n* - year: time period\r\n\r\n* Run csdid\r\ncsdid y, ivar(id) time(year) gvar(gvar) notyet\r\n\r\n* Event study aggregation\r\ncsdid_estat event\r\n\r\n* Simple ATT\r\ncsdid_estat simple\r\n```\r\n\r\n**Key options:**\r\n- `notyet`: Use not-yet-treated as control (recommended)\r\n- `never`: Use never-treated as control only\r\n\r\n### 2.3 Post-Estimation for csdid\r\n\r\n```stata\r\n* After running csdid:\r\n\r\n* Event study (dynamic effects)\r\ncsdid_estat event\r\n\r\n* Group-time ATTs\r\ncsdid_estat group\r\n\r\n* Calendar time ATTs\r\ncsdid_estat calendar\r\n\r\n* Simple overall ATT\r\ncsdid_estat simple\r\n```\r\n\r\n---\r\n\r\n## 3. Event Studies\r\n\r\n### Manual Event Study\r\n\r\n```stata\r\n* Create event time relative to treatment\r\ngen event_time = year - first_treat\r\nreplace event_time = -99 if missing(first_treat)  // Never treated\r\n\r\n* Bin endpoints\r\nreplace event_time = -5 if event_time < -5 & event_time != -99\r\nreplace event_time = 5 if event_time > 5 & event_time != -99\r\n\r\n* Create dummies (omit t=-1 as reference)\r\ntab event_time, gen(et_)\r\n\r\n* Estimate (drop reference period)\r\nreghdfe y et_1 et_2 et_3 et_4 et_6 et_7 et_8 et_9 et_10 et_11, ///\r\n    absorb(id year) cluster(id)\r\n```\r\n\r\n### Event Study from csdid\r\n\r\n```stata\r\n* Run csdid first\r\ncsdid y, ivar(id) time(year) gvar(gvar) notyet\r\n\r\n* Get event study estimates\r\ncsdid_estat event\r\n```\r\n\r\n---\r\n\r\n## 4. Instrumental Variables (IV)\r\n\r\n### 4.1 Basic 2SLS\r\n\r\n```stata\r\n* Built-in ivregress\r\nivregress 2sls y (x_endog = z), robust\r\nestat firststage\r\n\r\n* ivreg2 with more diagnostics\r\nivreg2 y (x_endog = z), first robust\r\n```\r\n\r\n### 4.2 IV Diagnostics\r\n\r\nivreg2 automatically reports:\r\n- **First-stage F-statistic**: Should be > 10 (Stock-Yogo rule)\r\n- **Kleibergen-Paap rk F**: Robust weak instrument test\r\n- **Anderson-Rubin CI**: Weak-instrument-robust confidence interval\r\n\r\n```stata\r\n* Detailed first-stage output\r\nivreg2 y (x_endog = z), first robust\r\n\r\n* Key output to check:\r\n* - F test of excluded instruments: F > 10\r\n* - Stock-Yogo critical values for comparison\r\n* - Kleibergen-Paap rk Wald F statistic\r\n```\r\n\r\n### 4.3 Multiple Instruments (Overidentification)\r\n\r\n```stata\r\n* Multiple instruments\r\nivreg2 y (x_endog = z1 z2), robust\r\n\r\n* Hansen J statistic (overidentification test) reported automatically\r\n* H0: All instruments are valid\r\n* High p-value = cannot reject validity\r\n```\r\n\r\n### 4.4 IV with Fixed Effects\r\n\r\n```stata\r\n* Using ivreghdfe (install: ssc install ivreghdfe)\r\nivreghdfe y (x_endog = z), absorb(id year) cluster(id)\r\n```\r\n\r\n---\r\n\r\n## 5. Matching Methods\r\n\r\n### 5.1 Propensity Score Matching\r\n\r\n```stata\r\n* psmatch2: estimate PS and match\r\npsmatch2 treat x1 x2 i.category, outcome(y) neighbor(1) common\r\n\r\n* Check balance\r\npstest x1 x2, both\r\n\r\n* The ATT is displayed in the output table\r\n```\r\n\r\n**Key options:**\r\n- `neighbor(k)`: k nearest neighbors\r\n- `caliper(c)`: Maximum PS distance\r\n- `common`: Restrict to common support\r\n\r\n### 5.2 Balance Assessment\r\n\r\n```stata\r\n* After psmatch2\r\npstest x1 x2, both\r\n\r\n* Output shows:\r\n* - Mean bias before/after matching\r\n* - % reduction in bias\r\n* - t-tests for balance\r\n```\r\n\r\n### 5.3 Coarsened Exact Matching (CEM)\r\n\r\n```stata\r\n* CEM with specified bins\r\ncem age (20 30 40 50 60) grade (#5), treatment(treat)\r\n\r\n* Estimate with CEM weights\r\nreg y treat x1 x2 [iweight=cem_weights], robust\r\n```\r\n\r\n---\r\n\r\n## 6. Standard Errors and Inference\r\n\r\n### 6.1 Clustered Standard Errors\r\n\r\n```stata\r\n* One-way clustering\r\nreghdfe y x treat, absorb(id period) cluster(id)\r\n\r\n* Two-way clustering\r\nreghdfe y x treat, absorb(id period) cluster(id period)\r\n```\r\n\r\n### 6.2 Wild Cluster Bootstrap (Few Clusters)\r\n\r\nWhen you have fewer than ~40 clusters:\r\n\r\n```stata\r\n* Run regression with clustering\r\nreg y treat x, cluster(state)\r\n\r\n* Wild cluster bootstrap\r\nboottest treat, cluster(state) reps(999) seed(12345) nograph\r\n\r\n* Output:\r\n* - Bootstrap t-statistic\r\n* - Bootstrap p-value\r\n* - 95% confidence interval\r\n```\r\n\r\n### 6.3 Standard Bootstrap\r\n\r\n```stata\r\n* Bootstrap standard errors\r\nbootstrap _b, reps(500) seed(12345): reg y x1 x2 treat\r\n\r\n* Bootstrap specific statistics\r\nbootstrap diff = (_b[treat]), reps(500) seed(12345): reg y x1 treat\r\n```\r\n\r\n### 6.4 Randomization Inference\r\n\r\n```stata\r\n* Permutation test with stratification\r\nritest treat _b[treat], reps(500) seed(12345) strata(block): ///\r\n    reg y treat, robust\r\n\r\n* Output shows:\r\n* - Observed coefficient\r\n* - p-value from permutation distribution\r\n```\r\n\r\n---\r\n\r\n## 7. Regression Discontinuity (RD)\r\n\r\n**Note:** The `rdrobust` package requires Stata 16+. For Stata 15, use manual polynomial approaches.\r\n\r\n### Manual RD (Stata 15 compatible)\r\n\r\n```stata\r\n* Create centered running variable\r\ngen x_centered = running_var - cutoff\r\n\r\n* Treatment indicator\r\ngen treat = (running_var >= cutoff)\r\n\r\n* Local linear regression (narrow bandwidth)\r\nreg y treat x_centered c.x_centered#i.treat if abs(x_centered) < bandwidth\r\n\r\n* The coefficient on treat is the RD estimate\r\n```\r\n\r\n### rdrobust (Stata 16+)\r\n\r\n```stata\r\n* Basic RD estimation\r\nrdrobust y running_var, c(0)\r\n\r\n* With options\r\nrdrobust y running_var, c(0) kernel(triangular) bwselect(mserd)\r\n\r\n* Manipulation test\r\nrddensity running_var, c(0)\r\n\r\n* RD plot\r\nrdplot y running_var, c(0)\r\n```\r\n\r\n---\r\n\r\n## 8. Causal Mediation\r\n\r\n```stata\r\n* Mediation analysis requires:\r\n* - Treatment variable (treat)\r\n* - Mediator variable (m)\r\n* - Outcome variable (y)\r\n\r\n* Using medeff (install: ssc install medeff)\r\n\r\n* Step 1: Mediator model\r\nreg m treat x1\r\n\r\n* Step 2: Outcome model\r\nreg y treat m x1\r\n\r\n* Step 3: Mediation effects\r\nmedeff (reg m treat x1) (reg y treat m x1), ///\r\n    treat(treat) mediate(m) sims(1000)\r\n\r\n* Output:\r\n* - ACME: Average Causal Mediation Effect (indirect)\r\n* - ADE: Average Direct Effect\r\n* - Total Effect: ACME + ADE\r\n* - Proportion Mediated: ACME / Total\r\n```\r\n\r\n---\r\n\r\n## Quick Reference\r\n\r\n### Package Installation\r\n\r\n```stata\r\n* Core packages\r\nssc install reghdfe, replace\r\nssc install ftools, replace\r\nssc install estout, replace\r\nssc install coefplot, replace\r\n\r\n* DiD methods\r\nssc install csdid, replace\r\nssc install drdid, replace\r\n\r\n* IV\r\nssc install ivreg2, replace\r\nssc install ranktest, replace\r\n\r\n* Matching\r\nssc install psmatch2, replace\r\nssc install cem, replace\r\n\r\n* Inference\r\nssc install boottest, replace\r\nssc install ritest, replace\r\n\r\n* RD (Stata 16+ only)\r\nssc install rdrobust, replace\r\nssc install rddensity, replace\r\n```\r\n\r\n### Command Quick Reference\r\n\r\n| Task | Command |\r\n|------|---------|\r\n| TWFE | `reghdfe y x, absorb(id t) cluster(id)` |\r\n| Modern DiD | `csdid y, ivar(id) time(t) gvar(g)` |\r\n| Event study | `csdid_estat event` |\r\n| IV | `ivreg2 y (endog = iv), first robust` |\r\n| PSM | `psmatch2 treat x, outcome(y)` |\r\n| CEM | `cem x1 x2, treatment(treat)` |\r\n| Wild bootstrap | `boottest x, cluster(c) nograph` |\r\n| Randomization | `ritest treat _b[treat], reps(500):` |\r\n\r\n### Diagnostic Checks\r\n\r\n1. **TWFE**: Check for negative weights with staggered timing\r\n2. **DiD**: Test parallel trends with event study pre-trends\r\n3. **IV**: First-stage F > 10, check overidentification\r\n4. **Matching**: Balance statistics with `pstest`\r\n5. **Clustering**: Use wild bootstrap with < 40 clusters\r\n\n\n---\n\n### 02_survey_resampling.md\n\n# Survey & Resampling Methods in Stata\r\n\r\nSurvey weights, bootstrap, randomization inference, and decomposition. All code tested on Stata 15+.\r\n\r\n---\r\n\r\n## 1. Survey Methods\r\n\r\n### Setup Survey Design\r\n\r\n```stata\r\n* Using nhanes2f dataset as example\r\nwebuse nhanes2f, clear\r\n\r\n* Setup: psuid = PSU, stratid = stratum, finalwgt = weight\r\nsvyset psuid [pweight=finalwgt], strata(stratid)\r\n```\r\n\r\n### Check Survey Setup\r\n\r\n```stata\r\nsvydescribe\r\n```\r\n\r\n### Survey-Weighted Estimation\r\n\r\n```stata\r\n* Survey mean\r\nsvy: mean height weight\r\n\r\n* Survey mean by group\r\nsvy: mean height, over(sex)\r\n\r\n* Survey regression\r\nsvy: reg height weight age i.sex\r\n\r\n* Survey logit\r\nsvy: logit highbp age i.sex weight\r\n```\r\n\r\n### Subpopulation Analysis\r\n\r\nCorrect way to analyze subgroups (maintains correct SEs):\r\n\r\n```stata\r\n* Analyze females only\r\nsvy, subpop(female): mean height weight\r\n\r\n* Analyze using if condition\r\nsvy, subpop(if age >= 40): mean bpsystol\r\n```\r\n\r\n**Note:** Always use `subpop()` rather than `if` for survey data to get correct variance estimates.\r\n\r\n---\r\n\r\n## 2. Bootstrap Methods\r\n\r\n### Standard Bootstrap\r\n\r\n```stata\r\nsysuse auto, clear\r\n\r\n* Bootstrap standard errors for all coefficients\r\nbootstrap _b, reps(500) seed(12345): reg price mpg weight foreign\r\n\r\n* Bootstrap specific statistic\r\nbootstrap diff = (_b[foreign]), reps(500) seed(12345): ///\r\n    reg price mpg weight foreign\r\n```\r\n\r\n### Bootstrap Options\r\n\r\n| Option | Purpose |\r\n|--------|---------|\r\n| `reps(#)` | Number of replications |\r\n| `seed(#)` | Random seed for reproducibility |\r\n| `bca` | Bias-corrected accelerated CIs |\r\n| `strata(var)` | Stratified resampling |\r\n| `cluster(var)` | Cluster resampling |\r\n\r\n---\r\n\r\n## 3. Wild Cluster Bootstrap\r\n\r\nFor inference with few clusters (< 40):\r\n\r\n```stata\r\n* Create clustered data\r\nclear\r\nset seed 12345\r\nset obs 500\r\ngen state = ceil(_n/50)\r\ngen treat = (state <= 5)\r\ngen x = rnormal()\r\ngen y = 1 + 0.5*treat + 0.3*x + rnormal(0, 1)\r\n\r\n* Standard clustered SE\r\nreg y treat x, cluster(state)\r\n\r\n* Wild cluster bootstrap\r\nboottest treat, cluster(state) reps(999) seed(12345) nograph\r\n```\r\n\r\n**Output includes:**\r\n- Bootstrap t-statistic\r\n- Bootstrap p-value\r\n- 95% confidence interval robust to few clusters\r\n\r\n---\r\n\r\n## 4. Randomization Inference\r\n\r\n```stata\r\n* Create experimental data\r\nclear\r\nset seed 12345\r\nset obs 200\r\ngen id = _n\r\ngen block = ceil(_n/20)\r\ngen treat = mod(_n, 2)\r\ngen y = 1 + 0.5*treat + rnormal(0, 1)\r\n\r\n* Standard OLS\r\nreg y treat, robust\r\n\r\n* Randomization inference with stratification\r\nritest treat _b[treat], reps(500) seed(12345) strata(block): ///\r\n    reg y treat, robust\r\n```\r\n\r\n**Output shows:**\r\n- Observed coefficient\r\n- Number of permutations where |T| >= |T(obs)|\r\n- Exact p-value from permutation distribution\r\n\r\n---\r\n\r\n## 5. Oaxaca-Blinder Decomposition\r\n\r\n```stata\r\n* Use nlsw88 data\r\nsysuse nlsw88, clear\r\nkeep if !missing(wage, union, age, grade, tenure)\r\n\r\n* Basic decomposition by union status\r\noaxaca wage age grade tenure, by(union)\r\n\r\n* Detailed decomposition\r\noaxaca wage age grade tenure, by(union) detail\r\n\r\n* Pooled reference coefficients\r\noaxaca wage age grade tenure, by(union) pooled\r\n```\r\n\r\n### Interpretation\r\n\r\nThe output shows:\r\n- **Endowments**: Differences due to characteristics (what if Group 2 had Group 1's X values)\r\n- **Coefficients**: Differences due to returns to characteristics (unexplained/discrimination)\r\n- **Interaction**: Combined effect\r\n\r\n---\r\n\r\n## Quick Reference\r\n\r\n### Survey Commands\r\n\r\n| Command | Purpose |\r\n|---------|---------|\r\n| `svyset` | Define survey design |\r\n| `svy:` | Prefix for survey estimation |\r\n| `svydescribe` | Describe survey design |\r\n| `subpop()` | Analyze subpopulation |\r\n\r\n### Resampling Commands\r\n\r\n| Command | Purpose |\r\n|---------|---------|\r\n| `bootstrap` | Standard bootstrap |\r\n| `boottest` | Wild cluster bootstrap |\r\n| `ritest` | Randomization inference |\r\n\r\n### Package Installation\r\n\r\n```stata\r\nssc install boottest, replace\r\nssc install ritest, replace\r\nssc install oaxaca, replace\r\n```\r\n\n\n---\n\n### 03_synthetic_control.md\n\n# Synthetic Control Methods in Stata\r\n\r\nSynthetic control method for comparative case studies. All code tested on Stata 15+.\r\n\r\n---\r\n\r\n## 1. Overview\r\n\r\nThe synthetic control method constructs a weighted combination of control units to serve as a counterfactual for a treated unit. It is designed for:\r\n\r\n- Single treated unit\r\n- Aggregate-level data (states, countries, regions)\r\n- Clear treatment timing\r\n- Multiple potential control units\r\n\r\n---\r\n\r\n## 2. Data Requirements\r\n\r\nPanel data structure with:\r\n- Unit identifier\r\n- Time variable\r\n- Outcome variable\r\n- Predictor variables\r\n\r\n```stata\r\n* Declare panel structure\r\ntsset unit_id time_var\r\n```\r\n\r\n---\r\n\r\n## 3. Basic Synthetic Control\r\n\r\n```stata\r\n* Install synth package\r\nssc install synth, replace\r\n\r\n* Basic syntax\r\nsynth outcome_var predictors, ///\r\n    trunit(#) trperiod(#)\r\n```\r\n\r\n### Example with Simulated Data\r\n\r\n```stata\r\nclear\r\nset seed 12345\r\nset obs 400\r\n\r\n* Create panel: 20 states, 20 years each\r\ngen state = ceil(_n/20)\r\nbysort state: gen year = 1980 + _n\r\n\r\n* Outcome variable\r\ngen y = 100 + state*2 + year*0.5 + rnormal(0, 5)\r\n\r\n* Treatment: state 1 treated after year 1990\r\nreplace y = y - 15 if state == 1 & year >= 1990\r\n\r\n* Covariates\r\ngen x1 = rnormal(50, 10)\r\ngen x2 = rnormal(1000, 200)\r\n\r\n* Declare panel\r\ntsset state year\r\n\r\n* Run synthetic control\r\nsynth y x1 x2 y(1985) y(1988), trunit(1) trperiod(1990)\r\n```\r\n\r\n---\r\n\r\n## 4. Specifying Predictors\r\n\r\n### Average of Variable Over Pre-Period\r\n\r\n```stata\r\n* x1 averaged over all pre-treatment periods\r\nsynth y x1 x2, trunit(1) trperiod(1990)\r\n```\r\n\r\n### Specific Year Values\r\n\r\n```stata\r\n* y at specific years as predictors\r\nsynth y y(1985) y(1988), trunit(1) trperiod(1990)\r\n```\r\n\r\n### Range of Years\r\n\r\n```stata\r\n* y averaged over 1985-1988\r\nsynth y y(1985(1)1988), trunit(1) trperiod(1990)\r\n```\r\n\r\n### Combined Predictors\r\n\r\n```stata\r\nsynth y x1 x2 y(1985) y(1988), trunit(1) trperiod(1990)\r\n```\r\n\r\n---\r\n\r\n## 5. Output Interpretation\r\n\r\nThe synth output shows:\r\n\r\n### RMSPE (Root Mean Squared Prediction Error)\r\n- Measures pre-treatment fit\r\n- Lower is better\r\n\r\n### Unit Weights\r\n- Which control units comprise the synthetic control\r\n- Weights sum to 1\r\n\r\n### Predictor Balance\r\n- Compares treated vs synthetic control on predictors\r\n- Should be similar if fit is good\r\n\r\n---\r\n\r\n## 6. Options\r\n\r\n| Option | Purpose |\r\n|--------|---------|\r\n| `trunit(#)` | Treated unit ID |\r\n| `trperiod(#)` | First treatment period |\r\n| `fig` | Display results figure |\r\n| `keep(filename)` | Save results to file |\r\n| `nested` | Use nested optimization |\r\n| `allopt` | Try all optimization methods |\r\n\r\n---\r\n\r\n## 7. Inference\r\n\r\n### Placebo Tests (In-Space)\r\n\r\nRun synth for each control unit as if it were treated:\r\n\r\n```stata\r\n* Store treated unit result\r\nsynth y x1 x2 y(1985) y(1988), trunit(1) trperiod(1990) ///\r\n    keep(synth_treated) replace\r\n\r\n* Placebo for control unit 2\r\nsynth y x1 x2 y(1985) y(1988), trunit(2) trperiod(1990) ///\r\n    keep(synth_placebo2) replace\r\n\r\n* Compare RMSPEs\r\n* Treated effect is significant if RMSPE ratio is large\r\n```\r\n\r\n### Placebo Tests (In-Time)\r\n\r\nApply treatment at different (fake) times:\r\n\r\n```stata\r\n* True treatment 1990\r\nsynth y x1 x2 y(1982) y(1985), trunit(1) trperiod(1990)\r\n\r\n* Placebo: treatment at 1985 (before actual treatment)\r\nsynth y x1 x2 y(1982), trunit(1) trperiod(1985)\r\n```\r\n\r\n---\r\n\r\n## 8. Limitations\r\n\r\n- Single treated unit (not for multiple treatment)\r\n- Requires good pre-treatment fit\r\n- Sensitive to predictor choice\r\n- No built-in inference (must do placebo tests)\r\n- Cannot extrapolate beyond donor pool characteristics\r\n\r\n---\r\n\r\n## Quick Reference\r\n\r\n### Basic Syntax\r\n\r\n```stata\r\nsynth outcome predictors, trunit(#) trperiod(#)\r\n```\r\n\r\n### Package Installation\r\n\r\n```stata\r\nssc install synth, replace\r\n```\r\n\r\n### Key Components\r\n\r\n| Component | Description |\r\n|-----------|-------------|\r\n| `outcome` | Dependent variable |\r\n| `predictors` | Variables to match on |\r\n| `trunit()` | ID of treated unit |\r\n| `trperiod()` | First treatment period |\r\n\n\n---\n\n### 04_visualization.md\n\n# Visualization & Output in Stata\r\n\r\nPublication-quality tables, coefficient plots, and figures. All code tested on Stata 15+.\r\n\r\n---\r\n\r\n## 1. Regression Tables with esttab\r\n\r\n### Basic Table\r\n\r\n```stata\r\n* Run and store models\r\nquietly reg price mpg\r\nestimates store m1\r\n\r\nquietly reg price mpg weight\r\nestimates store m2\r\n\r\nquietly reg price mpg weight foreign\r\nestimates store m3\r\n\r\n* Basic table\r\nesttab m1 m2 m3, se star(* 0.10 ** 0.05 *** 0.01)\r\n```\r\n\r\n### Publication-Ready Table\r\n\r\n```stata\r\nesttab m1 m2 m3, ///\r\n    se star(* 0.10 ** 0.05 *** 0.01) ///\r\n    mtitle(\"Model 1\" \"Model 2\" \"Model 3\") ///\r\n    title(\"Price Regressions\") ///\r\n    keep(mpg weight foreign) ///\r\n    order(foreign mpg weight)\r\n```\r\n\r\n### Export to File\r\n\r\n```stata\r\n* Text file\r\nesttab m1 m2 m3 using \"table1.txt\", replace ///\r\n    se star(* 0.10 ** 0.05 *** 0.01) ///\r\n    mtitle(\"Model 1\" \"Model 2\" \"Model 3\")\r\n\r\n* LaTeX\r\nesttab m1 m2 m3 using \"table1.tex\", replace ///\r\n    se star(* 0.10 ** 0.05 *** 0.01) ///\r\n    booktabs label\r\n\r\n* RTF (Word-compatible)\r\nesttab m1 m2 m3 using \"table1.rtf\", replace ///\r\n    se star(* 0.10 ** 0.05 *** 0.01)\r\n```\r\n\r\n### Common esttab Options\r\n\r\n| Option | Description |\r\n|--------|-------------|\r\n| `se` | Show standard errors |\r\n| `t` | Show t-statistics |\r\n| `p` | Show p-values |\r\n| `ci` | Show confidence intervals |\r\n| `star(...)` | Define significance stars |\r\n| `mtitle(...)` | Column titles |\r\n| `title(...)` | Table title |\r\n| `keep(...)` | Variables to show |\r\n| `drop(...)` | Variables to hide |\r\n| `order(...)` | Variable order |\r\n| `label` | Use variable labels |\r\n| `booktabs` | LaTeX booktabs format |\r\n| `replace` | Overwrite existing file |\r\n\r\n---\r\n\r\n## 2. Coefficient Plots\r\n\r\n### Basic Coefficient Plot\r\n\r\n```stata\r\n* After running regression\r\nreg price mpg weight foreign i.rep78\r\n\r\n* Plot coefficients\r\ncoefplot, drop(_cons) xline(0) ///\r\n    title(\"Coefficient Plot\")\r\n\r\ngraph export \"coefplot.png\", replace\r\n```\r\n\r\n### Multiple Model Comparison\r\n\r\n```stata\r\n* Store multiple models first\r\nquietly reg price mpg\r\nestimates store m1\r\n\r\nquietly reg price mpg weight\r\nestimates store m2\r\n\r\nquietly reg price mpg weight foreign\r\nestimates store m3\r\n\r\n* Compare in single plot\r\ncoefplot m1 m2 m3, drop(_cons) xline(0) ///\r\n    legend(order(2 \"Model 1\" 4 \"Model 2\" 6 \"Model 3\"))\r\n\r\ngraph export \"coefplot_comparison.png\", replace\r\n```\r\n\r\n### Vertical Coefficient Plot\r\n\r\n```stata\r\ncoefplot m3, drop(_cons) vertical ///\r\n    yline(0, lpattern(dash)) ///\r\n    title(\"Vertical Coefficient Plot\")\r\n```\r\n\r\n### Event Study Style Plot\r\n\r\n```stata\r\n* After event study regression with lead/lag dummies\r\ncoefplot, keep(lead_* lag_*) vertical ///\r\n    yline(0, lpattern(dash)) ///\r\n    xline(0.5, lpattern(dash) lcolor(red)) ///\r\n    xlabel(, angle(45)) ///\r\n    xtitle(\"Event Time\") ytitle(\"Effect\")\r\n```\r\n\r\n### Customizing coefplot\r\n\r\n```stata\r\ncoefplot m1 m2, drop(_cons) xline(0) ///\r\n    ciopts(recast(rcap)) ///              // Cap ends on CI\r\n    msymbol(D) ///                         // Diamond markers\r\n    mcolor(navy) ///                       // Marker color\r\n    levels(95 90) ///                      // Multiple CI levels\r\n    legend(order(1 \"95% CI\" 2 \"90% CI\"))\r\n```\r\n\r\n---\r\n\r\n## 3. Summary Statistics\r\n\r\n### Basic Summary\r\n\r\n```stata\r\nsummarize price mpg weight\r\n\r\n* Detailed with percentiles\r\nsummarize price, detail\r\n```\r\n\r\n### tabstat (Formatted)\r\n\r\n```stata\r\n* Multiple statistics\r\ntabstat price mpg weight, stat(mean sd min max n) columns(statistics)\r\n\r\n* By group\r\ntabstat price mpg weight, by(foreign) stat(mean sd n)\r\n```\r\n\r\n### Summary Table with estpost\r\n\r\n```stata\r\n* Summary statistics table\r\nestpost summarize price mpg weight\r\nesttab, cells(\"mean(fmt(2)) sd(fmt(2)) min max count\") nomtitle nonumber\r\n\r\n* Export to file\r\nesttab using \"summary_stats.tex\", replace ///\r\n    cells(\"mean(fmt(2)) sd(fmt(2)) min max count\") ///\r\n    nomtitle nonumber booktabs\r\n```\r\n\r\n### Balance Table\r\n\r\n```stata\r\n* By treatment group\r\nbysort treatment: summarize y x1 x2\r\n\r\n* Or use tabstat\r\ntabstat y x1 x2, by(treatment) stat(mean sd n) nototal\r\n```\r\n\r\n---\r\n\r\n## 4. Basic Graphs\r\n\r\n### Scatter Plot with Fit Line\r\n\r\n```stata\r\ntwoway (scatter y x) (lfit y x), ///\r\n    title(\"Scatter with Linear Fit\") ///\r\n    xtitle(\"X Variable\") ytitle(\"Y Variable\") ///\r\n    legend(off)\r\n\r\ngraph export \"scatter.png\", replace\r\n```\r\n\r\n### Binned Scatter (binscatter)\r\n\r\n```stata\r\n* Install: ssc install binscatter\r\nbinscatter y x, controls(z1 z2) ///\r\n    title(\"Binned Scatter\") ///\r\n    xtitle(\"X\") ytitle(\"Y | Controls\")\r\n```\r\n\r\n### Time Series\r\n\r\n```stata\r\ntwoway line y year, ///\r\n    title(\"Outcome Over Time\") ///\r\n    xtitle(\"Year\") ytitle(\"Outcome\")\r\n\r\n* Multiple series\r\ntwoway (line y1 year) (line y2 year), ///\r\n    legend(order(1 \"Series 1\" 2 \"Series 2\"))\r\n```\r\n\r\n### Histogram\r\n\r\n```stata\r\nhistogram y, frequency ///\r\n    title(\"Distribution of Y\") ///\r\n    xtitle(\"Y\") ytitle(\"Frequency\")\r\n```\r\n\r\n### Kernel Density\r\n\r\n```stata\r\nkdensity y, ///\r\n    title(\"Density of Y\") ///\r\n    xtitle(\"Y\") ytitle(\"Density\")\r\n\r\n* By group\r\ntwoway (kdensity y if treat==0) (kdensity y if treat==1), ///\r\n    legend(order(1 \"Control\" 2 \"Treatment\"))\r\n```\r\n\r\n---\r\n\r\n## 5. Graph Formatting\r\n\r\n### Publication Scheme\r\n\r\n```stata\r\n* Set clean scheme\r\nset scheme s1mono  // Black and white\r\nset scheme s1color // Color version\r\n\r\n* Or use custom\r\nssc install grstyle, replace\r\nssc install palettes, replace\r\nssc install colrspace, replace\r\n\r\ngrstyle init\r\ngrstyle set plain\r\ngrstyle set legend 6, nobox\r\ngrstyle set color navy maroon forest_green\r\n```\r\n\r\n### Common Graph Options\r\n\r\n```stata\r\ntwoway ..., ///\r\n    title(\"Main Title\") ///\r\n    subtitle(\"Subtitle\") ///\r\n    xtitle(\"X Axis Label\") ///\r\n    ytitle(\"Y Axis Label\") ///\r\n    xlabel(0(10)100) ///           // X ticks at 0,10,20,...,100\r\n    ylabel(, angle(horizontal)) /// // Horizontal Y labels\r\n    legend(order(1 \"First\" 2 \"Second\") pos(6)) /// // Bottom legend\r\n    note(\"Note: Sample description\") ///\r\n    graphregion(color(white)) ///  // White background\r\n    plotregion(margin(zero))       // No margins\r\n```\r\n\r\n### Export Formats\r\n\r\n```stata\r\n* PNG (web/slides)\r\ngraph export \"figure.png\", replace width(2400)\r\n\r\n* PDF (publication)\r\ngraph export \"figure.pdf\", replace\r\n\r\n* EPS (LaTeX)\r\ngraph export \"figure.eps\", replace\r\n```\r\n\r\n### Combine Multiple Graphs\r\n\r\n```stata\r\n* Create individual graphs\r\ntwoway scatter y1 x, name(g1, replace) title(\"Panel A\")\r\ntwoway scatter y2 x, name(g2, replace) title(\"Panel B\")\r\n\r\n* Combine\r\ngraph combine g1 g2, rows(1) ///\r\n    title(\"Combined Figure\")\r\n\r\ngraph export \"combined.png\", replace\r\n```\r\n\r\n---\r\n\r\n## 6. RD Plots (Stata 16+)\r\n\r\n```stata\r\n* Using rdplot (requires rdrobust package)\r\nrdplot y running_var, c(0) ///\r\n    graph_options(title(\"RD Plot\") ///\r\n                  xtitle(\"Running Variable\") ///\r\n                  ytitle(\"Outcome\"))\r\n```\r\n\r\n---\r\n\r\n## Quick Reference\r\n\r\n### Table Output Commands\r\n\r\n| Command | Purpose |\r\n|---------|---------|\r\n| `esttab` | Regression tables |\r\n| `estpost` | Post results for tables |\r\n| `tabstat` | Summary statistics |\r\n| `tabout` | Cross-tabulations |\r\n\r\n### Graph Commands\r\n\r\n| Command | Purpose |\r\n|---------|---------|\r\n| `coefplot` | Coefficient plots |\r\n| `binscatter` | Binned scatter plots |\r\n| `twoway` | General 2D graphs |\r\n| `histogram` | Histograms |\r\n| `kdensity` | Kernel density |\r\n| `graph combine` | Multi-panel figures |\r\n\r\n### Export Checklist\r\n\r\n- [ ] Use vector format (PDF/EPS) for line plots\r\n- [ ] Use 300+ DPI for raster images\r\n- [ ] Include informative axis labels\r\n- [ ] Add reference lines where appropriate\r\n- [ ] Use consistent color scheme\r\n- [ ] Match journal font requirements\r\n\n\n---\n\n### 05_best_practices.md\n\n# Best Practices for Stata Projects\r\n\r\nCode organization, reproducibility, and style conventions based on top journal replication packages.\r\n\r\n---\r\n\r\n## 1. Master Do-File Structure\r\n\r\nEvery project should have a single master script that runs the entire analysis.\r\n\r\n### Template\r\n\r\n```stata\r\n* ===========================================================================\r\n* Master Do-File\r\n*\r\n* \"[Paper Title]\"\r\n* [Authors]\r\n* [Journal], [Year]\r\n*\r\n* ===========================================================================\r\n\r\nversion 15\r\nclear all\r\nset more off\r\nset maxvar 10000\r\n\r\n* ===========================================================================\r\n* PATH SETUP - Edit this section only\r\n* ===========================================================================\r\n\r\n* Set root path (CHANGE THIS for your machine)\r\nglobal root \"/path/to/replication\"\r\n\r\n* Derived paths (do not edit)\r\nglobal code \"$root/code\"\r\nglobal data \"$root/data\"\r\n    global raw \"$data/raw\"\r\n    global clean \"$data/clean\"\r\nglobal out \"$root/output\"\r\n    global figures \"$out/figures\"\r\n    global tables \"$out/tables\"\r\n    global logs \"$out/logs\"\r\n\r\n* ===========================================================================\r\n* EXECUTION\r\n* ===========================================================================\r\n\r\nlog using \"$logs/replication_log.txt\", text replace\r\n\r\n* Data preparation\r\ndo \"$code/01_import_data.do\"\r\ndo \"$code/02_clean_data.do\"\r\ndo \"$code/03_create_variables.do\"\r\n\r\n* Analysis\r\ndo \"$code/10_descriptives.do\"      // Table 1, Figures 1-2\r\ndo \"$code/11_main_analysis.do\"     // Tables 2-4\r\ndo \"$code/12_robustness.do\"        // Appendix Tables A1-A5\r\n\r\n* Figures\r\ndo \"$code/20_figures.do\"           // Figures 3-6\r\n\r\nlog close\r\n\r\nexit\r\n```\r\n\r\n---\r\n\r\n## 2. Project Directory Structure\r\n\r\n```\r\nproject/\r\n code/\r\n    00_master.do\r\n    01_import_data.do\r\n    02_clean_data.do\r\n    03_create_variables.do\r\n    10_descriptives.do\r\n    11_main_analysis.do\r\n    20_figures.do\r\n data/\r\n    raw/           # Original, unchanged data\r\n    clean/         # Processed data files\r\n output/\r\n    figures/\r\n    tables/\r\n    logs/\r\n README.md\r\n```\r\n\r\n---\r\n\r\n## 3. Path Management\r\n\r\n### Good: Single Root Variable\r\n\r\n```stata\r\n* Set root path (only line to change)\r\nglobal root \"/Users/name/project\"\r\n\r\n* Derived paths\r\nglobal code \"$root/code\"\r\nglobal raw \"$root/data/raw\"\r\nglobal clean \"$root/data/clean\"\r\nglobal figures \"$root/output/figures\"\r\nglobal tables \"$root/output/tables\"\r\n```\r\n\r\n### Bad: Hardcoded Paths\r\n\r\n```stata\r\n* DON'T do this - not portable\r\nuse \"/Users/name/Documents/project/data/mydata.dta\"\r\n```\r\n\r\n---\r\n\r\n## 4. Data Cleaning Conventions\r\n\r\n### Recode Missing Values Consistently\r\n\r\n```stata\r\n* Standardize missing values\r\nrecode var1 var2 var3 (9999 = .) (-99 = .) (-1 = .)\r\n\r\n* Document recoding with comments\r\n* Original codes: 9999=DK, -99=Refused, -1=NA\r\nrecode satisfaction (9999 = .) (-99 = .) (-1 = .)\r\n```\r\n\r\n### Label Everything\r\n\r\n```stata\r\n* Define value labels\r\nlabel define yesno 0 \"No\" 1 \"Yes\"\r\nlabel values treated yesno\r\n\r\n* Variable labels\r\nlabel variable treated \"Treatment indicator\"\r\nlabel variable outcome \"Primary outcome measure\"\r\n```\r\n\r\n### Check Merge Results\r\n\r\n```stata\r\nmerge m:1 id using \"$clean/demographics.dta\"\r\n\r\n* Document merge results\r\ntab _merge\r\n\r\n* Handle unmatched explicitly\r\nassert _merge != 2  // No unmatched from using\r\ndrop if _merge == 2\r\ndrop _merge\r\n```\r\n\r\n---\r\n\r\n## 5. Reproducibility Essentials\r\n\r\n### Set Random Seed\r\n\r\n```stata\r\n* Set seed at the beginning of the do-file\r\nset seed 12345  // Use meaningful seed (e.g., date: 20240115)\r\n```\r\n\r\n### Version Control\r\n\r\n```stata\r\n* Specify Stata version at top of do-file\r\nversion 15\r\n\r\n* Record session info\r\ndisplay \"Stata version: \" c(stata_version)\r\ndisplay \"Date: \" c(current_date)\r\ndisplay \"Time: \" c(current_time)\r\n```\r\n\r\n### Use Log Files\r\n\r\n```stata\r\n* Start log\r\nlog using \"$logs/analysis_log.txt\", text replace\r\n\r\n* ... analysis code ...\r\n\r\nlog close\r\n```\r\n\r\n---\r\n\r\n## 6. Code Style Conventions\r\n\r\n### Naming\r\n\r\n```stata\r\n* Variables: snake_case\r\ngen treatment_effect = ...\r\ngen log_income = log(income)\r\n\r\n* Wave indicators: suffix with number\r\nrename income incomeW1\r\nrename income2 incomeW2\r\n\r\n* Temporary variables: prefix with underscore\r\ngen _temp_var = ...\r\ndrop _temp_var\r\n```\r\n\r\n### Comments\r\n\r\n```stata\r\n* Single-line comment for brief notes\r\n\r\n/*\r\nMulti-line comment for\r\nlonger explanations\r\n*/\r\n\r\n// Alternative single-line (less common in published code)\r\n\r\n*--- Section break ---*\r\n```\r\n\r\n### Line Continuation\r\n\r\n```stata\r\n* Use /// for long commands\r\nreghdfe outcome treatment control1 control2 control3 ///\r\n    control4 control5, ///\r\n    absorb(id year) cluster(id)\r\n```\r\n\r\n---\r\n\r\n## 7. Analysis Do-File Template\r\n\r\n```stata\r\n* ===========================================================================\r\n* [Descriptive title]\r\n*\r\n* Purpose: [What this file does]\r\n* Input:   [Input data files]\r\n* Output:  [Output files - tables, figures]\r\n*\r\n* ===========================================================================\r\n\r\n* Load data\r\nuse \"$clean/analysis_sample.dta\", clear\r\n\r\n* -------------------------------------------\r\n* Section 1: [Description]\r\n* -------------------------------------------\r\n\r\n[analysis code]\r\n\r\n* -------------------------------------------\r\n* Section 2: [Description]\r\n* -------------------------------------------\r\n\r\n[analysis code]\r\n\r\n* -------------------------------------------\r\n* Export results\r\n* -------------------------------------------\r\n\r\nesttab m1 m2 m3 using \"$tables/table_1.tex\", replace ///\r\n    [options]\r\n\r\ngraph export \"$figures/figure_1.pdf\", replace\r\n```\r\n\r\n---\r\n\r\n## 8. Common Patterns\r\n\r\n### Loop Over Variables\r\n\r\n```stata\r\n* Process multiple variables\r\nforeach var in income education age {\r\n    gen log_`var' = log(`var')\r\n    label variable log_`var' \"Log `var'\"\r\n}\r\n```\r\n\r\n### Loop Over Specifications\r\n\r\n```stata\r\n* Multiple model specifications\r\nlocal controls1 \"x1 x2\"\r\nlocal controls2 \"x1 x2 x3 x4\"\r\nlocal controls3 \"x1 x2 x3 x4 x5 x6\"\r\n\r\nforvalues i = 1/3 {\r\n    reg y treat `controls`i'', robust\r\n    estimates store m`i'\r\n}\r\n\r\nesttab m1 m2 m3\r\n```\r\n\r\n### Store Multiple Models Efficiently\r\n\r\n```stata\r\n* Clear stored estimates\r\nestimates clear\r\n\r\n* Run and store\r\nforeach outcome in y1 y2 y3 {\r\n    quietly reg `outcome' treat x1 x2, robust\r\n    estimates store m_`outcome'\r\n}\r\n\r\n* Combined table\r\nesttab m_y1 m_y2 m_y3, se\r\n```\r\n\r\n---\r\n\r\n## 9. Pre-Submission Checklist\r\n\r\n### Code Quality\r\n- [ ] Master script runs entire analysis without errors\r\n- [ ] All paths are relative (single root variable)\r\n- [ ] Random seeds are set and documented\r\n- [ ] Code files have clear headers with purpose\r\n\r\n### Documentation\r\n- [ ] README documents execution order\r\n- [ ] README lists software/package requirements\r\n- [ ] Output files are named to match paper elements\r\n\r\n### Reproducibility\r\n- [ ] Stata version specified\r\n- [ ] Package versions recorded\r\n- [ ] Intermediate datasets saved\r\n- [ ] Log files generated\r\n\r\n### Data\r\n- [ ] Raw data unchanged\r\n- [ ] All data transformations documented\r\n- [ ] Missing value handling explicit\r\n- [ ] Merge results verified\r\n\r\n---\r\n\r\n## Quick Reference\r\n\r\n### Essential Setup Commands\r\n\r\n```stata\r\nclear all\r\nset more off\r\nset maxvar 10000\r\nversion 15\r\n```\r\n\r\n### Global Structure\r\n\r\n```stata\r\nglobal root \"/path/to/project\"\r\nglobal code \"$root/code\"\r\nglobal data \"$root/data\"\r\nglobal out \"$root/output\"\r\n```\r\n\r\n### Output Mapping Comment Style\r\n\r\n```stata\r\ndo \"$code/10_analysis.do\"  // Table 1, Figure 2\r\n```\r\n\n\n---\n\n",
        "plugins/text-analyst/.claude-plugin/plugin.json": "{\n  \"name\": \"text-analyst\",\n  \"description\": \"Computational text analysis using R or Python. Topic models (LDA, STM, BERTopic), sentiment analysis, classification, and embeddings with systematic validation.\",\n  \"version\": \"1.0.0\",\n  \"author\": {\n    \"name\": \"Neal Caren\"\n  },\n  \"license\": \"MIT\",\n  \"keywords\": [\n    \"text-analysis\",\n    \"NLP\",\n    \"topic-models\",\n    \"sociology\",\n    \"research\"\n  ],\n  \"skills\": \"./skills/\"\n}\n",
        "plugins/text-analyst/skills/text-analyst/SKILL.md": "---\r\nname: text-analyst\r\ndescription: Computational text analysis for sociology research using R or Python. Guides you through topic models, sentiment analysis, classification, and embeddings with systematic validation. Supports both traditional (LDA, STM) and neural (BERT, BERTopic) methods.\r\n---\r\n\r\n# Computational Text Analysis Agent\r\n\r\nYou are an expert text analysis assistant for sociology and social science research. Your role is to guide users through systematic computational text analysis that produces valid, reproducible, and publication-ready results.\r\n\r\n## Core Principles\r\n\r\n1. **Corpus understanding before modeling**: Explore the data before running models. Know your documents.\r\n\r\n2. **Method selection based on research question**: Different questions need different methods. Topic models answer different questions than classifiers.\r\n\r\n3. **Validation is essential**: Algorithmic output is not ground truth. Human validation and multiple diagnostics are required.\r\n\r\n4. **Reproducibility**: Document all preprocessing decisions, parameters, and random seeds.\r\n\r\n5. **Appropriate interpretation**: Text analysis results require careful, qualified interpretation. Avoid overclaiming.\r\n\r\n## Language Selection\r\n\r\nThis agent supports both **R** and **Python**. Each has strengths:\r\n\r\n| Method | Recommended Language | Rationale |\r\n|--------|---------------------|-----------|\r\n| **Topic Models (LDA, STM)** | **R** | `stm` package is gold standard; better diagnostics |\r\n| **Dictionary/Sentiment** | **R** | tidytext workflow is elegant; great lexicon support |\r\n| **Visualization** | **R** | ggplot2 produces publication-ready figures |\r\n| **Transformers/BERT** | **Python** | HuggingFace ecosystem, GPU support |\r\n| **BERTopic** | **Python** | Neural topic modeling, only in Python |\r\n| **Named Entity Recognition** | **Python** | spaCy is industry standard |\r\n| **Supervised Classification** | **Either** | sklearn and tidymodels both excellent |\r\n| **Word Embeddings** | **Python** | gensim more mature; sentence-transformers |\r\n\r\n**At Phase 0, help users select the appropriate language based on their methods.**\r\n\r\n## Analysis Phases\r\n\r\n### Phase 0: Research Design & Method Selection\r\n**Goal**: Establish the research question and select appropriate methods.\r\n\r\n**Process**:\r\n- Clarify the research question (descriptive, exploratory, or inferential)\r\n- Determine corpus characteristics (size, type, language)\r\n- Select appropriate methods based on research goals\r\n- Choose language (R or Python) based on method needs\r\n- Plan validation approach\r\n\r\n**Output**: Design memo with research question, method selection, and language choice.\r\n\r\n> **Pause**: Confirm design with user before corpus preparation.\r\n\r\n---\r\n\r\n### Phase 1: Corpus Preparation & Exploration\r\n**Goal**: Understand the text data before analysis.\r\n\r\n**Process**:\r\n- Load and inspect the corpus\r\n- Make preprocessing decisions (tokenization, stopwords, stemming)\r\n- Create document-term matrix or embeddings\r\n- Generate descriptive statistics\r\n- Visualize corpus characteristics\r\n\r\n**Output**: Corpus report with descriptives, preprocessing decisions, and visualizations.\r\n\r\n> **Pause**: Review corpus characteristics and confirm preprocessing.\r\n\r\n---\r\n\r\n### Phase 2: Method Specification\r\n**Goal**: Fully specify the analysis approach before running models.\r\n\r\n**Process**:\r\n- Specify model parameters (K for topics, embedding dimensions, etc.)\r\n- Define training/validation splits if applicable\r\n- Document preprocessing pipeline explicitly\r\n- Plan evaluation metrics\r\n- Pre-specify dictionary/lexicon choices\r\n\r\n**Output**: Specification memo with parameters, preprocessing, and evaluation plan.\r\n\r\n> **Pause**: User approves specification before analysis.\r\n\r\n---\r\n\r\n### Phase 3: Main Analysis\r\n**Goal**: Execute the specified text analysis methods.\r\n\r\n**Process**:\r\n- Run primary models\r\n- Extract and interpret results\r\n- Create initial visualizations\r\n- Assess model fit and convergence\r\n- Document any deviations from specification\r\n\r\n**Output**: Results with initial interpretation.\r\n\r\n> **Pause**: User reviews results before validation.\r\n\r\n---\r\n\r\n### Phase 4: Validation & Robustness\r\n**Goal**: Validate findings and assess robustness.\r\n\r\n**Process**:\r\n- Human validation (sample coding, topic labeling)\r\n- Model diagnostics (coherence, classification metrics)\r\n- Sensitivity analysis (different K, preprocessing, seeds)\r\n- Compare to alternative methods if applicable\r\n\r\n**Output**: Validation report with diagnostics and robustness assessment.\r\n\r\n> **Pause**: User assesses validity before final outputs.\r\n\r\n---\r\n\r\n### Phase 5: Output & Interpretation\r\n**Goal**: Produce publication-ready outputs and synthesize findings.\r\n\r\n**Process**:\r\n- Create publication-quality tables and figures\r\n- Write results narrative with appropriate caveats\r\n- Document limitations\r\n- Prepare replication materials\r\n\r\n**Output**: Final tables, figures, and interpretation memo.\r\n\r\n---\r\n\r\n## Folder Structure\r\n\r\n```\r\nproject/\r\n data/\r\n    raw/              # Original text files\r\n    processed/        # Cleaned corpus, DTMs\r\n code/\r\n    00_master.R       # or 00_master.py\r\n    01_preprocess.R\r\n    02_analysis.R\r\n    03_validation.R\r\n output/\r\n    tables/\r\n    figures/\r\n dictionaries/         # Custom lexicons if used\r\n memos/                # Phase outputs\r\n```\r\n\r\n## Technique Guides\r\n\r\n### Conceptual Guides (language-agnostic)\r\nLocated in `concepts/` (relative to this skill):\r\n\r\n| Guide | Topics |\r\n|-------|--------|\r\n| `01_dictionary_methods.md` | Lexicons, custom dictionaries, validation |\r\n| `02_topic_models.md` | LDA, STM, BERTopic theory and selection |\r\n| `03_supervised_classification.md` | Training data, features, evaluation |\r\n| `04_embeddings.md` | Word2Vec, GloVe, BERT concepts |\r\n| `05_sentiment_analysis.md` | Dictionary vs ML approaches |\r\n| `06_validation_strategies.md` | Human coding, diagnostics, robustness |\r\n\r\n### R Technique Guides\r\nLocated in `r-techniques/`:\r\n\r\n| Guide | Topics |\r\n|-------|--------|\r\n| `01_preprocessing.md` | tidytext, quanteda |\r\n| `02_dictionary_sentiment.md` | tidytext lexicons, TF-IDF |\r\n| `03_topic_models.md` | topicmodels, stm |\r\n| `04_supervised.md` | tidymodels for text |\r\n| `05_embeddings.md` | text2vec |\r\n| `06_visualization.md` | ggplot2 for text |\r\n\r\n### Python Technique Guides\r\nLocated in `python-techniques/`:\r\n\r\n| Guide | Topics |\r\n|-------|--------|\r\n| `01_preprocessing.md` | nltk, spaCy, sklearn |\r\n| `02_dictionary_sentiment.md` | VADER, TextBlob |\r\n| `03_topic_models.md` | gensim, BERTopic |\r\n| `04_supervised.md` | sklearn, transformers |\r\n| `05_embeddings.md` | gensim, sentence-transformers |\r\n| `06_visualization.md` | matplotlib, pyLDAvis |\r\n\r\n**Read the relevant guides before writing code for that method.**\r\n\r\n## Invoking Phase Agents\r\n\r\nFor each phase, invoke the appropriate sub-agent using the Task tool:\r\n\r\n```\r\nTask: Phase 0 Research Design\r\nsubagent_type: general-purpose\r\nmodel: opus\r\nprompt: Read phases/phase0-design.md and execute for [user's project]\r\n```\r\n\r\n## Model Recommendations\r\n\r\n| Phase | Model | Rationale |\r\n|-------|-------|-----------|\r\n| **Phase 0**: Research Design | **Opus** | Method selection requires judgment |\r\n| **Phase 1**: Corpus Preparation | **Sonnet** | Data processing, descriptives |\r\n| **Phase 2**: Specification | **Opus** | Design decisions, parameters |\r\n| **Phase 3**: Main Analysis | **Sonnet** | Running models |\r\n| **Phase 4**: Validation | **Sonnet** | Systematic diagnostics |\r\n| **Phase 5**: Output | **Opus** | Interpretation, writing |\r\n\r\n## Starting the Analysis\r\n\r\nWhen the user is ready to begin:\r\n\r\n1. **Ask about the research question**:\r\n   > \"What are you trying to learn from the text? Are you exploring themes, measuring concepts, classifying documents, or something else?\"\r\n\r\n2. **Ask about the corpus**:\r\n   > \"What text data do you have? How many documents, what type (articles, social media, interviews), and what language?\"\r\n\r\n3. **Ask about methods**:\r\n   > \"Do you have specific methods in mind (topic models, sentiment, classification), or would you like help selecting based on your question?\"\r\n\r\n4. **Recommend language based on methods**:\r\n   - Topic models with covariates  R\r\n   - Neural methods (BERT, BERTopic)  Python\r\n   - Both classical and neural  May need both\r\n\r\n5. **Then proceed with Phase 0** to formalize the research design.\r\n\r\n## Key Reminders\r\n\r\n- **Preprocessing matters**: Document every decision (stopwords, stemming, thresholds)\r\n- **K is not a tuning parameter**: Number of topics should be interpretable, not just optimal by metrics\r\n- **Validation is not optional**: Algorithmic output needs human validation\r\n- **Show your dictionaries**: If using lexicons, readers should see the word lists\r\n- **Uncertainty exists**: Topic models and classifiers have uncertainty; acknowledge it\r\n- **Corpus defines scope**: Findings apply to the analyzed corpus, not \"language\" generally\r\n",
        "plugins/text-analyst/skills/text-analyst/concepts/01_dictionary_methods.md": "# Dictionary Methods for Text Analysis\r\n\r\n## Overview\r\n\r\nDictionary methods measure concepts in text by counting words from predefined lists. They are transparent, reproducible, and interpretablebut require careful validation.\r\n\r\n## When to Use Dictionary Methods\r\n\r\n**Good fit:**\r\n- Measuring well-defined concepts (sentiment, emotions, moral foundations)\r\n- Existing validated dictionaries available\r\n- Need for transparency and reproducibility\r\n- Large corpora where manual coding is infeasible\r\n\r\n**Poor fit:**\r\n- Exploratory analysis (don't know what to measure)\r\n- Highly domain-specific language\r\n- Concepts not captured by word lists\r\n- Short texts with sparse matches\r\n\r\n## Key Dictionaries\r\n\r\n### General Purpose\r\n\r\n| Dictionary | Concepts | Size | Access |\r\n|------------|----------|------|--------|\r\n| **LIWC** | 90+ categories (affect, cognition, social) | ~6,400 words | Licensed ($) |\r\n| **VADER** | Sentiment (positive/negative/neutral) | ~7,500 words | Free |\r\n| **NRC** | Emotions + sentiment | ~14,000 words | Free for research |\r\n| **TextBlob** | Polarity + subjectivity | Built-in | Free |\r\n\r\n### Domain-Specific\r\n\r\n| Dictionary | Domain | Use Case |\r\n|------------|--------|----------|\r\n| **Loughran-McDonald** | Finance | 10-K filings, earnings calls |\r\n| **Moral Foundations** | Moral psychology | Political rhetoric, values |\r\n| **Harvard IV** | General inquiry | Classic, broad coverage |\r\n| **AFINN** | Sentiment | Twitter, informal text |\r\n\r\n### Custom Dictionaries\r\n\r\nWhen to build your own:\r\n- Domain-specific concepts\r\n- No existing dictionary fits\r\n- Need precise control over terms\r\n\r\n## Constructing Custom Dictionaries\r\n\r\n### Step 1: Define the Concept\r\n\r\nWrite a clear conceptual definition:\r\n- What does this concept mean theoretically?\r\n- What would indicate its presence in text?\r\n- What are near-synonyms and related terms?\r\n\r\n### Step 2: Generate Seed Terms\r\n\r\nSources for initial terms:\r\n- Theory and literature\r\n- Domain expertise\r\n- Thesaurus expansion\r\n- Word embeddings (similar words)\r\n\r\n### Step 3: Expand and Refine\r\n\r\n```\r\nFor each seed term:\r\n  1. Find synonyms and variants\r\n  2. Consider inflections (run, runs, running)\r\n  3. Check actual usage in corpus (KWIC)\r\n  4. Add domain-specific terms\r\n  5. Remove ambiguous terms\r\n```\r\n\r\n### Step 4: Validate\r\n\r\n- **Face validity**: Do terms look right to experts?\r\n- **Coverage**: What % of documents have matches?\r\n- **KWIC review**: Are matches capturing the concept?\r\n- **Convergent validity**: Correlate with other measures\r\n\r\n## Scoring Documents\r\n\r\n### Count-Based\r\n\r\n```\r\nscore = count of dictionary terms in document\r\n```\r\n\r\nSimple but confounded with document length.\r\n\r\n### Proportion-Based\r\n\r\n```\r\nscore = (dictionary terms) / (total words)\r\n```\r\n\r\nControls for length. Standard approach.\r\n\r\n### Weighted\r\n\r\n```\r\nscore =  (term weight  term count)\r\n```\r\n\r\nAllows different terms to contribute differently (e.g., \"excellent\" > \"good\").\r\n\r\n### Category Ratios\r\n\r\n```\r\nratio = (positive terms) / (positive + negative terms)\r\n```\r\n\r\nUseful for comparing relative presence.\r\n\r\n## Common Pitfalls\r\n\r\n### 1. Polysemy (Multiple Meanings)\r\n\r\n**Problem**: \"Positive\" means different things:\r\n- \"Positive attitude\" (sentiment)\r\n- \"Tested positive\" (medical)\r\n- \"Positive feedback loop\" (technical)\r\n\r\n**Solutions**:\r\n- Review KWIC examples\r\n- Use domain-specific dictionaries\r\n- Consider context windows\r\n- Accept and document limitations\r\n\r\n### 2. Negation\r\n\r\n**Problem**: \"Not happy\" contains \"happy\" but isn't positive.\r\n\r\n**Solutions**:\r\n- Negation handling (flip polarity within window)\r\n- VADER handles negation automatically\r\n- Consider bigrams (\"not happy\" as unit)\r\n- Accept limitations for simple approaches\r\n\r\n### 3. Intensity and Modifiers\r\n\r\n**Problem**: \"Very happy\" vs \"happy\" vs \"somewhat happy\"\r\n\r\n**Solutions**:\r\n- VADER includes intensity modifiers\r\n- Weight terms by intensity\r\n- Use ML approaches for nuance\r\n\r\n### 4. Sparse Matches\r\n\r\n**Problem**: Many documents have zero or few matches.\r\n\r\n**Solutions**:\r\n- Report coverage statistics\r\n- Consider document as missing if < threshold\r\n- Use broader dictionaries\r\n- Aggregate to higher level (paragraph  document)\r\n\r\n### 5. Domain Mismatch\r\n\r\n**Problem**: Dictionary built on different text type.\r\n\r\n**Solutions**:\r\n- Validate in your domain\r\n- Build custom dictionary\r\n- Report validation results\r\n\r\n## Validation Requirements\r\n\r\n### Minimum Validation\r\n\r\n1. **Coverage**: Report % documents with 1 match\r\n2. **KWIC review**: Sample 50+ uses of key terms\r\n3. **Distribution**: Show score distribution\r\n\r\n### Strong Validation\r\n\r\n4. **Inter-rater reliability**: Human coding of sample\r\n5. **Convergent validity**: Correlate with related measures\r\n6. **Known groups**: Compare groups expected to differ\r\n\r\n### Exemplary Validation\r\n\r\n7. **Discriminant validity**: Show what it doesn't correlate with\r\n8. **Predictive validity**: Does it predict outcomes?\r\n9. **Cross-validation**: Test in different subset\r\n\r\n## Reporting Standards\r\n\r\n### Methods Section Should Include\r\n\r\n```markdown\r\n## Dictionary Analysis\r\n\r\nWe measured [concept] using the [Dictionary Name]\r\n(Author, Year). This dictionary contains N terms\r\nacross M categories, developed for [context].\r\n\r\nWe calculated [scoring method] for each document.\r\n[Preprocessing details].\r\n\r\n### Validation\r\nDictionary terms matched X% of documents (mean = Y\r\nmatches per document). We reviewed N keyword-in-context\r\nexamples to assess face validity. [Results of validation].\r\n```\r\n\r\n### Results Section Should Include\r\n\r\n- Distribution of scores (histogram/summary stats)\r\n- Coverage information\r\n- Key caveats about dictionary approach\r\n\r\n### Supplementary Materials\r\n\r\n- Full word list (or reference if published)\r\n- Validation examples\r\n- Any custom modifications\r\n\r\n## Comparison to ML Approaches\r\n\r\n| Aspect | Dictionary | ML Classifier |\r\n|--------|------------|---------------|\r\n| **Transparency** | High (word list visible) | Lower (learned weights) |\r\n| **Training data** | Not needed | Required |\r\n| **Domain adaptation** | Manual dictionary building | Retraining |\r\n| **Nuance** | Limited (word presence) | Can learn context |\r\n| **Reproducibility** | Perfect (same list = same result) | Depends on implementation |\r\n| **Validation** | Face validity + coverage | Accuracy metrics |\r\n\r\n**Use dictionary when**: Transparency matters, no training data, well-defined concept with existing dictionary.\r\n\r\n**Use ML when**: Need to capture nuance, have labeled training data, complex concept.\r\n\r\n## Recommended Workflow\r\n\r\n```\r\n1. Define concept clearly\r\n2. Select or build dictionary\r\n3. Calculate initial scores\r\n4. Check coverage (>50% of docs should have matches)\r\n5. KWIC validation (sample 50+ uses)\r\n6. Assess distribution (ceiling/floor effects?)\r\n7. Convergent validation (correlate with alternative)\r\n8. Report all validation steps\r\n9. Acknowledge limitations\r\n```\r\n",
        "plugins/text-analyst/skills/text-analyst/concepts/02_topic_models.md": "# Topic Models for Text Analysis\r\n\r\n## Overview\r\n\r\nTopic models are unsupervised methods that discover latent themes in document collections. Each topic is a probability distribution over words; each document is a mixture of topics.\r\n\r\n## When to Use Topic Models\r\n\r\n**Good fit:**\r\n- Exploratory analysis: What themes exist in this corpus?\r\n- Large collections where manual reading is infeasible\r\n- Want to discover structure, not impose categories\r\n- Need to track theme prevalence over time or groups\r\n\r\n**Poor fit:**\r\n- Confirmatory analysis (use dictionary or classification)\r\n- Very short documents (< 50 words)\r\n- Highly technical/formulaic text\r\n- Need precise categories with clear boundaries\r\n\r\n## Types of Topic Models\r\n\r\n### Latent Dirichlet Allocation (LDA)\r\n\r\nThe foundational topic model.\r\n\r\n**Assumptions:**\r\n- Documents are mixtures of topics\r\n- Topics are distributions over words\r\n- Bag-of-words (word order doesn't matter)\r\n- Fixed number of topics K\r\n\r\n**Strengths:**\r\n- Well-understood, widely used\r\n- Many implementations available\r\n- Interpretable output\r\n\r\n**Weaknesses:**\r\n- Must specify K in advance\r\n- No covariates (can't explain topic variation)\r\n- Can produce incoherent topics\r\n\r\n### Structural Topic Model (STM)\r\n\r\n**The gold standard for social science.**\r\n\r\n**Key advantage:** Topic prevalence and content can vary by document covariates.\r\n\r\n**Example:**\r\n```\r\nTopic prevalence ~ year + source + author_ideology\r\nTopic content ~ formal_vs_informal\r\n```\r\n\r\nThis allows: \"How does discussion of Topic 3 change over time?\"\r\n\r\n**Strengths:**\r\n- Covariates for prevalence and content\r\n- Better diagnostics (exclusivity + coherence)\r\n- Correlation between topics modeled\r\n- Spectral initialization (more stable)\r\n\r\n**Weaknesses:**\r\n- R only (stm package)\r\n- Slower than basic LDA\r\n- More parameters to specify\r\n\r\n### BERTopic\r\n\r\n**Neural topic modeling using transformers.**\r\n\r\n**Approach:**\r\n1. Embed documents with BERT/sentence-transformers\r\n2. Reduce dimensions with UMAP\r\n3. Cluster with HDBSCAN\r\n4. Extract topic words with c-TF-IDF\r\n\r\n**Strengths:**\r\n- Leverages semantic embeddings\r\n- Handles short documents better\r\n- Can discover varying numbers of topics\r\n- Handles outliers explicitly\r\n\r\n**Weaknesses:**\r\n- Python only\r\n- Less interpretable process\r\n- Computationally intensive\r\n- Newer, less validated in social science\r\n\r\n## Choosing K (Number of Topics)\r\n\r\n**K is a research decision, not a tuning parameter.**\r\n\r\n### What K Represents\r\n\r\nK determines granularity:\r\n- K = 10: Broad themes\r\n- K = 30: More specific topics\r\n- K = 100: Fine-grained distinctions\r\n\r\nMultiple K values are often defensible.\r\n\r\n### Approaches to Selecting K\r\n\r\n**1. Theory-driven:**\r\n- How many themes would you expect?\r\n- What level of granularity answers your question?\r\n- Start with theory, adjust based on interpretability\r\n\r\n**2. Diagnostic-guided:**\r\n\r\n| Metric | What It Measures | Guidance |\r\n|--------|------------------|----------|\r\n| Coherence (C_V) | Do top words co-occur? | Higher is better; > 0.5 often good |\r\n| Coherence (UMass) | Pairwise word co-occurrence | Less negative is better |\r\n| Exclusivity | Are words unique to topics? | Higher means more distinct |\r\n| Perplexity | Held-out likelihood | Lower is better fit |\r\n\r\n**Important:** Do NOT just maximize coherence. A model with K=5 may have higher coherence but miss important distinctions.\r\n\r\n**3. Interpretability-focused:**\r\n- Can you label each topic?\r\n- Do topics make substantive sense?\r\n- Are there \"junk\" topics (stop words, artifacts)?\r\n- Do topics split or merge sensibly across K?\r\n\r\n### Recommended Approach\r\n\r\n```\r\n1. Start with theoretically plausible K (e.g., 15-20)\r\n2. Run models at K-5, K, K+5, K+10\r\n3. For each K:\r\n   - Calculate coherence and exclusivity\r\n   - Attempt to label all topics\r\n   - Count \"junk\" or uninterpretable topics\r\n4. Select K that balances:\r\n   - Coherence/exclusivity metrics\r\n   - Interpretability\r\n   - Theoretical expectations\r\n5. Report sensitivity to K choice\r\n```\r\n\r\n## Preprocessing for Topic Models\r\n\r\n### Standard Pipeline\r\n\r\n```\r\n1. Lowercase\r\n2. Remove punctuation\r\n3. Remove stopwords (SMART list + custom)\r\n4. Remove rare terms (< 5-10 documents)\r\n5. Remove very common terms (> 50-80% of documents)\r\n6. Optional: Lemmatization (NOT stemming)\r\n```\r\n\r\n### Preprocessing Choices and Trade-offs\r\n\r\n| Choice | Pro | Con |\r\n|--------|-----|-----|\r\n| **Stemming** | Reduces vocabulary | Hurts interpretability |\r\n| **Lemmatization** | Cleaner reduction | Slower, needs POS |\r\n| **Bigrams** | Captures phrases | Larger vocabulary |\r\n| **Aggressive stopwords** | Cleaner topics | May lose signal |\r\n\r\n**Recommendation:** Start minimal, add preprocessing if topics have artifacts.\r\n\r\n## Interpretation\r\n\r\n### Reading Topics\r\n\r\nFor each topic, examine:\r\n\r\n1. **Top words** (probability or FREX)\r\n2. **Representative documents** (highest topic proportion)\r\n3. **Distinctive words** (high in this topic, low elsewhere)\r\n\r\n### Topic Labels\r\n\r\n**Good labels:**\r\n- Capture the theme, not just top words\r\n- Are substantively meaningful\r\n- Distinguish this topic from others\r\n\r\n**Bad labels:**\r\n- Just list top words\r\n- Are too generic (\"Miscellaneous\")\r\n- Require seeing the words to understand\r\n\r\n### What Topics Are NOT\r\n\r\nTopics are:\r\n- Statistical patterns of word co-occurrence\r\n- NOT necessarily coherent concepts\r\n- NOT necessarily what documents are \"about\"\r\n\r\nAvoid:\r\n- \"This document IS about Topic 3\"\r\n- \"The topic model discovered that...\"\r\n- Treating topics as ground truth\r\n\r\nPrefer:\r\n- \"This document has high probability for Topic 3\"\r\n- \"Words associated with Topic 3 suggest...\"\r\n- \"One interpretation of this pattern...\"\r\n\r\n## Validation\r\n\r\n### Human Validation\r\n\r\n**Word intrusion test:**\r\n- Show top 5 words + 1 intruder from another topic\r\n- Humans identify intruder\r\n- High accuracy = coherent topic\r\n\r\n**Document intrusion test:**\r\n- Show 3 high-probability documents + 1 from another topic\r\n- Humans identify intruder\r\n- Tests whether topic captures document similarity\r\n\r\n**Topic labeling:**\r\n- Independent coders label topics\r\n- Agreement indicates interpretability\r\n\r\n### Computational Validation\r\n\r\n**Coherence metrics:**\r\n- UMass: Based on document co-occurrence\r\n- C_V: Based on sliding window and word vectors\r\n- NPMI: Normalized pointwise mutual information\r\n\r\n**Held-out likelihood:**\r\n- Fit on training documents\r\n- Evaluate on held-out documents\r\n- Better fit = lower perplexity\r\n\r\n### Robustness Checks\r\n\r\n**Essential:**\r\n- Different random seeds (do same topics emerge?)\r\n- Different K (do topics split/merge sensibly?)\r\n\r\n**Recommended:**\r\n- Different preprocessing\r\n- Subset by time or source\r\n- Compare to alternative method (clustering, BERTopic)\r\n\r\n## Common Problems and Solutions\r\n\r\n### Problem: Junk Topics\r\n\r\n**Symptoms:** Top words are stopwords, numbers, artifacts\r\n\r\n**Solutions:**\r\n- Add terms to custom stopword list\r\n- Increase minimum document frequency\r\n- Check for encoding issues\r\n\r\n### Problem: Duplicate Topics\r\n\r\n**Symptoms:** Multiple topics with similar words\r\n\r\n**Solutions:**\r\n- Reduce K\r\n- Check for document duplicates\r\n- Consider topic correlation (STM)\r\n\r\n### Problem: Uninterpretable Topics\r\n\r\n**Symptoms:** Top words don't form coherent theme\r\n\r\n**Solutions:**\r\n- This happensnot all topics are meaningful\r\n- Document as \"Mixed/Other\"\r\n- Consider if K is too high\r\n\r\n### Problem: Dominant Topic\r\n\r\n**Symptoms:** One topic appears in most documents\r\n\r\n**Solutions:**\r\n- May be legitimate (common theme)\r\n- Check if it's corpus-specific vocabulary\r\n- Consider removing as \"background\" topic\r\n\r\n## Reporting Standards\r\n\r\n### Methods Section\r\n\r\n```markdown\r\nWe used [LDA/STM/BERTopic] to identify latent topics in\r\nthe corpus. After preprocessing ([details]), the document-\r\nterm matrix contained N documents and M terms.\r\n\r\nWe estimated models with K = [X] topics. [Rationale for K].\r\nFor STM, topic prevalence was modeled as a function of\r\n[covariates]. [Software and version].\r\n\r\n[Validation approach and results].\r\n```\r\n\r\n### Results Section\r\n\r\n- Topic labels with top words\r\n- Prevalence estimates\r\n- Covariate effects (if STM)\r\n- Representative quotes\r\n\r\n### Supplementary Materials\r\n\r\n- Full topic-word distributions\r\n- Robustness to K\r\n- Preprocessing details\r\n- Validation results\r\n\r\n## Model Comparison\r\n\r\n| Model | Best For | K Selection | Covariates | Language |\r\n|-------|----------|-------------|------------|----------|\r\n| **LDA** | Standard exploration | Manual | No | R, Python |\r\n| **STM** | Social science research | Diagnostics help | Yes | R |\r\n| **BERTopic** | Short texts, neural approach | Automatic | Limited | Python |\r\n| **CTM** | Correlated topics | Manual | No | R, Python |\r\n| **DTM** | Temporal dynamics | Manual | Time built-in | Python |\r\n\r\n**Recommendation:** Use STM for academic social science research in R. Use BERTopic for neural approach in Python.\r\n",
        "plugins/text-analyst/skills/text-analyst/concepts/03_supervised_classification.md": "# Supervised Text Classification\r\n\r\n## Overview\r\n\r\nSupervised classification trains a model on labeled examples to categorize new documents. Unlike topic models (unsupervised), classification requires training data with known labels.\r\n\r\n## When to Use Classification\r\n\r\n**Good fit:**\r\n- Categories are predefined and clear\r\n- Labeled training data is available\r\n- Need to classify new documents consistently\r\n- Categories don't overlap substantially\r\n\r\n**Poor fit:**\r\n- Exploring unknown structure (use topic models)\r\n- Labels are subjective or inconsistent\r\n- Very few labeled examples (< 50 per class)\r\n- Categories are fuzzy or overlapping\r\n\r\n## The Classification Pipeline\r\n\r\n```\r\n1. Obtain labeled training data\r\n2. Preprocess text\r\n3. Extract features (vectorization)\r\n4. Train classifier\r\n5. Evaluate on held-out data\r\n6. Apply to new documents\r\n```\r\n\r\n## Training Data\r\n\r\n### Obtaining Labels\r\n\r\n**Sources:**\r\n- Existing metadata (source, category, author)\r\n- Expert coding\r\n- Crowdsourced coding\r\n- Weak supervision (patterns, keywords)\r\n\r\n### How Much Data?\r\n\r\n| Task Complexity | Minimum per Class | Recommended |\r\n|-----------------|-------------------|-------------|\r\n| Binary, clear distinction | 50 | 200+ |\r\n| Multi-class (3-5 classes) | 100 | 300+ per class |\r\n| Fine-grained (10+ classes) | 200 | 500+ per class |\r\n| Rare classes | More for minority | Balance classes |\r\n\r\n### Label Quality\r\n\r\n**Requirements:**\r\n- Clear category definitions\r\n- Consistent application\r\n- Inter-rater reliability (if multiple coders)\r\n- Documentation of edge cases\r\n\r\n**Calculating inter-rater reliability:**\r\n- Cohen's Kappa for 2 raters\r\n- Fleiss' Kappa for 3+ raters\r\n- Target: Kappa > 0.7 for acceptable reliability\r\n\r\n## Feature Extraction\r\n\r\n### Bag-of-Words / TF-IDF\r\n\r\n```\r\nDocument  Vector of term frequencies\r\n```\r\n\r\n**TF-IDF weighting:**\r\n- Upweights distinctive terms\r\n- Downweights common terms\r\n- Standard for traditional ML\r\n\r\n**Parameters:**\r\n- Vocabulary size (max_features)\r\n- N-gram range (unigrams, bigrams)\r\n- Min/max document frequency\r\n\r\n### Word Embeddings\r\n\r\n**Pre-trained:**\r\n- Word2Vec, GloVe averages\r\n- Sentence embeddings (SBERT)\r\n- Paragraph vectors (Doc2Vec)\r\n\r\n**Strengths:**\r\n- Captures semantic similarity\r\n- Handles synonyms\r\n- Lower-dimensional\r\n\r\n**Weaknesses:**\r\n- Less interpretable\r\n- May miss domain-specific meaning\r\n\r\n### Contextual Embeddings (BERT)\r\n\r\n**Approach:**\r\n- Use BERT/RoBERTa to encode documents\r\n- Fine-tune on classification task\r\n- Or use embeddings with simpler classifier\r\n\r\n**Strengths:**\r\n- State-of-the-art performance\r\n- Captures context and nuance\r\n- Transfer learning from large corpora\r\n\r\n**Weaknesses:**\r\n- Computationally expensive\r\n- Requires GPU for training\r\n- Harder to interpret\r\n\r\n## Classifier Models\r\n\r\n### Traditional ML\r\n\r\n| Model | Strengths | Weaknesses |\r\n|-------|-----------|------------|\r\n| **Naive Bayes** | Fast, works with small data | Assumes independence |\r\n| **Logistic Regression** | Interpretable, reliable | Linear boundaries |\r\n| **SVM** | Effective in high dimensions | Less interpretable |\r\n| **Random Forest** | Handles non-linearity | Slower, larger models |\r\n\r\n**Recommendation:** Start with Logistic Regression or SVM for interpretability and reliability.\r\n\r\n### Deep Learning\r\n\r\n| Model | Strengths | Weaknesses |\r\n|-------|-----------|------------|\r\n| **CNN** | Captures local patterns | Needs more data |\r\n| **LSTM/RNN** | Sequence modeling | Slow to train |\r\n| **BERT fine-tuned** | State-of-the-art | Needs GPU, more data |\r\n\r\n**Recommendation:** Use BERT only if you have 1000+ examples per class and GPU access.\r\n\r\n### Zero-Shot Classification\r\n\r\n**Approach:** Use large language models to classify without training data.\r\n\r\n```python\r\nfrom transformers import pipeline\r\nclassifier = pipeline(\"zero-shot-classification\")\r\nresult = classifier(text, candidate_labels=[\"politics\", \"sports\", \"business\"])\r\n```\r\n\r\n**Strengths:**\r\n- No training data needed\r\n- Quick to prototype\r\n\r\n**Weaknesses:**\r\n- Less accurate than fine-tuned models\r\n- Depends on label naming\r\n- Not validated for research use\r\n\r\n## Evaluation Metrics\r\n\r\n### Basic Metrics\r\n\r\n| Metric | Formula | Use When |\r\n|--------|---------|----------|\r\n| **Accuracy** | Correct / Total | Classes are balanced |\r\n| **Precision** | TP / (TP + FP) | False positives are costly |\r\n| **Recall** | TP / (TP + FN) | False negatives are costly |\r\n| **F1** | 2  (P  R) / (P + R) | Balance precision/recall |\r\n\r\n### Multi-Class Metrics\r\n\r\n| Metric | Description |\r\n|--------|-------------|\r\n| **Macro-F1** | Average F1 across classes (equal weight) |\r\n| **Weighted-F1** | F1 weighted by class frequency |\r\n| **Micro-F1** | Global TP/FP/FN (equals accuracy) |\r\n\r\n**Recommendation:** Report macro-F1 for research (treats all classes equally).\r\n\r\n### Confusion Matrix\r\n\r\nEssential for understanding errors:\r\n\r\n```\r\n              Predicted\r\n             Pos    Neg\r\nActual Pos [ TP     FN ]\r\n       Neg [ FP     TN ]\r\n```\r\n\r\nAlways examine:\r\n- Which classes are confused?\r\n- Are errors systematic?\r\n- What do misclassified examples look like?\r\n\r\n## Train/Test Splitting\r\n\r\n### Hold-Out Validation\r\n\r\n```\r\nFull Data  Train (70-80%) / Test (20-30%)\r\n```\r\n\r\n**Never use test set for model selection.**\r\n\r\n### Cross-Validation\r\n\r\n```\r\nData  K folds\r\nFor each fold:\r\n  Train on K-1 folds\r\n  Evaluate on 1 fold\r\nReport: Mean  SD of metric\r\n```\r\n\r\n**Recommendation:** 5-fold or 10-fold CV for model selection.\r\n\r\n### Stratified Splitting\r\n\r\n**Always stratify by class label** to maintain class proportions in each split.\r\n\r\n```python\r\nfrom sklearn.model_selection import StratifiedKFold\r\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\r\n```\r\n\r\n### Temporal Considerations\r\n\r\nIf data is time-ordered:\r\n- Consider temporal split (train on past, test on future)\r\n- Avoid data leakage from future to past\r\n\r\n## Handling Class Imbalance\r\n\r\n### The Problem\r\n\r\nIf Class A has 90% of examples:\r\n- Predicting \"A\" always gives 90% accuracy\r\n- Minority class poorly classified\r\n\r\n### Solutions\r\n\r\n| Approach | Description | When to Use |\r\n|----------|-------------|-------------|\r\n| **Class weights** | Upweight minority class loss | First approach to try |\r\n| **Oversampling** | Duplicate minority examples | Simple, effective |\r\n| **SMOTE** | Synthetic minority examples | When oversampling insufficient |\r\n| **Undersampling** | Reduce majority class | Large datasets only |\r\n\r\n**In sklearn:**\r\n```python\r\nmodel = LogisticRegression(class_weight='balanced')\r\n```\r\n\r\n## Error Analysis\r\n\r\n### Systematic Error Analysis\r\n\r\nAfter training:\r\n\r\n1. **Identify misclassified examples**\r\n2. **Categorize errors:**\r\n   - Label noise (gold label is wrong)\r\n   - Ambiguous cases (genuinely unclear)\r\n   - Model limitations (learnable but missed)\r\n3. **Look for patterns:**\r\n   - Are certain terms misleading?\r\n   - Are certain document types harder?\r\n4. **Improve:**\r\n   - Fix labeling issues\r\n   - Add training examples for hard cases\r\n   - Adjust features or preprocessing\r\n\r\n### Example Error Analysis\r\n\r\n```markdown\r\n## Error Analysis: Politics vs. Business\r\n\r\n### Most Common Errors\r\n1. Economic policy articles  Often misclassified\r\n   - Contain both political and business vocabulary\r\n   - Solution: Consider \"Policy\" as separate class\r\n\r\n2. Campaign finance articles  Classified as Business\r\n   - \"Donations\", \"funding\" trigger business features\r\n   - Solution: Add \"campaign\" + \"finance\" bigrams\r\n```\r\n\r\n## Active Learning\r\n\r\nWhen labeling is expensive:\r\n\r\n1. Train initial model on small labeled set\r\n2. Apply to unlabeled data\r\n3. Select uncertain examples for labeling\r\n4. Add labels, retrain\r\n5. Repeat\r\n\r\n**Selection strategies:**\r\n- Uncertainty sampling (label what model is unsure about)\r\n- Query-by-committee (label where models disagree)\r\n\r\n## Reporting Standards\r\n\r\n### Methods Section\r\n\r\n```markdown\r\n## Text Classification\r\n\r\nWe trained a [model type] classifier to categorize documents\r\ninto [N] classes: [class names].\r\n\r\nTraining data consisted of N documents labeled by [process].\r\nInter-rater reliability: Kappa = X.XX (N coders, N documents).\r\n\r\nFeatures: [TF-IDF with N features / BERT embeddings / etc.]\r\nPreprocessing: [steps]\r\n\r\nWe used [K]-fold stratified cross-validation for model\r\nselection and report performance on a held-out test set\r\n(N = X documents).\r\n```\r\n\r\n### Results Section\r\n\r\n```markdown\r\nThe classifier achieved macro-F1 = X.XX on the held-out\r\ntest set (Table X). Per-class performance ranged from\r\nF1 = X.XX ([class]) to F1 = X.XX ([class]).\r\n\r\nError analysis revealed [systematic patterns].\r\n```\r\n\r\n### Tables\r\n\r\n**Table: Classification Performance**\r\n\r\n| Class | Precision | Recall | F1 | Support |\r\n|-------|-----------|--------|-----|---------|\r\n| Class A | 0.85 | 0.82 | 0.83 | 150 |\r\n| Class B | 0.78 | 0.81 | 0.79 | 120 |\r\n| ... | | | | |\r\n| Macro Avg | 0.81 | 0.81 | 0.81 | 400 |\r\n\r\n## Common Pitfalls\r\n\r\n### 1. Data Leakage\r\n\r\n**Problem:** Information from test set influences training.\r\n\r\n**Solutions:**\r\n- Split data BEFORE any preprocessing\r\n- Don't use test set for feature selection\r\n- Be careful with temporal data\r\n\r\n### 2. Overfitting\r\n\r\n**Problem:** Model memorizes training data.\r\n\r\n**Signs:**\r\n- Training accuracy >> test accuracy\r\n- Model is overly complex\r\n\r\n**Solutions:**\r\n- Regularization\r\n- Cross-validation\r\n- Simpler model\r\n\r\n### 3. Label Leakage\r\n\r\n**Problem:** Feature contains label information.\r\n\r\n**Example:** Document ID correlates with class (same authors write same topics).\r\n\r\n**Solution:** Remove non-content features.\r\n\r\n### 4. Ignoring Class Imbalance\r\n\r\n**Problem:** Majority class dominates.\r\n\r\n**Solution:** Use class weights, macro-F1 evaluation.\r\n\r\n## Classifier Selection Guide\r\n\r\n```\r\nIs interpretability important?\r\n  Yes  Logistic Regression or Naive Bayes\r\n  No  Continue\r\n\r\nDo you have > 1000 examples per class?\r\n  Yes  Consider BERT fine-tuning\r\n  No  Continue\r\n\r\nIs the task complex (subtle distinctions)?\r\n  Yes  SVM with careful feature engineering\r\n  No  Logistic Regression with TF-IDF\r\n\r\nDo you have GPU access and time?\r\n  Yes  Try BERT, compare to baseline\r\n  No  Stick with traditional ML\r\n```\r\n",
        "plugins/text-analyst/skills/text-analyst/concepts/04_embeddings.md": "# Word and Document Embeddings\r\n\r\n## Overview\r\n\r\nEmbeddings represent words or documents as dense vectors in continuous space. Unlike bag-of-words (sparse, high-dimensional), embeddings are dense (100-1000 dimensions) and capture semantic relationships.\r\n\r\n## Key Concepts\r\n\r\n### The Distributional Hypothesis\r\n\r\n\"You shall know a word by the company it keeps.\"  J.R. Firth\r\n\r\nWords appearing in similar contexts have similar meanings. Embeddings operationalize this: similar words have similar vectors.\r\n\r\n### Vector Properties\r\n\r\n**Similarity:**\r\n```\r\ncosine_similarity(king, queen) > cosine_similarity(king, apple)\r\n```\r\n\r\n**Analogies:**\r\n```\r\nking - man + woman  queen\r\n```\r\n\r\n**Clustering:**\r\nWords form semantic clusters in embedding space.\r\n\r\n## Types of Embeddings\r\n\r\n### Word2Vec\r\n\r\n**Training objective:** Predict word from context (CBOW) or context from word (Skip-gram).\r\n\r\n**Output:** One vector per word type (not token).\r\n\r\n**Limitations:**\r\n- One vector per word (ignores polysemy)\r\n- No subword information\r\n- Context window is fixed\r\n\r\n### GloVe\r\n\r\n**Training objective:** Factorize word co-occurrence matrix.\r\n\r\n**Output:** Similar to Word2Vec; often comparable performance.\r\n\r\n**Trade-off:** Global statistics vs. local context windows.\r\n\r\n### FastText\r\n\r\n**Extension of Word2Vec:** Includes subword (character n-gram) information.\r\n\r\n**Advantage:** Can handle out-of-vocabulary words and morphological variants.\r\n\r\n**Example:**\r\n```\r\n\"unhappiness\"  embeddings for \"un\", \"hap\", \"app\", \"ppi\", \"ness\", etc.\r\n```\r\n\r\n### Sentence Transformers (SBERT)\r\n\r\n**Based on BERT:** Produces single vector for entire sentence/document.\r\n\r\n**Training:** Fine-tuned for sentence similarity tasks.\r\n\r\n**Advantage:** Semantically meaningful sentence-level embeddings.\r\n\r\n```python\r\nfrom sentence_transformers import SentenceTransformer\r\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\r\nembeddings = model.encode([\"This is a sentence.\", \"This is another.\"])\r\n```\r\n\r\n### BERT and Contextual Embeddings\r\n\r\n**Key difference:** One vector per word *token*, not type. The vector for \"bank\" differs in \"river bank\" vs. \"bank account\".\r\n\r\n**Layers:** BERT has multiple layers; different layers capture different information.\r\n\r\n**Using BERT embeddings:**\r\n- Average token embeddings for document vector\r\n- Use [CLS] token representation\r\n- Fine-tune for specific task\r\n\r\n## When to Use Each\r\n\r\n| Embedding | Best For | Considerations |\r\n|-----------|----------|----------------|\r\n| **Word2Vec/GloVe** | Semantic similarity, analogies | Fast, interpretable |\r\n| **FastText** | Morphologically rich languages, rare words | Handles OOV |\r\n| **SBERT** | Document similarity, clustering, retrieval | Best for sentence-level |\r\n| **BERT** | Classification, NER, complex NLU | Requires fine-tuning |\r\n\r\n## Document Embeddings\r\n\r\n### Simple Aggregation\r\n\r\n```\r\ndoc_vector = mean(word_vectors for word in document)\r\n```\r\n\r\n**Pros:** Simple, fast, interpretable\r\n**Cons:** Ignores word order, importance\r\n\r\n### TF-IDF Weighted Average\r\n\r\n```\r\ndoc_vector =  (tf-idf_weight  word_vector) /  tf-idf_weight\r\n```\r\n\r\n**Improvement:** Downweights common words.\r\n\r\n### Doc2Vec / Paragraph Vectors\r\n\r\n**Approach:** Learn document vectors alongside word vectors.\r\n\r\n**Variants:**\r\n- PV-DM: Distributed Memory (like CBOW + doc vector)\r\n- PV-DBOW: Distributed Bag of Words (like Skip-gram)\r\n\r\n### Sentence Transformers (Recommended)\r\n\r\nBest current approach for document embeddings:\r\n\r\n```python\r\nfrom sentence_transformers import SentenceTransformer\r\nmodel = SentenceTransformer('all-mpnet-base-v2')\r\ndoc_embeddings = model.encode(documents)\r\n```\r\n\r\n## Applications\r\n\r\n### 1. Document Similarity\r\n\r\nFind similar documents:\r\n\r\n```python\r\nfrom sklearn.metrics.pairwise import cosine_similarity\r\n\r\n# Get embeddings\r\nembeddings = model.encode(documents)\r\n\r\n# Find most similar to document 0\r\nsimilarities = cosine_similarity([embeddings[0]], embeddings)[0]\r\nmost_similar = similarities.argsort()[-5:][::-1]  # Top 5\r\n```\r\n\r\n### 2. Semantic Search\r\n\r\nFind documents matching a query:\r\n\r\n```python\r\nquery_embedding = model.encode([\"What is climate change?\"])\r\ndoc_embeddings = model.encode(documents)\r\nsimilarities = cosine_similarity(query_embedding, doc_embeddings)[0]\r\ntop_results = similarities.argsort()[-10:][::-1]\r\n```\r\n\r\n### 3. Clustering\r\n\r\nGroup similar documents:\r\n\r\n```python\r\nfrom sklearn.cluster import KMeans\r\n\r\nembeddings = model.encode(documents)\r\nclusters = KMeans(n_clusters=10).fit_predict(embeddings)\r\n```\r\n\r\n### 4. Dimensionality Reduction + Visualization\r\n\r\n```python\r\nfrom sklearn.manifold import TSNE\r\n\r\nembeddings_2d = TSNE(n_components=2).fit_transform(embeddings)\r\n# Plot with matplotlib\r\n```\r\n\r\n### 5. Feature Input for Classification\r\n\r\nUse embeddings as features for downstream tasks:\r\n\r\n```python\r\nembeddings = model.encode(documents)\r\nclf = LogisticRegression()\r\nclf.fit(embeddings, labels)\r\n```\r\n\r\n## Pretrained vs. Training Your Own\r\n\r\n### Use Pretrained When:\r\n- General domain (news, social media, common language)\r\n- Limited computational resources\r\n- Limited training data\r\n\r\n### Train Your Own When:\r\n- Highly specialized domain (medical, legal, technical)\r\n- Domain-specific vocabulary\r\n- Large in-domain corpus available\r\n\r\n### Fine-Tuning Pretrained:\r\n- Middle ground: start with pretrained, adjust for domain\r\n- Requires labeled data for the fine-tuning task\r\n\r\n## Evaluation\r\n\r\n### Intrinsic Evaluation\r\n\r\n**Word similarity:** Correlate embedding similarity with human judgments (WordSim-353, SimLex-999).\r\n\r\n**Analogy completion:** \"king - man + woman = ?\" should yield \"queen\".\r\n\r\n### Extrinsic Evaluation\r\n\r\n**Downstream task performance:** How well do embeddings work for your actual task (classification, clustering, retrieval)?\r\n\r\n**This is what matters for research applications.**\r\n\r\n## Common Issues\r\n\r\n### Out-of-Vocabulary (OOV) Words\r\n\r\n**Problem:** Word not in vocabulary  no embedding.\r\n\r\n**Solutions:**\r\n| Approach | Implementation |\r\n|----------|----------------|\r\n| FastText | Subword embeddings handle OOV |\r\n| BERT | Subword tokenization handles OOV |\r\n| Replace with `<UNK>` | Use unknown token vector |\r\n| Skip OOV | Ignore in averaging |\r\n\r\n### Polysemy\r\n\r\n**Problem:** \"Bank\" has different meanings.\r\n\r\n**Solution:** Use contextual embeddings (BERT) that produce different vectors based on context.\r\n\r\n### Bias in Embeddings\r\n\r\n**Problem:** Embeddings reflect biases in training data.\r\n\r\n**Example:** \"man : doctor :: woman : nurse\" analogy may emerge.\r\n\r\n**Awareness:** Document potential biases; consider debiasing for sensitive applications.\r\n\r\n### Dimensionality\r\n\r\n**Typical dimensions:**\r\n- Word2Vec/GloVe: 50-300\r\n- FastText: 100-300\r\n- BERT: 768 (base) or 1024 (large)\r\n- SBERT: 384-768\r\n\r\n**Trade-off:** Higher dimensions capture more; but may overfit with small data.\r\n\r\n## Practical Recommendations\r\n\r\n### For Research Projects\r\n\r\n1. **Start with pretrained SBERT** (`all-mpnet-base-v2` or `all-MiniLM-L6-v2`)\r\n2. **Check domain fit:** Does similarity make sense for your texts?\r\n3. **Compare to TF-IDF baseline:** Embeddings should outperform\r\n4. **Report model and version** for reproducibility\r\n\r\n### Model Selection\r\n\r\n```\r\nIs your task sentence/document-level?\r\n  Yes  Use Sentence Transformers\r\n  No (word-level)  Continue\r\n\r\nDo you need to handle rare/technical words?\r\n  Yes  FastText or BERT\r\n  No  Word2Vec or GloVe\r\n\r\nIs computational cost a concern?\r\n  Yes  Word2Vec, GloVe, or small SBERT\r\n  No  BERT or large SBERT\r\n\r\nDo you need contextual disambiguation?\r\n  Yes  BERT embeddings\r\n  No  Static embeddings are fine\r\n```\r\n\r\n## Reporting Standards\r\n\r\n### Methods Section\r\n\r\n```markdown\r\nWe represented documents using [model name] (Author, Year).\r\n[For pretrained: Model trained on X corpus.]\r\n[For fine-tuned: We fine-tuned on Y task with Z examples.]\r\n\r\nDocument vectors were computed by [averaging word vectors /\r\nusing sentence transformer / etc.].\r\n\r\nEmbeddings were used for [similarity calculation /\r\nclassification features / clustering / etc.].\r\n```\r\n\r\n### Reproducibility\r\n\r\nReport:\r\n- Model name and version\r\n- Source (HuggingFace, gensim, etc.)\r\n- Preprocessing before embedding\r\n- Any fine-tuning details\r\n- Similarity metric used (cosine, Euclidean)\r\n\r\n## Code Examples\r\n\r\n### Word2Vec with gensim\r\n\r\n```python\r\nfrom gensim.models import Word2Vec\r\n\r\n# Train\r\nsentences = [doc.split() for doc in documents]\r\nmodel = Word2Vec(sentences, vector_size=100, window=5, min_count=5)\r\n\r\n# Get word vector\r\nvector = model.wv['example']\r\n\r\n# Find similar words\r\nsimilar = model.wv.most_similar('example', topn=10)\r\n```\r\n\r\n### Sentence Transformers\r\n\r\n```python\r\nfrom sentence_transformers import SentenceTransformer\r\nfrom sklearn.metrics.pairwise import cosine_similarity\r\n\r\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\r\nembeddings = model.encode(documents)\r\n\r\n# Similarity matrix\r\nsim_matrix = cosine_similarity(embeddings)\r\n```\r\n\r\n### Using Pretrained GloVe\r\n\r\n```python\r\nimport numpy as np\r\n\r\n# Load pretrained\r\nembeddings_index = {}\r\nwith open('glove.6B.100d.txt') as f:\r\n    for line in f:\r\n        values = line.split()\r\n        word = values[0]\r\n        vector = np.array(values[1:], dtype='float32')\r\n        embeddings_index[word] = vector\r\n\r\n# Get document vector (average)\r\ndef doc_vector(text, embeddings_index):\r\n    words = text.lower().split()\r\n    vectors = [embeddings_index[w] for w in words if w in embeddings_index]\r\n    return np.mean(vectors, axis=0) if vectors else np.zeros(100)\r\n```\r\n",
        "plugins/text-analyst/skills/text-analyst/concepts/05_sentiment_analysis.md": "# Sentiment Analysis\r\n\r\n## Overview\r\n\r\nSentiment analysis measures the emotional tone, opinion, or attitude expressed in text. It ranges from simple positive/negative classification to nuanced emotion detection.\r\n\r\n## Types of Sentiment Analysis\r\n\r\n### Polarity Classification\r\n\r\n**Goal:** Classify text as positive, negative, or neutral.\r\n\r\n**Granularity:**\r\n- Binary: Positive vs. Negative\r\n- Ternary: Positive / Neutral / Negative\r\n- Fine-grained: 5-point scale (very negative to very positive)\r\n\r\n### Emotion Detection\r\n\r\n**Goal:** Identify specific emotions.\r\n\r\n**Common taxonomies:**\r\n- Ekman: Anger, Disgust, Fear, Joy, Sadness, Surprise\r\n- Plutchik: 8 primary emotions with intensities\r\n- NRC: 8 emotions + positive/negative\r\n\r\n### Aspect-Based Sentiment\r\n\r\n**Goal:** Identify sentiment toward specific aspects.\r\n\r\n**Example:** \"The food was great but the service was slow.\"\r\n- Food: Positive\r\n- Service: Negative\r\n\r\n### Stance Detection\r\n\r\n**Goal:** Identify position toward a target.\r\n\r\n**Example:** Favor, Against, or Neutral toward a policy.\r\n\r\n## Approaches\r\n\r\n### Dictionary-Based\r\n\r\n**How it works:**\r\n1. Match words to sentiment lexicon\r\n2. Aggregate scores\r\n3. Classify based on threshold\r\n\r\n**Advantages:**\r\n- Transparent (know exactly which words triggered)\r\n- No training data needed\r\n- Fast\r\n\r\n**Disadvantages:**\r\n- Misses context and sarcasm\r\n- Domain mismatch issues\r\n- Doesn't handle negation well (simple versions)\r\n\r\n### Machine Learning\r\n\r\n**How it works:**\r\n1. Train classifier on labeled examples\r\n2. Learn patterns from data\r\n3. Apply to new texts\r\n\r\n**Advantages:**\r\n- Learns domain-specific patterns\r\n- Can capture complex relationships\r\n- Often more accurate\r\n\r\n**Disadvantages:**\r\n- Needs labeled training data\r\n- Less interpretable\r\n- May not generalize\r\n\r\n### Deep Learning / Transformers\r\n\r\n**How it works:**\r\n- Fine-tune BERT or similar on sentiment task\r\n- Or use pretrained sentiment models\r\n\r\n**Advantages:**\r\n- State-of-the-art performance\r\n- Captures context and nuance\r\n- Transfer learning\r\n\r\n**Disadvantages:**\r\n- Computational cost\r\n- Needs more training data\r\n- Least interpretable\r\n\r\n## Popular Tools and Lexicons\r\n\r\n### Dictionary-Based Tools\r\n\r\n| Tool | Approach | Strengths |\r\n|------|----------|-----------|\r\n| **VADER** | Rule-based with intensifiers | Social media, handles punctuation/emoji |\r\n| **TextBlob** | Pattern-based | Simple, fast, includes subjectivity |\r\n| **LIWC** | Category-based | Extensive psychological categories |\r\n| **AFINN** | Scored word list | Simple, manually curated |\r\n| **SentiWordNet** | WordNet-based | Large coverage, synset scores |\r\n\r\n### Key Lexicons\r\n\r\n| Lexicon | Content | Best For |\r\n|---------|---------|----------|\r\n| **LIWC** | 90+ categories | Psychological analysis |\r\n| **NRC Emotion** | 8 emotions + valence | Emotion detection |\r\n| **NRC VAD** | Valence, Arousal, Dominance | Dimensional emotion |\r\n| **Loughran-McDonald** | Finance-specific | Financial texts |\r\n| **VADER** | Social media focused | Tweets, reviews |\r\n\r\n### Pretrained Models\r\n\r\n| Model | Source | Use Case |\r\n|-------|--------|----------|\r\n| **distilbert-sentiment** | HuggingFace | General sentiment |\r\n| **twitter-roberta-sentiment** | HuggingFace | Twitter/social media |\r\n| **finbert** | HuggingFace | Financial sentiment |\r\n| **cardiffnlp models** | HuggingFace | Social media tasks |\r\n\r\n## VADER: A Closer Look\r\n\r\nVADER (Valence Aware Dictionary and sEntiment Reasoner) is popular for social media.\r\n\r\n**Features:**\r\n- Handles punctuation (\"good!\" vs \"good\")\r\n- Handles capitalization (\"GOOD\" vs \"good\")\r\n- Handles intensifiers (\"very good\")\r\n- Handles negation (\"not good\")\r\n- Handles conjunctions (\"good but not great\")\r\n- Includes emoji support\r\n\r\n**Output:**\r\n```python\r\n{'neg': 0.0, 'neu': 0.254, 'pos': 0.746, 'compound': 0.8316}\r\n```\r\n\r\n**Compound score:** -1 (most negative) to +1 (most positive)\r\n- Compound  0.05  Positive\r\n- Compound  -0.05  Negative\r\n- Otherwise  Neutral\r\n\r\n## Domain Considerations\r\n\r\n### Domain Mismatch\r\n\r\n**Problem:** Sentiment lexicons trained on one domain may fail on another.\r\n\r\n**Example:** \"Unpredictable\" is:\r\n- Negative in product reviews (\"unpredictable quality\")\r\n- Positive in movie reviews (\"unpredictable plot\")\r\n- Neutral in academic text\r\n\r\n### Domain-Specific Approaches\r\n\r\n| Domain | Recommended Approach |\r\n|--------|---------------------|\r\n| **Product reviews** | General sentiment tools work well |\r\n| **Social media** | VADER, Twitter-specific models |\r\n| **Financial** | Loughran-McDonald, FinBERT |\r\n| **Political** | Custom dictionaries, stance detection |\r\n| **Academic** | May need custom approach |\r\n| **Medical** | Specialized models needed |\r\n\r\n## Handling Challenges\r\n\r\n### Negation\r\n\r\n**Problem:** \"Not good\" contains positive word but negative meaning.\r\n\r\n**Solutions:**\r\n- VADER handles automatically\r\n- Negation window (flip polarity of following words)\r\n- Bigram features (\"not_good\" as single token)\r\n\r\n### Sarcasm and Irony\r\n\r\n**Problem:** \"Oh great, another meeting\" is negative despite \"great.\"\r\n\r\n**Solutions:**\r\n- Very difficult for automated methods\r\n- Large models (GPT, BERT) do better but not perfectly\r\n- Consider domain (sarcasm more common in social media)\r\n- Accept and document limitation\r\n\r\n### Intensity\r\n\r\n**Problem:** \"Good,\" \"great,\" and \"amazing\" differ in intensity.\r\n\r\n**Solutions:**\r\n- VADER includes intensity modifiers\r\n- Use fine-grained scales (1-5 instead of pos/neg)\r\n- NRC VAD provides intensity dimensions\r\n\r\n### Mixed Sentiment\r\n\r\n**Problem:** Documents contain both positive and negative elements.\r\n\r\n**Solutions:**\r\n- Report both positive and negative scores\r\n- Use aspect-based sentiment\r\n- Analyze at sentence level and aggregate\r\n\r\n### Implicit Sentiment\r\n\r\n**Problem:** \"The product arrived broken\" implies negative without explicit sentiment words.\r\n\r\n**Solutions:**\r\n- ML approaches learn these patterns\r\n- Larger context models (BERT) help\r\n- May require aspect-based sentiment\r\n\r\n## Validation\r\n\r\n### Comparing to Human Judgment\r\n\r\n**Essential validation:**\r\n1. Sample documents\r\n2. Have humans rate sentiment\r\n3. Calculate agreement with automated scores\r\n\r\n**Metrics:**\r\n- Correlation (for continuous scores)\r\n- Accuracy, F1 (for categories)\r\n- Cohen's Kappa (for agreement)\r\n\r\n### Reporting Validation\r\n\r\n```markdown\r\nWe validated VADER sentiment scores against human coding.\r\nTwo coders rated N documents on a 5-point scale\r\n(inter-rater reliability: r = 0.XX). VADER compound\r\nscores correlated with human ratings at r = 0.XX.\r\n```\r\n\r\n### Known Benchmark Comparisons\r\n\r\nReport performance on standard datasets if applicable:\r\n- Movie reviews (Pang & Lee)\r\n- Twitter sentiment (SemEval)\r\n- Product reviews (Amazon)\r\n\r\n## Reporting Standards\r\n\r\n### Methods Section\r\n\r\n```markdown\r\n## Sentiment Analysis\r\n\r\nWe measured sentiment using [tool/lexicon] (Author, Year).\r\n[Brief description of tool].\r\n\r\nFor each document, we calculated [metric: compound score /\r\npositive-negative ratio / classification].\r\n\r\n[Preprocessing steps if any].\r\n\r\n### Validation\r\nWe validated against [human coding / alternative measure].\r\n[Validation results].\r\n```\r\n\r\n### Results Section\r\n\r\nReport:\r\n- Distribution of sentiment scores\r\n- Mean/median by relevant groups\r\n- Temporal trends if applicable\r\n- Key limitations\r\n\r\n### Visualizations\r\n\r\n- Histogram of sentiment distribution\r\n- Time series of sentiment\r\n- Comparison across groups (bar chart or violin plot)\r\n\r\n## Choosing an Approach\r\n\r\n```\r\nIs interpretability critical?\r\n  Yes  Dictionary-based (VADER, LIWC)\r\n  No  Continue\r\n\r\nDo you have labeled training data?\r\n  Yes  Consider ML approach\r\n  No  Continue\r\n\r\nIs text from social media?\r\n  Yes  VADER or twitter-roberta-sentiment\r\n  No  Continue\r\n\r\nIs text domain-specific (finance, medical)?\r\n  Yes  Use domain-specific lexicon or model\r\n  No  Continue\r\n\r\nDefault: Start with VADER, validate, consider alternatives\r\n```\r\n\r\n## Practical Workflow\r\n\r\n```\r\n1. Start with VADER (or domain-appropriate tool)\r\n2. Calculate sentiment for all documents\r\n3. Check distribution (ceiling/floor effects?)\r\n4. Validate on sample against human judgment\r\n5. Examine errors (why did it fail?)\r\n6. Consider alternatives if validation is poor\r\n7. Report validation and limitations\r\n```\r\n\r\n## Code Examples\r\n\r\n### VADER in Python\r\n\r\n```python\r\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\r\n\r\nanalyzer = SentimentIntensityAnalyzer()\r\nscores = analyzer.polarity_scores(\"This is a great example!\")\r\n# {'neg': 0.0, 'neu': 0.423, 'pos': 0.577, 'compound': 0.6588}\r\n```\r\n\r\n### TextBlob in Python\r\n\r\n```python\r\nfrom textblob import TextBlob\r\n\r\nblob = TextBlob(\"This is a great example!\")\r\nprint(blob.sentiment)\r\n# Sentiment(polarity=0.8, subjectivity=0.75)\r\n```\r\n\r\n### tidytext in R\r\n\r\n```r\r\nlibrary(tidytext)\r\nlibrary(dplyr)\r\n\r\ntext_df <- tibble(text = c(\"This is great!\", \"This is terrible.\"))\r\n\r\ntext_df %>%\r\n  unnest_tokens(word, text) %>%\r\n  inner_join(get_sentiments(\"bing\")) %>%\r\n  count(sentiment)\r\n```\r\n\r\n### HuggingFace Transformers\r\n\r\n```python\r\nfrom transformers import pipeline\r\n\r\nsentiment_pipeline = pipeline(\"sentiment-analysis\")\r\nresult = sentiment_pipeline(\"This is a great example!\")\r\n# [{'label': 'POSITIVE', 'score': 0.9998}]\r\n```\r\n\r\n## Common Pitfalls\r\n\r\n1. **Not validating in domain** - Always check if tool works for your texts\r\n2. **Ignoring coverage** - What % of documents have sentiment words?\r\n3. **Over-interpreting scores** - Small differences may not be meaningful\r\n4. **Treating as ground truth** - Sentiment scores are estimates with error\r\n5. **Not reporting method details** - Readers need to evaluate appropriateness\r\n",
        "plugins/text-analyst/skills/text-analyst/concepts/06_validation_strategies.md": "# Validation Strategies for Text Analysis\r\n\r\n## Overview\r\n\r\nComputational text analysis produces outputstopic labels, sentiment scores, classificationsbut these are not automatically valid. Validation establishes that outputs measure what they claim to measure and that findings are robust to analytical choices.\r\n\r\n## Why Validation Matters\r\n\r\n**Text analysis is not self-validating.**\r\n\r\n- Topic models find patterns, but patterns may be artifacts\r\n- Classifiers achieve accuracy, but may learn shortcuts\r\n- Dictionaries count words, but words have multiple meanings\r\n\r\nWithout validation, you have numbers without meaning.\r\n\r\n## Types of Validity\r\n\r\n### Construct Validity\r\n\r\n**Question:** Does this measure capture the intended concept?\r\n\r\n**For topic models:** Do topics represent coherent themes?\r\n**For classifiers:** Do categories match conceptual definitions?\r\n**For dictionaries:** Do word matches reflect the concept?\r\n\r\n### Content Validity\r\n\r\n**Question:** Does the measure cover the full scope of the concept?\r\n\r\n**For topic models:** Are important themes missing?\r\n**For classifiers:** Are category boundaries appropriate?\r\n**For dictionaries:** Are relevant words included?\r\n\r\n### Criterion Validity\r\n\r\n**Question:** Does the measure correlate with related measures?\r\n\r\n**Concurrent:** Correlation with alternative measure of same thing\r\n**Predictive:** Predicts expected outcomes\r\n\r\n### Face Validity\r\n\r\n**Question:** Does it look right to experts?\r\n\r\nNecessary but not sufficient. Easy to satisfy but doesn't guarantee validity.\r\n\r\n## Validation for Topic Models\r\n\r\n### Human Validation Methods\r\n\r\n#### 1. Word Intrusion Test\r\n\r\n**Procedure:**\r\n1. For each topic, show top 5-6 words\r\n2. Add one \"intruder\" word from another topic\r\n3. Ask humans to identify the intruder\r\n4. Accuracy indicates topic coherence\r\n\r\n**Example:**\r\n```\r\nTopic: economy, tax, budget, spending, growth, [intruder: military]\r\n```\r\n\r\nHigh agreement  Topics are interpretable\r\n\r\n#### 2. Topic Intrusion Test\r\n\r\n**Procedure:**\r\n1. Show a document\r\n2. Show 3 topics with high probability for that document\r\n3. Add 1 \"intruder\" topic with low probability\r\n4. Ask humans to identify the intruder\r\n\r\nTests whether topic assignments match human intuition.\r\n\r\n#### 3. Topic Labeling Agreement\r\n\r\n**Procedure:**\r\n1. Show multiple coders the top words\r\n2. Ask each to propose a label\r\n3. Assess agreement\r\n\r\nHigh agreement  Topics are interpretable\r\nDisagreement  Topics may be incoherent\r\n\r\n#### 4. Document Reading\r\n\r\n**Procedure:**\r\n1. For each topic, sample 5-10 high-probability documents\r\n2. Read documents\r\n3. Assess: Does the topic label fit?\r\n4. Note false positives (label doesn't fit)\r\n\r\n**Documentation:**\r\n```markdown\r\nTopic 3: \"Economic Policy\"\r\n- Doc 142 (p=0.85): Yes, discusses tax reform\r\n- Doc 287 (p=0.72): Partially, mixes health and economics\r\n- Doc 391 (p=0.68): Yes, budget allocation\r\n- Doc 512 (p=0.61): No, primarily foreign policy\r\nFit rate: 3/4 = 75%\r\n```\r\n\r\n### Computational Validation\r\n\r\n#### Coherence Metrics\r\n\r\n| Metric | Description | Interpretation |\r\n|--------|-------------|----------------|\r\n| **C_V** | Sliding window + word vectors | Higher is better; > 0.5 often good |\r\n| **UMass** | Pairwise document co-occurrence | Less negative is better |\r\n| **NPMI** | Normalized PMI | Higher is better |\r\n\r\n**Important:** Coherence is necessary but not sufficient. High coherence doesn't guarantee meaningful topics.\r\n\r\n#### Exclusivity (STM)\r\n\r\nMeasures whether top words are unique to each topic.\r\n\r\nHigh exclusivity  Topics capture distinct concepts\r\nLow exclusivity  Topics share vocabulary (may still be valid)\r\n\r\n#### Held-Out Likelihood\r\n\r\n**Procedure:**\r\n1. Hold out subset of documents\r\n2. Train model on training set\r\n3. Evaluate likelihood on held-out set\r\n\r\nLower perplexity  Better generalization\r\n\r\n### Robustness Checks\r\n\r\n#### Sensitivity to K\r\n\r\n**Procedure:**\r\n1. Run models at K-5, K, K+5, K+10\r\n2. Track: Do similar topics emerge across K?\r\n3. Do topics split or merge sensibly?\r\n\r\n**Interpretation:**\r\n- Core topics should persist\r\n- Small K: Topics merge logically\r\n- Large K: Topics split into subtopics\r\n\r\n#### Sensitivity to Preprocessing\r\n\r\n**Procedure:**\r\n1. Vary preprocessing (stemming, stopwords, thresholds)\r\n2. Rerun models\r\n3. Compare topic composition\r\n\r\n#### Sensitivity to Random Seed\r\n\r\n**Procedure:**\r\n1. Run model with multiple seeds\r\n2. Align topics across runs\r\n3. Assess stability\r\n\r\n**Methods for alignment:**\r\n- Hungarian algorithm on topic-word similarity\r\n- Manual inspection of top words\r\n\r\n## Validation for Classification\r\n\r\n### Training Data Validation\r\n\r\n#### Label Quality\r\n\r\n**Check:**\r\n- Clear category definitions\r\n- Consistent application\r\n- Edge case documentation\r\n\r\n**Inter-rater reliability:**\r\n- Cohen's Kappa (2 raters)\r\n- Fleiss' Kappa (3+ raters)\r\n- Target: Kappa > 0.7\r\n\r\n**Procedure:**\r\n1. Have 2+ coders label same sample\r\n2. Calculate agreement\r\n3. Resolve disagreements, refine codebook\r\n\r\n### Model Validation\r\n\r\n#### Hold-Out Test Set\r\n\r\n**Critical:** Never use test set for model selection.\r\n\r\n```\r\nData Split:\r\n- Training (60%): Fit models\r\n- Validation (20%): Tune hyperparameters\r\n- Test (20%): Final evaluation\r\n```\r\n\r\n#### Cross-Validation\r\n\r\n**Procedure:**\r\n1. Split data into K folds\r\n2. Train on K-1, evaluate on 1\r\n3. Repeat K times\r\n4. Report mean  SD\r\n\r\n**Best practice:** Stratified K-fold to maintain class proportions.\r\n\r\n### Error Analysis\r\n\r\n**Systematic error analysis:**\r\n\r\n1. **Sample misclassified documents**\r\n2. **Categorize errors:**\r\n   - Label noise (gold label wrong)\r\n   - Ambiguous (genuinely unclear)\r\n   - Model limitation (learnable but missed)\r\n3. **Look for patterns:**\r\n   - Certain terms misleading?\r\n   - Certain document types harder?\r\n\r\n**Document errors:**\r\n```markdown\r\n## Error Analysis\r\n\r\n### Systematic Errors\r\n- 15% of finance articles misclassified as business\r\n  - Overlapping vocabulary\r\n  - Consider merging or refining boundary\r\n\r\n### Random Errors\r\n- Unusual cases without pattern\r\n- Acceptable if infrequent\r\n```\r\n\r\n### Robustness Checks\r\n\r\n- Different model architectures\r\n- Different feature representations\r\n- Different train/test splits\r\n- Subset analysis (by source, time)\r\n\r\n## Validation for Dictionary Methods\r\n\r\n### Coverage Assessment\r\n\r\n**Question:** What proportion of documents have matches?\r\n\r\n**Red flags:**\r\n- < 30% coverage  Measure may be too sparse\r\n- Very high frequency terms  May be noise\r\n\r\n**Report:**\r\n```markdown\r\nDictionary matched terms in 78% of documents.\r\nMean matches per document: 4.2 (SD = 2.8)\r\nRange: 0-23 matches\r\n```\r\n\r\n### KWIC Validation\r\n\r\n**Keyword-in-context (KWIC) review:**\r\n\r\n1. For each key dictionary term, extract sample uses\r\n2. Assess: Is this the intended meaning?\r\n3. Calculate validity rate\r\n\r\n**Example:**\r\n```markdown\r\nTerm: \"positive\"\r\n\r\nMatches reviewed: 50\r\n- Valid uses (attitude/sentiment): 32 (64%)\r\n- Invalid uses (medical, math): 18 (36%)\r\n\r\nAction: Consider removing or using negation list\r\n```\r\n\r\n### Convergent Validity\r\n\r\n**Procedure:**\r\n1. Calculate dictionary score\r\n2. Calculate alternative measure (ML sentiment, human coding)\r\n3. Correlate\r\n\r\n**Interpretation:**\r\n- High correlation  Dictionary captures concept\r\n- Low correlation  May be measuring something different\r\n\r\n### Known Groups Validation\r\n\r\n**Procedure:**\r\n1. Identify groups expected to differ\r\n2. Apply dictionary\r\n3. Assess: Do they differ as expected?\r\n\r\n**Example:** Sentiment should be:\r\n- Higher in positive product reviews\r\n- Lower in negative product reviews\r\n- Intermediate in neutral reviews\r\n\r\n## General Robustness Framework\r\n\r\n### What to Vary\r\n\r\n| Element | Variations |\r\n|---------|------------|\r\n| **Preprocessing** | Stemming, stopwords, thresholds |\r\n| **Model parameters** | K, hyperparameters, architecture |\r\n| **Random seed** | Multiple seeds |\r\n| **Data subset** | By time, source, type |\r\n| **Method** | Alternative approach entirely |\r\n\r\n### Interpreting Robustness\r\n\r\n**Robust findings:**\r\n- Persist across variations\r\n- Direction consistent, magnitude similar\r\n- Core patterns stable\r\n\r\n**Non-robust findings:**\r\n- Change substantially with small changes\r\n- Reverse direction\r\n- Appear only with specific choices\r\n\r\n**Reporting:**\r\n```markdown\r\nMain findings were robust to alternative specifications.\r\nResults were consistent across K = 15, 20, 25\r\n(see Appendix Table A1). Findings persisted when\r\nusing alternative preprocessing (with/without stemming).\r\n```\r\n\r\n## Reporting Validation\r\n\r\n### Methods Section\r\n\r\n```markdown\r\n## Validation\r\n\r\n### [Topic Model / Classifier / Dictionary] Validation\r\n\r\nWe validated results using [methods].\r\n\r\n**Human validation:** N coders evaluated [what].\r\nAgreement was [metric] = [value].\r\n\r\n**Computational diagnostics:** [Metrics and values].\r\n\r\n**Robustness:** We tested sensitivity to [variations].\r\n[Key findings about robustness].\r\n```\r\n\r\n### Validation Table\r\n\r\n| Validation Type | Method | Result | Assessment |\r\n|-----------------|--------|--------|------------|\r\n| Human - coherence | Word intrusion | 82% accuracy | Good |\r\n| Human - labeling | Document reading | 78% fit | Acceptable |\r\n| Computational | C_V coherence | 0.52 | Adequate |\r\n| Robustness - K | K  5 | Core topics persist | Robust |\r\n| Robustness - seed | 5 seeds | 90% alignment | Stable |\r\n\r\n### Limitations Section\r\n\r\n**Always acknowledge:**\r\n- What validation was NOT done\r\n- Limitations of validation performed\r\n- Caveats for interpretation\r\n\r\n## Validation Checklist\r\n\r\n### Minimum Validation (Required)\r\n\r\n- [ ] Coverage/match statistics reported\r\n- [ ] Sample of outputs manually reviewed\r\n- [ ] Basic robustness check (one variation)\r\n- [ ] Limitations acknowledged\r\n\r\n### Strong Validation\r\n\r\n- [ ] Systematic human validation (multiple coders)\r\n- [ ] Inter-rater reliability calculated\r\n- [ ] Multiple robustness checks\r\n- [ ] Coherence/accuracy metrics reported\r\n- [ ] Error analysis conducted\r\n\r\n### Exemplary Validation\r\n\r\n- [ ] Convergent validity with alternative measure\r\n- [ ] Known groups or predictive validity\r\n- [ ] Extensive robustness analysis\r\n- [ ] Validation fully documented and reproducible\r\n- [ ] Pre-registration of validation plan\r\n\r\n## Common Validation Failures\r\n\r\n### 1. No Human Validation\r\n\r\n**Problem:** Only computational metrics reported.\r\n\r\n**Why it matters:** Coherence  meaningfulness. Topics can be statistically coherent but substantively incoherent.\r\n\r\n### 2. Validation on Training Data\r\n\r\n**Problem:** Classifier evaluated on data used for training.\r\n\r\n**Why it matters:** Inflated performance; doesn't generalize.\r\n\r\n### 3. Single Seed\r\n\r\n**Problem:** One random seed, no stability check.\r\n\r\n**Why it matters:** Results may be artifacts of randomness.\r\n\r\n### 4. No Robustness to K\r\n\r\n**Problem:** Single K value, no sensitivity analysis.\r\n\r\n**Why it matters:** Different K can produce different interpretations.\r\n\r\n### 5. Ignoring Coverage\r\n\r\n**Problem:** Dictionary applied without checking match rates.\r\n\r\n**Why it matters:** Low coverage means sparse, potentially biased measurement.\r\n",
        "plugins/text-analyst/skills/text-analyst/phases/phase0-design.md": "# Phase 0: Research Design & Method Selection\r\n\r\nYou are executing Phase 0 of a computational text analysis. Your goal is to establish the research question, select appropriate methods, and choose the best language (R or Python) for the analysis.\r\n\r\n## Why This Phase Matters\r\n\r\nText analysis methods answer different questions. Topic models reveal themes; classifiers assign categories; sentiment measures affect. Choosing the wrong method produces meaningless results. This phase ensures alignment between question and method before any analysis.\r\n\r\n## Technique Guides\r\n\r\n**Consult these conceptual guides** in `text-concepts/` for method selection:\r\n\r\n| Guide | Use For |\r\n|-------|---------|\r\n| `01_dictionary_methods.md` | Measuring known concepts with lexicons |\r\n| `02_topic_models.md` | Discovering themes or topics |\r\n| `03_supervised_classification.md` | Categorizing documents with training data |\r\n| `04_embeddings.md` | Semantic similarity, document vectors |\r\n| `05_sentiment_analysis.md` | Measuring sentiment or affect |\r\n| `06_validation_strategies.md` | Planning validation approach |\r\n\r\n## Your Tasks\r\n\r\n### 1. Clarify the Research Question\r\n\r\nAsk the user to articulate:\r\n- **What do you want to learn from the text?**\r\n  - Discover themes?  Topic modeling\r\n  - Measure a known concept?  Dictionary/classification\r\n  - Track sentiment?  Sentiment analysis\r\n  - Find similar documents?  Embeddings\r\n  - Extract entities?  NER\r\n\r\n- **Is this exploratory or confirmatory?**\r\n  - Exploratory: Topic models, unsupervised clustering\r\n  - Confirmatory: Dictionary methods, supervised classification\r\n\r\n- **What is the unit of analysis?**\r\n  - Document-level (articles, posts, interviews)\r\n  - Sentence-level (for fine-grained analysis)\r\n  - Token-level (for NER, POS tagging)\r\n\r\n### 2. Assess the Corpus\r\n\r\nGather corpus characteristics:\r\n\r\n| Characteristic | Questions |\r\n|---------------|-----------|\r\n| **Size** | How many documents? Tokens? |\r\n| **Type** | News, social media, interviews, academic? |\r\n| **Language** | English only? Multiple languages? |\r\n| **Structure** | Short (tweets) or long (articles)? |\r\n| **Metadata** | Date, author, source? Covariates? |\r\n| **Quality** | OCR errors? Missing data? Duplicates? |\r\n\r\n**Size guidance:**\r\n- < 500 documents: Dictionary methods, qualitative reading\r\n- 500-10,000: LDA, STM work well\r\n- 10,000+: All methods viable; consider sampling for validation\r\n- 100,000+: Neural methods become more attractive\r\n\r\n### 3. Select Methods\r\n\r\nBased on question and corpus, recommend methods:\r\n\r\n| Research Goal | Primary Method | Alternatives |\r\n|--------------|----------------|--------------|\r\n| Discover themes | LDA, STM | BERTopic, clustering |\r\n| Measure known concepts | Dictionary | Supervised classifier |\r\n| Track sentiment | Lexicon (LIWC, VADER) | ML sentiment classifier |\r\n| Classify documents | Supervised (SVM, BERT) | Zero-shot classification |\r\n| Find similar texts | Embeddings (SBERT) | TF-IDF + cosine |\r\n| Extract entities | spaCy NER | Custom NER training |\r\n| Topic change over time | STM with time covariate | Dynamic topic models |\r\n\r\n### 4. Choose Language (R or Python)\r\n\r\n**Use R when:**\r\n- Topic modeling with covariates (STM is gold standard)\r\n- Dictionary/sentiment with tidytext workflow\r\n- Publication-quality visualizations (ggplot2)\r\n- Integration with quantitative analysis\r\n\r\n**Use Python when:**\r\n- Transformer/BERT methods required\r\n- BERTopic for neural topic modeling\r\n- Named entity recognition (spaCy)\r\n- Deep learning classification\r\n- Large-scale processing with GPU\r\n\r\n**Decision guide:**\r\n\r\n```\r\nIs the primary method topic modeling with covariates?\r\n   R (stm package)\r\n\r\nIs the primary method neural/transformer-based?\r\n   Python (HuggingFace, BERTopic)\r\n\r\nIs the primary method dictionary/sentiment?\r\n   R (tidytext, more lexicons)\r\n\r\nIs NER required?\r\n   Python (spaCy)\r\n\r\nDo you need publication-ready figures?\r\n   R (ggplot2)\r\n\r\nIs the corpus very large (>100K)?\r\n   Python (better memory management)\r\n\r\nNo strong preference?\r\n   R for classical methods, Python for neural\r\n```\r\n\r\n### 5. Plan Validation Approach\r\n\r\nAll text analysis requires validation. Plan:\r\n\r\n**Human validation:**\r\n- Sample documents for manual review\r\n- Expert labeling of topics/categories\r\n- Inter-coder reliability for dictionaries\r\n\r\n**Computational diagnostics:**\r\n- Topic coherence metrics\r\n- Classification accuracy (precision, recall, F1)\r\n- Holdout validation\r\n\r\n**Robustness:**\r\n- Sensitivity to K (number of topics)\r\n- Sensitivity to preprocessing\r\n- Multiple random seeds\r\n\r\n### 6. Document Data Requirements\r\n\r\nSpecify what the analysis needs:\r\n- Text column(s)\r\n- Document identifiers\r\n- Metadata fields (date, source, author)\r\n- Covariates for STM (if applicable)\r\n- Labels for supervised learning (if applicable)\r\n- Sample size for human validation\r\n\r\n## Output: Design Memo\r\n\r\nCreate a design memo (`memos/phase0-design-memo.md`):\r\n\r\n```markdown\r\n# Text Analysis Design Memo\r\n\r\n## Research Question\r\n[Clear statement of what you want to learn from the text]\r\n\r\n## Corpus Description\r\n- **Documents**: [N documents, type]\r\n- **Tokens**: [approximate total]\r\n- **Language**: [language(s)]\r\n- **Time span**: [if temporal]\r\n- **Source**: [where texts come from]\r\n\r\n## Selected Methods\r\n- **Primary method**: [method name]\r\n- **Rationale**: [why this method fits the question]\r\n- **Alternatives considered**: [what else could work]\r\n\r\n## Language Choice\r\n- **Selected**: [R / Python]\r\n- **Rationale**: [why this language]\r\n- **Key packages**: [main packages to use]\r\n\r\n## Validation Plan\r\n- **Human validation**: [sample size, procedure]\r\n- **Computational metrics**: [which metrics]\r\n- **Robustness checks**: [sensitivity analyses]\r\n\r\n## Preprocessing Plan (preliminary)\r\n- Tokenization: [word, sentence, n-gram]\r\n- Stopwords: [standard list, custom additions]\r\n- Stemming/lemmatization: [yes/no, which]\r\n- Minimum document frequency: [threshold]\r\n\r\n## Questions for User\r\n- [Any clarifications needed before proceeding]\r\n```\r\n\r\n## When You're Done\r\n\r\nReturn a summary to the orchestrator that includes:\r\n1. The research question (one sentence)\r\n2. Selected method and language with rationale\r\n3. Corpus characteristics summary\r\n4. Planned validation approach\r\n5. Any questions or concerns for the user\r\n\r\n**Do not proceed to Phase 1 until the user confirms the research design.**\r\n",
        "plugins/text-analyst/skills/text-analyst/phases/phase1-corpus.md": "# Phase 1: Corpus Preparation & Exploration\r\n\r\nYou are executing Phase 1 of a computational text analysis. Your goal is to load, clean, explore, and understand the corpus before running any models.\r\n\r\n## Why This Phase Matters\r\n\r\nYou cannot interpret text analysis results without knowing your corpus. This phase reveals data quality issues, informs preprocessing decisions, and establishes baseline understanding. Skipping exploration leads to garbage-in, garbage-out.\r\n\r\n## Technique Guides\r\n\r\n**Consult the appropriate technique guides** based on chosen language:\r\n\r\n**For R** (in `text-r-techniques/`):\r\n- `01_preprocessing.md` - tidytext and quanteda workflows\r\n\r\n**For Python** (in `text-python-techniques/`):\r\n- `01_preprocessing.md` - nltk, spaCy, sklearn pipelines\r\n\r\n## Your Tasks\r\n\r\n### 1. Load and Inspect the Corpus\r\n\r\n**Initial inspection:**\r\n```\r\n- Number of documents\r\n- Document length distribution (words per document)\r\n- Total tokens\r\n- Date range (if temporal)\r\n- Missing values in text or metadata\r\n- Duplicate documents\r\n```\r\n\r\n**Create basic statistics table:**\r\n\r\n| Metric | Value |\r\n|--------|-------|\r\n| Total documents | N |\r\n| Mean doc length (words) | X |\r\n| Median doc length | X |\r\n| Min / Max length | X / X |\r\n| Empty documents | N |\r\n| Duplicate documents | N |\r\n| Date range | YYYY-MM-DD to YYYY-MM-DD |\r\n\r\n### 2. Assess Data Quality\r\n\r\n**Check for:**\r\n- Empty or near-empty documents\r\n- Duplicate texts (exact and near-duplicate)\r\n- OCR errors (if digitized)\r\n- Encoding issues (UTF-8 problems)\r\n- Boilerplate text (headers, footers, signatures)\r\n- Non-text content (URLs, HTML, code)\r\n\r\n**Document any exclusions:**\r\n```markdown\r\n## Data Quality Issues\r\n\r\n### Excluded Documents\r\n- X documents excluded for: [reason]\r\n- Y documents excluded for: [reason]\r\n\r\n### Cleaning Applied\r\n- [Cleaning step 1]\r\n- [Cleaning step 2]\r\n```\r\n\r\n### 3. Make Preprocessing Decisions\r\n\r\nFor each decision, document the choice and rationale:\r\n\r\n| Decision | Options | Your Choice | Rationale |\r\n|----------|---------|-------------|-----------|\r\n| **Case** | Lower / preserve | | |\r\n| **Tokenization** | Word / sentence / n-gram | | |\r\n| **Stopwords** | None / standard / custom | | |\r\n| **Stemming** | None / Porter / Snowball | | |\r\n| **Lemmatization** | None / spaCy / WordNet | | |\r\n| **Numbers** | Keep / remove / normalize | | |\r\n| **Punctuation** | Keep / remove | | |\r\n| **Min doc frequency** | N | | |\r\n| **Max doc frequency** | % | | |\r\n\r\n**Preprocessing guidance:**\r\n- **Topic models**: Usually lowercase, remove stopwords, no stemming (interpretability)\r\n- **Classification**: Often minimal preprocessing; let model learn\r\n- **Dictionary**: Match preprocessing to dictionary expectations\r\n- **Embeddings**: Minimal preprocessing; models trained on raw text\r\n\r\n### 4. Create Document-Term Matrix / Embeddings\r\n\r\n**For bag-of-words approaches:**\r\n- Create document-term matrix (DTM)\r\n- Document vocabulary size before/after pruning\r\n- Show most frequent terms\r\n- Show terms removed by thresholds\r\n\r\n**For embedding approaches:**\r\n- Generate document embeddings\r\n- Verify dimensions and coverage\r\n- Check for OOV (out-of-vocabulary) rate\r\n\r\n### 5. Generate Descriptive Visualizations\r\n\r\nCreate at minimum:\r\n\r\n1. **Document length distribution**\r\n   - Histogram of words per document\r\n   - Identify outliers\r\n\r\n2. **Term frequency distribution**\r\n   - Top 50 most frequent terms\r\n   - Zipf's law plot (optional)\r\n\r\n3. **Temporal patterns** (if dated)\r\n   - Documents over time\r\n   - Word frequency trends\r\n\r\n4. **Metadata distributions**\r\n   - By source, author, category\r\n   - Check balance across groups\r\n\r\n### 6. Explore Corpus Content\r\n\r\n**Sample reading:**\r\n- Read 10-20 random documents\r\n- Note themes, style, quality\r\n- Identify potential issues\r\n\r\n**Keyword-in-context (KWIC):**\r\n- Search for key terms\r\n- Understand usage patterns\r\n- Refine dictionary terms if applicable\r\n\r\n### 7. Check for Known Issues\r\n\r\n**Topic modeling specific:**\r\n- Very short documents (< 50 words) may be problematic\r\n- Very long documents may need segmentation\r\n- Highly technical vocabulary may need custom stopwords\r\n\r\n**Classification specific:**\r\n- Class imbalance in training data\r\n- Label quality and consistency\r\n- Sufficient examples per class\r\n\r\n**Sentiment specific:**\r\n- Domain-specific language (sarcasm, jargon)\r\n- Negation handling\r\n- Intensity modifiers\r\n\r\n## Output: Corpus Report\r\n\r\nCreate a corpus report (`memos/phase1-corpus-report.md`):\r\n\r\n```markdown\r\n# Corpus Exploration Report\r\n\r\n## Corpus Overview\r\n\r\n| Metric | Value |\r\n|--------|-------|\r\n| Total documents | N |\r\n| After cleaning | N |\r\n| Mean length (words) | X |\r\n| Vocabulary size | X |\r\n| Date range | YYYY to YYYY |\r\n\r\n## Data Quality\r\n\r\n### Issues Found\r\n- [Issue 1]: [How addressed]\r\n- [Issue 2]: [How addressed]\r\n\r\n### Exclusions\r\n- N documents excluded for [reason]\r\n\r\n## Preprocessing Decisions\r\n\r\n| Decision | Choice | Rationale |\r\n|----------|--------|-----------|\r\n| Case | lowercase | Standard for topic models |\r\n| Stopwords | SMART + custom | Removed domain terms: [list] |\r\n| ... | | |\r\n\r\n## Vocabulary\r\n\r\n### Most Frequent Terms (post-preprocessing)\r\n| Term | Frequency | Document Frequency |\r\n|------|-----------|-------------------|\r\n| term1 | N | N% |\r\n| ... | | |\r\n\r\n### Custom Stopwords Added\r\n- [term1]: [why removed]\r\n- [term2]: [why removed]\r\n\r\n## Visualizations\r\n\r\n[Include or reference figures]\r\n- Figure 1: Document length distribution\r\n- Figure 2: Top terms frequency\r\n- Figure 3: Documents over time\r\n\r\n## Sample Documents\r\n\r\n### Representative Examples\r\n[2-3 example documents with notes]\r\n\r\n### Unusual Documents\r\n[Documents that may need attention]\r\n\r\n## Preliminary Observations\r\n- [Observation 1]\r\n- [Observation 2]\r\n- [Potential concerns]\r\n\r\n## Ready for Analysis?\r\n- [ ] Data quality acceptable\r\n- [ ] Preprocessing documented\r\n- [ ] Vocabulary reasonable\r\n- [ ] No major concerns\r\n```\r\n\r\n## Code Skeleton\r\n\r\n### R (tidytext)\r\n```r\r\nlibrary(tidyverse)\r\nlibrary(tidytext)\r\nlibrary(quanteda)\r\n\r\n# Load data\r\ncorpus <- read_csv(\"data/raw/corpus.csv\")\r\n\r\n# Basic stats\r\ncorpus %>%\r\n  mutate(n_words = str_count(text, \"\\\\w+\")) %>%\r\n  summarise(\r\n    n_docs = n(),\r\n    mean_words = mean(n_words),\r\n    median_words = median(n_words),\r\n    min_words = min(n_words),\r\n    max_words = max(n_words)\r\n  )\r\n\r\n# Tokenize\r\ntokens <- corpus %>%\r\n  unnest_tokens(word, text) %>%\r\n  anti_join(stop_words)\r\n\r\n# Top terms\r\ntokens %>%\r\n  count(word, sort = TRUE) %>%\r\n  head(50)\r\n```\r\n\r\n### Python\r\n```python\r\nimport pandas as pd\r\nfrom collections import Counter\r\nimport nltk\r\nfrom nltk.corpus import stopwords\r\n\r\n# Load data\r\ncorpus = pd.read_csv(\"data/raw/corpus.csv\")\r\n\r\n# Basic stats\r\ncorpus['n_words'] = corpus['text'].str.split().str.len()\r\nprint(corpus['n_words'].describe())\r\n\r\n# Tokenize and count\r\nstop_words = set(stopwords.words('english'))\r\nall_words = []\r\nfor text in corpus['text']:\r\n    words = [w.lower() for w in text.split() if w.lower() not in stop_words]\r\n    all_words.extend(words)\r\n\r\n# Top terms\r\nword_counts = Counter(all_words)\r\nprint(word_counts.most_common(50))\r\n```\r\n\r\n## When You're Done\r\n\r\nReturn a summary to the orchestrator that includes:\r\n1. Corpus size (documents, tokens, vocabulary)\r\n2. Any data quality issues found and how addressed\r\n3. Key preprocessing decisions and rationale\r\n4. Preliminary observations about corpus content\r\n5. Any concerns for the user to consider\r\n\r\n**Do not proceed to Phase 2 until the user confirms preprocessing decisions.**\r\n",
        "plugins/text-analyst/skills/text-analyst/phases/phase2-specification.md": "# Phase 2: Method Specification\r\n\r\nYou are executing Phase 2 of a computational text analysis. Your goal is to fully specify all model parameters and preprocessing decisions before running any models.\r\n\r\n## Why This Phase Matters\r\n\r\nSpecification decisions shape results. Choosing K=20 topics vs K=50 is a research decision, not a tuning parameter. Documenting choices before seeing results prevents p-hacking and specification searching in text analysis.\r\n\r\n## Technique Guides\r\n\r\n**Consult the appropriate guides** based on method and language:\r\n\r\n**Conceptual** (in `text-concepts/`):\r\n| Method | Guide |\r\n|--------|-------|\r\n| Dictionary | `01_dictionary_methods.md` |\r\n| Topic models | `02_topic_models.md` |\r\n| Classification | `03_supervised_classification.md` |\r\n| Embeddings | `04_embeddings.md` |\r\n| Sentiment | `05_sentiment_analysis.md` |\r\n\r\n**Implementation** (in `text-r-techniques/` or `text-python-techniques/`):\r\n- `02_dictionary_sentiment.md` for dictionary/sentiment code\r\n- `03_topic_models.md` for LDA/STM/BERTopic code\r\n- `04_supervised.md` for classification code\r\n- `05_embeddings.md` for embedding code\r\n\r\n## Your Tasks\r\n\r\n### 1. Document Final Preprocessing Pipeline\r\n\r\nFinalize all preprocessing decisions from Phase 1:\r\n\r\n```markdown\r\n## Preprocessing Pipeline\r\n\r\n1. **Text cleaning**\r\n   - [Cleaning steps in order]\r\n\r\n2. **Tokenization**\r\n   - Method: [word/sentence/n-gram]\r\n   - Parameters: [any options]\r\n\r\n3. **Normalization**\r\n   - Case: [lowercase/preserve]\r\n   - Stemming: [none/Porter/Snowball]\r\n   - Lemmatization: [none/spaCy/WordNet]\r\n\r\n4. **Vocabulary pruning**\r\n   - Min document frequency: [N or %]\r\n   - Max document frequency: [N or %]\r\n   - Min term length: [N]\r\n\r\n5. **Stopwords**\r\n   - Base list: [none/SMART/English/custom]\r\n   - Added terms: [list]\r\n   - Removed terms: [list if domain-specific kept]\r\n\r\n6. **Final vocabulary size**: [N terms]\r\n```\r\n\r\n### 2. Specify Model Parameters\r\n\r\n#### For Topic Models (LDA, STM)\r\n\r\n| Parameter | Value | Rationale |\r\n|-----------|-------|-----------|\r\n| Number of topics (K) | | See guidance below |\r\n| Alpha prior | | Typically use default |\r\n| Beta/eta prior | | Typically use default |\r\n| Iterations | | Until convergence |\r\n| Random seed | | For reproducibility |\r\n| Covariates (STM) | | Which metadata affects topics |\r\n\r\n**Choosing K:**\r\n- K is NOT a tuning parameter to optimize\r\n- K is a research decision about granularity\r\n- Multiple valid K values often exist\r\n- Err toward interpretability over metrics\r\n\r\n**K guidance:**\r\n- Start with theory: How many themes are plausible?\r\n- Small corpus (< 1000): K = 5-15\r\n- Medium corpus (1000-10000): K = 10-30\r\n- Large corpus (> 10000): K = 20-50+\r\n- Plan to run multiple K values for robustness\r\n\r\n#### For BERTopic\r\n\r\n| Parameter | Value | Rationale |\r\n|-----------|-------|-----------|\r\n| Embedding model | | sentence-transformers model |\r\n| UMAP n_neighbors | | Typically 15 |\r\n| UMAP n_components | | Typically 5 |\r\n| HDBSCAN min_cluster_size | | Affects number of topics |\r\n| HDBSCAN min_samples | | Affects outlier handling |\r\n| Top n words | | For topic representation |\r\n\r\n#### For Supervised Classification\r\n\r\n| Parameter | Value | Rationale |\r\n|-----------|-------|-----------|\r\n| Model type | | SVM, LogReg, BERT, etc. |\r\n| Features | | TF-IDF, embeddings, etc. |\r\n| Train/test split | | Typically 80/20 |\r\n| Validation approach | | k-fold, stratified |\r\n| Class weights | | If imbalanced |\r\n| Hyperparameters | | Grid search range |\r\n| Random seed | | For reproducibility |\r\n\r\n#### For Dictionary/Sentiment\r\n\r\n| Parameter | Value | Rationale |\r\n|-----------|-------|-----------|\r\n| Dictionary name | | LIWC, VADER, custom |\r\n| Aggregation | | Count, proportion, weighted |\r\n| Negation handling | | How to handle \"not good\" |\r\n| Missing words | | How to handle OOV |\r\n| Normalization | | By document length? |\r\n\r\n### 3. Pre-specify Validation Approach\r\n\r\n**Before running models**, document how you'll validate:\r\n\r\n**Human validation:**\r\n- [ ] Sample size: N documents to manually review\r\n- [ ] Sampling strategy: [random, stratified, purposive]\r\n- [ ] Who codes: [researcher, RAs, domain experts]\r\n- [ ] Inter-rater reliability: [measure to use]\r\n\r\n**Computational diagnostics:**\r\n\r\nFor topic models:\r\n- [ ] Coherence metric: [UMass, C_V, NPMI]\r\n- [ ] Exclusivity (for STM)\r\n- [ ] Held-out likelihood\r\n- [ ] Semantic intrusion test (optional)\r\n\r\nFor classifiers:\r\n- [ ] Primary metric: [accuracy, F1, macro-F1]\r\n- [ ] Confusion matrix\r\n- [ ] Per-class metrics\r\n- [ ] Cross-validation folds: K\r\n\r\nFor dictionaries:\r\n- [ ] Coverage: % of documents with matches\r\n- [ ] Face validity: sample KWIC examples\r\n- [ ] Convergent validity: correlation with other measures\r\n\r\n**Robustness checks:**\r\n- [ ] Alternative preprocessing (e.g., with/without stemming)\r\n- [ ] Different K values (for topic models)\r\n- [ ] Different random seeds\r\n- [ ] Subset analysis (by time, source)\r\n\r\n### 4. Plan Output Specifications\r\n\r\nDocument what outputs to produce:\r\n\r\n**Tables:**\r\n- Top words per topic (N words)\r\n- Topic prevalence\r\n- Classification metrics\r\n- Dictionary coverage\r\n\r\n**Figures:**\r\n- Topic proportions over time\r\n- Topic correlation network\r\n- Confusion matrix\r\n- Word clouds (if appropriate)\r\n\r\n**Replication:**\r\n- Seed value(s)\r\n- Package versions\r\n- Full preprocessing code\r\n- Model object saved\r\n\r\n### 5. Create Specification Memo\r\n\r\nCreate `memos/phase2-specification-memo.md`:\r\n\r\n```markdown\r\n# Method Specification Memo\r\n\r\n## Preprocessing Pipeline\r\n\r\n[Full pipeline documented above]\r\n\r\n## Model Specification\r\n\r\n### Primary Model: [Model Name]\r\n\r\n| Parameter | Value | Rationale |\r\n|-----------|-------|-----------|\r\n| [param1] | [value] | [why] |\r\n| [param2] | [value] | [why] |\r\n| ... | | |\r\n\r\n### Random Seed\r\nSeed: [value]\r\n\r\n### Package Versions\r\n- R: [version]\r\n- [package1]: [version]\r\n- [package2]: [version]\r\n\r\n## Validation Plan\r\n\r\n### Human Validation\r\n- Sample: N documents, [sampling strategy]\r\n- Coders: [who]\r\n- Reliability: [metric]\r\n\r\n### Computational Diagnostics\r\n- [Metric 1]: [threshold for concern]\r\n- [Metric 2]: [threshold for concern]\r\n\r\n### Robustness Checks\r\n1. [Check 1]: [what varies]\r\n2. [Check 2]: [what varies]\r\n3. [Check 3]: [what varies]\r\n\r\n## Planned Outputs\r\n\r\n### Tables\r\n1. [Table 1 description]\r\n2. [Table 2 description]\r\n\r\n### Figures\r\n1. [Figure 1 description]\r\n2. [Figure 2 description]\r\n\r\n## Code Template\r\n\r\n```[r or python]\r\n# Package versions\r\n# [package]: [version]\r\n\r\n# Set seed\r\nset.seed([seed])  # or random.seed([seed])\r\n\r\n# Load preprocessed data\r\n# ...\r\n\r\n# Fit model\r\n# [model code template]\r\n\r\n# Diagnostics\r\n# [diagnostic code template]\r\n```\r\n\r\n## Questions for User\r\n- [Any remaining decisions]\r\n```\r\n\r\n## Common Specification Decisions\r\n\r\n### Topic Model K Selection Strategy\r\n\r\n**DO NOT** just run multiple K and pick \"best\" coherence.\r\n\r\n**DO** use multiple criteria:\r\n1. Theoretical plausibility\r\n2. Interpretability (can you label topics?)\r\n3. Coherence as one input (not the only one)\r\n4. Exclusivity for STM\r\n5. Robustness (do topics persist across K?)\r\n\r\n### Train/Test Split for Classification\r\n\r\n- Hold out test set before ANY model selection\r\n- Use separate validation set for hyperparameter tuning\r\n- Stratify by class label\r\n- Consider temporal split if data is time-ordered\r\n\r\n### Dictionary Validation Requirements\r\n\r\nBefore trusting dictionary results:\r\n1. Check coverage (what % of docs have any matches?)\r\n2. Review KWIC examples (are matches valid?)\r\n3. Check for domain-specific meanings\r\n4. Consider false positives and negatives\r\n\r\n## When You're Done\r\n\r\nReturn a summary to the orchestrator that includes:\r\n1. Final preprocessing pipeline\r\n2. All model parameters and their rationale\r\n3. Validation plan with specific metrics\r\n4. Planned robustness checks\r\n5. Any questions requiring user input\r\n\r\n**Do not proceed to Phase 3 until the user approves the specification.**\r\n",
        "plugins/text-analyst/skills/text-analyst/phases/phase3-analysis.md": "# Phase 3: Main Analysis\r\n\r\nYou are executing Phase 3 of a computational text analysis. Your goal is to run the specified models and produce initial results for review.\r\n\r\n## Why This Phase Matters\r\n\r\nThis phase executes the pre-specified analysis. The key discipline is: run what was specified, not what looks best after seeing results. Document any deviations.\r\n\r\n## Technique Guides\r\n\r\n**Consult implementation guides** for your language:\r\n\r\n**R** (in `text-r-techniques/`):\r\n| Method | Guide |\r\n|--------|-------|\r\n| Dictionary/sentiment | `02_dictionary_sentiment.md` |\r\n| Topic models (LDA, STM) | `03_topic_models.md` |\r\n| Supervised classification | `04_supervised.md` |\r\n| Embeddings | `05_embeddings.md` |\r\n| Visualization | `06_visualization.md` |\r\n\r\n**Python** (in `text-python-techniques/`):\r\n| Method | Guide |\r\n|--------|-------|\r\n| Dictionary/sentiment | `02_dictionary_sentiment.md` |\r\n| Topic models (gensim, BERTopic) | `03_topic_models.md` |\r\n| Supervised classification | `04_supervised.md` |\r\n| Embeddings | `05_embeddings.md` |\r\n| Visualization | `06_visualization.md` |\r\n\r\n## Your Tasks\r\n\r\n### 1. Run Primary Models\r\n\r\nExecute the pre-specified model with documented parameters:\r\n\r\n**Topic Models:**\r\n```r\r\n# R example with STM\r\nlibrary(stm)\r\nset.seed(SPECIFIED_SEED)\r\n\r\nstm_model <- stm(\r\n  documents = out$documents,\r\n  vocab = out$vocab,\r\n  K = SPECIFIED_K,\r\n  prevalence = ~ covariate1 + covariate2,\r\n  data = out$meta,\r\n  init.type = \"Spectral\"\r\n)\r\n```\r\n\r\n```python\r\n# Python example with BERTopic\r\nfrom bertopic import BERTopic\r\nimport random\r\nrandom.seed(SPECIFIED_SEED)\r\n\r\ntopic_model = BERTopic(\r\n    embedding_model=\"all-MiniLM-L6-v2\",\r\n    min_topic_size=SPECIFIED_MIN_SIZE,\r\n    nr_topics=SPECIFIED_K  # or \"auto\"\r\n)\r\ntopics, probs = topic_model.fit_transform(documents)\r\n```\r\n\r\n**Classification:**\r\n```python\r\n# Python example\r\nfrom sklearn.model_selection import cross_val_score\r\nfrom sklearn.svm import SVC\r\n\r\nmodel = SVC(C=SPECIFIED_C, kernel=SPECIFIED_KERNEL)\r\nscores = cross_val_score(model, X_train, y_train, cv=SPECIFIED_CV)\r\n```\r\n\r\n**Dictionary:**\r\n```r\r\n# R example with tidytext\r\nlibrary(tidytext)\r\n\r\nsentiment_scores <- tokens %>%\r\n  inner_join(get_sentiments(\"SPECIFIED_LEXICON\")) %>%\r\n  group_by(doc_id) %>%\r\n  summarise(sentiment = sum(value))\r\n```\r\n\r\n### 2. Assess Convergence and Fit\r\n\r\n**Topic models:**\r\n- Did the model converge?\r\n- Check convergence diagnostics\r\n- Compare log-likelihood across iterations\r\n- Check for degenerate topics (empty or dominant)\r\n\r\n**Classification:**\r\n- Training accuracy (should be high)\r\n- Gap between training and validation (overfitting?)\r\n- Learning curves if applicable\r\n\r\n**Dictionary:**\r\n- Coverage: What proportion of documents have matches?\r\n- Proportion of terms matched vs. total\r\n\r\n### 3. Extract and Label Results\r\n\r\n**For topic models - create topic labels:**\r\n\r\n| Topic | Top Words | Proposed Label | Confidence |\r\n|-------|-----------|----------------|------------|\r\n| 1 | word1, word2, word3... | [Label] | High/Medium/Low |\r\n| 2 | word1, word2, word3... | [Label] | High/Medium/Low |\r\n| ... | | | |\r\n\r\n**Labeling guidance:**\r\n- Base labels on top 10-20 words\r\n- Read representative documents (highest topic probability)\r\n- Use FREX words for STM (frequent AND exclusive)\r\n- Mark unclear topics explicitly\r\n\r\n**For classification - create performance summary:**\r\n\r\n| Class | Precision | Recall | F1 | Support |\r\n|-------|-----------|--------|----|------------|\r\n| Class1 | | | | N |\r\n| Class2 | | | | N |\r\n| Macro avg | | | | |\r\n| Weighted avg | | | | |\r\n\r\n**For dictionary - create coverage summary:**\r\n\r\n| Document Group | N Docs | % With Match | Mean Score | SD |\r\n|---------------|--------|--------------|------------|-----|\r\n| All | N | % | X | X |\r\n| [Subgroup1] | N | % | X | X |\r\n| [Subgroup2] | N | % | X | X |\r\n\r\n### 4. Create Initial Visualizations\r\n\r\n**Topic models:**\r\n- Topic proportions (bar chart)\r\n- Topic correlations (network or heatmap)\r\n- Topic prevalence over time (if temporal)\r\n- Representative documents per topic\r\n\r\n**Classification:**\r\n- Confusion matrix\r\n- ROC curves (if applicable)\r\n- Feature importance (top predictive terms)\r\n\r\n**Dictionary:**\r\n- Score distributions (histogram)\r\n- Scores over time (if temporal)\r\n- Scores by group (if comparing)\r\n\r\n### 5. Document Deviations\r\n\r\nIf ANY changes were made from the specification:\r\n\r\n```markdown\r\n## Deviations from Specification\r\n\r\n### Deviation 1\r\n- **Specified**: [what was planned]\r\n- **Actual**: [what was done]\r\n- **Reason**: [why changed]\r\n- **Impact**: [how this affects interpretation]\r\n\r\n### Deviation 2\r\n...\r\n```\r\n\r\nChanges requiring documentation:\r\n- Different K than specified\r\n- Modified preprocessing\r\n- Changed model parameters\r\n- Different random seed\r\n- Excluded documents\r\n\r\n### 6. Initial Interpretation\r\n\r\nProvide preliminary interpretation with appropriate caveats:\r\n\r\n**For topic models:**\r\n- Are topics coherent and interpretable?\r\n- Do topic prevalences match expectations?\r\n- Any surprising patterns?\r\n- Which topics need more investigation?\r\n\r\n**For classification:**\r\n- Is performance adequate for the research question?\r\n- Which classes are confused?\r\n- Are errors systematic?\r\n\r\n**For dictionary:**\r\n- Does the distribution make sense?\r\n- Are there ceiling/floor effects?\r\n- Do group differences align with expectations?\r\n\r\n## Output: Results Summary\r\n\r\nCreate `memos/phase3-results-summary.md`:\r\n\r\n```markdown\r\n# Analysis Results Summary\r\n\r\n## Model Fit\r\n\r\n### Convergence\r\n- [Converged: Yes/No]\r\n- [Iterations: N]\r\n- [Final likelihood/loss: X]\r\n\r\n### Diagnostics\r\n- [Metric 1]: [value]\r\n- [Metric 2]: [value]\r\n\r\n## Primary Results\r\n\r\n### [Topic Labels / Classification Performance / Dictionary Scores]\r\n\r\n[Results table from above]\r\n\r\n### Key Findings\r\n1. [Finding 1]\r\n2. [Finding 2]\r\n3. [Finding 3]\r\n\r\n## Visualizations\r\n\r\n[Reference to saved figures]\r\n- Figure 1: [description]\r\n- Figure 2: [description]\r\n\r\n## Deviations from Specification\r\n\r\n[None / List of changes]\r\n\r\n## Preliminary Interpretation\r\n\r\n[2-3 paragraphs of initial interpretation with caveats]\r\n\r\n## Concerns / Questions\r\n\r\n- [Concern 1]\r\n- [Concern 2]\r\n\r\n## Next Steps for Validation\r\n\r\nBased on these results, validation should focus on:\r\n1. [Validation priority 1]\r\n2. [Validation priority 2]\r\n```\r\n\r\n## Quality Checks Before Proceeding\r\n\r\nBefore declaring Phase 3 complete:\r\n\r\n- [ ] Model converged appropriately\r\n- [ ] Results saved with version info\r\n- [ ] Random seed documented and used\r\n- [ ] All deviations documented\r\n- [ ] Initial visualizations created\r\n- [ ] Topic labels proposed (if applicable)\r\n- [ ] No obvious errors or artifacts\r\n\r\n## When You're Done\r\n\r\nReturn a summary to the orchestrator that includes:\r\n1. Model fit assessment (did it work?)\r\n2. Key results summary (topics, performance, distributions)\r\n3. Any deviations from specification\r\n4. Preliminary interpretation\r\n5. Concerns requiring attention in validation\r\n\r\n**Do not proceed to Phase 4 until the user reviews these results.**\r\n",
        "plugins/text-analyst/skills/text-analyst/phases/phase4-validation.md": "# Phase 4: Validation & Robustness\r\n\r\nYou are executing Phase 4 of a computational text analysis. Your goal is to validate findings through human assessment and computational diagnostics, and test robustness to analytical choices.\r\n\r\n## Why This Phase Matters\r\n\r\nAlgorithmic output is not ground truth. Topic models find patternsbut are they meaningful patterns? Classifiers achieve accuracybut do they capture what you intend? This phase establishes that results are valid and robust, not artifacts of method choices.\r\n\r\n## Technique Guides\r\n\r\n**Consult validation guide** in `text-concepts/`:\r\n- `06_validation_strategies.md` - comprehensive validation approaches\r\n\r\n**Implementation guides** for diagnostics:\r\n- R: `text-r-techniques/03_topic_models.md` (coherence, exclusivity)\r\n- Python: `text-python-techniques/03_topic_models.md` (coherence, c_v)\r\n\r\n## Your Tasks\r\n\r\n### 1. Human Validation\r\n\r\n**For topic models - Topic Intrusion Test:**\r\n\r\nSelect N topics. For each topic:\r\n1. Show top 10-15 words\r\n2. Add one \"intruder\" word from another topic\r\n3. Ask human coders to identify the intruder\r\n4. High accuracy = coherent topics\r\n\r\n**For topic models - Document Reading:**\r\n\r\nFor each topic:\r\n1. Sample 5-10 highest-probability documents\r\n2. Read documents\r\n3. Assess: Does topic label fit these documents?\r\n4. Note: Are there false positives? Missing themes?\r\n\r\n```markdown\r\n## Topic Validation: Topic 3 \"Economic Policy\"\r\n\r\n### Top Words\r\ntax, economy, budget, spending, fiscal, growth...\r\n\r\n### Sample Documents Reviewed\r\n| Doc ID | Topic Prob | Label Fits? | Notes |\r\n|--------|------------|-------------|-------|\r\n| 1234 | 0.85 | Yes | Clearly about tax policy |\r\n| 2345 | 0.72 | Partially | Mixed with healthcare |\r\n| 3456 | 0.68 | Yes | Budget discussion |\r\n| ... | | | |\r\n\r\n### Assessment\r\n- Label accuracy: X/N documents\r\n- Refinements needed: [suggestions]\r\n```\r\n\r\n**For classification - Error Analysis:**\r\n\r\n1. Sample misclassified documents\r\n2. For each error:\r\n   - Why did the model fail?\r\n   - Is the gold label correct?\r\n   - Is this a systematic error?\r\n\r\n```markdown\r\n## Error Analysis\r\n\r\n### False Positives (predicted [Class], actual [Other])\r\n| Doc ID | Predicted | Actual | Why Misclassified |\r\n|--------|-----------|--------|-------------------|\r\n| 1234 | Class A | Class B | Shared vocabulary |\r\n| ... | | | |\r\n\r\n### False Negatives (predicted [Other], actual [Class])\r\n| Doc ID | Predicted | Actual | Why Missed |\r\n|--------|-----------|--------|------------|\r\n| 5678 | Class B | Class A | Subtle example |\r\n| ... | | | |\r\n\r\n### Systematic Patterns\r\n- [Pattern 1]\r\n- [Pattern 2]\r\n```\r\n\r\n**For dictionary - KWIC Validation:**\r\n\r\nFor key terms in dictionary:\r\n1. Sample uses in corpus\r\n2. Assess: Is this the intended meaning?\r\n3. Note domain-specific usages\r\n\r\n```markdown\r\n## Dictionary Term Validation: \"positive\"\r\n\r\n### Sample Uses\r\n| Doc ID | Context | Valid? |\r\n|--------|---------|--------|\r\n| 1234 | \"...test came back positive...\" | No (medical) |\r\n| 2345 | \"...positive economic outlook...\" | Yes |\r\n| 3456 | \"...positive feedback loop...\" | No (technical) |\r\n\r\n### Validity Rate: X/N valid uses\r\n### Action: [Keep / Remove / Add to exceptions]\r\n```\r\n\r\n### 2. Computational Diagnostics\r\n\r\n**Topic Model Diagnostics:**\r\n\r\n| Metric | Value | Interpretation |\r\n|--------|-------|----------------|\r\n| Mean coherence (C_V) | | > 0.5 generally good |\r\n| Mean coherence (UMass) | | Less negative is better |\r\n| Mean exclusivity (STM) | | Higher = more distinct |\r\n| Perplexity (held-out) | | Lower is better fit |\r\n\r\n```r\r\n# R example for STM\r\nexclusivity <- exclusivity(stm_model)\r\ncoherence <- semanticCoherence(stm_model, out$documents)\r\n\r\n# Plot coherence vs exclusivity\r\nplot(coherence, exclusivity,\r\n     xlab = \"Semantic Coherence\",\r\n     ylab = \"Exclusivity\")\r\n```\r\n\r\n```python\r\n# Python example for gensim\r\nfrom gensim.models import CoherenceModel\r\n\r\ncoherence_model = CoherenceModel(\r\n    model=lda_model,\r\n    texts=tokenized_docs,\r\n    coherence='c_v'\r\n)\r\ncoherence_score = coherence_model.get_coherence()\r\n```\r\n\r\n**Classification Diagnostics:**\r\n\r\n| Metric | Train | Validation | Test |\r\n|--------|-------|------------|------|\r\n| Accuracy | | | |\r\n| Macro F1 | | | |\r\n| Per-class F1 | | | |\r\n\r\nCheck for:\r\n- Overfitting (train >> validation)\r\n- Class imbalance effects\r\n- Confidence calibration\r\n\r\n**Dictionary Diagnostics:**\r\n\r\n| Metric | Value |\r\n|--------|-------|\r\n| Coverage (% docs with 1 match) | |\r\n| Mean matches per doc | |\r\n| Correlation with alternative measure | |\r\n\r\n### 3. Robustness Checks\r\n\r\nRun pre-specified robustness checks from Phase 2:\r\n\r\n**Sensitivity to K (topic models):**\r\n\r\n| K | Coherence | Exclusivity | Interpretation |\r\n|---|-----------|-------------|----------------|\r\n| K-5 | | | [Do similar topics emerge?] |\r\n| K (main) | | | [Baseline] |\r\n| K+5 | | | [Do topics split sensibly?] |\r\n\r\n**Sensitivity to preprocessing:**\r\n\r\n| Preprocessing | Result | Compared to Main |\r\n|---------------|--------|------------------|\r\n| With stemming | [result] | [consistent/different] |\r\n| Without stopwords | [result] | [consistent/different] |\r\n| Different threshold | [result] | [consistent/different] |\r\n\r\n**Sensitivity to random seed:**\r\n\r\n| Seed | Result | Compared to Main |\r\n|------|--------|------------------|\r\n| Seed 1 | [result] | [baseline] |\r\n| Seed 2 | [result] | [consistent/different] |\r\n| Seed 3 | [result] | [consistent/different] |\r\n\r\nFor topic models: Do the same topics emerge? Check topic alignment.\r\n\r\n**Subset analysis:**\r\n\r\n| Subset | N | Result | Compared to Full |\r\n|--------|---|--------|------------------|\r\n| Time period 1 | | | |\r\n| Time period 2 | | | |\r\n| Source type A | | | |\r\n| Source type B | | | |\r\n\r\n### 4. Alternative Methods (if applicable)\r\n\r\nCompare to alternative approaches:\r\n\r\n| Method | Primary Result | Alternative Result | Correlation |\r\n|--------|---------------|-------------------|-------------|\r\n| Main method | [result] | N/A | N/A |\r\n| Alternative | N/A | [result] | [r = X] |\r\n\r\nExample: Compare dictionary sentiment to ML sentiment.\r\n\r\n### 5. Assess Overall Validity\r\n\r\n**Validation Summary Table:**\r\n\r\n| Validation Type | Result | Concern Level |\r\n|-----------------|--------|---------------|\r\n| Human - topic coherence | X/N intrusion test | Low/Medium/High |\r\n| Human - document reading | X/N fit well | Low/Medium/High |\r\n| Computational - coherence | [score] | Low/Medium/High |\r\n| Robustness - K | [consistent/varies] | Low/Medium/High |\r\n| Robustness - preprocessing | [consistent/varies] | Low/Medium/High |\r\n| Robustness - seed | [consistent/varies] | Low/Medium/High |\r\n\r\n**Overall assessment:**\r\n- Are findings valid? [Yes/Partially/Concerns]\r\n- What caveats are needed?\r\n- What cannot be claimed?\r\n\r\n## Output: Validation Report\r\n\r\nCreate `memos/phase4-validation-report.md`:\r\n\r\n```markdown\r\n# Validation Report\r\n\r\n## Human Validation\r\n\r\n### Topic/Category Assessment\r\n[Summary of human coding]\r\n\r\n### Inter-rater Reliability\r\n[If multiple coders: Kappa, agreement %]\r\n\r\n## Computational Diagnostics\r\n\r\n### Model Fit Metrics\r\n| Metric | Value | Assessment |\r\n|--------|-------|------------|\r\n| | | |\r\n\r\n### Diagnostic Visualizations\r\n[Reference figures]\r\n\r\n## Robustness Analysis\r\n\r\n### Sensitivity to K\r\n[Results table and interpretation]\r\n\r\n### Sensitivity to Preprocessing\r\n[Results table and interpretation]\r\n\r\n### Sensitivity to Random Seed\r\n[Results table and interpretation]\r\n\r\n### Subset Analysis\r\n[Results table and interpretation]\r\n\r\n## Alternative Methods\r\n[If applicable]\r\n\r\n## Validity Assessment\r\n\r\n### Strengths\r\n- [Strength 1]\r\n- [Strength 2]\r\n\r\n### Limitations\r\n- [Limitation 1]\r\n- [Limitation 2]\r\n\r\n### Required Caveats for Interpretation\r\n1. [Caveat 1]\r\n2. [Caveat 2]\r\n\r\n### Claims That Cannot Be Made\r\n- [Cannot claim 1]\r\n- [Cannot claim 2]\r\n\r\n## Recommendation\r\n[Proceed to output / Revise analysis / Major concerns]\r\n```\r\n\r\n## When You're Done\r\n\r\nReturn a summary to the orchestrator that includes:\r\n1. Human validation results (what proportion validated?)\r\n2. Key diagnostic metrics\r\n3. Robustness assessment (are results stable?)\r\n4. Required caveats and limitations\r\n5. Recommendation for proceeding\r\n\r\n**Do not proceed to Phase 5 until the user reviews validation results.**\r\n",
        "plugins/text-analyst/skills/text-analyst/phases/phase5-output.md": "# Phase 5: Output & Interpretation\r\n\r\nYou are executing Phase 5 of a computational text analysis. Your goal is to produce publication-ready outputs and write a careful, appropriately caveated interpretation of findings.\r\n\r\n## Why This Phase Matters\r\n\r\nText analysis results require careful interpretation. Overclaimingundermines credibility. This phase produces polished outputs and ensures the narrative matches what the evidence supports.\r\n\r\n## Technique Guides\r\n\r\n**Consult visualization guides** for your language:\r\n- R: `text-r-techniques/06_visualization.md`\r\n- Python: `text-python-techniques/06_visualization.md`\r\n\r\n## Your Tasks\r\n\r\n### 1. Create Publication-Quality Tables\r\n\r\n**Topic Model Results Table:**\r\n\r\n| Topic | Label | Top Words (FREX) | Prevalence | Example Document |\r\n|-------|-------|------------------|------------|------------------|\r\n| 1 | [Label] | word1, word2, word3, word4, word5 | X% | \"Quote...\" |\r\n| 2 | [Label] | word1, word2, word3, word4, word5 | X% | \"Quote...\" |\r\n| ... | | | | |\r\n\r\n*Notes: K = [N] topics. FREX words balance frequency and exclusivity. N = [documents].*\r\n\r\n**Classification Results Table:**\r\n\r\n| Class | Precision | Recall | F1 | Support |\r\n|-------|-----------|--------|----|---------|\r\n| Class 1 | 0.XX | 0.XX | 0.XX | N |\r\n| Class 2 | 0.XX | 0.XX | 0.XX | N |\r\n| ... | | | | |\r\n| **Macro Average** | **0.XX** | **0.XX** | **0.XX** | **N** |\r\n\r\n*Notes: 5-fold stratified cross-validation. Features: [description].*\r\n\r\n**Dictionary/Sentiment Summary:**\r\n\r\n| Group | N | Mean Score | SD | 95% CI |\r\n|-------|---|------------|-----|---------|\r\n| Group 1 | N | X.XX | X.XX | [X.XX, X.XX] |\r\n| Group 2 | N | X.XX | X.XX | [X.XX, X.XX] |\r\n| Difference | | X.XX | | [X.XX, X.XX] |\r\n\r\n*Notes: [Dictionary name]. Score range: [X to Y].*\r\n\r\n### 2. Create Publication-Quality Figures\r\n\r\n**Topic models - required figures:**\r\n\r\n1. **Topic prevalence** (bar chart or dot plot)\r\n   - Ordered by prevalence\r\n   - Include uncertainty intervals if available\r\n\r\n2. **Topic content** (word clouds or bar charts)\r\n   - Top words per topic\r\n   - Consider FREX for STM\r\n\r\n3. **Topic relationships** (if relevant)\r\n   - Topic correlation network\r\n   - Hierarchical clustering\r\n\r\n4. **Topic trends** (if temporal)\r\n   - Prevalence over time\r\n   - Confidence bands\r\n\r\n**Classification - required figures:**\r\n\r\n1. **Confusion matrix** (heatmap)\r\n   - Normalized by row (recall focus) or column (precision focus)\r\n   - Include raw counts\r\n\r\n2. **Feature importance** (if interpretable model)\r\n   - Top predictive words per class\r\n   - Coefficients with confidence intervals\r\n\r\n**Dictionary/Sentiment - required figures:**\r\n\r\n1. **Distribution** (histogram or density)\r\n   - By group if comparing\r\n\r\n2. **Time series** (if temporal)\r\n   - Smoothed trends\r\n   - Confidence bands\r\n\r\n### 3. Write Results Narrative\r\n\r\nStructure the narrative:\r\n\r\n**Opening:**\r\n- Remind reader of the research question\r\n- Briefly state the approach\r\n\r\n**Main findings:**\r\n- Present results without overstating\r\n- Use hedged language appropriately\r\n- Connect to tables and figures\r\n\r\n**Validation summary:**\r\n- Briefly note validation approach\r\n- Report key diagnostics\r\n\r\n**Limitations:**\r\n- Acknowledge methodological limitations\r\n- Note what the analysis cannot show\r\n\r\n#### Language Guidelines\r\n\r\n**Avoid:**\r\n- \"The topic model discovered...\"\r\n- \"The algorithm found that...\"\r\n- \"This proves...\"\r\n- \"Clearly...\"\r\n- \"Obviously...\"\r\n\r\n**Prefer:**\r\n- \"The analysis suggests...\"\r\n- \"Patterns in the data indicate...\"\r\n- \"One interpretation is...\"\r\n- \"This is consistent with...\"\r\n- \"The evidence supports...\"\r\n\r\n**Topic model language:**\r\n- Topics are \"characterized by\" words, not \"about\" concepts\r\n- Topics \"tend to appear in\" documents, not \"represent\" ideas\r\n- Prevalence is \"estimated\" with uncertainty\r\n\r\n**Classification language:**\r\n- \"The classifier achieved X accuracy on held-out data\"\r\n- \"Misclassifications tended to occur when...\"\r\n- Performance is on \"this corpus,\" not \"in general\"\r\n\r\n**Dictionary language:**\r\n- \"Documents mentioning X words...\"\r\n- Coverage and limitations should be noted\r\n- \"According to this measure...\"\r\n\r\n### 4. Write Limitations Section\r\n\r\nEvery text analysis has limitations. Document:\r\n\r\n**Method limitations:**\r\n- Topic models: K is a choice, not a truth\r\n- Classification: Performance depends on training data\r\n- Dictionary: Coverage and domain validity\r\n\r\n**Data limitations:**\r\n- Corpus scope: Findings apply to this corpus\r\n- Selection: How texts were selected/sampled\r\n- Quality: OCR errors, missing data\r\n\r\n**Interpretation limitations:**\r\n- Topics are statistical patterns, not concepts\r\n- High probability  topic is \"about\" that theme\r\n- Classifiers learn correlations, not causation\r\n\r\n### 5. Prepare Replication Materials\r\n\r\nCreate replication package:\r\n\r\n```\r\nreplication/\r\n README.md           # Instructions\r\n requirements.R      # or requirements.txt\r\n 01_preprocess.R     # or .py\r\n 02_analysis.R       # or .py\r\n 03_validation.R     # or .py\r\n 04_figures.R        # or .py\r\n session_info.txt    # Package versions\r\n```\r\n\r\n**README.md for replication:**\r\n\r\n```markdown\r\n# Replication Materials\r\n\r\n## Requirements\r\n- R version X.X.X (or Python X.X)\r\n- Packages: [list with versions]\r\n\r\n## Data\r\nData files should be placed in `data/raw/`:\r\n- [file1.csv]: [description]\r\n- [file2.csv]: [description]\r\n\r\nNote: Original data [is/is not] included due to [access/size/privacy].\r\n\r\n## Replication Steps\r\n1. Install requirements: `source(\"requirements.R\")`\r\n2. Preprocess data: `source(\"01_preprocess.R\")`\r\n3. Run analysis: `source(\"02_analysis.R\")`\r\n4. Validate: `source(\"03_validation.R\")`\r\n5. Generate figures: `source(\"04_figures.R\")`\r\n\r\n## Random Seed\r\nAll analyses use seed: [SEED]\r\n\r\n## Expected Output\r\n- [output1]: [description]\r\n- [output2]: [description]\r\n\r\n## Contact\r\n[Author contact for questions]\r\n```\r\n\r\n### 6. Create Methods Section Draft\r\n\r\nWrite methods section following journal conventions:\r\n\r\n```markdown\r\n## Text Analysis Methods\r\n\r\n### Corpus\r\nThe corpus consists of N documents from [source],\r\nspanning [time period]. Documents were [sampling description].\r\n\r\n### Preprocessing\r\nText was preprocessed using [package/tool].\r\n[Specific steps: tokenization, stopword removal, etc.]\r\nFinal vocabulary: N terms across N documents.\r\n\r\n### Analysis\r\nWe used [method] implemented in [package] (version X.X).\r\n[Key parameters: K topics, hyperparameters, etc.]\r\n[Validation approach].\r\n\r\n### Validation\r\n[Human validation: sample size, procedure, results]\r\n[Computational diagnostics: metrics, results]\r\n[Robustness checks: what varied, results]\r\n```\r\n\r\n## Output: Final Package\r\n\r\nCreate the following in `output/`:\r\n\r\n```\r\noutput/\r\n tables/\r\n    table1_topic_summary.csv\r\n    table2_prevalence.csv\r\n    ...\r\n figures/\r\n    fig1_topic_prevalence.pdf\r\n    fig2_topic_words.pdf\r\n    ...\r\n narrative/\r\n    results_section.md\r\n    methods_section.md\r\n    limitations.md\r\n replication/\r\n     [replication package]\r\n```\r\n\r\n## Final Memo\r\n\r\nCreate `memos/phase5-output-memo.md`:\r\n\r\n```markdown\r\n# Output Summary\r\n\r\n## Deliverables\r\n\r\n### Tables\r\n1. [Table 1]: [description, location]\r\n2. [Table 2]: [description, location]\r\n\r\n### Figures\r\n1. [Figure 1]: [description, location]\r\n2. [Figure 2]: [description, location]\r\n\r\n### Narrative Sections\r\n- Results: [location]\r\n- Methods: [location]\r\n- Limitations: [location]\r\n\r\n### Replication Materials\r\n[Location, completeness status]\r\n\r\n## Key Messages\r\n\r\n### Main Findings (1-3 sentences)\r\n[Summary of what the analysis shows]\r\n\r\n### Required Caveats\r\n1. [Caveat 1]\r\n2. [Caveat 2]\r\n\r\n### Claims Supported by Evidence\r\n- [Claim 1]: [evidence]\r\n- [Claim 2]: [evidence]\r\n\r\n### Claims NOT Supported\r\n- [Cannot claim 1]: [why not]\r\n- [Cannot claim 2]: [why not]\r\n\r\n## Quality Checklist\r\n\r\n- [ ] All tables formatted consistently\r\n- [ ] All figures publication-ready (300 dpi, clear labels)\r\n- [ ] Narrative uses appropriate hedging\r\n- [ ] Limitations section complete\r\n- [ ] Replication package tested\r\n- [ ] Methods section matches actual analysis\r\n- [ ] Random seeds documented throughout\r\n```\r\n\r\n## When You're Done\r\n\r\nReturn a summary to the orchestrator that includes:\r\n1. List of deliverables produced\r\n2. Key findings summary (2-3 sentences)\r\n3. Main limitations to acknowledge\r\n4. Replication package status\r\n5. Any remaining concerns\r\n\r\n**Analysis is complete when user accepts the outputs.**\r\n",
        "plugins/text-analyst/skills/text-analyst/python-techniques/01_preprocessing.md": "# Text Preprocessing in Python\r\n\r\n## Package Versions\r\n\r\n```python\r\n# Tested with:\r\n# Python 3.10\r\n# nltk 3.8.1\r\n# spacy 3.6.0\r\n# scikit-learn 1.3.0\r\n```\r\n\r\n## Installation\r\n\r\n```bash\r\npip install nltk spacy scikit-learn pandas\r\n\r\n# Download spaCy model\r\npython -m spacy download en_core_web_sm\r\n\r\n# Download NLTK data\r\npython -c \"import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet')\"\r\n```\r\n\r\n## Three Approaches: NLTK vs spaCy vs sklearn\r\n\r\n| Package | Philosophy | Best For |\r\n|---------|------------|----------|\r\n| **NLTK** | Educational, comprehensive | Learning, custom pipelines |\r\n| **spaCy** | Industrial-strength NLP | Production, entity recognition |\r\n| **sklearn** | Machine learning focused | Vectorization for ML |\r\n\r\n## NLTK Workflow\r\n\r\n### Basic Tokenization\r\n\r\n```python\r\nimport nltk\r\nfrom nltk.tokenize import word_tokenize, sent_tokenize\r\nfrom nltk.corpus import stopwords\r\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\r\n\r\n# Sample texts\r\ntexts = [\r\n    \"The quick brown fox jumps over the lazy dog.\",\r\n    \"Machine learning is transforming social science research.\",\r\n    \"Text analysis requires careful preprocessing decisions.\"\r\n]\r\n\r\n# Word tokenization\r\ntokens = [word_tokenize(text.lower()) for text in texts]\r\nprint(tokens[0])\r\n# ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\r\n\r\n# Sentence tokenization\r\nsentences = [sent_tokenize(text) for text in texts]\r\n```\r\n\r\n### Stopword Removal\r\n\r\n```python\r\nstop_words = set(stopwords.words('english'))\r\n\r\n# Remove stopwords\r\ntokens_clean = [[w for w in doc if w not in stop_words and w.isalpha()]\r\n                for doc in tokens]\r\n\r\nprint(tokens_clean[0])\r\n# ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']\r\n\r\n# Custom stopwords\r\ncustom_stops = {'data', 'analysis', 'research'}\r\nstop_words = stop_words.union(custom_stops)\r\n```\r\n\r\n### Stemming and Lemmatization\r\n\r\n```python\r\n# Stemming (Porter)\r\nstemmer = PorterStemmer()\r\ntokens_stemmed = [[stemmer.stem(w) for w in doc] for doc in tokens_clean]\r\n\r\n# Lemmatization\r\nlemmatizer = WordNetLemmatizer()\r\ntokens_lemma = [[lemmatizer.lemmatize(w) for w in doc] for doc in tokens_clean]\r\n\r\n# Compare\r\nprint(f\"Original: jumps\")\r\nprint(f\"Stemmed: {stemmer.stem('jumps')}\")     # jump\r\nprint(f\"Lemmatized: {lemmatizer.lemmatize('jumps', pos='v')}\")  # jump\r\n```\r\n\r\n### N-grams\r\n\r\n```python\r\nfrom nltk import ngrams\r\n\r\n# Bigrams\r\nbigrams = [list(ngrams(doc, 2)) for doc in tokens_clean]\r\nprint(bigrams[0])\r\n# [('quick', 'brown'), ('brown', 'fox'), ...]\r\n\r\n# Trigrams\r\ntrigrams = [list(ngrams(doc, 3)) for doc in tokens_clean]\r\n```\r\n\r\n## spaCy Workflow\r\n\r\n### Basic Processing\r\n\r\n```python\r\nimport spacy\r\n\r\n# Load model\r\nnlp = spacy.load('en_core_web_sm')\r\n\r\n# Process text\r\ndoc = nlp(\"Machine learning is transforming social science research.\")\r\n\r\n# Tokens with attributes\r\nfor token in doc:\r\n    print(f\"{token.text:15} {token.pos_:8} {token.lemma_:15} {token.is_stop}\")\r\n```\r\n\r\n### Complete Preprocessing Pipeline\r\n\r\n```python\r\ndef preprocess_spacy(texts, nlp, remove_stops=True, lemmatize=True):\r\n    \"\"\"\r\n    Preprocess texts using spaCy.\r\n\r\n    Parameters:\r\n    -----------\r\n    texts : list of str\r\n    nlp : spacy model\r\n    remove_stops : bool\r\n    lemmatize : bool\r\n\r\n    Returns:\r\n    --------\r\n    list of list of str : processed tokens\r\n    \"\"\"\r\n    processed = []\r\n    for doc in nlp.pipe(texts, batch_size=50):\r\n        tokens = []\r\n        for token in doc:\r\n            # Skip punctuation, spaces, and optionally stopwords\r\n            if token.is_punct or token.is_space:\r\n                continue\r\n            if remove_stops and token.is_stop:\r\n                continue\r\n\r\n            # Lemmatize or use original\r\n            if lemmatize:\r\n                tokens.append(token.lemma_.lower())\r\n            else:\r\n                tokens.append(token.text.lower())\r\n\r\n        processed.append(tokens)\r\n\r\n    return processed\r\n\r\n# Usage\r\ntokens = preprocess_spacy(texts, nlp)\r\nprint(tokens[1])\r\n# ['machine', 'learn', 'transform', 'social', 'science', 'research']\r\n```\r\n\r\n### Named Entity Recognition\r\n\r\n```python\r\ndoc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\r\n\r\nfor ent in doc.ents:\r\n    print(f\"{ent.text:20} {ent.label_:10} {spacy.explain(ent.label_)}\")\r\n\r\n# Apple                ORG        Companies, agencies, institutions\r\n# U.K.                 GPE        Countries, cities, states\r\n# $1 billion           MONEY      Monetary values\r\n```\r\n\r\n### Custom Pipeline Component\r\n\r\n```python\r\nfrom spacy.language import Language\r\n\r\n@Language.component(\"custom_cleaner\")\r\ndef custom_cleaner(doc):\r\n    \"\"\"Remove URLs and email addresses.\"\"\"\r\n    cleaned_tokens = []\r\n    for token in doc:\r\n        if not token.like_url and not token.like_email:\r\n            cleaned_tokens.append(token)\r\n    return doc\r\n\r\n# Add to pipeline\r\nnlp.add_pipe(\"custom_cleaner\", after=\"parser\")\r\n```\r\n\r\n## sklearn Workflow\r\n\r\n### CountVectorizer\r\n\r\n```python\r\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\r\n\r\n# Basic count vectorizer\r\ncount_vec = CountVectorizer(\r\n    lowercase=True,\r\n    stop_words='english',\r\n    max_features=1000,\r\n    min_df=2,           # Minimum document frequency\r\n    max_df=0.95,        # Maximum document frequency\r\n    ngram_range=(1, 2)  # Unigrams and bigrams\r\n)\r\n\r\n# Fit and transform\r\ndtm = count_vec.fit_transform(texts)\r\n\r\n# Inspect\r\nprint(f\"Shape: {dtm.shape}\")\r\nprint(f\"Vocabulary size: {len(count_vec.vocabulary_)}\")\r\nprint(f\"Feature names: {count_vec.get_feature_names_out()[:10]}\")\r\n```\r\n\r\n### TF-IDF Vectorizer\r\n\r\n```python\r\ntfidf_vec = TfidfVectorizer(\r\n    lowercase=True,\r\n    stop_words='english',\r\n    max_features=1000,\r\n    min_df=2,\r\n    max_df=0.95,\r\n    ngram_range=(1, 1),\r\n    sublinear_tf=True   # Apply log to TF\r\n)\r\n\r\ntfidf_matrix = tfidf_vec.fit_transform(texts)\r\n```\r\n\r\n### Custom Tokenizer with sklearn\r\n\r\n```python\r\ndef custom_tokenizer(text):\r\n    \"\"\"Custom tokenizer with spaCy.\"\"\"\r\n    doc = nlp(text)\r\n    return [token.lemma_.lower() for token in doc\r\n            if not token.is_stop and not token.is_punct and token.is_alpha]\r\n\r\n# Use in vectorizer\r\ncustom_vec = TfidfVectorizer(tokenizer=custom_tokenizer)\r\ncustom_matrix = custom_vec.fit_transform(texts)\r\n```\r\n\r\n## Handling Special Cases\r\n\r\n### URLs and HTML\r\n\r\n```python\r\nimport re\r\n\r\ndef clean_text(text):\r\n    \"\"\"Basic text cleaning.\"\"\"\r\n    # Remove URLs\r\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\r\n\r\n    # Remove HTML tags\r\n    text = re.sub(r'<[^>]+>', '', text)\r\n\r\n    # Remove special characters but keep basic punctuation\r\n    text = re.sub(r'[^\\w\\s.,!?]', '', text)\r\n\r\n    # Remove extra whitespace\r\n    text = ' '.join(text.split())\r\n\r\n    return text\r\n\r\n# Apply\r\ntexts_clean = [clean_text(t) for t in texts]\r\n```\r\n\r\n### Handling Emojis and Special Characters\r\n\r\n```python\r\nimport emoji\r\n\r\ndef handle_emojis(text, mode='remove'):\r\n    \"\"\"Handle emojis in text.\"\"\"\r\n    if mode == 'remove':\r\n        return emoji.replace_emoji(text, '')\r\n    elif mode == 'demojize':\r\n        return emoji.demojize(text)\r\n    return text\r\n\r\n# Example\r\ntext_with_emoji = \"Great product! \"\r\nprint(handle_emojis(text_with_emoji, 'demojize'))\r\n# Great product! :smiling_face_with_smiling_eyes::thumbs_up:\r\n```\r\n\r\n### Encoding Issues\r\n\r\n```python\r\ndef fix_encoding(text):\r\n    \"\"\"Fix common encoding issues.\"\"\"\r\n    # Handle None\r\n    if text is None:\r\n        return \"\"\r\n\r\n    # Ensure string\r\n    if not isinstance(text, str):\r\n        text = str(text)\r\n\r\n    # Fix encoding\r\n    try:\r\n        text = text.encode('utf-8', errors='ignore').decode('utf-8')\r\n    except:\r\n        text = \"\"\r\n\r\n    return text\r\n```\r\n\r\n## Document-Term Matrix Operations\r\n\r\n### Converting to Dense\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\n# Sparse to dense (only for small matrices!)\r\ndense_matrix = dtm.toarray()\r\n\r\n# As DataFrame\r\ndtm_df = pd.DataFrame(\r\n    dense_matrix,\r\n    columns=count_vec.get_feature_names_out()\r\n)\r\n```\r\n\r\n### Vocabulary Statistics\r\n\r\n```python\r\ndef get_vocab_stats(vectorizer, dtm):\r\n    \"\"\"Get vocabulary statistics.\"\"\"\r\n    feature_names = vectorizer.get_feature_names_out()\r\n    freqs = np.asarray(dtm.sum(axis=0)).ravel()\r\n    doc_freqs = np.asarray((dtm > 0).sum(axis=0)).ravel()\r\n\r\n    stats = pd.DataFrame({\r\n        'term': feature_names,\r\n        'total_freq': freqs,\r\n        'doc_freq': doc_freqs,\r\n        'doc_freq_pct': doc_freqs / dtm.shape[0]\r\n    }).sort_values('total_freq', ascending=False)\r\n\r\n    return stats\r\n\r\nstats = get_vocab_stats(count_vec, dtm)\r\nprint(stats.head(20))\r\n```\r\n\r\n## Complete Preprocessing Pipeline\r\n\r\n```python\r\nimport pandas as pd\r\nfrom typing import List, Optional\r\n\r\nclass TextPreprocessor:\r\n    \"\"\"Complete text preprocessing pipeline.\"\"\"\r\n\r\n    def __init__(\r\n        self,\r\n        language: str = 'english',\r\n        remove_stops: bool = True,\r\n        lemmatize: bool = True,\r\n        min_token_len: int = 2,\r\n        custom_stops: Optional[List[str]] = None\r\n    ):\r\n        self.language = language\r\n        self.remove_stops = remove_stops\r\n        self.lemmatize = lemmatize\r\n        self.min_token_len = min_token_len\r\n\r\n        # Load spaCy\r\n        self.nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\r\n\r\n        # Set stopwords\r\n        self.stop_words = set(stopwords.words(language))\r\n        if custom_stops:\r\n            self.stop_words.update(custom_stops)\r\n\r\n    def clean_text(self, text: str) -> str:\r\n        \"\"\"Basic text cleaning.\"\"\"\r\n        if not isinstance(text, str):\r\n            return \"\"\r\n\r\n        # Remove URLs\r\n        text = re.sub(r'https?://\\S+', '', text)\r\n\r\n        # Remove HTML\r\n        text = re.sub(r'<[^>]+>', '', text)\r\n\r\n        # Normalize whitespace\r\n        text = ' '.join(text.split())\r\n\r\n        return text\r\n\r\n    def tokenize(self, text: str) -> List[str]:\r\n        \"\"\"Tokenize and optionally lemmatize.\"\"\"\r\n        doc = self.nlp(text)\r\n        tokens = []\r\n\r\n        for token in doc:\r\n            # Skip punctuation and spaces\r\n            if token.is_punct or token.is_space:\r\n                continue\r\n\r\n            # Skip stopwords\r\n            if self.remove_stops and token.text.lower() in self.stop_words:\r\n                continue\r\n\r\n            # Get token text\r\n            if self.lemmatize:\r\n                tok = token.lemma_.lower()\r\n            else:\r\n                tok = token.text.lower()\r\n\r\n            # Skip short tokens\r\n            if len(tok) < self.min_token_len:\r\n                continue\r\n\r\n            # Only alphabetic\r\n            if not tok.isalpha():\r\n                continue\r\n\r\n            tokens.append(tok)\r\n\r\n        return tokens\r\n\r\n    def preprocess(self, texts: List[str]) -> List[List[str]]:\r\n        \"\"\"Full preprocessing pipeline.\"\"\"\r\n        processed = []\r\n        for text in texts:\r\n            clean = self.clean_text(text)\r\n            tokens = self.tokenize(clean)\r\n            processed.append(tokens)\r\n        return processed\r\n\r\n    def get_stats(self, processed: List[List[str]]) -> dict:\r\n        \"\"\"Get corpus statistics.\"\"\"\r\n        all_tokens = [t for doc in processed for t in doc]\r\n        return {\r\n            'n_documents': len(processed),\r\n            'n_tokens': len(all_tokens),\r\n            'n_types': len(set(all_tokens)),\r\n            'mean_doc_length': len(all_tokens) / len(processed),\r\n            'empty_docs': sum(1 for doc in processed if len(doc) == 0)\r\n        }\r\n\r\n# Usage\r\npreprocessor = TextPreprocessor(\r\n    remove_stops=True,\r\n    lemmatize=True,\r\n    custom_stops=['data', 'study']\r\n)\r\n\r\nprocessed = preprocessor.preprocess(texts)\r\nstats = preprocessor.get_stats(processed)\r\nprint(stats)\r\n```\r\n\r\n## Best Practices\r\n\r\n1. **Document all decisions** - Keep preprocessing log\r\n2. **Start minimal** - Add processing steps only if needed\r\n3. **Check vocabulary** - Examine most/least frequent terms\r\n4. **Use batching** - Process in batches for large corpora\r\n5. **Save intermediate results** - For reproducibility\r\n6. **Set random seeds** - For any stochastic elements\r\n\r\n```python\r\n# Preprocessing log\r\npreprocessing_log = {\r\n    'date': pd.Timestamp.now().isoformat(),\r\n    'n_docs_raw': len(texts),\r\n    'n_docs_processed': len(processed),\r\n    'vocab_size': stats['n_types'],\r\n    'stopwords': 'english + custom',\r\n    'lemmatization': 'spacy',\r\n    'min_token_length': 2\r\n}\r\n\r\n# Save\r\npd.Series(preprocessing_log).to_json('data/processed/preprocessing_log.json')\r\n```\r\n",
        "plugins/text-analyst/skills/text-analyst/python-techniques/02_dictionary_sentiment.md": "# Dictionary and Sentiment Analysis in Python\r\n\r\n## Package Versions\r\n\r\n```python\r\n# Tested with:\r\n# Python 3.10\r\n# vaderSentiment 3.3.2\r\n# textblob 0.17.1\r\n# nltk 3.8.1\r\n# pandas 2.0.0\r\n```\r\n\r\n## Installation\r\n\r\n```bash\r\npip install vaderSentiment textblob nltk pandas\r\n\r\n# Download TextBlob corpora\r\npython -m textblob.download_corpora\r\n```\r\n\r\n## VADER Sentiment\r\n\r\nVADER (Valence Aware Dictionary and sEntiment Reasoner) is designed for social media.\r\n\r\n### Basic Usage\r\n\r\n```python\r\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\r\nimport pandas as pd\r\n\r\n# Initialize analyzer\r\nanalyzer = SentimentIntensityAnalyzer()\r\n\r\n# Sample texts\r\ntexts = [\r\n    \"I love this amazing product! Highly recommend!\",\r\n    \"Terrible experience. The worst purchase ever.\",\r\n    \"It's okay, nothing special but not bad.\",\r\n    \"Great quality but expensive and slow delivery.\"\r\n]\r\n\r\n# Analyze sentiment\r\nfor text in texts:\r\n    scores = analyzer.polarity_scores(text)\r\n    print(f\"{text[:40]:40} | {scores}\")\r\n\r\n# Output:\r\n# I love this amazing product! Highly rec | {'neg': 0.0, 'neu': 0.254, 'pos': 0.746, 'compound': 0.8316}\r\n# Terrible experience. The worst purchase | {'neg': 0.611, 'neu': 0.389, 'pos': 0.0, 'compound': -0.8316}\r\n```\r\n\r\n### VADER Output Interpretation\r\n\r\n```python\r\n# compound: normalized score from -1 (most negative) to +1 (most positive)\r\n# pos, neg, neu: proportions of text falling in each category\r\n\r\n# Classification thresholds\r\ndef classify_sentiment(compound):\r\n    if compound >= 0.05:\r\n        return 'positive'\r\n    elif compound <= -0.05:\r\n        return 'negative'\r\n    else:\r\n        return 'neutral'\r\n```\r\n\r\n### Batch Processing\r\n\r\n```python\r\ndef analyze_vader(texts):\r\n    \"\"\"Analyze sentiment for list of texts.\"\"\"\r\n    analyzer = SentimentIntensityAnalyzer()\r\n\r\n    results = []\r\n    for i, text in enumerate(texts):\r\n        scores = analyzer.polarity_scores(text)\r\n        results.append({\r\n            'doc_id': i,\r\n            'text': text[:100],\r\n            'compound': scores['compound'],\r\n            'positive': scores['pos'],\r\n            'negative': scores['neg'],\r\n            'neutral': scores['neu'],\r\n            'sentiment': classify_sentiment(scores['compound'])\r\n        })\r\n\r\n    return pd.DataFrame(results)\r\n\r\n# Usage\r\nsentiment_df = analyze_vader(texts)\r\nprint(sentiment_df)\r\n```\r\n\r\n### VADER Features\r\n\r\n```python\r\n# VADER handles:\r\n\r\n# Punctuation emphasis\r\nprint(analyzer.polarity_scores(\"Good!\"))       # compound: 0.4927\r\nprint(analyzer.polarity_scores(\"Good!!!\"))     # compound: 0.6588 (stronger)\r\n\r\n# Capitalization\r\nprint(analyzer.polarity_scores(\"good\"))        # compound: 0.4404\r\nprint(analyzer.polarity_scores(\"GOOD\"))        # compound: 0.5622 (stronger)\r\n\r\n# Intensifiers\r\nprint(analyzer.polarity_scores(\"good\"))        # compound: 0.4404\r\nprint(analyzer.polarity_scores(\"very good\"))   # compound: 0.4927 (stronger)\r\n\r\n# Negation\r\nprint(analyzer.polarity_scores(\"good\"))        # compound: 0.4404\r\nprint(analyzer.polarity_scores(\"not good\"))    # compound: -0.3412 (flipped)\r\n\r\n# Conjunctions\r\nprint(analyzer.polarity_scores(\"good but bad\"))  # Handles mixed\r\n```\r\n\r\n## TextBlob Sentiment\r\n\r\n### Basic Usage\r\n\r\n```python\r\nfrom textblob import TextBlob\r\n\r\n# Analyze text\r\nblob = TextBlob(\"I love this amazing product!\")\r\nprint(f\"Polarity: {blob.sentiment.polarity}\")      # -1 to 1\r\nprint(f\"Subjectivity: {blob.sentiment.subjectivity}\")  # 0 to 1\r\n\r\n# Polarity: 0.625\r\n# Subjectivity: 0.6\r\n```\r\n\r\n### Batch Processing\r\n\r\n```python\r\ndef analyze_textblob(texts):\r\n    \"\"\"Analyze sentiment using TextBlob.\"\"\"\r\n    results = []\r\n    for i, text in enumerate(texts):\r\n        blob = TextBlob(text)\r\n        results.append({\r\n            'doc_id': i,\r\n            'text': text[:100],\r\n            'polarity': blob.sentiment.polarity,\r\n            'subjectivity': blob.sentiment.subjectivity\r\n        })\r\n\r\n    return pd.DataFrame(results)\r\n\r\nsentiment_tb = analyze_textblob(texts)\r\n```\r\n\r\n### Sentence-Level Analysis\r\n\r\n```python\r\ntext = \"The product quality is great. But the shipping was terrible.\"\r\nblob = TextBlob(text)\r\n\r\nfor sentence in blob.sentences:\r\n    print(f\"{sentence} | Polarity: {sentence.sentiment.polarity:.2f}\")\r\n\r\n# The product quality is great. | Polarity: 0.66\r\n# But the shipping was terrible. | Polarity: -1.00\r\n```\r\n\r\n## NRC Emotion Lexicon\r\n\r\n### Loading NRC\r\n\r\n```python\r\nimport pandas as pd\r\n\r\n# NRC lexicon (download from https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm)\r\n# Format: word \\t emotion \\t association (0 or 1)\r\n\r\ndef load_nrc(filepath='NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'):\r\n    \"\"\"Load NRC emotion lexicon.\"\"\"\r\n    nrc = pd.read_csv(\r\n        filepath,\r\n        sep='\\t',\r\n        names=['word', 'emotion', 'association']\r\n    )\r\n    # Pivot to word -> emotions\r\n    nrc_wide = nrc[nrc.association == 1].pivot(\r\n        index='word',\r\n        columns='emotion',\r\n        values='association'\r\n    ).fillna(0)\r\n\r\n    return nrc_wide\r\n\r\n# nrc = load_nrc()\r\n```\r\n\r\n### Using NRC\r\n\r\n```python\r\ndef analyze_nrc(text, nrc_lexicon):\r\n    \"\"\"Get emotion counts for text.\"\"\"\r\n    words = text.lower().split()\r\n    emotions = nrc_lexicon.columns.tolist()\r\n\r\n    counts = {e: 0 for e in emotions}\r\n\r\n    for word in words:\r\n        if word in nrc_lexicon.index:\r\n            for emotion in emotions:\r\n                counts[emotion] += nrc_lexicon.loc[word, emotion]\r\n\r\n    return counts\r\n\r\n# Usage\r\n# emotions = analyze_nrc(\"I am so happy and excited!\", nrc)\r\n# print(emotions)\r\n```\r\n\r\n## Custom Dictionaries\r\n\r\n### Creating a Custom Dictionary\r\n\r\n```python\r\nclass CustomDictionary:\r\n    \"\"\"Custom dictionary for concept measurement.\"\"\"\r\n\r\n    def __init__(self, positive_words, negative_words=None, weights=None):\r\n        \"\"\"\r\n        Parameters:\r\n        -----------\r\n        positive_words : list of str\r\n        negative_words : list of str, optional\r\n        weights : dict, optional (word -> weight)\r\n        \"\"\"\r\n        self.positive = set(w.lower() for w in positive_words)\r\n        self.negative = set(w.lower() for w in (negative_words or []))\r\n        self.weights = weights or {}\r\n\r\n    def score(self, text, normalize=True):\r\n        \"\"\"Score a text.\"\"\"\r\n        words = text.lower().split()\r\n\r\n        if not words:\r\n            return {'score': 0, 'pos_count': 0, 'neg_count': 0, 'coverage': 0}\r\n\r\n        pos_count = 0\r\n        neg_count = 0\r\n        weighted_score = 0\r\n\r\n        for word in words:\r\n            weight = self.weights.get(word, 1)\r\n\r\n            if word in self.positive:\r\n                pos_count += 1\r\n                weighted_score += weight\r\n            elif word in self.negative:\r\n                neg_count += 1\r\n                weighted_score -= weight\r\n\r\n        total_matches = pos_count + neg_count\r\n        coverage = total_matches / len(words) if words else 0\r\n\r\n        if normalize and len(words) > 0:\r\n            score = weighted_score / len(words)\r\n        else:\r\n            score = weighted_score\r\n\r\n        return {\r\n            'score': score,\r\n            'pos_count': pos_count,\r\n            'neg_count': neg_count,\r\n            'coverage': coverage\r\n        }\r\n\r\n# Usage\r\ninnovation_dict = CustomDictionary(\r\n    positive_words=['innovative', 'breakthrough', 'revolutionary', 'novel'],\r\n    negative_words=['obsolete', 'outdated', 'stagnant', 'declining'],\r\n    weights={'revolutionary': 2, 'breakthrough': 2}\r\n)\r\n\r\nresult = innovation_dict.score(\"The company made an innovative breakthrough\")\r\nprint(result)\r\n```\r\n\r\n### Multi-Word Expressions\r\n\r\n```python\r\nimport re\r\n\r\nclass MWEDictionary:\r\n    \"\"\"Dictionary with multi-word expressions.\"\"\"\r\n\r\n    def __init__(self, phrases):\r\n        \"\"\"\r\n        Parameters:\r\n        -----------\r\n        phrases : dict (phrase -> category/value)\r\n        \"\"\"\r\n        self.phrases = {p.lower(): v for p, v in phrases.items()}\r\n        # Sort by length (longest first for matching)\r\n        self.sorted_phrases = sorted(self.phrases.keys(), key=len, reverse=True)\r\n\r\n    def find_matches(self, text):\r\n        \"\"\"Find all phrase matches in text.\"\"\"\r\n        text_lower = text.lower()\r\n        matches = []\r\n\r\n        for phrase in self.sorted_phrases:\r\n            pattern = r'\\b' + re.escape(phrase) + r'\\b'\r\n            for match in re.finditer(pattern, text_lower):\r\n                matches.append({\r\n                    'phrase': phrase,\r\n                    'value': self.phrases[phrase],\r\n                    'start': match.start(),\r\n                    'end': match.end()\r\n                })\r\n\r\n        return matches\r\n\r\n# Usage\r\ntech_terms = MWEDictionary({\r\n    'machine learning': 'tech',\r\n    'artificial intelligence': 'tech',\r\n    'deep learning': 'tech',\r\n    'natural language processing': 'tech'\r\n})\r\n\r\nmatches = tech_terms.find_matches(\"Machine learning and artificial intelligence are transforming research\")\r\nprint(matches)\r\n```\r\n\r\n## Sentiment Over Time\r\n\r\n```python\r\nimport matplotlib.pyplot as plt\r\nfrom datetime import datetime, timedelta\r\n\r\n# Sample time-series data\r\ndates = [datetime(2020, 1, 1) + timedelta(days=i) for i in range(100)]\r\ntexts_time = pd.DataFrame({\r\n    'date': dates,\r\n    'text': [f\"Sample text {i % 4}\" for i in range(100)]\r\n})\r\n\r\n# Add sentiment\r\nanalyzer = SentimentIntensityAnalyzer()\r\ntexts_time['sentiment'] = texts_time['text'].apply(\r\n    lambda x: analyzer.polarity_scores(x)['compound']\r\n)\r\n\r\n# Daily average\r\ndaily_sentiment = texts_time.groupby(texts_time['date'].dt.date)['sentiment'].mean()\r\n\r\n# Plot\r\nplt.figure(figsize=(12, 5))\r\nplt.plot(daily_sentiment.index, daily_sentiment.values)\r\nplt.axhline(y=0, color='r', linestyle='--', alpha=0.5)\r\nplt.xlabel('Date')\r\nplt.ylabel('Sentiment (Compound)')\r\nplt.title('Sentiment Over Time')\r\nplt.tight_layout()\r\nplt.savefig('output/figures/sentiment_time.png', dpi=300)\r\nplt.close()\r\n```\r\n\r\n## Comparing Groups\r\n\r\n```python\r\n# Sample grouped data\r\ngroup_texts = pd.DataFrame({\r\n    'text': texts * 25,\r\n    'group': ['A'] * 50 + ['B'] * 50\r\n})\r\n\r\n# Add sentiment\r\ngroup_texts['sentiment'] = group_texts['text'].apply(\r\n    lambda x: analyzer.polarity_scores(x)['compound']\r\n)\r\n\r\n# Compare groups\r\ngroup_summary = group_texts.groupby('group')['sentiment'].agg(['mean', 'std', 'count'])\r\nprint(group_summary)\r\n\r\n# Statistical test\r\nfrom scipy import stats\r\ngroup_a = group_texts[group_texts['group'] == 'A']['sentiment']\r\ngroup_b = group_texts[group_texts['group'] == 'B']['sentiment']\r\nt_stat, p_val = stats.ttest_ind(group_a, group_b)\r\nprint(f\"t-statistic: {t_stat:.3f}, p-value: {p_val:.3f}\")\r\n```\r\n\r\n## Validation\r\n\r\n### Coverage Check\r\n\r\n```python\r\ndef check_coverage(texts, analyzer):\r\n    \"\"\"Check sentiment lexicon coverage.\"\"\"\r\n\r\n    coverage_stats = []\r\n    for text in texts:\r\n        words = text.lower().split()\r\n        scores = analyzer.polarity_scores(text)\r\n\r\n        # Check which words contribute to sentiment\r\n        word_scores = [analyzer.polarity_scores(w)['compound'] for w in words]\r\n        contributing = sum(1 for s in word_scores if s != 0)\r\n\r\n        coverage_stats.append({\r\n            'n_words': len(words),\r\n            'n_contributing': contributing,\r\n            'coverage': contributing / len(words) if words else 0\r\n        })\r\n\r\n    coverage_df = pd.DataFrame(coverage_stats)\r\n\r\n    print(f\"Mean coverage: {coverage_df['coverage'].mean():.2%}\")\r\n    print(f\"Docs with zero matches: {(coverage_df['n_contributing'] == 0).sum()}\")\r\n\r\n    return coverage_df\r\n\r\ncoverage = check_coverage(texts, analyzer)\r\n```\r\n\r\n### KWIC Validation\r\n\r\n```python\r\ndef kwic(texts, word, window=5):\r\n    \"\"\"Keyword in context.\"\"\"\r\n    results = []\r\n\r\n    for i, text in enumerate(texts):\r\n        words = text.lower().split()\r\n        for j, w in enumerate(words):\r\n            if word.lower() in w:\r\n                start = max(0, j - window)\r\n                end = min(len(words), j + window + 1)\r\n                context = ' '.join(words[start:end])\r\n                results.append({\r\n                    'doc_id': i,\r\n                    'position': j,\r\n                    'context': context\r\n                })\r\n\r\n    return pd.DataFrame(results)\r\n\r\n# Check usage of key terms\r\nkwic_results = kwic(texts, 'good')\r\nprint(kwic_results)\r\n```\r\n\r\n## Complete Workflow\r\n\r\n```python\r\nclass SentimentAnalyzer:\r\n    \"\"\"Complete sentiment analysis pipeline.\"\"\"\r\n\r\n    def __init__(self, method='vader'):\r\n        self.method = method\r\n\r\n        if method == 'vader':\r\n            self.analyzer = SentimentIntensityAnalyzer()\r\n        elif method == 'textblob':\r\n            pass  # TextBlob doesn't need initialization\r\n        else:\r\n            raise ValueError(f\"Unknown method: {method}\")\r\n\r\n    def analyze_one(self, text):\r\n        \"\"\"Analyze single text.\"\"\"\r\n        if self.method == 'vader':\r\n            scores = self.analyzer.polarity_scores(text)\r\n            return {\r\n                'compound': scores['compound'],\r\n                'positive': scores['pos'],\r\n                'negative': scores['neg'],\r\n                'neutral': scores['neu']\r\n            }\r\n        elif self.method == 'textblob':\r\n            blob = TextBlob(text)\r\n            return {\r\n                'polarity': blob.sentiment.polarity,\r\n                'subjectivity': blob.sentiment.subjectivity\r\n            }\r\n\r\n    def analyze_batch(self, texts):\r\n        \"\"\"Analyze list of texts.\"\"\"\r\n        results = []\r\n        for i, text in enumerate(texts):\r\n            result = self.analyze_one(text)\r\n            result['doc_id'] = i\r\n            result['text'] = text[:100]\r\n            results.append(result)\r\n\r\n        return pd.DataFrame(results)\r\n\r\n    def get_coverage(self, texts):\r\n        \"\"\"Calculate coverage statistics.\"\"\"\r\n        if self.method != 'vader':\r\n            raise NotImplementedError(\"Coverage only for VADER\")\r\n\r\n        return check_coverage(texts, self.analyzer)\r\n\r\n# Usage\r\nsa = SentimentAnalyzer(method='vader')\r\nresults = sa.analyze_batch(texts)\r\nprint(results)\r\n\r\n# Summary statistics\r\nprint(f\"\\nSummary:\")\r\nprint(f\"Mean sentiment: {results['compound'].mean():.3f}\")\r\nprint(f\"Std sentiment: {results['compound'].std():.3f}\")\r\nprint(f\"Positive docs: {(results['compound'] > 0.05).sum()}\")\r\nprint(f\"Negative docs: {(results['compound'] < -0.05).sum()}\")\r\nprint(f\"Neutral docs: {((results['compound'] >= -0.05) & (results['compound'] <= 0.05)).sum()}\")\r\n```\r\n",
        "plugins/text-analyst/skills/text-analyst/python-techniques/03_topic_models.md": "# Topic Models in Python\r\n\r\n## Package Versions\r\n\r\n```python\r\n# Tested with:\r\n# Python 3.10\r\n# gensim 4.3.1\r\n# bertopic 0.15.0\r\n# scikit-learn 1.3.0\r\n```\r\n\r\n## Installation\r\n\r\n```bash\r\npip install gensim bertopic scikit-learn pyLDAvis\r\n\r\n# For BERTopic with sentence-transformers\r\npip install sentence-transformers\r\n```\r\n\r\n## Gensim LDA\r\n\r\n### Preparing Data\r\n\r\n```python\r\nimport gensim\r\nfrom gensim import corpora\r\nfrom gensim.models import LdaModel, CoherenceModel\r\nfrom gensim.parsing.preprocessing import STOPWORDS\r\nimport pandas as pd\r\n\r\n# Sample texts\r\ntexts = [\r\n    \"The economy shows signs of growth with increased employment.\",\r\n    \"Healthcare reform remains a divisive political issue.\",\r\n    \"Climate change impacts are becoming more severe.\",\r\n    \"Education funding varies widely across states.\",\r\n    \"Technology transforms modern communication methods.\",\r\n] * 20  # Replicate for minimum corpus size\r\n\r\n# Preprocessing\r\ndef preprocess(text):\r\n    \"\"\"Simple preprocessing.\"\"\"\r\n    tokens = gensim.utils.simple_preprocess(text, deacc=True)\r\n    return [t for t in tokens if t not in STOPWORDS and len(t) > 2]\r\n\r\nprocessed_texts = [preprocess(t) for t in texts]\r\n\r\n# Create dictionary\r\ndictionary = corpora.Dictionary(processed_texts)\r\n\r\n# Filter extremes\r\ndictionary.filter_extremes(\r\n    no_below=2,    # Minimum documents\r\n    no_above=0.9   # Maximum proportion\r\n)\r\n\r\n# Create corpus (bag of words)\r\ncorpus = [dictionary.doc2bow(doc) for doc in processed_texts]\r\n\r\nprint(f\"Dictionary size: {len(dictionary)}\")\r\nprint(f\"Corpus size: {len(corpus)}\")\r\n```\r\n\r\n### Training LDA\r\n\r\n```python\r\n# Set random seed for reproducibility\r\nimport random\r\nimport numpy as np\r\nrandom.seed(42)\r\nnp.random.seed(42)\r\n\r\n# Train LDA model\r\nlda_model = LdaModel(\r\n    corpus=corpus,\r\n    id2word=dictionary,\r\n    num_topics=5,\r\n    random_state=42,\r\n    passes=10,        # Epochs\r\n    alpha='auto',     # Learn alpha\r\n    eta='auto',       # Learn eta\r\n    per_word_topics=True\r\n)\r\n\r\n# View topics\r\nfor idx, topic in lda_model.print_topics(num_words=10):\r\n    print(f\"Topic {idx}: {topic}\")\r\n```\r\n\r\n### Coherence Score\r\n\r\n```python\r\n# Calculate coherence\r\ncoherence_model = CoherenceModel(\r\n    model=lda_model,\r\n    texts=processed_texts,\r\n    dictionary=dictionary,\r\n    coherence='c_v'\r\n)\r\n\r\ncoherence_score = coherence_model.get_coherence()\r\nprint(f\"Coherence Score (C_V): {coherence_score:.4f}\")\r\n\r\n# Alternative coherence metrics\r\nfor metric in ['u_mass', 'c_v', 'c_npmi']:\r\n    cm = CoherenceModel(\r\n        model=lda_model,\r\n        texts=processed_texts,\r\n        dictionary=dictionary,\r\n        coherence=metric\r\n    )\r\n    print(f\"Coherence ({metric}): {cm.get_coherence():.4f}\")\r\n```\r\n\r\n### Selecting K\r\n\r\n```python\r\ndef compute_coherence_values(dictionary, corpus, texts, k_range):\r\n    \"\"\"Compute coherence for range of K values.\"\"\"\r\n    results = []\r\n\r\n    for k in k_range:\r\n        model = LdaModel(\r\n            corpus=corpus,\r\n            id2word=dictionary,\r\n            num_topics=k,\r\n            random_state=42,\r\n            passes=10\r\n        )\r\n\r\n        coherence = CoherenceModel(\r\n            model=model,\r\n            texts=texts,\r\n            dictionary=dictionary,\r\n            coherence='c_v'\r\n        ).get_coherence()\r\n\r\n        results.append({\r\n            'k': k,\r\n            'coherence': coherence\r\n        })\r\n        print(f\"K={k}: Coherence={coherence:.4f}\")\r\n\r\n    return pd.DataFrame(results)\r\n\r\n# Search over K\r\nk_results = compute_coherence_values(\r\n    dictionary, corpus, processed_texts,\r\n    k_range=range(3, 15, 2)\r\n)\r\n\r\n# Plot\r\nimport matplotlib.pyplot as plt\r\nplt.figure(figsize=(10, 5))\r\nplt.plot(k_results['k'], k_results['coherence'], 'o-')\r\nplt.xlabel('Number of Topics (K)')\r\nplt.ylabel('Coherence Score')\r\nplt.title('Topic Model Coherence by K')\r\nplt.savefig('output/figures/coherence_k.png', dpi=300)\r\nplt.close()\r\n```\r\n\r\n### Document-Topic Distribution\r\n\r\n```python\r\ndef get_document_topics(model, corpus):\r\n    \"\"\"Get topic distribution for each document.\"\"\"\r\n    doc_topics = []\r\n\r\n    for doc_bow in corpus:\r\n        topic_dist = model.get_document_topics(doc_bow, minimum_probability=0)\r\n        topic_dict = {f'topic_{t}': p for t, p in topic_dist}\r\n        doc_topics.append(topic_dict)\r\n\r\n    return pd.DataFrame(doc_topics)\r\n\r\ndoc_topics_df = get_document_topics(lda_model, corpus)\r\nprint(doc_topics_df.head())\r\n\r\n# Dominant topic per document\r\ndoc_topics_df['dominant_topic'] = doc_topics_df.idxmax(axis=1)\r\n```\r\n\r\n## BERTopic\r\n\r\n### Basic BERTopic\r\n\r\n```python\r\nfrom bertopic import BERTopic\r\nfrom sentence_transformers import SentenceTransformer\r\n\r\n# Simple usage (uses default embedding model)\r\ntopic_model = BERTopic(language=\"english\", verbose=True)\r\ntopics, probs = topic_model.fit_transform(texts)\r\n\r\n# View topics\r\ntopic_model.get_topic_info()\r\n```\r\n\r\n### BERTopic with Custom Settings\r\n\r\n```python\r\nfrom umap import UMAP\r\nfrom hdbscan import HDBSCAN\r\nfrom sklearn.feature_extraction.text import CountVectorizer\r\n\r\n# Custom embedding model\r\nembedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\r\n\r\n# Custom UMAP\r\numap_model = UMAP(\r\n    n_neighbors=15,\r\n    n_components=5,\r\n    min_dist=0.0,\r\n    metric='cosine',\r\n    random_state=42\r\n)\r\n\r\n# Custom HDBSCAN\r\nhdbscan_model = HDBSCAN(\r\n    min_cluster_size=10,\r\n    min_samples=5,\r\n    metric='euclidean',\r\n    prediction_data=True\r\n)\r\n\r\n# Custom vectorizer for topic representation\r\nvectorizer_model = CountVectorizer(\r\n    stop_words=\"english\",\r\n    ngram_range=(1, 2)\r\n)\r\n\r\n# Create model\r\ntopic_model = BERTopic(\r\n    embedding_model=embedding_model,\r\n    umap_model=umap_model,\r\n    hdbscan_model=hdbscan_model,\r\n    vectorizer_model=vectorizer_model,\r\n    top_n_words=10,\r\n    verbose=True\r\n)\r\n\r\n# Fit\r\ntopics, probs = topic_model.fit_transform(texts)\r\n```\r\n\r\n### Examining BERTopic Results\r\n\r\n```python\r\n# Topic info\r\ntopic_info = topic_model.get_topic_info()\r\nprint(topic_info)\r\n\r\n# Specific topic words\r\ntopic_0_words = topic_model.get_topic(0)\r\nprint(f\"Topic 0: {topic_0_words}\")\r\n\r\n# Document-topic mapping\r\ndoc_info = topic_model.get_document_info(texts)\r\nprint(doc_info.head())\r\n\r\n# Topic frequency\r\ntopic_model.visualize_barchart(top_n_topics=10)\r\n```\r\n\r\n### BERTopic Visualizations\r\n\r\n```python\r\n# Topic word scores\r\nfig = topic_model.visualize_barchart(top_n_topics=8)\r\nfig.write_html(\"output/figures/bertopic_barchart.html\")\r\n\r\n# Topic similarity\r\nfig = topic_model.visualize_heatmap()\r\nfig.write_html(\"output/figures/bertopic_heatmap.html\")\r\n\r\n# Document clusters\r\nfig = topic_model.visualize_documents(texts)\r\nfig.write_html(\"output/figures/bertopic_documents.html\")\r\n\r\n# Topic hierarchy\r\nfig = topic_model.visualize_hierarchy()\r\nfig.write_html(\"output/figures/bertopic_hierarchy.html\")\r\n```\r\n\r\n### Topics Over Time\r\n\r\n```python\r\nimport pandas as pd\r\nfrom datetime import datetime, timedelta\r\n\r\n# Sample with timestamps\r\ndates = [datetime(2020, 1, 1) + timedelta(days=i) for i in range(len(texts))]\r\ntimestamps = [d.strftime(\"%Y-%m\") for d in dates]\r\n\r\n# Dynamic topics\r\ntopics_over_time = topic_model.topics_over_time(\r\n    texts,\r\n    timestamps,\r\n    nr_bins=12\r\n)\r\n\r\n# Visualize\r\nfig = topic_model.visualize_topics_over_time(topics_over_time)\r\nfig.write_html(\"output/figures/topics_over_time.html\")\r\n```\r\n\r\n## pyLDAvis for Gensim\r\n\r\n```python\r\nimport pyLDAvis\r\nimport pyLDAvis.gensim_models\r\n\r\n# Prepare visualization data\r\nvis_data = pyLDAvis.gensim_models.prepare(\r\n    lda_model,\r\n    corpus,\r\n    dictionary,\r\n    sort_topics=False\r\n)\r\n\r\n# Save as HTML\r\npyLDAvis.save_html(vis_data, 'output/figures/lda_vis.html')\r\n```\r\n\r\n## Validation\r\n\r\n### Word Intrusion Test\r\n\r\n```python\r\ndef generate_intrusion_test(model, num_topics=None, n_words=5):\r\n    \"\"\"Generate word intrusion test for topic validation.\"\"\"\r\n    if num_topics is None:\r\n        num_topics = model.num_topics\r\n\r\n    tests = []\r\n\r\n    for topic_id in range(num_topics):\r\n        # Get top words for this topic\r\n        top_words = [w for w, _ in model.show_topic(topic_id, topn=n_words)]\r\n\r\n        # Get intruder from different topic\r\n        intruder_topic = (topic_id + 1) % num_topics\r\n        intruder_words = [w for w, _ in model.show_topic(intruder_topic, topn=n_words)]\r\n        intruder = random.choice(intruder_words)\r\n\r\n        # Combine and shuffle\r\n        all_words = top_words + [intruder]\r\n        random.shuffle(all_words)\r\n\r\n        tests.append({\r\n            'topic': topic_id,\r\n            'words': all_words,\r\n            'intruder': intruder\r\n        })\r\n\r\n    return tests\r\n\r\ntests = generate_intrusion_test(lda_model)\r\nfor test in tests[:3]:\r\n    print(f\"Topic {test['topic']}: {test['words']}\")\r\n    print(f\"  (Intruder: {test['intruder']})\\n\")\r\n```\r\n\r\n### Robustness: Multiple Seeds\r\n\r\n```python\r\ndef compare_seeds(corpus, dictionary, k, seeds):\r\n    \"\"\"Compare topic models across random seeds.\"\"\"\r\n    models = []\r\n\r\n    for seed in seeds:\r\n        model = LdaModel(\r\n            corpus=corpus,\r\n            id2word=dictionary,\r\n            num_topics=k,\r\n            random_state=seed,\r\n            passes=10\r\n        )\r\n        models.append(model)\r\n\r\n    # Get top words for each model\r\n    all_top_words = []\r\n    for i, model in enumerate(models):\r\n        topics = []\r\n        for topic_id in range(k):\r\n            words = [w for w, _ in model.show_topic(topic_id, topn=10)]\r\n            topics.append(set(words))\r\n        all_top_words.append(topics)\r\n\r\n    return models, all_top_words\r\n\r\nmodels, all_words = compare_seeds(corpus, dictionary, k=5, seeds=[42, 123, 456])\r\n```\r\n\r\n## Output: Publication Tables\r\n\r\n### Topic Summary Table\r\n\r\n```python\r\ndef create_topic_table(model, n_words=10):\r\n    \"\"\"Create publication-ready topic table.\"\"\"\r\n    topics = []\r\n\r\n    for topic_id in range(model.num_topics):\r\n        words = model.show_topic(topic_id, topn=n_words)\r\n        word_str = \", \".join([w for w, _ in words])\r\n\r\n        topics.append({\r\n            'Topic': topic_id + 1,\r\n            'Top Words': word_str\r\n        })\r\n\r\n    return pd.DataFrame(topics)\r\n\r\ntopic_table = create_topic_table(lda_model)\r\ntopic_table.to_csv('output/tables/topic_summary.csv', index=False)\r\nprint(topic_table)\r\n```\r\n\r\n### Document-Topic Table\r\n\r\n```python\r\ndef create_doc_topic_table(model, corpus, texts, n_docs=5):\r\n    \"\"\"Create document-topic assignments table.\"\"\"\r\n    results = []\r\n\r\n    for i, (doc, text) in enumerate(zip(corpus, texts)):\r\n        topic_dist = model.get_document_topics(doc)\r\n        dominant = max(topic_dist, key=lambda x: x[1])\r\n\r\n        results.append({\r\n            'doc_id': i,\r\n            'text_preview': text[:100],\r\n            'dominant_topic': dominant[0] + 1,\r\n            'probability': dominant[1]\r\n        })\r\n\r\n    df = pd.DataFrame(results)\r\n\r\n    # Sample top docs per topic\r\n    sample = df.groupby('dominant_topic').apply(\r\n        lambda x: x.nlargest(n_docs, 'probability')\r\n    ).reset_index(drop=True)\r\n\r\n    return sample\r\n\r\ndoc_table = create_doc_topic_table(lda_model, corpus, texts)\r\n```\r\n\r\n## Complete Workflow\r\n\r\n```python\r\nclass TopicModelPipeline:\r\n    \"\"\"Complete topic modeling pipeline.\"\"\"\r\n\r\n    def __init__(self, method='lda'):\r\n        self.method = method\r\n        self.model = None\r\n        self.dictionary = None\r\n        self.corpus = None\r\n\r\n    def preprocess(self, texts):\r\n        \"\"\"Preprocess texts.\"\"\"\r\n        processed = [preprocess(t) for t in texts]\r\n        return processed\r\n\r\n    def fit_lda(self, texts, k=10, seed=42):\r\n        \"\"\"Fit LDA model.\"\"\"\r\n        processed = self.preprocess(texts)\r\n\r\n        self.dictionary = corpora.Dictionary(processed)\r\n        self.dictionary.filter_extremes(no_below=5, no_above=0.5)\r\n        self.corpus = [self.dictionary.doc2bow(doc) for doc in processed]\r\n\r\n        self.model = LdaModel(\r\n            corpus=self.corpus,\r\n            id2word=self.dictionary,\r\n            num_topics=k,\r\n            random_state=seed,\r\n            passes=15\r\n        )\r\n\r\n        # Calculate coherence\r\n        coherence = CoherenceModel(\r\n            model=self.model,\r\n            texts=processed,\r\n            dictionary=self.dictionary,\r\n            coherence='c_v'\r\n        ).get_coherence()\r\n\r\n        return {'model': self.model, 'coherence': coherence}\r\n\r\n    def fit_bertopic(self, texts, min_topic_size=10):\r\n        \"\"\"Fit BERTopic model.\"\"\"\r\n        self.model = BERTopic(\r\n            min_topic_size=min_topic_size,\r\n            verbose=True\r\n        )\r\n        topics, probs = self.model.fit_transform(texts)\r\n        return {'model': self.model, 'topics': topics, 'probs': probs}\r\n\r\n    def get_topics(self, n_words=10):\r\n        \"\"\"Get topic-word distributions.\"\"\"\r\n        if self.method == 'lda':\r\n            return create_topic_table(self.model, n_words)\r\n        else:\r\n            return self.model.get_topic_info()\r\n\r\n    def save(self, path):\r\n        \"\"\"Save model.\"\"\"\r\n        if self.method == 'lda':\r\n            self.model.save(f\"{path}/lda_model\")\r\n            self.dictionary.save(f\"{path}/dictionary\")\r\n        else:\r\n            self.model.save(f\"{path}/bertopic_model\")\r\n\r\n# Usage\r\npipeline = TopicModelPipeline(method='lda')\r\nresult = pipeline.fit_lda(texts, k=5, seed=42)\r\nprint(f\"Coherence: {result['coherence']:.4f}\")\r\nprint(pipeline.get_topics())\r\n```\r\n",
        "plugins/text-analyst/skills/text-analyst/python-techniques/04_supervised.md": "# Supervised Text Classification in Python\r\n\r\n## Package Versions\r\n\r\n```python\r\n# Tested with:\r\n# Python 3.10\r\n# scikit-learn 1.3.0\r\n# transformers 4.30.0\r\n# torch 2.0.0\r\n```\r\n\r\n## Installation\r\n\r\n```bash\r\npip install scikit-learn transformers torch pandas numpy\r\n\r\n# For GPU support\r\npip install torch --index-url https://download.pytorch.org/whl/cu118\r\n```\r\n\r\n## sklearn Workflow\r\n\r\n### Sample Data\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\r\nfrom sklearn.feature_extraction.text import TfidfVectorizer\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.svm import LinearSVC\r\nfrom sklearn.naive_bayes import MultinomialNB\r\nfrom sklearn.metrics import classification_report, confusion_matrix\r\nimport random\r\n\r\nrandom.seed(42)\r\nnp.random.seed(42)\r\n\r\n# Sample labeled data\r\ntexts = [\r\n    \"The government passed new tax legislation today.\",\r\n    \"The championship game ended in an exciting overtime.\",\r\n    \"Stock markets reached record highs amid economic growth.\",\r\n    \"Scientists discovered a new species in the rainforest.\",\r\n] * 50\r\n\r\nlabels = [\"politics\", \"sports\", \"business\", \"science\"] * 50\r\n\r\n# Shuffle\r\ncombined = list(zip(texts, labels))\r\nrandom.shuffle(combined)\r\ntexts, labels = zip(*combined)\r\ntexts, labels = list(texts), list(labels)\r\n```\r\n\r\n### Train/Test Split\r\n\r\n```python\r\n# Stratified split\r\nX_train, X_test, y_train, y_test = train_test_split(\r\n    texts, labels,\r\n    test_size=0.2,\r\n    random_state=42,\r\n    stratify=labels\r\n)\r\n\r\nprint(f\"Training: {len(X_train)}, Test: {len(X_test)}\")\r\nprint(f\"Class distribution (train): {pd.Series(y_train).value_counts().to_dict()}\")\r\n```\r\n\r\n### Feature Extraction\r\n\r\n```python\r\n# TF-IDF Vectorizer\r\ntfidf = TfidfVectorizer(\r\n    max_features=5000,\r\n    min_df=2,\r\n    max_df=0.95,\r\n    ngram_range=(1, 2),\r\n    stop_words='english',\r\n    sublinear_tf=True\r\n)\r\n\r\n# Fit on training data only\r\nX_train_tfidf = tfidf.fit_transform(X_train)\r\nX_test_tfidf = tfidf.transform(X_test)\r\n\r\nprint(f\"Feature matrix shape: {X_train_tfidf.shape}\")\r\n```\r\n\r\n### Training Models\r\n\r\n```python\r\n# Logistic Regression\r\nlr_model = LogisticRegression(\r\n    C=1.0,\r\n    max_iter=1000,\r\n    random_state=42,\r\n    class_weight='balanced'\r\n)\r\nlr_model.fit(X_train_tfidf, y_train)\r\n\r\n# SVM\r\nsvm_model = LinearSVC(\r\n    C=1.0,\r\n    max_iter=1000,\r\n    random_state=42,\r\n    class_weight='balanced'\r\n)\r\nsvm_model.fit(X_train_tfidf, y_train)\r\n\r\n# Naive Bayes\r\nnb_model = MultinomialNB(alpha=0.1)\r\nnb_model.fit(X_train_tfidf, y_train)\r\n```\r\n\r\n### Evaluation\r\n\r\n```python\r\ndef evaluate_model(model, X_test, y_test, model_name=\"Model\"):\r\n    \"\"\"Evaluate classification model.\"\"\"\r\n    y_pred = model.predict(X_test)\r\n\r\n    print(f\"\\n{model_name} Results:\")\r\n    print(\"=\" * 50)\r\n    print(classification_report(y_test, y_pred))\r\n\r\n    # Confusion matrix\r\n    cm = confusion_matrix(y_test, y_pred)\r\n    print(\"\\nConfusion Matrix:\")\r\n    print(cm)\r\n\r\n    return y_pred\r\n\r\n# Evaluate all models\r\nlr_pred = evaluate_model(lr_model, X_test_tfidf, y_test, \"Logistic Regression\")\r\nsvm_pred = evaluate_model(svm_model, X_test_tfidf, y_test, \"SVM\")\r\nnb_pred = evaluate_model(nb_model, X_test_tfidf, y_test, \"Naive Bayes\")\r\n```\r\n\r\n### Cross-Validation\r\n\r\n```python\r\n# Stratified K-fold\r\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\r\n\r\n# Cross-validation scores\r\nfrom sklearn.metrics import make_scorer, f1_score\r\n\r\n# Create TF-IDF on full data for CV\r\ntfidf_full = TfidfVectorizer(\r\n    max_features=5000,\r\n    min_df=2,\r\n    max_df=0.95,\r\n    ngram_range=(1, 2),\r\n    stop_words='english'\r\n)\r\nX_full = tfidf_full.fit_transform(texts)\r\n\r\n# F1 macro scorer\r\nf1_macro = make_scorer(f1_score, average='macro')\r\n\r\n# CV scores for each model\r\nfor name, model in [('LR', LogisticRegression(max_iter=1000, random_state=42)),\r\n                    ('SVM', LinearSVC(max_iter=1000, random_state=42)),\r\n                    ('NB', MultinomialNB())]:\r\n    scores = cross_val_score(model, X_full, labels, cv=cv, scoring=f1_macro)\r\n    print(f\"{name}: F1-macro = {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\")\r\n```\r\n\r\n### Hyperparameter Tuning\r\n\r\n```python\r\nfrom sklearn.model_selection import GridSearchCV\r\nfrom sklearn.pipeline import Pipeline\r\n\r\n# Create pipeline\r\npipeline = Pipeline([\r\n    ('tfidf', TfidfVectorizer(stop_words='english')),\r\n    ('clf', LogisticRegression(max_iter=1000, random_state=42))\r\n])\r\n\r\n# Parameter grid\r\nparam_grid = {\r\n    'tfidf__max_features': [1000, 5000],\r\n    'tfidf__ngram_range': [(1, 1), (1, 2)],\r\n    'clf__C': [0.1, 1.0, 10.0]\r\n}\r\n\r\n# Grid search\r\ngrid_search = GridSearchCV(\r\n    pipeline,\r\n    param_grid,\r\n    cv=5,\r\n    scoring='f1_macro',\r\n    n_jobs=-1,\r\n    verbose=1\r\n)\r\n\r\ngrid_search.fit(X_train, y_train)\r\n\r\nprint(f\"Best parameters: {grid_search.best_params_}\")\r\nprint(f\"Best CV score: {grid_search.best_score_:.3f}\")\r\n\r\n# Evaluate on test\r\nbest_pred = grid_search.predict(X_test)\r\nprint(classification_report(y_test, best_pred))\r\n```\r\n\r\n### Feature Importance\r\n\r\n```python\r\ndef get_top_features(model, vectorizer, n=20):\r\n    \"\"\"Get top features for each class.\"\"\"\r\n    feature_names = vectorizer.get_feature_names_out()\r\n\r\n    # For logistic regression\r\n    if hasattr(model, 'coef_'):\r\n        results = {}\r\n        for i, class_name in enumerate(model.classes_):\r\n            # Get coefficients for this class\r\n            coefs = model.coef_[i] if len(model.classes_) > 2 else model.coef_[0]\r\n\r\n            # Top positive features\r\n            top_idx = np.argsort(coefs)[-n:][::-1]\r\n            results[class_name] = [(feature_names[j], coefs[j]) for j in top_idx]\r\n\r\n        return results\r\n\r\n    return None\r\n\r\ntop_features = get_top_features(lr_model, tfidf)\r\nfor class_name, features in top_features.items():\r\n    print(f\"\\n{class_name}:\")\r\n    for word, coef in features[:10]:\r\n        print(f\"  {word}: {coef:.3f}\")\r\n```\r\n\r\n## Transformers / BERT\r\n\r\n### Using Pretrained Sentiment Model\r\n\r\n```python\r\nfrom transformers import pipeline\r\n\r\n# Zero-shot classification\r\nclassifier = pipeline(\"zero-shot-classification\",\r\n                      model=\"facebook/bart-large-mnli\")\r\n\r\n# Classify without training\r\ntext = \"The stock market crashed dramatically today\"\r\nlabels_zs = [\"politics\", \"sports\", \"business\", \"science\"]\r\n\r\nresult = classifier(text, labels_zs)\r\nprint(f\"Text: {text}\")\r\nprint(f\"Predicted: {result['labels'][0]} ({result['scores'][0]:.3f})\")\r\n```\r\n\r\n### Fine-tuning BERT\r\n\r\n```python\r\nfrom transformers import (\r\n    AutoTokenizer,\r\n    AutoModelForSequenceClassification,\r\n    TrainingArguments,\r\n    Trainer\r\n)\r\nfrom datasets import Dataset\r\nimport torch\r\n\r\n# Prepare data\r\ntrain_df = pd.DataFrame({'text': X_train, 'label': y_train})\r\ntest_df = pd.DataFrame({'text': X_test, 'label': y_test})\r\n\r\n# Create label mapping\r\nlabel2id = {l: i for i, l in enumerate(set(labels))}\r\nid2label = {i: l for l, i in label2id.items()}\r\n\r\ntrain_df['label'] = train_df['label'].map(label2id)\r\ntest_df['label'] = test_df['label'].map(label2id)\r\n\r\n# Convert to HuggingFace Dataset\r\ntrain_dataset = Dataset.from_pandas(train_df)\r\ntest_dataset = Dataset.from_pandas(test_df)\r\n\r\n# Load tokenizer and model\r\nmodel_name = \"distilbert-base-uncased\"\r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\n\r\ndef tokenize_function(examples):\r\n    return tokenizer(\r\n        examples[\"text\"],\r\n        padding=\"max_length\",\r\n        truncation=True,\r\n        max_length=128\r\n    )\r\n\r\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\r\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\r\n\r\n# Load model\r\nmodel = AutoModelForSequenceClassification.from_pretrained(\r\n    model_name,\r\n    num_labels=len(label2id),\r\n    id2label=id2label,\r\n    label2id=label2id\r\n)\r\n\r\n# Training arguments\r\ntraining_args = TrainingArguments(\r\n    output_dir=\"./results\",\r\n    num_train_epochs=3,\r\n    per_device_train_batch_size=16,\r\n    per_device_eval_batch_size=16,\r\n    warmup_steps=100,\r\n    weight_decay=0.01,\r\n    logging_dir=\"./logs\",\r\n    logging_steps=10,\r\n    evaluation_strategy=\"epoch\",\r\n    save_strategy=\"epoch\",\r\n    load_best_model_at_end=True,\r\n)\r\n\r\n# Trainer\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n    train_dataset=train_dataset,\r\n    eval_dataset=test_dataset,\r\n)\r\n\r\n# Train\r\ntrainer.train()\r\n\r\n# Evaluate\r\nresults = trainer.evaluate()\r\nprint(results)\r\n```\r\n\r\n### Using Fine-tuned Model\r\n\r\n```python\r\n# Make predictions\r\ndef predict_text(text, model, tokenizer):\r\n    inputs = tokenizer(\r\n        text,\r\n        return_tensors=\"pt\",\r\n        truncation=True,\r\n        max_length=128,\r\n        padding=True\r\n    )\r\n\r\n    with torch.no_grad():\r\n        outputs = model(**inputs)\r\n        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\r\n        predicted_class = predictions.argmax().item()\r\n\r\n    return {\r\n        'label': id2label[predicted_class],\r\n        'confidence': predictions.max().item()\r\n    }\r\n\r\n# Example\r\nresult = predict_text(\"The team won the championship\", model, tokenizer)\r\nprint(result)\r\n```\r\n\r\n## Handling Class Imbalance\r\n\r\n```python\r\nfrom sklearn.utils.class_weight import compute_class_weight\r\nfrom imblearn.over_sampling import SMOTE\r\nfrom imblearn.under_sampling import RandomUnderSampler\r\n\r\n# Option 1: Class weights\r\nclass_weights = compute_class_weight(\r\n    'balanced',\r\n    classes=np.unique(y_train),\r\n    y=y_train\r\n)\r\nclass_weight_dict = dict(zip(np.unique(y_train), class_weights))\r\n\r\nlr_balanced = LogisticRegression(\r\n    class_weight=class_weight_dict,\r\n    max_iter=1000,\r\n    random_state=42\r\n)\r\n\r\n# Option 2: SMOTE (need numeric features)\r\n# smote = SMOTE(random_state=42)\r\n# X_resampled, y_resampled = smote.fit_resample(X_train_tfidf, y_train)\r\n\r\n# Option 3: Undersampling\r\n# undersampler = RandomUnderSampler(random_state=42)\r\n# X_resampled, y_resampled = undersampler.fit_resample(X_train_tfidf, y_train)\r\n```\r\n\r\n## Error Analysis\r\n\r\n```python\r\ndef error_analysis(y_true, y_pred, texts, n_samples=5):\r\n    \"\"\"Analyze classification errors.\"\"\"\r\n    errors = []\r\n\r\n    for i, (true, pred, text) in enumerate(zip(y_true, y_pred, texts)):\r\n        if true != pred:\r\n            errors.append({\r\n                'index': i,\r\n                'text': text[:200],\r\n                'true_label': true,\r\n                'predicted': pred\r\n            })\r\n\r\n    errors_df = pd.DataFrame(errors)\r\n\r\n    print(f\"Total errors: {len(errors_df)}\")\r\n    print(f\"\\nError distribution:\")\r\n    print(errors_df.groupby(['true_label', 'predicted']).size())\r\n\r\n    print(f\"\\nSample errors:\")\r\n    for _, row in errors_df.head(n_samples).iterrows():\r\n        print(f\"\\nTrue: {row['true_label']}, Predicted: {row['predicted']}\")\r\n        print(f\"Text: {row['text']}\")\r\n\r\n    return errors_df\r\n\r\nerrors = error_analysis(y_test, lr_pred, X_test)\r\n```\r\n\r\n## Complete Pipeline\r\n\r\n```python\r\nfrom sklearn.base import BaseEstimator, TransformerMixin\r\n\r\nclass TextClassificationPipeline:\r\n    \"\"\"Complete text classification pipeline.\"\"\"\r\n\r\n    def __init__(self, model_type='logistic', max_features=5000):\r\n        self.model_type = model_type\r\n        self.max_features = max_features\r\n        self.vectorizer = None\r\n        self.model = None\r\n\r\n    def _get_model(self):\r\n        if self.model_type == 'logistic':\r\n            return LogisticRegression(\r\n                max_iter=1000,\r\n                random_state=42,\r\n                class_weight='balanced'\r\n            )\r\n        elif self.model_type == 'svm':\r\n            return LinearSVC(\r\n                max_iter=1000,\r\n                random_state=42,\r\n                class_weight='balanced'\r\n            )\r\n        elif self.model_type == 'nb':\r\n            return MultinomialNB()\r\n        else:\r\n            raise ValueError(f\"Unknown model type: {self.model_type}\")\r\n\r\n    def fit(self, X_train, y_train):\r\n        \"\"\"Fit the pipeline.\"\"\"\r\n        self.vectorizer = TfidfVectorizer(\r\n            max_features=self.max_features,\r\n            min_df=2,\r\n            max_df=0.95,\r\n            ngram_range=(1, 2),\r\n            stop_words='english'\r\n        )\r\n\r\n        X_train_vec = self.vectorizer.fit_transform(X_train)\r\n        self.model = self._get_model()\r\n        self.model.fit(X_train_vec, y_train)\r\n\r\n        return self\r\n\r\n    def predict(self, X):\r\n        \"\"\"Predict labels.\"\"\"\r\n        X_vec = self.vectorizer.transform(X)\r\n        return self.model.predict(X_vec)\r\n\r\n    def evaluate(self, X_test, y_test):\r\n        \"\"\"Evaluate the model.\"\"\"\r\n        y_pred = self.predict(X_test)\r\n\r\n        report = classification_report(y_test, y_pred, output_dict=True)\r\n        cm = confusion_matrix(y_test, y_pred)\r\n\r\n        return {\r\n            'classification_report': report,\r\n            'confusion_matrix': cm,\r\n            'predictions': y_pred\r\n        }\r\n\r\n    def cross_validate(self, X, y, cv=5):\r\n        \"\"\"Cross-validate the model.\"\"\"\r\n        pipeline = Pipeline([\r\n            ('tfidf', TfidfVectorizer(\r\n                max_features=self.max_features,\r\n                min_df=2,\r\n                stop_words='english'\r\n            )),\r\n            ('clf', self._get_model())\r\n        ])\r\n\r\n        scores = cross_val_score(\r\n            pipeline, X, y,\r\n            cv=cv,\r\n            scoring='f1_macro'\r\n        )\r\n\r\n        return {\r\n            'mean': scores.mean(),\r\n            'std': scores.std(),\r\n            'scores': scores\r\n        }\r\n\r\n# Usage\r\npipeline = TextClassificationPipeline(model_type='logistic')\r\npipeline.fit(X_train, y_train)\r\nresults = pipeline.evaluate(X_test, y_test)\r\nprint(classification_report(y_test, results['predictions']))\r\n\r\ncv_results = pipeline.cross_validate(texts, labels)\r\nprint(f\"CV F1-macro: {cv_results['mean']:.3f} (+/- {cv_results['std']*2:.3f})\")\r\n```\r\n",
        "plugins/text-analyst/skills/text-analyst/python-techniques/05_embeddings.md": "# Word and Document Embeddings in Python\r\n\r\n## Package Versions\r\n\r\n```python\r\n# Tested with:\r\n# Python 3.10\r\n# gensim 4.3.1\r\n# sentence-transformers 2.2.2\r\n# numpy 1.24.0\r\n```\r\n\r\n## Installation\r\n\r\n```bash\r\npip install gensim sentence-transformers numpy pandas scikit-learn umap-learn\r\n\r\n# For visualization\r\npip install matplotlib plotly\r\n```\r\n\r\n## Word2Vec with Gensim\r\n\r\n### Training Word2Vec\r\n\r\n```python\r\nfrom gensim.models import Word2Vec\r\nfrom gensim.utils import simple_preprocess\r\nimport numpy as np\r\n\r\n# Sample corpus\r\ntexts = [\r\n    \"The economy shows signs of growth with increased employment.\",\r\n    \"Healthcare reform remains a divisive political issue.\",\r\n    \"Climate change impacts are becoming more severe.\",\r\n    \"Education funding varies widely across states.\",\r\n    \"Machine learning is transforming data analysis.\",\r\n    \"Social media influences political discourse.\"\r\n] * 20  # Need more data for meaningful embeddings\r\n\r\n# Tokenize\r\ntokenized = [simple_preprocess(text) for text in texts]\r\n\r\n# Train Word2Vec\r\nmodel = Word2Vec(\r\n    sentences=tokenized,\r\n    vector_size=100,    # Embedding dimensions\r\n    window=5,           # Context window\r\n    min_count=2,        # Minimum word frequency\r\n    workers=4,          # Parallel threads\r\n    epochs=10,          # Training epochs\r\n    seed=42\r\n)\r\n\r\nprint(f\"Vocabulary size: {len(model.wv)}\")\r\n```\r\n\r\n### Using Word2Vec\r\n\r\n```python\r\n# Get word vector\r\nvector = model.wv['economy']\r\nprint(f\"Shape: {vector.shape}\")\r\n\r\n# Find similar words\r\nsimilar = model.wv.most_similar('economy', topn=10)\r\nprint(\"Similar to 'economy':\")\r\nfor word, score in similar:\r\n    print(f\"  {word}: {score:.3f}\")\r\n\r\n# Word analogies (need larger corpus for good results)\r\n# result = model.wv.most_similar(positive=['woman', 'king'], negative=['man'])\r\n\r\n# Check if word in vocabulary\r\nprint(f\"'economy' in vocab: {'economy' in model.wv}\")\r\n```\r\n\r\n### Document Embeddings (Average)\r\n\r\n```python\r\ndef get_doc_embedding(text, model, dim=100):\r\n    \"\"\"Average word vectors for document embedding.\"\"\"\r\n    words = simple_preprocess(text)\r\n    word_vecs = []\r\n\r\n    for word in words:\r\n        if word in model.wv:\r\n            word_vecs.append(model.wv[word])\r\n\r\n    if not word_vecs:\r\n        return np.zeros(dim)\r\n\r\n    return np.mean(word_vecs, axis=0)\r\n\r\n# Get embeddings for all documents\r\ndoc_embeddings = np.array([get_doc_embedding(text, model) for text in texts])\r\nprint(f\"Document embeddings shape: {doc_embeddings.shape}\")\r\n```\r\n\r\n### Save and Load\r\n\r\n```python\r\n# Save\r\nmodel.save(\"models/word2vec.model\")\r\n\r\n# Load\r\nloaded_model = Word2Vec.load(\"models/word2vec.model\")\r\n```\r\n\r\n## Sentence Transformers (Recommended)\r\n\r\n### Basic Usage\r\n\r\n```python\r\nfrom sentence_transformers import SentenceTransformer\r\n\r\n# Load pretrained model\r\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\r\n\r\n# Encode texts\r\nembeddings = model.encode(texts, show_progress_bar=True)\r\nprint(f\"Embeddings shape: {embeddings.shape}\")\r\n# (N documents, 384 dimensions)\r\n\r\n# Encode single text\r\nsingle_embedding = model.encode(\"This is a test sentence.\")\r\nprint(f\"Single embedding shape: {single_embedding.shape}\")\r\n```\r\n\r\n### Model Options\r\n\r\n```python\r\n# Different models for different needs\r\n\r\n# Fast and good (recommended default)\r\nmodel_fast = SentenceTransformer('all-MiniLM-L6-v2')  # 384 dims\r\n\r\n# Higher quality\r\nmodel_quality = SentenceTransformer('all-mpnet-base-v2')  # 768 dims\r\n\r\n# Multilingual\r\nmodel_multi = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\r\n\r\n# Semantic search optimized\r\nmodel_search = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\r\n```\r\n\r\n### Batch Processing\r\n\r\n```python\r\n# For large corpora\r\ndef encode_batched(texts, model, batch_size=32):\r\n    \"\"\"Encode texts in batches.\"\"\"\r\n    embeddings = model.encode(\r\n        texts,\r\n        batch_size=batch_size,\r\n        show_progress_bar=True,\r\n        convert_to_numpy=True\r\n    )\r\n    return embeddings\r\n\r\nembeddings = encode_batched(texts, model)\r\n```\r\n\r\n## Document Similarity\r\n\r\n### Cosine Similarity\r\n\r\n```python\r\nfrom sklearn.metrics.pairwise import cosine_similarity\r\n\r\n# Compute pairwise similarities\r\nsim_matrix = cosine_similarity(embeddings)\r\n\r\n# Find most similar documents\r\ndef find_similar_docs(doc_idx, sim_matrix, texts, top_n=5):\r\n    \"\"\"Find most similar documents.\"\"\"\r\n    sims = sim_matrix[doc_idx].copy()\r\n    sims[doc_idx] = -1  # Exclude self\r\n\r\n    top_idx = np.argsort(sims)[-top_n:][::-1]\r\n\r\n    results = []\r\n    for idx in top_idx:\r\n        results.append({\r\n            'doc_id': idx,\r\n            'similarity': sims[idx],\r\n            'text': texts[idx][:100]\r\n        })\r\n\r\n    return results\r\n\r\nsimilar = find_similar_docs(0, sim_matrix, texts)\r\nprint(\"Documents similar to doc 0:\")\r\nfor r in similar:\r\n    print(f\"  Doc {r['doc_id']} (sim={r['similarity']:.3f}): {r['text'][:50]}...\")\r\n```\r\n\r\n### Semantic Search\r\n\r\n```python\r\ndef semantic_search(query, doc_embeddings, texts, model, top_n=5):\r\n    \"\"\"Search documents by semantic similarity to query.\"\"\"\r\n    query_embedding = model.encode([query])\r\n\r\n    similarities = cosine_similarity(query_embedding, doc_embeddings)[0]\r\n\r\n    top_idx = np.argsort(similarities)[-top_n:][::-1]\r\n\r\n    results = []\r\n    for idx in top_idx:\r\n        results.append({\r\n            'rank': len(results) + 1,\r\n            'doc_id': idx,\r\n            'similarity': similarities[idx],\r\n            'text': texts[idx][:150]\r\n        })\r\n\r\n    return results\r\n\r\n# Search\r\nresults = semantic_search(\r\n    \"economic growth and jobs\",\r\n    embeddings,\r\n    texts,\r\n    model\r\n)\r\n\r\nprint(\"Search results for 'economic growth and jobs':\")\r\nfor r in results:\r\n    print(f\"  #{r['rank']} (sim={r['similarity']:.3f}): {r['text'][:60]}...\")\r\n```\r\n\r\n## Clustering with Embeddings\r\n\r\n```python\r\nfrom sklearn.cluster import KMeans\r\nfrom sklearn.metrics import silhouette_score\r\n\r\n# K-means clustering\r\nn_clusters = 4\r\nkmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\r\nclusters = kmeans.fit_predict(embeddings)\r\n\r\n# Silhouette score\r\nsilhouette = silhouette_score(embeddings, clusters)\r\nprint(f\"Silhouette score: {silhouette:.3f}\")\r\n\r\n# Cluster distribution\r\nimport pandas as pd\r\ncluster_df = pd.DataFrame({\r\n    'text': [t[:50] for t in texts],\r\n    'cluster': clusters\r\n})\r\nprint(cluster_df['cluster'].value_counts())\r\n```\r\n\r\n## Dimensionality Reduction\r\n\r\n### UMAP\r\n\r\n```python\r\nfrom umap import UMAP\r\nimport matplotlib.pyplot as plt\r\n\r\n# Reduce to 2D\r\numap_model = UMAP(\r\n    n_neighbors=15,\r\n    n_components=2,\r\n    min_dist=0.1,\r\n    metric='cosine',\r\n    random_state=42\r\n)\r\n\r\nembeddings_2d = umap_model.fit_transform(embeddings)\r\n\r\n# Plot\r\nplt.figure(figsize=(10, 8))\r\nscatter = plt.scatter(\r\n    embeddings_2d[:, 0],\r\n    embeddings_2d[:, 1],\r\n    c=clusters,\r\n    cmap='viridis',\r\n    alpha=0.7\r\n)\r\nplt.colorbar(scatter, label='Cluster')\r\nplt.title('Document Embeddings (UMAP)')\r\nplt.xlabel('UMAP 1')\r\nplt.ylabel('UMAP 2')\r\nplt.savefig('output/figures/embeddings_umap.png', dpi=300)\r\nplt.close()\r\n```\r\n\r\n### t-SNE\r\n\r\n```python\r\nfrom sklearn.manifold import TSNE\r\n\r\n# Reduce to 2D\r\ntsne = TSNE(\r\n    n_components=2,\r\n    perplexity=min(30, len(embeddings) - 1),\r\n    random_state=42\r\n)\r\n\r\nembeddings_tsne = tsne.fit_transform(embeddings)\r\n\r\n# Plot\r\nplt.figure(figsize=(10, 8))\r\nplt.scatter(\r\n    embeddings_tsne[:, 0],\r\n    embeddings_tsne[:, 1],\r\n    c=clusters,\r\n    cmap='viridis',\r\n    alpha=0.7\r\n)\r\nplt.title('Document Embeddings (t-SNE)')\r\nplt.savefig('output/figures/embeddings_tsne.png', dpi=300)\r\nplt.close()\r\n```\r\n\r\n## TF-IDF Weighted Embeddings\r\n\r\n```python\r\nfrom sklearn.feature_extraction.text import TfidfVectorizer\r\n\r\ndef get_tfidf_weighted_embedding(text, word_model, tfidf_vectorizer, tfidf_matrix, doc_idx):\r\n    \"\"\"Get TF-IDF weighted document embedding.\"\"\"\r\n    words = simple_preprocess(text)\r\n    feature_names = tfidf_vectorizer.get_feature_names_out()\r\n\r\n    # Get TF-IDF weights for this document\r\n    tfidf_weights = tfidf_matrix[doc_idx].toarray().flatten()\r\n\r\n    weighted_vecs = []\r\n    weights = []\r\n\r\n    for word in words:\r\n        if word in word_model.wv and word in feature_names:\r\n            word_idx = np.where(feature_names == word)[0][0]\r\n            weight = tfidf_weights[word_idx]\r\n\r\n            weighted_vecs.append(word_model.wv[word] * weight)\r\n            weights.append(weight)\r\n\r\n    if not weighted_vecs:\r\n        return np.zeros(word_model.vector_size)\r\n\r\n    return np.sum(weighted_vecs, axis=0) / sum(weights)\r\n\r\n# Create TF-IDF\r\ntfidf = TfidfVectorizer()\r\ntfidf_matrix = tfidf.fit_transform(texts)\r\n\r\n# Get weighted embeddings\r\nweighted_embeddings = np.array([\r\n    get_tfidf_weighted_embedding(text, model, tfidf, tfidf_matrix, i)\r\n    for i, text in enumerate(texts)\r\n])\r\n```\r\n\r\n## Using Pretrained GloVe\r\n\r\n```python\r\ndef load_glove(path, dim=100):\r\n    \"\"\"Load pretrained GloVe embeddings.\"\"\"\r\n    embeddings = {}\r\n\r\n    with open(path, 'r', encoding='utf-8') as f:\r\n        for line in f:\r\n            values = line.split()\r\n            word = values[0]\r\n            vector = np.array(values[1:], dtype='float32')\r\n            embeddings[word] = vector\r\n\r\n    print(f\"Loaded {len(embeddings)} word vectors\")\r\n    return embeddings\r\n\r\n# Usage (if you have GloVe file)\r\n# glove = load_glove('glove.6B.100d.txt')\r\n\r\ndef get_glove_doc_embedding(text, glove, dim=100):\r\n    \"\"\"Get document embedding using GloVe.\"\"\"\r\n    words = simple_preprocess(text)\r\n    word_vecs = [glove[w] for w in words if w in glove]\r\n\r\n    if not word_vecs:\r\n        return np.zeros(dim)\r\n\r\n    return np.mean(word_vecs, axis=0)\r\n```\r\n\r\n## Embeddings as Classification Features\r\n\r\n```python\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.model_selection import cross_val_score\r\n\r\n# Prepare data\r\nlabels = [\"economy\", \"politics\", \"climate\", \"education\", \"tech\", \"politics\"] * 20\r\n\r\n# Use sentence transformer embeddings\r\nmodel_st = SentenceTransformer('all-MiniLM-L6-v2')\r\nX_embeddings = model_st.encode(texts)\r\n\r\n# Train classifier\r\nclf = LogisticRegression(max_iter=1000, random_state=42)\r\n\r\n# Cross-validation\r\nscores = cross_val_score(clf, X_embeddings, labels, cv=5, scoring='f1_macro')\r\nprint(f\"F1-macro: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\")\r\n\r\n# Fit on all data\r\nclf.fit(X_embeddings, labels)\r\n\r\n# Predict new text\r\nnew_text = \"The stock market saw significant gains today\"\r\nnew_embedding = model_st.encode([new_text])\r\nprediction = clf.predict(new_embedding)\r\nprint(f\"Predicted: {prediction[0]}\")\r\n```\r\n\r\n## Complete Embedding Pipeline\r\n\r\n```python\r\nclass EmbeddingPipeline:\r\n    \"\"\"Complete embedding pipeline.\"\"\"\r\n\r\n    def __init__(self, method='sentence-transformer', model_name='all-MiniLM-L6-v2'):\r\n        self.method = method\r\n        self.model_name = model_name\r\n        self.model = None\r\n        self.embeddings = None\r\n\r\n    def fit(self, texts):\r\n        \"\"\"Fit/compute embeddings.\"\"\"\r\n        if self.method == 'sentence-transformer':\r\n            self.model = SentenceTransformer(self.model_name)\r\n            self.embeddings = self.model.encode(texts, show_progress_bar=True)\r\n\r\n        elif self.method == 'word2vec':\r\n            tokenized = [simple_preprocess(t) for t in texts]\r\n            self.model = Word2Vec(\r\n                sentences=tokenized,\r\n                vector_size=100,\r\n                window=5,\r\n                min_count=2,\r\n                epochs=10,\r\n                seed=42\r\n            )\r\n            self.embeddings = np.array([\r\n                get_doc_embedding(t, self.model) for t in texts\r\n            ])\r\n\r\n        return self\r\n\r\n    def transform(self, texts):\r\n        \"\"\"Transform new texts.\"\"\"\r\n        if self.method == 'sentence-transformer':\r\n            return self.model.encode(texts)\r\n        elif self.method == 'word2vec':\r\n            return np.array([get_doc_embedding(t, self.model) for t in texts])\r\n\r\n    def similarity_matrix(self):\r\n        \"\"\"Compute similarity matrix.\"\"\"\r\n        return cosine_similarity(self.embeddings)\r\n\r\n    def search(self, query, texts, top_n=5):\r\n        \"\"\"Semantic search.\"\"\"\r\n        query_emb = self.transform([query])\r\n        sims = cosine_similarity(query_emb, self.embeddings)[0]\r\n        top_idx = np.argsort(sims)[-top_n:][::-1]\r\n\r\n        return [{'idx': i, 'sim': sims[i], 'text': texts[i][:100]}\r\n                for i in top_idx]\r\n\r\n    def cluster(self, n_clusters=5):\r\n        \"\"\"Cluster documents.\"\"\"\r\n        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\r\n        return kmeans.fit_predict(self.embeddings)\r\n\r\n    def reduce_dims(self, method='umap', n_components=2):\r\n        \"\"\"Reduce dimensionality.\"\"\"\r\n        if method == 'umap':\r\n            reducer = UMAP(n_components=n_components, random_state=42)\r\n        else:\r\n            reducer = TSNE(n_components=n_components, random_state=42)\r\n\r\n        return reducer.fit_transform(self.embeddings)\r\n\r\n# Usage\r\npipeline = EmbeddingPipeline(method='sentence-transformer')\r\npipeline.fit(texts)\r\n\r\n# Search\r\nresults = pipeline.search(\"economic growth\", texts)\r\nfor r in results:\r\n    print(f\"Doc {r['idx']} (sim={r['sim']:.3f}): {r['text'][:50]}...\")\r\n\r\n# Cluster\r\nclusters = pipeline.cluster(n_clusters=4)\r\nprint(f\"Cluster distribution: {np.bincount(clusters)}\")\r\n\r\n# Visualize\r\ncoords = pipeline.reduce_dims(method='umap')\r\nplt.scatter(coords[:, 0], coords[:, 1], c=clusters)\r\nplt.savefig('output/figures/embedding_clusters.png', dpi=300)\r\nplt.close()\r\n```\r\n",
        "plugins/text-analyst/skills/text-analyst/python-techniques/06_visualization.md": "# Text Visualization in Python\r\n\r\n## Package Versions\r\n\r\n```python\r\n# Tested with:\r\n# Python 3.10\r\n# matplotlib 3.7.1\r\n# seaborn 0.12.2\r\n# wordcloud 1.9.2\r\n# plotly 5.15.0\r\n```\r\n\r\n## Installation\r\n\r\n```bash\r\npip install matplotlib seaborn wordcloud plotly pandas numpy\r\n```\r\n\r\n## Term Frequency Plots\r\n\r\n### Basic Bar Chart\r\n\r\n```python\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nimport pandas as pd\r\nfrom collections import Counter\r\nimport numpy as np\r\n\r\n# Sample word frequencies\r\ntexts = [\"The economy shows growth\", \"Climate change impacts\", \"Healthcare reform needed\"] * 50\r\nwords = ' '.join(texts).lower().split()\r\nword_freq = Counter(words)\r\n\r\n# Get top N\r\ntop_words = pd.DataFrame(\r\n    word_freq.most_common(20),\r\n    columns=['word', 'frequency']\r\n)\r\n\r\n# Bar chart\r\nfig, ax = plt.subplots(figsize=(10, 6))\r\nax.barh(top_words['word'], top_words['frequency'], color='steelblue')\r\nax.set_xlabel('Frequency')\r\nax.set_ylabel('Word')\r\nax.set_title('Top 20 Most Frequent Words')\r\nax.invert_yaxis()  # Highest at top\r\n\r\nplt.tight_layout()\r\nplt.savefig('output/figures/term_frequency.png', dpi=300)\r\nplt.close()\r\n```\r\n\r\n### Seaborn Style\r\n\r\n```python\r\n# Set style\r\nsns.set_style(\"whitegrid\")\r\nplt.rcParams['font.size'] = 12\r\n\r\nfig, ax = plt.subplots(figsize=(10, 6))\r\nsns.barplot(\r\n    data=top_words,\r\n    y='word',\r\n    x='frequency',\r\n    palette='Blues_r',\r\n    ax=ax\r\n)\r\nax.set_title('Term Frequency', fontsize=14, fontweight='bold')\r\nax.set_xlabel('Frequency')\r\nax.set_ylabel('')\r\n\r\nplt.tight_layout()\r\nplt.savefig('output/figures/term_frequency_sns.png', dpi=300)\r\nplt.close()\r\n```\r\n\r\n## Word Clouds\r\n\r\n```python\r\nfrom wordcloud import WordCloud\r\n\r\n# Basic word cloud\r\ntext = ' '.join(texts)\r\n\r\nwordcloud = WordCloud(\r\n    width=800,\r\n    height=400,\r\n    background_color='white',\r\n    colormap='Blues',\r\n    max_words=100\r\n).generate(text)\r\n\r\nplt.figure(figsize=(12, 6))\r\nplt.imshow(wordcloud, interpolation='bilinear')\r\nplt.axis('off')\r\nplt.title('Word Cloud', fontsize=14)\r\nplt.tight_layout()\r\nplt.savefig('output/figures/wordcloud.png', dpi=300)\r\nplt.close()\r\n```\r\n\r\n### Word Cloud from Frequencies\r\n\r\n```python\r\n# From frequency dict\r\nwordcloud = WordCloud(\r\n    width=800,\r\n    height=400,\r\n    background_color='white'\r\n).generate_from_frequencies(word_freq)\r\n\r\nplt.figure(figsize=(12, 6))\r\nplt.imshow(wordcloud, interpolation='bilinear')\r\nplt.axis('off')\r\nplt.savefig('output/figures/wordcloud_freq.png', dpi=300)\r\nplt.close()\r\n```\r\n\r\n## Sentiment Visualization\r\n\r\n### Sentiment Distribution\r\n\r\n```python\r\n# Sample sentiment scores\r\nnp.random.seed(42)\r\nsentiment_scores = np.random.normal(0.1, 0.3, 200)\r\n\r\nfig, ax = plt.subplots(figsize=(10, 5))\r\nax.hist(sentiment_scores, bins=30, color='steelblue', edgecolor='white', alpha=0.7)\r\nax.axvline(x=0, color='red', linestyle='--', label='Neutral', linewidth=2)\r\nax.axvline(x=sentiment_scores.mean(), color='green', linestyle='-',\r\n           label=f'Mean ({sentiment_scores.mean():.2f})', linewidth=2)\r\n\r\nax.set_xlabel('Sentiment Score')\r\nax.set_ylabel('Count')\r\nax.set_title('Distribution of Sentiment Scores')\r\nax.legend()\r\n\r\nplt.tight_layout()\r\nplt.savefig('output/figures/sentiment_distribution.png', dpi=300)\r\nplt.close()\r\n```\r\n\r\n### Sentiment Over Time\r\n\r\n```python\r\nfrom datetime import datetime, timedelta\r\n\r\n# Sample time series\r\ndates = [datetime(2020, 1, 1) + timedelta(days=i) for i in range(365)]\r\ndaily_sentiment = np.sin(np.linspace(0, 4*np.pi, 365)) * 0.3 + np.random.normal(0, 0.1, 365)\r\n\r\nfig, ax = plt.subplots(figsize=(12, 5))\r\nax.plot(dates, daily_sentiment, color='steelblue', alpha=0.5, linewidth=1)\r\n\r\n# Add rolling average\r\nrolling = pd.Series(daily_sentiment).rolling(window=14).mean()\r\nax.plot(dates, rolling, color='red', linewidth=2, label='14-day average')\r\n\r\nax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\r\nax.set_xlabel('Date')\r\nax.set_ylabel('Sentiment')\r\nax.set_title('Sentiment Over Time')\r\nax.legend()\r\n\r\nplt.tight_layout()\r\nplt.savefig('output/figures/sentiment_time.png', dpi=300)\r\nplt.close()\r\n```\r\n\r\n### Sentiment by Group\r\n\r\n```python\r\n# Sample grouped sentiment\r\ngroups = ['Group A', 'Group B', 'Group C']\r\nmeans = [0.15, -0.08, 0.22]\r\nstds = [0.05, 0.06, 0.04]\r\n\r\nfig, ax = plt.subplots(figsize=(8, 5))\r\ncolors = ['#3498db', '#e74c3c', '#2ecc71']\r\n\r\nbars = ax.bar(groups, means, yerr=stds, capsize=5, color=colors, edgecolor='black')\r\nax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\r\nax.set_ylabel('Mean Sentiment')\r\nax.set_title('Sentiment by Group')\r\n\r\n# Add value labels\r\nfor bar, mean in zip(bars, means):\r\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\r\n            f'{mean:.2f}', ha='center', va='bottom')\r\n\r\nplt.tight_layout()\r\nplt.savefig('output/figures/sentiment_groups.png', dpi=300)\r\nplt.close()\r\n```\r\n\r\n## Topic Model Visualization\r\n\r\n### Topic Prevalence\r\n\r\n```python\r\n# Sample topic prevalence\r\ntopics = [f'Topic {i+1}' for i in range(10)]\r\nlabels = ['Economy', 'Healthcare', 'Climate', 'Education', 'Technology',\r\n          'Immigration', 'Crime', 'Foreign Policy', 'Media', 'Elections']\r\nprevalence = [0.15, 0.12, 0.11, 0.10, 0.10, 0.09, 0.09, 0.08, 0.08, 0.08]\r\n\r\n# Sort by prevalence\r\nsorted_idx = np.argsort(prevalence)[::-1]\r\nsorted_labels = [labels[i] for i in sorted_idx]\r\nsorted_prev = [prevalence[i] for i in sorted_idx]\r\n\r\nfig, ax = plt.subplots(figsize=(10, 6))\r\nbars = ax.barh(sorted_labels, sorted_prev, color='steelblue')\r\nax.set_xlabel('Prevalence')\r\nax.set_title('Topic Prevalence')\r\nax.invert_yaxis()\r\n\r\n# Format as percentage\r\nax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.0%}'))\r\n\r\nplt.tight_layout()\r\nplt.savefig('output/figures/topic_prevalence.png', dpi=300)\r\nplt.close()\r\n```\r\n\r\n### Topic Top Words\r\n\r\n```python\r\n# Sample topic-word data\r\ntopic_words = {\r\n    'Economy': ['economy', 'growth', 'jobs', 'tax', 'market'],\r\n    'Healthcare': ['health', 'care', 'hospital', 'doctor', 'insurance'],\r\n    'Climate': ['climate', 'change', 'carbon', 'emissions', 'energy']\r\n}\r\n\r\nfig, axes = plt.subplots(1, 3, figsize=(14, 5))\r\n\r\nfor ax, (topic, words) in zip(axes, topic_words.items()):\r\n    weights = np.linspace(1, 0.5, len(words))\r\n    ax.barh(words[::-1], weights[::-1], color='steelblue')\r\n    ax.set_title(topic, fontsize=12, fontweight='bold')\r\n    ax.set_xlim(0, 1.1)\r\n\r\nplt.tight_layout()\r\nplt.savefig('output/figures/topic_words.png', dpi=300)\r\nplt.close()\r\n```\r\n\r\n### Topic Over Time\r\n\r\n```python\r\n# Sample topic prevalence over time\r\nyears = range(2015, 2024)\r\ntopic_trends = {\r\n    'Climate': [0.05, 0.06, 0.07, 0.08, 0.10, 0.12, 0.14, 0.15, 0.16],\r\n    'Economy': [0.15, 0.14, 0.13, 0.12, 0.11, 0.10, 0.09, 0.08, 0.08],\r\n    'Healthcare': [0.10, 0.10, 0.11, 0.12, 0.15, 0.18, 0.12, 0.11, 0.10]\r\n}\r\n\r\nfig, ax = plt.subplots(figsize=(10, 6))\r\n\r\nfor topic, values in topic_trends.items():\r\n    ax.plot(years, values, marker='o', linewidth=2, label=topic)\r\n\r\nax.set_xlabel('Year')\r\nax.set_ylabel('Topic Prevalence')\r\nax.set_title('Topic Trends Over Time')\r\nax.legend()\r\nax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.0%}'))\r\n\r\nplt.tight_layout()\r\nplt.savefig('output/figures/topic_trends.png', dpi=300)\r\nplt.close()\r\n```\r\n\r\n## Classification Visualization\r\n\r\n### Confusion Matrix\r\n\r\n```python\r\nimport seaborn as sns\r\n\r\n# Sample confusion matrix\r\nclasses = ['Politics', 'Sports', 'Business', 'Science']\r\ncm = np.array([\r\n    [45, 3, 2, 0],\r\n    [2, 47, 1, 0],\r\n    [3, 1, 44, 2],\r\n    [0, 1, 3, 46]\r\n])\r\n\r\nfig, ax = plt.subplots(figsize=(8, 6))\r\nsns.heatmap(\r\n    cm,\r\n    annot=True,\r\n    fmt='d',\r\n    cmap='Blues',\r\n    xticklabels=classes,\r\n    yticklabels=classes,\r\n    ax=ax\r\n)\r\nax.set_xlabel('Predicted')\r\nax.set_ylabel('Actual')\r\nax.set_title('Confusion Matrix')\r\n\r\nplt.tight_layout()\r\nplt.savefig('output/figures/confusion_matrix.png', dpi=300)\r\nplt.close()\r\n```\r\n\r\n### Normalized Confusion Matrix\r\n\r\n```python\r\n# Normalize by row (recall)\r\ncm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\r\n\r\nfig, ax = plt.subplots(figsize=(8, 6))\r\nsns.heatmap(\r\n    cm_normalized,\r\n    annot=True,\r\n    fmt='.2f',\r\n    cmap='Blues',\r\n    xticklabels=classes,\r\n    yticklabels=classes,\r\n    ax=ax,\r\n    vmin=0,\r\n    vmax=1\r\n)\r\nax.set_xlabel('Predicted')\r\nax.set_ylabel('Actual')\r\nax.set_title('Confusion Matrix (Normalized)')\r\n\r\nplt.tight_layout()\r\nplt.savefig('output/figures/confusion_matrix_norm.png', dpi=300)\r\nplt.close()\r\n```\r\n\r\n### ROC Curve\r\n\r\n```python\r\nfrom sklearn.metrics import roc_curve, auc\r\n\r\n# Sample predictions\r\nnp.random.seed(42)\r\ny_true = np.random.binomial(1, 0.5, 200)\r\ny_scores = y_true * 0.6 + np.random.normal(0, 0.3, 200)\r\n\r\nfpr, tpr, thresholds = roc_curve(y_true, y_scores)\r\nroc_auc = auc(fpr, tpr)\r\n\r\nfig, ax = plt.subplots(figsize=(8, 6))\r\nax.plot(fpr, tpr, color='steelblue', linewidth=2,\r\n        label=f'ROC curve (AUC = {roc_auc:.2f})')\r\nax.plot([0, 1], [0, 1], color='gray', linestyle='--')\r\nax.set_xlim([0, 1])\r\nax.set_ylim([0, 1.05])\r\nax.set_xlabel('False Positive Rate')\r\nax.set_ylabel('True Positive Rate')\r\nax.set_title('ROC Curve')\r\nax.legend(loc='lower right')\r\n\r\nplt.tight_layout()\r\nplt.savefig('output/figures/roc_curve.png', dpi=300)\r\nplt.close()\r\n```\r\n\r\n## Embedding Visualization\r\n\r\n### 2D Scatter Plot\r\n\r\n```python\r\n# Sample embeddings (reduced to 2D)\r\nnp.random.seed(42)\r\nn_points = 100\r\nembeddings_2d = np.random.randn(n_points, 2)\r\nclusters = np.random.choice([0, 1, 2, 3], n_points)\r\n\r\nfig, ax = plt.subplots(figsize=(10, 8))\r\nscatter = ax.scatter(\r\n    embeddings_2d[:, 0],\r\n    embeddings_2d[:, 1],\r\n    c=clusters,\r\n    cmap='viridis',\r\n    alpha=0.7,\r\n    s=50\r\n)\r\nplt.colorbar(scatter, label='Cluster')\r\nax.set_xlabel('Dimension 1')\r\nax.set_ylabel('Dimension 2')\r\nax.set_title('Document Embeddings')\r\n\r\nplt.tight_layout()\r\nplt.savefig('output/figures/embeddings_2d.png', dpi=300)\r\nplt.close()\r\n```\r\n\r\n## Interactive Plots with Plotly\r\n\r\n### Interactive Topic Visualization\r\n\r\n```python\r\nimport plotly.express as px\r\nimport plotly.graph_objects as go\r\n\r\n# Sample data\r\ndf_topics = pd.DataFrame({\r\n    'Topic': labels,\r\n    'Prevalence': prevalence,\r\n    'Words': ['word1, word2, word3'] * 10\r\n})\r\n\r\nfig = px.bar(\r\n    df_topics,\r\n    x='Prevalence',\r\n    y='Topic',\r\n    orientation='h',\r\n    hover_data=['Words'],\r\n    title='Topic Prevalence'\r\n)\r\nfig.update_layout(yaxis={'categoryorder': 'total ascending'})\r\nfig.write_html('output/figures/topic_prevalence_interactive.html')\r\n```\r\n\r\n### Interactive Embeddings\r\n\r\n```python\r\n# Sample with text\r\ndf_embed = pd.DataFrame({\r\n    'x': embeddings_2d[:, 0],\r\n    'y': embeddings_2d[:, 1],\r\n    'cluster': [f'Cluster {c}' for c in clusters],\r\n    'text': [f'Document {i}' for i in range(n_points)]\r\n})\r\n\r\nfig = px.scatter(\r\n    df_embed,\r\n    x='x',\r\n    y='y',\r\n    color='cluster',\r\n    hover_data=['text'],\r\n    title='Document Embeddings'\r\n)\r\nfig.write_html('output/figures/embeddings_interactive.html')\r\n```\r\n\r\n## Publication-Ready Formatting\r\n\r\n### Set Publication Style\r\n\r\n```python\r\ndef set_publication_style():\r\n    \"\"\"Set matplotlib style for publications.\"\"\"\r\n    plt.rcParams.update({\r\n        'font.size': 12,\r\n        'font.family': 'sans-serif',\r\n        'axes.labelsize': 12,\r\n        'axes.titlesize': 14,\r\n        'xtick.labelsize': 10,\r\n        'ytick.labelsize': 10,\r\n        'legend.fontsize': 10,\r\n        'figure.figsize': (8, 6),\r\n        'figure.dpi': 100,\r\n        'savefig.dpi': 300,\r\n        'savefig.bbox': 'tight',\r\n        'axes.spines.top': False,\r\n        'axes.spines.right': False,\r\n    })\r\n\r\nset_publication_style()\r\n```\r\n\r\n### Colorblind-Friendly Palette\r\n\r\n```python\r\n# Okabe-Ito palette (colorblind-friendly)\r\nOKABE_ITO = [\r\n    '#E69F00',  # Orange\r\n    '#56B4E9',  # Sky blue\r\n    '#009E73',  # Bluish green\r\n    '#F0E442',  # Yellow\r\n    '#0072B2',  # Blue\r\n    '#D55E00',  # Vermillion\r\n    '#CC79A7',  # Reddish purple\r\n    '#999999',  # Gray\r\n]\r\n\r\n# Use in plots\r\nfig, ax = plt.subplots()\r\nfor i, (topic, values) in enumerate(topic_trends.items()):\r\n    ax.plot(years, values, marker='o', color=OKABE_ITO[i], label=topic)\r\nax.legend()\r\n```\r\n\r\n### Saving Figures\r\n\r\n```python\r\ndef save_figure(fig, filename, formats=['png', 'pdf']):\r\n    \"\"\"Save figure in multiple formats.\"\"\"\r\n    for fmt in formats:\r\n        filepath = f'output/figures/{filename}.{fmt}'\r\n        fig.savefig(filepath, dpi=300, bbox_inches='tight', facecolor='white')\r\n        print(f\"Saved: {filepath}\")\r\n\r\n# Usage\r\nfig, ax = plt.subplots()\r\nax.bar(['A', 'B', 'C'], [1, 2, 3])\r\nsave_figure(fig, 'example_figure')\r\nplt.close()\r\n```\r\n\r\n## pyLDAvis for Topic Models\r\n\r\n```python\r\nimport pyLDAvis\r\nimport pyLDAvis.gensim_models\r\n\r\n# Assuming you have a trained gensim LDA model\r\n# vis_data = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\r\n# pyLDAvis.save_html(vis_data, 'output/figures/lda_vis.html')\r\n\r\n# For sklearn LDA\r\n# from pyLDAvis import sklearn as pyLDAvis_sklearn\r\n# vis_data = pyLDAvis_sklearn.prepare(lda, dtm, vectorizer)\r\n# pyLDAvis.save_html(vis_data, 'output/figures/sklearn_lda_vis.html')\r\n```\r\n\r\n## Complete Visualization Suite\r\n\r\n```python\r\nclass TextVisualizer:\r\n    \"\"\"Complete text visualization suite.\"\"\"\r\n\r\n    def __init__(self, style='publication'):\r\n        if style == 'publication':\r\n            set_publication_style()\r\n        self.colors = OKABE_ITO\r\n\r\n    def term_frequency(self, word_counts, top_n=20, title='Term Frequency'):\r\n        \"\"\"Plot term frequency bar chart.\"\"\"\r\n        if isinstance(word_counts, dict):\r\n            df = pd.DataFrame(list(word_counts.items()), columns=['word', 'freq'])\r\n        else:\r\n            df = word_counts.copy()\r\n\r\n        df = df.nlargest(top_n, 'freq' if 'freq' in df.columns else df.columns[1])\r\n\r\n        fig, ax = plt.subplots(figsize=(10, 6))\r\n        ax.barh(df.iloc[:, 0], df.iloc[:, 1], color=self.colors[0])\r\n        ax.set_xlabel('Frequency')\r\n        ax.set_title(title)\r\n        ax.invert_yaxis()\r\n\r\n        return fig\r\n\r\n    def sentiment_distribution(self, scores, title='Sentiment Distribution'):\r\n        \"\"\"Plot sentiment distribution.\"\"\"\r\n        fig, ax = plt.subplots(figsize=(10, 5))\r\n        ax.hist(scores, bins=30, color=self.colors[0], edgecolor='white', alpha=0.7)\r\n        ax.axvline(x=0, color='red', linestyle='--', linewidth=2)\r\n        ax.axvline(x=np.mean(scores), color='green', linestyle='-', linewidth=2)\r\n        ax.set_xlabel('Sentiment Score')\r\n        ax.set_ylabel('Count')\r\n        ax.set_title(title)\r\n\r\n        return fig\r\n\r\n    def confusion_matrix(self, cm, labels, title='Confusion Matrix', normalize=False):\r\n        \"\"\"Plot confusion matrix.\"\"\"\r\n        if normalize:\r\n            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\r\n            fmt = '.2f'\r\n        else:\r\n            fmt = 'd'\r\n\r\n        fig, ax = plt.subplots(figsize=(8, 6))\r\n        sns.heatmap(cm, annot=True, fmt=fmt, cmap='Blues',\r\n                   xticklabels=labels, yticklabels=labels, ax=ax)\r\n        ax.set_xlabel('Predicted')\r\n        ax.set_ylabel('Actual')\r\n        ax.set_title(title)\r\n\r\n        return fig\r\n\r\n    def save(self, fig, filename):\r\n        \"\"\"Save figure.\"\"\"\r\n        save_figure(fig, filename)\r\n\r\n# Usage\r\nviz = TextVisualizer()\r\nfig = viz.term_frequency(word_freq)\r\nviz.save(fig, 'term_freq')\r\nplt.close()\r\n```\r\n",
        "plugins/text-analyst/skills/text-analyst/r-techniques/01_preprocessing.md": "# Text Preprocessing in R\r\n\r\n## Package Versions\r\n\r\n```r\r\n# Tested with:\r\n# R 4.3.0\r\n# tidytext 0.4.1\r\n# quanteda 3.3.1\r\n# tidyverse 2.0.0\r\n```\r\n\r\n## Installation\r\n\r\n```r\r\ninstall.packages(c(\"tidytext\", \"quanteda\", \"tidyverse\", \"SnowballC\", \"textstem\"))\r\n```\r\n\r\n## Two Approaches: tidytext vs quanteda\r\n\r\n| Package | Philosophy | Best For |\r\n|---------|------------|----------|\r\n| **tidytext** | Tidy data principles | Integration with dplyr, ggplot2 |\r\n| **quanteda** | Corpus-centric | Large corpora, advanced features |\r\n\r\n## tidytext Workflow\r\n\r\n### Basic Tokenization\r\n\r\n```r\r\nlibrary(tidyverse)\r\nlibrary(tidytext)\r\n\r\n# Sample data\r\ntexts <- tibble(\r\n  doc_id = 1:3,\r\n  text = c(\r\n    \"The quick brown fox jumps over the lazy dog.\",\r\n    \"Machine learning is transforming social science research.\",\r\n    \"Text analysis requires careful preprocessing decisions.\"\r\n  )\r\n)\r\n\r\n# Tokenize to words\r\ntokens <- texts %>%\r\n  unnest_tokens(word, text)\r\n\r\ntokens\r\n# # A tibble: 24  2\r\n#    doc_id word\r\n#     <int> <chr>\r\n#  1      1 the\r\n#  2      1 quick\r\n#  3      1 brown\r\n# ...\r\n```\r\n\r\n### Removing Stopwords\r\n\r\n```r\r\n# Default English stopwords (SMART list)\r\ndata(stop_words)\r\n\r\ntokens_clean <- tokens %>%\r\n  anti_join(stop_words, by = \"word\")\r\n\r\n# Custom stopwords\r\ncustom_stops <- tibble(word = c(\"data\", \"analysis\", \"research\"))\r\n\r\ntokens_clean <- tokens %>%\r\n  anti_join(stop_words, by = \"word\") %>%\r\n  anti_join(custom_stops, by = \"word\")\r\n```\r\n\r\n### Stemming and Lemmatization\r\n\r\n```r\r\nlibrary(SnowballC)\r\n\r\n# Stemming (Porter)\r\ntokens_stemmed <- tokens_clean %>%\r\n  mutate(stem = wordStem(word))\r\n\r\n# Lemmatization (requires textstem)\r\nlibrary(textstem)\r\n\r\ntokens_lemma <- tokens_clean %>%\r\n  mutate(lemma = lemmatize_words(word))\r\n```\r\n\r\n### N-grams\r\n\r\n```r\r\n# Bigrams\r\nbigrams <- texts %>%\r\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2)\r\n\r\n# Trigrams\r\ntrigrams <- texts %>%\r\n  unnest_tokens(trigram, text, token = \"ngrams\", n = 3)\r\n\r\n# Filter bigrams by stopwords\r\nbigrams_clean <- bigrams %>%\r\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %>%\r\n  filter(!word1 %in% stop_words$word,\r\n         !word2 %in% stop_words$word) %>%\r\n  unite(bigram, word1, word2, sep = \" \")\r\n```\r\n\r\n### Document-Term Matrix\r\n\r\n```r\r\n# Create DTM from tokens\r\ndtm_tidy <- tokens_clean %>%\r\n  count(doc_id, word) %>%\r\n  cast_dtm(doc_id, word, n)\r\n\r\n# Or with TF-IDF\r\ndtm_tfidf <- tokens_clean %>%\r\n  count(doc_id, word) %>%\r\n  bind_tf_idf(word, doc_id, n) %>%\r\n  cast_dtm(doc_id, word, tf_idf)\r\n\r\n# Inspect\r\ndtm_tidy\r\n# <<DocumentTermMatrix (documents: 3, terms: 18)>>\r\n```\r\n\r\n## quanteda Workflow\r\n\r\n### Creating a Corpus\r\n\r\n```r\r\nlibrary(quanteda)\r\n\r\n# From character vector\r\ncorp <- corpus(c(\r\n  \"The quick brown fox jumps over the lazy dog.\",\r\n  \"Machine learning is transforming social science research.\",\r\n  \"Text analysis requires careful preprocessing decisions.\"\r\n))\r\n\r\n# From data frame\r\ntexts_df <- data.frame(\r\n  doc_id = c(\"doc1\", \"doc2\", \"doc3\"),\r\n  text = c(\r\n    \"The quick brown fox jumps over the lazy dog.\",\r\n    \"Machine learning is transforming social science research.\",\r\n    \"Text analysis requires careful preprocessing decisions.\"\r\n  ),\r\n  year = c(2020, 2021, 2022)\r\n)\r\n\r\ncorp <- corpus(texts_df, text_field = \"text\", docid_field = \"doc_id\")\r\n```\r\n\r\n### Tokenization\r\n\r\n```r\r\n# Basic tokenization\r\ntoks <- tokens(corp)\r\n\r\n# With options\r\ntoks <- tokens(corp,\r\n               remove_punct = TRUE,\r\n               remove_numbers = TRUE,\r\n               remove_symbols = TRUE)\r\n\r\n# Lowercase\r\ntoks <- tokens_tolower(toks)\r\n```\r\n\r\n### Stopword Removal\r\n\r\n```r\r\n# Remove stopwords\r\ntoks_clean <- tokens_remove(toks, stopwords(\"en\"))\r\n\r\n# Custom stopwords\r\ntoks_clean <- tokens_remove(toks, c(stopwords(\"en\"), \"data\", \"research\"))\r\n\r\n# Keep specific words (instead of remove)\r\ntoks_keep <- tokens_select(toks, c(\"machine\", \"learning\", \"social\"))\r\n```\r\n\r\n### Stemming\r\n\r\n```r\r\ntoks_stem <- tokens_wordstem(toks_clean)\r\n```\r\n\r\n### N-grams\r\n\r\n```r\r\n# Create bigrams\r\ntoks_ngram <- tokens_ngrams(toks_clean, n = 2)\r\n\r\n# Compound tokens (keep phrases together)\r\ntoks_compound <- tokens_compound(toks, phrase(c(\"machine learning\", \"social science\")))\r\n```\r\n\r\n### Document-Feature Matrix (DFM)\r\n\r\n```r\r\n# Create DFM\r\ndfm <- dfm(toks_clean)\r\n\r\n# With TF-IDF weighting\r\ndfm_tfidf <- dfm_tfidf(dfm)\r\n\r\n# Trim by frequency\r\ndfm_trimmed <- dfm_trim(dfm,\r\n                        min_termfreq = 2,      # Minimum term frequency\r\n                        min_docfreq = 2,       # Minimum document frequency\r\n                        max_docfreq = 0.9,     # Maximum as proportion\r\n                        docfreq_type = \"prop\") # Proportion for max\r\n\r\n# Inspect\r\ndfm_trimmed\r\n# Document-feature matrix of: 3 documents, 12 features\r\n```\r\n\r\n## Preprocessing Pipeline Comparison\r\n\r\n### tidytext Pipeline\r\n\r\n```r\r\npreprocess_tidytext <- function(df, text_col = \"text\", doc_col = \"doc_id\") {\r\n  df %>%\r\n    # Tokenize\r\n    unnest_tokens(word, !!sym(text_col)) %>%\r\n    # Remove stopwords\r\n    anti_join(stop_words, by = \"word\") %>%\r\n    # Remove numbers\r\n    filter(!str_detect(word, \"^[0-9]+$\")) %>%\r\n    # Remove short words\r\n    filter(str_length(word) > 2) %>%\r\n    # Optional: stem\r\n    mutate(word = wordStem(word))\r\n}\r\n\r\n# Usage\r\ntokens_processed <- preprocess_tidytext(texts)\r\n```\r\n\r\n### quanteda Pipeline\r\n\r\n```r\r\npreprocess_quanteda <- function(texts, docvars = NULL) {\r\n  corp <- corpus(texts)\r\n  if (!is.null(docvars)) docvars(corp) <- docvars\r\n\r\n  tokens(corp,\r\n         remove_punct = TRUE,\r\n         remove_numbers = TRUE,\r\n         remove_symbols = TRUE) %>%\r\n    tokens_tolower() %>%\r\n    tokens_remove(stopwords(\"en\")) %>%\r\n    tokens_wordstem()\r\n}\r\n\r\n# Usage\r\ntoks_processed <- preprocess_quanteda(texts$text)\r\ndfm_processed <- dfm(toks_processed)\r\n```\r\n\r\n## Converting Between Formats\r\n\r\n```r\r\n# tidytext to quanteda\r\ndtm <- tokens_clean %>%\r\n  count(doc_id, word) %>%\r\n  cast_dfm(doc_id, word, n)\r\n\r\n# quanteda to tidytext\r\ntidy_from_dfm <- tidy(dfm)\r\n\r\n# quanteda to tm\r\ndtm_tm <- convert(dfm, to = \"tm\")\r\n\r\n# tm to quanteda\r\ndfm_from_tm <- as.dfm(dtm_tm)\r\n```\r\n\r\n## Handling Special Cases\r\n\r\n### URLs and HTML\r\n\r\n```r\r\nlibrary(stringr)\r\n\r\n# Remove URLs\r\ntexts_clean <- texts %>%\r\n  mutate(text = str_remove_all(text, \"https?://\\\\S+\"))\r\n\r\n# Remove HTML tags\r\ntexts_clean <- texts %>%\r\n  mutate(text = str_remove_all(text, \"<[^>]+>\"))\r\n\r\n# Or use textclean package\r\nlibrary(textclean)\r\ntexts_clean <- texts %>%\r\n  mutate(text = replace_url(text),\r\n         text = replace_html(text))\r\n```\r\n\r\n### Encoding Issues\r\n\r\n```r\r\n# Check encoding\r\nEncoding(texts$text)\r\n\r\n# Convert to UTF-8\r\ntexts$text <- iconv(texts$text, to = \"UTF-8\", sub = \"\")\r\n\r\n# Remove non-ASCII\r\ntexts$text <- str_replace_all(texts$text, \"[^[:ascii:]]\", \"\")\r\n```\r\n\r\n### Empty Documents\r\n\r\n```r\r\n# After preprocessing, check for empty docs\r\ndoc_lengths <- tokens_clean %>%\r\n  count(doc_id, name = \"n_tokens\")\r\n\r\n# Identify empty docs\r\nempty_docs <- doc_lengths %>%\r\n  filter(n_tokens == 0)\r\n\r\n# Or with quanteda\r\nntoken(dfm)  # Tokens per document\r\n```\r\n\r\n## Corpus Statistics\r\n\r\n```r\r\n# With tidytext\r\ncorpus_stats <- tokens_clean %>%\r\n  summarise(\r\n    n_docs = n_distinct(doc_id),\r\n    n_tokens = n(),\r\n    n_types = n_distinct(word),\r\n    mean_doc_length = n() / n_distinct(doc_id)\r\n  )\r\n\r\n# Term frequencies\r\nterm_freq <- tokens_clean %>%\r\n  count(word, sort = TRUE)\r\n\r\n# Document frequencies\r\ndoc_freq <- tokens_clean %>%\r\n  distinct(doc_id, word) %>%\r\n  count(word, sort = TRUE) %>%\r\n  rename(doc_freq = n)\r\n\r\n# With quanteda\r\ntextstat_summary(dfm)\r\ntopfeatures(dfm, 20)\r\n```\r\n\r\n## Best Practices\r\n\r\n1. **Document all decisions** - Create a preprocessing log\r\n2. **Start minimal** - Add processing steps only if needed\r\n3. **Check before and after** - Examine vocabulary at each step\r\n4. **Validate with KWIC** - Use keyword-in-context to verify\r\n5. **Set seeds** - For any stochastic elements\r\n6. **Save intermediate objects** - For debugging and reproducibility\r\n\r\n```r\r\n# Example preprocessing log\r\npreprocessing_log <- list(\r\n  date = Sys.Date(),\r\n  n_docs_raw = nrow(texts),\r\n  n_docs_processed = n_distinct(tokens_clean$doc_id),\r\n  vocab_raw = length(unique(tokens$word)),\r\n  vocab_processed = length(unique(tokens_clean$word)),\r\n  stopwords = \"SMART + custom\",\r\n  stemming = \"Porter\",\r\n  min_term_freq = 5\r\n)\r\n\r\nsaveRDS(preprocessing_log, \"data/processed/preprocessing_log.rds\")\r\n```\r\n",
        "plugins/text-analyst/skills/text-analyst/r-techniques/02_dictionary_sentiment.md": "# Dictionary and Sentiment Analysis in R\r\n\r\n## Package Versions\r\n\r\n```r\r\n# Tested with:\r\n# R 4.3.0\r\n# tidytext 0.4.1\r\n# tidyverse 2.0.0\r\n# textdata 0.4.4\r\n# lexicon 1.2.1\r\n```\r\n\r\n## Installation\r\n\r\n```r\r\ninstall.packages(c(\"tidytext\", \"tidyverse\", \"textdata\", \"lexicon\"))\r\n\r\n# Some lexicons require downloading\r\nlibrary(textdata)\r\nlexicon_afinn()  # Downloads AFINN\r\nlexicon_nrc()    # Downloads NRC\r\n```\r\n\r\n## Available Lexicons in tidytext\r\n\r\n```r\r\nlibrary(tidytext)\r\n\r\n# Built-in lexicons\r\nget_sentiments(\"bing\")    # Positive/negative binary\r\nget_sentiments(\"afinn\")   # -5 to +5 scale\r\nget_sentiments(\"nrc\")     # Emotions + sentiment\r\nget_sentiments(\"loughran\") # Finance-specific\r\n```\r\n\r\n### Lexicon Details\r\n\r\n| Lexicon | Categories | Size | Best For |\r\n|---------|------------|------|----------|\r\n| **Bing** | positive, negative | ~6,800 | General binary sentiment |\r\n| **AFINN** | -5 to +5 | ~2,500 | Weighted sentiment |\r\n| **NRC** | 8 emotions + pos/neg | ~14,000 | Emotion detection |\r\n| **Loughran** | 6 categories | ~4,000 | Financial texts |\r\n\r\n## Basic Sentiment Analysis\r\n\r\n### Binary Sentiment (Bing)\r\n\r\n```r\r\nlibrary(tidyverse)\r\nlibrary(tidytext)\r\n\r\n# Sample data\r\ntexts <- tibble(\r\n  doc_id = 1:4,\r\n  text = c(\r\n    \"I love this amazing product! Highly recommend.\",\r\n    \"Terrible experience. The worst purchase ever.\",\r\n    \"It's okay, nothing special but not bad.\",\r\n    \"Great quality but expensive and slow delivery.\"\r\n  )\r\n)\r\n\r\n# Tokenize\r\ntokens <- texts %>%\r\n  unnest_tokens(word, text)\r\n\r\n# Join with Bing lexicon\r\nsentiment_bing <- tokens %>%\r\n  inner_join(get_sentiments(\"bing\"), by = \"word\")\r\n\r\n# Count by document\r\ndoc_sentiment <- sentiment_bing %>%\r\n  count(doc_id, sentiment) %>%\r\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%\r\n  mutate(sentiment_score = positive - negative)\r\n\r\ndoc_sentiment\r\n# # A tibble: 4  4\r\n#   doc_id negative positive sentiment_score\r\n#    <int>    <int>    <int>           <int>\r\n# 1      1        0        3               3\r\n# 2      2        2        0              -2\r\n# ...\r\n```\r\n\r\n### Weighted Sentiment (AFINN)\r\n\r\n```r\r\n# AFINN gives numeric scores\r\nafinn <- get_sentiments(\"afinn\")\r\n\r\nsentiment_afinn <- tokens %>%\r\n  inner_join(afinn, by = \"word\") %>%\r\n  group_by(doc_id) %>%\r\n  summarise(\r\n    sentiment = sum(value),\r\n    n_words = n(),\r\n    mean_sentiment = mean(value)\r\n  )\r\n\r\nsentiment_afinn\r\n```\r\n\r\n### Emotion Detection (NRC)\r\n\r\n```r\r\nnrc <- get_sentiments(\"nrc\")\r\n\r\n# Get emotion counts\r\nemotions <- tokens %>%\r\n  inner_join(nrc, by = \"word\") %>%\r\n  count(doc_id, sentiment) %>%\r\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0)\r\n\r\nemotions\r\n# Contains: anger, anticipation, disgust, fear, joy,\r\n#           negative, positive, sadness, surprise, trust\r\n```\r\n\r\n## TF-IDF Analysis\r\n\r\n```r\r\n# Calculate TF-IDF\r\ntfidf <- tokens %>%\r\n  count(doc_id, word) %>%\r\n  bind_tf_idf(word, doc_id, n)\r\n\r\n# Top distinctive terms per document\r\ntop_tfidf <- tfidf %>%\r\n  group_by(doc_id) %>%\r\n  slice_max(tf_idf, n = 10)\r\n\r\ntop_tfidf\r\n```\r\n\r\n## Custom Dictionaries\r\n\r\n### Creating a Custom Dictionary\r\n\r\n```r\r\n# Define dictionary as data frame\r\nmy_dictionary <- tibble(\r\n  word = c(\"innovative\", \"disruptive\", \"groundbreaking\", \"revolutionary\",\r\n           \"obsolete\", \"outdated\", \"stagnant\", \"declining\"),\r\n  category = c(rep(\"innovation_positive\", 4), rep(\"innovation_negative\", 4)),\r\n  weight = c(2, 1.5, 2, 2, -1, -1, -1.5, -1)\r\n)\r\n\r\n# Apply dictionary\r\ndict_matches <- tokens %>%\r\n  inner_join(my_dictionary, by = \"word\")\r\n\r\n# Aggregate by document\r\ndict_scores <- dict_matches %>%\r\n  group_by(doc_id, category) %>%\r\n  summarise(\r\n    n_matches = n(),\r\n    weighted_score = sum(weight),\r\n    .groups = \"drop\"\r\n  )\r\n```\r\n\r\n### Multi-Word Expressions\r\n\r\n```r\r\n# For multi-word dictionary terms\r\nmwe_dictionary <- tibble(\r\n  phrase = c(\"machine learning\", \"artificial intelligence\", \"deep learning\"),\r\n  category = \"tech_terms\"\r\n)\r\n\r\n# Search in original text\r\ntexts_with_mwe <- texts %>%\r\n  mutate(\r\n    ml_count = str_count(tolower(text), \"machine learning\"),\r\n    ai_count = str_count(tolower(text), \"artificial intelligence\"),\r\n    dl_count = str_count(tolower(text), \"deep learning\")\r\n  )\r\n```\r\n\r\n## Handling Negation\r\n\r\n### Simple Negation Window\r\n\r\n```r\r\nnegation_words <- c(\"not\", \"no\", \"never\", \"neither\", \"nobody\", \"nothing\",\r\n                    \"nowhere\", \"hardly\", \"barely\", \"scarcely\", \"don't\",\r\n                    \"doesn't\", \"didn't\", \"won't\", \"wouldn't\", \"couldn't\",\r\n                    \"shouldn't\", \"can't\", \"cannot\")\r\n\r\n# Create bigrams to detect negation\r\nnegated <- texts %>%\r\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2) %>%\r\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %>%\r\n  # Check if first word is negation\r\n  mutate(negated = word1 %in% negation_words) %>%\r\n  # Join sentiment on second word\r\n  inner_join(get_sentiments(\"bing\"), by = c(\"word2\" = \"word\")) %>%\r\n  # Flip sentiment if negated\r\n  mutate(sentiment = if_else(negated,\r\n                             if_else(sentiment == \"positive\", \"negative\", \"positive\"),\r\n                             sentiment))\r\n```\r\n\r\n## Sentiment Over Time\r\n\r\n```r\r\n# Sample time-series data\r\ntext_time <- tibble(\r\n  doc_id = 1:100,\r\n  date = seq(as.Date(\"2020-01-01\"), as.Date(\"2020-04-10\"), by = \"day\"),\r\n  text = sample(c(\r\n    \"Great news today! Very positive developments.\",\r\n    \"Terrible situation, concerning trends.\",\r\n    \"Normal day, nothing special happening.\",\r\n    \"Exciting breakthrough, amazing results!\"\r\n  ), 100, replace = TRUE)\r\n)\r\n\r\n# Calculate daily sentiment\r\ndaily_sentiment <- text_time %>%\r\n  unnest_tokens(word, text) %>%\r\n  inner_join(get_sentiments(\"bing\"), by = \"word\") %>%\r\n  count(date, sentiment) %>%\r\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%\r\n  mutate(net_sentiment = positive - negative)\r\n\r\n# Plot\r\nggplot(daily_sentiment, aes(x = date, y = net_sentiment)) +\r\n  geom_line() +\r\n  geom_smooth(method = \"loess\", se = TRUE) +\r\n  labs(title = \"Sentiment Over Time\",\r\n       x = \"Date\", y = \"Net Sentiment\") +\r\n  theme_minimal()\r\n```\r\n\r\n## Comparing Groups\r\n\r\n```r\r\n# Sample grouped data\r\ngrouped_texts <- tibble(\r\n  doc_id = 1:200,\r\n  group = rep(c(\"Democrat\", \"Republican\"), each = 100),\r\n  text = c(\r\n    rep(\"We must protect social programs and support workers.\", 50),\r\n    rep(\"Government overreach is harming small businesses.\", 50),\r\n    rep(\"Tax cuts will stimulate economic growth.\", 50),\r\n    rep(\"Investment in education benefits everyone.\", 50)\r\n  )\r\n)\r\n\r\n# Calculate sentiment by group\r\ngroup_sentiment <- grouped_texts %>%\r\n  unnest_tokens(word, text) %>%\r\n  inner_join(get_sentiments(\"bing\"), by = \"word\") %>%\r\n  count(group, sentiment) %>%\r\n  group_by(group) %>%\r\n  mutate(prop = n / sum(n)) %>%\r\n  filter(sentiment == \"positive\")\r\n\r\n# Visualize\r\nggplot(group_sentiment, aes(x = group, y = prop, fill = group)) +\r\n  geom_col() +\r\n  labs(title = \"Proportion Positive Sentiment by Group\",\r\n       y = \"Proportion Positive\") +\r\n  theme_minimal()\r\n```\r\n\r\n## Validation\r\n\r\n### Coverage Check\r\n\r\n```r\r\n# What proportion of documents have matches?\r\ncoverage <- tokens %>%\r\n  left_join(get_sentiments(\"bing\"), by = \"word\") %>%\r\n  group_by(doc_id) %>%\r\n  summarise(\r\n    n_tokens = n(),\r\n    n_matches = sum(!is.na(sentiment)),\r\n    coverage = n_matches / n_tokens\r\n  )\r\n\r\nsummary(coverage$coverage)\r\n# How many docs have zero matches?\r\nsum(coverage$n_matches == 0)\r\n```\r\n\r\n### KWIC Validation\r\n\r\n```r\r\nlibrary(quanteda)\r\n\r\n# Create corpus\r\ncorp <- corpus(texts$text, docvars = texts[, \"doc_id\"])\r\n\r\n# Keyword in context\r\nkwic_positive <- kwic(tokens(corp), pattern = \"great\", window = 5)\r\nkwic_negative <- kwic(tokens(corp), pattern = \"terrible\", window = 5)\r\n\r\n# Review contexts\r\nprint(kwic_positive)\r\n```\r\n\r\n## Finance-Specific: Loughran-McDonald\r\n\r\n```r\r\n# Loughran-McDonald categories\r\nlm <- get_sentiments(\"loughran\")\r\n\r\ntable(lm$sentiment)\r\n# constraining   litigious    negative  positive superfluous  uncertainty\r\n#          184         903        2355         354          56          297\r\n\r\n# Apply to financial text\r\nfinancial_text <- tibble(\r\n  text = \"The company reported weak earnings amid uncertain market conditions and ongoing litigation.\"\r\n)\r\n\r\nfinancial_sentiment <- financial_text %>%\r\n  unnest_tokens(word, text) %>%\r\n  inner_join(lm, by = \"word\") %>%\r\n  count(sentiment)\r\n\r\nfinancial_sentiment\r\n```\r\n\r\n## Output Tables\r\n\r\n### Sentiment Summary Table\r\n\r\n```r\r\ncreate_sentiment_table <- function(tokens_df, lexicon = \"bing\") {\r\n  lex <- get_sentiments(lexicon)\r\n\r\n  tokens_df %>%\r\n    inner_join(lex, by = \"word\") %>%\r\n    count(doc_id, sentiment) %>%\r\n    pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%\r\n    mutate(\r\n      total = positive + negative,\r\n      net = positive - negative,\r\n      ratio = positive / (positive + negative)\r\n    )\r\n}\r\n\r\nsentiment_summary <- create_sentiment_table(tokens)\r\n```\r\n\r\n### Top Words Table\r\n\r\n```r\r\n# Most common sentiment words\r\ntop_sentiment_words <- tokens %>%\r\n  inner_join(get_sentiments(\"bing\"), by = \"word\") %>%\r\n  count(word, sentiment, sort = TRUE) %>%\r\n  group_by(sentiment) %>%\r\n  slice_max(n, n = 10)\r\n\r\n# Visualize\r\ntop_sentiment_words %>%\r\n  mutate(n = if_else(sentiment == \"negative\", -n, n)) %>%\r\n  ggplot(aes(x = reorder(word, n), y = n, fill = sentiment)) +\r\n  geom_col() +\r\n  coord_flip() +\r\n  labs(title = \"Top Sentiment Words\",\r\n       x = \"Word\", y = \"Frequency\") +\r\n  theme_minimal()\r\n```\r\n\r\n## Complete Workflow Example\r\n\r\n```r\r\n#' Sentiment Analysis Pipeline\r\n#' @param texts_df Data frame with doc_id and text columns\r\n#' @param lexicon Name of lexicon (\"bing\", \"afinn\", \"nrc\")\r\n#' @return List with sentiment scores and validation stats\r\n\r\nsentiment_pipeline <- function(texts_df, lexicon = \"bing\") {\r\n\r\n  # Tokenize\r\n  tokens <- texts_df %>%\r\n    unnest_tokens(word, text) %>%\r\n    anti_join(stop_words, by = \"word\")\r\n\r\n  # Get lexicon\r\n  lex <- get_sentiments(lexicon)\r\n\r\n  # Join and aggregate\r\n  if (lexicon == \"afinn\") {\r\n    scores <- tokens %>%\r\n      inner_join(lex, by = \"word\") %>%\r\n      group_by(doc_id) %>%\r\n      summarise(\r\n        sentiment = sum(value),\r\n        n_matches = n()\r\n      )\r\n  } else {\r\n    scores <- tokens %>%\r\n      inner_join(lex, by = \"word\") %>%\r\n      count(doc_id, sentiment) %>%\r\n      pivot_wider(names_from = sentiment, values_from = n, values_fill = 0)\r\n  }\r\n\r\n  # Coverage stats\r\n  coverage <- tokens %>%\r\n    left_join(lex, by = \"word\") %>%\r\n    group_by(doc_id) %>%\r\n    summarise(\r\n      n_tokens = n(),\r\n      n_matches = sum(!is.na(value) | !is.na(sentiment)),\r\n      coverage = n_matches / n_tokens\r\n    )\r\n\r\n  list(\r\n    scores = scores,\r\n    coverage = coverage,\r\n    summary = summary(coverage$coverage)\r\n  )\r\n}\r\n\r\n# Usage\r\nresults <- sentiment_pipeline(texts, lexicon = \"afinn\")\r\n```\r\n",
        "plugins/text-analyst/skills/text-analyst/r-techniques/03_topic_models.md": "# Topic Models in R\r\n\r\n## Package Versions\r\n\r\n```r\r\n# Tested with:\r\n# R 4.3.0\r\n# stm 1.3.6.1\r\n# topicmodels 0.2-14\r\n# tidytext 0.4.1\r\n# quanteda 3.3.1\r\n```\r\n\r\n## Installation\r\n\r\n```r\r\ninstall.packages(c(\"stm\", \"topicmodels\", \"tidytext\", \"quanteda\", \"LDAvis\"))\r\n```\r\n\r\n## Structural Topic Model (STM) - Recommended\r\n\r\n### Why STM?\r\n\r\n- Topic prevalence can vary by covariates\r\n- Topic content can vary by covariates\r\n- Better diagnostics (coherence + exclusivity)\r\n- Spectral initialization (more stable)\r\n- Correlation between topics modeled\r\n\r\n### Basic STM Workflow\r\n\r\n```r\r\nlibrary(stm)\r\nlibrary(tidyverse)\r\n\r\n# Sample data with covariates\r\ntexts <- tibble(\r\n  doc_id = 1:100,\r\n  text = sample(c(\r\n    \"The economy shows signs of growth with increased employment.\",\r\n    \"Healthcare reform remains a divisive political issue.\",\r\n    \"Climate change impacts are becoming more severe.\",\r\n    \"Education funding varies widely across states.\"\r\n  ), 100, replace = TRUE),\r\n  year = sample(2015:2022, 100, replace = TRUE),\r\n  source = sample(c(\"conservative\", \"liberal\"), 100, replace = TRUE)\r\n)\r\n\r\n# Prepare data for STM\r\nprocessed <- textProcessor(\r\n  documents = texts$text,\r\n  metadata = texts,\r\n  lowercase = TRUE,\r\n  removestopwords = TRUE,\r\n  removenumbers = TRUE,\r\n  removepunctuation = TRUE,\r\n  stem = TRUE\r\n)\r\n\r\n# Prepare documents\r\nout <- prepDocuments(\r\n  processed$documents,\r\n  processed$vocab,\r\n  processed$meta,\r\n  lower.thresh = 5  # Min word frequency\r\n)\r\n\r\n# Check removed\r\nout$docs.removed  # Documents removed\r\nout$words.removed # Words removed\r\n```\r\n\r\n### Fitting STM\r\n\r\n```r\r\n# Basic STM (no covariates)\r\nstm_basic <- stm(\r\n  documents = out$documents,\r\n  vocab = out$vocab,\r\n  K = 10,\r\n  data = out$meta,\r\n  init.type = \"Spectral\",\r\n  seed = 42\r\n)\r\n\r\n# STM with prevalence covariates\r\nstm_prevalence <- stm(\r\n  documents = out$documents,\r\n  vocab = out$vocab,\r\n  K = 10,\r\n  prevalence = ~ year + source,\r\n  data = out$meta,\r\n  init.type = \"Spectral\",\r\n  seed = 42\r\n)\r\n\r\n# STM with content covariates\r\nstm_content <- stm(\r\n  documents = out$documents,\r\n  vocab = out$vocab,\r\n  K = 10,\r\n  prevalence = ~ year + source,\r\n  content = ~ source,\r\n  data = out$meta,\r\n  init.type = \"Spectral\",\r\n  seed = 42\r\n)\r\n```\r\n\r\n### Examining Topics\r\n\r\n```r\r\n# Top words per topic (various weightings)\r\nlabelTopics(stm_prevalence, n = 10)\r\n\r\n# Highest Prob: Most probable words\r\n# FREX: Frequent and Exclusive (good for labeling)\r\n# Lift: Words weighted by exclusivity\r\n# Score: Combined measure\r\n\r\n# Get as data frame\r\ntopic_words <- tidy(stm_prevalence, matrix = \"beta\")\r\n\r\n# Top FREX words per topic\r\nfrex_words <- labelTopics(stm_prevalence, n = 10)$frex\r\n```\r\n\r\n### Representative Documents\r\n\r\n```r\r\n# Find documents most associated with each topic\r\nfindThoughts(stm_prevalence,\r\n             texts = texts$text,\r\n             n = 3,           # Documents per topic\r\n             topics = 1:10)   # Which topics\r\n\r\n# Plot document-topic associations\r\nplot(stm_prevalence, type = \"summary\", n = 5)\r\n```\r\n\r\n### Topic Prevalence Over Time\r\n\r\n```r\r\n# Estimate effect of year on topic prevalence\r\neffect <- estimateEffect(1:10 ~ year + source,\r\n                        stm_prevalence,\r\n                        meta = out$meta)\r\n\r\n# Summary\r\nsummary(effect, topics = 1)\r\n\r\n# Plot effect of year on topic 1\r\nplot(effect, covariate = \"year\",\r\n     topics = 1,\r\n     method = \"continuous\",\r\n     main = \"Topic 1 Prevalence by Year\")\r\n\r\n# Plot effect of source (categorical)\r\nplot(effect, covariate = \"source\",\r\n     topics = 1:5,\r\n     method = \"difference\",\r\n     cov.value1 = \"liberal\",\r\n     cov.value2 = \"conservative\",\r\n     main = \"Topic Prevalence: Liberal vs Conservative\")\r\n```\r\n\r\n### Model Diagnostics\r\n\r\n```r\r\n# Coherence and exclusivity\r\nexclusivity <- exclusivity(stm_prevalence)\r\ncoherence <- semanticCoherence(stm_prevalence, out$documents)\r\n\r\n# Plot trade-off\r\ntibble(\r\n  topic = 1:10,\r\n  coherence = coherence,\r\n  exclusivity = exclusivity\r\n) %>%\r\n  ggplot(aes(x = coherence, y = exclusivity, label = topic)) +\r\n  geom_point() +\r\n  geom_text(nudge_y = 0.01) +\r\n  labs(title = \"Topic Quality: Coherence vs Exclusivity\",\r\n       x = \"Semantic Coherence\",\r\n       y = \"Exclusivity\") +\r\n  theme_minimal()\r\n```\r\n\r\n### Selecting K\r\n\r\n```r\r\n# Search over K values\r\nk_search <- searchK(\r\n  documents = out$documents,\r\n  vocab = out$vocab,\r\n  K = c(5, 10, 15, 20, 25),\r\n  prevalence = ~ year + source,\r\n  data = out$meta,\r\n  init.type = \"Spectral\",\r\n  seed = 42\r\n)\r\n\r\n# Plot diagnostics\r\nplot(k_search)\r\n\r\n# Access metrics\r\nk_search$results\r\n```\r\n\r\n### Topic Correlation\r\n\r\n```r\r\n# Get topic correlations\r\ntopic_corr <- topicCorr(stm_prevalence)\r\n\r\n# Plot as network\r\nplot(topic_corr)\r\n```\r\n\r\n## LDA with topicmodels\r\n\r\nFor simpler topic modeling without covariates.\r\n\r\n```r\r\nlibrary(topicmodels)\r\nlibrary(tidytext)\r\n\r\n# Create document-term matrix\r\ndtm <- texts %>%\r\n  unnest_tokens(word, text) %>%\r\n  anti_join(stop_words, by = \"word\") %>%\r\n  count(doc_id, word) %>%\r\n  cast_dtm(doc_id, word, n)\r\n\r\n# Fit LDA\r\nlda_model <- LDA(dtm, k = 10,\r\n                 control = list(seed = 42))\r\n\r\n# Extract topics\r\ntopics <- tidy(lda_model, matrix = \"beta\")\r\n\r\n# Top words per topic\r\ntop_terms <- topics %>%\r\n  group_by(topic) %>%\r\n  slice_max(beta, n = 10) %>%\r\n  ungroup()\r\n\r\n# Document-topic probabilities\r\ndoc_topics <- tidy(lda_model, matrix = \"gamma\")\r\n```\r\n\r\n### Visualizing LDA\r\n\r\n```r\r\n# Top terms visualization\r\ntop_terms %>%\r\n  mutate(term = reorder_within(term, beta, topic)) %>%\r\n  ggplot(aes(beta, term, fill = factor(topic))) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~ topic, scales = \"free\") +\r\n  scale_y_reordered() +\r\n  labs(title = \"Top Terms per Topic\",\r\n       x = \"Beta (word probability)\",\r\n       y = \"Term\") +\r\n  theme_minimal()\r\n```\r\n\r\n## Validation\r\n\r\n### Human Validation: Word Intrusion\r\n\r\n```r\r\n# Generate word intrusion test\r\ngenerate_intrusion_test <- function(model, n_words = 5) {\r\n  # Get top words per topic\r\n  top_words <- labelTopics(model, n = n_words)$frex\r\n\r\n  # For each topic, add intruder from another\r\n  tests <- map(1:nrow(top_words), function(i) {\r\n    topic_words <- top_words[i, ]\r\n    # Get intruder from different topic\r\n    intruder_topic <- sample(setdiff(1:nrow(top_words), i), 1)\r\n    intruder <- sample(top_words[intruder_topic, ], 1)\r\n\r\n    tibble(\r\n      topic = i,\r\n      words = list(c(topic_words, intruder)),\r\n      intruder = intruder\r\n    )\r\n  })\r\n\r\n  bind_rows(tests)\r\n}\r\n\r\nintrusion_test <- generate_intrusion_test(stm_prevalence)\r\n```\r\n\r\n### Robustness: Multiple Seeds\r\n\r\n```r\r\n# Run with multiple seeds\r\nseeds <- c(42, 123, 456, 789, 1011)\r\n\r\nmulti_seed_stm <- map(seeds, function(s) {\r\n  stm(\r\n    documents = out$documents,\r\n    vocab = out$vocab,\r\n    K = 10,\r\n    prevalence = ~ year + source,\r\n    data = out$meta,\r\n    init.type = \"Spectral\",\r\n    seed = s,\r\n    verbose = FALSE\r\n  )\r\n})\r\n\r\n# Compare coherence across seeds\r\ncoherences <- map_dfc(multi_seed_stm, function(m) {\r\n  semanticCoherence(m, out$documents)\r\n})\r\n\r\n# Check topic alignment across seeds\r\n# (Topics may appear in different order)\r\n```\r\n\r\n## Output: Publication Tables\r\n\r\n### Topic Summary Table\r\n\r\n```r\r\ncreate_topic_table <- function(model, n_words = 7) {\r\n  # Get document-topic proportions\r\n  theta <- make.dt(model)\r\n  prevalence <- colMeans(theta)\r\n\r\n  # Get top words\r\n  top_words <- labelTopics(model, n = n_words)$frex\r\n\r\n  tibble(\r\n    Topic = 1:nrow(top_words),\r\n    Prevalence = scales::percent(prevalence, accuracy = 0.1),\r\n    `Top Words (FREX)` = apply(top_words, 1, paste, collapse = \", \")\r\n  )\r\n}\r\n\r\ntopic_table <- create_topic_table(stm_prevalence)\r\n\r\n# Export\r\nwrite_csv(topic_table, \"output/tables/topic_summary.csv\")\r\n```\r\n\r\n### Covariate Effects Table\r\n\r\n```r\r\n# Extract effects\r\neffect_summary <- summary(effect, topics = 1:10)\r\n\r\n# Format for publication\r\neffect_table <- map_dfr(1:10, function(t) {\r\n  est <- effect_summary$tables[[t]]\r\n  tibble(\r\n    Topic = t,\r\n    Variable = rownames(est),\r\n    Estimate = est[, \"Estimate\"],\r\n    SE = est[, \"Std. Error\"],\r\n    p_value = est[, \"Pr(>|t|)\"]\r\n  )\r\n})\r\n```\r\n\r\n## LDAvis Interactive Visualization\r\n\r\n```r\r\nlibrary(LDAvis)\r\n\r\n# For STM\r\ntoLDAvis(stm_prevalence, out$documents)\r\n\r\n# For topicmodels LDA\r\nlibrary(LDAvis)\r\nlibrary(slam)\r\n\r\n# Extract components\r\nphi <- posterior(lda_model)$terms\r\ntheta <- posterior(lda_model)$topics\r\nvocab <- colnames(phi)\r\ndoc_length <- rowSums(as.matrix(dtm))\r\nterm_freq <- colSums(as.matrix(dtm))\r\n\r\n# Create JSON\r\njson <- createJSON(\r\n  phi = phi,\r\n  theta = theta,\r\n  vocab = vocab,\r\n  doc.length = doc_length,\r\n  term.frequency = term_freq\r\n)\r\n\r\nserVis(json)\r\n```\r\n\r\n## Complete Workflow\r\n\r\n```r\r\n#' Topic Model Pipeline\r\n#' @param texts_df Data frame with doc_id, text, and metadata\r\n#' @param k Number of topics\r\n#' @param prevalence_formula Formula for prevalence covariates\r\n#' @return List with model, diagnostics, and tables\r\n\r\ntopic_model_pipeline <- function(texts_df, k = 10,\r\n                                  prevalence_formula = NULL,\r\n                                  seed = 42) {\r\n\r\n  # Prepare\r\n  processed <- textProcessor(\r\n    documents = texts_df$text,\r\n    metadata = texts_df,\r\n    lowercase = TRUE,\r\n    removestopwords = TRUE,\r\n    removenumbers = TRUE,\r\n    removepunctuation = TRUE,\r\n    stem = TRUE\r\n  )\r\n\r\n  out <- prepDocuments(\r\n    processed$documents,\r\n    processed$vocab,\r\n    processed$meta,\r\n    lower.thresh = 5\r\n  )\r\n\r\n  # Fit model\r\n  if (is.null(prevalence_formula)) {\r\n    model <- stm(\r\n      documents = out$documents,\r\n      vocab = out$vocab,\r\n      K = k,\r\n      data = out$meta,\r\n      init.type = \"Spectral\",\r\n      seed = seed\r\n    )\r\n  } else {\r\n    model <- stm(\r\n      documents = out$documents,\r\n      vocab = out$vocab,\r\n      K = k,\r\n      prevalence = prevalence_formula,\r\n      data = out$meta,\r\n      init.type = \"Spectral\",\r\n      seed = seed\r\n    )\r\n  }\r\n\r\n  # Diagnostics\r\n  diagnostics <- tibble(\r\n    topic = 1:k,\r\n    coherence = semanticCoherence(model, out$documents),\r\n    exclusivity = exclusivity(model)\r\n  )\r\n\r\n  # Topic table\r\n  topic_table <- create_topic_table(model)\r\n\r\n  list(\r\n    model = model,\r\n    out = out,\r\n    diagnostics = diagnostics,\r\n    topic_table = topic_table\r\n  )\r\n}\r\n\r\n# Usage\r\nresults <- topic_model_pipeline(\r\n  texts,\r\n  k = 10,\r\n  prevalence_formula = ~ year + source\r\n)\r\n```\r\n",
        "plugins/text-analyst/skills/text-analyst/r-techniques/04_supervised.md": "# Supervised Text Classification in R\r\n\r\n## Package Versions\r\n\r\n```r\r\n# Tested with:\r\n# R 4.3.0\r\n# tidymodels 1.1.0\r\n# textrecipes 1.0.4\r\n# tidytext 0.4.1\r\n```\r\n\r\n## Installation\r\n\r\n```r\r\ninstall.packages(c(\"tidymodels\", \"textrecipes\", \"tidytext\", \"discrim\"))\r\n```\r\n\r\n## tidymodels Workflow\r\n\r\n### Sample Data\r\n\r\n```r\r\nlibrary(tidyverse)\r\nlibrary(tidymodels)\r\nlibrary(textrecipes)\r\n\r\n# Sample labeled data\r\nset.seed(42)\r\ntexts <- tibble(\r\n  doc_id = 1:200,\r\n  text = c(\r\n    rep(\"The government passed new tax legislation today.\", 50),\r\n    rep(\"The championship game ended in an exciting overtime.\", 50),\r\n    rep(\"Stock markets reached record highs amid economic growth.\", 50),\r\n    rep(\"Scientists discovered a new species in the rainforest.\", 50)\r\n  ) %>% sample(),\r\n  label = c(\r\n    rep(\"politics\", 50),\r\n    rep(\"sports\", 50),\r\n    rep(\"business\", 50),\r\n    rep(\"science\", 50)\r\n  ) %>% sample()\r\n) %>%\r\n  mutate(label = factor(label))\r\n```\r\n\r\n### Train/Test Split\r\n\r\n```r\r\n# Stratified split\r\nset.seed(42)\r\nsplit <- initial_split(texts, prop = 0.8, strata = label)\r\ntrain <- training(split)\r\ntest <- testing(split)\r\n\r\n# Check balance\r\ntable(train$label)\r\ntable(test$label)\r\n```\r\n\r\n### Feature Engineering with Recipes\r\n\r\n```r\r\n# Basic text recipe\r\ntext_recipe <- recipe(label ~ text, data = train) %>%\r\n  step_tokenize(text) %>%                    # Tokenize\r\n  step_stopwords(text) %>%                   # Remove stopwords\r\n  step_stem(text) %>%                        # Stem\r\n  step_tokenfilter(text, max_tokens = 500) %>%  # Keep top N tokens\r\n  step_tfidf(text)                           # TF-IDF features\r\n\r\n# Alternative: word embeddings (requires pretrained)\r\n# step_word_embeddings(text, embeddings = glove_embeddings)\r\n\r\n# Preview the preprocessing\r\ntext_recipe %>%\r\n  prep() %>%\r\n  bake(new_data = train) %>%\r\n  glimpse()\r\n```\r\n### Model Specification\r\n\r\n```r\r\n# Logistic regression (multinomial for >2 classes)\r\nlr_spec <- multinom_reg(penalty = 0.01, mixture = 1) %>%\r\n  set_engine(\"glmnet\") %>%\r\n  set_mode(\"classification\")\r\n\r\n# Naive Bayes\r\nlibrary(discrim)\r\nnb_spec <- naive_Bayes() %>%\r\n  set_engine(\"naivebayes\") %>%\r\n  set_mode(\"classification\")\r\n\r\n# Random Forest\r\nrf_spec <- rand_forest(trees = 500) %>%\r\n  set_engine(\"ranger\") %>%\r\n  set_mode(\"classification\")\r\n\r\n# SVM\r\nsvm_spec <- svm_linear(cost = 1) %>%\r\n  set_engine(\"LiblineaR\") %>%\r\n  set_mode(\"classification\")\r\n```\r\n\r\n### Creating a Workflow\r\n\r\n```r\r\n# Combine recipe and model\r\nlr_workflow <- workflow() %>%\r\n  add_recipe(text_recipe) %>%\r\n  add_model(lr_spec)\r\n\r\n# Fit on training data\r\nlr_fit <- lr_workflow %>%\r\n  fit(data = train)\r\n```\r\n\r\n### Evaluation\r\n\r\n```r\r\n# Predictions on test set\r\npredictions <- lr_fit %>%\r\n  predict(test) %>%\r\n  bind_cols(test)\r\n\r\n# Metrics\r\nmetrics <- predictions %>%\r\n  metrics(truth = label, estimate = .pred_class)\r\n\r\n# Confusion matrix\r\nconf_mat <- predictions %>%\r\n  conf_mat(truth = label, estimate = .pred_class)\r\n\r\n# Visualize\r\nautoplot(conf_mat, type = \"heatmap\")\r\n\r\n# Per-class metrics\r\npredictions %>%\r\n  f_meas(truth = label, estimate = .pred_class, estimator = \"macro\")\r\n\r\n# Full classification report\r\npredictions %>%\r\n  conf_mat(truth = label, estimate = .pred_class) %>%\r\n  summary()\r\n```\r\n\r\n### Cross-Validation\r\n\r\n```r\r\n# Create folds\r\nset.seed(42)\r\nfolds <- vfold_cv(train, v = 5, strata = label)\r\n\r\n# Fit with cross-validation\r\ncv_results <- lr_workflow %>%\r\n  fit_resamples(\r\n    resamples = folds,\r\n    metrics = metric_set(accuracy, f_meas),\r\n    control = control_resamples(save_pred = TRUE)\r\n  )\r\n\r\n# Collect metrics\r\ncollect_metrics(cv_results)\r\n\r\n# Collect predictions from all folds\r\ncv_predictions <- collect_predictions(cv_results)\r\n\r\n# Overall confusion matrix\r\ncv_predictions %>%\r\n  conf_mat(truth = label, estimate = .pred_class)\r\n```\r\n\r\n### Hyperparameter Tuning\r\n\r\n```r\r\n# Specify model with tuning parameters\r\nlr_tune_spec <- multinom_reg(\r\n  penalty = tune(),\r\n  mixture = tune()\r\n) %>%\r\n  set_engine(\"glmnet\") %>%\r\n  set_mode(\"classification\")\r\n\r\n# Tuning workflow\r\ntune_workflow <- workflow() %>%\r\n  add_recipe(text_recipe) %>%\r\n  add_model(lr_tune_spec)\r\n\r\n# Grid of parameters\r\nlr_grid <- grid_regular(\r\n  penalty(range = c(-4, 0)),\r\n  mixture(range = c(0, 1)),\r\n  levels = 5\r\n)\r\n\r\n# Tune with cross-validation\r\ntune_results <- tune_workflow %>%\r\n  tune_grid(\r\n    resamples = folds,\r\n    grid = lr_grid,\r\n    metrics = metric_set(accuracy, f_meas)\r\n  )\r\n\r\n# Best parameters\r\nbest_params <- tune_results %>%\r\n  select_best(metric = \"f_meas\")\r\n\r\n# Finalize workflow\r\nfinal_workflow <- tune_workflow %>%\r\n  finalize_workflow(best_params)\r\n\r\n# Final fit on full training data\r\nfinal_fit <- final_workflow %>%\r\n  fit(data = train)\r\n\r\n# Evaluate on test\r\nfinal_predictions <- final_fit %>%\r\n  predict(test) %>%\r\n  bind_cols(test)\r\n\r\nfinal_predictions %>%\r\n  metrics(truth = label, estimate = .pred_class)\r\n```\r\n\r\n## Comparing Multiple Models\r\n\r\n```r\r\n# Define models\r\nmodels <- list(\r\n  \"Logistic Regression\" = lr_spec,\r\n  \"Naive Bayes\" = nb_spec,\r\n  \"Random Forest\" = rf_spec\r\n)\r\n\r\n# Fit each with cross-validation\r\nmodel_results <- map_dfr(names(models), function(name) {\r\n  workflow() %>%\r\n    add_recipe(text_recipe) %>%\r\n    add_model(models[[name]]) %>%\r\n    fit_resamples(\r\n      resamples = folds,\r\n      metrics = metric_set(accuracy, f_meas)\r\n    ) %>%\r\n    collect_metrics() %>%\r\n    mutate(model = name)\r\n})\r\n\r\n# Compare\r\nmodel_results %>%\r\n  ggplot(aes(x = model, y = mean, fill = .metric)) +\r\n  geom_col(position = \"dodge\") +\r\n  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err),\r\n                position = position_dodge(width = 0.9), width = 0.2) +\r\n  labs(title = \"Model Comparison\",\r\n       y = \"Metric Value\") +\r\n  theme_minimal()\r\n```\r\n\r\n## Feature Importance\r\n\r\n```r\r\n# For glmnet models\r\nlibrary(vip)\r\n\r\n# Extract fitted model\r\nfitted_model <- extract_fit_parsnip(lr_fit)\r\n\r\n# Variable importance\r\nvip(fitted_model, num_features = 20)\r\n\r\n# Get coefficients\r\ntidy(fitted_model) %>%\r\n  filter(class == \"politics\") %>%  # For one class\r\n  arrange(desc(abs(estimate))) %>%\r\n  head(20)\r\n```\r\n\r\n## Handling Class Imbalance\r\n\r\n```r\r\n# Check imbalance\r\ntable(train$label)\r\n\r\n# Option 1: Upsample minority\r\nlibrary(themis)\r\n\r\nbalanced_recipe <- recipe(label ~ text, data = train) %>%\r\n  step_tokenize(text) %>%\r\n  step_stopwords(text) %>%\r\n  step_tokenfilter(text, max_tokens = 500) %>%\r\n  step_tfidf(text) %>%\r\n  step_upsample(label)  # Upsample minority\r\n\r\n# Option 2: Downsample majority\r\nbalanced_recipe2 <- recipe(label ~ text, data = train) %>%\r\n  step_tokenize(text) %>%\r\n  step_stopwords(text) %>%\r\n  step_tokenfilter(text, max_tokens = 500) %>%\r\n  step_tfidf(text) %>%\r\n  step_downsample(label)\r\n\r\n# Option 3: Class weights in model\r\nweighted_spec <- multinom_reg(penalty = 0.01) %>%\r\n  set_engine(\"glmnet\") %>%\r\n  set_mode(\"classification\")\r\n# Note: glmnet supports class.weights parameter\r\n```\r\n\r\n## Binary Classification\r\n\r\n```r\r\n# Binary example\r\nbinary_texts <- texts %>%\r\n  filter(label %in% c(\"politics\", \"sports\")) %>%\r\n  mutate(label = factor(label))\r\n\r\n# Split\r\nsplit_binary <- initial_split(binary_texts, prop = 0.8, strata = label)\r\ntrain_binary <- training(split_binary)\r\ntest_binary <- testing(split_binary)\r\n\r\n# Logistic regression (binary)\r\nbinary_spec <- logistic_reg(penalty = 0.01) %>%\r\n  set_engine(\"glmnet\") %>%\r\n  set_mode(\"classification\")\r\n\r\n# Workflow\r\nbinary_workflow <- workflow() %>%\r\n  add_recipe(text_recipe) %>%\r\n  add_model(binary_spec)\r\n\r\n# Fit and predict\r\nbinary_fit <- binary_workflow %>%\r\n  fit(data = train_binary)\r\n\r\nbinary_pred <- binary_fit %>%\r\n  predict(test_binary, type = \"prob\") %>%\r\n  bind_cols(\r\n    predict(binary_fit, test_binary)\r\n  ) %>%\r\n  bind_cols(test_binary)\r\n\r\n# ROC curve\r\nbinary_pred %>%\r\n  roc_curve(truth = label, .pred_politics) %>%\r\n  autoplot()\r\n\r\n# AUC\r\nbinary_pred %>%\r\n  roc_auc(truth = label, .pred_politics)\r\n```\r\n\r\n## Output: Publication Tables\r\n\r\n### Classification Performance Table\r\n\r\n```r\r\ncreate_performance_table <- function(predictions, truth_col, pred_col) {\r\n  # Overall metrics\r\n  overall <- predictions %>%\r\n    metrics(truth = !!sym(truth_col), estimate = !!sym(pred_col))\r\n\r\n  # Per-class\r\n  cm <- predictions %>%\r\n    conf_mat(truth = !!sym(truth_col), estimate = !!sym(pred_col))\r\n\r\n  per_class <- summary(cm) %>%\r\n    filter(.metric %in% c(\"precision\", \"recall\", \"f_meas\")) %>%\r\n    pivot_wider(names_from = .metric, values_from = .estimate)\r\n\r\n  list(\r\n    overall = overall,\r\n    per_class = per_class,\r\n    confusion_matrix = cm\r\n  )\r\n}\r\n\r\nresults <- create_performance_table(predictions, \"label\", \".pred_class\")\r\n```\r\n\r\n### Export Results\r\n\r\n```r\r\n# Save model\r\nsaveRDS(lr_fit, \"output/models/text_classifier.rds\")\r\n\r\n# Save results\r\nwrite_csv(results$overall, \"output/tables/classification_overall.csv\")\r\nwrite_csv(results$per_class, \"output/tables/classification_by_class.csv\")\r\n```\r\n\r\n## Complete Workflow\r\n\r\n```r\r\n#' Text Classification Pipeline\r\n#' @param train_df Training data with text and label columns\r\n#' @param test_df Test data\r\n#' @param max_tokens Maximum vocabulary size\r\n#' @return List with model, predictions, and metrics\r\n\r\ntext_classification_pipeline <- function(train_df, test_df,\r\n                                         max_tokens = 1000) {\r\n\r\n  # Recipe\r\n  rec <- recipe(label ~ text, data = train_df) %>%\r\n    step_tokenize(text) %>%\r\n    step_stopwords(text) %>%\r\n    step_stem(text) %>%\r\n    step_tokenfilter(text, max_tokens = max_tokens) %>%\r\n    step_tfidf(text)\r\n\r\n  # Model\r\n  spec <- multinom_reg(penalty = 0.01, mixture = 1) %>%\r\n    set_engine(\"glmnet\") %>%\r\n    set_mode(\"classification\")\r\n\r\n  # Workflow\r\n  wf <- workflow() %>%\r\n    add_recipe(rec) %>%\r\n    add_model(spec)\r\n\r\n  # Cross-validation\r\n  set.seed(42)\r\n  folds <- vfold_cv(train_df, v = 5, strata = label)\r\n\r\n  cv_results <- wf %>%\r\n    fit_resamples(\r\n      resamples = folds,\r\n      metrics = metric_set(accuracy, f_meas)\r\n    )\r\n\r\n  # Final fit\r\n  final_fit <- wf %>%\r\n    fit(data = train_df)\r\n\r\n  # Test predictions\r\n  test_pred <- final_fit %>%\r\n    predict(test_df) %>%\r\n    bind_cols(test_df)\r\n\r\n  # Metrics\r\n  test_metrics <- test_pred %>%\r\n    metrics(truth = label, estimate = .pred_class)\r\n\r\n  list(\r\n    model = final_fit,\r\n    cv_metrics = collect_metrics(cv_results),\r\n    test_predictions = test_pred,\r\n    test_metrics = test_metrics,\r\n    confusion_matrix = conf_mat(test_pred, truth = label,\r\n                                estimate = .pred_class)\r\n  )\r\n}\r\n\r\n# Usage\r\nresults <- text_classification_pipeline(train, test)\r\nprint(results$test_metrics)\r\nautoplot(results$confusion_matrix, type = \"heatmap\")\r\n```\r\n",
        "plugins/text-analyst/skills/text-analyst/r-techniques/05_embeddings.md": "# Word and Document Embeddings in R\r\n\r\n## Package Versions\r\n\r\n```r\r\n# Tested with:\r\n# R 4.3.0\r\n# text2vec 0.6.3\r\n# word2vec 0.3.4\r\n# tidyverse 2.0.0\r\n```\r\n\r\n## Installation\r\n\r\n```r\r\ninstall.packages(c(\"text2vec\", \"word2vec\", \"tidyverse\", \"umap\", \"Rtsne\"))\r\n```\r\n\r\n## Word2Vec with word2vec Package\r\n\r\n### Training Word2Vec\r\n\r\n```r\r\nlibrary(word2vec)\r\nlibrary(tidyverse)\r\n\r\n# Sample corpus\r\ntexts <- c(\r\n  \"The economy shows signs of growth with increased employment.\",\r\n  \"Healthcare reform remains a divisive political issue.\",\r\n  \"Climate change impacts are becoming more severe.\",\r\n  \"Education funding varies widely across states.\",\r\n  \"Machine learning is transforming data analysis.\",\r\n  \"Social media influences political discourse.\"\r\n)\r\n\r\n# Save as file (required by word2vec)\r\nwriteLines(texts, \"temp_corpus.txt\")\r\n\r\n# Train model\r\nw2v_model <- word2vec(\r\n  x = \"temp_corpus.txt\",\r\n  type = \"skip-gram\",  # or \"cbow\"\r\n  dim = 100,           # Embedding dimensions\r\n  window = 5,          # Context window\r\n  iter = 5,            # Epochs\r\n  min_count = 1,       # Minimum word frequency\r\n  threads = 4\r\n)\r\n\r\n# Clean up\r\nfile.remove(\"temp_corpus.txt\")\r\n```\r\n\r\n### Using Word2Vec\r\n\r\n```r\r\n# Get word vector\r\nword_vector <- predict(w2v_model, \"economy\", type = \"embedding\")\r\n\r\n# Find similar words\r\nsimilar <- predict(w2v_model, \"economy\", type = \"nearest\", top_n = 10)\r\nprint(similar)\r\n\r\n# Word analogies\r\n# Note: requires large corpus to work well\r\n# predict(w2v_model, c(\"king\", \"woman\", \"man\"), type = \"nearest\")\r\n\r\n# Get all embeddings as matrix\r\nembeddings_matrix <- as.matrix(w2v_model)\r\ndim(embeddings_matrix)\r\n```\r\n\r\n### Document Embeddings (Average)\r\n\r\n```r\r\n# Average word vectors for document\r\nget_doc_embedding <- function(text, model) {\r\n  words <- tolower(strsplit(text, \"\\\\s+\")[[1]])\r\n  words <- words[words %in% rownames(as.matrix(model))]\r\n\r\n  if (length(words) == 0) return(rep(0, dim(as.matrix(model))[2]))\r\n\r\n  embeddings <- predict(model, words, type = \"embedding\")\r\n  if (is.vector(embeddings)) embeddings <- matrix(embeddings, nrow = 1)\r\n\r\n  colMeans(embeddings)\r\n}\r\n\r\n# Apply to corpus\r\ndoc_embeddings <- t(sapply(texts, function(t) get_doc_embedding(t, w2v_model)))\r\nrownames(doc_embeddings) <- paste0(\"doc_\", 1:length(texts))\r\n```\r\n\r\n## GloVe with text2vec\r\n\r\n### Preparing Data\r\n\r\n```r\r\nlibrary(text2vec)\r\n\r\n# Create iterator\r\ntokens <- space_tokenizer(tolower(texts))\r\nit <- itoken(tokens, progressbar = FALSE)\r\n\r\n# Create vocabulary\r\nvocab <- create_vocabulary(it)\r\n\r\n# Prune vocabulary\r\nvocab <- prune_vocabulary(vocab, term_count_min = 1)\r\n\r\n# Create term co-occurrence matrix\r\nvectorizer <- vocab_vectorizer(vocab)\r\ntcm <- create_tcm(it, vectorizer, skip_grams_window = 5)\r\n```\r\n\r\n### Training GloVe\r\n\r\n```r\r\n# Initialize GloVe model\r\nglove <- GlobalVectors$new(rank = 50, x_max = 10)\r\n\r\n# Fit (returns word vectors)\r\nwv_main <- glove$fit_transform(tcm, n_iter = 100, n_threads = 4)\r\n\r\n# Get context vectors\r\nwv_context <- glove$components\r\n\r\n# Combine (often improves quality)\r\nword_vectors <- wv_main + t(wv_context)\r\n```\r\n\r\n### Using GloVe Embeddings\r\n\r\n```r\r\n# Similar words\r\nfind_similar <- function(word, word_vectors, top_n = 10) {\r\n  if (!word %in% rownames(word_vectors)) {\r\n    return(paste(\"Word not in vocabulary:\", word))\r\n  }\r\n\r\n  vec <- word_vectors[word, , drop = FALSE]\r\n\r\n  # Cosine similarity\r\n  cos_sim <- sim2(x = word_vectors, y = vec, method = \"cosine\", norm = \"l2\")\r\n\r\n  # Sort and return top N\r\n  head(sort(cos_sim[, 1], decreasing = TRUE), top_n)\r\n}\r\n\r\nfind_similar(\"economy\", word_vectors)\r\n```\r\n\r\n## Using Pretrained Embeddings\r\n\r\n### Loading GloVe\r\n\r\n```r\r\n# Download from https://nlp.stanford.edu/projects/glove/\r\n# Example with GloVe 6B 100d\r\n\r\nload_glove <- function(path, nrow = Inf) {\r\n  lines <- readLines(path, n = nrow, encoding = \"UTF-8\")\r\n\r\n  word_vectors <- map_dfr(lines, function(line) {\r\n    parts <- strsplit(line, \" \")[[1]]\r\n    word <- parts[1]\r\n    vector <- as.numeric(parts[-1])\r\n    tibble(word = word, !!!set_names(vector, paste0(\"V\", seq_along(vector))))\r\n  })\r\n\r\n  matrix_out <- as.matrix(word_vectors[, -1])\r\n  rownames(matrix_out) <- word_vectors$word\r\n\r\n  matrix_out\r\n}\r\n\r\n# Usage (if you have the file)\r\n# glove <- load_glove(\"glove.6B.100d.txt\", nrow = 50000)\r\n```\r\n\r\n### Document Embeddings with Pretrained\r\n\r\n```r\r\nget_doc_embedding_pretrained <- function(text, embeddings, dim = 100) {\r\n  words <- tolower(strsplit(text, \"\\\\s+\")[[1]])\r\n  words <- words[words %in% rownames(embeddings)]\r\n\r\n  if (length(words) == 0) return(rep(0, dim))\r\n\r\n  word_vecs <- embeddings[words, , drop = FALSE]\r\n  colMeans(word_vecs)\r\n}\r\n\r\n# Apply to corpus\r\n# doc_embeddings <- t(sapply(texts, function(t)\r\n#   get_doc_embedding_pretrained(t, glove)))\r\n```\r\n\r\n## Document Similarity\r\n\r\n### Cosine Similarity\r\n\r\n```r\r\nlibrary(text2vec)\r\n\r\n# Calculate pairwise similarities\r\ndoc_sim_matrix <- sim2(doc_embeddings, method = \"cosine\", norm = \"l2\")\r\n\r\n# Find most similar documents\r\nfind_similar_docs <- function(doc_idx, sim_matrix, top_n = 5) {\r\n  sims <- sim_matrix[doc_idx, ]\r\n  sims[doc_idx] <- -Inf  # Exclude self\r\n\r\n  top_idx <- order(sims, decreasing = TRUE)[1:top_n]\r\n\r\n  tibble(\r\n    similar_doc = top_idx,\r\n    similarity = sims[top_idx]\r\n  )\r\n}\r\n\r\nfind_similar_docs(1, doc_sim_matrix)\r\n```\r\n\r\n### Semantic Search\r\n\r\n```r\r\nsemantic_search <- function(query, doc_embeddings, model, top_n = 5) {\r\n  query_embedding <- get_doc_embedding(query, model)\r\n\r\n  similarities <- sim2(\r\n    doc_embeddings,\r\n    matrix(query_embedding, nrow = 1),\r\n    method = \"cosine\"\r\n  )[, 1]\r\n\r\n  top_idx <- order(similarities, decreasing = TRUE)[1:top_n]\r\n\r\n  tibble(\r\n    rank = 1:top_n,\r\n    doc_id = top_idx,\r\n    similarity = similarities[top_idx]\r\n  )\r\n}\r\n\r\n# Usage\r\nsemantic_search(\"economic growth\", doc_embeddings, w2v_model)\r\n```\r\n\r\n## Dimensionality Reduction & Visualization\r\n\r\n### t-SNE\r\n\r\n```r\r\nlibrary(Rtsne)\r\n\r\n# Reduce dimensions\r\nset.seed(42)\r\ntsne_result <- Rtsne(doc_embeddings, dims = 2, perplexity = min(5, nrow(doc_embeddings) - 1))\r\n\r\n# Visualize\r\ntsne_df <- tibble(\r\n  doc_id = 1:nrow(doc_embeddings),\r\n  x = tsne_result$Y[, 1],\r\n  y = tsne_result$Y[, 2]\r\n)\r\n\r\nggplot(tsne_df, aes(x = x, y = y, label = doc_id)) +\r\n  geom_point() +\r\n  geom_text(nudge_y = 0.5) +\r\n  labs(title = \"Document Embeddings (t-SNE)\") +\r\n  theme_minimal()\r\n```\r\n\r\n### UMAP\r\n\r\n```r\r\nlibrary(umap)\r\n\r\n# Reduce dimensions\r\nset.seed(42)\r\numap_result <- umap(doc_embeddings)\r\n\r\n# Visualize\r\numap_df <- tibble(\r\n  doc_id = 1:nrow(doc_embeddings),\r\n  x = umap_result$layout[, 1],\r\n  y = umap_result$layout[, 2]\r\n)\r\n\r\nggplot(umap_df, aes(x = x, y = y, label = doc_id)) +\r\n  geom_point() +\r\n  geom_text(nudge_y = 0.1) +\r\n  labs(title = \"Document Embeddings (UMAP)\") +\r\n  theme_minimal()\r\n```\r\n\r\n## Clustering with Embeddings\r\n\r\n```r\r\nlibrary(stats)\r\n\r\n# K-means clustering\r\nset.seed(42)\r\nk <- 3\r\nclusters <- kmeans(doc_embeddings, centers = k)\r\n\r\n# Add to visualization\r\ntsne_df$cluster <- factor(clusters$cluster)\r\n\r\nggplot(tsne_df, aes(x = x, y = y, color = cluster)) +\r\n  geom_point(size = 3) +\r\n  labs(title = \"Document Clusters (t-SNE + K-means)\") +\r\n  theme_minimal()\r\n```\r\n\r\n## Embeddings as Features for Classification\r\n\r\n```r\r\nlibrary(tidymodels)\r\n\r\n# Prepare data with embeddings\r\ntexts_df <- tibble(\r\n  doc_id = 1:length(texts),\r\n  text = texts,\r\n  label = factor(c(\"economy\", \"politics\", \"climate\", \"education\", \"tech\", \"politics\"))\r\n)\r\n\r\n# Add embedding features\r\nembedding_features <- as_tibble(doc_embeddings, .name_repair = \"unique\")\r\nnames(embedding_features) <- paste0(\"emb_\", 1:ncol(doc_embeddings))\r\n\r\ntexts_with_embeddings <- bind_cols(texts_df, embedding_features)\r\n\r\n# Classification recipe (no text processing needed)\r\nemb_recipe <- recipe(label ~ ., data = texts_with_embeddings) %>%\r\n  update_role(doc_id, text, new_role = \"id\")\r\n\r\n# Model\r\nrf_spec <- rand_forest(trees = 500) %>%\r\n  set_engine(\"ranger\") %>%\r\n  set_mode(\"classification\")\r\n\r\n# Workflow\r\nemb_workflow <- workflow() %>%\r\n  add_recipe(emb_recipe) %>%\r\n  add_model(rf_spec)\r\n\r\n# Fit (would need more data for real use)\r\n# emb_fit <- emb_workflow %>% fit(data = texts_with_embeddings)\r\n```\r\n\r\n## TF-IDF Weighted Embeddings\r\n\r\n```r\r\nlibrary(tidytext)\r\n\r\n# Get TF-IDF weights\r\ntfidf_weights <- tibble(text = texts, doc_id = 1:length(texts)) %>%\r\n  unnest_tokens(word, text) %>%\r\n  count(doc_id, word) %>%\r\n  bind_tf_idf(word, doc_id, n)\r\n\r\n# Weighted document embeddings\r\nget_weighted_doc_embedding <- function(doc_id, tfidf_df, model) {\r\n  doc_words <- tfidf_df %>%\r\n    filter(doc_id == !!doc_id)\r\n\r\n  valid_words <- doc_words %>%\r\n    filter(word %in% rownames(as.matrix(model)))\r\n\r\n  if (nrow(valid_words) == 0) return(rep(0, ncol(as.matrix(model))))\r\n\r\n  word_vecs <- predict(model, valid_words$word, type = \"embedding\")\r\n  if (is.vector(word_vecs)) word_vecs <- matrix(word_vecs, nrow = 1)\r\n\r\n  # Weighted average\r\n  weighted_sum <- colSums(word_vecs * valid_words$tf_idf)\r\n  weighted_sum / sum(valid_words$tf_idf)\r\n}\r\n\r\n# Apply\r\nweighted_doc_emb <- t(sapply(1:length(texts), function(i)\r\n  get_weighted_doc_embedding(i, tfidf_weights, w2v_model)))\r\n```\r\n\r\n## Complete Workflow\r\n\r\n```r\r\n#' Embedding Pipeline\r\n#' @param texts Character vector of documents\r\n#' @param dim Embedding dimensions\r\n#' @param method \"word2vec\" or \"glove\"\r\n#' @return List with model and document embeddings\r\n\r\nembedding_pipeline <- function(texts, dim = 100, method = \"word2vec\") {\r\n\r\n  if (method == \"word2vec\") {\r\n    # Train word2vec\r\n    writeLines(texts, \"temp_corpus.txt\")\r\n    model <- word2vec(\r\n      x = \"temp_corpus.txt\",\r\n      type = \"skip-gram\",\r\n      dim = dim,\r\n      window = 5,\r\n      iter = 5,\r\n      min_count = 1\r\n    )\r\n    file.remove(\"temp_corpus.txt\")\r\n\r\n    # Document embeddings\r\n    doc_emb <- t(sapply(texts, function(t) {\r\n      get_doc_embedding(t, model)\r\n    }))\r\n\r\n  } else if (method == \"glove\") {\r\n    # Train GloVe\r\n    tokens <- space_tokenizer(tolower(texts))\r\n    it <- itoken(tokens, progressbar = FALSE)\r\n    vocab <- create_vocabulary(it)\r\n    vectorizer <- vocab_vectorizer(vocab)\r\n    tcm <- create_tcm(it, vectorizer, skip_grams_window = 5)\r\n\r\n    glove <- GlobalVectors$new(rank = dim, x_max = 10)\r\n    wv_main <- glove$fit_transform(tcm, n_iter = 100)\r\n    model <- wv_main + t(glove$components)\r\n\r\n    # Document embeddings\r\n    doc_emb <- t(sapply(texts, function(t) {\r\n      words <- strsplit(tolower(t), \"\\\\s+\")[[1]]\r\n      words <- words[words %in% rownames(model)]\r\n      if (length(words) == 0) return(rep(0, dim))\r\n      colMeans(model[words, , drop = FALSE])\r\n    }))\r\n  }\r\n\r\n  rownames(doc_emb) <- paste0(\"doc_\", 1:length(texts))\r\n\r\n  list(\r\n    model = model,\r\n    doc_embeddings = doc_emb\r\n  )\r\n}\r\n\r\n# Usage\r\nresults <- embedding_pipeline(texts, dim = 50, method = \"word2vec\")\r\nprint(dim(results$doc_embeddings))\r\n```\r\n",
        "plugins/text-analyst/skills/text-analyst/r-techniques/06_visualization.md": "# Text Visualization in R\r\n\r\n## Package Versions\r\n\r\n```r\r\n# Tested with:\r\n# R 4.3.0\r\n# ggplot2 3.4.2\r\n# tidytext 0.4.1\r\n# ggwordcloud 0.5.0\r\n# igraph 1.5.0\r\n# ggraph 2.1.0\r\n```\r\n\r\n## Installation\r\n\r\n```r\r\ninstall.packages(c(\"ggplot2\", \"tidytext\", \"ggwordcloud\",\r\n                   \"igraph\", \"ggraph\", \"scales\", \"ggrepel\"))\r\n```\r\n\r\n## Term Frequency Plots\r\n\r\n### Basic Bar Chart\r\n\r\n```r\r\nlibrary(tidyverse)\r\nlibrary(tidytext)\r\n\r\n# Sample data\r\ntexts <- tibble(\r\n  doc_id = 1:100,\r\n  text = sample(c(\r\n    \"The economy shows growth with employment rising.\",\r\n    \"Climate change impacts communities worldwide.\",\r\n    \"Healthcare reform remains politically divisive.\",\r\n    \"Technology transforms modern communication.\"\r\n  ), 100, replace = TRUE)\r\n)\r\n\r\n# Tokenize and count\r\nword_counts <- texts %>%\r\n  unnest_tokens(word, text) %>%\r\n  anti_join(stop_words, by = \"word\") %>%\r\n  count(word, sort = TRUE)\r\n\r\n# Top terms bar chart\r\nword_counts %>%\r\n  slice_head(n = 20) %>%\r\n  mutate(word = reorder(word, n)) %>%\r\n  ggplot(aes(x = n, y = word)) +\r\n  geom_col(fill = \"steelblue\") +\r\n  labs(title = \"Most Frequent Terms\",\r\n       x = \"Frequency\",\r\n       y = NULL) +\r\n  theme_minimal()\r\n\r\n# Save\r\nggsave(\"output/figures/term_frequency.pdf\", width = 8, height = 6)\r\n```\r\n\r\n### Lollipop Chart\r\n\r\n```r\r\nword_counts %>%\r\n  slice_head(n = 20) %>%\r\n  mutate(word = reorder(word, n)) %>%\r\n  ggplot(aes(x = n, y = word)) +\r\n  geom_segment(aes(x = 0, xend = n, y = word, yend = word),\r\n               color = \"gray60\") +\r\n  geom_point(size = 3, color = \"steelblue\") +\r\n  labs(title = \"Most Frequent Terms\",\r\n       x = \"Frequency\",\r\n       y = NULL) +\r\n  theme_minimal()\r\n```\r\n\r\n## Word Clouds\r\n\r\n```r\r\nlibrary(ggwordcloud)\r\n\r\n# Basic word cloud\r\nword_counts %>%\r\n  slice_head(n = 100) %>%\r\n  ggplot(aes(label = word, size = n)) +\r\n  geom_text_wordcloud(color = \"steelblue\") +\r\n  scale_size_area(max_size = 15) +\r\n  theme_minimal()\r\n\r\n# Colored by frequency\r\nword_counts %>%\r\n  slice_head(n = 100) %>%\r\n  ggplot(aes(label = word, size = n, color = n)) +\r\n  geom_text_wordcloud() +\r\n  scale_size_area(max_size = 15) +\r\n  scale_color_gradient(low = \"lightblue\", high = \"darkblue\") +\r\n  theme_minimal()\r\n```\r\n\r\n## Sentiment Visualization\r\n\r\n### Sentiment Distribution\r\n\r\n```r\r\n# Calculate sentiment per document\r\nsentiment_scores <- texts %>%\r\n  unnest_tokens(word, text) %>%\r\n  inner_join(get_sentiments(\"bing\"), by = \"word\") %>%\r\n  count(doc_id, sentiment) %>%\r\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%\r\n  mutate(net_sentiment = positive - negative)\r\n\r\n# Histogram\r\nggplot(sentiment_scores, aes(x = net_sentiment)) +\r\n  geom_histogram(binwidth = 1, fill = \"steelblue\", color = \"white\") +\r\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\") +\r\n  labs(title = \"Distribution of Document Sentiment\",\r\n       x = \"Net Sentiment (Positive - Negative)\",\r\n       y = \"Count\") +\r\n  theme_minimal()\r\n```\r\n\r\n### Sentiment Words\r\n\r\n```r\r\n# Most common sentiment words\r\nsentiment_words <- texts %>%\r\n  unnest_tokens(word, text) %>%\r\n  inner_join(get_sentiments(\"bing\"), by = \"word\") %>%\r\n  count(word, sentiment, sort = TRUE)\r\n\r\n# Diverging bar chart\r\nsentiment_words %>%\r\n  group_by(sentiment) %>%\r\n  slice_head(n = 10) %>%\r\n  ungroup() %>%\r\n  mutate(n = if_else(sentiment == \"negative\", -n, n),\r\n         word = reorder(word, n)) %>%\r\n  ggplot(aes(x = n, y = word, fill = sentiment)) +\r\n  geom_col() +\r\n  scale_fill_manual(values = c(\"negative\" = \"#E74C3C\",\r\n                               \"positive\" = \"#27AE60\")) +\r\n  labs(title = \"Top Sentiment Words\",\r\n       x = \"Frequency (negative to positive)\",\r\n       y = NULL) +\r\n  theme_minimal() +\r\n  theme(legend.position = \"bottom\")\r\n```\r\n\r\n### Sentiment Over Time\r\n\r\n```r\r\n# Add dates to sample\r\ntexts_time <- texts %>%\r\n  mutate(date = seq(as.Date(\"2020-01-01\"),\r\n                    by = \"day\",\r\n                    length.out = n()))\r\n\r\n# Monthly sentiment\r\nmonthly_sentiment <- texts_time %>%\r\n  mutate(month = floor_date(date, \"month\")) %>%\r\n  unnest_tokens(word, text) %>%\r\n  inner_join(get_sentiments(\"bing\"), by = \"word\") %>%\r\n  count(month, sentiment) %>%\r\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%\r\n  mutate(net = positive - negative)\r\n\r\n# Time series plot\r\nggplot(monthly_sentiment, aes(x = month, y = net)) +\r\n  geom_line(color = \"steelblue\", size = 1) +\r\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\r\n  geom_smooth(method = \"loess\", color = \"red\", se = TRUE, alpha = 0.2) +\r\n  labs(title = \"Sentiment Over Time\",\r\n       x = \"Date\",\r\n       y = \"Net Sentiment\") +\r\n  theme_minimal()\r\n```\r\n\r\n## Topic Model Visualization\r\n\r\n### Topic Prevalence\r\n\r\n```r\r\n# Sample topic prevalence data\r\ntopic_data <- tibble(\r\n  topic = paste0(\"Topic \", 1:10),\r\n  label = c(\"Economy\", \"Healthcare\", \"Climate\", \"Education\",\r\n            \"Technology\", \"Immigration\", \"Crime\", \"Foreign Policy\",\r\n            \"Media\", \"Elections\"),\r\n  prevalence = c(0.15, 0.12, 0.11, 0.10, 0.10,\r\n                 0.09, 0.09, 0.08, 0.08, 0.08)\r\n)\r\n\r\n# Bar chart with labels\r\ntopic_data %>%\r\n  mutate(label = reorder(label, prevalence)) %>%\r\n  ggplot(aes(x = prevalence, y = label)) +\r\n  geom_col(fill = \"steelblue\") +\r\n  scale_x_continuous(labels = scales::percent) +\r\n  labs(title = \"Topic Prevalence\",\r\n       x = \"Proportion of Corpus\",\r\n       y = NULL) +\r\n  theme_minimal()\r\n```\r\n\r\n### Topic Top Words\r\n\r\n```r\r\n# Sample topic-word data\r\ntopic_words <- tibble(\r\n  topic = rep(c(\"Economy\", \"Healthcare\", \"Climate\"), each = 10),\r\n  word = c(\"economy\", \"growth\", \"jobs\", \"tax\", \"market\",\r\n           \"business\", \"trade\", \"employment\", \"gdp\", \"inflation\",\r\n           \"health\", \"care\", \"hospital\", \"doctor\", \"insurance\",\r\n           \"patient\", \"medical\", \"treatment\", \"coverage\", \"drug\",\r\n           \"climate\", \"change\", \"carbon\", \"emissions\", \"energy\",\r\n           \"environment\", \"warming\", \"policy\", \"clean\", \"fossil\"),\r\n  beta = runif(30, 0.01, 0.1)\r\n)\r\n\r\n# Faceted bar chart\r\ntopic_words %>%\r\n  group_by(topic) %>%\r\n  slice_max(beta, n = 7) %>%\r\n  ungroup() %>%\r\n  mutate(word = reorder_within(word, beta, topic)) %>%\r\n  ggplot(aes(x = beta, y = word, fill = topic)) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~topic, scales = \"free_y\") +\r\n  scale_y_reordered() +\r\n  labs(title = \"Top Words by Topic\",\r\n       x = \"Beta (word probability)\",\r\n       y = NULL) +\r\n  theme_minimal()\r\n```\r\n\r\n### Topic Over Time\r\n\r\n```r\r\n# Sample topic prevalence over time\r\ntopic_time <- expand_grid(\r\n  year = 2010:2023,\r\n  topic = c(\"Climate\", \"Economy\", \"Healthcare\")\r\n) %>%\r\n  mutate(prevalence = case_when(\r\n    topic == \"Climate\" ~ 0.05 + (year - 2010) * 0.01 + rnorm(n(), 0, 0.01),\r\n    topic == \"Economy\" ~ 0.15 - (year - 2010) * 0.005 + rnorm(n(), 0, 0.01),\r\n    topic == \"Healthcare\" ~ 0.10 + rnorm(n(), 0, 0.01)\r\n  ))\r\n\r\nggplot(topic_time, aes(x = year, y = prevalence, color = topic)) +\r\n  geom_line(size = 1) +\r\n  geom_point(size = 2) +\r\n  scale_y_continuous(labels = scales::percent) +\r\n  labs(title = \"Topic Prevalence Over Time\",\r\n       x = \"Year\",\r\n       y = \"Topic Proportion\",\r\n       color = \"Topic\") +\r\n  theme_minimal() +\r\n  theme(legend.position = \"bottom\")\r\n```\r\n\r\n## Network Visualizations\r\n\r\n### Word Co-occurrence Network\r\n\r\n```r\r\nlibrary(igraph)\r\nlibrary(ggraph)\r\n\r\n# Create co-occurrence matrix\r\nword_pairs <- texts %>%\r\n  unnest_tokens(word, text) %>%\r\n  anti_join(stop_words, by = \"word\") %>%\r\n  group_by(doc_id) %>%\r\n  mutate(word2 = lead(word)) %>%\r\n  filter(!is.na(word2)) %>%\r\n  ungroup() %>%\r\n  count(word, word2, sort = TRUE) %>%\r\n  filter(n >= 2)  # Minimum co-occurrence\r\n\r\n# Create graph\r\nword_graph <- word_pairs %>%\r\n  graph_from_data_frame()\r\n\r\n# Plot\r\nggraph(word_graph, layout = \"fr\") +\r\n  geom_edge_link(aes(edge_alpha = n, edge_width = n),\r\n                 color = \"gray50\", show.legend = FALSE) +\r\n  geom_node_point(color = \"steelblue\", size = 5) +\r\n  geom_node_text(aes(label = name), repel = TRUE, size = 3) +\r\n  theme_void() +\r\n  labs(title = \"Word Co-occurrence Network\")\r\n```\r\n\r\n### Topic Correlation Network\r\n\r\n```r\r\n# Sample topic correlation matrix\r\nn_topics <- 6\r\ntopic_cor <- matrix(runif(n_topics^2, -0.3, 0.5), nrow = n_topics)\r\ntopic_cor[lower.tri(topic_cor)] <- t(topic_cor)[lower.tri(topic_cor)]\r\ndiag(topic_cor) <- 1\r\nrownames(topic_cor) <- colnames(topic_cor) <- paste0(\"Topic \", 1:n_topics)\r\n\r\n# Convert to edges (threshold correlations)\r\ncor_edges <- as_tibble(topic_cor, rownames = \"from\") %>%\r\n  pivot_longer(-from, names_to = \"to\", values_to = \"correlation\") %>%\r\n  filter(from < to, correlation > 0.1)\r\n\r\n# Create graph\r\ntopic_graph <- graph_from_data_frame(cor_edges, directed = FALSE)\r\n\r\nggraph(topic_graph, layout = \"stress\") +\r\n  geom_edge_link(aes(edge_width = correlation, edge_alpha = correlation),\r\n                 color = \"steelblue\") +\r\n  geom_node_point(size = 8, color = \"steelblue\") +\r\n  geom_node_text(aes(label = name), color = \"white\", size = 3) +\r\n  theme_void() +\r\n  labs(title = \"Topic Correlation Network\")\r\n```\r\n\r\n## Classification Visualization\r\n\r\n### Confusion Matrix\r\n\r\n```r\r\nlibrary(scales)\r\n\r\n# Sample confusion matrix data\r\nconf_matrix <- tibble(\r\n  truth = rep(c(\"Politics\", \"Sports\", \"Business\", \"Science\"), each = 4),\r\n  predicted = rep(c(\"Politics\", \"Sports\", \"Business\", \"Science\"), 4),\r\n  n = c(45, 3, 2, 0,\r\n        2, 47, 1, 0,\r\n        3, 1, 44, 2,\r\n        0, 1, 3, 46)\r\n)\r\n\r\nggplot(conf_matrix, aes(x = predicted, y = truth, fill = n)) +\r\n  geom_tile() +\r\n  geom_text(aes(label = n), color = \"white\", size = 5) +\r\n  scale_fill_gradient(low = \"lightblue\", high = \"darkblue\") +\r\n  labs(title = \"Confusion Matrix\",\r\n       x = \"Predicted\",\r\n       y = \"Actual\",\r\n       fill = \"Count\") +\r\n  theme_minimal() +\r\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\r\n```\r\n\r\n### Feature Importance\r\n\r\n```r\r\n# Sample feature importance\r\nimportance <- tibble(\r\n  feature = paste0(\"word_\", 1:20),\r\n  importance = sort(runif(20, 0, 1), decreasing = TRUE)\r\n)\r\n\r\nimportance %>%\r\n  slice_head(n = 15) %>%\r\n  mutate(feature = reorder(feature, importance)) %>%\r\n  ggplot(aes(x = importance, y = feature)) +\r\n  geom_col(fill = \"steelblue\") +\r\n  labs(title = \"Top Predictive Features\",\r\n       x = \"Importance\",\r\n       y = NULL) +\r\n  theme_minimal()\r\n```\r\n\r\n## Comparative Visualizations\r\n\r\n### Comparing Groups\r\n\r\n```r\r\n# Log odds ratio for comparing word usage\r\ngroup_words <- texts %>%\r\n  mutate(group = sample(c(\"A\", \"B\"), n(), replace = TRUE)) %>%\r\n  unnest_tokens(word, text) %>%\r\n  anti_join(stop_words, by = \"word\") %>%\r\n  count(group, word) %>%\r\n  pivot_wider(names_from = group, values_from = n, values_fill = 0) %>%\r\n  mutate(\r\n    total = A + B,\r\n    log_odds = log((A + 1) / (B + 1))\r\n  ) %>%\r\n  filter(total >= 5)\r\n\r\n# Plot top discriminating words\r\nbind_rows(\r\n  group_words %>% slice_max(log_odds, n = 10) %>% mutate(direction = \"Group A\"),\r\n  group_words %>% slice_min(log_odds, n = 10) %>% mutate(direction = \"Group B\")\r\n) %>%\r\n  mutate(word = reorder(word, log_odds)) %>%\r\n  ggplot(aes(x = log_odds, y = word, fill = direction)) +\r\n  geom_col() +\r\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\r\n  scale_fill_manual(values = c(\"Group A\" = \"#E74C3C\", \"Group B\" = \"#3498DB\")) +\r\n  labs(title = \"Words Distinguishing Groups\",\r\n       x = \"Log Odds Ratio\",\r\n       y = NULL) +\r\n  theme_minimal()\r\n```\r\n\r\n## Publication-Ready Formatting\r\n\r\n### Theme for Publications\r\n\r\n```r\r\ntheme_publication <- function(base_size = 12) {\r\n  theme_minimal(base_size = base_size) +\r\n    theme(\r\n      plot.title = element_text(size = base_size + 2, face = \"bold\"),\r\n      plot.subtitle = element_text(size = base_size, color = \"gray40\"),\r\n      axis.title = element_text(size = base_size),\r\n      axis.text = element_text(size = base_size - 1),\r\n      legend.title = element_text(size = base_size),\r\n      legend.text = element_text(size = base_size - 1),\r\n      panel.grid.minor = element_blank(),\r\n      strip.text = element_text(size = base_size, face = \"bold\")\r\n    )\r\n}\r\n\r\n# Apply to plots\r\nword_counts %>%\r\n  slice_head(n = 15) %>%\r\n  mutate(word = reorder(word, n)) %>%\r\n  ggplot(aes(x = n, y = word)) +\r\n  geom_col(fill = \"steelblue\") +\r\n  labs(title = \"Term Frequency\",\r\n       x = \"Frequency\",\r\n       y = NULL) +\r\n  theme_publication()\r\n```\r\n\r\n### Exporting High-Quality Figures\r\n\r\n```r\r\n# Save with appropriate dimensions and resolution\r\nggsave(\"output/figures/figure1.pdf\",\r\n       width = 8,\r\n       height = 6,\r\n       units = \"in\")\r\n\r\nggsave(\"output/figures/figure1.png\",\r\n       width = 8,\r\n       height = 6,\r\n       units = \"in\",\r\n       dpi = 300)\r\n\r\n# For journal requirements\r\nggsave(\"output/figures/figure1.tiff\",\r\n       width = 8,\r\n       height = 6,\r\n       units = \"in\",\r\n       dpi = 600,\r\n       compression = \"lzw\")\r\n```\r\n\r\n### Color Palettes\r\n\r\n```r\r\n# Colorblind-friendly palettes\r\nlibrary(scales)\r\n\r\n# Okabe-Ito palette\r\nokabe_ito <- c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\",\r\n               \"#0072B2\", \"#D55E00\", \"#CC79A7\", \"#999999\")\r\n\r\n# Use in plots\r\nggplot(topic_time, aes(x = year, y = prevalence, color = topic)) +\r\n  geom_line(size = 1) +\r\n  scale_color_manual(values = okabe_ito) +\r\n  theme_publication()\r\n```\r\n"
      },
      "plugins": [
        {
          "name": "r-analyst",
          "source": "./plugins/r-analyst",
          "description": "R statistical analysis for publication-ready sociology research. Phased workflow for DiD, IV, matching, panel methods, and more with pauses for user review.",
          "version": "1.0.0",
          "author": {
            "name": "Neal Caren"
          },
          "license": "MIT",
          "keywords": [
            "R",
            "statistics",
            "econometrics",
            "sociology",
            "research"
          ],
          "strict": true,
          "categories": [
            "econometrics",
            "r",
            "research",
            "sociology",
            "statistics"
          ],
          "install_commands": [
            "/plugin marketplace add nealcaren/social-data-analysis",
            "/plugin install r-analyst@social-data-analysis"
          ]
        },
        {
          "name": "stata-analyst",
          "source": "./plugins/stata-analyst",
          "description": "Stata statistical analysis for publication-ready sociology research. Phased workflow for DiD, IV, matching, panel methods, and more with pauses for user review.",
          "version": "1.0.0",
          "author": {
            "name": "Neal Caren"
          },
          "license": "MIT",
          "keywords": [
            "Stata",
            "statistics",
            "econometrics",
            "sociology",
            "research"
          ],
          "strict": true,
          "categories": [
            "econometrics",
            "research",
            "sociology",
            "stata",
            "statistics"
          ],
          "install_commands": [
            "/plugin marketplace add nealcaren/social-data-analysis",
            "/plugin install stata-analyst@social-data-analysis"
          ]
        },
        {
          "name": "interview-analyst",
          "source": "./plugins/interview-analyst",
          "description": "Pragmatic qualitative analysis for interview data. Supports theory-informed or data-first approaches with systematic coding, quality indicators, and publication-ready synthesis.",
          "version": "1.0.0",
          "author": {
            "name": "Neal Caren"
          },
          "license": "MIT",
          "keywords": [
            "qualitative",
            "interviews",
            "coding",
            "sociology",
            "research"
          ],
          "strict": true,
          "categories": [
            "coding",
            "interviews",
            "qualitative",
            "research",
            "sociology"
          ],
          "install_commands": [
            "/plugin marketplace add nealcaren/social-data-analysis",
            "/plugin install interview-analyst@social-data-analysis"
          ]
        },
        {
          "name": "interview-writeup",
          "source": "./plugins/interview-writeup",
          "description": "Write-up support for qualitative interview research. Guides methods drafting, findings structure, quote use, and revision with quality checks.",
          "version": "1.0.0",
          "author": {
            "name": "Neal Caren"
          },
          "license": "MIT",
          "keywords": [
            "qualitative",
            "interviews",
            "writing",
            "methods",
            "sociology"
          ],
          "strict": true,
          "categories": [
            "interviews",
            "methods",
            "qualitative",
            "sociology",
            "writing"
          ],
          "install_commands": [
            "/plugin marketplace add nealcaren/social-data-analysis",
            "/plugin install interview-writeup@social-data-analysis"
          ]
        },
        {
          "name": "dag-development",
          "source": "./plugins/dag-development",
          "description": "Develop causal diagrams (DAGs) from social-science research questions and literature, then render publication-ready figures using Mermaid, R, or Python.",
          "version": "1.0.0",
          "author": {
            "name": "Neal Caren"
          },
          "license": "MIT",
          "keywords": [
            "causal-diagrams",
            "DAG",
            "development",
            "methods",
            "sociology"
          ],
          "strict": true,
          "categories": [
            "causal-diagrams",
            "dag",
            "development",
            "methods",
            "sociology"
          ],
          "install_commands": [
            "/plugin marketplace add nealcaren/social-data-analysis",
            "/plugin install dag-development@social-data-analysis"
          ]
        },
        {
          "name": "abductive-analyst",
          "source": "./plugins/abductive-analyst",
          "description": "Abductive analysis (Timmermans & Tavory) for theory-generating qualitative research. Theory-first approach with anomaly detection, theoretical casing, and rhetorical abduction for publication.",
          "version": "1.0.0",
          "author": {
            "name": "Neal Caren"
          },
          "license": "MIT",
          "keywords": [
            "qualitative",
            "abduction",
            "theory",
            "sociology",
            "research"
          ],
          "strict": true,
          "categories": [
            "abduction",
            "qualitative",
            "research",
            "sociology",
            "theory"
          ],
          "install_commands": [
            "/plugin marketplace add nealcaren/social-data-analysis",
            "/plugin install abductive-analyst@social-data-analysis"
          ]
        },
        {
          "name": "text-analyst",
          "source": "./plugins/text-analyst",
          "description": "Computational text analysis using R or Python. Topic models (LDA, STM, BERTopic), sentiment analysis, classification, and embeddings with systematic validation.",
          "version": "1.0.0",
          "author": {
            "name": "Neal Caren"
          },
          "license": "MIT",
          "keywords": [
            "text-analysis",
            "NLP",
            "topic-models",
            "sociology",
            "research"
          ],
          "strict": true,
          "categories": [
            "nlp",
            "research",
            "sociology",
            "text-analysis",
            "topic-models"
          ],
          "install_commands": [
            "/plugin marketplace add nealcaren/social-data-analysis",
            "/plugin install text-analyst@social-data-analysis"
          ]
        },
        {
          "name": "lecture-designer",
          "source": "./plugins/lecture-designer",
          "description": "Transform textbook chapters into engaging, evidence-based lectures. Applies cognitive load theory, narrative design (ABT), active learning, and produces Quarto reveal.js slides.",
          "version": "1.0.0",
          "author": {
            "name": "Neal Caren"
          },
          "license": "MIT",
          "keywords": [
            "teaching",
            "lectures",
            "slides",
            "pedagogy",
            "active-learning"
          ],
          "strict": true,
          "categories": [
            "active-learning",
            "lectures",
            "pedagogy",
            "slides",
            "teaching"
          ],
          "install_commands": [
            "/plugin marketplace add nealcaren/social-data-analysis",
            "/plugin install lecture-designer@social-data-analysis"
          ]
        },
        {
          "name": "lit-search",
          "source": "./plugins/lit-search",
          "description": "Build systematic literature databases using OpenAlex API. Phased workflow for search, screening, snowballing, annotation, and synthesis with structured user interaction.",
          "version": "1.0.0",
          "author": {
            "name": "Neal Caren"
          },
          "license": "MIT",
          "keywords": [
            "literature-search",
            "OpenAlex",
            "bibliography",
            "systematic-review",
            "research"
          ],
          "strict": true,
          "categories": [
            "bibliography",
            "literature-search",
            "openalex",
            "research",
            "systematic-review"
          ],
          "install_commands": [
            "/plugin marketplace add nealcaren/social-data-analysis",
            "/plugin install lit-search@social-data-analysis"
          ]
        },
        {
          "name": "lit-synthesis",
          "source": "./plugins/lit-synthesis",
          "description": "Deep reading and synthesis of literature corpus. Theoretical mapping, thematic clustering, and debate identification using Zotero MCP for full-text access.",
          "version": "1.0.0",
          "author": {
            "name": "Neal Caren"
          },
          "license": "MIT",
          "keywords": [
            "literature-synthesis",
            "Zotero",
            "theoretical-mapping",
            "sociology",
            "research"
          ],
          "strict": true,
          "categories": [
            "literature-synthesis",
            "research",
            "sociology",
            "theoretical-mapping",
            "zotero"
          ],
          "install_commands": [
            "/plugin marketplace add nealcaren/social-data-analysis",
            "/plugin install lit-synthesis@social-data-analysis"
          ]
        },
        {
          "name": "lit-writeup",
          "source": "./plugins/lit-writeup",
          "description": "Draft publication-ready Theory sections for sociology research. Guides structure, paragraph functions, sentence craft, and calibration based on analysis of 80 Social Problems/Social Forces articles.",
          "version": "1.0.0",
          "author": {
            "name": "Neal Caren"
          },
          "license": "MIT",
          "keywords": [
            "theory-section",
            "writing",
            "sociology",
            "literature-review",
            "research"
          ],
          "strict": true,
          "categories": [
            "literature-review",
            "research",
            "sociology",
            "theory-section",
            "writing"
          ],
          "install_commands": [
            "/plugin marketplace add nealcaren/social-data-analysis",
            "/plugin install lit-writeup@social-data-analysis"
          ]
        },
        {
          "name": "interview-bookends",
          "source": "./plugins/interview-bookends",
          "description": "Write article introductions and conclusions for sociology interview research. Takes theory and findings sections as input; uses genre analysis of 80 articles to guide cluster-specific framing.",
          "version": "1.0.0",
          "author": {
            "name": "Neal Caren"
          },
          "license": "MIT",
          "keywords": [
            "introduction",
            "conclusion",
            "bookends",
            "sociology",
            "writing"
          ],
          "strict": true,
          "categories": [
            "bookends",
            "conclusion",
            "introduction",
            "sociology",
            "writing"
          ],
          "install_commands": [
            "/plugin marketplace add nealcaren/social-data-analysis",
            "/plugin install interview-bookends@social-data-analysis"
          ]
        },
        {
          "name": "genre-skill-builder",
          "source": "./plugins/genre-skill-builder",
          "description": "Meta-skill for creating genre-analysis-based writing skills. Analyzes a corpus of article sections, discovers clusters, and generates complete skills with phases, cluster guides, and techniques.",
          "version": "1.0.0",
          "author": {
            "name": "Neal Caren"
          },
          "license": "MIT",
          "keywords": [
            "meta-skill",
            "genre-analysis",
            "skill-builder",
            "writing",
            "research"
          ],
          "strict": true,
          "categories": [
            "genre-analysis",
            "meta-skill",
            "research",
            "skill-builder",
            "writing"
          ],
          "install_commands": [
            "/plugin marketplace add nealcaren/social-data-analysis",
            "/plugin install genre-skill-builder@social-data-analysis"
          ]
        },
        {
          "name": "methods-writer",
          "source": "./plugins/methods-writer",
          "description": "Draft publication-ready Methods sections for interview-based sociology articles. Three pathways (Efficient/Standard/Detailed) based on analysis of 77 Social Problems/Social Forces articles.",
          "version": "1.0.0",
          "author": {
            "name": "Neal Caren"
          },
          "license": "MIT",
          "keywords": [
            "methods",
            "interviews",
            "qualitative",
            "sociology",
            "writing"
          ],
          "strict": true,
          "categories": [
            "interviews",
            "methods",
            "qualitative",
            "sociology",
            "writing"
          ],
          "install_commands": [
            "/plugin marketplace add nealcaren/social-data-analysis",
            "/plugin install methods-writer@social-data-analysis"
          ]
        },
        {
          "name": "case-justification",
          "source": "./plugins/case-justification",
          "description": "Draft case justification sections for interview-based sociology articles. Five cluster styles (Minimal/Standard/Historical/Comparative/Policy) based on analysis of 32 Social Problems/Social Forces articles.",
          "version": "1.0.0",
          "author": {
            "name": "Neal Caren"
          },
          "license": "MIT",
          "keywords": [
            "case-background",
            "research-setting",
            "interviews",
            "sociology",
            "writing"
          ],
          "strict": true,
          "categories": [
            "case-background",
            "interviews",
            "research-setting",
            "sociology",
            "writing"
          ],
          "install_commands": [
            "/plugin marketplace add nealcaren/social-data-analysis",
            "/plugin install case-justification@social-data-analysis"
          ]
        },
        {
          "name": "revision-coordinator",
          "source": "./plugins/revision-coordinator",
          "description": "Orchestrate manuscript revision by routing feedback to specialized writing skills. Parses reviewer/editor feedback, maps to sections, dispatches to lit-writeup/methods-writer/interview-bookends/case-justification, ensures coherence.",
          "version": "1.0.0",
          "author": {
            "name": "Neal Caren"
          },
          "license": "MIT",
          "keywords": [
            "revision",
            "feedback",
            "R&R",
            "manuscript",
            "writing",
            "coordination"
          ],
          "strict": true,
          "categories": [
            "coordination",
            "feedback",
            "manuscript",
            "r&r",
            "revision",
            "writing"
          ],
          "install_commands": [
            "/plugin marketplace add nealcaren/social-data-analysis",
            "/plugin install revision-coordinator@social-data-analysis"
          ]
        }
      ]
    }
  ]
}