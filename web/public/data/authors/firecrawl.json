{
  "author": {
    "id": "firecrawl",
    "display_name": "Firecrawl",
    "avatar_url": "https://avatars.githubusercontent.com/u/135057108?v=4"
  },
  "marketplaces": [
    {
      "name": "firecrawl",
      "version": null,
      "description": "Scrape, search, crawl, and map the web with a single command.",
      "repo_full_name": "firecrawl/cli",
      "repo_url": "https://github.com/firecrawl/cli",
      "repo_description": "CLI tool and Agent Skill for Firecrawl - scrape, crawl, and extract LLM-ready data from websites",
      "signals": {
        "stars": 76,
        "forks": 11,
        "pushed_at": "2026-02-01T01:45:10Z"
      },
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"firecrawl\",\n  \"owner\": {\n    \"name\": \"Firecrawl\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"firecrawl\",\n      \"source\": \"./\",\n      \"description\": \"Scrape, search, crawl, and map the web with a single command.\",\n      \"skills\": [\"./skills/firecrawl-cli\"]\n    }\n  ]\n}",
        ".claude-plugin/plugin.json": "{\n    \"name\": \"firecrawl\",\n    \"description\": \"Scrape, search, crawl, and map the web with a single command.\",\n    \"version\": \"0.0.1\",\n    \"author\": {\n        \"name\": \"Firecrawl\"\n    },\n    \"skills\": [\"./skills/firecrawl-cli\"]\n}",
        "README.md": "# üî• Firecrawl CLI\n\nCommand-line interface for Firecrawl. Scrape, crawl, and extract data from any website directly from your terminal.\n\n## Installation\n\n```bash\nnpm install -g firecrawl-cli\n```\n\nIf you are using in any AI agent like Claude Code, you can install the skill with:\n\n```bash\nnpx skills add firecrawl/cli\n```\n\n## Quick Start\n\nJust run a command - the CLI will prompt you to authenticate if needed:\n\n```bash\nfirecrawl https://example.com\n```\n\n## Authentication\n\nOn first run, you'll be prompted to authenticate:\n\n```\n  üî• firecrawl cli\n  Turn websites into LLM-ready data\n\nWelcome! To get started, authenticate with your Firecrawl account.\n\n  1. Login with browser (recommended)\n  2. Enter API key manually\n\nTip: You can also set FIRECRAWL_API_KEY environment variable\n\nEnter choice [1/2]:\n```\n\n### Authentication Methods\n\n```bash\n# Interactive (prompts automatically when needed)\nfirecrawl\n\n# Browser login\nfirecrawl login\n\n# Direct API key\nfirecrawl login --api-key fc-your-api-key\n\n# Environment variable\nexport FIRECRAWL_API_KEY=fc-your-api-key\n\n# Per-command API key\nfirecrawl scrape https://example.com --api-key fc-your-api-key\n```\n\n### Self-hosted / Local Development\n\nFor self-hosted Firecrawl instances or local development, use the `--api-url` option:\n\n```bash\n# Use a local Firecrawl instance (no API key required)\nfirecrawl --api-url http://localhost:3002 scrape https://example.com\n\n# Or set via environment variable\nexport FIRECRAWL_API_URL=http://localhost:3002\nfirecrawl scrape https://example.com\n\n# Self-hosted with API key\nfirecrawl --api-url https://firecrawl.mycompany.com --api-key fc-xxx scrape https://example.com\n```\n\nWhen using a custom API URL (anything other than `https://api.firecrawl.dev`), authentication is automatically skipped, allowing you to use local instances without an API key.\n\n---\n\n## Commands\n\n### `scrape` - Scrape a single URL\n\nExtract content from any webpage in various formats.\n\n```bash\n# Basic usage (outputs markdown)\nfirecrawl https://example.com\nfirecrawl scrape https://example.com\n\n# Get raw HTML\nfirecrawl https://example.com --html\nfirecrawl https://example.com -H\n\n# Multiple formats (outputs JSON)\nfirecrawl https://example.com --format markdown,links,images\n\n# Save to file\nfirecrawl https://example.com -o output.md\nfirecrawl https://example.com --format json -o data.json --pretty\n```\n\n#### Scrape Options\n\n| Option                   | Description                                             |\n| ------------------------ | ------------------------------------------------------- |\n| `-f, --format <formats>` | Output format(s), comma-separated                       |\n| `-H, --html`             | Shortcut for `--format html`                            |\n| `--only-main-content`    | Extract only main content (removes navs, footers, etc.) |\n| `--wait-for <ms>`        | Wait time before scraping (for JS-rendered content)     |\n| `--screenshot`           | Take a screenshot                                       |\n| `--include-tags <tags>`  | Only include specific HTML tags                         |\n| `--exclude-tags <tags>`  | Exclude specific HTML tags                              |\n| `-o, --output <path>`    | Save output to file                                     |\n| `--pretty`               | Pretty print JSON output                                |\n| `--timing`               | Show request timing info                                |\n\n#### Available Formats\n\n| Format       | Description                |\n| ------------ | -------------------------- |\n| `markdown`   | Clean markdown (default)   |\n| `html`       | Cleaned HTML               |\n| `rawHtml`    | Original HTML              |\n| `links`      | All links on the page      |\n| `screenshot` | Screenshot as base64       |\n| `json`       | Structured JSON extraction |\n\n#### Examples\n\n```bash\n# Extract only main content as markdown\nfirecrawl https://blog.example.com --only-main-content\n\n# Wait for JS to render, then scrape\nfirecrawl https://spa-app.com --wait-for 3000\n\n# Get all links from a page\nfirecrawl https://example.com --format links\n\n# Screenshot + markdown\nfirecrawl https://example.com --format markdown --screenshot\n\n# Extract specific elements only\nfirecrawl https://example.com --include-tags article,main\n\n# Exclude navigation and ads\nfirecrawl https://example.com --exclude-tags nav,aside,.ad\n```\n\n---\n\n### `search` - Search the web\n\nSearch the web and optionally scrape content from search results.\n\n```bash\n# Basic search\nfirecrawl search \"firecrawl web scraping\"\n\n# Limit results\nfirecrawl search \"AI news\" --limit 10\n\n# Search news sources\nfirecrawl search \"tech startups\" --sources news\n\n# Search images\nfirecrawl search \"landscape photography\" --sources images\n\n# Multiple sources\nfirecrawl search \"machine learning\" --sources web,news,images\n\n# Filter by category (GitHub, research papers, PDFs)\nfirecrawl search \"web scraping python\" --categories github\nfirecrawl search \"transformer architecture\" --categories research\nfirecrawl search \"machine learning\" --categories github,research\n\n# Time-based search\nfirecrawl search \"AI announcements\" --tbs qdr:d   # Past day\nfirecrawl search \"tech news\" --tbs qdr:w          # Past week\n\n# Location-based search\nfirecrawl search \"restaurants\" --location \"San Francisco,California,United States\"\nfirecrawl search \"local news\" --country DE\n\n# Search and scrape results\nfirecrawl search \"firecrawl tutorials\" --scrape\nfirecrawl search \"API documentation\" --scrape --scrape-formats markdown,links\n\n# Output as pretty JSON\nfirecrawl search \"web scraping\"\n```\n\n#### Search Options\n\n| Option                       | Description                                                                                 |\n| ---------------------------- | ------------------------------------------------------------------------------------------- |\n| `--limit <n>`                | Maximum results (default: 5, max: 100)                                                      |\n| `--sources <sources>`        | Comma-separated: `web`, `images`, `news` (default: web)                                     |\n| `--categories <categories>`  | Comma-separated: `github`, `research`, `pdf`                                                |\n| `--tbs <value>`              | Time filter: `qdr:h` (hour), `qdr:d` (day), `qdr:w` (week), `qdr:m` (month), `qdr:y` (year) |\n| `--location <location>`      | Geo-targeting (e.g., \"Germany\", \"San Francisco,California,United States\")                   |\n| `--country <code>`           | ISO country code (default: US)                                                              |\n| `--timeout <ms>`             | Timeout in milliseconds (default: 60000)                                                    |\n| `--ignore-invalid-urls`      | Exclude URLs invalid for other Firecrawl endpoints                                          |\n| `--scrape`                   | Enable scraping of search results                                                           |\n| `--scrape-formats <formats>` | Scrape formats when `--scrape` enabled (default: markdown)                                  |\n| `--only-main-content`        | Include only main content when scraping (default: true)                                     |\n| `-o, --output <path>`        | Save to file                                                                                |\n| `--json`                     | Output as compact JSON (use `-p` for pretty JSON)                                           |\n\n#### Examples\n\n```bash\n# Research a topic with recent results\nfirecrawl search \"React Server Components\" --tbs qdr:m --limit 10\n\n# Find GitHub repositories\nfirecrawl search \"web scraping library\" --categories github --limit 20\n\n# Search and get full content\nfirecrawl search \"firecrawl documentation\" --scrape --scrape-formats markdown -p -o results.json\n\n# Find research papers\nfirecrawl search \"large language models\" --categories research -p\n\n# Search with location targeting\nfirecrawl search \"best coffee shops\" --location \"Berlin,Germany\" --country DE\n\n# Get news from the past week\nfirecrawl search \"AI startups funding\" --sources news --tbs qdr:w --limit 15\n```\n\n---\n\n### `map` - Discover all URLs on a website\n\nQuickly discover all URLs on a website without scraping content.\n\n```bash\n# List all URLs (one per line)\nfirecrawl map https://example.com\n\n# Output as JSON\nfirecrawl map https://example.com --json\n\n# Search for specific URLs\nfirecrawl map https://example.com --search \"blog\"\n\n# Limit results\nfirecrawl map https://example.com --limit 500\n```\n\n#### Map Options\n\n| Option                      | Description                       |\n| --------------------------- | --------------------------------- |\n| `--limit <n>`               | Maximum URLs to discover          |\n| `--search <query>`          | Filter URLs by search query       |\n| `--sitemap <mode>`          | `include`, `skip`, or `only`      |\n| `--include-subdomains`      | Include subdomains                |\n| `--ignore-query-parameters` | Dedupe URLs with different params |\n| `--timeout <seconds>`       | Request timeout                   |\n| `--json`                    | Output as JSON                    |\n| `-o, --output <path>`       | Save to file                      |\n\n#### Examples\n\n```bash\n# Find all product pages\nfirecrawl map https://shop.example.com --search \"product\"\n\n# Get sitemap URLs only\nfirecrawl map https://example.com --sitemap only\n\n# Save URL list to file\nfirecrawl map https://example.com -o urls.txt\n\n# Include subdomains\nfirecrawl map https://example.com --include-subdomains --limit 1000\n```\n\n---\n\n### `crawl` - Crawl an entire website\n\nCrawl multiple pages from a website.\n\n```bash\n# Start a crawl (returns job ID)\nfirecrawl crawl https://example.com\n\n# Wait for crawl to complete\nfirecrawl crawl https://example.com --wait\n\n# With progress indicator\nfirecrawl crawl https://example.com --wait --progress\n\n# Check crawl status\nfirecrawl crawl <job-id>\n\n# Limit pages\nfirecrawl crawl https://example.com --limit 100 --max-depth 3\n```\n\n#### Crawl Options\n\n| Option                      | Description                              |\n| --------------------------- | ---------------------------------------- |\n| `--wait`                    | Wait for crawl to complete               |\n| `--progress`                | Show progress while waiting              |\n| `--limit <n>`               | Maximum pages to crawl                   |\n| `--max-depth <n>`           | Maximum crawl depth                      |\n| `--include-paths <paths>`   | Only crawl matching paths                |\n| `--exclude-paths <paths>`   | Skip matching paths                      |\n| `--sitemap <mode>`          | `include`, `skip`, or `only`             |\n| `--allow-subdomains`        | Include subdomains                       |\n| `--allow-external-links`    | Follow external links                    |\n| `--crawl-entire-domain`     | Crawl entire domain                      |\n| `--ignore-query-parameters` | Treat URLs with different params as same |\n| `--delay <ms>`              | Delay between requests                   |\n| `--max-concurrency <n>`     | Max concurrent requests                  |\n| `--timeout <seconds>`       | Timeout when waiting                     |\n| `--poll-interval <seconds>` | Status check interval                    |\n\n#### Examples\n\n```bash\n# Crawl blog section only\nfirecrawl crawl https://example.com --include-paths /blog,/posts\n\n# Exclude admin pages\nfirecrawl crawl https://example.com --exclude-paths /admin,/login\n\n# Crawl with rate limiting\nfirecrawl crawl https://example.com --delay 1000 --max-concurrency 2\n\n# Deep crawl with high limit\nfirecrawl crawl https://example.com --limit 1000 --max-depth 10 --wait --progress\n\n# Save results\nfirecrawl crawl https://example.com --wait -o crawl-results.json --pretty\n```\n\n---\n\n### `credit-usage` - Check your credits\n\n```bash\n# Show credit usage\nfirecrawl credit-usage\n\n# Output as JSON\nfirecrawl credit-usage --json --pretty\n```\n\n---\n\n### `config` - Configure and view settings\n\n```bash\n# View current configuration\nfirecrawl config\n\n# Configure with custom API URL\nfirecrawl config --api-url https://firecrawl.mycompany.com\nfirecrawl config --api-url http://localhost:3002 --api-key fc-xxx\n```\n\nShows authentication status and stored credentials location.\n\n---\n\n### `login` / `logout`\n\n```bash\n# Login\nfirecrawl login\nfirecrawl login --method browser\nfirecrawl login --method manual\nfirecrawl login --api-key fc-xxx\n\n# Login to self-hosted instance\nfirecrawl login --api-url https://firecrawl.mycompany.com\nfirecrawl login --api-url http://localhost:3002 --api-key fc-xxx\n\n# Logout\nfirecrawl logout\n```\n\n---\n\n## Global Options\n\nThese options work with any command:\n\n| Option                | Description                                            |\n| --------------------- | ------------------------------------------------------ |\n| `--status`            | Show version, auth, concurrency, and credits           |\n| `-k, --api-key <key>` | Use specific API key                                   |\n| `--api-url <url>`     | Use custom API URL (for self-hosted/local development) |\n| `-V, --version`       | Show version                                           |\n| `-h, --help`          | Show help                                              |\n\n### Check Status\n\n```bash\nfirecrawl --status\n```\n\n```\n  üî• firecrawl cli v1.0.2\n\n  ‚óè Authenticated via stored credentials\n  Concurrency: 0/100 jobs (parallel scrape limit)\n  Credits: 500,000 / 1,000,000 (50% left this cycle)\n```\n\n---\n\n## Output Handling\n\n### Stdout vs File\n\n```bash\n# Output to stdout (default)\nfirecrawl https://example.com\n\n# Pipe to another command\nfirecrawl https://example.com | head -50\n\n# Save to file\nfirecrawl https://example.com -o output.md\n\n# JSON output\nfirecrawl https://example.com --format links --pretty\n```\n\n### Format Behavior\n\n- **Single format**: Outputs raw content (markdown text, HTML, etc.)\n- **Multiple formats**: Outputs JSON with all requested data\n\n```bash\n# Raw markdown output\nfirecrawl https://example.com --format markdown\n\n# JSON output with multiple formats\nfirecrawl https://example.com --format markdown,links,images\n```\n\n---\n\n## Tips & Tricks\n\n### Scrape multiple URLs\n\n```bash\n# Using a loop\nfor url in https://example.com/page1 https://example.com/page2; do\n  firecrawl \"$url\" -o \"$(echo $url | sed 's/[^a-zA-Z0-9]/_/g').md\"\ndone\n\n# From a file\ncat urls.txt | xargs -I {} firecrawl {} -o {}.md\n```\n\n### Combine with other tools\n\n```bash\n# Extract links and process with jq\nfirecrawl https://example.com --format links | jq '.links[].url'\n\n# Convert to PDF (with pandoc)\nfirecrawl https://example.com | pandoc -o document.pdf\n\n# Search within scraped content\nfirecrawl https://example.com | grep -i \"keyword\"\n```\n\n### CI/CD Usage\n\n```bash\n# Set API key via environment\nexport FIRECRAWL_API_KEY=${{ secrets.FIRECRAWL_API_KEY }}\nfirecrawl crawl https://docs.example.com --wait -o docs.json\n\n# Use self-hosted instance\nexport FIRECRAWL_API_URL=${{ secrets.FIRECRAWL_API_URL }}\nfirecrawl scrape https://example.com -o output.md\n```\n\n---\n\n## Telemetry\n\nThe CLI collects anonymous usage data during authentication to help improve the product:\n\n- CLI version, OS, and Node.js version\n- Detect development tools (e.g., Cursor, VS Code, Claude Code)\n\n**No command data, URLs, or file contents are collected via the CLI.**\n\nTo disable telemetry, set the environment variable:\n\n```bash\nexport FIRECRAWL_NO_TELEMETRY=1\n```\n\n---\n\n## Documentation\n\nFor more details, visit the [Firecrawl Documentation](https://docs.firecrawl.dev).\n"
      },
      "plugins": [
        {
          "name": "firecrawl",
          "source": "./",
          "description": "Scrape, search, crawl, and map the web with a single command.",
          "skills": [
            "./skills/firecrawl-cli"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add firecrawl/cli",
            "/plugin install firecrawl@firecrawl"
          ]
        }
      ]
    }
  ]
}