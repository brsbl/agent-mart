{
  "author": {
    "id": "emraher",
    "display_name": "Emrah Er",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/1182257?u=d38fbf872f34a48bb619b06bc488522aa36fb64b&v=4",
    "url": "https://github.com/emraher",
    "bio": null,
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 10,
      "total_commands": 0,
      "total_skills": 9,
      "total_stars": 0,
      "total_forks": 0
    }
  },
  "marketplaces": [
    {
      "name": "anti-slop-skills",
      "version": null,
      "description": "Quality enforcement layer for code, text, and design. Detects and prevents generic AI-generated patterns. Works complementary with any learning resource (books, courses, Posit skills). Includes active detection scripts.",
      "owner_info": {
        "name": "Anti-Slop Team",
        "email": "contact@anti-slop.org"
      },
      "keywords": [],
      "repo_full_name": "emraher/eerskills",
      "repo_url": "https://github.com/emraher/eerskills",
      "repo_description": null,
      "homepage": null,
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2026-01-29T07:52:50Z",
        "created_at": "2026-01-21T19:35:28Z",
        "license": "MIT"
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 3496
        },
        {
          "path": "README.md",
          "type": "blob",
          "size": 7432
        },
        {
          "path": "anti-slop",
          "type": "tree",
          "size": null
        },
        {
          "path": "anti-slop/SKILL.md",
          "type": "blob",
          "size": 12741
        },
        {
          "path": "cpp",
          "type": "tree",
          "size": null
        },
        {
          "path": "cpp/anti-slop",
          "type": "tree",
          "size": null
        },
        {
          "path": "cpp/anti-slop/SKILL.md",
          "type": "blob",
          "size": 6962
        },
        {
          "path": "design",
          "type": "tree",
          "size": null
        },
        {
          "path": "design/anti-slop",
          "type": "tree",
          "size": null
        },
        {
          "path": "design/anti-slop/SKILL.md",
          "type": "blob",
          "size": 10774
        },
        {
          "path": "examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "examples/README.md",
          "type": "blob",
          "size": 1510
        },
        {
          "path": "julia",
          "type": "tree",
          "size": null
        },
        {
          "path": "julia/anti-slop",
          "type": "tree",
          "size": null
        },
        {
          "path": "julia/anti-slop/SKILL.md",
          "type": "blob",
          "size": 7447
        },
        {
          "path": "python",
          "type": "tree",
          "size": null
        },
        {
          "path": "python/anti-slop",
          "type": "tree",
          "size": null
        },
        {
          "path": "python/anti-slop/SKILL.md",
          "type": "blob",
          "size": 12356
        },
        {
          "path": "quarto",
          "type": "tree",
          "size": null
        },
        {
          "path": "quarto/anti-slop",
          "type": "tree",
          "size": null
        },
        {
          "path": "quarto/anti-slop/SKILL.md",
          "type": "blob",
          "size": 18796
        },
        {
          "path": "r",
          "type": "tree",
          "size": null
        },
        {
          "path": "r/anti-slop",
          "type": "tree",
          "size": null
        },
        {
          "path": "r/anti-slop/SKILL.md",
          "type": "blob",
          "size": 10684
        },
        {
          "path": "text",
          "type": "tree",
          "size": null
        },
        {
          "path": "text/anti-slop",
          "type": "tree",
          "size": null
        },
        {
          "path": "text/anti-slop/SKILL.md",
          "type": "blob",
          "size": 16984
        },
        {
          "path": "toolkit",
          "type": "tree",
          "size": null
        },
        {
          "path": "toolkit/SKILL.md",
          "type": "blob",
          "size": 19559
        },
        {
          "path": "toolkit/scripts",
          "type": "tree",
          "size": null
        },
        {
          "path": "toolkit/scripts/README.md",
          "type": "blob",
          "size": 422
        },
        {
          "path": "toolkit/tests",
          "type": "tree",
          "size": null
        },
        {
          "path": "toolkit/tests/README.md",
          "type": "blob",
          "size": 639
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"anti-slop-skills\",\n  \"owner\": {\n    \"name\": \"Anti-Slop Team\",\n    \"email\": \"contact@anti-slop.org\"\n  },\n  \"metadata\": {\n    \"description\": \"Quality enforcement layer for code, text, and design. Detects and prevents generic AI-generated patterns. Works complementary with any learning resource (books, courses, Posit skills). Includes active detection scripts.\",\n    \"version\": \"3.0.0\",\n    \"examples\": [\n      {\n        \"name\": \"Detect R code slop\",\n        \"command\": \"Rscript toolkit/scripts/detect_slop.R script.R --verbose\"\n      },\n      {\n        \"name\": \"Clean text slop with backup\",\n        \"command\": \"python3 toolkit/scripts/clean_slop.py README.md --save\"\n      },\n      {\n        \"name\": \"Check Python type coverage\",\n        \"command\": \"mypy script.py --strict\"\n      }\n    ]\n  },\n  \"plugins\": [\n    {\n      \"name\": \"anti-slop\",\n      \"description\": \"Meta-skill that coordinates all anti-slop domain skills. Auto-loads appropriate skill based on file type.\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./anti-slop\"\n      ]\n    },\n    {\n      \"name\": \"toolkit\",\n      \"description\": \"Active detection and cleanup scripts. Automated slop scoring for text and R code files.\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./toolkit\"\n      ]\n    },\n    {\n      \"name\": \"text\",\n      \"description\": \"Comprehensive text quality coordinator. Three-layer system: (1) pattern removal, (2) Strunk's principles, (3) human voice. One invocation applies all layers automatically.\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./text/anti-slop\"\n      ]\n    },\n    {\n      \"name\": \"humanizer\",\n      \"description\": \"Wikipedia's 24-pattern checklist for removing AI writing signatures. Adds personality and voice to text.\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./external/humanizer\"\n      ]\n    },\n    {\n      \"name\": \"design\",\n      \"description\": \"Visual design quality enforcement. Detects cookie-cutter layouts, generic gradients, and 'AI startup' aesthetic.\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./design/anti-slop\"\n      ]\n    },\n    {\n      \"name\": \"r\",\n      \"description\": \"R code quality enforcement. Namespace qualification, explicit returns, no generic names (df, data, result).\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./r/anti-slop\"\n      ]\n    },\n    {\n      \"name\": \"python\",\n      \"description\": \"Python code quality enforcement. Type hints, docstrings, PEP 8 compliance, pandas best practices.\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./python/anti-slop\"\n      ]\n    },\n    {\n      \"name\": \"quarto\",\n      \"description\": \"Quarto/RMarkdown quality enforcement. Prevents generic research documents, ensures reproducibility.\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./quarto/anti-slop\"\n      ]\n    },\n    {\n      \"name\": \"julia\",\n      \"description\": \"Julia code quality enforcement for scientific computing. Type stability and multiple dispatch.\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./julia/anti-slop\"\n      ]\n    },\n    {\n      \"name\": \"cpp\",\n      \"description\": \"C++ and Rcpp quality enforcement for performance-critical code. Memory safety and Rcpp best practices.\",\n      \"source\": \"./\",\n      \"strict\": false,\n      \"skills\": [\n        \"./cpp/anti-slop\"\n      ]\n    }\n  ]\n}",
        "README.md": "# Anti-Slop Skills\n\n**Version:** v2.0.0\n\nCatch generic AI patterns before they ship. Works with any learning resource.\n\n## The Problem\n\nAI writes code like this:\n```python\ndef process_data(data):\n    \"\"\"Process the data.\"\"\"\n    result = do_something(data)\n    return result\n```\n\nVariables named `data` and `result`. Circular documentation. Functions that \"process\" things.\n\nAnti-slop catches this before it reaches production.\n\n## What You Get\n\n**Learning resources** teach syntax. Books show patterns. Anti-slop enforces quality.\n\n- Automated detection scripts that score your code (0-100)\n- Domain-specific skills for R, Python, Julia, C++\n- Text analysis that flags buzzwords and filler\n- Design pattern detection for cookie-cutter layouts\n\n## Installation\n\nClone and initialize:\n\n```bash\ngit clone https://github.com/your-username/anti-slop-skills.git ~/.claude/skills/anti-slop-skills\ncd ~/.claude/skills/anti-slop-skills\ngit submodule update --init --recursive\n```\n\nVerify you have the core directories:\n\n```bash\nls -la\n# anti-slop/, r/, python/, text/, design/, quarto/, toolkit/, external/\n```\n\n### Staying Updated\n\nPull latest changes:\n\n```bash\ngit pull\ngit submodule update --remote --merge\n```\n\nThat's it. Main repo + all submodules updated.\n\n## Repository Structure\n\n```\nanti-slop-skills/\n├── anti-slop/              # Meta-skill coordinator\n├── r/anti-slop/           # R code quality\n├── python/anti-slop/      # Python code quality\n├── text/anti-slop/        # Technical writing\n├── design/anti-slop/      # Visual quality\n├── quarto/anti-slop/      # Reproducible research\n├── toolkit/               # Automated detection scripts\n├── docs/                  # Documentation and guides\n├── external/              # External skill repositories (submodules)\n│   ├── humanizer/         # [SUBMODULE] Voice & personality\n│   ├── cc-polymath/       # [SUBMODULE] Additional anti-slop patterns\n│   └── elements-of-style/ # [SUBMODULE] Strunk's writing principles\n├── CLAUDE.md              # Context for Claude Code\n└── README.md              # This file\n```\n\n### Submodules (in external/)\n\nExternal skill collections included as git submodules:\n\n1. **external/humanizer/** - [blader/humanizer](https://github.com/blader/humanizer)\n   - Wikipedia's 24-pattern checklist for removing AI writing signatures\n   - Adds personality and voice to text\n\n2. **external/elements-of-style/** - [emraher/the-elements-of-style](https://github.com/emraher/the-elements-of-style)\n   - Strunk's 18 timeless writing principles\n   - Clear, concise prose for documentation and technical writing\n\n3. **external/cc-polymath/** - [rand/cc-polymath](https://github.com/rand/cc-polymath/tree/main/skills/anti-slop)\n   - Additional anti-slop patterns and skills\n   - Community-contributed quality enforcement\n\n## What's Inside\n\n**Code enforcement:**\n- `r/anti-slop` - No more `df` and `data` variables. Always use `pkg::function()`. Explicit `return()` statements.\n- `python/anti-slop` - Type hints required. Docstrings required. No mutable default arguments.\n- `julia/anti-slop` - Type stability. Multiple dispatch best practices.\n- `cpp/anti-slop` - Memory safety. Const-correctness for Rcpp.\n\n**Writing cleanup:**\n- `text/anti-slop` - Kills \"delve into\", \"navigate the complexity\", \"in order to\"\n- `humanizer` - Wikipedia's 24 AI patterns. Adds actual voice.\n- `elements-of-style` - Strunk's rules. Active voice. Omit needless words.\n- `quarto/anti-slop` - No template documents. Reproducibility checks.\n\n**Visual quality:**\n- `design/anti-slop` - Detects purple gradients, floating 3D shapes, \"Empower your business\"\n\n**Automation:**\n- `toolkit` - Scripts that actually run: `detect_slop.py`, `detect_slop.R`, `clean_slop.py`\n\n**Meta-coordinator:**\n- `anti-slop` - Auto-loads the right skill based on file type\n\n## Quick Commands\n\n### Detection Scripts\n\n```bash\n# Detect slop in text files (returns score 0-100)\npython3 toolkit/scripts/detect_slop.py <file.md> [--verbose]\n\n# Detect slop in R code\nRscript toolkit/scripts/detect_slop.R <file.R> [--verbose]\n\n# Clean up text files (with backup)\npython3 toolkit/scripts/clean_slop.py <file.md> --save\n```\n\n### What the Scores Mean\n\n| Score | Translation | What to Do |\n|-------|-------------|------------|\n| 0-20 | Looks human | Ship it (maybe tweak a line or two) |\n| 20-40 | Some AI fingerprints | Fix the flagged patterns |\n| 40-60 | Generic as hell | Major surgery needed |\n| 60+ | ChatGPT called | Start over |\n\n## How to Use This\n\n**Step 1:** Learn from whatever source you prefer (books, docs, tutorials)\n\n**Step 2:** Write your code\n\n**Step 3:** Catch the slop:\n\n```bash\n# Check text files\npython toolkit/scripts/detect_slop.py README.md --verbose\n\n# Check R code\nRscript toolkit/scripts/detect_slop.R R/\n\n# Score: 0-20 is good, 60+ means rewrite it\n```\n\n**Step 4:** Apply the fixes using skill-specific guidance\n\n**Real example** - you write an R function with a variable called `df`. Detection script flags it. You check `r/anti-slop/SKILL.md`, see the forbidden names list, rename it to `survey_data`. Run detection again. Clean.\n\nThis isn't a tutorial. It's the thing that catches what tutorials miss.\n\n## Working with Submodules\n\nFor full details, see [docs/SUBMODULES.md](docs/SUBMODULES.md).\n\n### Update a specific submodule\n\n```bash\n# Update just humanizer\ncd external/humanizer && git pull origin main && cd ../..\n\n# Update just elements-of-style\ncd external/elements-of-style && git pull origin main && cd ../..\n\n\n# Update just cc-polymath\ncd external/cc-polymath && git pull origin main && cd ../..\n```\n\n### Add new submodules\n\n```bash\ngit submodule add <repository-url> <path>\ngit submodule update --init --recursive\n```\n\n### Remove a submodule\n\n```bash\n# Remove submodule\ngit submodule deinit <path>\ngit rm <path>\nrm -rf .git/modules/<path>\n```\n\n## Contributing\n\nFound a pattern we're missing? Add it.\n\n**For this repo:**\n- Skills go in root directories: `r/`, `python/`, `text/`\n- Follow v2.0.0 structure: main `SKILL.md` + `reference/` files\n- Show before/after examples\n\n**For submodules:**\n- Humanizer: https://github.com/blader/humanizer\n- Elements of Style: https://github.com/emraher/the-elements-of-style\n- CC-Polymath: https://github.com/rand/cc-polymath\n\n## Documentation\n\n- **[docs/SUBMODULES.md](docs/SUBMODULES.md)** - Submodule management guide\n- **CLAUDE.md** - Context for Claude Code\n- Individual **SKILL.md** files in each skill directory\n\n## License\n\nSee individual repositories for license information:\n- Anti-slop skills: [LICENSE](./LICENSE)\n- Humanizer: [blader/humanizer](https://github.com/blader/humanizer/blob/main/README.md#license)\n- Elements of Style: Public Domain - [emraher/the-elements-of-style](https://github.com/emraher/the-elements-of-style/blob/main/LICENSE)\n- CC-Polymath: [rand/cc-polymath](https://github.com/rand/cc-polymath/blob/main/LICENSE)\n\n## Version History\n\n- **v2.0.0** (2026-01-21) - Complete restructure with workflow-focused patterns\n  - All main skills restructured (8/8 completed)\n  - Progressive disclosure pattern implemented (main SKILL.md + reference/)\n  - Decision tables and realistic workflows added\n  - Integration guide created (INTEGRATION.md)\n  - Submodules integrated (humanizer, cc-polymath, elements-of-style)\n  - Automated detection scripts in toolkit/ updated\n\n- **v1.0.0** - Initial release with pattern catalogs\n",
        "anti-slop/SKILL.md": "---\nname: anti-slop\ndescription: >\n  Meta-skill that enforces quality standards across code, text, and design.\n  Automatically applies appropriate domain skills based on file type.\n  Detects and prevents generic AI-generated patterns.\nincludes:\n  - r/anti-slop\n  - python/anti-slop\n  - julia/anti-slop\n  - cpp/anti-slop\n  - text/anti-slop\n  - design/anti-slop\n  - quarto/anti-slop\n  - toolkit\nversion: 2.0.0\n---\n\n# Anti-Slop: The Dispatcher\n\n## What This Does\n\nYou invoke \"anti-slop\" without specifying which domain. This skill figures it out.\n\nWorking on a `.R` file? Routes to r/anti-slop (catches `df`, missing `::`, implicit returns).\nEditing a `.py` file? Routes to python/anti-slop (checks type hints, descriptive names).\nWriting `.md` docs? Routes to text/anti-slop (kills \"delve into\", buzzwords, meta-commentary).\n\nOne skill name, automatic routing.\n\n## When to Use This\n\n**Use anti-slop (this meta-skill) when:**\n- ✓ You want automatic domain detection\n- ✓ Working with multiple file types (code + docs + design)\n- ✓ User says \"check for slop\" without specifying where\n\n**Use specific skills (r/anti-slop, python/anti-slop, etc.) when:**\n- You know exactly which domain\n- Need domain-specific deep dive\n- Configuring custom rules\n\n## Quick Overview\n\nThis meta-skill automatically routes to:\n\n| File Type | Routes To | Focus Area |\n|-----------|-----------|------------|\n| `*.R`, `*.Rmd` | r/anti-slop | Namespace qualification, explicit returns, snake_case |\n| `*.py` | python/anti-slop | Type hints, descriptive names, PEP 8 |\n| `*.jl` | julia/anti-slop | Type annotations, dispatch patterns |\n| `*.cpp`, `*.h` | cpp/anti-slop | RAII, const correctness, modern C++ |\n| `*.md`, `*.txt` | text/anti-slop | Direct language, no buzzwords, structural clarity |\n| `*.qmd`, `*.ipynb` | quarto/anti-slop | Cross-references, reproducibility, proper YAML |\n| Design files | design/anti-slop | Data viz quality, avoiding generic UI patterns |\n| Automated checks | toolkit | Detection scripts, CI/CD integration |\n\n## Domain Skill Summaries\n\n### r/anti-slop\n**Focus**: Production-quality R code\n\n**Key rules**:\n- ALWAYS namespace qualify with `::`\n- ALWAYS explicit `return()` statements\n- ALWAYS `snake_case` naming\n- NO generic names (`df`, `data`, `result`)\n- Native pipe `|>` preferred over `%>%`\n\n**Quick example**:\n```r\n# Bad\ndf <- data %>% filter(x > 0)\n\n# Good\ncustomer_data <- customer_data |>\n  dplyr::filter(status == \"active\")\nreturn(customer_data)\n```\n\n**Use when**: Writing or reviewing R code, R packages, or R analysis scripts.\n\n---\n\n### python/anti-slop\n**Focus**: Type-safe, professional Python\n\n**Key rules**:\n- ALWAYS use type hints\n- ALWAYS descriptive variable names\n- Follow PEP 8 conventions\n- NO generic names (`data`, `result`, `temp`)\n- Explicit error handling\n\n**Quick example**:\n```python\n# Bad\ndef process(data):\n    result = data + 1\n    return result\n\n# Good\ndef calculate_adjusted_score(raw_scores: list[float]) -> list[float]:\n    adjusted_scores = [score + 1.0 for score in raw_scores]\n    return adjusted_scores\n```\n\n**Use when**: Writing or reviewing Python code, modules, or packages.\n\n---\n\n### julia/anti-slop\n**Focus**: Type-stable, idiomatic Julia\n\n**Key rules**:\n- Type annotations on function signatures\n- Multiple dispatch patterns\n- Descriptive variable names\n- Explicit return types\n\n**Quick example**:\n```julia\n# Bad\nfunction process(data)\n    result = data .+ 1\n    return result\nend\n\n# Good\nfunction calculate_adjusted_scores(raw_scores::Vector{Float64})::Vector{Float64}\n    adjusted_scores = raw_scores .+ 1.0\n    return adjusted_scores\nend\n```\n\n**Use when**: Writing or reviewing Julia code for scientific computing.\n\n---\n\n### cpp/anti-slop\n**Focus**: Modern C++ with RAII and type safety\n\n**Key rules**:\n- RAII for resource management\n- `const` correctness everywhere\n- Smart pointers over raw pointers\n- Modern C++ (C++11/14/17/20)\n- Descriptive names\n\n**Quick example**:\n```cpp\n// Bad\ndouble* process(double* data, int size) {\n    double* result = new double[size];\n    // ...\n    return result;\n}\n\n// Good\nstd::vector<double> calculate_adjusted_scores(\n    const std::vector<double>& raw_scores\n) {\n    std::vector<double> adjusted_scores;\n    adjusted_scores.reserve(raw_scores.size());\n    // ...\n    return adjusted_scores;\n}\n```\n\n**Use when**: Writing or reviewing C++ code, especially for R packages (Rcpp).\n\n---\n\n### text/anti-slop\n**Focus**: Direct, clear writing without AI patterns\n\n**Key rules**:\n- NO \"delve into\", \"navigate complexities\", \"in today's world\"\n- NO meta-commentary (\"it's important to note\")\n- NO buzzwords (\"leverage\", \"synergistic\")\n- Simplify wordy phrases\n- Be direct and specific\n\n**Quick example**:\n```markdown\n# Bad\nIt's important to note that we will delve into the complexities\nof data analysis in today's fast-paced world.\n\n# Good\nWe analyze customer data to identify retention patterns.\n```\n\n**Use when**: Writing or reviewing documentation, README files, technical writing.\n\n---\n\n### design/anti-slop\n**Focus**: Intentional design avoiding generic patterns\n\n**Key rules**:\n- NO generic gradients (purple/pink/cyan)\n- NO template-driven layouts\n- NO \"Empower your business\" copy\n- Data visualization: proper labels, scales, colors\n- Content-first design\n\n**Quick example**:\n```\nBad visualization:\n- Default ggplot2 gray background\n- Unlabeled axes\n- Generic title: \"Plot 1\"\n\nGood visualization:\n- Clean white background with subtle gridlines\n- Axes: \"Revenue ($M)\" and \"Quarter (2023)\"\n- Title: \"Q4 Revenue Growth Exceeds Projections\"\n```\n\n**Use when**: Creating visualizations, designing interfaces, reviewing UI/UX.\n\n---\n\n### quarto/anti-slop\n**Focus**: Reproducible research documents\n\n**Key rules**:\n- Complete YAML metadata (author, affiliation, date)\n- Label ALL figures/tables/equations\n- ALWAYS use cross-references (@fig-*, @tbl-*, @eq-*)\n- Bibliography management with BibTeX\n- Relative paths only\n- Cache long computations\n\n**Quick example**:\n```yaml\n# Bad\n---\ntitle: \"Analysis\"\noutput: pdf_document\n---\n\nFigure 1 shows results.\n\n# Good\n---\ntitle: \"Customer Retention Analysis\"\nauthor:\n  - name: Jane Researcher\n    affiliation: University Name\nbibliography: references.bib\n---\n\n@fig-retention shows customer retention rates.\n\n```{r}\n#| label: fig-retention\n#| fig-cap: \"Retention by Cohort\"\n```\n```\n\n**Use when**: Creating Quarto/RMarkdown documents, academic papers, technical reports.\n\n---\n\n### toolkit\n**Focus**: Automated detection and cleanup\n\n**Key tools**:\n- `detect_slop.py` - Text file detection\n- `clean_slop.py` - Text file cleanup\n- `detect_slop.R` - R code detection\n\n**Quick example**:\n```bash\n# Detect slop\npython3 scripts/detect_slop.py report.md --verbose\n\n# Clean slop\npython3 scripts/clean_slop.py report.md --save\n\n# Check R code\nRscript scripts/detect_slop.R analysis.R\n```\n\n**Use when**: Automating quality checks, CI/CD integration, batch processing.\n\n## Decision Tree\n\nWhen anti-slop meta-skill is invoked, route based on:\n\n```\n1. Check file extension:\n   - *.R, *.Rmd → r/anti-slop\n   - *.py → python/anti-slop\n   - *.jl → julia/anti-slop\n   - *.cpp, *.h, *.hpp → cpp/anti-slop\n   - *.md, *.txt, *.rst → text/anti-slop\n   - *.qmd, *.ipynb → quarto/anti-slop\n   - No file/mixed → prompt user\n\n2. Check context:\n   - Mentions \"visualization\" or \"plot\" → also consider design/anti-slop\n   - Mentions \"documentation\" → text/anti-slop\n   - Mentions \"package\" → language-specific + quarto/anti-slop for vignettes\n   - Mentions \"CI/CD\" or \"automated\" → toolkit\n\n3. Check user request:\n   - \"Review my code\" → language-specific anti-slop\n   - \"Check documentation\" → text/anti-slop or quarto/anti-slop\n   - \"Audit codebase\" → toolkit\n   - \"Improve design\" → design/anti-slop\n```\n\n## Integration with Learning Resources\n\nThe anti-slop skills are **quality enforcement tools** that work alongside any learning resource:\n\n### Complementary Use with Other Resources\n\n| Resource Type | Teaches | Anti-Slop Enforces |\n|--------------|---------|-------------------|\n| R books | Language features | Production-quality code |\n| Python courses | Syntax & patterns | Type hints & naming |\n| Design systems | UI components | Intentional choices |\n| Writing guides | Structure | Direct language |\n\n## When to Invoke Domain Skills Directly\n\n### Use r/anti-slop directly when:\n- Working exclusively with R code\n- Need R-specific rules (namespace qualification, pipes)\n- Reviewing R package structure\n\n### Use python/anti-slop directly when:\n- Working exclusively with Python code\n- Need type hinting enforcement\n- Reviewing Python package structure\n\n### Use text/anti-slop directly when:\n- Working exclusively with prose\n- Need humanizer checklist integration\n- Cleaning documentation files\n\n### Use quarto/anti-slop directly when:\n- Creating reproducible research documents\n- Need cross-referencing enforcement\n- Preparing academic manuscripts\n\n### Use toolkit directly when:\n- Setting up CI/CD quality checks\n- Batch processing multiple files\n- Automating detection across codebase\n\n## Common Workflows\n\n### Workflow 1: Comprehensive Project Review\n\n**Context**: Review entire project for quality standards.\n\n**Steps**:\n1. Run toolkit detection across all files\n2. Review high-priority issues by domain\n3. Apply domain-specific skills for refactoring\n4. Verify with re-detection\n\n```bash\n# Detect across project\nfind . -name \"*.md\" -exec python3 scripts/detect_slop.py {} \\;\nfind . -name \"*.R\" -exec Rscript scripts/detect_slop.R {} \\;\n\n# Apply domain skills as needed\n# r/anti-slop for R files\n# text/anti-slop for documentation\n# quarto/anti-slop for analysis notebooks\n```\n\n---\n\n### Workflow 2: Pre-Release Quality Check\n\n**Context**: Ensure quality before public release.\n\n**Steps**:\n1. Code review with language-specific skills\n2. Documentation review with text/anti-slop\n3. Visualization review with design/anti-slop\n4. Reproducibility check with quarto/anti-slop\n\n**Checklist**:\n- [ ] All code files score < 30 on slop detection\n- [ ] Documentation has no high-risk phrases\n- [ ] Visualizations have proper labels and titles\n- [ ] Quarto docs render reproducibly\n\n---\n\n### Workflow 3: Establish Team Standards\n\n**Context**: Create quality guidelines for team.\n\n**Steps**:\n1. Review all domain-specific anti-slop skills\n2. Identify patterns most relevant to team\n3. Set thresholds (e.g., max slop score = 40)\n4. Integrate toolkit into CI/CD\n5. Document exceptions\n\n**Example standards document**:\n```markdown\n# Code Quality Standards\n\n## R Code\n- Use r/anti-slop standards\n- Max slop score: 30\n- Namespace qualification required\n- Explicit returns required\n\n## Documentation\n- Use text/anti-slop standards\n- Max slop score: 40\n- No high-risk phrases\n- Direct language required\n\n## Analysis Documents\n- Use quarto/anti-slop standards\n- All cross-references required\n- Bibliography required for papers\n```\n\n## Relationship with Other Skills\n\n### Equal Standing, Not Subordinate\n\nThe anti-slop skills are **NOT** subordinate to any other skill set. They are:\n\n1. **Complementary equals** - Work alongside books, courses, documentation\n2. **Quality enforcers** - Prevent generic AI patterns\n3. **Domain-specific** - Each skill has unique expertise\n4. **Independently valuable** - Useful without other resources\n\n### Integration Pattern\n\n```\nUser writes code/docs\n    ↓\nLearning resource teaches syntax/features (books, docs, tutorials)\n    ↓\nAnti-slop enforces quality standards\n    ↓\nProduction-ready output\n```\n\nBoth steps are essential. Neither is subordinate.\n\n## Resources\n\n### All Domain Skills\n\n- **[r/anti-slop](../r/anti-slop/SKILL.md)** - R code quality standards\n- **[python/anti-slop](../python/anti-slop/SKILL.md)** - Python code quality standards\n- **[julia/anti-slop](../julia/anti-slop/SKILL.md)** - Julia code quality standards\n- **[cpp/anti-slop](../cpp/anti-slop/SKILL.md)** - C++ code quality standards\n- **[text/anti-slop](../text/anti-slop/SKILL.md)** - Technical writing standards\n- **[design/anti-slop](../design/anti-slop/SKILL.md)** - Visualization & UI standards\n- **[quarto/anti-slop](../quarto/anti-slop/SKILL.md)** - Reproducible document standards\n- **[toolkit](../toolkit/SKILL.md)** - Automated detection and cleanup tools\n\n### Additional Resources\n\n- **INTEGRATION.md** - Comprehensive guide to using anti-slop with other skills\n- **toolkit/scripts/** - Detection and cleanup automation scripts\n\n## Version History\n\n### 2.0.0 (Current)\n- Restructured as workflow-focused meta-skill\n- Added clear decision tree for routing\n- Emphasized complementary nature with learning resources\n- Added comprehensive domain skill summaries\n- Clarified equal standing with other skill sets\n\n### 1.0.0\n- Initial meta-skill implementation\n- Basic domain skill coordination\n",
        "cpp/anti-slop/SKILL.md": "---\nname: cpp-anti-slop\ndescription: >\n  Enforce best practices for Rcpp and C++ extensions. \n  Prevents generic AI patterns through memory safety, const-correctness, \n  and Rcpp sugar usage for performance-critical code.\napplies_to:\n  - \"**/*.cpp\"\n  - \"**/*.h\"\n  - \"**/*.hpp\"\ntags: [cpp, rcpp, performance, memory-safety, r-extensions]\nrelated_skills:\n  - r/anti-slop\nversion: 2.0.0\n---\n\n# C++ Anti-Slop Skill for Rcpp Extensions\n\n## When to Use This Skill\n\nUse cpp-anti-slop when:\n- ✓ Writing C++ extensions for R using Rcpp\n- ✓ Optimizing performance-critical bottlenecks in R packages\n- ✓ Reviewing AI-generated C++ code for memory safety\n- ✓ Refactoring legacy Rcpp code for modern standards\n- ✓ Implementing complex algorithms that require low-level control\n- ✓ Ensuring RAII and const-correctness in R-adjacent code\n\nDo NOT use when:\n- Writing general-purpose C++ apps unconnected to R (though standards apply)\n- Performance is not a bottleneck (use R or Julia first)\n- Working with legacy C code that cannot be wrapped in C++\n\n## Quick Example\n\n**Before (AI Slop)**:\n```cpp\n// Generic C-style processing\n// [[Rcpp::export]]\nNumericVector process(NumericVector x) {\n    int n = x.size();\n    double* res = new double[n]; // Manual memory management (unsafe!)\n    for(int i=0; i<n; i++) {\n        res[i] = x[i] * 2.0;\n    }\n    NumericVector out(n);\n    for(int i=0; i<n; i++) out[i] = res[i];\n    delete[] res;\n    return out;\n}\n```\n\n**After (Anti-Slop)**:\n```cpp\n#include <Rcpp.h>\nusing namespace Rcpp;\n\n//' Calculate Doubled Values Efficiently\n//'\n//' @param raw_values Input numeric vector\n//' @return Vector with all elements doubled\n// [[Rcpp::export]]\nNumericVector calculate_doubled_values(const NumericVector& raw_values) {\n    // Rcpp sugar provides vectorized operations (no explicit loop needed)\n    // No manual memory management; Rcpp handles allocation/cleanup\n    return raw_values * 2.0;\n}\n```\n\n**What changed**:\n- ✓ Descriptive name (`calculate_doubled_values` not `process`)\n- ✓ Pass by const reference (`const NumericVector&`) to avoid copies\n- ✓ Leveraged Rcpp sugar for vectorized performance\n- ✓ Eliminated unsafe manual memory management (`new`/`delete`)\n- ✓ Proper roxygen2 documentation\n\n## When to Use What\n\n| If you need to... | Do this | Details |\n|-------------------|---------|---------|\n| Manage memory | Use RAII and Rcpp containers | reference/memory.md |\n| Write fast loops | Use `const` references + Rcpp sugar | reference/performance.md |\n| Interface with R | Use `// [[Rcpp::export]]` | reference/rcpp-api.md |\n| Handle errors | Use `Rcpp::stop()` for R-friendly errors | reference/errors.md |\n| Vectorize | Use Rcpp sugar functions | reference/sugar.md |\n| Linear Algebra | Use `RcppArmadillo` | reference/armadillo.md |\n\n## Core Workflow\n\n### 5-Step Quality Check\n\n1. **Memory Safety** - No manual `new`/`delete`; use Rcpp containers or smart pointers\n   ```cpp\n   // Good\n   NumericVector results(n);\n   \n   // Bad\n   double* results = new double[n];\n   ```\n\n2. **Const Correctness** - Mark read-only inputs as `const&`\n   ```cpp\n   double calculate_sum(const NumericVector& x)\n   ```\n\n3. **Rcpp Sugar** - Use vectorized R-like syntax where available\n   ```cpp\n   return mean(x) + sd(y);\n   ```\n\n4. **Input Validation** - Check dimensions and types early\n   ```cpp\n   if (x.size() != y.size()) {\n       stop(\"Incompatible dimensions\");\n   }\n   ```\n\n5. **Modern C++** - Use `auto`, range-based for loops, and STL algorithms\n   ```cpp\n   for (const auto& val : x) { ... }\n   ```\n\n## Quick Reference Checklist\n\nBefore committing C++ code, verify:\n\n- [ ] All exported functions have `// [[Rcpp::export]]`\n- [ ] Large objects passed by `const&` to avoid copies\n- [ ] No manual memory management (`new`/`delete`)\n- [ ] Rcpp sugar used for vectorization where possible\n- [ ] Informative error messages with `stop()`\n- [ ] Headers organized: Rcpp → standard library → local\n- [ ] No single-letter variables except for standard indices\n- [ ] Roxygen2 documentation for all public functions\n- [ ] Type safety ensured throughout the algorithm\n- [ ] Properly pre-allocated containers\n\n## Common Workflows\n\n### Workflow 1: Optimize an R Loop in C++\n\n**Context**: An R loop is too slow and needs a C++ implementation.\n\n**Steps**:\n\n1. **Create the skeleton**\n   ```cpp\n   #include <Rcpp.h>\n   using namespace Rcpp;\n   // [[Rcpp::export]]\n   ```\n\n2. **Define the signature with references**\n   ```cpp\n   NumericVector fast_algorithm(const NumericVector& input_data)\n   ```\n\n3. **Pre-allocate the result**\n   ```cpp\n   int n = input_data.size();\n   NumericVector results = no_init(n); // Fast allocation\n   ```\n\n4. **Implement the logic with Rcpp sugar**\n   ```cpp\n   results = exp(input_data) / sum(input_data);\n   ```\n\n**Expected outcome**: Significantly faster execution with R-compatible output\n\n---\n\n### Workflow 2: Safe Memory Management\n\n**Context**: Dealing with complex data structures without leaking memory.\n\n**Steps**:\n\n1. **Avoid raw pointers**\n   ```cpp\n   // Instead of double* arr, use:\n   std::vector<double> dynamic_buffer;\n   ```\n\n2. **Use Rcpp containers for R-interop**\n   ```cpp\n   List output = List::create(Named(\"data\") = results);\n   ```\n\n3. **Protect against exceptions**\n   - Rcpp handles C++ exceptions and translates them to R errors automatically.\n\n**Expected outcome**: Crash-proof code that cleans up after itself\n\n---\n\n### Workflow 3: Matrix Operations with Armadillo\n\n**Context**: High-performance linear algebra.\n\n**Steps**:\n\n1. **Add the dependency**\n   ```cpp\n   // [[Rcpp::depends(RcppArmadillo)]]\n   #include <RcppArmadillo.h>\n   ```\n\n2. **Use Armadillo types**\n   ```cpp\n   arma::mat compute_inverse(const arma::mat& X) {\n       return inv(X);\n   }\n   ```\n\n**Expected outcome**: BLAS/LAPACK optimized matrix calculations\n\n## Mandatory Rules Summary\n\n### 1. No Manual Memory Management\nAlways use Rcpp classes (`NumericVector`, `List`, `DataFrame`) or STL containers (`std::vector`, `std::unique_ptr`).\n\n### 2. Pass by Reference\nAvoid unnecessary copies. Pass large objects as `const Type&`.\n\n### 3. R-Friendly Errors\nUse `Rcpp::stop(\"message\")` instead of `std::runtime_error` or `exit()`.\n\n### 4. Rcpp Sugar\nFavor `mean(x)` over manual loops for simple vector operations.\n\n### 5. RAII Principles\nEnsure resources are tied to object lifetime to prevent leaks.\n\n## Resources & Advanced Topics\n\n### Reference Files (Planned)\n\n- **reference/memory.md** - RAII and container management\n- **reference/performance.md** - Profiling and optimization\n- **reference/rcpp-api.md** - Deep dive into Rcpp classes\n- **reference/sugar.md** - Vectorized sugar functions\n- **reference/armadillo.md** - Linear algebra with Armadillo\n\n### Related Skills\n\n- **r/anti-slop** - The primary consumer of Rcpp extensions\n\n### Tools\n\n- `microbenchmark` - For verifying performance gains in R\n- `valgrind` - For memory leak detection\n- `Rcpp::sourceCpp()` - For rapid development and testing\n",
        "design/anti-slop/SKILL.md": "---\nname: design-anti-slop\ndescription: >\n  Detect and eliminate generic AI design patterns. Identifies cookie-cutter layouts,\n  overused gradients, stock aesthetics, and buzzword-heavy copy. Use when reviewing\n  designs for authenticity and avoiding the \"AI startup\" look.\napplies_to:\n  - \"**/*.figma\"\n  - \"**/*.sketch\"\n  - \"design reviews\"\ntags: [design, ui, ux, visual-design, authenticity]\nrelated_skills:\n  - text/anti-slop\nversion: 2.0.0\n---\n\n# Design Anti-Slop Skill\n\n## When to Use This Skill\n\nUse design-anti-slop when:\n- ✓ Reviewing AI-generated designs before implementation\n- ✓ Conducting design audits for generic patterns\n- ✓ Catching \"template syndrome\" in landing pages\n- ✓ Ensuring brand authenticity in visual design\n- ✓ Teaching design quality standards to teams\n- ✓ Detecting the \"AI startup aesthetic\"\n\nDo NOT use when:\n- Following specific brand guidelines that happen to use common patterns\n- Working with established design systems (different context)\n- Intentionally using templates for rapid prototyping\n\n## Quick Example\n\n**Before (AI Design Slop)**:\n- Purple/pink/cyan mesh gradient hero\n- \"Empower Your Business\" headline\n- Three feature cards with generic icons\n- Floating 3D geometric shapes\n- Stats section with big numbers\n- Generic testimonials\n- \"Get Started Today\" CTA\n\n**After (Authentic Design)**:\n- Brand-specific color palette\n- Specific value proposition headline\n- Content-driven layout based on user needs\n- Visual elements that serve purpose\n- Real customer data/quotes with context\n- Clear, action-oriented CTA\n\n**What changed**:\n- ✓ Removed generic gradient background\n- ✓ Replaced buzzword headline with specific value\n- ✓ Designed layout for actual content, not template\n- ✓ Eliminated decorative 3D elements\n- ✓ Used authentic customer proof\n- ✓ Specified the action, not generic \"get started\"\n\n## When to Use What\n\n| If you see... | Red flag because... | Better approach |\n|---------------|---------------------|-----------------|\n| Purple/pink/cyan gradient | \"AI startup\" cliché | Brand colors, solid backgrounds |\n| \"Empower your business\" | Generic buzzword headline | Specific value: \"Cut API response time 80%\" |\n| Floating 3D shapes | Decorative without purpose | Purpose-driven visuals or remove |\n| Everything in cards | Template thinking | Vary layouts based on content type |\n| Inter for everything | Default font choice | Fonts matching brand personality |\n| Center-aligned everything | Lazy symmetry | Intentional alignment, left for readability |\n| Generic icon + text cards | Cookie-cutter pattern | Content-first layout variations |\n| \"Get Started\" CTA | Vague action | Specific: \"Start Free Trial\" \"Download Report\" |\n\n## Core Workflow\n\n### 3-Step Slop Detection\n\n1. **Visual audit** - Scan for generic patterns\n   ```\n   Check for:\n   - Generic gradients (purple/pink/cyan mesh)\n   - Floating 3D geometric shapes\n   - Glass morphism overuse\n   - Stock photo aesthetics\n   - Same font for everything (usually Inter)\n   ```\n\n2. **Layout audit** - Check structure\n   ```\n   Red flags:\n   - Cookie-cutter landing page template\n   - Everything in cards\n   - Center-aligned everything\n   - Identical spacing everywhere\n   - Template-driven, not content-driven\n   ```\n\n3. **Copy audit** - Review microcopy\n   ```\n   Buzzword alerts:\n   - \"Empower your business\"\n   - \"Take control of your future\"\n   - \"Unlock your potential\"\n   - \"Seamless experience\"\n   - Generic CTAs (\"Get Started\", \"Learn More\")\n   ```\n\n## Quick Reference Checklist\n\nDesign slop indicators:\n\n- [ ] Purple/pink/cyan gradient background\n- [ ] Floating 3D shapes (cubes, spheres, toruses)\n- [ ] Glass morphism on every element\n- [ ] Stock photo aesthetics (diverse workplace, people pointing at screens)\n- [ ] Inter for all text\n- [ ] Everything in cards\n- [ ] Everything center-aligned\n- [ ] \"Empower your business\" type headlines\n- [ ] Generic three-column feature layout\n- [ ] Stats section with big numbers but no context\n- [ ] Fake-looking testimonials\n- [ ] Generic pricing cards\n- [ ] \"Get Started Today\" CTA\n\n## Common Workflows\n\n### Workflow 1: Audit Landing Page Design\n\n**Context**: Reviewing a landing page for generic AI patterns.\n\n**Steps**:\n\n1. **Check hero section**\n   ```\n   Red flags:\n   - Gradient mesh background (purple/pink/cyan)\n   - \"Empower/Transform/Revolutionize Your Business\"\n   - Generic stock photo or 3D shapes\n   - Center-aligned everything\n\n   Better:\n   - Brand-specific colors\n   - Specific value prop: \"Process 10k transactions/sec\"\n   - Real product screenshot or authentic imagery\n   - Strategic alignment\n   ```\n\n2. **Check features section**\n   ```\n   Red flags:\n   - Exactly three cards\n   - Generic icons (rocket, shield, chart)\n   - \"Feature 1\" \"Feature 2\" \"Feature 3\" layout\n   - All cards identical size regardless of content\n\n   Better:\n   - Content-driven layout (2, 4, 5 items if that's what you need)\n   - Specific icons or remove them\n   - Vary presentation based on feature importance\n   - Size cards based on content\n   ```\n\n3. **Check social proof**\n   ```\n   Red flags:\n   - Generic testimonials with stock photos\n   - \"10k+ users\" \"99% satisfaction\" without context\n   - Logo wall of companies you can't verify\n\n   Better:\n   - Real customer quotes with full names, roles, companies\n   - Specific metrics: \"Reduced support tickets 40% in Q3 2024\"\n   - Verifiable customer relationships\n   ```\n\n4. **Check CTA**\n   ```\n   Red flags:\n   - \"Get Started\"\n   - \"Learn More\"\n   - \"Try for Free\"\n   - Gradient button with excessive shadow\n\n   Better:\n   - Specific action: \"Start 14-Day Trial\" \"Download Whitepaper\"\n   - \"See Live Demo\" \"Request Pricing\"\n   - Simple, clear button styling\n   ```\n\n**Expected outcome**: Landing page that reflects actual product/brand\n\n---\n\n### Workflow 2: Review SaaS Dashboard Design\n\n**Context**: Checking for generic dashboard patterns.\n\n**Steps**:\n\n1. **Check color usage**\n   ```\n   Red flags:\n   - Purple accent color (generic SaaS palette)\n   - Pastel everything\n   - Pure black (#000) and pure white (#FFF)\n\n   Better:\n   - Brand-specific palette\n   - Functional color system (success, warning, error)\n   - Accessible contrast ratios (WCAG AA minimum)\n   ```\n\n2. **Check layout patterns**\n   ```\n   Red flags:\n   - Every section in a card\n   - Cards within cards\n   - Everything spaced identically\n   - Floating elements with excessive shadows\n\n   Better:\n   - Vary visual treatment based on content type\n   - Group related data visually\n   - Use spacing to create hierarchy\n   - Subtle shadows where needed for depth\n   ```\n\n3. **Check data visualization**\n   ```\n   Red flags:\n   - Default chart library colors\n   - Gradients on every chart\n   - 3D charts (rarely useful)\n   - Decorative animations\n\n   Better:\n   - Purpose-driven color choices\n   - Simple, readable charts\n   - 2D charts (easier to read)\n   - Animation only for state changes\n   ```\n\n**Expected outcome**: Dashboard that prioritizes data clarity over aesthetics\n\n---\n\n### Workflow 3: Review Mobile App Design\n\n**Context**: Checking mobile UI for generic patterns.\n\n**Steps**:\n\n1. **Check navigation**\n   ```\n   Red flags:\n   - Generic bottom tab bar with 5 items\n   - Icons without labels\n   - Hamburger menu for everything\n\n   Better:\n   - Navigation based on user flow\n   - Most important actions surfaced\n   - Labels for clarity\n   - Consider alternatives to hamburger\n   ```\n\n2. **Check components**\n   ```\n   Red flags:\n   - Fully rounded buttons (pill shape)\n   - Gradient buttons everywhere\n   - Ghost buttons as primary CTAs\n   - Every input has an icon inside\n\n   Better:\n   - Appropriate border radius (4-8px usually)\n   - Style hierarchy (primary solid, secondary outline)\n   - Icons in inputs only when helpful\n   ```\n\n3. **Check typography**\n   ```\n   Red flags:\n   - Same font family for everything\n   - Body text < 16px\n   - Insufficient line height (< 1.5)\n   - Center-aligned paragraphs\n\n   Better:\n   - Clear hierarchy with font sizes/weights\n   - Readable body text (16px+)\n   - Comfortable line height (1.5-1.8)\n   - Left-aligned body text\n   ```\n\n**Expected outcome**: Mobile UI that serves users, not trends\n\n## High-Confidence Slop Patterns\n\nThese are almost always AI-generated or template-derived:\n\n### Visual\n- **Purple (#7F5AF0) + Cyan (#2CB67D) + Pink (#FF6AC1) palette** - The \"AI startup\" gradient\n- **Floating 3D geometric shapes** - Cubes, spheres, toruses with no purpose\n- **Glassmorphism on everything** - Blurred backgrounds, frosted glass effects overused\n- **Stock workplace diversity photos** - Generic \"people collaborating\" imagery\n\n### Layout\n- **Exact three-column feature cards** - The universal AI template\n- **Everything in cards** - Cards within cards within cards\n- **Hero → 3 features → stats → testimonials → pricing → CTA** - The AI landing page formula\n- **Center-aligned everything** - Lazy symmetry without purpose\n\n### Typography\n- **Inter for everything** - The default modern font choice\n- **Montserrat + Open Sans** - The generic pairing\n- **Excessive font variation** - 5+ fonts trying to look \"designed\"\n\n### Copy\n- **\"Empower your business\"** - Generic value prop\n- **\"Seamless experience\"** - Meaningless modifier\n- **\"Get Started\" CTA** - Vague action\n- **\"Trusted by industry leaders\"** - Unverifiable claim\n\n## Context Matters\n\nNot all patterns are always slop:\n\n- **Brand guidelines** - If brand actually uses purple, it's authentic\n- **Industry standards** - Some patterns are conventions (e.g., ecommerce layouts)\n- **Accessibility** - Some \"generic\" patterns exist for good UX reasons\n- **Established systems** - Material Design, iOS HIG have reasons for patterns\n\nThe issue is **thoughtless copying** without considering if it serves your specific users and content.\n\n## Resources & Advanced Topics\n\n### Reference Files\n\n- **[reference/visual.md](reference/visual.md)** - Complete visual design pattern catalog\n- **[reference/layout.md](reference/layout.md)** - Layout antipatterns and alternatives\n- **[reference/ux-writing.md](reference/ux-writing.md)** - Microcopy and button text patterns\n\n### Related Skills\n\n- **text/anti-slop** - For cleaning up design copy and documentation\n- **humanizer** - For making design copy sound human\n\n### Tools\n\nDesign audit workflow:\n1. Screenshot key sections\n2. Compare against generic AI patterns\n3. Identify specific slop instances\n4. Redesign based on content/brand needs\n\n## Complementary Use\n\nDesign anti-slop focuses on **pattern detection**. It doesn't teach design principles.\n\nFor learning design:\n- Study design systems: Material Design, iOS HIG, Polaris\n- Learn accessibility: WCAG guidelines\n- Study color theory, typography, layout principles\n\nThis skill helps you **avoid generic AI output** once you know what you're doing.\n",
        "examples/README.md": "# Anti-Slop Examples\n\nThis directory contains paired examples of \"slop\" (generic/AI-generated) and \"clean\" (production-quality) code and text. Use these to test the detection toolkit or as a reference for quality standards.\n\n## How to Use\n\nRun the toolkit detection scripts against these files to see the difference in scoring.\n\n### Text Analysis\n\n```bash\n# Detect slop (High score expected)\npython3 ../toolkit/scripts/detect_slop.py text/before-after/documentation_slop.md\n\n# Detect clean (Low score expected)\npython3 ../toolkit/scripts/detect_slop.py text/before-after/documentation_clean.md\n```\n\n### R Code Analysis\n\n```bash\n# Detect slop (High score expected)\nRscript ../toolkit/scripts/detect_slop.R r/before-after/analysis_slop.R\n\n# Detect clean (Low score expected)\nRscript ../toolkit/scripts/detect_slop.R r/before-after/analysis_clean.R\n```\n\n### Python Analysis\n\nCurrently, the toolkit focuses on text and R code detection. For Python, compare `python/before-after/processing_slop.py` and `python/before-after/processing_clean.py` manually against the [python/anti-slop](../python/anti-slop/SKILL.md) standards.\n\n## Structure\n\n- **r/before-after/**: R analysis scripts (slop vs clean)\n- **python/before-after/**: Python data processing scripts (slop vs clean)\n- **text/before-after/**: Technical documentation examples (slop vs clean)\n- **workflows/**: Complete workflows showing skill application\n- **integration/**: Using Posit + Anti-Slop skills together\n- **bad/**: High-slop examples for testing\n",
        "julia/anti-slop/SKILL.md": "---\nname: julia-anti-slop\ndescription: >\n  Enforce high-performance Julia conventions for scientific computing. \n  Prevents generic AI patterns through type stability, multiple dispatch \n  best practices, and DataFrames.jl standards.\napplies_to:\n  - \"**/*.jl\"\ntags: [julia, scientific-computing, type-stability, data-frames]\nrelated_skills:\n  - r/anti-slop\n  - python/anti-slop\nversion: 2.0.0\n---\n\n# Julia Anti-Slop Skill for Scientific Computing\n\n## When to Use This Skill\n\nUse julia-anti-slop when:\n- ✓ Writing new Julia code for scientific computing or data science\n- ✓ Reviewing AI-generated Julia code before committing\n- ✓ Refactoring existing code for performance and type stability\n- ✓ Building high-performance packages or models\n- ✓ Transitioning from R or Python to Julia\n- ✓ Enforcing Julia community coding standards\n\nDo NOT use when:\n- Writing quick exploratory scripts where performance is irrelevant\n- Working with legacy code that cannot be modified\n- Using Julia for general-purpose web development (though standards still apply)\n\n## Quick Example\n\n**Before (AI Slop)**:\n```julia\n# Process data\nfunction process(data)\n    result = []\n    for x in data\n        push!(result, x * 2.0)\n    end\n    return result\nend\n```\n\n**After (Anti-Slop)**:\n```julia\n\"\"\"\n    calculate_scaled_values(raw_inputs::AbstractVector{T}) where T<:Real\n\nCalculate scaled values by doubling raw inputs. Returns a new vector of same type.\n\"\"\"\nfunction calculate_scaled_values(raw_inputs::AbstractVector{T})::Vector{T} where T<:Real\n    # Pre-allocate for performance\n    scaled_results = similar(raw_inputs)\n    \n    # Broadcast operation is faster and type-stable\n    scaled_results .= raw_inputs .* 2.0\n    \n    return scaled_results\nend\n```\n\n**What changed**:\n- ✓ Descriptive name (`calculate_scaled_values` not `process`)\n- ✓ Type annotations on function signature (`AbstractVector{T}`)\n- ✓ Explicit return type (`Vector{T}`)\n- ✓ Pre-allocation (`similar`) instead of growing an array\n- ✓ Broadcasting (`.`) for performance and clarity\n- ✓ Comprehensive docstring\n\n## When to Use What\n\n| If you need to... | Do this | Details |\n|-------------------|---------|---------|\n| Name variables | `snake_case`, descriptive names | reference/naming.md |\n| Define functions | Use type annotations + multiple dispatch | reference/dispatch.md |\n| Handle data | Use `DataFrames.jl` with `@chain` | reference/dataframes.md |\n| Optimize code | Ensure type stability (`@code_warntype`) | reference/performance.md |\n| Plot data | Use `Plots.jl` or `Makie.jl` | reference/plotting.md |\n| Manage projects | Use `Pkg` with `Project.toml` | reference/reproducibility.md |\n\n## Core Workflow\n\n### 5-Step Quality Check\n\n1. **Type stability** - Check with `@code_warntype`\n   ```julia\n   @code_warntype my_function(args...)\n   ```\n\n2. **Function signatures** - Use specific or parametric types\n   ```julia\n   # Good\n   function solve(x::AbstractVector{T}) where T<:Real\n   \n   # Bad\n   function solve(x)\n   ```\n\n3. **Pre-allocation** - Avoid growing arrays in loops\n   ```julia\n   results = zeros(n)\n   for i in 1:n\n       results[i] = compute(i)\n   end\n   ```\n\n4. **Broadcasting** - Use native `.` syntax for vectorization\n   ```julia\n   y = @. exp(x) * sin(x)\n   ```\n\n5. **In-place operations** - Use `!` suffix for functions that modify inputs\n   ```julia\n   sort!(my_vector)\n   normalize!(data_matrix)\n   ```\n\n## Quick Reference Checklist\n\nBefore committing Julia code, verify:\n\n- [ ] All functions have type annotations\n- [ ] Functions are type-stable (verified with `@code_warntype`)\n- [ ] No global variables in hot loops\n- [ ] Arrays are pre-allocated where possible\n- [ ] Use `!` for functions that mutate arguments\n- [ ] Broadcasting (`.`) used instead of explicit loops where appropriate\n- [ ] Docstrings follow Julia standard format\n- [ ] No single-letter variables except for standard indices\n- [ ] Multiple dispatch used to handle different types\n- [ ] Project has `Project.toml` and `Manifest.toml`\n\n## Common Workflows\n\n### Workflow 1: Optimize AI-Generated Julia Code\n\n**Context**: AI generated a function that is slow or type-unstable.\n\n**Steps**:\n\n1. **Identify instability**\n   ```julia\n   @code_warntype my_function(data)\n   # Look for Any or Union types in red\n   ```\n\n2. **Add type constraints**\n   ```julia\n   # Before\n   function process(x) ...\n   \n   # After\n   function process(x::Vector{Float64}) ...\n   ```\n\n3. **Remove type-changing assignments**\n   ```julia\n   # Before\n   val = 0  # Int\n   val = 0.5 # Now Float64 - unstable!\n   \n   # After\n   val = 0.0 # Float64 from the start\n   ```\n\n4. **Verify with BenchmarkTools**\n   ```julia\n   using BenchmarkTools\n   @benchmark my_function(data)\n   ```\n\n**Expected outcome**: Type-stable, high-performance function\n\n---\n\n### Workflow 2: Build a Data Pipeline\n\n**Context**: Processing a large CSV with `DataFrames.jl`.\n\n**Steps**:\n\n1. **Load data with explicit types**\n   ```julia\n   using CSV, DataFrames\n   customer_data = CSV.read(\"data.csv\", DataFrame, types=Dict(:id => Int64))\n   ```\n\n2. **Use `@chain` for transformations**\n   ```julia\n   using Chain\n   summary = @chain customer_data begin\n       filter(:age => >=(18), _)\n       groupby(:region)\n       combine(:revenue => sum => :total_revenue)\n   end\n   ```\n\n3. **Ensure output type stability**\n   ```julia\n   function get_summary(df::DataFrame)::DataFrame\n       # pipeline...\n   end\n   ```\n\n**Expected outcome**: Readable, maintainable, and fast data pipeline\n\n---\n\n### Workflow 3: Prepare Package for Release\n\n**Context**: Standardizing code for a Julia package.\n\n**Steps**:\n\n1. **Organize exports**\n   ```julia\n   module MyPackage\n   export solve_problem, MyType\n   # ...\n   end\n   ```\n\n2. **Add comprehensive docstrings**\n   ```julia\n   \"\"\"\n       solve_problem(data::AbstractArray)\n   \n   Explain the algorithm and return types.\n   \"\"\"\n   ```\n\n3. **Verify Project.toml**\n   ```julia\n   # Ensure all dependencies are tracked\n   using Pkg\n   Pkg.status()\n   ```\n\n4. **Add tests**\n   ```julia\n   using Test\n   @testset \"MyPackage.jl\" begin\n       @test solve_problem([1, 2]) == 3\n   end\n   ```\n\n**Expected outcome**: Professional Julia package following community standards\n\n## Mandatory Rules Summary\n\n### 1. Type Stability\nFunctions must not change the type of a variable within their scope. Check with `@code_warntype`.\n\n### 2. Parametric Types\nUse `AbstractVector{T} where T<:Real` instead of just `Vector` or `Any`.\n\n### 3. Mutating Functions\nAlways append `!` to functions that modify their arguments (e.g., `sort!`, `push!`).\n\n### 4. No Globals in Loops\nNever access or modify global variables inside performance-critical loops.\n\n### 5. Standard Naming\n- Variables/Functions: `snake_case`\n- Types/Modules: `PascalCase`\n\n## Resources & Advanced Topics\n\n### Reference Files (Planned)\n\n- **reference/performance.md** - Type stability and benchmarking\n- **reference/dispatch.md** - Multiple dispatch patterns\n- **reference/dataframes.md** - Data manipulation standards\n- **reference/plotting.md** - Visualization best practices\n- **reference/reproducibility.md** - Pkg and environment management\n\n### Related Skills\n\n- **r/anti-slop** - For users moving from R to Julia\n- **python/anti-slop** - For users moving from Python to Julia\n\n### Tools\n\n- `BenchmarkTools.jl` - Accurate benchmarking\n- `Chain.jl` - Readable pipelines\n- `Revise.jl` - Live code updates without restarting\n- `LanguageServer.jl` - IDE support",
        "python/anti-slop/SKILL.md": "---\nname: python-anti-slop\ndescription: >\n  Enforce production-quality Python code standards. Prevents generic AI patterns\n  through PEP 8 compliance, type hints, and pandas conventions. Use when writing\n  or reviewing Python data science code.\napplies_to:\n  - \"**/*.py\"\n  - \"**/*.ipynb\"\ntags: [python, pep8, pandas, data-science, type-hints]\nrelated_skills:\n  - r/anti-slop\n  - text/anti-slop\nversion: 2.0.0\n---\n\n# Python Anti-Slop: No More `data = df`\n\n## When to Use This\n\nUse for:\n- ✓ Python code going to production or teammates\n- ✓ AI-generated code (catches missing type hints, `def process_data(data):`)\n- ✓ Data science pipelines (pandas, numpy, sklearn)\n- ✓ Any code that needs to be maintained\n\nSkip for:\n- Quick notebook experiments (but habits stick)\n- Legacy code you inherited\n- Different style guides (Google, etc.) that override\n\n## Quick Example\n\n**Before (AI Slop)**:\n```python\n# Import libraries\nimport pandas as pd\n\n# Load data\ndata = pd.read_csv(\"data.csv\")\n\n# Process\nresult = data[data['x'] > 0]\n```\n\n**After (Anti-Slop)**:\n```python\nfrom pathlib import Path\nfrom typing import Optional\n\nimport pandas as pd\n\n\ndef load_customer_data(file_path: str) -> pd.DataFrame:\n    \"\"\"\n    Load customer data from CSV file.\n\n    Parameters\n    ----------\n    file_path : str\n        Path to customer CSV file\n\n    Returns\n    -------\n    pd.DataFrame\n        Customer data with columns: id, name, revenue, status\n    \"\"\"\n    data_path = Path(file_path)\n\n    if not data_path.exists():\n        raise FileNotFoundError(f\"Data file not found: {file_path}\")\n\n    customer_data = pd.read_csv(data_path)\n\n    return customer_data\n\n\ndef filter_active_customers(\n    customer_data: pd.DataFrame,\n    min_revenue: float = 0.0\n) -> pd.DataFrame:\n    \"\"\"Filter customers by active status and minimum revenue.\"\"\"\n    active_customers = (\n        customer_data\n        .query(\"status == 'active' & revenue > @min_revenue\")\n        .copy()\n    )\n\n    return active_customers\n```\n\n**What changed**:\n- ✓ Descriptive names (`customer_data` not `data`)\n- ✓ Type hints for all function signatures\n- ✓ Comprehensive docstrings (NumPy style)\n- ✓ Import organization (stdlib, third-party, local)\n- ✓ Input validation with specific errors\n- ✓ pandas method chaining with `.copy()`\n\n## When to Use What\n\n| If you need to... | Do this | Details |\n|-------------------|---------|---------|\n| Name variables | Use `snake_case`, no `data`/`df`/`result` | reference/naming.md |\n| Define functions | Add type hints + NumPy docstrings | reference/type-hints.md |\n| Import packages | Organize: stdlib → third-party → local | reference/imports.md |\n| Use pandas | Method chain with `.copy()` | reference/pandas.md |\n| Handle errors | Specific exceptions + informative messages | reference/error-handling.md |\n| Format code | Use `black` or `ruff format` | reference/formatting.md |\n| Check types | Use `mypy` | reference/type-hints.md |\n| Test code | Use `pytest` with fixtures | reference/testing.md |\n\n## Core Workflow\n\n### 5-Step Quality Check\n\n1. **Type hints** - All functions have typed signatures\n   ```python\n   # Good\n   def calculate_rate(numerator: float, denominator: float) -> float:\n       return numerator / denominator\n\n   # Bad\n   def calculate_rate(numerator, denominator):\n       return numerator / denominator\n   ```\n\n2. **Docstrings** - All functions documented (NumPy/Google style)\n   ```python\n   # Good\n   def process_data(data: pd.DataFrame, threshold: float) -> pd.DataFrame:\n       \"\"\"\n       Process data by filtering and transforming.\n\n       Parameters\n       ----------\n       data : pd.DataFrame\n           Input dataframe with 'value' column\n       threshold : float\n           Minimum value threshold\n\n       Returns\n       -------\n       pd.DataFrame\n           Filtered and processed data\n       \"\"\"\n       ...\n   ```\n\n3. **Naming conventions** - All objects use `snake_case`\n   ```python\n   # Good\n   customer_lifetime_value = calculate_clv(customer_data)\n\n   # Bad\n   customerLifetimeValue = calculate_clv(data)\n   ```\n\n4. **Import organization** - Grouped and sorted\n   ```python\n   # Good\n   # Standard library\n   import os\n   from pathlib import Path\n   from typing import List, Optional\n\n   # Third-party\n   import numpy as np\n   import pandas as pd\n   from sklearn.linear_model import LinearRegression\n\n   # Local\n   from my_package.utils import load_data\n   ```\n\n5. **Format and validate**\n   ```bash\n   black script.py\n   ruff check script.py\n   mypy script.py\n   ```\n\n## Quick Reference Checklist\n\nBefore committing Python code, verify:\n\n- [ ] All functions have type hints\n- [ ] All functions have docstrings (NumPy/Google style)\n- [ ] All variables use `snake_case`\n- [ ] No generic names (`data`, `df`, `result`, `temp`)\n- [ ] Imports organized (stdlib → third-party → local)\n- [ ] Use `.copy()` when modifying DataFrames\n- [ ] Specific exception types (not bare `except:`)\n- [ ] Informative error messages\n- [ ] No mutable default arguments\n- [ ] Formatted with `black` or `ruff`\n- [ ] Passes `mypy` type checking\n- [ ] Statistical tests report SE and CI\n\n## Common Workflows\n\n### Workflow 1: Clean Up AI-Generated Python Script\n\n**Context**: AI generated a data analysis script with generic patterns.\n\n**Steps**:\n\n1. **Fix imports**\n   ```python\n   # Before\n   from pandas import *\n   import numpy as np\n   from my_module import *\n\n   # After\n   from typing import Optional\n\n   import numpy as np\n   import pandas as pd\n\n   from my_module import load_data, process_data\n   ```\n\n2. **Add type hints**\n   ```python\n   # Before\n   def calculate_stats(data, column):\n       return data[column].mean()\n\n   # After\n   def calculate_stats(data: pd.DataFrame, column: str) -> float:\n       \"\"\"Calculate mean for specified column.\"\"\"\n       return data[column].mean()\n   ```\n\n3. **Add docstrings**\n   ```python\n   # Before\n   def filter_data(df, threshold):\n       return df[df['value'] > threshold]\n\n   # After\n   def filter_data(\n       data: pd.DataFrame,\n       threshold: float\n   ) -> pd.DataFrame:\n       \"\"\"\n       Filter dataframe by value threshold.\n\n       Parameters\n       ----------\n       data : pd.DataFrame\n           Input dataframe with 'value' column\n       threshold : float\n           Minimum value to keep\n\n       Returns\n       -------\n       pd.DataFrame\n           Filtered dataframe\n       \"\"\"\n       filtered_data = data.query(\"value > @threshold\").copy()\n       return filtered_data\n   ```\n\n4. **Fix pandas operations**\n   ```python\n   # Before\n   df['new_col'] = df['value'] * 2  # modifies original\n   result = df.groupby('category').agg({'value': 'mean'}).reset_index().sort_values('value')\n\n   # After\n   processed_data = data.copy()\n   processed_data['new_col'] = processed_data['value'] * 2\n\n   summary = (\n       data\n       .groupby('category')\n       .agg(mean_value=('value', 'mean'))\n       .reset_index()\n       .sort_values('mean_value')\n   )\n   ```\n\n5. **Format and validate**\n   ```bash\n   black script.py\n   ruff check script.py\n   mypy script.py\n   ```\n\n**Expected outcome**: Clean, type-safe, production-ready code\n\n---\n\n### Workflow 2: Add Error Handling\n\n**Context**: Functions don't validate inputs or handle errors.\n\n**Steps**:\n\n1. **Add input validation**\n   ```python\n   def calculate_growth(initial: float, final: float, periods: int) -> float:\n       \"\"\"Calculate compound growth rate.\"\"\"\n\n       # Validate inputs\n       if initial <= 0:\n           raise ValueError(\n               f\"initial must be positive, got {initial}\"\n           )\n\n       if periods <= 0:\n           raise ValueError(\n               f\"periods must be positive, got {periods}\"\n           )\n\n       growth_rate = (final / initial) ** (1 / periods) - 1\n       return growth_rate\n   ```\n\n2. **Use specific exceptions**\n   ```python\n   # Before\n   try:\n       data = pd.read_csv(file_path)\n   except:\n       print(\"Error\")\n       return None\n\n   # After\n   try:\n       data = pd.read_csv(file_path)\n   except FileNotFoundError:\n       raise FileNotFoundError(\n           f\"Data file not found: {file_path}\\n\"\n           f\"Current directory: {os.getcwd()}\"\n       )\n   except pd.errors.ParserError as e:\n       raise ValueError(\n           f\"Failed to parse CSV: {file_path}\\n\"\n           f\"Error: {str(e)}\"\n       ) from e\n   ```\n\n3. **Validate DataFrame structure**\n   ```python\n   def validate_columns(\n       data: pd.DataFrame,\n       required_cols: List[str]\n   ) -> None:\n       \"\"\"Validate dataframe has required columns.\"\"\"\n       missing = set(required_cols) - set(data.columns)\n\n       if missing:\n           raise ValueError(\n               f\"Missing required columns: {missing}\\n\"\n               f\"Available: {list(data.columns)}\"\n           )\n   ```\n\n**Expected outcome**: Robust code with clear error messages\n\n---\n\n### Workflow 3: Prepare Module for Distribution\n\n**Context**: Preparing code for PyPI or internal distribution.\n\n**Steps**:\n\n1. **Add type hints everywhere**\n   ```bash\n   mypy --strict my_module/\n   ```\n\n2. **Ensure all functions documented**\n   ```python\n   # Every public function needs docstring\n   def public_function(arg: str) -> int:\n       \"\"\"\n       Public API function.\n\n       Parameters\n       ----------\n       arg : str\n           Description\n\n       Returns\n       -------\n       int\n           Description\n       \"\"\"\n       ...\n   ```\n\n3. **Format code**\n   ```bash\n   black my_module/\n   ruff check my_module/ --fix\n   ```\n\n4. **Add tests**\n   ```python\n   # Use pytest with type-checked test functions\n   def test_calculate_rate() -> None:\n       \"\"\"Test rate calculation.\"\"\"\n       result = calculate_rate(10.0, 2.0)\n       assert result == 5.0\n   ```\n\n5. **Check test coverage**\n   ```bash\n   pytest --cov=my_module tests/\n   ```\n\n**Expected outcome**: Professional, distributable package\n\n## Mandatory Rules Summary\n\n### 1. Type Hints Required\n**All function signatures must have type hints**\n\n```python\nfrom typing import List, Dict, Optional, Tuple\n\ndef process(data: pd.DataFrame, threshold: float = 0.5) -> pd.DataFrame:\n    ...\n```\n\n### 2. Docstrings Required\n**All functions need NumPy or Google style docstrings**\n\n### 3. Naming: snake_case\n**All objects use `snake_case`**\n- Variables: `customer_data` not `customerData`\n- Functions: `calculate_rate` not `calculateRate`\n- No generic: `data`, `df`, `result`, `temp`\n\n### 4. Import Organization\n**Group and sort imports**\n1. Standard library\n2. Third-party (alphabetical)\n3. Local imports\n\n### 5. No Mutable Defaults\n**Never use mutable default arguments**\n\n```python\n# Bad\ndef append_to_list(item, my_list=[]):\n    ...\n\n# Good\ndef append_to_list(item, my_list=None):\n    if my_list is None:\n        my_list = []\n    ...\n```\n\n## PEP 8 Compliance\n\nFollow [PEP 8](https://pep8.org/) style guide rigorously:\n\n1. **Use automatic formatters**: `black`, `ruff`\n2. **Type hints for clarity**: All public APIs\n3. **Explicit over implicit**: Be clear about intentions\n4. **Readable structure**: Format for humans\n\nSee **reference/pep8.md** for complete PEP 8 guidelines.\n\n## Resources & Advanced Topics\n\n### Reference Files\n\n- **[reference/type-hints.md](reference/type-hints.md)** - Complete type hinting guide\n- **[reference/pandas.md](reference/pandas.md)** - pandas method chaining and best practices\n- **[reference/error-handling.md](reference/error-handling.md)** - Exception handling patterns\n- **[reference/testing.md](reference/testing.md)** - pytest patterns and fixtures\n- **[reference/imports.md](reference/imports.md)** - Import organization\n- **[reference/formatting.md](reference/formatting.md)** - black, ruff, isort usage\n\n### Related Skills\n\n- **r/anti-slop** - For R users transitioning to Python\n- **text/anti-slop** - For cleaning docstring prose\n\n### Tools\n\n- `black` - Uncompromising code formatter\n- `ruff` - Fast linter and formatter\n- `mypy` - Type checking\n- `pytest` - Testing framework\n- `isort` - Import sorting\n\n## Integration with R Background\n\nFor R users, key differences:\n\n| Concept | R | Python |\n|---------|---|--------|\n| Indexing | 1-based | 0-based |\n| Assignment | `<-` or `=` | `=` only |\n| Pipe equivalent | `\\|>` | Method chaining `.` |\n| Missing values | `NA` | `None`, `np.nan` |\n| Data frames | `tibble` | `pd.DataFrame` |\n| True/False | `TRUE`/`FALSE` | `True`/`False` |\n\nSee **reference/r-to-python.md** for complete migration guide.\n",
        "quarto/anti-slop/SKILL.md": "---\nname: quarto-anti-slop\ndescription: >\n  Enforce reproducible research document standards for Quarto and RMarkdown.\n  Prevents generic AI-generated documents through proper cross-referencing,\n  bibliography management, and PDF-first practices for papers.\napplies_to:\n  - \"**/*.qmd\"\n  - \"**/*.Rmd\"\n  - \"**/*.ipynb\"\ntags: [quarto, rmarkdown, reproducibility, academic-writing, literate-programming]\nrelated_skills:\n  - r/anti-slop\n  - python/anti-slop\n  - text/anti-slop\nversion: 2.0.0\n---\n\n# Quarto & RMarkdown Anti-Slop Skill\n\n## When to Use This Skill\n\nUse quarto-anti-slop when:\n- ✓ Creating academic papers or technical reports\n- ✓ Building reproducible research documents\n- ✓ Reviewing AI-generated Quarto/RMarkdown documents\n- ✓ Preparing manuscripts for journal submission\n- ✓ Creating data analysis notebooks\n- ✓ Building presentations with code\n- ✓ Enforcing reproducibility standards\n\nDo NOT use when:\n- Writing quick exploratory notebooks (though standards still help)\n- Working with non-technical documents (use text/anti-slop)\n- Creating pure code projects (use language-specific anti-slop)\n\n## Quick Example\n\n**Before (AI Slop)**:\n```yaml\n---\ntitle: \"Analysis\"\noutput: pdf_document\n---\n\nFigure 1 shows the results.\n\n```{r}\ndata <- read.csv(\"data.csv\")\nplot(data$x, data$y)\n```\n```\n\n**After (Anti-Slop)**:\n```yaml\n---\ntitle: \"Customer Retention Analysis\"\nauthor:\n  - name: Jane Researcher\n    affiliation: University Name\ndate: today\nformat:\n  pdf:\n    number-sections: true\n    keep-tex: true\nbibliography: references.bib\nexecute:\n  echo: false\n  cache: true\n---\n\n@fig-retention shows customer retention rates across cohorts.\n\n```{r}\n#| label: fig-retention\n#| fig-cap: \"Customer Retention by Cohort\"\n#| fig-width: 7\n#| fig-height: 5\n\ncustomer_data <- readr::read_csv(\"data/customers.csv\")\n\nggplot2::ggplot(customer_data, ggplot2::aes(x = month, y = retention_rate)) +\n  ggplot2::geom_line() +\n  ggplot2::theme_minimal()\n```\n```\n\n**What changed**:\n- ✓ Complete metadata (author, affiliation, date)\n- ✓ Proper cross-references (`@fig-retention`)\n- ✓ Labeled code chunks with descriptive names\n- ✓ Bibliography configuration\n- ✓ Caching for reproducibility\n- ✓ Descriptive figure captions\n\n## When to Use What\n\n| If you need to... | Do this | Reference |\n|-------------------|---------|-----------|\n| Academic paper | PDF format + cross-refs + citations | workflow-1-pdf-paper |\n| Technical report | HTML format + code folding | workflow-2-html-report |\n| Presentation | Revealjs or Beamer | YAML section |\n| Cross-reference figures | `@fig-label` | Cross-Referencing |\n| Cross-reference tables | `@tbl-label` | Cross-Referencing |\n| Cross-reference equations | `@eq-label` | Mathematical Typesetting |\n| Manage citations | BibTeX + CSL | Citations and References |\n| Cache computations | `cache: true` + `dependson:` | Caching Strategy |\n| Reproducibility | Session info + relative paths | Reproducibility Documentation |\n| Fix generic YAML | Use complete metadata | YAML Configuration |\n\n## Core Workflow\n\n### 5-Step Quality Check\n\n1. **Complete YAML metadata**\n   ```yaml\n   ---\n   title: \"Specific Descriptive Title\"\n   author:\n     - name: Author Name\n       affiliation: Institution\n   date: today\n   format:\n     pdf:  # or html\n       number-sections: true\n   bibliography: references.bib\n   ---\n   ```\n\n2. **Label all outputs**\n   ```{r}\n   #| label: fig-descriptive-name\n   #| fig-cap: \"Complete descriptive caption\"\n   ```\n\n3. **Use cross-references**\n   ```markdown\n   @fig-name shows... @tbl-name presents... @eq-name defines...\n   ```\n\n4. **Cite properly**\n   ```markdown\n   Recent work [@author2023; @other2024] demonstrates...\n   ```\n\n5. **Verify reproducibility**\n   - All paths relative to project root\n   - Computations cached appropriately\n   - Session info included\n\n## Quick Reference Checklist\n\nBefore rendering final document:\n\n- [ ] Title, author, affiliation complete\n- [ ] Abstract present (if academic paper)\n- [ ] All figures labeled: `#| label: fig-*`\n- [ ] All tables labeled: `#| label: tbl-*`\n- [ ] All equations labeled: `{#eq-*}`\n- [ ] Cross-references use `@` syntax\n- [ ] `bibliography:` in YAML with valid .bib file\n- [ ] All citations use `@key` format\n- [ ] Echo settings appropriate (false for papers)\n- [ ] All paths relative to project root\n- [ ] Long computations cached with `cache: true`\n- [ ] Session info included in appendix\n- [ ] Document renders from clean state\n- [ ] No `editor: visual` line in YAML\n\n## Common Workflows\n\n### Workflow 1: Create PDF Research Paper\n\n**Context**: Writing academic paper with statistical analysis.\n\n**Steps**:\n\n1. **Set up YAML with complete metadata**\n   ```yaml\n   ---\n   title: \"Your Research Title\"\n   author:\n     - name: First Author\n       affiliation: University Name\n       email: author@university.edu\n   date: today\n   date-format: \"MMMM D, YYYY\"\n   format:\n     pdf:\n       documentclass: article\n       geometry: margin=1in\n       number-sections: true\n       colorlinks: true\n       keep-tex: true\n   abstract: |\n     Brief summary of research question, methods, and findings.\n   keywords: [keyword1, keyword2, keyword3]\n   bibliography: references.bib\n   csl: american-statistical-association.csl\n   execute:\n     echo: false\n     warning: false\n     message: false\n     cache: true\n   ---\n   ```\n\n2. **Structure with cross-referenced sections**\n   ```markdown\n   # Introduction {#sec-intro}\n\n   # Data {#sec-data}\n\n   # Methods {#sec-methods}\n\n   # Results {#sec-results}\n\n   As discussed in @sec-methods, we use...\n   ```\n\n3. **Create labeled figures**\n   ```{r}\n   #| label: fig-scatterplot\n   #| fig-cap: \"Relationship between X and Y with regression line\"\n   #| fig-width: 7\n   #| fig-height: 5\n   #| echo: false\n\n   library(ggplot2)\n\n   ggplot2::ggplot(data, ggplot2::aes(x = predictor, y = outcome)) +\n     ggplot2::geom_point(alpha = 0.5) +\n     ggplot2::geom_smooth(method = \"lm\", se = TRUE) +\n     ggplot2::theme_minimal(base_size = 12) +\n     ggplot2::labs(\n       x = \"Predictor Variable\",\n       y = \"Outcome Variable\"\n     )\n   ```\n\n4. **Add regression tables**\n   ```{r}\n   #| label: tbl-regression\n   #| tbl-cap: \"Linear Regression Results\"\n\n   library(modelsummary)\n\n   models <- list(\n     \"Model 1\" = lm(outcome ~ predictor1, data = data),\n     \"Model 2\" = lm(outcome ~ predictor1 + predictor2, data = data)\n   )\n\n   modelsummary::modelsummary(\n     models,\n     stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),\n     gof_map = c(\"nobs\", \"r.squared\", \"adj.r.squared\")\n   )\n   ```\n\n5. **Add citations and references**\n   ```markdown\n   Recent work [@smith2020; @jones2021] shows...\n\n   # References {.unnumbered}\n\n   ::: {#refs}\n   :::\n   ```\n\n6. **Include reproducibility info**\n   ```markdown\n   # Appendix: Computational Details {.unnumbered}\n\n   ```{r}\n   #| label: session-info\n   #| echo: false\n\n   sessionInfo()\n   ```\n   ```\n\n**Expected outcome**: Publication-ready PDF with all cross-references working\n\n---\n\n### Workflow 2: Build HTML Technical Report\n\n**Context**: Creating interactive analysis report with code visibility.\n\n**Steps**:\n\n1. **Set up HTML-specific YAML**\n   ```yaml\n   ---\n   title: \"Technical Analysis Report\"\n   author: \"Your Name\"\n   date: today\n   format:\n     html:\n       toc: true\n       toc-depth: 3\n       toc-location: left\n       code-fold: true\n       code-tools: true\n       embed-resources: true\n       theme: cosmo\n   execute:\n     echo: true\n     warning: false\n     message: false\n   ---\n   ```\n\n2. **Create analysis sections with code folding**\n   ```markdown\n   # Data Loading {#sec-data}\n\n   ```{r}\n   #| label: load-data\n   #| code-fold: show\n\n   customer_data <- readr::read_csv(\"data/customers.csv\")\n   ```\n\n   # Exploratory Analysis {#sec-eda}\n\n   ```{r}\n   #| label: fig-distribution\n   #| fig-cap: \"Distribution of Customer Ages\"\n\n   ggplot2::ggplot(customer_data, ggplot2::aes(x = age)) +\n     ggplot2::geom_histogram(bins = 30)\n   ```\n   ```\n\n3. **Add interactive tables**\n   ```{r}\n   #| label: tbl-summary\n   #| tbl-cap: \"Summary Statistics by Region\"\n\n   library(gtsummary)\n\n   customer_data |>\n     dplyr::select(region, age, revenue) |>\n     gtsummary::tbl_summary(\n       by = region,\n       statistic = list(\n         gtsummary::all_continuous() ~ \"{mean} ({sd})\"\n       )\n     )\n   ```\n\n4. **Use tabsets for multiple views**\n   ```markdown\n   ## Results by Region {.tabset}\n\n   ### East\n\n   Content for East region...\n\n   ### West\n\n   Content for West region...\n   ```\n\n5. **Test embedded resources**\n   - Verify document is self-contained\n   - Check all images display\n   - Confirm code folding works\n\n**Expected outcome**: Self-contained HTML with interactive features\n\n---\n\n### Workflow 3: Fix Generic Quarto Document\n\n**Context**: AI generated a document with generic patterns.\n\n**Steps**:\n\n1. **Fix incomplete YAML**\n   ```yaml\n   # Before\n   ---\n   title: \"Analysis\"\n   output: pdf_document\n   ---\n\n   # After\n   ---\n   title: \"Customer Retention Analysis: 2023 Cohort Study\"\n   author:\n     - name: Jane Researcher\n       affiliation: Data Science Team\n   date: today\n   format:\n     pdf:\n       number-sections: true\n       keep-tex: true\n   bibliography: references.bib\n   execute:\n     echo: false\n     cache: true\n   ---\n   ```\n\n2. **Replace hard-coded references with cross-refs**\n   ```markdown\n   # Before\n   Figure 1 shows the results.\n   Table 2 presents the estimates.\n\n   # After\n   @fig-results shows the distribution across cohorts.\n   @tbl-estimates presents the regression coefficients.\n   ```\n\n3. **Add labels to all code chunks**\n   ```{r}\n   # Before\n   ```{r}\n   plot(x, y)\n   ```\n\n   # After\n   ```{r}\n   #| label: fig-scatter\n   #| fig-cap: \"Relationship between X and Y\"\n   #| fig-width: 6\n   #| fig-height: 4\n\n   ggplot2::ggplot(data, ggplot2::aes(x = x, y = y)) +\n     ggplot2::geom_point() +\n     ggplot2::theme_minimal()\n   ```\n   ```\n\n4. **Fix relative paths**\n   ```{r}\n   # Before\n   data <- read.csv(\"/Users/someone/Desktop/data.csv\")\n\n   # After\n   data <- readr::read_csv(\"data/analysis_data.csv\")\n   ```\n\n5. **Add caching to long computations**\n   ```{r}\n   #| label: expensive-model\n   #| cache: true\n\n   large_model <- fit_complex_model(data)\n   ```\n\n6. **Include session info**\n   ```markdown\n   # Appendix {.unnumbered}\n\n   ```{r}\n   #| label: session-info\n   sessionInfo()\n   ```\n   ```\n\n**Expected outcome**: Professional document that follows reproducibility standards\n\n---\n\n### Workflow 4: Prepare Manuscript for Journal\n\n**Context**: Converting analysis notebook to journal submission format.\n\n**Steps**:\n\n1. **Update YAML for journal requirements**\n   ```yaml\n   ---\n   title: \"Full Paper Title\"\n   author:\n     - name: First Author\n       affiliation: University\n       orcid: 0000-0000-0000-0000\n   abstract: |\n     Complete abstract following journal guidelines.\n   keywords: [key1, key2, key3]\n   format:\n     pdf:\n       documentclass: article\n       geometry: margin=1in\n       number-sections: true\n       keep-tex: true\n   bibliography: references.bib\n   csl: journal-of-statistics.csl  # Journal-specific\n   ---\n   ```\n\n2. **Verify all cross-references**\n   ```bash\n   # Check for hard-coded references\n   grep -n \"Figure [0-9]\" manuscript.qmd\n   grep -n \"Table [0-9]\" manuscript.qmd\n\n   # Should find none - all should use @fig-* or @tbl-*\n   ```\n\n3. **Format equations with labels**\n   ```markdown\n   $$\n   y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n   $$ {#eq-model}\n\n   The model in @eq-model assumes...\n   ```\n\n4. **Check bibliography completeness**\n   - All citations in .bib file\n   - DOIs included where available\n   - Journal names not abbreviated\n\n5. **Add line numbers (if required)**\n   ```yaml\n   format:\n     pdf:\n       include-in-header:\n         text: |\n           \\usepackage{lineno}\n           \\linenumbers\n   ```\n\n6. **Verify reproducibility**\n   ```bash\n   # Clean render from scratch\n   quarto render manuscript.qmd\n   ```\n\n**Expected outcome**: Submission-ready manuscript meeting journal standards\n\n## Mandatory Rules Summary\n\n### 1. Complete YAML Metadata\n**ALWAYS include**: title, author with affiliation, date, format settings\n\n### 2. Label All Outputs\n**Figures**: `#| label: fig-name`\n**Tables**: `#| label: tbl-name`\n**Equations**: `{#eq-name}`\n\n### 3. Use Cross-References\n**Never**: \"Figure 1\", \"Table 2\"\n**Always**: `@fig-name`, `@tbl-name`, `@eq-name`\n\n### 4. Relative Paths Only\n**Never**: `/Users/name/Desktop/data.csv`\n**Always**: `data/analysis_data.csv`\n\n### 5. Bibliography Management\n**ALWAYS**: Use BibTeX + `bibliography:` in YAML\n**Never**: Manual reference lists\n\n### 6. Reproducibility Documentation\n**ALWAYS**: Include session info, cache long computations, set seeds\n\n## Quarto vs RMarkdown\n\n**Quarto** (recommended for new projects):\n- Language-agnostic (R, Python, Julia, Observable)\n- Modern YAML with `#|` chunk options\n- Better cross-referencing\n- Native multiple format support\n\n**RMarkdown** (legacy, still widely used):\n- R-specific but mature ecosystem\n- Traditional chunk options in header\n- Needs bookdown for advanced cross-refs\n\n**This skill covers both** - most patterns are compatible.\n\n## Forbidden Patterns\n\n### Never Do These\n\n**1. Hard-Coded Figure References**\n```markdown\n# WRONG\nFigure 1 shows the results.\n\n# CORRECT\n@fig-results shows the distribution.\n```\n\n**2. Missing Chunk Labels**\n```{r}\n# WRONG - no label\nplot(x, y)\n\n# CORRECT\n#| label: fig-scatter\n#| fig-cap: \"X vs Y relationship\"\nplot(x, y)\n```\n\n**3. Absolute Paths**\n```{r}\n# WRONG\ndata <- read_csv(\"/Users/me/Desktop/data.csv\")\n\n# CORRECT\ndata <- readr::read_csv(\"data/analysis_data.csv\")\n```\n\n**4. Visual Editor Artifacts**\n```yaml\n# WRONG - remove this\neditor: visual\n```\n\n**5. Inline Code for Papers (OK for reports)**\n```markdown\n# WRONG in published papers\nThe sample size is `r nrow(data)`.\n\n# CORRECT in papers\nThe sample size is 1,542 observations.\n\n# OK in technical reports/notebooks\nThe sample size is `r nrow(data)`.\n```\n\n## Mathematical Typesetting\n\n### Inline vs Display\n\n```markdown\nCORRECT inline: The mean is $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$.\n\nCORRECT display:\n$$\n\\hat{\\beta} = (X^TX)^{-1}X^Ty\n$$ {#eq-ols}\n\nReference: The OLS estimator (@eq-ols) minimizes...\n```\n\n### Common Notation\n\n```latex\n# Greek letters\n$\\alpha, \\beta, \\gamma, \\mu, \\sigma$\n\n# Statistics\n$\\bar{x}$ (mean), $\\hat{\\beta}$ (estimate), $\\tilde{x}$ (alternative)\n$\\mathbb{E}[X]$ (expectation), $\\text{Var}(X)$ (variance)\n\n# Matrices\n$X^T$ (transpose), $X^{-1}$ (inverse), $\\|x\\|$ (norm)\n```\n\n## Citations and References\n\n### Setup\n\n```yaml\nbibliography: references.bib\ncsl: journal-style.csl  # From https://github.com/citation-style-language/styles\nlink-citations: true\n```\n\n### In-Text Citations\n\n```markdown\nRecent work [@smith2020; @jones2021] shows...\n\nAccording to @brown2019, the effect is significant.\n\nMultiple studies [e.g., @davis2018; @wilson2022] demonstrate...\n```\n\n### BibTeX Format\n\n```bibtex\n@article{smith2020,\n  title = {An Important Study},\n  author = {Smith, John and Doe, Jane},\n  journal = {Journal of Important Research},\n  volume = {10},\n  pages = {123--145},\n  year = {2020},\n  doi = {10.1234/journal.2020.001}\n}\n```\n\n## Caching Strategy\n\n### Document-Level\n```yaml\nexecute:\n  cache: true\n  freeze: auto  # Prevents re-execution unless code changes\n```\n\n### Chunk-Level with Dependencies\n```{r}\n#| label: load-data\n#| cache: true\n\nraw_data <- readr::read_csv(\"large_dataset.csv\")\n```\n\n```{r}\n#| label: analyze\n#| cache: true\n#| dependson: \"load-data\"\n\nmodel <- lm(outcome ~ predictor, data = raw_data)\n```\n\n## Code Chunk Best Practices\n\n### Quarto Syntax\n```{r}\n#| label: fig-name\n#| fig-cap: \"Caption here\"\n#| fig-width: 7\n#| fig-height: 5\n#| echo: false\n#| message: false\n\n# Code here\n```\n\n### Critical Options\n\n**For Figures:**\n- `label: fig-*` - Required for cross-refs\n- `fig-cap:` - Descriptive caption\n- `fig-width:`, `fig-height:` - Explicit dimensions\n\n**For Tables:**\n- `label: tbl-*` - Required for cross-refs\n- `tbl-cap:` - Descriptive caption\n\n**For Computations:**\n- `cache: true` - Cache expensive operations\n- `dependson:` - Specify dependencies\n\n## Document Structure Templates\n\n### Research Paper\n```markdown\n# Introduction {#sec-intro}\n\n# Literature Review {#sec-literature}\n\n# Data {#sec-data}\n\n# Methods {#sec-methods}\n\n# Results {#sec-results}\n\n# Discussion {#sec-discussion}\n\n# Conclusion {#sec-conclusion}\n\n# References {.unnumbered}\n\n::: {#refs}\n:::\n\n# Appendix {.unnumbered #sec-appendix}\n```\n\n### Technical Report\n```markdown\n# Setup {#sec-setup}\n\n# Data Loading {#sec-data}\n\n# Exploratory Analysis {#sec-eda}\n\n# Statistical Modeling {#sec-modeling}\n\n# Conclusions {#sec-conclusions}\n```\n\n## Reproducibility Documentation\n\n### Session Information\n```{r}\n#| label: session-info\n#| echo: false\n\nsessionInfo()\n# Or more detailed:\n# sessioninfo::session_info()\n```\n\n### Computational Environment\n```markdown\n# Appendix: Computational Details {.unnumbered}\n\nAll analyses conducted in R version `r R.version.string`.\n```\n\n## Presentations\n\n### Reveal.js (HTML)\n```markdown\n## Slide Title\n\nContent goes here\n\n::: {.incremental}\n- Appears first\n- Appears second\n:::\n\n## Columns\n\n:::: {.columns}\n::: {.column width=\"50%\"}\nLeft content\n:::\n\n::: {.column width=\"50%\"}\nRight content\n:::\n::::\n```\n\n### Beamer (PDF)\n```yaml\n---\ntitle: \"Presentation\"\nformat:\n  beamer:\n    theme: Madrid\n    colortheme: default\n---\n```\n\n## Resources & Advanced Topics\n\n### Reference Materials\n\nComprehensive coverage in main document above covers:\n- YAML configuration patterns\n- Cross-referencing systems\n- Bibliography management\n- Code chunk options\n- Caching strategies\n- Mathematical typesetting\n- Document structure templates\n\n### Related Skills\n\n- **r/anti-slop** - For cleaning R code in chunks\n- **python/anti-slop** - For cleaning Python code in chunks\n- **text/anti-slop** - For cleaning prose sections\n\n### Tools\n\n- `quarto render` - Render document\n- `quarto preview` - Live preview\n- `quarto check` - Verify installation\n\n## Integration with Posit Skills\n\nThis skill focuses on **preventing generic AI-generated documents** through reproducibility standards.\n\nUse together with Posit skills for complete coverage:\n\n| Task | Use This Skill | + Posit Skill |\n|------|----------------|---------------|\n| Write new Quarto doc | quarto/anti-slop (quality) | + quarto/authoring (syntax) |\n| Convert from Rmd | quarto/anti-slop (standards) | + quarto/authoring (migration) |\n| Cross-reference figures | quarto/anti-slop (enforce) | + quarto/authoring (syntax) |\n| Create presentation | quarto/anti-slop (quality) | + quarto/authoring (features) |\n\n**Key distinction**:\n- **Posit skills** teach Quarto syntax and features\n- **This skill** enforces quality and prevents generic patterns\n\nBoth are complementary equals for creating professional reproducible documents.\n",
        "r/anti-slop/SKILL.md": "---\nname: r-anti-slop\ndescription: >\n  Enforce production-quality R code standards. Prevents generic AI patterns\n  through namespace qualification, explicit returns, and tidyverse conventions.\n  Use when writing or reviewing R code for data analysis or packages.\napplies_to:\n  - \"**/*.R\"\n  - \"**/*.Rmd\"\n  - \"**/*.qmd\"\ntags: [r, tidyverse, code-quality, data-science]\nrelated_skills:\n  - quarto/anti-slop\n  - text/anti-slop\nversion: 2.0.0\n---\n\n# R Anti-Slop: Stop Writing `df <- data`\n\n## When to Use This\n\nUse this for:\n- ✓ Any R code leaving your machine (analysis, packages, scripts)\n- ✓ AI-generated code review (catches `df`, `result`, missing `::`)\n- ✓ CRAN submissions (they'll reject generic code anyway)\n- ✓ Team code standards\n\nSkip for:\n- Quick console experiments (though habits form fast)\n- Legacy code you can't touch\n- Bioconductor or other style guides that override this\n\n## Quick Example\n\n**Before (AI Slop)**:\n```r\n# Load the library\nlibrary(dplyr)\n\n# Read the data\ndf <- read.csv(\"data.csv\")\n\n# Filter the data\nresult <- df %>% filter(x > 0)\n```\n\n**After (Anti-Slop)**:\n```r\ncustomer_data <- readr::read_csv(\"data/customers.csv\")\n\nactive_customers <- customer_data |>\n  dplyr::filter(status == \"active\", revenue > 0)\n\nreturn(active_customers)\n```\n\n**What changed**:\n- ✓ Descriptive names (`customer_data` not `df`)\n- ✓ Namespace qualification (`dplyr::`, `readr::`)\n- ✓ Native pipe (`|>` not `%>%`)\n- ✓ No obvious comments\n- ✓ Explicit return\n\n## When to Use What\n\n| If you need to... | Do this | Details |\n|-------------------|---------|---------|\n| Name variables | Use `snake_case`, no `df`/`data`/`result` | reference/naming.md |\n| Call tidyverse functions | Always use `::` (e.g., `dplyr::filter()`) | reference/tidyverse.md |\n| Return from function | Always explicit `return()` statement | reference/naming.md |\n| Write pipe chains | Use `\\|>`, break at 8+ operations | reference/tidyverse.md |\n| Document functions | Specific `@param`, `@return`, no circular text | reference/documentation.md |\n| Handle missing data | Explicit strategy + report data loss | reference/statistical-rigor.md |\n| Validate data | Check assumptions with `stopifnot()` | reference/statistical-rigor.md |\n| Format code | Use `styler::style_file()` | reference/tidyverse.md |\n| Check code quality | Use `lintr::lint()` | reference/tidyverse.md |\n\n## Core Workflow\n\n### 5-Step Quality Check\n\n1. **Namespace qualification** - All external functions use `::`\n   ```r\n   # Good\n   dplyr::filter(data, x > 0)\n   # Bad\n   filter(data, x > 0)\n   ```\n\n2. **Explicit returns** - Every function has `return()`\n   ```r\n   # Good\n   my_function <- function(x) {\n     result <- x + 1\n     return(result)\n   }\n   # Bad\n   my_function <- function(x) {\n     x + 1\n   }\n   ```\n\n3. **Naming conventions** - All objects use `snake_case`\n   ```r\n   # Good\n   customer_lifetime_value <- calculate_clv(data)\n   # Bad\n   df <- calculate_clv(data)\n   customerLifetimeValue <- calculate_clv(data)\n   ```\n\n4. **Documentation quality** - No generic descriptions\n   ```r\n   # Good\n   #' @param deaths Data frame with `age_group` and `count` columns\n   # Bad\n   #' @param data The data\n   ```\n\n5. **Code formatting** - Run styler and lintr\n   ```r\n   styler::style_file(\"script.R\")\n   lintr::lint(\"script.R\")\n   ```\n\n## Quick Reference Checklist\n\nBefore committing R code, verify:\n\n- [ ] All external functions qualified with `::`\n- [ ] All functions have explicit `return()`\n- [ ] All objects use `snake_case`\n- [ ] No generic names (`df`, `data`, `result`, `temp`)\n- [ ] Pipes (`|>`) have space before, end lines\n- [ ] Long pipelines (>8 ops) broken into named steps\n- [ ] Complex operations have WHY comments\n- [ ] Data validated after transformations\n- [ ] Seeds set before random operations\n- [ ] Uncertainty reported (SE, CI) for statistical models\n- [ ] No `attach()` calls\n- [ ] No right-hand assignment (`->`)\n- [ ] Roxygen documentation is specific\n- [ ] Examples are realistic and run\n\n## Common Workflows\n\n### Workflow 1: Clean Up AI-Generated R Script\n\n**Context**: AI generated an analysis script with generic patterns.\n\n**Steps**:\n\n1. **Run detection script**\n   ```bash\n   Rscript toolkit/scripts/detect_slop.R analysis.R --verbose\n   ```\n\n2. **Fix high-priority issues first**\n   ```r\n   # Replace df, data, result with descriptive names\n   # Before\n   df <- readr::read_csv(\"data.csv\")\n   result <- df %>% filter(x > 0)\n\n   # After\n   customer_data <- readr::read_csv(\"data/customers.csv\")\n   active_customers <- customer_data |> dplyr::filter(status == \"active\")\n   ```\n\n3. **Add namespace qualification**\n   ```r\n   # Before\n   data %>% filter(x > 0) %>% summarize(mean(y))\n\n   # After\n   data |>\n     dplyr::filter(x > 0) |>\n     dplyr::summarize(mean_y = mean(y))\n   ```\n\n4. **Add explicit returns**\n   ```r\n   # Before\n   calculate_rate <- function(numerator, denominator) {\n     numerator / denominator\n   }\n\n   # After\n   calculate_rate <- function(numerator, denominator) {\n     rate <- numerator / denominator\n     return(rate)\n   }\n   ```\n\n5. **Break long pipes**\n   ```r\n   # Before (12 operations in one chain)\n   result <- data |>\n     filter(...) |> mutate(...) |> group_by(...) |>\n     summarize(...) |> arrange(...) |> [7 more ops]\n\n   # After\n   clean_data <- data |>\n     dplyr::filter(!is.na(value)) |>\n     dplyr::mutate(category = categorize(value))\n\n   summary_stats <- clean_data |>\n     dplyr::group_by(category) |>\n     dplyr::summarize(mean_val = mean(value))\n   ```\n\n6. **Format and validate**\n   ```r\n   styler::style_file(\"analysis.R\")\n   lintr::lint(\"analysis.R\")\n   ```\n\n**Expected outcome**: Score drops from 60+ to <20\n\n---\n\n### Workflow 2: Fix Generic Package Documentation\n\n**Context**: R package has generic roxygen documentation.\n\n**Steps**:\n\n1. **Identify generic patterns**\n   ```r\n   # Bad\n   #' Process Data\n   #'\n   #' @description This function processes the data.\n   #' @param data The data.\n   #' @return The result.\n   ```\n\n2. **Make description specific**\n   ```r\n   # Good\n   #' Calculate age-adjusted mortality rates\n   #'\n   #' Computes mortality rates per 100,000 population, standardized to the\n   #' 2000 US Census age distribution using direct standardization.\n   ```\n\n3. **Describe parameter structure**\n   ```r\n   # Good\n   #' @param deaths Data frame with columns `age_group` and `count`.\n   #' @param population Data frame with columns `age_group` and `pop_size`.\n   ```\n\n4. **Specify return value**\n   ```r\n   # Good\n   #' @return A tibble with columns:\n   #'   \\describe{\n   #'     \\item{county}{County FIPS code}\n   #'     \\item{rate}{Age-adjusted rate per 100,000}\n   #'     \\item{se}{Standard error of the rate}\n   #'   }\n   ```\n\n5. **Add realistic examples**\n   ```r\n   # Good\n   #' @examples\n   #' counties <- data.frame(\n   #'   county = c(\"A\", \"B\"),\n   #'   deaths = c(150, 200),\n   #'   population = c(50000, 80000)\n   #' )\n   #'\n   #' adjust_rates(counties, rate_per = 100000)\n   #' #> # A tibble: 2 x 3\n   #' #>   county  rate    se\n   #' #> 1 A       312.  25.4\n   #' #> 2 B       258.  18.2\n   ```\n\n**Expected outcome**: Documentation that teaches, not restates\n\n---\n\n### Workflow 3: Prepare Package for CRAN\n\n**Context**: Final checks before CRAN submission.\n\n**Steps**:\n\n1. **Run all quality checks**\n   ```r\n   # Standard checks\n   devtools::check()\n\n   # Anti-slop checks\n   lapply(list.files(\"R\", full.names = TRUE), function(f) {\n     system(paste(\"Rscript toolkit/scripts/detect_slop.R\", f))\n   })\n   ```\n\n2. **Fix documentation**\n   - Check all `@param` descriptions are specific\n   - Verify `@examples` run and are realistic\n   - Ensure `@return` describes structure\n\n3. **Validate code quality**\n   ```r\n   # Format all files\n   styler::style_dir(\"R/\")\n\n   # Check lints\n   lintr::lint_package()\n   ```\n\n4. **Check CRAN-specific requirements**\n   - Validate URLs in DESCRIPTION and documentation\n   - Check examples run in < 5 seconds\n   - Verify package structure meets CRAN standards\n\n**Expected outcome**: Clean `R CMD check` with no slop patterns\n\n## Mandatory Rules Summary\n\n### 1. Namespace Qualification\n**ALWAYS use `::` for external packages**\n\nExceptions (don't need `::`):\n- Base R: `mean()`, `sum()`, `log()`, etc.\n- stats: `lm()`, `glm()`, `t.test()`, etc.\n- utils: `head()`, `tail()`, `str()`, etc.\n\n### 2. Explicit Returns\n**ALWAYS use `return()` - never implicit**\n\n### 3. Naming: snake_case\n**All objects use `snake_case`**\n- Variables: `customer_data` not `customerData` or `df`\n- Functions: `calculate_rate` not `calculateRate`\n- Arguments: `input_data` not `inputData`\n\n### 4. Native Pipe\n**Prefer `|>` over `%>%`** (unless R < 4.1)\n\n### 5. No Generic Names\n**Never use**: `df`, `data`, `result`, `temp`, `x`, `n` (except standard math notation)\n\n## Tidyverse Philosophy\n\nFollow [Tidyverse Style Guide](https://style.tidyverse.org/) as primary reference:\n\n1. **Design for humans** - Code should be readable and intuitive\n2. **Reuse existing data structures** - Work with tibbles and data frames\n3. **Compose simple functions with pipes** - Build complexity through composition\n4. **Embrace functional programming** - Functions are first-class objects\n\nSee **reference/tidyverse.md** for complete tidyverse conventions.\n\n## Resources & Advanced Topics\n\n### Reference Files\n\n- **[reference/naming.md](reference/naming.md)** - Complete naming conventions and forbidden patterns\n- **[reference/tidyverse.md](reference/tidyverse.md)** - Pipe conventions, formatting, ggplot2 standards\n- **[reference/documentation.md](reference/documentation.md)** - Roxygen2, vignettes, README quality\n- **[reference/statistical-rigor.md](reference/statistical-rigor.md)** - Validation, uncertainty, reproducibility\n- **[reference/forbidden-patterns.md](reference/forbidden-patterns.md)** - Complete antipattern catalog\n\n### Related Skills\n\n- **text/anti-slop** - For cleaning prose in documentation\n- **quarto/anti-slop** - For cleaning vignettes and documentation\n\n### Tools\n\n- `styler::style_file()` - Auto-format code\n- `lintr::lint()` - Check code quality\n- `Rscript toolkit/scripts/detect_slop.R` - Detect AI patterns\n\n## Integration with Posit Skills\n\nThis skill focuses on **code quality and avoiding generic patterns**.\n\nUse together with Posit skills for complete coverage:\n\n| Task | Use This Skill | + Posit Skill |\n|------|----------------|---------------|\n| Write error messages | r/anti-slop (quality) | + r-lib/cli (structure) |\n| Write tests | r/anti-slop (code quality) | + r-lib/testing (test patterns) |\n| Prepare for CRAN | r/anti-slop (no slop) | + r-lib/cran-extrachecks (requirements) |\n| Document lifecycle | r/anti-slop (doc quality) | + r-lib/lifecycle (deprecation) |\n",
        "text/anti-slop/SKILL.md": "---\nname: text-anti-slop\ndescription: >\n  Comprehensive text quality enforcement. Coordinates three complementary systems:\n  (1) removes AI slop patterns (transitions, buzzwords, meta-commentary),\n  (2) applies Strunk's writing principles (active voice, concrete language, brevity),\n  (3) adds authentic human voice (Wikipedia's 24 AI patterns + personality).\n  Use for any prose humans will read—documentation, READMEs, blog posts, technical content.\napplies_to:\n  - \"**/*.md\"\n  - \"**/*.txt\"\n  - \"**/*.rst\"\ntags: [writing, documentation, technical-writing, clarity, quality]\nrelated_skills:\n  - external/humanizer\n  - external/elements-of-style\n  - r/anti-slop\n  - python/anti-slop\n  - quarto/anti-slop\nversion: 3.0.0\n---\n\n# Text Anti-Slop: Comprehensive Writing Quality Coordinator\n\n## What This Skill Does\n\nThis skill **coordinates three complementary writing quality systems** in a single pass:\n\n1. **Pattern Removal** (text/anti-slop core) → Remove transitions, buzzwords, meta-commentary, filler\n2. **Strunk's Principles** (elements-of-style) → Active voice, positive form, concrete language, brevity\n3. **Human Voice** (humanizer) → Remove Wikipedia's 24 AI patterns, add personality\n\n**One invocation = complete text quality enforcement.**\n\n## When to Use This\n\nUse this skill when writing or reviewing:\n- ✓ Technical docs, READMEs, guides, tutorials\n- ✓ AI-generated documentation (before anyone sees it)\n- ✓ Blog posts, articles, technical explanations\n- ✓ Package documentation (roxygen, docstrings)\n- ✓ Commit messages, PR descriptions\n- ✓ Anything going to users or teammates\n\nSkip this for:\n- Creative fiction (different rules)\n- Legal documents (specific phrasings required)\n- Marketing copy (persuasion has its own game)\n- Style guide compliance (AP, Chicago override this)\n\n## Quick Example\n\n**Before (AI Slop)**:\n> In this document, we will delve into the complexities of API design. It's important to note that in today's fast-paced landscape, leveraging best practices is a testament to your commitment to quality. We'll navigate through various approaches—REST, GraphQL, and gRPC—ultimately providing you with actionable insights to empower your development workflow. Additionally, the ecosystem boasts vibrant community support.\n\n**After (Anti-Slop)**:\n> This guide covers three API design patterns: REST, GraphQL, and gRPC. Each section shows code examples and trade-offs. I'll be honest—I keep coming back to REST for most projects. It's boring, but that's the point.\n\n**What changed**:\n1. **Pattern Removal**: Deleted \"delve into\", \"navigate\", \"leverage\", \"empower\", meta-commentary\n2. **Strunk's Principles**: Active voice (\"guide covers\" not \"will be provided\"), concrete language (specific patterns not \"approaches\"), omit needless words\n3. **Human Voice**: Added first person (\"I'll be honest\"), opinion, personality, removed \"boasts vibrant\"\n\n## Three-Layer System\n\n### Layer 1: Pattern Removal (Core Anti-Slop)\n\n**Removes immediately**:\n- **Transitions**: \"delve into\", \"navigate complexities\", \"in today's landscape\"\n- **Buzzwords**: \"leverage\", \"empower\", \"unlock\", \"synergy\", \"paradigm shift\"\n- **Meta-commentary**: \"In this section...\", \"Let's explore...\", \"As we'll see...\"\n- **Filler**: \"in order to\" → \"to\", \"due to the fact that\" → \"because\"\n\n**Detection script**: `python3 toolkit/scripts/detect_slop.py file.md --verbose`\n\n### Layer 2: Strunk's Principles (Elements of Style)\n\n**Applies 18 rules** (from external/elements-of-style), especially:\n\n- **Rule 10: Active voice**\n  - Bad: \"The function was implemented by the team\"\n  - Good: \"The team implemented the function\"\n\n- **Rule 11: Positive form**\n  - Bad: \"The API is not very reliable\"\n  - Good: \"The API fails frequently\"\n\n- **Rule 12: Concrete language**\n  - Bad: \"The system provides various capabilities\"\n  - Good: \"The system validates emails, stores users, and sends alerts\"\n\n- **Rule 13: Omit needless words**\n  - Bad: \"In order to utilize the functionality\"\n  - Good: \"To use this feature\"\n\n**Source**: external/elements-of-style/skills/writing-clearly-and-concisely/elements-of-style.md\n\n### Layer 3: Human Voice (Humanizer)\n\n**Removes 24+ Wikipedia AI patterns** (from external/humanizer), including:\n\n- **Inflated symbolism**: \"stands as a testament\", \"marks a pivotal moment\"\n- **Promotional language**: \"boasts\", \"vibrant\", \"nestled\", \"stunning\"\n- **-ing analyses**: \"highlighting its importance, showcasing the benefits\"\n- **AI vocabulary**: \"delve\", \"landscape\" (abstract), \"tapestry\", \"intricate\"\n- **Copula avoidance**: \"serves as\" → \"is\"\n- **Rule of three**: Forcing everything into groups of 3\n\n**Adds personality**:\n- First person when appropriate (\"I keep thinking about...\")\n- Opinions (\"This is clever but feels fragile\")\n- Varied rhythm (short punchy. Then longer flowing sentences.)\n- Acknowledgment of uncertainty (\"I genuinely don't know\")\n\n**Source**: external/humanizer/SKILL.md\n\n## When to Use What\n\n| If you see... | Remove/Replace | Layer | Details |\n|---------------|----------------|-------|---------|\n| \"delve into\" | \"examine\" or delete | 1 + 3 | Core pattern + AI vocab |\n| \"leverage\" | \"use\" | 1 | Buzzword |\n| \"in order to\" | \"to\" | 1 + 2 | Filler + Rule 13 |\n| Passive voice | Active voice | 2 | Rule 10 |\n| \"not reliable\" | \"unreliable\" or \"fails\" | 2 | Rule 11 (positive form) |\n| \"various things\" | Specific list | 2 | Rule 12 (concrete) |\n| \"stands as a testament\" | Delete or be specific | 3 | Inflated symbolism |\n| \"boasts 500 features\" | \"has 500 features\" | 3 | Promotional + copula |\n| Every sentence same length | Vary rhythm | 3 | Add personality |\n\n## Core Workflow\n\n### Comprehensive 4-Step Process\n\n**Step 1: Pattern Detection & Removal**\n```markdown\n# Run detection first\npython3 toolkit/scripts/detect_slop.py README.md --verbose\n\n# Before (score: 65 - high slop)\nIn this document, we will delve into the complexities of configuration\nmanagement. It's important to note that in today's fast-paced world,\nleveraging automated solutions is crucial for success.\n\n# After (Layer 1 applied)\nThis guide covers configuration management. Automated solutions\nreduce manual errors.\n```\n\n**Step 2: Apply Strunk's Principles**\n```markdown\n# Before (passive, vague)\nConfiguration files are stored by the system in /etc/config.\nVarious validation checks are performed.\n\n# After (Layer 2: Rules 10, 12)\nThe system stores configuration files in /etc/config.\nThe validator checks syntax, required fields, and data types.\n```\n\n**Step 3: Add Human Voice**\n```markdown\n# Before (soulless but clean)\nThe system performs validation. Some users report issues.\nThe implications remain unclear.\n\n# After (Layer 3: personality + opinion)\nThe validator catches 90% of errors—but I keep seeing users\ntrip over the same edge case with nested arrays. Not sure if\nthat's a docs problem or a design problem. Probably both.\n```\n\n**Step 4: Verify & Iterate**\n```bash\n# Check final score\npython3 toolkit/scripts/detect_slop.py README.md --verbose\n\n# Target: < 20 (low slop)\n# If > 20, review flagged patterns and refine\n```\n\n## Quick Reference Checklist\n\nBefore publishing, verify all three layers:\n\n### Layer 1: Pattern Removal\n- [ ] No meta-commentary (\"In this guide...\", \"Let's explore...\")\n- [ ] No high-risk transitions (\"delve into\", \"navigate complexities\")\n- [ ] No corporate buzzwords (\"leverage\", \"synergistic\", \"paradigm shift\")\n- [ ] No wordy constructions (\"in order to\" → \"to\")\n\n### Layer 2: Strunk's Principles\n- [ ] Active voice (Rule 10)\n- [ ] Positive statements (Rule 11: \"fails\" not \"not reliable\")\n- [ ] Concrete language (Rule 12: specifics not \"various things\")\n- [ ] No needless words (Rule 13: ruthlessly cut)\n- [ ] Related words together (Rule 16)\n\n### Layer 3: Human Voice\n- [ ] No inflated symbolism (\"testament\", \"pivotal moment\")\n- [ ] No promotional language (\"boasts\", \"vibrant\", \"stunning\")\n- [ ] No AI vocabulary overuse (\"delve\", \"landscape\", \"tapestry\")\n- [ ] No copula avoidance (\"is\" not \"serves as\")\n- [ ] Varied sentence rhythm\n- [ ] Personality present (first person, opinions, mixed feelings)\n\n## Common Workflows\n\n### Workflow 1: Clean AI-Generated Documentation\n\n**Context**: AI generated a README with heavy slop patterns.\n\n**Apply all three layers**:\n\n```markdown\n# Before (score: 72 - severe slop)\nIn today's fast-paced landscape of software development, configuration\nmanagement stands as a testament to the importance of automation.\nThis comprehensive guide will delve into the intricacies of our\ncutting-edge solution, which boasts vibrant features and serves as\nan essential tool for developers. Leveraging advanced techniques,\nit enables teams to unlock their full potential.\n\n# After Layer 1 (pattern removal)\nThis guide covers configuration management automation. Our tool\nincludes multiple features for development teams.\n\n# After Layer 2 (Strunk's principles)\nThis tool automates configuration management. It validates syntax,\ntracks changes, and rolls back errors.\n\n# After Layer 3 (add voice)\nThis tool automates config management—validates syntax, tracks\nchanges, rolls back errors. I've been using it for six months.\nThe rollback feature saved me twice. The syntax validator? Still\nlearning its edge cases.\n\n# Final score: 12 (low slop)\n```\n\n---\n\n### Workflow 2: Technical Blog Post\n\n**Context**: Writing a blog post about API design.\n\n**Progressive refinement**:\n\n```markdown\n# Draft (needs all three layers)\nIn this article, we will delve into the complexities of API design\npatterns. Additionally, it's important to note that REST, GraphQL,\nand gRPC each serve as viable approaches, showcasing the vibrant\nlandscape of modern development. Ultimately, the choice was made\nto leverage REST due to its simplicity.\n\n# After Layer 1 (remove slop)\nThis article covers API design patterns. REST, GraphQL, and gRPC\nare approaches in modern development. We chose REST for its\nsimplicity.\n\n# After Layer 2 (Strunk's rules)\nThis post compares three API patterns: REST, GraphQL, and gRPC.\nWe chose REST because it's simple.\n\n# After Layer 3 (add personality)\nI keep coming back to REST. Everyone gets excited about GraphQL—\nunderfetching! type safety!—and they're not wrong. But for this\nproject? REST just works. Sometimes boring is the right answer.\n```\n\n---\n\n### Workflow 3: Package Documentation\n\n**Context**: Writing roxygen2 docs for an R package function.\n\n**Apply focused layers**:\n\n```markdown\n# Before (AI-generated roxygen)\n#' Process Data Function\n#'\n#' This function is designed to process data in a comprehensive manner.\n#' It serves as an essential tool for data manipulation, enabling users\n#' to leverage advanced techniques. The function boasts robust error\n#' handling capabilities.\n#'\n#' @param data A data frame that contains the data to be processed\n#' @return A processed data frame will be returned\n#' @export\n\n# After all three layers\n#' Validate and normalize customer records\n#'\n#' Checks required fields (name, email, phone), normalizes phone\n#' numbers to E.164 format, and flags duplicates by email.\n#'\n#' @param data Data frame with columns: name, email, phone\n#' @return Data frame with validated records, invalid rows removed\n#' @export\n```\n\n**What improved**:\n- Layer 1: Removed \"comprehensive manner\", \"enabling users\", \"boasts\"\n- Layer 2: Active voice (\"Checks\" not \"will be returned\"), concrete (\"E.164 format\" not \"advanced techniques\")\n- Layer 3: Removed copula avoidance (\"is designed to\"), specific instead of promotional\n\n---\n\n## Mandatory Rules Summary\n\n### Layer 1: Pattern Removal\n\n**1. No Meta-Commentary**\n- Bad: \"In this section...\" \"Let's explore...\" \"As we'll see...\"\n- Good: Just say the thing.\n\n**2. Lead with the Point**\n- Bad: Preamble → context → point\n- Good: Point → details\n\n**3. Simple Not Wordy**\n- \"in order to\" → \"to\"\n- \"due to the fact that\" → \"because\"\n- \"has the ability to\" → \"can\"\n\n### Layer 2: Strunk's Principles (Most Critical)\n\n**Rule 10: Active Voice**\n- Bad: \"The bug was fixed by the team\"\n- Good: \"The team fixed the bug\"\n\n**Rule 11: Positive Form**\n- Bad: \"not uncommon\", \"not very reliable\"\n- Good: \"common\", \"unreliable\" or \"fails frequently\"\n\n**Rule 12: Concrete Language**\n- Bad: \"various capabilities\", \"multiple features\"\n- Good: List specific capabilities\n\n**Rule 13: Omit Needless Words**\n- Ruthlessly cut. Every word must earn its place.\n\n### Layer 3: Human Voice\n\n**1. Remove AI Signatures**\n- \"stands as a testament\", \"marks a pivotal moment\" → delete or be specific\n- \"boasts\", \"vibrant\", \"nestled\" → neutral language\n- \"serves as\", \"stands as\" → \"is\"\n\n**2. Add Personality**\n- First person when appropriate\n- Opinions and reactions\n- Varied rhythm\n- Acknowledge uncertainty\n\n## Resources & Reference Files\n\n### Detection & Cleanup Scripts\n\n```bash\n# Detect slop patterns (0-100 score)\npython3 toolkit/scripts/detect_slop.py README.md --verbose\n\n# Automated cleanup (with backup)\npython3 toolkit/scripts/clean_slop.py README.md --save\npython3 toolkit/scripts/clean_slop.py README.md --save --aggressive\n```\n\n### External References\n\nThis skill coordinates these external resources:\n\n1. **external/humanizer/SKILL.md** (~2,500 tokens)\n   - Wikipedia's 24 AI writing patterns\n   - Personality and voice guidance\n   - Before/after examples\n\n2. **external/elements-of-style/skills/writing-clearly-and-concisely/elements-of-style.md** (~1,400 tokens)\n   - Strunk's 18 writing rules\n   - Grammar and composition principles\n   - Technical writing examples\n\n3. **Text/anti-slop core patterns** (this file)\n   - Transitions catalog\n   - Buzzword detection\n   - Meta-commentary patterns\n   - Filler phrase replacements\n\n### Reference Files (Planned)\n\n- **reference/transitions.md** - Overused transition phrases to avoid\n- **reference/buzzwords.md** - Corporate jargon catalog and replacements\n- **reference/filler.md** - Wordy constructions and alternatives\n- **reference/meta-commentary.md** - Self-referential patterns to delete\n- **reference/structure.md** - Document organization principles\n\n## Scoring Guide\n\nDetection scripts output scores (0-100, higher is worse):\n\n| Score | Meaning | Action |\n|-------|---------|--------|\n| 0-20 | Low slop (authentic) | Minor tweaks |\n| 20-40 | Moderate (some patterns) | Review flagged items |\n| 40-60 | High (many patterns) | Significant cleanup |\n| 60+ | Severe (heavily generic) | Apply all three layers |\n\n**Target**: < 20 for all published content\n\n## Context Awareness\n\nNot all patterns are always slop. Consider:\n\n- **Audience**: Academic writing may need more hedging than blog posts\n- **Purpose**: Legal docs need specific phrases; marketing needs energy\n- **Length**: Longer pieces need some transitions (but not \"delve\")\n- **Domain**: Technical docs have different norms than creative writing\n- **Brand**: If your brand voice uses first person, lean into it\n\nThe issue is **overuse** and **unconscious AI-generated repetition**, not pattern existence.\n\n## How This Differs from Individual Skills\n\n### Using Skills Separately (Old Workflow)\n```\n1. Invoke text/anti-slop (patterns only)\n2. Invoke elements-of-style (Strunk's rules)\n3. Invoke humanizer (voice and personality)\n→ Three separate invocations, easy to skip steps\n```\n\n### Using This Coordinator (New Workflow)\n```\n1. Invoke text/anti-slop (applies all three automatically)\n→ One invocation, comprehensive coverage\n```\n\n**Benefits**:\n- Consistent application across all writing\n- Can't accidentally skip layers\n- Patterns reinforce each other (e.g., Rule 13 + filler removal)\n- Single quality score\n\n**Trade-off**:\n- Less granular control if you only want one layer\n- (If you need that, invoke external skills directly)\n\n## Integration with Code Quality Skills\n\nFor domain-specific documentation:\n\n| Content Type | Use This Skill + | Also Apply |\n|--------------|------------------|------------|\n| R package docs | text/anti-slop (all 3 layers) | r/anti-slop (namespace, returns) |\n| Python docstrings | text/anti-slop (all 3 layers) | python/anti-slop (type hints, PEP 8) |\n| Quarto documents | text/anti-slop (all 3 layers) | quarto/anti-slop (reproducibility) |\n| General markdown | text/anti-slop (all 3 layers) | (sufficient alone) |\n\n**Workflow example** (R package documentation):\n1. Write roxygen2 docs\n2. Apply text/anti-slop (removes patterns, applies Strunk, adds voice)\n3. Apply r/anti-slop (enforces R-specific quality: `::`, explicit `return()`)\n4. Run detection: `Rscript toolkit/scripts/detect_slop.R R/`\n5. Target score < 20\n\n## Version History\n\n- **v3.0.0** (2026-01): Restructured as comprehensive coordinator, integrates humanizer + elements-of-style\n- **v2.0.0** (2025-12): Progressive disclosure pattern, decision tables\n- **v1.0.0**: Original text/anti-slop (patterns only)\n\n## Bottom Line\n\n**Writing for humans?** This skill applies three complementary systems automatically:\n1. Removes AI slop patterns\n2. Applies Strunk's timeless principles\n3. Adds authentic human voice\n\nOne invocation. Complete coverage. Professional results.\n",
        "toolkit/SKILL.md": "---\nname: anti-slop-toolkit\ndescription: >\n  Automated detection and cleanup tools for AI slop patterns. Provides Python\n  and R scripts for active slop detection, scoring, and cleanup across codebases.\n  Use when you need automated quality enforcement.\napplies_to:\n  - \"**/*.py\"\n  - \"**/*.R\"\n  - \"**/*.md\"\n  - \"**/*.txt\"\ntags: [automation, quality-assurance, ci-cd, linting, code-review]\nrelated_skills:\n  - r/anti-slop\n  - python/anti-slop\n  - text/anti-slop\n  - anti-slop\nversion: 2.0.0\n---\n\n# Anti-Slop Toolkit\n\n## When to Use This Skill\n\nUse anti-slop-toolkit when:\n- ✓ Running automated slop detection across a codebase\n- ✓ Integrating slop checks into CI/CD pipelines\n- ✓ Setting up pre-commit hooks for quality enforcement\n- ✓ Auditing large projects for generic AI patterns\n- ✓ Batch processing multiple files for cleanup\n- ✓ Establishing quantitative quality thresholds\n\nDo NOT use when:\n- Manual review is more appropriate (use domain-specific anti-slop skills)\n- Dealing with non-text files (images, binaries)\n- Working with languages not covered by scripts (use manual skills)\n\n## Quick Example\n\n**Running Detection**:\n```bash\n# Text file detection\n$ python3 scripts/detect_slop.py report.md --verbose\n\nAnalyzing: report.md\nOverall Slop Score: 45/100\n\nHigh-Risk Phrases (3 found):\n  Line 12: \"delve into the complexities\"\n  Line 34: \"in today's fast-paced world\"\n  Line 67: \"it's important to note that\"\n\nRecommendations:\n- Remove meta-commentary\n- Simplify wordy phrases\n- Replace buzzwords with direct language\n\n# R code detection\n$ Rscript scripts/detect_slop.R analysis.R --verbose\n\nAnalyzing: analysis.R\nOverall Slop Score: 52/100\n\nGeneric Variables (5 found):\n  Line 8: df\n  Line 15: data\n  Line 23: result\n\nRecommendations:\n- Replace generic variable names\n- Add namespace qualification\n- Remove obvious comments\n```\n\n**Running Cleanup**:\n```bash\n# Preview changes\n$ python3 scripts/clean_slop.py report.md\n\nWould remove:\n  Line 12: \"delve into the complexities\" → \"examine\"\n  Line 34: \"in today's fast-paced world\" → [deleted]\n\n# Apply changes (creates backup)\n$ python3 scripts/clean_slop.py report.md --save\n\nBackup created: report.md.backup\nCleaned: report.md\nRemoved 15 slop patterns\n```\n\n## When to Use What\n\n| If you need to... | Use this tool | Options |\n|-------------------|---------------|---------|\n| Detect text slop | `detect_slop.py <file>` | `--verbose` for details |\n| Clean text slop | `clean_slop.py <file> --save` | `--aggressive` for more changes |\n| Detect R code slop | `detect_slop.R <file>` | Works on files or directories |\n| Audit entire codebase | workflow-1-audit-codebase | Batch processing |\n| Set up CI/CD checks | workflow-2-ci-cd-integration | Exit codes for pass/fail |\n| Create pre-commit hook | workflow-3-pre-commit-hook | Automatic enforcement |\n\n## Core Workflow\n\n### 3-Step Automated Quality Check\n\n1. **Run detection to assess scope**\n   ```bash\n   # Single file\n   python3 scripts/detect_slop.py document.md --verbose\n\n   # Multiple files\n   find . -name \"*.md\" -exec python3 scripts/detect_slop.py {} \\;\n   ```\n\n2. **Review findings and set thresholds**\n   ```bash\n   # Acceptable: score < 30\n   # Needs work: score 30-50\n   # Must fix: score > 50\n   ```\n\n3. **Apply automated cleanup where safe**\n   ```bash\n   # Conservative cleanup (preserves meaning)\n   python3 scripts/clean_slop.py document.md --save\n\n   # Aggressive cleanup (may change nuance)\n   python3 scripts/clean_slop.py document.md --save --aggressive\n   ```\n\n## Quick Reference Checklist\n\nFor CI/CD integration:\n\n- [ ] Scripts installed in project (copy from toolkit/scripts/)\n- [ ] Detection thresholds defined (e.g., fail if score > 40)\n- [ ] File types to check specified\n- [ ] Exceptions documented (files that can't be cleaned)\n- [ ] Backup strategy for cleanup operations\n- [ ] Exit codes configured for pipeline failures\n\nFor manual use:\n\n- [ ] Run detection first (never blind cleanup)\n- [ ] Review verbose output for context\n- [ ] Test cleanup on copy before applying\n- [ ] Verify cleaned output maintains meaning\n- [ ] Commit cleaned files separately for easy review\n\n## Common Workflows\n\n### Workflow 1: Audit Entire Codebase\n\n**Context**: Review project for generic AI patterns before release.\n\n**Steps**:\n\n1. **Run comprehensive detection**\n   ```bash\n   #!/bin/bash\n   # audit_slop.sh\n\n   echo \"=== Text Files ===\"\n   find . -name \"*.md\" -o -name \"*.txt\" | while read file; do\n     score=$(python3 scripts/detect_slop.py \"$file\" | grep \"Score:\" | awk '{print $3}')\n     echo \"$file: $score\"\n   done\n\n   echo \"\"\n   echo \"=== R Files ===\"\n   find . -name \"*.R\" | while read file; do\n     score=$(Rscript scripts/detect_slop.R \"$file\" | grep \"Score:\" | awk '{print $3}')\n     echo \"$file: $score\"\n   done\n   ```\n\n2. **Generate summary report**\n   ```bash\n   # Count files by severity\n   echo \"Files by severity:\"\n   echo \"Low (0-20): $(grep -c \"1[0-9]/100\\|[0-9]/100\" audit.log)\"\n   echo \"Moderate (20-40): $(grep -c \"[23][0-9]/100\" audit.log)\"\n   echo \"High (40-60): $(grep -c \"[45][0-9]/100\" audit.log)\"\n   echo \"Severe (60+): $(grep -c \"6[0-9]/100\\|7[0-9]/100\\|8[0-9]/100\\|9[0-9]/100\" audit.log)\"\n   ```\n\n3. **Prioritize high-severity files**\n   ```bash\n   # List files with score > 50\n   grep -E \"[5-9][0-9]/100\" audit.log | sort -t: -k2 -rn > high_priority.txt\n   ```\n\n4. **Apply cleanup to high-priority files**\n   ```bash\n   # Clean files with score > 50\n   while read line; do\n     file=$(echo $line | cut -d: -f1)\n     python3 scripts/clean_slop.py \"$file\" --save\n   done < high_priority.txt\n   ```\n\n5. **Verify improvements**\n   ```bash\n   # Re-run detection on cleaned files\n   while read line; do\n     file=$(echo $line | cut -d: -f1)\n     python3 scripts/detect_slop.py \"$file\"\n   done < high_priority.txt\n   ```\n\n**Expected outcome**: Prioritized cleanup plan with before/after scores\n\n---\n\n### Workflow 2: CI/CD Pipeline Integration\n\n**Context**: Enforce quality standards in continuous integration.\n\n**Steps**:\n\n1. **Create detection script for CI**\n   ```bash\n   #!/bin/bash\n   # ci_check_slop.sh\n\n   THRESHOLD=40\n   FAILED=0\n\n   # Check markdown files\n   for file in $(find . -name \"*.md\"); do\n     score=$(python3 scripts/detect_slop.py \"$file\" | grep -oP \"Score: \\K\\d+\")\n     if [ \"$score\" -gt \"$THRESHOLD\" ]; then\n       echo \"FAIL: $file (score: $score, threshold: $THRESHOLD)\"\n       FAILED=1\n     else\n       echo \"PASS: $file (score: $score)\"\n     fi\n   done\n\n   # Check R files\n   for file in $(find . -name \"*.R\"); do\n     score=$(Rscript scripts/detect_slop.R \"$file\" | grep -oP \"Score: \\K\\d+\")\n     if [ \"$score\" -gt \"$THRESHOLD\" ]; then\n       echo \"FAIL: $file (score: $score, threshold: $THRESHOLD)\"\n       FAILED=1\n     fi\n   done\n\n   exit $FAILED\n   ```\n\n2. **Add to GitHub Actions**\n   ```yaml\n   # .github/workflows/quality-check.yml\n   name: Quality Check\n\n   on: [pull_request]\n\n   jobs:\n     anti-slop:\n       runs-on: ubuntu-latest\n       steps:\n         - uses: actions/checkout@v3\n         - name: Set up Python\n           uses: actions/setup-python@v4\n           with:\n             python-version: '3.x'\n         - name: Set up R\n           uses: r-lib/actions/setup-r@v2\n         - name: Check for slop\n           run: |\n             chmod +x ci_check_slop.sh\n             ./ci_check_slop.sh\n   ```\n\n3. **Configure for GitLab CI**\n   ```yaml\n   # .gitlab-ci.yml\n   quality_check:\n     stage: test\n     image: rocker/r-ver:latest\n     before_script:\n       - apt-get update && apt-get install -y python3\n     script:\n       - chmod +x ci_check_slop.sh\n       - ./ci_check_slop.sh\n     allow_failure: false\n   ```\n\n4. **Add quality gate**\n   ```bash\n   # Only fail on severe slop (score > 60)\n   THRESHOLD=60\n   # Warn on moderate slop (score 40-60) but don't fail\n   WARN_THRESHOLD=40\n   ```\n\n5. **Generate CI report**\n   ```bash\n   # Add to CI script\n   if [ \"$score\" -gt \"$THRESHOLD\" ]; then\n     echo \"::error file=$file::Slop score $score exceeds threshold $THRESHOLD\"\n   elif [ \"$score\" -gt \"$WARN_THRESHOLD\" ]; then\n     echo \"::warning file=$file::Slop score $score exceeds warning threshold\"\n   fi\n   ```\n\n**Expected outcome**: Automated quality enforcement in pull requests\n\n---\n\n### Workflow 3: Pre-commit Hook Setup\n\n**Context**: Prevent slop from entering repository with pre-commit hooks.\n\n**Steps**:\n\n1. **Create pre-commit hook**\n   ```bash\n   #!/bin/bash\n   # .git/hooks/pre-commit\n\n   # Get staged files\n   STAGED_MD=$(git diff --cached --name-only --diff-filter=ACM | grep \"\\.md$\")\n   STAGED_R=$(git diff --cached --name-only --diff-filter=ACM | grep \"\\.R$\")\n\n   THRESHOLD=50\n   ISSUES=0\n\n   # Check markdown files\n   for file in $STAGED_MD; do\n     if [ -f \"$file\" ]; then\n       score=$(python3 scripts/detect_slop.py \"$file\" | grep -oP \"Score: \\K\\d+\")\n       if [ \"$score\" -gt \"$THRESHOLD\" ]; then\n         echo \"ERROR: $file has slop score $score (threshold: $THRESHOLD)\"\n         ISSUES=1\n       fi\n     fi\n   done\n\n   # Check R files\n   for file in $STAGED_R; do\n     if [ -f \"$file\" ]; then\n       score=$(Rscript scripts/detect_slop.R \"$file\" | grep -oP \"Score: \\K\\d+\")\n       if [ \"$score\" -gt \"$THRESHOLD\" ]; then\n         echo \"ERROR: $file has slop score $score (threshold: $THRESHOLD)\"\n         ISSUES=1\n       fi\n     fi\n   done\n\n   if [ $ISSUES -ne 0 ]; then\n     echo \"\"\n     echo \"Commit rejected: Files exceed slop threshold\"\n     echo \"Run 'python3 scripts/clean_slop.py <file> --save' to clean\"\n     echo \"Or use 'git commit --no-verify' to bypass (not recommended)\"\n     exit 1\n   fi\n\n   exit 0\n   ```\n\n2. **Make hook executable**\n   ```bash\n   chmod +x .git/hooks/pre-commit\n   ```\n\n3. **Test hook**\n   ```bash\n   # Create test file with slop\n   echo \"It's important to note that we will delve into...\" > test.md\n   git add test.md\n   git commit -m \"Test\"\n   # Should fail with error message\n   ```\n\n4. **Add auto-fix option**\n   ```bash\n   # Add to hook after detecting issues\n   echo \"\"\n   echo \"Attempt auto-fix? (y/n)\"\n   read -r response\n   if [ \"$response\" = \"y\" ]; then\n     for file in $STAGED_MD; do\n       python3 scripts/clean_slop.py \"$file\" --save\n       git add \"$file\"\n     done\n     echo \"Files cleaned and re-staged\"\n     exit 0\n   fi\n   ```\n\n5. **Share hook with team**\n   ```bash\n   # Create shared hooks directory\n   mkdir -p .githooks\n   cp .git/hooks/pre-commit .githooks/\n\n   # Configure git to use shared hooks\n   git config core.hooksPath .githooks\n\n   # Document in README\n   echo \"Run: git config core.hooksPath .githooks\" >> README.md\n   ```\n\n**Expected outcome**: Automatic slop prevention before commits\n\n## Script Reference\n\n### detect_slop.py\n\n**Purpose**: Analyze text files for AI slop patterns.\n\n**Usage**:\n```bash\npython scripts/detect_slop.py <file> [--verbose]\n```\n\n**What it detects**:\n- High-risk phrases (delve, navigate complexities, fast-paced world)\n- Wordy constructions (in order to, due to the fact that)\n- Meta-commentary (it's important to note)\n- Buzzwords (leverage, synergistic, paradigm)\n- Excessive hedging (might possibly, could potentially)\n\n**Output**:\n```\nOverall Slop Score: 45/100\n\nHigh-Risk Phrases (3):\n  Line 12: \"delve into\"\n  Line 34: \"in today's world\"\n\nRecommendations:\n- Remove meta-commentary\n- Simplify wordy phrases\n```\n\n**Scoring**:\n- 0-20: Low slop (authentic)\n- 20-40: Moderate slop (some patterns)\n- 40-60: High slop (many patterns)\n- 60+: Severe slop (heavily generic)\n\n---\n\n### clean_slop.py\n\n**Purpose**: Automatically remove common slop patterns from text.\n\n**Usage**:\n```bash\n# Preview changes\npython scripts/clean_slop.py <file>\n\n# Apply changes (creates backup)\npython scripts/clean_slop.py <file> --save\n\n# Output to different file\npython scripts/clean_slop.py <file> --output cleaned.txt\n\n# Aggressive mode (may change meaning)\npython scripts/clean_slop.py <file> --save --aggressive\n```\n\n**What it cleans**:\n- High-risk phrases → direct alternatives\n- Wordy constructions → concise equivalents\n- Meta-commentary → deleted\n- Buzzwords → plain language\n- Redundant qualifiers → removed\n- Empty intensifiers → deleted\n\n**Safety**:\n- Always creates `.backup` file when using `--save`\n- Preview mode shows all changes before applying\n- Conservative by default (preserves meaning)\n- Aggressive mode removes more but may alter nuance\n\n**Example transformations**:\n```\nBefore: \"It's important to note that we will delve into...\"\nAfter: \"We examine...\"\n\nBefore: \"In order to understand this, due to the fact that...\"\nAfter: \"To understand this, because...\"\n\nBefore: \"Leverage synergistic paradigms...\"\nAfter: \"Use cooperative approaches...\"\n```\n\n---\n\n### detect_slop.R\n\n**Purpose**: Analyze R code files for AI slop patterns.\n\n**Usage**:\n```bash\n# Single file\nRscript scripts/detect_slop.R <file.R> [--verbose]\n\n# Directory\nRscript scripts/detect_slop.R <directory> [--verbose]\n\n# Package R/ folder\nRscript scripts/detect_slop.R mypackage/R/\n```\n\n**What it detects**:\n- Generic variable names (df, data, temp, result, res, out)\n- Generic function names (process_*, do_*, helper)\n- Obvious comments (\"Load the library\", \"Filter the data\")\n- Unnecessary single pipes (`x %>% f()` instead of `f(x)`)\n- Overly long pipe chains (>8 operations)\n- Generic roxygen documentation (@param data The data)\n\n**Output**:\n```\nAnalyzing: analysis.R\nOverall Slop Score: 52/100\n\nGeneric Variables (5):\n  Line 8: df\n  Line 15: data\n  Line 23: result\n\nGeneric Function Names (2):\n  Line 45: process_data()\n  Line 78: helper()\n\nObvious Comments (4):\n  Line 5: \"# Load the library\"\n  Line 12: \"# Filter the data\"\n\nRecommendations:\n- Replace generic variable names with descriptive ones\n- Add namespace qualification (use ::)\n- Remove obvious comments\n- Simplify single-pipe constructs\n```\n\n**Scoring**:\n- 0-20: Low slop (thoughtful R code)\n- 20-40: Moderate slop (some generic patterns)\n- 40-60: High slop (many AI patterns)\n- 60+: Severe slop (heavily generic code)\n\n## Best Practices\n\n### When to Use Automation\n\n**Good use cases**:\n- Large codebases with many files\n- Consistent pattern enforcement across team\n- Pre-release quality audits\n- Continuous integration checks\n- Batch cleanup of known patterns\n\n**Poor use cases**:\n- Nuanced writing requiring context\n- Academic papers with domain conventions\n- Code with intentional generic names (e.g., `data` in function examples)\n- First-time quality review (start manual)\n\n### Setting Thresholds\n\n**Conservative thresholds** (strict standards):\n- Fail: score > 30\n- Warn: score > 20\n- Good for: Public releases, documentation, client deliverables\n\n**Moderate thresholds** (balanced):\n- Fail: score > 50\n- Warn: score > 30\n- Good for: Internal code, team projects, iterative work\n\n**Lenient thresholds** (pragmatic):\n- Fail: score > 70\n- Warn: score > 50\n- Good for: Exploratory work, drafts, legacy code\n\n### Cleanup Strategy\n\n**Always**:\n1. Run detection first (understand scope)\n2. Preview cleanup changes (review transformations)\n3. Create backups (use `--save` for automatic backup)\n4. Test output (verify meaning preserved)\n5. Commit separately (easy to review/revert)\n\n**Never**:\n- Blind batch cleanup without review\n- Aggressive mode on important documents\n- Skip manual verification of automated changes\n- Apply to files with domain-specific conventions\n\n## Integration Patterns\n\n### With Git Workflow\n\n```bash\n# Before committing\ngit diff --name-only | grep \"\\.md$\" | while read file; do\n  python scripts/detect_slop.py \"$file\"\ndone\n\n# Clean before commit\npython scripts/clean_slop.py changed_file.md --save\ngit add changed_file.md\ngit commit -m \"Clean slop from documentation\"\n```\n\n### With Make/Build System\n\n```makefile\n# Makefile\n.PHONY: check-slop clean-slop\n\ncheck-slop:\n\t@find . -name \"*.md\" | while read f; do \\\n\t\tpython scripts/detect_slop.py \"$$f\"; \\\n\tdone\n\nclean-slop:\n\t@find . -name \"*.md\" | while read f; do \\\n\t\tpython scripts/clean_slop.py \"$$f\" --save; \\\n\tdone\n\n# Usage: make check-slop\n```\n\n### With R Package Development\n\n```r\n# R/zzz.R\ncheck_slop <- function(path = \"R/\") {\n  r_files <- list.files(path, pattern = \"\\\\.R$\", full.names = TRUE)\n  lapply(r_files, function(f) {\n    system(paste(\"Rscript scripts/detect_slop.R\", f))\n  })\n}\n\n# Usage: check_slop()\n```\n\n## Limitations\n\n### Script Coverage\n\n**detect_slop.py**:\n- Text files only (.md, .txt)\n- English language optimized\n- Can't understand all contexts\n- May flag acceptable domain conventions\n\n**detect_slop.R**:\n- R code files only (.R)\n- Doesn't detect all R-specific patterns\n- May flag intentional generic names in examples\n- Doesn't check roxygen completeness (just generic patterns)\n\n**clean_slop.py**:\n- Text files only\n- May alter meaning in aggressive mode\n- Can't handle all edge cases\n- Requires manual review\n\n### Manual Skills for Other Cases\n\n- **Python code**: Use python/anti-slop skill (manual)\n- **Design/visualization**: Use design/anti-slop skill (manual)\n- **Julia/C++ code**: Use julia/anti-slop or cpp/anti-slop (manual)\n- **Complex documentation**: Use text/anti-slop skill (manual)\n\n### Context Sensitivity\n\nScripts can't understand:\n- Domain-specific conventions\n- Acceptable generic names in specific contexts\n- Intentional stylistic choices\n- Academic writing requirements\n- Legal/regulatory language needs\n\n**Always combine automated detection with manual review.**\n\n## Troubleshooting\n\n### Script Not Found\n\n```bash\n# Ensure scripts directory exists\nls toolkit/scripts/\n\n# Should see:\n# detect_slop.py\n# clean_slop.py\n# detect_slop.R\n```\n\n### Python Dependencies\n\n```bash\n# Scripts use only standard library\n# No dependencies needed\npython3 scripts/detect_slop.py --help\n```\n\n### R Script Fails\n\n```bash\n# Ensure R is installed\nwhich Rscript\n\n# Scripts use only base R\n# No packages needed\nRscript scripts/detect_slop.R --help\n```\n\n### Permissions\n\n```bash\n# Make scripts executable\nchmod +x scripts/*.py\nchmod +x scripts/*.R\n```\n\n### High False Positive Rate\n\n- Adjust thresholds (increase threshold values)\n- Use manual review for edge cases\n- Whitelist specific files in CI scripts\n- Document acceptable exceptions\n\n## Resources & Advanced Topics\n\n### Script Locations\n\nAll scripts in `toolkit/scripts/`:\n- `detect_slop.py` - Text detection\n- `clean_slop.py` - Text cleanup\n- `detect_slop.R` - R code detection\n\n### Related Skills\n\n- **r/anti-slop** - Manual R code review\n- **python/anti-slop** - Manual Python code review\n- **text/anti-slop** - Manual text review\n- **anti-slop** - Meta-skill coordinator\n\n### Extending the Toolkit\n\n**Add new pattern detection**:\n```python\n# In detect_slop.py\nnew_patterns = [\n    \"your custom pattern\",\n    \"another pattern\"\n]\n```\n\n**Add new cleanup rules**:\n```python\n# In clean_slop.py\ncleanup_rules = [\n    (\"old phrase\", \"replacement\"),\n    (\"another old\", \"another new\")\n]\n```\n\n**Create language-specific detector**:\n```bash\n# Model on detect_slop.R structure\n# Define patterns for your language\n# Score based on pattern frequency\n```\n\n## Integration with Domain Skills\n\nThis toolkit provides **automated detection and cleanup**.\n\nUse together with domain skills for complete coverage:\n\n| Task | Use Toolkit | + Domain Skill |\n|------|-------------|----------------|\n| Audit codebase | detect_slop.py/R | + manual review with domain skill |\n| Clean text files | clean_slop.py | + text/anti-slop for review |\n| Check R code | detect_slop.R | + r/anti-slop for refactoring |\n| CI/CD integration | All scripts | + domain skills for exceptions |\n\n**Key distinction**:\n- **Toolkit** provides automated detection/cleanup\n- **Domain skills** provide context and manual refinement\n\nBoth are complementary equals for maintaining quality standards.\n",
        "toolkit/scripts/README.md": "# Toolkit Scripts\n\nThis directory contains automated detection and cleanup tools.\n\n## Scripts\n\n- **detect_slop.py** - Wraps the Python detection script from `external/cc-polymath`.\n- **clean_slop.py** - Wraps the Python cleanup script from `external/cc-polymath`.\n- **detect_slop.R** - Native R implementation of slop detection (unique to this repo).\n\n## Usage\n\nSee [toolkit/SKILL.md](../SKILL.md) for usage instructions.\n",
        "toolkit/tests/README.md": "# Toolkit Tests\n\nThis directory contains unit tests for the anti-slop toolkit scripts.\n\n## Running Tests\n\nYou can run the tests using `unittest`:\n\n```bash\n# Run all tests\npython3 -m unittest discover toolkit/tests\n\n# Run specific test file\npython3 toolkit/tests/test_detect_slop.py\npython3 toolkit/tests/test_clean_slop.py\n```\n\n## Test Structure\n\n- **test_detect_slop.py**: Tests the detection logic (scoring, pattern matching).\n- **test_clean_slop.py**: Tests the cleanup logic (replacements, aggressive mode).\n\nThese tests import the core logic from the `external/cc-polymath` submodule to ensure the wrapped scripts behave as expected.\n"
      },
      "plugins": [
        {
          "name": "anti-slop",
          "description": "Meta-skill that coordinates all anti-slop domain skills. Auto-loads appropriate skill based on file type.",
          "source": "./",
          "strict": false,
          "skills": [
            "./anti-slop"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add emraher/eerskills",
            "/plugin install anti-slop@anti-slop-skills"
          ]
        },
        {
          "name": "toolkit",
          "description": "Active detection and cleanup scripts. Automated slop scoring for text and R code files.",
          "source": "./",
          "strict": false,
          "skills": [
            "./toolkit"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add emraher/eerskills",
            "/plugin install toolkit@anti-slop-skills"
          ]
        },
        {
          "name": "text",
          "description": "Comprehensive text quality coordinator. Three-layer system: (1) pattern removal, (2) Strunk's principles, (3) human voice. One invocation applies all layers automatically.",
          "source": "./",
          "strict": false,
          "skills": [
            "./text/anti-slop"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add emraher/eerskills",
            "/plugin install text@anti-slop-skills"
          ]
        },
        {
          "name": "humanizer",
          "description": "Wikipedia's 24-pattern checklist for removing AI writing signatures. Adds personality and voice to text.",
          "source": "./",
          "strict": false,
          "skills": [
            "./external/humanizer"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add emraher/eerskills",
            "/plugin install humanizer@anti-slop-skills"
          ]
        },
        {
          "name": "design",
          "description": "Visual design quality enforcement. Detects cookie-cutter layouts, generic gradients, and 'AI startup' aesthetic.",
          "source": "./",
          "strict": false,
          "skills": [
            "./design/anti-slop"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add emraher/eerskills",
            "/plugin install design@anti-slop-skills"
          ]
        },
        {
          "name": "r",
          "description": "R code quality enforcement. Namespace qualification, explicit returns, no generic names (df, data, result).",
          "source": "./",
          "strict": false,
          "skills": [
            "./r/anti-slop"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add emraher/eerskills",
            "/plugin install r@anti-slop-skills"
          ]
        },
        {
          "name": "python",
          "description": "Python code quality enforcement. Type hints, docstrings, PEP 8 compliance, pandas best practices.",
          "source": "./",
          "strict": false,
          "skills": [
            "./python/anti-slop"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add emraher/eerskills",
            "/plugin install python@anti-slop-skills"
          ]
        },
        {
          "name": "quarto",
          "description": "Quarto/RMarkdown quality enforcement. Prevents generic research documents, ensures reproducibility.",
          "source": "./",
          "strict": false,
          "skills": [
            "./quarto/anti-slop"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add emraher/eerskills",
            "/plugin install quarto@anti-slop-skills"
          ]
        },
        {
          "name": "julia",
          "description": "Julia code quality enforcement for scientific computing. Type stability and multiple dispatch.",
          "source": "./",
          "strict": false,
          "skills": [
            "./julia/anti-slop"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add emraher/eerskills",
            "/plugin install julia@anti-slop-skills"
          ]
        },
        {
          "name": "cpp",
          "description": "C++ and Rcpp quality enforcement for performance-critical code. Memory safety and Rcpp best practices.",
          "source": "./",
          "strict": false,
          "skills": [
            "./cpp/anti-slop"
          ],
          "categories": [],
          "install_commands": [
            "/plugin marketplace add emraher/eerskills",
            "/plugin install cpp@anti-slop-skills"
          ]
        }
      ]
    }
  ]
}