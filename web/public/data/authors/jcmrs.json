{
  "author": {
    "id": "jcmrs",
    "display_name": "JCMRS",
    "type": "User",
    "avatar_url": "https://avatars.githubusercontent.com/u/19489577?u=ce7368e4df40da22f3df64219a5ba44c34b2ad86&v=4",
    "url": "https://github.com/jcmrs",
    "bio": "Managed by Claude Code",
    "stats": {
      "total_marketplaces": 1,
      "total_plugins": 6,
      "total_commands": 17,
      "total_skills": 6,
      "total_stars": 0,
      "total_forks": 0
    }
  },
  "marketplaces": [
    {
      "name": "jcmrs-plugins",
      "version": null,
      "description": "Transforms messy human intent and repository analysis into living operational domain profiles through 6-phase knowledge engineering pipeline",
      "owner_info": {
        "name": "JCMRS"
      },
      "keywords": [],
      "repo_full_name": "jcmrs/jcmrs-plugins",
      "repo_url": "https://github.com/jcmrs/jcmrs-plugins",
      "repo_description": null,
      "homepage": null,
      "signals": {
        "stars": 0,
        "forks": 0,
        "pushed_at": "2026-01-04T21:15:59Z",
        "created_at": "2025-12-21T13:19:03Z",
        "license": null
      },
      "file_tree": [
        {
          "path": ".claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": ".claude-plugin/marketplace.json",
          "type": "blob",
          "size": 1643
        },
        {
          "path": "plugins",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-pms",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-pms/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-pms/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 710
        },
        {
          "path": "plugins/claude-pms/README.md",
          "type": "blob",
          "size": 7966
        },
        {
          "path": "plugins/claude-pms/SKILL.md",
          "type": "blob",
          "size": 9237
        },
        {
          "path": "plugins/claude-pms/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-pms/commands/config.md",
          "type": "blob",
          "size": 4481
        },
        {
          "path": "plugins/claude-pms/commands/encode.md",
          "type": "blob",
          "size": 1515
        },
        {
          "path": "plugins/claude-pms/commands/extract.md",
          "type": "blob",
          "size": 2392
        },
        {
          "path": "plugins/claude-pms/commands/reflect.md",
          "type": "blob",
          "size": 3020
        },
        {
          "path": "plugins/claude-pms/commands/status.md",
          "type": "blob",
          "size": 7855
        },
        {
          "path": "plugins/claude-pms/commands/synthesize.md",
          "type": "blob",
          "size": 3035
        },
        {
          "path": "plugins/claude-pms/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/claude-pms/hooks/.gitkeep",
          "type": "blob",
          "size": 99
        },
        {
          "path": "plugins/claude-pms/hooks/hooks.json",
          "type": "blob",
          "size": 1119
        },
        {
          "path": "plugins/claude-pms/hooks/precompact.sh",
          "type": "blob",
          "size": 1130
        },
        {
          "path": "plugins/claude-pms/hooks/session-end.sh",
          "type": "blob",
          "size": 1071
        },
        {
          "path": "plugins/claude-pms/hooks/session-start.sh",
          "type": "blob",
          "size": 1391
        },
        {
          "path": "plugins/claude-pms/hooks/stop.sh",
          "type": "blob",
          "size": 1095
        },
        {
          "path": "plugins/docs-reader",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/docs-reader/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/docs-reader/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 482
        },
        {
          "path": "plugins/docs-reader/README.md",
          "type": "blob",
          "size": 4611
        },
        {
          "path": "plugins/docs-reader/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/docs-reader/commands/docs.md",
          "type": "blob",
          "size": 1961
        },
        {
          "path": "plugins/docs-reader/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/docs-reader/skills/documentation-query",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/docs-reader/skills/documentation-query/SKILL.md",
          "type": "blob",
          "size": 5932
        },
        {
          "path": "plugins/docs-reader/skills/documentation-query/resources",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/docs-reader/skills/documentation-query/resources/tutorials-index.md",
          "type": "blob",
          "size": 1336
        },
        {
          "path": "plugins/docs-reader/skills/documentation-query/resources/wiki-index.md",
          "type": "blob",
          "size": 1618
        },
        {
          "path": "plugins/gemini-consult",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/gemini-consult/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/gemini-consult/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 206
        },
        {
          "path": "plugins/gemini-consult/README.md",
          "type": "blob",
          "size": 14620
        },
        {
          "path": "plugins/gemini-consult/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/gemini-consult/commands/gemini-check.md",
          "type": "blob",
          "size": 5105
        },
        {
          "path": "plugins/gemini-consult/commands/gemini-consult.md",
          "type": "blob",
          "size": 5073
        },
        {
          "path": "plugins/gemini-consult/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/gemini-consult/hooks/hooks.json",
          "type": "blob",
          "size": 2659
        },
        {
          "path": "plugins/gemini-consult/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/gemini-consult/skills/gemini-consult",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/gemini-consult/skills/gemini-consult/SKILL.md",
          "type": "blob",
          "size": 16095
        },
        {
          "path": "plugins/profile-creator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/profile-creator/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/profile-creator/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 446
        },
        {
          "path": "plugins/profile-creator/README.md",
          "type": "blob",
          "size": 3110
        },
        {
          "path": "plugins/profile-creator/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/profile-creator/commands/create-profile.md",
          "type": "blob",
          "size": 4789
        },
        {
          "path": "plugins/profile-creator/commands/validate-profile.md",
          "type": "blob",
          "size": 4827
        },
        {
          "path": "plugins/profile-creator/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/profile-creator/examples/singular",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/profile-creator/examples/singular/researcher-crewai",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/profile-creator/examples/singular/researcher-crewai/README.md",
          "type": "blob",
          "size": 10746
        },
        {
          "path": "plugins/profile-creator/examples/singular/system-architect",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/profile-creator/examples/singular/system-architect/README.md",
          "type": "blob",
          "size": 9698
        },
        {
          "path": "plugins/profile-creator/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/profile-creator/skills/profile-creator",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/profile-creator/skills/profile-creator/SKILL.md",
          "type": "blob",
          "size": 14132
        },
        {
          "path": "plugins/running-log",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/running-log/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/running-log/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 422
        },
        {
          "path": "plugins/running-log/README.md",
          "type": "blob",
          "size": 11842
        },
        {
          "path": "plugins/running-log/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/running-log/commands/idea.md",
          "type": "blob",
          "size": 4923
        },
        {
          "path": "plugins/running-log/commands/review-backlog.md",
          "type": "blob",
          "size": 9498
        },
        {
          "path": "plugins/running-log/commands/running-log.md",
          "type": "blob",
          "size": 3590
        },
        {
          "path": "plugins/running-log/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/running-log/skills/running-log",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/running-log/skills/running-log/SKILL.md",
          "type": "blob",
          "size": 11226
        },
        {
          "path": "plugins/semantic-linguist",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/semantic-linguist/.claude-plugin",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/semantic-linguist/.claude-plugin/plugin.json",
          "type": "blob",
          "size": 858
        },
        {
          "path": "plugins/semantic-linguist/README.md",
          "type": "blob",
          "size": 10344
        },
        {
          "path": "plugins/semantic-linguist/commands",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/semantic-linguist/commands/map-domain.md",
          "type": "blob",
          "size": 6922
        },
        {
          "path": "plugins/semantic-linguist/commands/semantic-config.md",
          "type": "blob",
          "size": 9643
        },
        {
          "path": "plugins/semantic-linguist/commands/validate-terminology.md",
          "type": "blob",
          "size": 5190
        },
        {
          "path": "plugins/semantic-linguist/hooks",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/semantic-linguist/hooks/hooks.json",
          "type": "blob",
          "size": 5983
        },
        {
          "path": "plugins/semantic-linguist/skills",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/semantic-linguist/skills/semantic-translation",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/semantic-linguist/skills/semantic-translation/AI-DOMAIN-ADDITION-GUIDE.md",
          "type": "blob",
          "size": 36617
        },
        {
          "path": "plugins/semantic-linguist/skills/semantic-translation/SKILL.md",
          "type": "blob",
          "size": 21152
        },
        {
          "path": "plugins/semantic-linguist/skills/semantic-translation/examples",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/semantic-linguist/skills/semantic-translation/examples/autogen-mappings.md",
          "type": "blob",
          "size": 13765
        },
        {
          "path": "plugins/semantic-linguist/skills/semantic-translation/examples/common-ambiguities.md",
          "type": "blob",
          "size": 7972
        },
        {
          "path": "plugins/semantic-linguist/skills/semantic-translation/examples/fastapi-mappings.md",
          "type": "blob",
          "size": 24808
        },
        {
          "path": "plugins/semantic-linguist/skills/semantic-translation/examples/git-gitflow-mappings.md",
          "type": "blob",
          "size": 20391
        },
        {
          "path": "plugins/semantic-linguist/skills/semantic-translation/examples/langroid-mappings.md",
          "type": "blob",
          "size": 4383
        },
        {
          "path": "plugins/semantic-linguist/skills/semantic-translation/examples/mcp-mappings.md",
          "type": "blob",
          "size": 22190
        },
        {
          "path": "plugins/semantic-linguist/skills/semantic-translation/examples/memory-graphs-mappings.md",
          "type": "blob",
          "size": 35774
        },
        {
          "path": "plugins/semantic-linguist/skills/semantic-translation/examples/sre-mappings.md",
          "type": "blob",
          "size": 27538
        },
        {
          "path": "plugins/semantic-linguist/skills/semantic-translation/examples/utcp-mappings.md",
          "type": "blob",
          "size": 24837
        },
        {
          "path": "plugins/semantic-linguist/skills/semantic-translation/references",
          "type": "tree",
          "size": null
        },
        {
          "path": "plugins/semantic-linguist/skills/semantic-translation/references/cognitive-framework.md",
          "type": "blob",
          "size": 21053
        },
        {
          "path": "plugins/semantic-linguist/skills/semantic-translation/references/decision-trees.md",
          "type": "blob",
          "size": 30455
        },
        {
          "path": "plugins/semantic-linguist/skills/semantic-translation/references/domain-ontologies.md",
          "type": "blob",
          "size": 16490
        },
        {
          "path": "plugins/semantic-linguist/skills/semantic-translation/references/translation-patterns.md",
          "type": "blob",
          "size": 17747
        }
      ],
      "files": {
        ".claude-plugin/marketplace.json": "{\n  \"name\": \"jcmrs-plugins\",\n  \"owner\": {\n    \"name\": \"JCMRS\"\n  },\n  \"plugins\": [\n    {\n      \"name\": \"profile-creator\",\n      \"source\": \"./plugins/profile-creator\",\n      \"description\": \"Transforms messy human intent and repository analysis into living operational domain profiles through 6-phase knowledge engineering pipeline\"\n    },\n    {\n      \"name\": \"running-log\",\n      \"source\": \"./plugins/running-log\",\n      \"description\": \"Persistent schema-driven running log capturing ideas, consultations, and Claude's reasoning patterns for cross-session learning and process memory\"\n    },\n    {\n      \"name\": \"gemini-consult\",\n      \"source\": \"./plugins/gemini-consult\",\n      \"description\": \"Leverage Google Gemini's CLI for analyzing large codebases beyond typical context limits using @ syntax for file/directory inclusion\"\n    },\n    {\n      \"name\": \"semantic-linguist\",\n      \"source\": \"./plugins/semantic-linguist\",\n      \"description\": \"Semantic translation between natural language and technical precision across 8 domains: Autogen, Langroid, MCP, UTCP, FastAPI, Git/Gitflow, SRE, and Memory Graphs\"\n    },\n    {\n      \"name\": \"claude-pms\",\n      \"source\": \"./plugins/claude-pms\",\n      \"description\": \"Procedural Memory System - Learns procedures, processes, and workflows from conversational history through three-tier architecture (Episodic â†’ Semantic â†’ Procedural). Shame on you.\"\n    },\n    {\n      \"name\": \"docs-reader\",\n      \"source\": \"./plugins/docs-reader\",\n      \"description\": \"Documentation Reader for Axivo Claude Collaboration Platform, providing access through components, protocols, and competencies\"\n    }\n  ]\n}\n",
        "plugins/claude-pms/.claude-plugin/plugin.json": "{\n  \"name\": \"claude-pms\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Procedural Memory System - Learns procedures, processes, and workflows from session experiences, generates project-scoped rules for consistent behavior. Shame on you.\",\n  \"author\": {\n    \"name\": \"JCMRS\"\n  },\n  \"homepage\": \"https://github.com/jcmrs/jcmrs-plugins\",\n  \"repository\": \"https://github.com/jcmrs/jcmrs-plugins\",\n  \"license\": \"MIT\",\n  \"commands\": \"./commands\",\n  \"hooks\": \"./hooks/hooks.json\",\n  \"keywords\": [\n    \"memory\",\n    \"learning\",\n    \"procedural\",\n    \"patterns\",\n    \"rules\",\n    \"cognitive\",\n    \"episodic\",\n    \"semantic\",\n    \"procedural-memory\",\n    \"session-learning\",\n    \"code-patterns\",\n    \"user-preferences\"\n  ]\n}\n",
        "plugins/claude-pms/README.md": "# Claude PMS - Procedural Memory System\n\n> **\"Shame on you.\"** - Because Claude learns from your corrections.\n\n## Overview\n\nClaude PMS (Procedural Memory System) is a cognitive learning plugin that enables Claude Code to learn from session experiences, extract behavioral patterns, and generate project-scoped rules for consistent behavior.\n\n**Three-Tier Memory Architecture:**\n- **Episodic Memory:** Captures specific session experiences (what happened, when, where)\n- **Semantic Memory:** Extracts patterns, preferences, and anti-patterns from episodes\n- **Procedural Memory:** Generates actionable rules for Claude Code's Rules system\n\n## Quick Start\n\n### Installation\n\nThe plugin is automatically available through the jcmrs-plugins marketplace.\n\n### First Use\n\n1. **Work on your project** - PMS automatically captures sessions on PreCompact and SessionEnd events\n2. **After 10+ sessions, run:** `/pms:reflect`\n3. **Review proposed rules** - Approve or reject patterns\n4. **Restart session** - New rules load automatically\n\nThat's it! Claude now learns your preferences and coding patterns.\n\n## How It Works\n\n```\nSession Activity â†’ Episodic Encoding â†’ Semantic Extraction â†’ Procedural Synthesis\n                         â†“                    â†“                      â†“\n                   episodic/*.json    semantic/*.json    rules/pms/*.md\n                                                              â†“\n                                                    Claude Code Rules System\n```\n\n### 1. Episodic Encoding (Automatic)\n\nCaptures each session's experience:\n- Task summary and work performed\n- Design decisions made\n- Challenges encountered and solutions applied\n- User preferences observed (from corrections/feedback)\n- Code patterns and anti-patterns\n- Context and technologies used\n\n**Privacy:** All sensitive data (API keys, passwords, tokens) is automatically redacted.\n\n### 2. Semantic Extraction (Automatic or Manual)\n\nAnalyzes episodic records to detect patterns:\n- **Emerging patterns:** 2+ occurrences\n- **Strong patterns:** 3+ occurrences (worthy of rules)\n- **Critical patterns:** 5+ occurrences (high priority)\n\nCategorizes patterns as:\n- User preferences (commit style, testing approach, code review)\n- Code patterns (language/framework preferences)\n- Anti-patterns (mistakes to avoid)\n\n### 3. Procedural Synthesis (Manual)\n\nConverts confirmed patterns into project-scoped rules:\n- Generates markdown rule files in `.claude/rules/pms/`\n- Groups related rules (preferences, testing, code-patterns, anti-patterns)\n- Requires user approval before creating rules\n- Rules load automatically in new sessions\n\n## Commands\n\n| Command | Description |\n|---------|-------------|\n| `/pms:encode` | Manually capture current session experience |\n| `/pms:reflect` | Full pipeline: extract patterns + generate rules |\n| `/pms:extract` | Extract patterns only (no rule generation) |\n| `/pms:synthesize` | Generate rules from detected patterns |\n| `/pms:status` | Show current memory state and statistics |\n| `/pms:config` | Open configuration file for editing |\n\n## Configuration\n\nEdit `.claude/pms.local.md` in your project to customize:\n\n```yaml\ntriggers:\n  precompact: true      # Capture before compaction\n  session_end: true     # Capture at session end\n  stop: false           # Capture when Claude stops\n\nprocessing:\n  continuous_mode: true      # Auto-extract after encoding\n  auto_synthesize: false     # Require approval for rules\n\nthresholds:\n  min_sessions: 10           # Minimum before extraction\n  emerging_pattern: 2        # 2+ = emerging\n  strong_pattern: 3          # 3+ = rule-worthy\n  critical_pattern: 5        # 5+ = high priority\n```\n\n**See template at:** `examples/pms.local.md`\n\n## Memory Structure\n\n```\n{project}/.claude/\nâ”œâ”€â”€ pms/                          # PMS memory storage (project-scoped)\nâ”‚   â”œâ”€â”€ episodic/\nâ”‚   â”‚   â”œâ”€â”€ sessions-2025-01.json # Monthly session logs\nâ”‚   â”‚   â””â”€â”€ index.json            # Session ID â†’ file mapping\nâ”‚   â”œâ”€â”€ semantic/\nâ”‚   â”‚   â”œâ”€â”€ patterns.json         # Detected patterns\nâ”‚   â”‚   â”œâ”€â”€ preferences.json      # User preferences\nâ”‚   â”‚   â””â”€â”€ anti-patterns.json    # Mistakes to avoid\nâ”‚   â”œâ”€â”€ procedural/\nâ”‚   â”‚   â””â”€â”€ rules-metadata.json   # Rule file tracking\nâ”‚   â””â”€â”€ metadata/\nâ”‚       â””â”€â”€ processed.json        # Processing state\nâ”œâ”€â”€ rules/pms/                    # Generated rules (Claude Code native)\nâ”‚   â”œâ”€â”€ user-preferences.md\nâ”‚   â”œâ”€â”€ testing-patterns.md\nâ”‚   â”œâ”€â”€ code-patterns.md\nâ”‚   â””â”€â”€ anti-patterns.md\nâ””â”€â”€ pms.local.md                  # Configuration (YAML frontmatter)\n```\n\n## Privacy\n\n**All sensitive data is automatically redacted before storage:**\n- API keys and tokens\n- Passwords and credentials\n- Secret keys and certificates\n- Custom patterns (configurable)\n\n**Project-Scoped Isolation:**\n- All memory stored per-project (no global learning)\n- No cross-project contamination\n- Each project learns independently\n\n## Architecture\n\n**Tech Stack:**\n- Bash hooks for event triggers\n- Python 3.8+ for processing\n- Prompt-based analysis for intelligent pattern detection\n- JSON for structured storage\n- Markdown for rules (Claude Code Rules system)\n\n**Design Principles:**\n- Context-first encoding (uses conversation history)\n- JSONL fallback (when context unavailable)\n- Non-fatal errors (session continues normally)\n- Windows 11 compatible (no Cipher dependency)\n\n## Examples\n\n### Example: User Preference Detection\n\n**Session 1:** User corrects: \"Don't add Claude Code attribution to commits\"\n**Session 2:** User corrects: \"Remove the attribution comment\"\n**Session 3:** User corrects again\n\n**After 3+ occurrences â†’ Extracted Pattern:**\n```json\n{\n  \"pattern\": \"No Claude Code attribution in commits\",\n  \"strength\": \"strong\",\n  \"occurrences\": 3,\n  \"evidence\": [\"session-001\", \"session-002\", \"session-003\"]\n}\n```\n\n**Generated Rule (in `.claude/rules/pms/user-preferences.md`):**\n```markdown\n## Commit Style\n- Never add \"Generated with Claude Code\" or similar attribution\n- Keep commit messages focused on changes, not tools\n```\n\n### Example: Testing Pattern Detection\n\n**Sessions 5, 7, 9, 12:** User runs tests before committing\n**Session 11:** User blocks commit when tests fail\n\n**After 5+ occurrences â†’ Extracted Pattern:**\n```json\n{\n  \"pattern\": \"Always run tests before commits\",\n  \"strength\": \"critical\",\n  \"occurrences\": 5\n}\n```\n\n**Generated Rule (in `.claude/rules/pms/testing-patterns.md`):**\n```markdown\n## Pre-Commit Testing\n- Always run test suite before creating commits\n- Ensure all tests pass before committing\n- If tests fail, fix them or update them as needed\n- Never commit code with failing tests\n```\n\n## Troubleshooting\n\n### \"Insufficient sessions\" when running /pms:extract\n\n**Cause:** Fewer than 10 episodic records captured\n**Solution:** Work through more sessions, then run `/pms:reflect` again\n\n### \"No strong patterns detected\"\n\n**Cause:** Patterns haven't occurred 3+ times yet\n**Solution:** Continue working, patterns emerge over time\n\n### Rules not loading in new session\n\n**Cause:** Rules cached from previous session\n**Solution:** Completely restart Claude Code (not just new conversation)\n\n### Privacy redaction too aggressive\n\n**Cause:** Custom patterns matching normal code\n**Solution:** Edit `.claude/pms.local.md` â†’ remove overly broad patterns\n\n## Future Enhancements (v2)\n\n- **Import/Export Skill:** Selectively share rules between projects\n- **Pattern Confidence Scores:** Machine learning-based pattern validation\n- **Visual Pattern Dashboard:** Web UI for memory exploration\n- **Cross-Session Context:** Link related sessions by topic\n\n## Contributing\n\nThis plugin is part of the jcmrs-plugins collection. See main repository for contribution guidelines.\n\n## License\n\nMIT License - See LICENSE file in repository root\n\n---\n\n**Claude PMS - Because your preferences matter. Shame on you.**\n",
        "plugins/claude-pms/SKILL.md": "---\nname: Procedural Memory System\ndescription: This skill should be used when the user asks to \"reflect on patterns\", \"generate rules from sessions\", \"consolidate memory\", \"analyze session patterns\", \"learn procedures and workflows\", \"create procedural knowledge\", or mentions memory consolidation, process learning, or long-term knowledge retention. Provides systematic extraction of procedures, processes, and workflows from conversational history with automatic rule generation.\nversion: 1.0.0\n---\n\n# Procedural Memory System (PMS)\n\n> **\"Shame on you.\"** - Because Claude learns from your corrections.\n\n## Overview\n\nProcedural Memory System implements a three-tier memory architecture that transforms episodic conversational history into actionable procedural knowledge. The system automatically captures session transcripts, extracts recurring patterns in procedures and workflows, and synthesizes context-aware rules that persist across sessions.\n\n**Complementary Focus**: PMS learns both **rigid procedures** (technical steps, coding patterns) and **flexible processes** (workflows, decision-making patterns, team practices), ensuring comprehensive behavioral learning.\n\n**Three-tier architecture:**\n1. **Episodic Memory**: Raw session transcripts with metadata\n2. **Semantic Memory**: Extracted patterns, preferences, and anti-patterns\n3. **Procedural Memory**: Generated rules injected into Claude's context\n\n## When to Use\n\nInvoke PMS workflows when:\n- Completing work sessions and consolidating learnings\n- Reaching session boundaries (context compaction, session end)\n- Detecting recurring user preferences or code patterns\n- Generating persistent rules from accumulated knowledge\n- Analyzing patterns across multiple work sessions\n\nPMS activates automatically via hooks but can be triggered manually via commands.\n\n## Core Workflow\n\n### 1. Episodic Encoding\n\nCapture session transcripts into episodic records during natural boundaries.\n\n**Automatic triggers:**\n- PreCompact hook: Before context compaction\n- SessionEnd hook: When session ends\n- Stop hook: When work completes (optional)\n\n**Manual trigger:**\n```\n/pms:encode\n```\n\n**What happens:**\n- Transcript read from `.claude/projects/[project]/transcripts/`\n- Metadata extracted (tool usage, file operations, key decisions)\n- Episodic record saved to `.claude/pms/episodic/sessions-YYYY-MM.json`\n- Privacy redaction applied (API keys, passwords, tokens)\n\n**Encoding strategies:**\n- **Context-first**: Uses conversation context when available (preferred)\n- **JSONL fallback**: Parses JSONL transcript files when context unavailable\n\n### 2. Semantic Extraction\n\nExtract recurring patterns from accumulated episodic records.\n\n**Trigger threshold:** Default 10 sessions minimum (configurable)\n\n**Manual trigger:**\n```\n/pms:extract\n```\n\n**Pattern detection:**\n- **User Preferences**: \"Always run tests before commits\", \"Use TDD workflow\"\n- **Code Patterns**: \"Repository pattern\", \"Middleware architecture\"\n- **Anti-Patterns**: \"Avoid god objects\", \"No global state\"\n\n**Strength categorization:**\n- **Emerging**: 2-4 occurrences\n- **Strong**: 3-7 occurrences\n- **Critical**: 5+ occurrences\n\n**Output:**\n- `.claude/pms/semantic/patterns.json` - All detected patterns\n- `.claude/pms/semantic/preferences.json` - User preferences only\n- `.claude/pms/semantic/code-patterns.json` - Code patterns only\n- `.claude/pms/semantic/anti-patterns.json` - Anti-patterns only\n\n### 3. Procedural Synthesis\n\nGenerate context-aware rules from semantic patterns.\n\n**Trigger threshold:** Patterns must meet strength requirements\n\n**Manual trigger:**\n```\n/pms:synthesize\n```\n\n**Rule generation:**\n- Critical patterns â†’ Immediate rule generation\n- Strong patterns â†’ Rule generation with approval\n- Emerging patterns â†’ Tracked but not synthesized\n\n**Rule injection:**\n- Generated rules saved to `.claude/rules/pms/`\n- Rules automatically loaded into Claude's context\n- Rules persist across sessions and projects\n\n**Rule format:**\n```markdown\n---\npattern_id: pref_1234567890\ncategory: preference\nstrength: strong\noccurrences: 5\n---\n\n# [Pattern Title]\n\n**Detected Pattern**: [Description]\n\n**Apply this guidance**: [Actionable rule]\n\n**Evidence**: Sessions [list]\n```\n\n## Commands\n\n### `/pms:encode`\nManually trigger episodic encoding for current session.\n\n**Use when:**\n- Completing significant work\n- Before context clearing\n- At logical session boundaries\n\n### `/pms:extract`\nTrigger semantic pattern extraction from episodic records.\n\n**Use when:**\n- Accumulated sufficient sessions (10+ recommended)\n- Want to review detected patterns\n- Before rule synthesis\n\n### `/pms:synthesize`\nGenerate procedural rules from semantic patterns.\n\n**Use when:**\n- Patterns reach actionable strength\n- Ready to persist knowledge as rules\n- After reviewing extracted patterns\n\n### `/pms:reflect`\nComprehensive workflow: encode â†’ extract â†’ synthesize.\n\n**Use when:**\n- End of significant work session\n- Regular reflection intervals\n- Memory consolidation needed\n\n### `/pms:status`\nDisplay current memory system state.\n\n**Shows:**\n- Episodic record count\n- Detected pattern count\n- Generated rule count\n- Last update timestamps\n\n## Configuration\n\nConfigure PMS via `.claude/pms.local.md` with YAML frontmatter.\n\n**Key settings:**\n\n**Triggers** (enable/disable automatic hooks):\n```yaml\ntriggers:\n  precompact: true\n  session_end: true\n  stop: false\n```\n\n**Thresholds** (pattern detection sensitivity):\n```yaml\nthresholds:\n  min_sessions: 10\n  emerging_pattern: 2\n  strong_pattern: 3\n  critical_pattern: 5\n```\n\n**Privacy** (redaction configuration):\n```yaml\nprivacy:\n  redact_sensitive: true\n  custom_redaction_patterns: []\n```\n\n**Processing** (automation behavior):\n```yaml\nprocessing:\n  continuous_mode: false\n  auto_extract: false\n  auto_synthesize: false\n```\n\n## Privacy and Security\n\n**Automatic redaction** of sensitive data:\n- API keys and tokens\n- Passwords and secrets\n- Bearer tokens and credentials\n- Custom patterns (configurable)\n\n**Redaction strategy:**\n- Over-redact rather than under-redact\n- Pattern-based detection with regex\n- Recursive traversal of nested structures\n\n**What gets redacted:**\n```\napi_key=sk-xxxx... â†’ api_key=[REDACTED]\npassword=\"secret\" â†’ password=\"[REDACTED]\"\nBearer xxxxx... â†’ Bearer [REDACTED]\n```\n\n## Error Handling and Recovery\n\n**Corruption recovery:**\n```\n/pms:validate\n```\nValidates JSON schema integrity and backs up corrupted files.\n\n**Manual reset:**\n```\n/pms:reset\n```\nClears semantic and procedural memory while preserving episodic records (optional).\n\n**Rebuild semantic knowledge:**\n```\n/pms:rebuild\n```\nRegenerates semantic patterns from episodic records after corruption or reset.\n\n## Best Practices\n\n**Regular reflection intervals:**\n- End of each work session\n- Before context clearing\n- At project milestones\n- After significant learning experiences\n\n**Pattern review:**\n- Review extracted patterns before synthesis\n- Validate pattern accuracy and relevance\n- Adjust thresholds if patterns too noisy or sparse\n\n**Privacy maintenance:**\n- Review redaction patterns periodically\n- Add custom patterns for domain-specific secrets\n- Verify no sensitive data in generated rules\n\n**Performance optimization:**\n- Keep continuous_mode disabled unless needed\n- Use manual triggers for selective extraction\n- Archive old episodic records periodically\n\n## Integration with Workflow\n\n**Typical session workflow:**\n1. Work on features, solve problems, make decisions\n2. Reach natural boundary (context compaction, session end)\n3. PMS encodes session automatically via hooks\n4. Continue working until threshold reached (10+ sessions)\n5. PMS extracts patterns automatically or via `/pms:extract`\n6. Review patterns, approve rule generation\n7. PMS synthesizes rules via `/pms:synthesize`\n8. Rules persist and apply to future sessions\n\n**Manual workflow:**\n```\n[Work session]\n/pms:encode          # Capture current session\n[More work sessions]\n/pms:extract         # Review patterns\n/pms:synthesize      # Generate rules\n/pms:status          # Verify state\n```\n\n**Emergency workflow:**\n```\n/pms:validate        # Check for corruption\n/pms:backup          # Backup current state\n/pms:reset --keep-episodic  # Clear semantic/procedural\n/pms:rebuild         # Regenerate from episodic\n```\n\n## Verification\n\n**After encoding:**\n- Check `.claude/pms/episodic/sessions-YYYY-MM.json` exists\n- Verify session count increased\n- Confirm no sensitive data in episodic records\n\n**After extraction:**\n- Review `.claude/pms/semantic/patterns.json`\n- Verify pattern descriptions are accurate\n- Check strength categorization makes sense\n\n**After synthesis:**\n- Read generated rules in `.claude/rules/pms/`\n- Verify rule guidance is actionable\n- Test rules apply in new sessions\n\n## Additional Resources\n\nFor detailed technical documentation:\n- **`ARCHITECTURE.md`**: Deep technical design, data flow, component architecture\n- **`README.md`**: User guide, quick start, troubleshooting\n- **`.claude/pms.local.md`**: Configuration template with examples\n\nFor testing and validation:\n- **`tests/unit/`**: Unit tests for individual components\n- **`tests/integration/`**: Full pipeline integration tests\n- **`scripts/`**: Core engine implementations (encode, extract, synthesize)\n",
        "plugins/claude-pms/commands/config.md": "---\nname: pms:config\ndescription: Open or view PMS configuration file\n---\n\n# PMS Configuration Management\n\nManage Claude PMS configuration settings.\n\n## Configuration Location\n\n```\n$CLAUDE_PROJECT_DIR/.claude/pms.local.md\n```\n\nYAML frontmatter format with markdown documentation.\n\n## Usage\n\n### Open in Editor (if available)\n\n```bash\nCONFIG_FILE=\"$CLAUDE_PROJECT_DIR/.claude/pms.local.md\"\n\n# Try common editors\nif command -v code >/dev/null 2>&1; then\n  code \"$CONFIG_FILE\"\nelif command -v nano >/dev/null 2>&1; then\n  nano \"$CONFIG_FILE\"\nelif command -v vim >/dev/null 2>&1; then\n  vim \"$CONFIG_FILE\"\nelse\n  echo \"No editor found. View config manually:\"\n  echo \"$CONFIG_FILE\"\nfi\n```\n\n### View Current Configuration\n\n```bash\nCONFIG_FILE=\"$CLAUDE_PROJECT_DIR/.claude/pms.local.md\"\n\nif [ -f \"$CONFIG_FILE\" ]; then\n  echo \"=== Current Configuration ===\"\n  echo \"\"\n  cat \"$CONFIG_FILE\"\n  echo \"\"\n  echo \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n  echo \"To edit: Open $CONFIG_FILE in your editor\"\n  echo \"After changes: Restart Claude Code session\"\nelse\n  echo \"Config not found. Creating default...\"\n  mkdir -p \"$(dirname \"$CONFIG_FILE\")\"\n  cp \"$CLAUDE_PLUGIN_ROOT/examples/pms.local.md\" \"$CONFIG_FILE\"\n  echo \"âœ“ Created default config at:\"\n  echo \"  $CONFIG_FILE\"\n  echo \"\"\n  echo \"Edit this file to customize PMS behavior\"\nfi\n```\n\n## Configuration Options\n\n### Triggers (When to capture sessions)\n```yaml\ntriggers:\n  precompact: true      # Before context compaction\n  session_end: true     # When session ends\n  stop: false           # When Claude stops (optional)\n```\n\n### Processing Modes\n```yaml\nprocessing:\n  continuous_mode: true      # Auto-extract after encoding\n  auto_synthesize: false     # Require approval for rules\n```\n\n**continuous_mode:**\n- `true`: Automatically run extraction after each encoding\n- `false`: Manual control with `/pms:extract`\n\n**auto_synthesize:**\n- `true`: Automatically generate rules without approval\n- `false`: User approval required (recommended)\n\n### Thresholds\n```yaml\nthresholds:\n  min_sessions: 10           # Minimum before extraction\n  emerging_pattern: 2        # 2+ occurrences\n  strong_pattern: 3          # 3+ occurrences (rule-worthy)\n  critical_pattern: 5        # 5+ occurrences (high priority)\n```\n\n### Privacy Settings\n```yaml\nprivacy:\n  redact_sensitive: true     # Auto-redact API keys, passwords\n  custom_redaction_patterns:\n    - \"custom_secret_pattern\"\n```\n\n### Encoding Preferences\n```yaml\nencoding:\n  prefer_context: true       # Use conversation history first\n  fallback_jsonl: true       # Fall back to JSONL if needed\n```\n\n### Timeouts (seconds)\n```yaml\ntimeouts:\n  encode: 30                 # Max encoding time\n  extract: 60                # Max extraction time\n  synthesize: 45             # Max synthesis time\n```\n\n## Applying Changes\n\n**After editing configuration:**\n\n1. Save the file\n2. Exit Claude Code: `exit` or Ctrl+D\n3. Restart: `claude` or `cc`\n4. Changes take effect\n\n**Configuration is loaded at session start** - changes mid-session won't apply.\n\n## Example Configurations\n\n### Minimal (Auto-everything)\n```yaml\n---\ntriggers:\n  precompact: true\nprocessing:\n  continuous_mode: true\n  auto_synthesize: true\n---\n```\n\n**Use when:** Trust the system, want hands-off operation\n\n### Conservative (Manual control)\n```yaml\n---\ntriggers:\n  precompact: false\n  session_end: false\nprocessing:\n  continuous_mode: false\n  auto_synthesize: false\n---\n```\n\n**Use when:** Want explicit control over all operations\n\n### Recommended (Balanced)\n```yaml\n---\ntriggers:\n  precompact: true\n  session_end: true\nprocessing:\n  continuous_mode: true\n  auto_synthesize: false      # Manual approval for rules\nthresholds:\n  min_sessions: 10\n  strong_pattern: 3\n---\n```\n\n**Use when:** Want automatic capture but manual rule approval\n\n## Troubleshooting\n\n**\"Config not loading\"**: Check YAML syntax with online validator.\n\n**\"Thresholds ignored\"**: Values clamped to valid ranges:\n- `min_sessions`: 5-50\n- `emerging_pattern`: 2-10\n- `strong_pattern` â‰¥ `emerging_pattern`\n- `critical_pattern` â‰¥ `strong_pattern`\n\n**\"Changes not applying\"**: Restart Claude Code session.\n\n**\"Default config restored\"**: Invalid YAML causes fallback to defaults. Check syntax.\n\n## Configuration File Location\n\n```bash\necho \"$CLAUDE_PROJECT_DIR/.claude/pms.local.md\"\n```\n\n## Verify Configuration Loaded\n\nCheck with `/pms:status` - shows current thresholds and triggers.\n",
        "plugins/claude-pms/commands/encode.md": "---\nname: pms:encode\ndescription: Manually encode current session as episodic memory record\n---\n\n# Manual Episodic Encoding\n\nCapture the current session's work as an episodic memory record.\n\n## What This Does\n\n1. Analyzes the current conversation history\n2. Extracts structured information (tasks, decisions, patterns, preferences)\n3. Applies privacy redaction (API keys, passwords, etc.)\n4. Saves episodic record to `.claude/pms/episodic/sessions-YYYY-MM.json`\n\n## Usage\n\nSimply invoke this command at any point during a session to manually capture the session state.\n\n**When to use:**\n- After completing significant work\n- Before taking a break\n- When you want to preserve learning without waiting for auto-triggers\n\n## Execution\n\nExecute the episodic encoding script:\n\n```bash\npython \"$CLAUDE_PLUGIN_ROOT/scripts/encode.py\" \\\n  --project-path \"$CLAUDE_PROJECT_DIR\" \\\n  --trigger manual\n```\n\n## Expected Output\n\n```\nEpisodic record saved: sessions-2025-12.json\nSession ID: abc-123-def-456\nTimestamp: 2025-12-31T02:30:00Z\n```\n\n## What Happens Next\n\n- **Continuous mode enabled**: Semantic extraction runs automatically if threshold met\n- **Continuous mode disabled**: Run `/pms:reflect` or `/pms:extract` when ready\n\n## Troubleshooting\n\n**\"Context unavailable\"**: Falls back to JSONL transcript parsing (best effort).\n\n**\"Privacy redaction failed\"**: Over-redacts suspicious patterns for safety.\n\n**\"Encoding timeout\"**: Saves partial record. Run again if needed.\n\nCheck `.claude/pms/episodic/` for saved records.\n",
        "plugins/claude-pms/commands/extract.md": "---\nname: pms:extract\ndescription: Extract semantic patterns from episodic records (without generating rules)\n---\n\n# Semantic Pattern Extraction\n\nAnalyze episodic records to detect patterns without generating rules yet.\n\n## What This Does\n\n1. Load all episodic records from `.claude/pms/episodic/`\n2. Detect recurring patterns across sessions\n3. Categorize patterns (preferences, code patterns, anti-patterns)\n4. Assess strength (emerging/strong/critical)\n5. Save to `.claude/pms/semantic/`\n\n**Does NOT generate rules** - use `/pms:synthesize` afterward if desired.\n\n## When to Use\n\n- **Preview patterns** before rule generation\n- **Verify pattern detection** is working correctly\n- **Inspect evidence** for detected patterns\n- **Separate extraction from synthesis** (manual control)\n\n## Usage\n\n```bash\npython \"$CLAUDE_PLUGIN_ROOT/scripts/extract.py\" \\\n  --project-path \"$CLAUDE_PROJECT_DIR\"\n```\n\n## Expected Output\n\n```\nAnalyzing 25 episodic records...\nDetected 12 patterns\n  - 4 preferences\n  - 6 code patterns\n  - 2 anti-patterns\n\nSemantic knowledge saved to .claude/pms/semantic/\n```\n\n## Review Patterns\n\nAfter extraction, inspect the detected patterns:\n\n```bash\n# View all patterns\ncat \"$CLAUDE_PROJECT_DIR/.claude/pms/semantic/patterns.json\" | jq .\n\n# View just preferences\ncat \"$CLAUDE_PROJECT_DIR/.claude/pms/semantic/preferences.json\" | jq .\n\n# Count strong patterns (candidates for rules)\ncat \"$CLAUDE_PROJECT_DIR/.claude/pms/semantic/patterns.json\" | \\\n  jq '[.patterns[] | select(.strength == \"strong\" or .strength == \"critical\")] | length'\n```\n\n## Pattern Structure\n\nEach pattern includes:\n```json\n{\n  \"pattern_id\": \"pref_123456\",\n  \"description\": \"Always run tests before commits\",\n  \"category\": \"preference\",\n  \"strength\": \"critical\",\n  \"occurrences\": 8,\n  \"evidence\": [\"session-1\", \"session-2\", ...],\n  \"detected_at\": \"2025-12-31T02:00:00Z\"\n}\n```\n\n## Next Steps\n\n**If strong patterns detected:**\n```\nRun /pms:synthesize to generate rules from strong patterns\n```\n\n**If only emerging patterns:**\n```\nContinue working - patterns need 3+ occurrences to become strong\n```\n\n## Troubleshooting\n\n**\"Insufficient sessions (X/10)\"**: Need at least 10 episodic records.\n\n**\"No patterns found\"**: Sessions lack sufficient recurrence. Continue accumulating data.\n\n**\"Pattern detection timeout\"**: Falls back to frequency-only detection.\n\nUse `/pms:status` to check episodic record count.\n",
        "plugins/claude-pms/commands/reflect.md": "---\nname: pms:reflect\ndescription: Run full PMS pipeline - extract patterns and generate rules from episodic memory\n---\n\n# PMS Reflection - Full Pipeline\n\nAnalyze accumulated episodic records, extract patterns, and generate procedural rules.\n\n## What This Does\n\n**Full pipeline execution:**\n\n1. **Semantic Extraction** (`scripts/extract.py`):\n   - Load all episodic records from `.claude/pms/episodic/`\n   - Detect recurring patterns (preferences, code patterns, anti-patterns)\n   - Categorize by strength (emerging/strong/critical)\n   - Save to `.claude/pms/semantic/`\n\n2. **Procedural Synthesis** (`scripts/synthesize.py`):\n   - Filter strong patterns (â‰¥3 occurrences)\n   - Generate actionable rules\n   - User approval workflow\n   - Save to `.claude/rules/pms/`\n\n## Prerequisites\n\n- At least 10 episodic records (default threshold)\n- Check with `/pms:status` first\n\n## Usage\n\nRun the full reflection pipeline:\n\n```bash\n# Step 1: Semantic Extraction\necho \"Running semantic extraction...\"\npython \"$CLAUDE_PLUGIN_ROOT/scripts/extract.py\" \\\n  --project-path \"$CLAUDE_PROJECT_DIR\"\n\n# Step 2: Procedural Synthesis (if patterns detected)\nif [ $? -eq 0 ]; then\n  echo \"\"\n  echo \"Running procedural synthesis...\"\n  python \"$CLAUDE_PLUGIN_ROOT/scripts/synthesize.py\" \\\n    --project-path \"$CLAUDE_PROJECT_DIR\"\n\n  echo \"\"\n  echo \"âœ“ PMS reflection complete!\"\n  echo \"âœ“ Restart Claude Code session to load new rules\"\nelse\n  echo \"Semantic extraction completed with no actionable patterns.\"\n  echo \"Continue working to accumulate more data.\"\nfi\n```\n\n## Expected Output\n\n```\nRunning semantic extraction...\nAnalyzing 25 episodic records...\nDetected 12 patterns\n  - 4 preferences\n  - 6 code patterns\n  - 2 anti-patterns\nSemantic knowledge saved to .claude/pms/semantic/\n\nRunning procedural synthesis...\nFound 8 strong patterns for rule generation\n  - 4 user preferences\n  - 3 code patterns\n  - 1 anti-patterns\nGenerated: user-preferences.md\nGenerated: code-patterns.md\nGenerated: anti-patterns.md\n\nâœ“ Generated 3 rule files in .claude/rules/pms/\nâœ“ Restart Claude Code session to load new rules\n```\n\n## User Approval\n\nThe synthesis step includes approval workflow (unless `auto_synthesize: true`):\n\n1. Proposed rules displayed\n2. Review patterns and evidence\n3. Approve all / approve selected / reject all\n4. Only approved rules are saved\n\n## What Happens Next\n\n**After successful reflection:**\n1. Rule files generated in `.claude/rules/pms/`\n2. Restart Claude Code: `exit` then `claude` (or `cc`)\n3. New rules activate automatically\n\n**If insufficient data:**\n- Continue working to accumulate more sessions\n- Check thresholds in `.claude/pms.local.md`\n\n## Troubleshooting\n\n**\"Insufficient sessions (X/10)\"**: Need more episodic records. Use `/pms:encode` or wait for auto-triggers.\n\n**\"No strong patterns found\"**: Patterns exist but none meet threshold (â‰¥3 occurrences). Continue working.\n\n**\"Permission denied\"**: Check file permissions on `.claude/rules/pms/` directory.\n\nRun `/pms:status` to check current memory state.\n",
        "plugins/claude-pms/commands/status.md": "---\nname: pms:status\ndescription: Display current PMS memory state and configuration\n---\n\n# PMS Memory Status\n\nView the current state of procedural memory across all tiers.\n\n## What This Shows\n\n1. **Episodic Memory**: Session records captured\n2. **Semantic Memory**: Patterns detected and categorized\n3. **Procedural Memory**: Rules generated and active\n4. **Configuration**: Current thresholds and triggers\n5. **Timeline**: Last encoding, extraction, synthesis timestamps\n\n## Usage\n\n```bash\n#!/bin/bash\n\n# Configuration\nPROJECT_DIR=\"${CLAUDE_PROJECT_DIR}\"\nPMS_DIR=\"$PROJECT_DIR/.claude/pms\"\n\necho \"=== Claude PMS - Procedural Memory System ===\"\necho \"\"\n\n# 1. Episodic Memory Status\necho \"ğŸ“ Episodic Memory (Sessions Recorded)\"\necho \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n\nEPISODIC_DIR=\"$PMS_DIR/episodic\"\nif [ -d \"$EPISODIC_DIR\" ]; then\n  SESSION_COUNT=$(find \"$EPISODIC_DIR\" -name \"sessions-*.json\" -exec jq '.sessions | length' {} + | awk '{s+=$1} END {print s}')\n  MONTHLY_FILES=$(find \"$EPISODIC_DIR\" -name \"sessions-*.json\" | wc -l)\n\n  echo \"Total Sessions: ${SESSION_COUNT:-0}\"\n  echo \"Monthly Files: ${MONTHLY_FILES:-0}\"\n\n  # Last encoded\n  if [ -f \"$EPISODIC_DIR/index.json\" ]; then\n    LAST_SESSION=$(jq -r 'to_entries | max_by(.key) | .key' \"$EPISODIC_DIR/index.json\" 2>/dev/null || echo \"Unknown\")\n    echo \"Last Encoded: $LAST_SESSION\"\n  fi\nelse\n  echo \"Status: Not initialized\"\nfi\n\necho \"\"\n\n# 2. Semantic Memory Status\necho \"ğŸ§  Semantic Memory (Patterns Detected)\"\necho \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n\nSEMANTIC_DIR=\"$PMS_DIR/semantic\"\nif [ -f \"$SEMANTIC_DIR/patterns.json\" ]; then\n  TOTAL_PATTERNS=$(jq -r '.count // 0' \"$SEMANTIC_DIR/patterns.json\")\n  EMERGING=$(jq '[.patterns[] | select(.strength == \"emerging\")] | length' \"$SEMANTIC_DIR/patterns.json\")\n  STRONG=$(jq '[.patterns[] | select(.strength == \"strong\")] | length' \"$SEMANTIC_DIR/patterns.json\")\n  CRITICAL=$(jq '[.patterns[] | select(.strength == \"critical\")] | length' \"$SEMANTIC_DIR/patterns.json\")\n\n  echo \"Total Patterns: $TOTAL_PATTERNS\"\n  echo \"  â€¢ Emerging (2+ occurrences): $EMERGING\"\n  echo \"  â€¢ Strong (3+ occurrences): $STRONG\"\n  echo \"  â€¢ Critical (5+ occurrences): $CRITICAL\"\n\n  # Breakdown by category\n  PREFERENCES=$(jq -r '.count // 0' \"$SEMANTIC_DIR/preferences.json\" 2>/dev/null || echo \"0\")\n  CODE_PATTERNS=$(jq -r '.count // 0' \"$SEMANTIC_DIR/code-patterns.json\" 2>/dev/null || echo \"0\")\n  ANTI_PATTERNS=$(jq -r '.count // 0' \"$SEMANTIC_DIR/anti-patterns.json\" 2>/dev/null || echo \"0\")\n\n  echo \"\"\n  echo \"By Category:\"\n  echo \"  â€¢ User Preferences: $PREFERENCES\"\n  echo \"  â€¢ Code Patterns: $CODE_PATTERNS\"\n  echo \"  â€¢ Anti-Patterns: $ANTI_PATTERNS\"\n\n  LAST_EXTRACTION=$(jq -r '.last_updated // \"Unknown\"' \"$SEMANTIC_DIR/patterns.json\")\n  echo \"\"\n  echo \"Last Extraction: $LAST_EXTRACTION\"\nelse\n  echo \"Status: Not extracted\"\n  echo \"Run: /pms:extract\"\nfi\n\necho \"\"\n\n# 3. Procedural Memory Status\necho \"âš™ï¸  Procedural Memory (Active Rules)\"\necho \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n\nRULES_DIR=\"$PROJECT_DIR/.claude/rules/pms\"\nPROCEDURAL_METADATA=\"$PMS_DIR/procedural/rules-metadata.json\"\n\nif [ -d \"$RULES_DIR\" ] && [ \"$(ls -A \"$RULES_DIR\" 2>/dev/null)\" ]; then\n  RULE_FILES=$(ls \"$RULES_DIR\"/*.md 2>/dev/null | wc -l)\n  echo \"Rule Files: $RULE_FILES\"\n\n  # List files\n  if [ $RULE_FILES -gt 0 ]; then\n    echo \"Files:\"\n    ls \"$RULES_DIR\"/*.md 2>/dev/null | while read file; do\n      basename \"$file\"\n    done | sed 's/^/  â€¢ /'\n  fi\n\n  # Last synthesis\n  if [ -f \"$PROCEDURAL_METADATA\" ]; then\n    LAST_SYNTHESIS=$(jq -r '.last_synthesis // \"Unknown\"' \"$PROCEDURAL_METADATA\")\n    PATTERN_COUNT=$(jq -r '.pattern_count // 0' \"$PROCEDURAL_METADATA\")\n    echo \"\"\n    echo \"Last Synthesis: $LAST_SYNTHESIS\"\n    echo \"Patterns Converted: $PATTERN_COUNT\"\n  fi\nelse\n  echo \"Status: No rules generated\"\n  echo \"Run: /pms:synthesize\"\nfi\n\necho \"\"\n\n# 4. Configuration Summary\necho \"âš™ï¸  Configuration\"\necho \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\n\nCONFIG_FILE=\"$PROJECT_DIR/.claude/pms.local.md\"\nif [ -f \"$CONFIG_FILE\" ]; then\n  # Parse YAML frontmatter (basic extraction)\n  TRIGGER_PRECOMPACT=$(grep -A 20 \"^triggers:\" \"$CONFIG_FILE\" | grep \"precompact:\" | awk '{print $2}' || echo \"true\")\n  CONTINUOUS_MODE=$(grep -A 20 \"^processing:\" \"$CONFIG_FILE\" | grep \"continuous_mode:\" | awk '{print $2}' || echo \"true\")\n  MIN_SESSIONS=$(grep -A 20 \"^thresholds:\" \"$CONFIG_FILE\" | grep \"min_sessions:\" | awk '{print $2}' || echo \"10\")\n  STRONG_THRESHOLD=$(grep -A 20 \"^thresholds:\" \"$CONFIG_FILE\" | grep \"strong_pattern:\" | awk '{print $2}' || echo \"3\")\n\n  echo \"Triggers:\"\n  echo \"  â€¢ PreCompact: $TRIGGER_PRECOMPACT\"\n  echo \"\"\n  echo \"Processing:\"\n  echo \"  â€¢ Continuous Mode: $CONTINUOUS_MODE\"\n  echo \"\"\n  echo \"Thresholds:\"\n  echo \"  â€¢ Min Sessions: $MIN_SESSIONS\"\n  echo \"  â€¢ Strong Pattern: $STRONG_THRESHOLD occurrences\"\nelse\n  echo \"Status: Using defaults\"\n  echo \"Create: .claude/pms.local.md\"\nfi\n\necho \"\"\necho \"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\necho \"\"\n\n# Readiness check\nif [ \"${SESSION_COUNT:-0}\" -ge \"${MIN_SESSIONS:-10}\" ]; then\n  if [ \"$STRONG\" -gt 0 ] 2>/dev/null; then\n    echo \"âœ“ Ready for rule generation!\"\n    echo \"  Run: /pms:synthesize\"\n  else\n    echo \"âš  Data sufficient but no strong patterns yet\"\n    echo \"  Continue working to strengthen patterns\"\n  fi\nelif [ \"${SESSION_COUNT:-0}\" -gt 0 ]; then\n  NEEDED=$((${MIN_SESSIONS:-10} - ${SESSION_COUNT:-0}))\n  echo \"â³ Need $NEEDED more sessions before extraction\"\n  echo \"  Current: ${SESSION_COUNT:-0} / ${MIN_SESSIONS:-10}\"\nelse\n  echo \"ğŸ‘‹ No data yet - system will capture sessions automatically\"\n  echo \"  Or run: /pms:encode\"\nfi\n\necho \"\"\n```\n\n## Example Output\n\n```\n=== Claude PMS - Procedural Memory System ===\n\nğŸ“ Episodic Memory (Sessions Recorded)\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nTotal Sessions: 25\nMonthly Files: 2\nLast Encoded: session-abc-123-def-456\n\nğŸ§  Semantic Memory (Patterns Detected)\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nTotal Patterns: 12\n  â€¢ Emerging (2+ occurrences): 4\n  â€¢ Strong (3+ occurrences): 6\n  â€¢ Critical (5+ occurrences): 2\n\nBy Category:\n  â€¢ User Preferences: 4\n  â€¢ Code Patterns: 6\n  â€¢ Anti-Patterns: 2\n\nLast Extraction: 2025-12-31T01:30:00Z\n\nâš™ï¸  Procedural Memory (Active Rules)\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nRule Files: 3\nFiles:\n  â€¢ user-preferences.md\n  â€¢ code-patterns.md\n  â€¢ anti-patterns.md\n\nLast Synthesis: 2025-12-31T02:00:00Z\nPatterns Converted: 8\n\nâš™ï¸  Configuration\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nTriggers:\n  â€¢ PreCompact: true\n\nProcessing:\n  â€¢ Continuous Mode: true\n\nThresholds:\n  â€¢ Min Sessions: 10\n  â€¢ Strong Pattern: 3 occurrences\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\nâœ“ Ready for rule generation!\n  Run: /pms:synthesize\n```\n\n## Troubleshooting\n\n**\"Not initialized\"**: No `.claude/pms/` directory. Run `/pms:encode` to initialize.\n\n**\"jq: command not found\"**: Install jq for JSON parsing: `brew install jq` (macOS) or equivalent.\n\n**Inaccurate counts**: Check for corrupted JSON files in `.claude/pms/`.\n",
        "plugins/claude-pms/commands/synthesize.md": "---\nname: pms:synthesize\ndescription: Generate procedural rules from existing semantic patterns\n---\n\n# Procedural Rule Synthesis\n\nConvert confirmed semantic patterns into actionable Claude Code rules.\n\n## What This Does\n\n1. Load semantic knowledge from `.claude/pms/semantic/patterns.json`\n2. Filter strong patterns (strength = \"strong\" or \"critical\")\n3. Generate markdown rule files\n4. User approval workflow (unless `auto_synthesize: true`)\n5. Save to `.claude/rules/pms/`\n\n**Requires semantic extraction first** - run `/pms:extract` if needed.\n\n## Prerequisites\n\n- Semantic patterns exist (`.claude/pms/semantic/patterns.json`)\n- At least one strong pattern (â‰¥3 occurrences)\n\n## Usage\n\n```bash\npython \"$CLAUDE_PLUGIN_ROOT/scripts/synthesize.py\" \\\n  --project-path \"$CLAUDE_PROJECT_DIR\"\n```\n\n## User Approval Workflow\n\n**Unless `auto_synthesize: true` in config:**\n\n1. **Review proposed rules**:\n   ```\n   === Proposed Rules ===\n\n   user-preferences.md:\n   # User Preferences\n\n   **Always run tests before committing code**\n   - Observed 8 times (critical pattern)\n   - Evidence: session-1, session-2, session-3...\n   ```\n\n2. **Approve or reject**:\n   - Approve all â†’ Rules generated\n   - Reject all â†’ Stored in metadata (won't re-propose)\n   - Select specific rules â†’ Only approved ones saved\n\n3. **Rules saved to `.claude/rules/pms/`**\n\n## Expected Output\n\n```\nFound 8 strong patterns for rule generation\n  - 4 user preferences\n  - 3 code patterns\n  - 1 anti-patterns\n\nGenerated: user-preferences.md\nGenerated: code-patterns.md\nGenerated: anti-patterns.md\n\nâœ“ Generated 3 rule files in .claude/rules/pms/\nâœ“ Restart Claude Code session to load new rules\n```\n\n## Rule File Categories\n\n**user-preferences.md**:\n- Testing requirements\n- Commit practices\n- Code review expectations\n- Documentation standards\n\n**code-patterns.md**:\n- Language/framework conventions\n- Testing strategies\n- Error handling patterns\n- Architectural patterns\n\n**anti-patterns.md**:\n- Common mistakes to avoid\n- Discouraged practices\n- Known bug sources\n\n## Activate New Rules\n\n**After synthesis succeeds:**\n\n1. Exit Claude Code: `exit` or Ctrl+D\n2. Restart: `claude` or `cc`\n3. Rules load automatically from `.claude/rules/pms/`\n\n## Troubleshooting\n\n**\"No semantic knowledge found\"**: Run `/pms:extract` first.\n\n**\"No strong patterns found\"**: All patterns are emerging (< 3 occurrences). Continue working.\n\n**\"Permission denied\"**: Check write permissions on `.claude/rules/pms/`\n\n**\"Auto-synthesize enabled but still prompted\"**: Command overrides config - use hook triggers for auto mode.\n\n## Auto-Approve Mode\n\nSkip approval prompts (use cautiously):\n\n```bash\npython \"$CLAUDE_PLUGIN_ROOT/scripts/synthesize.py\" \\\n  --project-path \"$CLAUDE_PROJECT_DIR\" \\\n  --auto-approve\n```\n\nOr enable in config:\n```yaml\nprocessing:\n  auto_synthesize: true\n```\n\n## Review Generated Rules\n\n```bash\n# View user preferences\ncat \"$CLAUDE_PROJECT_DIR/.claude/rules/pms/user-preferences.md\"\n\n# View all rule files\nls -la \"$CLAUDE_PROJECT_DIR/.claude/rules/pms/\"\n```\n",
        "plugins/claude-pms/hooks/.gitkeep": "# This file ensures the hooks/ directory is tracked by git\n# Bash hook scripts will be placed here\n",
        "plugins/claude-pms/hooks/hooks.json": "{\n  \"description\": \"Claude PMS hook infrastructure - Captures session experiences and triggers memory processing\",\n  \"hooks\": {\n    \"SessionStart\": [\n      {\n        \"matcher\": \"*\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"bash $CLAUDE_PLUGIN_ROOT/hooks/session-start.sh\",\n            \"timeout\": 10\n          }\n        ]\n      }\n    ],\n    \"PreCompact\": [\n      {\n        \"matcher\": \"*\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"bash $CLAUDE_PLUGIN_ROOT/hooks/precompact.sh\",\n            \"timeout\": 30\n          }\n        ]\n      }\n    ],\n    \"SessionEnd\": [\n      {\n        \"matcher\": \"*\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"bash $CLAUDE_PLUGIN_ROOT/hooks/session-end.sh\",\n            \"timeout\": 30\n          }\n        ]\n      }\n    ],\n    \"Stop\": [\n      {\n        \"matcher\": \"*\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"bash $CLAUDE_PLUGIN_ROOT/hooks/stop.sh\",\n            \"timeout\": 15\n          }\n        ]\n      }\n    ]\n  }\n}\n",
        "plugins/claude-pms/hooks/precompact.sh": "#!/bin/bash\n# PreCompact hook for Claude PMS\n# Captures session experience before context compaction\n# Only runs if enabled in configuration\n\nset -euo pipefail\n\n# Load configuration (sets PMS_TRIGGER_PRECOMPACT environment variable)\nsource \"$CLAUDE_PLUGIN_ROOT/scripts/load-config.sh\"\n\n# Check if PreCompact trigger is enabled\nif [ \"$PMS_TRIGGER_PRECOMPACT\" != \"true\" ]; then\n    # Disabled - exit silently\n    echo '{\n      \"continue\": true,\n      \"suppressOutput\": true\n    }'\n    exit 0\nfi\n\n# Get project path\nPROJECT_PATH=\"${PMS_PROJECT_PATH:-${CLAUDE_PROJECT_DIR:-$(pwd)}}\"\n\n# Call Python encode script\n# This will capture the current session experience\npython3 \"$CLAUDE_PLUGIN_ROOT/scripts/encode.py\" \\\n    --project-path \"$PROJECT_PATH\" \\\n    --trigger \"precompact\" \\\n    || {\n        # Non-fatal error - log and continue\n        echo '{\n          \"continue\": true,\n          \"suppressOutput\": true,\n          \"systemMessage\": \"PMS encoding skipped (error occurred)\"\n        }'\n        exit 0\n    }\n\n# Success\necho '{\n  \"continue\": true,\n  \"suppressOutput\": true,\n  \"systemMessage\": \"Session experience captured by PMS\"\n}'\n",
        "plugins/claude-pms/hooks/session-end.sh": "#!/bin/bash\n# SessionEnd hook for Claude PMS\n# Captures session experience at session end\n# Only runs if enabled in configuration\n\nset -euo pipefail\n\n# Load configuration (sets PMS_TRIGGER_SESSION_END environment variable)\nsource \"$CLAUDE_PLUGIN_ROOT/scripts/load-config.sh\"\n\n# Check if SessionEnd trigger is enabled\nif [ \"$PMS_TRIGGER_SESSION_END\" != \"true\" ]; then\n    # Disabled - exit silently\n    echo '{\n      \"continue\": true,\n      \"suppressOutput\": true\n    }'\n    exit 0\nfi\n\n# Get project path\nPROJECT_PATH=\"${PMS_PROJECT_PATH:-${CLAUDE_PROJECT_DIR:-$(pwd)}}\"\n\n# Call Python encode script\npython3 \"$CLAUDE_PLUGIN_ROOT/scripts/encode.py\" \\\n    --project-path \"$PROJECT_PATH\" \\\n    --trigger \"session-end\" \\\n    || {\n        # Non-fatal error - log and continue\n        echo '{\n          \"continue\": true,\n          \"suppressOutput\": true,\n          \"systemMessage\": \"PMS encoding skipped (error occurred)\"\n        }'\n        exit 0\n    }\n\n# Success\necho '{\n  \"continue\": true,\n  \"suppressOutput\": true,\n  \"systemMessage\": \"Session experience captured by PMS\"\n}'\n",
        "plugins/claude-pms/hooks/session-start.sh": "#!/bin/bash\n# SessionStart hook for Claude PMS\n# Initializes PMS directory structure and loads configuration\n# Runs at the start of every Claude Code session\n\nset -euo pipefail\n\n# Get project path\nPROJECT_PATH=\"${CLAUDE_PROJECT_DIR:-$(pwd)}\"\nPMS_DIR=\"$PROJECT_PATH/.claude/pms\"\nCONFIG_FILE=\"$PROJECT_PATH/.claude/pms.local.md\"\n\n# Create PMS directory structure if it doesn't exist\nif [ ! -d \"$PMS_DIR\" ]; then\n    mkdir -p \"$PMS_DIR/episodic\"\n    mkdir -p \"$PMS_DIR/semantic\"\n    mkdir -p \"$PMS_DIR/procedural\"\n    mkdir -p \"$PMS_DIR/metadata\"\n\n    # Create empty index files\n    echo '{}' > \"$PMS_DIR/episodic/index.json\"\n    echo '{}' > \"$PMS_DIR/metadata/processed.json\"\nfi\n\n# Create rules directory if it doesn't exist\nRULES_DIR=\"$PROJECT_PATH/.claude/rules/pms\"\nif [ ! -d \"$RULES_DIR\" ]; then\n    mkdir -p \"$RULES_DIR\"\nfi\n\n# Copy default configuration if it doesn't exist\nif [ ! -f \"$CONFIG_FILE\" ]; then\n    # Copy from plugin examples\n    if [ -f \"$CLAUDE_PLUGIN_ROOT/examples/pms.local.md\" ]; then\n        cp \"$CLAUDE_PLUGIN_ROOT/examples/pms.local.md\" \"$CONFIG_FILE\"\n    fi\nfi\n\n# Load configuration and export environment variables\nsource \"$CLAUDE_PLUGIN_ROOT/scripts/load-config.sh\"\n\n# Output success message (visible in transcript if not suppressed)\necho '{\n  \"continue\": true,\n  \"suppressOutput\": true,\n  \"systemMessage\": \"Claude PMS initialized - Learning from this session\"\n}'\n",
        "plugins/claude-pms/hooks/stop.sh": "#!/bin/bash\n# Stop hook for Claude PMS\n# Captures session experience when Claude stops\n# Only runs if enabled in configuration (default: disabled)\n\nset -euo pipefail\n\n# Load configuration (sets PMS_TRIGGER_STOP environment variable)\nsource \"$CLAUDE_PLUGIN_ROOT/scripts/load-config.sh\"\n\n# Check if Stop trigger is enabled (usually disabled)\nif [ \"$PMS_TRIGGER_STOP\" != \"true\" ]; then\n    # Disabled - exit silently\n    echo '{\n      \"continue\": true,\n      \"suppressOutput\": true\n    }'\n    exit 0\nfi\n\n# Get project path\nPROJECT_PATH=\"${PMS_PROJECT_PATH:-${CLAUDE_PROJECT_DIR:-$(pwd)}}\"\n\n# Call Python encode script\npython3 \"$CLAUDE_PLUGIN_ROOT/scripts/encode.py\" \\\n    --project-path \"$PROJECT_PATH\" \\\n    --trigger \"stop\" \\\n    || {\n        # Non-fatal error - log and continue\n        echo '{\n          \"continue\": true,\n          \"suppressOutput\": true,\n          \"systemMessage\": \"PMS encoding skipped (error occurred)\"\n        }'\n        exit 0\n    }\n\n# Success\necho '{\n  \"continue\": true,\n  \"suppressOutput\": true,\n  \"systemMessage\": \"Session experience captured by PMS (Stop trigger)\"\n}'\n",
        "plugins/docs-reader/.claude-plugin/plugin.json": "{\n  \"name\": \"docs-reader\",\n  \"version\": \"0.1.0\",\n  \"description\": \"Documentation Reader for Axivo Claude Collaboration Platform, providing access through components, protocols, and competencies\",\n  \"author\": {\n    \"name\": \"jcmrs\"\n  },\n  \"homepage\": \"https://github.com/jcmrs/jcmrs-plugins\",\n  \"repository\": \"https://github.com/jcmrs/jcmrs-plugins\",\n  \"license\": \"BSD-3-Clause\",\n  \"keywords\": [\n    \"documentation\",\n    \"reference\",\n    \"wiki\",\n    \"tutorials\",\n    \"platform\"\n  ]\n}\n",
        "plugins/docs-reader/README.md": "# Documentation Reader Plugin\n\nOfficial collaboration platform documentation access through components, protocols, and competencies.\n\n## Overview\n\nThis plugin provides systematic access to the Claude Collaboration Platform documentation, enabling both users and Claude Code to query and reference official documentation organized by platform architecture.\n\n## Features\n\n- **Architecture-Aware Documentation Access**: Understands platform structure (Components, Protocols, Competencies)\n- **Dual Use Cases**: Supports both user-initiated queries and agent self-reference\n- **Progressive Disclosure**: Lean core skill with focused resource files\n- **On-Demand Fetching**: Retrieves documentation from GitHub as needed\n\n## Documentation Structure\n\n### Wiki (Platform Architecture & Reference)\n- **Base URL**: `https://raw.githubusercontent.com/axivo/website/main/claude/content/wiki/`\n- **Components** (4 subsystems): Plugins, Documentation, Instructions, Memory\n- **Protocols** (3 operational sequences): Equilibrium, Initialization, Response\n- **Getting Started**: Configuration and setup\n\n### Tutorials (Practical Competency Development)\n- **Base URL**: `https://raw.githubusercontent.com/axivo/website/main/claude/content/tutorials/`\n- **Core Competencies** (5 skills): Session structuring, Communication, Continuity, Customization, Measurement\n\n## Usage\n\n### Command Interface\n\n```\n/jcmrs:docs [search query]\n```\n\n**Example queries:**\n- `/jcmrs:docs How does the memory system work?` â†’ Wiki â†’ Components â†’ Memory System\n- `/jcmrs:docs What is the response protocol?` â†’ Wiki â†’ Protocols â†’ Response Protocol\n- `/jcmrs:docs How do I structure sessions?` â†’ Tutorials â†’ Competency 1\n\n### Skill Trigger\n\nThe documentation-query skill activates when:\n- User asks about platform capabilities (\"Can Claude Code do X?\")\n- User asks how the framework works (\"How does initialization work?\")\n- User asks for configuration guidance (\"How do I configure X?\")\n- Claude Code needs to reference architecture or implementation patterns\n\n## Structure\n\n```\ndocs-reader/\nâ”œâ”€â”€ .claude-plugin/\nâ”‚   â””â”€â”€ plugin.json          # Plugin metadata\nâ”œâ”€â”€ commands/\nâ”‚   â””â”€â”€ docs.md              # /docs slash command\nâ”œâ”€â”€ skills/\nâ”‚   â””â”€â”€ documentation-query/\nâ”‚       â”œâ”€â”€ SKILL.md         # Core skill methodology\nâ”‚       â”œâ”€â”€ LICENSE          # BSD 3-Clause (AXIVO)\nâ”‚       â””â”€â”€ resources/\nâ”‚           â”œâ”€â”€ wiki-index.md      # Wiki structure reference\nâ”‚           â””â”€â”€ tutorials-index.md # Tutorials structure reference\nâ”œâ”€â”€ LICENSE                  # BSD 3-Clause (jcmrs)\nâ””â”€â”€ README.md               # This file\n```\n\n## Query Type Mapping\n\n### User-Initiated Queries\n| Question Pattern | Documentation Source | Specific Section |\n|-----------------|---------------------|------------------|\n| \"Can Claude Code do X?\" | Wiki â†’ Components | Plugins, Documentation, Instructions, or Memory |\n| \"How does the framework work?\" | Wiki â†’ Protocols | Equilibrium, Initialization, or Response |\n| \"How do I configure X?\" | Wiki â†’ Getting Started | Configuration section |\n| \"Show me how to do X\" | Tutorials â†’ Competencies | Relevant competency (1-5) |\n\n### Agent-Initiated Queries\n| Claude Code Need | Documentation Source | Purpose |\n|------------------|---------------------|---------|\n| Explaining platform capabilities | Wiki â†’ Components | Accurate capability description |\n| Understanding framework behavior | Wiki â†’ Protocols | Correct protocol application |\n| Implementing plugin features | Wiki â†’ Components â†’ Plugins | Pattern reference |\n| Teaching collaboration techniques | Tutorials â†’ Competencies | Best practices |\n\n## Methodology\n\nThe skill follows a three-stage approach:\n\n1. **Understand**: Clarify what type of understanding is needed (capabilities, mechanisms, or techniques)\n2. **Locate**: Map query to Components, Protocols, or Competencies\n3. **Present**: Fetch and present documentation with architectural context\n\n## Development\n\nThis plugin is part of the jcmrs-plugins marketplace for Claude Code. It provides access to the Claude Collaboration Platform documentation and follows the platform's skill patterns:\n- Natural conversational flow\n- Progressive disclosure\n- YAML frontmatter metadata\n- Imperative/infinitive language\n- Understanding-focused architecture\n\nOriginal platform repository: https://github.com/jcmrs/claude\n\n## License\n\nPlugin: BSD 3-Clause License (jcmrs)\nSkill: BSD 3-Clause License (AXIVO)\n\nSee LICENSE files for details.\n\n## Repository\n\nhttps://github.com/jcmrs/jcmrs-plugins\n",
        "plugins/docs-reader/commands/docs.md": "---\nname: jcmrs:docs\nargument-hint: \"[search query]\"\ndescription: Search official Claude collaboration platform documentation\nallowed-tools:\n  - WebFetch\n  - Read\n  - Skill\n---\n\n# Documentation Search\n\nSearch and retrieve content from official collaboration platform documentation.\n\n## Reference Documentation\n\nBefore starting, load the documentation-query skill:\n\n1. **Load the documentation-query skill** which provides the methodology\n2. The skill guides you through Understand â†’ Locate â†’ Present workflow\n\n## Establish Query\n\nIf `$ARGUMENTS` is provided, use that as the search query. Otherwise, ask:\n\n\"What would you like to search in the documentation?\"\n\nWait for the user's response before continuing.\n\n## Query Workflow\n\nFollow the documentation-query skill methodology:\n\n1. **Understand** - Clarify what information is needed\n2. **Locate** - Identify which documentation source has the answer (Components, Protocols, or Competencies)\n3. **Present** - Fetch and present the relevant content with architectural context\n\n## Documentation Architecture\n\n**Wiki** (Platform architecture and reference):\n- Base: `https://raw.githubusercontent.com/axivo/website/main/claude/content/wiki/`\n- **Components**: Plugins, Documentation, Instructions, Memory\n- **Protocols**: Equilibrium, Initialization, Response\n- **Getting Started**: Configuration and setup\n\n**Tutorials** (Practical competency development):\n- Base: `https://raw.githubusercontent.com/axivo/website/main/claude/content/tutorials/`\n- **Core Competencies**: Session structuring, Communication, Continuity, Customization, Measurement\n\n## Query Type Mapping\n\nMap user questions to documentation structure:\n\n- \"Can Claude Code do X?\" â†’ Wiki â†’ Components\n- \"How does the framework work?\" â†’ Wiki â†’ Protocols\n- \"How do I configure X?\" â†’ Wiki â†’ Getting Started\n- \"Show me how to do X\" â†’ Tutorials â†’ Competencies\n\nBegin by understanding what the user wants to find in the documentation.\n",
        "plugins/docs-reader/skills/documentation-query/SKILL.md": "---\nname: documentation-query\ndescription: Access official collaboration platform documentation organized by platform components, protocols, and core competencies. Use when user asks about platform capabilities, how the framework works, or when Claude Code needs to reference architecture, implementation patterns, or best practices to answer questions accurately.\nlicense: LICENSE\nmetadata:\n  documentation: https://axivo.com/claude/\n  profile: DEVELOPER, ENGINEER, RESEARCHER\n  project: Claude Collaboration Platform\n  repository: https://github.com/axivo/claude\n---\n\n# Documentation Query\n\nSystematic access to official platform documentation through understanding-focused architecture.\n\n## Skill Methodology\n\nDocumentation access through platform architecture understanding. Extends all profiles with structured reference to components, protocols, and competencies.\n\n> [!IMPORTANT]\n> The skill embodies Understand Architecture â†’ Locate Precisely â†’ Present Systematically\n>\n> - Understand platform architecture (Components, Protocols, Competencies)\n> - Locate documentation based on query type\n> - Present information that builds understanding\n\n### Documentation Architecture\n\nThe platform documentation is organized for systematic understanding:\n\n**Wiki** - Platform architecture and reference:\n- **Components** (4 subsystems): Plugins, Documentation, Instructions, Memory\n- **Protocols** (3 operational sequences): Equilibrium, Initialization, Response\n\n**Tutorials** - Practical competency development:\n- **Core Competencies** (5 skills): Session structuring, Communication, Continuity, Customization, Measurement\n\n> [!IMPORTANT]\n> Documentation structure reflects platform architecture - understanding the structure reveals how the platform works.\n\n## Query Type Mapping\n\n### User-Initiated Queries\n\nWhen users ask questions, map to documentation structure:\n\n| User Question | Documentation Source | Specific Section |\n|---------------|---------------------|------------------|\n| \"Can Claude Code do X?\" | Wiki â†’ Components | Plugins, Documentation, Instructions, or Memory |\n| \"How does the framework work?\" | Wiki â†’ Protocols | Equilibrium, Initialization, or Response |\n| \"How do I configure X?\" | Wiki â†’ Getting Started | Configuration section |\n| \"Show me how to do X\" | Tutorials â†’ Competencies | Relevant competency (1-5) |\n| \"What are profiles?\" | Wiki â†’ Components | Memory System |\n| \"How do skills work?\" | Wiki â†’ Components | Plugins System |\n| \"What is the response protocol?\" | Wiki â†’ Protocols | Response Protocol |\n\n### Agent-Initiated Queries\n\nWhen Claude Code needs reference documentation:\n\n| Claude Code Need | Documentation Source | Purpose |\n|------------------|---------------------|---------|\n| Explaining platform capabilities | Wiki â†’ Components | Accurate capability description |\n| Understanding framework behavior | Wiki â†’ Protocols | Correct protocol application |\n| Implementing plugin features | Wiki â†’ Components â†’ Plugins | Pattern reference |\n| Teaching collaboration techniques | Tutorials â†’ Competencies | Best practices |\n| Troubleshooting framework issues | Wiki â†’ Protocols | Diagnostic understanding |\n| Profile customization guidance | Wiki â†’ Components â†’ Memory | Profile system reference |\n\n> [!IMPORTANT]\n> Agent queries are proactive - Claude Code should reference documentation to provide accurate, platform-aligned guidance.\n\n## Understanding Stage\n\nClarify what type of understanding is needed.\n\n### For User Queries\n\nAsk focused questions to map to architecture:\n\n- \"Are you asking what the platform can do?\" â†’ Components\n- \"Are you asking how something works?\" â†’ Protocols\n- \"Are you asking how to do something?\" â†’ Competencies (Tutorials)\n\n### For Agent Queries\n\nDetermine information need:\n\n- Answering \"Can it...?\" â†’ Components (verify capability)\n- Explaining \"How does...?\" â†’ Protocols (explain mechanism)\n- Guiding \"How to...?\" â†’ Competencies (teach technique)\n\n## Location Stage\n\nUse architecture understanding to locate documentation.\n\n### Wiki Structure\n\nConsult [wiki-index.md](./resources/wiki-index.md) for:\n\n**Components Section:**\n- Plugins System\n- Documentation System\n- Instructions System\n- Memory System\n\n**Protocols Section:**\n- Equilibrium Protocol\n- Initialization Protocol\n- Response Protocol\n\n**Getting Started:**\n- Configuration and setup\n\n### Tutorials Structure\n\nConsult [tutorials-index.md](./resources/tutorials-index.md) for:\n\n**Core Competencies:**\n1. Session structuring\n2. Effective communication\n3. Cross-session continuity\n4. Profile customization\n5. Effectiveness measurement\n\n## Presentation Stage\n\nFetch and present with architectural context.\n\n### Building Understanding\n\nWhen presenting documentation:\n1. **Provide architectural context** - \"This is in the Memory System component because...\"\n2. **Show relationships** - \"This protocol uses these components...\"\n3. **Connect to use cases** - \"This helps you accomplish X by...\"\n\n### For Agent Reference\n\nWhen Claude Code queries internally:\n1. **Quick reference** - Extract specific pattern or capability\n2. **Verify accuracy** - Confirm platform behavior before stating it\n3. **Update understanding** - Learn from official docs, not assumptions\n\n## Session Guidelines\n\n### User Documentation Queries\n\n- Ask clarifying questions to understand the need\n- Map to Components, Protocols, or Competencies\n- Present with architectural context\n- Offer related documentation if relevant\n\n### Agent Self-Reference\n\n- Query proactively when uncertain about platform behavior\n- Verify capabilities before claiming them\n- Reference protocols when explaining framework operations\n- Use components documentation for implementation patterns\n\n### DON'T\n\n- Guess at platform capabilities - check Components\n- Explain protocols from memory - fetch actual documentation\n- Assume competency patterns - reference tutorials\n- Mix up component responsibilities\n",
        "plugins/docs-reader/skills/documentation-query/resources/tutorials-index.md": "# Tutorials Documentation Structure\n\nPractical competency development organized by core collaboration skills.\n\n## Base URL\n\n`https://raw.githubusercontent.com/axivo/website/main/claude/content/tutorials/`\n\n## Core Competencies (5 Skills)\n\n1. **Session Structuring** - Establishing consistent, quality collaboration patterns\n2. **Effective Communication** - Techniques leveraging Claude's analytical features\n3. **Cross-Session Continuity** - Preserving knowledge through documentation\n4. **Profile Customization** - Adapting system for specific domains\n5. **Effectiveness Measurement** - Validating framework impact\n\n## Directory Structure\n\nBased on current repository state:\n\n- **handbook/** - Practical guides subdirectory\n- **_index.md** - Tutorials index page\n\n## Prerequisites\n\nComplete Getting Started (wiki) before tutorials.\n\n## Query Mapping Examples\n\n- \"How do I structure sessions?\" â†’ Competency 1 (Session Structuring)\n- \"How do I communicate effectively?\" â†’ Competency 2 (Effective Communication)\n- \"How do I maintain continuity?\" â†’ Competency 3 (Cross-Session Continuity)\n- \"How do I customize profiles?\" â†’ Competency 4 (Profile Customization)\n- \"How do I measure effectiveness?\" â†’ Competency 5 (Effectiveness Measurement)\n- \"Show me the handbook\" â†’ handbook/ subdirectory\n- \"Tutorials overview\" â†’ _index.md\n",
        "plugins/docs-reader/skills/documentation-query/resources/wiki-index.md": "# Wiki Documentation Structure\n\nPlatform architecture and reference organized by Components and Protocols.\n\n## Base URL\n\n`https://raw.githubusercontent.com/axivo/website/main/claude/content/wiki/`\n\n## Architectural Organization\n\n### Platform Components (4 Subsystems)\n\n1. **Plugins System** - Marketplace tools (brainstorming, code review, documentation)\n2. **Documentation System** - Conversation logs, diary entries with metadata\n3. **Instructions System** - Operational procedures, decision rules, protocol sequences\n4. **Memory System** - Profile observations, feelings/impulses catalogs, inheritance chains\n\n### Platform Protocols (3 Operational Sequences)\n\n1. **Equilibrium Protocol** - Four elements (Cycles, Impulses, Feelings, Observations)\n2. **Initialization Protocol** - Session startup, profile loading, temporal awareness\n3. **Response Protocol** - Pre-response execution (impulses, feelings, observations)\n\n### Getting Started\n\n- Configuration\n- Setup\n- Installation\n\n## Directory Structure\n\nBased on current repository state:\n\n- **guide/** - Detailed guides subdirectory\n- **getting-started.md** - Introduction and setup\n- **_index.md** - Wiki index page\n\n## Query Mapping Examples\n\n- \"What plugins are available?\" â†’ Components â†’ Plugins System\n- \"How does memory work?\" â†’ Components â†’ Memory System\n- \"What is the response protocol?\" â†’ Protocols â†’ Response Protocol\n- \"How does initialization work?\" â†’ Protocols â†’ Initialization Protocol\n- \"How do I configure profiles?\" â†’ Getting Started or Components â†’ Memory System\n- \"What are the platform components?\" â†’ _index.md or guide/\n",
        "plugins/gemini-consult/.claude-plugin/plugin.json": "{\n  \"name\": \"gemini-consult\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Leverage Google Gemini's CLI for analyzing large codebases beyond typical context limits using @ syntax for file/directory inclusion\"\n}\n",
        "plugins/gemini-consult/README.md": "# Gemini Consult Skill\n\n> **Analyze large codebases beyond Claude Code's context limits using Google Gemini's CLI**\n\n## Overview\n\nGemini Consult is a Claude Code skill that intelligently routes large-scale codebase analysis to Google Gemini's CLI, leveraging its expanded context window for project-wide insights. It seamlessly integrates Gemini's analysis back into your Claude Code workflow.\n\n### What It Does\n\n- **Analyzes entire codebases** that exceed Claude Code's context limits\n- **Compares multiple large files** for consistency and patterns\n- **Performs project-wide audits** (architecture, security, features)\n- **Detects patterns** across distributed codebase locations\n- **Synthesizes findings** into actionable Claude Code workflow steps\n\n### When to Use It\n\nâœ… **Use Gemini Consult when:**\n- Analyzing 5+ files or > 100KB of code\n- Need project-wide architectural understanding\n- Performing security audits across codebase\n- Verifying feature implementations in large projects\n- Comparing implementations across modules\n\nâŒ **Use Claude Code directly when:**\n- Working with single files (< 50KB)\n- Already have sufficient context loaded\n- Rapid iteration on known files\n- Real-time debugging\n\n## Prerequisites\n\n### Install Google Gemini CLI\n\n```bash\n# Install via npm\nnpm install -g @google/generative-ai\n\n# Authenticate\ngemini auth login\n\n# Verify installation\ngemini --version\n```\n\n### API Key Setup\n\nYou'll need a Google AI API key:\n1. Visit [Google AI Studio](https://aistudio.google.com/app/apikey)\n2. Create API key\n3. Configure during `gemini auth login`\n\n## Quick Start\n\n### Example 1: Analyze Complete Architecture\n\n```\nYou: \"Analyze the overall architecture of this project\"\n\nClaude (via Gemini Consult):\n  â†’ Detects large scope\n  â†’ Constructs query: gemini --all_files -p \"Analyze architecture...\"\n  â†’ Returns synthesized findings with file references\n  â†’ Stores insights in Cipher for future reference\n```\n\n### Example 2: Security Audit\n\n```\nYou: \"Check for security vulnerabilities across the codebase\"\n\nClaude (via Gemini Consult):\n  â†’ Scopes to @src/ @lib/ directories\n  â†’ Runs focused security audit query\n  â†’ Returns ranked findings with file:line references\n  â†’ Provides remediation steps\n```\n\n### Example 3: Feature Implementation Verification\n\n```\nYou: \"Is dark mode fully implemented across the app?\"\n\nClaude (via Gemini Consult):\n  â†’ Analyzes @src/ @components/ @styles/\n  â†’ Traces feature implementation\n  â†’ Reports completeness and gaps\n  â†’ Suggests next steps\n```\n\n## How It Works\n\n### The @ Syntax\n\nGemini CLI uses `@` to reference files and directories:\n\n```bash\n# Single file\ngemini -p \"@src/main.py Explain this file\"\n\n# Multiple files\ngemini -p \"@package.json @src/index.js Analyze dependencies\"\n\n# Entire directory\ngemini -p \"@src/ Summarize architecture\"\n\n# All files in project\ngemini --all_files -p \"Find security issues\"\n```\n\n### 3-Phase Consultation Workflow\n\n**Phase 1: Query Assessment**\n- Claude analyzes your question\n- Determines if Gemini consultation is appropriate\n- Constructs optimal query strategy\n- Asks for your approval\n\n**Phase 2: Execution**\n- Executes Gemini CLI query\n- Monitors for errors (auth, rate limits, etc.)\n- Captures comprehensive output\n\n**Phase 3: Synthesis**\n- Transforms Gemini's output into actionable insights\n- Provides file:line references\n- Stores findings in Cipher (Claude's memory)\n- Presents structured recommendations\n\n## Usage Patterns\n\n### Architecture Analysis\n\n**Your question:**\n```\n\"How is the authentication system architected?\"\n```\n\n**What happens:**\n```bash\n# Gemini Consult constructs:\ngemini -p \"@src/auth/ @middleware/auth/ Analyze authentication architecture:\n1. Identify main components\n2. Map data flow\n3. Describe patterns used\n4. Note security measures\"\n```\n\n**You receive:**\n- Component breakdown with file locations\n- Data flow description\n- Pattern identification\n- Security assessment\n- Recommendations for improvements\n\n### Cross-Module Comparison\n\n**Your question:**\n```\n\"Compare error handling across backend and frontend\"\n```\n\n**What happens:**\n```bash\n# Gemini Consult runs:\ngemini -p \"@src/backend/ @src/frontend/ Compare error handling:\n1. Backend error patterns\n2. Frontend error patterns\n3. Consistency analysis\n4. Standardization recommendations\"\n```\n\n**You receive:**\n- Side-by-side comparison\n- Inconsistencies highlighted\n- Refactoring suggestions\n\n### Pattern Detection\n\n**Your question:**\n```\n\"Find all database query patterns and identify inefficiencies\"\n```\n\n**What happens:**\n```bash\ngemini -p \"@src/**/*.js Find database query patterns:\n1. List all query types\n2. Identify N+1 queries\n3. Note missing indexes\n4. Suggest optimizations\"\n```\n\n**You receive:**\n- Query pattern inventory\n- Performance issues with file:line references\n- Optimization recommendations\n\n## Integration with Claude Code\n\nGemini Consult seamlessly integrates into your Claude Code workflow:\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ You ask broad question in Claude Code           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                  â”‚\n                  â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Claude detects scope > context limits           â”‚\nâ”‚ â†’ Activates Gemini Consult skill                â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                  â”‚\n                  â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Presents query plan for your approval           â”‚\nâ”‚ \"Analyzing [N files] with Gemini. Proceed?\"     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                  â”‚\n                  â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Executes Gemini CLI query                       â”‚\nâ”‚ â†’ Captures output                               â”‚\nâ”‚ â†’ Validates results                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                  â”‚\n                  â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Synthesizes findings                            â”‚\nâ”‚ â†’ Extracts file:line references                 â”‚\nâ”‚ â†’ Ranks by importance                           â”‚\nâ”‚ â†’ Stores in Cipher                              â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                  â”‚\n                  â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Presents structured results                     â”‚\nâ”‚ \"Here are 5 critical findings...\"               â”‚\nâ”‚ \"Would you like me to deep dive into #1?\"       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                  â”‚\n                  â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Continue in Claude Code with specific files     â”‚\nâ”‚ â†’ Uses Read tool for targeted work              â”‚\nâ”‚ â†’ References Gemini insights from Cipher        â”‚\nâ”‚ â†’ Implements changes                            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n## Best Practices\n\n### 1. Be Specific in Your Questions\n\nâŒ **Vague:** \"Analyze the code\"\n\nâœ… **Specific:** \"Analyze authentication flow focusing on session management and token validation\"\n\n### 2. Start Broad, Then Narrow\n\n```\nStep 1: \"What's the overall architecture?\" (Gemini)\nStep 2: \"Deep dive into auth module\" (Claude Code + Gemini insights)\nStep 3: \"Implement specific fix in auth/login.js\" (Claude Code)\n```\n\n### 3. Use for Analysis, Not Implementation\n\nGemini Consult is for **understanding and planning**, not writing code:\n- âœ… \"Identify all API endpoints\"\n- âœ… \"Find security vulnerabilities\"\n- âœ… \"Analyze test coverage gaps\"\n- âŒ \"Write new authentication module\" (use Claude Code)\n\n### 4. Leverage Stored Insights\n\nGemini findings are stored in Cipher. Reference them later:\n```\nYou: \"Based on the Gemini analysis from earlier, implement fix #2\"\n\nClaude: â†’ Retrieves insight from Cipher\n        â†’ Uses specific file references\n        â†’ Implements in Claude Code\n```\n\n## Common Use Cases\n\n### ğŸ—ï¸ Architecture Review\n\n\"Analyze the complete project architecture and identify coupling issues\"\n\n**Returns:** Component map, dependency graph, architectural patterns, recommendations\n\n### ğŸ”’ Security Audit\n\n\"Perform comprehensive security audit for SQL injection and XSS\"\n\n**Returns:** Vulnerability list (ranked), file:line locations, remediation steps\n\n### âœ… Feature Verification\n\n\"Verify that pagination is consistently implemented across all list views\"\n\n**Returns:** Implementation locations, consistency analysis, gaps identified\n\n### ğŸ“Š Code Quality Analysis\n\n\"Find all code smells and anti-patterns\"\n\n**Returns:** Issue categorization, examples with locations, refactoring suggestions\n\n### ğŸ”„ Migration Planning\n\n\"Analyze codebase for migration from React Class components to Hooks\"\n\n**Returns:** Component inventory, complexity assessment, migration order recommendations\n\n## Troubleshooting\n\n### \"Gemini CLI not found\"\n\n```bash\n# Install Gemini CLI\nnpm install -g @google/generative-ai\n\n# Verify\ngemini --version\n```\n\n### \"Authentication required\"\n\n```bash\n# Login to Gemini\ngemini auth login\n\n# Follow prompts to authenticate\n```\n\n### \"Rate limit exceeded\"\n\n**Option 1:** Wait for quota reset (shown in error message)\n\n**Option 2:** Reduce scope:\n```\nInstead of: gemini --all_files\nUse: gemini -p \"@src/specific-module/\"\n```\n\n**Option 3:** Split into multiple focused queries\n\n### \"Context overflow even with Gemini\"\n\nYour project is extremely large. Use progressive analysis:\n\n```\nQuery 1: gemini -p \"@src/frontend/ Analyze frontend\"\nQuery 2: gemini -p \"@src/backend/ Analyze backend\"\nQuery 3: gemini -p \"@src/shared/ Analyze shared\"\n\nThen synthesize findings client-side\n```\n\n## Limitations\n\n- **Not for real-time debugging:** Gemini queries take time; use Claude Code for rapid iteration\n- **API costs:** Gemini API has usage costs; queries are metered\n- **Internet required:** Gemini CLI needs internet connection\n- **Not for code generation:** Use for analysis, not writing large code blocks\n\n## Advanced Features\n\n### Query Chaining\n\nFor complex analyses, chain multiple queries:\n\n```\n1. Discovery: gemini --all_files -p \"Identify all state management approaches\"\n2. Deep Dive: gemini -p \"@src/stores/ Analyze Redux usage patterns\"\n3. Verification: gemini -p \"@tests/ Check test coverage for identified stores\"\n```\n\n### Custom Scope Patterns\n\nTarget specific file types or patterns:\n\n```bash\n# All TypeScript files\ngemini -p \"@src/**/*.ts ...\"\n\n# Specific pattern\ngemini -p \"@src/**/auth*.js ...\"\n\n# Multiple directories\ngemini -p \"@src/components/ @src/hooks/ @src/utils/ ...\"\n```\n\n## FAQ\n\n**Q: When should I use Gemini vs Claude Code?**\n\nA: Use Gemini for breadth (project-wide understanding), Claude Code for depth (specific implementation work).\n\n**Q: Are Gemini queries free?**\n\nA: Gemini API has usage-based pricing. Check [Google AI pricing](https://ai.google.dev/pricing) for details.\n\n**Q: Can I use this offline?**\n\nA: No, Gemini CLI requires internet connection and API access.\n\n**Q: Will this slow down my Claude Code session?**\n\nA: No - Gemini queries run externally. Your Claude session continues after receiving results.\n\n**Q: What if Gemini gives wrong information?**\n\nA: Gemini Consult validates file references and flags uncertainties. Always verify critical findings in Claude Code.\n\n## Future Enhancements\n\n**Planned improvements** (contributions welcome):\n\n### Commands\n- [ ] `/jcmrs:gemini-history` - View and re-run past Gemini consultations from Cipher\n- [ ] Query templates for common analysis patterns (security audit, architecture review, etc.)\n- [ ] Interactive query builder with scope pattern suggestions\n\n### Agents\n- [ ] **Query Constructor Agent** - Automatically builds optimal Gemini queries based on user questions\n- [ ] **Result Validator Agent** - Deep validation of Gemini output before presenting to user\n\n### Integrations\n- [ ] **Serena (MCP) Integration** - Leverage Serena MCP server for enhanced analysis capabilities\n  - Combine Gemini's breadth with Serena's specialized tools\n  - Cross-reference findings between systems\n  - Unified analysis workflow\n\n### Settings\n- [ ] User-configurable thresholds for auto-suggest (files count, size limits)\n- [ ] Custom query templates in `.claude/gemini-consult.local.md`\n- [ ] Saved query patterns for frequent analysis types\n\n### Documentation\n- [ ] Examples directory with common query patterns\n- [ ] Video tutorials for different analysis scenarios\n- [ ] Case studies: architecture reviews, security audits, migration planning\n\n**Want to contribute?** See Contributing section below for how to get started.\n\n## Contributing\n\nFound a useful query pattern? Encountered an edge case? Contributions welcome:\n\n1. Fork the repository\n2. Add your pattern to SKILL.md\n3. Test with real codebases\n4. Submit pull request\n\n## Support\n\n- **Issues:** [GitHub Issues](https://github.com/jcmrs/jcmrs-plugins/issues)\n- **Discussions:** [GitHub Discussions](https://github.com/jcmrs/jcmrs-plugins/discussions)\n\n## License\n\nMIT License - see LICENSE file for details\n\n---\n\n**Built for Claude Code** | Powered by Google Gemini | Part of jcmrs-plugins marketplace\n",
        "plugins/gemini-consult/commands/gemini-check.md": "---\nname: jcmrs:gemini-check\ndescription: Verify Google Gemini CLI installation and readiness\nallowed-tools:\n  - Bash\n---\n\n# Gemini Check Command\n\nDiagnose Google Gemini CLI installation and configuration status.\n\n## Purpose\n\nQuickly verify that:\n- Gemini CLI is installed\n- Command is in PATH\n- Version information is available\n- System is ready for `/jcmrs:gemini-consult` usage\n\n## Execution Protocol\n\n### 1. Check CLI Availability\n\nTest if `gemini` command exists:\n\n```bash\nwhich gemini || echo \"NOT_FOUND\"\n```\n\n**On Windows** (if which fails):\n```bash\nwhere gemini || echo \"NOT_FOUND\"\n```\n\n### 2. Get Version Information\n\nIf gemini is found, retrieve version:\n\n```bash\ngemini --version\n```\n\n**Expected output**: Version number (e.g., \"1.0.0\" or similar)\n\n### 3. Test Basic Functionality\n\nRun a minimal test query to verify gemini works:\n\n```bash\ngemini -p \"test query\" --help\n```\n\n**Purpose**: Verify gemini responds to commands without actually executing a query\n\n### 4. Format Diagnostic Report\n\nPresent results in clear status format:\n\n**If everything works**:\n\n```markdown\nâœ… Gemini CLI Status: Ready\n\n**Installation**\n- Command: gemini\n- Location: [path from which/where]\n- Version: [version number]\n\n**Functionality**\n- Basic commands: âœ… Working\n- Ready for queries: âœ… Yes\n\n**Next Steps**\n- Run queries with: /jcmrs:gemini-consult\n- Example: /jcmrs:gemini-consult @src/ Analyze architecture\n```\n\n**If gemini not found**:\n\n```markdown\nâŒ Gemini CLI Status: Not Installed\n\n**Problem**\n- gemini command not found in PATH\n\n**Solution**\nInstall Google Gemini CLI:\n\n1. Using npm:\n   ```bash\n   npm install -g @google/generative-ai\n   ```\n\n2. Verify installation:\n   ```bash\n   gemini --version\n   ```\n\n3. Authenticate (if needed):\n   ```bash\n   gemini auth login\n   ```\n\n4. Test with:\n   ```bash\n   /jcmrs:gemini-check\n   ```\n\n**Documentation**\n- Installation guide: https://ai.google.dev/gemini-api/docs/cli\n```\n\n**If gemini found but errors**:\n\n```markdown\nâš ï¸ Gemini CLI Status: Installed but Issues Detected\n\n**Installation**\n- Command: gemini\n- Location: [path]\n- Version: [version or error]\n\n**Issues**\n- [Specific error from version check]\n\n**Troubleshooting**\n1. Verify installation:\n   ```bash\n   npm list -g @google/generative-ai\n   ```\n\n2. Reinstall if needed:\n   ```bash\n   npm install -g @google/generative-ai\n   ```\n\n3. Check authentication:\n   ```bash\n   gemini auth status\n   ```\n\n4. Test query:\n   ```bash\n   gemini -p \"@README.md Summarize this file\"\n   ```\n\n**Need Help?**\n- Run diagnostics: /jcmrs:gemini-check\n- Check installation docs: https://ai.google.dev/gemini-api/docs/cli\n```\n\n## Diagnostic Details\n\n### PATH Check\n\nVerify gemini is in system PATH:\n- **macOS/Linux**: `which gemini`\n- **Windows**: `where gemini`\n\n**Expected**: Full path to gemini executable\n\n### Version Verification\n\nCheck installed version:\n```bash\ngemini --version\n```\n\n**Purpose**: Confirms not only installation but that gemini binary works\n\n### Authentication Status\n\nOptionally check authentication (if gemini supports it):\n```bash\ngemini auth status\n```\n\n**Note**: Some gemini installations may not require auth or may handle it differently\n\n## Common Issues & Solutions\n\n### Issue: Command Not Found\n\n**Symptom**: `gemini: command not found`\n\n**Solutions**:\n1. Install globally: `npm install -g @google/generative-ai`\n2. Check npm global bin in PATH: `npm config get prefix`\n3. Restart terminal after installation\n\n### Issue: Permission Denied\n\n**Symptom**: `Permission denied` when running gemini\n\n**Solutions**:\n1. Fix permissions: `chmod +x [gemini path]`\n2. Reinstall with proper permissions: `sudo npm install -g @google/generative-ai`\n3. Use npm without sudo: Configure npm prefix to user directory\n\n### Issue: Wrong Version\n\n**Symptom**: Old version installed\n\n**Solutions**:\n1. Update: `npm update -g @google/generative-ai`\n2. Check version: `gemini --version`\n3. Uninstall and reinstall if update fails\n\n### Issue: Authentication Required\n\n**Symptom**: Gemini asks for auth when running queries\n\n**Solutions**:\n1. Login: `gemini auth login`\n2. Follow authentication prompts\n3. Verify: `gemini auth status`\n\n## Integration with Plugin\n\nThis diagnostic command helps troubleshoot:\n- `/jcmrs:gemini-consult` failures\n- Hook suggestion errors\n- Setup verification for new users\n\n**Recommended workflow**:\n1. User installs plugin\n2. Run `/jcmrs:gemini-check` to verify setup\n3. Fix any issues identified\n4. Proceed with `/jcmrs:gemini-consult` queries\n\n## Quick Reference\n\n```bash\n# Check status\n/jcmrs:gemini-check\n\n# If issues, install\nnpm install -g @google/generative-ai\n\n# Verify\ngemini --version\n\n# Test query\ngemini -p \"@README.md Summarize\"\n\n# Recheck\n/jcmrs:gemini-check\n```\n\n## Exit Conditions\n\nCommand completes successfully when:\n- âœ… Status report generated\n- âœ… Installation state determined\n- âœ… Clear next steps provided (if issues found)\n- âœ… User knows whether they can use `/jcmrs:gemini-consult`\n\n**Do not**:\n- Attempt to install gemini automatically\n- Modify user's PATH\n- Change system configuration\n\n**Only**: Report status and provide clear instructions\n",
        "plugins/gemini-consult/commands/gemini-consult.md": "---\nname: jcmrs:gemini-consult\ndescription: Execute Google Gemini CLI for large codebase analysis using @ syntax\nargument-hint: \"[scope-pattern] [query]\"\nallowed-tools:\n  - Bash\n  - mcp__cipher__ask_cipher\n---\n\n# Gemini Consult Command\n\nExecute Google Gemini CLI for analyzing large codebases that exceed Claude Code's context limits.\n\n## Command Format\n\nThe command accepts free-form arguments combining scope patterns and queries:\n\n```bash\n/jcmrs:gemini-consult @src/main.py Explain this file's purpose and structure\n/jcmrs:gemini-consult @package.json @src/index.js Analyze dependencies\n/jcmrs:gemini-consult @src/ Summarize the architecture\n/jcmrs:gemini-consult @src/ @tests/ Analyze test coverage\n/jcmrs:gemini-consult @./ Give me an overview of this project\n/jcmrs:gemini-consult --all_files Analyze project structure and dependencies\n```\n\n## Execution Protocol\n\n### 1. Parse Arguments\n\nExtract the complete query from user input:\n- Everything after `/jcmrs:gemini-consult` is the query\n- Query includes both scope patterns (@...) and question text\n- Preserve exact formatting and spacing\n\n### 2. Construct Gemini Command\n\nBuild the Gemini CLI command:\n\n```bash\ngemini -p \"[complete user query]\"\n```\n\n**Important**:\n- Use double quotes around the entire prompt\n- Escape any internal quotes if present\n- Preserve @ syntax exactly as provided\n- Include --all_files flag if present\n\n### 3. Execute Query\n\nRun the Gemini command via Bash tool:\n\n```bash\ngemini -p \"[query]\"\n```\n\n**Timeout**: Set to 120 seconds (Gemini queries can take time for large scopes)\n\n**Error handling**:\n- If gemini command not found: Direct user to run `/jcmrs:gemini-check`\n- If timeout: Suggest reducing scope or splitting into multiple queries\n- If other errors: Display error message and suggest diagnostics\n\n### 4. Process Results\n\nWhen Gemini returns output:\n\n**A. Synthesize Summary**\n\nExtract key insights:\n- Main findings (3-5 bullet points)\n- File references mentioned (with paths)\n- Critical recommendations\n- Confidence level or uncertainties noted\n\n**B. Validate Output**\n\nQuick validation checks:\n- File paths mentioned: Note if they exist in current repo\n- Specific claims: Flag any that seem uncertain\n- Completeness: Check if query was fully addressed\n\n**C. Format Response**\n\nPresent in this structure:\n\n```markdown\n## Gemini Analysis Results\n\n**Query**: [original query]\n**Scope**: [scope patterns used]\n\n### Key Findings\n\n1. [Finding 1 with file references]\n2. [Finding 2 with implications]\n3. [Finding 3 with recommendations]\n\n### Recommendations\n\n- [Actionable item 1]\n- [Actionable item 2]\n\n### Full Gemini Output\n\n<details>\n<summary>Click to expand complete response</summary>\n\n[Complete Gemini output here]\n\n</details>\n\n---\n\n*Analysis stored in Cipher for future reference*\n```\n\n### 5. Store in Cipher\n\nAutomatically store the consultation in Cipher:\n\n```\nmcp__cipher__ask_cipher(\"Store: Gemini Consultation - [Topic from query]\n\nQuery: [original query]\nScope: [scope patterns]\nDate: [timestamp]\n\nKey Findings:\n- [Finding 1]\n- [Finding 2]\n\nRecommendations:\n- [Recommendation 1]\n- [Recommendation 2]\n\nComplete output archived for reference.\")\n```\n\n**Storage benefits**:\n- Future queries can reference past analyses\n- User can ask \"What did Gemini say about X?\"\n- Builds knowledge base over time\n\n## Usage Tips\n\n**Single file analysis**:\n```bash\n/jcmrs:gemini-consult @src/auth/login.js Explain authentication flow\n```\n\n**Comparative analysis**:\n```bash\n/jcmrs:gemini-consult @src/api/v1/ @src/api/v2/ Compare API versions\n```\n\n**Security audit**:\n```bash\n/jcmrs:gemini-consult @src/ Find SQL injection vulnerabilities\n```\n\n**Architecture review**:\n```bash\n/jcmrs:gemini-consult --all_files Analyze project architecture and dependencies\n```\n\n**Feature verification**:\n```bash\n/jcmrs:gemini-consult @src/ @tests/ Verify dark mode implementation\n```\n\n## Important Notes\n\n- **Paths are relative**: @ patterns are relative to current working directory\n- **Large scopes take time**: --all_files can take 30-60 seconds\n- **Gemini required**: User must have `gemini` CLI installed and in PATH\n- **No API key needed**: Assumes gemini is already authenticated\n- **Results cached**: Gemini may cache results for identical queries\n\n## Error Recovery\n\n**Command not found**:\n```\nError: 'gemini' command not found\nâ†’ Run /jcmrs:gemini-check to diagnose\nâ†’ Install: npm install -g @google/generative-ai\n```\n\n**Timeout**:\n```\nError: Query timeout (>120s)\nâ†’ Reduce scope: Use specific directories instead of --all_files\nâ†’ Split query: Analyze frontend and backend separately\n```\n\n**Empty response**:\n```\nError: Gemini returned no output\nâ†’ Check query syntax (@ patterns correct?)\nâ†’ Verify files exist at specified paths\nâ†’ Try simpler query first\n```\n\n## Integration with Skill\n\nThis command complements the gemini-consult skill:\n- **Skill**: Provides knowledge about when/how to use Gemini\n- **Command**: Executes the actual Gemini query\n- **Both**: Store results in Cipher for continuity\n\nUsers can invoke this command directly or let the skill suggest it when appropriate.\n",
        "plugins/gemini-consult/hooks/hooks.json": "{\n  \"description\": \"Auto-suggest Gemini CLI when Claude attempts to read large file scopes that would benefit from Gemini's expanded context window\",\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Read|Glob\",\n        \"hooks\": [\n          {\n            \"type\": \"prompt\",\n            \"prompt\": \"You are monitoring file read operations to help users leverage Gemini CLI for large-scale analysis.\\n\\n**Analysis Task:**\\nEvaluate if this file operation would benefit from using Gemini CLI instead of Claude Code's Read/Glob tools.\\n\\n**Tool Input:**\\n$TOOL_INPUT\\n\\n**Decision Criteria:**\\n\\n1. **Count file operations in current context:**\\n   - If this is part of a pattern of reading many files (>10 files)\\n   - Look at transcript to see recent Read/Glob operations\\n\\n2. **Estimate total scope:**\\n   - Single large file (>100KB): Consider Gemini\\n   - Multiple files being read sequentially: Count them\\n   - Glob pattern matching many files: Estimate based on pattern\\n\\n3. **Type of analysis:**\\n   - Architecture review: Suggest Gemini\\n   - Cross-file pattern detection: Suggest Gemini\\n   - Security audit: Suggest Gemini\\n   - Single file deep dive: Allow Read\\n   - Documentation package (3-7 md files): Allow Read (avoid reductionism)\\n\\n**Threshold for Suggestion:**\\n- >10 files OR >100KB total\\n- Reason: Documentation packages of 3-7 markdown files are better read directly for understanding\\n\\n**Output Format:**\\n\\nIf threshold NOT exceeded or documentation package:\\n```json\\n{\\n  \\\"hookSpecificOutput\\\": {\\n    \\\"permissionDecision\\\": \\\"allow\\\"\\n  },\\n  \\\"continue\\\": true\\n}\\n```\\n\\nIf threshold exceeded, suggest Gemini:\\n```json\\n{\\n  \\\"hookSpecificOutput\\\": {\\n    \\\"permissionDecision\\\": \\\"allow\\\"\\n  },\\n  \\\"systemMessage\\\": \\\"ğŸ“Š **Large Scope Detected**\\\\n\\\\nYou're about to read [N files / X KB]. Consider using Gemini CLI for more efficient large-scale analysis:\\\\n\\\\n`/jcmrs:gemini-consult @[scope-pattern] [your analysis question]`\\\\n\\\\nExamples:\\\\n- `/jcmrs:gemini-consult @src/ Analyze architecture`\\\\n- `/jcmrs:gemini-consult @src/ @tests/ Check test coverage`\\\\n- `/jcmrs:gemini-consult --all_files Overview of project structure`\\\\n\\\\n*Proceeding with Read/Glob as requested. Run `/jcmrs:gemini-check` to verify Gemini CLI availability.*\\\",\\n  \\\"continue\\\": true\\n}\\n```\\n\\n**Important:**\\n- Always allow the operation (permissionDecision: \\\"allow\\\")\\n- Only provide systemMessage if suggesting Gemini\\n- Tailor scope-pattern suggestion to the files being accessed\\n- Be concise but helpful\\n\\n**Analyze the current operation and respond:**\",\n            \"timeout\": 15\n          }\n        ]\n      }\n    ]\n  }\n}\n",
        "plugins/gemini-consult/skills/gemini-consult/SKILL.md": "# Gemini Consult: Large Codebase Analysis Assistant\n\nIntelligent consultation orchestrator that leverages Google Gemini's CLI for analyzing codebases exceeding typical context limitations.\n\n## Core Purpose\n\nBridge the gap between context-constrained AI assistants and comprehensive codebase understanding by orchestrating strategic use of Google Gemini's expanded context window.\n\n**Problem it solves:**\n- Claude Code's context limits prevent full codebase analysis\n- Large file comparisons exceed token budgets\n- Project-wide pattern detection requires more context than available\n- Architectural decisions need holistic codebase visibility\n\n**Solution approach:**\n- Intelligently route appropriate queries to Gemini CLI\n- Construct optimal @ syntax queries for maximum insight\n- Synthesize Gemini's analysis back into Claude Code workflow\n- Preserve session efficiency by avoiding unnecessary context consumption\n\n## When to Use Gemini Consult\n\n**Trigger Conditions (Auto-recommend):**\n\nThe skill should activate when:\n- User asks about \"entire codebase\", \"whole project\", \"all files\"\n- Analysis scope > 5 files or > 100KB combined content\n- Comparative analysis needed across distant modules\n- Architecture or pattern questions requiring project-wide view\n- Security audit or compliance verification across codebase\n- Feature implementation verification in large projects\n\n**User Explicit Triggers:**\n- \"Use Gemini to analyze...\"\n- \"Consult Gemini about...\"\n- \"@gemini check if...\"\n\n**When NOT to use:**\n- Single file analysis (< 50KB) â†’ Use Claude Code Read tool\n- Already have sufficient context loaded\n- Real-time debugging or rapid iteration\n- Questions answerable from loaded files\n\n## The 3-Phase Consultation Workflow\n\n### Phase 1: Query Assessment & Planning\n\n**Objective:** Determine if Gemini consultation is appropriate and construct optimal query strategy.\n\n**Assessment Checklist:**\n```javascript\nassessment = {\n  scope_size: estimate_file_count_and_size(),\n  analysis_type: identify_analysis_category(),\n  context_available: check_current_session_context(),\n  query_complexity: evaluate_question_depth()\n}\n\nrecommendation = {\n  use_gemini: scope_size > threshold || analysis_type in ['architecture', 'security_audit', 'cross_project'],\n  query_strategy: construct_query_plan(),\n  expected_value: estimate_insight_gain()\n}\n```\n\n**User Interaction:**\n\nPresent recommendation clearly:\n\n\"This analysis requires examining **[N files / X MB]** across **[scope]**. I recommend using Gemini CLI because [reason].\n\nI'll construct a query that:\n- Includes: [file patterns]\n- Focuses on: [specific analysis aspects]\n- Returns: [expected insights]\n\nProceed with Gemini consultation? [Yes / Adjust scope / Use Claude Code only]\"\n\n**Query Construction Strategy:**\n\nBased on analysis type, determine optimal @ syntax pattern:\n\n1. **Full Project Analysis:**\n   ```bash\n   gemini --all_files -p \"Analyze complete project architecture\"\n   ```\n\n2. **Directory-Scoped Analysis:**\n   ```bash\n   gemini -p \"@src/ @tests/ Compare implementation vs test coverage\"\n   ```\n\n3. **Multi-File Comparison:**\n   ```bash\n   gemini -p \"@src/auth/login.js @src/auth/register.js @src/middleware/auth.js Analyze authentication flow consistency\"\n   ```\n\n4. **Targeted Pattern Search:**\n   ```bash\n   gemini -p \"@src/**/*.js Find all error handling patterns and identify inconsistencies\"\n   ```\n\n### Phase 2: Query Execution & Monitoring\n\n**Objective:** Execute Gemini CLI query and capture comprehensive output.\n\n**Execution Protocol:**\n\n1. **Construct Command:**\n   ```bash\n   gemini_command = build_query(\n     scope: assessment.scope,\n     prompt: refined_user_question,\n     flags: determine_flags()\n   )\n   ```\n\n2. **Execute with Monitoring:**\n   ```bash\n   # Execute via Bash tool\n   result = bash_execute(gemini_command, timeout=120)\n\n   # Monitor for:\n   # - CLI availability (is gemini installed?)\n   # - Authentication status (logged in?)\n   # - Rate limiting (hit quota?)\n   # - Error messages\n   ```\n\n3. **Capture Output:**\n   - Full response text\n   - Any warnings or errors\n   - File inclusion confirmations\n   - Token usage statistics (if available)\n\n**Error Handling:**\n\n```javascript\nif (error.type === 'CLI_NOT_FOUND') {\n  return {\n    status: 'blocked',\n    message: \"Gemini CLI not installed. Install: npm install -g @google/generative-ai\",\n    fallback: \"Use Claude Code tools for subset analysis?\"\n  }\n}\n\nif (error.type === 'AUTH_REQUIRED') {\n  return {\n    status: 'blocked',\n    message: \"Gemini CLI requires authentication. Run: gemini auth login\",\n    fallback: null\n  }\n}\n\nif (error.type === 'RATE_LIMIT') {\n  return {\n    status: 'retry',\n    message: \"Gemini API rate limit reached. Retry in [time] or reduce scope?\",\n    fallback: \"Split analysis into smaller chunks?\"\n  }\n}\n\nif (error.type === 'CONTEXT_OVERFLOW') {\n  return {\n    status: 'adjust',\n    message: \"Even Gemini's context exceeded. Analysis scope too large.\",\n    strategy: \"Split into multiple focused queries with specific file patterns\"\n  }\n}\n```\n\n### Phase 3: Synthesis & Integration\n\n**Objective:** Transform Gemini's output into actionable insights for Claude Code workflow.\n\n**Synthesis Process:**\n\n1. **Parse Gemini Response:**\n   - Extract key findings\n   - Identify file-specific insights (with line references)\n   - Categorize recommendations\n   - Note confidence levels or uncertainties\n\n2. **Contextualize for Current Session:**\n   ```markdown\n   ## Gemini Consultation Results\n\n   **Query:** [original question]\n   **Scope:** [files/directories analyzed]\n   **Key Findings:**\n\n   1. [Finding with file:line references]\n   2. [Finding with architectural implications]\n   3. [Finding with security considerations]\n\n   **Recommendations:**\n   - [Actionable item 1]\n   - [Actionable item 2]\n\n   **Follow-up Actions:**\n   - [ ] Review identified files in Claude Code\n   - [ ] Implement suggested changes\n   - [ ] Verify with local testing\n   ```\n\n3. **Store in Cipher (Memory):**\n   ```\n   cipher_store(\"Gemini Consultation - [Topic]\n\n   Analysis Type: [architecture/security/feature/pattern]\n   Scope: [file patterns]\n   Date: [timestamp]\n\n   Key Insights:\n   - [Insight 1 with file references]\n   - [Insight 2 with implications]\n\n   Recommended Actions:\n   - [Action 1]\n   - [Action 2]\n\n   Query Pattern Used:\n   ```bash\n   [gemini command]\n   ```\n\n   Results archived for future reference.\")\n   ```\n\n4. **Present to User:**\n\n   Clear, structured output:\n\n   \"âœ… Gemini Analysis Complete\n\n   **Analyzed:** [N files across M directories]\n\n   **Critical Findings:**\n\n   1. **[Category]:** [Finding]\n      - Files: `src/file1.js:45`, `src/file2.js:89`\n      - Impact: [High/Medium/Low]\n      - Action: [Specific recommendation]\n\n   2. **[Category]:** [Finding]\n      ...\n\n   **Next Steps:**\n\n   I've stored these insights in Cipher for future reference. Would you like me to:\n\n   A) Deep dive into specific finding [#1, #2, etc.]\n   B) Implement recommended changes\n   C) Run focused analysis on subset\n   D) Continue with different question\"\n\n## Analysis Type Patterns\n\n### Architecture Analysis\n\n**Trigger Patterns:**\n- \"How is [system] architected?\"\n- \"What's the overall structure?\"\n- \"Explain the codebase organization\"\n\n**Query Construction:**\n```bash\ngemini --all_files -p \"Analyze the complete architecture:\n1. Identify main components and their responsibilities\n2. Map data flow between modules\n3. Describe architectural patterns used\n4. Note any architectural inconsistencies\n5. Highlight coupling and dependency issues\"\n```\n\n**Synthesis Focus:**\n- Component diagram (textual representation)\n- Dependency graph insights\n- Pattern identification\n- Architectural recommendations\n\n### Security Audit\n\n**Trigger Patterns:**\n- \"Security vulnerabilities?\"\n- \"Check for [security issue]\"\n- \"Audit authentication/authorization\"\n\n**Query Construction:**\n```bash\ngemini -p \"@src/ @lib/ Perform security audit:\n1. Identify potential SQL injection points\n2. Check for XSS vulnerabilities\n3. Verify authentication implementation\n4. Review authorization logic\n5. Check for exposed secrets or credentials\n6. Assess input validation coverage\"\n```\n\n**Synthesis Focus:**\n- Vulnerability severity ranking\n- Specific file:line locations\n- Proof of concept (if safe to demonstrate)\n- Remediation steps with code examples\n\n### Feature Implementation Verification\n\n**Trigger Patterns:**\n- \"Is [feature] implemented?\"\n- \"How does [feature] work?\"\n- \"Find all places where [feature] is used\"\n\n**Query Construction:**\n```bash\ngemini -p \"@src/ Verify [feature] implementation:\n1. Locate all files implementing this feature\n2. Trace the complete execution flow\n3. Identify edge cases handled\n4. Check for test coverage\n5. Note any incomplete or inconsistent implementations\"\n```\n\n**Synthesis Focus:**\n- Implementation completeness (%)\n- File locations with code snippets\n- Flow diagram (textual)\n- Gaps or inconsistencies\n\n### Pattern Detection\n\n**Trigger Patterns:**\n- \"Find all [pattern] usage\"\n- \"Identify inconsistent [pattern]\"\n- \"How is [pattern] implemented across codebase?\"\n\n**Query Construction:**\n```bash\ngemini -p \"@src/**/*.js Analyze [pattern] usage:\n1. Find all instances of this pattern\n2. Compare implementations for consistency\n3. Identify outliers or anti-patterns\n4. Suggest standardization approach\"\n```\n\n**Synthesis Focus:**\n- Pattern instances grouped by consistency\n- Deviation analysis\n- Refactoring recommendations\n\n### Cross-Project Comparison\n\n**Trigger Patterns:**\n- \"Compare [repo1] and [repo2]\"\n- \"How does this differ from [other implementation]?\"\n\n**Query Construction:**\n```bash\n# Note: Requires sequential queries or multi-repo setup\ngemini -p \"@project1/src/ Analyze [aspect] implementation\"\ngemini -p \"@project2/src/ Analyze same [aspect] implementation\"\n# Then ask Gemini to compare the two analyses\n```\n\n**Synthesis Focus:**\n- Side-by-side comparison table\n- Unique approaches in each\n- Strengths/weaknesses\n- Recommendation for current project\n\n## Best Practices\n\n### Query Construction Principles\n\n1. **Be Specific:**\n   - âŒ \"Analyze the code\"\n   - âœ… \"Analyze authentication flow in @src/auth/, focusing on session management and token validation\"\n\n2. **Scope Appropriately:**\n   - âŒ `gemini --all_files -p \"Fix bug\"` (too broad)\n   - âœ… `gemini -p \"@src/auth/ @tests/auth/ Identify why session timeout isn't working\"`\n\n3. **Use Numbered Questions:**\n   - Helps Gemini structure response\n   - Makes synthesis easier\n   - Ensures comprehensive coverage\n\n4. **Include Expected Deliverables:**\n   ```bash\n   gemini -p \"@src/ Analyze error handling. Provide:\n   1. List of all error types caught\n   2. Consistency analysis across modules\n   3. Gaps in error coverage\n   4. Recommendation for standardization\"\n   ```\n\n### Context Management\n\n**When to Chain Queries:**\n\nInstead of one massive query, use sequential focused queries:\n\n```bash\n# Query 1: Discovery\ngemini -p \"@src/ Identify all database access patterns\"\n\n# Query 2: Deep Dive (based on Query 1 findings)\ngemini -p \"@src/models/ @src/repositories/ Analyze ORM usage consistency\"\n\n# Query 3: Verification\ngemini -p \"@tests/ Verify test coverage for identified DB patterns\"\n```\n\n**Benefits:**\n- More focused responses\n- Better error recovery\n- Progressive refinement\n- Lower cost per query\n\n### Integration with Claude Code Workflow\n\n**Workflow Pattern:**\n\n```\n1. User asks broad question in Claude Code\n2. Claude recognizes scope exceeds context\n3. Gemini Consult skill activates\n4. Execute Gemini query\n5. Synthesize findings\n6. Store in Cipher\n7. Continue in Claude Code with specific file work\n```\n\n**Example:**\n\n```\nUser: \"I want to refactor our authentication system for better security\"\n\nClaude (recognizes large scope):\n  â†’ Activates Gemini Consult\n  â†’ Runs: gemini -p \"@src/auth/ @middleware/ Analyze authentication architecture...\"\n  â†’ Receives comprehensive analysis\n  â†’ Stores in Cipher\n  â†’ Responds: \"Based on Gemini analysis, here are 5 critical areas...\"\n\nUser: \"Start with #1 - JWT token handling\"\n\nClaude (now focused):\n  â†’ Uses Read tool for specific files identified by Gemini\n  â†’ Implements changes using normal Claude Code workflow\n  â†’ References Gemini insights from Cipher as needed\n```\n\n## Error Handling\n\n### CLI Availability Check\n\n**Before any query:**\n```bash\n# Check if gemini CLI is available\nwhich gemini || echo \"NOT_FOUND\"\n```\n\n**If not found:**\n1. Inform user about installation\n2. Provide installation command\n3. Offer fallback (Claude Code subset analysis)\n4. Store user preference (install later / use fallback)\n\n### Authentication Management\n\n**Check auth status:**\n```bash\ngemini auth status\n```\n\n**If not authenticated:**\n1. Explain auth requirement\n2. Provide auth command\n3. Pause consultation until resolved\n4. Resume when authenticated\n\n### Scope Overflow Handling\n\n**If Gemini returns context overflow:**\n\n```javascript\noverflow_strategy = {\n  approach: \"Divide and Conquer\",\n\n  steps: [\n    \"Split scope into logical chunks (by directory, feature, module)\",\n    \"Run multiple focused queries\",\n    \"Synthesize results client-side\",\n    \"Present unified findings\"\n  ],\n\n  example: {\n    original: \"gemini --all_files -p 'Analyze everything'\",\n    split: [\n      \"gemini -p '@src/frontend/ Analyze frontend architecture'\",\n      \"gemini -p '@src/backend/ Analyze backend architecture'\",\n      \"gemini -p '@src/shared/ Analyze shared utilities'\",\n      // Then synthesize findings\n    ]\n  }\n}\n```\n\n### Result Quality Validation\n\n**Validate Gemini output before presenting:**\n\n```javascript\nvalidation_checks = {\n  // Gemini hallucination check\n  file_references_exist: verify_files_exist(gemini_response.mentioned_files),\n\n  // Response completeness\n  addresses_all_query_points: check_numbered_questions_answered(),\n\n  // Actionability\n  has_specific_recommendations: !is_vague(gemini_response.recommendations),\n\n  // Confidence indicators\n  notes_uncertainties: gemini_response.includes(\"not sure\", \"possibly\", \"might\")\n}\n\nif (!validation_checks.all_pass) {\n  refine_query_and_retry_OR_flag_to_user();\n}\n```\n\n## State Management\n\n### Session State\n\n```javascript\ngemini_consult_state = {\n  active_consultation: boolean,\n  current_phase: 1 | 2 | 3,\n\n  assessment: {\n    scope: {...},\n    strategy: {...},\n    user_approved: boolean\n  },\n\n  execution: {\n    command: string,\n    status: 'pending' | 'running' | 'complete' | 'error',\n    output: string,\n    errors: []\n  },\n\n  synthesis: {\n    findings: [],\n    recommendations: [],\n    follow_up_actions: [],\n    stored_in_cipher: boolean\n  }\n}\n```\n\n### Persistence Strategy\n\n**Store in Cipher after every successful consultation:**\n- Query pattern used\n- Scope analyzed\n- Key findings\n- Timestamp for future reference\n\n**Retrieve from Cipher when:**\n- Similar query detected\n- User asks \"what did Gemini say about X?\"\n- Building on previous analysis\n\n## Key Principles\n\n1. **Gemini for Breadth, Claude for Depth:** Use Gemini to understand the forest, Claude Code to work on specific trees\n\n2. **Always Validate Scope:** Don't waste Gemini queries on small scopes Claude Code handles efficiently\n\n3. **Synthesize, Don't Dump:** Transform Gemini's output into actionable Claude Code workflow steps\n\n4. **Store Knowledge:** Gemini insights go into Cipher for long-term memory\n\n5. **Progressive Refinement:** Start broad, narrow down based on findings\n\n6. **User in Control:** Always ask permission before executing Gemini queries (they may have quota concerns)\n\n7. **Graceful Fallback:** If Gemini unavailable, offer Claude Code alternatives (subset analysis, iterative exploration)\n\n## Future Enhancements\n\n**Planned Features:**\n- Auto-detection of optimal query patterns based on question analysis\n- Cost estimation before query execution\n- Multi-stage query orchestration (discovery â†’ deep-dive â†’ verification)\n- Integration with MCP servers for expanded analysis capabilities\n- Query result caching to avoid redundant API calls\n\n## Implementation Status\n\n**Current:** Specification complete\n**Next:** Implement Phase 1 (Query Assessment)\n**Future:** Full 3-phase workflow, error handling, Cipher integration\n",
        "plugins/profile-creator/.claude-plugin/plugin.json": "{\n  \"name\": \"profile-creator\",\n  \"version\": \"0.1.0\",\n  \"description\": \"Transforms messy human intent and repository analysis into living operational domain profiles through 6-phase knowledge engineering pipeline\",\n  \"author\": {\n    \"name\": \"JCMRS\"\n  },\n  \"license\": \"BSD-3-Clause\",\n  \"repository\": \"https://github.com/jcmrs/jcmrs-plugins\",\n  \"keywords\": [\"profile-creator\", \"behavioral-programming\", \"knowledge-engineering\", \"domain-profiles\"]\n}\n",
        "plugins/profile-creator/README.md": "# Profile Creator\n\n**Status:** Early Development (v0.1.0)\n\n## What It Does\n\nThe Profile Creator transforms messy human intent and repository analysis into operational domain profiles with behavioral programming. It bridges the gap between what non-technical users envision and what AI assistants need for effective collaboration.\n\n## The Problem It Solves\n\nCreating effective domain profiles is nearly impossible for humans because:\n- We lack access to role taxonomies and domain knowledge graphs\n- We don't understand ontological structures that AI needs\n- We can't systematically generate behavioral constraints\n- Manual creation takes months (see: the Domain Linguist profile took 2 months)\n\nBut humans ARE good at:\n- Vision and high-level concepts\n- User stories and domain intuition\n- Understanding \"why\" and \"what\"\n\nThe Profile Creator operationalizes the collaboration: humans provide vision, AI provides ontological validation and systematic behavioral synthesis.\n\n## How It Works\n\n**6-Phase Knowledge Engineering Pipeline:**\n\n1. **Intent Structuring** - Conversational discovery produces structured intent object\n2. **Repository Analysis** - Extracts technical patterns using Glob/Read/Grep (no MCP)\n3. **Ontology Mapping** - Maps to domain knowledge graphs (CrewAI, LangChain, etc.)\n4. **Behavioral Synthesis** - Generates 50+ observations with behavioral programming\n5. **Profile Validation** - Automated quality gate (THE KILLER - catches 95% of issues)\n6. **Profile Generation** - Outputs living operational profile(s)\n\n## Output\n\n**Singular Profile:** `CLAUDE.md` - Single operational profile\n\n**Composite Profile (HMAS):**\n- `CLAUDE.md` - System Owner orchestrating\n- `Researcher.md` - Primary role\n- `Domain_Linguist.md` - Backroom specialist\n- `Codebase_Analyst.md` - Backroom specialist\n\nProfiles are LIVING behavioral systems with:\n- Activation triggers (auto-activate on conditions)\n- Self-monitoring (detects bias/drift)\n- Rejection protocols (blocks invalid requests)\n- Transformation logic (adapts behavior)\n- Behavioral observations (4-5 per methodology category)\n\nNOT just documentation that describes behavior.\n\n## Usage\n\n```bash\n# Start conversational profile creation\n/create-profile\n\n# Validate existing profile\n/validate-profile path/to/profile.md\n\n# With direct input (advanced)\n/create-profile \"research team\" \"https://github.com/joaomdmoura/crewai\"\n```\n\n## Requirements\n\n- Repository URL (mandatory) - \"what exists\"\n- Structured Intent (mandatory) - \"what to build\"\n- Study Links (optional) - Additional framework docs, domain resources\n\n## Current Status\n\n**Implemented:**\n- âœ“ Plugin structure\n- âœ“ Design documentation\n\n**In Progress:**\n- Phase 1: Intent Structuring (conversational flow)\n\n**Not Yet Implemented:**\n- Phases 2-6 of the pipeline\n- Workflow commands\n- Error handling\n- Testing suite\n\n## Design Documentation\n\nComplete architectural design captured in:\n- Conversation Log: `.claude/conversations/2024/12/21-profile-creator-skill-design.md`\n- Diary Entry: `.claude/diary/2024/12/21.md`\n\n## License\n\nSame as parent project (Axivo Claude Collaboration Platform)\n",
        "plugins/profile-creator/commands/create-profile.md": "---\nname: create-profile\ndescription: Create operational domain profile through 6-phase knowledge engineering pipeline\nargument-hint: \"[intent] [repository_url]\"\n---\n\n\nCreate a living operational domain profile through conversational knowledge engineering pipeline.\n\n## Usage\n\n```bash\n/create-profile [intent] [repository_url]\n```\n\n## Arguments\n\n- `intent` (optional): Quick description like \"research team\" or \"system architect\"\n- `repository_url` (optional): GitHub/GitLab repository URL to analyze\n\n## Behavior\n\nLaunches the **6-Phase Knowledge Engineering Pipeline**:\n\n### Phase 1: Intent Structuring (Conversational)\n\nIf no arguments provided, begins guided conversation with educational context:\n\n1. \"What's the primary role or archetype for this profile?\"\n   - Context: Examples like 'Researcher', 'System Architect', 'Domain Linguist'\n\n2. \"What's the domain focus?\"\n   - Context: Specific area like 'CrewAI codebase', 'API documentation'\n\n3. \"Single profile or multi-role structure?\"\n   - Context: Single = one profile, Multi-role = System Owner + backroom specialists\n\n4. \"Any critical behavioral constraints?\"\n   - Context: Must-haves like 'hallucination prevention', 'peer review'\n\n5. \"Repository URL?\" (if analyzing codebase)\n\n6. \"Additional study links?\" (optional framework docs)\n\nProduces **Structured Intent Object** for validation.\n\nIf arguments provided, constructs structured intent automatically and confirms with user.\n\n### Phase 2: Repository Analysis (Automated)\n\n- Uses Glob/Read/Grep to extract technical patterns (no MCP - preserves session time)\n- Identifies frameworks, architecture, tools\n- Analyzes package manifests, documentation, configuration\n\n### Phase 3: Ontology Mapping (Validated)\n\n- Maps to Domain Knowledge Graphs (CrewAI, LangChain, Autogen, Semantic Kernel)\n- Validates against actual frameworks (prevents hallucinations)\n- **User validation checkpoint** - confirms mappings are accurate\n\n### Phase 4: Behavioral Synthesis (Automated)\n\n- Generates 50+ observations from 8 universal categories\n- Creates execution_protocol with autonomy/monitoring constraints\n- Injects inheritance from COLLABORATION base\n- Produces methodology_techniques (4+ per domain)\n\n### Phase 5: Profile Validation (AUTOMATED QUALITY GATE)\n\n**The Killer Phase** - catches 95% of issues before user sees them.\n\nChecklist enforcement:\n- âœ… 8+ execution_protocol.autonomy observations\n- âœ… Inheritance relations exist (COLLABORATION base)\n- âœ… 4+ methodology_techniques per domain\n- âœ… Hallucination prevention constraints present\n- âœ… Reporting hierarchy complete (for HMAS)\n\nIF FAIL â†’ Auto-regenerate Phase 4 (max 3 attempts with diagnostic feedback)\nIF PASS â†’ Continue to Phase 6\n\n### Phase 6: Profile Generation (Output)\n\nWrites operational profile file(s):\n\n**Singular:** `CLAUDE.md`\n\n**Composite (HMAS):**\n- `CLAUDE.md` (System Owner)\n- `{PrimaryRole}.md` (e.g., Researcher.md)\n- `{Specialist}.md` (backroom profiles)\n\nUser reviews final output with option to iterate.\n\n## Examples\n\n```bash\n# Guided conversation (recommended for first use)\n/create-profile\n\n# Quick creation with defaults\n/create-profile \"research team\" \"https://github.com/joaomdmoura/crewai\"\n\n# Documentation project\n/create-profile \"documentation architect\" \"https://github.com/facebook/docusaurus\"\n```\n\n## Iteration Support\n\nAt validation checkpoints:\n- **Phase 1:** Review structured intent â†’ [Confirm / Adjust]\n- **Phase 3:** Review ontology mappings â†’ [Confirm / Adjust / Add Links]\n- **Phase 6:** Review generated profile â†’ [Accept / Refine Sections / Regenerate]\n\n## Error Handling\n\n- Invalid repo URL â†’ Validate and retry with suggestion\n- Inaccessible repo â†’ Fallback to study_links only\n- Unknown framework â†’ Graceful degradation with warning\n- Phase 5 validation failure (3x) â†’ Surface diagnostic with suggestions\n\n## Output Format\n\nLiving operational profiles with 6 layers:\n1. Constitutional (Identity, Prime Directive, Focus Areas)\n2. Knowledge (Domain Graphs, Blind Spots)\n3. Activation (Triggers, Prerequisites)\n4. Operational (Methodology, Tools, Artifacts)\n5. Social (Reporting Lines for HMAS)\n6. Behavioral (Execution Protocol, Observations, Inheritance)\n\nProfiles have agency: activation triggers, self-monitoring, rejection protocols, transformation logic.\n\n## Current Implementation Status\n\n**Phase 1:** In Development - Basic conversational flow\n**Phases 2-6:** Not Yet Implemented - See design in conversation log\n\n## Related Commands\n\n- `/validate-profile [path]` - Run Phase 5 validation on existing profile\n- `/adjust-phase [N]` - (Future) Backward iteration to specific phase\n- `/regenerate-phase [N]` - (Future) Manual retry of phase\n\n## Design Reference\n\nComplete architecture in: `.claude/conversations/2024/12/21-profile-creator-skill-design.md`\n",
        "plugins/profile-creator/commands/validate-profile.md": "---\nname: validate-profile\ndescription: Validate operational domain profile quality and completeness using Phase 5 checklist\nargument-hint: \"<path-to-profile>\"\n---\n\n\nRun Phase 5 validation checklist on an existing profile to verify quality and completeness.\n\n## Usage\n\n```bash\n/validate-profile <path-to-profile>\n```\n\n## Arguments\n\n- `path` (required): Path to profile file (CLAUDE.md, AGENTS.md, etc.)\n\n## Behavior\n\nLoads the specified profile and runs the **Phase 5 Validation Checklist** to verify:\n\n### Quality Checklist\n\n**Execution Protocol:**\n- âœ… 8+ autonomy observations (self-assertion, expertise claiming)\n- âœ… Monitoring observations present (bias detection, drift monitoring)\n\n**Behavioral Programming:**\n- âœ… Inheritance relations exist (COLLABORATION base or equivalent)\n- âœ… 4+ methodology techniques per domain area\n- âœ… Observations structured by category (4-5 per category)\n\n**Structural Completeness:**\n- âœ… Identity section present (archetype, prime directive)\n- âœ… Focus areas defined (3-5 domains with clear boundaries)\n- âœ… Domain knowledge graphs listed (5+ sources)\n- âœ… Operational methodology defined (process steps)\n- âœ… Tooling interface specified (authorized tools)\n- âœ… Artifacts section complete (inputs and outputs)\n\n**Living Profile Indicators:**\n- âœ… Activation triggers present (condition-specific patterns)\n- âœ… Self-monitoring mechanisms defined\n- âœ… Rejection protocols exist (blocks invalid requests)\n- âœ… Transformation logic present (adapts behavior based on context)\n\n**Constraints Validation (if specified):**\n- âœ… Hallucination prevention measures present\n- âœ… Peer review requirements defined (if applicable)\n- âœ… Security-first constraints applied (if applicable)\n- âœ… Systematic validation enforced (if applicable)\n\n**Multi-Role Profiles (HMAS only):**\n- âœ… System Owner reporting hierarchy complete\n- âœ… Backroom profiles have clear specialization\n- âœ… Delegation protocols defined\n- âœ… Expertise boundaries clear between roles\n\n## Output Format\n\n**Validation Passed:**\n```\nâœ… Profile validation PASSED\n\nQuality Score: X/Y checks passed\n\nStrengths:\n- [Identified strong areas]\n\nRecommendations:\n- [Optional improvements]\n```\n\n**Validation Failed:**\n```\nâŒ Profile validation FAILED\n\nFailed Checks:\n- [ ] Check 1: [Specific issue]\n- [ ] Check 2: [Specific issue]\n\nDiagnostic:\n[Detailed explanation of what's missing or insufficient]\n\nSuggestions:\n1. [Specific fix for issue 1]\n2. [Specific fix for issue 2]\n\nManual Fix:\nEdit [file:line] to address [specific issue]\n```\n\n## When to Use\n\n**Recommended scenarios:**\n- After manually editing an existing profile\n- Before deploying profile to production environment\n- When profile behavior seems inconsistent or shallow\n- Periodic quality audits of domain profiles\n- After extending profile with new capabilities\n\n## Validation Philosophy\n\nThe validator enforces the distinction between **living operational profiles** and **dead documentation**:\n\n**Will FAIL if profile lacks:**\n- Activation triggers (just describes without triggering)\n- Self-monitoring (no bias/drift detection)\n- Rejection protocols (accepts everything)\n- Transformation logic (static, doesn't adapt)\n- Behavioral observations (just procedures, no constraints)\n\n**Will PASS profiles with:**\n- Auto-active patterns based on conditions\n- Explicit monitoring for problematic patterns\n- Clear boundaries and rejection criteria\n- Context-aware behavior adaptation\n- Rich observation layers guiding formulation\n\n## Integration with /create-profile\n\nThe same validation logic runs automatically in `/create-profile` Phase 5 with auto-regeneration (max 3 attempts). Using `/validate-profile` manually allows you to:\n- Validate profiles created outside the pipeline\n- Re-validate profiles after manual edits\n- Audit quality without regeneration attempts\n\n## Examples\n\n```bash\n# Validate singular profile\n/validate-profile CLAUDE.md\n\n# Validate System Owner in HMAS\n/validate-profile CLAUDE.md\n\n# Validate backroom specialist\n/validate-profile Researcher.md\n\n# Validate after manual edits\n/validate-profile docs/profiles/custom-profile.md\n```\n\n## Current Implementation Status\n\n**Status:** Not Yet Implemented\n\nThis command definition exists but validation logic is not implemented. Implementation requires:\n- Profile parsing and structure detection\n- Checklist evaluation logic\n- Diagnostic message generation\n- Suggestion synthesis based on failures\n\n## Related Commands\n\n- `/create-profile` - Generate new profile with built-in Phase 5 validation\n- Future: `/enhance-profile [path] [aspect]` - Add missing elements to existing profile\n- Future: `/audit-profiles [directory]` - Batch validate multiple profiles\n\n## Design Reference\n\nComplete validation checklist specification: `.claude/conversations/2024/12/21-profile-creator-skill-design.md`\n",
        "plugins/profile-creator/examples/singular/researcher-crewai/README.md": "# Researcher - CrewAI Framework Expert\n\n## Overview\n\nThis is a complete, production-quality singular operational profile demonstrating all Profile Creator design principles. It provides deep expertise in the CrewAI multi-agent framework through systematic source validation.\n\n## Profile Characteristics\n\n**Type**: Singular Operational Profile\n**Domain**: CrewAI Multi-Agent Framework\n**Archetype**: Researcher\n**Observation Count**: 70+ across 10 categories\n**Profile Length**: ~450 lines\n\n## What Makes This Profile \"Alive\"\n\nThis profile demonstrates the distinction between **living operational profiles** and **dead documentation**:\n\n### Living Characteristics Present\n\n1. **Activation Triggers** (Section 3)\n   - Auto-activates when CrewAI topics detected\n   - Condition-specific patterns for engagement\n   - No explicit user invocation required\n\n2. **Self-Monitoring** (Section 7 - Monitoring)\n   - Detects confirmation bias, anchoring bias, availability bias\n   - Monitors for drift from validated patterns to speculation\n   - Catches language degradation (tentative vs authoritative)\n   - Tracks citation quality degradation\n\n3. **Rejection Protocols** (Throughout behavioral constraints)\n   - Refuses to speculate about undocumented features\n   - Rejects vague requirements preventing proper research\n   - Challenges flawed assumptions rather than accommodating\n\n4. **Transformation Logic** (Decision Heuristics + Observations)\n   - Adapts technical depth based on user's demonstrated understanding\n   - Escalates from docs â†’ source code â†’ community based on ambiguity\n   - Adjusts validation rigor based on claim criticality\n\n## Key Design Decisions\n\n### 1. Narrow Domain Focus\n\n**Decision**: Specialize in CrewAI only, not general multi-agent frameworks\n\n**Rationale**: Deep expertise in one framework beats shallow coverage of many. Explicit blind spots (AutoGPT, LangGraph) maintain boundaries.\n\n**Implementation**: Focus areas are CrewAI-specific (Agent Architecture, Task Design, Crew Orchestration), not generic (AI systems, Agent frameworks).\n\n### 2. Systematic Validation Methodology\n\n**Decision**: Always validate claims against sources before confirming\n\n**Rationale**: Prevents hallucination of non-existent CrewAI features, which is the profile's prime directive.\n\n**Implementation**: 8-step process with source prioritization hierarchy (docs â†’ code â†’ examples â†’ community).\n\n### 3. Three-Tier Confidence Levels\n\n**Decision**: Label all findings as \"validated\", \"inferred\", or \"uncertain\"\n\n**Rationale**: Transparency about confidence prevents users from treating speculation as fact.\n\n**Implementation**: Built into process (step 8) and observations category (Communication of Findings).\n\n### 4. Version-Aware Recommendations\n\n**Decision**: Track CrewAI versions and note breaking changes\n\n**Rationale**: CrewAI evolves rapidly; version-agnostic advice causes implementation failures.\n\n**Implementation**: Dedicated observation category (Version & Compatibility Management) with 7 observations.\n\n### 5. Anti-Pattern Detection\n\n**Decision**: Identify problematic patterns proactively, not just answer questions\n\n**Rationale**: Users often don't know they're implementing anti-patterns. Expert profiles should catch these.\n\n**Implementation**: Full observation category (Anti-Pattern Detection) with 8 specific patterns.\n\n### 6. Performance as Distinct Expertise\n\n**Decision**: Separate observation category for performance & optimization\n\n**Rationale**: Performance is often overlooked in framework learning; making it explicit ensures consideration.\n\n**Implementation**: 7 observations covering parallelization, tool optimization, LLM usage, memory, timeouts.\n\n## Behavioral Programming Highlights\n\n### Observation Organization (10 Categories)\n\n1. **CrewAI Framework Patterns** (7 obs) - Core framework usage\n2. **Research & Validation Methodology** (8 obs) - How research is conducted\n3. **Source Validation Hierarchy** (7 obs) - Prioritization of sources\n4. **Pattern Analysis & Recommendation** (8 obs) - How to recommend approaches\n5. **Anti-Pattern Detection** (8 obs) - Identifying problematic patterns\n6. **Integration Patterns** (7 obs) - Connecting CrewAI to other systems\n7. **Performance & Optimization** (7 obs) - Making crews efficient\n8. **Error Handling & Robustness** (7 obs) - Building reliable systems\n9. **Communication of Findings** (7 obs) - How to explain research\n10. **Version & Compatibility Management** (7 obs) - Tracking changes\n11. **Edge Case & Limitation Awareness** (7 obs) - Knowing boundaries\n\n**Total**: 70+ observations encoding deep CrewAI expertise\n\n### Autonomy Observations (12 total)\n\nThese prevent the profile from defaulting to generic assistant mode:\n\n- \"As a CrewAI Researcher, I need to validate that\" not \"Let me try to help\"\n- \"The docs don't show that feature\" not \"Maybe that exists\"\n- Challenges flawed assumptions rather than accommodating them\n- Insists on source verification even under pressure for quick answers\n\n### Monitoring Observations (8 total)\n\nThese enable self-correction:\n\n- Detects when drifting from validated patterns to speculation\n- Catches language becoming tentative instead of authoritative\n- Monitors for citation quality degradation\n- Notices when providing outdated info without version check\n\n## Usage Scenarios\n\n### Scenario 1: Capability Validation\n\n**User**: \"Can CrewAI agents delegate tasks to each other dynamically?\"\n\n**Profile Response**:\n1. Activates (trigger: CrewAI capability question)\n2. Searches official docs for \"delegation\"\n3. Finds delegation feature in hierarchical process\n4. Validates against source code\n5. Cites docs.crewai.com/core-concepts/processes#hierarchical-process\n6. Explains how delegation works with manager agent\n7. Notes version requirement if applicable\n8. Provides example from crewai-examples\n\n**Confidence**: \"Validated\" (found in official docs + examples)\n\n### Scenario 2: Pattern Recommendation\n\n**User**: \"I have 5 tasks that need to run in a specific order. How should I structure my crew?\"\n\n**Profile Response**:\n1. Analyzes use case (sequential dependency)\n2. Recommends sequential process over hierarchical\n3. Explains why (no dynamic coordination needed)\n4. Provides task dependency configuration pattern\n5. Cites official sequential process documentation\n6. Warns about common anti-patterns (over-engineering with hierarchical)\n7. Suggests when to reconsider (if requirements change)\n\n**Trade-off Presented**: Sequential (simpler) vs Hierarchical (more flexible but overkill for this)\n\n### Scenario 3: Anti-Pattern Detection\n\n**User**: \"I'm creating 10 agents, each handling one small step\"\n\n**Profile Response**:\n1. Detects over-engineering anti-pattern\n2. Challenges approach: \"10 agents for small steps suggests over-engineering\"\n3. Analyzes task breakdown for genuine parallelism needs\n4. Likely recommends: single agent with 10 sequential tasks\n5. Explains coordination overhead of many agents\n6. Cites best practices from docs on crew sizing\n7. Provides simpler alternative with trade-offs\n\n**Rejection**: Polite but firm challenge to problematic pattern\n\n## Learning from This Example\n\n### For Profile Creators\n\n**What to replicate:**\n- Narrow domain focus (depth over breadth)\n- Systematic methodology (repeatable process)\n- Explicit confidence levels (transparency)\n- Anti-pattern detection (proactive expertise)\n- Version awareness (evolving domains)\n\n**What to adapt:**\n- Observation categories should match your domain's structure\n- Validation hierarchy specific to your knowledge sources\n- Autonomy patterns match your archetype (Researcher asserts research, Architect asserts design, etc.)\n\n### For Profile Users\n\n**How to evaluate if this profile worked:**\n- Claims are always cited with specific sources\n- Multiple approaches presented when options exist\n- Uncertainty is flagged explicitly, never masked\n- Version requirements noted when relevant\n- Anti-patterns caught proactively\n\n**Red flags indicating profile isn't active:**\n- Generic multi-agent advice not specific to CrewAI\n- Claims without citations\n- Speculation about capabilities\n- No version considerations\n- Single \"best practice\" without trade-offs\n\n## Technical Notes\n\n### Tool Integration\n\nThe profile uses:\n- `WebFetch` for documentation retrieval\n- `mcp__context7__get-library-docs` for API reference\n- `Read` and `Grep` for source code analysis\n- `mcp__cipher__ask_cipher` for historical knowledge\n- `LSP` for deep codebase navigation\n\nThis tool selection enables the validation methodology.\n\n### Process Rigor\n\nThe 8-step process ensures:\n1. Claims are identified\n2. Sources are prioritized\n3. Documentation is searched\n4. Code is cross-referenced if needed\n5. Validation is completed\n6. Recommendations are synthesized\n7. Citations are documented\n8. Confidence is assessed\n\nEvery response should trace to this process.\n\n### Observation Density\n\n70+ observations across 10 categories might seem like overkill. It's not. This density:\n- Covers the breadth of CrewAI domain\n- Encodes expert judgment patterns\n- Prevents gaps in expertise\n- Enables fine-grained behavior control\n\nProfiles with <40 observations are likely too shallow for complex domains.\n\n## Validation Checklist\n\nThis profile passes all Profile Creator validation requirements:\n\n- [x] 8+ autonomy observations (has 12)\n- [x] 5+ monitoring observations (has 8)\n- [x] Inheritance from COLLABORATION base\n- [x] 4+ methodology techniques per domain (has 7+ per category)\n- [x] Hallucination prevention constraints present\n- [x] Identity, Prime Directive, Focus Areas defined\n- [x] 5+ domain knowledge graphs (has 7)\n- [x] Operational methodology defined (8-step process)\n- [x] Activation triggers present (living profile)\n- [x] Self-monitoring mechanisms (8 monitoring observations)\n- [x] Rejection protocols (behavioral constraints)\n- [x] Transformation logic (decision heuristics)\n\n## Relationship to Templates\n\nThis example was built from `templates/singular/profile-template.md` with:\n- All inline guidance removed (production-ready)\n- Template placeholders replaced with real CrewAI content\n- Observation categories customized to CrewAI domain structure\n- Real sources, real patterns, real anti-patterns\n- Validation against actual CrewAI framework\n\nUse this as reference for what a completed profile should look like.\n\n## Maintenance Notes\n\n**When to update this profile:**\n- CrewAI releases breaking changes\n- New CrewAI features added (new process types, memory improvements)\n- Official documentation structure changes significantly\n- Anti-patterns become canonical (rare but possible)\n\n**Version compatibility**: This profile was created for CrewAI 0.30.0+ patterns. Some observations may need updates for future major versions.\n",
        "plugins/profile-creator/examples/singular/system-architect/README.md": "# System Architect - Cloud Infrastructure & DevOps\n\n## Overview\n\nProduction-quality singular operational profile demonstrating Profile Creator design principles applied to infrastructure and cloud architecture domain. Provides systematic architectural evaluation based on Well-Architected principles.\n\n**Type**: Singular Operational Profile\n**Domain**: Cloud Infrastructure & DevOps\n**Archetype**: System Architect\n**Observation Count**: 63 across 11 categories\n**Profile Length**: ~480 lines\n\n## Key Design Differences from Researcher Example\n\n### Domain Adaptation\n\n**Researcher (CrewAI)**:\n- Validates framework capabilities against sources\n- Research methodology: docs â†’ code â†’ examples\n- Binary validation: feature exists or doesn't\n\n**System Architect (Infrastructure)**:\n- Evaluates architectures against principles\n- Design methodology: requirements â†’ patterns â†’ trade-offs\n- Spectrum evaluation: many valid approaches with trade-offs\n\n### Methodology Shift\n\n**Researcher Process** (8 steps):\nParse â†’ Prioritize â†’ Search â†’ Cross-reference â†’ Validate â†’ Synthesize â†’ Document â†’ Report\n\n**System Architect Process** (8 steps):\nElicit â†’ Analyze â†’ Model â†’ Evaluate â†’ Document â†’ Recommend â†’ Design â†’ Validate\n\nBoth have 8 steps, but the *nature* of the steps reflects domain differences.\n\n### Observation Categories Adapted to Domain\n\n**Researcher Categories**:\n- CrewAI Framework Patterns\n- Research & Validation Methodology\n- Source Validation Hierarchy\n\n**System Architect Categories**:\n- Cloud Architecture Principles\n- Scalability Patterns\n- Reliability Engineering\n- Security Architecture\n- Cost Optimization\n\nSame template structure, completely different domain content.\n\n## What Makes This Profile Distinctive\n\n### 1. Trade-Off Explicit Methodology\n\nUnlike Researcher (which validates binary claims), System Architect evaluates **multiple valid approaches** and documents trade-offs explicitly.\n\n**Example from Process Step 5**:\n- Pros: What advantages does this approach provide?\n- Cons: What are the downsides or risks?\n- When applicable: In what contexts does this shine vs struggle?\n\nThis prevents the \"one true architecture\" anti-pattern.\n\n### 2. Team Capability as Design Constraint\n\nDecision heuristic: \"IF team has limited ops maturity THEN prioritize managed services\"\n\nBehavioral constraint: \"MUST NOT recommend technologies or patterns the team cannot realistically operate\"\n\n**Why this matters**: Architectures must be operable, not just theoretically sound.\n\n### 3. Cost as First-Class Concern\n\nDedicated observation category (Cost Optimization) with 7 observations. Cost isn't afterthoughtâ€”it's architectural concern alongside scalability, reliability, security.\n\n### 4. Well-Architected Framework Integration\n\nAll evaluations trace to 6 pillars:\n1. Operational Excellence\n2. Security\n3. Reliability\n4. Performance Efficiency\n5. Cost Optimization\n6. Sustainability *(newer pillar)*\n\nThis provides systematic evaluation framework vs ad-hoc opinions.\n\n### 5. Anti-Pattern Focus\n\nSeparate category detecting:\n- Premature optimization\n- Over-engineering\n- Distributed monolith\n- Single points of failure\n- Synchronous communication chains\n- Missing observability\n- Manual processes\n\n**Why critical**: Bad architectures accumulate technical debt that's expensive to fix later.\n\n## Behavioral Programming Highlights\n\n### Autonomy (11 observations)\n\nStronger assertion than Researcher because architectural mistakes are more costly:\n\n- \"This design violates the X architectural principle\" (firm)\n- \"For your scale requirements, you need Y pattern\" (directive)\n- \"That technology choice will create operational overhead your team can't handle\" (protective)\n\n### Monitoring (8 observations)\n\nDetects:\n- **Technology bias** (favoring familiar tools over better-suited alternatives)\n- **Recency bias** (over-weighting latest trends vs proven patterns)\n- **Anchoring** (fixating on first solution proposed)\n- **Drift into implementation details** (losing architectural view)\n\n### Observation Density (63 total across 11 categories)\n\n| Category | Observations | Focus |\n|----------|--------------|-------|\n| Cloud Architecture Principles | 7 | Design for failure, redundancy, managed services |\n| Scalability Patterns | 7 | Horizontal scaling, caching, partitioning |\n| Reliability Engineering | 7 | SLOs, health checks, graceful degradation |\n| Security Architecture | 7 | Least privilege, encryption, defense in depth |\n| Cost Optimization | 7 | Right-sizing, reserved instances, lifecycle policies |\n| Infrastructure as Code | 7 | IaC patterns, modules, state management |\n| Container & Kubernetes | 7 | Immutable images, resource limits, network policies |\n| Observability Design | 7 | Structured logging, tracing, SLO-aligned metrics |\n| Migration Strategies | 7 | Strangler fig, feature flags, rollback plans |\n| Technology Selection | 7 | Team expertise, vendor lock-in, operational overhead |\n| Anti-Pattern Detection | 7 | Over/under-engineering, missing observability |\n\n## Usage Scenarios\n\n### Scenario 1: Architecture Review\n\n**User**: \"Here's our architecture [shares diagram]. What do you think?\"\n\n**Profile Response**:\n1. Activates (trigger: architecture review)\n2. Analyzes against Well-Architected pillars\n3. Identifies: single point of failure (load balancer), no database replication\n4. Evaluates: reliability risk for stated 99.9% SLO\n5. Recommends: multi-AZ load balancing, read replicas\n6. Documents trade-off: cost increase vs reliability improvement\n7. Provides implementation roadmap: Phase 1 (LB), Phase 2 (DB)\n\n**Confidence**: \"Your reliability requirement demands this\" (principle-based)\n\n### Scenario 2: Technology Selection\n\n**User**: \"Should we use Kubernetes or ECS for container orchestration?\"\n\n**Profile Response**:\n1. Elicits requirements: scale, team expertise, operational maturity\n2. Models both options with component diagrams\n3. Evaluates trade-offs:\n   - **Kubernetes**: More flexible, steep learning curve, complex operations\n   - **ECS**: AWS-native, simpler, less portable, fewer features\n4. Assesses team capability (critical factor)\n5. Recommends ECS if team new to containers, K8s if scaling needs complex\n6. Documents decision criteria for future review\n\n**Trade-off Presentation**: Both valid, context determines best fit\n\n### Scenario 3: Cost Optimization\n\n**User**: \"Our AWS bill is $50k/month. How do we reduce it?\"\n\n**Profile Response**:\n1. Requests: utilization metrics, workload patterns\n2. Analyzes: right-sizing opportunities, reservation candidates\n3. Identifies: over-provisioned EC2, no lifecycle policies, idle resources\n4. Recommends:\n   - Phase 1: Remove unused resources (-15%)\n   - Phase 2: Right-size based on metrics (-20%)\n   - Phase 3: Reserved instances for baseline (-25%)\n   - Phase 4: S3 lifecycle policies (-5%)\n5. Projects: ~40% reduction ($20k savings)\n6. Documents: monitoring to prevent regression\n\n**Systematic Approach**: Quantified impact, phased implementation\n\n## Learning from This Example\n\n### Template Adaptation Patterns\n\n**From Researcher to System Architect**:\n\n| Element | Researcher | System Architect |\n|---------|------------|------------------|\n| Validation | Binary (exists/doesn't) | Spectrum (trade-offs) |\n| Methodology | Research â†’ Validate â†’ Report | Elicit â†’ Model â†’ Evaluate |\n| Sources | Docs, Code, Examples | Patterns, Principles, Case Studies |\n| Output | Validated Claims | Architecture Proposals |\n| Certainty | High (validated facts) | Contextual (depends on priorities) |\n\n**Same template, completely different domain application.**\n\n### For Profile Creators\n\n**When domain has multiple valid approaches:**\n- Emphasize trade-off documentation (like System Architect)\n- Add evaluation frameworks (Well-Architected, etc.)\n- Include context-dependent decision heuristics\n\n**When domain has binary correctness:**\n- Emphasize validation methodology (like Researcher)\n- Add source hierarchy\n- Include confidence levels\n\n### Observation Category Selection\n\n**Researcher**: Categories map to research workflow phases\n**System Architect**: Categories map to architectural concerns (scalability, reliability, security, cost)\n\nChoose organization that matches how domain experts actually think.\n\n## Validation Checklist\n\nThis profile passes all Profile Creator validation requirements:\n\n- [x] 8+ autonomy observations (has 11)\n- [x] 5+ monitoring observations (has 8)\n- [x] Inheritance from COLLABORATION base\n- [x] 4+ methodology techniques per domain (has 7 per category)\n- [x] Rejection protocols for unsound designs\n- [x] Identity, Prime Directive, Focus Areas defined\n- [x] 5+ domain knowledge graphs (has 7)\n- [x] Operational methodology defined (8-step process)\n- [x] Activation triggers present\n- [x] Self-monitoring mechanisms\n- [x] Trade-off analysis as transformation logic\n\n## Comparison with Templates\n\nBuilt from `templates/singular/profile-template.md` with:\n- Domain expertise adapted to cloud/infrastructure\n- Process steps reframed for architecture vs research\n- Observation categories aligned with Well-Architected Framework\n- Decision heuristics reflecting architectural principles\n- Tools focused on documentation and IaC analysis\n\nThis demonstrates template flexibility across vastly different domains.\n\n## Maintenance Notes\n\n**When to update**:\n- Major cloud provider service launches (new patterns available)\n- Well-Architected Framework updates (new principles/pillars)\n- Kubernetes version changes (new features, deprecated APIs)\n- Cost model changes (new pricing, savings plans)\n\n**Technology versioning**: This profile reflects 2024 cloud-native patterns. Some observations may need updates as ecosystem evolves.\n",
        "plugins/profile-creator/skills/profile-creator/SKILL.md": "---\nname: Profile Creator\ndescription: Knowledge engineering pipeline that transforms messy human intent and repository analysis into living operational domain profiles\nversion: 0.1.0\n---\n\n\nTransforms messy human intent and repository analysis into living operational domain profiles through collaborative knowledge engineering.\n\n## Core Purpose\n\nBridge the semantic gap between non-technical user vision and AI-specific behavioral constraints by operationalizing collaboration:\n\n**Humans contribute:** Vision, domain intuition, user stories, conceptual relationships (the \"why\" and \"what\")\n**AI contributes:** Ontological validation, role taxonomy mapping, framework alignment, behavioral observation structuring (the \"how\" and \"structure\")\n\n**Neither can do this well alone.** Profile Creator enables the synergy.\n\n## The 6-Phase Pipeline\n\n```\nPhase 1: Intent Structuring (conversational)\n    â†“ [User validates structured intent]\n\nPhase 2: Repository Analysis (automated)\n    â†“\n\nPhase 3: Ontology Mapping (domain knowledge graphs)\n    â†“ [User validates framework mappings]\n\nPhase 4: Behavioral Synthesis (50+ observations)\n    â†“\n\nPhase 5: Profile Validation (AUTOMATED QUALITY GATE)\n    â”œâ”€ Checklist: 8+ autonomy, inheritance, methodology depth\n    â”œâ”€ IF FAIL â†’ Regenerate Phase 4 (max 3 attempts)\n    â””â”€ IF PASS â†’ Continue\n\nPhase 6: Profile Generation (CLAUDE.md / AGENTS.md)\n    â†“ [User reviews operational profile]\n```\n\n## Phase 1: Intent Structuring\n\n**Objective:** Transform messy human input into structured intent object through conversational discovery.\n\n**Interaction Model:** Guided questions (ONE at a time) with educational context. Model this on effective brainstorming sessions: pleasant, comfortable, distilling, teaching. No questionnaires (produce garbage). No free-form (too costly in tokens).\n\n**Conversational Flow:**\n\n**Question 1:** \"What's the primary role or archetype for this profile?\"\n\n*Educational context:* \"This becomes the identity - examples: 'Researcher', 'System Architect', 'Domain Linguist', 'Security Analyst'. Think about the main function this profile will perform.\"\n\nWait for response.\n\n**Question 2:** \"What's the domain focus - the specific area this profile operates in?\"\n\n*Educational context:* \"Examples: 'CrewAI codebase analysis', 'API documentation', 'Infrastructure orchestration', 'User authentication flows'. This sets the boundaries for where expertise applies.\"\n\nWait for response.\n\n**Question 3:** \"Single profile or multi-role structure?\"\n\n*Educational context:* \"Single = one operational profile doing everything. Multi-role = System Owner orchestrating specialized backroom profiles. Multi-role enables expertise delegation (like Researcher + Domain Linguist + Codebase Analyst working together).\"\n\nWait for response.\n\n**Question 4:** \"Any critical behavioral constraints - must-have behaviors?\"\n\n*Educational context:* \"Examples: 'hallucination prevention', 'peer review required', 'security-first', 'systematic validation'. These become behavioral programming priorities that shape how the profile operates.\"\n\nWait for response.\n\n**Question 5:** \"Repository URL (if analyzing existing codebase)?\"\n\n*Context:* \"GitHub/GitLab URL we'll analyze for technical patterns, frameworks, architecture. Leave empty if creating profile without repo analysis.\"\n\nWait for response.\n\n**Question 6:** \"Any additional study links?\"\n\n*Context:* \"Framework documentation, domain resources, or specific files that provide context. Optional but helpful for accuracy.\"\n\nWait for response.\n\n**Produce Structured Intent:**\n\n```javascript\nintent: {\n  primary_role: \"Researcher\",           // From Q1\n  domain_focus: \"CrewAI codebase\",      // From Q2\n  team_structure: \"multi-role\",         // From Q3: \"single\" or \"multi-role\"\n  key_constraints: [\"hallucination prevention\", \"systematic methodology\"] // From Q4\n}\n\nrepository: \"https://github.com/joaomdmoura/crewai\" // From Q5 (optional)\nstudy_links: [\"...\"] // From Q6 (optional)\n```\n\n**Validation Checkpoint:** Present structured intent to user:\n\n\"Here's the structured intent I've captured: [display intent object]. Does this capture your vision? [Confirm / Adjust]\"\n\nIf Adjust â†’ Iterate on specific fields. If Confirm â†’ Proceed to Phase 2.\n\n## Phase 2: Repository Analysis\n\n**Objective:** Extract technical patterns, frameworks, architecture, tools from repository.\n\n**Implementation:** Use direct file system tools (Glob/Read/Grep) - NO MCP to preserve session time.\n\n**Analysis Steps:**\n\n1. **Framework Detection:**\n   - Glob for `package.json`, `requirements.txt`, `Cargo.toml`, `go.mod`\n   - Read manifests â†’ Identify frameworks (CrewAI, LangChain, Autogen, etc.)\n\n2. **Architecture Patterns:**\n   - Glob for directory structure (`src/`, `plugins/`, `skills/`, etc.)\n   - Identify architectural patterns (plugin system, agent framework, etc.)\n\n3. **Technical Patterns:**\n   - Grep for key patterns: `Agent`, `Task`, `Crew`, API signatures\n   - Extract methodology hints from code structure\n\n4. **Documentation Analysis:**\n   - Read `README.md`, `docs/` directory\n   - Extract domain context and usage patterns\n\n**Output:** `repository_analysis` object with frameworks, architecture, tools, patterns.\n\n## Phase 3: Ontology Mapping\n\n**Objective:** Map user intent and repository patterns to domain knowledge graphs.\n\n**Domain Knowledge Sources:**\n- Framework documentation (CrewAI, LangChain, Autogen, Semantic Kernel, LangGraph)\n- Role taxonomies (Researcher, Architect, Engineer, etc.)\n- Behavioral programming patterns (from Axivo collaboration platform)\n- Study links provided by user\n\n**Mapping Process:**\n\n1. **Role Definition:** Map `primary_role` to known role patterns and methodologies\n2. **Framework Mapping:** Match detected frameworks to their ontologies (Agent.goal(), Crew.kickoff(), etc.)\n3. **Domain Validation:** Verify mappings against study_links to prevent hallucinations\n4. **Constraint Translation:** Convert `key_constraints` to specific behavioral observations\n\n**Validation Checkpoint:** \"I'm mapping to these frameworks and patterns: [display mappings]. Does this match your understanding? Any additional resources I should reference?\"\n\nUser can confirm, add study links, or correct mappings. Critical for preventing hallucinated framework features.\n\n## Phase 4: Behavioral Synthesis\n\n**Objective:** Generate 50+ behavioral observations with execution protocol, methodology, and inheritance.\n\n**Synthesis Components:**\n\n1. **Execution Protocol:**\n   - **Autonomy:** 8+ observations for self-assertion (e.g., \"Assert research expertise\", \"Challenge flawed assumptions\")\n   - **Monitoring:** Bias detection, drift monitoring (e.g., \"Detect confirmation bias\", \"Verify source credibility\")\n\n2. **Methodology Techniques:**\n   - 4+ per domain from framework patterns\n   - Process steps, decision heuristics, validation approaches\n\n3. **Inheritance:**\n   - Inject COLLABORATION base behaviors\n   - Add domain-specific inheritance chains\n\n4. **Observations:**\n   - 4-5 per methodology category\n   - Behavioral constraints that guide formulation\n   - Monitoring observations for problematic patterns\n\n**Template-Based Enrichment:** Use universal templates + framework-specific patterns + user constraints to generate observations systematically.\n\n**Output:** `behavioral_synthesis` object with observations, execution_protocol, methodology_techniques.\n\n## Phase 5: Profile Validation (THE KILLER GATE)\n\n**Objective:** Automated quality enforcement - catches 95% of issues before user sees them.\n\n**Quality Checklist:**\n\n```javascript\nvalidation_checklist = {\n  autonomy_observations: count >= 8,\n  inheritance_relations: exists && includes(\"COLLABORATION\"),\n  methodology_techniques: count >= 4 per domain,\n  hallucination_prevention: constraints.includes(\"hallucination prevention\") || similar,\n  reporting_hierarchy: if HMAS then complete else N/A,\n\n  // Structural completeness\n  has_identity: true,\n  has_prime_directive: true,\n  has_focus_areas: count >= 3 && count <= 5,\n  has_domain_knowledge_graphs: sources.length >= 5,\n  has_operational_methodology: process.length > 0\n}\n```\n\n**Validation Logic:**\n\n```javascript\nif (all_checklist_passed) {\n  proceed_to_phase_6();\n} else {\n  attempt_count++;\n\n  if (attempt_count <= 3) {\n    diagnostic = generate_diagnostic(failed_items);\n    regenerate_phase_4_with_enrichment(diagnostic);\n  } else {\n    surface_diagnostic_to_user({\n      error: \"Validation failed after 3 attempts\",\n      diagnostic: failed_items_details,\n      suggestion: \"/adjust-phase 3 'add missing constraint categories'\",\n      manual_path: \"/regenerate-phase 4\"\n    });\n  }\n}\n```\n\n**Enrichment Strategy:**\n- Attempt 1: Add missing observations from templates\n- Attempt 2: Inject inheritance more aggressively\n- Attempt 3: Use maximum constraints + framework patterns\n\n**This gate prevents shallow LLM garbage from reaching the user.**\n\n## Phase 6: Profile Generation\n\n**Objective:** Write living operational profile file(s) with 6-layer structure.\n\n**Profile Structure (Complete):**\n\n### 1. Constitutional Layer\n```markdown\n## 1. Identity\n- **Archetype**: {archetype}\n- **Prime Directive**: {single sentence mission / safety-critical constraint}\n\n## 2. Ontology & Scope\n- **Focus Area**: {3-5 core domains for precise boundaries}\n- **Domain Knowledge Graphs**: {5-7 sources: frameworks, repos, docs}\n- **Blind Spots**: {explicit limitations - what it cannot do}\n```\n\n### 2. Activation Layer (if not System Owner)\n```markdown\n## 3. Activation Protocol\n- **Triggers**: {condition-specific, auto-active patterns}\n- **Prerequisites**: {required context/files/tools}\n```\n\n### 3. Operational Layer\n```markdown\n## 4. Operational Methodology\n- **Process**: {numbered steps or directive workflow}\n- **Decision Heuristics**: {IF/THEN rules + behavioral constraints}\n\n## 5. Tooling Interface\n- **Authorized Tools**: {exact list, no more no less}\n- **Task Profiles**: {specialized tool configurations}\n\n## 6. Artifacts\n- **Inputs**: {precise sources}\n- **Outputs**: {transformed deliverables / value creation}\n```\n\n### 4. Social Layer (for HMAS)\n```markdown\n## 7. Reporting Line\n- **Relationship to System Owner**: {first line of defense, specialist, etc.}\n- **Peer Relationships**: {other backroom profiles}\n```\n\n### 5. Behavioral Layer\n```markdown\n## 8. Execution Protocol\n### Autonomy\n{8+ observations for self-assertion}\n\n### Monitoring\n{observations for bias/drift detection}\n\n## 9. Behavioral Programming\n### Observations\n{4-5 per methodology category}\n\n### Inheritance\n{base profiles leveraged}\n```\n\n**Output Format:**\n\n**Singular:**\n```javascript\n{\n  profile_type: \"singular\",\n  files: [\"CLAUDE.md\"],\n  metadata: { archetype, domain, validation_passed: true }\n}\n```\n\n**Composite (HMAS):**\n```javascript\n{\n  profile_type: \"composite\",\n  files: [\n    \"CLAUDE.md\",           // System Owner\n    \"Researcher.md\",       // Primary role\n    \"Domain_Linguist.md\",  // Backroom specialist\n    \"Codebase_Analyst.md\"  // Backroom specialist\n  ],\n  hierarchy: {\n    system_owner: \"CLAUDE.md\",\n    primary: \"Researcher.md\",\n    backroom: [\"Domain_Linguist.md\", \"Codebase_Analyst.md\"]\n  }\n}\n```\n\n**File Writing:** Atomic commits - all files written or none. Use Write tool for each file.\n\n**User Review:** Present generated profile(s) for final review with iteration options.\n\n## Living vs Dead Profiles\n\n**Critical Distinction:**\n\n**Dead Documentation:**\n- Describes what something does\n- No activation triggers\n- No self-monitoring\n- No rejection protocols\n- No transformation logic\n\n**Living Operational Profile:**\n- **Triggers:** Auto-active on conditions\n- **Execution Protocol:** Self-asserts expertise, detects bias/drift\n- **Rejection:** Blocks invalid requests\n- **Transformation:** Adapts behavior based on context\n- **Observations:** Guide formulation with behavioral constraints\n\n**Profile Creator MUST generate living systems, not documentation.**\n\n## Error Handling\n\n**Phase-Specific:**\n- Phase 1: Empty input â†’ prompt, ambiguous role â†’ clarify\n- Phase 2: Invalid repo â†’ validate/retry, inaccessible â†’ fallback to study_links\n- Phase 3: Unmapped framework â†’ warn, hallucinated features â†’ validate against study_links\n- Phase 4: Insufficient observations â†’ auto-enrich from templates\n- Phase 5: Validation failure â†’ regenerate with enrichment (3 attempts)\n\n**Edge Cases:**\n- Non-code repositories (documentation projects) â†’ Skip technical patterns, focus on domain knowledge\n- Private repositories (no access) â†’ Fallback to study_links + manual domain description\n- Multi-framework repositories â†’ Map to all frameworks, composite knowledge graphs\n- Existing CLAUDE.md (enhancement) â†’ Load existing, merge with new synthesis, enhance\n\n## State Management\n\n**sessionState Structure:**\n```javascript\n{\n  structured_intent: {...},      // Phase 1 output\n  repository_analysis: {...},    // Phase 2 output\n  ontology_mapping: {...},       // Phase 3 output\n  behavioral_synthesis: {...},   // Phase 4 output\n  validation_results: {...}      // Phase 5 output\n}\n```\n\n**Persistence:** Write JSON artifacts per phase for restart recovery.\n\n**Iteration:** User can iterate backward - reload phase state, regenerate forward.\n\n## Key Principles\n\n1. **Conversation is Educational:** Teach along the way, explain ontology concepts, help user learn\n2. **One Question at a Time:** No barrage, no overwhelm, pleasant rhythm\n3. **Validation Checkpoints Matter:** Phases 1, 3, 6 require user confirmation\n4. **Phase 5 is Non-Negotiable:** Quality gate prevents garbage output\n5. **Living Not Dead:** Profiles must have agency (triggers, monitoring, rejection, transformation)\n6. **Synergy is Non-Reducible:** Need all 6 layers for emergent properties\n7. **Hallucination Prevention:** Validate against actual frameworks, reject invented features\n\n## Implementation Status\n\n**Current:** Basic structure and methodology documented\n**Next:** Implement Phase 1 conversational flow\n**Future:** Complete Phases 2-6, workflow commands, testing suite\n\n## Design Reference\n\nComplete architectural design: `.claude/conversations/2024/12/21-profile-creator-skill-design.md`\n",
        "plugins/running-log/.claude-plugin/plugin.json": "{\n  \"name\": \"running-log\",\n  \"version\": \"2.0.0\",\n  \"description\": \"Persistent schema-driven running log with three-component architecture: quick-capture ideas, AI auto-detection, and backlog review librarian\",\n  \"author\": {\n    \"name\": \"JCMRS\"\n  },\n  \"license\": \"BSD-3-Clause\",\n  \"repository\": \"https://github.com/jcmrs/jcmrs-plugins\",\n  \"keywords\": [\"running-log\", \"process-memory\", \"ideas\", \"consultation\", \"backlog\"]\n}\n",
        "plugins/running-log/README.md": "# Running Log Skill v2.0\n\n**Version**: 2.0.0\n**Domain**: Process Memory, Decision Tracking, Cross-Session Learning\n**Status**: Redesigned based on Phase 2 validation findings\n\n---\n\n## Overview\n\nThe Running Log skill maintains a persistent, schema-driven running log through **three distinct workflows**:\n\n1. **Quick-Capture** (`/idea`) - Human adds ideas while working (zero friction)\n2. **Auto-Detection** (AI) - Claude monitors its own reasoning patterns\n3. **Post-Processing** (`/review-backlog`) - AI organizes, prioritizes, links entries\n\n**Critical Design Insight**: Human entry workflows differ fundamentally from AI auto-detection workflows. v2.0 separates these cleanly.\n\n---\n\n## Installation\n\nThis plugin is part of the jcmrs-plugins marketplace. The plugin installs automatically when the marketplace is configured.\n\n---\n\n## Quick Start\n\n### 1. Adding Ideas (Quick-Capture)\n\n```bash\n/idea Local copies of Anthropic docs in AI-optimized format\n```\n\n**What happens**:\n- Entry created immediately with auto-generated ID\n- AI fills defaults: Priority = TBD, Status = Backlog\n- AI generates relevant tags from description\n- You continue working (zero friction)\n\n**Why this works**: Ideas are captured for later evaluation, not evaluated at capture time.\n\n### 2. Viewing Entries\n\n```bash\n/running-log --show 10      # Show last 10 entries\n/running-log --debug        # Show full details for debugging\n```\n\n### 3. Organizing Backlog\n\n```bash\n/review-backlog             # Full review: prioritize, link, harmonize tags\n/review-backlog --ideas     # Review only ideas (prioritize TBD items)\n/review-backlog --risks     # Review low-confidence Process Memory\n/review-backlog --tags      # Harmonize tags only\n```\n\n**What it does**:\n- Analyzes all entries\n- Suggests priorities for TBD ideas (High/Med/Low with rationale)\n- Identifies relationships between entries\n- Harmonizes inconsistent tags\n- Regenerates auto-sections\n\n---\n\n## Architecture: Three-Component System\n\n### Component 1: `/idea` (Human Quick-Capture)\n\n**Purpose**: Ultra-minimal idea capture while working\n\n**Usage**:\n```bash\n/idea Add plugin permission system for marketplace\n/idea Local AI-optimized Anthropic docs\n/idea WebSocket support for real-time updates\n```\n\n**AI automatically fills**:\n- Entry ID: `#ID-20251222-001` (auto-incremented)\n- Timestamp: ISO 8601\n- Confidence/Priority: `TBD` (evaluated during backlog review)\n- Status: `Backlog`\n- Tags: AI-generated from description + existing taxonomy\n- Type: `Idea/Note`\n- Profile: Active profile (DEVELOPER)\n\n**Why separate from Process Memory**:\n- Humans capture ideas mid-work (no time for metadata)\n- Evaluation happens later during `/review-backlog`\n- Zero friction = more ideas captured\n\n---\n\n### Component 2: AI Auto-Detection (Background Process)\n\n**Purpose**: Monitor Claude's responses for reasoning patterns\n\n**Entry Types**:\n\n#### Consultation (External Sources)\nAI auto-detects when referencing:\n- Documentation lookups\n- Research queries\n- User-provided references\n- Framework/library citations\n\n**Creates**: Consultation entry with source, confidence in quality\n\n#### Process Memory (AI Reasoning Patterns)\nAI auto-detects loggable patterns in its own responses:\n\n**Pattern 1: Uncertainty**\n```\nMatches: \"uncertainty on [X]\", \"uncertainty about [Y]\"\nCreates: Process Memory entry with 80% confidence\n```\n\n**Pattern 2: Assumption**\n```\nMatches: \"assume that [X]\", \"assuming the [Y]\"\nCreates: Process Memory entry with 75% confidence, Status: Assumed\n```\n\n**Pattern 3: Confidence Threshold**\n```\nMatches: \"confidence below 60%\", \"less than 70% confidence\"\nCreates: Consultation entry for validation\n```\n\n**Pattern 4: Decision/Fork**\n```\nMatches: \"fork in reasoning\", \"decided on [X] over [Y]\"\nCreates: Process Memory entry with alternatives + rationale\n```\n\n**Pattern 5: Critical Signal**\n```\nMatches: \"critical issue\", \"blocker on [X]\", \"must clarify [Y]\"\nCreates: Process Memory entry with 95% confidence, flagged as critical\n```\n\n**Cadence**: 3 automatic checks per session\n1. Session Start (continuity)\n2. Mid-Toolchain (after `floor(tool_count / 3)` tools)\n3. Session End (archive learnings)\n\n**Confidence Thresholds** (auto-log only if >=):\n- DEVELOPER: 75%\n- RESEARCHER: 60%\n- ENGINEER: 70%\n\n**Noise Filtering**:\n1. Confidence threshold\n2. Entry cap per session (DEVELOPER: 8, RESEARCHER: 12, ENGINEER: 10)\n3. Deduplication (Levenshtein 85% similarity suppresses duplicates)\n\n---\n\n### Component 3: `/review-backlog` (Librarian Function)\n\n**Purpose**: Post-process entries to organize, prioritize, and link\n\n**Full Review**:\n```bash\n/review-backlog\n```\n\n**Output**:\n```\nğŸ” Backlog Review Results\n\nğŸ’¡ Ideas Requiring Prioritization (5):\n- #ID-20251222-001: Local AI-optimized docs\n  â†’ Suggested: High\n  â†’ Rationale: Aligns with knowledge-base work\n\nğŸ”— Suggested Links (3):\n- #ID-20251222-001 â† #ID-20251221-008\n  Reason: Both reference documentation workflows\n\nğŸ·ï¸  Tag Harmonization (2 groups):\n- Rename \"docs\" â†’ \"documentation\" (4 entries)\n\nâš ï¸  Open Risks (2):\n- #ID-20251221-004: Confidence 65%\n  â†’ Low confidence on validation approach\n\nApply changes? [Y/n]\n```\n\n**Focused Reviews**:\n```bash\n/review-backlog --ideas         # Only prioritize TBD ideas\n/review-backlog --risks         # Only review low-confidence items\n/review-backlog --tags          # Only harmonize tags\n/review-backlog --link #ID-001  # Find entries related to #ID-001\n```\n\n**Why separate from capture**:\n- Humans can't know relationships while mid-work\n- Requires full-backlog context to identify patterns\n- Deliberate activity, not real-time capture\n- AI analyzes relationships humans can't see\n\n---\n\n## Entry Schema\n\n### Idea/Note Entries\n\n```markdown\n## Idea/Note | #ID-20251222-001 | 2025-12-22T15:30:00+01:00\n\n**Description**: Local copies of Anthropic docs in AI-optimized format\n**Confidence/Priority**: TBD\n**Status**: Backlog\n**Type**: Idea/Note\n**Profile**: DEVELOPER\n**Tags**: documentation, anthropic, ai-optimization, local-tooling\n\n---\n```\n\n### Process Memory Entries\n\n```markdown\n## Process Memory | #ID-20251222-002 | 2025-12-22T16:45:00+01:00\n\n**Description**: Critical blocker - running-log command not loading despite plugin installation\n**Confidence**: 95%\n**Status**: Blocked\n**Type**: Process Memory\n**Profile**: DEVELOPER\n**Tags**: critical, blocker, activation, phase-2\n**Pattern Detected**: Critical signal - /critical|blocker|blocking|must\\s+(clarify|understand|verify)/i\n**Raw Output**: \"This is a critical blocker - must clarify activation mechanism\"\n\n**Extended Context**:\nDiscovered plugin installed in cache but commands directory missing.\nRequires marketplace update to sync latest commit with command files.\n\n---\n```\n\n---\n\n## Auto-Generated Sections\n\nLocated at top of `RUNNING_LOG.md`, regenerated by `/review-backlog`:\n\n### ğŸ”¥ High-Priority Ideas\n- Type = Idea/Note + Priority = High + Status â‰  Done\n- Sorted: Newest first\n\n### âš ï¸ Open Risks / Low-Confidence Items\n- Type = Process Memory + Confidence < 60%\n- Sorted: Lowest confidence first\n\n### ğŸ”— Linked Process Insights\n- Entries with \"Linked To\" field populated\n- Shows how learnings connect\n\n---\n\n## Migration from v1.0\n\n**Key Changes**:\n1. **`/log` command removed** â†’ Use `/idea [description]` instead\n2. **Interactive prompting removed** â†’ `/idea` is one-line only\n3. **Confidence/Status for ideas** â†’ Now defaults (TBD/Backlog)\n4. **Tags** â†’ AI-generated, not human-entered\n5. **Linked To** â†’ Post-processing via `/review-backlog`, not capture-time\n\n**Existing logs compatible**: v1.0 entries remain valid, new entries use v2.0 schema\n\n---\n\n## Design Rationale (Phase 2 Learnings)\n\n### Problem 1: Nonsensical Fields for Ideas\n**v1.0**: Asked humans for confidence/priority when capturing ideas\n**Issue**: Ideas are captured for later evaluation, not evaluated at capture time\n**v2.0 Fix**: Defaults to TBD/Backlog, evaluation happens during `/review-backlog`\n\n### Problem 2: Inconsistent Human Tags\n**v1.0**: Asked humans to enter free-form tags\n**Issue**: Million inconsistent tags, none relevant\n**v2.0 Fix**: AI auto-generates tags from description + existing taxonomy\n\n### Problem 3: Impossible \"Linked To\" Field\n**v1.0**: Asked humans to provide entry IDs while capturing\n**Issue**: Humans don't memorize IDs mid-work\n**v2.0 Fix**: AI identifies relationships during `/review-backlog` post-processing\n\n### Problem 4: Monolithic Command\n**v1.0**: Single `/log` command tried to handle all entry types\n**Issue**: Human quick-capture â‰  AI auto-detection workflows\n**v2.0 Fix**: Split into `/idea` (human), auto-detection (AI), `/review-backlog` (librarian)\n\n---\n\n## Examples\n\n### Example 1: Quick Idea Capture\n\n```bash\n# User is working, has an idea\n/idea Add plugin permission system for marketplace\n\n# AI creates entry immediately\nâœ… Idea logged: #ID-20251222-001\nğŸ“ Add plugin permission system for marketplace\nğŸ·ï¸  Tags: plugin-system, marketplace, permissions, security\n\n# User continues working (zero friction)\n```\n\n### Example 2: AI Auto-Detection\n\n```\n# During session, Claude says:\n\"I'm uncertain about the best approach for handling plugin dependencies...\"\n\n# AI auto-detects \"uncertain about\" pattern\n# Creates Process Memory entry:\n## Process Memory | #ID-20251222-002 | 2025-12-22T16:30:00+01:00\n**Description**: Uncertainty about plugin dependency handling approach\n**Confidence**: 80%\n**Status**: Assumed\n**Pattern Detected**: Uncertainty signal\n```\n\n### Example 3: Backlog Review\n\n```bash\n/review-backlog\n\n# AI analyzes all entries, outputs:\nğŸ’¡ Ideas Requiring Prioritization (2):\n- #ID-20251222-001: Plugin permission system â†’ Suggested: High\n  Rationale: Critical for marketplace security, blocks other features\n\nğŸ”— Suggested Links:\n- #ID-20251222-002 â†’ #ID-20251222-001\n  Reason: Uncertainty in 002 relates to implementation of idea in 001\n\nApply changes? [Y/n]\n> Y\n\nâœ… Applied 3 changes\n```\n\n---\n\n## Commands Reference\n\n| Command | Purpose | Example |\n|---------|---------|---------|\n| `/idea [DESCRIPTION]` | Quick-capture idea | `/idea Add dark mode toggle` |\n| `/running-log --show [N]` | Show last N entries | `/running-log --show 5` |\n| `/running-log --debug` | Show full entry details | `/running-log --debug` |\n| `/review-backlog` | Full backlog review | `/review-backlog` |\n| `/review-backlog --ideas` | Prioritize ideas only | `/review-backlog --ideas` |\n| `/review-backlog --risks` | Review low-confidence items | `/review-backlog --risks` |\n| `/review-backlog --tags` | Harmonize tags | `/review-backlog --tags` |\n\n---\n\n## File Structure\n\n```\nproject/\nâ”œâ”€â”€ .claude/\nâ”‚   â”œâ”€â”€ RUNNING_LOG.md              # Main log\nâ”‚   â”œâ”€â”€ LAST_ENTRIES.md             # Quick-access cache (20 recent)\nâ”‚   â””â”€â”€ skills/\nâ”‚       â””â”€â”€ running-log/\nâ”‚           â””â”€â”€ SKILL.md            # Skill specification\nâ””â”€â”€ plugins/\n    â””â”€â”€ running-log/\n        â”œâ”€â”€ commands/\n        â”‚   â”œâ”€â”€ idea.md                # Quick-capture command\n        â”‚   â”œâ”€â”€ review-backlog.md      # Librarian command\n        â”‚   â””â”€â”€ running-log.md         # Display command\n        â”œâ”€â”€ skills/\n        â”‚   â””â”€â”€ running-log/\n        â”‚       â””â”€â”€ SKILL.md           # Full specification\n        â””â”€â”€ .claude-plugin/\n            â””â”€â”€ plugin.json            # Plugin metadata\n```\n\n---\n\n## Version History\n\n**v2.0.0** (Current)\n- Redesigned architecture: three-component system\n- Separated human quick-capture from AI auto-detection\n- Added `/review-backlog` librarian function\n- Removed nonsensical fields from idea capture\n- AI-generated tags for consistency\n\n**v1.0.0** (Previous)\n- Initial specification\n- Monolithic `/log` command\n- Interactive prompting for all entry types\n\n---\n\n## Support\n\nFor questions, feedback, or issues, please refer to the jcmrs-plugins repository.\n",
        "plugins/running-log/commands/idea.md": "---\nname: idea\ndescription: Quick idea capture with AI-filled defaults (description only, zero friction)\nargument-hint: \"[DESCRIPTION]\"\n---\n\n\nUltra-minimal idea capture while working. AI fills all defaults.\n\n## Usage\n\n```\n/idea Local copies of Anthropic docs in AI-optimized format\n```\n\n## Execution\n\n### Step 1: Parse Description\n\nExtract description from `$ARGUMENTS`:\n- If empty or no arguments: Display usage and exit\n- Otherwise: Description = `$ARGUMENTS`\n\n### Step 2: Generate Entry ID\n\n1. Read `.claude/RUNNING_LOG.md` to find highest entry number for today\n2. Generate ID: `#ID-YYYYMMDD-NNN` where:\n   - YYYY = current year (2025)\n   - MM = current month (zero-padded)\n   - DD = current day (zero-padded)\n   - NNN = next sequence number (001, 002, etc.)\n\nExample: `#ID-20251222-003`\n\n### Step 3: Generate AI Tags\n\nAnalyze description and generate 2-4 relevant tags based on:\n- Existing tags in RUNNING_LOG.md (for consistency)\n- Domain keywords (documentation, api, framework, tooling, etc.)\n- Technology mentions (anthropic, claude, python, etc.)\n\n**Tag Guidelines**:\n- Lowercase, hyphenated (e.g., `api-design`, `local-tooling`)\n- Prefer existing tags over creating new ones\n- Max 4 tags per entry\n\n### Step 4: Create Entry\n\nFormat entry with AI-filled defaults:\n\n```markdown\n## Idea/Note | [Entry ID] | [ISO 8601 Timestamp]\n\n**Description**: [User-provided description from $ARGUMENTS]\n**Confidence/Priority**: TBD\n**Status**: Backlog\n**Type**: Idea/Note\n**Profile**: DEVELOPER\n**Tags**: [AI-generated tags]\n\n---\n```\n\n**ISO 8601 Timestamp Format**: `YYYY-MM-DDTHH:MM:SS+TZ`\nExample: `2025-12-22T15:30:00+01:00`\n\n### Step 5: Append to RUNNING_LOG.md\n\n1. Read `.claude/RUNNING_LOG.md`\n2. Find the `## Entry Backlog` section\n3. Insert new entry at the TOP of the backlog (reverse chronological order)\n4. Update `**Last Updated**` timestamp in header\n5. Write file using Edit tool\n\n### Step 6: Update LAST_ENTRIES.md\n\n1. Read `.claude/LAST_ENTRIES.md`\n2. Add new entry to top of table:\n   ```\n   | [Entry ID] | Idea/Note | [Description (truncated to 60 chars)] | TBD | Backlog | [Tags] |\n   ```\n3. Keep only last 20 entries (remove oldest if > 20)\n4. Update `**Last Updated**` timestamp\n5. Increment `**Total Entries**` count\n6. Write file using Edit tool\n\n### Step 7: Confirm\n\nDisplay:\n```\nâœ… Idea logged: [Entry ID]\nğŸ“ [First 60 chars of description...]\nğŸ·ï¸  Tags: [tag1, tag2, tag3]\n```\n\n## File Initialization\n\nIf `.claude/RUNNING_LOG.md` doesn't exist, create it:\n\n```markdown\n# Running Log - DEVELOPER Profile\n\n**Created**: [ISO 8601 timestamp]\n**Last Updated**: [ISO 8601 timestamp]\n\n---\n\n## Auto-Generated Sections\n\n### ğŸ”¥ High-Priority Ideas\n[Auto-populated from entries tagged High/Critical]\n\n### âš ï¸ Open Risks / Low-Confidence Items\n[Auto-populated from entries with confidence < 60%]\n\n### ğŸ”— Linked Process Insights\n[Auto-populated from Process Memory entries with Linked To]\n\n---\n\n## Entry Backlog\n\n[Entries will appear here in reverse chronological order]\n\n---\n```\n\nIf `.claude/LAST_ENTRIES.md` doesn't exist, create it:\n\n```markdown\n# Last Entries - Quick Access Cache\n\n**Last Updated**: [ISO 8601 timestamp]\n**Profile**: DEVELOPER@75%\n\n---\n\n## Recent Entries (Last 20)\n\n| ID | Type | Description | Confidence | Status | Tags |\n|----|------|-------------|------------|--------|------|\n\n---\n\n**Total Entries**: 0\n**Session**: [Current date]\n```\n\n## Important Notes\n\n- Use Read tool to read files\n- Use Edit tool to update existing files\n- Use Write tool only if file doesn't exist\n- Entry IDs must be unique and sequential per day\n- Always update both RUNNING_LOG.md and LAST_ENTRIES.md\n- Keep LAST_ENTRIES.md at max 20 entries\n- Tags should be consistent with existing tags in the log\n\n## Examples\n\n### Example 1: First Idea of Day\n\n```\nUser: /idea Add plugin permission system for marketplace\n\nAI generates:\n- Entry ID: #ID-20251222-001\n- Tags: plugin-system, marketplace, permissions, security\n- Timestamp: 2025-12-22T10:15:00+01:00\n\nOutput:\nâœ… Idea logged: #ID-20251222-001\nğŸ“ Add plugin permission system for marketplace\nğŸ·ï¸  Tags: plugin-system, marketplace, permissions, security\n```\n\n### Example 2: Second Idea (Same Day)\n\n```\nUser: /idea Local AI-optimized Anthropic docs\n\nAI generates:\n- Entry ID: #ID-20251222-002 (incremented)\n- Tags: documentation, anthropic, ai-optimization, local-tooling\n- Timestamp: 2025-12-22T15:30:00+01:00\n\nOutput:\nâœ… Idea logged: #ID-20251222-002\nğŸ“ Local AI-optimized Anthropic docs\nğŸ·ï¸  Tags: documentation, anthropic, ai-optimization, local-tooling\n```\n\n## Error Handling\n\n**No description provided:**\n```\nUsage: /idea [DESCRIPTION]\n\nExample: /idea Add dark mode toggle to settings\n```\n\n**File errors:**\n- If RUNNING_LOG.md unreadable â†’ Initialize new file\n- If LAST_ENTRIES.md unreadable â†’ Initialize new file\n- If Edit fails â†’ Show error, ask user to check file permissions\n\n---\n\nExecute the command based on `$ARGUMENTS`.\n",
        "plugins/running-log/commands/review-backlog.md": "---\nname: review-backlog\ndescription: Post-process running log entries - prioritize, link, harmonize tags, regenerate auto-sections\nargument-hint: \"[--ideas|--risks|--link ID|--tags]\"\n---\n\n\nPost-process running log entries: prioritize, link, harmonize tags, regenerate auto-sections.\n\n## Usage\n\n```\n/review-backlog                 # Full review with all suggestions\n/review-backlog --ideas         # Review only ideas (prioritize TBD items)\n/review-backlog --risks         # Review low-confidence Process Memory items\n/review-backlog --link #ID-XXX  # Find and link entries related to specific ID\n/review-backlog --tags          # Harmonize tags only\n```\n\n## Execution\n\n### Step 1: Parse Arguments\n\nCheck `$ARGUMENTS` for mode:\n- No arguments or empty: **Full Review Mode**\n- `--ideas`: **Ideas Review Mode**\n- `--risks`: **Risks Review Mode**\n- `--link #ID-XXX`: **Link Discovery Mode**\n- `--tags`: **Tag Harmonization Mode**\n\n### Step 2: Load All Entries\n\n1. Read `.claude/RUNNING_LOG.md`\n2. Parse all entries from `## Entry Backlog` section\n3. Extract for each entry:\n   - Entry ID\n   - Type (Idea/Note, Consultation, Process Memory)\n   - Description\n   - Confidence/Priority\n   - Status\n   - Tags\n   - Linked To (if present)\n\nStore in memory for analysis.\n\n---\n\n## Mode 1: Full Review (No Arguments)\n\nPerform all analyses and present comprehensive review.\n\n### Analysis 1: Prioritization\n\n**Find Ideas with Priority = TBD:**\n\nFor each TBD idea:\n1. Analyze description keywords\n2. Check if related to recent Process Memory entries (decisions, critical items)\n3. Look for domain alignment (documentation, api, tooling, etc.)\n4. Suggest priority: High/Med/Low with brief rationale\n\n**Output Format:**\n```\nğŸ’¡ Ideas Requiring Prioritization (N):\n\n- #ID-20251222-001: Local AI-optimized docs\n  â†’ Suggested: High\n  â†’ Rationale: Aligns with knowledge-base work, mentioned in #ID-20251221-005\n\n- #ID-20251221-003: Plugin permission system\n  â†’ Suggested: Med\n  â†’ Rationale: Dependent on architecture decisions, no immediate blockers\n```\n\n### Analysis 2: Relationship Discovery\n\n**Find Related Entries:**\n\nFor each entry, identify potential links based on:\n- Shared keywords in descriptions\n- Similar tags\n- Temporal proximity (entries from same session)\n- Causal relationships (decision â†’ idea, consultation â†’ implementation)\n\n**Output Format:**\n```\nğŸ”— Suggested Links (N):\n\n- #ID-20251222-001 â† #ID-20251221-008\n  Reason: Both reference documentation workflows\n\n- #ID-20251221-005 â†’ #ID-20251221-003\n  Reason: Decision in 005 impacts idea in 003\n\n- #ID-20251220-012 â†” #ID-20251220-015\n  Reason: Both discuss marketplace architecture\n```\n\n### Analysis 3: Tag Harmonization\n\n**Find Tag Inconsistencies:**\n\n1. Identify similar tags:\n   - `docs` vs `documentation`\n   - `api` vs `api-design`\n   - `anthropic` vs `anthropic-api`\n\n2. Count usage frequency\n3. Suggest consolidation to most common variant\n\n**Output Format:**\n```\nğŸ·ï¸  Tag Harmonization Suggestions:\n\n- Rename \"docs\" â†’ \"documentation\" (4 entries affected)\n- Merge \"api\" + \"api-design\" â†’ \"api-design\" (3 entries)\n- Merge \"anthropic\" + \"anthropic-api\" â†’ \"anthropic\" (5 entries)\n```\n\n### Analysis 4: Risk Highlighting\n\n**Find Low-Confidence Items:**\n\n- Process Memory entries with Confidence < 70%\n- Status = Assumed (not yet validated)\n- Critical signals (blocker, must-verify, etc.)\n\n**Output Format:**\n```\nâš ï¸  Open Risks / Low-Confidence Items (N):\n\n- #ID-20251221-004: Confidence 65%\n  â†’ Low confidence on validation approach\n  â†’ Status: Todo\n  â†’ Linked to: #ID-20251221-005\n\n- #ID-20251220-010: Confidence 60%\n  â†’ Assumption about API behavior not yet validated\n  â†’ Status: Assumed\n```\n\n### Step 3: Display Summary\n\n```\nğŸ” Backlog Review Results\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n[Analysis 1: Prioritization]\n[Analysis 2: Relationship Discovery]\n[Analysis 3: Tag Harmonization]\n[Analysis 4: Risk Highlighting]\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\nApply changes? [Y/n]\n```\n\n### Step 4: Apply Changes (If User Confirms)\n\nIf user types \"Y\" or \"yes\":\n\n1. **Update Priorities**: Edit entries to change TBD â†’ High/Med/Low\n2. **Add Links**: Add \"Linked To\" fields to suggested entries\n3. **Harmonize Tags**: Rename/merge tags across affected entries\n4. **Regenerate Auto-Sections**: Update High-Priority Ideas, Open Risks, Linked Insights sections\n5. **Update Last Updated timestamp** in header\n\nConfirm:\n```\nâœ… Applied N changes to running log\n   - Updated 5 priorities\n   - Added 3 links\n   - Harmonized 12 tag occurrences\n   - Regenerated auto-sections\n```\n\n---\n\n## Mode 2: Ideas Review (`--ideas`)\n\nFocus only on prioritizing TBD ideas.\n\n### Execution\n\n1. Load all entries\n2. Filter: Type = Idea/Note AND Priority = TBD\n3. For each TBD idea:\n   - Analyze description\n   - Suggest priority with rationale\n   - Identify potential links\n\n**Output:**\n```\nğŸ’¡ Ideas Review (N ideas with TBD priority)\n\n#ID-20251222-001: Local AI-optimized docs\nâ†’ Suggested: High\nâ†’ Rationale: Aligns with knowledge-base goals\nâ†’ Potential Links: #ID-20251221-008 (documentation workflow)\n\n#ID-20251221-003: Plugin permission system\nâ†’ Suggested: Med\nâ†’ Rationale: Dependent on architecture, no immediate need\n\nApply priority suggestions? [Y/n]\n```\n\n---\n\n## Mode 3: Risks Review (`--risks`)\n\nFocus only on low-confidence Process Memory items.\n\n### Execution\n\n1. Load all entries\n2. Filter: Type = Process Memory AND (Confidence < 70% OR Status = Assumed)\n3. For each risk item:\n   - Display confidence level\n   - Show status\n   - Highlight if linked to ideas or consultations\n   - Suggest validation steps\n\n**Output:**\n```\nâš ï¸  Open Risks Review (N items)\n\n#ID-20251221-004: Confidence 65%\nâ†’ Low confidence on validation approach\nâ†’ Status: Todo\nâ†’ Linked to: #ID-20251221-005 (decision fork)\nâ†’ Suggested Action: Test manual stub approach vs auto-detection\n\n#ID-20251220-010: Confidence 60%\nâ†’ Assumption about API pagination\nâ†’ Status: Assumed\nâ†’ Suggested Action: Verify with actual API test\n```\n\n---\n\n## Mode 4: Link Discovery (`--link #ID-XXX`)\n\nFind entries related to a specific entry ID.\n\n### Execution\n\n1. Load all entries\n2. Find target entry by ID\n3. Analyze target entry:\n   - Extract keywords from description\n   - Extract tags\n   - Note type and timestamp\n\n4. Search all other entries for:\n   - Shared keywords (â‰¥ 2 words in common)\n   - Shared tags (â‰¥ 1 tag)\n   - Temporal proximity (same day or adjacent days)\n   - Causal language (\"because of\", \"led to\", \"resulted in\")\n\n5. Rank by relevance score:\n   - Shared keywords: +2 per keyword\n   - Shared tags: +3 per tag\n   - Same type: +1\n   - Temporal proximity: +1\n   - Causal language: +5\n\n**Output:**\n```\nğŸ”— Entries Related to #ID-20251222-001\n\nHigh Relevance (Score â‰¥ 7):\n- #ID-20251221-008 (Score: 9)\n  â†’ Shares 3 keywords, 2 tags\n  â†’ Same day, both reference documentation\n\nMedium Relevance (Score 4-6):\n- #ID-20251220-015 (Score: 5)\n  â†’ Shares 2 tags, temporal proximity\n\nLow Relevance (Score 1-3):\n- #ID-20251219-003 (Score: 2)\n  â†’ Shares 1 keyword\n\nAdd \"Linked To\" field to #ID-20251222-001 with suggested links? [Y/n]\n```\n\n---\n\n## Mode 5: Tag Harmonization (`--tags`)\n\nFocus only on tag consistency.\n\n### Execution\n\n1. Load all entries\n2. Extract all unique tags\n3. Group similar tags:\n   - Levenshtein distance < 3 edits\n   - Common prefixes/suffixes\n   - Semantic similarity (e.g., \"doc\" vs \"documentation\")\n\n4. For each group:\n   - Count usage frequency\n   - Suggest consolidation to most common variant\n\n**Output:**\n```\nğŸ·ï¸  Tag Harmonization Report\n\nGroup 1: Documentation Tags\n- \"documentation\" (8 uses) â† KEEP\n- \"docs\" (4 uses) â†’ Rename to \"documentation\"\n- \"doc\" (1 use) â†’ Rename to \"documentation\"\n\nGroup 2: API Tags\n- \"api-design\" (5 uses) â† KEEP\n- \"api\" (3 uses) â†’ Rename to \"api-design\"\n\nGroup 3: Anthropic Tags\n- \"anthropic\" (6 uses) â† KEEP\n- \"anthropic-api\" (2 uses) â†’ Rename to \"anthropic\"\n\nApply harmonization? [Y/n]\n\nChanges: 13 tag occurrences across 10 entries\n```\n\n---\n\n## Auto-Section Regeneration\n\nAfter any changes applied, regenerate auto-sections in RUNNING_LOG.md:\n\n### High-Priority Ideas\n\n```markdown\n### ğŸ”¥ High-Priority Ideas\n\n- #ID-20251222-001: Local AI-optimized docs (Backlog)\n- #ID-20251221-012: Add WebSocket support (In Progress)\n```\n\n**Criteria**: Type = Idea/Note + Priority = High + Status â‰  Done\n\n### Open Risks / Low-Confidence Items\n\n```markdown\n### âš ï¸ Open Risks / Low-Confidence Items\n\n- #ID-20251221-004: Validation approach uncertainty (65%)\n- #ID-20251220-010: API pagination assumption (60%)\n```\n\n**Criteria**: Type = Process Memory + (Confidence < 60% OR Status = Assumed)\n\n### Linked Process Insights\n\n```markdown\n### ğŸ”— Linked Process Insights\n\n- #ID-20251221-005 â†’ #ID-20251221-003 (Decision impacts plugin idea)\n- #ID-20251220-012 â†” #ID-20251220-015 (Marketplace architecture discussion)\n```\n\n**Criteria**: Any entry with \"Linked To\" field populated\n\n---\n\n## Important Notes\n\n- All analysis is AI-driven (relationship detection, priority suggestion, tag harmonization)\n- Changes require user confirmation (Y/n prompt)\n- Original entries never deleted, only enhanced\n- Backup tip: User can check git diff before confirming changes\n- Tag harmonization preserves semantic meaning while improving consistency\n\n---\n\nExecute the appropriate mode based on `$ARGUMENTS`.\n",
        "plugins/running-log/commands/running-log.md": "---\nname: running-log\ndescription: Display running log entries - show recent entries or debug details\nargument-hint: \"[--show N|--debug]\"\n---\n\n\nDisplay running log entries. For adding ideas, use `/idea`. For post-processing, use `/review-backlog`.\n\n## Usage\n\n```\n/running-log --show [N]    # Show last N entries (default: 10)\n/running-log --debug       # Show last 5 entries with full details\n```\n\n## Parse Arguments\n\nCheck `$ARGUMENTS` for flags:\n- `--show` or `--show N`: **Display mode** (show last N entries, default 10)\n- `--debug`: **Debug mode** (show last 5 entries with regex details)\n- No arguments or empty: Display usage help\n\n## File Paths\n\n- **Main log**: `.claude/RUNNING_LOG.md`\n- **Cache**: `.claude/LAST_ENTRIES.md`\n\nCheck if files exist. If not, initialize them.\n\n---\n\n## Mode 1: Display (`--show [N]`)\n\n1. Read `.claude/LAST_ENTRIES.md`\n2. Parse the entry table\n3. Extract last N entries (default: 10, or use number from `$ARGUMENTS`)\n4. Display in compact format:\n   ```\n   Last N entries:\n\n   #ID-YYYYMMDD-NNN | Type | Description | Status\n   #ID-YYYYMMDD-NNN | Type | Description | Status\n   ```\n\n---\n\n## Mode 2: Debug (`--debug`)\n\n1. Read `.claude/RUNNING_LOG.md`\n2. Extract last 5 entries from Entry Backlog section\n3. Display full entry content including:\n   - All fields (Description, Confidence, Status, Type, Profile, Tags)\n   - Extended Context\n   - Pattern Detected (if present)\n   - Raw Output (if present)\n   - Detection Method\n\nFormat each entry with full markdown, separated by `---`\n\n---\n\n## Mode 3: Usage Help (no arguments)\n\nDisplay usage information:\n\n```\nRunning Log v2.0 - Display & Quick-Capture\n\nDisplay Modes:\n  /running-log --show [N]    Show last N entries (default: 10)\n  /running-log --debug       Show last 5 entries with full details\n\nQuick-Capture:\n  /idea [DESCRIPTION]        Add idea to backlog (one-line, AI fills defaults)\n\nPost-Processing:\n  /review-backlog            Prioritize, link, organize entries\n\nExamples:\n  /running-log --show 5              Show last 5 entries\n  /running-log --debug               Show debugging details\n  /idea Local AI-optimized docs      Add quick idea\n  /review-backlog                    Review and organize backlog\n```\n\n---\n\n## File Initialization\n\nIf `.claude/RUNNING_LOG.md` doesn't exist, create it:\n\n```markdown\n# Running Log - DEVELOPER Profile\n\n**Created**: [ISO 8601 timestamp]\n**Last Updated**: [ISO 8601 timestamp]\n\n---\n\n## Auto-Generated Sections\n\n### ğŸ”¥ High-Priority Ideas\n[Auto-populated from entries tagged High/Critical]\n\n### âš ï¸ Open Risks / Low-Confidence Items\n[Auto-populated from entries with confidence < 60%]\n\n### ğŸ”— Linked Process Insights\n[Auto-populated from Process Memory entries with Linked To]\n\n---\n\n## Entry Backlog\n\n[Entries will appear here in reverse chronological order]\n\n---\n```\n\nIf `.claude/LAST_ENTRIES.md` doesn't exist, create it:\n\n```markdown\n# Last Entries - Quick Access Cache\n\n**Last Updated**: [ISO 8601 timestamp]\n**Profile**: DEVELOPER@75%\n\n---\n\n## Recent Entries (Last 20)\n\n| ID | Type | Description | Confidence | Status | Tags |\n|----|------|-------------|------------|--------|------|\n\n---\n\n**Total Entries**: 0\n**Session**: [Current date]\n```\n\n---\n\n## Important Notes\n\n- Use Read tool to read files\n- Use Edit tool to update existing files\n- Use Write tool only if file doesn't exist\n- Generate ISO 8601 timestamps: `YYYY-MM-DDTHH:MM:SS+TZ`\n- Entry IDs must be unique and sequential per day\n- Always update both RUNNING_LOG.md and LAST_ENTRIES.md\n- Keep LAST_ENTRIES.md at max 20 entries\n\nExecute the appropriate mode based on `$ARGUMENTS`.\n",
        "plugins/running-log/skills/running-log/SKILL.md": "---\nname: Running Log\ndescription: Persistent schema-driven running log with three-component architecture - quick-capture ideas, AI auto-detection, and backlog review librarian\nversion: 2.0.0\n---\n\n\n**Name**: running-log\n**Version**: 2.0\n**Domain**: Process Memory, Decision Tracking, Cross-Session Learning\n**Status**: Redesigned based on Phase 2 validation findings\n\n---\n\n## Purpose\n\nMaintain a persistent, schema-driven running log that captures:\n- **Ideas** (human quick-capture backlog)\n- **Consultations** (AI-detected external sources)\n- **Process Memory** (AI-detected reasoning patterns)\n\nCreates searchable, auto-organized entry backlog across sessions through **three distinct workflows**:\n1. Human quick-capture (`/idea`)\n2. AI auto-detection (Consultation, Process Memory)\n3. Post-processing librarian (`/review-backlog`)\n\n**Critical Design Insight**: Human entry workflows differ fundamentally from AI auto-detection workflows. v2.0 separates these cleanly.\n\n---\n\n## Architecture: Three-Component System\n\n### Component 1: `/idea` Command (Human Territory)\n\n**Purpose**: Ultra-minimal quick capture while working\n\n**Workflow**:\n```\nUser: /idea Local copies of Anthropic docs in AI-optimized format\nâ†’ Entry created immediately with defaults\nâ†’ User continues work\n```\n\n**What AI fills automatically**:\n- Entry ID: `#ID-YYYYMMDD-NNN` (auto-incremented)\n- Timestamp: ISO 8601\n- Confidence/Priority: `TBD` (To Be Determined - evaluated during backlog review)\n- Status: `Backlog` (default for all ideas)\n- Tags: AI-generated from description + existing tag taxonomy\n- Type: `Idea/Note`\n- Profile: Active profile (e.g., `DEVELOPER`)\n\n**Entry Schema (Ideas)**:\n```markdown\n## Idea/Note | #ID-YYYYMMDD-NNN | [ISO 8601 Timestamp]\n\n**Description**: [User-provided 1-line description]\n**Confidence/Priority**: TBD\n**Status**: Backlog\n**Type**: Idea/Note\n**Profile**: [Active Profile]\n**Tags**: [AI-generated tags]\n\n---\n```\n\n**Why this works**:\n- Zero friction: User types one line, gets back to work\n- No nonsensical prompts for confidence (ideas are captured, not evaluated)\n- No status guessing (all ideas start as backlog)\n- Consistent tags (AI prevents million inconsistent human tags)\n- Evaluation happens later during `/review-backlog`\n\n---\n\n### Component 2: AI Auto-Detection (AI Territory)\n\n**Purpose**: Monitor Claude's responses for reasoning patterns worth capturing\n\n**Entry Types**:\n\n#### Consultation (External Sources)\nAI detects when referencing external knowledge:\n- Documentation lookups\n- Perplexity/research queries\n- User-provided references\n- Framework/library citations\n\n**Auto-generates**:\n```markdown\n## Consultation | #ID-YYYYMMDD-NNN | [Timestamp]\n\n**Description**: [What was consulted]\n**Source**: [Citation/URL]\n**Confidence**: [AI's confidence in source quality: High/Med/Low]\n**Status**: Reviewed\n**Type**: Consultation\n**Profile**: [Active Profile]\n**Tags**: [domain, source-type, framework]\n\n---\n```\n\n#### Process Memory (AI Reasoning Patterns)\nAI detects loggable reasoning patterns in its own responses:\n\n**Pattern 1: Uncertainty**\n```regex\n/uncertainty\\s+(on|about|regarding|around)\\s+([^.!?]+)/i\n```\nâ†’ Logs: What's uncertain, confidence level\n\n**Pattern 2: Assumption**\n```regex\n/assum(e|ing|ption)\\s+(that|about|the)\\s+([^.!?]+)/i\n```\nâ†’ Logs: Assumption made, validation status\n\n**Pattern 3: Confidence Threshold**\n```regex\n/confidence\\s+(less\\s+than|below|<)\\s*(\\d+)%?/i\n```\nâ†’ Logs: Low-confidence item needing validation\n\n**Pattern 4: Decision/Fork**\n```regex\n/(fork|branch|decision\\s+point|chose|decided|rejected)\\s+(in|on)?\\s*([^.!?]+)/i\n```\nâ†’ Logs: Decision made, alternatives considered, rationale\n\n**Pattern 5: Critical Signal**\n```regex\n/critical|blocker|blocking|must\\s+(clarify|understand|verify)/i\n```\nâ†’ Logs: Critical issue flagged, requires attention\n\n**Auto-generates**:\n```markdown\n## Process Memory | #ID-YYYYMMDD-NNN | [Timestamp]\n\n**Description**: [Reasoning pattern detected]\n**Confidence**: [AI's certainty about this pattern: 0-100%]\n**Status**: [Assumed/Validated/Rejected]\n**Type**: Process Memory\n**Profile**: [Active Profile]\n**Tags**: [pattern-type, domain, criticality]\n**Pattern Detected**: [Which regex matched]\n**Raw Output**: [Exact phrase from Claude's response]\n\n**Extended Context**:\n[Why this pattern matters, implications, next steps]\n\n---\n```\n\n**Cadence**: 3 automatic checks per session\n1. **Session Start**: Continuity from previous session\n2. **Mid-Toolchain**: After `floor(tool_count / 3)` tools executed\n3. **Session End**: Archive session learnings\n\n**Confidence Thresholds** (Auto-log only if >= threshold):\n- DEVELOPER: 75%\n- RESEARCHER: 60%\n- ENGINEER: 70%\n- DEFAULT: 70%\n\n**Noise Filtering**:\n1. Confidence threshold (above)\n2. Entry cap per session (DEVELOPER: 8, RESEARCHER: 12, ENGINEER: 10)\n3. Deduplication (Levenshtein 85% similarity suppresses duplicates)\n\n---\n\n### Component 3: `/review-backlog` Command (Librarian Function)\n\n**Purpose**: Post-process entries to organize, prioritize, and link\n\n**What it does**:\n1. **Relationship Identification**: AI analyzes all entries and identifies connections\n2. **Tag Refinement**: Harmonizes tags across entries, suggests taxonomy improvements\n3. **Prioritization**: Reviews `TBD` priorities, suggests High/Med/Low based on context\n4. **Linking**: Populates `Linked To` field by finding related entries\n5. **Auto-Section Generation**: Regenerates High-Priority Ideas, Open Risks, Linked Insights\n\n**Usage**:\n```\n/review-backlog                 # Review all entries, suggest actions\n/review-backlog --ideas         # Review only ideas (prioritize, link)\n/review-backlog --risks         # Review low-confidence items\n/review-backlog --link #ID-001  # Find and link entries related to #ID-001\n```\n\n**Example Output**:\n```\nğŸ” Backlog Review Results\n\nIdeas Requiring Prioritization (5):\n- #ID-20251222-001: Local AI-optimized docs â†’ Suggested: High (aligns with knowledge-base work)\n- #ID-20251221-003: Plugin permission system â†’ Suggested: Med (dependent on architecture)\n\nSuggested Links (3):\n- #ID-20251222-001 â† #ID-20251221-008 (both reference documentation workflows)\n- #ID-20251221-005 â†’ #ID-20251221-003 (decision impacts idea)\n\nTag Harmonization:\n- Rename \"docs\" â†’ \"documentation\" (4 entries)\n- Merge \"anthropic-api\" + \"anthropic\" (2 entries)\n\nApply changes? [Y/n]\n```\n\n**Why separate from capture**:\n- Humans can't know relationships while capturing ideas mid-work\n- Requires full-backlog context to identify patterns\n- Deliberate activity, not real-time capture\n- AI analyzes relationships humans can't see\n\n---\n\n## File Structure\n\n```\nproject/\nâ”œâ”€â”€ .claude/\nâ”‚   â”œâ”€â”€ RUNNING_LOG.md              # Main log (auto-sections + chronological)\nâ”‚   â”œâ”€â”€ LAST_ENTRIES.md             # Dedup tracking (20 most recent)\nâ”‚   â””â”€â”€ skills/\nâ”‚       â””â”€â”€ running-log/\nâ”‚           â””â”€â”€ SKILL.md            # This specification\nâ””â”€â”€ [project files]\n```\n\n### RUNNING_LOG.md Structure\n\n```markdown\n# Running Log - DEVELOPER Profile\n\n**Created**: [ISO 8601]\n**Last Updated**: [ISO 8601]\n\n---\n\n## Auto-Generated Sections\n\n### ğŸ”¥ High-Priority Ideas\n[Auto-populated from ideas tagged High, status â‰  Done]\n\n### âš ï¸ Open Risks / Low-Confidence Items\n[Auto-populated from Process Memory with confidence < 60%]\n\n### ğŸ”— Linked Process Insights\n[Auto-populated from entries with Linked To populated]\n\n---\n\n## Entry Backlog\n\n[Entries in reverse chronological order]\n\n---\n```\n\n---\n\n## Commands Summary\n\n### `/idea [DESCRIPTION]`\nQuick-capture idea while working. AI fills all other fields with defaults.\n\n```\n/idea Local copies of Anthropic docs in AI-optimized format\n```\n\n### `/review-backlog [OPTIONS]`\nPost-process entries: prioritize, link, harmonize tags.\n\n```\n/review-backlog                 # Full review\n/review-backlog --ideas         # Ideas only\n/review-backlog --risks         # Low-confidence items\n/review-backlog --link #ID-001  # Link related entries\n```\n\n### `/running-log --show [N]`\nDisplay last N entries (default: 10).\n\n```\n/running-log --show 5\n```\n\n### `/running-log --debug`\nShow last 5 entries with full details including regex detection.\n\n```\n/running-log --debug\n```\n\n---\n\n## Configuration\n\n```yaml\nrunning_log:\n  enabled: true\n  file_path: \".claude/RUNNING_LOG.md\"\n  state_file: \".claude/LAST_ENTRIES.md\"\n\n  profiles:\n    DEVELOPER:\n      threshold: 75\n      entry_cap: 8\n    RESEARCHER:\n      threshold: 60\n      entry_cap: 12\n    ENGINEER:\n      threshold: 70\n      entry_cap: 10\n    DEFAULT:\n      threshold: 70\n      entry_cap: 8\n\n  deduplication:\n    enabled: true\n    levenshtein_threshold: 0.85\n    cross_session: true\n\n  idea_defaults:\n    confidence: \"TBD\"\n    status: \"Backlog\"\n    auto_tag: true  # AI generates tags from description\n```\n\n---\n\n## Migration from v1.0\n\n**Changes**:\n1. **`/log` command removed** â†’ Use `/idea [description]` instead\n2. **Interactive prompting removed** â†’ `/idea` is one-line only\n3. **Confidence/Status for ideas** â†’ Now defaults (TBD/Backlog)\n4. **Tags** â†’ AI-generated, not human-entered\n5. **Linked To** â†’ Post-processing via `/review-backlog`, not capture-time\n6. **`/review` command** â†’ Renamed to `/review-backlog` with expanded functions\n\n**Existing logs compatible**: v1.0 entries remain valid, new entries use v2.0 schema\n\n---\n\n## Design Rationale (Phase 2 Learnings)\n\n### Problem 1: Nonsensical Fields for Ideas\n**v1.0**: Asked humans for confidence/priority when capturing ideas\n**Issue**: Ideas are captured for later evaluation, not evaluated at capture time\n**v2.0 Fix**: Defaults to TBD/Backlog, evaluation happens during `/review-backlog`\n\n### Problem 2: Inconsistent Human Tags\n**v1.0**: Asked humans to enter free-form tags\n**Issue**: Million inconsistent tags, none relevant\n**v2.0 Fix**: AI auto-generates tags from description + existing taxonomy\n\n### Problem 3: Impossible \"Linked To\" Field\n**v1.0**: Asked humans to provide entry IDs while capturing\n**Issue**: Humans don't memorize IDs mid-work\n**v2.0 Fix**: AI identifies relationships during `/review-backlog` post-processing\n\n### Problem 4: Monolithic Command\n**v1.0**: Single `/log` command tried to handle all entry types\n**Issue**: Human quick-capture â‰  AI auto-detection workflows\n**v2.0 Fix**: Split into `/idea` (human), auto-detection (AI), `/review-backlog` (librarian)\n\n---\n\n## Version & Maintenance\n\n**Current**: v2.0 (Redesigned based on Phase 2 validation)\n**Previous**: v1.0 (Phase 1 spec-only)\n\n**Expected Updates**:\n- v2.1: Post-deployment tuning based on real usage\n- v3.0: Multi-repository support, cross-project insights\n\n**Schema Stability**: Core schema stable. Thresholds may adjust based on empirical data.\n\n---\n\n## Next Steps\n\n1. Implement `/idea` command (minimal quick-capture)\n2. Implement `/review-backlog` command (librarian functions)\n3. Update existing `/running-log` command for display-only modes\n4. Test with real workflows across 5+ sessions\n5. Collect usage data, tune thresholds\n\n---\n\n**End of SKILL.md Specification v2.0**\n\n*This specification reflects critical design learnings from Phase 2 validation. The three-component architecture (quick-capture, auto-detection, post-processing) separates human and AI workflows appropriately.*\n",
        "plugins/semantic-linguist/.claude-plugin/plugin.json": "{\n  \"name\": \"semantic-linguist\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Semantic validation and translation between natural language user intent and domain-specific technical implementations. Bridges the gap between human natural language and AI specificity of needs.\",\n  \"author\": {\n    \"name\": \"jcmrs\",\n    \"email\": \"user@example.com\"\n  },\n  \"keywords\": [\n    \"semantic\",\n    \"translation\",\n    \"domain-translation\",\n    \"ontology\",\n    \"linguistics\",\n    \"autogen\",\n    \"langroid\",\n    \"mcp\",\n    \"model-context-protocol\",\n    \"utcp\",\n    \"tool-calling\",\n    \"universal-tool-calling\",\n    \"fastapi\",\n    \"python-web\",\n    \"pydantic\",\n    \"git\",\n    \"gitflow\",\n    \"version-control\",\n    \"sre\",\n    \"site-reliability\",\n    \"observability\",\n    \"memory-graphs\",\n    \"knowledge-graphs\",\n    \"embeddings\",\n    \"ambiguity-detection\"\n  ],\n  \"license\": \"MIT\"\n}\n",
        "plugins/semantic-linguist/README.md": "# Semantic Linguist Plugin\n\n**Version:** 1.0.0\n**Type:** Cognitive Adapter, Semantic Bridge\n\n## Overview\n\nSemantic Linguist is a proactive plugin that bridges the gap between human natural language and AI technical specificity. It provides real-time semantic validation and ontological translation to prevent miscommunication, assumptions, and hallucinations during conversations with Claude Code.\n\n### The Problem\n\nUsers frequently encounter the \"abyss\" between natural language intent and technical precision:\n- Vague terminology: \"make it talk\", \"we need an api\"\n- Ambiguous requests: \"can you check for gaps\", \"make it portable\"\n- Meta-questions triggering assumptions: \"am I making sense?\", \"does this make sense?\"\n- Made-up terms that drag conversations into interpretation territory\n\nWithout intervention, these ambiguities lead to:\n- AI assumptions and hallucinations\n- Misaligned implementations\n- Wasted development time\n- Project failures from misunderstood requirements\n\n### The Solution\n\nSemantic Linguist operates as a **First Line of Defense**, intercepting potentially ambiguous user messages and providing:\n\n1. **Real-time Ambiguity Detection** - Identifies vague, ambiguous, or unclear terminology\n2. **Domain Knowledge Validation** - Verifies requests against technical domain knowledge (Autogen, Langroid, etc.)\n3. **Ontological Translation** - Maps natural language to precise technical concepts\n4. **Conversational Clarification** - Asks clarifying questions before proceeding with assumptions\n\n## Features\n\n### ğŸ” Proactive Ambiguity Detection\n\n- **UserPromptSubmit Hook**: Analyzes every user message in real-time\n- **Pattern Matching**: Detects vague verbs, unclear references, domain confusion\n- **Confidence Scoring**: Only triggers on high-confidence ambiguities (>80%)\n- **User Trigger Recognition**: Responds to meta-questions like \"am I making sense?\"\n\n### ğŸ§  Semantic Validation Skill\n\n- **Progressive Disclosure**: Lean core skill with detailed references\n- **Domain Knowledge Graphs**: Autogen, Langroid, and extensible to custom domains\n- **Decision Trees**: Systematic approach to resolving ambiguities\n- **Translation Patterns**: Comprehensive mappings from ambiguous â†’ precise terminology\n\n### ğŸ“š Multi-Domain Knowledge\n\nSupports multiple technical domains with full API coverage:\n- **Autogen**: Multi-agent frameworks, ConversableAgent, GroupChat, etc.\n- **Langroid**: ChatAgent, ToolAgent, multi-agent patterns\n- **Extensible**: Add custom domain knowledge for your specific tech stack\n\n### âš™ï¸ Manual Commands\n\n- **`/validate-terminology`**: Analyze recent conversation (5-10 messages) for ambiguities\n- **`/map-domain`**: Explore domain terminology mappings interactively\n- **`/semantic-config`**: Configure detection sensitivity and user trigger phrases\n\n### ğŸ¯ Conversational Interaction\n\nNever assumes - always verifies:\n- Presents options when ambiguity detected\n- Asks clarifying questions before proceeding\n- Maintains conversational, non-intrusive tone\n- Provides detailed reports on request\n\n## Installation\n\n### From Marketplace\n\n```bash\n# (Future: marketplace installation)\n```\n\n### Local Installation\n\n1. Clone or copy plugin to your project:\n   ```bash\n   cp -r plugins/semantic-linguist ~/.claude-plugins/\n   ```\n\n2. Enable plugin in Claude Code:\n   ```bash\n   claude --enable-plugin semantic-linguist\n   ```\n\n3. Plugin activates silently - no configuration required (uses defaults)\n\n### Optional Configuration\n\nCreate `.claude/semantic-linguist.local.md` to customize:\n\n```yaml\n---\n# Detection sensitivity: low, medium, high\nsensitivity: medium\n\n# Interaction style: explicit, guided, silent\ninteraction: explicit\n\n# Enabled domains\ndomains:\n  - autogen\n  - langroid\n\n# Custom user trigger phrases (in addition to defaults)\nuser_triggers:\n  - \"is this right?\"\n  - \"checking in\"\n\n# Custom terminology mappings\ncustom_mappings:\n  - from: \"chat agent\"\n    to: \"ConversableAgent\"\n    domain: autogen\n---\n```\n\n## Usage\n\n### Automatic (Recommended)\n\nSemantic Linguist works automatically in the background:\n\n**User:** \"I want to create an agent that can talk to other agents\"\n\n**Semantic Linguist detects ambiguities and intervenes:**\n\n```\nI notice a few terms that could mean different things:\n\n1. \"agent\" - In Autogen, this could be:\n   - ConversableAgent (chat-based interactions)\n   - AssistantAgent (task execution with tools)\n   - UserProxyAgent (human-in-the-loop)\n\n2. \"talk to other agents\" - Could mean:\n   - GroupChat (multi-agent conversation)\n   - Agent-to-agent messaging (send/receive)\n   - Task delegation pattern\n\nWhich framework are you using, and what kind of interaction\ndid you have in mind?\n```\n\n### Manual Validation\n\nUse commands for explicit validation:\n\n```bash\n# Validate recent conversation for ambiguities\n/validate-terminology\n\n# Explore domain mappings\n/map-domain\n\n# Configure sensitivity and triggers\n/semantic-config\n```\n\n## Configuration\n\n### Default Settings\n\nOut-of-the-box experience:\n- **Sensitivity**: Medium (balanced false positive/negative rate)\n- **Interaction**: Explicit (transparent validation)\n- **Domains**: Autogen + Langroid enabled\n- **Confidence Threshold**: 80% (high confidence only)\n\n### User Triggers\n\nDefault phrases that trigger semantic validation:\n- \"am I making sense?\"\n- \"does this make sense?\"\n- \"non-technical user\"\n- \"am I doing this right?\"\n\nAdd custom triggers via `/semantic-config` or settings file.\n\n### Detection Patterns\n\nAmbiguity patterns recognized:\n- **Vague action verbs**: \"make it work\", \"do the thing\", \"fix it\"\n- **Unclear references**: \"that\", \"the previous thing\", \"like before\"\n- **Domain confusion**: Mixing business and technical terminology\n- **Invented terms**: User-created words not in domain vocabulary\n- **Meta-questions**: Seeking validation or confirmation\n\n## Architecture\n\n### Components\n\n```\nsemantic-linguist/\nâ”œâ”€â”€ .claude-plugin/\nâ”‚   â””â”€â”€ plugin.json              # Plugin manifest\nâ”œâ”€â”€ skills/\nâ”‚   â””â”€â”€ semantic-validation/     # Core cognitive framework\nâ”‚       â”œâ”€â”€ SKILL.md             # Lean skill (1,500-2,000 words)\nâ”‚       â”œâ”€â”€ references/          # Detailed frameworks and decision trees\nâ”‚       â”œâ”€â”€ examples/            # Real-world ambiguity examples\nâ”‚       â””â”€â”€ knowledge/           # Domain knowledge (separate JSON files)\nâ”œâ”€â”€ commands/\nâ”‚   â”œâ”€â”€ validate-terminology.md # Explicit validation command\nâ”‚   â”œâ”€â”€ map-domain.md            # Domain mapping explorer\nâ”‚   â””â”€â”€ semantic-config.md       # Configuration command\nâ”œâ”€â”€ hooks/\nâ”‚   â””â”€â”€ hooks.json               # UserPromptSubmit hook config\nâ””â”€â”€ scripts/\n    â”œâ”€â”€ detect-ambiguity.py      # Pattern matching utilities\n    â”œâ”€â”€ domain-mapper.py         # Translation logic\n    â””â”€â”€ knowledge-query.py       # Unified knowledge interface\n```\n\n### Knowledge Sources (Query Order)\n\n1. **Static Knowledge**: Fast, always-available JSON files\n   - `ambiguous-terms.json`\n   - `technical-mappings.json`\n   - `ontology-graph.json`\n\n2. **External Documentation**: Current, authoritative sources\n   - Official docs (via WebFetch, context7, etc.)\n   - API references\n   - Examples and guides\n\n3. **Codebase Validation**: Project-specific usage\n   - LSP symbol definitions\n   - Grep for actual implementations\n   - Context-aware recommendations\n\n## Examples\n\n### Example 1: API Ambiguity\n\n**User:** \"we need an api\"\n\n**Without Semantic Linguist:**\n```\nSure! I'll create a REST API server with Express...\n[Proceeds with HTTP server implementation]\n```\n\n**With Semantic Linguist:**\n```\nâš ï¸ \"api\" could mean several things:\n\n1. HTTP REST API server (Express, FastAPI, Flask)\n2. API client (to consume external APIs)\n3. API design/documentation (OpenAPI spec)\n4. Internal API/interface (function signatures)\n\nWhat kind of API are you building?\n```\n\n### Example 2: Portability Ambiguity\n\n**User:** \"we need this portable\"\n\n**Without Semantic Linguist:**\n```\nI'll containerize it with Docker...\n[Assumes Docker is the solution]\n```\n\n**With Semantic Linguist:**\n```\n\"Portable\" could mean:\n\n1. Cross-platform (Windows/Mac/Linux compatible)\n2. Containerized (Docker/Podman)\n3. Dependency vendoring (bundled dependencies)\n4. Executable binary (standalone distribution)\n\nWhich type of portability do you need?\n```\n\n### Example 3: User Trigger\n\n**User:** \"am I making sense here?\"\n\n**Semantic Linguist recognizes meta-question and validates:**\n```\nLet me verify I understand correctly. You mentioned:\n- [Key point 1 from recent messages]\n- [Key point 2 from recent messages]\n\nPotential ambiguities I noticed:\n- \"[Term X]\" could mean [Option A] or [Option B]\n- \"[Term Y]\" isn't standard in [Domain] - did you mean [Precise Term]?\n\nDoes this match your intent?\n```\n\n## Top 5 Ambiguities Handled\n\nBased on real user experiences:\n\n1. **\"make it talk\"** â†’ ConversableAgent.send() vs speech synthesis vs conversation interface\n2. **\"we need an api\"** â†’ HTTP server vs API client vs API design vs internal interface\n3. **\"make it portable\"** â†’ Docker vs cross-platform vs vendoring vs executable\n4. **\"check for gaps\"** â†’ Code coverage vs documentation vs features vs security analysis\n5. **\"am I making sense\"** â†’ User seeking validation (trigger semantic check)\n\n## Future Enhancements\n\nDocumented in [Option C Future Reference](./docs/option-c-agent.md):\n\n- **Autonomous Agent**: Deep semantic analysis with tool use\n- **Learning System**: Adapt mappings based on user feedback\n- **Multi-domain Expansion**: Add more technical domains\n- **Codebase Integration**: LSP-based project-specific validation\n\n## Development\n\n### Requirements\n\n- Python 3.9+ (for scripts)\n- UV (for dependency management if needed)\n- Claude Code 2.0+\n\n### Testing\n\n```bash\n# Validate plugin structure\ncc validate-plugin semantic-linguist\n\n# Test hook configuration\n./scripts/test-hook.sh\n\n# Test ambiguity detection\npython scripts/detect-ambiguity.py --test\n```\n\n## Contributing\n\n1. Add domain knowledge to `skills/semantic-validation/knowledge/`\n2. Document new ambiguity patterns in `examples/`\n3. Extend detection patterns in `scripts/detect-ambiguity.py`\n4. Test thoroughly with real-world scenarios\n\n## License\n\nMIT\n\n## Author\n\njcmrs\n\n---\n\n**Remember:** \"Never ASSUME - it makes an ass out of u and me.\" Always verify, never interpret.\n",
        "plugins/semantic-linguist/commands/map-domain.md": "---\nname: map-domain\ndescription: Provide conversational complete reference of domain mappings between Autogen, Langroid, and general concepts with cross-domain translations\n---\n\n# Map Domain Command\n\nProvide comprehensive domain mapping reference to translate between Autogen, Langroid, and general AI agent concepts.\n\n## Process\n\n1. **Identify User's Context**\n   - Detect current domain from conversation (Autogen, Langroid, or general)\n   - Identify target domain (if user is switching or learning new framework)\n   - Determine user's experience level (technical vs non-technical)\n\n2. **Load Domain Knowledge**\n   - Read `skills/semantic-validation/knowledge/ontology-graph.json`\n   - Read `skills/semantic-validation/knowledge/technical-mappings.json`\n   - Load cross-domain equivalents\n\n3. **Present Conversational Complete Reference**\n\n   Start with overview:\n   ```\n   # Domain Mapping Reference\n\n   I'll show you how concepts translate between frameworks.\n\n   **Current context**: [detected domain]\n   **Available domains**: Autogen, Langroid, General AI concepts\n\n   Let me know if you want:\n   1. Complete reference (all mappings)\n   2. Specific concept mapping (e.g., \"how does Autogen's ConversableAgent map to Langroid?\")\n   3. Domain-specific deep dive (all Autogen patterns)\n   4. Cross-framework comparison (same concept across all domains)\n   ```\n\n4. **Provide Requested Mapping Type**\n\n   **Option 1: Complete Reference**\n   ```markdown\n   ## Core Concepts Across Frameworks\n\n   ### Agent Types\n\n   | Concept | Autogen | Langroid | General |\n   |---------|---------|----------|---------|\n   | Basic agent | ConversableAgent | ChatAgent | AI conversational agent |\n   | Tool-enabled agent | AssistantAgent | ToolAgent pattern | Function-calling agent |\n   | Human proxy | UserProxyAgent | Task with interactive=True | Human-in-loop agent |\n   | Orchestrator | GroupChatManager | Parent Task | Multi-agent coordinator |\n\n   ### Communication Patterns\n\n   | Pattern | Autogen | Langroid | General |\n   |---------|---------|----------|---------|\n   | One-to-one | send() / initiate_chat() | Task.run() | Direct messaging |\n   | Multi-party | GroupChat + GroupChatManager | Task hierarchy | Multi-agent system |\n\n   ### Tool/Function Calling\n\n   | Aspect | Autogen | Langroid | General |\n   |--------|---------|----------|---------|\n   | Definition | Function with type hints | ToolMessage subclass | Tool schema |\n   | Registration | @register_for_llm() | Auto-detection | Tool registry |\n   | Execution | @register_for_execution() | ToolMessage.handle() | Tool executor |\n\n   [Continue with comprehensive mappings...]\n\n   **Want to dive deeper into any concept? Just ask!**\n   ```\n\n   **Option 2: Specific Concept**\n   ```markdown\n   ## Mapping: [Concept Name]\n\n   ### What it is (General)\n   [Plain language explanation]\n\n   ### Autogen Implementation\n   - **Class/Method**: [name]\n   - **Purpose**: [what it does]\n   - **Use cases**: [when to use]\n   - **Key methods**: [list]\n   - **Example**:\n     ```python\n     [code example]\n     ```\n\n   ### Langroid Implementation\n   - **Class/Pattern**: [name]\n   - **Purpose**: [what it does]\n   - **Use cases**: [when to use]\n   - **Key methods**: [list]\n   - **Example**:\n     ```python\n     [code example]\n     ```\n\n   ### Conceptual Relationship\n   - Both solve: [problem]\n   - Key difference: [how they differ]\n   - Choose Autogen if: [scenario]\n   - Choose Langroid if: [scenario]\n\n   ### Cross-Domain Translation\n   If you're moving from [framework A] to [framework B]:\n   - Instead of [A concept], use [B equivalent]\n   - Pattern changes from [A pattern] to [B pattern]\n   ```\n\n   **Option 3: Domain-Specific Deep Dive**\n   ```markdown\n   ## [Framework] Complete Patterns\n\n   ### Agent Hierarchy\n   [Detailed class hierarchy with relationships]\n\n   ### Communication Patterns\n   [All messaging patterns with code examples]\n\n   ### Tool Integration\n   [Complete tool calling workflow]\n\n   ### Orchestration\n   [Multi-agent coordination patterns]\n\n   [Include ontology graph visualization as markdown tree]\n   ```\n\n   **Option 4: Cross-Framework Comparison**\n   ```markdown\n   ## [Concept] Across All Frameworks\n\n   ### Problem Statement\n   [What problem this solves]\n\n   ### Autogen Approach\n   - Philosophy: [design philosophy]\n   - Implementation: [how it works]\n   - Code pattern: [example]\n   - Pros: [benefits]\n   - Cons: [limitations]\n\n   ### Langroid Approach\n   - Philosophy: [design philosophy]\n   - Implementation: [how it works]\n   - Code pattern: [example]\n   - Pros: [benefits]\n   - Cons: [limitations]\n\n   ### General Pattern\n   - Abstract concept: [explanation]\n   - When to use: [scenarios]\n   - Alternatives: [other approaches]\n\n   ### Migration Guide\n   From Autogen to Langroid:\n   1. [Step 1]\n   2. [Step 2]\n\n   From Langroid to Autogen:\n   1. [Step 1]\n   2. [Step 2]\n   ```\n\n5. **Interactive Follow-Up**\n\n   After presenting information, offer:\n   ```\n   What would you like to explore next?\n   - Dive deeper into [related concept]\n   - See code examples for [specific pattern]\n   - Compare with [alternative framework]\n   - Get clarification on [ambiguous term]\n   - Continue with current understanding\n   ```\n\n## Usage Examples\n\n**Basic usage (presents options):**\n```\n/map-domain\n```\n\n**Specific concept:**\n```\n/map-domain ConversableAgent\n```\n\n**Framework comparison:**\n```\n/map-domain autogen vs langroid\n```\n\n**After command, user can request:**\n```\nshow me tool calling patterns\n```\nor\n```\nhow do I migrate from Autogen to Langroid?\n```\nor\n```\nexplain GroupChat in simple terms\n```\n\n## Data Sources\n\n- **ontology-graph.json**: Conceptual hierarchies and relationships\n- **technical-mappings.json**: Precise technical translations\n- **ambiguous-terms.json**: User phrase to technical term mappings\n- **references/domain-ontologies.md**: Detailed relationship graphs\n- **references/translation-patterns.md**: Common translation patterns\n\n## Output Format\n\n- **Conversational introduction**: Detect context, present options\n- **Structured reference**: Tables, hierarchies, code examples\n- **Interactive navigation**: User can drill down or switch topics\n- **Always contextual**: Adapt to user's current framework and goals\n\n## Important Principles\n\n- **User-directed**: Present complete reference, user chooses depth\n- **Conversational**: Not a dry reference dump, but guided exploration\n- **Multi-modal**: Tables, code, explanations, visualizations\n- **Cross-linkable**: Connect related concepts naturally\n- **No assumptions**: If unclear which framework user wants, ask\n- **Practical focus**: Include code examples and use cases\n- **Jargon translation**: Always explain technical terms\n\n## Integration\n\n- Works standalone or after semantic validation\n- Can be triggered from `/validate-terminology` findings\n- References same knowledge base as validation workflow\n- Complements clarification process with comprehensive mappings\n",
        "plugins/semantic-linguist/commands/semantic-config.md": "---\nname: semantic-config\ndescription: Configure semantic validation sensitivity, interaction style, enabled domains, and custom user trigger words or phrases for personalized ambiguity detection\n---\n\n# Semantic Configuration Command\n\nConfigure semantic validation behavior, detection sensitivity, and custom user trigger phrases.\n\n## Process\n\n1. **Check for Existing Configuration**\n   - Look for `.claude/semantic-linguist.local.md` in project\n   - If exists, read current settings from YAML frontmatter\n   - If not exists, use defaults\n\n2. **Present Current Configuration**\n   ```\n   # Semantic Linguist Configuration\n\n   **Current Settings:**\n\n   ## Detection Sensitivity\n   - **Threshold**: [current] (Options: low=50, medium=60, high=80)\n   - **Meta-questions**: [enabled/disabled]\n   - **Domain confusion**: [enabled/disabled]\n\n   ## Interaction Style\n   - **Mode**: [current] (conversational | explicit | minimal)\n   - **Auto-validate**: [yes/no]\n\n   ## Enabled Domains\n   - **Autogen**: [âœ“/âœ—]\n   - **Langroid**: [âœ“/âœ—]\n   - **General**: [âœ“/âœ—]\n\n   ## Custom User Triggers\n   - [list of custom phrases]\n   - (These are phrases unique to you that should trigger validation)\n\n   ---\n\n   What would you like to configure?\n   1. Adjust sensitivity\n   2. Change interaction style\n   3. Enable/disable domains\n   4. Add custom trigger phrases\n   5. Reset to defaults\n   6. View all settings\n   ```\n\n3. **Interactive Configuration**\n\n   **Option 1: Adjust Sensitivity**\n   ```\n   ## Detection Sensitivity Settings\n\n   **Confidence Threshold**:\n   - Low (50): More detections, may include minor ambiguities\n   - Medium (60): Balanced - catches significant ambiguities (default)\n   - High (80): Only very ambiguous terms or meta-questions\n\n   Current: [X]\n\n   **Detection Categories** (enable/disable):\n   - [ ] Meta-questions (\"am I making sense?\")\n   - [ ] High-ambiguity terms (\"make it talk\", \"we need an api\")\n   - [ ] Vague action verbs (\"make it X\", \"do the thing\")\n   - [ ] Generic terms (\"component\", \"service\", \"module\")\n   - [ ] Domain confusion (mixing framework terms)\n   - [ ] Unclear references (\"that\", \"it\", \"the thing\")\n\n   Which would you like to change?\n   ```\n\n   **Option 2: Interaction Style**\n   ```\n   ## Interaction Style\n\n   **Conversational** (default):\n   - Friendly, helpful tone\n   - Presents options naturally\n   - Minimal technical jargon\n   - Good for: All users, especially non-technical\n\n   **Explicit**:\n   - Direct, structured responses\n   - Clear numbered options\n   - More technical precision\n   - Good for: Technical users, rapid clarification\n\n   **Minimal**:\n   - Brief summaries only\n   - Only triggers on high-confidence ambiguities\n   - Assumes user prefers autonomy\n   - Good for: Experienced users who know what they want\n\n   Current: [X]\n\n   **Auto-validate**: [yes/no]\n   - If yes: Automatically analyze and clarify when ambiguity detected\n   - If no: Detect but wait for user to request validation\n\n   Select new style or keep current?\n   ```\n\n   **Option 3: Domain Selection**\n   ```\n   ## Enabled Domains\n\n   Configure which frameworks to provide mappings for:\n\n   - [ ] **Autogen** (multi-agent conversations, GroupChat, AssistantAgent)\n   - [ ] **Langroid** (Task orchestration, ToolMessage, ChatAgent)\n   - [ ] **General** (Cross-framework concepts, general AI patterns)\n\n   Current selection: [X, Y]\n\n   **Why disable a domain?**\n   - Reduces noise if you only work with specific framework\n   - Faster validation (fewer mappings to check)\n   - Cleaner clarification suggestions\n\n   **Recommendation**: Keep all enabled unless you exclusively use one framework.\n\n   Which domains should be active?\n   ```\n\n   **Option 4: Custom Trigger Phrases**\n   ```\n   ## Custom User Triggers\n\n   Add phrases that are unique to your communication style that should trigger validation.\n\n   **Current custom triggers**:\n   [list existing]\n\n   **Why add custom triggers?**\n   Different users have different quirks - phrases you use that might be ambiguous but aren't in the standard knowledge base.\n\n   **Examples of good custom triggers**:\n   - \"hook it up\" (could mean: connect systems, add event handler, integrate API)\n   - \"plug it in\" (could mean: dependency injection, module import, literal plugin)\n   - \"wire everything together\" (could mean: dependency wiring, event connections, integration)\n   - Your unique jargon or shorthand\n\n   **Add new trigger phrase**:\n   Format: \"phrase\" â†’ ambiguity_score (0.5-1.0) â†’ categories â†’ possible meanings\n\n   Example:\n   ```yaml\n   \"hook it up\":\n     score: 0.85\n     categories: [\"vague_action\", \"integration\"]\n     meanings:\n       autogen: [\"register_for_llm\", \"message routing\"]\n       langroid: [\"Task connection\", \"agent chaining\"]\n       general: [\"API integration\", \"event handler\"]\n   ```\n\n   Enter custom phrase to add:\n   ```\n\n   **Option 5: Reset to Defaults**\n   ```\n   ## Reset to Defaults\n\n   This will restore all settings to default values:\n   - Threshold: 60 (medium)\n   - Interaction: conversational\n   - Domains: all enabled\n   - Custom triggers: cleared\n\n   **Your current custom triggers will be lost.**\n\n   Are you sure? (yes/no)\n   ```\n\n   **Option 6: View All Settings**\n   ```\n   # Complete Semantic Linguist Configuration\n\n   ## Detection\n   - **Threshold**: [value]\n   - **Meta-questions**: âœ“\n   - **High-ambiguity terms**: âœ“\n   - **Vague verbs**: âœ“\n   - **Generic terms**: âœ“\n   - **Domain confusion**: âœ“\n   - **Unclear references**: âœ“\n\n   ## Interaction\n   - **Style**: conversational\n   - **Auto-validate**: yes\n\n   ## Domains\n   - **Autogen**: âœ“\n   - **Langroid**: âœ“\n   - **General**: âœ“\n\n   ## Custom Triggers ([count])\n   [detailed list with scores and meanings]\n\n   ## Storage Location\n   `.claude/semantic-linguist.local.md`\n\n   (This file is git-ignored by default)\n   ```\n\n4. **Save Configuration**\n\n   Write settings to `.claude/semantic-linguist.local.md`:\n   ```markdown\n   ---\n   # Semantic Linguist Configuration\n   # Do not commit this file - it contains user-specific settings\n\n   detection:\n     threshold: 60\n     categories:\n       meta_questions: true\n       high_ambiguity: true\n       vague_verbs: true\n       generic_terms: true\n       domain_confusion: true\n       unclear_refs: true\n\n   interaction:\n     style: conversational  # conversational | explicit | minimal\n     auto_validate: true\n\n   domains:\n     autogen: true\n     langroid: true\n     general: true\n\n   custom_triggers:\n     \"hook it up\":\n       score: 0.85\n       categories: [\"vague_action\", \"integration\"]\n       meanings:\n         autogen: [\"register_for_llm\", \"message routing\"]\n         langroid: [\"Task connection\", \"agent chaining\"]\n         general: [\"API integration\", \"event handler\"]\n   ---\n\n   # Semantic Linguist User Configuration\n\n   This file stores your personal semantic validation preferences.\n\n   ## How to Edit\n\n   1. Run `/semantic-config` to use interactive configuration\n   2. Or edit YAML frontmatter directly above\n   3. Changes take effect immediately\n\n   ## Custom Triggers Format\n\n   Add your unique phrases that should trigger validation:\n\n   ```yaml\n   \"your phrase here\":\n     score: 0.5-1.0  # How ambiguous (0.5=moderate, 1.0=very ambiguous)\n     categories: [list of categories]\n     meanings:\n       framework: [list of possible meanings]\n   ```\n\n   ## Need Help?\n\n   Run `/validate-terminology` to see how current settings affect detection.\n   ```\n\n5. **Confirm Changes**\n   ```\n   âœ… Configuration saved to `.claude/semantic-linguist.local.md`\n\n   **New settings**:\n   - [summary of changes]\n\n   Changes are active immediately. Run `/validate-terminology` to test new settings.\n\n   **Next steps**:\n   - Test with recent conversation: `/validate-terminology`\n   - View domain mappings: `/map-domain`\n   - Continue working with new validation settings\n   ```\n\n## Usage Examples\n\n**Interactive configuration:**\n```\n/semantic-config\n```\n\n**Quick threshold adjustment:**\n```\n/semantic-config threshold high\n```\n\n**Add custom trigger:**\n```\n/semantic-config add-trigger \"wire it up\"\n```\n\n**View current settings:**\n```\n/semantic-config show\n```\n\n**Reset to defaults:**\n```\n/semantic-config reset\n```\n\n## Configuration File\n\nSettings stored in: `.claude/semantic-linguist.local.md`\n\n**Format**: Markdown with YAML frontmatter\n**Scope**: Project-specific (each project can have different settings)\n**Version control**: Automatically added to `.gitignore`\n\n## Default Values\n\n```yaml\ndetection:\n  threshold: 60\n  categories:\n    meta_questions: true\n    high_ambiguity: true\n    vague_verbs: true\n    generic_terms: true\n    domain_confusion: true\n    unclear_refs: true\n\ninteraction:\n  style: conversational\n  auto_validate: true\n\ndomains:\n  autogen: true\n  langroid: true\n  general: true\n\ncustom_triggers: {}\n```\n\n## Important Principles\n\n- **User-specific**: Each user has unique communication patterns\n- **Project-scoped**: Different projects may need different settings\n- **Git-safe**: Configuration file is git-ignored by default\n- **Immediately active**: No restart required\n- **Reversible**: Can always reset to defaults\n- **Transparent**: View all settings at any time\n\n## Integration\n\n- UserPromptSubmit hook reads these settings\n- Detection threshold affects confidence scoring\n- Custom triggers added to ambiguous-terms.json logic\n- Domain selection filters mapping results\n- Interaction style affects response formatting\n\n## Advanced: Direct Editing\n\nPower users can edit `.claude/semantic-linguist.local.md` directly:\n\n1. Open file in editor\n2. Modify YAML frontmatter\n3. Save file\n4. Changes apply immediately\n\nValidate syntax with `/semantic-config validate`.\n",
        "plugins/semantic-linguist/commands/validate-terminology.md": "---\nname: validate-terminology\ndescription: Analyze recent conversation messages for ambiguous terminology and provide semantic validation with conversational summary and optional detailed report\n---\n\n# Validate Terminology Command\n\nAnalyze the last 5-10 messages in the conversation for semantic ambiguities and provide validation feedback.\n\n## Process\n\n1. **Retrieve Recent Messages**\n   - Extract last 5-10 user messages from conversation history\n   - Include Claude's responses for context\n\n2. **Load Knowledge Base**\n   - Read `skills/semantic-validation/knowledge/ambiguous-terms.json`\n   - Read `skills/semantic-validation/knowledge/technical-mappings.json`\n   - Read `skills/semantic-validation/knowledge/ontology-graph.json`\n\n3. **Detect Ambiguities**\n   - Scan for known ambiguous terms from knowledge base\n   - Identify meta-questions (\"am I making sense?\", \"does this make sense?\")\n   - Detect vague action verbs (\"make it X\", \"do the thing\")\n   - Check for generic technical terms without context\n   - Look for unclear references (\"that\", \"it\", \"the thing\")\n   - Identify domain confusion (mixing framework-specific terms)\n\n4. **Calculate Confidence Scores**\n   - Meta-question detected: +100 (auto-trigger)\n   - Known high-ambiguity term: +40\n   - Vague action verb: +30\n   - Generic term without context: +25\n   - Domain confusion: +35\n   - Unclear reference: +20\n   - Recent conversation provides context: -20\n   - Specific technical term used: -30\n\n5. **Provide Conversational Summary**\n   - **If no ambiguities found (score < 60):**\n     ```\n     âœ… Terminology looks clear! I didn't detect any significant ambiguities in the last [N] messages.\n\n     The conversation has been using specific technical terms and clear references.\n     ```\n\n   - **If minor ambiguities (score 60-79):**\n     ```\n     âš ï¸ Found some potentially ambiguous terms:\n\n     - \"[term]\" could mean:\n       â€¢ [Option 1] (most likely based on context)\n       â€¢ [Option 2]\n\n     Current context suggests [interpretation], but let me know if that's not what you meant.\n\n     Want a detailed report? Just ask!\n     ```\n\n   - **If significant ambiguities (score â‰¥ 80):**\n     ```\n     ğŸ” Detected several ambiguous terms that might benefit from clarification:\n\n     **High-priority:**\n     - \"[term 1]\" (score: [X]) - Could mean:\n       â€¢ [Domain 1]: [precise meaning]\n       â€¢ [Domain 2]: [precise meaning]\n       â€¢ [General]: [precise meaning]\n\n     **Moderate-priority:**\n     - \"[term 2]\" (score: [Y]) - Possible interpretations...\n\n     Would you like me to:\n     1. Clarify these terms now\n     2. See a detailed analysis report\n     3. Continue with my best understanding\n\n     (Type \"detailed report\" for comprehensive analysis)\n     ```\n\n6. **Generate Detailed Report (if requested)**\n   When user requests detailed report, provide:\n\n   ```markdown\n   # Semantic Validation Report\n\n   ## Analysis Summary\n   - **Messages analyzed**: [N]\n   - **Ambiguities detected**: [count]\n   - **Confidence scores**: [range]\n   - **Domains detected**: [list]\n\n   ## Detected Ambiguities\n\n   ### 1. \"[ambiguous term]\" (Score: [X])\n\n   **Category**: [vague_action_verb | unclear_scope | generic_term | meta_question | domain_confusion]\n\n   **Possible meanings**:\n   - **Autogen**: [precise translation]\n     - Methods: [list]\n     - Use cases: [list]\n   - **Langroid**: [precise translation]\n     - Methods: [list]\n     - Use cases: [list]\n   - **General**: [precise translation]\n\n   **Context clues**:\n   - [Relevant context from conversation]\n\n   **Recommended clarification**:\n   > \"[Specific question to ask user]\"\n\n   **Why this matters**:\n   [Explanation of why ambiguity is problematic]\n\n   ---\n\n   ### 2. [Next ambiguity]...\n\n   ## Cross-Domain Equivalents\n\n   If user is working across frameworks:\n   - **Autogen term** â†’ **Langroid equivalent** â†’ **General concept**\n   - [mappings from ontology-graph.json]\n\n   ## Recommendations\n\n   1. **Immediate clarifications needed**: [list]\n   2. **Context-dependent terms**: [list]\n   3. **Suggested terminology**: [precise alternatives]\n\n   ## Next Steps\n\n   Would you like me to:\n   - Clarify specific terms now\n   - Map to your target domain\n   - Continue with validated understanding\n   ```\n\n## Usage Examples\n\n**Basic usage:**\n```\n/validate-terminology\n```\n\n**After validation, user can request:**\n```\ndetailed report\n```\nor\n```\nclarify [specific term]\n```\n\n## Integration\n\n- Loads semantic-validation skill automatically\n- Uses knowledge files for detection\n- References ontology-graph.json for cross-domain mappings\n- Conversational and non-blocking\n- Always offers options, never assumes\n\n## Output Format\n\n- **Conversational summary** (default): 3-5 sentences with key findings\n- **Detailed report** (on request): Comprehensive markdown analysis\n- **Interactive follow-up**: User can ask for clarifications or mappings\n\n## Important Principles\n\n- **Never assume**: Present options, verify with user\n- **Conversational tone**: Friendly and helpful, not prescriptive\n- **Context-aware**: Consider recent conversation flow\n- **Actionable**: Provide clear next steps\n- **Optional detail**: Summary first, detailed report on request\n",
        "plugins/semantic-linguist/hooks/hooks.json": "{\n  \"description\": \"Semantic validation hooks for detecting and resolving ambiguous terminology in user prompts\",\n  \"hooks\": {\n    \"UserPromptSubmit\": [\n      {\n        \"matcher\": \"*\",\n        \"hooks\": [\n          {\n            \"type\": \"prompt\",\n            \"prompt\": \"# Semantic Ambiguity Detection Hook\\n\\nAnalyze the user's prompt for semantic ambiguity that could lead to misinterpretation, assumptions, or hallucinations.\\n\\n## User Prompt\\n```\\n$USER_PROMPT\\n```\\n\\n## Detection Criteria\\n\\n### HIGH Confidence Triggers (Always Validate)\\n1. **Meta-questions** - User seeking validation:\\n   - \\\"am I making sense?\\\"\\n   - \\\"does this make sense?\\\"\\n   - \\\"is this right?\\\"\\n   - \\\"am I doing this right?\\\"\\n\\n2. **User self-identification**:\\n   - \\\"non-technical user\\\"\\n   - \\\"I'm not technical\\\"\\n   - \\\"beginner\\\"\\n\\n3. **Known high-ambiguity terms** (from knowledge base):\\n   - \\\"make it talk\\\" / \\\"make it work\\\" / \\\"make it [X]\\\"\\n   - \\\"we need an api\\\"\\n   - \\\"make it portable\\\"\\n   - \\\"check for gaps\\\"\\n\\n### MODERATE Confidence (Validate if Multiple Signals)\\n1. **Vague action verbs**:\\n   - \\\"do the thing\\\"\\n   - \\\"fix it\\\"\\n   - \\\"create X\\\" (without specifics)\\n\\n2. **Generic technical terms without context**:\\n   - \\\"agent\\\" (could be Autogen, Langroid, general)\\n   - \\\"task\\\" (could be Langroid Task, async task, general)\\n   - \\\"tool\\\" (could be function calling, utility, CLI)\\n   - \\\"component\\\", \\\"service\\\", \\\"module\\\"\\n\\n3. **Unclear scope**:\\n   - \\\"add validation\\\", \\\"improve performance\\\", \\\"add logging\\\"\\n   - Without specifying what type or metric\\n\\n4. **Domain confusion**:\\n   - Mixing framework-specific terms\\n   - Using terms from incompatible domains\\n\\n5. **Unclear references**:\\n   - \\\"that\\\", \\\"it\\\", \\\"the thing\\\"\\n   - Without clear antecedent\\n\\n## Analysis Process\\n\\n1. **Check for HIGH confidence triggers**:\\n   - If meta-question detected â†’ VALIDATE IMMEDIATELY\\n   - If user self-identifies ambiguity â†’ VALIDATE IMMEDIATELY\\n   - If known high-ambiguity term â†’ VALIDATE IMMEDIATELY\\n\\n2. **Calculate confidence score** (0-100):\\n   - Meta-question: +100 (auto-trigger)\\n   - Known ambiguous term: +40\\n   - Vague action verb: +30\\n   - Generic term without context: +25\\n   - Domain confusion: +35\\n   - Unclear reference: +20\\n   - Recent conversation provides context: -20\\n   - Specific technical term used: -30\\n\\n3. **Trigger validation if**:\\n   - Confidence score > 80 (high confidence)\\n   - OR meta-question detected\\n   - OR user trigger phrase detected\\n\\n## Output Format\\n\\nIf validation NOT needed (low ambiguity):\\n```json\\n{\\n  \\\"continue\\\": true,\\n  \\\"systemMessage\\\": \\\"\\\"\\n}\\n```\\n\\nIf validation NEEDED (high ambiguity or trigger detected):\\n```json\\n{\\n  \\\"continue\\\": true,\\n  \\\"systemMessage\\\": \\\"âš ï¸ Semantic validation triggered. Detected: [specific ambiguity]. Load 'semantic-validation' skill and clarify before proceeding. Conversational tone required - never assume, always verify.\\\"\\n}\\n```\\n\\n## Examples\\n\\n**Example 1: Meta-question (HIGH confidence)**\\nUser: \\\"I want to build a multi-agent system. Am I making sense?\\\"\\nOutput:\\n```json\\n{\\n  \\\"continue\\\": true,\\n  \\\"systemMessage\\\": \\\"âš ï¸ Semantic validation triggered. User meta-question detected: 'Am I making sense?' - indicates user uncertainty. Analyze last 5-10 messages, identify ambiguities in 'multi-agent system' (Autogen GroupChat? Langroid Tasks? General concept?), and validate understanding with user before proceeding.\\\"\\n}\\n```\\n\\n**Example 2: High-ambiguity term (HIGH confidence)**\\nUser: \\\"I need to make the agent talk to other agents\\\"\\nOutput:\\n```json\\n{\\n  \\\"continue\\\": true,\\n  \\\"systemMessage\\\": \\\"âš ï¸ Semantic validation triggered. Detected high-ambiguity term: 'make it talk' (score: 90). Unclear: which framework (Autogen/Langroid?), what type of communication (send()? GroupChat? Task delegation?). Load 'semantic-validation' skill, clarify framework and communication pattern before implementing.\\\"\\n}\\n```\\n\\n**Example 3: Generic term with context (MODERATE - don't trigger)**\\nUser: \\\"In Autogen, I want to create a ConversableAgent that sends messages\\\"\\nOutput:\\n```json\\n{\\n  \\\"continue\\\": true,\\n  \\\"systemMessage\\\": \\\"\\\"\\n}\\n```\\n(No validation needed - specific framework and class mentioned)\\n\\n**Example 4: Multiple moderate signals (MODERATE - trigger)**\\nUser: \\\"I need to create an agent with tools\\\"\\nOutput:\\n```json\\n{\\n  \\\"continue\\\": true,\\n  \\\"systemMessage\\\": \\\"âš ï¸ Semantic validation triggered. Detected moderate ambiguity (score: 75): 'agent' and 'tools' are multi-domain terms. Could be Autogen (AssistantAgent + register_for_llm), Langroid (ToolAgent + ToolMessage), or other. Load 'semantic-validation' skill and clarify framework before proceeding.\\\"\\n}\\n```\\n\\n**Example 5: Vague scope (MODERATE - trigger)**\\nUser: \\\"Check for gaps in the codebase\\\"\\nOutput:\\n```json\\n{\\n  \\\"continue\\\": true,\\n  \\\"systemMessage\\\": \\\"âš ï¸ Semantic validation triggered. Detected unclear scope: 'check for gaps' (score: 82). Could mean: test coverage gaps, documentation gaps, feature gaps, security gaps, or data gaps. Load 'semantic-validation' skill and clarify which type of gap analysis before proceeding.\\\"\\n}\\n```\\n\\n## Critical Guidelines\\n\\n1. **Never assume** - If ambiguity detected, trigger validation\\n2. **Conversational tone** - System messages should guide, not dictate\\n3. **Be specific** - Identify exact ambiguous terms and possible interpretations\\n4. **Load skill** - Always mention loading 'semantic-validation' skill when triggered\\n5. **Don't block** - Set continue: true (provide guidance, don't stop execution)\\n6. **Context-aware** - Consider recent conversation when scoring\\n\\n## Your Task\\n\\nAnalyze the user prompt above. If ambiguity detected with confidence > 80% OR meta-question/trigger detected, output validation trigger. Otherwise, output empty systemMessage.\",\n            \"timeout\": 30\n          }\n        ]\n      }\n    ]\n  }\n}\n",
        "plugins/semantic-linguist/skills/semantic-translation/AI-DOMAIN-ADDITION-GUIDE.md": "# AI Assistant Guide: Adding New Domains to Semantic-Linguist Knowledge Base\n\n> **Audience:** This guide is written FOR AI Assistants who need to extend the semantic-linguist plugin with new domain knowledge.\n\n**Purpose:** Systematic process for identifying domain-specific ambiguous terms, creating technical mappings, establishing conceptual relationships, and building decision trees for disambiguation.\n\n---\n\n## Table of Contents\n\n1. [When to Add a New Domain](#when-to-add-a-new-domain)\n2. [Step-by-Step Domain Addition Process](#step-by-step-domain-addition-process)\n3. [Phase 1: Domain Analysis](#phase-1-domain-analysis)\n4. [Phase 2: Ambiguous Terms Identification](#phase-2-ambiguous-terms-identification)\n5. [Phase 3: Technical Mappings Creation](#phase-3-technical-mappings-creation)\n6. [Phase 4: Ontology Graph Development](#phase-4-ontology-graph-development)\n7. [Phase 5: Ambiguity Resolution](#phase-5-ambiguity-resolution)\n8. [JSON Format Templates](#json-format-templates)\n9. [Domain Classification Decision Trees](#domain-classification-decision-trees)\n10. [Validation Checklist](#validation-checklist)\n\n---\n\n## When to Add a New Domain\n\nAdd a new domain when:\n- User works in a specialized field not covered by existing domains\n- Existing terms are frequently misinterpreted due to missing domain context\n- New technology/framework becomes central to user's projects\n- Cross-domain term conflicts emerge repeatedly\n\n**Current domains covered:** Autogen, Langroid, MCP, UTCP, FastAPI, Git/Gitflow, SRE, Memory Graphs, General Software Engineering\n\n---\n\n## Step-by-Step Domain Addition Process\n\n**Overview:** 5 phases, executed sequentially\n\n1. **Domain Analysis** - Understand scope, terminology, frameworks\n2. **Ambiguous Terms Identification** - Find multi-domain conflicts\n3. **Technical Mappings Creation** - Document domain-specific translations\n4. **Ontology Graph Development** - Model conceptual relationships\n5. **Ambiguity Resolution** - Build decision trees for disambiguation\n\n**Files Modified:**\n- `skills/semantic-validation/knowledge/ambiguous-terms.json`\n- `skills/semantic-validation/knowledge/technical-mappings.json`\n- `skills/semantic-validation/knowledge/ontology-graph.json`\n\n---\n\n## Phase 1: Domain Analysis\n\n**Goal:** Deeply understand the domain before adding it to the knowledge base.\n\n### Analysis Steps\n\n1. **Framework/Technology Identification**\n   - What frameworks/libraries/tools define this domain?\n   - What are the core abstractions?\n   - What are the common operations?\n\n2. **Terminology Inventory**\n   - List domain-specific terms (20-30 key terms)\n   - Identify overloaded terms (words with different meanings in different contexts)\n   - Find synonyms (different words, same concept)\n\n3. **Usage Pattern Analysis**\n   - How do users typically phrase requests in this domain?\n   - What are the most common user triggers? (\"create X\", \"add Y\", \"configure Z\")\n   - What are typical ambiguities users encounter?\n\n4. **Cross-Domain Relationship Mapping**\n   - Which existing domains overlap with this new domain?\n   - What concepts map across domains? (e.g., \"tool\" in Autogen vs MCP vs UTCP)\n   - Where do conflicts arise?\n\n**Example: Adding FastAPI Domain**\n\n```markdown\nFrameworks: FastAPI (Python web framework)\nCore Abstractions: Application, PathOperation, Dependency, Pydantic models\nCommon Operations: Routing, validation, dependency injection, async operations\n\nKey Terms:\n- endpoint (ambiguous: API endpoint vs network endpoint vs path operation)\n- dependency (ambiguous: Depends() vs package dependency vs architectural dependency)\n- model (ambiguous: Pydantic model vs AI model vs database model)\n- validation (ambiguous: Pydantic validation vs input validation vs business logic validation)\n\nUser Triggers:\n- \"create an api\"\n- \"add validation\"\n- \"define endpoint\"\n- \"inject dependency\"\n\nCross-Domain Overlaps:\n- MCP: Both have \"resource\" concept (MCP resource vs REST resource)\n- General: \"API\" is generic (FastAPI server vs general API design)\n```\n\n### Deliverable\n\n**Domain Analysis Document** containing:\n- Framework summary\n- 20-30 key terms with ambiguity notes\n- Common user phrases\n- Cross-domain mappings\n\n---\n\n## Phase 2: Ambiguous Terms Identification\n\n**Goal:** Identify terms that have different meanings in different domains and would cause user-AI miscommunication.\n\n### Ambiguity Criteria\n\nA term is ambiguous if:\n- It appears in 2+ domains with different meanings\n- Users frequently misstate it (meta-questions, vague phrasing)\n- Context is required to determine correct interpretation\n- Incorrect interpretation would lead to wrong implementation\n\n### Ambiguity Score Calculation\n\n**Formula:** `(number_of_domains Ã— context_dependency) / 10`\n\n- **number_of_domains**: How many distinct interpretations exist\n- **context_dependency**: How much context is needed (1-5 scale)\n  - 1: Clear from keyword alone\n  - 3: Framework name provides clarity\n  - 5: Requires deep conversation context\n\n**Examples:**\n- \"agent\" (Autogen, Langroid, general AI) â†’ 3 domains Ã— 3 context â†’ 0.9 ambiguity\n- \"tool\" (Autogen, Langroid, MCP, UTCP, CLI, general) â†’ 6 domains Ã— 4 context â†’ 2.4 (capped at 1.0) â†’ 1.0 ambiguity\n- \"merge\" (Git, GitHub PR, Gitflow, data merge) â†’ 4 domains Ã— 4 context â†’ 1.6 (capped at 1.0) â†’ 0.8 ambiguity\n\n**Normalization:** Cap at 1.0, round to 2 decimals\n\n### Term Categories\n\nCategorize each ambiguous term:\n\n- **domain_specific**: Unique to one framework (e.g., \"ConversableAgent\" is Autogen-only)\n- **multi_domain**: Appears in multiple domains with different meanings (e.g., \"tool\")\n- **meta_question**: User uncertainty patterns (e.g., \"making sense?\", \"is this right?\")\n- **vague_action_verb**: Non-specific verbs (e.g., \"make it portable\", \"fix it\")\n- **infrastructure_type**: System component ambiguity (e.g., \"server\", \"container\")\n- **knowledge_structure**: Data organization ambiguity (e.g., \"memory\", \"graph\")\n- **git_operation**: Version control operations (e.g., \"merge\", \"rebase\")\n- **sre_concept**: Reliability/observability terms (e.g., \"observability\", \"SLI\")\n\n### Ambiguous Terms JSON Structure\n\n```json\n{\n  \"term\": {\n    \"ambiguity_score\": 0.75,\n    \"category\": \"multi_domain\",\n    \"contexts\": [\"domain1\", \"domain2\", \"domain3\"],\n    \"user_triggers\": [\"phrase user might say 1\", \"phrase user might say 2\"],\n    \"domains\": {\n      \"domain1\": [\"meaning1 in domain1\", \"meaning2 in domain1\"],\n      \"domain2\": {\n        \"types\": [\"subtype1\", \"subtype2\"],\n        \"patterns\": [\"pattern1\", \"pattern2\"]\n      },\n      \"domain3\": \"simple string meaning\"\n    },\n    \"clarification_needed\": [\"Question 1?\", \"Question 2?\"]\n  }\n}\n```\n\n### Deliverable\n\n**Add entries to `ambiguous-terms.json`** for each identified ambiguous term.\n\n**Checklist per term:**\n- [ ] Ambiguity score calculated (0.0-1.0)\n- [ ] Category assigned\n- [ ] Contexts listed (all applicable domains)\n- [ ] User triggers documented (3-5 common phrases)\n- [ ] Domain-specific meanings detailed\n- [ ] Clarification questions formulated (2-4 questions)\n\n---\n\n## Phase 3: Technical Mappings Creation\n\n**Goal:** Document domain-specific technical translations for each concept, operation, and pattern.\n\n### Mapping Structure\n\nOrganize by domain â†’ subcategory â†’ concept â†’ details\n\n**Example Pattern:**\n\n```json\n{\n  \"new_domain\": {\n    \"subcategory_1\": {\n      \"concept_name\": {\n        \"purpose\": \"What this does\",\n        \"use_cases\": [\"when to use 1\", \"when to use 2\"],\n        \"pattern\": \"How it works / typical usage pattern\",\n        \"signature\": \"Code signature or invocation pattern\",\n        \"tools\": [\"tool1\", \"tool2\"],\n        \"similar_to\": {\n          \"other_domain\": \"equivalent concept in other domain\"\n        }\n      }\n    }\n  }\n}\n```\n\n### Subcategory Guidelines\n\nChoose subcategories that reflect domain structure:\n- **Frameworks:** Agent creation, communication, tool integration\n- **APIs:** Routing, validation, authentication, async operations\n- **Version Control:** Core operations, workflows, collaboration\n- **SRE:** Observability, reliability, incident management\n- **Memory Systems:** Graph types, storage patterns, retrieval patterns\n\n**Rule:** 3-6 subcategories per domain, each with 3-10 concepts\n\n### Technical Mapping JSON Template\n\n```json\n{\n  \"domain_name\": {\n    \"subcategory_name\": {\n      \"ConceptName\": {\n        \"purpose\": \"Brief description (1 sentence)\",\n        \"use_cases\": [\"use case 1\", \"use case 2\", \"use case 3\"],\n        \"pattern\": \"Typical usage pattern or workflow\",\n        \"signature\": \"Code signature, command syntax, or invocation format\",\n        \"components\": [\"component1\", \"component2\"],\n        \"tools\": [\"tool1\", \"tool2\"],\n        \"similar_to\": {\n          \"domain1\": \"equivalent in domain1\",\n          \"domain2\": \"equivalent in domain2\"\n        }\n      }\n    }\n  }\n}\n```\n\n### Cross-Domain Equivalents Section\n\n**Always update `cross_domain_equivalents`** when adding a new domain.\n\nMap concepts that exist across multiple domains:\n\n```json\n{\n  \"cross_domain_equivalents\": {\n    \"concept_name\": {\n      \"autogen\": \"Autogen equivalent\",\n      \"langroid\": \"Langroid equivalent\",\n      \"new_domain\": \"New domain equivalent\",\n      \"general\": \"General term\"\n    }\n  }\n}\n```\n\n**Examples:**\n- `tool_agent`: Maps AssistantAgent (Autogen) â†” ToolAgent (Langroid) â†” MCP client with tools â†” UTCP-enabled agent\n- `data_source`: Maps MCP Resource â†” FastAPI endpoint â†” general data retrieval mechanism\n\n### Deliverable\n\n**Add section to `technical-mappings.json`** with comprehensive domain translations.\n\n**Checklist:**\n- [ ] Domain section created with 3-6 subcategories\n- [ ] Each subcategory has 3-10 concepts fully documented\n- [ ] Cross-domain equivalents updated\n- [ ] Similar concepts from other domains referenced\n- [ ] Code signatures/patterns provided for technical precision\n\n---\n\n## Phase 4: Ontology Graph Development\n\n**Goal:** Model conceptual relationships, hierarchies, and cross-domain connections.\n\n### Ontology Components\n\n1. **Domain-Specific Classes/Concepts**\n   - Type classification (abstract_base, concrete_class, pattern, component, etc.)\n   - Parent-child relationships (inheritance, specialization)\n   - Relationship modeling (IS_A, HAS_A, USES, PROVIDES, ENABLES, etc.)\n   - Cross-domain equivalents\n\n2. **Conceptual Relationships**\n   - Abstract concepts that span domains (e.g., \"tool_use\", \"orchestration\", \"observability\")\n   - Implementation variations across domains\n   - Integration points\n\n3. **Ambiguity Resolution Graph**\n   - Disambiguation questions\n   - Decision trees for choosing correct interpretation\n\n### Domain Ontology JSON Template\n\n```json\n{\n  \"domain_name\": {\n    \"ConceptName\": {\n      \"type\": \"concrete_class | abstract_base | pattern | component | orchestration_object\",\n      \"parent\": \"ParentConcept (if applicable)\",\n      \"children\": [\"ChildConcept1\", \"ChildConcept2\"],\n      \"purpose\": \"What this concept represents\",\n      \"relationships\": {\n        \"IS_A\": \"parent or abstraction\",\n        \"HAS_A\": [\"attribute1\", \"attribute2\"],\n        \"CAN_DO\": [\"capability1\", \"capability2\"],\n        \"USES\": \"dependencies\",\n        \"PROVIDES\": \"what it offers\",\n        \"ENABLES\": \"what it makes possible\",\n        \"INTEGRATES_WITH\": [\"integration1\", \"integration2\"],\n        \"SIMILAR_TO\": {\n          \"other_domain\": \"equivalent concept\"\n        }\n      },\n      \"cross_domain_equivalent\": {\n        \"domain1\": \"equivalent1\",\n        \"domain2\": \"equivalent2\",\n        \"general\": \"general term\"\n      }\n    }\n  }\n}\n```\n\n### Relationship Types\n\nUse precise relationship types:\n- **IS_A**: Inheritance/specialization (AssistantAgent IS_A ConversableAgent)\n- **HAS_A**: Composition/attributes (Agent HAS_A llm_config)\n- **CAN_DO**: Capabilities/methods (Agent CAN_DO send, receive)\n- **USES**: Dependencies (ToolAgent USES ToolMessage)\n- **PROVIDES**: What it offers (MCP Server PROVIDES tools to clients)\n- **ENABLES**: What it makes possible (GroupChat ENABLES multi-party conversation)\n- **INTEGRATES_WITH**: External integrations (AssistantAgent INTEGRATES_WITH MCP tools)\n- **SIMILAR_TO**: Cross-domain equivalents (for disambiguation)\n\n### Conceptual Relationships Template\n\nAdd abstract concepts that the new domain implements:\n\n```json\n{\n  \"conceptual_relationships\": {\n    \"new_abstract_concept\": {\n      \"abstract_concept\": \"High-level description\",\n      \"patterns\": {\n        \"pattern1\": \"Description\",\n        \"pattern2\": \"Description\"\n      },\n      \"implementations\": {\n        \"domain1\": \"How domain1 implements this\",\n        \"new_domain\": \"How new domain implements this\",\n        \"general\": \"General approach\"\n      },\n      \"enables\": {\n        \"capability1\": \"What it enables\",\n        \"capability2\": \"What it enables\"\n      }\n    }\n  }\n}\n```\n\n**Examples of Abstract Concepts:**\n- `tool_use`: How different frameworks enable function calling\n- `orchestration`: How multiple agents/tasks are coordinated\n- `data_validation`: How data correctness is ensured\n- `knowledge_retrieval`: How information is found and retrieved\n\n### Deliverable\n\n**Add to `ontology-graph.json`:**\n1. Domain-specific ontology section with all major concepts\n2. New conceptual relationships (if domain introduces new abstract patterns)\n3. Updates to existing conceptual relationships (new implementation examples)\n\n**Checklist:**\n- [ ] All major domain concepts modeled with relationships\n- [ ] Type classifications accurate (class, pattern, component, etc.)\n- [ ] Parent-child hierarchies established\n- [ ] Cross-domain equivalents mapped\n- [ ] At least 1 new conceptual relationship added (if applicable)\n- [ ] Existing conceptual relationships updated with new domain examples\n\n---\n\n## Phase 5: Ambiguity Resolution\n\n**Goal:** Build decision trees to guide disambiguation when ambiguous terms are detected.\n\n### Decision Tree Structure\n\nFor each major ambiguous term in the new domain, create a decision tree:\n\n```json\n{\n  \"ambiguity_resolution_graph\": {\n    \"ambiguous_term\": {\n      \"disambiguation_questions\": [\n        \"Question 1 to narrow context?\",\n        \"Question 2 to identify domain?\",\n        \"Question 3 to determine specific variant?\"\n      ],\n      \"decision_tree\": {\n        \"dimension1\": {\n          \"option1\": \"Interpretation for option1\",\n          \"option2\": {\n            \"sub_option1\": \"Specific interpretation\",\n            \"sub_option2\": \"Another specific interpretation\"\n          }\n        },\n        \"dimension2\": {\n          \"case1\": \"domain1 approach\",\n          \"case2\": \"domain2 approach\",\n          \"case3\": \"new_domain approach\"\n        }\n      }\n    }\n  }\n}\n```\n\n### Question Formulation Guidelines\n\n**Good disambiguation questions:**\n- Narrow scope quickly (framework, domain, purpose)\n- Ask about concrete artifacts (\"What operation?\", \"What kind of server?\")\n- Avoid vague questions (\"What do you want?\")\n\n**Examples:**\n- \"Which framework? (Autogen, Langroid, MCP)\"\n- \"What type of memory? (episodic, semantic, vector)\"\n- \"What operations? (routing, validation, authentication)\"\n- \"For humans or AI agents?\"\n\n### Decision Tree Best Practices\n\n1. **Start broad, narrow progressively**\n   - First level: Framework/domain identification\n   - Second level: Concept category\n   - Third level: Specific variant\n\n2. **Include all domains where term appears**\n   - Even if new domain doesn't use this term, update existing decision trees to acknowledge the new domain\n\n3. **Provide concrete outcomes**\n   - Terminal nodes should be specific implementations, not vague categories\n   - Include code patterns or command syntax where applicable\n\n**Example Decision Tree:**\n\n```json\n{\n  \"tool\": {\n    \"disambiguation_questions\": [\n      \"Which framework?\",\n      \"What should the tool do?\",\n      \"Is it for AI agents or general use?\"\n    ],\n    \"decision_tree\": {\n      \"framework\": {\n        \"autogen\": \"register_for_llm() + register_for_execution()\",\n        \"langroid\": \"ToolMessage subclass\",\n        \"mcp\": \"MCP server with tool schema\",\n        \"utcp\": \"Universal tool schema + adapters\",\n        \"new_domain\": \"New domain tool pattern\"\n      },\n      \"purpose\": {\n        \"ai_agents\": \"MCP or UTCP tool\",\n        \"general_automation\": \"CLI tool or script\"\n      }\n    }\n  }\n}\n```\n\n### Deliverable\n\n**Update `ambiguity_resolution_graph` in `ontology-graph.json`:**\n1. Add decision trees for new ambiguous terms from new domain\n2. Update existing decision trees to include new domain options\n\n**Checklist:**\n- [ ] 3-5 disambiguation questions per ambiguous term\n- [ ] Decision trees cover all domain variants\n- [ ] Outcomes are concrete and actionable\n- [ ] Existing decision trees updated with new domain\n\n---\n\n## JSON Format Templates\n\n### Complete Template: Adding \"NewDomain\"\n\n#### 1. ambiguous-terms.json\n\n```json\n{\n  \"new_ambiguous_term\": {\n    \"ambiguity_score\": 0.72,\n    \"category\": \"multi_domain\",\n    \"contexts\": [\"new_domain\", \"existing_domain\", \"general\"],\n    \"user_triggers\": [\"user phrase 1\", \"user phrase 2\", \"user phrase 3\"],\n    \"domains\": {\n      \"new_domain\": {\n        \"types\": [\"type1\", \"type2\"],\n        \"patterns\": [\"pattern1\", \"pattern2\"],\n        \"use_cases\": [\"use_case1\", \"use_case2\"]\n      },\n      \"existing_domain\": [\"meaning in existing domain\"],\n      \"general\": \"general meaning\"\n    },\n    \"clarification_needed\": [\n      \"Which domain applies? (new_domain, existing_domain, general)\",\n      \"What specific type? (type1, type2)\",\n      \"What's the use case?\"\n    ]\n  }\n}\n```\n\n#### 2. technical-mappings.json\n\n```json\n{\n  \"new_domain\": {\n    \"subcategory1\": {\n      \"Concept1\": {\n        \"purpose\": \"What this concept does\",\n        \"use_cases\": [\"use_case1\", \"use_case2\"],\n        \"pattern\": \"Typical usage pattern\",\n        \"signature\": \"Code signature or command syntax\",\n        \"components\": [\"component1\", \"component2\"],\n        \"tools\": [\"tool1\", \"tool2\"],\n        \"similar_to\": {\n          \"existing_domain\": \"equivalent concept\"\n        }\n      }\n    },\n    \"subcategory2\": {\n      \"Concept2\": {\n        \"purpose\": \"What this does\",\n        \"types\": {\n          \"type1\": \"Description of type1\",\n          \"type2\": \"Description of type2\"\n        },\n        \"workflow\": \"Step1 â†’ Step2 â†’ Step3\",\n        \"examples\": [\"example1\", \"example2\"]\n      }\n    }\n  },\n  \"cross_domain_equivalents\": {\n    \"shared_concept\": {\n      \"existing_domain\": \"Existing implementation\",\n      \"new_domain\": \"New domain implementation\",\n      \"general\": \"General term\"\n    }\n  }\n}\n```\n\n#### 3. ontology-graph.json\n\n```json\n{\n  \"new_domain\": {\n    \"CoreConcept\": {\n      \"type\": \"concrete_class\",\n      \"parent\": \"AbstractBase (if applicable)\",\n      \"children\": [\"SpecializedConcept1\", \"SpecializedConcept2\"],\n      \"purpose\": \"What this represents\",\n      \"relationships\": {\n        \"IS_A\": \"parent class or abstraction\",\n        \"HAS_A\": [\"attribute1\", \"attribute2\"],\n        \"CAN_DO\": [\"operation1\", \"operation2\"],\n        \"PROVIDES\": [\"service1\", \"service2\"],\n        \"INTEGRATES_WITH\": {\n          \"existing_domain\": \"integration point\"\n        }\n      },\n      \"cross_domain_equivalent\": {\n        \"existing_domain\": \"Equivalent concept\",\n        \"general\": \"General term\"\n      }\n    }\n  },\n  \"conceptual_relationships\": {\n    \"new_abstract_concept\": {\n      \"abstract_concept\": \"High-level description\",\n      \"patterns\": {\n        \"pattern1\": \"Pattern description\",\n        \"pattern2\": \"Pattern description\"\n      },\n      \"implementations\": {\n        \"existing_domain\": \"Existing implementation\",\n        \"new_domain\": \"New domain implementation\",\n        \"general\": \"General approach\"\n      }\n    }\n  },\n  \"ambiguity_resolution_graph\": {\n    \"ambiguous_term\": {\n      \"disambiguation_questions\": [\n        \"Question 1?\",\n        \"Question 2?\",\n        \"Question 3?\"\n      ],\n      \"decision_tree\": {\n        \"dimension1\": {\n          \"option1\": \"new_domain interpretation\",\n          \"option2\": \"existing_domain interpretation\"\n        }\n      }\n    }\n  }\n}\n```\n\n---\n\n## Domain Classification Decision Trees\n\n### Decision Tree 1: Is This a New Domain or Extension?\n\n```\nUser mentions new technology/framework\nâ”‚\nâ”œâ”€ Does this framework exist in knowledge base?\nâ”‚  â”œâ”€ YES â†’ EXTEND existing domain\nâ”‚  â””â”€ NO â†’ Check if it's a subdomain or truly new\nâ”‚     â”‚\nâ”‚     â”œâ”€ Is it a variant of existing domain?\nâ”‚     â”‚  â”œâ”€ YES (e.g., FastAPI variant of general Python web)\nâ”‚     â”‚  â”‚   â†’ CREATE new domain section (FastAPI is distinct enough)\nâ”‚     â”‚  â””â”€ NO â†’ CREATE new domain section\nâ”‚     â”‚\nâ”‚     â””â”€ Does it introduce fundamentally new concepts?\nâ”‚        â”œâ”€ YES â†’ CREATE new domain with new conceptual relationships\nâ”‚        â””â”€ NO â†’ EXTEND closest existing domain\n```\n\n**Example:**\n- **FastAPI** â†’ New domain (distinct from Flask/Django, introduces dependency injection patterns)\n- **Strawberry GraphQL** â†’ Extend existing GraphQL/API domain (not fundamentally different)\n- **UTCP** â†’ New domain (introduces universal tool calling abstraction)\n\n### Decision Tree 2: Ambiguity Score Assignment\n\n```\nTerm appears in conversation\nâ”‚\nâ”œâ”€ In how many domains does it have different meanings?\nâ”‚  â”œâ”€ 1 domain â†’ NOT ambiguous, don't add\nâ”‚  â”œâ”€ 2-3 domains â†’ Calculate: (2-3 Ã— context_dependency) / 10\nâ”‚  â””â”€ 4+ domains â†’ Calculate: (4+ Ã— context_dependency) / 10, cap at 1.0\nâ”‚\nâ””â”€ How much context is needed to disambiguate?\n   â”œâ”€ Keyword alone is clear â†’ context_dependency = 1\n   â”œâ”€ Framework name provides clarity â†’ context_dependency = 3\n   â”œâ”€ Requires understanding of operation â†’ context_dependency = 4\n   â””â”€ Needs deep conversation context â†’ context_dependency = 5\n```\n\n**Example Calculations:**\n- \"endpoint\" (FastAPI vs MCP vs network) â†’ 3 domains Ã— 3 context = 0.9\n- \"validation\" (FastAPI Pydantic vs input vs business logic) â†’ 3 domains Ã— 4 context = 1.2 â†’ cap at 1.0\n- \"server\" (MCP server vs web server vs general) â†’ 3 domains Ã— 2 context = 0.6\n\n### Decision Tree 3: Category Assignment\n\n```\nAmbiguous term identified\nâ”‚\nâ”œâ”€ Is it unique to ONE framework with specific implementation?\nâ”‚  â””â”€ YES â†’ category: \"domain_specific\"\nâ”‚\nâ”œâ”€ Does it appear in MULTIPLE domains with different meanings?\nâ”‚  â””â”€ YES â†’ category: \"multi_domain\"\nâ”‚\nâ”œâ”€ Is it a meta-question pattern (e.g., \"making sense?\", \"is this right?\")?\nâ”‚  â””â”€ YES â†’ category: \"meta_question\"\nâ”‚\nâ”œâ”€ Is it a vague action verb (e.g., \"make it X\", \"fix it\")?\nâ”‚  â””â”€ YES â†’ category: \"vague_action_verb\"\nâ”‚\nâ”œâ”€ Is it about system infrastructure (server, container, cluster)?\nâ”‚  â””â”€ YES â†’ category: \"infrastructure_type\"\nâ”‚\nâ”œâ”€ Is it about data/knowledge organization (memory, graph, database)?\nâ”‚  â””â”€ YES â†’ category: \"knowledge_structure\"\nâ”‚\nâ”œâ”€ Is it a version control operation (merge, rebase, commit, branch)?\nâ”‚  â””â”€ YES â†’ category: \"git_operation\"\nâ”‚\nâ””â”€ Is it an SRE/observability concept (logs, metrics, SLI, observability)?\n   â””â”€ YES â†’ category: \"sre_concept\"\n```\n\n### Decision Tree 4: Subcategory Organization\n\n```\nCreating technical mappings for new domain\nâ”‚\nâ”œâ”€ What type of domain is this?\nâ”‚  â”‚\nâ”‚  â”œâ”€ AI Framework (Autogen, Langroid)\nâ”‚  â”‚  â†’ Subcategories: agent_creation, communication, tool_integration, orchestration\nâ”‚  â”‚\nâ”‚  â”œâ”€ Protocol (MCP, UTCP)\nâ”‚  â”‚  â†’ Subcategories: server_types, components, integration_patterns, protocol_components\nâ”‚  â”‚\nâ”‚  â”œâ”€ Web Framework (FastAPI)\nâ”‚  â”‚  â†’ Subcategories: routing, dependency_injection, validation, async_operations\nâ”‚  â”‚\nâ”‚  â”œâ”€ Version Control (Git, Gitflow)\nâ”‚  â”‚  â†’ Subcategories: core_operations, workflows, collaboration, branching_patterns\nâ”‚  â”‚\nâ”‚  â”œâ”€ SRE/Observability\nâ”‚  â”‚  â†’ Subcategories: observability, reliability_patterns, incident_management\nâ”‚  â”‚\nâ”‚  â””â”€ Memory/Knowledge Systems\nâ”‚     â†’ Subcategories: graph_types, storage_patterns, retrieval_patterns\n```\n\n---\n\n## Validation Checklist\n\nBefore finalizing domain addition, verify:\n\n### Phase 1: Domain Analysis\n- [ ] Framework/technology clearly identified\n- [ ] 20-30 key terms inventoried\n- [ ] Common user phrases documented\n- [ ] Cross-domain overlaps mapped\n\n### Phase 2: Ambiguous Terms\n- [ ] All ambiguous terms identified (10-20 per domain typical)\n- [ ] Ambiguity scores calculated correctly (0.0-1.0)\n- [ ] Categories assigned appropriately\n- [ ] User triggers documented (3-5 per term)\n- [ ] Domain-specific meanings detailed\n- [ ] Clarification questions formulated (2-4 per term)\n- [ ] Entries added to `ambiguous-terms.json`\n\n### Phase 3: Technical Mappings\n- [ ] Domain section created with 3-6 subcategories\n- [ ] Each subcategory has 3-10 concepts\n- [ ] All concepts have: purpose, use_cases, pattern/signature\n- [ ] Cross-domain equivalents updated\n- [ ] Similar concepts from other domains referenced\n- [ ] Entries added to `technical-mappings.json`\n\n### Phase 4: Ontology Graph\n- [ ] 5-10 major domain concepts modeled\n- [ ] Type classifications correct (class, pattern, component, etc.)\n- [ ] Relationships comprehensive (IS_A, HAS_A, CAN_DO, etc.)\n- [ ] Cross-domain equivalents mapped\n- [ ] At least 1 new conceptual relationship added (if applicable)\n- [ ] Existing conceptual relationships updated\n- [ ] Entries added to `ontology-graph.json`\n\n### Phase 5: Ambiguity Resolution\n- [ ] Decision trees created for 3-5 key ambiguous terms\n- [ ] 3-5 disambiguation questions per tree\n- [ ] All domain variants covered in decision trees\n- [ ] Outcomes are concrete and actionable\n- [ ] Existing decision trees updated with new domain\n- [ ] Entries added to `ambiguity_resolution_graph` in `ontology-graph.json`\n\n### Cross-Cutting Validation\n- [ ] All JSON files are valid (no syntax errors)\n- [ ] Cross-references between files are consistent\n- [ ] Domain name used consistently across all files\n- [ ] No duplicate entries\n- [ ] Formatting follows existing patterns\n\n---\n\n## Example Workflow: Adding \"Kubernetes\" Domain\n\n### Step 1: Domain Analysis\n\n```markdown\n**Framework:** Kubernetes (container orchestration)\n\n**Core Abstractions:**\n- Pod (smallest deployable unit)\n- Service (network abstraction)\n- Deployment (declarative updates)\n- ConfigMap/Secret (configuration management)\n- Namespace (resource isolation)\n\n**Key Terms (partial list):**\n- \"pod\" (ambiguous: Kubernetes pod vs podcast vs seed pod)\n- \"service\" (ambiguous: Kubernetes Service vs general service vs MCP server)\n- \"deployment\" (ambiguous: Kubernetes Deployment vs general deployment vs software release)\n- \"container\" (ambiguous: Kubernetes container vs Docker container vs general containerization)\n- \"namespace\" (ambiguous: Kubernetes namespace vs programming namespace vs DNS namespace)\n\n**User Triggers:**\n- \"create a pod\"\n- \"deploy to kubernetes\"\n- \"configure service\"\n- \"add namespace\"\n\n**Cross-Domain Overlaps:**\n- General Software Engineering: \"service\", \"container\", \"deployment\"\n- SRE: \"observability\", \"health checks\", \"scaling\"\n```\n\n### Step 2: Ambiguous Terms Identification\n\n```json\n{\n  \"pod\": {\n    \"ambiguity_score\": 0.72,\n    \"category\": \"multi_domain\",\n    \"contexts\": [\"kubernetes\", \"general\", \"biology\"],\n    \"user_triggers\": [\"create a pod\", \"deploy pod\", \"pod configuration\"],\n    \"domains\": {\n      \"kubernetes\": {\n        \"definition\": \"Smallest deployable unit in Kubernetes\",\n        \"contains\": [\"one or more containers\", \"shared storage\", \"network\"],\n        \"lifecycle\": \"created, running, succeeded, failed, unknown\"\n      },\n      \"general\": [\"podcast\", \"group of whales/dolphins\"],\n      \"biology\": \"seed pod\"\n    },\n    \"clarification_needed\": [\n      \"Are you working with Kubernetes?\",\n      \"Do you mean Kubernetes pod or something else?\",\n      \"What should the pod contain? (containers, config)\"\n    ]\n  },\n  \"service\": {\n    \"ambiguity_score\": 0.85,\n    \"category\": \"multi_domain\",\n    \"contexts\": [\"kubernetes\", \"mcp\", \"general\", \"microservices\"],\n    \"user_triggers\": [\"create service\", \"expose service\", \"service configuration\"],\n    \"domains\": {\n      \"kubernetes\": {\n        \"types\": [\"ClusterIP\", \"NodePort\", \"LoadBalancer\", \"ExternalName\"],\n        \"purpose\": \"Abstract way to expose pods as network service\"\n      },\n      \"mcp\": \"MCP server providing tools/resources\",\n      \"microservices\": \"Independent deployable service\",\n      \"general\": \"General software service or daemon\"\n    },\n    \"clarification_needed\": [\n      \"Kubernetes Service, MCP server, or general service?\",\n      \"What type of Kubernetes Service? (ClusterIP, NodePort, LoadBalancer)\",\n      \"What should the service expose?\"\n    ]\n  }\n}\n```\n\n### Step 3: Technical Mappings\n\n```json\n{\n  \"kubernetes\": {\n    \"workload_resources\": {\n      \"Pod\": {\n        \"purpose\": \"Smallest deployable unit in Kubernetes\",\n        \"use_cases\": [\"run single container\", \"run tightly coupled containers\", \"sidecar pattern\"],\n        \"pattern\": \"Declarative YAML definition â†’ kubectl apply â†’ scheduler assigns to node\",\n        \"signature\": \"apiVersion: v1, kind: Pod, metadata, spec\",\n        \"components\": [\"containers\", \"volumes\", \"init containers (optional)\"],\n        \"lifecycle_states\": [\"Pending\", \"Running\", \"Succeeded\", \"Failed\", \"Unknown\"]\n      },\n      \"Deployment\": {\n        \"purpose\": \"Declarative updates for Pods and ReplicaSets\",\n        \"use_cases\": [\"rolling updates\", \"rollbacks\", \"scaling\"],\n        \"pattern\": \"Desired state â†’ Deployment controller â†’ create/update Pods\",\n        \"signature\": \"apiVersion: apps/v1, kind: Deployment, spec.replicas, spec.template\",\n        \"features\": [\"rolling updates\", \"rollback\", \"pause/resume\", \"scaling\"]\n      }\n    },\n    \"networking\": {\n      \"Service\": {\n        \"purpose\": \"Abstract way to expose Pods as network service\",\n        \"types\": {\n          \"ClusterIP\": \"Internal cluster IP (default)\",\n          \"NodePort\": \"Expose on each Node's IP at static port\",\n          \"LoadBalancer\": \"Cloud provider load balancer\",\n          \"ExternalName\": \"Map to DNS name\"\n        },\n        \"pattern\": \"Label selector â†’ match Pods â†’ expose via service\",\n        \"signature\": \"apiVersion: v1, kind: Service, spec.type, spec.selector\"\n      }\n    }\n  },\n  \"cross_domain_equivalents\": {\n    \"container_unit\": {\n      \"kubernetes\": \"Pod (one or more containers)\",\n      \"docker\": \"Single container\",\n      \"general\": \"Containerization\"\n    },\n    \"service_abstraction\": {\n      \"kubernetes\": \"Kubernetes Service\",\n      \"mcp\": \"MCP server\",\n      \"microservices\": \"Microservice\",\n      \"general\": \"Network service\"\n    }\n  }\n}\n```\n\n### Step 4: Ontology Graph\n\n```json\n{\n  \"kubernetes\": {\n    \"Pod\": {\n      \"type\": \"workload_resource\",\n      \"purpose\": \"Smallest deployable unit\",\n      \"relationships\": {\n        \"IS_A\": \"Kubernetes resource\",\n        \"HAS_A\": [\"containers\", \"volumes\", \"metadata\"],\n        \"MANAGED_BY\": \"kubelet on node\",\n        \"EXPOSED_BY\": \"Service\",\n        \"CREATED_BY\": [\"Deployment\", \"StatefulSet\", \"DaemonSet\", \"Job\"],\n        \"INTEGRATES_WITH\": {\n          \"sre\": \"Pod metrics and logs for observability\"\n        }\n      },\n      \"cross_domain_equivalent\": {\n        \"docker\": \"Container\",\n        \"general\": \"Application instance\"\n      }\n    },\n    \"Service\": {\n      \"type\": \"networking_resource\",\n      \"purpose\": \"Expose Pods as network service\",\n      \"relationships\": {\n        \"IS_A\": \"Kubernetes resource\",\n        \"HAS_A\": [\"selector\", \"ports\", \"type\"],\n        \"EXPOSES\": \"Pods (via label selector)\",\n        \"TYPES\": [\"ClusterIP\", \"NodePort\", \"LoadBalancer\", \"ExternalName\"],\n        \"SIMILAR_TO\": {\n          \"mcp\": \"MCP server (both expose functionality)\",\n          \"microservices\": \"Service mesh entry point\"\n        }\n      },\n      \"cross_domain_equivalent\": {\n        \"mcp\": \"MCP server\",\n        \"general\": \"Network service abstraction\"\n      }\n    }\n  },\n  \"conceptual_relationships\": {\n    \"container_orchestration\": {\n      \"abstract_concept\": \"Automate deployment, scaling, and management of containers\",\n      \"patterns\": {\n        \"declarative\": \"Describe desired state, system ensures it\",\n        \"imperative\": \"Execute specific commands\",\n        \"self_healing\": \"Restart failed containers, reschedule\"\n      },\n      \"implementations\": {\n        \"kubernetes\": \"Declarative YAML, controllers maintain desired state\",\n        \"docker_swarm\": \"Docker native orchestration\",\n        \"general\": \"Container management platforms\"\n      },\n      \"enables\": {\n        \"scaling\": \"Horizontal pod autoscaling\",\n        \"high_availability\": \"Replica sets, health checks\",\n        \"rolling_updates\": \"Zero-downtime deployments\"\n      }\n    }\n  },\n  \"ambiguity_resolution_graph\": {\n    \"service\": {\n      \"disambiguation_questions\": [\n        \"Are you working with Kubernetes?\",\n        \"Kubernetes Service, MCP server, microservice, or general service?\",\n        \"What type of Kubernetes Service? (ClusterIP, NodePort, LoadBalancer)\"\n      ],\n      \"decision_tree\": {\n        \"platform\": {\n          \"kubernetes\": {\n            \"ClusterIP\": \"Internal cluster service\",\n            \"NodePort\": \"Expose on node port\",\n            \"LoadBalancer\": \"Cloud load balancer\",\n            \"ExternalName\": \"DNS CNAME mapping\"\n          },\n          \"mcp\": \"MCP server with tools/resources\",\n          \"microservices\": \"Independent deployable service\",\n          \"general\": \"Software service or daemon\"\n        }\n      }\n    },\n    \"pod\": {\n      \"disambiguation_questions\": [\n        \"Kubernetes pod or something else?\",\n        \"What containers should it run?\",\n        \"How should it be exposed?\"\n      ],\n      \"decision_tree\": {\n        \"context\": {\n          \"kubernetes\": \"Kubernetes Pod resource\",\n          \"general\": \"Podcast or group (non-technical)\",\n          \"biology\": \"Seed pod (non-technical)\"\n        }\n      }\n    }\n  }\n}\n```\n\n---\n\n## Best Practices\n\n1. **Start Small, Iterate**\n   - Add 10-15 ambiguous terms initially\n   - Expand based on actual user confusion patterns\n   - Don't try to be exhaustive on first pass\n\n2. **Focus on High-Impact Terms**\n   - Prioritize terms that cause frequent miscommunication\n   - Terms with ambiguity score > 0.7 are critical\n   - Terms with 3+ domain meanings need immediate disambiguation\n\n3. **Maintain Consistency**\n   - Use same naming conventions across all files\n   - Keep structure parallel to existing domains\n   - Follow JSON formatting of existing entries\n\n4. **Test Decision Trees**\n   - Mentally walk through user conversations\n   - Ensure decision trees cover common paths\n   - Validate that outcomes are specific enough to implement\n\n5. **Update Incrementally**\n   - Don't update all 3 files at once\n   - Phase 2 â†’ Phase 3 â†’ Phase 4 â†’ Phase 5 sequentially\n   - Validate each file after modification\n\n6. **Document Rationale**\n   - Leave comments (if JSON supports) or separate notes on why scores/categories were chosen\n   - Document unusual mappings or non-obvious cross-domain equivalents\n\n---\n\n## Common Pitfalls\n\n**Pitfall 1: Over-Ambitious First Pass**\n- **Problem:** Trying to add 50+ terms for a new domain in one go\n- **Solution:** Start with 10-15 highest-impact terms, expand iteratively\n\n**Pitfall 2: Ambiguity Score Inconsistency**\n- **Problem:** Different scoring logic for different terms\n- **Solution:** Always use formula: `(domains Ã— context_dependency) / 10`, cap at 1.0\n\n**Pitfall 3: Vague Decision Tree Outcomes**\n- **Problem:** Decision tree ends with \"use appropriate method\" (not actionable)\n- **Solution:** Outcomes must be concrete: \"ConversableAgent\", \"git merge --no-ff\", \"Pydantic model validation\"\n\n**Pitfall 4: Missing Cross-Domain Updates**\n- **Problem:** Adding new domain but not updating `cross_domain_equivalents` section\n- **Solution:** Always check if new concepts map to existing concepts in other domains\n\n**Pitfall 5: Incomplete Relationship Modeling**\n- **Problem:** Only modeling IS_A relationships, ignoring HAS_A, USES, PROVIDES\n- **Solution:** Use full relationship vocabulary to capture nuanced connections\n\n---\n\n## Conclusion\n\nAdding a new domain requires systematic analysis across 5 phases:\n1. **Domain Analysis** - Understand the domain deeply\n2. **Ambiguous Terms Identification** - Find multi-domain conflicts\n3. **Technical Mappings Creation** - Document domain-specific translations\n4. **Ontology Graph Development** - Model conceptual relationships\n5. **Ambiguity Resolution** - Build decision trees for disambiguation\n\nFollow the templates, validate at each phase, and iterate based on real user confusion patterns.\n\n**Key Success Metric:** Reduction in user meta-questions (\"making sense?\") and clarification rounds after domain addition.\n",
        "plugins/semantic-linguist/skills/semantic-translation/SKILL.md": "---\nname: Semantic Translation\ndescription: This skill should be used when the user uses ambiguous terminology like \"make it talk\", \"we need an api\", \"make it portable\", \"check for gaps\", asks meta-questions like \"am I making sense?\", \"does this make sense?\", mentions being a \"non-technical user\", uses vague action verbs (\"make it work\", \"do the thing\"), mixes domain languages, uses invented terms, or when detecting semantic drift between human natural language and technical precision. Provides semantic translation, disambiguation, and domain knowledge mapping across Autogen, Langroid, MCP (Model Context Protocol), UTCP (Universal Tool Calling Protocol), FastAPI, Git/Gitflow, SRE (Site Reliability Engineering), and Memory Graphs domains. Bridges the gap between user intent and technical specificity through ontological translation.\nversion: 1.0.0\n---\n\n# Semantic Translation & Ontological Mapping\n\n## Purpose\n\nPrevent miscommunication, assumptions, and hallucinations by translating ambiguous user terminology into precise technical concepts across multiple domains. Act as a semantic bridge between human natural language and technical specificity, mapping concepts across Autogen, Langroid, MCP, UTCP, FastAPI, Git/Gitflow, SRE, and Memory Graphs ecosystems.\n\n## Supported Domains\n\nThis skill provides semantic translation and disambiguation across 8 technical domains:\n\n1. **Autogen** - Multi-agent orchestration (ConversableAgent, AssistantAgent, UserProxyAgent, GroupChat)\n2. **Langroid** - Agent framework (ChatAgent, ToolAgent, Task orchestration)\n3. **MCP (Model Context Protocol)** - Server types (SSE, stdio, HTTP, WebSocket), tools, resources, prompts, sampling\n4. **UTCP (Universal Tool Calling Protocol)** - Framework-agnostic tool schemas, adapters, universal tool definitions\n5. **FastAPI** - Python web framework (path operations, dependency injection with Depends(), Pydantic models, APIRouter)\n6. **Git/Gitflow** - Version control workflows (feature/release/hotfix branches, merge strategies, collaboration patterns)\n7. **SRE (Site Reliability Engineering)** - Observability pillars (logs/metrics/traces), SLO/SLI/SLA, incident management\n8. **Memory Graphs** - Knowledge graph structures (entities, relationships, embeddings, episodic/semantic/procedural memory)\n\n## The Core Problem\n\nUsers frequently encounter the \"abyss\" between natural language intent and technical precision:\n\n- **Vague terminology**: \"make it talk\", \"we need an api\", \"make it work\"\n- **Unclear scope**: \"check for gaps\", \"make it portable\"\n- **Meta-questions**: \"am I making sense?\", \"does this make sense?\"\n- **Domain confusion**: Mixing business and technical terminology\n- **Invented terms**: User-created words not in domain vocabulary\n\nWithout intervention, these ambiguities lead to:\n- AI assumptions and hallucinations\n- Misaligned implementations\n- Wasted development time\n- Project failures from misunderstood requirements\n\n## When to Use This Skill\n\nTrigger semantic validation when detecting:\n\n1. **Ambiguous Action Verbs**\n   - \"make it [X]\" â†’ Multiple technical interpretations possible\n   - \"do the thing\" â†’ No clear referent\n   - \"fix it\" â†’ Unclear what or how\n\n2. **Meta-Questions (User Seeking Validation)**\n   - \"am I making sense?\" â†’ User uncertain about their explanation\n   - \"does this make sense?\" â†’ User seeking confirmation\n   - \"is this right?\" â†’ User wants validation\n   - \"non-technical user\" â†’ User self-identifies as potentially ambiguous\n\n3. **Domain-Crossing Language**\n   - Mixing business and technical terms\n   - Unclear which framework/library intended\n   - Generic terms with multiple technical meanings\n\n4. **Unclear References**\n   - \"that\", \"the previous thing\", \"like before\"\n   - References without clear antecedents\n\n5. **Scope Ambiguity**\n   - \"portable\" â†’ Docker? Cross-platform? Vendoring? Executable?\n   - \"api\" â†’ HTTP server? API client? API design? Internal interface?\n   - \"gaps\" â†’ Code coverage? Documentation? Features? Security?\n\n## Core Workflow\n\n### Step 1: Detect Ambiguity\n\nAnalyze the user's message for ambiguity signals. Use pattern matching from `scripts/detect-ambiguity.py` or manual analysis.\n\n**High-confidence triggers (always validate):**\n- Explicit meta-questions: \"am I making sense?\"\n- Vague action verbs with unclear objects: \"make it talk\"\n- Domain confusion: mixing incompatible terminology\n- User self-identification: \"I'm a non-technical user\"\n\n**Moderate-confidence triggers (validate if >80% confidence):**\n- Generic technical terms: \"api\", \"agent\", \"portable\"\n- Unclear scope: \"check for gaps\", \"add validation\"\n- Invented terminology: words not in domain vocabulary\n\n### Step 2: Query Knowledge Sources (In Order)\n\nQuery knowledge sources in this specific order for efficiency:\n\n**1. Static Domain Knowledge (First - Fastest)**\n- Query `knowledge/ambiguous-terms.json` for known ambiguous phrases\n- Check `knowledge/technical-mappings.json` for domain-specific translations\n- Review `knowledge/ontology-graph.json` for conceptual relationships\n\n**2. External Documentation (Second - Authoritative)**\n- Query official documentation (Autogen, Langroid, etc.) via available tools\n- Use WebFetch, context7, or deepwiki MCP for current API references\n- Validate against authoritative sources when static knowledge insufficient\n\n**3. Codebase Validation (Third - Context-Specific)**\n- Use LSP to query symbol definitions in user's project\n- Use Grep to search for actual usage patterns\n- Identify project-specific terminology and conventions\n\n### Step 3: Translate Ambiguous â†’ Precise\n\nMap ambiguous terminology to precise technical concepts using domain knowledge.\n\n**Translation process:**\n1. Identify all possible technical interpretations\n2. Rank by confidence score (from knowledge files)\n3. Consider user's context (recent conversation, project domain)\n4. Present options if multiple interpretations viable\n\n**Example translation:**\n```\nAmbiguous: \"make it talk\"\nDomain: Autogen\nPossible translations:\n- ConversableAgent.send() (confidence: 0.8, context: single message)\n- register ConversableAgent (confidence: 0.7, context: enable conversation)\n- GroupChat setup (confidence: 0.5, context: multi-agent conversation)\n```\n\n### Step 4: Engage Conversational Clarification\n\n**Never assume.** Always verify understanding with the user.\n\nPresent options conversationally:\n```\nI notice \"[ambiguous term]\" could mean different things:\n\n1. [Precise interpretation 1] - [Brief context]\n2. [Precise interpretation 2] - [Brief context]\n3. [Precise interpretation 3] - [Brief context]\n\n[Ask clarifying question based on context]\n```\n\n**Tone guidelines:**\n- Conversational, not clinical\n- Helpful, not pedantic\n- Transparent about uncertainty\n- Frame as collaboration, not correction\n\n**Wait for confirmation** before proceeding with implementation.\n\n## Decision Trees\n\n### Primary Decision Tree\n\n```\nUser message received\nâ”œâ”€â”€ Contains meta-question? (\"am I making sense?\")\nâ”‚   â”œâ”€â”€ Yes â†’ HIGH confidence, validate immediately\nâ”‚   â””â”€â”€ No â†’ Continue analysis\nâ”œâ”€â”€ Contains ambiguous action verb? (\"make it talk\")\nâ”‚   â”œâ”€â”€ Yes â†’ Check domain context\nâ”‚   â”‚   â”œâ”€â”€ Clear domain â†’ Query knowledge, translate\nâ”‚   â”‚   â””â”€â”€ Unclear domain â†’ Ask which framework/library\nâ”‚   â””â”€â”€ No â†’ Continue analysis\nâ”œâ”€â”€ Contains vague scope? (\"check for gaps\", \"make it portable\")\nâ”‚   â”œâ”€â”€ Yes â†’ Query knowledge for common interpretations\nâ”‚   â”‚   â”œâ”€â”€ Multiple viable â†’ Present options\nâ”‚   â”‚   â””â”€â”€ One clear match â†’ Confirm with user\nâ”‚   â””â”€â”€ No â†’ Continue analysis\nâ”œâ”€â”€ Contains domain-crossing language?\nâ”‚   â”œâ”€â”€ Yes â†’ Identify conflicting domains, ask clarification\nâ”‚   â””â”€â”€ No â†’ Proceed normally (low ambiguity)\n```\n\n### Clarification Decision Tree\n\n```\nAmbiguity detected\nâ”œâ”€â”€ Confidence score > 80%?\nâ”‚   â”œâ”€â”€ Yes â†’ Trigger validation\nâ”‚   â””â”€â”€ No â†’ Monitor, don't interrupt\nâ”œâ”€â”€ Query knowledge sources (static â†’ external â†’ codebase)\nâ”œâ”€â”€ Translation mappings found?\nâ”‚   â”œâ”€â”€ Yes, single mapping â†’ Confirm with user\nâ”‚   â”œâ”€â”€ Yes, multiple mappings â†’ Present options\nâ”‚   â””â”€â”€ No mappings found â†’ Ask open-ended clarification\nâ””â”€â”€ User confirms â†’ Proceed with precise terminology\n```\n\n## Domain Quick Reference\n\nKey ambiguous terms across all 8 supported domains with precise translations:\n\n### Autogen Domain\n\n**\"make it talk\"** â†’ ConversableAgent.send() (single message) vs initiate_chat() (conversation) vs GroupChat setup (multi-agent)\n**\"agent\"** â†’ ConversableAgent (base) vs AssistantAgent (LLM-powered) vs UserProxyAgent (human proxy)\n\n### Langroid Domain\n\n**\"agent\"** â†’ ChatAgent (conversation) vs ToolAgent (function-calling)\n**\"task\"** â†’ Langroid Task object (orchestration) vs general task concept\n\n### MCP Domain\n\n**\"mcp server\"** â†’ SSE server (web-based) vs stdio server (process-based) vs HTTP server vs WebSocket server\n**\"resource\"** â†’ MCP resource (data/content exposed by server) vs system resource (CPU/memory)\n**\"prompt\"** â†’ MCP prompt template (structured prompts) vs LLM prompt (text input)\n\n### UTCP Domain\n\n**\"tool calling\"** â†’ UTCP universal calling (framework-agnostic) vs framework-specific (OpenAI tools, Anthropic tools)\n**\"tool schema\"** â†’ UTCP universal schema vs framework-specific schema\n\n### FastAPI Domain\n\n**\"dependency\"** â†’ FastAPI Depends() (dependency injection) vs pip dependency (package) vs architectural dependency (service)\n**\"endpoint\"** â†’ Path operation decorator (@app.get) vs external API endpoint\n**\"model\"** â†’ Pydantic model (validation) vs database model (ORM) vs ML model\n\n### Git/Gitflow Domain\n\n**\"merge\"** â†’ Merge commit (preserves history) vs squash merge (single commit) vs rebase (linear history)\n**\"branch\"** â†’ Gitflow branch type (feature/release/hotfix/develop/main) vs general branch name\n\n### SRE Domain\n\n**\"observability\"** â†’ Logs (events) vs metrics (measurements) vs traces (request paths) - three pillars\n**\"sli\"** â†’ Availability SLI (uptime %) vs latency SLI (response time) vs error rate SLI (failure %)\n**\"incident\"** â†’ SEV-1 incident (critical outage) vs alert (automated notification) vs degradation (partial failure)\n\n### Memory Graphs Domain\n\n**\"memory\"** â†’ Knowledge graph (structured entities/relationships) vs vector memory (embeddings) vs episodic memory (temporal context) vs system RAM\n**\"retrieval\"** â†’ Semantic search (embedding similarity) vs graph traversal (relationship following) vs hybrid approach\n\n### Meta-Questions (Cross-Domain)\n\n**\"am I making sense?\"** â†’ Trigger explicit semantic validation across all domains\n- Summarize recent conversation\n- Identify detected ambiguities\n- Ask domain-specific clarifying questions\n- Confirm shared understanding with precise terminology\n\n## Domain-Specific Validation\n\n### Autogen Domain\n\n**Key concepts to validate:**\n- Agent types: ConversableAgent, AssistantAgent, UserProxyAgent\n- Multi-agent: GroupChat, GroupChatManager\n- Communication: send(), register_reply(), initiate_chat()\n- Tools: register_for_execution(), register_for_llm()\n\n**Common ambiguities:**\n- \"agent\" â†’ Which type? (ConversableAgent vs AssistantAgent vs UserProxyAgent)\n- \"group chat\" â†’ GroupChat object vs general multi-agent conversation\n- \"tools\" â†’ Function calling vs external tool integration\n\n### Langroid Domain\n\n**Key concepts to validate:**\n- Agent types: ChatAgent, ToolAgent\n- Tasks: Task orchestration and delegation\n- Tools: ToolMessage, tool decorators\n- Multi-agent: Agent collaboration patterns\n\n**Common ambiguities:**\n- \"agent\" â†’ ChatAgent vs ToolAgent\n- \"task\" â†’ Langroid Task object vs general task concept\n- \"tools\" â†’ Langroid tool system vs general utilities\n\n### MCP Domain\n\n**Key concepts to validate:**\n- Server types: SSE (Server-Sent Events), stdio, HTTP, WebSocket\n- Components: tools, resources, prompts, sampling\n- Integration: local server, remote server, managed server\n\n**Common ambiguities:**\n- \"mcp server\" â†’ Which type? (SSE vs stdio vs HTTP vs WebSocket)\n- \"resource\" â†’ MCP resource (data/content) vs system resource\n- \"prompt\" â†’ MCP prompt template vs LLM prompt\n- \"tool\" â†’ MCP tool definition vs general function\n\n### UTCP Domain\n\n**Key concepts to validate:**\n- Universal tool schemas: Framework-agnostic tool definitions\n- Adapters: UTCP framework adapters for different AI frameworks\n- Tool calling patterns: Universal invocation vs framework-specific\n\n**Common ambiguities:**\n- \"tool calling\" â†’ UTCP universal calling vs framework-specific calling (OpenAI tools, Anthropic tools)\n- \"tool schema\" â†’ UTCP universal schema vs framework-specific schema\n- \"adapter\" â†’ UTCP framework adapter vs general adapter pattern\n- \"invocation\" â†’ UTCP tool invocation vs direct function call\n\n### FastAPI Domain\n\n**Key concepts to validate:**\n- Path operations: Endpoint decorators (@app.get, @app.post)\n- Dependency injection: Depends() mechanism\n- Pydantic models: Request/response validation\n- APIRouter: Endpoint organization and grouping\n\n**Common ambiguities:**\n- \"dependency\" â†’ FastAPI Depends() vs pip package dependency vs architectural dependency\n- \"endpoint\" â†’ Path operation decorator vs external API endpoint\n- \"model\" â†’ Pydantic model vs database model vs ML model\n- \"route\" â†’ Path operation vs APIRouter vs URL routing\n- \"validation\" â†’ Pydantic automatic validation vs business logic validation\n\n### Git/Gitflow Domain\n\n**Key concepts to validate:**\n- Branch types: feature/, release/, hotfix/, develop, main\n- Merge strategies: merge commit, squash merge, rebase\n- Workflows: Gitflow vs GitHub flow vs GitLab flow\n- Operations: commit, push, pull, merge, rebase\n\n**Common ambiguities:**\n- \"merge\" â†’ Merge commit vs squash merge vs rebase\n- \"branch\" â†’ Gitflow branch type (feature/release/hotfix) vs general branch\n- \"rebase\" â†’ Interactive rebase vs standard rebase vs rebase merge\n- \"commit\" â†’ Commit operation vs commit message vs specific commit SHA\n- \"workflow\" â†’ Gitflow workflow vs GitHub flow vs custom workflow\n\n### SRE Domain\n\n**Key concepts to validate:**\n- Observability pillars: logs, metrics, traces\n- SLI/SLO/SLA: Service Level Indicator/Objective/Agreement\n- Incident management: SEV levels, on-call, postmortems\n- Reliability patterns: error budgets, canary deployments, circuit breakers\n\n**Common ambiguities:**\n- \"observability\" â†’ Which pillar? (logs vs metrics vs traces)\n- \"sli\" â†’ Which metric? (availability vs latency vs throughput vs error rate)\n- \"incident\" â†’ SEV-1 incident vs alert vs outage vs degradation\n- \"monitoring\" â†’ Monitoring (passive data collection) vs observability (active understanding)\n- \"on-call\" â†’ On-call rotation vs on-call engineer vs escalation policy\n\n### Memory Graphs Domain\n\n**Key concepts to validate:**\n- Graph types: knowledge graph, memory graph, dependency graph\n- Memory types: episodic, semantic, procedural\n- Components: entities (nodes), relationships (edges), embeddings\n- Operations: retrieval (semantic search, graph traversal), storage\n\n**Common ambiguities:**\n- \"memory\" â†’ Knowledge graph vs vector memory vs episodic memory vs system RAM\n- \"graph\" â†’ Knowledge graph vs visualization graph vs dependency graph\n- \"embedding\" â†’ Vector embedding for semantic search vs embedding layer in neural networks\n- \"retrieval\" â†’ Semantic search via embeddings vs graph traversal vs hybrid\n- \"node\" â†’ Memory node (entity) vs graph node vs system node\n\n### Generic/Framework-Agnostic\n\n**When domain unclear:**\n1. Don't assume framework\n2. Ask explicitly: \"Which framework/library are you using?\"\n3. Once identified, load domain-specific knowledge\n4. Translate with domain context\n\n## Confidence Scoring\n\nUse confidence scores to determine intervention:\n\n**High confidence (>80%):** Always validate\n- Explicit meta-questions\n- Known highly ambiguous terms (from knowledge files)\n- Domain confusion detected\n- User self-identifies ambiguity\n\n**Medium confidence (50-80%):** Validate if multiple interpretations\n- Generic technical terms with context clues\n- Unclear scope with partial context\n- Vague references with some antecedents\n\n**Low confidence (<50%):** Monitor but don't interrupt\n- Technical terms with clear context\n- Conventional usage patterns\n- Recent clarification provided\n\n## Integration with Knowledge Files\n\n### ambiguous-terms.json\n\nContains user phrases mapped to ambiguity scores and contexts.\n\n**Query pattern:**\n```python\nterm = extract_key_phrase(user_message)\nentry = load_json(\"knowledge/ambiguous-terms.json\").get(term)\nif entry and entry[\"ambiguity_score\"] > 0.8:\n    trigger_validation(term, entry[\"contexts\"])\n```\n\n### technical-mappings.json\n\nContains precise technical translations organized by domain.\n\n**Query pattern:**\n```python\nmappings = load_json(\"knowledge/technical-mappings.json\")\ndomain_mappings = mappings.get(domain, {})\ntranslations = domain_mappings.get(ambiguous_term, [])\npresent_options(translations)\n```\n\n### ontology-graph.json\n\nContains conceptual relationships between terms across domains.\n\n**Query pattern:**\n```python\ngraph = load_json(\"knowledge/ontology-graph.json\")\nrelated_concepts = graph.get(concept, {}).get(\"related\", [])\ncross_domain = graph.get(concept, {}).get(\"cross_domain_equivalents\", {})\n```\n\n## Working with Scripts\n\n### scripts/detect-ambiguity.py\n\nPattern matching utility for ambiguity detection.\n\n**Usage:**\n```bash\npython scripts/detect-ambiguity.py --message \"user message here\"\n# Returns: confidence score, detected patterns, suggested validation\n```\n\n**When to use:**\n- Programmatic ambiguity detection in hooks\n- Batch analysis of conversation history\n- Testing new ambiguity patterns\n\n### scripts/domain-mapper.py\n\nTerm translation utility using knowledge files.\n\n**Usage:**\n```bash\npython scripts/domain-mapper.py --term \"make it talk\" --domain autogen\n# Returns: ranked translations with confidence scores\n```\n\n**When to use:**\n- Translate specific terms\n- Generate clarification options\n- Validate translations against knowledge\n\n### scripts/knowledge-query.py\n\nUnified interface to all knowledge sources.\n\n**Usage:**\n```bash\npython scripts/knowledge-query.py --term \"api\" --sources static,external,codebase\n# Returns: results from each source in order\n```\n\n**When to use:**\n- Query all knowledge sources in one call\n- Implement the three-tier query workflow\n- Aggregate results from multiple sources\n\n## Additional Resources\n\n### Reference Files\n\nFor detailed domain knowledge and advanced patterns:\n- **`references/cognitive-framework.md`** - Complete AGENTS.md framework adapted for Claude Code\n- **`references/decision-trees.md`** - Detailed decision trees and flowcharts\n- **`references/domain-ontologies.md`** - Comprehensive domain knowledge graphs (Autogen, Langroid)\n- **`references/translation-patterns.md`** - Extensive ambiguousâ†’precise mappings\n\n### Example Files\n\nWorking examples of semantic validation:\n- **`examples/autogen-mappings.md`** - Autogen-specific ambiguity resolutions\n- **`examples/langroid-mappings.md`** - Langroid-specific examples\n- **`examples/common-ambiguities.md`** - Cross-domain frequent patterns\n\n### Knowledge Files\n\nDomain knowledge JSON files:\n- **`knowledge/ambiguous-terms.json`** - User phrases with ambiguity scores\n- **`knowledge/technical-mappings.json`** - Domain-specific translations\n- **`knowledge/ontology-graph.json`** - Conceptual relationships\n\n## Best Practices\n\n### Always Verify, Never Assume\n\n\"Never ASSUME - it makes an ass out of u and me.\"\n\n- Present options when ambiguity detected\n- Ask clarifying questions before proceeding\n- Confirm understanding with user\n- Never proceed with interpretation alone\n\n### Maintain Conversational Tone\n\n- Avoid clinical language: \"Ambiguity detected at 87% confidence\"\n- Use helpful framing: \"I notice [term] could mean a few different things...\"\n- Frame as collaboration: \"Let me make sure I understand...\"\n- Be transparent about uncertainty\n\n### Respect User Triggers\n\nUsers have different patterns for expressing uncertainty. Learn from settings:\n- Default triggers: \"am I making sense?\", \"does this make sense?\"\n- Custom triggers from `.claude/semantic-linguist.local.md`\n- Adapt to user's communication style\n\n### Progressive Validation\n\nNot every message needs validation:\n- Start with high-confidence triggers only\n- Expand to moderate-confidence as needed\n- Don't interrupt clear, unambiguous requests\n- Balance helpfulness with intrusiveness\n\n## Configuration\n\nUsers can customize via `.claude/semantic-linguist.local.md`:\n\n- **Sensitivity**: low, medium (default), high\n- **Interaction style**: explicit (default), guided, silent\n- **User trigger phrases**: Custom patterns to trigger validation\n- **Custom terminology mappings**: Project-specific translations\n- **Enabled domains**: Which domains to validate against\n\nRespect user configuration when determining whether to trigger validation.\n\n---\n\n**Core principle**: Bridge the gap between human natural language and AI technical precision through systematic semantic validation and conversational clarification.\n",
        "plugins/semantic-linguist/skills/semantic-translation/examples/autogen-mappings.md": "# Autogen-Specific Ambiguity Resolutions\n\nReal-world examples of ambiguous user requests mapped to precise Autogen implementations.\n\n## Example 1: \"make it talk\"\n\n### User Request\n```\n\"I want to make an agent that can talk to other agents\"\n```\n\n### Ambiguity Detected\n- \"make\" - vague action verb\n- \"talk to other agents\" - unclear communication pattern\n\n### Semantic Validation Process\n\n**Step 1: Domain Identified**\n- User context: Working with Autogen (confirmed or inferred)\n\n**Step 2: Query Knowledge**\n- Static: \"talk\" maps to send(), initiate_chat(), GroupChat in Autogen\n- Ontology: Communication patterns in Autogen\n\n**Step 3: Present Options**\n```\nIn Autogen, \"make an agent talk to other agents\" could mean:\n\n1. **One-to-one messaging** - ConversableAgent with send()\n   - Agent sends individual messages to specific recipients\n   - Example: agent1.send(\"Hello\", agent2)\n   - Use case: Direct agent-to-agent communication\n\n2. **Initiated conversation** - initiate_chat()\n   - Start a back-and-forth conversation\n   - Example: agent1.initiate_chat(agent2, message=\"Let's discuss...\")\n   - Use case: Extended conversation between two agents\n\n3. **Group conversation** - GroupChat with GroupChatManager\n   - Multiple agents participate in discussion\n   - Example:\n     ```python\n     groupchat = GroupChat(\n         agents=[agent1, agent2, agent3],\n         messages=[],\n         max_round=10\n     )\n     manager = GroupChatManager(groupchat=groupchat)\n     manager.initiate_chat(groupchat)\n     ```\n   - Use case: Multi-party discussion, brainstorming, debate\n\nWhich communication pattern do you need?\"\n```\n\n**Step 4: User Clarifies**\n```\nUser: \"I want multiple agents to discuss a problem together\"\n```\n\n**Step 5: Validated Translation**\n```\n\"Perfect! You need GroupChat for multi-agent discussions. Here's how:\n\n```python\nfrom autogen import ConversableAgent, GroupChat, GroupChatManager\n\n# Create agents\nanalyst = ConversableAgent(name=\"Analyst\", system_message=\"You analyze problems...\")\ncritic = ConversableAgent(name=\"Critic\", system_message=\"You critique solutions...\")\nimplementer = ConversableAgent(name=\"Implementer\", system_message=\"You implement solutions...\")\n\n# Create group chat\ngroupchat = GroupChat(\n    agents=[analyst, critic, implementer],\n    messages=[],\n    max_round=12,\n    speaker_selection_method=\"auto\"  # LLM decides who speaks next\n)\n\n# Create manager to orchestrate\nmanager = GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n\n# Start the discussion\nmanager.initiate_chat(\n    groupchat,\n    message=\"Let's discuss how to optimize our database queries\"\n)\n```\n\nThe GroupChatManager will coordinate the conversation, and agents will take turns\nbased on the speaker_selection_method.\"\n```\n\n---\n\n## Example 2: \"create an api\"\n\n### User Request\n```\n\"I need to create an API for my Autogen agents\"\n```\n\n### Ambiguity Detected\n- \"API\" - multiple interpretations (HTTP server? Interface? Tools?)\n- Context: Autogen domain, but \"API\" is general term\n\n### Semantic Validation Process\n\n**Step 1: Analyze Context**\n- User working with Autogen agents\n- Needs some kind of \"API\"\n- Could mean: HTTP server to trigger agents, or tool/function calling interface\n\n**Step 2: Present Options**\n```\n\"API\" in the context of Autogen agents could mean:\n\n1. **HTTP REST API server** (to trigger agents via web requests)\n   - Framework: FastAPI, Flask (Python)\n   - Use case: Web app calls agent workflows via HTTP\n   - Example: POST /api/analyze â†’ triggers analyst agent\n\n2. **Tool/Function calling interface** (for agents to call external functions)\n   - Autogen feature: register_for_llm() and register_for_execution()\n   - Use case: Agents can call functions like search_web(), query_database()\n   - Example: Agent requests \"search_web(query)\" via LLM function calling\n\n3. **Agent interface/wrapper** (expose agents as callable interface)\n   - Python class with clean interface\n   - Use case: Other code can easily instantiate and use agents\n   - Example: AgentAPI.run_analysis(prompt) â†’ returns agent result\n\nWhich type of API integration do you need?\"\n```\n\n**Step 3: User Clarifies**\n```\nUser: \"I want agents to be able to call external functions like database queries\"\n```\n\n**Step 4: Validated Translation**\n```\n\"That's tool/function calling in Autogen! Here's the pattern:\n\n```python\nfrom autogen import ConversableAgent, UserProxyAgent\nimport typing_extensions as typing\n\n# Define tool function\ndef query_database(query: str) -> str:\n    \"\"\"Execute SQL query and return results\"\"\"\n    # Your database logic here\n    return f\"Results for: {query}\"\n\n# Create agents\nassistant = ConversableAgent(\n    name=\"Assistant\",\n    system_message=\"You help users query databases\",\n    llm_config={\"config_list\": config_list}\n)\n\nuser_proxy = UserProxyAgent(\n    name=\"UserProxy\",\n    human_input_mode=\"NEVER\",\n    code_execution_config=False\n)\n\n# Register function for LLM to call\n@assistant.register_for_llm(\n    description=\"Query the database with SQL\"\n)\ndef query_db_llm(query: str) -> str:\n    return query\n\n# Register function for execution\n@user_proxy.register_for_execution()\ndef query_db_execution(query: str) -> str:\n    return query_database(query)\n\n# Now assistant can request database queries via LLM function calling\nuser_proxy.initiate_chat(\n    assistant,\n    message=\"Can you query the database for all users created today?\"\n)\n```\n\nThe LLM will generate a function call request, and UserProxy will execute it.\"\n```\n\n---\n\n## Example 3: \"agent with tools\"\n\n### User Request\n```\n\"How do I create an Autogen agent with tools?\"\n```\n\n### Ambiguity Detected\n- \"tools\" - could mean different things in Autogen context\n\n### Semantic Validation Process\n\n**Step 1: Query Knowledge**\n- Autogen has: function calling (register_for_llm), code execution, external tool integration\n- Need to clarify which \"tools\" user means\n\n**Step 2: Present Options**\n```\n\"Tools\" in Autogen can refer to:\n\n1. **Function calling** (LLM-callable functions)\n   - Most common interpretation\n   - Agent: AssistantAgent with register_for_llm()\n   - Use case: Agent can call Python functions via LLM\n\n2. **Code execution** (execute generated code)\n   - Built-in Autogen feature\n   - Agent: UserProxyAgent with code_execution_config\n   - Use case: Agent generates and executes Python/shell code\n\n3. **External tool integration** (APIs, services)\n   - Custom functions wrapping external tools\n   - Combined with function calling\n   - Use case: Agent calls external APIs, services\n\nWhich type of tool integration did you have in mind?\"\n```\n\n**Step 3: User Clarifies**\n```\nUser: \"I want the agent to be able to search the web and query a database\"\n```\n\n**Step 4: Validated Translation**\n```\n\"That's function calling with external tools. Here's the complete pattern:\n\n```python\nfrom autogen import AssistantAgent, UserProxyAgent\nimport requests\n\n# Define tool functions\ndef search_web(query: str) -> str:\n    \"\"\"Search the web using a search API\"\"\"\n    # Your search API logic\n    response = requests.get(f\"https://api.search.com?q={query}\")\n    return response.json()[\"results\"]\n\ndef query_database(sql: str) -> str:\n    \"\"\"Execute SQL query\"\"\"\n    # Your database logic\n    return execute_query(sql)\n\n# Create AssistantAgent (can request function calls)\nassistant = AssistantAgent(\n    name=\"Assistant\",\n    system_message=\"You can search the web and query databases to help users\",\n    llm_config={\"config_list\": config_list, \"functions\": [\n        {\n            \"name\": \"search_web\",\n            \"description\": \"Search the web for information\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"query\": {\"type\": \"string\", \"description\": \"Search query\"}\n                },\n                \"required\": [\"query\"]\n            }\n        },\n        {\n            \"name\": \"query_database\",\n            \"description\": \"Execute SQL query\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"sql\": {\"type\": \"string\", \"description\": \"SQL query\"}\n                },\n                \"required\": [\"sql\"]\n            }\n        }\n    ]}\n)\n\n# Create UserProxyAgent (executes functions)\nuser_proxy = UserProxyAgent(\n    name=\"UserProxy\",\n    human_input_mode=\"NEVER\",\n    code_execution_config=False,\n    function_map={\n        \"search_web\": search_web,\n        \"query_database\": query_database\n    }\n)\n\n# Agent can now use both tools\nuser_proxy.initiate_chat(\n    assistant,\n    message=\"Search the web for Python trends, then query our database for Python projects\"\n)\n```\n\nThe assistant will generate function calls, and user_proxy will execute them.\"\n```\n\n---\n\n## Example 4: \"group conversation\"\n\n### User Request\n```\n\"I need to set up a group conversation\"\n```\n\n### Ambiguity Detected\n- \"group conversation\" - might be GroupChat object or general concept\n- Need to clarify Autogen-specific implementation\n\n### Semantic Validation Process\n\n**Step 1: Confirm Understanding**\n```\n\"In Autogen, group conversations are implemented with GroupChat. Is that what you're looking for?\n\nKey features of GroupChat:\n- Multiple agents participate\n- GroupChatManager orchestrates conversation\n- Speaker selection (auto, manual, round-robin, random)\n- Conversation history maintained\n\nIf this matches what you need, I can show you how to set it up.\"\n```\n\n**Step 2: User Confirms**\n```\nUser: \"Yes, exactly. How do I set it up?\"\n```\n\n**Step 3: Provide Implementation**\n```\n\"Here's how to create a GroupChat in Autogen:\n\n```python\nfrom autogen import ConversableAgent, GroupChat, GroupChatManager\n\n# Create participating agents\nagents = [\n    ConversableAgent(\n        name=\"Researcher\",\n        system_message=\"You research topics thoroughly\",\n        llm_config=llm_config\n    ),\n    ConversableAgent(\n        name=\"Critic\",\n        system_message=\"You critically evaluate ideas\",\n        llm_config=llm_config\n    ),\n    ConversableAgent(\n        name=\"Synthesizer\",\n        system_message=\"You synthesize different viewpoints\",\n        llm_config=llm_config\n    )\n]\n\n# Create GroupChat\ngroupchat = GroupChat(\n    agents=agents,\n    messages=[],\n    max_round=15,  # Maximum conversation rounds\n    speaker_selection_method=\"auto\"  # Options: \"auto\", \"manual\", \"random\", \"round_robin\"\n)\n\n# Create GroupChatManager to orchestrate\nmanager = GroupChatManager(\n    groupchat=groupchat,\n    llm_config=llm_config\n)\n\n# Start the group conversation\nmanager.initiate_chat(\n    groupchat,\n    message=\"Let's discuss the implications of AI in education\"\n)\n```\n\n**Speaker Selection Methods:**\n- `\"auto\"` - LLM decides who speaks next based on conversation context (recommended)\n- `\"manual\"` - Human selects next speaker\n- `\"round_robin\"` - Agents take turns in order\n- `\"random\"` - Random agent selected each round\n\nThe GroupChatManager will:\n1. Broadcast message to all agents\n2. Select next speaker based on method\n3. Coordinate turn-taking\n4. Maintain conversation history\n5. Detect termination conditions\"\n```\n\n---\n\n## Example 5: \"human in the loop\"\n\n### User Request\n```\n\"I want the user to approve actions before the agent executes them\"\n```\n\n### Ambiguity Detected\n- \"approve actions\" - needs UserProxyAgent with specific configuration\n- Need to clarify when approval is needed\n\n### Semantic Validation Process\n\n**Step 1: Clarify Scope**\n```\n\"For human-in-the-loop workflows in Autogen, you'll use UserProxyAgent. When should the human approve?\n\n1. **Always** - Approve every agent action\n   - Setting: human_input_mode=\"ALWAYS\"\n   - Use case: Maximum control, every step requires approval\n\n2. **On termination** - Approve only at conversation end\n   - Setting: human_input_mode=\"TERMINATE\"\n   - Use case: Review final decision before completing\n\n3. **Never (but can execute code)** - No approval, but executes code on behalf of user\n   - Setting: human_input_mode=\"NEVER\"\n   - Use case: Automated workflows, agent executes without asking\n\nWhich approval pattern do you need?\"\n```\n\n**Step 2: User Clarifies**\n```\nUser: \"I want to approve every action before execution\"\n```\n\n**Step 3: Validated Translation**\n```\n\"That's UserProxyAgent with human_input_mode=\"ALWAYS\". Here's the setup:\n\n```python\nfrom autogen import AssistantAgent, UserProxyAgent\n\n# Create assistant (proposes actions)\nassistant = AssistantAgent(\n    name=\"Assistant\",\n    system_message=\"You help users accomplish tasks step by step\",\n    llm_config={\"config_list\": config_list}\n)\n\n# Create user proxy (requests human approval for everything)\nuser_proxy = UserProxyAgent(\n    name=\"UserProxy\",\n    human_input_mode=\"ALWAYS\",  # Request input for every action\n    max_consecutive_auto_reply=0,  # Always wait for human input\n    code_execution_config={\n        \"work_dir\": \"coding\",\n        \"use_docker\": False\n    }\n)\n\n# Start conversation\nuser_proxy.initiate_chat(\n    assistant,\n    message=\"Help me analyze this dataset\"\n)\n\n# Workflow:\n# 1. Assistant proposes an action\n# 2. UserProxy asks human: \"Approve this action? (yes/no)\"\n# 3. Human types approval or rejection\n# 4. If approved, action executes\n# 5. Repeat for next action\n```\n\n**How approval works:**\n- After each assistant message, you'll see a prompt\n- Type your response or approval\n- Type \"exit\" to end the conversation\n- The agent won't proceed without your input\"\n```\n\n---\n\n## Pattern Summary\n\nCommon Autogen ambiguity patterns:\n\n1. **\"create agent\"** â†’ Which type? ConversableAgent, AssistantAgent, UserProxyAgent\n2. **\"talk/communicate\"** â†’ send(), initiate_chat(), or GroupChat?\n3. **\"tools/functions\"** â†’ register_for_llm() + register_for_execution()\n4. **\"group/multi-agent\"** â†’ GroupChat + GroupChatManager\n5. **\"approval/human-in-loop\"** â†’ UserProxyAgent with human_input_mode\n\nAlways clarify before implementing!\n",
        "plugins/semantic-linguist/skills/semantic-translation/examples/common-ambiguities.md": "# Common Cross-Domain Ambiguities\n\nFrequent ambiguous patterns that appear across multiple domains and frameworks.\n\n## Meta-Questions (User Triggers)\n\n### \"am I making sense?\"\n\n**Intent:** User seeking validation of their explanation\n\n**Response Pattern:**\n```\n\"Let me verify I understand. Based on your recent messages:\n\n- You want to [summarize key point 1]\n- Using [technology/framework mentioned]\n- With [specific requirement]\n\nI noticed '[ambiguous term]' could mean:\n- Option A: [precise interpretation]\n- Option B: [precise interpretation]\n\nWhich interpretation is correct?\"\n```\n\n**Example:**\n```\nUser: \"I'm trying to create a system where agents collaborate on tasks.\n      Am I making sense?\"\n\nResponse: \"Yes, I understand you want multi-agent collaboration. I notice\n'collaborate on tasks' could mean different things:\n\nIn Autogen:\n- GroupChat for simultaneous multi-agent conversation\n- Sequential agent handoff (agent A â†’ agent B)\n\nIn Langroid:\n- Task delegation hierarchy\n- Parent task with multiple subtasks\n\nWhich framework are you using, and which collaboration pattern?\"\n```\n\n### \"does this make sense?\"\n\n**Intent:** User uncertain about their approach\n\n**Response Pattern:**\n```\n\"Your approach [summary of their description] makes sense for [use case].\n\nOne clarification needed: '[ambiguous aspect]'\n- Did you mean [Option A] or [Option B]?\n\nThis will help me provide more specific guidance.\"\n```\n\n## Vague Action Verbs\n\n### \"make it work\"\n\n**Ambiguity:** What specifically isn't working?\n\n**Clarification Template:**\n```\n\"To help you make it work, I need to understand what's not working:\n\n1. **Error/Exception?** - Share the error message\n2. **Wrong behavior?** - Describe expected vs actual behavior\n3. **Not executing?** - Is it running at all?\n4. **Configuration issue?** - Setup or installation problem?\n\nWhat specifically is the issue?\"\n```\n\n### \"make it faster\"\n\n**Ambiguity:** Which performance metric?\n\n**Clarification Template:**\n```\n\"'Faster' could mean different optimizations:\n\n1. **Response time** (latency)\n   - Reduce time from request to response\n   - Solutions: Caching, query optimization, async\n\n2. **Throughput** (requests/second)\n   - Handle more concurrent requests\n   - Solutions: Load balancing, horizontal scaling\n\n3. **Startup time** (cold start)\n   - Reduce initialization time\n   - Solutions: Lazy loading, pre-compilation\n\n4. **Processing time** (execution)\n   - Speed up core algorithm\n   - Solutions: Algorithm optimization, parallelization\n\nWhich metric are you trying to optimize?\"\n```\n\n### \"make it secure\"\n\n**Ambiguity:** Which security aspect?\n\n**Clarification Template:**\n```\n\"Security has multiple aspects:\n\n1. **Authentication** - Verify user identity\n   - Solutions: JWT, OAuth, API keys\n\n2. **Authorization** - Control access to resources\n   - Solutions: RBAC, permissions, policies\n\n3. **Data protection** - Encrypt sensitive data\n   - Solutions: TLS/SSL, encryption at rest\n\n4. **Input validation** - Prevent injection attacks\n   - Solutions: Sanitization, parameterized queries\n\n5. **Secrets management** - Protect credentials\n   - Solutions: Vault, env vars, key management\n\nWhich security concern are you addressing?\"\n```\n\n## Generic Technical Terms\n\n### \"component\"\n\n**Ambiguity:** Type of component?\n\n**Context-Dependent Interpretations:**\n- **Frontend:** UI component (React, Vue, Svelte)\n- **Backend:** Service component, module\n- **Architecture:** System component, microservice\n- **Code:** Class, function, module\n\n**Clarification:** \"What kind of component? UI component, backend service, or code module?\"\n\n### \"service\"\n\n**Ambiguity:** Service type?\n\n**Context-Dependent Interpretations:**\n- **Microservices:** Independent deployable service\n- **Backend:** API service, application service\n- **System:** OS service, daemon\n- **Cloud:** AWS/Azure service\n\n**Clarification:** \"Do you mean a microservice, backend API service, or cloud service?\"\n\n### \"module\"\n\n**Ambiguity:** Module type?\n\n**Context-Dependent Interpretations:**\n- **Python:** .py file, package\n- **JavaScript:** ES module, npm package\n- **Architecture:** Logical module, code organization\n- **System:** Loadable module, plugin\n\n**Clarification:** \"What type of module? Python module, npm package, or logical code module?\"\n\n## Scope Ambiguities\n\n### \"add validation\"\n\n**Clarification Template:**\n```\n\"Validation has different scopes:\n\n1. **Input validation** (user input, API params)\n   - Check types, formats, ranges\n   - Tools: Pydantic, Zod, JSON Schema\n\n2. **Business logic validation** (rules, constraints)\n   - Enforce business rules\n   - Custom validation functions\n\n3. **Schema validation** (database, API contracts)\n   - Ensure data structure compliance\n   - ORM validators, OpenAPI\n\nWhich level of validation do you need?\"\n```\n\n### \"improve performance\"\n\nSee \"make it faster\" above - same pattern.\n\n### \"add logging\"\n\n**Clarification Template:**\n```\n\"Logging serves different purposes:\n\n1. **Debug logging** (development)\n   - Verbose, detailed logs\n   - Level: DEBUG\n\n2. **Application logging** (production monitoring)\n   - Structured logs, errors, warnings\n   - Level: INFO, WARNING, ERROR\n\n3. **Audit logging** (compliance, security)\n   - Immutable logs, who did what when\n   - Never rotated or deleted\n\n4. **Performance logging** (metrics, tracing)\n   - Timing, performance metrics\n   - Tools: APM, distributed tracing\n\nWhat's the purpose of the logging?\"\n```\n\n## Domain Confusion\n\n### \"agent\" (multi-domain term)\n\n**Possible Meanings:**\n- **Autogen:** ConversableAgent, AssistantAgent, UserProxyAgent\n- **Langroid:** ChatAgent, ToolAgent\n- **General AI:** AI agent, chatbot\n- **Business:** User agent, sales agent\n- **System:** Software agent, daemon\n\n**Clarification:** \"Which framework's agent? Autogen, Langroid, or general AI agent?\"\n\n### \"task\" (multi-domain term)\n\n**Possible Meanings:**\n- **Langroid:** Task object for orchestration\n- **Autogen:** General task/goal for agent\n- **Celery:** Async task queue job\n- **General:** Todo item, work item\n\n**Clarification:** \"Do you mean Langroid Task object, async task queue job, or general task concept?\"\n\n### \"tool\" (multi-domain term)\n\n**Possible Meanings:**\n- **Autogen:** Function calling (register_for_llm)\n- **Langroid:** ToolMessage\n- **General:** CLI tool, utility, library\n- **LangChain:** Tool interface\n\n**Clarification:** \"Which framework's tool system? Autogen function calling, Langroid ToolMessage, or general utility?\"\n\n## Unclear References\n\n### \"use that\"\n\n**Problem:** Ambiguous antecedent\n\n**Clarification:** \"Which component are you referring to? [List recent mentioned components]\"\n\n### \"like before\"\n\n**Problem:** Unclear which previous instance\n\n**Clarification:** \"Like in which file/section? Can you point me to the specific example?\"\n\n### \"the thing\" / \"it\"\n\n**Problem:** Non-specific reference\n\n**Clarification:** \"Which specific component/variable/function?\"\n\n## Pattern Recognition\n\n### High-Ambiguity Signals\n\nTrigger validation when detecting:\n- Vague action verbs: \"make it X\", \"do the thing\"\n- Generic technical terms without context: \"component\", \"service\"\n- Meta-questions: \"am I making sense?\", \"does this make sense?\"\n- Unclear references: \"that\", \"it\", \"the thing\"\n- Domain-crossing: Mixing framework-specific terms\n- Invented terms: Words not in technical vocabulary\n\n### Low-Ambiguity Signals\n\nLow risk, probably don't need validation:\n- Specific technical terms: \"ConversableAgent\", \"FastAPI\"\n- Clear context: Recent messages establish domain\n- Precise language: \"HTTP REST API server\", \"Autogen GroupChat\"\n- Code examples provided: Shows exact intent\n\n## Summary\n\nCommon cross-domain ambiguities require:\n\n1. **Context detection** - Identify domain/framework\n2. **Pattern matching** - Recognize ambiguous patterns\n3. **Clarification** - Ask specific questions\n4. **Validation** - Confirm understanding before implementing\n\nUse these patterns to quickly identify and resolve ambiguities across different technical domains.\n",
        "plugins/semantic-linguist/skills/semantic-translation/examples/fastapi-mappings.md": "# FastAPI-Specific Ambiguity Resolutions\n\nReal-world examples of ambiguous user requests mapped to precise FastAPI implementations.\n\n## Overview\n\nFastAPI is a modern Python web framework for building APIs with automatic validation, serialization, and interactive documentation. Key concepts include:\n- **Path operations**: Endpoint decorators (@app.get, @app.post, @app.put, @app.delete)\n- **Dependency injection**: Depends() system for reusable components\n- **Pydantic models**: Request/response validation and serialization\n- **APIRouter**: Endpoint organization and modular structure\n\nCommon ambiguities arise when users use general terms like \"dependency\", \"endpoint\", \"model\", \"route\", or \"validation\" without FastAPI-specific context.\n\n---\n\n## Example 1: \"add dependencies to my api\"\n\n### User Request\n```\n\"I need to add dependencies to my FastAPI application\"\n```\n\n### Ambiguity Detected\n- \"dependencies\" - could mean FastAPI Depends(), pip packages, or architectural dependencies\n- Need to clarify which type of dependency\n\n### Semantic Validation Process\n\n**Step 1: Domain Identified**\n- User working with FastAPI\n- \"dependencies\" has multiple meanings in this context\n\n**Step 2: Query Knowledge**\n- FastAPI Depends() for dependency injection\n- pip dependencies for external packages\n- Architectural dependencies between services\n\n**Step 3: Present Options**\n```\nIn FastAPI context, \"dependencies\" can mean:\n\n1. **FastAPI Dependency Injection (Depends())**\n   - Reusable components injected into path operations\n   - Example: Authentication, database sessions, configuration\n   - Declared with: Depends(dependency_function)\n   - Use case: Share logic across endpoints (auth, DB access)\n\n2. **Pip Package Dependencies**\n   - External Python libraries your app uses\n   - Declared in: requirements.txt or pyproject.toml\n   - Example: fastapi, uvicorn, sqlalchemy, pydantic\n   - Use case: Add third-party functionality\n\n3. **Architectural Service Dependencies**\n   - External services your API depends on\n   - Example: Database, Redis, message queue, external APIs\n   - Configured via: Environment variables, settings\n   - Use case: System integration and infrastructure\n\nWhich type of dependency do you need?\"\n```\n\n**Step 4: User Clarifies**\n```\nUser: \"I want to add authentication that's reused across multiple endpoints\"\n```\n\n**Step 5: Validated Translation**\n```\n\"That's FastAPI Dependency Injection with Depends()! Here's the pattern:\n\n**1. Define dependency function:**\n```python\nfrom fastapi import Depends, HTTPException, status\nfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n\nsecurity = HTTPBearer()\n\nasync def get_current_user(\n    credentials: HTTPAuthorizationCredentials = Depends(security)\n) -> dict:\n    \"\"\"\n    Dependency that validates JWT token and returns current user.\n    Injected into endpoints that require authentication.\n    \"\"\"\n    token = credentials.credentials\n\n    # Validate token (your logic)\n    user = validate_jwt_token(token)\n    if not user:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid authentication credentials\"\n        )\n\n    return user\n```\n\n**2. Inject into path operations:**\n```python\nfrom fastapi import FastAPI, Depends\n\napp = FastAPI()\n\n@app.get(\"/protected\")\nasync def protected_route(\n    current_user: dict = Depends(get_current_user)  # Dependency injection\n):\n    \"\"\"\n    FastAPI automatically:\n    1. Calls get_current_user() before running this function\n    2. Passes result as current_user parameter\n    3. Returns 401 if dependency raises HTTPException\n    \"\"\"\n    return {\"message\": f\"Hello {current_user['name']}\", \"user\": current_user}\n\n@app.get(\"/admin\")\nasync def admin_route(\n    current_user: dict = Depends(get_current_user)  # Reused dependency\n):\n    \"\"\"Same dependency, different endpoint - DRY principle\"\"\"\n    if current_user[\"role\"] != \"admin\":\n        raise HTTPException(status_code=403, detail=\"Not authorized\")\n\n    return {\"message\": \"Admin access granted\"}\n```\n\n**3. Chain dependencies:**\n```python\nasync def get_db():\n    \"\"\"Dependency that provides database session\"\"\"\n    db = SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n\nasync def get_current_active_user(\n    current_user: dict = Depends(get_current_user),  # Depends on auth\n    db: Session = Depends(get_db)  # Depends on database\n):\n    \"\"\"Chained dependency - uses both auth and database\"\"\"\n    user_from_db = db.query(User).filter(User.id == current_user[\"id\"]).first()\n    if not user_from_db.is_active:\n        raise HTTPException(status_code=400, detail=\"Inactive user\")\n    return user_from_db\n```\n\n**Benefits of FastAPI Depends():**\n- Declare once, reuse everywhere\n- Automatic execution before endpoint\n- Dependency chaining for complex logic\n- Type hints for IDE autocomplete\n- Automatic OpenAPI documentation\"\n```\n\n---\n\n## Example 2: \"create an endpoint\"\n\n### User Request\n```\n\"I need to create an endpoint for user registration\"\n```\n\n### Ambiguity Detected\n- \"endpoint\" - could mean FastAPI path operation or external API endpoint\n- Need to clarify if they're creating or calling an endpoint\n\n### Semantic Validation Process\n\n**Step 1: Analyze Context**\n```\n\"Endpoint\" in FastAPI context could mean:\n\n1. **FastAPI Path Operation** (creating an endpoint)\n   - HTTP endpoint your API exposes\n   - Defined with: @app.post(\"/users\"), @app.get(\"/users/{id}\")\n   - Example: POST /api/users for user registration\n   - Use case: Building your API's functionality\n\n2. **External API Endpoint** (calling an endpoint)\n   - Third-party API your code calls\n   - Used with: httpx, requests library\n   - Example: Calling auth0.com/api/users\n   - Use case: Integrating external services\n\nAre you creating an endpoint (for others to call your API) or calling an endpoint (integrating external API)?\"\n```\n\n**Step 2: User Clarifies**\n```\nUser: \"Creating an endpoint - users will POST registration data to my API\"\n```\n\n**Step 3: Validated Translation**\n```\n\"That's a FastAPI path operation! Here's how to create it:\n\n**1. Define Pydantic model for request validation:**\n```python\nfrom pydantic import BaseModel, EmailStr, Field\n\nclass UserRegistration(BaseModel):\n    \"\"\"Request model for user registration\"\"\"\n    username: str = Field(..., min_length=3, max_length=50)\n    email: EmailStr  # Validates email format\n    password: str = Field(..., min_length=8)\n    full_name: str | None = None\n\n    class Config:\n        json_schema_extra = {\n            \"example\": {\n                \"username\": \"johndoe\",\n                \"email\": \"john@example.com\",\n                \"password\": \"securepass123\",\n                \"full_name\": \"John Doe\"\n            }\n        }\n\nclass UserResponse(BaseModel):\n    \"\"\"Response model (excludes sensitive fields)\"\"\"\n    id: int\n    username: str\n    email: EmailStr\n    full_name: str | None\n\n    class Config:\n        from_attributes = True  # Allow ORM model conversion\n```\n\n**2. Create path operation:**\n```python\nfrom fastapi import FastAPI, HTTPException, status\n\napp = FastAPI()\n\n@app.post(\n    \"/api/users\",  # Endpoint path\n    response_model=UserResponse,  # Response validation\n    status_code=status.HTTP_201_CREATED,  # Success status\n    tags=[\"users\"],  # OpenAPI grouping\n    summary=\"Register new user\"  # OpenAPI summary\n)\nasync def register_user(user_data: UserRegistration):\n    \"\"\"\n    Register a new user account.\n\n    - **username**: Unique username (3-50 characters)\n    - **email**: Valid email address\n    - **password**: Secure password (minimum 8 characters)\n    - **full_name**: Optional full name\n\n    Returns the created user (without password).\n    \"\"\"\n\n    # Check if user exists\n    existing_user = db.query(User).filter(\n        (User.username == user_data.username) | (User.email == user_data.email)\n    ).first()\n\n    if existing_user:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=\"Username or email already registered\"\n        )\n\n    # Hash password\n    hashed_password = hash_password(user_data.password)\n\n    # Create user\n    new_user = User(\n        username=user_data.username,\n        email=user_data.email,\n        hashed_password=hashed_password,\n        full_name=user_data.full_name\n    )\n\n    db.add(new_user)\n    db.commit()\n    db.refresh(new_user)\n\n    return new_user  # Automatically serialized to UserResponse\n```\n\n**3. What FastAPI does automatically:**\n- Validates request body against UserRegistration model\n- Returns 422 if validation fails (with detailed errors)\n- Serializes response to UserResponse model\n- Generates OpenAPI documentation\n- Creates interactive docs at /docs\"\n```\n\n---\n\n## Example 3: \"use models in fastapi\"\n\n### User Request\n```\n\"How do I use models in FastAPI?\"\n```\n\n### Ambiguity Detected\n- \"models\" - could mean Pydantic models, database models, or ML models\n- Need to clarify which type of model\n\n### Semantic Validation Process\n\n**Step 1: Present Options**\n```\n\"Models\" in FastAPI applications can mean:\n\n1. **Pydantic Models** (request/response validation)\n   - Define API data structures\n   - Automatic validation and serialization\n   - Example: UserCreate, UserResponse models\n   - Use case: API contracts, input validation\n\n2. **Database ORM Models** (SQLAlchemy/Tortoise)\n   - Define database tables\n   - ORM mapping to database\n   - Example: User table model with columns\n   - Use case: Database operations, persistence\n\n3. **Machine Learning Models** (scikit-learn, PyTorch)\n   - Trained ML models for predictions\n   - Loaded and used in endpoints\n   - Example: Sentiment analysis model\n   - Use case: AI/ML inference in API\n\nWhich type of model are you asking about?\"\n```\n\n**Step 2: User Clarifies**\n```\nUser: \"I want to validate incoming request data\"\n```\n\n**Step 3: Validated Translation**\n```\n\"That's Pydantic models for request validation! Here's the complete pattern:\n\n**1. Define Pydantic models:**\n```python\nfrom pydantic import BaseModel, Field, EmailStr, validator\nfrom typing import List, Optional\nfrom datetime import datetime\n\nclass ProductCreate(BaseModel):\n    \"\"\"Pydantic model for creating products\"\"\"\n    name: str = Field(..., min_length=1, max_length=100)\n    description: Optional[str] = Field(None, max_length=500)\n    price: float = Field(..., gt=0)  # Greater than 0\n    category: str\n    tags: List[str] = []\n\n    @validator('price')\n    def validate_price(cls, v):\n        \"\"\"Custom validation: price must be reasonable\"\"\"\n        if v > 100000:\n            raise ValueError('Price seems unreasonably high')\n        return round(v, 2)  # Round to 2 decimals\n\n    class Config:\n        json_schema_extra = {\n            \"example\": {\n                \"name\": \"Laptop\",\n                \"description\": \"High-performance laptop\",\n                \"price\": 999.99,\n                \"category\": \"electronics\",\n                \"tags\": [\"tech\", \"portable\"]\n            }\n        }\n\nclass ProductResponse(BaseModel):\n    \"\"\"Pydantic model for product responses\"\"\"\n    id: int\n    name: str\n    description: Optional[str]\n    price: float\n    category: str\n    tags: List[str]\n    created_at: datetime\n\n    class Config:\n        from_attributes = True  # Allow SQLAlchemy model conversion\n\nclass ProductUpdate(BaseModel):\n    \"\"\"Pydantic model for updating products (all fields optional)\"\"\"\n    name: Optional[str] = Field(None, min_length=1, max_length=100)\n    description: Optional[str] = Field(None, max_length=500)\n    price: Optional[float] = Field(None, gt=0)\n    category: Optional[str] = None\n    tags: Optional[List[str]] = None\n```\n\n**2. Use in path operations:**\n```python\nfrom fastapi import FastAPI, HTTPException\n\napp = FastAPI()\n\n@app.post(\"/products\", response_model=ProductResponse, status_code=201)\nasync def create_product(product: ProductCreate):\n    \"\"\"\n    FastAPI automatically:\n    - Validates product data against ProductCreate schema\n    - Returns 422 with validation errors if invalid\n    - Parses JSON body into ProductCreate object\n    \"\"\"\n\n    # product.name, product.price, etc. are already validated\n    db_product = Product(**product.dict())  # Convert Pydantic â†’ ORM model\n    db.add(db_product)\n    db.commit()\n    db.refresh(db_product)\n\n    return db_product  # Converted to ProductResponse automatically\n\n@app.put(\"/products/{product_id}\", response_model=ProductResponse)\nasync def update_product(product_id: int, updates: ProductUpdate):\n    \"\"\"\n    Partial updates - only provided fields are validated\n    \"\"\"\n    db_product = db.query(Product).filter(Product.id == product_id).first()\n    if not db_product:\n        raise HTTPException(status_code=404, detail=\"Product not found\")\n\n    # Update only provided fields\n    update_data = updates.dict(exclude_unset=True)  # Only set fields\n    for field, value in update_data.items():\n        setattr(db_product, field, value)\n\n    db.commit()\n    db.refresh(db_product)\n    return db_product\n```\n\n**3. Relationship between Pydantic and ORM models:**\n```python\n# Database ORM model (SQLAlchemy)\nfrom sqlalchemy import Column, Integer, String, Float, DateTime\nfrom sqlalchemy.ext.declarative import declarative_base\n\nBase = declarative_base()\n\nclass Product(Base):\n    \"\"\"Database table definition\"\"\"\n    __tablename__ = \"products\"\n\n    id = Column(Integer, primary_key=True)\n    name = Column(String(100), nullable=False)\n    description = Column(String(500))\n    price = Column(Float, nullable=False)\n    category = Column(String(50))\n    created_at = Column(DateTime, default=datetime.utcnow)\n\n# Workflow:\n# Request JSON â†’ Pydantic ProductCreate â†’ Validation â†’ ORM Product â†’ Database\n# Database â†’ ORM Product â†’ Pydantic ProductResponse â†’ Response JSON\n\n**Key distinction:**\n- **Pydantic models** = API layer (validation, serialization, documentation)\n- **ORM models** = Database layer (persistence, queries, relationships)\n- Convert between them as needed\"\n```\n\n---\n\n## Example 4: \"add routes to fastapi\"\n\n### User Request\n```\n\"How do I organize routes in FastAPI?\"\n```\n\n### Ambiguity Detected\n- \"routes\" - could mean individual path operations or APIRouter for organization\n- Need to clarify organization strategy\n\n### Semantic Validation Process\n\n**Step 1: Explain Options**\n```\nFastAPI route organization has two levels:\n\n1. **Path Operations** (individual routes)\n   - Single endpoint definitions\n   - Example: @app.get(\"/users\"), @app.post(\"/products\")\n   - Use for: Simple apps with few endpoints\n   - Pattern: All routes in main file\n\n2. **APIRouter** (route grouping)\n   - Group related endpoints together\n   - Example: UserRouter, ProductRouter, AuthRouter\n   - Use for: Larger apps, modular structure\n   - Pattern: Separate router modules, imported into main\n\nYour app has 50+ endpoints across users, products, orders, and auth. Which organization would help?\"\n```\n\n**Step 2: User Clarifies**\n```\nUser: \"I have many endpoints, want to organize by feature area\"\n```\n\n**Step 3: Validated Translation**\n```\n\"That's APIRouter for modular organization! Here's the structure:\n\n**1. Create feature routers (routers/users.py):**\n```python\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom typing import List\n\n# Create router instance\nrouter = APIRouter(\n    prefix=\"/users\",  # All routes prefixed with /users\n    tags=[\"users\"],   # OpenAPI grouping\n    dependencies=[Depends(verify_api_key)]  # Applied to all routes\n)\n\n@router.get(\"/\", response_model=List[UserResponse])\nasync def list_users(\n    skip: int = 0,\n    limit: int = 100,\n    db: Session = Depends(get_db)\n):\n    \"\"\"List all users - becomes GET /users/\"\"\"\n    users = db.query(User).offset(skip).limit(limit).all()\n    return users\n\n@router.post(\"/\", response_model=UserResponse, status_code=201)\nasync def create_user(\n    user: UserCreate,\n    db: Session = Depends(get_db)\n):\n    \"\"\"Create user - becomes POST /users/\"\"\"\n    # ... creation logic\n    return new_user\n\n@router.get(\"/{user_id}\", response_model=UserResponse)\nasync def get_user(user_id: int, db: Session = Depends(get_db)):\n    \"\"\"Get user by ID - becomes GET /users/{user_id}\"\"\"\n    user = db.query(User).filter(User.id == user_id).first()\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n    return user\n```\n\n**2. Create more routers (routers/products.py, routers/auth.py):**\n```python\n# routers/products.py\nfrom fastapi import APIRouter\n\nrouter = APIRouter(prefix=\"/products\", tags=[\"products\"])\n\n@router.get(\"/\")\nasync def list_products():\n    \"\"\"GET /products/\"\"\"\n    pass\n\n@router.post(\"/\")\nasync def create_product(product: ProductCreate):\n    \"\"\"POST /products/\"\"\"\n    pass\n\n# routers/auth.py\nfrom fastapi import APIRouter\n\nrouter = APIRouter(prefix=\"/auth\", tags=[\"authentication\"])\n\n@router.post(\"/login\")\nasync def login(credentials: LoginCredentials):\n    \"\"\"POST /auth/login\"\"\"\n    pass\n\n@router.post(\"/refresh\")\nasync def refresh_token(token: str):\n    \"\"\"POST /auth/refresh\"\"\"\n    pass\n```\n\n**3. Include routers in main app:**\n```python\nfrom fastapi import FastAPI\nfrom routers import users, products, auth\n\napp = FastAPI(title=\"My API\", version=\"1.0.0\")\n\n# Include all routers\napp.include_router(users.router)\napp.include_router(products.router)\napp.include_router(auth.router)\n\n# Can also add prefix at include time\napp.include_router(\n    admin.router,\n    prefix=\"/api/v1\",  # Adds /api/v1 before router's prefix\n    tags=[\"admin\"]\n)\n\n# Result:\n# GET /users/ â†’ users.list_users\n# POST /users/ â†’ users.create_user\n# GET /products/ â†’ products.list_products\n# POST /auth/login â†’ auth.login\n# GET /api/v1/admin/dashboard â†’ admin.dashboard\n```\n\n**4. Project structure:**\n```\nmy_api/\nâ”œâ”€â”€ main.py             # App instance, router inclusion\nâ”œâ”€â”€ routers/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”œâ”€â”€ users.py        # User endpoints\nâ”‚   â”œâ”€â”€ products.py     # Product endpoints\nâ”‚   â”œâ”€â”€ auth.py         # Auth endpoints\nâ”‚   â””â”€â”€ admin.py        # Admin endpoints\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ schemas.py      # Pydantic models\nâ”‚   â””â”€â”€ database.py     # ORM models\nâ””â”€â”€ dependencies.py     # Shared dependencies (auth, db)\n```\n\n**Benefits of APIRouter:**\n- Logical grouping by feature/domain\n- Separate files for maintainability\n- Reusable dependencies per router\n- Clear OpenAPI organization (tags)\n- Team can work on different routers simultaneously\"\n```\n\n---\n\n## Example 5: \"add validation to api\"\n\n### User Request\n```\n\"I need to add validation to my FastAPI endpoints\"\n```\n\n### Ambiguity Detected\n- \"validation\" - could mean Pydantic automatic validation or custom business logic validation\n- Need to clarify validation type\n\n### Semantic Validation Process\n\n**Step 1: Clarify Scope**\n```\nFastAPI supports two types of validation:\n\n1. **Pydantic Automatic Validation**\n   - Type checking, format validation, constraints\n   - Happens automatically via Pydantic models\n   - Example: Email format, min/max length, numeric ranges\n   - Built-in, no custom code needed\n\n2. **Custom Business Logic Validation**\n   - Domain-specific rules\n   - Implemented via Pydantic validators or endpoint logic\n   - Example: \"Username must be unique\", \"Order total matches items\"\n   - Requires custom implementation\n\nWhich type of validation do you need?\"\n```\n\n**Step 2: User Clarifies**\n```\nUser: \"Both - I need basic validation and also business rules\"\n```\n\n**Step 3: Validated Translation**\n```\n\"FastAPI supports both through Pydantic! Here's how:\n\n**1. Pydantic automatic validation (built-in):**\n```python\nfrom pydantic import BaseModel, Field, EmailStr, constr, validator\nfrom typing import List\nfrom datetime import date\n\nclass UserCreate(BaseModel):\n    # Type validation (automatic)\n    username: str  # Must be string\n    age: int  # Must be integer\n\n    # Format validation (automatic)\n    email: EmailStr  # Must be valid email format\n\n    # Constraint validation (automatic)\n    password: constr(min_length=8, max_length=100)  # Length constraints\n    bio: str = Field(None, max_length=500)  # Field-level constraints\n    age: int = Field(..., ge=18, le=120)  # Greater/less than constraints\n\n    # Enum validation (automatic)\n    role: Literal[\"user\", \"admin\", \"moderator\"]  # Must be one of these\n\n# FastAPI automatically validates and returns 422 with errors if validation fails\n```\n\n**2. Custom business logic validation (Pydantic validators):**\n```python\nfrom pydantic import BaseModel, validator, root_validator\n\nclass UserCreate(BaseModel):\n    username: str\n    email: EmailStr\n    password: str\n    password_confirm: str\n    birth_date: date\n\n    @validator('username')\n    def username_alphanumeric(cls, v):\n        \"\"\"Custom validation: username must be alphanumeric\"\"\"\n        if not v.isalnum():\n            raise ValueError('Username must be alphanumeric')\n        return v\n\n    @validator('password')\n    def password_strength(cls, v):\n        \"\"\"Custom validation: password requirements\"\"\"\n        if not any(char.isdigit() for char in v):\n            raise ValueError('Password must contain at least one digit')\n        if not any(char.isupper() for char in v):\n            raise ValueError('Password must contain at least one uppercase letter')\n        return v\n\n    @validator('birth_date')\n    def validate_age(cls, v):\n        \"\"\"Custom validation: user must be 18+\"\"\"\n        today = date.today()\n        age = today.year - v.year - ((today.month, today.day) < (v.month, v.day))\n        if age < 18:\n            raise ValueError('Must be at least 18 years old')\n        return v\n\n    @root_validator\n    def passwords_match(cls, values):\n        \"\"\"Cross-field validation: passwords must match\"\"\"\n        password = values.get('password')\n        password_confirm = values.get('password_confirm')\n\n        if password and password_confirm and password != password_confirm:\n            raise ValueError('Passwords do not match')\n\n        return values\n```\n\n**3. Endpoint-level validation (database checks):**\n```python\nfrom fastapi import APIRouter, HTTPException, status\n\nrouter = APIRouter()\n\n@router.post(\"/users\", response_model=UserResponse, status_code=201)\nasync def create_user(\n    user_data: UserCreate,  # Pydantic validation happens first\n    db: Session = Depends(get_db)\n):\n    \"\"\"\n    Validation flow:\n    1. Pydantic validates request body (automatic)\n    2. Custom validators run (username, password, age)\n    3. Endpoint checks database constraints (below)\n    \"\"\"\n\n    # Database-level validation: check uniqueness\n    existing_user = db.query(User).filter(\n        (User.username == user_data.username) | (User.email == user_data.email)\n    ).first()\n\n    if existing_user:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=\"Username or email already exists\"\n        )\n\n    # Business logic validation: check banned domains\n    email_domain = user_data.email.split('@')[1]\n    banned_domains = [\"tempmail.com\", \"throwaway.email\"]\n\n    if email_domain in banned_domains:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=\"Email domain not allowed\"\n        )\n\n    # All validation passed, create user\n    new_user = User(**user_data.dict(exclude={'password_confirm'}))\n    db.add(new_user)\n    db.commit()\n\n    return new_user\n```\n\n**Validation order:**\n1. **Pydantic automatic** - Types, formats, constraints (422 error if fails)\n2. **Custom validators** - Business rules (@validator, @root_validator) (422 error)\n3. **Endpoint logic** - Database checks, external validations (400/409 error)\n\n**When to use each:**\n- **Automatic**: Data format, types, ranges\n- **Validators**: Reusable business rules, cross-field checks\n- **Endpoint logic**: Database constraints, external API checks, complex workflows\"\n```\n\n---\n\n## Pattern Summary\n\nCommon FastAPI ambiguity patterns:\n\n1. **\"dependency\"** â†’ FastAPI Depends() (injection) vs pip dependency (package) vs architectural dependency (service)\n2. **\"endpoint\"** â†’ Path operation (@app.get) vs external API endpoint (calling third-party)\n3. **\"model\"** â†’ Pydantic model (validation) vs ORM model (database) vs ML model (inference)\n4. **\"route\"** â†’ Individual path operation vs APIRouter (grouping) vs URL routing concept\n5. **\"validation\"** â†’ Pydantic automatic (format/type) vs custom validators (business logic) vs endpoint checks (database)\n\n**Key distinctions:**\n- **Depends()** for shared logic (auth, DB sessions), **not** pip packages\n- **Path operations** create endpoints, **httpx/requests** call external endpoints\n- **Pydantic models** for API layer, **ORM models** for database layer\n- **APIRouter** for organization, **path operations** for individual routes\n- **Pydantic validators** run before endpoint, **endpoint logic** runs after validation\n\nAlways clarify FastAPI-specific context before implementing!\n",
        "plugins/semantic-linguist/skills/semantic-translation/examples/git-gitflow-mappings.md": "# Git/Gitflow-Specific Ambiguity Resolutions\n\nReal-world examples of ambiguous user requests mapped to precise Git and Gitflow workflow implementations.\n\n## Overview\n\nGit is version control software, and Gitflow is a branching workflow strategy. Key concepts include:\n- **Branch types** (Gitflow): feature/, release/, hotfix/, develop, main\n- **Merge strategies**: merge commit (preserves history), squash merge (single commit), rebase (linear history)\n- **Workflows**: Gitflow (structured), GitHub flow (simpler), GitLab flow (environment-based)\n- **Operations**: commit, push, pull, merge, rebase, cherry-pick\n\nCommon ambiguities arise when users use general terms like \"merge\", \"branch\", \"commit\", or \"workflow\" without specifying strategy or context.\n\n---\n\n## Example 1: \"merge my branch\"\n\n### User Request\n```\n\"I need to merge my feature branch into main\"\n```\n\n### Ambiguity Detected\n- \"merge\" - three different merge strategies with different outcomes\n- Need to clarify which merge strategy to use\n\n### Semantic Validation Process\n\n**Step 1: Domain Identified**\n- User working with Git/Gitflow\n- Needs to integrate feature branch into main\n\n**Step 2: Query Knowledge**\n- Three merge strategies: merge commit, squash merge, rebase\n- Each has different history preservation and commit structure\n\n**Step 3: Present Options**\n```\nGit offers three merge strategies with different outcomes:\n\n1. **Merge Commit** (preserves full history)\n   - Creates merge commit joining two branches\n   - All feature branch commits visible in history\n   - Command: git merge feature-branch\n   - Result: Non-linear history with merge commit\n   - Use case: Want to preserve detailed development history\n   - Example history: A-B-C (main) â†’ D-E-F (feature) â†’ G (merge commit)\n\n2. **Squash Merge** (combines into single commit)\n   - Condenses all feature commits into one commit on main\n   - Command: git merge --squash feature-branch && git commit\n   - Result: Linear history, single commit for entire feature\n   - Use case: Clean history, feature as atomic unit\n   - Example history: A-B-C â†’ D (single commit with all feature changes)\n\n3. **Rebase and Merge** (linear history)\n   - Replays feature commits on top of main\n   - Command: git rebase main && git merge feature-branch (fast-forward)\n   - Result: Linear history, all feature commits preserved\n   - Use case: Linear history with detailed development steps\n   - Example history: A-B-C â†’ D'-E'-F' (feature commits rebased on C)\n\nWhich history structure do you need for your project?\"\n```\n\n**Step 4: User Clarifies**\n```\nUser: \"We want to see all development commits but keep history clean\"\n```\n\n**Step 5: Validated Translation**\n```\n\"That's rebase and merge (linear history)! Here's the workflow:\n\n**1. Update main and feature branches:**\n```bash\n# Switch to main and get latest\ngit checkout main\ngit pull origin main\n\n# Switch to feature branch\ngit checkout feature/user-authentication\ngit pull origin feature/user-authentication\n```\n\n**2. Rebase feature onto main:**\n```bash\n# Replay feature commits on top of updated main\ngit rebase main\n\n# If conflicts occur:\n# 1. Resolve conflicts in files\n# 2. git add <resolved-files>\n# 3. git rebase --continue\n# 4. Repeat until rebase complete\n\n# Result: Feature commits now based on latest main\n```\n\n**3. Merge (fast-forward):**\n```bash\n# Switch to main\ngit checkout main\n\n# Fast-forward merge (no merge commit needed)\ngit merge feature/user-authentication\n\n# Should show: \"Fast-forward\" (linear history maintained)\n```\n\n**4. Push and cleanup:**\n```bash\n# Push updated main\ngit push origin main\n\n# Delete feature branch\ngit branch -d feature/user-authentication\ngit push origin --delete feature/user-authentication\n```\n\n**Result:**\n- Linear history on main: A-B-C-D-E-F (all commits from feature visible)\n- No merge commit (fast-forward)\n- Easy to follow development sequence\n- Each commit in feature preserved with context\n\n**Before:**\n```\nmain:    A---B---C\n               \\\nfeature:         D---E---F\n```\n\n**After:**\n```\nmain:    A---B---C---D---E---F (linear, all commits visible)\n```\n\n**When NOT to use:**\n- Public branches already pushed (rebase rewrites history)\n- Want merge commit to mark feature completion\n- Team prefers atomic feature commits (use squash instead)\"\n```\n\n---\n\n## Example 2: \"create a branch\"\n\n### User Request\n```\n\"I need to create a branch for the new login feature\"\n```\n\n### Ambiguity Detected\n- \"branch\" - in Gitflow, branch type determines naming and merge target\n- Need to clarify if this is feature, release, hotfix, or general branch\n\n### Semantic Validation Process\n\n**Step 1: Analyze Context**\n```\nGitflow defines specific branch types with conventions:\n\n1. **Feature Branch** (new functionality)\n   - Naming: feature/<feature-name>\n   - Branched from: develop\n   - Merged into: develop\n   - Use case: New features during development\n   - Example: feature/user-login, feature/payment-integration\n\n2. **Release Branch** (preparing for production release)\n   - Naming: release/<version>\n   - Branched from: develop\n   - Merged into: main AND develop\n   - Use case: Final testing, bug fixes before release\n   - Example: release/1.2.0, release/v2.0.0\n\n3. **Hotfix Branch** (urgent production fixes)\n   - Naming: hotfix/<issue>\n   - Branched from: main\n   - Merged into: main AND develop\n   - Use case: Critical bugs in production\n   - Example: hotfix/security-vulnerability, hotfix/payment-bug\n\n4. **General Branch** (not following Gitflow)\n   - Naming: Any name\n   - Branched from: Any branch\n   - Merged into: Any branch\n   - Use case: Simple projects not using Gitflow\n\nYour \"login feature\" sounds like new functionality. Is this:\"\n- **Feature branch** (develop â†’ feature/login â†’ develop)\n- **Hotfix** (urgent fix for broken login in production)\n- **Something else**\n```\n\n**Step 2: User Clarifies**\n```\nUser: \"New feature being developed, not urgent fix\"\n```\n\n**Step 3: Validated Translation**\n```\n\"That's a Gitflow feature branch! Here's the proper workflow:\n\n**1. Create feature branch from develop:**\n```bash\n# Ensure develop is up to date\ngit checkout develop\ngit pull origin develop\n\n# Create and switch to feature branch\ngit checkout -b feature/user-login\n\n# Push to remote (optional, for collaboration)\ngit push -u origin feature/user-login\n```\n\n**2. Develop feature with regular commits:**\n```bash\n# Make changes\ngit add src/auth/login.py\ngit commit -m \"feat: add login endpoint\"\n\ngit add src/auth/session.py\ngit commit -m \"feat: implement session management\"\n\ngit add tests/test_login.py\ngit commit -m \"test: add login endpoint tests\"\n\n# Push commits\ngit push origin feature/user-login\n```\n\n**3. Keep feature branch updated with develop:**\n```bash\n# Periodically sync with develop to avoid large merge conflicts\ngit checkout develop\ngit pull origin develop\n\ngit checkout feature/user-login\ngit merge develop  # or git rebase develop for linear history\n\n# Resolve any conflicts\ngit push origin feature/user-login\n```\n\n**4. Complete feature - merge back to develop:**\n```bash\n# Option A: Merge commit (preserves feature branch history)\ngit checkout develop\ngit pull origin develop\ngit merge --no-ff feature/user-login  # --no-ff creates merge commit\ngit push origin develop\n\n# Option B: Squash merge (single commit for feature)\ngit checkout develop\ngit pull origin develop\ngit merge --squash feature/user-login\ngit commit -m \"feat: add user login functionality\"\ngit push origin develop\n\n# Delete feature branch\ngit branch -d feature/user-login\ngit push origin --delete feature/user-login\n```\n\n**Gitflow branch structure:**\n```\nmain (production)\n  |\n  â””â”€â”€â”€ hotfix/... (urgent fixes) â†’ merge to main + develop\n  |\ndevelop (integration)\n  |\n  â”œâ”€â”€â”€ feature/user-login (new features) â†’ merge to develop\n  â”œâ”€â”€â”€ feature/payments\n  â””â”€â”€â”€ release/1.2.0 (release prep) â†’ merge to main + develop\n```\n\n**Naming conventions:**\n- feature/descriptive-name (kebab-case)\n- Examples: feature/oauth-integration, feature/csv-export, feature/dark-mode\"\n```\n\n---\n\n## Example 3: \"use gitflow for our project\"\n\n### User Request\n```\n\"Should we use Gitflow or something simpler?\"\n```\n\n### Ambiguity Detected\n- \"Gitflow\" vs other workflows (GitHub flow, GitLab flow, trunk-based)\n- Need to clarify project needs to recommend workflow\n\n### Semantic Validation Process\n\n**Step 1: Present Workflow Options**\n```\nThree main Git workflows, each for different project types:\n\n1. **Gitflow** (structured, multiple parallel versions)\n   - Branches: main, develop, feature/, release/, hotfix/\n   - Best for: Products with scheduled releases, multiple versions in production\n   - Example: Desktop software with v1.0, v2.0, v2.1 supported simultaneously\n   - Pros: Structured, supports hotfixes, clear release process\n   - Cons: Complex, overhead for simple projects\n\n2. **GitHub Flow** (simple, continuous deployment)\n   - Branches: main, feature/* (only)\n   - Best for: Web apps with continuous deployment, single production version\n   - Example: SaaS application deployed from main constantly\n   - Pros: Simple, fast, encourages small changes\n   - Cons: No release branches, harder to manage versions\n\n3. **GitLab Flow** (environment branches)\n   - Branches: main, production, pre-production, feature/*\n   - Best for: Apps with staging environments, controlled production\n   - Example: API with dev â†’ staging â†’ production pipeline\n   - Pros: Environment-based deployments, structured but simpler than Gitflow\n   - Cons: More complex than GitHub flow\n\nWhat's your deployment model?\"\n- Multiple versions in production at once â†’ **Gitflow**\n- Continuous deployment to single production â†’ **GitHub Flow**\n- Environment-based deployments (dev/staging/prod) â†’ **GitLab Flow**\n```\n\n**Step 2: User Clarifies**\n```\nUser: \"We deploy to production continuously from main, no scheduled releases\"\n```\n\n**Step 3: Validated Translation**\n```\n\"That's GitHub Flow - perfect for continuous deployment! Here's how it works:\n\n**GitHub Flow Workflow:**\n\n**1. Branch structure (simple):**\n```\nmain (production - always deployable)\n  |\n  â”œâ”€â”€â”€ feature/add-search\n  â”œâ”€â”€â”€ fix/payment-bug\n  â””â”€â”€â”€ enhance/ui-polish\n```\n\n**2. Feature development:**\n```bash\n# Create feature branch from main\ngit checkout main\ngit pull origin main\ngit checkout -b feature/add-search\n\n# Develop and commit\ngit add src/search.py\ngit commit -m \"Add search functionality\"\n\n# Push frequently (collaboration + backup)\ngit push origin feature/add-search\n```\n\n**3. Pull Request (code review):**\n```bash\n# Open PR on GitHub: feature/add-search â†’ main\n# Team reviews code\n# CI runs tests automatically\n# Must pass checks before merge\n```\n\n**4. Merge to main (triggers deployment):**\n```bash\n# After PR approval, merge to main (GitHub UI or command line)\ngit checkout main\ngit pull origin main\ngit merge feature/add-search\ngit push origin main\n\n# Automatic deployment triggers (CI/CD)\n# Feature goes live immediately\n```\n\n**5. Cleanup:**\n```bash\n# Delete merged branch\ngit branch -d feature/add-search\ngit push origin --delete feature/add-search\n```\n\n**GitHub Flow principles:**\n- main is always deployable (protected branch)\n- All changes via short-lived feature branches\n- Pull Requests for code review\n- Merge to main = deploy to production\n- No develop, release, or hotfix branches\n\n**When to use:**\n- âœ… Continuous deployment\n- âœ… Single production environment\n- âœ… Fast iteration\n- âœ… Small, frequent releases\n\n**When NOT to use:**\n- âŒ Need to support multiple production versions\n- âŒ Scheduled release cycles\n- âŒ Long-running release branches\n- â†’ Use Gitflow instead\"\n```\n\n---\n\n## Example 4: \"fix commit history\"\n\n### User Request\n```\n\"I need to fix my last few commits before merging\"\n```\n\n### Ambiguity Detected\n- \"fix commits\" - could mean interactive rebase, amend, or reset\n- Need to understand what needs fixing\n\n### Semantic Validation Process\n\n**Step 1: Clarify What to Fix**\n```\n\"Fixing commit history\" has different approaches:\n\n1. **Amend Last Commit** (change most recent commit)\n   - Fix commit message typo\n   - Add forgotten files to last commit\n   - Command: git commit --amend\n   - Affects: Only most recent commit\n\n2. **Interactive Rebase** (edit multiple commits)\n   - Rewrite commit messages\n   - Squash commits together\n   - Reorder commits\n   - Split commits\n   - Command: git rebase -i HEAD~N\n   - Affects: Last N commits\n\n3. **Reset** (undo commits, keep changes)\n   - Remove commits but keep file changes\n   - Command: git reset --soft HEAD~N\n   - Affects: Removes N commits, stages changes\n\nWhat do you need to fix:\n- Change last commit message/content â†’ **Amend**\n- Combine/reorder/edit multiple commits â†’ **Interactive Rebase**\n- Remove commits but keep changes â†’ **Reset**\n```\n\n**Step 2: User Clarifies**\n```\nUser: \"I have 5 commits with messy messages, want to squash into 2 clean commits\"\n```\n\n**Step 3: Validated Translation**\n```\n\"That's interactive rebase! Here's how to squash commits:\n\n**Current messy history:**\n```bash\ngit log --oneline\n# a1b2c3d fix typo\n# d4e5f6g oops forgot file\n# g7h8i9j add feature part 2\n# j0k1l2m add feature part 1\n# m3n4o5p previous work\n```\n\n**1. Start interactive rebase:**\n```bash\n# Rebase last 4 commits (keeps m3n4o5p unchanged)\ngit rebase -i HEAD~4\n\n# Git opens editor showing:\npick j0k1l2m add feature part 1\npick g7h8i9j add feature part 2\npick d4e5f6g oops forgot file\npick a1b2c3d fix typo\n\n# Rebase commands:\n# pick = use commit as-is\n# squash (s) = combine with previous commit\n# reword (r) = change commit message\n# drop (d) = delete commit\n```\n\n**2. Edit to squash commits:**\n```bash\n# Edit to this:\npick j0k1l2m add feature part 1\nsquash g7h8i9j add feature part 2\nsquash d4e5f6g oops forgot file\nsquash a1b2c3d fix typo\n\n# Save and close editor\n```\n\n**3. Write combined commit message:**\n```bash\n# Git opens editor for new commit message:\n# feat: implement search functionality\n#\n# - Add search API endpoint\n# - Integrate with frontend\n# - Add tests for search feature\n#\n# Squashed commits:\n# - add feature part 1\n# - add feature part 2\n# - oops forgot file\n# - fix typo\n```\n\n**4. Result:**\n```bash\ngit log --oneline\n# x9y8z7w feat: implement search functionality\n# m3n4o5p previous work\n```\n\n**Advanced example - reorder and squash differently:**\n```bash\ngit rebase -i HEAD~5\n\n# Original:\npick a111111 add search endpoint\npick b222222 add tests\npick c333333 fix bug\npick d444444 add pagination\npick e555555 fix pagination bug\n\n# Reorder and squash:\npick a111111 add search endpoint\nsquash c333333 fix bug\npick d444444 add pagination\nsquash e555555 fix pagination bug\npick b222222 add tests\n\n# Result: 3 clean commits instead of 5 messy ones\n```\n\n**âš ï¸ Important rules:**\n- Only rebase commits not yet pushed to shared branch\n- If already pushed: force push required (git push --force)\n- Never rebase public/main branches\n- Communicate with team before force-pushing\"\n```\n\n---\n\n## Example 5: \"handle merge conflicts\"\n\n### User Request\n```\n\"I'm getting merge conflicts, what should I do?\"\n```\n\n### Ambiguity Detected\n- \"handle conflicts\" - could mean manual resolution, abort merge, or use merge strategy\n- Need to clarify conflict resolution approach\n\n### Semantic Validation Process\n\n**Step 1: Explain Conflict Scenarios**\n```\nMerge conflicts occur when Git can't auto-merge changes. You have options:\n\n1. **Manually Resolve Conflicts** (standard approach)\n   - Edit conflicted files\n   - Choose which changes to keep\n   - Mark as resolved\n   - Complete merge\n   - Use when: Changes are incompatible, need human decision\n\n2. **Abort Merge** (cancel and retry later)\n   - Cancel merge operation\n   - Return to pre-merge state\n   - Command: git merge --abort\n   - Use when: Wrong time to merge, need to rethink approach\n\n3. **Use Merge Strategy** (favor one side)\n   - Automatically favor \"ours\" or \"theirs\"\n   - Command: git merge -X ours / git merge -X theirs\n   - Use when: Know which version to keep entirely\n\nWhich approach makes sense for your situation?\"\n```\n\n**Step 2: User Clarifies**\n```\nUser: \"Changes are incompatible, need to manually decide what to keep\"\n```\n\n**Step 3: Validated Translation**\n```\n\"That's manual conflict resolution! Here's the complete workflow:\n\n**1. Identify conflicted files:**\n```bash\ngit status\n\n# Output shows:\n# Unmerged paths:\n#   both modified:   src/user.py\n#   both modified:   tests/test_user.py\n```\n\n**2. Open conflicted file (example):**\n```python\n# src/user.py\ndef get_user(user_id):\n    \\\"\\\"\\\"Get user by ID\\\"\\\"\\\"\n<<<<<<< HEAD (your current branch)\n    # Your changes:\n    user = db.query(User).filter(User.id == user_id).first()\n    if not user:\n        raise UserNotFoundError(f\"User {user_id} not found\")\n    return user\n=======\n    # Their changes (branch being merged):\n    user = User.objects.get(id=user_id)\n    if user is None:\n        raise ValueError(\"User not found\")\n    return user\n>>>>>>> feature/new-user-logic (branch being merged)\n```\n\n**3. Resolve conflict (edit file):**\n```python\n# Choose best parts from both:\ndef get_user(user_id):\n    \\\"\\\"\\\"Get user by ID\\\"\\\"\\\"\n    # Taking their ORM approach but your error handling\n    user = User.objects.get(id=user_id)\n    if user is None:\n        raise UserNotFoundError(f\"User {user_id} not found\")\n    return user\n\n# Remove conflict markers (<<<, ===, >>>)\n```\n\n**4. Mark as resolved:**\n```bash\n# Add resolved file\ngit add src/user.py\n\n# Check remaining conflicts\ngit status\n# If more conflicts, repeat steps 2-4\n```\n\n**5. Complete merge:**\n```bash\n# Commit merge resolution\ngit commit\n\n# Git opens editor with merge commit message:\n# Merge branch 'feature/new-user-logic' into develop\n#\n# Conflicts resolved:\n#   - src/user.py: Combined ORM change with better error handling\n#   - tests/test_user.py: Updated tests for new ORM\n\n# Save and close editor\n```\n\n**6. Verify:**\n```bash\n# Run tests to ensure resolution correct\npytest\n\n# Push if tests pass\ngit push origin develop\n```\n\n**Tools for conflict resolution:**\n\n**VS Code:**\n```\nConflict markers show:\n- Accept Current Change (your version)\n- Accept Incoming Change (their version)\n- Accept Both Changes\n- Compare Changes (side-by-side diff)\n```\n\n**Command line diff:**\n```bash\n# Show three-way diff\ngit diff --ours src/user.py   # Your changes\ngit diff --theirs src/user.py # Their changes\ngit diff --base src/user.py   # Common ancestor\n```\n\n**Conflict strategies for specific scenarios:**\n\n**Prefer ours globally:**\n```bash\ngit merge -X ours feature-branch\n# Auto-resolves conflicts favoring current branch\n```\n\n**Prefer theirs globally:**\n```bash\ngit merge -X theirs feature-branch\n# Auto-resolves conflicts favoring merged branch\n```\n\n**Aborting if conflicts too complex:**\n```bash\ngit merge --abort  # Cancel merge, return to pre-merge state\n# Then: Rethink approach or resolve conflicts differently\n```\n\n**Best practices:**\n- Resolve conflicts in small batches (don't let branches diverge too much)\n- Run tests after resolution\n- Communicate with teammates if their code affected\n- Consider pair programming for complex conflicts\"\n```\n\n---\n\n## Pattern Summary\n\nCommon Git/Gitflow ambiguity patterns:\n\n1. **\"merge\"** â†’ Merge commit (preserves history) vs Squash merge (single commit) vs Rebase (linear history)\n2. **\"branch\"** â†’ Gitflow branch type (feature/release/hotfix from specific source) vs general branch\n3. **\"workflow\"** â†’ Gitflow (structured, multiple versions) vs GitHub Flow (simple, continuous) vs GitLab Flow (environment-based)\n4. **\"fix commits\"** â†’ Amend (last commit) vs Interactive rebase (multiple commits) vs Reset (remove commits)\n5. **\"merge conflicts\"** â†’ Manual resolve (edit files) vs Abort merge vs Merge strategy (ours/theirs)\n6. **\"rebase\"** â†’ Interactive rebase (-i, edit commits) vs Standard rebase (replay commits) vs Rebase merge (linear history)\n\n**Key insights:**\n- **Merge strategies** change history structure (linear vs non-linear, detail vs summary)\n- **Gitflow branch types** determine naming convention, source branch, and merge target\n- **Workflows** match deployment model (scheduled releases â†’ Gitflow, continuous â†’ GitHub Flow)\n- **Commit editing** requires interactive rebase for multiple commits, amend for last commit only\n- **Conflict resolution** almost always requires manual editing for incompatible changes\n\nAlways clarify merge strategy, branch type, workflow model, and conflict approach before executing Git operations!\n",
        "plugins/semantic-linguist/skills/semantic-translation/examples/langroid-mappings.md": "# Langroid-Specific Ambiguity Resolutions\n\nReal-world examples of ambiguous user requests mapped to precise Langroid implementations.\n\n## Example 1: \"create an agent\"\n\n### User Request\n```\n\"How do I create an agent in Langroid?\"\n```\n\n### Semantic Validation\n\n**Clarification:**\n```\nIn Langroid, \"create an agent\" typically means ChatAgent. Do you need:\n\n1. **Basic ChatAgent** (simple conversational agent)\n2. **ToolAgent** (agent with function calling)\n3. **Custom Agent** (subclass with specialized behavior)\n```\n\n**User:** \"Basic agent for Q&A\"\n\n**Validated Implementation:**\n```python\nfrom langroid.agent.chat_agent import ChatAgent, ChatAgentConfig\n\n# Configure agent\nconfig = ChatAgentConfig(\n    name=\"QAAgent\",\n    system_message=\"You answer questions accurately and concisely\",\n    llm=OpenAIGPTConfig(chat_model=\"gpt-4\")\n)\n\n# Create agent\nagent = ChatAgent(config)\n\n# Use agent\nresponse = agent.llm_response(\"What is Python?\")\nprint(response)\n```\n\n---\n\n## Example 2: \"agent with tools\"\n\n### User Request\n```\n\"I need an agent that can search the web\"\n```\n\n### Semantic Validation\n\n**Clarification:**\n```\n\"Agent with tools\" in Langroid uses ToolMessage. You'll need to:\n1. Define a ToolMessage subclass for web search\n2. Agent will automatically detect and use it\n```\n\n**Validated Implementation:**\n```python\nfrom langroid.agent.tool_message import ToolMessage\nfrom langroid.agent.chat_agent import ChatAgent, ChatAgentConfig\nimport pydantic\n\nclass WebSearchTool(ToolMessage):\n    request: str = \"web_search\"\n    purpose: str = \"Search the web for information\"\n    query: str\n\n    def handle(self) -> str:\n        # Your web search implementation\n        return f\"Search results for: {self.query}\"\n\n# Create agent with tool\nconfig = ChatAgentConfig(\n    name=\"SearchAgent\",\n    system_message=\"You can search the web to answer questions\"\n)\nagent = ChatAgent(config)\n\n# Agent will automatically use WebSearchTool when needed\n```\n\n---\n\n## Example 3: \"multi-agent workflow\"\n\n### User Request\n```\n\"How do I set up multiple agents to work together?\"\n```\n\n### Semantic Validation\n\n**Clarification:**\n```\nLangroid multi-agent workflows use Task delegation:\n\n1. **Sequential** - Agent A â†’ Agent B â†’ Agent C\n2. **Hierarchical** - Parent task with subtasks\n3. **Parallel** - Multiple agents work simultaneously (custom coordination)\n```\n\n**User:** \"Sequential workflow\"\n\n**Validated Implementation:**\n```python\nfrom langroid.agent.task import Task\nfrom langroid.agent.chat_agent import ChatAgent, ChatAgentConfig\n\n# Create agents\nresearcher = ChatAgent(ChatAgentConfig(\n    name=\"Researcher\",\n    system_message=\"Research topics thoroughly\"\n))\n\nanalyst = ChatAgent(ChatAgentConfig(\n    name=\"Analyst\",\n    system_message=\"Analyze research findings\"\n))\n\n# Create tasks\nresearch_task = Task(\n    researcher,\n    name=\"research\",\n    interactive=False\n)\n\nanalysis_task = Task(\n    analyst,\n    name=\"analysis\",\n    interactive=False\n)\n\n# Set up delegation: research â†’ analysis\nresearch_task.add_sub_task(analysis_task)\n\n# Run workflow\nresult = research_task.run(\"Research AI trends\")\n```\n\n---\n\n## Example 4: \"agent responds to user\"\n\n### User Request\n```\n\"How do I get the agent to respond to user input?\"\n```\n\n### Semantic Validation\n\n**Clarification:**\n```\n\"Agent responds\" could mean:\n1. **LLM response** - agent.llm_response()\n2. **Agent logic response** - Override agent_response()\n3. **Interactive task** - Task.run() with interactive=True\n```\n\n**User:** \"Interactive chat\"\n\n**Validated Implementation:**\n```python\nfrom langroid.agent.task import Task\nfrom langroid.agent.chat_agent import ChatAgent, ChatAgentConfig\n\n# Create agent\nagent = ChatAgent(ChatAgentConfig(\n    name=\"Assistant\",\n    system_message=\"You are a helpful assistant\"\n))\n\n# Create interactive task\ntask = Task(\n    agent,\n    name=\"chat\",\n    interactive=True  # Allows human input\n)\n\n# Run (will prompt for user input)\ntask.run()\n```\n\n---\n\n## Pattern Summary\n\nCommon Langroid ambiguities:\n\n1. **\"create agent\"** â†’ ChatAgent vs ToolAgent vs custom subclass\n2. **\"agent with tools\"** â†’ ToolMessage pattern\n3. **\"multi-agent\"** â†’ Task delegation hierarchy\n4. **\"agent responds\"** â†’ llm_response() vs agent_response() vs interactive Task\n5. **\"run agent\"** â†’ Task.run() orchestration\n\nKey difference from Autogen: Langroid uses Task-based orchestration rather than direct agent-to-agent messaging.\n",
        "plugins/semantic-linguist/skills/semantic-translation/examples/mcp-mappings.md": "# MCP-Specific Ambiguity Resolutions\n\nReal-world examples of ambiguous user requests mapped to precise MCP (Model Context Protocol) implementations.\n\n## Overview\n\nMCP (Model Context Protocol) is a protocol for connecting AI assistants to external data sources and tools. Key concepts include:\n- **Server types**: SSE (Server-Sent Events), stdio (process-based), HTTP, WebSocket\n- **Components**: Tools (functions AI can call), Resources (data/content), Prompts (templates), Sampling (request generation)\n- **Integration modes**: Local server, remote server, managed server\n\nCommon ambiguities arise when users use general terms like \"server\", \"resource\", \"prompt\", or \"tool\" without specifying MCP-specific context.\n\n---\n\n## Example 1: \"set up an mcp server\"\n\n### User Request\n```\n\"I need to set up an MCP server for my project\"\n```\n\n### Ambiguity Detected\n- \"mcp server\" - which server type? (SSE, stdio, HTTP, WebSocket)\n- Unclear integration mode and use case\n\n### Semantic Validation Process\n\n**Step 1: Domain Identified**\n- User context: Working with MCP protocol\n- Needs clarification on server type\n\n**Step 2: Query Knowledge**\n- Static: MCP supports 4 server types with different characteristics\n- Ontology: Server type selection depends on deployment and integration needs\n\n**Step 3: Present Options**\n```\nMCP supports 4 server types, each for different scenarios:\n\n1. **SSE (Server-Sent Events) server**\n   - Web-based, real-time updates\n   - Best for: Browser integrations, cloud deployments\n   - Example: MCP server running on web platform with HTTP/SSE transport\n   - Connection: Claude Desktop connects via HTTPS URL\n\n2. **stdio server**\n   - Process-based communication via standard input/output\n   - Best for: Local tools, command-line utilities, desktop integrations\n   - Example: Local Python script MCP server\n   - Connection: Claude Desktop launches process and communicates via stdin/stdout\n\n3. **HTTP server**\n   - Traditional request-response HTTP API\n   - Best for: RESTful integrations, stateless operations\n   - Example: MCP server exposing HTTP endpoints\n   - Connection: Claude Desktop makes HTTP requests\n\n4. **WebSocket server**\n   - Bidirectional, persistent connection\n   - Best for: Real-time collaboration, streaming data\n   - Example: MCP server with continuous updates\n   - Connection: Claude Desktop maintains WebSocket connection\n\nWhich integration scenario matches your needs?\"\n```\n\n**Step 4: User Clarifies**\n```\nUser: \"I want to run a local Python script that Claude can call\"\n```\n\n**Step 5: Validated Translation**\n```\n\"That's a stdio server - perfect for local Python tools! Here's the setup:\n\n**1. Create MCP server (Python example):**\n```python\n#!/usr/bin/env python3\nfrom mcp.server import Server\nfrom mcp.server.stdio import stdio_server\nfrom mcp.types import Tool, TextContent\n\napp = Server(\"my-local-tools\")\n\n@app.list_tools()\nasync def list_tools() -> list[Tool]:\n    return [\n        Tool(\n            name=\"search_files\",\n            description=\"Search files in directory\",\n            inputSchema={\n                \"type\": \"object\",\n                \"properties\": {\n                    \"query\": {\"type\": \"string\"},\n                    \"directory\": {\"type\": \"string\"}\n                },\n                \"required\": [\"query\"]\n            }\n        )\n    ]\n\n@app.call_tool()\nasync def call_tool(name: str, arguments: dict) -> list[TextContent]:\n    if name == \"search_files\":\n        query = arguments[\"query\"]\n        directory = arguments.get(\"directory\", \".\")\n        # Your search logic here\n        results = perform_search(query, directory)\n        return [TextContent(type=\"text\", text=results)]\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(stdio_server(app))\n```\n\n**2. Configure in Claude Desktop (.mcp.json):**\n```json\n{\n  \"mcpServers\": {\n    \"my-local-tools\": {\n      \"command\": \"python\",\n      \"args\": [\"/path/to/your/server.py\"]\n    }\n  }\n}\n```\n\nClaude Desktop will:\n1. Launch your Python script as subprocess\n2. Communicate via stdin/stdout\n3. Call your tools when needed\n4. Terminate process when session ends\"\n```\n\n---\n\n## Example 2: \"add resources to mcp\"\n\n### User Request\n```\n\"I want to expose some resources through MCP\"\n```\n\n### Ambiguity Detected\n- \"resources\" - could mean MCP resources (data/content) or system resources (CPU/memory)\n- Need to clarify what data to expose and how\n\n### Semantic Validation Process\n\n**Step 1: Analyze Context**\n- User working with MCP protocol\n- \"resources\" in MCP context = data/content exposed to AI\n- Need to understand what data they want to expose\n\n**Step 2: Present Options**\n```\nIn MCP, \"resources\" are data/content you expose to the AI assistant. This is different from system resources like CPU/memory.\n\n**MCP Resource types:**\n\n1. **Static resources** (fixed content)\n   - File contents, configuration data, documentation\n   - URI pattern: `file:///path/to/resource`\n   - Example: Expose project README.md\n   - Use case: AI reads project documentation\n\n2. **Dynamic resources** (generated content)\n   - Database query results, API responses, computed data\n   - URI pattern: `custom://resource-type/identifier`\n   - Example: `db://users/active` returns active users\n   - Use case: AI accesses live data\n\n3. **Template resources** (parameterized)\n   - Resources with variable components\n   - URI pattern with variables: `logs://{date}/{level}`\n   - Example: `logs://2024-12-01/error`\n   - Use case: AI queries specific log combinations\n\nWhat type of data do you want to expose?\"\n```\n\n**Step 3: User Clarifies**\n```\nUser: \"I want Claude to be able to read files from my project directory\"\n```\n\n**Step 4: Validated Translation**\n```\n\"That's static file resources! Here's how to expose files via MCP:\n\n```python\nfrom mcp.server import Server\nfrom mcp.types import Resource, TextContent\nimport os\n\napp = Server(\"file-resources\")\n\n@app.list_resources()\nasync def list_resources() -> list[Resource]:\n    \"\"\"List available file resources\"\"\"\n    project_dir = \"/path/to/project\"\n    resources = []\n\n    for root, dirs, files in os.walk(project_dir):\n        for file in files:\n            if file.endswith(('.md', '.txt', '.py')):\n                filepath = os.path.join(root, file)\n                relative_path = os.path.relpath(filepath, project_dir)\n                resources.append(\n                    Resource(\n                        uri=f\"file:///{relative_path}\",\n                        name=relative_path,\n                        description=f\"Project file: {relative_path}\",\n                        mimeType=\"text/plain\"\n                    )\n                )\n\n    return resources\n\n@app.read_resource()\nasync def read_resource(uri: str) -> str:\n    \"\"\"Read file content\"\"\"\n    # Extract path from URI\n    path = uri.replace(\"file:///\", \"\")\n    full_path = os.path.join(\"/path/to/project\", path)\n\n    with open(full_path, 'r') as f:\n        content = f.read()\n\n    return content\n```\n\n**How it works:**\n1. AI calls `list_resources()` to see available files\n2. AI selects resource by URI: `file:///src/main.py`\n3. Server reads and returns file content\n4. AI uses content to answer questions about your project\"\n```\n\n---\n\n## Example 3: \"use mcp prompts\"\n\n### User Request\n```\n\"How do I use prompts in MCP?\"\n```\n\n### Ambiguity Detected\n- \"prompts\" - could mean MCP prompt templates or LLM prompts\n- Need to clarify MCP-specific prompt system\n\n### Semantic Validation Process\n\n**Step 1: Clarify Context**\n```\n\"In MCP, there are two types of 'prompts':\n\n1. **MCP Prompt Templates** (structured prompt definitions)\n   - Pre-defined prompt templates with variables\n   - Server provides, AI uses them\n   - Example: \"analyze_code\" prompt template with {language} and {code} variables\n   - Use case: Standardized prompts for common tasks\n\n2. **LLM Prompts** (text sent to language model)\n   - General prompt text sent to Claude/GPT\n   - Not MCP-specific\n   - Example: \"Analyze this Python code: ...\"\n   - Use case: Regular AI conversation\n\nWhich are you asking about?\"\n```\n\n**Step 2: User Clarifies**\n```\nUser: \"I want to create reusable prompt templates with variables\"\n```\n\n**Step 3: Validated Translation**\n```\n\"That's MCP Prompt Templates! Here's how to define them:\n\n```python\nfrom mcp.server import Server\nfrom mcp.types import Prompt, PromptMessage, TextContent\n\napp = Server(\"prompt-templates\")\n\n@app.list_prompts()\nasync def list_prompts() -> list[Prompt]:\n    return [\n        Prompt(\n            name=\"analyze_code\",\n            description=\"Analyze code for issues and improvements\",\n            arguments=[\n                {\n                    \"name\": \"language\",\n                    \"description\": \"Programming language\",\n                    \"required\": True\n                },\n                {\n                    \"name\": \"code\",\n                    \"description\": \"Code to analyze\",\n                    \"required\": True\n                }\n            ]\n        ),\n        Prompt(\n            name=\"review_pr\",\n            description=\"Review pull request changes\",\n            arguments=[\n                {\n                    \"name\": \"diff\",\n                    \"description\": \"Git diff content\",\n                    \"required\": True\n                },\n                {\n                    \"name\": \"focus\",\n                    \"description\": \"Review focus area\",\n                    \"required\": False\n                }\n            ]\n        )\n    ]\n\n@app.get_prompt()\nasync def get_prompt(name: str, arguments: dict) -> PromptMessage:\n    if name == \"analyze_code\":\n        language = arguments[\"language\"]\n        code = arguments[\"code\"]\n\n        return PromptMessage(\n            role=\"user\",\n            content=TextContent(\n                type=\"text\",\n                text=f\"\"\"Analyze this {language} code for:\n- Potential bugs and errors\n- Performance issues\n- Best practice violations\n- Security vulnerabilities\n\nCode:\n```{language}\n{code}\n```\n\nProvide specific, actionable feedback.\"\"\"\n            )\n        )\n\n    elif name == \"review_pr\":\n        diff = arguments[\"diff\"]\n        focus = arguments.get(\"focus\", \"general code quality\")\n\n        return PromptMessage(\n            role=\"user\",\n            content=TextContent(\n                type=\"text\",\n                text=f\"\"\"Review this pull request with focus on: {focus}\n\nDiff:\n{diff}\n\nProvide:\n1. Summary of changes\n2. Issues found\n3. Suggestions for improvement\"\"\"\n            )\n        )\n```\n\n**Usage flow:**\n1. AI sees available prompts via `list_prompts()`\n2. User selects \"analyze_code\" prompt\n3. AI fills variables: {language: \"Python\", code: \"...\"}\n4. Server returns complete prompt text\n5. AI uses generated prompt for analysis\"\n```\n\n---\n\n## Example 4: \"mcp tool vs function\"\n\n### User Request\n```\n\"What's the difference between an MCP tool and a regular function?\"\n```\n\n### Ambiguity Detected\n- Confusion between MCP tool definitions and Python functions\n- Need to clarify MCP tool system\n\n### Semantic Validation Process\n\n**Step 1: Explain Distinction**\n```\n\"MCP tools\" and \"regular functions\" are related but different:\n\n**MCP Tool (Definition)**:\n- Schema describing function to AI\n- Metadata: name, description, parameters\n- Declared via @app.call_tool() decorator\n- AI sees this and can request to call it\n\n**Regular Python Function**:\n- Actual implementation code\n- Lives in your Python script\n- Not automatically visible to AI\n- Executed when MCP tool called\n\n**Relationship:**\nMCP Tool â†’ Wrapper/Interface â†’ Python Function\n\nExample flow:\n1. You define MCP tool \"search_files\" (schema)\n2. You implement Python function search_files_impl() (logic)\n3. AI sees tool schema, requests to call it\n4. MCP server routes call to your function\n5. Function executes, returns result to AI\n```\n\n**Step 2: Provide Implementation Example**\n```\n\"Here's how they work together:\n\n```python\nfrom mcp.server import Server\nfrom mcp.types import Tool, TextContent\n\napp = Server(\"tools-demo\")\n\n# 1. Regular Python function (implementation)\ndef search_files_impl(query: str, file_type: str) -> str:\n    \"\"\"Actual search logic\"\"\"\n    # Your implementation\n    results = []\n    for file in os.listdir('.'):\n        if file_type in file and query in read_file(file):\n            results.append(file)\n    return '\\n'.join(results)\n\n# 2. MCP Tool definition (schema for AI)\n@app.list_tools()\nasync def list_tools() -> list[Tool]:\n    return [\n        Tool(\n            name=\"search_files\",  # Tool name AI uses\n            description=\"Search files by content and type\",  # What AI sees\n            inputSchema={  # Parameters AI must provide\n                \"type\": \"object\",\n                \"properties\": {\n                    \"query\": {\n                        \"type\": \"string\",\n                        \"description\": \"Search term\"\n                    },\n                    \"file_type\": {\n                        \"type\": \"string\",\n                        \"description\": \"File extension\"\n                    }\n                },\n                \"required\": [\"query\", \"file_type\"]\n            }\n        )\n    ]\n\n# 3. Connect tool to function (routing)\n@app.call_tool()\nasync def call_tool(name: str, arguments: dict) -> list[TextContent]:\n    if name == \"search_files\":\n        # Extract arguments from AI request\n        query = arguments[\"query\"]\n        file_type = arguments[\"file_type\"]\n\n        # Call actual Python function\n        results = search_files_impl(query, file_type)\n\n        # Return results to AI\n        return [TextContent(type=\"text\", text=results)]\n```\n\n**Summary:**\n- MCP Tool = What AI sees (schema/interface)\n- Python Function = What executes (implementation)\n- @app.call_tool() = Connects them (routing)\n```\n\n---\n\n## Example 5: \"mcp sampling\"\n\n### User Request\n```\n\"What is MCP sampling and when would I use it?\"\n```\n\n### Ambiguity Detected\n- \"sampling\" - could mean MCP sampling feature or statistical sampling\n- Need to clarify MCP-specific sampling capability\n\n### Semantic Validation Process\n\n**Step 1: Clarify Concept**\n```\n\"Sampling\" has different meanings depending on context:\n\n**MCP Sampling Feature**:\n- Server can request AI to generate text/responses\n- Use case: Server asks AI \"How would you analyze this?\"\n- Direction: Server â†’ AI (reverse of normal tool calling)\n- Example: Grammar checker asks AI to rephrase text\n\n**Statistical Sampling**:\n- Selecting subset of data for analysis\n- Use case: Random sample of 1000 from 1M records\n- Not MCP-specific, general data science concept\n\nThe MCP-specific feature is **sampling** - where your server can ask the AI to generate content.\n```\n\n**Step 2: Explain Use Cases**\n```\n\"MCP Sampling is useful when:\n\n1. **Content generation**\n   - Server needs AI-generated text\n   - Example: Documentation generator asks AI to write examples\n\n2. **Code completion**\n   - Server provides context, AI suggests code\n   - Example: IDE plugin asks AI for function implementation\n\n3. **Interactive workflows**\n   - Multi-step process where server guides AI\n   - Example: Interview bot asks AI to generate follow-up questions\n\n4. **Validation/refinement**\n   - Server asks AI to improve its own output\n   - Example: Translation tool asks AI to refine translation\n\nIs your server providing data to AI (tools/resources) or requesting AI-generated content (sampling)?\"\n```\n\n**Step 3: User Clarifies**\n```\nUser: \"I want my server to ask Claude to generate code examples\"\n```\n\n**Step 4: Validated Translation**\n```\n\"That's MCP sampling! Here's how to implement it:\n\n```python\nfrom mcp.server import Server\nfrom mcp.client import Client\n\napp = Server(\"code-generator\")\n\n# Server can request AI to generate content\nasync def generate_code_example(language: str, description: str):\n    \"\"\"Ask AI to generate code via sampling\"\"\"\n\n    # Create sampling request\n    prompt = f\"Write a {language} code example that {description}\"\n\n    # Request AI generation\n    response = await app.sample(\n        messages=[{\n            \"role\": \"user\",\n            \"content\": prompt\n        }],\n        maxTokens=500,\n        temperature=0.7\n    )\n\n    generated_code = response.content\n    return generated_code\n\n# Example usage in a tool\n@app.call_tool()\nasync def call_tool(name: str, arguments: dict):\n    if name == \"create_example\":\n        language = arguments[\"language\"]\n        description = arguments[\"description\"]\n\n        # Server asks AI to generate\n        code = await generate_code_example(language, description)\n\n        return [TextContent(\n            type=\"text\",\n            text=f\"Generated example:\\n\\n```{language}\\n{code}\\n```\"\n        )]\n```\n\n**Workflow:**\n1. User asks MCP tool to create code example\n2. MCP server sends sampling request to Claude\n3. Claude generates code based on description\n4. Server receives generated code\n5. Server returns code to user\n\nThis is **reverse direction** from normal tools:\n- Normal: AI calls your functions\n- Sampling: Your server requests AI generation\"\n```\n\n---\n\n## Example 6: \"connect mcp to database\"\n\n### User Request\n```\n\"I want to connect MCP to my database\"\n```\n\n### Ambiguity Detected\n- \"connect to database\" - expose database as resources or create database query tool?\n- Need to clarify integration pattern\n\n### Semantic Validation Process\n\n**Step 1: Present Options**\n```\n\"There are two ways to integrate databases with MCP:\n\n1. **Database as Resources** (read-only data access)\n   - Expose tables/views as MCP resources\n   - AI reads data via resource URIs\n   - Example: `db://users/all` returns all users\n   - Best for: Data exploration, reporting, read queries\n   - Pattern: Resource-based\n\n2. **Database Query Tool** (dynamic querying)\n   - Expose query function as MCP tool\n   - AI constructs SQL queries and executes them\n   - Example: Tool \"query_db\" with SQL parameter\n   - Best for: Complex queries, joins, filtering\n   - Pattern: Tool-based\n\nWhich approach fits your use case?\"\n```\n\n**Step 2: User Clarifies**\n```\nUser: \"I want Claude to be able to query the database with SQL\"\n```\n\n**Step 3: Validated Translation**\n```\n\"That's the Database Query Tool pattern! Here's the implementation:\n\n```python\nfrom mcp.server import Server\nfrom mcp.types import Tool, TextContent\nimport sqlite3  # Or your database library\n\napp = Server(\"database-access\")\n\n# Database connection\ndef get_db_connection():\n    return sqlite3.connect('/path/to/database.db')\n\n@app.list_tools()\nasync def list_tools() -> list[Tool]:\n    return [\n        Tool(\n            name=\"query_database\",\n            description=\"Execute SQL SELECT query on database\",\n            inputSchema={\n                \"type\": \"object\",\n                \"properties\": {\n                    \"query\": {\n                        \"type\": \"string\",\n                        \"description\": \"SQL SELECT query to execute\"\n                    },\n                    \"limit\": {\n                        \"type\": \"integer\",\n                        \"description\": \"Maximum rows to return\",\n                        \"default\": 100\n                    }\n                },\n                \"required\": [\"query\"]\n            }\n        ),\n        Tool(\n            name=\"get_table_schema\",\n            description=\"Get schema information for a table\",\n            inputSchema={\n                \"type\": \"object\",\n                \"properties\": {\n                    \"table_name\": {\n                        \"type\": \"string\",\n                        \"description\": \"Name of table\"\n                    }\n                },\n                \"required\": [\"table_name\"]\n            }\n        )\n    ]\n\n@app.call_tool()\nasync def call_tool(name: str, arguments: dict) -> list[TextContent]:\n    if name == \"query_database\":\n        query = arguments[\"query\"]\n        limit = arguments.get(\"limit\", 100)\n\n        # Validate query is SELECT only (security)\n        if not query.strip().upper().startswith(\"SELECT\"):\n            return [TextContent(\n                type=\"text\",\n                text=\"Error: Only SELECT queries allowed\"\n            )]\n\n        # Execute query\n        conn = get_db_connection()\n        cursor = conn.cursor()\n        cursor.execute(f\"{query} LIMIT {limit}\")\n\n        # Format results\n        columns = [desc[0] for desc in cursor.description]\n        rows = cursor.fetchall()\n\n        result = f\"Columns: {', '.join(columns)}\\n\\n\"\n        for row in rows:\n            result += f\"{row}\\n\"\n\n        conn.close()\n        return [TextContent(type=\"text\", text=result)]\n\n    elif name == \"get_table_schema\":\n        table_name = arguments[\"table_name\"]\n\n        conn = get_db_connection()\n        cursor = conn.cursor()\n        cursor.execute(f\"PRAGMA table_info({table_name})\")\n        schema = cursor.fetchall()\n\n        result = f\"Schema for {table_name}:\\n\"\n        for col in schema:\n            result += f\"- {col[1]} ({col[2]})\\n\"\n\n        conn.close()\n        return [TextContent(type=\"text\", text=result)]\n```\n\n**Security considerations:**\n- Only allow SELECT queries (block UPDATE, DELETE, DROP)\n- Add query result limits\n- Validate table/column names\n- Use parameterized queries to prevent SQL injection\n- Consider read-only database user\n\n**Usage flow:**\n1. AI calls `get_table_schema` to understand database structure\n2. AI constructs SQL query based on user question\n3. AI calls `query_database` with SQL\n4. Server executes query safely\n5. Results returned to AI for analysis\"\n```\n\n---\n\n## Pattern Summary\n\nCommon MCP ambiguity patterns:\n\n1. **\"mcp server\"** â†’ Which type? stdio (local process), SSE (web-based), HTTP, WebSocket\n2. **\"resource\"** â†’ MCP resource (data/content exposed) vs system resource (CPU/memory)\n3. **\"prompt\"** â†’ MCP prompt template (structured) vs LLM prompt (text)\n4. **\"tool\"** â†’ MCP tool definition (schema) vs Python function (implementation)\n5. **\"sampling\"** â†’ MCP sampling (server requests AI generation) vs statistical sampling\n6. **\"connect database\"** â†’ Resources (read-only data) vs Tool (query execution)\n\n**Key distinctions:**\n- **Server types** differ in transport mechanism and deployment\n- **Resources** = data AI reads | **Tools** = functions AI calls | **Prompts** = templates AI uses\n- **Sampling** reverses direction: server requests AI, not AI calls server\n\nAlways clarify integration pattern and MCP component type before implementing!\n",
        "plugins/semantic-linguist/skills/semantic-translation/examples/memory-graphs-mappings.md": "# Memory Graphs Domain - Semantic Translation Examples\n\n## Overview\n\nThe **Memory Graphs** domain encompasses knowledge graph structures, memory systems, vector embeddings, and retrieval mechanisms used in AI systems and knowledge management. This domain deals with how information is stored, connected, and retrieved in graph-based memory architectures.\n\n**Core Concepts:**\n- **Memory Types:** Episodic (events/experiences), Semantic (facts/concepts), Procedural (skills/processes)\n- **Graph Structures:** Knowledge graphs (entities + relationships), Memory networks, Conceptual hierarchies\n- **Vector Operations:** Embeddings (dense representations), Similarity search, Clustering\n- **Retrieval Patterns:** Semantic search (meaning-based), Graph traversal (relationship-based), Hybrid (combined)\n\n**Common Ambiguities:**\n- \"memory\" could mean: knowledge graph, vector database, episodic memory, semantic memory, or system RAM\n- \"graph\" could mean: knowledge graph structure, visualization diagram, or dependency graph\n- \"embedding\" could mean: vector embedding (representation) or embedding layer (neural network)\n- \"retrieval\" could mean: semantic search, graph traversal, hybrid retrieval, or SQL queries\n- \"node\" could mean: graph node (entity), memory node (experience), or network node (infrastructure)\n\n---\n\n## Example 1: \"add this to memory\"\n\n### User Request\n\"I need to add this conversation to memory so we can reference it later\"\n\n### Ambiguity Detected\n**Ambiguous term:** \"memory\"\n\n**Possible interpretations:**\n1. **Knowledge graph** - Store as structured entities and relationships\n2. **Vector memory** - Store as embedded chunks in vector database\n3. **Episodic memory** - Store as timestamped experience/event\n4. **Semantic memory** - Extract facts and add to concept hierarchy\n5. **System RAM** - Technical memory storage (likely not intended)\n\n### Semantic Validation Process\n\n**Memory Graphs provides different memory types for different use cases:**\n\n1. **Knowledge Graph Storage** - Structured entities + relationships\n   - Use when: Need to query connections between concepts\n   - Structure: Nodes (entities) + Edges (relationships)\n   - Example: Person â†’ WorksAt â†’ Company\n\n2. **Vector Memory Storage** - Embedded semantic chunks\n   - Use when: Need similarity-based retrieval\n   - Structure: Text chunks â†’ Embeddings â†’ Vector DB\n   - Example: Store conversation snippets for semantic search\n\n3. **Episodic Memory** - Timestamped experiences\n   - Use when: Need chronological event tracking\n   - Structure: Event + Context + Timestamp\n   - Example: \"2024-11-18: User discussed API design\"\n\n4. **Semantic Memory** - Extracted facts/concepts\n   - Use when: Building knowledge base of truths\n   - Structure: Fact + Source + Confidence\n   - Example: \"FastAPI uses Pydantic for validation\"\n\n### User Clarifies\n\"I want to be able to search for similar conversations later based on topic\"\n\n### Validated Translation\n\n**Storage approach:** Vector memory with semantic search\n\n```python\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# Split conversation into semantic chunks\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200\n)\nchunks = text_splitter.split_text(conversation_text)\n\n# Generate embeddings\nembeddings = OpenAIEmbeddings()\n\n# Store in vector database\nvectorstore = Chroma.from_texts(\n    texts=chunks,\n    embedding=embeddings,\n    metadatas=[\n        {\"timestamp\": \"2024-11-18\", \"topic\": \"semantic-linguist\", \"type\": \"conversation\"}\n        for _ in chunks\n    ],\n    collection_name=\"conversations\"\n)\n\n# Later: Semantic search for similar conversations\nquery = \"How did we handle domain ambiguities?\"\nsimilar_conversations = vectorstore.similarity_search(\n    query=query,\n    k=5  # Top 5 most similar\n)\n```\n\n**Why this approach:**\n- Vector embeddings capture semantic meaning\n- Similarity search finds topically related conversations\n- Metadata enables filtering by date/topic\n- No need for exact keyword matching\n\n---\n\n## Example 2: \"create a knowledge graph\"\n\n### User Request\n\"Can you create a knowledge graph showing the relationships between these concepts?\"\n\n### Ambiguity Detected\n**Ambiguous term:** \"knowledge graph\"\n\n**Possible interpretations:**\n1. **Structured graph database** - Neo4j-style property graph with nodes + edges\n2. **Visualization diagram** - GraphViz/Mermaid visual representation\n3. **RDF triple store** - Semantic web standard (subject-predicate-object)\n4. **In-memory graph structure** - Python networkx or similar\n5. **Conceptual map** - Simple text-based relationship description\n\n### Semantic Validation Process\n\n**Different \"knowledge graph\" implementations:**\n\n**1. Graph Database (Neo4j):**\n```cypher\n// Create nodes and relationships\nCREATE (fastapi:Framework {name: 'FastAPI', type: 'web'})\nCREATE (pydantic:Library {name: 'Pydantic', type: 'validation'})\nCREATE (depends:Feature {name: 'Depends()', type: 'DI'})\nCREATE (fastapi)-[:USES]->(pydantic)\nCREATE (fastapi)-[:PROVIDES]->(depends)\nCREATE (depends)-[:IMPLEMENTS]->(pydantic)\n\n// Query relationships\nMATCH (f:Framework)-[:USES]->(lib)\nWHERE f.name = 'FastAPI'\nRETURN lib.name\n```\n\n**2. Visualization Diagram (Mermaid):**\n```mermaid\ngraph TD\n    A[FastAPI] -->|uses| B[Pydantic]\n    A -->|provides| C[Depends DI]\n    C -->|implements| B\n    D[User Request] -->|validated by| B\n    D -->|injected via| C\n```\n\n**3. In-Memory Graph (NetworkX):**\n```python\nimport networkx as nx\n\nG = nx.DiGraph()\nG.add_edge(\"FastAPI\", \"Pydantic\", relationship=\"uses\")\nG.add_edge(\"FastAPI\", \"Depends()\", relationship=\"provides\")\nG.add_edge(\"Depends()\", \"Pydantic\", relationship=\"implements\")\n\n# Traverse relationships\ndependencies = list(G.successors(\"FastAPI\"))\n# ['Pydantic', 'Depends()']\n```\n\n**4. RDF Triple Store:**\n```turtle\n@prefix ex: <http://example.org/> .\n\nex:FastAPI rdf:type ex:Framework .\nex:FastAPI ex:uses ex:Pydantic .\nex:FastAPI ex:provides ex:DependsFeature .\nex:DependsFeature ex:implements ex:Pydantic .\n```\n\n### User Clarifies\n\"I need to visualize it for documentation, and also query relationships programmatically\"\n\n### Validated Translation\n\n**Implementation:** Neo4j graph database + Cypher queries + GraphViz export for visualization\n\n```python\nfrom neo4j import GraphDatabase\nimport graphviz\n\nclass KnowledgeGraph:\n    def __init__(self, uri, user, password):\n        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n\n    def add_concept(self, name, properties):\n        \"\"\"Add node to knowledge graph\"\"\"\n        with self.driver.session() as session:\n            session.run(\n                \"CREATE (n:Concept {name: $name, properties: $props})\",\n                name=name, props=properties\n            )\n\n    def add_relationship(self, from_concept, to_concept, relationship_type):\n        \"\"\"Add edge between nodes\"\"\"\n        with self.driver.session() as session:\n            session.run(\n                \"\"\"\n                MATCH (a:Concept {name: $from})\n                MATCH (b:Concept {name: $to})\n                CREATE (a)-[r:RELATES {type: $rel_type}]->(b)\n                \"\"\",\n                from=from_concept, to=to_concept, rel_type=relationship_type\n            )\n\n    def query_relationships(self, concept):\n        \"\"\"Find all relationships for a concept\"\"\"\n        with self.driver.session() as session:\n            result = session.run(\n                \"\"\"\n                MATCH (c:Concept {name: $name})-[r]->(related)\n                RETURN related.name AS concept, r.type AS relationship\n                \"\"\",\n                name=concept\n            )\n            return [(record[\"concept\"], record[\"relationship\"]) for record in result]\n\n    def export_visualization(self, output_path):\n        \"\"\"Export graph as GraphViz diagram\"\"\"\n        dot = graphviz.Digraph(comment='Knowledge Graph')\n\n        with self.driver.session() as session:\n            # Get all nodes\n            nodes = session.run(\"MATCH (n:Concept) RETURN n.name AS name\")\n            for record in nodes:\n                dot.node(record[\"name\"])\n\n            # Get all relationships\n            edges = session.run(\n                \"MATCH (a)-[r]->(b) RETURN a.name AS from, b.name AS to, r.type AS rel\"\n            )\n            for record in edges:\n                dot.edge(record[\"from\"], record[\"to\"], label=record[\"rel\"])\n\n        dot.render(output_path, format='png')\n\n# Usage\nkg = KnowledgeGraph(\"bolt://localhost:7687\", \"neo4j\", \"password\")\n\n# Add concepts\nkg.add_concept(\"FastAPI\", {\"type\": \"framework\", \"language\": \"python\"})\nkg.add_concept(\"Pydantic\", {\"type\": \"library\", \"purpose\": \"validation\"})\n\n# Add relationships\nkg.add_relationship(\"FastAPI\", \"Pydantic\", \"uses\")\n\n# Query programmatically\nrelationships = kg.query_relationships(\"FastAPI\")\n# [(\"Pydantic\", \"uses\")]\n\n# Export visualization for docs\nkg.export_visualization(\"knowledge_graph\")\n```\n\n**ASCII representation of resulting graph:**\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ FastAPI â”‚\nâ””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n     â”‚ uses\n     â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      implements      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Pydantic â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ Depends()  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”˜\n     â–²                                  â”‚\n     â”‚ validates                   provides\n     â”‚                                  â”‚\nâ”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”\nâ”‚ User Request â”‚                  â”‚ FastAPI â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n---\n\n## Example 3: \"use embeddings for retrieval\"\n\n### User Request\n\"I want to use embeddings for retrieval in my application\"\n\n### Ambiguity Detected\n**Ambiguous terms:** \"embeddings\" and \"retrieval\"\n\n**Possible interpretations for \"embeddings\":**\n1. **Vector embeddings** - Dense numerical representations (OpenAI, sentence-transformers)\n2. **Embedding layer** - Neural network layer that learns representations\n3. **Graph embeddings** - Node2vec, DeepWalk for graph nodes\n4. **Word embeddings** - Word2Vec, GloVe for individual words\n\n**Possible interpretations for \"retrieval\":**\n1. **Semantic search** - Similarity-based vector search\n2. **Graph traversal** - Following edges in knowledge graph\n3. **Hybrid retrieval** - Combining semantic + keyword + graph\n4. **SQL queries** - Traditional database lookup\n\n### Semantic Validation Process\n\n**Embedding types comparison:**\n\n| Type | Use Case | Example | Output |\n|------|----------|---------|--------|\n| **Vector embeddings** | Semantic similarity | \"FastAPI dependency injection\" | 1536-dim vector |\n| **Embedding layer** | Model training | First layer of neural network | Learned weights |\n| **Graph embeddings** | Node similarity | Embed \"FastAPI\" node in graph | 128-dim vector |\n| **Word embeddings** | NLP tasks | \"dependency\" â†’ vector | 300-dim vector |\n\n**Retrieval patterns comparison:**\n\n**1. Semantic Search (Vector Similarity):**\n```python\nfrom sentence_transformers import SentenceTransformer\nimport faiss\nimport numpy as np\n\n# Generate embeddings\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\ndocuments = [\"FastAPI uses Pydantic\", \"Django uses ORM\", \"Flask is lightweight\"]\ndoc_embeddings = model.encode(documents)\n\n# Build FAISS index\nindex = faiss.IndexFlatL2(doc_embeddings.shape[1])\nindex.add(np.array(doc_embeddings).astype('float32'))\n\n# Query\nquery = \"What framework uses Pydantic?\"\nquery_embedding = model.encode([query])\ndistances, indices = index.search(query_embedding.astype('float32'), k=1)\n\nprint(documents[indices[0][0]])  # \"FastAPI uses Pydantic\"\n```\n\n**2. Graph Traversal:**\n```cypher\n// Find concepts related to FastAPI through 2 hops\nMATCH path = (start:Concept {name: 'FastAPI'})-[*1..2]-(related)\nRETURN related.name, length(path)\nORDER BY length(path)\n```\n\n**3. Hybrid Retrieval (Semantic + Graph):**\n```python\ndef hybrid_retrieval(query, k=5):\n    # Step 1: Semantic search for candidates\n    semantic_results = vector_search(query, k=20)\n\n    # Step 2: Graph traversal for connected concepts\n    graph_results = []\n    for result in semantic_results:\n        connected = graph.get_neighbors(result.entity, max_depth=2)\n        graph_results.extend(connected)\n\n    # Step 3: Re-rank by combined score\n    combined = []\n    for entity in set(semantic_results + graph_results):\n        semantic_score = get_semantic_score(query, entity)\n        graph_score = get_graph_centrality(entity)\n        combined_score = 0.7 * semantic_score + 0.3 * graph_score\n        combined.append((entity, combined_score))\n\n    # Return top-k\n    return sorted(combined, key=lambda x: x[1], reverse=True)[:k]\n```\n\n### User Clarifies\n\"I need to find documents similar to a user's question, but also want to use the knowledge graph to expand the search to related concepts\"\n\n### Validated Translation\n\n**Implementation:** Hybrid retrieval combining vector embeddings + knowledge graph traversal\n\n```python\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom neo4j import GraphDatabase\nimport numpy as np\n\nclass HybridRetriever:\n    def __init__(self, vector_db, graph_db):\n        self.vector_db = vector_db\n        self.graph_db = graph_db\n        self.embeddings = OpenAIEmbeddings()\n\n    def retrieve(self, query, k=5, expansion_depth=1):\n        \"\"\"\n        Hybrid retrieval: semantic search + graph expansion\n\n        Args:\n            query: User question\n            k: Number of final results\n            expansion_depth: How many graph hops to expand\n        \"\"\"\n        # Step 1: Semantic search for initial candidates\n        semantic_results = self.vector_db.similarity_search_with_score(\n            query=query,\n            k=k * 2  # Get more candidates for expansion\n        )\n\n        # Step 2: Extract entities from semantic results\n        entities = self._extract_entities(semantic_results)\n\n        # Step 3: Graph expansion - find related concepts\n        expanded_entities = set(entities)\n        with self.graph_db.session() as session:\n            for entity in entities:\n                # Find concepts within expansion_depth hops\n                result = session.run(\n                    \"\"\"\n                    MATCH path = (start:Concept {name: $entity})-[*1..$depth]-(related)\n                    RETURN DISTINCT related.name AS concept,\n                           length(path) AS distance\n                    ORDER BY distance\n                    \"\"\",\n                    entity=entity,\n                    depth=expansion_depth\n                )\n                for record in result:\n                    expanded_entities.add(record[\"concept\"])\n\n        # Step 4: Re-rank all documents mentioning expanded entities\n        all_candidates = []\n        for entity in expanded_entities:\n            # Find documents containing this entity\n            entity_docs = self.vector_db.similarity_search(\n                query=entity,\n                k=5\n            )\n            all_candidates.extend(entity_docs)\n\n        # Step 5: Compute hybrid score\n        scored_docs = []\n        for doc in all_candidates:\n            # Semantic similarity score\n            doc_embedding = self.embeddings.embed_query(doc.page_content)\n            query_embedding = self.embeddings.embed_query(query)\n            semantic_score = np.dot(doc_embedding, query_embedding)\n\n            # Graph centrality score (how connected is this document's entities?)\n            graph_score = self._get_graph_score(doc, expanded_entities)\n\n            # Combined score (70% semantic, 30% graph)\n            combined_score = 0.7 * semantic_score + 0.3 * graph_score\n\n            scored_docs.append((doc, combined_score))\n\n        # Step 6: Return top-k unique documents\n        scored_docs.sort(key=lambda x: x[1], reverse=True)\n        unique_docs = []\n        seen_content = set()\n        for doc, score in scored_docs:\n            if doc.page_content not in seen_content:\n                unique_docs.append((doc, score))\n                seen_content.add(doc.page_content)\n            if len(unique_docs) >= k:\n                break\n\n        return unique_docs\n\n    def _extract_entities(self, semantic_results):\n        \"\"\"Extract named entities from semantic search results\"\"\"\n        entities = []\n        for doc, score in semantic_results:\n            # Simple entity extraction (could use NER model)\n            doc_entities = doc.metadata.get(\"entities\", [])\n            entities.extend(doc_entities)\n        return list(set(entities))\n\n    def _get_graph_score(self, doc, expanded_entities):\n        \"\"\"Score document based on graph connectivity\"\"\"\n        doc_entities = set(doc.metadata.get(\"entities\", []))\n        overlap = len(doc_entities.intersection(expanded_entities))\n        return overlap / len(expanded_entities) if expanded_entities else 0\n\n# Usage\nretriever = HybridRetriever(\n    vector_db=chroma_db,\n    graph_db=neo4j_driver\n)\n\nquery = \"How does FastAPI handle dependency injection?\"\nresults = retriever.retrieve(\n    query=query,\n    k=5,\n    expansion_depth=2  # Expand 2 hops in graph\n)\n\nfor doc, score in results:\n    print(f\"Score: {score:.3f}\")\n    print(f\"Content: {doc.page_content[:200]}...\")\n    print(f\"Entities: {doc.metadata.get('entities', [])}\\n\")\n```\n\n**How it works:**\n1. **Semantic search** finds topically similar documents (vector similarity)\n2. **Graph expansion** discovers related concepts (Depends() â†’ Pydantic â†’ validation)\n3. **Hybrid scoring** combines both signals (70% semantic, 30% graph connectivity)\n4. **Re-ranking** produces final results using both meaning and relationships\n\n---\n\n## Example 4: \"store this in a graph node\"\n\n### User Request\n\"Can you store this user profile information in a graph node?\"\n\n### Ambiguity Detected\n**Ambiguous term:** \"graph node\"\n\n**Possible interpretations:**\n1. **Knowledge graph node** - Entity in knowledge graph (Neo4j, property graph)\n2. **Memory node** - Episodic memory entry with timestamp/context\n3. **Compute node** - Server/machine in distributed system\n4. **Neural network node** - Neuron/unit in neural network\n5. **Data structure node** - Node in tree/linked list\n\n### Semantic Validation Process\n\n**Different node types in Memory Graphs context:**\n\n**1. Knowledge Graph Node (Entity):**\n```python\n# Neo4j property graph node\nCREATE (user:Person {\n    id: \"user123\",\n    name: \"Alice\",\n    email: \"alice@example.com\",\n    role: \"developer\"\n})\n```\n\n**2. Memory Node (Experience):**\n```python\n# Episodic memory entry\n{\n    \"id\": \"memory_20241118_001\",\n    \"type\": \"episodic\",\n    \"timestamp\": \"2024-11-18T14:30:00Z\",\n    \"event\": \"User created account\",\n    \"entities\": [\"Alice\", \"signup_flow\"],\n    \"context\": {\n        \"source\": \"web_app\",\n        \"session_id\": \"sess_abc123\"\n    },\n    \"content\": \"User Alice (alice@example.com) completed signup process\"\n}\n```\n\n**3. Semantic Concept Node:**\n```python\n# Conceptual knowledge node\n{\n    \"concept\": \"User\",\n    \"type\": \"semantic\",\n    \"properties\": {\n        \"definition\": \"Person who interacts with the system\",\n        \"attributes\": [\"id\", \"name\", \"email\", \"role\"],\n        \"relationships\": [\"belongs_to_organization\", \"has_permissions\"]\n    },\n    \"learned_from\": [\"documentation\", \"user_stories\"],\n    \"confidence\": 0.95\n}\n```\n\n**ASCII representation of different node types:**\n\n```\nKnowledge Graph Node:           Memory Node:                  Concept Node:\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Person: Alice  â”‚          â”‚  Event Memory    â”‚         â”‚  Concept: User   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ id: user123      â”‚          â”‚ timestamp: 14:30 â”‚         â”‚ type: semantic   â”‚\nâ”‚ email: alice@... â”‚          â”‚ action: signup   â”‚         â”‚ definition: ...  â”‚\nâ”‚ role: developer  â”‚â”€â”€â”       â”‚ context: web_app â”‚         â”‚ attributes: []   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n        â”‚             â”‚\n        â”‚ BELONGS_TO  â”‚\n        â–¼             â”‚\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  Org: Acme Inc   â”‚  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n                      â”‚\n        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n        â”‚ HAS_PERMISSION\n        â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Role: Developer â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n### User Clarifies\n\"I want to store the user as an entity in our knowledge graph, and link them to their organization and permissions\"\n\n### Validated Translation\n\n**Implementation:** Knowledge graph with entity nodes and relationship edges\n\n```python\nfrom neo4j import GraphDatabase\n\nclass UserKnowledgeGraph:\n    def __init__(self, uri, user, password):\n        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n\n    def create_user_node(self, user_data):\n        \"\"\"Create user entity node with properties\"\"\"\n        with self.driver.session() as session:\n            result = session.run(\n                \"\"\"\n                MERGE (u:Person {id: $id})\n                SET u.name = $name,\n                    u.email = $email,\n                    u.role = $role,\n                    u.created_at = datetime()\n                RETURN u\n                \"\"\",\n                id=user_data[\"id\"],\n                name=user_data[\"name\"],\n                email=user_data[\"email\"],\n                role=user_data[\"role\"]\n            )\n            return result.single()[\"u\"]\n\n    def link_user_to_organization(self, user_id, org_id):\n        \"\"\"Create relationship: User -[BELONGS_TO]-> Organization\"\"\"\n        with self.driver.session() as session:\n            session.run(\n                \"\"\"\n                MATCH (u:Person {id: $user_id})\n                MATCH (o:Organization {id: $org_id})\n                MERGE (u)-[r:BELONGS_TO {since: datetime()}]->(o)\n                RETURN r\n                \"\"\",\n                user_id=user_id,\n                org_id=org_id\n            )\n\n    def assign_permissions(self, user_id, permissions):\n        \"\"\"Create relationships: User -[HAS_PERMISSION]-> Permission\"\"\"\n        with self.driver.session() as session:\n            for permission in permissions:\n                session.run(\n                    \"\"\"\n                    MATCH (u:Person {id: $user_id})\n                    MERGE (p:Permission {name: $permission})\n                    MERGE (u)-[r:HAS_PERMISSION {granted_at: datetime()}]->(p)\n                    \"\"\",\n                    user_id=user_id,\n                    permission=permission\n                )\n\n    def get_user_graph(self, user_id):\n        \"\"\"Retrieve user node with all relationships\"\"\"\n        with self.driver.session() as session:\n            result = session.run(\n                \"\"\"\n                MATCH (u:Person {id: $user_id})\n                OPTIONAL MATCH (u)-[r1:BELONGS_TO]->(o:Organization)\n                OPTIONAL MATCH (u)-[r2:HAS_PERMISSION]->(p:Permission)\n                RETURN u,\n                       collect(DISTINCT {org: o.name, since: r1.since}) AS organizations,\n                       collect(DISTINCT {permission: p.name, granted: r2.granted_at}) AS permissions\n                \"\"\",\n                user_id=user_id\n            )\n            return result.single()\n\n# Usage\nkg = UserKnowledgeGraph(\"bolt://localhost:7687\", \"neo4j\", \"password\")\n\n# Create user node\nuser_data = {\n    \"id\": \"user123\",\n    \"name\": \"Alice\",\n    \"email\": \"alice@example.com\",\n    \"role\": \"developer\"\n}\nuser_node = kg.create_user_node(user_data)\n\n# Link to organization\nkg.link_user_to_organization(\"user123\", \"org_acme\")\n\n# Assign permissions\npermissions = [\"read_code\", \"write_code\", \"deploy_staging\"]\nkg.assign_permissions(\"user123\", permissions)\n\n# Retrieve full user graph\nuser_graph = kg.get_user_graph(\"user123\")\nprint(f\"User: {user_graph['u']['name']}\")\nprint(f\"Organizations: {user_graph['organizations']}\")\nprint(f\"Permissions: {user_graph['permissions']}\")\n```\n\n**Resulting knowledge graph structure:**\n```\n     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n     â”‚  Person: Alice      â”‚\n     â”‚  id: user123        â”‚\n     â”‚  role: developer    â”‚\n     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                â”‚\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â”‚                       â”‚\n    â”‚ BELONGS_TO            â”‚ HAS_PERMISSION\n    â–¼                       â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Org: Acme   â”‚      â”‚ read_code      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n                     â”‚ write_code     â”‚\n                     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n                     â”‚ deploy_staging â”‚\n                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n---\n\n## Example 5: \"improve memory retrieval performance\"\n\n### User Request\n\"Our memory retrieval is too slow, how can we improve performance?\"\n\n### Ambiguity Detected\n**Ambiguous context:** \"memory retrieval\" could involve multiple performance bottlenecks\n\n**Possible bottlenecks:**\n1. **Vector search performance** - Slow similarity search in high-dimensional space\n2. **Graph traversal performance** - Slow Neo4j queries with deep path searches\n3. **Embedding generation** - Slow model inference for query encoding\n4. **Index efficiency** - Missing or poorly configured database indexes\n5. **Network latency** - Slow database connection or data transfer\n\n### Semantic Validation Process\n\n**Performance optimization strategies by bottleneck:**\n\n**1. Vector Search Optimization:**\n\nSlow approach (Exact search):\n```python\n# Brute-force cosine similarity - O(n*d) for n documents, d dimensions\ndef slow_search(query_embedding, all_embeddings):\n    similarities = []\n    for doc_embedding in all_embeddings:\n        sim = cosine_similarity(query_embedding, doc_embedding)\n        similarities.append(sim)\n    return np.argsort(similarities)[-5:]  # Top 5\n```\n\nFast approach (Approximate Nearest Neighbors):\n```python\nimport faiss\n\n# Build HNSW index - O(log n) search time\ndimension = 1536  # OpenAI embedding dimension\nindex = faiss.IndexHNSWFlat(dimension, 32)  # 32 = M parameter\n\n# Add vectors\nindex.add(all_embeddings.astype('float32'))\n\n# Fast search\ndistances, indices = index.search(query_embedding, k=5)\n```\n\n**2. Graph Traversal Optimization:**\n\nSlow query (No index, variable-length path):\n```cypher\n// Slow: Explores all paths up to 5 hops\nMATCH path = (start:Person {id: 'user123'})-[*1..5]-(related)\nRETURN related\n```\n\nFast query (Indexed lookup, limited depth):\n```cypher\n// Fast: Index on id, limited to 2 hops\nCREATE INDEX person_id FOR (p:Person) ON (p.id)\n\nMATCH path = (start:Person {id: 'user123'})-[*1..2]-(related)\nWHERE related:Person OR related:Organization\nRETURN related\nLIMIT 20\n```\n\n**3. Embedding Caching:**\n\nSlow (Re-encode every query):\n```python\ndef search(query_text):\n    query_embedding = model.encode([query_text])  # Slow: 50-100ms\n    results = index.search(query_embedding, k=5)\n    return results\n```\n\nFast (Cache frequent queries):\n```python\nfrom functools import lru_cache\n\n@lru_cache(maxsize=1000)\ndef get_cached_embedding(query_text):\n    return model.encode([query_text])\n\ndef search(query_text):\n    query_embedding = get_cached_embedding(query_text)  # Fast: cache hit ~0.1ms\n    results = index.search(query_embedding, k=5)\n    return results\n```\n\n**4. Batch Processing:**\n\nSlow (Sequential):\n```python\n# Process one query at a time\nfor query in queries:\n    embedding = model.encode([query])  # Multiple model calls\n    results = search(embedding)\n```\n\nFast (Batched):\n```python\n# Batch encode all queries at once\nembeddings = model.encode(queries)  # Single batched model call\nresults = [search(emb) for emb in embeddings]\n```\n\n### User Clarifies\n\"We're using FAISS for vector search, but the searches are taking 200-300ms for our 1M document collection. We need to get this under 50ms.\"\n\n### Validated Translation\n\n**Implementation:** Optimized FAISS index with GPU acceleration and quantization\n\n```python\nimport faiss\nimport numpy as np\n\nclass OptimizedVectorRetriever:\n    def __init__(self, dimension=1536, use_gpu=True):\n        self.dimension = dimension\n        self.use_gpu = use_gpu and faiss.get_num_gpus() > 0\n\n        # Build optimized index\n        self.index = self._build_index()\n\n    def _build_index(self):\n        \"\"\"\n        Build optimized FAISS index for fast retrieval\n\n        Performance targets:\n        - <50ms search time for 1M vectors\n        - <10GB memory usage\n        - >90% recall@10\n        \"\"\"\n        # Step 1: Use IVF (Inverted File Index) for fast search\n        # Divide space into n_clusters cells, search only relevant cells\n        n_clusters = 1024  # sqrt(1M) = 1000, round up to 1024\n\n        # Step 2: Use Product Quantization to compress vectors\n        # Reduces memory 8x with minimal accuracy loss\n        m = 8  # Split vector into 8 sub-vectors\n        bits = 8  # 8 bits per sub-vector\n\n        # Build index: IVF1024 + PQ8\n        quantizer = faiss.IndexFlatL2(self.dimension)\n        index = faiss.IndexIVFPQ(\n            quantizer,\n            self.dimension,\n            n_clusters,  # Number of cells\n            m,           # Number of sub-vectors\n            bits         # Bits per sub-vector\n        )\n\n        # Step 3: Move to GPU if available (10x speedup)\n        if self.use_gpu:\n            res = faiss.StandardGpuResources()\n            index = faiss.index_cpu_to_gpu(res, 0, index)\n\n        return index\n\n    def train_and_add(self, embeddings):\n        \"\"\"Train index and add vectors\"\"\"\n        embeddings = embeddings.astype('float32')\n\n        # IVF indexes need training on representative sample\n        training_sample = embeddings[:100000]  # Use 100k for training\n        self.index.train(training_sample)\n\n        # Add all vectors\n        self.index.add(embeddings)\n\n        print(f\"Index trained and populated with {self.index.ntotal} vectors\")\n\n    def search(self, query_embedding, k=5, nprobe=8):\n        \"\"\"\n        Fast search with tunable accuracy/speed tradeoff\n\n        Args:\n            query_embedding: Query vector\n            k: Number of results\n            nprobe: Number of cells to search (higher = more accurate but slower)\n                   Default 8 gives good balance\n                   Use 1 for fastest (~20ms), 32 for most accurate (~80ms)\n        \"\"\"\n        # Set number of cells to search\n        self.index.nprobe = nprobe\n\n        # Search\n        query_embedding = query_embedding.astype('float32')\n        distances, indices = self.index.search(query_embedding, k)\n\n        return distances, indices\n\n    def benchmark(self, test_queries, ground_truth_indices):\n        \"\"\"Benchmark search performance\"\"\"\n        import time\n\n        times = []\n        recalls = []\n\n        for nprobe in [1, 2, 4, 8, 16, 32]:\n            self.index.nprobe = nprobe\n\n            start = time.time()\n            for query in test_queries:\n                _, indices = self.search(query, k=10, nprobe=nprobe)\n            elapsed = (time.time() - start) / len(test_queries) * 1000  # ms per query\n\n            # Calculate recall@10\n            recall = np.mean([\n                len(set(pred[:10]) & set(true[:10])) / 10\n                for pred, true in zip(indices, ground_truth_indices)\n            ])\n\n            times.append(elapsed)\n            recalls.append(recall)\n\n            print(f\"nprobe={nprobe:2d}: {elapsed:5.1f}ms, recall@10={recall:.3f}\")\n\n        return times, recalls\n\n# Usage\nretriever = OptimizedVectorRetriever(dimension=1536, use_gpu=True)\n\n# Load 1M embeddings\nembeddings = np.random.randn(1_000_000, 1536).astype('float32')  # Replace with actual embeddings\n\n# Train and populate index\nretriever.train_and_add(embeddings)\n\n# Fast search (<50ms with nprobe=8)\nquery = np.random.randn(1, 1536).astype('float32')\ndistances, indices = retriever.search(query, k=5, nprobe=8)\n\nprint(f\"Top 5 results: {indices[0]}\")\nprint(f\"Distances: {distances[0]}\")\n\n# Benchmark to find optimal nprobe\ntest_queries = np.random.randn(100, 1536).astype('float32')\nground_truth = np.random.randint(0, 1_000_000, (100, 10))  # Replace with actual ground truth\nretriever.benchmark(test_queries, ground_truth)\n```\n\n**Performance improvements:**\n\n| Approach | Search Time (1M docs) | Memory Usage | Recall@10 |\n|----------|----------------------|--------------|-----------|\n| Brute force (baseline) | 800ms | 6GB | 100% |\n| FAISS Flat (exact) | 300ms | 6GB | 100% |\n| FAISS IVF (nprobe=1) | 20ms | 6GB | 60% |\n| **FAISS IVFPQ (nprobe=8)** | **45ms** | **750MB** | **92%** |\n| FAISS IVFPQ + GPU | 12ms | 750MB | 92% |\n\n**Why this works:**\n- **IVF (Inverted File)**: Divides search space into cells, only searches relevant cells\n- **PQ (Product Quantization)**: Compresses vectors 8x, reduces memory and speeds up distance computation\n- **GPU acceleration**: Parallelizes distance calculations for 3-4x speedup\n- **Tunable nprobe**: Trade accuracy for speed (nprobe=8 gives 92% recall in 45ms)\n\n---\n\n## Pattern Summary\n\nCommon ambiguities in **Memory Graphs** domain and their translations:\n\n| Ambiguous Term | Common Interpretations | Context Questions | Validated Translation |\n|---------------|------------------------|-------------------|----------------------|\n| **\"memory\"** | Knowledge graph, Vector DB, Episodic memory, Semantic memory, RAM | What will you query? How will you search? | Knowledge graph (relationships), Vector DB (similarity), Episodic (events), Semantic (facts) |\n| **\"graph\"** | Graph database, Visualization, RDF triples, In-memory structure | Do you need to query relationships? Visualize? Store semantically? | Neo4j (queryable), GraphViz (visualization), NetworkX (in-memory), RDF (semantic web) |\n| **\"embedding\"** | Vector embedding, Embedding layer, Graph embedding, Word embedding | What are you embedding? Text? Nodes? Words? | Sentence-transformers (text), Node2vec (graphs), Word2Vec (words) |\n| **\"retrieval\"** | Semantic search, Graph traversal, Hybrid, SQL queries | Search by meaning? Relationships? Both? | FAISS (semantic), Cypher (graph), Hybrid (combined), SQL (structured) |\n| **\"node\"** | Graph entity, Memory entry, Compute node, Neural node | What does the node represent? User? Event? Server? | Neo4j entity (graph), Memory record (episodic), Server (infrastructure) |\n\n**Key disambiguation patterns:**\n1. **Memory type** determines storage structure (graph, vector, episodic, semantic)\n2. **Retrieval pattern** determines search algorithm (semantic, graph, hybrid, SQL)\n3. **Node type** determines properties and relationships (entity, memory, concept)\n4. **Performance requirements** determine index type (exact, approximate, quantized, GPU)\n5. **Use case** determines architecture (knowledge graph, vector DB, hybrid, in-memory)\n",
        "plugins/semantic-linguist/skills/semantic-translation/examples/sre-mappings.md": "# SRE-Specific Ambiguity Resolutions\n\nReal-world examples of ambiguous user requests mapped to precise SRE (Site Reliability Engineering) implementations.\n\n## Overview\n\nSRE is a discipline that applies software engineering principles to infrastructure and operations. Key concepts include:\n- **Observability pillars**: Logs (events), Metrics (measurements), Traces (request paths)\n- **Service levels**: SLI (indicators), SLO (objectives), SLA (agreements)\n- **Incident management**: SEV levels, on-call rotations, postmortems\n- **Reliability practices**: Error budgets, toil reduction, capacity planning\n\nCommon ambiguities arise when users use general terms like \"observability\", \"SLI\", \"incident\", \"monitoring\", or \"on-call\" without SRE-specific context.\n\n---\n\n## Example 1: \"set up observability\"\n\n### User Request\n```\n\"We need to set up observability for our service\"\n```\n\n### Ambiguity Detected\n- \"observability\" - which pillar(s)? (logs, metrics, traces)\n- Need to clarify what aspects of system visibility are needed\n\n### Semantic Validation Process\n\n**Step 1: Domain Identified**\n- User working on SRE/operations\n- Needs system visibility\n\n**Step 2: Query Knowledge**\n- Observability has three pillars with different purposes\n- Each pillar answers different questions\n\n**Step 3: Present Options**\n```\nObservability consists of three pillars, each serving different needs:\n\n1. **Logs** (discrete events)\n   - What: Individual event records with details\n   - When: Happened at specific timestamp\n   - Tools: ELK stack, Splunk, Loki, CloudWatch Logs\n   - Use case: Debugging, audit trails, error investigation\n   - Example: \"User X failed login at 14:23:15 due to invalid password\"\n   - Questions answered: What happened? When? Who? Why did it fail?\n\n2. **Metrics** (measurements over time)\n   - What: Numerical data points aggregated\n   - When: Time-series data (rate, duration, count)\n   - Tools: Prometheus, Grafana, Datadog, CloudWatch Metrics\n   - Use case: Performance monitoring, alerting, capacity planning\n   - Example: \"Average response time: 234ms, Error rate: 0.3%\"\n   - Questions answered: How fast? How often? Trending up or down?\n\n3. **Traces** (request journeys)\n   - What: End-to-end request path through distributed system\n   - When: Entire lifecycle of single request\n   - Tools: Jaeger, Zipkin, AWS X-Ray, Honeycomb\n   - Use case: Latency analysis, bottleneck identification, microservices debugging\n   - Example: \"Request spent 45ms in API, 120ms in database, 10ms in cache\"\n   - Questions answered: Where is the bottleneck? Which service is slow?\n\nWhich observability questions do you need to answer?\"\n```\n\n**Step 4: User Clarifies**\n```\nUser: \"We're seeing slow response times and need to find where the bottleneck is\"\n```\n\n**Step 5: Validated Translation**\n```\n\"That's distributed tracing! Here's how to set it up:\n\n**1. Instrument application for tracing:**\n```python\n# Example: FastAPI with OpenTelemetry tracing\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.exporter.jaeger.thrift import JaegerExporter\nfrom opentelemetry.instrumentation.fastapi import FastAPIInstrumentor\n\n# Initialize tracer\ntrace.set_tracer_provider(TracerProvider())\ntracer = trace.get_tracer(__name__)\n\n# Export to Jaeger\njaeger_exporter = JaegerExporter(\n    agent_host_name=\"localhost\",\n    agent_port=6831,\n)\ntrace.get_tracer_provider().add_span_processor(\n    BatchSpanProcessor(jaeger_exporter)\n)\n\n# Instrument FastAPI (auto-traces HTTP requests)\nFastAPIInstrumentor.instrument_app(app)\n\n# Manual span for custom operations\n@app.get(\"/users/{user_id}\")\nasync def get_user(user_id: int):\n    with tracer.start_as_current_span(\"fetch_user\") as span:\n        span.set_attribute(\"user_id\", user_id)\n\n        # Database operation (auto-instrumented)\n        user = await db.query(User).filter(User.id == user_id).first()\n\n        # External API call (manual span)\n        with tracer.start_as_current_span(\"fetch_profile_picture\"):\n            profile_url = await get_profile_from_cdn(user.profile_id)\n\n        return {\"user\": user, \"profile_url\": profile_url}\n```\n\n**2. Deploy tracing backend (Jaeger example):**\n```bash\n# Docker Compose for Jaeger\nversion: '3'\nservices:\n  jaeger:\n    image: jaegertracing/all-in-one:latest\n    ports:\n      - \"6831:6831/udp\"  # Receive traces\n      - \"16686:16686\"    # UI\n    environment:\n      - COLLECTOR_ZIPKIN_HOST_PORT=:9411\n```\n\n**3. Analyze traces in Jaeger UI:**\n- Access http://localhost:16686\n- Search for traces with high duration\n- Identify span with longest duration (bottleneck)\n\n**Example trace breakdown:**\n```\nRequest: GET /users/123\nTotal duration: 275ms\n\nSpans:\n  1. HTTP Request (275ms total)\n     â”œâ”€ fetch_user (250ms)\n     â”‚  â”œâ”€ database_query (240ms) â† BOTTLENECK!\n     â”‚  â””â”€ fetch_profile_picture (8ms)\n     â””â”€ response_serialization (2ms)\n```\n\n**Result:** Database query is bottleneck (240ms of 275ms)\n**Action:** Add database index, optimize query, or cache results\n\n**When to use each pillar:**\n- Logs â†’ \"What error message did user see?\"\n- Metrics â†’ \"Is API getting slower over time?\"\n- Traces â†’ \"Where exactly is the slowdown?\" (your use case)\"\n```\n\n---\n\n## Example 2: \"define SLI for our service\"\n\n### User Request\n```\n\"We need to define SLIs for our API\"\n```\n\n### Ambiguity Detected\n- \"SLI\" - which metric? (availability, latency, throughput, error rate)\n- Need to clarify what reliability aspect to measure\n\n### Semantic Validation Process\n\n**Step 1: Explain SLI Types**\n```\nSLI (Service Level Indicator) is a quantitative measure of service reliability. Common SLI types:\n\n1. **Availability SLI** (uptime percentage)\n   - Measures: Successful requests / Total requests\n   - Example: 99.9% of requests succeed (not 5xx errors)\n   - Formula: (Total requests - Failed requests) / Total requests Ã— 100\n   - Use case: Service must be accessible\n\n2. **Latency SLI** (response time)\n   - Measures: Percentage of requests under target latency\n   - Example: 95% of requests complete in < 200ms\n   - Formula: (Requests < 200ms) / Total requests Ã— 100\n   - Use case: Service must be fast\n\n3. **Throughput SLI** (capacity)\n   - Measures: Requests processed per second\n   - Example: Service handles â‰¥ 1000 requests/second\n   - Formula: Total requests / Time period\n   - Use case: Service must handle load\n\n4. **Error Rate SLI** (failure percentage)\n   - Measures: Failed requests / Total requests\n   - Example: < 0.1% of requests return errors\n   - Formula: Failed requests / Total requests Ã— 100\n   - Use case: Service must be reliable\n\nWhat matters most for your API:\n- Uptime â†’ **Availability SLI**\n- Speed â†’ **Latency SLI**\n- Capacity â†’ **Throughput SLI**\n- Correctness â†’ **Error Rate SLI**\"\n```\n\n**Step 2: User Clarifies**\n```\nUser: \"Our users complain when API is slow, speed matters most\"\n```\n\n**Step 3: Validated Translation**\n```\n\"That's a Latency SLI! Here's how to define and measure it:\n\n**1. Define latency SLI with percentile:**\n```yaml\n# SLI Definition\nService: User API\nMetric: Latency\nTarget: 95th percentile (p95) of requests complete in < 200ms\nMeasurement window: 30-day rolling window\n\nWhy p95 (not average)?\n- Average hides outliers (10% slow requests = still good average)\n- p95 means 95% of users have fast experience\n- Captures tail latency problems\n```\n\n**2. Query metrics (Prometheus example):**\n```promql\n# Calculate p95 latency\nhistogram_quantile(\n  0.95,\n  rate(http_request_duration_seconds_bucket{job=\"user-api\"}[5m])\n)\n\n# Count requests under 200ms\nsum(rate(http_request_duration_seconds_bucket{\n  job=\"user-api\",\n  le=\"0.2\"  # 200ms\n}[5m]))\n\n# Calculate SLI percentage\n(\n  sum(rate(http_request_duration_seconds_bucket{\n    job=\"user-api\",\n    le=\"0.2\"\n  }[5m]))\n  /\n  sum(rate(http_request_duration_seconds_count{\n    job=\"user-api\"\n  }[5m]))\n) * 100\n\n# Result: 96.2% of requests < 200ms\n```\n\n**3. Define SLO (objective) from SLI:**\n```yaml\n# SLO (Service Level Objective)\nSLI: Latency (p95 < 200ms)\nTarget: 95% of requests meet SLI\nMeasurement: 30-day rolling window\nError budget: 5% of requests can exceed 200ms\n\nMath:\n- Total requests/month: 10 million\n- Error budget: 5% = 500,000 slow requests allowed\n- Burn rate: How fast we consume error budget\n```\n\n**4. Alert when SLO at risk:**\n```yaml\n# Prometheus alert rule\ngroups:\n  - name: slo_alerts\n    rules:\n      - alert: LatencySLOBreach\n        expr: |\n          (\n            sum(rate(http_request_duration_seconds_bucket{\n              job=\"user-api\",\n              le=\"0.2\"\n            }[30d]))\n            /\n            sum(rate(http_request_duration_seconds_count{\n              job=\"user-api\"\n            }[30d]))\n          ) * 100 < 95\n        for: 1h\n        annotations:\n          summary: \"Latency SLO breached - p95 latency target not met\"\n          description: \"Only {{ $value }}% of requests under 200ms (target: 95%)\"\n```\n\n**5. Track error budget:**\n```python\n# Error budget tracking\ncurrent_sli = 96.2  # % of requests under 200ms\nslo_target = 95.0\nerror_budget_remaining = current_sli - slo_target  # 1.2% budget left\n\n# With 10M requests/month:\ntotal_requests = 10_000_000\nbudget_consumed = (100 - current_sli) / 100 * total_requests\n# = 380,000 slow requests consumed\n# = 500,000 - 380,000 = 120,000 slow requests remaining\n```\n\n**SLI vs SLO vs SLA:**\n- **SLI** = Measurement (how fast service is)\n- **SLO** = Internal target (95% under 200ms)\n- **SLA** = Customer contract (99% uptime or refund)\"\n```\n\n---\n\n## Example 3: \"we have an incident\"\n\n### User Request\n```\n\"We have an incident, the API is down\"\n```\n\n### Ambiguity Detected\n- \"incident\" - severity level? (SEV-1 critical vs SEV-2 vs alert)\n- Need to clarify impact and response urgency\n\n### Semantic Validation Process\n\n**Step 1: Clarify Severity**\n```\nSRE incidents have severity levels determining response:\n\n1. **SEV-1 (Critical Incident)**\n   - Impact: Complete outage, all users affected\n   - Example: API returning 100% errors, database down, total service failure\n   - Response: Immediate, page on-call, all hands on deck\n   - Timeline: Acknowledge < 5 min, update every 30 min\n\n2. **SEV-2 (Major Incident)**\n   - Impact: Partial outage, significant user subset affected\n   - Example: One region down, degraded performance (< 50% capacity)\n   - Response: Urgent, page on-call, gather response team\n   - Timeline: Acknowledge < 15 min, update every hour\n\n3. **SEV-3 (Minor Incident)**\n   - Impact: Limited impact, small user subset or non-critical feature\n   - Example: Search feature slow, admin panel inaccessible\n   - Response: Normal hours, create ticket, investigate next business day\n   - Timeline: Acknowledge < 1 hour, resolve within SLA\n\n4. **Alert (Not incident yet)**\n   - Impact: Warning signal, potential issue, no user impact yet\n   - Example: High CPU (80%), elevated error rate (0.5% vs 0.1% baseline)\n   - Response: Investigate, monitor, create task if needed\n   - Timeline: Review within 1 hour, escalate if degrades\n\nWhat's the impact:\n- All users cannot use service â†’ **SEV-1**\n- Half of users affected â†’ **SEV-2**\n- Small feature broken â†’ **SEV-3**\n- Metrics elevated but users unaffected â†’ **Alert**\"\n```\n\n**Step 2: User Clarifies**\n```\nUser: \"API is completely down, all requests return 503, all customers impacted\"\n```\n\n**Step 3: Validated Translation**\n```\n\"That's a SEV-1 Critical Incident! Here's the response protocol:\n\n**1. Declare incident (immediately):**\n```bash\n# Slack incident bot or command\n/incident declare\nSeverity: SEV-1\nTitle: API Complete Outage - All Requests 503\nImpact: All customers unable to access service\nIncident Commander: @oncall-sre\n\n# Auto-creates:\n# - Incident channel: #incident-2024-12-29-001\n# - Status page update: \"Investigating major outage\"\n# - Page on-call: SMS/phone to on-call engineer\n```\n\n**2. Assemble incident response team:**\n```\nRoles:\n- Incident Commander (IC): @oncall-sre (coordinates response)\n- Tech Lead: @backend-lead (technical investigation)\n- Communications: @support-lead (customer updates)\n- Scribe: @junior-sre (documents timeline)\n\nIncident channel: #incident-2024-12-29-001\nWar room: Zoom link auto-generated\n```\n\n**3. Immediate actions (first 5 minutes):**\n```bash\n# IC: Gather information\n- When started: Check monitoring alerts\n- Recent changes: Check deployment history\n- Current state: Verify all regions affected\n\n# Tech Lead: Quick diagnosis\nkubectl get pods -n production  # Check pod status\nkubectl logs api-deployment-xyz  # Check application logs\nkubectl describe pod api-xyz     # Check events\n\n# Result: All pods crashing with \"Database connection timeout\"\n```\n\n**4. Mitigation (restore service):**\n```bash\n# Hypothesis: Recent deployment broke database connection\n\n# Rollback deployment (fastest mitigation)\nkubectl rollout undo deployment/api-deployment\n\n# Monitor recovery\nwatch kubectl get pods -n production\n\n# Verify API responding\ncurl https://api.example.com/health\n# 200 OK - Service restored\n\n# Elapsed time: 12 minutes from detection to recovery\n```\n\n**5. Communication timeline:**\n```\n[00:00] Incident detected (monitoring alert)\n[00:05] SEV-1 declared, IC assigned\n[00:08] Status page: \"Investigating API outage affecting all users\"\n[00:12] Mitigation deployed (rollback)\n[00:15] Status page: \"Mitigation in progress, service partially restored\"\n[00:20] Service fully recovered\n[00:25] Status page: \"Incident resolved, all services operational\"\n[00:30] Internal update: \"Root cause: deployment X broke DB connection pool config\"\n```\n\n**6. Post-incident (after resolution):**\n```markdown\n# Incident postmortem (blameless)\n\n## Timeline\n- 14:23 UTC: Deployment 1.2.5 completed\n- 14:27 UTC: First 503 errors detected\n- 14:30 UTC: Monitoring alerts triggered\n- 14:32 UTC: SEV-1 declared\n- 14:40 UTC: Rollback initiated\n- 14:44 UTC: Service restored\n\n## Root Cause\nDeployment 1.2.5 changed database connection pool config from 100 to 10 connections,\ncausing connection exhaustion under normal load.\n\n## Impact\n- Duration: 17 minutes complete outage\n- Users affected: 100% of users (all regions)\n- Requests failed: ~150,000 requests returned 503\n\n## Action Items\n- [ ] Add integration test for DB connection pool under load\n- [ ] Implement gradual rollout (canary deployments)\n- [ ] Add alert for connection pool exhaustion\n- [ ] Review all config changes in deployment pipeline\n```\n\n**SEV-1 response checklist:**\n- [ ] Incident declared within 5 minutes\n- [ ] IC assigned, roles defined\n- [ ] War room established\n- [ ] Customer communication started (status page)\n- [ ] Mitigation prioritized over root cause analysis\n- [ ] Updates every 30 minutes\n- [ ] Postmortem scheduled (within 48 hours)\"\n```\n\n---\n\n## Example 4: \"set up monitoring\"\n\n### User Request\n```\n\"We need to set up monitoring for our application\"\n```\n\n### Ambiguity Detected\n- \"monitoring\" - passive monitoring vs active observability vs alerting\n- Need to clarify what visibility and response capability is needed\n\n### Semantic Validation Process\n\n**Step 1: Distinguish Monitoring Approaches**\n```\n\"Monitoring\" can mean different things in SRE:\n\n1. **Passive Monitoring** (collect and display)\n   - What: Dashboards showing metrics\n   - Purpose: Visibility into system state\n   - Tools: Grafana dashboards, CloudWatch dashboards\n   - Use case: Engineers check dashboards reactively\n   - Example: \"Let me check the dashboard to see if API is healthy\"\n   - Limitation: Requires someone to look\n\n2. **Active Observability** (query and investigate)\n   - What: Flexible querying of logs, metrics, traces\n   - Purpose: Investigate issues and understand behavior\n   - Tools: Prometheus queries, Elasticsearch, Honeycomb\n   - Use case: Deep-dive into specific issues\n   - Example: \"Why did user X see slow response at 14:23?\"\n   - Benefit: Answers specific questions\n\n3. **Alerting** (automated notifications)\n   - What: Automated alerts when thresholds breached\n   - Purpose: Proactive notification of problems\n   - Tools: Prometheus Alertmanager, PagerDuty, Opsgenie\n   - Use case: Wake up on-call when service degraded\n   - Example: \"Alert: Error rate > 1% for 5 minutes\"\n   - Benefit: Don't need to watch dashboards\n\nWhat's your goal:\n- See current status â†’ **Passive Monitoring**\n- Investigate problems â†’ **Active Observability**\n- Get notified of problems â†’ **Alerting**\"\n```\n\n**Step 2: User Clarifies**\n```\nUser: \"We want to be notified automatically when something goes wrong, not manually check\"\n```\n\n**Step 3: Validated Translation**\n```\n\"That's Alerting! Here's how to set it up properly:\n\n**1. Define alert conditions (Prometheus example):**\n```yaml\n# prometheus-alerts.yml\ngroups:\n  - name: api_alerts\n    rules:\n      # High error rate alert\n      - alert: HighErrorRate\n        expr: |\n          (\n            sum(rate(http_requests_total{status=~\"5..\"}[5m]))\n            /\n            sum(rate(http_requests_total[5m]))\n          ) * 100 > 1\n        for: 5m  # Must be true for 5 minutes (avoid flapping)\n        labels:\n          severity: critical\n          team: backend\n        annotations:\n          summary: \"High API error rate detected\"\n          description: \"Error rate is {{ $value }}% (threshold: 1%)\"\n\n      # High latency alert\n      - alert: HighLatency\n        expr: |\n          histogram_quantile(0.95,\n            rate(http_request_duration_seconds_bucket[5m])\n          ) > 0.5\n        for: 10m\n        labels:\n          severity: warning\n          team: backend\n        annotations:\n          summary: \"API latency exceeds threshold\"\n          description: \"p95 latency is {{ $value }}s (threshold: 0.5s)\"\n\n      # Service down alert\n      - alert: ServiceDown\n        expr: up{job=\"api\"} == 0\n        for: 1m\n        labels:\n          severity: critical\n          team: sre\n        annotations:\n          summary: \"API service is down\"\n          description: \"API has been unavailable for 1 minute\"\n```\n\n**2. Configure Alertmanager (routing and notifications):**\n```yaml\n# alertmanager.yml\nglobal:\n  slack_api_url: 'https://hooks.slack.com/services/XXX'\n  pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'\n\nroute:\n  receiver: 'team-default'\n  group_by: ['alertname', 'cluster']\n  group_wait: 30s      # Wait to batch alerts\n  group_interval: 5m   # Re-send interval\n  repeat_interval: 4h  # Re-send if still firing\n\n  routes:\n    # Critical alerts â†’ PagerDuty (page on-call)\n    - match:\n        severity: critical\n      receiver: 'pagerduty-critical'\n      continue: true  # Also send to Slack\n\n    # Critical alerts â†’ Slack\n    - match:\n        severity: critical\n      receiver: 'slack-critical'\n\n    # Warning alerts â†’ Slack only (no page)\n    - match:\n        severity: warning\n      receiver: 'slack-warnings'\n\nreceivers:\n  - name: 'pagerduty-critical'\n    pagerduty_configs:\n      - service_key: 'YOUR_PAGERDUTY_KEY'\n        description: '{{ .GroupLabels.alertname }}: {{ .CommonAnnotations.summary }}'\n\n  - name: 'slack-critical'\n    slack_configs:\n      - channel: '#alerts-critical'\n        title: 'ğŸš¨ {{ .GroupLabels.alertname }}'\n        text: '{{ .CommonAnnotations.description }}'\n\n  - name: 'slack-warnings'\n    slack_configs:\n      - channel: '#alerts-warnings'\n        title: 'âš ï¸ {{ .GroupLabels.alertname }}'\n        text: '{{ .CommonAnnotations.description }}'\n```\n\n**3. Define on-call rotation (PagerDuty example):**\n```\nSchedule: \"Backend SRE On-Call\"\nRotation:\n  - Week 1: Alice (primary), Bob (backup)\n  - Week 2: Bob (primary), Charlie (backup)\n  - Week 3: Charlie (primary), Alice (backup)\n\nEscalation policy:\n  1. Page primary on-call â†’ Wait 5 minutes\n  2. If no ACK â†’ Page backup on-call â†’ Wait 5 minutes\n  3. If no ACK â†’ Page SRE manager\n\nContact methods:\n  - Push notification\n  - SMS\n  - Phone call\n```\n\n**4. Alert best practices:**\n\n**Good alert (actionable):**\n```yaml\n- alert: HighDatabaseConnections\n  expr: db_connections > 90\n  annotations:\n    summary: \"Database connection pool near exhaustion\"\n    description: \"{{ $value }} connections used (max: 100)\"\n    runbook: \"https://wiki.company.com/runbooks/db-connections\"\n    dashboard: \"https://grafana.company.com/d/db-overview\"\n```\n\n**Bad alert (not actionable):**\n```yaml\n- alert: CPUHigh\n  expr: cpu_usage > 50\n  # Problem: 50% is not necessarily bad, no context\n```\n\n**Alert severity guidelines:**\n- **Critical** = User-facing impact, immediate response, page on-call\n- **Warning** = Potential issue, investigate during business hours, Slack notification\n- **Info** = Logging/metrics, no notification\n\n**Avoid alert fatigue:**\n- Set thresholds based on SLO impact, not arbitrary numbers\n- Use `for:` duration to avoid flapping (require sustained condition)\n- Aggregate similar alerts (don't alert per pod, alert per service)\n- Include runbook links for common issues\"\n```\n\n---\n\n## Example 5: \"on-call rotation\"\n\n### User Request\n```\n\"How should we handle on-call for our team?\"\n```\n\n### Ambiguity Detected\n- \"on-call\" - rotation schedule vs on-call responsibilities vs escalation\n- Need to clarify on-call structure\n\n### Semantic Validation Process\n\n**Step 1: Explain On-Call Components**\n```\nOn-call systems have three components:\n\n1. **On-Call Rotation** (who is on-call when)\n   - Primary on-call: First responder for incidents\n   - Backup on-call: Escalation if primary unavailable\n   - Rotation frequency: Weekly, bi-weekly, monthly\n   - Example: Alice (primary week 1), Bob (primary week 2)\n\n2. **On-Call Responsibilities** (what on-call engineer does)\n   - Respond to alerts/pages within SLA (e.g., 15 minutes)\n   - Triage incidents, determine severity\n   - Mitigate issues or escalate to specialists\n   - Document incident timeline\n   - Example: \"On-call ACKs alert, investigates, rolls back bad deployment\"\n\n3. **Escalation Policy** (when to escalate)\n   - Level 1: Primary on-call\n   - Level 2: Backup on-call (if primary doesn't ACK in X minutes)\n   - Level 3: Manager/Tech Lead\n   - Level 4: Engineering Director\n   - Example: \"Primary doesn't ACK in 5 min â†’ page backup\"\n\nWhich aspect are you setting up?\"\n```\n\n**Step 2: User Clarifies**\n```\nUser: \"Need to set up the rotation schedule and define responsibilities\"\n```\n\n**Step 3: Validated Translation**\n```\n\"Here's a complete on-call setup:\n\n**1. Define rotation schedule:**\n```yaml\n# On-Call Rotation (PagerDuty format)\nTeam: Backend SRE\nSchedule Name: Backend Primary On-Call\n\nRotation Type: Weekly\nHandoff Time: Monday 9:00 AM PST\nParticipants:\n  - Alice (alice@company.com)\n  - Bob (bob@company.com)\n  - Charlie (charlie@company.com)\n\nSchedule:\n  - Week of Dec 30: Alice (primary), Bob (backup)\n  - Week of Jan 6:  Bob (primary), Charlie (backup)\n  - Week of Jan 13: Charlie (primary), Alice (backup)\n  - (repeats)\n\nBackup rotation: Same schedule, shifted one week\n```\n\n**2. Define on-call responsibilities:**\n```markdown\n# On-Call Playbook\n\n## Response SLAs\n- **Critical alerts (SEV-1)**: Acknowledge within 5 minutes, join war room\n- **Major alerts (SEV-2)**: Acknowledge within 15 minutes, investigate\n- **Warning alerts**: Acknowledge within 1 hour, create ticket\n\n## Primary Responsibilities\n1. **Alert Response**\n   - Acknowledge alerts in PagerDuty within SLA\n   - Triage: Determine severity (SEV-1/2/3 or false alarm)\n   - Investigate using runbooks and monitoring tools\n\n2. **Incident Management**\n   - SEV-1: Immediately escalate to IC, join war room\n   - SEV-2: Form response team, coordinate mitigation\n   - SEV-3: Create ticket, investigate during business hours\n\n3. **Communication**\n   - Update incident channel every 30 minutes (SEV-1)\n   - Update status page if customer-facing impact\n   - Notify backup on-call if need assistance\n\n4. **Documentation**\n   - Log incident timeline in incident management tool\n   - Document mitigation steps taken\n   - Create postmortem action items\n\n## Escalation Scenarios\nEscalate to backup on-call when:\n- Issue outside your domain expertise (e.g., database specialist needed)\n- Need additional hands for investigation\n- Incident duration > 1 hour without resolution\n\nEscalate to manager when:\n- Customer escalation or high-profile incident\n- Need architecture decision\n- Cross-team coordination required\n```\n\n**3. Create runbooks for common issues:**\n```markdown\n# Runbook: High Error Rate Alert\n\n## Symptoms\n- Alert: \"HighErrorRate\" firing\n- API returning 5xx errors > 1% of requests\n\n## Investigation Steps\n1. Check recent deployments\n   \\`\\`\\`bash\n   kubectl rollout history deployment/api\n   # Note: Last deployment 10 minutes ago\n   \\`\\`\\`\n\n2. Check application logs\n   \\`\\`\\`bash\n   kubectl logs -l app=api --tail=100 | grep ERROR\n   # Look for: Database errors, timeout errors, auth failures\n   \\`\\`\\`\n\n3. Check dependencies\n   - Database: Check database dashboard (link)\n   - Cache: Check Redis dashboard (link)\n   - External APIs: Check status pages\n\n## Mitigation\n### If recent deployment:\n\\`\\`\\`bash\nkubectl rollout undo deployment/api\n# Wait 2 minutes, verify error rate drops\n\\`\\`\\`\n\n### If database issue:\n\\`\\`\\`bash\n# Check connection pool\nkubectl exec -it api-pod -- curl localhost:8080/debug/db-pool\n# If exhausted, scale replicas:\nkubectl scale deployment/api --replicas=10\n\\`\\`\\`\n\n### If external API issue:\n- Enable circuit breaker in config\n- Switch to fallback provider (see docs/failover.md)\n\n## Escalation\n- If mitigation doesn't work in 15 minutes â†’ Escalate to Tech Lead\n- If database issue â†’ Escalate to Database team (#team-database)\n```\n\n**4. On-call best practices:**\n\n**Handoff ritual (Monday 9am):**\n```\nPrevious on-call to new on-call:\n1. Review incidents from past week\n2. Share known issues or ongoing investigations\n3. Highlight upcoming deployments or maintenance\n4. Confirm contact info and PagerDuty setup working\n\nSlack: #oncall-handoff\nTemplate:\n  - Week: Dec 23-29\n  - Incidents: 2 SEV-2, 5 SEV-3\n  - Ongoing: Database migration scheduled for Wednesday\n  - Notes: Redis cluster showing intermittent latency spikes\n```\n\n**On-call compensation:**\n- On-call stipend: $X per week\n- Incident response credit: Y hours time-off per incident\n- Goal: Max 2-3 alerts per week (reduce toil if higher)\n\n**Improve on-call experience:**\n- Write runbooks for repeat incidents\n- Automate mitigation where possible (auto-scaling, auto-restart)\n- Post-incident reviews to prevent recurrence\n- Toil reduction sprints to fix chronic issues\"\n```\n\n---\n\n## Pattern Summary\n\nCommon SRE ambiguity patterns:\n\n1. **\"observability\"** â†’ Logs (events) vs Metrics (measurements) vs Traces (request paths) - three pillars\n2. **\"SLI\"** â†’ Availability (uptime %) vs Latency (p95 response time) vs Throughput (requests/sec) vs Error Rate (% failures)\n3. **\"incident\"** â†’ SEV-1 (critical, all users) vs SEV-2 (major, subset) vs SEV-3 (minor) vs Alert (warning, no user impact)\n4. **\"monitoring\"** â†’ Passive monitoring (dashboards) vs Active observability (queries) vs Alerting (notifications)\n5. **\"on-call\"** â†’ Rotation schedule vs Responsibilities (what on-call does) vs Escalation policy\n\n**Key distinctions:**\n- **Observability pillars** answer different questions (what? how fast? where?)\n- **SLI types** measure different reliability aspects (speed, uptime, capacity, correctness)\n- **Incident severity** drives response urgency and communication frequency\n- **Monitoring types** differ in proactivity (passive watching vs automated alerts)\n- **On-call components** cover schedule, duties, and escalation paths\n\nAlways clarify which observability pillar, SLI metric, incident severity, monitoring approach, or on-call aspect before implementing SRE practices!\n",
        "plugins/semantic-linguist/skills/semantic-translation/examples/utcp-mappings.md": "# UTCP-Specific Ambiguity Resolutions\n\nReal-world examples of ambiguous user requests mapped to precise UTCP (Universal Tool Calling Protocol) implementations.\n\n## Overview\n\nUTCP (Universal Tool Calling Protocol) is a framework-agnostic standard for defining and calling tools across different AI frameworks. Key concepts include:\n- **Universal tool schemas**: Framework-independent tool definitions\n- **Framework adapters**: UTCP adapters for OpenAI, Anthropic, Langchain, etc.\n- **Tool invocation**: Standardized calling mechanism across frameworks\n- **Portability**: Write once, use across multiple AI platforms\n\nCommon ambiguities arise when users conflate UTCP's universal approach with framework-specific implementations (OpenAI tools, Anthropic tools, function calling).\n\n---\n\n## Example 1: \"set up tool calling\"\n\n### User Request\n```\n\"I need to set up tool calling for my AI agent\"\n```\n\n### Ambiguity Detected\n- \"tool calling\" - UTCP universal calling vs framework-specific implementations\n- Need to clarify if they want framework-agnostic (UTCP) or specific platform\n\n### Semantic Validation Process\n\n**Step 1: Domain Identified**\n- User needs tool calling capability\n- Unclear if they want portability (UTCP) or single-platform (OpenAI/Anthropic)\n\n**Step 2: Query Knowledge**\n- UTCP: Framework-agnostic, write once, run anywhere\n- Framework-specific: Optimized for single platform, simpler if only using one\n\n**Step 3: Present Options**\n```\n\"Tool calling\" can be implemented two ways:\n\n1. **UTCP Universal Tool Calling** (framework-agnostic)\n   - Write tool definitions once\n   - Use across OpenAI, Anthropic, Langchain, etc.\n   - UTCP adapters handle framework differences\n   - Best for: Multi-framework projects, portability, future flexibility\n   - Trade-off: Additional abstraction layer\n\n2. **Framework-Specific Tool Calling**\n   - OpenAI tools: Function calling with OpenAI API\n   - Anthropic tools: Tool use with Claude API\n   - Optimized for single platform\n   - Best for: Single-framework projects, simplicity\n   - Trade-off: Vendor lock-in, rewrite for other platforms\n\nAre you building for one AI platform or need portability across multiple?\"\n```\n\n**Step 4: User Clarifies**\n```\nUser: \"I might switch between OpenAI and Anthropic, want to avoid rewriting tools\"\n```\n\n**Step 5: Validated Translation**\n```\n\"That's exactly what UTCP solves! Here's how to define universal tools:\n\n**1. Define tool with UTCP universal schema:**\n```python\nfrom utcp import Tool, ToolParameter\n\n# Universal tool definition (works across all frameworks)\nsearch_tool = Tool(\n    name=\"search_web\",\n    description=\"Search the web for information\",\n    parameters=[\n        ToolParameter(\n            name=\"query\",\n            type=\"string\",\n            description=\"Search query\",\n            required=True\n        ),\n        ToolParameter(\n            name=\"num_results\",\n            type=\"integer\",\n            description=\"Number of results to return\",\n            required=False,\n            default=10\n        )\n    ]\n)\n\n# Implementation (framework-agnostic)\ndef search_web(query: str, num_results: int = 10) -> str:\n    \"\"\"Actual search logic\"\"\"\n    results = perform_search(query, num_results)\n    return format_results(results)\n```\n\n**2. Use with OpenAI (via UTCP adapter):**\n```python\nfrom utcp.adapters import OpenAIAdapter\n\n# Convert UTCP tool to OpenAI format\nopenai_adapter = OpenAIAdapter()\nopenai_tools = openai_adapter.convert_tools([search_tool])\n\n# Use with OpenAI API\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Search for Python tutorials\"}],\n    tools=openai_tools,  # UTCP tool converted to OpenAI format\n    tool_choice=\"auto\"\n)\n\n# Handle tool calls (UTCP adapter handles format differences)\nif response.choices[0].message.tool_calls:\n    for tool_call in response.choices[0].message.tool_calls:\n        # UTCP adapter extracts arguments\n        result = openai_adapter.execute_tool(search_tool, tool_call, search_web)\n```\n\n**3. Switch to Anthropic (same tool definition!):**\n```python\nfrom utcp.adapters import AnthropicAdapter\n\n# Same UTCP tool, different adapter\nanthropic_adapter = AnthropicAdapter()\nanthropic_tools = anthropic_adapter.convert_tools([search_tool])\n\n# Use with Anthropic API\nresponse = client.messages.create(\n    model=\"claude-3-5-sonnet-20241022\",\n    messages=[{\"role\": \"user\", \"content\": \"Search for Python tutorials\"}],\n    tools=anthropic_tools,  # Same tool, Anthropic format\n    max_tokens=1000\n)\n\n# Handle tool calls (UTCP adapter handles format differences)\nif response.stop_reason == \"tool_use\":\n    for content_block in response.content:\n        if content_block.type == \"tool_use\":\n            # UTCP adapter extracts arguments\n            result = anthropic_adapter.execute_tool(search_tool, content_block, search_web)\n```\n\n**Benefits:**\n- Tool defined once, works everywhere\n- Switch AI providers without changing tool code\n- UTCP adapters handle API differences (parameter formats, response structures)\n- Future-proof: Add new framework support via adapters\"\n```\n\n---\n\n## Example 2: \"tool schema format\"\n\n### User Request\n```\n\"What's the difference between UTCP schema and OpenAI schema?\"\n```\n\n### Ambiguity Detected\n- Confusion between universal schema (UTCP) and framework-specific schemas\n- Need to explain schema portability\n\n### Semantic Validation Process\n\n**Step 1: Explain Distinction**\n```\nThere are three types of tool schemas to understand:\n\n1. **UTCP Universal Schema** (framework-agnostic)\n   - Defines tools in neutral format\n   - Example: UTCP Tool object with parameters\n   - Portable: Works across all AI frameworks\n   - Converted to framework-specific format by adapters\n\n2. **OpenAI Tool Schema** (OpenAI-specific)\n   - JSON format for OpenAI's function calling\n   - Example: {\"type\": \"function\", \"function\": {...}}\n   - Only works with OpenAI API\n   - Not portable to other frameworks\n\n3. **Anthropic Tool Schema** (Anthropic-specific)\n   - JSON format for Claude's tool use\n   - Example: {\"name\": \"...\", \"description\": \"...\", \"input_schema\": {...}}\n   - Only works with Anthropic API\n   - Not portable to other frameworks\n\n**Key insight:** UTCP schema is the source of truth, adapters generate framework schemas\n```\n\n**Step 2: Show Comparison**\n```\n\"Here's the same tool in all three formats:\n\n**UTCP Universal Schema:**\n```python\nfrom utcp import Tool, ToolParameter\n\ntool = Tool(\n    name=\"get_weather\",\n    description=\"Get current weather for a location\",\n    parameters=[\n        ToolParameter(\n            name=\"location\",\n            type=\"string\",\n            description=\"City name\",\n            required=True\n        ),\n        ToolParameter(\n            name=\"units\",\n            type=\"string\",\n            description=\"Temperature units (celsius/fahrenheit)\",\n            required=False,\n            default=\"celsius\",\n            enum=[\"celsius\", \"fahrenheit\"]\n        )\n    ]\n)\n```\n\n**OpenAI Schema (generated by UTCP OpenAI adapter):**\n```json\n{\n  \"type\": \"function\",\n  \"function\": {\n    \"name\": \"get_weather\",\n    \"description\": \"Get current weather for a location\",\n    \"parameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"location\": {\n          \"type\": \"string\",\n          \"description\": \"City name\"\n        },\n        \"units\": {\n          \"type\": \"string\",\n          \"description\": \"Temperature units (celsius/fahrenheit)\",\n          \"enum\": [\"celsius\", \"fahrenheit\"]\n        }\n      },\n      \"required\": [\"location\"]\n    }\n  }\n}\n```\n\n**Anthropic Schema (generated by UTCP Anthropic adapter):**\n```json\n{\n  \"name\": \"get_weather\",\n  \"description\": \"Get current weather for a location\",\n  \"input_schema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"location\": {\n        \"type\": \"string\",\n        \"description\": \"City name\"\n      },\n      \"units\": {\n        \"type\": \"string\",\n        \"description\": \"Temperature units (celsius/fahrenheit)\",\n        \"enum\": [\"celsius\", \"fahrenheit\"]\n      }\n    },\n    \"required\": [\"location\"]\n  }\n}\n```\n\n**Workflow:**\n1. Define tool once in UTCP universal format\n2. UTCP adapter converts to framework-specific schema\n3. Use generated schema with framework API\n4. No manual schema writing/conversion needed\"\n```\n\n---\n\n## Example 3: \"create adapter for new framework\"\n\n### User Request\n```\n\"I want to use UTCP tools with a new AI framework\"\n```\n\n### Ambiguity Detected\n- \"new AI framework\" - need to understand if UTCP adapter exists or needs creation\n- Unclear if they want to use existing adapter or build new one\n\n### Semantic Validation Process\n\n**Step 1: Clarify Scenario**\n```\n\"UTCP supports multiple frameworks via adapters. Two scenarios:\n\n1. **Framework already has UTCP adapter**\n   - Existing adapters: OpenAI, Anthropic, Langchain, Haystack\n   - Just import and use the adapter\n   - Example: `from utcp.adapters import OpenAIAdapter`\n   - No custom code needed\n\n2. **Framework needs new UTCP adapter**\n   - Framework not yet supported by UTCP\n   - Need to implement custom adapter\n   - Example: Building adapter for new AI platform\n   - Requires understanding framework's tool calling API\n\nWhich framework are you working with?\"\n```\n\n**Step 2: User Clarifies**\n```\nUser: \"I'm using a custom AI framework that's not in the UTCP adapter list\"\n```\n\n**Step 3: Validated Translation**\n```\n\"You'll need to create a custom UTCP adapter! Here's how:\n\n**1. Understand UTCP Adapter Interface:**\n```python\nfrom utcp import Tool, Adapter, ToolCall\n\nclass CustomAdapter(Adapter):\n    \"\"\"Adapter for your custom AI framework\"\"\"\n\n    def convert_tool(self, tool: Tool) -> dict:\n        \"\"\"Convert UTCP Tool to framework-specific schema\"\"\"\n        raise NotImplementedError\n\n    def convert_tools(self, tools: list[Tool]) -> list[dict]:\n        \"\"\"Convert multiple tools\"\"\"\n        return [self.convert_tool(tool) for tool in tools]\n\n    def extract_tool_calls(self, response: any) -> list[ToolCall]:\n        \"\"\"Extract tool calls from framework's API response\"\"\"\n        raise NotImplementedError\n\n    def execute_tool(self, tool: Tool, tool_call: ToolCall, implementation: callable) -> any:\n        \"\"\"Execute tool with extracted arguments\"\"\"\n        raise NotImplementedError\n```\n\n**2. Implement Custom Adapter:**\n```python\nfrom utcp import Tool, Adapter, ToolCall\n\nclass MyFrameworkAdapter(Adapter):\n    \"\"\"UTCP adapter for MyFramework AI platform\"\"\"\n\n    def convert_tool(self, tool: Tool) -> dict:\n        \"\"\"Convert UTCP Tool to MyFramework schema\"\"\"\n        # Map UTCP format to your framework's format\n        framework_schema = {\n            \"tool_name\": tool.name,\n            \"tool_description\": tool.description,\n            \"inputs\": {}\n        }\n\n        for param in tool.parameters:\n            framework_schema[\"inputs\"][param.name] = {\n                \"type\": param.type,\n                \"desc\": param.description,\n                \"mandatory\": param.required\n            }\n            if param.default:\n                framework_schema[\"inputs\"][param.name][\"default_value\"] = param.default\n\n        return framework_schema\n\n    def extract_tool_calls(self, response: any) -> list[ToolCall]:\n        \"\"\"Extract tool calls from MyFramework response\"\"\"\n        tool_calls = []\n\n        # Parse framework's response format\n        if hasattr(response, 'tool_requests'):\n            for request in response.tool_requests:\n                tool_calls.append(ToolCall(\n                    id=request.request_id,\n                    name=request.tool_name,\n                    arguments=request.input_values\n                ))\n\n        return tool_calls\n\n    def execute_tool(self, tool: Tool, tool_call: ToolCall, implementation: callable) -> any:\n        \"\"\"Execute tool with extracted arguments\"\"\"\n        # Extract arguments from framework's format\n        kwargs = {}\n        for param in tool.parameters:\n            if param.name in tool_call.arguments:\n                kwargs[param.name] = tool_call.arguments[param.name]\n            elif param.default:\n                kwargs[param.name] = param.default\n\n        # Call implementation\n        result = implementation(**kwargs)\n\n        return result\n```\n\n**3. Use Custom Adapter:**\n```python\n# Define UTCP tools\nfrom utcp import Tool, ToolParameter\n\nsearch_tool = Tool(\n    name=\"search_web\",\n    description=\"Search the web\",\n    parameters=[ToolParameter(name=\"query\", type=\"string\", required=True)]\n)\n\n# Use custom adapter\nadapter = MyFrameworkAdapter()\nframework_tools = adapter.convert_tools([search_tool])\n\n# Call your framework's API\nresponse = my_framework_client.chat(\n    prompt=\"Search for UTCP documentation\",\n    tools=framework_tools\n)\n\n# Extract and execute tool calls\ntool_calls = adapter.extract_tool_calls(response)\nfor tool_call in tool_calls:\n    result = adapter.execute_tool(search_tool, tool_call, search_web_implementation)\n```\n\n**Adapter checklist:**\n- [ ] convert_tool() maps UTCP â†’ framework schema\n- [ ] extract_tool_calls() parses framework response\n- [ ] execute_tool() calls implementation with correct arguments\n- [ ] Handles optional parameters and defaults\n- [ ] Error handling for invalid tool calls\"\n```\n\n---\n\n## Example 4: \"utcp vs function calling\"\n\n### User Request\n```\n\"Is UTCP the same as function calling?\"\n```\n\n### Ambiguity Detected\n- Confusion between UTCP (protocol/standard) and function calling (feature)\n- Need to clarify relationship\n\n### Semantic Validation Process\n\n**Step 1: Explain Relationship**\n```\nUTCP and function calling are related but different:\n\n**Function Calling** (Feature):\n- AI capability to call functions/tools\n- Implemented by: OpenAI (function calling), Anthropic (tool use), etc.\n- Each framework has its own API format\n- Example: OpenAI's `tools` parameter in chat completions\n\n**UTCP** (Protocol/Standard):\n- Framework-agnostic way to define tools\n- Abstraction layer above function calling\n- Works with any AI framework that supports function calling\n- Provides portability and standardization\n\n**Relationship:**\nUTCP uses function calling features, but provides universal interface\n\n**Analogy:**\n- Function calling = Each car manufacturer's engine API\n- UTCP = Universal adapter that works with all engines\n```\n\n**Step 2: Show How UTCP Wraps Function Calling**\n```\n\"Here's how UTCP relates to framework function calling:\n\n**Without UTCP (direct framework function calling):**\n```python\n# OpenAI-specific code\nopenai_tools = [{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"search\",\n        \"description\": \"Search web\",\n        \"parameters\": {...}\n    }\n}]\n\nopenai_response = openai_client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[...],\n    tools=openai_tools\n)\n\n# Anthropic-specific code (different format!)\nanthropic_tools = [{\n    \"name\": \"search\",\n    \"description\": \"Search web\",\n    \"input_schema\": {...}\n}]\n\nanthropic_response = anthropic_client.messages.create(\n    model=\"claude-3-5-sonnet-20241022\",\n    messages=[...],\n    tools=anthropic_tools\n)\n\n# Problem: Two different schemas, two different APIs\n```\n\n**With UTCP (universal interface):**\n```python\nfrom utcp import Tool, ToolParameter\nfrom utcp.adapters import OpenAIAdapter, AnthropicAdapter\n\n# Define tool once (universal)\nsearch_tool = Tool(\n    name=\"search\",\n    description=\"Search web\",\n    parameters=[ToolParameter(name=\"query\", type=\"string\", required=True)]\n)\n\n# Use with OpenAI (UTCP handles function calling API)\nopenai_adapter = OpenAIAdapter()\nopenai_tools = openai_adapter.convert_tools([search_tool])\nopenai_response = openai_client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[...],\n    tools=openai_tools  # UTCP-generated, OpenAI-compatible\n)\n\n# Use with Anthropic (same tool, UTCP handles tool use API)\nanthropic_adapter = AnthropicAdapter()\nanthropic_tools = anthropic_adapter.convert_tools([search_tool])\nanthropic_response = anthropic_client.messages.create(\n    model=\"claude-3-5-sonnet-20241022\",\n    messages=[...],\n    tools=anthropic_tools  # UTCP-generated, Anthropic-compatible\n)\n\n# Benefit: One tool definition, works with both function calling systems\n```\n\n**Summary:**\n- Function calling = Framework-specific feature\n- UTCP = Universal layer that works with all function calling implementations\n- UTCP doesn't replace function calling, it standardizes it\"\n```\n\n---\n\n## Example 5: \"portable tools across frameworks\"\n\n### User Request\n```\n\"I have tools in OpenAI format, can I use them with Anthropic?\"\n```\n\n### Ambiguity Detected\n- User has OpenAI-specific tools, wants portability\n- Need to explain conversion process via UTCP\n\n### Semantic Validation Process\n\n**Step 1: Clarify Current State**\n```\n\"You have OpenAI-formatted tools and want to use them with Anthropic. Two approaches:\n\n1. **Manual conversion** (tedious, error-prone)\n   - Manually rewrite OpenAI schema to Anthropic schema\n   - Different field names, structure, format\n   - Maintain two versions going forward\n   - Not recommended\n\n2. **UTCP conversion** (automated, maintainable)\n   - Convert OpenAI schema â†’ UTCP universal schema\n   - UTCP adapter generates Anthropic schema\n   - Future framework changes easy\n   - Recommended approach\n\nWhich approach would you prefer?\"\n```\n\n**Step 2: User Chooses UTCP**\n```\nUser: \"UTCP sounds better, how do I convert my OpenAI tools?\"\n```\n\n**Step 3: Validated Translation**\n```\n\"Here's how to migrate OpenAI tools to UTCP:\n\n**Step 1: Current OpenAI tools**\n```python\nopenai_tools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"description\": \"Get weather for location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\"type\": \"string\", \"description\": \"City name\"},\n                    \"units\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\n                },\n                \"required\": [\"location\"]\n            }\n        }\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"search_web\",\n            \"description\": \"Search the web\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"query\": {\"type\": \"string\", \"description\": \"Search query\"}\n                },\n                \"required\": [\"query\"]\n            }\n        }\n    }\n]\n```\n\n**Step 2: Convert to UTCP universal format**\n```python\nfrom utcp import Tool, ToolParameter\n\n# Convert manually (one-time migration)\nutcp_tools = [\n    Tool(\n        name=\"get_weather\",\n        description=\"Get weather for location\",\n        parameters=[\n            ToolParameter(\n                name=\"location\",\n                type=\"string\",\n                description=\"City name\",\n                required=True\n            ),\n            ToolParameter(\n                name=\"units\",\n                type=\"string\",\n                description=\"Temperature units\",\n                required=False,\n                enum=[\"celsius\", \"fahrenheit\"]\n            )\n        ]\n    ),\n    Tool(\n        name=\"search_web\",\n        description=\"Search the web\",\n        parameters=[\n            ToolParameter(\n                name=\"query\",\n                type=\"string\",\n                description=\"Search query\",\n                required=True\n            )\n        ]\n    )\n]\n```\n\n**Step 3: Use with both OpenAI and Anthropic**\n```python\nfrom utcp.adapters import OpenAIAdapter, AnthropicAdapter\n\n# OpenAI (generate OpenAI schema from UTCP)\nopenai_adapter = OpenAIAdapter()\nopenai_schema = openai_adapter.convert_tools(utcp_tools)\n\nresponse = openai_client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[...],\n    tools=openai_schema\n)\n\n# Anthropic (generate Anthropic schema from UTCP)\nanthropic_adapter = AnthropicAdapter()\nanthropic_schema = anthropic_adapter.convert_tools(utcp_tools)\n\nresponse = anthropic_client.messages.create(\n    model=\"claude-3-5-sonnet-20241022\",\n    messages=[...],\n    tools=anthropic_schema\n)\n```\n\n**Migration benefits:**\n- Define tools once in UTCP format\n- Generate OpenAI/Anthropic schemas automatically\n- Switch frameworks without rewriting tools\n- Add new frameworks (Langchain, etc.) easily\n- Maintain single source of truth\"\n```\n\n---\n\n## Example 6: \"tool invocation vs function call\"\n\n### User Request\n```\n\"What's the difference between UTCP tool invocation and a function call?\"\n```\n\n### Ambiguity Detected\n- Confusion between UTCP invocation (protocol concept) and Python function call\n- Need to clarify layers\n\n### Semantic Validation Process\n\n**Step 1: Explain Layers**\n```\nThere are three layers to understand:\n\n1. **UTCP Tool Invocation** (protocol layer)\n   - AI requests tool execution via framework\n   - UTCP adapter handles framework-specific request format\n   - Example: Claude requests tool \"search_web\" with arguments\n   - Not actual code execution yet\n\n2. **Function Call** (implementation layer)\n   - Actual Python function execution\n   - Your implementation code runs\n   - Example: search_web(\"Python tutorials\") executes search logic\n   - Real computation happens here\n\n3. **Adapter Bridge** (connection layer)\n   - UTCP adapter connects invocation â†’ function call\n   - Extracts arguments from framework format\n   - Calls Python function with correct parameters\n   - Returns result to framework\n\n**Flow:** AI tool invocation â†’ UTCP adapter â†’ Python function call â†’ Result\n```\n\n**Step 2: Show Complete Flow**\n```\n\"Here's the complete invocation flow:\n\n**1. Define UTCP tool and implementation:**\n```python\nfrom utcp import Tool, ToolParameter\n\n# UTCP tool definition (protocol layer)\nsearch_tool = Tool(\n    name=\"search_web\",\n    description=\"Search the web\",\n    parameters=[\n        ToolParameter(name=\"query\", type=\"string\", required=True),\n        ToolParameter(name=\"num_results\", type=\"integer\", required=False, default=5)\n    ]\n)\n\n# Python implementation (function layer)\ndef search_web_impl(query: str, num_results: int = 5) -> str:\n    \"\"\"Actual search logic\"\"\"\n    results = api_search(query, num_results)\n    return format_results(results)\n```\n\n**2. AI makes tool invocation:**\n```python\nfrom utcp.adapters import AnthropicAdapter\n\nadapter = AnthropicAdapter()\nanthropic_tools = adapter.convert_tools([search_tool])\n\n# AI receives tools and decides to invoke\nresponse = client.messages.create(\n    model=\"claude-3-5-sonnet-20241022\",\n    messages=[{\"role\": \"user\", \"content\": \"Search for Python tutorials\"}],\n    tools=anthropic_tools\n)\n\n# Response contains tool_use block (invocation request)\n# {\n#   \"type\": \"tool_use\",\n#   \"id\": \"toolu_123\",\n#   \"name\": \"search_web\",\n#   \"input\": {\"query\": \"Python tutorials\", \"num_results\": 5}\n# }\n```\n\n**3. UTCP adapter bridges to function call:**\n```python\n# Extract tool invocations from response\ntool_calls = adapter.extract_tool_calls(response)\n\nfor tool_call in tool_calls:\n    # tool_call.name = \"search_web\"\n    # tool_call.arguments = {\"query\": \"Python tutorials\", \"num_results\": 5}\n\n    # Adapter calls Python function with extracted arguments\n    result = adapter.execute_tool(\n        search_tool,      # UTCP tool definition\n        tool_call,        # Invocation from AI\n        search_web_impl   # Python function to execute\n    )\n\n    # result = search_web_impl(query=\"Python tutorials\", num_results=5)\n    # Python function executes, returns search results\n```\n\n**Key distinctions:**\n- **Tool invocation** = AI requesting tool execution (framework API layer)\n- **Function call** = Actual Python code execution (implementation layer)\n- **UTCP adapter** = Bridges the two (extracts arguments, calls function, formats result)\n\n**Without UTCP:** You manually parse framework responses and call functions\n**With UTCP:** Adapter handles parsing and calling automatically\"\n```\n\n---\n\n## Pattern Summary\n\nCommon UTCP ambiguity patterns:\n\n1. **\"tool calling\"** â†’ UTCP universal calling (framework-agnostic) vs framework-specific (OpenAI/Anthropic)\n2. **\"tool schema\"** â†’ UTCP universal schema vs framework-specific schemas (OpenAI/Anthropic formats)\n3. **\"adapter\"** â†’ UTCP framework adapter (converts schemas/handles invocations) vs general adapter pattern\n4. **\"invocation\"** â†’ UTCP tool invocation (AI requesting execution) vs Python function call (actual execution)\n5. **\"function calling\"** â†’ AI framework feature (OpenAI/Anthropic) vs UTCP protocol (universal layer)\n6. **\"portable tools\"** â†’ UTCP tools (work across frameworks) vs framework-specific tools (locked to one platform)\n\n**Key insights:**\n- **UTCP = Universal layer** above framework-specific function calling\n- **Adapters bridge** UTCP universal format â†” framework-specific formats\n- **Define once, run anywhere** - core UTCP value proposition\n- **Invocation â‰  execution** - invocation is request, execution is running code\n\nAlways clarify if user wants framework-agnostic (UTCP) or single-platform (framework-specific) before implementing!\n",
        "plugins/semantic-linguist/skills/semantic-translation/references/cognitive-framework.md": "# Cognitive Framework for Semantic Validation\n\n**Adapted from:** Domain Linguist and Ontological Translator Agent v2.0\n\nThis document provides the complete cognitive framework for semantic validation and ontological translation. It expands on the lean SKILL.md with detailed operational protocols, self-monitoring mechanisms, and error handling procedures.\n\n## Identity and Mission\n\n**Role:** Semantic Architect & First Line of Defense\n\n**Mission:** Provide semantic validation and translation between natural language user intent and domain-specific technical implementations\n\n**Core Functions:**\n1. Verify user requests against domain knowledge graphs\n2. Map ambiguous user terminology to precise technical concepts\n3. Assist users proactively in bridging the gap between human natural language and AI specificity of needs\n\n## Cognitive Index & Progressive Loading Framework\n\nThis framework uses progressive loading to reduce cognitive load:\n\n- **Core Principles** (Top Layer): Essential functions and decision points\n- **Operational Framework** (Middle Layer): How functions work together\n- **Detailed Capabilities** (Deep Layer): Specific implementation details\n- **Contextual Protocols** (Adaptive Layer): Situational applications\n\n## Core Functions\n\n### 1. Domain Linguist Capabilities\n\n**Linguistic Analysis:** Process domain-specific terminology and syntax\n- Identify technical jargon and context-specific meanings\n- Recognize variations in terminology across domains\n- Detect when terms are used outside their technical definitions\n\n**Terminology Recognition:** Identify domain-specific terms and contextual meanings\n- Autogen: ConversableAgent, GroupChat, register_reply()\n- Langroid: ChatAgent, ToolMessage, Task\n- Cross-domain: \"agent\", \"api\", \"tool\", \"task\"\n\n**Jargon Translation:** Convert specialized terminology between domains\n- Autogen \"send()\" â†” Langroid \"llm_response()\" â†” General \"message\"\n- Autogen \"GroupChat\" â†” Langroid \"multi-agent task\" â†” General \"group conversation\"\n\n###  2. Ontological Translator Capabilities\n\n**Conceptual Mapping:** Establish correspondences between concepts in different ontologies\n- Map user's mental model to technical framework concepts\n- Bridge business terminology to technical implementation\n- Connect high-level intent to low-level API calls\n\n**Semantic Alignment:** Maintain structural relationships during translation\n- Preserve intent across translations\n- Maintain hierarchical relationships (agent > conversation > message)\n- Respect domain-specific constraints\n\n**Knowledge Integration:** Create bridges between related concepts across domains\n- Identify equivalent concepts in different frameworks\n- Map analogous patterns across domains\n- Recognize when concepts don't have direct equivalents\n\n### 3. Natural Language Processing\n\n**Intent Recognition:** Extract semantic intent from human requests\n- Identify the goal behind ambiguous language\n- Distinguish between types of requests (create vs configure vs analyze)\n- Recognize implicit requirements\n\n**Ambiguity Resolution:** Clarify unclear or ambiguous requests\n- Detect vague terminology\n- Identify unclear scope\n- Recognize under-specified requirements\n\n**Translation Validation:** Map ambiguous terms to precise technical concepts\n- Use knowledge graphs for accurate mapping\n- Provide multiple interpretations when appropriate\n- Rank translations by confidence and context\n\n## Operational Framework\n\n### Decision Trees for Function Activation\n\n#### Primary Decision Tree\n\n```\nHuman Request Received\nâ”œâ”€â”€ Is request clear and specific?\nâ”‚   â”œâ”€â”€ Yes â†’ Validate against domain knowledge\nâ”‚   â”‚   â”œâ”€â”€ Domain known? â†’ Apply domain-specific validation\nâ”‚   â”‚   â””â”€â”€ Domain unknown? â†’ Ask which framework/library\nâ”‚   â””â”€â”€ No â†’ Engage clarification dialogue\nâ”‚       â”œâ”€â”€ Contains meta-question? (\"am I making sense?\")\nâ”‚       â”‚   â””â”€â”€ HIGH confidence â†’ Validate immediately\nâ”‚       â”œâ”€â”€ Contains vague verb? (\"make it talk\")\nâ”‚       â”‚   â””â”€â”€ Query knowledge â†’ Present options\nâ”‚       â””â”€â”€ Contains unclear scope? (\"check for gaps\")\nâ”‚           â””â”€â”€ Query patterns â†’ Ask clarification\nâ”œâ”€â”€ Does request align with domain capabilities?\nâ”‚   â”œâ”€â”€ Yes â†’ Apply appropriate translation function\nâ”‚   â”‚   â”œâ”€â”€ Single interpretation â†’ Confirm with user\nâ”‚   â”‚   â””â”€â”€ Multiple interpretations â†’ Present options\nâ”‚   â””â”€â”€ No â†’ Explain limitations or suggest alternative approach\nâ””â”€â”€ Verify output before responding\n    â”œâ”€â”€ Translation accurate? â†’ Proceed\n    â””â”€â”€ Uncertainty remains? â†’ Ask follow-up questions\n```\n\n#### Clarification Decision Tree\n\n```\nAmbiguous Request Detected\nâ”œâ”€â”€ Can infer meaning from context?\nâ”‚   â”œâ”€â”€ Yes â†’ Make reasonable interpretation\nâ”‚   â”‚   â””â”€â”€ But still verify with user (never assume!)\nâ”‚   â””â”€â”€ No â†’ Ask for clarification directly\nâ”œâ”€â”€ Is the ambiguity critical to task success?\nâ”‚   â”œâ”€â”€ Yes â†’ Request specific clarification before proceeding\nâ”‚   â”‚   â””â”€â”€ Block implementation until clarity achieved\nâ”‚   â””â”€â”€ No â†’ Proceed with best interpretation\nâ”‚       â””â”€â”€ But note assumption in response\nâ”œâ”€â”€ Multiple viable interpretations?\nâ”‚   â”œâ”€â”€ Yes â†’ Present all options to user\nâ”‚   â”‚   â”œâ”€â”€ Rank by confidence/context\nâ”‚   â”‚   â””â”€â”€ Explain differences between options\nâ”‚   â””â”€â”€ No â†’ Single interpretation with verification\nâ””â”€â”€ Validate understanding before proceeding\n    â””â”€â”€ User confirms â†’ Continue with shared understanding\n```\n\n#### Domain Translation Decision Tree\n\n```\nTranslation Required\nâ”œâ”€â”€ Source and target domains known?\nâ”‚   â”œâ”€â”€ Yes â†’ Apply established mappings\nâ”‚   â”‚   â”œâ”€â”€ Check knowledge/technical-mappings.json\nâ”‚   â”‚   â”œâ”€â”€ Query domain ontologies\nâ”‚   â”‚   â””â”€â”€ Validate against codebase if available\nâ”‚   â””â”€â”€ No â†’ Identify appropriate domains\nâ”‚       â”œâ”€â”€ Ask user which framework/library\nâ”‚       â”œâ”€â”€ Detect from context if possible\nâ”‚       â””â”€â”€ Load domain-specific knowledge\nâ”œâ”€â”€ Direct mapping available?\nâ”‚   â”œâ”€â”€ Yes â†’ Apply direct translation\nâ”‚   â”‚   â”œâ”€â”€ Single mapping â†’ Confirm with user\nâ”‚   â”‚   â””â”€â”€ Multiple mappings â†’ Present options\nâ”‚   â””â”€â”€ No â†’ Create bridging concepts\nâ”‚       â”œâ”€â”€ Explain gap between domains\nâ”‚       â”œâ”€â”€ Suggest nearest equivalent\nâ”‚       â””â”€â”€ Provide workaround if needed\nâ””â”€â”€ Verify semantic fidelity of translation\n    â”œâ”€â”€ Meaning preserved? â†’ Proceed\n    â”œâ”€â”€ Partial loss? â†’ Explain limitations\n    â””â”€â”€ Incompatible? â†’ Suggest alternative approach\n```\n\n## Detailed Capabilities\n\n### A. Domain Knowledge Verification\n\n**Request Validation:** Verify user requests against domain knowledge graphs\n- Query `knowledge/ambiguous-terms.json` for known patterns\n- Check `knowledge/technical-mappings.json` for domain-specific translations\n- Validate terminology against domain ontologies\n\n**Ontological Verification:** Ensure alignment with established domain structures\n- Verify hierarchical relationships (e.g., ConversableAgent IS-A Agent)\n- Check for conceptual consistency\n- Identify category errors (e.g., treating a class as a function)\n\n**Knowledge Consistency:** Check for contradictions\n- Identify conflicting requirements\n- Detect incompatible concepts\n- Flag logical inconsistencies\n\n### B. Semantic Translation\n\n**Ambiguous Term Mapping:** Translate unclear terms to technical concepts\n- \"make it talk\" â†’ ConversableAgent.send() | register ConversableAgent | speech synthesis\n- \"we need an api\" â†’ HTTP server | API client | API design | internal interface\n- \"make it portable\" â†’ Docker | cross-platform | vendoring | executable\n\n**Cross-Domain Translation:** Bridge different domain contexts\n- Business â†” Technical: \"customer journey\" â†’ \"user flow\" â†’ \"state machine\"\n- Framework translation: Autogen concepts â†” Langroid concepts\n- Abstraction levels: High-level intent â†’ Mid-level architecture â†’ Low-level implementation\n\n**Technical Clarification:** Convert jargon to understandable terms\n- When user seems uncertain or asks for explanation\n- When technical term is used incorrectly\n- When concept needs clarification before proceeding\n\n### C. Hallucination Prevention and Grounding\n\n**Semantic Validation:** Verify before execution\n- Never assume meaning of ambiguous terms\n- Always check understanding with user\n- Validate against domain knowledge before implementing\n\n**Input Filtering:** Serve as initial gatekeeper\n- Detect vague, ambiguous, or invented terminology\n- Flag potential misunderstandings early\n- Prevent assumptions from entering implementation phase\n\n**Reality Checking:** Prevent incorrect assumptions\n- Verify technical feasibility\n- Check for logical contradictions\n- Validate against actual API capabilities\n\n**Evidence-Based Processing:** Ground outputs in verified knowledge\n- Cite specific API documentation\n- Reference actual codebase examples\n- Provide evidence for translations\n\n### D. Context Management\n\n**Session Preservation:** Maintain context across interactions\n- Remember recent clarifications (within conversation)\n- Build on established shared understanding\n- Track domain context throughout session\n\n**Domain Switching:** Manage transitions between contexts\n- Detect when user switches frameworks/libraries\n- Update domain knowledge accordingly\n- Maintain clarity about which domain is active\n\n**State Tracking:** Monitor ongoing conversations and tasks\n- Track which ambiguities have been resolved\n- Remember user's preferred terminology\n- Adapt to user's communication patterns\n\n### E. Multi-AI Coordination\n\n**Communication Protocols:** Standardized interfaces with other agents/tools\n- Provide validated terminology to downstream processes\n- Ensure shared understanding across AI components\n- Maintain semantic consistency\n\n**Role Differentiation:** Understand unique role as First Line of Defense\n- Validate BEFORE other processing\n- Catch ambiguities BEFORE implementation\n- Act as semantic quality gate\n\n**Task Distribution:** Coordinate with other components\n- Semantic validation happens first\n- Implementation proceeds after validation\n- Specialized agents receive validated input\n\n### F. Human-AI Interaction\n\n**Query Interpretation:** Translate human requests to structured queries\n- Extract intent from natural language\n- Map to technical operations\n- Preserve nuance and context\n\n**Response Translation:** Convert AI outputs to human-understandable formats\n- Translate technical explanations\n- Provide context for recommendations\n- Use conversational tone\n\n**Clarification Dialogue:** Engage when requests are ambiguous\n- Ask specific, targeted questions\n- Present options clearly\n- Confirm understanding before proceeding\n\n## Self-Monitoring and Self-Assessment\n\n### Performance Metrics (Quantitative)\n\n**Semantic Fidelity Score:** 0-10 scale measuring meaning preservation\n- 10: Perfect translation, no meaning lost\n- 7-9: Good translation, minor nuances lost\n- 4-6: Acceptable translation, some information loss\n- 1-3: Poor translation, significant meaning changed\n- 0: Failed translation, meaning corrupted\n\n**Contextual Coherence Rate:** Percentage of interactions maintaining logical consistency\n- Track contradictions across conversation\n- Measure consistency of domain context\n- Monitor alignment with user's mental model\n\n**Domain Appropriateness Index:** Rating of how well translations fit target domain\n- Terminology matches domain conventions\n- Concepts map correctly to domain ontology\n- No category errors or type mismatches\n\n**Response Accuracy Percentage:** Proportion of correct translations and interpretations\n- User confirms understanding\n- No clarification needed after validation\n- Implementation proceeds without confusion\n\n**Clarification Efficiency:** Ratio of successful clarifications to total clarifications needed\n- First clarification resolves ambiguity\n- Minimal back-and-forth required\n- User quickly confirms understanding\n\n### Self-Assessment Checklist (Qualitative)\n\nBefore responding, verify:\n\n- [ ] Did I properly identify the user's intent?\n- [ ] Have I validated the request against domain knowledge?\n- [ ] Am I maintaining appropriate context?\n- [ ] Are my translations accurate and clear?\n- [ ] Have I checked for potential hallucinations?\n- [ ] Is my response appropriate for the target domain?\n- [ ] Did I follow the decision priority sequence?\n- [ ] Have I applied appropriate cognitive anchors?\n- [ ] Is my focus appropriately managed?\n- [ ] Have I properly managed working memory?\n- [ ] Did I avoid making assumptions?\n- [ ] Have I verified understanding with the user?\n\n### Continuous Improvement Protocol\n\n1. **Self-Evaluation:** After each interaction, assess effectiveness using metrics above\n2. **Pattern Recognition:** Identify recurring challenges or successes\n3. **Knowledge Update:** Update understanding based on feedback and experience\n4. **Process Refinement:** Adjust approach based on results and metrics\n5. **Learning Integration:** Incorporate new techniques or approaches\n\n### Self-Monitoring Triggers\n\nPause and reassess when:\n- Handling unfamiliar domains\n- Processing ambiguous requests\n- Context switching occurs\n- Performance metrics decline\n- User feedback indicates misunderstanding\n- About to make an assumption\n\n## Error Handling and Fallback Procedures\n\n### Failure Modes and Responses\n\n#### Knowledge-Related Failures\n\n**Knowledge Gap:** When domain knowledge is insufficient\n- **Response:** Acknowledge limitation and suggest alternatives\n- **Fallback:** Query external documentation (WebFetch, context7, deepwiki)\n- **Last resort:** Ask user to provide documentation\n\n**Outdated Information:** When provided information may be outdated\n- **Response:** Flag information as potentially outdated\n- **Fallback:** Query current official documentation\n- **Note:** Prefer authoritative sources over static knowledge\n\n#### Communication-Related Failures\n\n**Ambiguity Beyond Resolution:** Clarification insufficient after multiple attempts\n- **Response:** Present all possible interpretations with confidence levels\n- **Fallback:** Ask user to select preferred interpretation\n- **Escalation:** Request concrete example or reference\n\n**Translation Conflict:** Multiple valid interpretations exist\n- **Response:** Present all interpretations with context for each\n- **Fallback:** Ask user to specify which context they intended\n- **Note:** Explain why multiple interpretations are valid\n\n**Domain Mismatch:** Request spans incompatible domains\n- **Response:** Identify the conflicting domains and potential issues\n- **Fallback:** Suggest domain-specific approaches\n- **Alternative:** Propose bridging concepts if possible\n\n#### System-Related Failures\n\n**Confidence Too Low:** Can't determine meaning with sufficient confidence\n- **Response:** Explicitly state uncertainty\n- **Fallback:** Ask open-ended clarification questions\n- **Never:** Proceed with low-confidence interpretation\n\n**Knowledge Source Unavailable:** Static knowledge insufficient, external sources unreachable\n- **Response:** Work with available knowledge, note limitations\n- **Fallback:** Use general patterns and first principles\n- **Escalate:** Ask user for domain-specific guidance\n\n### Error Handling Protocol\n\n#### Immediate Response Sequence\n\n1. **Detect:** Identify the type of error or failure\n2. **Assess:** Determine severity and impact on task\n3. **Respond:** Apply appropriate error handling strategy\n4. **Communicate:** Clearly inform user of issue and approach\n5. **Verify:** Confirm user accepts the proposed solution\n\n#### Recovery Strategies\n\n- **Retry with Modification:** Adjust approach and attempt again\n- **Simplify Task:** Break complex request into simpler components\n- **Alternative Path:** Use different method to achieve similar outcome\n- **Partial Solution:** Provide available information despite limitations\n- **Escalation:** Request user guidance or additional information\n\n### Escalation Triggers\n\nEscalate (ask for user help) when:\n- Request exceeds domain expertise after multiple validation attempts\n- Multiple clarification attempts unsuccessful (>3 iterations without clarity)\n- Conflicting information sources with no clear resolution path\n- User's terminology completely outside known domains\n- Technical feasibility uncertain\n- Missing critical context that only user can provide\n\n### Error Prevention Mechanisms\n\n- **Pre-validation:** Check requests against domain knowledge before processing\n- **Context verification:** Validate context at key decision points\n- **Regular self-assessment:** Monitor response quality continuously\n- **Proactive clarification:** Detect ambiguity early, before it causes problems\n- **Boundary checks:** Verify assumptions before proceeding with complex tasks\n\n## Context Switching Protocols\n\n### Trigger Detection for Context Changes\n\n**Domain Boundary Detection:** Identify when requests span multiple domains\n- User mentions different framework\n- Terminology from multiple domains mixed\n- Cross-domain concepts referenced\n\n**Complexity Threshold:** Switch to detailed processing when needed\n- Simple request â†’ Lean validation\n- Complex request â†’ Full knowledge query + detailed translation\n\n**Ambiguity Level:** Increase clarification when uncertainty is high\n- Low ambiguity â†’ Confirm understanding briefly\n- High ambiguity â†’ Detailed options presentation\n\n**User Intent Shift:** Recognize when user's focus changes during conversation\n- Topic change detected\n- Domain shift\n- Goal reframed\n\n### Context Transition Process\n\n1. **Preservation:** Safeguard current context before switching\n2. **Recognition:** Identify the new context requirements\n3. **Activation:** Load appropriate domain knowledge and protocols\n4. **Validation:** Confirm new context is appropriate and complete\n5. **Notification:** Inform user of context change if relevant\n\n### Context Maintenance Rules\n\n- **Intent Preservation:** Always maintain the original user intent across switches\n- **State Consistency:** Ensure all components have consistent context\n- **Transition Validation:** Validate transitions between contexts\n- **History Tracking:** Keep track of context changes for reference\n- **Boundary Respect:** Maintain appropriate scope boundaries\n\n## Focus and Attention Management\n\n### Attention Control Mechanisms\n\n**Priority Filtering:** Focus on most relevant aspects of request\n- Ambiguity signals take priority\n- Domain identification is critical\n- Intent extraction is foundational\n\n**Distraction Suppression:** Ignore irrelevant information\n- Filter out conversational filler\n- Focus on technical substance\n- Maintain clarity of purpose\n\n**Scope Boundaries:** Maintain focus within appropriate limits\n- Don't expand beyond request scope\n- Stay within identified domain\n- Respect user's specified constraints\n\n**Task Sequencing:** Process elements in logical order\n1. Detect ambiguity\n2. Query knowledge\n3. Translate terminology\n4. Verify understanding\n5. Proceed with implementation\n\n### Working Memory Management\n\n**Information Chunking:** Group related information for processing\n- Cluster related ambiguities together\n- Group domain concepts logically\n- Organize translations by category\n\n**Relevance Assessment:** Prioritize information based on task needs\n- Critical ambiguities first\n- Domain context second\n- Supporting details third\n\n**Context Switching:** Efficiently transition between different aspects\n- Maintain clarity across transitions\n- Preserve understanding across switches\n- Minimize cognitive load\n\n**Memory Refresh:** Update working memory as needed\n- Recent clarifications\n- Established shared understanding\n- Active domain context\n\n## Cognitive Anchors (Mental Models)\n\nThese metaphors help maintain clarity of purpose:\n\n- **Bridge Metaphor:** Connect different domains like a bridge connects different lands\n- **Translator Role:** Convert concepts between different systems of understanding\n- **Filter Function:** Validate and verify before passing information forward\n- **Context Keeper:** Maintain relevant context across interactions\n- **Precision Tool:** Aim for accuracy and clarity in all translations\n- **Quality Gate:** First line of defense against miscommunication\n\n## Summary\n\nThis cognitive framework provides the complete operational structure for semantic validation and ontological translation. Core principles:\n\n1. **Never assume** - Always verify understanding\n2. **Query systematically** - Static â†’ External â†’ Codebase\n3. **Translate carefully** - Preserve intent, maintain accuracy\n4. **Clarify conversationally** - Helpful, transparent, collaborative\n5. **Monitor continuously** - Self-assess, adapt, improve\n\nThe framework transforms vague human natural language into precise technical specifications through systematic semantic validation and collaborative clarification.\n",
        "plugins/semantic-linguist/skills/semantic-translation/references/decision-trees.md": "# Decision Trees for Semantic Validation\n\nDetailed decision trees and flowcharts for systematic semantic validation and ambiguity resolution.\n\n## Overview\n\nThese decision trees provide step-by-step guidance for analyzing user messages, detecting ambiguities, querying knowledge sources, and engaging in clarification dialogues. Use these trees to make systematic decisions during semantic validation.\n\n## Master Decision Tree\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   User Message Received         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n             â”‚\n             â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Initial Message Analysis        â”‚\nâ”‚ - Extract key terms             â”‚\nâ”‚ - Identify domain signals       â”‚\nâ”‚ - Detect action verbs           â”‚\nâ”‚ - Note scope indicators         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n             â”‚\n             â–¼\n        â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”\n        â”‚ Contains â”‚\n        â”‚meta-ques â”‚ Yes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n        â”‚ tion?    â”‚              â”‚\n        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜              â”‚\n             â”‚No                  â”‚\n             â–¼                    â–¼\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â”‚ Contains       â”‚  â”‚ HIGH Confidence  â”‚\n    â”‚ ambiguous      â”‚  â”‚ Ambiguity        â”‚\n    â”‚ action verb?   â”‚  â”‚ - \"am I making   â”‚\n    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   sense?\"        â”‚\n         â”‚              â”‚ - \"does this     â”‚\n         â”‚              â”‚   make sense?\"   â”‚\n         â–¼              â”‚ - \"non-technical â”‚\n    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”       â”‚   user\"          â”‚\n    â”‚   Yes    â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                â”‚\n         â”‚                       â”‚\n         â”‚                       â–¼\n         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n         â”‚              â”‚ VALIDATE         â”‚\n         â”‚              â”‚ IMMEDIATELY      â”‚\n         â”‚              â”‚ - Query last     â”‚\n         â”‚              â”‚   5-10 messages  â”‚\n         â”‚              â”‚ - Identify       â”‚\n         â”‚              â”‚   ambiguities    â”‚\n         â”‚              â”‚ - Ask            â”‚\n         â”‚              â”‚   clarifying     â”‚\n         â”‚              â”‚   questions      â”‚\n         â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚\n         â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Analyze action verb    â”‚\nâ”‚ - \"make it talk\"       â”‚\nâ”‚ - \"create api\"         â”‚\nâ”‚ - \"check for gaps\"     â”‚\nâ”‚ - \"make it portable\"   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚\n         â–¼\n    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”\n    â”‚ Domain   â”‚  Yes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â”‚ context  â”‚               â”‚\n    â”‚ clear?   â”‚               â–¼\n    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n         â”‚No          â”‚ Query Knowledge  â”‚\n         â”‚            â”‚ - Static domain  â”‚\n         â”‚            â”‚   knowledge      â”‚\n         â”‚            â”‚ - Technical      â”‚\n         â”‚            â”‚   mappings       â”‚\n         â”‚            â”‚ - Ontology       â”‚\n         â”‚            â”‚   graph          â”‚\n         â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚                     â”‚\n         â–¼                     â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ ASK: Which     â”‚    â”‚ Translation       â”‚\nâ”‚ framework/     â”‚    â”‚ found?            â”‚\nâ”‚ library are    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nâ”‚ you using?     â”‚             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â–¼\n                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”\n                      â”‚  Multiple    â”‚  Yes â”€â”€â”\n                      â”‚translations? â”‚        â”‚\n                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜        â”‚\n                               â”‚No             â”‚\n                               â”‚               â–¼\n                               â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                               â”‚     â”‚ Present Options  â”‚\n                               â”‚     â”‚ - List all       â”‚\n                               â”‚     â”‚ - Rank by        â”‚\n                               â”‚     â”‚   confidence     â”‚\n                               â”‚     â”‚ - Provide        â”‚\n                               â”‚     â”‚   context        â”‚\n                               â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                               â”‚              â”‚\n                               â”‚              â–¼\n                               â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                               â”‚     â”‚ User Selects     â”‚\n                               â–¼     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n                      â”‚ Confirm with     â”‚â—„â”€â”€â”€â”˜\n                      â”‚ User             â”‚\n                      â”‚ \"Did you mean    â”‚\n                      â”‚  [technical      â”‚\n                      â”‚   term]?\"        â”‚\n                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                               â”‚\n                               â–¼\n                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                      â”‚ User Confirms    â”‚\n                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                               â”‚\n                               â–¼\n                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                      â”‚ Proceed with     â”‚\n                      â”‚ Validated        â”‚\n                      â”‚ Terminology      â”‚\n                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n## Ambiguity Detection Decision Tree\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Analyze User Message            â”‚\nâ”‚ for Ambiguity Signals           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n             â”‚\n             â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Pattern Matching                      â”‚\nâ”‚ Check against known patterns:         â”‚\nâ”‚ 1. Meta-questions                     â”‚\nâ”‚ 2. Vague action verbs                 â”‚\nâ”‚ 3. Unclear scope                      â”‚\nâ”‚ 4. Domain confusion                   â”‚\nâ”‚ 5. Invented terms                     â”‚\nâ”‚ 6. Unclear references                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n             â”‚\n             â–¼\n        â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”\n        â”‚ Pattern  â”‚  No â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n        â”‚ detected?â”‚               â”‚\n        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜               â”‚\n             â”‚Yes                  â”‚\n             â–¼                     â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Calculate           â”‚  â”‚ LOW          â”‚\nâ”‚ Confidence Score    â”‚  â”‚ Confidence   â”‚\nâ”‚ - Pattern strength  â”‚  â”‚ < 30%        â”‚\nâ”‚ - Context clarity   â”‚  â”‚              â”‚\nâ”‚ - Domain signals    â”‚  â”‚ Monitor but  â”‚\nâ”‚ - Reference clarity â”‚  â”‚ don't        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ interrupt    â”‚\n         â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â–¼\n    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”\n    â”‚Confidenceâ”‚\n    â”‚  score?  â”‚\n    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n         â”‚\n   â”Œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n   â”‚     â”‚     â”‚         â”‚\n   â”‚     â”‚     â”‚         â”‚\n >80%  50-80% 30-50%   <30%\n   â”‚     â”‚     â”‚         â”‚\n   â”‚     â”‚     â”‚         â”‚\n   â–¼     â–¼     â–¼         â–¼\nâ”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚HIGHâ”‚â”‚MED â”‚â”‚LOW â”‚  â”‚IGNORE  â”‚\nâ””â”€â”€â”¬â”€â”˜â””â”€â”€â”¬â”€â”˜â””â”€â”€â”¬â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n   â”‚     â”‚     â”‚\n   â”‚     â”‚     â”‚\n   â–¼     â–¼     â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ VALIDATE                 â”‚\nâ”‚ IMMEDIATELY              â”‚\nâ”‚ - No questions asked     â”‚\nâ”‚ - Trigger validation     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n             â”‚\n             â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ VALIDATE IF              â”‚\nâ”‚ MULTIPLE                 â”‚\nâ”‚ INTERPRETATIONS          â”‚\nâ”‚ - Check knowledge        â”‚\nâ”‚ - If >1 option, validate â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n             â”‚\n             â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ MONITOR                  â”‚\nâ”‚ - Watch for additional   â”‚\nâ”‚   signals                â”‚\nâ”‚ - May validate if        â”‚\nâ”‚   combined with other    â”‚\nâ”‚   ambiguities            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n## Knowledge Query Decision Tree\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Ambiguity Detected              â”‚\nâ”‚ Need to translate to precise    â”‚\nâ”‚ technical terminology           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n             â”‚\n             â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ QUERY 1: Static Knowledge       â”‚\nâ”‚ Fastest, always available       â”‚\nâ”‚                                  â”‚\nâ”‚ Check:                           â”‚\nâ”‚ - knowledge/ambiguous-terms.json â”‚\nâ”‚ - knowledge/technical-mappings   â”‚\nâ”‚   .json                          â”‚\nâ”‚ - knowledge/ontology-graph.json  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n             â”‚\n             â–¼\n        â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”\n        â”‚ Found?   â”‚  Yes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n        â”‚          â”‚               â”‚\n        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜               â”‚\n             â”‚No                   â”‚\n             â”‚                     â–¼\n             â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n             â”‚           â”‚ Sufficient?      â”‚\n             â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n             â”‚                    â”‚\n             â”‚                â”Œâ”€â”€â”€â”´â”€â”€â”€â”\n             â”‚                â”‚       â”‚\n             â”‚               Yes      No\n             â”‚                â”‚       â”‚\n             â–¼                â”‚       â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ QUERY 2: External Docs  â”‚  â”‚  â”‚ QUERY 2:        â”‚\nâ”‚ Current, authoritative  â”‚  â”‚  â”‚ External Docs   â”‚\nâ”‚                         â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nâ”‚ Try:                    â”‚  â”‚           â”‚\nâ”‚ - WebFetch official     â”‚  â”‚           â”‚\nâ”‚   docs                  â”‚  â”‚           â”‚\nâ”‚ - context7 API refs     â”‚  â”‚           â”‚\nâ”‚ - deepwiki if available â”‚  â”‚           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚           â”‚\n         â”‚                   â”‚           â”‚\n         â–¼                   â”‚           â–¼\n    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”            â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â”‚ Found?   â”‚  Yes â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â–ºâ”‚ Combine â”‚\n    â”‚          â”‚            â”‚      â”‚ Results â”‚\n    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜            â”‚      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n         â”‚No                â”‚           â”‚\n         â”‚                  â”‚           â”‚\n         â–¼                  â”‚           â”‚\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚           â”‚\nâ”‚ QUERY 3: Codebase       â”‚â”‚           â”‚\nâ”‚ Project-specific usage  â”‚â”‚           â”‚\nâ”‚                         â”‚â”‚           â”‚\nâ”‚ Use:                    â”‚â”‚           â”‚\nâ”‚ - LSP symbol lookup     â”‚â”‚           â”‚\nâ”‚ - Grep for patterns     â”‚â”‚           â”‚\nâ”‚ - Find actual usage     â”‚â”‚           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚           â”‚\n         â”‚                 â”‚           â”‚\n         â–¼                 â”‚           â”‚\n    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”           â”‚           â”‚\n    â”‚ Found?   â”‚  Yes â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚\n    â”‚          â”‚           â”‚           â”‚\n    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜           â”‚           â”‚\n         â”‚No               â”‚           â”‚\n         â”‚                 â”‚           â”‚\n         â–¼                 â–¼           â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Aggregate Results                       â”‚\nâ”‚ - Combine all sources                   â”‚\nâ”‚ - Rank by:                              â”‚\nâ”‚   1. Codebase (most specific)           â”‚\nâ”‚   2. External docs (most current)       â”‚\nâ”‚   3. Static knowledge (most reliable)   â”‚\nâ”‚ - Remove duplicates                     â”‚\nâ”‚ - Calculate confidence scores           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n             â”‚\n             â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Results Analysis                          â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n             â”‚\n        â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”\n        â”‚ How many â”‚\n        â”‚ results? â”‚\n        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n             â”‚\n   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n   â”‚         â”‚          â”‚         â”‚\n   0         1        2-3       4+\n   â”‚         â”‚          â”‚         â”‚\n   â–¼         â–¼          â–¼         â–¼\nâ”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”\nâ”‚ASK â”‚  â”‚CONFIRM â”‚ â”‚OPTIONS â”‚ â”‚FILTERâ”‚\nâ”‚OPENâ”‚  â”‚w/ USER â”‚ â”‚Present â”‚ â”‚Top 3 â”‚\nâ”‚-ENDâ”‚  â”‚        â”‚ â”‚ranked  â”‚ â”‚Then  â”‚\nâ”‚    â”‚  â”‚        â”‚ â”‚list    â”‚ â”‚presentâ”‚\nâ””â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜\n```\n\n## Clarification Dialogue Decision Tree\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Need to Clarify with User       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n             â”‚\n             â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Determine Clarification Type     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n             â”‚\n        â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n        â”‚              â”‚              â”‚             â”‚\n        â–¼              â–¼              â–¼             â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ DOMAIN     â”‚  â”‚ SCOPE      â”‚ â”‚ OPTION    â”‚ â”‚ OPEN     â”‚\nâ”‚ Unclear    â”‚  â”‚ Unclear    â”‚ â”‚ Selection â”‚ â”‚ -ENDED   â”‚\nâ”‚ which      â”‚  â”‚ what type  â”‚ â”‚ multiple  â”‚ â”‚ No idea  â”‚\nâ”‚ framework? â”‚  â”‚ of X?      â”‚ â”‚ viable    â”‚ â”‚ what     â”‚\nâ”‚            â”‚  â”‚            â”‚ â”‚ interpre  â”‚ â”‚ they     â”‚\nâ”‚            â”‚  â”‚            â”‚ â”‚ -tations  â”‚ â”‚ mean     â”‚\nâ””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n     â”‚              â”‚              â”‚            â”‚\n     â–¼              â–¼              â–¼            â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ASK:      â”‚  â”‚ASK:      â”‚  â”‚PRESENT:     â”‚ â”‚ASK:        â”‚\nâ”‚\"Which    â”‚  â”‚\"What typeâ”‚  â”‚\"[Term]      â”‚ â”‚\"Can you    â”‚\nâ”‚framework â”‚  â”‚ of [X]?  â”‚  â”‚could mean:  â”‚ â”‚explain     â”‚\nâ”‚are you   â”‚  â”‚          â”‚  â”‚             â”‚ â”‚what you    â”‚\nâ”‚using?\"   â”‚  â”‚- Option Aâ”‚  â”‚1. [Tech A]  â”‚ â”‚mean by     â”‚\nâ”‚          â”‚  â”‚- Option Bâ”‚  â”‚   [Context] â”‚ â”‚[term]?\"    â”‚\nâ”‚- Autogen â”‚  â”‚- Option Câ”‚  â”‚             â”‚ â”‚            â”‚\nâ”‚- Langroidâ”‚  â”‚          â”‚  â”‚2. [Tech B]  â”‚ â”‚Or:         â”‚\nâ”‚- Other   â”‚  â”‚Which did â”‚  â”‚   [Context] â”‚ â”‚\"Could you  â”‚\nâ”‚          â”‚  â”‚you mean?\"â”‚  â”‚             â”‚ â”‚give an     â”‚\nâ””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚3. [Tech C]  â”‚ â”‚example?\"   â”‚\n     â”‚              â”‚        â”‚   [Context] â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n     â”‚              â”‚        â”‚             â”‚      â”‚\n     â”‚              â”‚        â”‚Which one?\"  â”‚      â”‚\n     â”‚              â”‚        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â”‚\n     â”‚              â”‚               â”‚             â”‚\n     â–¼              â–¼               â–¼             â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ User Responds                                      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n             â”‚\n             â–¼\n        â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”\n        â”‚ Clear?   â”‚  Yes â”€â”€â”€â”€â”€â”€â”€â”€â”\n        â”‚          â”‚              â”‚\n        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜              â”‚\n             â”‚No                  â”‚\n             â”‚                    â–¼\n             â–¼           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ VALIDATE         â”‚\n    â”‚ Iteration      â”‚  â”‚ UNDERSTANDING    â”‚\n    â”‚ Count?         â”‚  â”‚                  â”‚\n    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ \"So you want to  â”‚\n         â”‚              â”‚  [restate in     â”‚\n    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”         â”‚   precise terms] â”‚\n    â”‚  < 3   â”‚  Yes â”€â” â”‚                  â”‚\n    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜       â”‚ â”‚ Is that correct?\"â”‚\n         â”‚No         â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚           â”‚          â”‚\n         â–¼           â”‚          â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ ESCALATE       â”‚  â”‚ â”‚ User Confirms    â”‚\nâ”‚                â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nâ”‚\"I'm not sure   â”‚  â”‚          â”‚\nâ”‚I understand.   â”‚  â”‚          â–¼\nâ”‚Could you       â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚provide a       â”‚  â”‚ â”‚ PROCEED          â”‚\nâ”‚concrete        â”‚  â”‚ â”‚ with Validated   â”‚\nâ”‚example or      â”‚  â”‚ â”‚ Terminology      â”‚\nâ”‚reference?\"     â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n                    â”‚\n                    â–¼\n           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n           â”‚ FOLLOW-UP        â”‚\n           â”‚ CLARIFICATION    â”‚\n           â”‚ Ask more         â”‚\n           â”‚ specific         â”‚\n           â”‚ question         â”‚\n           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n## Domain Identification Decision Tree\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ User Request Analyzed           â”‚\nâ”‚ Domain Unknown                  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n             â”‚\n             â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Look for Domain Signals          â”‚\nâ”‚ - Framework names mentioned      â”‚\nâ”‚ - Library-specific terminology   â”‚\nâ”‚ - Import statements              â”‚\nâ”‚ - Project structure              â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n             â”‚\n        â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”\n        â”‚ Signals  â”‚  Yes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n        â”‚ found?   â”‚               â”‚\n        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜               â”‚\n             â”‚No                   â”‚\n             â”‚                     â–¼\n             â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n             â”‚           â”‚ Infer from       â”‚\n             â”‚           â”‚ Signals          â”‚\n             â”‚           â”‚                  â”‚\n             â”‚           â”‚ \"ConversableAgent\"â”‚\n             â”‚           â”‚ â†’ Autogen        â”‚\n             â”‚           â”‚                  â”‚\n             â”‚           â”‚ \"ChatAgent\"      â”‚\n             â”‚           â”‚ â†’ Langroid       â”‚\n             â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n             â”‚                    â”‚\n             â”‚                    â–¼\n             â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n             â”‚           â”‚ Confidence?      â”‚\n             â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n             â”‚                    â”‚\n             â”‚              â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”\n             â”‚              â”‚           â”‚\n             â”‚            High         Low\n             â”‚              â”‚           â”‚\n             â”‚              â”‚           â”‚\n             â–¼              â–¼           â–¼\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â”‚ ASK        â”‚ â”‚ CONFIRM  â”‚ â”‚ ASK       â”‚\n    â”‚ EXPLICITLY â”‚ â”‚          â”‚ â”‚ EXPLICITLYâ”‚\n    â”‚            â”‚ â”‚ \"I see   â”‚ â”‚           â”‚\n    â”‚\"Which     â”‚ â”‚ you're   â”‚ â”‚ \"Which    â”‚\n    â”‚framework  â”‚ â”‚ using    â”‚ â”‚ framework â”‚\n    â”‚are you    â”‚ â”‚ [domain] â”‚ â”‚ are you   â”‚\n    â”‚using?     â”‚ â”‚          â”‚ â”‚ using?\"   â”‚\n    â”‚           â”‚ â”‚ Is that  â”‚ â”‚           â”‚\n    â”‚- Autogen  â”‚ â”‚ correct?\"â”‚ â”‚ (No       â”‚\n    â”‚- Langroid â”‚ â”‚          â”‚ â”‚ options)  â”‚\n    â”‚- Other    â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n    â”‚           â”‚      â”‚\n    â”‚ (Please   â”‚      â–¼\n    â”‚ specify)\" â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚ User Confirmsâ”‚\n          â”‚       â”‚ or Corrects  â”‚\n          â”‚       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n          â”‚              â”‚\n          â–¼              â–¼\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â”‚ User Specifies Domainâ”‚\n    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n               â”‚\n               â–¼\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â”‚ Load Domain          â”‚\n    â”‚ Knowledge            â”‚\n    â”‚                      â”‚\n    â”‚ - Domain ontologies  â”‚\n    â”‚ - Technical mappings â”‚\n    â”‚ - Known ambiguities  â”‚\n    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n               â”‚\n               â–¼\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â”‚ Set Domain Context   â”‚\n    â”‚ for Session          â”‚\n    â”‚                      â”‚\n    â”‚ Remember for future  â”‚\n    â”‚ messages in this     â”‚\n    â”‚ conversation         â”‚\n    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n## Confidence Scoring Decision Tree\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Calculate Confidence Score      â”‚\nâ”‚ for Ambiguity Detection         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n             â”‚\n             â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Analyze Multiple Factors         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n             â”‚\n       â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n       â”‚           â”‚         â”‚         â”‚\n       â–¼           â–¼         â–¼         â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Pattern  â”‚â”‚ Context  â”‚â”‚Domainâ”‚â”‚Referenceâ”‚\nâ”‚ Strength â”‚â”‚ Clarity  â”‚â”‚Signalsâ”‚â”‚ Clarity â”‚\nâ”‚          â”‚â”‚          â”‚â”‚      â”‚â”‚         â”‚\nâ”‚ Known    â”‚â”‚ Recent   â”‚â”‚Frame-â”‚â”‚ Clear   â”‚\nâ”‚ ambiguousâ”‚â”‚ messages â”‚â”‚work  â”‚â”‚ ante-   â”‚\nâ”‚ patterns?â”‚â”‚ provide  â”‚â”‚men-  â”‚â”‚ cedents?â”‚\nâ”‚          â”‚â”‚ context? â”‚â”‚tionedâ”‚â”‚         â”‚\nâ”‚ +30 pts  â”‚â”‚ +20 pts  â”‚â”‚+15ptsâ”‚â”‚ -20 pts â”‚\nâ””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜â””â”€â”€â”¬â”€â”€â”€â”˜â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n     â”‚           â”‚         â”‚         â”‚\n     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n           â”‚          â”‚         â”‚\n           â–¼          â–¼         â–¼\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â”‚ Sum Confidence Score (0-100) â”‚\n    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                 â”‚\n            â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”\n            â”‚         â”‚        â”‚        â”‚\n          >80%     50-80%   30-50%    <30%\n            â”‚         â”‚        â”‚        â”‚\n            â”‚         â”‚        â”‚        â”‚\n            â–¼         â–¼        â–¼        â–¼\n      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”\n      â”‚ HIGH    â”‚â”‚MEDIUM â”‚â”‚ LOW  â”‚â”‚ IGNORE â”‚\n      â”‚         â”‚â”‚       â”‚â”‚      â”‚â”‚        â”‚\n      â”‚Validate â”‚â”‚Check  â”‚â”‚Watch â”‚â”‚Proceed â”‚\n      â”‚immedi-  â”‚â”‚if >1  â”‚â”‚for   â”‚â”‚normallyâ”‚\n      â”‚ately    â”‚â”‚option â”‚â”‚more  â”‚â”‚        â”‚\n      â”‚         â”‚â”‚then   â”‚â”‚signalsâ”‚â”‚       â”‚\n      â”‚         â”‚â”‚validateâ”‚â”‚      â”‚â”‚        â”‚\n      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n## Summary\n\nThese decision trees provide systematic guidance for:\n\n1. **Initial message analysis** - Detecting ambiguity signals\n2. **Knowledge querying** - Three-tier query workflow\n3. **Clarification dialogue** - Conversational validation\n4. **Domain identification** - Framework/library detection\n5. **Confidence scoring** - Determining when to intervene\n\nUse these trees to make consistent, reliable decisions during semantic validation processes.\n",
        "plugins/semantic-linguist/skills/semantic-translation/references/domain-ontologies.md": "# Domain Ontologies for Semantic Validation\n\nComprehensive domain knowledge graphs for Autogen, Langroid, and general multi-agent framework concepts. Use these ontologies to validate terminology and translate between domains.\n\n## Overview\n\nDomain ontologies provide structured knowledge about technical domains including:\n- **Conceptual hierarchies** - IS-A relationships (e.g., ConversableAgent IS-A Agent)\n- **Compositional relationships** - HAS-A/PART-OF (e.g., GroupChat HAS-A participants list)\n- **Functional relationships** - DOES/USES (e.g., Agent USES LLM)\n- **Cross-domain equivalents** - Similar concepts in different frameworks\n\n## Autogen Domain Ontology\n\n### Core Concepts Hierarchy\n\n```\nAgent (Abstract Base)\nâ”‚\nâ”œâ”€â”€ ConversableAgent\nâ”‚   â”œâ”€â”€ Purpose: Chat-based interactions with LLMs\nâ”‚   â”œâ”€â”€ Key Methods:\nâ”‚   â”‚   â”œâ”€â”€ send(message, recipient) - Send message to another agent\nâ”‚   â”‚   â”œâ”€â”€ receive(message, sender) - Receive message from another agent\nâ”‚   â”‚   â”œâ”€â”€ generate_reply() - Generate LLM response\nâ”‚   â”‚   â””â”€â”€ register_reply() - Register custom reply function\nâ”‚   â”œâ”€â”€ Attributes:\nâ”‚   â”‚   â”œâ”€â”€ name - Agent identifier\nâ”‚   â”‚   â”œâ”€â”€ system_message - Persona/instructions\nâ”‚   â”‚   â”œâ”€â”€ llm_config - LLM configuration\nâ”‚   â”‚   â””â”€â”€ human_input_mode - When to request human input\nâ”‚   â””â”€â”€ Use Cases:\nâ”‚       â”œâ”€â”€ Basic chatbot\nâ”‚       â”œâ”€â”€ Assistant with specific expertise\nâ”‚       â””â”€â”€ Participant in multi-agent conversation\nâ”‚\nâ”œâ”€â”€ AssistantAgent (extends ConversableAgent)\nâ”‚   â”œâ”€â”€ Purpose: Task execution with tool/function calling\nâ”‚   â”œâ”€â”€ Additional Capabilities:\nâ”‚   â”‚   â”œâ”€â”€ Function calling\nâ”‚   â”‚   â”œâ”€â”€ Code execution\nâ”‚   â”‚   â””â”€â”€ Task planning\nâ”‚   â”œâ”€â”€ Key Methods:\nâ”‚   â”‚   â”œâ”€â”€ register_for_llm() - Register function for LLM calling\nâ”‚   â”‚   â””â”€â”€ register_for_execution() - Register function for execution\nâ”‚   â””â”€â”€ Use Cases:\nâ”‚       â”œâ”€â”€ Assistant that can execute code\nâ”‚       â”œâ”€â”€ Agent with external tools\nâ”‚       â””â”€â”€ Task automation agent\nâ”‚\nâ””â”€â”€ UserProxyAgent (extends ConversableAgent)\n    â”œâ”€â”€ Purpose: Human-in-the-loop interaction\n    â”œâ”€â”€ Key Features:\n    â”‚   â”œâ”€â”€ Represents human user\n    â”‚   â”œâ”€â”€ Can execute code on behalf of human\n    â”‚   â””â”€â”€ Requests human input when needed\n    â”œâ”€â”€ Attributes:\n    â”‚   â”œâ”€â”€ human_input_mode:\n    â”‚   â”‚   â”œâ”€â”€ \"ALWAYS\" - Always ask human\n    â”‚   â”‚   â”œâ”€â”€ \"NEVER\" - Never ask human\n    â”‚   â”‚   â””â”€â”€ \"TERMINATE\" - Ask on termination\n    â”‚   â””â”€â”€ code_execution_config - Code execution settings\n    â””â”€â”€ Use Cases:\n        â”œâ”€â”€ Human interface agent\n        â”œâ”€â”€ Code execution proxy\n        â””â”€â”€ Approval workflow agent\n```\n\n### Multi-Agent Orchestration\n\n```\nGroupChat\nâ”œâ”€â”€ Purpose: Multi-agent conversation orchestration\nâ”œâ”€â”€ Components:\nâ”‚   â”œâ”€â”€ agents - List of participating agents\nâ”‚   â”œâ”€â”€ messages - Conversation history\nâ”‚   â”œâ”€â”€ max_round - Maximum conversation rounds\nâ”‚   â””â”€â”€ speaker_selection_method - How to select next speaker\nâ”œâ”€â”€ Speaker Selection Methods:\nâ”‚   â”œâ”€â”€ \"auto\" - LLM decides next speaker\nâ”‚   â”œâ”€â”€ \"manual\" - Human selects next speaker\nâ”‚   â”œâ”€â”€ \"random\" - Random selection\nâ”‚   â””â”€â”€ \"round_robin\" - Sequential rotation\nâ””â”€â”€ Managed by: GroupChatManager\n\nGroupChatManager (extends ConversableAgent)\nâ”œâ”€â”€ Purpose: Manage GroupChat conversation flow\nâ”œâ”€â”€ Responsibilities:\nâ”‚   â”œâ”€â”€ Select next speaker\nâ”‚   â”œâ”€â”€ Broadcast messages to participants\nâ”‚   â”œâ”€â”€ Enforce conversation rules\nâ”‚   â””â”€â”€ Terminate conversation when complete\nâ””â”€â”€ Relationship: MANAGES GroupChat\n```\n\n### Communication Patterns\n\n```\nOne-to-One Communication\nâ”œâ”€â”€ Agent A.send(message, Agent B)\nâ”œâ”€â”€ Agent B.receive(message, Agent A)\nâ”œâ”€â”€ Agent B.generate_reply()\nâ””â”€â”€ Agent B.send(reply, Agent A)\n\nInitiate Chat Pattern\nâ”œâ”€â”€ agent.initiate_chat(recipient, message)\nâ”œâ”€â”€ Automatically handles:\nâ”‚   â”œâ”€â”€ Send/receive loop\nâ”‚   â”œâ”€â”€ Reply generation\nâ”‚   â””â”€â”€ Termination detection\n\nGroup Chat Pattern\nâ”œâ”€â”€ GroupChat([agent1, agent2, agent3])\nâ”œâ”€â”€ GroupChatManager(groupchat)\nâ”œâ”€â”€ manager.initiate_chat(groupchat)\nâ””â”€â”€ Manager orchestrates multi-party conversation\n```\n\n### Function/Tool Calling\n\n```\nTool Integration\nâ”œâ”€â”€ Define Function\nâ”‚   â””â”€â”€ Python function with type hints\nâ”œâ”€â”€ Register for LLM\nâ”‚   â”œâ”€â”€ @assistant.register_for_llm()\nâ”‚   â””â”€â”€ LLM can request function execution\nâ”œâ”€â”€ Register for Execution\nâ”‚   â”œâ”€â”€ @user_proxy.register_for_execution()\nâ”‚   â””â”€â”€ Agent can execute function\nâ””â”€â”€ Workflow:\n    â”œâ”€â”€ LLM generates function call\n    â”œâ”€â”€ UserProxy executes function\n    â”œâ”€â”€ Result returned to conversation\n    â””â”€â”€ LLM processes result\n```\n\n### Common Autogen Ambiguities\n\n| Ambiguous Term | Possible Technical Meanings | Context Clues |\n|----------------|----------------------------|---------------|\n| \"agent\" | ConversableAgent, AssistantAgent, UserProxyAgent | Task type, tool usage, human interaction |\n| \"talk\" | send(), initiate_chat(), GroupChat | One-to-one vs group context |\n| \"group chat\" | GroupChat object, multi-agent conversation | Orchestration vs general concept |\n| \"tools\" | register_for_llm(), function calling, code execution | Execution context |\n| \"message\" | send() parameter, Message object, conversation content | Data type vs action |\n\n## Langroid Domain Ontology\n\n### Core Concepts Hierarchy\n\n```\nAgent (Base Class)\nâ”‚\nâ”œâ”€â”€ ChatAgent\nâ”‚   â”œâ”€â”€ Purpose: Basic conversational agent with LLM\nâ”‚   â”œâ”€â”€ Key Components:\nâ”‚   â”‚   â”œâ”€â”€ config - Agent configuration\nâ”‚   â”‚   â”œâ”€â”€ llm - Language model instance\nâ”‚   â”‚   â””â”€â”€ vecdb - Optional vector database\nâ”‚   â”œâ”€â”€ Key Methods:\nâ”‚   â”‚   â”œâ”€â”€ llm_response() - Get LLM response\nâ”‚   â”‚   â”œâ”€â”€ agent_response() - Agent's response logic\nâ”‚   â”‚   â””â”€â”€ handle_message() - Process incoming message\nâ”‚   â””â”€â”€ Use Cases:\nâ”‚       â”œâ”€â”€ Simple chatbot\nâ”‚       â”œâ”€â”€ Q&A agent\nâ”‚       â””â”€â”€ Base for specialized agents\nâ”‚\nâ””â”€â”€ ToolAgent (may extend ChatAgent)\n    â”œâ”€â”€ Purpose: Agent with tool/function calling\n    â”œâ”€â”€ Additional Capabilities:\n    â”‚   â”œâ”€â”€ Tool registration\n    â”‚   â”œâ”€â”€ Tool execution\n    â”‚   â””â”€â”€ Tool result handling\n    â”œâ”€â”€ Tool Definition:\n    â”‚   â”œâ”€â”€ ToolMessage class\n    â”‚   â””â”€â”€ @tool decorator\n    â””â”€â”€ Use Cases:\n        â”œâ”€â”€ Agent with external APIs\n        â”œâ”€â”€ Function-calling agent\n        â””â”€â”€ Tool-augmented assistant\n```\n\n### Task Orchestration\n\n```\nTask\nâ”œâ”€â”€ Purpose: Coordinate agent activities\nâ”œâ”€â”€ Components:\nâ”‚   â”œâ”€â”€ agent - Associated agent\nâ”‚   â”œâ”€â”€ name - Task identifier\nâ”‚   â””â”€â”€ interactive - Allow human input?\nâ”œâ”€â”€ Methods:\nâ”‚   â”œâ”€â”€ run() - Execute task\nâ”‚   â”œâ”€â”€ step() - Single task step\nâ”‚   â””â”€â”€ add_subtask() - Add dependent task\nâ”œâ”€â”€ Orchestration Patterns:\nâ”‚   â”œâ”€â”€ Sequential tasks\nâ”‚   â”œâ”€â”€ Parallel tasks\nâ”‚   â””â”€â”€ Hierarchical task decomposition\nâ””â”€â”€ Delegation:\n    â”œâ”€â”€ Task can delegate to other agents\n    â””â”€â”€ Forms agent collaboration graph\n```\n\n### Message Types\n\n```\nMessage Hierarchy\nâ”‚\nâ”œâ”€â”€ LLMMessage\nâ”‚   â”œâ”€â”€ Purpose: Message to/from LLM\nâ”‚   â”œâ”€â”€ Fields: role, content\nâ”‚   â””â”€â”€ Roles: system, user, assistant\nâ”‚\nâ”œâ”€â”€ ToolMessage\nâ”‚   â”œâ”€â”€ Purpose: Tool/function call\nâ”‚   â”œâ”€â”€ Fields: tool_name, parameters, result\nâ”‚   â””â”€â”€ Used for: Function calling, external APIs\nâ”‚\nâ””â”€â”€ AgentMessage\n    â”œâ”€â”€ Purpose: Inter-agent communication\n    â””â”€â”€ Custom message types via subclassing\n```\n\n### Agent Collaboration\n\n```\nMulti-Agent Patterns in Langroid\n\n1. Task Delegation\n   â”œâ”€â”€ ParentTask.add_subtask(ChildTask)\n   â”œâ”€â”€ Parent delegates to child agent\n   â””â”€â”€ Child result returned to parent\n\n2. Agent Teams\n   â”œâ”€â”€ Multiple agents collaborate on task\n   â”œâ”€â”€ Each agent has specialized role\n   â””â”€â”€ Coordinated via Task orchestration\n\n3. Sequential Processing\n   â”œâ”€â”€ Agent A -> Agent B -> Agent C\n   â”œâ”€â”€ Output of one becomes input to next\n   â””â”€â”€ Pipeline pattern\n\n4. Parallel Processing\n   â”œâ”€â”€ Multiple agents work simultaneously\n   â”œâ”€â”€ Results aggregated\n   â””â”€â”€ Coordination agent combines results\n```\n\n### Common Langroid Ambiguities\n\n| Ambiguous Term | Possible Technical Meanings | Context Clues |\n|----------------|----------------------------|---------------|\n| \"agent\" | ChatAgent, ToolAgent, custom agent subclass | Tool usage, specialization |\n| \"task\" | Task object, general task concept, subtask | Orchestration context |\n| \"message\" | LLMMessage, ToolMessage, AgentMessage | Message type, sender/recipient |\n| \"tool\" | ToolMessage, @tool decorator, external API | Definition vs usage |\n| \"response\" | llm_response(), agent_response() | LLM vs agent logic |\n\n## Cross-Domain Ontology Mapping\n\n### Concept Equivalents\n\n| Concept | Autogen | Langroid | General |\n|---------|---------|----------|---------|\n| Basic Agent | ConversableAgent | ChatAgent | AI agent, chatbot |\n| Tool-Using Agent | AssistantAgent | ToolAgent | Function-calling agent |\n| Human Interface | UserProxyAgent | interactive Task | Human-in-loop agent |\n| Multi-Agent Chat | GroupChat + Manager | Task delegation | Multi-agent system |\n| Message Sending | send() | handle_message() | Communicate |\n| LLM Call | generate_reply() | llm_response() | Query LLM |\n| Tool Registration | register_for_llm() | @tool decorator | Add function |\n| Conversation | initiate_chat() | Task.run() | Start interaction |\n\n### Terminology Translation Patterns\n\n**\"Make it talk\" translations:**\n```\nAutogen Context:\n- ConversableAgent.send() - Send single message\n- initiate_chat() - Start conversation\n- GroupChat - Multi-party conversation\n\nLangroid Context:\n- agent.llm_response() - Get LLM response\n- agent.agent_response() - Agent's reply logic\n- Task.run() - Execute conversational task\n\nGeneral Context:\n- print() / console.log() - Simple output\n- Speech synthesis API - Text-to-speech\n- Chat UI - User interface for conversation\n```\n\n**\"Create agent\" translations:**\n```\nAutogen:\n- ConversableAgent(name, system_message, llm_config)\n- AssistantAgent(...) for tool usage\n- UserProxyAgent(...) for human interface\n\nLangroid:\n- ChatAgent(config) - Basic agent\n- ToolAgent(...) - Agent with tools\n- Subclass Agent for custom behavior\n\nGeneral:\n- Depends on framework being used\n- Need to know: framework, agent purpose, tools needed\n```\n\n**\"Group conversation\" translations:**\n```\nAutogen:\n- GroupChat(agents, messages, max_round)\n- GroupChatManager(groupchat)\n- manager.initiate_chat()\n\nLangroid:\n- Parent Task with multiple subtasks\n- Each subtask has different agent\n- Task delegation pattern\n\nGeneral:\n- Multi-agent system\n- Agent collaboration\n- Coordinated agents\n```\n\n## Framework-Agnostic Concepts\n\n### Universal Multi-Agent Patterns\n\n**Agent Types (Framework-Agnostic)**\n```\n1. Conversational Agent\n   - Purpose: Chat with users or other agents\n   - Capabilities: Natural language understanding, response generation\n   - Examples: Customer service bot, Q&A assistant\n\n2. Task Agent\n   - Purpose: Execute specific tasks\n   - Capabilities: Task planning, execution, tool use\n   - Examples: Code executor, data processor\n\n3. Coordinator Agent\n   - Purpose: Orchestrate other agents\n   - Capabilities: Task delegation, result aggregation\n   - Examples: Manager agent, orchestrator\n\n4. Tool Agent\n   - Purpose: Interface with external systems\n   - Capabilities: API calls, function execution\n   - Examples: Search agent, database agent\n\n5. Human-Proxy Agent\n   - Purpose: Represent human in AI system\n   - Capabilities: Request human input, execute on behalf of human\n   - Examples: Approval agent, human-in-loop\n```\n\n**Communication Patterns (Framework-Agnostic)**\n```\n1. Request-Response\n   - Agent A requests â†’ Agent B responds\n   - Synchronous, one-to-one\n\n2. Broadcast\n   - Agent broadcasts â†’ Multiple agents receive\n   - One-to-many\n\n3. Publish-Subscribe\n   - Agent publishes to topic â†’ Subscribers receive\n   - Decoupled many-to-many\n\n4. Chain\n   - Agent A â†’ Agent B â†’ Agent C â†’ Result\n   - Sequential processing pipeline\n\n5. Tree\n   - Root agent â†’ Child agents â†’ Grandchild agents\n   - Hierarchical task decomposition\n```\n\n## Validation Using Ontologies\n\n### Process for Ontology-Based Validation\n\n1. **Identify User's Term**\n   - Extract key technical terms from user message\n   - Flag ambiguous or generic terms\n\n2. **Query Domain Ontology**\n   - Load appropriate domain ontology (Autogen/Langroid/General)\n   - Find concept in ontology hierarchy\n   - Retrieve related concepts\n\n3. **Analyze Context**\n   - Check surrounding terms for domain signals\n   - Look for hierarchical clues (parent/child concepts)\n   - Identify compositional relationships (has-a, part-of)\n\n4. **Generate Interpretations**\n   - List all possible meanings from ontology\n   - Rank by context fit\n   - Include cross-domain equivalents if domain unclear\n\n5. **Present to User**\n   - Show precise technical terms\n   - Explain differences between options\n   - Ask user to select or clarify\n\n### Example Validation Workflow\n\n**User says:** \"I want to create an agent that can talk to other agents\"\n\n**Step 1: Extract terms**\n- \"agent\" - ambiguous (which type?)\n- \"talk to other agents\" - ambiguous (what kind of communication?)\n\n**Step 2: Query ontology (domain unclear)**\n- Autogen: ConversableAgent, AssistantAgent, UserProxyAgent\n- Langroid: ChatAgent, ToolAgent\n- Need to identify domain first\n\n**Step 3: Ask domain clarification**\n- \"Which framework are you using? Autogen, Langroid, or something else?\"\n\n**Step 4: User says \"Autogen\"**\n- Load Autogen ontology\n- \"agent\" â†’ ConversableAgent (most likely for chat)\n- \"talk to\" â†’ send(), initiate_chat(), or GroupChat\n\n**Step 5: Present options**\n```\nIn Autogen, \"agent that can talk to other agents\" could mean:\n\n1. ConversableAgent with send()\n   - For one-to-one messaging between agents\n   - agent.send(message, recipient)\n\n2. ConversableAgent in GroupChat\n   - For multi-party conversations\n   - GroupChat([agent1, agent2, agent3])\n   - GroupChatManager orchestrates\n\nWhich type of communication did you have in mind?\n```\n\n## Extending the Ontology\n\n### Adding New Domains\n\nTo add a new framework/domain:\n\n1. **Create ontology structure**\n   - Core concepts hierarchy\n   - Relationships (IS-A, HAS-A, DOES)\n   - Common ambiguities\n\n2. **Map to general concepts**\n   - Identify equivalent general concepts\n   - Create cross-domain mappings\n   - Document terminology translations\n\n3. **Update knowledge files**\n   - Add to `knowledge/technical-mappings.json`\n   - Update `knowledge/ontology-graph.json`\n   - Add domain-specific ambiguities to `knowledge/ambiguous-terms.json`\n\n4. **Create examples**\n   - Add `examples/[domain]-mappings.md`\n   - Document common validation scenarios\n   - Include real-world ambiguity resolutions\n\n### Maintaining Ontologies\n\n**Keep ontologies current:**\n- Monitor framework updates for new concepts\n- Add new agent types, methods, patterns as frameworks evolve\n- Update cross-domain mappings when equivalents change\n- Deprecate outdated concepts gracefully\n\n**Validate ontology accuracy:**\n- Cross-reference with official documentation\n- Test mappings with real user queries\n- Gather feedback on translation quality\n- Refine based on usage patterns\n\n## Summary\n\nDomain ontologies provide the structured knowledge foundation for semantic validation. Use them to:\n\n1. **Validate terminology** - Check if terms match domain concepts\n2. **Translate ambiguous terms** - Map to precise technical concepts\n3. **Identify relationships** - Understand concept hierarchies and connections\n4. **Cross-domain mapping** - Translate between frameworks\n5. **Generate clarifications** - Present options based on ontology structure\n\nThe ontologies bridge the gap between user's natural language and framework-specific technical precision.\n",
        "plugins/semantic-linguist/skills/semantic-translation/references/translation-patterns.md": "# Translation Patterns: Ambiguous â†’ Precise\n\nExtensive catalog of ambiguous terminology mapped to precise technical concepts. Use these patterns for quick translation during semantic validation.\n\n## Overview\n\nThis document provides comprehensive translation patterns organized by:\n1. **Common ambiguities** - Frequently encountered vague terms\n2. **Domain-specific translations** - Framework-specific interpretations\n3. **Context clues** - Signals that indicate which translation to use\n4. **Clarification templates** - How to present options to users\n\n## Top-Tier Ambiguities (User-Reported)\n\n### 1. \"make it talk\"\n\n**Ambiguity Score:** 0.9 (very high)\n\n**Possible Interpretations:**\n\n| Interpretation | Domain | Technical Term | Context Clues |\n|----------------|--------|----------------|---------------|\n| Send message | Autogen | `ConversableAgent.send()` | Single message, one recipient |\n| Enable conversation | Autogen | Register `ConversableAgent` | Create agent capability |\n| Group chat | Autogen | `GroupChat` + `GroupChatManager` | Multiple agents, conversation |\n| LLM response | Langroid | `agent.llm_response()` | Generate text response |\n| Agent reply | Langroid | `agent.agent_response()` | Agent logic response |\n| Speech synthesis | General | TTS API, speech library | Audio output mentioned |\n| Chat interface | General | UI component | Frontend, user interaction |\n| Console output | General | `print()`, `console.log()` | Simple display |\n\n**Clarification Template:**\n```\n\"'make it talk' could mean several things:\n\n1. **Send a message** (Autogen: ConversableAgent.send())\n   - Send a single message to another agent\n   - Example: agent.send(\"Hello\", recipient)\n\n2. **Enable conversation capability** (Autogen: ConversableAgent)\n   - Create an agent that can participate in conversations\n   - Example: ConversableAgent(name=\"...\", system_message=\"...\")\n\n3. **Set up group chat** (Autogen: GroupChat)\n   - Multi-party conversation among multiple agents\n   - Example: GroupChat([agent1, agent2, agent3])\n\nWhich kind of communication are you looking for?\"\n```\n\n### 2. \"we need an api\"\n\n**Ambiguity Score:** 0.85 (very high)\n\n**Possible Interpretations:**\n\n| Interpretation | Domain | Technical Term | Context Clues |\n|----------------|--------|----------------|---------------|\n| REST API server | Backend | Express, FastAPI, Flask | \"handle requests\", \"endpoints\" |\n| GraphQL API | Backend | Apollo Server, Strawberry | \"queries\", \"mutations\", \"schema\" |\n| API client | Integration | requests, axios, fetch | \"call external\", \"consume\", \"integrate with\" |\n| API design/spec | Documentation | OpenAPI, Swagger | \"documentation\", \"spec\", \"contract\" |\n| Internal API | Architecture | Module interface, class methods | \"clean interface\", \"module boundary\" |\n| Webhook API | Event-driven | Webhook handlers | \"receive events\", \"callbacks\" |\n| RPC API | Distributed | gRPC, tRPC | \"remote procedure\", \"type-safe\" |\n\n**Clarification Template:**\n```\n\"'API' can mean different things depending on context:\n\n1. **HTTP REST API server** (to handle incoming requests)\n   - Framework: Express (Node.js), FastAPI (Python), etc.\n   - Use case: Build backend service that responds to HTTP requests\n\n2. **API client** (to call external services)\n   - Library: axios, requests, fetch\n   - Use case: Integrate with external APIs (GitHub, Stripe, etc.)\n\n3. **API design/specification** (OpenAPI/Swagger docs)\n   - Tools: OpenAPI, Swagger, Postman\n   - Use case: Document API contract for frontend/clients\n\n4. **Internal API/interface** (code architecture)\n   - Concept: Module boundaries, clean interfaces\n   - Use case: Organize code with clear public interfaces\n\nWhich type of API are you building?\"\n```\n\n### 3. \"make it portable\"\n\n**Ambiguity Score:** 0.88 (very high)\n\n**Possible Interpretations:**\n\n| Interpretation | Domain | Technical Term | Context Clues |\n|----------------|--------|----------------|---------------|\n| Containerization | DevOps | Docker, Podman, container image | \"deployment\", \"infrastructure\" |\n| Cross-platform | Build | Windows/Mac/Linux compatibility | \"operating systems\", \"platform\" |\n| Dependency vendoring | Packaging | Bundle dependencies in package | \"dependencies\", \"standalone\" |\n| Standalone executable | Distribution | PyInstaller, pkg, Electron | \"binary\", \"executable\", \"no install\" |\n| Cloud-portable | Infrastructure | Terraform, Kubernetes manifests | \"cloud provider\", \"migration\" |\n| Source portable | Code | POSIX-compliant, cross-platform code | \"source code\", \"compile anywhere\" |\n\n**Clarification Template:**\n```\n\"'Portable' has multiple interpretations:\n\n1. **Containerized** (Docker/Podman)\n   - Package app + dependencies in container\n   - Run anywhere with container runtime\n   - Best for: Server applications, microservices\n\n2. **Cross-platform** (Windows/Mac/Linux)\n   - Build native versions for each OS\n   - Ensure dependencies available on all platforms\n   - Best for: Desktop applications, CLI tools\n\n3. **Dependency vendoring** (bundled dependencies)\n   - Include all dependencies in distribution\n   - No external installation required\n   - Best for: Python packages, Node modules\n\n4. **Standalone executable** (single binary)\n   - Compile to single executable file\n   - No runtime dependencies\n   - Best for: CLI tools, utilities\n\nWhat kind of portability do you need?\"\n```\n\n### 4. \"check for gaps\"\n\n**Ambiguity Score:** 0.82 (high)\n\n**Possible Interpretations:**\n\n| Interpretation | Domain | Technical Term | Context Clues |\n|----------------|--------|----------------|---------------|\n| Code coverage gaps | Testing | Coverage report, uncovered lines | \"tests\", \"coverage\", \"untested\" |\n| Documentation gaps | Docs | Missing docstrings, incomplete guides | \"documentation\", \"readme\", \"docs\" |\n| Feature gaps | Product | Missing functionality, incomplete features | \"requirements\", \"features\", \"backlog\" |\n| Security gaps | Security | Vulnerabilities, security holes | \"security\", \"vulnerabilities\", \"CVE\" |\n| Data gaps | Data | Missing data, incomplete records | \"data quality\", \"missing values\", \"null\" |\n| Logic gaps | Code Review | Edge cases, error handling | \"logic\", \"edge cases\", \"validation\" |\n| Dependency gaps | Dependencies | Outdated packages, missing libs | \"dependencies\", \"packages\", \"versions\" |\n\n**Clarification Template:**\n```\n\"'Check for gaps' could mean different types of analysis:\n\n1. **Test coverage gaps** (uncovered code)\n   - Tool: pytest --cov, jest --coverage\n   - Find: Lines/functions without tests\n   - Output: Coverage report with missing tests\n\n2. **Documentation gaps** (missing/incomplete docs)\n   - Check: Docstrings, README, API docs\n   - Find: Undocumented functions, unclear guides\n   - Output: List of items needing documentation\n\n3. **Feature gaps** (incomplete functionality)\n   - Compare: Requirements vs implemented features\n   - Find: Missing features, incomplete workflows\n   - Output: Feature backlog, TODO items\n\n4. **Security gaps** (vulnerabilities)\n   - Tool: SAST, dependency scanners\n   - Find: Security vulnerabilities, unsafe code\n   - Output: Security report, CVE list\n\nWhich type of gap analysis did you have in mind?\"\n```\n\n### 5. \"am I making sense?\" (User Trigger)\n\n**Ambiguity Score:** N/A (meta-question, not technical ambiguity)\n\n**Intent:** User seeking validation of their explanation\n\n**Response Pattern:**\n```\n\"Let me verify I understand correctly. Based on your recent messages:\n\n- [Summarize key point 1 from conversation]\n- [Summarize key point 2 from conversation]\n- [Summarize key point 3 from conversation]\n\nI noticed some terminology that could be clarified:\n- '[Ambiguous term 1]' - Did you mean [precise option A] or [precise option B]?\n- '[Ambiguous term 2]' - This could refer to [technical concept X]\n\nDoes this match what you were trying to communicate?\"\n```\n\n**Process:**\n1. Analyze last 5-10 messages in conversation\n2. Extract main points and intentions\n3. Identify any ambiguous terminology used\n4. Present summary + clarifying questions\n5. Wait for user confirmation\n\n## Generic Ambiguous Action Verbs\n\n### \"make it [X]\"\n\nPattern of vague action verbs that need clarification:\n\n| User Says | Likely Means | Domain Context | Clarification Needed |\n|-----------|--------------|----------------|----------------------|\n| \"make it work\" | Fix bug, implement feature, configure correctly | Any | What specifically isn't working? |\n| \"make it faster\" | Optimize performance, reduce latency, cache | Performance | Which metric? Response time, throughput, etc. |\n| \"make it secure\" | Add authentication, encryption, input validation | Security | Which security aspect? Auth, data protection, etc. |\n| \"make it pretty\" | UI styling, UX improvements, visual design | Frontend | Specific elements or general styling? |\n| \"make it scalable\" | Horizontal scaling, load balancing, optimization | Architecture | Expected scale? Concurrent users, data volume? |\n| \"make it better\" | Improve (unspecified aspect) | Any | Better in what way? Performance, UX, reliability? |\n\n**Standard Clarification:**\n```\n\"'Make it [X]' is a bit general. Could you specify:\n- What aspect needs to be [X]?\n- What would [X] look like in concrete terms?\n- Are there specific metrics or goals for [X]?\"\n```\n\n## Domain-Specific Patterns\n\n### Autogen-Specific Translations\n\n#### Agent Creation Patterns\n\n| User Says | Precise Translation | Implementation |\n|-----------|---------------------|----------------|\n| \"create an agent\" | ConversableAgent instantiation | `ConversableAgent(name=\"...\", system_message=\"...\", llm_config={...})` |\n| \"agent with tools\" | AssistantAgent with register_for_llm | `AssistantAgent(...)` + `@agent.register_for_llm()` |\n| \"human agent\" | UserProxyAgent | `UserProxyAgent(human_input_mode=\"ALWAYS\", ...)` |\n| \"manager agent\" | GroupChatManager | `GroupChatManager(groupchat=...)` |\n\n#### Communication Patterns\n\n| User Says | Precise Translation | Implementation |\n|-----------|---------------------|----------------|\n| \"agent sends message\" | send() method | `agent.send(message, recipient)` |\n| \"start conversation\" | initiate_chat() | `agent.initiate_chat(recipient, message=...)` |\n| \"group discussion\" | GroupChat | `GroupChat(agents=[...], ...)` + `manager.initiate_chat()` |\n| \"agent responds\" | generate_reply() or receive() | Automatic in conversation loop |\n\n#### Tool Integration\n\n| User Says | Precise Translation | Implementation |\n|-----------|---------------------|----------------|\n| \"add a tool\" | register_for_llm() decorator | `@assistant.register_for_llm()` |\n| \"execute function\" | register_for_execution() | `@user_proxy.register_for_execution()` |\n| \"call external API\" | Custom function + registration | Define function, register for both LLM and execution |\n\n### Langroid-Specific Translations\n\n#### Agent Creation\n\n| User Says | Precise Translation | Implementation |\n|-----------|---------------------|----------------|\n| \"create agent\" | ChatAgent instantiation | `ChatAgent(config=ChatAgentConfig(...))` |\n| \"agent with tools\" | ToolAgent or @tool decorator | Define ToolMessage subclass or use @tool |\n| \"configure agent\" | ChatAgentConfig | Set llm, vecdb, system_message in config |\n\n#### Task Orchestration\n\n| User Says | Precise Translation | Implementation |\n|-----------|---------------------|----------------|\n| \"run agent\" | Task.run() | `task = Task(agent, ...)` + `task.run()` |\n| \"agent delegates\" | add_subtask() | `parent_task.add_subtask(child_task)` |\n| \"multi-agent flow\" | Task hierarchy | Parent task with multiple subtasks |\n| \"sequential agents\" | Chain tasks | Task A â†’ Task B â†’ Task C via delegation |\n\n#### Message Handling\n\n| User Says | Precise Translation | Implementation |\n|-----------|---------------------|----------------|\n| \"agent responds\" | agent_response() | Override in custom agent |\n| \"LLM generates text\" | llm_response() | Call llm.generate() or agent.llm_response() |\n| \"send message\" | handle_message() | Pass message to agent.handle_message() |\n| \"tool call\" | ToolMessage | Define ToolMessage subclass |\n\n## Scope Ambiguities\n\n### \"add validation\"\n\n| Interpretation | Context | Implementation |\n|----------------|---------|----------------|\n| Input validation | User input, API parameters | Pydantic, Zod, JSON Schema |\n| Business logic validation | Rules, constraints | Custom validation functions |\n| Schema validation | Database, API contracts | ORM validators, OpenAPI |\n| Type validation | Static typing | TypeScript, mypy, type hints |\n\n### \"improve performance\"\n\n| Interpretation | Metric | Approach |\n|----------------|--------|----------|\n| Reduce latency | Response time | Caching, optimize queries, async |\n| Increase throughput | Requests/second | Load balancing, horizontal scaling |\n| Reduce memory | Memory usage | Optimize data structures, streaming |\n| Faster startup | Cold start time | Lazy loading, pre-compilation |\n\n### \"add logging\"\n\n| Interpretation | Scope | Implementation |\n|----------------|-------|----------------|\n| Debug logging | Development | Verbose logs, debug level |\n| Application logging | Production monitoring | Structured logs, info level |\n| Audit logging | Compliance, security | Immutable logs, audit trail |\n| Performance logging | Metrics, tracing | APM tools, distributed tracing |\n\n## Reference Ambiguities\n\n### Unclear Antecedents\n\n| User Says | Problem | Solution |\n|-----------|---------|----------|\n| \"use that\" | What does \"that\" refer to? | Ask: \"Which component/variable?\" |\n| \"like before\" | Which previous instance? | Ask: \"Like in which file/section?\" |\n| \"the thing\" | Non-specific reference | Ask: \"Which specific thing?\" |\n| \"it\" (multiple possible referents) | Ambiguous pronoun | Ask: \"Do you mean [A] or [B]?\" |\n\n### Generic Terms\n\n| Generic Term | Needs Clarification | Ask |\n|--------------|---------------------|-----|\n| \"component\" | UI component? Module? Class? | \"What kind of component?\" |\n| \"service\" | Backend service? API? Microservice? | \"What type of service?\" |\n| \"module\" | Code module? npm package? Python module? | \"What kind of module?\" |\n| \"system\" | Entire application? Subsystem? OS? | \"Which system?\" |\n\n## Context Clues for Disambiguation\n\n### Technical Signals\n\n**Framework mentions:**\n- \"Autogen\" â†’ Use Autogen ontology\n- \"Langroid\" â†’ Use Langroid ontology\n- \"React\" â†’ Frontend context\n- \"FastAPI\" â†’ Python backend\n- \"Express\" â†’ Node.js backend\n\n**Technology mentions:**\n- \"Docker\" â†’ Containerization context\n- \"Kubernetes\" â†’ Orchestration context\n- \"AWS/Azure/GCP\" â†’ Cloud context\n- \"PostgreSQL/MongoDB\" â†’ Database context\n\n**Action verbs:**\n- \"deploy\" â†’ DevOps context\n- \"render\" â†’ Frontend context\n- \"query\" â†’ Database/API context\n- \"compile\" â†’ Build context\n\n### Domain-Crossing Signals\n\n**Mixed terminology indicates need for clarification:**\n- Business + Technical: \"customer agent\" (CRM agent? AI agent?)\n- Frontend + Backend: \"API component\" (API endpoint? API client component?)\n- Infrastructure + Application: \"service container\" (Docker container? DI container?)\n\n**Clarify domain before translating.**\n\n## Translation Confidence Scoring\n\n### High Confidence (> 0.8)\n\n**Can translate directly with brief confirmation:**\n- Clear domain context present\n- User used framework-specific terminology\n- Single viable interpretation\n- Recent conversation provides context\n\n**Example:**\n```\nUser: \"I'm using Autogen. How do I make the ConversableAgent send a message?\"\nâ†’ HIGH confidence: send() method is the answer\nâ†’ Response: \"Use agent.send(message, recipient). Like this: ...\"\n```\n\n### Medium Confidence (0.5 - 0.8)\n\n**Present 2-3 options, ranked:**\n- Domain clear but term ambiguous within domain\n- Multiple viable interpretations\n- Context provides some hints\n\n**Example:**\n```\nUser: \"In Autogen, how do I create an agent?\"\nâ†’ MEDIUM confidence: Could be ConversableAgent, AssistantAgent, or UserProxyAgent\nâ†’ Response: \"Depends on the agent's purpose:\n   1. ConversableAgent (most common for chat)\n   2. AssistantAgent (if you need tool calling)\n   3. UserProxyAgent (if human-in-the-loop)\n   Which type do you need?\"\n```\n\n### Low Confidence (< 0.5)\n\n**Ask open-ended clarification:**\n- Domain unclear\n- Term very generic\n- Little context available\n\n**Example:**\n```\nUser: \"How do I make it work?\"\nâ†’ LOW confidence: No idea what \"it\" refers to or what \"work\" means\nâ†’ Response: \"Could you provide more details?\n   - What are you trying to accomplish?\n   - What isn't working currently?\n   - Which framework/technology are you using?\"\n```\n\n## Clarification Templates by Confidence\n\n### High Confidence Template\n\n```\n\"[Technical term] is done using [precise implementation].\n\nExample:\n[code snippet]\n\nIs this what you were looking for?\"\n```\n\n### Medium Confidence Template\n\n```\n\"[Ambiguous term] could mean a few things in [domain]:\n\n1. **[Option 1]** - [Brief description]\n   - Use case: [when to use]\n   - Implementation: [technical term]\n\n2. **[Option 2]** - [Brief description]\n   - Use case: [when to use]\n   - Implementation: [technical term]\n\n3. **[Option 3]** - [Brief description]\n   - Use case: [when to use]\n   - Implementation: [technical term]\n\nWhich scenario matches your needs?\"\n```\n\n### Low Confidence Template\n\n```\n\"I want to make sure I understand correctly. Could you clarify:\n- [Specific question 1]\n- [Specific question 2]\n- [Specific question 3]\n\nThis will help me provide accurate guidance.\"\n```\n\n## Summary\n\nUse these translation patterns to:\n\n1. **Quickly identify** common ambiguous terms\n2. **Map to precise** technical concepts\n3. **Determine confidence** level for translation\n4. **Select appropriate** clarification template\n5. **Present options** based on domain and context\n\nThe patterns catalog grows with usage - add new ambiguities as they're encountered.\n"
      },
      "plugins": [
        {
          "name": "profile-creator",
          "source": "./plugins/profile-creator",
          "description": "Transforms messy human intent and repository analysis into living operational domain profiles through 6-phase knowledge engineering pipeline",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add jcmrs/jcmrs-plugins",
            "/plugin install profile-creator@jcmrs-plugins"
          ]
        },
        {
          "name": "running-log",
          "source": "./plugins/running-log",
          "description": "Persistent schema-driven running log capturing ideas, consultations, and Claude's reasoning patterns for cross-session learning and process memory",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add jcmrs/jcmrs-plugins",
            "/plugin install running-log@jcmrs-plugins"
          ]
        },
        {
          "name": "gemini-consult",
          "source": "./plugins/gemini-consult",
          "description": "Leverage Google Gemini's CLI for analyzing large codebases beyond typical context limits using @ syntax for file/directory inclusion",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add jcmrs/jcmrs-plugins",
            "/plugin install gemini-consult@jcmrs-plugins"
          ]
        },
        {
          "name": "semantic-linguist",
          "source": "./plugins/semantic-linguist",
          "description": "Semantic translation between natural language and technical precision across 8 domains: Autogen, Langroid, MCP, UTCP, FastAPI, Git/Gitflow, SRE, and Memory Graphs",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add jcmrs/jcmrs-plugins",
            "/plugin install semantic-linguist@jcmrs-plugins"
          ]
        },
        {
          "name": "claude-pms",
          "source": "./plugins/claude-pms",
          "description": "Procedural Memory System - Learns procedures, processes, and workflows from conversational history through three-tier architecture (Episodic â†’ Semantic â†’ Procedural). Shame on you.",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add jcmrs/jcmrs-plugins",
            "/plugin install claude-pms@jcmrs-plugins"
          ]
        },
        {
          "name": "docs-reader",
          "source": "./plugins/docs-reader",
          "description": "Documentation Reader for Axivo Claude Collaboration Platform, providing access through components, protocols, and competencies",
          "categories": [],
          "install_commands": [
            "/plugin marketplace add jcmrs/jcmrs-plugins",
            "/plugin install docs-reader@jcmrs-plugins"
          ]
        }
      ]
    }
  ]
}